#index 454613
#* Proceedings of the 22th International Conference on Very Large Data Bases
#@ T. M. Vijayaraman;Alejandro P. Buchmann;C. Mohan;Nandlal L. Sarda
#t 1996
#c 4

#index 454614
#* Proceedings of the 23rd International Conference on Very Large Data Bases
#@ Matthias Jarke;Michael J. Carey;Klaus R. Dittrich;Frederick H. Lochovsky;Pericles Loucopoulos;Manfred A. Jeusfeld
#t 1997
#c 4

#index 454616
#* Proceedings of the 25th International Conference on Very Large Data Bases
#@ Malcolm P. Atkinson;Maria E. Orlowska;Patrick Valduriez;Stanley B. Zdonik;Michael L. Brodie
#t 1999
#c 4

#index 454618
#* Proceedings of the 27th International Conference on Very Large Data Bases
#@ Peter M. G. Apers;Paolo Atzeni;Stefano Ceri;Stefano Paraboschi;Kotagiri Ramamohanarao;Richard Thomas Snodgrass
#t 2001
#c 4

#index 479448
#* Using Versions in Update Transactions: Application to Integrity Checking
#@ François Llirbat;Eric Simon;Dimitri Tombroff
#t 1997
#c 4
#% 9241
#% 102754
#% 116055
#% 190645
#% 201869
#% 268750
#% 318392
#% 411564
#% 411741
#% 462961
#% 463261
#% 480288

#index 479449
#* Don't Scrap It, Wrap It! A Wrapper Architecture for Legacy Data Sources
#@ Mary Tork Roth;Peter M. Schwarz
#t 1997
#c 4
#% 43162
#% 111910
#% 111913
#% 136740
#% 168676
#% 169060
#% 194963
#% 210178
#% 380441
#% 459241
#% 463919
#% 479452
#% 479937
#% 481923
#% 614579

#index 479450
#* Fast Computation of Sparse Datacubes
#@ Kenneth A. Ross;Divesh Srivastava
#t 1997
#c 4
#% 210182
#% 227880
#% 464215
#% 481951

#index 479451
#* Merging Ranks from Heterogeneous Internet Sources
#@ Luis Gravano;Hector Garcia-Molina
#t 1997
#c 4
#% 67565
#% 194246
#% 210172
#% 213981
#% 227891
#% 459241
#% 533355

#index 479452
#* Optimizing Queries Across Diverse Data Sources
#@ Laura M. Haas;Donald Kossmann;Edward L. Wimmers;Jun Yang
#t 1997
#c 4
#% 32889
#% 43162
#% 58377
#% 152980
#% 169061
#% 210177
#% 213982
#% 380441
#% 411554
#% 463919
#% 464203
#% 479449
#% 480273
#% 480788
#% 481101
#% 481923
#% 481930
#% 481939
#% 482067
#% 614579

#index 479453
#* Spatial Joins Using R-trees: Breadth-First Traversal with Global Optimizations
#@ Yun-Wu Huang;Ning Jing;Elke A. Rundensteiner
#t 1997
#c 4
#% 13041
#% 86950
#% 152937
#% 153260
#% 210186
#% 210187
#% 285932
#% 427199
#% 462057
#% 462957
#% 463595
#% 480093
#% 503853
#% 527162

#index 479454
#* Data Manager for Evolvable Real-time Command and Control Systems
#@ Eric Hughes;Roman Ginis;Bhavani M. Thuraisingham;Peter C. Krupp;John A. Maurer
#t 1997
#c 4
#% 235914
#% 314501
#% 561901

#index 479455
#* Efficient Testing of High Performance Transaction Processing Systems
#@ D. Wildfogel;Ramana Yerneni
#t 1997
#c 4
#% 43201
#% 114582
#% 479589
#% 480258
#% 602679

#index 479456
#* The Microsoft Repository
#@ Philip A. Bernstein;Brian Harry;Paul Sanders;David Shutt;Jason Zander
#t 1997
#c 4
#% 82258
#% 161102
#% 172054
#% 184573
#% 201869
#% 236243
#% 380441
#% 435142
#% 481446

#index 479457
#* 1-Safe Algorithms for Symmetric Site Configurations
#@ Rune Humborstad;Maitrayi Sabaratnam;Svein-Olaf Hvasshovd;Øystein Torbjørnsen
#t 1997
#c 4
#% 91620
#% 116070
#% 291888
#% 463442
#% 480427
#% 481590

#index 479458
#* Mining Insurance Data at Swiss Life
#@ Jörg-Uwe Kietz;Ulrich Reimer;Martin Staudt
#t 1997
#c 4
#% 103267
#% 136350
#% 163545
#% 451052
#% 459008

#index 479459
#* A Language for Manipulating Arrays
#@ Arunprasad P. Marathe;Kenneth Salem
#t 1997
#c 4
#% 91027
#% 137862
#% 201873
#% 210184
#% 442705
#% 442826
#% 480253
#% 481428

#index 479460
#* Groupwise Processing of Relational Queries
#@ Damianos Chatziantoniou;Kenneth A. Ross
#t 1997
#c 4
#% 36117
#% 115661
#% 136740
#% 191175
#% 201883
#% 210183
#% 214667
#% 287005
#% 288619
#% 346845
#% 458550
#% 461897
#% 461921
#% 463735
#% 480091
#% 481288
#% 481604
#% 481608
#% 482067
#% 482082

#index 479461
#* Parallel Query Scheduling and Optimization with Time- and Space-Shared Resources
#@ Minos N. Garofalakis;Yannis E. Ioannidis
#t 1997
#c 4
#% 3771
#% 115661
#% 116040
#% 116041
#% 118673
#% 136740
#% 158047
#% 172907
#% 188719
#% 210199
#% 211569
#% 213861
#% 213976
#% 339717
#% 411554
#% 442700
#% 480954
#% 481104
#% 481289
#% 481775
#% 481784

#index 479462
#* M-tree: An Efficient Access Method for Similarity Search in Metric Spaces
#@ Paolo Ciaccia;Marco Patella;Pavel Zezula
#t 1997
#c 4
#% 36672
#% 86950
#% 169940
#% 172949
#% 193239
#% 201893
#% 227937
#% 427199
#% 434753
#% 460862
#% 480093
#% 481279
#% 481460
#% 481599
#% 481956

#index 479463
#* Dynamic Memory Adjustment for External Mergesort
#@ Weiye Zhang;Per-Åke Larson
#t 1997
#c 4
#% 83132
#% 136740
#% 152913
#% 172911
#% 481271
#% 481275

#index 479464
#* Principles of Optimally Placing Data in Tertiary Storage Libraries
#@ Stavros Christodoulakis;Peter Triantafillou;Fenia Zioga
#t 1997
#c 4
#% 91633
#% 199455
#% 248018
#% 249264
#% 374002
#% 443345

#index 479465
#* DataGuides: Enabling Query Formulation and Optimization in Semistructured Databases
#@ Roy Goldman;Jennifer Widom
#t 1997
#c 4
#% 82353
#% 139837
#% 210214
#% 236416
#% 395735
#% 442665
#% 462062
#% 463919
#% 464724
#% 481276
#% 481602
#% 482087

#index 479466
#* Caprera: An Activity Framework for Transaction Processing on Wide-Area Networks
#@ Suresh Kumar;Eng-Kee Kwang;Divyakant Agrawal
#t 1997
#c 4
#% 9241
#% 32897
#% 114582
#% 122911
#% 286836
#% 403195

#index 479467
#* The Case for Enhanced Abstract Data Types
#@ Praveen Seshadri;Miron Livny;Raghu Ramakrishnan
#t 1997
#c 4
#% 116090
#% 172939
#% 201936
#% 210178
#% 210203
#% 320232
#% 395735
#% 442705
#% 462643
#% 464007
#% 464041
#% 464053
#% 480759
#% 481101
#% 481428
#% 481915
#% 564428
#% 565457
#% 603136
#% 648890

#index 479468
#* The Oracle Universal Server Buffer
#@ William Bridge;Ashok Joshi;M. Keihl;Tirthankar Lahiri;Juan Loaiza;N. MacNaughton
#t 1997
#c 4
#% 481937

#index 479469
#* Integrating SQL Databases with Content-Specific Search Engines
#@ Stefan Deßloch;Nelson Mendonça Mattos
#t 1997
#c 4
#% 67565
#% 108978
#% 111372
#% 152940
#% 168676
#% 194253
#% 211931
#% 224702
#% 227864
#% 286178
#% 286181
#% 380441
#% 427199
#% 464007
#% 480286
#% 481101
#% 481599
#% 481919
#% 565447

#index 479470
#* Incremental Organization for Data Recording and Warehousing
#@ H. V. Jagadish;P. P. S. Narayan;S. Seshadri;S. Sudarshan;Rama Kanneganti
#t 1997
#c 4
#% 83183
#% 114582
#% 198467
#% 208047
#% 287672

#index 479471
#* To Weave the Web
#@ Paolo Atzeni;Giansalvatore Mecca;Paolo Merialdo
#t 1997
#c 4
#% 38696
#% 42401
#% 83228
#% 116091
#% 161722
#% 168676
#% 210214
#% 237194
#% 384978
#% 395735
#% 464720
#% 481602
#% 481923
#% 614598

#index 479472
#* A Region Splitting Strategy for Physical Database Design of Multidimensional File Organizations
#@ Jong-Hak Lee;Young-Koo Lee;Kyu-Young Whang;Il-Yeol Song
#t 1997
#c 4
#% 1748
#% 32898
#% 36119
#% 77928
#% 88056
#% 140389
#% 285932
#% 321455
#% 411694
#% 435124
#% 442868
#% 452800
#% 480587
#% 510675

#index 479473
#* A Generic Approach to Bulk Loading Multidimensional Index Structures
#@ Jochen Van den Bercken;Bernhard Seeger;Peter Widmayer
#t 1997
#c 4
#% 2115
#% 41684
#% 58371
#% 77928
#% 86950
#% 153260
#% 172909
#% 214597
#% 286237
#% 427199
#% 435141
#% 480587
#% 481428
#% 481599
#% 527012
#% 571296

#index 479474
#* Logical and Physical Versioning in Main Memory Databases
#@ Rajeev Rastogi;S. Seshadri;Philip Bohannon;Dennis W. Leinbaugh;Abraham Silberschatz;S. Sudarshan
#t 1997
#c 4
#% 36118
#% 116055
#% 116087
#% 287797
#% 408638
#% 411741
#% 416048
#% 427195
#% 442707
#% 463261
#% 479769
#% 480589
#% 480959
#% 481454

#index 479475
#* Parallel Algorithms for High-dimensional Similarity Joins for Data Mining Applications
#@ John C. Shafer;Rakesh Agrawal
#t 1997
#c 4
#% 13032
#% 86951
#% 152937
#% 172949
#% 210186
#% 210187
#% 227932
#% 285932
#% 415957
#% 442700
#% 443397
#% 460862
#% 462070
#% 464205
#% 481609
#% 527012

#index 479476
#* Materialized Views Selection in a Multidimensional Database
#@ Elena Baralis;Stefano Paraboschi;Ernest Teniente
#t 1997
#c 4
#% 201928
#% 208037
#% 210182
#% 210208
#% 462204
#% 464215
#% 464706
#% 481948
#% 481951

#index 479477
#* Distributed Processing over Stand-alone Systems and Applications
#@ Gustavo Alonso;Claus Hagen;Hans-Jörg Schek;Markus Tresch
#t 1997
#c 4
#% 210196
#% 237199
#% 340308
#% 340614
#% 459005
#% 481955
#% 527036

#index 479478
#* Towards an ODMG-Compliant Visual Object Query Language
#@ Manoj Chavda;Peter T. Wood
#t 1997
#c 4
#% 32904
#% 119814
#% 152949
#% 157133
#% 157234
#% 164415
#% 236256
#% 380441
#% 442703
#% 442887
#% 452804

#index 479479
#* Effective Memory Use in a Media Server
#@ Edward Y. Chang;Hector Garcia-Molina
#t 1997
#c 4
#% 151340
#% 159079
#% 159084
#% 193237
#% 201679
#% 219916
#% 464031
#% 481438
#% 649918
#% 661673
#% 1797486

#index 479480
#* Architecture of Oracle Parallel Server
#@ Roger Bamford;D. Butler;Boris Klots;N. MacNaughton
#t 1998
#c 4

#index 479481
#* An Asynchronous Avoidance-Based Cache Consistency Algorithm for Client Caching DBMSs
#@ M. Tamer Özsu;Kaladhar Voruganti;Ronald C. Unrau
#t 1998
#c 4
#% 27057
#% 83126
#% 102802
#% 102803
#% 172220
#% 172936
#% 201871
#% 210177
#% 235017
#% 240229
#% 243742
#% 704855

#index 479482
#* Ratio Rules: A New Paradigm for Fast, Quantifiable Data Mining
#@ Flip Korn;Alexandros Labrinidis;Yannis Kotidis;Christos Faloutsos
#t 1998
#c 4
#% 124009
#% 132779
#% 136350
#% 152934
#% 201894
#% 210160
#% 216499
#% 227919
#% 406493
#% 443082
#% 443087
#% 443091
#% 443092
#% 452821
#% 481290
#% 481588
#% 481754
#% 481758

#index 479483
#* Design, Implementation, and Performance of the LHAM Log-Structured History Data Access Method
#@ Peter Muth;Patrick E. O'Neil;Achim Pick;Gerhard Weikum
#t 1998
#c 4
#% 32910
#% 58371
#% 86953
#% 108512
#% 131555
#% 182672
#% 194942
#% 208047
#% 227864
#% 427199
#% 462804
#% 463749
#% 479470
#% 479473
#% 481256
#% 571296

#index 479484
#* Scalable Techniques for Mining Causal Structures
#@ Craig Silverstein;Sergey Brin;Rajeev Motwani;Jeffrey D. Ullman
#t 1998
#c 4
#% 129987
#% 152934
#% 227919
#% 420055
#% 420059
#% 1650659

#index 479485
#* Heterogeneous Database Query Optimization in DB2 Universal DataJoiner
#@ Shivakumar Venkataraman;Tian Zhang
#t 1998
#c 4
#% 85086
#% 116043
#% 479449
#% 479452
#% 479651
#% 480788

#index 479486
#* Fast High-Dimensional Data Search in Incomplete Databases
#@ Beng Chin Ooi;Cheng Hian Goh;Kian-Lee Tan
#t 1998
#c 4
#% 77648
#% 86950
#% 100831
#% 121989
#% 169940
#% 227937
#% 227939
#% 285932
#% 339614
#% 382477
#% 427199
#% 435141
#% 463414
#% 480093
#% 480967
#% 481780
#% 481786
#% 481956

#index 479487
#* Bank of America Case Study: The Information Currency Advantage
#@ Felipe Cariño;Mark Jahnke
#t 1998
#c 4
#% 115684
#% 191405
#% 462069
#% 481099
#% 499803

#index 479488
#* Plan-Per-Tuple Optimization Solution - Parallel Execution of Expensive User-Defined Functions
#@ Felipe Cariño;William O'Connell
#t 1998
#c 4
#% 32890
#% 116043
#% 152940
#% 172900
#% 191405
#% 210172
#% 249985
#% 252368
#% 411554
#% 481099
#% 481915
#% 481930
#% 499803

#index 479489
#* DMS: A Parallel Data Mining Server
#@ Felicity George
#t 1998
#c 4

#index 479490
#* Querying Continuous Time Sequences
#@ Ling Lin;Tore Risch
#t 1998
#c 4
#% 32915
#% 172939
#% 172950
#% 201923
#% 214602
#% 214722
#% 240153
#% 240193
#% 287221
#% 380546
#% 384872
#% 411554
#% 460862
#% 461885
#% 464196
#% 467630
#% 479467
#% 479914
#% 480096
#% 481609
#% 481611
#% 482088

#index 479617
#* Hash Joins and Hash Teams in Microsoft SQL Server
#@ Goetz Graefe;Ross Bunker;Shaun Cooper
#t 1998
#c 4
#% 3200
#% 32910
#% 36360
#% 77963
#% 136740
#% 210206
#% 411554
#% 427195
#% 463759
#% 476622
#% 479753
#% 479905
#% 480590
#% 480595

#index 479618
#* nD-SQL: A Multi-Dimensional Language for Interoperability and OLAP
#@ Frédéric Gingras;Laks V. S. Lakshmanan
#t 1998
#c 4
#% 64148
#% 102748
#% 111913
#% 116091
#% 123121
#% 158908
#% 169057
#% 213969
#% 223781
#% 227880
#% 227989
#% 380441
#% 435111
#% 459024
#% 464203
#% 464215
#% 479452
#% 480788
#% 481096
#% 481944
#% 481951
#% 482093
#% 535964

#index 479619
#* MapInfo SpatialWare: A Spatial Information Server for RDBMS
#@ Chebel Mina
#t 1998
#c 4

#index 479620
#* Algorithms for Querying by Spatial Structure
#@ Dimitris Papadias;Nikos Mamoulis;Vasilis Delis
#t 1998
#c 4
#% 13041
#% 75817
#% 86950
#% 116335
#% 152937
#% 172908
#% 181409
#% 201880
#% 239575
#% 261903
#% 319244
#% 427199
#% 435140
#% 443054
#% 443133
#% 462957
#% 463595
#% 464205
#% 479453
#% 534159
#% 534160

#index 479621
#* Expiring Data in a Warehouse
#@ Hector Garcia-Molina;Wilburt Labio;Jun Yang
#t 1998
#c 4
#% 13016
#% 53390
#% 54712
#% 59350
#% 152928
#% 198467
#% 201929
#% 210211
#% 340301
#% 442767
#% 458556
#% 481786
#% 564419

#index 479622
#* Issues in Developing Very Large Data Warehouses
#@ Lyman Do;Pamela Drew;Wei Jin;Vish Jumani;David Van Rossum
#t 1998
#c 4
#% 83933
#% 201928
#% 227944
#% 340300
#% 340301

#index 479623
#* Reducing the Braking Distance of an SQL Query Engine
#@ Michael J. Carey;Donald Kossmann
#t 1998
#c 4
#% 82346
#% 115661
#% 116084
#% 191175
#% 210172
#% 210190
#% 227894
#% 286916
#% 287300
#% 340670
#% 427219
#% 480774
#% 480966
#% 481439
#% 482092

#index 479624
#* A Database System for Real-Time Event Aggregation in Telecommunication
#@ Jerry Baulier;Stephen Blott;Henry F. Korth;Abraham Silberschatz
#t 1998
#c 4
#% 198467
#% 481454

#index 479625
#* Is Web-site Management a Database Problem?
#@ Daniela Florescu;Alon Y. Levy;Dan Suciu
#t 1998
#c 4

#index 479626
#* Technology and the Future of Commerce and Finance (Abstract)
#@ David Elliot Shaw
#t 1998
#c 4

#index 479627
#* On the Discovery of Interesting Patterns in Association Rules
#@ Sridhar Ramaswamy;Sameer Mahajan;Abraham Silberschatz
#t 1998
#c 4
#% 42401
#% 152934
#% 201894
#% 210160
#% 210162
#% 232136
#% 319244
#% 463738
#% 464839
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779

#index 479628
#* The Drill Down Benchmark
#@ Peter A. Boncz;Tim Rühl;Fred Kwakkel
#t 1998
#c 4
#% 442832
#% 464816
#% 481290
#% 566122

#index 479629
#* Incremental Maintenance for Materialized Views over Semistructured Data
#@ Serge Abiteboul;Jason McHugh;Michael Rys;Vasilis Vassalos;Janet L. Wiener
#t 1998
#c 4
#% 13015
#% 13016
#% 32914
#% 102780
#% 152928
#% 169812
#% 172927
#% 201929
#% 210205
#% 210210
#% 234797
#% 236416
#% 237191
#% 238413
#% 340295
#% 395735
#% 443137
#% 458608
#% 462062
#% 462213
#% 462235
#% 463919
#% 464222
#% 464720
#% 464724
#% 479465
#% 480958
#% 481602
#% 481623
#% 481955
#% 482083
#% 562141
#% 562293
#% 564207

#index 479630
#* Small Materialized Aggregates: A Light Weight Index Structure for Data Warehousing
#@ Guido Moerkotte
#t 1998
#c 4
#% 136740
#% 207552
#% 208037
#% 210182
#% 223781
#% 227861
#% 287317
#% 317933
#% 322974
#% 427199
#% 464215
#% 466953
#% 480091
#% 481948

#index 479631
#* Oracle Industrial Exhibit
#@ Amy Pogue
#t 1998
#c 4

#index 479632
#* DataBlitz: A High Performance Main-Memory Storage Manager
#@ Jerry Baulier;Philip Bohannon;S. Gogate;S. Joshi;C. Gupta;A. Khivesera;Henry F. Korth;Peter McIlroy;J. Miller;P. P. S. Narayan;M. Nemeth;Rajeev Rastogi;Abraham Silberschatz;S. Sudarshan
#t 1998
#c 4

#index 479633
#* A Single Pass Computing Engine for Interactive Analysis of VLDBs
#@ Ted Mihalisin
#t 1998
#c 4

#index 479634
#* Inferring Function Semantics to Optimize Queries
#@ Mitch Cherniack;Stanley B. Zdonik
#t 1998
#c 4
#% 32890
#% 82858
#% 116043
#% 135438
#% 210203
#% 248789
#% 287005
#% 395735
#% 462060
#% 464053
#% 464364
#% 481101
#% 481293
#% 565457
#% 709312

#index 479635
#* Starting (and Sometimes Ending) a Database Company
#@ Jack A. Orenstein
#t 1998
#c 4

#index 479636
#* Filtering with Approximate Predicates
#@ Narayanan Shivakumar;Hector Garcia-Molina;Chandra Chekuri
#t 1998
#c 4
#% 36683
#% 46803
#% 77944
#% 152940
#% 164369
#% 201935
#% 204673
#% 210187
#% 210207
#% 210353
#% 408396
#% 411554
#% 437405
#% 443052
#% 481101
#% 481915
#% 482116
#% 656735

#index 479637
#* The Cubetree Storage Organization
#@ Nick Roussopoulos;Yannis Kotidis
#t 1998
#c 4

#index 479638
#* The ADABAS Buffer Pool Manager
#@ Harald Schöning
#t 1998
#c 4
#% 735
#% 152943
#% 244119
#% 458561
#% 479468
#% 566110

#index 479639
#* Safely and Efficiently Updating References During On-line Reorganization
#@ Chendong Zou;Betty Salzberg
#t 1998
#c 4
#% 104926
#% 114582
#% 116086
#% 210174
#% 210175
#% 442860
#% 458528
#% 463264
#% 480589
#% 480939

#index 479640
#* PUBLIC: A Decision Tree Classifier that Integrates Building and Pruning
#@ Rajeev Rastogi;Kyuseok Shim
#t 1998
#c 4
#% 42994
#% 61792
#% 129301
#% 136350
#% 191910
#% 369349
#% 380342
#% 449588
#% 452821
#% 459008
#% 479787
#% 480940
#% 481945
#% 481949

#index 479641
#* Checkpointing in Oracle
#@ Ashok Joshi;William Bridge;Juan Loaiza;Tirthankar Lahiri
#t 1998
#c 4
#% 117

#index 479642
#* Determining Text Databases to Search in the Internet
#@ Weiyi Meng;King-Lup Liu;Clement T. Yu;Xiaodong Wang;Yuhsi Chang;Naphtali Rishe
#t 1998
#c 4
#% 1049
#% 67565
#% 176501
#% 194246
#% 194275
#% 287015
#% 287459
#% 359132
#% 479451
#% 481748
#% 567255
#% 978381

#index 479643
#* Algorithms for Mining Association Rules for Binary Segmentations of Huge Categorical Databases
#@ Yasuhiko Morimoto;Takeshi Fukuda;Hirofumi Matsuzawa;Takeshi Tokuyama;Kunikazu Yoda
#t 1998
#c 4
#% 2115
#% 8949
#% 66937
#% 136350
#% 152934
#% 182570
#% 201894
#% 210162
#% 213977
#% 449588
#% 481290
#% 481588
#% 481949

#index 479644
#* IBM's DB2 Universal Database demonstrations at VLDB'98
#@ K. Bernhard Schiefer;Jim Kleewein;Karen Brannon;Guy M. Lohman;Gene Fuh
#t 1998
#c 4

#index 479645
#* On Optimal Node Splitting for R-trees
#@ Yván J. García;Mario A. Lopez;Scott T. Leutenegger
#t 1998
#c 4
#% 86950
#% 153260
#% 286237
#% 427199
#% 462059
#% 462218
#% 480093
#% 481455
#% 1837577

#index 479646
#* Materialized View Selection for Multidimensional Datasets
#@ Amit Shukla;Prasad Deshpande;Jeffrey F. Naughton
#t 1998
#c 4
#% 210182
#% 227880
#% 248806
#% 462204
#% 463760
#% 464215
#% 464706
#% 479450
#% 479476
#% 481948
#% 481951

#index 479647
#* Secure Buffering in Firm Real-Time Database Systems
#@ Binto George;Jayant R. Haritsa
#t 1998
#c 4
#% 735
#% 117903
#% 124815
#% 227954
#% 288821
#% 322637
#% 403195
#% 507390

#index 479648
#* Optimal Histograms with Quality Guarantees
#@ H. V. Jagadish;Nick Koudas;S. Muthukrishnan;Viswanath Poosala;Kenneth C. Sevcik;Torsten Suel
#t 1998
#c 4
#% 54047
#% 82346
#% 116084
#% 201921
#% 210190
#% 408237
#% 427219
#% 479931
#% 481266
#% 482092

#index 479649
#* A Quantitative Analysis and Performance Study for Similarity-Search Methods in High-Dimensional Spaces
#@ Roger Weber;Hans-Jörg Schek;Stephen Blott
#t 1998
#c 4
#% 1921
#% 23321
#% 36683
#% 68091
#% 86950
#% 88056
#% 164360
#% 172908
#% 227856
#% 227939
#% 237187
#% 237204
#% 285932
#% 317313
#% 317380
#% 317950
#% 359751
#% 411694
#% 427199
#% 435141
#% 437405
#% 458741
#% 463425
#% 479462
#% 480093
#% 481956
#% 527026

#index 479650
#* Resource Scheduling for Composite Multimedia Objects
#@ Minos N. Garofalakis;Yannis E. Ioannidis;Banu Özden
#t 1998
#c 4
#% 25998
#% 36117
#% 113701
#% 163240
#% 176305
#% 197761
#% 217814
#% 238169
#% 244328
#% 281846
#% 319244
#% 408396
#% 443496
#% 479461
#% 481438
#% 481766
#% 493866

#index 479651
#* The Heterogeneity Problem and Middleware Technology: Experiences with and Performance of Database Gateways
#@ Fernando de Ferreira Rezende;Klaudia Hergula
#t 1998
#c 4
#% 531907

#index 479652
#* Bridging Heterogeneity: Research and Practice of Database Middleware Technology
#@ Fernando de Ferreira Rezende;Günter Sauter
#t 1998
#c 4

#index 479653
#* A Raster Approximation For Processing of Spatial Joins
#@ Geraldo Zimbrao;Jano Moreira de Souza
#t 1998
#c 4
#% 68091
#% 152937
#% 172908
#% 210186
#% 210187
#% 463425
#% 463595
#% 464205
#% 479453
#% 481941
#% 481956
#% 489213
#% 527012
#% 527162

#index 479654
#* Diag-Join: An Opportunistic Join Algorithm for 1:N Relationships
#@ Sven Helmer;Till Westmann;Guido Moerkotte
#t 1998
#c 4
#% 3771
#% 18614
#% 57955
#% 77936
#% 77963
#% 83232
#% 86928
#% 114577
#% 168251
#% 171732
#% 191154
#% 207552
#% 208037
#% 287489
#% 340670
#% 427195
#% 442918
#% 463759
#% 479753
#% 479905
#% 479924
#% 480272
#% 481417
#% 481435
#% 571094
#% 571218
#% 677322

#index 479655
#* Improving Adaptable Similarity Query Processing by Using Approximations
#@ Mihael Ankerst;Bernhard Braunmüller;Hans-Peter Kriegel;Thomas Seidl
#t 1998
#c 4
#% 86792
#% 86950
#% 102772
#% 158905
#% 169940
#% 172949
#% 227856
#% 227999
#% 237187
#% 248797
#% 252304
#% 421052
#% 427199
#% 443889
#% 460862
#% 463425
#% 480093
#% 481609
#% 481947
#% 481956
#% 482109
#% 527026

#index 479656
#* Massive Stochastic Testing of SQL
#@ Donald R. Slutz
#t 1998
#c 4
#% 69389
#% 138236
#% 235542

#index 479657
#* Selectivity Estimation in Extensible Databases - A Neural Network Approach
#@ M. Seetha Lakshmi;Shaoyu Zhou
#t 1998
#c 4
#% 116084
#% 152917
#% 152940
#% 172902
#% 201921
#% 210189
#% 210190
#% 213975
#% 380546
#% 481749

#index 479658
#* Incremental Clustering for Mining in a Data Warehousing Environment
#@ Martin Ester;Hans-Peter Kriegel;Jörg Sander;Michael Wimmer;Xiaowei Xu
#t 1998
#c 4
#% 86950
#% 210173
#% 227869
#% 420078
#% 435137
#% 443034
#% 452747
#% 459012
#% 464204
#% 479462
#% 481281
#% 481290
#% 482098
#% 527022

#index 479659
#* Clustering Categorical Data: An Approach Based on Dynamical Systems
#@ David Gibson;Jon M. Kleinberg;Prabhakar Raghavan
#t 1998
#c 4
#% 14511
#% 92135
#% 114736
#% 152934
#% 189842
#% 210173
#% 217824
#% 227917
#% 232136
#% 268079
#% 282905
#% 359751
#% 363592
#% 408396
#% 437405
#% 481279
#% 481779
#% 656714

#index 479660
#* The National Medical Knowledge Bank
#@ Warren Sterling
#t 1998
#c 4
#% 191405
#% 210170
#% 444839
#% 592393

#index 479661
#* Low-Cost Compensation-Based Query Processing
#@ Øystein Grøvlen;Svein-Olaf Hvasshovd;Øystein Torbjørnsen
#t 1998
#c 4
#% 115661
#% 116055
#% 116083
#% 159079
#% 174748
#% 201692
#% 227944
#% 403195
#% 411741
#% 463261
#% 463429
#% 479468
#% 481590

#index 479781
#* Bulk-Loading Techniques for Object Databases and an Application to Relational Data
#@ Sihem Amer-Yahia;Sophie Cluet;Claude Delobel
#t 1998
#c 4
#% 36683
#% 83933
#% 102780
#% 116057
#% 125600
#% 210176
#% 223779
#% 223780
#% 235914
#% 458608
#% 459247
#% 462051
#% 464505
#% 480958
#% 481622
#% 535345
#% 681410

#index 479782
#* DTL's DataSpot: Database Exploration Using Plain Language
#@ Shaul Dar;Gadi Entin;Shai Geva;Eran Palmon
#t 1998
#c 4
#% 210214
#% 227995
#% 236416
#% 237193
#% 463919
#% 479803
#% 481602
#% 481923

#index 479783
#* Using Schema Matching to Simplify Heterogeneous Data Translation
#@ Tova Milo;Sagit Zohar
#t 1998
#c 4
#% 22948
#% 90847
#% 111913
#% 166203
#% 172927
#% 198479
#% 201934
#% 210214
#% 248799
#% 458607
#% 462051
#% 463919
#% 464222
#% 464716
#% 464720
#% 480969
#% 481923
#% 504570
#% 527108
#% 535966
#% 581730
#% 614579

#index 479784
#* Performance Measurements of Tertiary Storage Devices
#@ Theodore Johnson;Ethan L. Miller
#t 1998
#c 4
#% 91633
#% 146203
#% 159079
#% 184254
#% 201707
#% 209896
#% 210181
#% 220580
#% 244119
#% 374002
#% 462211
#% 479464
#% 481586
#% 481613
#% 482086
#% 482117
#% 482118
#% 503721
#% 511335
#% 565466
#% 591369
#% 612073
#% 612095
#% 612099

#index 479785
#* Mining Surprising Patterns Using Temporal Description Length
#@ Soumen Chakrabarti;Sunita Sarawagi;Byron Dom
#t 1998
#c 4
#% 116159
#% 172386
#% 203296
#% 227919
#% 232106
#% 232136
#% 246747
#% 369349
#% 443092
#% 452821
#% 464204
#% 464839
#% 481611
#% 592023

#index 479786
#* Design and Analysis of Parametric Query Optimization Algorithms
#@ Sumit Ganguly
#t 1998
#c 4
#% 58375
#% 172900
#% 289148
#% 411554
#% 463444
#% 480955

#index 479787
#* RainForest - A Framework for Fast Decision Tree Construction of Large Datasets
#@ Johannes Gehrke;Raghu Ramakrishnan;Venkatesh Ganti
#t 1998
#c 4
#% 25208
#% 90661
#% 92542
#% 129301
#% 136350
#% 153021
#% 162950
#% 191910
#% 369349
#% 408396
#% 449588
#% 452821
#% 459008
#% 479640
#% 479643
#% 480940
#% 481749
#% 481945
#% 481949
#% 677030

#index 479788
#* MindReader: Querying Databases Through Multiple Examples
#@ Yoshiharu Ishikawa;Ravishankar Subramanya;Christos Faloutsos
#t 1998
#c 4
#% 41230
#% 86950
#% 115473
#% 164360
#% 169940
#% 185265
#% 201893
#% 227939
#% 437509
#% 458521
#% 481956
#% 482109

#index 479789
#* Buffering and Read-Ahead Strategies for External Mergesort
#@ Weiye Zhang;Per-Åke Larson
#t 1998
#c 4
#% 64730
#% 136740
#% 443033
#% 479463
#% 516036

#index 479790
#* Information, Communication, and Money: For What Can We Charge and How Can We Meter It?
#@ Stephen Blott;Henry F. Korth;Abraham Silberschatz
#t 1998
#c 4

#index 479791
#* Algorithms for Mining Distance-Based Outliers in Large Datasets
#@ Edwin M. Knorr;Raymond T. Ng
#t 1998
#c 4
#% 2115
#% 51647
#% 68091
#% 152934
#% 210173
#% 230138
#% 237204
#% 321455
#% 420056
#% 427199
#% 443083
#% 451057
#% 480940
#% 480964
#% 481281
#% 681822

#index 479792
#* Materialized Views in Oracle
#@ Randall G. Bello;Karl Dias;Alan Downing;James J. Feenan, Jr.;James L. Finnerty;William D. Norcott;Harry Sun;Andrew Witkowski;Mohamed Ziauddin
#t 1998
#c 4
#% 13015
#% 152928
#% 172954
#% 227869
#% 462789
#% 464056
#% 481604
#% 482081
#% 565469

#index 479793
#* From Data Independence to Knowledge Independence: An on-going Story
#@ Laurent Vieille
#t 1998
#c 4
#% 87049
#% 152982
#% 362199
#% 384978
#% 395735
#% 560057
#% 565273

#index 479794
#* TOPAZ: a Cost-Based, Rule-Driven, Multi-Phase Parallelizer
#@ Clara Nippl;Bernhard Mitschang
#t 1998
#c 4
#% 43162
#% 83154
#% 83232
#% 115661
#% 116040
#% 152915
#% 159337
#% 188719
#% 201885
#% 213976
#% 217052
#% 248816
#% 435116
#% 442700
#% 442850
#% 461895
#% 464215
#% 479452
#% 479461
#% 480615
#% 481104
#% 481110
#% 481133
#% 481753
#% 481784
#% 481930
#% 482115
#% 509136
#% 1502579

#index 479795
#* Computing Iceberg Queries Efficiently
#@ Min Fang;Narayanan Shivakumar;Hector Garcia-Molina;Rajeev Motwani;Jeffrey D. Ullman
#t 1998
#c 4
#% 2833
#% 36683
#% 46803
#% 69273
#% 201894
#% 204673
#% 210353
#% 227917
#% 255137
#% 287222
#% 481290
#% 504572

#index 479796
#* R-Tree Based Indexing of Now-Relative Bitemporal Data
#@ Rasa Bliujute;Christian S. Jensen;Simonas Saltenis;Giedrius Slivinskas
#t 1998
#c 4
#% 16028
#% 18615
#% 56081
#% 68091
#% 86950
#% 86951
#% 209730
#% 225004
#% 286256
#% 287070
#% 361445
#% 382477
#% 427199
#% 443181
#% 462218
#% 480093
#% 481455
#% 481599
#% 482112
#% 527802
#% 562166

#index 479797
#* Scalable Sweeping-Based Spatial Join
#@ Lars Arge;Octavian Procopiuc;Sridhar Ramaswamy;Torsten Suel;Jeffrey Scott Vitter
#t 1998
#c 4
#% 2115
#% 13041
#% 18614
#% 25924
#% 41684
#% 58369
#% 68091
#% 69316
#% 86950
#% 86952
#% 114709
#% 116065
#% 152937
#% 172909
#% 172911
#% 172962
#% 210185
#% 210186
#% 210187
#% 227914
#% 227932
#% 282908
#% 285932
#% 427199
#% 445701
#% 462331
#% 462957
#% 463436
#% 463595
#% 479453
#% 480093
#% 481304
#% 481309
#% 527012
#% 547451
#% 560837

#index 479798
#* Evaluating Functional Joins Along Nested Reference Sets in Object-Relational and Object-Oriented Databases
#@ Reinhard Braumandl;Jens Claußen;Alfons Kemper
#t 1998
#c 4
#% 5379
#% 18614
#% 23952
#% 83330
#% 86948
#% 86954
#% 111351
#% 136740
#% 164748
#% 172939
#% 235914
#% 339790
#% 442665
#% 463884
#% 481417
#% 481619
#% 481938
#% 489368
#% 571094
#% 571218

#index 479799
#* WaveCluster: A Multi-Resolution Clustering Approach for Very Large Spatial Databases
#@ Gholamhosein Sheikholeslami;Surojit Chatterjee;Aidong Zhang
#t 1998
#c 4
#% 58636
#% 154387
#% 177401
#% 210173
#% 241129
#% 481281
#% 566128

#index 479800
#* Atomicity versus Anonymity: Distributed Transactions for Electronic Commerce
#@ J. D. Tygar
#t 1998
#c 4
#% 63785
#% 78053
#% 191721
#% 223279
#% 239739
#% 281751
#% 319994
#% 403195
#% 456169
#% 484868
#% 656820
#% 978234
#% 978258
#% 978267
#% 978268
#% 978272
#% 978275
#% 978280

#index 479801
#* Binding Propagation in Disjunctive Databases
#@ Sergio Greco
#t 1998
#c 4
#% 11797
#% 23902
#% 58363
#% 77941
#% 103705
#% 114723
#% 154319
#% 181038
#% 231786
#% 235018
#% 368248
#% 384978
#% 455080
#% 464531
#% 473193
#% 493843
#% 499509
#% 556918

#index 479802
#* Active Storage for Large-Scale Data Mining and Multimedia
#@ Erik Riedel;Garth A. Gibson;Christos Faloutsos
#t 1998
#c 4
#% 17857
#% 25017
#% 43172
#% 115661
#% 151540
#% 166984
#% 169940
#% 173051
#% 202140
#% 202141
#% 202153
#% 213470
#% 213487
#% 214986
#% 227914
#% 232806
#% 237187
#% 340670
#% 359751
#% 393784
#% 437405
#% 437509
#% 443091
#% 463597
#% 480281
#% 481290
#% 481956
#% 682384
#% 979348

#index 479803
#* Proximity Search in Databases
#@ Roy Goldman;Narayanan Shivakumar;Suresh Venkatasubramanian;Hector Garcia-Molina
#t 1998
#c 4
#% 67565
#% 172952
#% 175965
#% 213981
#% 219474
#% 236416
#% 327432
#% 408638
#% 463919
#% 479469
#% 479782
#% 481107
#% 493693

#index 479804
#* Aqua: A Fast Decision Support Systems Using Approximate Query Answers
#@ Swarup Acharya;Phillip B. Gibbons;Viswanath Poosala
#t 1999
#c 4
#% 248812
#% 259995
#% 273909
#% 452838
#% 479984
#% 482123
#% 504019

#index 479805
#* Microsoft English Query 7.5: Automatic Extraction of Semantics from Relational Databases and OLAP Cubes
#@ Adam Blum
#t 1999
#c 4

#index 479806
#* Query Optimization for XML
#@ Jason McHugh;Jennifer Widom
#t 1999
#c 4
#% 23959
#% 36188
#% 136740
#% 199776
#% 210205
#% 210214
#% 236409
#% 236416
#% 237191
#% 245788
#% 248819
#% 395735
#% 411554
#% 427219
#% 442942
#% 443137
#% 462060
#% 462235
#% 463919
#% 466953
#% 479465
#% 481121
#% 481623
#% 481938

#index 479807
#* Building Light-Weight Wrappers for Legacy Web Data-Sources Using W4F
#@ Arnaud Sahuguet;Fabien Azavant
#t 1999
#c 4
#% 246350
#% 248808
#% 464825
#% 511897
#% 511902

#index 479808
#* Performance Measurements of Compressed Bitmap Indices
#@ Theodore Johnson
#t 1999
#c 4
#% 118767
#% 191154
#% 227861
#% 248814
#% 462217
#% 466953

#index 479809
#* Curio: A Novel Solution for Efficient Storage and Indexing in Data Warehouses
#@ Anindya Datta;Krithi Ramamritham;Helen M. Thomas
#t 1999
#c 4
#% 208037
#% 210182
#% 223781
#% 227861
#% 466953

#index 479810
#* The New Locking, Logging, and Recovery Architecture of Microsoft SQL Server 7.0
#@ David Campbell
#t 1999
#c 4

#index 479811
#* Unrolling Cycles to Decide Trigger Termination
#@ Sin Yeung Lee;Tok Wang Ling
#t 1999
#c 4
#% 58361
#% 72705
#% 111368
#% 116045
#% 120696
#% 153004
#% 179619
#% 314491
#% 443204
#% 459018
#% 459266
#% 461891
#% 480435
#% 480779
#% 481456

#index 479812
#* Aggregation Algorithms for Very Large Compressed Data Warehouses
#@ Jianzhong Li;Doron Rotem;Jaideep Srivastava
#t 1999
#c 4
#% 2248
#% 60659
#% 113841
#% 136740
#% 146203
#% 210182
#% 211575
#% 227868
#% 227880
#% 248805
#% 420053
#% 442685
#% 442695
#% 462204
#% 481951
#% 482049
#% 482082

#index 479813
#* Quality-driven Integration of Heterogenous Information Systems
#@ Felix Naumann;Ulf Leser;Johann Christoph Freytag
#t 1999
#c 4
#% 85086
#% 116303
#% 172898
#% 242233
#% 242234
#% 242235
#% 242237
#% 287336
#% 464717
#% 464727
#% 471752
#% 481923
#% 482108
#% 535837
#% 912094

#index 479814
#* Implementation of Two Semantic Query Optimization Techniques in DB2 Universal Database
#@ Qi Cheng;Jarek Gryz;Fred Koo;T. Y. Cliff Leung;Linqi Liu;Xiaoyan Qian;K. Bernhard Schiefer
#t 1999
#c 4
#% 69272
#% 116043
#% 172889
#% 210169
#% 210207
#% 427222
#% 442677
#% 442706
#% 462058
#% 463761
#% 480280
#% 481288
#% 481293

#index 479815
#* High-Performance Extensible Indexing
#@ Marcel Kornacker
#t 1999
#c 4
#% 88056
#% 114582
#% 116085
#% 227864
#% 252304
#% 317933
#% 321455
#% 427199
#% 481599

#index 479816
#* Evaluating Top-k Selection Queries
#@ Surajit Chaudhuri;Luis Gravano
#t 1999
#c 4
#% 41230
#% 210172
#% 210190
#% 213981
#% 227894
#% 248010
#% 248797
#% 406493
#% 479451
#% 479623
#% 479642
#% 481947
#% 482092

#index 479817
#* Building Hierarchical Classifiers Using Class Proximity
#@ Ke Wang;Senqiang Zhou;Shiang Chen Liew
#t 1999
#c 4
#% 136350
#% 152934
#% 194283
#% 248810
#% 442814
#% 465747
#% 465754
#% 481290
#% 481588
#% 481758
#% 482113
#% 571073
#% 1499570

#index 479818
#* VOODB: A Generic Discrete-Event Random Simulation Model To Evaluate the Performances of OODBs
#@ Jérôme Darmont;Michel Schneider
#t 1999
#c 4
#% 3652
#% 54075
#% 58374
#% 77672
#% 102746
#% 111345
#% 111351
#% 116057
#% 150625
#% 152904
#% 220756
#% 273954
#% 459017
#% 488069
#% 562325
#% 600525

#index 479819
#* Cache Conscious Indexing for Decision-Support in Main Memory
#@ Jun Rao;Kenneth A. Ross
#t 1999
#c 4
#% 12638
#% 13033
#% 24409
#% 32885
#% 54583
#% 68142
#% 100621
#% 172911
#% 201951
#% 211723
#% 275367
#% 282235
#% 317933
#% 318024
#% 411554
#% 442836
#% 462044
#% 479617
#% 479769
#% 481454
#% 566122

#index 479820
#* An Adaptive Hybrid Server Architecture for Client Caching ODBMSs
#@ Kaladhar Voruganti;M. Tamer Özsu;Ronald C. Unrau
#t 1999
#c 4
#% 111351
#% 116057
#% 116061
#% 152904
#% 169848
#% 172879
#% 172936
#% 172939
#% 172943
#% 201871
#% 201895
#% 210177
#% 210194
#% 235017
#% 239973
#% 442704
#% 458993
#% 463884
#% 479481
#% 480597
#% 481431
#% 481916
#% 555017

#index 479821
#* Database Architecture Optimized for the New Bottleneck: Memory Access
#@ Peter A. Boncz;Stefan Manegold;Martin L. Kersten
#t 1999
#c 4
#% 18614
#% 68142
#% 172911
#% 201344
#% 286258
#% 437941
#% 442835
#% 464816
#% 479628
#% 479769
#% 565258
#% 566122

#index 479822
#* Hierarchical Prefix Cubes for Range-Sum Queries
#@ Chee Yong Chan;Yannis E. Ioannidis
#t 1999
#c 4
#% 227861
#% 227866
#% 227868
#% 227869
#% 248040
#% 248814
#% 273902
#% 479630
#% 631947

#index 479947
#* In Cyber Space No One can Hear You Scream
#@ Chris Pound
#t 1999
#c 4
#% 382002
#% 387033
#% 419178

#index 479948
#* Extending Practical Pre-Aggregation in On-Line Analytical Processing
#@ Torben Bach Pedersen;Christian S. Jensen;Curtis E. Dyreson
#t 1999
#c 4
#% 199537
#% 208037
#% 210182
#% 227869
#% 227944
#% 266357
#% 420053
#% 462204
#% 464706
#% 479476
#% 481604
#% 481780
#% 481948
#% 482081
#% 482110
#% 482111
#% 503400
#% 503731
#% 631925

#index 479949
#* Issues in Network Management in the Next Millennium
#@ Michael L. Brodie;Surajit Chaudhuri
#t 1999
#c 4

#index 479950
#* Optimization of Run-time Management of Data Intensive Web-sites
#@ Daniela Florescu;Alon Y. Levy;Dan Suciu;Khaled Yagoub
#t 1999
#c 4
#% 36117
#% 210176
#% 210206
#% 210217
#% 227949
#% 248787
#% 248799
#% 248819
#% 261741
#% 281184
#% 287198
#% 442888
#% 458746
#% 459019
#% 462204
#% 464706
#% 464825
#% 479471
#% 479476
#% 481916
#% 482110
#% 565262

#index 479951
#* Cost Models DO Matter: Providing Cost Information for Diverse Data Sources in a Federated System
#@ Mary Tork Roth;Fatma Ozcan;Laura M. Haas
#t 1999
#c 4
#% 58377
#% 152940
#% 169815
#% 201926
#% 210176
#% 210178
#% 214683
#% 227719
#% 227875
#% 248795
#% 273034
#% 411554
#% 463919
#% 464692
#% 479449
#% 479452
#% 480788
#% 481101
#% 481923
#% 481939
#% 565470

#index 479952
#* Dynamic Load Balancing for Parallel Association Rule Mining on Heterogenous PC Cluster Systems
#@ Masahisa Tamura;Masaru Kitsuregawa
#t 1999
#c 4
#% 115661
#% 199538
#% 227922
#% 248786
#% 275367
#% 340290
#% 340291
#% 340595
#% 347040
#% 443091
#% 481290
#% 501207

#index 479953
#* High Level Indexing of User-Defined Types
#@ Weidong Chen;Jyh-Herng Chow;You-Chin Fuh;Jean Grandbois;Michelle Jou;Nelson Mendonça Mattos;Brian T. Tran;Yun Wang
#t 1999
#c 4
#% 86951
#% 152940
#% 169940
#% 172908
#% 172949
#% 201876
#% 248796
#% 252304
#% 317933
#% 427199
#% 445701
#% 462207
#% 463425
#% 464007
#% 479465
#% 479636
#% 480286
#% 481599
#% 481915
#% 482831
#% 511650

#index 479954
#* XML Repository and Active Views Demonstration
#@ Serge Abiteboul;Vincent Aguilera;Sébastien Ailleret;Bernd Amann;Sophie Cluet;Brendan Hills;Frédéric Hubert;Jean-Claude Mamou;Amélie Marian;Laurent Mignet;Tova Milo;Cassio Souza dos Santos;Bruno Tessier;Anne-Marie Vercoustre
#t 1999
#c 4
#% 169812
#% 480121

#index 479955
#* Context-Based Prefetch for Implementing Objects on Relations
#@ Philip A. Bernstein;Shankar Pal;David Shutt
#t 1999
#c 4
#% 58374
#% 111351
#% 152904
#% 152939
#% 154337
#% 166211
#% 172220
#% 235914
#% 236243
#% 262249
#% 264996
#% 281976
#% 427224
#% 435133
#% 442849
#% 463594
#% 479456
#% 480780
#% 481605
#% 571216

#index 479956
#* Relational Databases for Querying XML Documents: Limitations and Opportunities
#@ Jayavel Shanmugasundaram;Kristin Tufte;Chun Zhang;Gang He;David J. DeWitt;Jeffrey F. Naughton
#t 1999
#c 4
#% 36117
#% 172927
#% 210214
#% 236416
#% 273922
#% 286258
#% 287754
#% 411663
#% 416030
#% 462235

#index 479957
#* Explaining Differences in Multidimensional Aggregates
#@ Sunita Sarawagi
#t 1999
#c 4
#% 115608
#% 223781
#% 459025
#% 481588

#index 479958
#* Multi-Dimensional Substring Selectivity Estimation
#@ H. V. Jagadish;Olga Kapitskaia;Raymond T. Ng;Divesh Srivastava
#t 1999
#c 4
#% 43163
#% 184710
#% 201921
#% 210189
#% 210190
#% 222164
#% 238087
#% 248812
#% 268747
#% 273705
#% 289010
#% 411554
#% 461918
#% 479648
#% 481266
#% 482092

#index 479959
#* Industrial Panel on Data Warehousing Technologies: Experiences, Challenges, and Directions
#@ Umeshwar Dayal
#t 1999
#c 4

#index 479960
#* PM3: An Orthogonal Persistent Systems Programming Language - Design, Implementation, Performance
#@ Antony L. Hosking;Jiawan Chen
#t 1999
#c 4
#% 23920
#% 32893
#% 38691
#% 57876
#% 58345
#% 69820
#% 83515
#% 83566
#% 94544
#% 111351
#% 113857
#% 121873
#% 126835
#% 151533
#% 152904
#% 172936
#% 172939
#% 172943
#% 176601
#% 178015
#% 178643
#% 184258
#% 198979
#% 211189
#% 212673
#% 221392
#% 235914
#% 237259
#% 256210
#% 258379
#% 287622
#% 287716
#% 322978
#% 435151
#% 481440
#% 554861
#% 555022
#% 555039
#% 555168
#% 676994
#% 753880

#index 479961
#* Exploiting Versions for Handling Updates in Broadcast Disks
#@ Evaggelia Pitoura;Panos K. Chrysanthis
#t 1999
#c 4
#% 9241
#% 66172
#% 116055
#% 124011
#% 137870
#% 172874
#% 201869
#% 201897
#% 201928
#% 227944
#% 237238
#% 245014
#% 273893
#% 384050
#% 443127
#% 462077
#% 481777
#% 635795
#% 978507

#index 479962
#* Optimal Grid-Clustering: Towards Breaking the Curse of Dimensionality in High-Dimensional Clustering
#@ Alexander Hinneburg;Daniel A. Keim
#t 1999
#c 4
#% 102772
#% 131061
#% 201893
#% 210173
#% 238764
#% 248792
#% 462243
#% 471932
#% 479799
#% 481281
#% 527022
#% 566128
#% 937189

#index 479963
#* Generalised Hash Teams for Join and Group-by
#@ Alfons Kemper;Donald Kossmann;Christian Wiesner
#t 1999
#c 4
#% 3771
#% 77938
#% 136740
#% 172913
#% 191154
#% 227861
#% 248814
#% 285936
#% 287294
#% 321250
#% 322884
#% 462217
#% 463735
#% 463759
#% 466953
#% 479617
#% 479654
#% 479753
#% 481288
#% 482121
#% 571094
#% 571218

#index 479964
#* GHOST: Fine Granularity Buffering of Indexes
#@ Cheng Hian Goh;Beng Chin Ooi;D. Sim;Kian-Lee Tan
#t 1999
#c 4
#% 735
#% 1727
#% 152943
#% 248796
#% 317933
#% 320113
#% 427199
#% 479486
#% 480100
#% 480967
#% 482046

#index 479965
#* Networked Data Management Design Points
#@ James R. Hamilton
#t 1999
#c 4

#index 479966
#* A Scalable and Highly Available Networked Database Architecture
#@ Roger Bamford;Rafiul Ahad;Angelo Pruscino
#t 1999
#c 4

#index 479967
#* Probabilistic Optimization of Top N Queries
#@ Donko Donjerkovic;Raghu Ramakrishnan
#t 1999
#c 4
#% 3771
#% 102784
#% 132779
#% 210190
#% 227883
#% 227894
#% 248795
#% 248821
#% 273694
#% 479623
#% 479648
#% 479795
#% 481923

#index 479968
#* On Efficiently Implementing SchemaSQL on an SQL Database System
#@ Laks V. S. Lakshmanan;Fereidoon Sadri;Subbu N. Subramanian
#t 1999
#c 4
#% 13016
#% 102748
#% 116091
#% 123121
#% 140407
#% 152928
#% 210182
#% 213969
#% 248800
#% 434818
#% 435111
#% 452821
#% 459027
#% 462204
#% 464056
#% 464061
#% 479618
#% 481604
#% 481944
#% 482081
#% 571169
#% 706084

#index 479969
#* Extracting Large-Scale Knowledge Bases from the Web
#@ Ravi Kumar;Prabhakar Raghavan;Sridhar Rajagopalan;Andrew Tomkins
#t 1999
#c 4
#% 157709
#% 197751
#% 214673
#% 248784
#% 255137
#% 255165
#% 255170
#% 255197
#% 261741
#% 262061
#% 268073
#% 268079
#% 268186
#% 281209
#% 281214
#% 282905
#% 481290
#% 1394202

#index 479970
#* Spatio-Temporal Retrieval with RasDaMan
#@ Peter Baumann;Andreas Dehmel;Paula Furtado;Roland Ritsch;Norbert Widmann
#t 1999
#c 4
#% 125600
#% 248863
#% 380441
#% 463760
#% 479459
#% 482106

#index 479971
#* SPIRIT: Sequential Pattern Mining with Regular Expression Constraints
#@ Minos N. Garofalakis;Rajeev Rastogi;Kyuseok Shim
#t 1999
#c 4
#% 172892
#% 248785
#% 383546
#% 420063
#% 443194
#% 459006
#% 463903
#% 481290
#% 481611

#index 479972
#* User-Defined Table Operators: Enhancing Extensibility for ORDBMS
#@ Michael Jaedicke;Bernhard Mitschang
#t 1999
#c 4
#% 43162
#% 58377
#% 82858
#% 86952
#% 89628
#% 115661
#% 136740
#% 152940
#% 152941
#% 169801
#% 210170
#% 210187
#% 210206
#% 227934
#% 227962
#% 248816
#% 252481
#% 341233
#% 463594
#% 464007
#% 479467
#% 479469
#% 481915
#% 509136
#% 571294

#index 479973
#* Similarity Search in High Dimensions via Hashing
#@ Aristides Gionis;Piotr Indyk;Rajeev Motwani
#t 1999
#c 4
#% 24108
#% 68091
#% 114667
#% 140588
#% 169940
#% 190611
#% 201073
#% 209623
#% 212690
#% 227939
#% 236617
#% 248017
#% 248798
#% 248820
#% 248821
#% 248822
#% 248831
#% 249321
#% 282552
#% 317313
#% 321455
#% 406493
#% 434806
#% 437405
#% 479649
#% 482112

#index 479974
#* A Novel Index Supporting High Volume Data Warehouse Insertion
#@ Chris Jermaine;Anindya Datta;Edward Omiecinski
#t 1999
#c 4
#% 131555
#% 204575
#% 208047
#% 211085
#% 227861
#% 443093
#% 479470

#index 479975
#* Miro Web: Integrating Multiple Data Sources through Semistructured Data Types
#@ Luc Bouganim;Tatiana Chan-Sine-Ying;Tuyet-Tram Dang-Ngoc;Jean-Luc Darroux;Georges Gardarin;Fei Sha
#t 1999
#c 4
#% 194984
#% 248799
#% 463919
#% 479465
#% 481923
#% 566130
#% 614579
#% 631868

#index 479976
#* Online Feedback for Nested Aggregate Queries with Multi-Threading
#@ Kian-Lee Tan;Cheng Hian Goh;Beng Chin Ooi
#t 1999
#c 4
#% 32878
#% 58348
#% 145196
#% 210353
#% 214602
#% 227883
#% 273910
#% 277347
#% 287005
#% 340635
#% 479756
#% 480091
#% 503719
#% 632001

#index 479977
#* Implementation of SQL3 Structured Types with Inheritance and Value Substitutability
#@ You-Chin Fuh;Stefan Deßloch;Weidong Chen;Nelson Mendonça Mattos;Brian T. Tran;Bruce G. Lindsay;Linda DeMichel;Serge Rielau;Danko Mannhaupt
#t 1999
#c 4
#% 252481
#% 322880
#% 479979

#index 479978
#* Fast Algorithms for Maintaining Replica Consistency in Lazy Master Replicated Databases
#@ Esther Pacitti;Pascale Minet;Eric Simon
#t 1999
#c 4
#% 77005
#% 150431
#% 152928
#% 176447
#% 184212
#% 201928
#% 202146
#% 210179
#% 223781
#% 235084
#% 237700
#% 264263
#% 403195
#% 461887
#% 479779
#% 480085
#% 481106
#% 481584
#% 507601
#% 511029
#% 518631
#% 635767
#% 635831
#% 635832
#% 635834

#index 479979
#* O-O, What Have They Done to DB2?
#@ Michael J. Carey;Donald D. Chamberlin;Srinivasa Narayanan;Bennet Vance;Doug Doole;Serge Rielau;Richard Swagerman;Nelson Mendonça Mattos
#t 1999
#c 4
#% 32886
#% 43210
#% 115661
#% 227875
#% 235914
#% 238413
#% 322880
#% 341169
#% 380546
#% 411663
#% 427221
#% 442706
#% 462624
#% 481098
#% 481919
#% 614579

#index 479980
#* Distributed Hypertext Resource Discovery Through Examples
#@ Soumen Chakrabarti;Martin van den Berg;Byron Dom
#t 1999
#c 4
#% 165110
#% 176502
#% 197843
#% 201976
#% 237193
#% 247322
#% 248810
#% 249995
#% 252481
#% 260001
#% 262061
#% 268073
#% 268079
#% 268087
#% 268098
#% 268114
#% 281209
#% 281251
#% 281253
#% 281255
#% 282905
#% 479803
#% 571073
#% 618427

#index 479981
#* Data-Driven, One-To-One Web Site Generation for Data-Intensive Applications
#@ Stefano Ceri;Piero Fraternali;Stefano Paraboschi
#t 1999
#c 4
#% 106916
#% 187985
#% 248819
#% 255155
#% 275367
#% 278608
#% 296736
#% 479471
#% 565262

#index 479982
#* Active Storage Hierarchy, Database Systems and Applications - Socratic Exegesis
#@ Felipe Cariño;William O'Connell;John Burgess;Joel H. Saltz
#t 1999
#c 4
#% 262113
#% 479802
#% 480281

#index 479983
#* Repeating History Beyond ARIES
#@ C. Mohan
#t 1999
#c 4
#% 117
#% 4619
#% 9241
#% 32896
#% 38226
#% 40740
#% 77648
#% 77980
#% 85543
#% 112697
#% 114582
#% 116055
#% 116061
#% 116062
#% 116063
#% 116069
#% 116085
#% 116086
#% 116087
#% 152926
#% 152991
#% 169822
#% 172879
#% 172939
#% 194928
#% 194942
#% 213079
#% 227864
#% 236365
#% 245790
#% 272235
#% 273932
#% 317988
#% 319549
#% 320902
#% 381812
#% 415995
#% 435108
#% 442700
#% 442706
#% 445788
#% 458605
#% 461871
#% 462968
#% 463442
#% 463573
#% 464055
#% 479589
#% 480288
#% 480589
#% 480604
#% 481264
#% 481624
#% 481762
#% 486850
#% 565347

#index 479984
#* Histogram-Based Approximation of Set-Valued Query-Answers
#@ Yannis E. Ioannidis;Viswanath Poosala
#t 1999
#c 4
#% 16794
#% 82346
#% 172902
#% 210190
#% 227883
#% 248812
#% 259995
#% 273909
#% 277351
#% 427219
#% 443698
#% 452838
#% 463260
#% 482092
#% 482123
#% 504019
#% 689389

#index 479985
#* The Value of Merge-Join and Hash-Join in SQL Server
#@ Goetz Graefe
#t 1999
#c 4
#% 3200
#% 3771
#% 248855
#% 411554
#% 427195
#% 476622
#% 479617

#index 479986
#* Finding Intensional Knowledge of Distance-Based Outliers
#@ Edwin M. Knorr;Raymond T. Ng
#t 1999
#c 4
#% 152934
#% 227919
#% 230138
#% 248785
#% 248792
#% 464837
#% 479484
#% 479791
#% 480940

#index 479987
#* The Mirror MMDBMS Architecture
#@ Arjen P. de Vries;Mark G. L. M. van Doorn;Henk M. Blanken;Peter M. G. Apers
#t 1999
#c 4
#% 176530
#% 232117
#% 464816
#% 677703

#index 480118
#* Generating Call-Level Interfaces for Advanced Database Application Programming
#@ Udo Nink;Theo Härder;Norbert Ritter
#t 1999
#c 4
#% 103369
#% 152904
#% 159233
#% 184573
#% 383594
#% 435151
#% 463433
#% 480118
#% 555006
#% 566110

#index 480119
#* DBMSs on a Modern Processor: Where Does Time Go?
#@ Anastassia Ailamaki;David J. DeWitt;Mark D. Hill;David A. Wood
#t 1999
#c 4
#% 109757
#% 172911
#% 176360
#% 202154
#% 209853
#% 227914
#% 248824
#% 251473
#% 251474
#% 251477
#% 252458
#% 262154
#% 271492
#% 291631
#% 365700
#% 566122
#% 610161

#index 480120
#* Online Dynamic Reordering for Interactive Data Processing
#@ Vijayshankar Raman;Bhaskaran Raman;Joseph M. Hellerstein
#t 1999
#c 4
#% 1358
#% 64791
#% 136740
#% 148007
#% 210206
#% 210207
#% 227880
#% 227883
#% 227885
#% 227894
#% 227965
#% 248812
#% 248821
#% 273910
#% 375017
#% 438135
#% 464215
#% 464224
#% 503719
#% 571294

#index 480121
#* Active Views for Electronic Commerce
#@ Serge Abiteboul;Bernd Amann;Sophie Cluet;Adi Eyal;Laurent Mignet;Tova Milo
#t 1999
#c 4
#% 169812
#% 248029
#% 273702
#% 275314
#% 286901
#% 394417
#% 461899
#% 479629
#% 571069

#index 480122
#* Datawarehousing Has More Colours Than Just Black & White
#@ Thomas Zurek;Markus Sinnwell
#t 1999
#c 4
#% 223781
#% 256700
#% 383277
#% 383669

#index 480123
#* What can Hierarchies do for Data Warehouses?
#@ H. V. Jagadish;Laks V. S. Lakshmanan;Divesh Srivastava
#t 1999
#c 4
#% 18614
#% 191154
#% 199537
#% 208037
#% 210182
#% 223781
#% 227861
#% 227880
#% 248814
#% 273897
#% 317933
#% 459010
#% 459024
#% 464215
#% 466953
#% 479450
#% 479476
#% 481944
#% 481951
#% 482082
#% 562299
#% 631946

#index 480124
#* Semantic Compression and Pattern Extraction with Fascicles
#@ H. V. Jagadish;J. Madar;Raymond T. Ng
#t 1999
#c 4
#% 11805
#% 80995
#% 152934
#% 172386
#% 201893
#% 210173
#% 227917
#% 227924
#% 248790
#% 248792
#% 420062
#% 443122
#% 481281
#% 481290

#index 480125
#* Combining Histograms and Parametric Curve Fitting for Feedback-Driven Query Result-size Estimation
#@ Arnd Christian König;Gerhard Weikum
#t 1999
#c 4
#% 1435
#% 54047
#% 102784
#% 132779
#% 135388
#% 152585
#% 152917
#% 172902
#% 201921
#% 210188
#% 210190
#% 210353
#% 242366
#% 248014
#% 248793
#% 248812
#% 248821
#% 248822
#% 366958
#% 464044
#% 471932
#% 479648
#% 481266
#% 482123

#index 480126
#* Comparing Hierarchical Data in External Memory
#@ Sudarshan S. Chawathe
#t 1999
#c 4
#% 2011
#% 66654
#% 84755
#% 103525
#% 172892
#% 210212
#% 227859
#% 227998
#% 248023
#% 288885
#% 288987
#% 288988
#% 442886
#% 481931

#index 480127
#* What Do Those Weird XML Types Want, Anyway?
#@ Steven J. DeRose
#t 1999
#c 4
#% 27046
#% 51397
#% 237316
#% 249110
#% 249111
#% 262045
#% 262069
#% 360717
#% 380836
#% 464041

#index 480128
#* Loading a Cache with Query Results
#@ Laura M. Haas;Donald Kossmann;Ioana Ursu
#t 1999
#c 4
#% 43162
#% 98469
#% 121873
#% 152939
#% 154337
#% 169848
#% 210177
#% 227875
#% 227894
#% 248829
#% 340619
#% 381472
#% 411554
#% 435155
#% 442703
#% 479449
#% 479452
#% 479951
#% 480780
#% 481916
#% 571068

#index 480129
#* Capturing and Querying Multiple Aspects of Semistructured Data
#@ Curtis E. Dyreson;Michael H. Böhlen;Christian S. Jensen
#t 1999
#c 4
#% 210214
#% 236409
#% 236416
#% 237191
#% 244109
#% 246339
#% 248809
#% 261741
#% 275922
#% 462212
#% 464825
#% 479465
#% 481928
#% 504571
#% 561920
#% 562141

#index 480130
#* Integrating Heterogenous Overlapping Databases through Object-Oriented Transformations
#@ Vanja Josifovski;Tore Risch
#t 1999
#c 4
#% 116303
#% 184112
#% 194963
#% 194968
#% 229827
#% 286831
#% 292226
#% 442833
#% 443235
#% 458608
#% 463417
#% 479452
#% 481765
#% 571096

#index 480131
#* Ordering Information, Conference Organizers, Program Committees, Additional Reviewers, Additional Demonstrations Reviewers, Sponsors, VLDB Endowment, Preface, Foreword
#@ 
#t 2000
#c 4

#index 480132
#* What Is the Nearest Neighbor in High Dimensional Spaces?
#@ Alexander Hinneburg;Charu C. Aggarwal;Daniel A. Keim
#t 2000
#c 4
#% 67565
#% 131061
#% 169940
#% 238764
#% 252304
#% 273891
#% 300131
#% 427199
#% 443248
#% 479649
#% 481947
#% 481956
#% 482109

#index 480133
#* The A-tree: An Index Structure for High-Dimensional Spaces Using Relative Approximation
#@ Yasushi Sakurai;Masatoshi Yoshikawa;Shunsuke Uemura;Haruhiko Kojima
#t 2000
#c 4
#% 86950
#% 188519
#% 201876
#% 227939
#% 237187
#% 252304
#% 282552
#% 427199
#% 437509
#% 464195
#% 479649
#% 479973
#% 480133
#% 481455
#% 481956
#% 482112
#% 527026
#% 632035

#index 480134
#* Schema Mapping as Query Discovery
#@ Renée J. Miller;Laura M. Haas;Mauricio A. Hernández
#t 2000
#c 4
#% 11284
#% 50073
#% 134572
#% 158906
#% 158907
#% 166203
#% 172933
#% 213983
#% 220425
#% 226080
#% 248799
#% 248800
#% 269088
#% 464716
#% 479449
#% 479783
#% 480969
#% 503836
#% 564416
#% 614579

#index 480135
#* Toward Learning Based Web Query Processing
#@ Yanlei Diao;Hongjun Lu;Songting Chen;Zengping Tian
#t 2000
#c 4
#% 227987
#% 236409
#% 241033
#% 244103
#% 248810
#% 249110
#% 261741
#% 266215
#% 268079
#% 273925
#% 274141
#% 298221
#% 464825
#% 479807
#% 481602
#% 1499473

#index 480136
#* The Evolution of the Web and Implications for an Incremental Crawler
#@ Junghoo Cho;Hector Garcia-Molina
#t 2000
#c 4
#% 376
#% 232912
#% 268079
#% 268087
#% 281154
#% 281251
#% 282905
#% 300139
#% 978374

#index 480137
#* Panel: Future Directions of Database Research - The VLDB Broadening Strategy, Part 2
#@ Michael L. Brodie
#t 2000
#c 4

#index 480138
#* Semantic Access: Semantic Interface for Querying Databases
#@ Naphtali Rishe;Jun Yuan;Rukshan Athauda;Shu-Ching Chen;Xiaoling Lu;Xiaobin Ma;Alexander Vaschillo;Artyom Shaposhnikov;Dmitry Vasilevsky
#t 2000
#c 4
#% 93209
#% 154382
#% 168696
#% 168698
#% 191172

#index 480139
#* INSITE: A Tool for Interpreting Users? Interaction with a Web Space
#@ Cyrus Shahabi;Adil Faisal;Farnoush Banaei Kashani;Jabed Faruque
#t 2000
#c 4
#% 210173
#% 240208
#% 287041
#% 481281
#% 527022
#% 614610

#index 480140
#* A Database Platform for Bioinformatics
#@ Sandeepan Banerjee
#t 2000
#c 4

#index 480141
#* Performance Issues in Incremental Warehouse Maintenance
#@ Wilburt Labio;Jun Yang;Yingwei Cui;Hector Garcia-Molina;Jennifer Widom
#t 2000
#c 4
#% 152928
#% 201928
#% 201929
#% 223781
#% 227869
#% 252358
#% 273918
#% 479792
#% 481931

#index 480142
#* Research Directions in Biodiversity Informatics
#@ John L. Schnase
#t 2000
#c 4

#index 480143
#* Decision Tables: Scalable Classification Exploring RDBMS Capabilities
#@ Hongjun Lu;Hongyan Liu
#t 2000
#c 4
#% 90661
#% 136350
#% 152934
#% 227880
#% 246832
#% 252481
#% 280439
#% 385563
#% 420053
#% 452821
#% 459008
#% 480940
#% 481945
#% 481951
#% 566123
#% 631966
#% 641015

#index 480144
#* Using SQL to Build New Aggregates and Extenders for Object- Relational Systems
#@ Haixun Wang;Carlo Zaniolo
#t 2000
#c 4
#% 211931
#% 227883
#% 227951
#% 234756
#% 248813
#% 248820
#% 442705
#% 479460
#% 481255
#% 481290
#% 481945
#% 482082
#% 482088
#% 631966
#% 632070

#index 480145
#* The Challenge of Process Data Warehousing
#@ Matthias Jarke;Thomas List;Jörg Köller
#t 2000
#c 4
#% 183913
#% 237184
#% 261339
#% 261340
#% 388009
#% 482110
#% 503884
#% 589226
#% 765186
#% 1829997

#index 480146
#* Fast Time Sequence Indexing for Arbitrary Lp Norms
#@ Byoung-Kee Yi;Christos Faloutsos
#t 2000
#c 4
#% 34077
#% 86950
#% 116390
#% 137711
#% 172949
#% 201893
#% 227857
#% 273902
#% 273919
#% 427199
#% 460862
#% 462231
#% 464196
#% 479973
#% 480093
#% 481609
#% 481956
#% 534183
#% 616530
#% 1763253

#index 480147
#* Publish/Subscribe on the Web at Extreme Speed
#@ João Pereira;Françoise Fabret;François Llirbat;Radu Preotiuc-Pietro;Kenneth A. Ross;Dennis Shasha
#t 2000
#c 4
#% 86945
#% 248840
#% 271199
#% 275367
#% 386455
#% 511917
#% 661478

#index 480148
#* Media360 Workflow-Implementing a Workflow Engine Inside a Database
#@ Carsten Blecken
#t 2000
#c 4

#index 480149
#* A Scalable Algorithm for Answering Queries Using Views
#@ Rachel Pottinger;Alon Y. Levy
#t 2000
#c 4
#% 36181
#% 36683
#% 198465
#% 198466
#% 210182
#% 237190
#% 248038
#% 248800
#% 273696
#% 273698
#% 273700
#% 273924
#% 296931
#% 300138
#% 300168
#% 462214
#% 464056
#% 464203
#% 479792
#% 481604
#% 481923
#% 496091
#% 564419
#% 571091
#% 571169
#% 599549
#% 1499470

#index 480150
#* A Case-Based Approach to Information Integration
#@ Maurizio Panti;Luca Spalazzi;Alberto Giretti
#t 2000
#c 4
#% 85086
#% 111910
#% 111913
#% 111922
#% 116303
#% 158200
#% 194282
#% 213442
#% 227992
#% 229827
#% 237189
#% 359837
#% 395253
#% 395735
#% 464056
#% 464717
#% 490302
#% 497469
#% 511906
#% 564419
#% 588691
#% 633400

#index 480151
#* Process Automation as the Foundation for E-Business
#@ Fabio Casati;Ming-Chien Shan
#t 2000
#c 4
#% 665396

#index 480152
#* Efficiently Publishing Relational Data as XML Documents
#@ Jayavel Shanmugasundaram;Eugene J. Shekita;Rimon Barr;Michael J. Carey;Bruce G. Lindsay;Hamid Pirahesh;Berthold Reinwald
#t 2000
#c 4
#% 3771
#% 246333
#% 273922
#% 287754
#% 309851
#% 461897
#% 479956
#% 556436

#index 480153
#* Rethinking Database System Architecture: Towards a Self-Tuning RISC-Style Database System
#@ Surajit Chaudhuri;Gerhard Weikum
#t 2000
#c 4
#% 32889
#% 38687
#% 116629
#% 170893
#% 218149
#% 258238
#% 275367
#% 286178
#% 355177
#% 442699
#% 442705
#% 442706
#% 479477
#% 479985
#% 480158
#% 481459
#% 481919
#% 482082
#% 482100

#index 480154
#* Mining Frequent Itemsets Using Support Constraints
#@ Ke Wang;Yu He;Jiawei Han
#t 2000
#c 4
#% 152934
#% 201894
#% 227917
#% 227919
#% 248012
#% 280409
#% 280439
#% 280487
#% 300120
#% 479484
#% 479817
#% 481290
#% 481588
#% 481754
#% 481758
#% 632029

#index 480155
#* Asera: Extranet Architecture for B2B Solutions
#@ Anil Nori
#t 2000
#c 4

#index 480156
#* Identifying Representative Trends in Massive Time Series Data Sets Using Sketches
#@ Piotr Indyk;Nick Koudas;S. Muthukrishnan
#t 2000
#c 4
#% 172949
#% 201893
#% 248798
#% 248822
#% 249321
#% 273902
#% 273903
#% 460862
#% 479973
#% 482092
#% 631926

#index 480157
#* Toto, We're Not in Kansas Anymore: On Transitioning from Research to the Real (Invited Industrial Talk)
#@ Michael J. Carey
#t 2000
#c 4

#index 480158
#* Automated Selection of Materialized Views and Indexes in SQL Databases
#@ Sanjay Agrawal;Surajit Chaudhuri;Vivek R. Narasayya
#t 2000
#c 4
#% 36119
#% 174515
#% 210182
#% 248815
#% 273917
#% 443390
#% 462204
#% 464706
#% 479476
#% 479646
#% 481290
#% 481948
#% 482100
#% 482111
#% 566118

#index 480159
#* A 20/20 Vision of the VLDB-2020?
#@ S. Misbah Deen;Anant Jhingran;Shamkant B. Navathe;Erich J. Neuhold;Gio Wiederhold
#t 2000
#c 4

#index 480291
#* Integration of Data Mining with Database Technology
#@ Amir Netz;Surajit Chaudhuri;Jeff Bernhardt;Usama M. Fayyad
#t 2000
#c 4
#% 232136
#% 280521
#% 420108
#% 631966

#index 480292
#* Temporal Integrity Constraints with Indeterminacy
#@ Wes Cowley;Dimitris Plexousakis
#t 2000
#c 4
#% 91401
#% 135384
#% 163440
#% 166497
#% 220285
#% 259487
#% 319244
#% 442962
#% 442970
#% 480934
#% 481260
#% 489360
#% 501942
#% 529326
#% 542584
#% 553964
#% 571295
#% 618569
#% 704007

#index 480293
#* High-Performance and Scalability through Application Tier,In-Memory Data Management
#@ 
#t 2000
#c 4
#% 273945
#% 632032

#index 480294
#* Push Technology Personalization through Event Correlation
#@ Asaf Adi;David Botzer;Opher Etzion;Tali Yatzkar-Haham
#t 2000
#c 4
#% 177755
#% 296074
#% 459270
#% 480938
#% 481457
#% 1829875

#index 480295
#* Data Mining in the Bioinformatics Domain
#@ Shalom Tsur
#t 2000
#c 4

#index 480296
#* Efficient Filtering of XML Documents for Selective Dissemination of Information
#@ Mehmet Altinel;Michael J. Franklin
#t 2000
#c 4
#% 58361
#% 67565
#% 86944
#% 86946
#% 116082
#% 124004
#% 124009
#% 158911
#% 210214
#% 246334
#% 248838
#% 274142
#% 300179
#% 339373
#% 443298
#% 631962

#index 480297
#* The TreeScape System: Reuse of Pre-Computed Aggregates over Irregular OLAP Hierarchies
#@ Torben Bach Pedersen;Christian S. Jensen;Curtis E. Dyreson
#t 2000
#c 4
#% 479948
#% 481948
#% 1227358

#index 480298
#* Multi-Dimensional Database Allocation for Parallel Data Warehouses
#@ Thomas Stöhr;Holger Märtens;Erhard Rahm
#t 2000
#c 4
#% 43171
#% 115661
#% 116042
#% 135877
#% 149953
#% 158050
#% 159275
#% 191154
#% 201928
#% 223781
#% 227861
#% 252304
#% 264966
#% 420053
#% 462217
#% 462650
#% 466953
#% 480604
#% 507282
#% 509241
#% 571066
#% 571084

#index 480299
#* Managing Intervals Efficiently in Object-Relational Databases
#@ Hans-Peter Kriegel;Marco Pötke;Thomas Seidl
#t 2000
#c 4
#% 2115
#% 64431
#% 68089
#% 68091
#% 70370
#% 86950
#% 102759
#% 135384
#% 137893
#% 182700
#% 206915
#% 213001
#% 264939
#% 296159
#% 427199
#% 443320
#% 463749
#% 464856
#% 480093
#% 480422
#% 481599
#% 527175
#% 527181
#% 562809
#% 631949
#% 656697

#index 480300
#* Controlling Data Warehouses with Knowledge Networks
#@ Elvira Schaefer;Jan-Dirk Becker;Andreas Boehmer;Matthias Jarke
#t 2000
#c 4
#% 388009
#% 442973
#% 488115

#index 480301
#* Agora: Living with XML and Relational
#@ Ioana Manolescu;Daniela Florescu;Donald Kossmann;Florian Xhumari;Dan Olteanu
#t 2000
#c 4
#% 273912
#% 274160
#% 300143
#% 309726
#% 464717
#% 480317
#% 481444

#index 480302
#* FALCON: Feedback Adaptive Loop for Content-Based Retrieval
#@ Leejay Wu;Christos Faloutsos;Katia P. Sycara;Terry R. Payne
#t 2000
#c 4
#% 86950
#% 152937
#% 319273
#% 427199
#% 479462

#index 480303
#* Hypothetical knowledge and counterfactual reasoning
#@ Joseph Y. Halpern
#t 1998
#c 4
#% 123232
#% 781182
#! Samet introduced a notion of hypothetical knowledge and showed how it could be used to capture the type of counterfactual reasoning necessary to force the backwards induction solution in a game of perfect information. He argued that while hypothetical knowledge and the extended information structures used to model it bear some resemblance to the way philosophers have used conditional logic to model counterfactuals, hypothetical knowledge cannot be reduced to conditional logic together with epistemic logic. Here it is shown that in fact hypothetical knowledge can be captured using the standard counterfactual operator "" and the knowledge operator "K", provided that some assumptions are made regarding the interaction between the two. It is argued, however, that these assumptions are unreasonable in general, as are the axioms that follow from them. Some implications for game theory are discussed.

#index 480304
#* Contrast Plots and P-Sphere Trees: Space vs. Time in Nearest Neighbour Searches
#@ Jonathan Goldstein;Raghu Ramakrishnan
#t 2000
#c 4
#% 86950
#% 120270
#% 145895
#% 164360
#% 169940
#% 188519
#% 212689
#% 212690
#% 217292
#% 227856
#% 227937
#% 227939
#% 232766
#% 237204
#% 238545
#% 249321
#% 280452
#% 282552
#% 306893
#% 317468
#% 435141
#% 460862
#% 463414
#% 464195
#% 479649
#% 479973
#% 481460
#% 481620
#% 481941
#% 481956
#% 711097

#index 480305
#* Design and Development of a Stream Service in a Heterogenous Client Environment
#@ Nikos Pappas;Stavros Christodoulakis
#t 2000
#c 4
#% 114572
#% 151340
#% 159079
#% 164679
#% 172881
#% 173689
#% 204542
#% 239689
#% 285362
#% 434670
#% 437335
#% 443286
#% 479464
#% 479479
#% 513572
#% 513727
#% 571066
#% 591369
#% 612099
#% 632238

#index 480306
#* Approximate Query Processing Using Wavelets
#@ Kaushik Chakrabarti;Minos N. Garofalakis;Rajeev Rastogi;Kyuseok Shim
#t 2000
#c 4
#% 82346
#% 168862
#% 210190
#% 227883
#% 248806
#% 248822
#% 257637
#% 259995
#% 273902
#% 273909
#% 273919
#% 463760
#% 479984
#% 480306
#% 482092
#% 504019
#% 718437

#index 480307
#* Local Dimensionality Reduction: A New Approach to Indexing High Dimensional Spaces
#@ Kaushik Chakrabarti;Sharad Mehrotra
#t 2000
#c 4
#% 80995
#% 169940
#% 190611
#% 201893
#% 210173
#% 227939
#% 248790
#% 248792
#% 248798
#% 273891
#% 403195
#% 427199
#% 435141
#% 479649
#% 481281
#% 481956
#% 631963

#index 480308
#* An Ultra Highly Available DBMS
#@ Svein-Olaf Hvasshovd;Svein Erik Bratsberg;Øystein Torbjørnsen
#t 2000
#c 4

#index 480309
#* Focused Crawling Using Context Graphs
#@ Michelangelo Diligenti;Frans Coetzee;Steve Lawrence;C. Lee Giles;Marco Gori
#t 2000
#c 4
#% 262061
#% 268073
#% 268087
#% 268186
#% 281251
#% 290830
#% 311027
#% 376266
#% 406493
#% 420495
#% 466250
#% 479980
#% 1273824

#index 480310
#* Don't Be Lazy, Be Consistent: Postgres-R, A New Way to Implement Database Replication
#@ Bettina Kemme;Gustavo Alonso
#t 2000
#c 4
#% 9241
#% 100593
#% 102804
#% 198082
#% 204908
#% 210179
#% 237196
#% 248825
#% 273894
#% 323980
#% 461887
#% 461902
#% 479978
#% 481584
#% 507601
#% 617303
#% 648931

#index 480311
#* The Zero Latency Enterprise
#@ Dave Liles
#t 2000
#c 4

#index 480312
#* Biodiversity Informatics: The Challenge of Rapid Development, Large Databases, and Complex Data (Keynote)
#@ Meridith A. Lane;James L. Edwards;Ebbe Nielsen
#t 2000
#c 4

#index 480313
#* Evolution of Groupware for Business Applications: A Database Perspective on Lotus Domino/Notes
#@ C. Mohan;Ron Barber;S. Watts;A. Somani;Markos Zaharioudakis
#t 2000
#c 4
#% 83188
#% 114582
#% 116087
#% 152926
#% 169822
#% 172879
#% 194942
#% 201939
#% 463573
#% 464055
#% 479983

#index 480314
#* Concurrency in the Data Warehouse
#@ Richard Taylor
#t 2000
#c 4

#index 480315
#* CheeTah: a Lightweight Transaction Server for Plug-and-Play Internet Data Management
#@ Guy Pardon;Gustavo Alonso
#t 2000
#c 4
#% 54033
#% 91076
#% 235084
#% 237199
#% 273708
#% 310234
#% 403195
#% 458568

#index 480316
#* Panel: Future Directions of Database Research - The VLDB Broadening Strategy, Part 1
#@ Hans-Jörg Schek
#t 2000
#c 4
#% 218149
#% 479477
#% 480137
#% 631998
#% 665528

#index 480317
#* XPERANTO: Middleware for Publishing Object-Relational Data as XML Documents
#@ Michael J. Carey;Jerry Kiernan;Jayavel Shanmugasundaram;Eugene J. Shekita;Subbu N. Subramanian
#t 2000
#c 4
#% 246333
#% 281149
#% 309851
#% 480152
#% 480317

#index 480318
#* The 3W Model and Algebra for Unified Data Mining
#@ Theodore Johnson;Laks V. S. Lakshmanan;Raymond T. Ng
#t 2000
#c 4
#% 86950
#% 152934
#% 164406
#% 172386
#% 190332
#% 210173
#% 216508
#% 227919
#% 246560
#% 248784
#% 248785
#% 248792
#% 248813
#% 273890
#% 273898
#% 273899
#% 292679
#% 420063
#% 427199
#% 449588
#% 462238
#% 464215
#% 479484
#% 479971
#% 480123
#% 481281
#% 481290
#% 481588
#% 481954

#index 480319
#* Model-Based Information Integration in a Neuroscience Mediator System
#@ Bertram Ludäscher;Amarnath Gupta;Maryann E. Martone
#t 2000
#c 4
#% 189739
#% 229827
#% 248799
#% 274160
#% 275922
#% 536292
#% 617885

#index 480320
#* Building Scalable Internet Applications with Oracle8i Server
#@ Julie Basu;José Alberto Fernández;Olga Peschansky
#t 2000
#c 4

#index 480321
#* Linking Business to Deliver Value: A Data Management Challenge
#@ Anand Deshpande
#t 2000
#c 4

#index 480322
#* Panel: Is Generic Metadata Management Feasible?
#@ Philip A. Bernstein;Laura M. Haas;Matthias Jarke;Erhard Rahm;Gio Wiederhold
#t 2000
#c 4
#% 278445
#% 281979
#% 287733
#% 411575
#% 479783
#% 641044

#index 480323
#* Hierarchical Compact Cube for Range-Max Queries
#@ Sin Yeung Lee;Tok Wang Ling;Hua-Gang Li
#t 2000
#c 4
#% 210182
#% 227866
#% 227869
#% 319601
#% 461921
#% 464215
#% 479822
#% 481604
#% 481951
#% 631947

#index 480324
#* Rainbow: Distributed Database System for Classroom Education and Experimental Research
#@ Abdelsalam Helal;Hua Li
#t 2000
#c 4
#% 121
#% 264263
#% 403617
#% 462359

#index 480325
#* Telcordia's Database Reconciliation and Data Quality Analysis Tool
#@ Francesco Caruso;Munir Cochinwala;Uma Ganapathy;Gail Lalk;Paolo Missier
#t 2000
#c 4
#% 201889
#% 235941
#% 287222

#index 480326
#* Work and Information Practices in the Sciences of Biodiversity
#@ Geoffrey C. Bowker
#t 2000
#c 4
#% 338284

#index 480327
#* Integrating the UB-Tree into a Database System Kernel
#@ Frank Ramsak;Volker Markl;Robert Fenk;Martin Zirkel;Klaus Elhardt;Rudolf Bayer
#t 2000
#c 4
#% 273903
#% 287715
#% 415957
#% 435124
#% 462207
#% 462240
#% 479815
#% 479953
#% 481599
#% 481759
#% 482092
#% 482831
#% 571218
#% 631949
#% 637792

#index 480328
#* Approximating Aggregate Queries about Web Pages via Random Walks
#@ Ziv Bar-Yossef;Alexander Berg;Steve Chien;Jittat Fakcharoenphol;Dror Weitz
#t 2000
#c 4
#% 256609
#% 268114
#% 281166
#% 298221
#% 309748
#% 309749
#% 740360

#index 480329
#* Optimizing Queries on Compressed Bitmaps
#@ Sihem Amer-Yahia;Theodore Johnson
#t 2000
#c 4
#% 554
#% 32877
#% 43161
#% 54047
#% 86949
#% 118767
#% 191154
#% 227861
#% 248814
#% 273904
#% 273905
#% 273943
#% 318049
#% 411554
#% 462217
#% 479808
#% 617840
#% 617842

#index 480330
#* Optimizing Multi-Feature Queries for Image Databases
#@ Ulrich Güntzer;Wolf-Tilo Balke;Werner Kießling
#t 2000
#c 4
#% 169940
#% 194277
#% 210172
#% 213981
#% 248010
#% 535977
#% 614579

#index 480331
#* Practical Applications of Triggers and Constraints: Success and Lingering Issues (10-Year Award)
#@ Stefano Ceri;Roberta Cochrane;Jennifer Widom
#t 2000
#c 4
#% 83315
#% 86939
#% 111368
#% 111372
#% 116044
#% 225756
#% 234797
#% 275367
#% 300199
#% 341169
#% 341233
#% 463267
#% 479792
#% 480620
#% 480623
#% 481106
#% 481773
#% 481952

#index 480332
#* Offering a Precision-Performance Tradeoff for Aggregation Queries over Replicated Data
#@ Chris Olston;Jennifer Widom
#t 2000
#c 4
#% 9241
#% 32879
#% 77969
#% 201872
#% 210179
#% 227883
#% 248812
#% 248854
#% 268788
#% 273691
#% 274153
#% 288975
#% 300134
#% 340607
#% 408396
#% 438135
#% 458535
#% 458601
#% 479813
#% 480332
#% 480951
#% 503869
#% 631962
#% 689769

#index 480458
#* Oracle8i Index-Organized Table and Its Application to New Domains
#@ Jagannathan Srinivasan;Souripriya Das;Chuck Freiwald;Eugene Inseok Chong;Mahesh Jagannath;Aravind Yalamanchi;Ramkumar Krishnan;Anh-Tuan Tran;Samuel DeFazio;Jayanta Banerjee
#t 2000
#c 4
#% 872
#% 223201
#% 227861
#% 273889
#% 274156
#% 275340
#% 275341
#% 275367
#% 284172
#% 317933
#% 365700
#% 411562
#% 466944
#% 479639
#% 479792
#% 480926
#% 632099
#% 669819

#index 480459
#* Manipulating Interpolated Data is Easier than You Thought
#@ Stéphane Grumbach;Philippe Rigaux;Luc Segoufin
#t 2000
#c 4
#% 102756
#% 228800
#% 257440
#% 260062
#% 268788
#% 273706
#% 274143
#% 298596
#% 300173
#% 300174
#% 320062
#% 421073
#% 461923
#% 527166
#% 527175
#% 527176
#% 560839
#% 618583
#% 758494

#index 480460
#* Temporal Queries in OLAP
#@ Alberto O. Mendelzon;Alejandro A. Vaisman
#t 2000
#c 4
#% 83606
#% 287047
#% 289370
#% 361445
#% 443198
#% 459010
#% 459026
#% 459028
#% 459296
#% 464372
#% 562301
#% 631925
#% 631946

#index 480461
#* Social, Educational, and Governmental Change Enabled through Information Technology
#@ Krithi Ramamritham;Yeha El Atfi;Carlo Batini;Michael Eitan;Valerie Gregg;D. B. Phatak
#t 2000
#c 4

#index 480462
#* Design and Implementation of a Genetic-Based Algorithm for Data Mining
#@ Sunil Choenni
#t 2000
#c 4
#% 53706
#% 166235
#% 210160
#% 234988
#% 452821
#% 480940
#% 480964
#% 481945
#% 536024
#% 573224
#% 588563

#index 480463
#* Set Containment Joins: The Good, The Bad and The Ugly
#@ Karthikeyan Ramasamy;Jignesh M. Patel;Jeffrey F. Naughton;Raghav Kaushik
#t 2000
#c 4
#% 42401
#% 86948
#% 210186
#% 210187
#% 227875
#% 227934
#% 340670
#% 380546
#% 411663
#% 427195
#% 461897
#% 476621
#% 479753
#% 479956
#% 482121
#% 676541

#index 480464
#* What Happens During a Join? Dissecting CPU and Memory Optimization Effects
#@ Stefan Manegold;Peter A. Boncz;Martin L. Kersten
#t 2000
#c 4
#% 18614
#% 251473
#% 251474
#% 286258
#% 479628
#% 479821
#% 480119
#% 566122
#% 571056
#% 610161
#% 651006

#index 480465
#* Dynamic Maintenance of Wavelet-Based Histograms
#@ Yossi Matias;Jeffrey Scott Vitter;Min Wang
#t 2000
#c 4
#% 2833
#% 43163
#% 168862
#% 210190
#% 248820
#% 248822
#% 257637
#% 259995
#% 273901
#% 273902
#% 273903
#% 427219
#% 482092
#% 482123

#index 480466
#* E.piphany Epicenter Technology Overview
#@ Sridhar Ramaswamy
#t 2000
#c 4

#index 480467
#* Computing Geographical Scopes of Web Resources
#@ Junyan Ding;Luis Gravano;Narayanan Shivakumar
#t 2000
#c 4
#% 67565
#% 194246
#% 248810
#% 249110
#% 262065
#% 268073
#% 268079
#% 282905
#% 287463
#% 375017
#% 438136
#% 479642
#% 742446
#% 840580

#index 480468
#* Information Integration: The MOMIS Project Demonstration
#@ Domenico Beneventano;Sonia Bergamaschi;Silvana Castano;Alberto Corni;R. Guidetti;G. Malvezzi;Michele Melchiori;Maurizio Vincini
#t 2000
#c 4
#% 198058
#% 278445
#% 443218
#% 463919
#% 479449
#% 637829

#index 480469
#* Biodiversity Informatics Infrastructure: An Information Commons for the Biodiversity Community
#@ Gladys A. Cotter;Barbara T. Bauldock
#t 2000
#c 4

#index 480470
#* Hypothetical Queries in an OLAP Environment
#@ Andrey Balmin;Thanos Papadimitriou;Yannis Papakonstantinou
#t 2000
#c 4
#% 13016
#% 58377
#% 71544
#% 152928
#% 174161
#% 198465
#% 210182
#% 227868
#% 227869
#% 227896
#% 273696
#% 273918
#% 482081

#index 480471
#* ICICLES: Self-Tuning Samples for Approximate Query Answering
#@ Venkatesh Ganti;Mong-Li Lee;Raghu Ramakrishnan
#t 2000
#c 4
#% 1331
#% 248806
#% 248812
#% 259995
#% 273902
#% 273908
#% 273909
#% 274152
#% 280448
#% 452838
#% 504019

#index 480472
#* Demonstration: Enabling Scalable Online Personalization on the Web
#@ Kaushik Dutta;Anindya Datta;Debra E. VanderMeer;Krithi Ramamritham;Helen M. Thomas
#t 2000
#c 4
#% 152934
#% 202011
#% 459006
#% 463903
#% 479809
#% 1650569

#index 480473
#* Novel Approaches in Query Processing for Moving Object Trajectories
#@ Dieter Pfoser;Christian S. Jensen;Yannis Theodoridis
#t 2000
#c 4
#% 29602
#% 201876
#% 201880
#% 260043
#% 260045
#% 273706
#% 273886
#% 300174
#% 315005
#% 319244
#% 421072
#% 427199
#% 443263
#% 459011
#% 503882
#% 527166
#% 527176
#% 588714
#% 1180064

#index 480474
#* Caching Strategies for Data-Intensive Web Sites
#@ Khaled Yagoub;Daniela Florescu;Valérie Issarny;Patrick Valduriez
#t 2000
#c 4
#% 210217
#% 248799
#% 248819
#% 286466
#% 296736
#% 300177
#% 309729
#% 309851
#% 464825
#% 479471
#% 479950
#% 480121
#% 504584
#% 504585
#% 566137
#% 571036
#% 963903
#% 978368
#% 1303525

#index 480475
#* The BT-tree: A Branched and Temporal Access Method
#@ Linan Jiang;Betty Salzberg;David B. Lomet;Manuel Barrena García
#t 2000
#c 4
#% 10392
#% 16028
#% 56081
#% 86953
#% 102809
#% 182672
#% 287070
#% 435156
#% 479483
#% 526866

#index 480476
#* Approximate Query Translation Across Heterogeneous Information Sources
#@ Kevin Chen-Chuan Chang;Hector Garcia-Molina
#t 2000
#c 4
#% 55490
#% 101649
#% 116303
#% 137873
#% 154336
#% 158908
#% 164369
#% 198466
#% 213982
#% 227894
#% 227992
#% 237184
#% 248799
#% 249139
#% 273914
#% 375017
#% 408638
#% 442692
#% 445255
#% 459241
#% 464222
#% 464716
#% 464717
#% 479449
#% 479452
#% 479623
#% 479636
#% 481614
#% 481923
#% 631935
#% 632001
#% 632002
#% 707146
#% 1499471

#index 480477
#* AgFlow: Agent-based Cross-Enterprise Workflow Management System
#@ Liangzhao Zeng;Boualem Benatallah;Phuong Nguyen;Anne H. H. Ngu
#t 2001
#c 4

#index 480478
#* Similarity Search for Adaptive Ellipsoid Queries Using Spatial Transformation
#@ Yasushi Sakurai;Masatoshi Yoshikawa;Ryoji Kataoka;Shunsuke Uemura
#t 2001
#c 4
#% 86950
#% 227937
#% 244119
#% 252304
#% 437405
#% 443889
#% 479462
#% 479655
#% 479788
#% 480133
#% 481956
#% 482109
#% 527026

#index 480479
#* Crawling the Hidden Web
#@ Sriram Raghavan;Hector Garcia-Molina
#t 2001
#c 4
#% 115462
#% 235411
#% 261741
#% 268081
#% 268087
#% 281251
#% 300139
#% 330782
#% 331011
#% 424292
#% 459241
#% 480136
#% 480309

#index 480480
#* FeedbackBypass: A New Approach to Interactive Similarity Query Processing
#@ Ilaria Bartolini;Paolo Ciaccia;Florian Waas
#t 2001
#c 4
#% 672
#% 2115
#% 67565
#% 162297
#% 239709
#% 359751
#% 479462
#% 479788
#% 481956
#% 482109
#% 1857498

#index 480481
#* Transaction Timestamping in (Temporal) Databases
#@ Christian S. Jensen;David B. Lomet
#t 2001
#c 4
#% 9241
#% 123589
#% 225004
#% 264998
#% 339626
#% 340610
#% 403195
#% 442705
#% 442967
#% 443257
#% 480096
#% 570888

#index 480482
#* Efficient Index Structures for String Databases
#@ Tamer Kahveci;Ambuj K. Singh
#t 2001
#c 4
#% 120649
#% 143306
#% 235941
#% 248797
#% 300105
#% 325324
#% 464851
#% 481947
#% 545961

#index 480483
#* Lineage Tracing for General Data Warehouse Transformations
#@ Yingwei Cui;Jennifer Widom
#t 2001
#c 4
#% 201949
#% 223781
#% 298598
#% 300127
#% 318704
#% 341233
#% 462072
#% 464185
#% 481269
#% 481614
#% 481944
#% 482095
#% 519702
#% 632040
#% 674103
#% 715288

#index 480484
#* A Database Index to Large Biological Sequences
#@ Ela Hunt;Malcolm P. Atkinson;Robert W. Irving
#t 2001
#c 4
#% 70370
#% 121278
#% 143306
#% 194111
#% 198770
#% 221392
#% 235941
#% 269546
#% 271801
#% 289010
#% 290703
#% 297674
#% 300181
#% 300312
#% 333679
#% 333822
#% 348023
#% 388393
#% 488080
#% 513791
#% 546101
#% 555186
#% 593861
#% 742753
#% 742757

#index 480485
#* Discovering Web Services: An Overview
#@ Vadim Draluk
#t 2001
#c 4

#index 480486
#* A Prototype Content-Based Retrieval System that Uses Virtual Images to Save Space
#@ Leonard Brown;Le Gruenwald
#t 2001
#c 4
#% 18547
#% 443243
#% 489515
#% 625746

#index 480487
#* Et tu, XML? The downfall of the relational empire (abstract)
#@ Philip Wadler
#t 2001
#c 4

#index 480488
#* Estimating the Selectivity of XML Path Expressions for Internet Scale Applications
#@ Ashraf Aboulnaga;Alaa R. Alameldeen;Jeffrey F. Naughton
#t 2001
#c 4
#% 210189
#% 273705
#% 299984
#% 465018
#% 479465
#% 479806
#% 479958
#% 481938

#index 480489
#* Indexing and Querying XML Data for Regular Path Expressions
#@ Quanzhong Li;Bongki Moon
#t 2001
#c 4
#% 204662
#% 237192
#% 268066
#% 281149
#% 281150
#% 309726
#% 333981
#% 462235
#% 479465
#% 479806
#% 480296
#% 481599
#% 504578
#% 584940
#% 598374

#index 480490
#* LoPiX: A System for XML Data Integration and Manipulation
#@ Wolfgang May
#t 2001
#c 4
#% 495273

#index 480491
#* Scientific OLAP for the Biotech Domain
#@ Nam Huyn
#t 2001
#c 4
#% 223781
#% 428154
#% 924615

#index 480492
#* Ambient Intelligence with the Ubiquitous Network, the Embedded Computer Devices and the Hidden Databases (abstract)
#@ Egbert-Jan Sol
#t 2001
#c 4

#index 480493
#* SIT-IN: a Real-Life Spatio-Temporal Information System
#@ Giuseppe Sindoni;Leonardo Tininini;Amedea Ambrosetti;Cristina Bedeschi;Stefano De Francisci;Orietta Gargano;Rossella Molinaro;Mario Paolucci;Paola Patteri;Pina Ticca
#t 2001
#c 4
#% 480460

#index 480494
#* Cache Portal: Technology for Accelerating Database-driven e-commerce Web Sites
#@ Wen-Syan Li;K. Selçuk Candan;Wang-Pin Hsiung;Oliver Po;Divyakant Agrawal;Qiong Luo;Wei-Kuang Waine Huang;Yusuf Akca;Cemal Yilmaz
#t 2001
#c 4
#% 333995

#index 480495
#* Form-Based Proxy Caching for Database-Backed Web Sites
#@ Qiong Luo;Jeffrey F. Naughton
#t 2001
#c 4
#% 44638
#% 198465
#% 210176
#% 281184
#% 287200
#% 300177
#% 333995
#% 480495
#% 481916
#% 504584
#% 571216
#% 993397
#% 1303525

#index 480496
#* Declarative Data Cleaning: Language, Model, and Algorithms
#@ Helena Galhardas;Daniela Florescu;Dennis Shasha;Eric Simon;Cristian-Augustin Saita
#t 2001
#c 4
#% 36672
#% 169940
#% 201889
#% 223781
#% 248801
#% 301169
#% 420072
#% 480483
#% 480496
#% 480499
#% 480654
#% 481944

#index 480497
#* A Sequential Pattern Query Language for Supporting Instant Data Mining for e-Services
#@ Reza Sadri;Carlo Zaniolo;Amir M. Zarkesh;Jafar Adibi
#t 2001
#c 4
#% 333850
#% 393792
#% 480144

#index 480498
#* Storage and Retrieval of XML Data Using Relational Databases
#@ Surajit Chaudhuri;Kyuseok Shim
#t 2001
#c 4
#% 102798
#% 139837
#% 273922
#% 281149
#% 291299
#% 300143
#% 300153
#% 300157
#% 309851
#% 333935
#% 333981
#% 442665
#% 462062
#% 462235
#% 464988
#% 479465
#% 479806
#% 479956
#% 480152
#% 480317
#% 480489
#% 480624
#% 480656
#% 481125
#% 504578
#% 572305

#index 480499
#* Potter's Wheel: An Interactive Data Cleaning System
#@ Vijayshankar Raman;Joseph M. Hellerstein
#t 2001
#c 4
#% 140407
#% 189872
#% 223781
#% 248800
#% 248808
#% 300157
#% 301169
#% 312860
#% 333943
#% 333997
#% 375017
#% 420072
#% 480120
#% 480134
#% 481944
#% 542161

#index 480500
#* Data Staging for On-Demand Broadcast
#@ Demet Aksoy;Michael J. Franklin;Stanley B. Zdonik
#t 2001
#c 4
#% 152943
#% 201931
#% 202140
#% 235857
#% 248838
#% 259634
#% 290747
#% 374261
#% 479955
#% 480780
#% 481450
#% 595184
#% 628099
#% 632261

#index 480628
#* Surfing Wavelets on Streams: One-Pass Summaries for Approximate Aggregate Queries
#@ Anna C. Gilbert;Yannis Kotidis;S. Muthukrishnan;Martin Strauss
#t 2001
#c 4
#% 146300
#% 214073
#% 242366
#% 248812
#% 248822
#% 259995
#% 273682
#% 273901
#% 273902
#% 273903
#% 273907
#% 302724
#% 310488
#% 310500
#% 333872
#% 333926
#% 333931
#% 333983
#% 420123
#% 438133
#% 479795
#% 479816
#% 480306
#% 480465
#% 482123
#% 593957
#% 594012
#% 594029
#% 631923

#index 480629
#* Storage and Querying of E-Commerce Data
#@ Rakesh Agrawal;Amit Somani;Yirong Xu
#t 2001
#c 4
#% 6797
#% 53706
#% 64791
#% 102748
#% 123121
#% 248800
#% 286258
#% 330701
#% 411747
#% 420108
#% 464177
#% 479968
#% 481944
#% 571056
#% 641015
#% 669288
#% 768268

#index 480630
#* Mining Multi-Dimensional Constrained Gradients in Data Cubes
#@ Guozhu Dong;Jiawei Han;Joyce M. W. Lam;Jian Pei;Ke Wang
#t 2001
#c 4
#% 223781
#% 248785
#% 273916
#% 280409
#% 280458
#% 333925
#% 459025
#% 464989
#% 479795
#% 481951

#index 480631
#* An Extendible Hash for Multi-Precision Similarity Querying of Image Databases
#@ Shu Lin;M. Tamer Özsu;Vincent Oria;Raymond T. Ng
#t 2001
#c 4
#% 60660
#% 86950
#% 120270
#% 169940
#% 227939
#% 285932
#% 390132
#% 415984
#% 443319
#% 443889
#% 462009
#% 462187
#% 464195
#% 466832
#% 527900

#index 480632
#* Indexing the Distance: An Efficient Method to KNN Processing
#@ Cui Yu;Beng Chin Ooi;Kian-Lee Tan;H. V. Jagadish
#t 2001
#c 4
#% 210173
#% 227937
#% 227939
#% 248790
#% 248796
#% 299978
#% 382477
#% 390132
#% 427199
#% 464195
#% 479462
#% 479649
#% 480133
#% 480304
#% 480307
#% 481956
#% 632035

#index 480633
#* Flexible and scalable digital library search
#@ Henk Ernst Blok;Menzo Windhouwer;Roelof van Zwol;Milan Petkovic;Peter M. G. Apers;Martin L. Kersten;Willem Jonker
#t 2001
#c 4
#% 316554
#% 489542

#index 480634
#* Data Management for Pervasive Computing
#@ Mitch Cherniack;Michael J. Franklin;Stanley B. Zdonik
#t 2001
#c 4

#index 480635
#* Self-similarity in the Web
#@ Stephen Dill;Ravi Kumar;Kevin S. McCurley;Sridhar Rajagopalan;D. Sivakumar;Andrew Tomkins
#t 2001
#c 4
#% 109307
#% 176500
#% 197751
#% 214673
#% 232912
#% 255165
#% 255179
#% 255197
#% 260312
#% 262061
#% 268073
#% 268079
#% 281214
#% 281251
#% 281255
#% 283833
#% 290830
#% 300079
#% 309749
#% 321053
#% 424281
#% 479969
#% 593994
#% 656281

#index 480636
#* Aggregate Maintenance for Data Warehousing in Informix Red Brick Vista
#@ Craig J. Bunker;Latha S. Colby;Richard L. Cole;William J. McKenna;Gopal Mulagund;David Wilhite
#t 2001
#c 4
#% 208943
#% 210182
#% 279221
#% 300138
#% 300199
#% 479792
#% 480158
#% 480314
#% 482081
#% 565469

#index 480637
#* A Data Warehousing Architecture for Enabling Service Provisioning Process
#@ Yannis Kotidis
#t 2001
#c 4
#% 13026
#% 58364
#% 150862
#% 227861
#% 227866
#% 248814
#% 248820
#% 300134
#% 442726
#% 462217
#% 479473
#% 480289
#% 480602
#% 481951
#% 618597
#% 631947

#index 480638
#* Italian Electronic Identity Card - principle and architecture
#@ Mario Gentili
#t 2001
#c 4

#index 480639
#* Collaborative Analytical Processing - Dream or Reality? (Panel abstract)
#@ William O'Connell;Andrew Witkowski;Goetz Graefe
#t 2001
#c 4

#index 480640
#* The WorlInfo Assistant: Spatio-Temporal Information Integration on the Web
#@ José Luis Ambite;Craig A. Knoblock;Mohammad R. Kolahdouzan;Maria Muslea;Cyrus Shahabi;Snehal Thakkar
#t 2001
#c 4
#% 330786
#% 632018

#index 480641
#* Enabling End-users to Construct Data-intensive Web-sites from XML Repositories: An Example-based Approach
#@ Atsuyuki Morishima;Seiichi Koizumi;Hiroyuki Kitagawa;Satoshi Takano
#t 2001
#c 4
#% 115521
#% 210950
#% 281150
#% 296736
#% 528054

#index 480642
#* Dynamic Pipeline Scheduling for Improving Interactive Query Performance
#@ Tolga Urhan;Michael J. Franklin
#t 2001
#c 4
#% 159337
#% 227894
#% 228000
#% 248793
#% 273910
#% 273911
#% 300167
#% 340635
#% 479467
#% 479623
#% 480120
#% 482070

#index 480643
#* Supervised Wrapper Generation with Lixto
#@ Robert Baumgartner;Sergio Flesca;Georg Gottlob
#t 2001
#c 4
#% 480648

#index 480644
#* Improving Business Process Quality through Exception Understanding, Prediction, and Prevention
#@ Daniela Grigori;Fabio Casati;Umeshwar Dayal;Ming-Chien Shan
#t 2001
#c 4
#% 284600
#% 480666
#% 511640

#index 480645
#* Generic Schema Matching with Cupid
#@ Jayant Madhavan;Philip A. Bernstein;Erhard Rahm
#t 2001
#c 4
#% 69271
#% 278445
#% 307632
#% 328429
#% 333990
#% 479783
#% 480134
#% 637829

#index 480646
#* Views in a Large Scale XML Repository
#@ Sophie Cluet;Pierangelo Veltri;Dan Vodislav
#t 2001
#c 4
#% 36683
#% 210214
#% 235914
#% 281149
#% 300143
#% 479629
#% 495278
#% 495411
#% 562297
#% 571038

#index 480647
#* Comparing Hybrid Peer-to-Peer Systems
#@ Beverly Yang;Hector Garcia-Molina
#t 2001
#c 4
#% 172922
#% 173905
#% 213786
#% 249153
#% 262099
#% 403195
#% 480926
#% 481439
#% 568188
#% 711215

#index 480648
#* Visual Web Information Extraction with Lixto
#@ Robert Baumgartner;Sergio Flesca;Georg Gottlob
#t 2001
#c 4
#% 237194
#% 248808
#% 271065
#% 275915
#% 281149
#% 281150
#% 291299
#% 299975
#% 312858
#% 333997
#% 424302
#% 479807
#% 480643
#% 511902
#% 705442
#% 752812

#index 480649
#* WebFilter: A High-throughput XML-based Publish and Subscribe System
#@ João Pereira;Françoise Fabret;Hans-Arno Jacobsen;François Llirbat;Dennis Shasha
#t 2001
#c 4
#% 274142
#% 307470
#% 333938
#% 333982

#index 480650
#* French government activity in the conservation of data and electronic documents
#@ Serge Novaretti
#t 2001
#c 4

#index 480651
#* An Evaluation of Generic Bulk Loading Techniques
#@ Jochen Van den Bercken;Bernhard Seeger
#t 2001
#c 4
#% 41684
#% 77928
#% 86950
#% 88056
#% 136740
#% 153260
#% 201893
#% 227783
#% 248805
#% 252304
#% 286237
#% 301163
#% 411694
#% 427199
#% 443190
#% 458741
#% 462059
#% 462218
#% 479462
#% 479470
#% 479473
#% 479974
#% 480825
#% 481304
#% 481428
#% 481956
#% 567865
#% 570884
#% 571296

#index 480652
#* Fast Evaluation Techniques for Complex Similarity Queries
#@ Klemens Böhm;Michael Mlivoncic;Hans-Jörg Schek;Roger Weber
#t 2001
#c 4
#% 86950
#% 115473
#% 212690
#% 213981
#% 227939
#% 254089
#% 458742
#% 479462
#% 479649
#% 480132
#% 480133
#% 480330
#% 527026
#% 591565
#% 632035
#% 632060
#% 658290

#index 480653
#* The Propel Distributed Services Platform
#@ Michael J. Carey;Steve Kirsch;Mary Roth;Bert Van der Linden;Nicolas Adiba;Michael Blow;Daniela Florescu;David Li;Ivan Oprencak;Rajendra Panwar;Runping Qi;David Rieber;John C. Shafer;Brian Sterling;Tolga Urhan;Brian Vickery;Dan Wineman;Kuan Yee
#t 2001
#c 4
#% 115661

#index 480654
#* Approximate String Joins in a Database (Almost) for Free
#@ Luis Gravano;Panagiotis G. Ipeirotis;H. V. Jagadish;Nick Koudas;S. Muthukrishnan;Divesh Srivastava
#t 2001
#c 4
#% 121278
#% 224702
#% 227937
#% 248801
#% 269899
#% 333679
#% 480774
#% 481460
#% 546257
#% 547438
#% 617188
#% 656733
#% 978159

#index 480655
#* Ontology-based Support for Digital Government
#@ Athman Bouguettaya;Ahmed K. Elmagarmid;Brahim Medjahed;M. Ouzzani
#t 2001
#c 4
#% 328418
#% 430419
#% 438366
#% 438369

#index 480656
#* A Fast Index for Semistructured Data
#@ Brian Cooper;Neal Sample;Michael J. Franklin;Gisli R. Hjaltason;Moshe Shadmon
#t 2001
#c 4
#% 18614
#% 210205
#% 210214
#% 236416
#% 252608
#% 273922
#% 300181
#% 317933
#% 325324
#% 339373
#% 435135
#% 443205
#% 462235
#% 463603
#% 464720
#% 479465
#% 479806
#% 479956
#% 481417
#% 482090
#% 504578

#index 480657
#* Querying XML Views of Relational Data
#@ Jayavel Shanmugasundaram;Jerry Kiernan;Eugene J. Shekita;Catalina Fan;John Funderburk
#t 2001
#c 4
#% 58377
#% 274160
#% 300143
#% 309851
#% 333935
#% 461897
#% 480152
#% 481944
#% 632064
#% 632066
#% 713792

#index 480658
#* Hyperqueries: Dynamic Distributed Query Processing on the Internet
#@ Alfons Kemper;Christian Wiesner
#t 2001
#c 4
#% 85086
#% 136740
#% 172089
#% 188853
#% 210206
#% 300169
#% 300179
#% 301082
#% 328427
#% 427214
#% 479449
#% 479452
#% 570882
#% 571045
#% 571217
#% 572300
#% 631868

#index 480659
#* Change-Centric Management of Versions in an XML Warehouse
#@ Amélie Marian;Serge Abiteboul;Gregory Cobena;Laurent Mignet
#t 2001
#c 4
#% 2011
#% 83206
#% 210192
#% 291299
#% 297191
#% 299489
#% 300153
#% 333982
#% 443219
#% 480096
#% 480296
#% 480827

#index 480660
#* VXMLR: A Visual XML-Relational Database System
#@ Aoying Zhou;Hongjun Lu;Shihui Zheng;Yuqi Liang;Long Zhang;Wenyun Ji;Zengping Tian
#t 2001
#c 4

#index 480661
#* Discovery of Influence Sets in Frequently Updated Databases
#@ Ioana Stanoi;Mirek Riedewald;Divyakant Agrawal;Amr El Abbadi
#t 2001
#c 4
#% 235114
#% 237187
#% 300163
#% 462239
#% 527026
#% 632009

#index 480662
#* WARLOCK: A Data Allocation Tool for Parallel Warehouses
#@ Thomas Stöhr;Erhard Rahm
#t 2001
#c 4
#% 191154
#% 480298

#index 480663
#* Query Engines for Web-Accessible XML Data
#@ Leonidas Fegaras;Ramez Elmasri
#t 2001
#c 4
#% 116090
#% 172927
#% 172939
#% 294600
#% 300239
#% 313640
#% 335725
#% 464825
#% 479954
#% 479956
#% 504578

#index 480664
#* Operating System Extensions for the Teradata Parallel VLDB
#@ John Catozzi;Sorana Rabinovici
#t 2001
#c 4

#index 480665
#* Business Process Coordination: State of the Art, Trends, and Open Issues
#@ Umeshwar Dayal;Meichun Hsu;Rivka Ladin
#t 2001
#c 4
#% 32897
#% 86939
#% 122904
#% 122917
#% 151529
#% 213226
#% 226594
#% 248840
#% 273709
#% 303020
#% 383646
#% 459021
#% 461899
#% 461926
#% 464211
#% 464836
#% 480151
#% 480644
#% 480666
#% 480771
#% 511923
#% 562349
#% 660664

#index 480666
#* Warehousing Workflow Data: Challenges and Opportunities
#@ Angela Bonifati;Fabio Casati;Umeshwar Dayal;Ming-Chien Shan
#t 2001
#c 4
#% 284600
#% 480644

#index 480667
#* Tavant System Architecture for Sell-side Channel Management
#@ Srinivasa Narayanan;Subbu N. Subramanian
#t 2001
#c 4

#index 480668
#* The Long-Term Preservation of Authentic Electronic Records
#@ Luciana Duranti
#t 2001
#c 4

#index 480669
#* Analyzing Quantitative Databases: Image is Everything
#@ Amihood Amir;Reuven Kashi;Nathan S. Netanyahu
#t 2001
#c 4
#% 34077
#% 71889
#% 152934
#% 210160
#% 210162
#% 210190
#% 213977
#% 227919
#% 242366
#% 280436
#% 280458
#% 403085
#% 443086
#% 452821
#% 462234
#% 463742
#% 477656
#% 479482
#% 481758

#index 480670
#* A Formal Perspective on the View Selection Problem
#@ Rada Chirkova;Alon Y. Halevy;Dan Suciu
#t 2001
#c 4
#% 210182
#% 210208
#% 273697
#% 300237
#% 462204
#% 464706
#% 479476
#% 480158
#% 482110
#% 482111
#% 536422
#% 572311

#index 480671
#* Efficient Progressive Skyline Computation
#@ Kian-Lee Tan;Pin-Kwang Eng;Beng Chin Ooi
#t 2001
#c 4
#% 2115
#% 100803
#% 189319
#% 288976
#% 299978
#% 317933
#% 386623
#% 465167

#index 480801
#* Dynamic Update Cube for Range-sum Queries
#@ Seok-Ju Chun;Chin-Wan Chung;Ju-Hong Lee;Seok-Lyong Lee
#t 2001
#c 4
#% 86950
#% 227866
#% 312399
#% 383669
#% 427199
#% 479822
#% 480093
#% 481956
#% 631947

#index 480802
#* Online Scaling in a Highly Available Database
#@ Svein Erik Bratsberg;Rune Humborstad
#t 2001
#c 4
#% 210174
#% 300129
#% 300164
#% 479639
#% 489376

#index 480803
#* LEO - DB2's LEarning Optimizer
#@ Michael Stillger;Guy M. Lohman;Volker Markl;Mokhtar Kandil
#t 2001
#c 4
#% 54023
#% 102784
#% 137882
#% 169841
#% 172902
#% 210190
#% 248793
#% 248795
#% 273901
#% 411554
#% 480283
#% 482092
#% 482123

#index 480804
#* Functional Properties of Information Filtering
#@ Rie Sawai;Masahiko Tsukamoto;Yin-Huei Loh;Tsutomu Terada;Shojiro Nishio
#t 2001
#c 4
#% 96269
#% 124003
#% 124004
#% 124005
#% 218979
#% 237052
#% 241036
#% 262178
#% 297191
#% 511805
#% 1499473

#index 480805
#* Distinct Sampling for Highly-Accurate Answers to Distinct Values Queries and Event Reports
#@ Phillip B. Gibbons
#t 2001
#c 4
#% 1331
#% 2833
#% 58348
#% 69273
#% 99463
#% 102786
#% 210190
#% 214073
#% 227883
#% 242366
#% 243166
#% 248812
#% 248821
#% 259995
#% 273902
#% 273909
#% 273910
#% 274152
#% 277347
#% 299989
#% 300195
#% 333955
#% 333977
#% 336610
#% 479984
#% 480306
#% 480471
#% 481749
#% 482123
#% 504019

#index 480806
#* Storage Service Providers: a Solution for Storage Management? (Panel)
#@ Banu Özden;Eran Gabber;Bruce Hillyer;Wee Teck Ng;Elizabeth A. M. Shriver;David J. DeWitt;Bruce Gordon;Jim Gray;John Wilkes
#t 2001
#c 4

#index 480807
#* Are Web Services the Next Revolution in e-Commerce? (Panel)
#@ Shalom Tsur;Serge Abiteboul;Rakesh Agrawal;Umeshwar Dayal;Johannes Klein;Gerhard Weikum
#t 2001
#c 4
#% 480485

#index 480808
#* SMOOTH - A Distributed Multimedia Database System
#@ Harald Kosch;László Böszörményi;Alexander Bachlechner;Christian Hanin;Christian Hofbauer;Margit Lang;Carmen Riedler;Roland Tusch
#t 2001
#c 4
#% 334561
#% 390631
#% 422950

#index 480809
#* Navigating large-scale semi-structured data in business portals
#@ Man Abrol;Neil Latarche;Uma Mahadevan;Jianchang Mao;Rajat Mukherjee;Prabhakar Raghavan;Michel Tourn;John Wang;Grace Zhang
#t 2001
#c 4
#% 190581
#% 260001

#index 480810
#* Approximate Query Processing: Taming the TeraBytes
#@ Minos N. Garofalakis;Phillip B. Gibbon
#t 2001
#c 4
#% 1331
#% 2833
#% 58348
#% 82346
#% 102784
#% 145196
#% 152585
#% 172902
#% 201921
#% 210188
#% 210190
#% 210353
#% 214073
#% 227883
#% 227924
#% 242366
#% 243166
#% 245654
#% 248812
#% 248815
#% 248821
#% 248822
#% 257637
#% 273682
#% 273901
#% 273902
#% 273903
#% 273906
#% 273908
#% 273909
#% 273910
#% 277347
#% 285924
#% 299989
#% 300193
#% 300195
#% 302725
#% 333872
#% 333926
#% 333931
#% 333946
#% 333947
#% 333954
#% 333955
#% 333977
#% 333986
#% 338425
#% 411554
#% 420114
#% 427219
#% 452838
#% 463285
#% 465162
#% 479648
#% 479984
#% 480124
#% 480125
#% 480306
#% 480465
#% 480471
#% 480628
#% 480805
#% 481266
#% 481749
#% 482092
#% 482100
#% 482123
#% 689389
#% 718437

#index 480811
#* User-Optimizer Communication using Abstract Plans in Sybase ASE
#@ Mihnea Andrei;Patrick Valduriez
#t 2001
#c 4
#% 18614
#% 32878
#% 32889
#% 32890
#% 43162
#% 58375
#% 58377
#% 86929
#% 114577
#% 116071
#% 136740
#% 172900
#% 211088
#% 211569
#% 237190
#% 248014
#% 287005
#% 411554
#% 442706
#% 442850
#% 445771
#% 458550
#% 461897
#% 463444
#% 479786
#% 480091
#% 480158
#% 481104
#% 481604
#% 481608
#% 565457
#% 689389

#index 480812
#* C2P: Clustering based on Closest Pairs
#@ Alexandros Nanopoulos;Yannis Theodoridis;Yannis Manolopoulos
#t 2001
#c 4
#% 1331
#% 2115
#% 68091
#% 153260
#% 205024
#% 210173
#% 248790
#% 248804
#% 273890
#% 282894
#% 288695
#% 296738
#% 300131
#% 300132
#% 300162
#% 316536
#% 333933
#% 438137
#% 451052
#% 465152
#% 479962
#% 481281
#% 527022
#% 566128
#% 570889

#index 480813
#* The Semantic Web Paving the Way to the Knowledge Society
#@ Pierre-Paul Sondag
#t 2001
#c 4

#index 480814
#* Caching Technologies for Web Applications
#@ C. Mohan
#t 2001
#c 4
#% 302760
#% 330581
#% 330682
#% 333995
#% 334036
#% 433831
#% 480474
#% 480494
#% 480495
#% 480816
#% 480818
#% 661477

#index 480815
#* Developing an Indexing Scheme for XML Document Collection using the Oracle8i Extensibility Framework
#@ Seema Sundara;Ying Hu;Timothy Chorma;Nipun Agarwal;Jagannathan Srinivasan
#t 2001
#c 4
#% 479465
#% 480458
#% 632066
#% 632099

#index 480816
#* Update Propagation Strategies for Improving the Quality of Data on the Web
#@ Alexandros Labrinidis;Nick Roussopoulos
#t 2001
#c 4
#% 201922
#% 275367
#% 290747
#% 300139
#% 300177
#% 333969
#% 458544
#% 635831
#% 963655

#index 480817
#* MV3R-Tree: A Spatio-Temporal Access Method for Timestamp and Interval Queries
#@ Yufei Tao;Dimitris Papadias
#t 2001
#c 4
#% 76907
#% 86950
#% 102759
#% 152937
#% 273706
#% 287070
#% 296090
#% 300173
#% 300174
#% 443130
#% 443181
#% 443444
#% 481934
#% 503882
#% 571296
#% 664835

#index 480818
#* A Comparative Study of Alternative Middle Tier Caching Solutions to Support Dynamic Web Content Acceleration
#@ Anindya Datta;Kaushik Dutta;Helen M. Thomas;Debra E. VanderMeer;Krithi Ramamritham;Dan Fishman
#t 2001
#c 4
#% 480474
#% 978365

#index 480819
#* Supporting Incremental Join Queries on Ranked Inputs
#@ Apostol Natsev;Yuan-Chi Chang;John R. Smith;Chung-Sheng Li;Jeffrey Scott Vitter
#t 2001
#c 4
#% 174161
#% 213981
#% 333854
#% 443243
#% 480330

#index 480820
#* Intelligent Rollups in Multidimensional OLAP Data
#@ Gayatri Sathe;Sunita Sarawagi
#t 2001
#c 4
#% 223781
#% 479957
#% 480964
#% 481588
#% 1290056

#index 480821
#* Weaving Relations for Cache Performance
#@ Anastassia Ailamaki;David J. DeWitt;Mark D. Hill;Marios Skounakis
#t 2001
#c 4
#% 872
#% 69094
#% 114582
#% 172908
#% 172939
#% 204182
#% 210187
#% 227861
#% 251474
#% 251477
#% 252458
#% 262123
#% 286258
#% 365700
#% 387173
#% 390132
#% 411554
#% 427199
#% 438342
#% 464843
#% 479821
#% 480119
#% 480272
#% 481428
#% 566122

#index 480822
#* Answering XML Queries on Heterogeneous Data Sources
#@ Ioana Manolescu;Daniela Florescu;Donald Kossmann
#t 2001
#c 4
#% 198465
#% 229827
#% 248799
#% 273912
#% 273922
#% 300143
#% 309851
#% 333935
#% 333965
#% 398240
#% 443235
#% 479452
#% 479956
#% 480149
#% 480152
#% 480301
#% 480657

#index 480823
#* Architectures for Internal Web Services Deployment
#@ Oded Shmueli
#t 2001
#c 4
#% 591573

#index 480824
#* RoadRunner: Towards Automatic Data Extraction from Large Web Sites
#@ Valter Crescenzi;Giansalvatore Mecca;Paolo Merialdo
#t 2001
#c 4
#% 25469
#% 237194
#% 248808
#% 271065
#% 275915
#% 275917
#% 278109
#% 287202
#% 312860
#% 479471
#% 511902
#% 535703
#% 542161

#index 480825
#* XXL - A Library Approach to Supporting Efficient Implementations of Advanced Database Queries
#@ Jochen Van den Bercken;Björn Blohsfeld;Jens-Peter Dittrich;Jürgen Krämer;Tobias Schäfer;Martin Schneider;Bernhard Seeger
#t 2001
#c 4
#% 55314
#% 86950
#% 201874
#% 210187
#% 211566
#% 227883
#% 287466
#% 333681
#% 342595
#% 411557
#% 435157
#% 442850
#% 443326
#% 479473
#% 479797
#% 480153
#% 481599
#% 526847
#% 567865
#% 631937
#% 632105

#index 480826
#* On Processing XML in LDAP
#@ Pedro José Marrón;Georg Lausen
#t 2001
#c 4
#% 273897
#% 300143
#% 479806
#% 479956
#% 480474
#% 504574
#% 504584
#% 504585
#% 576209
#% 589076
#% 676367

#index 480827
#* Efficient Management of Multiversion Documents by Object Referencing
#@ Shu-Yao Chien;Vassilis J. Tsotras;Carlo Zaniolo
#t 2001
#c 4
#% 2011
#% 10392
#% 58371
#% 76907
#% 182672
#% 188746
#% 287070
#% 442967
#% 443130
#% 462212
#% 462321
#% 479776
#% 479932
#% 480092
#% 480659
#% 504576
#% 571296

#index 480828
#* Analyzing energy behavior of spatial access methods for memory-resident data
#@ Ning An;Anand Sivasubramaniam;Narayanan Vijaykrishnan;Mahmut T. Kandemir;Mary Jane Irwin;Sudhanva Gurumurthi
#t 2001
#c 4
#% 3453
#% 13033
#% 68091
#% 83319
#% 116065
#% 152954
#% 153260
#% 172876
#% 201876
#% 252304
#% 427199
#% 438217
#% 438302
#% 443258
#% 464859
#% 480119
#% 526840
#% 526845
#% 706750

#index 480829
#* Cache-Conscious Concurrency Control of Main-Memory Indexes on Shared-Memory Multiprocessor Systems
#@ Sang K. Cha;Sangyong Hwang;Kihong Kim;Keunjoo Kwon
#t 2001
#c 4
#% 83183
#% 114582
#% 116087
#% 135557
#% 286929
#% 291855
#% 300194
#% 333940
#% 465019
#% 479474
#% 479769
#% 479819

#index 480830
#* Walking Through a Very Large Virtual Environment in Real-time
#@ Lidan Shou;Jason Chionh;Zhiyong Huang;Yixin Ruan;Kian-Lee Tan
#t 2001
#c 4
#% 131750
#% 149133
#% 173267
#% 261875
#% 334013
#% 427199
#% 462224
#% 527033

#index 480831
#* Cache Fusion: Extending Shared-Disk Clusters with Shared Caches
#@ Tirthankar Lahiri;Vinay Srihari;Wilson Chan;N. MacNaughton;Sashikanth Chandrasekaran
#t 2001
#c 4
#% 479468

#index 480832
#* ACTIVIEW: Adaptive data presentation using SuperSQL
#@ Yoko Maeda;Motomichi Toyama
#t 2001
#c 4
#% 248866
#% 296929
#% 479981

#index 481774
#* Effective & Efficient Document Ranking without using a Large Lexicon
#@ Yasushi Ogawa
#t 1996
#c 4
#% 36279
#% 39436
#% 85448
#% 115462
#% 144012
#% 144044
#% 144072
#% 157880
#% 169781
#% 194247
#% 194263
#% 194265
#% 198335
#% 228097
#% 394709
#% 463737
#% 511003

#index 481775
#* Estimation of Query-Result Distribution and its Application in Parallel-Join Load Balancing
#@ Viswanath Poosala;Yannis E. Ioannidis
#t 1996
#c 4
#% 1331
#% 3771
#% 43163
#% 54577
#% 58352
#% 102784
#% 152924
#% 201921
#% 210190
#% 285924
#% 427219
#% 442698
#% 442700
#% 442706
#% 463901
#% 479920
#% 480596
#% 480608
#% 480761
#% 480966
#% 481266

#index 481776
#* The XPS Approach to Loading and Unloading Terabyte Databases
#@ Sanket Atal
#t 1996
#c 4

#index 481777
#* Disseminating Updates on Broadcast Disks
#@ Swarup Acharya;Michael J. Franklin;Stanley B. Zdonik
#t 1996
#c 4
#% 3948
#% 32884
#% 66172
#% 77005
#% 86477
#% 100060
#% 124011
#% 151529
#% 169835
#% 172874
#% 172876
#% 175253
#% 201869
#% 201897
#% 243299
#% 360716
#% 464065
#% 464214
#% 480438
#% 566124

#index 481778
#* The Changing Landscape of the Software Industry and its Implications for India
#@ Umang Gupta
#t 1996
#c 4

#index 481779
#* Sampling Large Databases for Association Rules
#@ Hannu Toivonen
#t 1996
#c 4
#% 77967
#% 116084
#% 152934
#% 164368
#% 172386
#% 201894
#% 232136
#% 481290
#% 481588
#% 481754
#% 481758
#% 566123

#index 481780
#* Information Retrieval from an Incomplete Data Cube
#@ Curtis E. Dyreson
#t 1996
#c 4
#% 99137
#% 123589
#% 152936
#% 163440
#% 199537
#% 210182
#% 481604

#index 481781
#* Very Large Databases in a Commercial Application Environment
#@ Karl-Heinz Hess
#t 1996
#c 4

#index 481782
#* Modeling Skewed Distribution Using Multifractals and the `80-20' Law
#@ Christos Faloutsos;Yossi Matias;Abraham Silberschatz
#t 1996
#c 4
#% 102316
#% 152585
#% 201921
#% 214073
#% 321250
#% 411554
#% 480948
#% 481620
#% 481749

#index 481783
#* Extracting Large Data Sets using DB2 Parallel Edition
#@ Sriram Padmanabhan
#t 1996
#c 4
#% 188719

#index 481784
#* Dynamic Load Balancing in Hierarchical Parallel Database Systems
#@ Luc Bouganim;Daniela Florescu;Patrick Valduriez
#t 1996
#c 4
#% 116041
#% 136740
#% 152915
#% 152924
#% 158047
#% 172907
#% 201885
#% 210199
#% 285936
#% 442698
#% 442700
#% 442835
#% 458548
#% 479753
#% 480596
#% 480615
#% 480761
#% 480966
#% 481110
#% 481289
#% 481617
#% 481753
#% 500306

#index 481785
#* Further Improvements on Integrity Constraint Checking for Stratifiable Deductive Databases
#@ Sin Yeung Lee;Tok Wang Ling
#t 1996
#c 4
#% 877
#% 25640
#% 30098
#% 36683
#% 61357
#% 152911
#% 164364
#% 458576
#% 479770
#% 480103
#% 481128
#% 564081
#% 599549

#index 481786
#* Obtaining Complete Answers from Incomplete Databases
#@ Alon Y. Levy
#t 1996
#c 4
#% 36181
#% 36683
#% 53400
#% 59350
#% 67457
#% 122396
#% 123118
#% 140410
#% 159113
#% 164371
#% 188853
#% 268764
#% 287336
#% 289266
#% 481128
#% 481923
#% 599549
#% 1499471

#index 481787
#* Modeling Design Versions
#@ R. Ramakrishnan;D. Janaki Ram
#t 1996
#c 4
#% 23960
#% 32903
#% 55234
#% 58373
#% 59114
#% 86478
#% 98459
#% 98689
#% 146201
#% 464162
#% 479776
#% 480092
#% 480251
#% 1784171

#index 481788
#* Scalablity and Availability in Oracle7 7.3
#@ Dieter Gawlick
#t 1996
#c 4

#index 481789
#* DISNIC-PLAN: A NICNET Based Distributed Database for Micro-level Planning in India
#@ M. Moni
#t 1996
#c 4

#index 481915
#* Optimization of Queries with User-defined Predicates
#@ Surajit Chaudhuri;Kyuseok Shim
#t 1996
#c 4
#% 68142
#% 77944
#% 86949
#% 116040
#% 116043
#% 152940
#% 172930
#% 411554
#% 479938
#% 480944
#% 481101
#% 481288

#index 481916
#* Semantic Data Caching and Replacement
#@ Shaul Dar;Michael J. Franklin;Björn Þór Jónsson;Divesh Srivastava;Michael Tan
#t 1996
#c 4
#% 6798
#% 86951
#% 169844
#% 172936
#% 172939
#% 210177
#% 360716
#% 464026
#% 464056
#% 480597
#% 481431
#% 555017
#% 564419
#% 571216

#index 481917
#* Loading the Data Warehouse Across Various Parallel Architectures
#@ Vijay V. Raghavan
#t 1996
#c 4

#index 481918
#* Applying Data Mining Techniques to a Health Insurance Information System
#@ Marisa S. Viveros;John P. Nearhos;Michael J. Rothman
#t 1996
#c 4
#% 60576
#% 152934
#% 232106
#% 232170
#% 481290

#index 481919
#* Of Objects and Databases: A Decade of Turmoil
#@ Michael J. Carey;David J. DeWitt
#t 1996
#c 4
#% 18606
#% 23959
#% 33000
#% 38691
#% 43210
#% 168676
#% 172939
#% 182903
#% 210178
#% 286178
#% 286179
#% 286181
#% 286190
#% 286193
#% 286831
#% 341169
#% 380441
#% 411663
#% 427224
#% 464007
#% 479932
#% 481098
#% 481428
#% 614579
#% 750960

#index 481920
#* Filter Trees for Managing Spatial Data over a Range of Size Granularities
#@ Kenneth C. Sevcik;Nick Koudas
#t 1996
#c 4
#% 68091
#% 86950
#% 86951
#% 152937
#% 153260
#% 427199
#% 445701
#% 462041
#% 462781
#% 480093
#% 481281
#% 481455
#% 605182

#index 481921
#* Supporting Procedural Constructs in SQL Compilers
#@ Nelson Mendonça Mattos
#t 1996
#c 4

#index 481922
#* Supporting State-Wide Immunisation Tracking Using Multi-Paradigm Workflow Technology
#@ Amit P. Sheth;Krys Kochut;John A. Miller;Devashish Worah;Souvik Das;Chenye Lin;Devanand Palaniswami;John Lynch;Ivan Shevchenko
#t 1996
#c 4
#% 116185
#% 146202
#% 152995
#% 169054
#% 185412
#% 185413
#% 201962
#% 461900
#% 463878
#% 480961
#% 481261

#index 481923
#* Querying Heterogeneous Information Sources Using Source Descriptions
#@ Alon Y. Levy;Anand Rajaraman;Joann J. Ordille
#t 1996
#c 4
#% 111922
#% 159113
#% 188853
#% 198466
#% 210176
#% 213982
#% 213983
#% 277328
#% 459241
#% 464056
#% 481602
#% 482081
#% 564419
#% 1499471

#index 481924
#* TPC-D: The Challenges, Issues and Results
#@ Ramesh Bhashyam
#t 1996
#c 4

#index 481925
#* The Query Optimizer in Tandem's new ServerWare SQL Product
#@ Pedro Celis
#t 1996
#c 4

#index 481926
#* Practical Issues with Commercial Use of Federated Databases
#@ Jim Kleewein
#t 1996
#c 4

#index 481927
#* Using Referential Integrity To Easily Define Consistent Subset Replicas
#@ Brad Hammond
#t 1996
#c 4

#index 481928
#* Coalescing in Temporal Databases
#@ Michael H. Böhlen;Richard Thomas Snodgrass;Michael D. Soo
#t 1996
#c 4
#% 4695
#% 18615
#% 18772
#% 36683
#% 43028
#% 49596
#% 111284
#% 123589
#% 136740
#% 152940
#% 159227
#% 163440
#% 172930
#% 287222
#% 319244
#% 361445
#% 361831
#% 427195
#% 442918
#% 458554
#% 463735
#% 463751
#% 463761
#% 480774
#% 480797
#% 527789
#% 564241

#index 481929
#* Supporting Periodic Authorizations and Temporal Reasoning in Database Access Control
#@ Elisa Bertino;Claudio Bettini;Elena Ferrari;Pierangela Samarati
#t 1996
#c 4
#% 103705
#% 145194
#% 181034
#% 443012
#% 481929

#index 481930
#* EROC: A Toolkit for Building NEATO Query Optimizers
#@ William J. McKenna;Louis Burger;Chi Hoang;Melissa Truong
#t 1996
#c 4
#% 32878
#% 32889
#% 58345
#% 86949
#% 103298
#% 152940
#% 152942
#% 166189
#% 210207
#% 287005
#% 395756
#% 480091
#% 480430
#% 480759
#% 481288
#% 481293
#% 481429
#% 481604
#% 564426
#% 565457
#% 571062

#index 481931
#* Efficient Snapshot Differential Algorithms for Data Warehousing
#@ Wilburt Labio;Hector Garcia-Molina
#t 1996
#c 4
#% 3771
#% 13015
#% 114577
#% 201928
#% 201935
#% 201949
#% 210212
#% 320220
#% 480085
#% 571094
#% 671919
#% 672575

#index 481932
#* MineSet(tm): A System for High-End Data Mining and Visualization
#@ 
#t 1996
#c 4

#index 481933
#* Incremental Maintenance of Externally Materialized Views
#@ Martin Staudt;Matthias Jarke
#t 1996
#c 4
#% 6798
#% 36683
#% 90639
#% 152928
#% 183913
#% 201928
#% 201930
#% 287324
#% 341233
#% 368248
#% 441196
#% 458556
#% 480289
#% 480623
#% 481584
#% 571216

#index 481934
#* Query Processing Techniques for Multiversion Access Methods
#@ Jochen Van den Bercken;Bernhard Seeger
#t 1996
#c 4
#% 10392
#% 32898
#% 58371
#% 86953
#% 102759
#% 112504
#% 286256
#% 427199
#% 458545
#% 479753
#% 480093
#% 480096
#% 526866
#% 527802
#% 566113

#index 481935
#* Object Fusion in Mediator Systems
#@ Yannis Papakonstantinou;Serge Abiteboul;Hector Garcia-Molina
#t 1996
#c 4
#% 85089
#% 102748
#% 111913
#% 116091
#% 140407
#% 158200
#% 463919
#% 464222
#% 480429
#% 481935
#% 614579

#index 481936
#* What is the Data Warehousing Problem? (Are Materialized Views the Answer?)
#@ Ashish Gupta;Inderpal Singh Mumick
#t 1996
#c 4

#index 481937
#* Cache Coherency in Oracle Parallel Server
#@ Boris Klots
#t 1996
#c 4

#index 481938
#* Cost-based Selection of Path Expression Processing Algorithms in Object-Oriented Databases
#@ Georges Gardarin;Jean-Robert Gruser;Zhao-Hui Tang
#t 1996
#c 4
#% 3771
#% 57955
#% 58372
#% 86954
#% 102761
#% 116091
#% 126338
#% 152942
#% 170061
#% 320113
#% 395735
#% 462798
#% 463288
#% 479938
#% 481623

#index 481939
#* Calibrating the Query Optimizer Cost Model of IRO-DB, an Object-Oriented Federated Database System
#@ Georges Gardarin;Fei Sha;Zhao-Hui Tang
#t 1996
#c 4
#% 3771
#% 83541
#% 102761
#% 111913
#% 116091
#% 152904
#% 194984
#% 199537
#% 286831
#% 395735
#% 437126
#% 459261
#% 480788
#% 481104
#% 481442
#% 481623
#% 511162

#index 481940
#* The Structured Information Manager: A Database System for SGML Documents
#@ Ron Sacks-Davis
#t 1996
#c 4
#% 480926

#index 481941
#* Analysis of n-Dimensional Quadtrees using the Hausdorff Fractal Dimension
#@ Christos Faloutsos;Volker Gaede
#t 1996
#c 4
#% 42091
#% 58369
#% 86950
#% 102772
#% 118213
#% 164360
#% 172949
#% 319508
#% 407995
#% 415957
#% 427199
#% 435137
#% 443128
#% 445701
#% 463597
#% 480093
#% 481620
#% 527005

#index 481942
#* Implementation and Analysis of a Parallel Collection Query Language
#@ Dan Suciu
#t 1996
#c 4
#% 58352
#% 68672
#% 80192
#% 102763
#% 115661
#% 119415
#% 139234
#% 148281
#% 152904
#% 162111
#% 162964
#% 172939
#% 194343
#% 201873
#% 395735
#% 435157
#% 442703
#% 463602
#% 464540
#% 480113
#% 480960

#index 481943
#* Tribeca: A Stream Database Manager for Network Traffic Analysis
#@ Mark Sullivan
#t 1996
#c 4

#index 481944
#* SchemaSQL - A Language for Interoperability in Relational Multi-Database Systems
#@ Laks V. S. Lakshmanan;Fereidoon Sadri;Iyer N. Subramanian
#t 1996
#c 4
#% 13014
#% 82346
#% 102748
#% 111913
#% 116091
#% 123121
#% 158200
#% 158908
#% 169057
#% 189739
#% 213969
#% 268747
#% 435103
#% 435111
#% 464215
#% 480285
#% 481096
#% 535964

#index 481945
#* SPRINT: A Scalable Parallel Classifier for Data Mining
#@ John C. Shafer;Rakesh Agrawal;Manish Mehta
#t 1996
#c 4
#% 4868
#% 90661
#% 136350
#% 153021
#% 191910
#% 340670
#% 369236
#% 369349
#% 442700
#% 449588
#% 452821
#% 459008
#% 480940
#% 481945
#% 672250

#index 481946
#* Bellcore's ADAPT/X Harness System for Managing Information on Internet and Intranets
#@ Amit P. Sheth
#t 1996
#c 4

#index 481947
#* Fast Nearest Neighbor Search in Medical Image Databases
#@ Flip Korn;Nikolaos Sidiropoulos;Christos Faloutsos;Eliot Siegel;Zenon Protopapas
#t 1996
#c 4
#% 13041
#% 25442
#% 58636
#% 58638
#% 64431
#% 86950
#% 86951
#% 88056
#% 102772
#% 103936
#% 169940
#% 172949
#% 176247
#% 201876
#% 217205
#% 319508
#% 321455
#% 322258
#% 377548
#% 411694
#% 427199
#% 437405
#% 452795
#% 460862
#% 462503
#% 480093
#% 480610
#% 481455
#% 565447
#% 837641

#index 481948
#* Storage Estimation for Multidimensional Aggregates in the Presence of Hierarchies
#@ Amit Shukla;Prasad Deshpande;Jeffrey F. Naughton;Karthikeyan Ramasamy
#t 1996
#c 4
#% 2833
#% 210182
#% 464215
#% 481749
#% 481951

#index 481949
#* Constructing Efficient Decision Trees by Using Optimized Numeric Association Rules
#@ Takeshi Fukuda;Yasuhiko Morimoto;Shinichi Morishita;Takeshi Tokuyama
#t 1996
#c 4
#% 8949
#% 61792
#% 136350
#% 144288
#% 210162
#% 213977
#% 252370
#% 281856
#% 408396
#% 449588
#% 452821
#% 459008
#% 480940

#index 481950
#* ZOO: A Desktop Experiment Management Environment
#@ Yannis E. Ioannidis;Miron Livny;S. Gupta;Nagavamsi Ponnekanti
#t 1996
#c 4
#% 102748
#% 108505
#% 169944
#% 172939
#% 188958
#% 479593
#% 481750
#% 503555
#% 503710
#% 562125
#% 563768
#% 725461

#index 481951
#* On the Computation of Multidimensional Aggregates
#@ Sameet Agarwal;Rakesh Agrawal;Prasad Deshpande;Ashish Gupta;Jeffrey F. Naughton;Raghu Ramakrishnan;Sunita Sarawagi
#t 1996
#c 4
#% 136740
#% 201883
#% 210182
#% 408396
#% 442685
#% 442695
#% 442918
#% 464215
#% 481749
#% 481948
#% 482049

#index 481952
#* Integrating Triggers and Declarative Constraints in SQL Database Sytems
#@ Roberta Cochrane;Hamid Pirahesh;Nelson Mendonça Mattos
#t 1996
#c 4
#% 1797
#% 37972
#% 62020
#% 86942
#% 86946
#% 123589
#% 149590
#% 172962
#% 391760
#% 443019
#% 463267
#% 480288
#% 480435
#% 480620
#% 480765
#% 480768
#% 480776
#% 481773

#index 481953
#* How System 11 SQL Server Became Fast
#@ T. K. Rengarajan
#t 1996
#c 4

#index 481954
#* A New SQL-like Operator for Mining Association Rules
#@ Rosa Meo;Giuseppe Psaila;Stefano Ceri
#t 1996
#c 4
#% 36683
#% 90661
#% 136347
#% 152934
#% 172949
#% 196825
#% 201894
#% 460862
#% 463883
#% 463903
#% 464215
#% 480940
#% 481290
#% 481588
#% 481609
#% 481611
#% 481758

#index 481955
#* Intra-Transaction Parallelism in the Mapping of an Object Model to a Relational Multi-Processor System
#@ Michael Rys;Moira C. Norrie;Hans-Jörg Schek
#t 1996
#c 4
#% 32887
#% 32914
#% 58372
#% 59348
#% 91076
#% 116185
#% 122917
#% 124626
#% 158047
#% 286174
#% 435120
#% 443029
#% 463594

#index 481956
#* The X-tree: An Index Structure for High-Dimensional Data
#@ Stefan Berchtold;Daniel A. Keim;Hans-Peter Kriegel
#t 1996
#c 4
#% 86950
#% 102772
#% 121989
#% 131061
#% 169940
#% 201893
#% 238764
#% 285932
#% 411694
#% 427199
#% 435141
#% 460862
#% 462969
#% 463414
#% 464195
#% 480093
#% 480587

#index 482081
#* Answering Queries with Aggregation Using Views
#@ Divesh Srivastava;Shaul Dar;H. V. Jagadish;Alon Y. Levy
#t 1996
#c 4
#% 44638
#% 86946
#% 137867
#% 169844
#% 172874
#% 188853
#% 198467
#% 201898
#% 201928
#% 411750
#% 458550
#% 458556
#% 463735
#% 464056
#% 477212
#% 481288
#% 481293
#% 481444
#% 481604
#% 481923
#% 564419
#% 565259

#index 482082
#* Querying Multiple Features of Groups in Relational Databases
#@ Damianos Chatziantoniou;Kenneth A. Ross
#t 1996
#c 4
#% 102748
#% 111912
#% 191175
#% 210183
#% 403746
#% 458550
#% 463735
#% 464215
#% 481288
#% 481293
#% 481604
#% 481608
#% 565259
#% 568155

#index 482083
#* Query Decomposition and View Maintenance for Query Languages for Unstructured Data
#@ Dan Suciu
#t 1996
#c 4
#% 201929
#% 210214
#% 213982
#% 459260
#% 463919
#% 481602
#% 979358

#index 482084
#* Database Management Systems and the Internet
#@ Susan Malaika
#t 1996
#c 4

#index 482085
#* DWMS: Data Warehouse Management System
#@ Narendra Mohan
#t 1996
#c 4

#index 482086
#* Reordering Query Execution in Tertiary Memory Databases
#@ Sunita Sarawagi;Michael Stonebraker
#t 1996
#c 4
#% 121
#% 32884
#% 43170
#% 77648
#% 86929
#% 102761
#% 111368
#% 136740
#% 152939
#% 159337
#% 169848
#% 172876
#% 172900
#% 201897
#% 202140
#% 340669
#% 389798
#% 442714
#% 452769
#% 462941
#% 463415
#% 463444
#% 463578
#% 464065
#% 481606
#% 481613
#% 503721
#% 571217
#% 978725

#index 482087
#* PESTO: An Integrated Query/Browser for Object Databases
#@ Michael J. Carey;Laura M. Haas;Vivekananda Maganty;John H. Williams
#t 1996
#c 4
#% 77933
#% 82353
#% 157234
#% 196474
#% 238757
#% 318439
#% 395735
#% 427204
#% 463868
#% 481100
#% 534683
#% 614579

#index 482088
#* The Design and Implementation of a Sequence Database System
#@ Praveen Seshadri;Miron Livny;Raghu Ramakrishnan
#t 1996
#c 4
#% 32915
#% 91459
#% 102763
#% 123111
#% 152936
#% 163442
#% 172939
#% 172950
#% 464007
#% 464058
#% 480938
#% 480946
#% 481255
#% 481428

#index 482089
#* The Role of Integrity Constraints in Database Interoperation
#@ Mark W. W. Vermeer;Peter M. G. Apers
#t 1996
#c 4
#% 22948
#% 85086
#% 146205
#% 172972
#% 197383
#% 463445
#% 464074
#% 487756
#% 489209
#% 535351
#% 535513

#index 482090
#* Clustering Techniques for Minimizing External Path Length
#@ A. A. Diwan;Sanjeeva Rane;S. Seshadri;S. Sudarshan
#t 1996
#c 4
#% 8917
#% 47621
#% 58369
#% 58374
#% 86950
#% 86951
#% 86952
#% 102745
#% 102746
#% 116057
#% 137889
#% 168772
#% 287716
#% 427199
#% 480093
#% 480790
#% 527002
#% 527802

#index 482091
#* Querying a Multilevel Database: A Logical Analysis
#@ Frédéric Cuppens
#t 1996
#c 4
#% 59542
#% 69537
#% 109945
#% 480945
#% 488180
#% 507241
#% 507255

#index 482092
#* Selectivity Estimation Without the Attribute Value Independence Assumption
#@ Viswanath Poosala;Yannis E. Ioannidis
#t 1997
#c 4
#% 3453
#% 43163
#% 82346
#% 86951
#% 132779
#% 152585
#% 152917
#% 201921
#% 210190
#% 285924
#% 287747
#% 411554
#% 427219
#% 481266
#% 481455
#% 482092
#% 689389

#index 482093
#* A Foundation for Multi-dimensional Databases
#@ Marc Gyssens;Laks V. S. Lakshmanan
#t 1997
#c 4
#% 201950
#% 210182
#% 211575
#% 213969
#% 214667
#% 289370
#% 464215
#% 481951
#% 482049
#% 552970

#index 482094
#* Concurrent Garbage Collection in O2
#@ Marcin Skubiszewski;Patrick Valduriez
#t 1997
#c 4
#% 117
#% 44948
#% 111351
#% 125595
#% 139030
#% 212673
#% 227930
#% 320266
#% 403195
#% 473366
#% 481756
#% 507436

#index 482095
#* Recovering Information from Summary Data
#@ Christos Faloutsos;H. V. Jagadish;Nikolaos Sidiropoulos
#t 1997
#c 4
#% 663
#% 132779
#% 152588
#% 152928
#% 154387
#% 172902
#% 182427
#% 198467
#% 199537
#% 199848
#% 201921
#% 210182
#% 213975
#% 285924
#% 394984
#% 404762
#% 411554
#% 464215
#% 480249
#% 480279
#% 481604
#% 482081

#index 482096
#* Critical Database Technologies for High Energy Physics
#@ David M. Malon;Edward N. May
#t 1997
#c 4
#% 340605
#% 380441

#index 482097
#* Integrating Reliable Memory in Databases
#@ Wee Teck Ng;Peter M. Chen
#t 1997
#c 4
#% 117
#% 713
#% 12638
#% 70161
#% 77978
#% 107720
#% 116078
#% 128310
#% 151536
#% 151540
#% 153787
#% 156387
#% 159275
#% 160425
#% 176348
#% 183453
#% 202140
#% 202154
#% 213467
#% 214984
#% 239972
#% 317988
#% 319473
#% 422875
#% 427195
#% 442373
#% 442832
#% 473081
#% 480096
#% 480617
#% 482070
#% 531907
#% 602679

#index 482098
#* Multiple-View Self-Maintenance in Data Warehousing Environments
#@ Nam Huyn
#t 1997
#c 4
#% 36181
#% 36683
#% 54712
#% 135476
#% 152928
#% 164364
#% 200966
#% 201928
#% 340301
#% 458556
#% 481128
#% 481933
#% 599549

#index 482099
#* Garbage Collection in Object Oriented Databases Using Transactional Cyclic Reference Counting
#@ Srinivas Ashwin;Prasan Roy;S. Seshadri;Abraham Silberschatz;S. Sudarshan
#t 1997
#c 4
#% 2904
#% 70099
#% 152904
#% 172938
#% 463741
#% 481756

#index 482100
#* An Efficient Cost-Driven Index Selection Tool for Microsoft SQL Server
#@ Surajit Chaudhuri;Vivek R. Narasayya
#t 1997
#c 4
#% 36119
#% 91603
#% 146056
#% 210182
#% 210208
#% 346894
#% 458523
#% 462079
#% 462204
#% 565429
#% 566118

#index 482101
#* Innovation in Database Management: Computer Science vs. Engineering
#@ Kenneth R. Jacobs
#t 1997
#c 4

#index 482102
#* Implementing Abstract Objects with Inheritance in Datalogneg
#@ Hasan M. Jamil
#t 1997
#c 4
#% 30566
#% 57386
#% 152907
#% 189739
#% 411614
#% 458534
#% 464695
#% 480952
#% 501120

#index 482103
#* Optimizing Queries with Universal Quantification in Object-Oriented and Object-Relational Databases
#@ Jens Claußen;Alfons Kemper;Guido Moerkotte;Klaus Peithner
#t 1997
#c 4
#% 3771
#% 32878
#% 58359
#% 78103
#% 83148
#% 86947
#% 88050
#% 123589
#% 136740
#% 152942
#% 189638
#% 210183
#% 380441
#% 411662
#% 415987
#% 462783
#% 463871
#% 463894
#% 480091
#% 481621
#% 482121
#% 836134

#index 482104
#* A One-Pass Algorithm for Accurately Estimating Quantiles for Disk-Resident Data
#@ Khaled Alsabti;Sanjay Ranka;Vineet Singh
#t 1997
#c 4
#% 2152
#% 43163
#% 152934
#% 158987
#% 210160
#% 210190
#% 321186
#% 340670
#% 427219
#% 481609

#index 482105
#* Efficient Construction of Regression Trees with Range and Region Splitting
#@ Yasuhiko Morimoto;Hiromu Ishii;Shinichi Morishita
#t 1997
#c 4
#% 136350
#% 152934
#% 201894
#% 210160
#% 210162
#% 213977
#% 281856
#% 449588
#% 459008
#% 481290
#% 481588
#% 481949

#index 482106
#* Geo/Environmental and Medical Data Management in the RasDaMan System
#@ Peter Baumann;Paula Furtado;Roland Ritsch;Norbert Widmann
#t 1997
#c 4
#% 71862
#% 125600
#% 237198
#% 380441
#% 435138
#% 442826

#index 482107
#* Adaptive Data Broadcast in Hybrid Networks
#@ Konstantinos Stathatos;Nick Roussopoulos;John S. Baras
#t 1997
#c 4
#% 32884
#% 66172
#% 86748
#% 124011
#% 152943
#% 169835
#% 172876
#% 175253
#% 201897
#% 210777
#% 227885
#% 290477
#% 462077
#% 464065
#% 628099
#% 1862956

#index 482108
#* Using Probabilistic Information in Data Integration
#@ Daniela Florescu;Daphne Koller;Alon Y. Levy
#t 1997
#c 4
#% 44876
#% 152585
#% 159113
#% 161754
#% 210176
#% 213437
#% 464717
#% 481923
#% 648114
#% 1478789
#% 1499470

#index 482109
#* Efficient User-Adaptable Similarity Search in Large Multimedia Databases
#@ Thomas Seidl;Hans-Peter Kriegel
#t 1997
#c 4
#% 1693
#% 86950
#% 102772
#% 158905
#% 169940
#% 172908
#% 172949
#% 227856
#% 227999
#% 237187
#% 427199
#% 443889
#% 445701
#% 460862
#% 480093
#% 481609
#% 481947
#% 481956
#% 527004
#% 527026
#% 571100

#index 482110
#* Data Warehouse Configuration
#@ Dimitri Theodoratos;Timos K. Sellis
#t 1997
#c 4
#% 13016
#% 36117
#% 44638
#% 59350
#% 83315
#% 98469
#% 152928
#% 169337
#% 181035
#% 198465
#% 199537
#% 201928
#% 201929
#% 210182
#% 210208
#% 340300
#% 340301
#% 368248
#% 442663
#% 442767
#% 458556
#% 462204
#% 462645
#% 464056
#% 464706
#% 481128
#% 481444
#% 481604
#% 481933
#% 482081
#% 564419
#% 566126

#index 482111
#* Algorithms for Materialized View Design in Data Warehousing Environment
#@ Jian Yang;Kamalakar Karlapalem;Qing Li
#t 1997
#c 4
#% 182899
#% 209633
#% 210182
#% 213206
#% 463735
#% 480091
#% 481288
#% 614618
#% 635798
#% 788999

#index 482112
#* Multidimensional Access Methods: Trees Have Grown Everywhere
#@ Timos K. Sellis;Nick Roussopoulos;Christos Faloutsos
#t 1997
#c 4

#index 482113
#* Using Taxonomy, Discriminants, and Signatures for Navigating in Text Databases
#@ Soumen Chakrabarti;Byron Dom;Rakesh Agrawal;Prabhakar Raghavan
#t 1997
#c 4
#% 46803
#% 80995
#% 90661
#% 99690
#% 103267
#% 115462
#% 115476
#% 115608
#% 136350
#% 144023
#% 144031
#% 165110
#% 169781
#% 191680
#% 194283
#% 201073
#% 204430
#% 232717
#% 282102
#% 375017
#% 406493
#% 465747

#index 482114
#* GTE SuperPages: Using IR Techniques for Searching Complex Objects
#@ Steven D. Whitehead;Himanshu Sinha;Michael Murphy
#t 1997
#c 4

#index 482115
#* The Complexity of Transformation-Based Join Enumeration
#@ Arjan Pellenkoft;César A. Galindo-Legaria;Martin L. Kersten
#t 1997
#c 4
#% 32877
#% 32889
#% 43161
#% 83154
#% 102765
#% 152942
#% 210166
#% 210206
#% 210207
#% 220425
#% 458550
#% 464558
#% 481104
#% 565457
#% 651607

#index 482116
#* Describing and Using Query Capabilities of Heterogeneous Sources
#@ Vasilis Vassalos;Yannis Papakonstantinou
#t 1997
#c 4
#% 58354
#% 64424
#% 198465
#% 198466
#% 213982
#% 227987
#% 340302
#% 368248
#% 384978
#% 459241
#% 464203
#% 479449
#% 479452
#% 481923
#% 903261

#index 482117
#* On-Demand Data Elevation in Hierarchical Multimedia Storage Servers
#@ Peter Triantafillou;Thomas Papadakis
#t 1997
#c 4
#% 43172
#% 172881
#% 173689
#% 211475
#% 249264
#% 464031
#% 1797483

#index 482118
#* Vertical Data Migration in Large Near-Line Document Archives Based on Markov-Chain Predictions
#@ Achim Kraiss;Gerhard Weikum
#t 1997
#c 4
#% 13031
#% 32910
#% 43171
#% 58374
#% 79312
#% 91633
#% 102744
#% 102745
#% 116057
#% 152939
#% 152943
#% 159079
#% 169839
#% 170893
#% 187079
#% 201696
#% 202140
#% 210181
#% 212063
#% 319504
#% 340631
#% 374002
#% 460666
#% 462043
#% 462203
#% 464040
#% 464228
#% 480780
#% 481603
#% 481613
#% 511335
#% 555029
#% 565466
#% 566126
#% 612099

#index 482119
#* The Network as a Global Database: Challenges of Interoperability, Proactivity, Interactiveness, Legacy
#@ Peter C. Lockemann;Ulrike Kölsch;Arne Koschel;Ralf Kramer;Ralf Nikolai;Mechtild Wallrath;Hans-Dirk Walter
#t 1997
#c 4
#% 39317
#% 86332
#% 91724
#% 116185
#% 156979
#% 181537
#% 181579
#% 211571
#% 212556
#% 223772
#% 224186
#% 441237
#% 480796
#% 501927
#% 501941
#% 563580
#% 567352
#% 592801
#% 622210

#index 482120
#* Finding Data in the Neighborhood
#@ André Eickler;Alfons Kemper;Donald Kossmann
#t 1997
#c 4
#% 152947
#% 158845
#% 172918
#% 172939
#% 179135
#% 286929
#% 340297
#% 481296
#% 481619
#% 564253
#% 674559

#index 482121
#* Evaluation of Main Memory Join Algorithms for Joins with Set Comparison Join Predicates
#@ Sven Helmer;Guido Moerkotte
#t 1997
#c 4
#% 3771
#% 5379
#% 18614
#% 42401
#% 57955
#% 77936
#% 77963
#% 83232
#% 86928
#% 86948
#% 114577
#% 152937
#% 152938
#% 171732
#% 191154
#% 210166
#% 210186
#% 210187
#% 287489
#% 318437
#% 340670
#% 380441
#% 427195
#% 442918
#% 463595
#% 463759
#% 476621
#% 479753
#% 479905
#% 479924
#% 480272
#% 480774
#% 481417
#% 481435
#% 481589
#% 562280
#% 676541
#% 677322

#index 482122
#* Facilitating Multimedia Database Exploration through Visual Interfaces and Perpetual Query Reformulations
#@ Wen-Syan Li;K. Selçuk Candan;Kyoji Hirata;Yoshinori Hara
#t 1997
#c 4
#% 67565
#% 144023
#% 169815
#% 172725
#% 198058
#% 211510
#% 219847
#% 437405
#% 437407
#% 464046
#% 482087
#% 632344

#index 482123
#* Fast Incremental Maintenance of Approximate Histograms
#@ Phillip B. Gibbons;Yossi Matias;Viswanath Poosala
#t 1997
#c 4
#% 1331
#% 102784
#% 210190
#% 242366
#% 285924
#% 411554
#% 481775
#% 689389

#index 564416
#* Physical Data Independence, Constraints, and Optimization with Universal Plans
#@ Alin Deutsch;Lucian Popa;Val Tannen
#t 1999
#c 4
#% 583
#% 18614
#% 32877
#% 58356
#% 69272
#% 77656
#% 83148
#% 83537
#% 86954
#% 111372
#% 116090
#% 116303
#% 120672
#% 145178
#% 169844
#% 188853
#% 198465
#% 198466
#% 210176
#% 210184
#% 210203
#% 235914
#% 238413
#% 256719
#% 286189
#% 287316
#% 287336
#% 340619
#% 384978
#% 411554
#% 442665
#% 462060
#% 463919
#% 464056
#% 464203
#% 481444
#% 562153
#% 564419
#% 571091
#% 599549

#index 564420
#* OLAP++: Powerful and Easy-to-Use Federations of OLAP and Object Databases
#@ Junmin Gu;Torben Bach Pedersen;Arie Shoshani
#t 2000
#c 4
#% 188958
#% 235914

#index 564437
#* Large Databases for Remote Sensing and GIS
#@ A. R. Dasgupta
#t 1996
#c 4

#index 565184
#* Information Management for Genome Level Bioinformatics
#@ Norman W. Paton;Carole A. Goble
#t 2001
#c 4

#index 565185
#* Managing Business Processes via Workflow Technology
#@ Frank Leymann
#t 2001
#c 4

#index 566125
#* Performance of Future Database Systems: Bottlenecks and Bonananzas
#@ Chaitanya K. Baru
#t 1996
#c 4

#index 566126
#* WATCHMAN: A Data Warehouse Intelligent Cache Manager
#@ Peter Scheuermann;Junho Shim;Radek Vingralek
#t 1996
#c 4
#% 735
#% 32910
#% 43171
#% 44638
#% 58358
#% 135476
#% 152943
#% 169844
#% 172930
#% 199537
#% 201951
#% 210182
#% 261208
#% 287767
#% 404765
#% 408396
#% 442388
#% 442888
#% 480260
#% 481604
#% 571216
#% 635198

#index 566127
#* Resource Scheduling in Enhanced Pay-Per-View Continuous Media Databases
#@ Minos N. Garofalakis;Banu Özden;Abraham Silberschatz
#t 1997
#c 4
#% 43172
#% 57049
#% 149275
#% 151340
#% 172881
#% 183436
#% 201844
#% 204542
#% 243299
#% 288821
#% 288942
#% 395431
#% 395619
#% 408396
#% 452791
#% 462075
#% 481438

#index 566128
#* STING: A Statistical Information Grid Approach to Spatial Data Mining
#@ Wei Wang;Jiong Yang;Richard R. Muntz
#t 1997
#c 4
#% 68091
#% 152902
#% 210173
#% 443082
#% 481281
#% 527022

#index 566129
#* KODA - The Architecture And Interface of a Data Model Independent Kernel
#@ Gopalan Arun;Ashok Joshi
#t 1998
#c 4
#% 480767

#index 566130
#* Experiences in Federated Databases: From IRO-DB to MIRO-Web
#@ Peter Fankhauser;Georges Gardarin;M. Lopez;J. Muñoz;Anthony Tomasic
#t 1998
#c 4
#% 85086
#% 194964
#% 194984
#% 443235
#% 459261
#% 481939
#% 565470

#index 566131
#* Objectivity Industrial Exhibit
#@  Objectivity
#t 1998
#c 4

#index 566132
#* Mining Deviants in a Time Series Database
#@ H. V. Jagadish;Nick Koudas;S. Muthukrishnan
#t 1999
#c 4
#% 201921
#% 210190
#% 479648
#% 479791
#% 481266
#% 482092
#% 482123

#index 566133
#* Hyper-Programming in Java
#@ Evangelos Zirintsis;Graham N. C. Kirby;Ronald Morrison
#t 1999
#c 4
#% 221392
#% 489996
#% 568968

#index 566134
#* The Persistent Cache: Improving OID Indexing in Temporal Object-Oriented Database Systems
#@ Kjetil Nørvåg
#t 1999
#c 4
#% 36683
#% 42405
#% 58371
#% 368248
#% 463430
#% 479483
#% 481619
#% 562804
#% 705538

#index 566135
#* Efficient Numerical Error Bounding for Replicated Network Services
#@ Haifeng Yu;Amin Vahdat
#t 2000
#c 4
#% 77005
#% 86950
#% 100593
#% 102804
#% 124019
#% 150431
#% 152911
#% 176491
#% 227945
#% 237205
#% 240016
#% 248028
#% 262135
#% 286469
#% 287324
#% 458535
#% 458544
#% 480103
#% 602781
#% 610673
#% 665384

#index 566136
#* Memex: A Browsing Assistant for Collaborative Archiving and Mining of Surf Trails
#@ Soumen Chakrabarti;Sandeep Srivastava;Mallela Subramanyam;Mitul Tiwari
#t 2000
#c 4
#% 144023
#% 209690
#% 268106
#% 281251
#% 281253
#% 571073

#index 566137
#* Building and Customizing Data-Intensive Web Sites Using Weave
#@ Khaled Yagoub;Daniela Florescu;Valérie Issarny;Patrick Valduriez
#t 2000
#c 4

#index 566138
#* PicoDMBS: Scaling Down Database Techniques for the Smartcard
#@ Christophe Bobineau;Luc Bouganim;Philippe Pucheral;Patrick Valduriez
#t 2000
#c 4
#% 18614
#% 136740
#% 259996
#% 411664
#% 440254
#% 464835
#% 480439
#% 978215
#% 978249

#index 566139
#* PicoDBMS: Validation and Experience
#@ Nicolas Anciaux;Christophe Bobineau;Luc Bouganim;Philippe Pucheral;Patrick Valduriez
#t 2001
#c 4
#% 566138

#index 566140
#* NetCube: A Scalable Tool for Fast Data Mining and Compression
#@ Dimitris Margaritis;Christos Faloutsos;Sebastian Thrun
#t 2001
#c 4
#% 44876
#% 210182
#% 273902
#% 273903
#% 273905
#% 273916
#% 280436
#% 280502
#% 464215
#% 479484
#% 479808
#% 479822
#% 479984
#% 527664
#% 527691

#index 566141
#* DB2 Spatial Extender - Spatial data within the RDBMS
#@ David W. Adler
#t 2001
#c 4
#% 479619

#index 824650
#* Why search engines are used increasingly to offload queries from databases
#@ Bjørn Olstad
#t 2005
#c 4
#! The development of future search engine technology is no longer limited to free text. Rather, the aim is to build core indexing services that focus on extreme performance and scalability for retrieval and analysis across structured and unstructured data sources alike. In addition, binary query evaluation is being replaced with advanced frameworks that provide both fuzzy matching and ranking schemes, to separate value from noise. As another trend, analytical applications are being enabled by the computation of contextual concept relationships across billions of documents/records on-the-fly.Based on these developments in search engine technology, a set of new information retrieval infrastructure patterns are appearing:1. the mirroring of DB content into a search engine in order to improve query capacity and user experience,2. the use of search engine technology as the default access pattern to both structured and unstructured data in applications such as CRM and storage and document management, and3. a paradigm shift is predicted in business intelligence.The presentation will review key trends from search engine development and relate these to concrete user scenarios.

#index 824651
#* Database-inspired search
#@ David Konopnicki;Oded Shmueli
#t 2005
#c 4
#% 64902
#% 107693
#% 115436
#% 188853
#% 238757
#% 248819
#% 248852
#% 261741
#% 268079
#% 273927
#% 275922
#% 278831
#% 303893
#% 333845
#% 340141
#% 340295
#% 348137
#% 397608
#% 464825
#% 479471
#% 479649
#% 480330
#% 480632
#% 481125
#% 481439
#% 571040
#% 571297
#% 577302
#% 614036
#% 614598
#% 631988
#% 632035
#% 642993
#% 643566
#% 754116
#% 765408
#% 765410
#% 765418
#% 765464
#% 783560
#% 787547
#% 801677
#% 805850
#% 805884
#% 807446
#% 840878
#% 1016183
#% 1016191
#% 1016197
#! "W3QL: A Query Language for the WWW", published in 1995, presented a language with several distinctive features. Employing existing indexes as access paths, it allowed the selection of documents using conditions on semi-structured documents and maintaining dynamic views of navigational queries. W3QL was capable of automatically filling out forms and navigating through them. Finally, in the SQL tradition, it was a declarative query language, that could be the subject of optimization.Ten years later, we examine some current trends in the domain of search, namely the emergence of system-level search services and of the semantic web. In this context, we explore whether W3QL's ideas are still relevant to help improve information search and retrieval. We identify two main environments for searching, the enterprise and the web at large. Both environments could benefit from database-inspired integration language, and an execution system that implements it.

#index 824652
#* Sketching streams through the net: distributed approximate query tracking
#@ Graham Cormode;Minos Garofalakis
#t 2005
#c 4
#% 214073
#% 273682
#% 333931
#% 379445
#% 397354
#% 397385
#% 480628
#% 480805
#% 492912
#% 654443
#% 654482
#% 654488
#% 654497
#% 800582
#% 801695
#% 810009
#% 816392
#% 993960
#% 1016155
#% 1016178
#! Emerging large-scale monitoring applications require continuous tracking of complex data-analysis queries over collections of physically-distributed streams. Effective solutions have to be simultaneously space/time efficient (at each remote monitor site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality approximate query answers. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking a broad class of complex aggregate queries in such a distributed-streams setting. Our tracking schemes maintain approximate query answers with provable error guarantees, while simultaneously optimizing the storage space and processing time at each remote site, and the communication cost across the network. They rely on tracking general-purpose randomized sketch summaries of local streams at remote sites along with concise prediction models of local site behavior in order to produce highly communication- and space/time-efficient solutions. The result is a powerful approximate query tracking framework that readily incorporates several complex analysis queries (including distributed join and multi-join aggregates, and approximate wavelet representations), thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model.

#index 824653
#* Summarizing and mining inverse distributions on data streams via dynamic inverse sampling
#@ Graham Cormode;S. Muthukrishnan;Irina Rozenbaum
#t 2005
#c 4
#% 1331
#% 2833
#% 190611
#% 214073
#% 238182
#% 248812
#% 248821
#% 273908
#% 299989
#% 336610
#% 378388
#% 379445
#% 397426
#% 446438
#% 453512
#% 480805
#% 482123
#% 548654
#% 576119
#% 654463
#% 654497
#% 654508
#% 654510
#% 765404
#% 805745
#% 993960
#! Emerging data stream management systems approach the challenge of massive data distributions which arrive at high speeds while there is only small storage by summarizing and mining the distributions using samples or sketches. However, data distributions can be "viewed" in different ways. A data stream of integer values can be viewed either as the forward distribution f (x), ie., the number of occurrences of x in the stream, or as its inverse, f-1 (i), which is the number of items that appear i times. While both such "views" are equivalent in stored data systems, over data streams that entail approximations, they may be significantly different. In other words, samples and sketches developed for the forward distribution may be ineffective for summarizing or mining the inverse distribution. Yet, many applications such as IP traffic monitoring naturally rely on mining inverse distributions.We formalize the problems of managing and mining inverse distributions and show provable differences between summarizing the forward distribution vs the inverse distribution. We present methods for summarizing and mining inverse distributions of data streams: they rely on a novel technique to maintain a dynamic sample over the stream with provable guarantees which can be used for variety of summarization tasks (building quantiles or equidepth histograms) and mining (anomaly detection: finding heavy hitters, and measuring the number of rare items), all with provable guarantees on quality of approximations and time/space used by our streaming methods.We also complement our analytical and algorithmic results by presenting an experimental study of the methods over network data streams.

#index 824654
#* Adaptive stream filters for entity-based queries with non-value tolerance
#@ Reynold Cheng;Ben Kao;Sunil Prabhakar;Alan Kwan;Yicheng Tu
#t 2005
#c 4
#% 188641
#% 295512
#% 333863
#% 631935
#% 654443
#% 654487
#% 654488
#% 654508
#% 742565
#% 765402
#% 801695
#% 823653
#% 1015297
#% 1015325
#% 1016178
#% 1016196
#! We study the problem of applying adaptive filters for approximate query processing in a distributed stream environment. We propose filter bound assignment protocols with the objective of reducing communication cost. Most previous works focus on value-based queries (e.g., average) with numerical error tolerance. In this paper, we cover entity-based queries (e.g., nearest neighbor) with non-value-based error tolerance. We investigate different non-value-based error tolerance definitions and discuss how they are applied to two classes of entity-based queries: non-rank-based and rank-based queries. Extensive experiments show that our protocols achieve significant savings in both communication overhead and server computation.

#index 824655
#* Improving database performance on simultaneous multithreading processors
#@ Jingren Zhou;John Cieslewicz;Kenneth A. Ross;Mihir Shah
#t 2005
#c 4
#% 202503
#% 251477
#% 270919
#% 300194
#% 333940
#% 333942
#% 333949
#% 337054
#% 337055
#% 397362
#% 479819
#% 479821
#% 480119
#% 610295
#% 657626
#% 745066
#% 745501
#% 765417
#% 1015288
#% 1016184
#! Simultaneous multithreading (SMT) allows multiple threads to supply instructions to the instruction pipeline of a superscalar processor. Because threads share processor resources, an SMT system is inherently different from a multiprocessor system and, therefore, utilizing multiple threads on an SMT processor creates new challenges for database implementers.We investigate three thread-based techniques to exploit SMT architectures on memory-resident data. First, we consider running independent operations in separate threads, a technique applied to conventional multi-processor systems. Second, we describe a novel implementation strategy in which individual operators are implemented in a multi-threaded fashion. Finally, we introduce a new data-structure called a work-ahead set that allows us to use one of the threads to aggressively preload data into the cache.We evaluate each method with respect to its performance, implementation complexity, and other measures. We also provide guidance regarding when and how to best utilize the various threading techniques. Our experimental results show that by taking advantage of SMT technology we achieve a 30% to 70% improvement in throughput over single threaded implementations on in-memory database operations.

#index 824656
#* Parallel querying with non-dedicated computers
#@ Vijayshankar Raman;Wei Han;Inderpal Narang
#t 2005
#c 4
#% 86929
#% 115661
#% 188719
#% 211567
#% 300167
#% 340663
#% 397393
#% 442698
#% 442700
#% 458611
#% 463095
#% 479449
#% 481099
#% 481617
#% 481753
#% 765470
#% 785188
#% 1015282
#! We present DITN, a new method of parallel querying based on dynamic outsourcing of join processing tasks to non-dedicated, heterogeneous computers. In DITN, partitioning is not the means of parallelism. Data layout decisions are taken outside the scope of the DBMS, and handled within the storage software; query processors see a "Data In The Network" image. This allows gradual scaleout as the workload grows, by using non-dedicated computers.A typical operator in a parallel query plan is Exchange [7]. We argue that Exchange is unsuitable for non-dedicated machines because it poorly addresses node heterogeneity, and is vulnerable to failures or load spikes during query execution. DITN uses an alternate intra-fragment parallelism where each node executes an independent select-project-join-aggregate-group by block, with no tuple exchange between nodes. This method cleanly handles heterogeneous nodes, and well adapts during execution to node failures or load spikes.Initial experiments suggest that DITN performs competitively with a traditional configuration of dedicated machines and well-partitioned data for up to 10 processors at least. At the same time, DITN gives significant flexibility in terms of gradual scaleout and handling of heterogeneity, load bursts, and failures.

#index 824657
#* Optimistic intra-transaction parallelism on chip multiprocessors
#@ Christopher B. Colohan;Anastassia Ailamaki;J. Gregory Steffan;Todd C. Mowry
#t 2005
#c 4
#% 32897
#% 114582
#% 148195
#% 190645
#% 286836
#% 296181
#% 303987
#% 337039
#% 340308
#% 365700
#% 439991
#% 440294
#% 481955
#% 581173
#% 610212
#% 979743
#! With the advent of chip multiprocessors, exploiting intra-transaction parallelism is an attractive way of improving transaction performance. However, exploiting intra-transaction parallelism in existing database systems is difficult, for two reasons: first, significant changes are required to avoid races or conflicts within the DBMS, and second, adding threads to transactions requires a high level of sophistication from transaction programmers. In this paper we show how dividing a transaction into speculative threads solves both problems---it minimizes the changes required to the DBMS, and the details of parallelization are hidden from the transaction programmer. Our technique requires a limited number of small, localized changes to a subset of the low-level data structures in the DBMS. Through this method of parallelizing transactions we can dramatically improve performance: on a simulated 4-processor chip-multiprocessor, we improve the response time by 36-74% for three of the five TPC-C transactions.

#index 824658
#* Information preserving XML schema embedding
#@ Philip Bohannon;Wenfei Fan;Michael Flaster;P. P. S. Narayan
#t 2005
#c 4
#% 11284
#% 50073
#% 166203
#% 198465
#% 248038
#% 291299
#% 307632
#% 328424
#% 332166
#% 333857
#% 333990
#% 378409
#% 378410
#% 443408
#% 479783
#% 480645
#% 480969
#% 481944
#% 572314
#% 641044
#% 654457
#% 654468
#% 660001
#% 723448
#% 809236
#% 809248
#% 810067
#% 824660
#% 994015
#! A fundamental concern of information integration in an XML context is the ability to embed one or more source documents in a target document so that (a) the target document conforms to a target schema and (b) the information in the source document(s) is preserved. In this paper, information preservation for XML is formally studied, and the results of this study guide the definition of a novel notion of schema embedding between two XML DTD schemas represented as graphs. Schema embedding generalizes the conventional notion of graph similarity by allowing an edge in a source DTD schema to be mapped to a path in the target DTD. Instance-level embeddings can be defined from the schema embedding in a straightforward manner, such that conformance to a target schema and information preservation are guaranteed. We show that it is NP-complete to find an embedding between two DTD schemas. We also provide efficient heuristic algorithms to find candidate embeddings, along with experimental results to evaluate and compare the algorithms. These yield the first systematic and effective approach to finding information preserving XML mappings.

#index 824659
#* Light-weight domain-based form assistant: querying web databases on the fly
#@ Zhen Zhang;Bin He;Kevin Chen-Chuan Chang
#t 2005
#c 4
#% 198466
#% 227992
#% 248799
#% 248859
#% 333990
#% 334025
#% 340302
#% 443052
#% 459241
#% 480645
#% 481923
#% 572306
#% 572311
#% 654458
#% 654459
#% 765409
#% 765410
#% 769890
#% 783791
#! The Web has been rapidly "deepened" by myriad searchable databases online, where data are hidden behind query forms. Helping users query alternative "deep Web" sources in the same domain (e.g., Books, Airfares) is an important task with broad applications. As a core component of those applications, dynamic query translation (i.e., translating a user's query across dynamically selected sources) has not been extensively explored. While existing works focus on isolated subproblems (e.g., schema matching, query rewriting) to study, we target at building a complete query translator and thus face new challenges: 1) To complete the translator, we need to solve the predicate mapping problem (i.e., map a source predicate to target predicates), which is largely unexplored by existing works; 2) To satisfy our application requirements, we need to design a customizable system architecture to assemble various components addressing respective subproblems (i.e., schema matching, predicate mapping, query rewriting). Tackling these challenges, we develop a light-weight domain-based form assistant, which can generally handle alternative sources in the same domain and is easily customizable to new domains. Our experiment shows the effectiveness of our form assistant in translating queries for real Web sources.

#index 824660
#* Designing information-preserving mapping schemes for XML
#@ Denilson Barbosa;Juliana Freire;Alberto O. Mendelzon
#t 2005
#c 4
#% 11284
#% 14720
#% 262724
#% 273922
#% 279164
#% 333979
#% 333981
#% 384978
#% 397366
#% 411759
#% 413648
#% 465059
#% 479956
#% 480317
#% 480429
#% 480969
#% 730031
#% 745467
#% 783696
#% 810027
#% 824658
#% 836134
#% 942355
#% 994015
#% 1015270
#% 1016152
#% 1393700
#! An XML-to-relational mapping scheme consists of a procedure for shredding documents into relational databases, a procedure for publishing databases back as documents, and a set of constraints the databases must satisfy. In previous work, we defined two notions of information preservation for mapping schemes: losslessness, which guarantees that any document can be reconstructed from its corresponding database; and validation, which requires every legal database to correspond to a valid document. We also described one information-preserving mapping scheme, called Edge++, and showed that, under reasonable assumptions, losslessness and validation are both undecidable. This leads to the question we study in this paper: how to design mapping schemes that are information-preserving. We propose to do it by starting with a scheme known to be information-preserving and applying to it equivalence-preserving transformations written in weakly recursive ILOG. We study an instance of this framework, the LILO algorithm, and show that it provides significant performance improvements over Edge++ and introduces constraints that are efficiently enforced in practice.

#index 824661
#* Rewriting XPath queries using materialized views
#@ Wanhong Xu;Z. Meral Özsoyoglu
#t 2005
#c 4
#% 198465
#% 273924
#% 333989
#% 378393
#% 397374
#% 397375
#% 465051
#% 465053
#% 465065
#% 564264
#% 572311
#% 576102
#% 576108
#% 599549
#% 632039
#% 654450
#% 659999
#% 733593
#% 765405
#% 801686
#% 1015260
#% 1015267
#% 1016134
#% 1016139
#! As a simple XML query language but with enough expressive power, XPath has become very popular. To expedite evaluation of XPath queries, we consider the problem of rewriting XPath queries using materialized XPath views. This problem is very important and arises not only from query optimization in server side but also from semantic caching in client side. We consider the problem of deciding whether there exists a rewriting of a query using XPath views and the problem of finding minimal rewritings. We first consider those two problems for a very practical XPath fragment containing the descendent, child, wildcard and branch features. We show that the rewriting existence problem is coNP-hard and the problem of finding minimal rewritings is Σp3. We also consider those two rewriting problems for three subclasses of this XPath fragment, each of which contains child feature and two of descendent, wildcard and branch features, and show that both rewriting problems can be polynomially solved. Finally, we give an algorithm for finding minimal rewritings, which is sound for the XPath fragment, but is also complete and runs in polynomial time for its three subclasses.

#index 824662
#* Benefits of path summaries in an XML query optimizer supporting multiple access methods
#@ Attila Barta;Mariano P. Consens;Alberto O. Mendelzon
#t 2005
#c 4
#% 191574
#% 210205
#% 397375
#% 453191
#% 462235
#% 479465
#% 479806
#% 480488
#% 551859
#% 562456
#% 564416
#% 570875
#% 654450
#% 654452
#% 659999
#% 660000
#% 745461
#% 745463
#% 800521
#% 916842
#% 1015272
#% 1015273
#% 1015277
#% 1015338
#! We compare several optimization strategies implemented in an XML query evaluation system. The strategies incorporate the use of path summaries into the query optimizer, and rely on heuristics that exploit data statistics.We present experimental results that demonstrate a wide range of performance improvements for the different strategies supported. In addition, we compare the speedups obtained using path summaries with those reported for index-based methods. The comparison shows that low-cost path summaries combined with optimization strategies achieve essentially the same benefits as more expensive index structures.

#index 824663
#* Efficient processing of XML path queries using the disk-based F&B Index
#@ Wei Wang;Haifeng Jiang;Hongzhi Wang;Xuemin Lin;Hongjun Lu;Jianzhong Li
#t 2005
#c 4
#% 236416
#% 333981
#% 397358
#% 397360
#% 397375
#% 464883
#% 479465
#% 479806
#% 654452
#% 745468
#% 745477
#% 765442
#% 765466
#% 993951
#% 994015
#% 1015273
#% 1015277
#% 1016224
#! With the proliferation of XML data and applications on the Internet, efficient XML query processing techniques are in great demand. Answering queries using XML indexes is a natural approach. A number of XML indexes have been proposed in the literature: among them, F&B Index is one powerful index as it is the smallest index that answers all twig queries. However, an F&B Index suffers from the following two problems: (1) it was originally proposed as a memory-based index while its size is usually large in practice and (2) answering queries using an F&B Index is not fully optimized. These problems limit the benefits and even applications of F&B Indexes in practice.In this paper, we propose a highly optimized disk organization method for an F&B Index; the result is a disk-based F&B Index with good clustering properties. In addition, novel query processing algorithms exploiting the physical organization of the disk-based F&B Indexes are proposed. Experimental results verify that our disk-based F&B Index can scale up for large data size with good query performance compared with state-of-the-art XML query processing algorithms.

#index 824664
#* Customizable parallel execution of scientific stream queries
#@ Milena Ivanova;Tore Risch
#t 2005
#c 4
#% 140385
#% 264263
#% 397353
#% 397398
#% 442850
#% 481093
#% 637799
#% 654497
#% 765470
#% 979303
#% 993949
#% 1015282
#! Scientific applications require processing high-volume on-line streams of numerical data from instruments and simulations. We present an extensible stream database system that allows scalable and flexible continuous queries on such streams. Application dependent streams and query functions are defined through an object-relational model. Distributed execution plans for continuous queries are described as high-level data flow distribution templates. Using a generic template we define two partitioning strategies for scalable parallel execution of expensive stream queries: window split and window distribute. Window split provides operators for parallel execution of query functions by reducing the size of stream data units using application dependent functions as parameters. By contrast, window distribute provides operators for customized distribution of entire data units without reducing their size. We evaluate these strategies for a typical high volume scientific stream application and show that window split is favorable when expensive queries are executed on limited resources, while window distribution is better otherwise.

#index 824665
#* Using association rules for fraud detection in web advertising networks
#@ Ahmed Metwally;Divyakant Agrawal;Amr El Abbadi
#t 2005
#c 4
#% 152934
#% 280473
#% 281144
#% 296646
#% 300120
#% 379445
#% 420063
#% 463903
#% 492912
#% 548479
#% 576119
#% 729959
#% 783708
#% 805840
#% 805890
#% 978241
#% 993960
#% 1015262
#% 1016146
#% 1700144
#! Discovering associations between elements occurring in a stream is applicable in numerous applications, including predictive caching and fraud detection. These applications require a new model of association between pairs of elements in streams. We develop an algorithm, Streaming-Rules, to report association rules with tight guarantees on errors, using limited processing per element, and minimal space. The modular design of Streaming-Rules allows for integration with current stream management systems, since it employs existing techniques for finding frequent elements. The presentation emphasizes the applicability of the algorithm to fraud detection in advertising networks. Such fraud instances have not been successfully detected by current techniques. Our experiments on synthetic data demonstrate scalability and efficiency. On real data, potential fraud was discovered.

#index 824666
#* Parameter free bursty events detection in text streams
#@ Gabriel Pui Cheong Fung;Jeffrey Xu Yu;Philip S. Yu;Hongjun Lu
#t 2005
#c 4
#% 46803
#% 46809
#% 172949
#% 259633
#% 262042
#% 262043
#% 287196
#% 309096
#% 340904
#% 344447
#% 397132
#% 397187
#% 445316
#% 481609
#% 577220
#% 641150
#% 643016
#% 677440
#% 765412
#% 771924
#% 800568
#% 1279298
#! Text classification is a major data mining task. An advanced text classification technique is known as partially supervised text classification, which can build a text classifier using a small set of positive examples only. This leads to our curiosity whether it is possible to find a set of features that can be used to describe the positive examples. Therefore, users do not even need to specify a set of positive examples. As the first step, in this paper, we formalize it as a new problem, called hot bursty events detection, to detect bursty events from a text stream which is a sequence of chronologically ordered documents. Here, a bursty event is a set of bursty features, and is considered as a potential category to build a text classifier. It is important to know that the hot bursty events detection problem, we study in this paper, is different from TDT (topic detection and tracking) which attempts to cluster documents as events using clustering techniques. In other words, our focus is on detecting a set of bursty features for a bursty event. In this paper, we propose a new novel parameter free probabilistic approach, called feature-pivot clustering. Our main technique is to fully utilize the time information to determine a set of bursty features which may occur in differenttime windows. We detect bursty events based on the feature distributions. There is no need to tune or estimate any parameters. We conduct experiments using real life data, a major English newspaper in Hong Kong, and show that the parameter free feature-pivot clustering approach can detect the bursty events with a high success rate.

#index 824667
#* From region encoding to extended dewey: on efficient processing of XML twig pattern matching
#@ Jiaheng Lu;Tok Wang Ling;Chee-Yong Chan;Ting Chen
#t 2005
#c 4
#% 172924
#% 333981
#% 397366
#% 397375
#% 480489
#% 654450
#% 659999
#% 745461
#% 745479
#% 765405
#% 765406
#% 765488
#% 783546
#% 783547
#% 800523
#% 800535
#% 810046
#% 810051
#% 1015277
#% 1016142
#% 1698885
#! Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries. A number of algorithms have been proposed to process a twig query based on region encoding labeling scheme. While region encoding supports efficient determination of structural relationship between two elements, we observe that the information within a single label is very limited. In this paper, we propose a new labeling scheme, called extended Dewey. This is a powerful labeling scheme, since from the label of an element alone, we can derive all the elements names along the path from the root to the element. Based on extended Dewey, we design a novel holistic twig join algorithm, called TJFast. Unlike all previous algorithms based on region encoding, to answer a twig query, TJFast only needs to access the labels of the leaf query nodes. Through this, not only do we reduce disk access, but we also support the efficient evaluation of queries with wildcards in branching nodes, which is very difficult to be answered by algorithms based on region encoding. Finally, we report our experimental results to show that our algorithms are superior to previous approaches in terms of the number of elements scanned, the size of intermediate results and query performance.

#index 824668
#* Tree-pattern queries on a lightweight XML processor
#@ Mirella M. Moro;Zografoula Vagena;Vassilis J. Tsotras
#t 2005
#c 4
#% 172924
#% 397359
#% 397360
#% 397375
#% 465061
#% 479465
#% 479806
#% 480656
#% 654450
#% 654477
#% 659999
#% 731408
#% 745461
#% 745477
#% 783547
#% 783787
#% 800535
#% 810046
#% 993953
#% 1015273
#% 1015277
#! Popular XML languages, like XPath, use "tree-pattern" queries to select nodes based on their structural characteristics. While many processing methods have already been proposed for such queries, none of them has found its way to any of the existing "lightweight" XML engines (i.e. engines without optimization modules). The main reason is the lack of a systematic comparison of query methods under a common storage model. In this work, we aim to fill this gap and answer two important questions: what the relative similarities and important differences among the tree-pattern query methods are, and if there is a prominent method among them in terms of effectiveness and robustness that an XML processor should support. For the first question, we propose a novel classification of the methods according to their matching process. We then describe a common storage model and demonstrate that the access pattern of each class conforms or can be adapted to conform to this model. Finally, we perform an experimental evaluation to compare their relative performance. Based on the evaluation results, we conclude that the family of holistic processing methods, which provides performance guarantees, is the most robust alternative for such an environment.

#index 824669
#* FiST: scalable XML document filtering by sequencing twig patterns
#@ Joonho Kwon;Praveen Rao;Bongki Moon;Sukho Lee
#t 2005
#c 4
#% 397375
#% 480296
#% 480489
#% 654476
#% 654477
#% 659995
#% 731408
#% 745461
#% 765441
#% 993950
#! In recent years, publish-subscribe (pub-sub) systems based on XML document filtering have received much attention. In a typical pub-sub system, subscribed users specify their interest in profiles expressed in the XPath language, and each new content is matched against the user profiles so that the content is delivered to only the interested subscribers. As the number of subscribed users and their profiles can grow very large, the scalability of the system is critical to the success of pub-sub services. In this paper, we propose a novel scalable filtering system called FiST (Filtering by Sequencing Twigs) that transforms twig patterns expressed in XPath and XML documents into sequences using Prüfer's method. As a consequence, instead of matching linear paths of twig patterns individually and merging the matches during post-processing, FiST performs holistic matching of twig patterns with incoming documents. FiST organizes the sequences into a dynamic hash based index for efficient filtering. We demonstrate that our holistic matching approach yields lower filtering cost and good scalability under various situations.

#index 824670
#* Maximal vector computation in large data sets
#@ Parke Godfrey;Ryan Shipley;Jarek Gryz
#t 2005
#c 4
#% 62323
#% 287414
#% 288976
#% 289148
#% 458873
#% 465167
#% 480671
#% 642659
#% 654480
#% 993954
#% 1016207
#! Finding the maximals in a collection of vectors is relevant to many applications. The maximal set is related to the convex hull---and hence, linear optimization---and nearest neighbors. The maximal vector problem has resurfaced with the advent of skyline queries for relational databases and skyline algorithms that are external and relationally well behaved.The initial algorithms proposed for maximals are based on divide-and-conquer. These established good average and worst case asymptotic running times, showing it to be O(n) average-case. where n is the number of vectors. However, they are not amenable to externalizing. We prove, furthermore, that their performance is quite bad with respect to the dimensionality, k, of the problem. We demonstrate that the more recent external skyline algorithms are actually better behaved, although they do not have as good an apparent asymptotic complexity. We introduce a new external algorithm, LESS, that combines the best features of these. experimentally evaluate its effectiveness and improvement over the field, and prove its average-case running time is O(kn).

#index 824671
#* Efficient computation of the skyline cube
#@ Yidong Yuan;Xuemin Lin;Qing Liu;Wei Wang;Jeffrey Xu Yu;Qing Zhang
#t 2005
#c 4
#% 62323
#% 227880
#% 273916
#% 288976
#% 289148
#% 333951
#% 464215
#% 479450
#% 480671
#% 481951
#% 654480
#% 745506
#% 799759
#% 800555
#% 824670
#% 824672
#% 993954
#% 1015294
#% 1016207
#! Skyline has been proposed as an important operator for multi-criteria decision making, data mining and visualization, and user-preference queries. In this paper. we consider the problem of efficiently computing a SKYCUBE, which consists of skylines of all possible non-empty subsets of a given set of dimensions. While existing skyline computation algorithms can be immediately extended to computing each skyline query independently, such "shared-nothing" algorithms are inefficient. We develop several computation sharing strategies based on effectively identifying the computation dependencies among multiple related skyline queries. Based on these sharing strategies, two novel algorithms, Bottom-Up and Top-Down algorithms, are proposed to compute SKYCUBE efficiently. Finally, our extensive performance evaluations confirm the effectiveness of the sharing strategies. It is shown that new algorithms significantly outperform the naïve ones.

#index 824672
#* Catching the best views of skyline: a semantic approach based on decisive subspaces
#@ Jian Pei;Wen Jin;Martin Ester;Yufei Tao
#t 2005
#c 4
#% 100803
#% 288976
#% 333854
#% 384416
#% 465167
#% 479986
#% 480671
#% 654480
#% 800555
#% 824671
#% 993954
#% 993996
#! The skyline operator is important for multi-criteria decision making applications. Although many recent studies developed efficient methods to compute skyline objects in a specific space, the fundamental problem on the semantics of skylines remains open: Why and in which subspaces is (or is not) an object in the skyline? Practically, users may also be interested in the skylines in any subspaces. Then, what is the relationship between the skylines in the subspaces and those in the super-spaces? How can we effectively analyze the subspace skylines? Can we efficiently compute skylines in various subspaces?In this paper, we investigate the semantics of skylines, propose the subspace skyline analysis, and extend the full-space skyline computation to subspace skyline computation. We introduce a novel notion of skyline group which essentially is a group of objects that are coincidentally in the skylines of some subspaces. We identify the decisive subspaces that qualify skyline groups in the subspace skylines. The new notions concisely capture the semantics and the structures of skylines in various subspaces. Multidimensional roll-up and drilldown analysis is introduced. We also develop an efficient algorithm, Skyey, to compute the set of skyline groups and, for each subspace, the set of objects that are in the subspace skyline. A performance study is reported to evaluate our approach.

#index 824673
#* Efficient evaluation of XQuery over streaming data
#@ Xiaogang Li;Gagan Agrawal
#t 2005
#c 4
#% 3888
#% 116050
#% 243759
#% 248788
#% 273922
#% 287005
#% 333926
#% 348858
#% 378388
#% 397366
#% 397405
#% 428155
#% 438596
#% 461922
#% 479806
#% 480296
#% 480657
#% 480822
#% 562135
#% 562456
#% 564426
#% 569754
#% 570875
#% 570879
#% 578560
#% 578855
#% 582207
#% 654477
#% 659987
#% 729943
#% 742565
#% 993950
#% 994015
#% 1015272
#% 1015274
#% 1015338
#% 1015354
#% 1016148
#! With the growing popularity of XML and emergence of streaming data model, processing queries over streaming XML has become an important topic. This paper presents a new framework and a set of techniques for processing XQuery over streaming data. As compared to the existing work on supporting XPath/XQuery over data streams, we make the following three contributions:1. We propose a series of optimizations which transform XQuery queries so that they can be correctly executed with a single pass on the dataset.2. We present a methodology for determining when an XQuery query, possibly after the transformations we introduce, can be correctly executed with only a single pass on the dataset.3. We describe a code generation approach which can handle XQuery queries with user-defined aggregates, including recursive functions. We aggressively use static analysis and generate executable code, i.e., do not require a query plan to be interpreted at runtime.We have evaluated our implementation using several XMark benchmarks and three other XQuery queries driven by real applications. Our experimental results show that as compared to Qizx/Open, Saxon, and Galax, our system: 1) is at least 25% faster on XMark queries with small datasets, 2) is significantly faster on XMark queries with larger datasets, 3) at least one order of magnitude faster on the queries driven by real applications, as unlike other systems, we can transform them to execute with a single pass, and 4) executes queries efficiently on large datasets when other systems often have memory overflows.

#index 824674
#* Semantic query optimization for XQuery over XML streams
#@ Hong Su;Elke A. Rundensteiner;Murali Mani
#t 2005
#c 4
#% 69272
#% 273702
#% 333989
#% 397407
#% 462060
#% 462235
#% 465061
#% 479814
#% 570879
#% 570880
#% 654476
#% 654477
#% 730045
#% 993950
#% 994015
#% 1015274
#% 1015276
#% 1015338
#% 1016254
#% 1016258
#! We study XML stream-specific schema-based optimization. We assume a widely-adopted automata-based execution model for XQuery evaluation. Criteria are established regarding what schema constraints are useful to a particular query. How to apply multiple optimization techniques on an XQuery is then addressed. Finally we present how to correctly and efficiently execute a plan enhanced with our SQO techniques. Our experimentation on both real and synthetic data illustrates that these techniques bring significant performance improvement with little overhead.

#index 824675
#* Statistical learning techniques for costing XML queries
#@ Ning Zhang;Peter J. Haas;Vanja Josifovski;Guy M. Lohman;Chun Zhang
#t 2005
#c 4
#% 340306
#% 397364
#% 397375
#% 411554
#% 480488
#% 480803
#% 482123
#% 650962
#% 745477
#% 745518
#% 753710
#% 765423
#% 803121
#% 1015273
#% 1016149
#% 1016225
#! Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators; to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data; the set of "simple path" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called "transform regression" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.

#index 824676
#* Approximate matching of hierarchical data using pq-grams
#@ Nikolaus Augsten;Michael Böhlen;Johann Gamper
#t 2005
#c 4
#% 66654
#% 103525
#% 121278
#% 183302
#% 210212
#% 289193
#% 333679
#% 333981
#% 349489
#% 397373
#% 397375
#% 480654
#% 654493
#% 659999
#% 749906
#% 765423
#% 766669
#% 806218
#% 1015277
#! When integrating data from autonomous sources, exact matches of data items that represent the same real world object often fail due to a lack of common keys. Yet in many cases structural information is available and can be used to match such data. As a running example we use residential address information. Addresses are hierarchical structures and are present in many databases. Often they are the best, if not only, relationship between autonomous data sources. Typically the matching has to be approximate since the representations in the sources differ.We propose pq-grams to approximately match hierarchical information from autonomous sources. We define the pq-gram distance between ordered labeled trees as an effective and efficient approximation of the well-known tree edit distance. We analyze the properties of the pq-gram distance and compare it with the edit distance and alternative approximations. Experiments with synthetic and real world data confirm the analytic results and the scalability of our approach.

#index 824677
#* The TEXTURE benchmark: measuring performance of text queries on a relational DBMS
#@ Vuk Ercegovac;David J. DeWitt;Raghu Ramakrishnan
#t 2005
#c 4
#% 1921
#% 118760
#% 169814
#% 172913
#% 318455
#% 342592
#% 365700
#% 375076
#% 387427
#% 476484
#% 480286
#% 480948
#% 481439
#% 677269
#! We introduce a benchmark called TEXTURE (TEXT Under RElations) to measure the relative strengths and weaknesses of combining text processing with a relational workload in an RDBMS. While the well-known TREC benchmarks focus on quality, we focus on efficiency. TEXTURE is a micro-benchmark for query workloads, and considers two central text support issues that previous benchmarks did not: (1) queries with relevance ranking, rather than those that just compute all answers, and (2) a richer mix of text and relational processing, reflecting the trend toward seamless integration. In developing this benchmark, we had to address the problem of generating large text collections that reflected the (performance) characteristics of a given "seed" collection; this is essential for a controlled study of specific data characteristics and their effects on performance. In addition to presenting the benchmark, with performance numbers for three commercial DBMSs, we present and validate a synthetic generator for populating text fields.

#index 824678
#* n-gram/2L: a space and time efficient two-level n-gram inverted index structure
#@ Min-Soo Kim;Kyu-Young Whang;Jae-Gil Lee;Min-Jae Lee
#t 2005
#c 4
#% 36683
#% 131061
#% 213786
#% 219044
#% 230435
#% 238413
#% 262144
#% 290703
#% 333679
#% 387427
#% 392275
#% 397151
#% 443469
#% 607789
#% 643047
#% 797999
#% 800613
#! The n-gram inverted index has two major advantages: language-neutral and error-tolerant. Due to these advantages, it has been widely used in information retrieval or in similar sequence matching for DNA and protein databases. Nevertheless, the n-gram inverted index also has drawbacks: the size tends to be very large, and the performance of queries tends to be bad. In this paper, we propose the two-level n-gram inverted index (simply, the n-gram/2L index) that significantly reduces the size and improves the query performance while preserving the advantages of the n-gram inverted index. The proposed index eliminates the redundancy of the position information that exists in the n-gram inverted index. The proposed index is constructed in two steps: 1) extracting subsequences of length m from documents and 2) extracting n-grams from those subsequences. We formally prove that this two-step construction is identical to the relational normalization process that removes the redundancy caused by a non-trivial multivalued dependency. The n-gram/2L index has excellent properties: 1) it significantly reduces the size and improves the performance compared with the n-gram inverted index with these improvements becoming more marked as the database size gets larger; 2) the query processing time increases only very slightly as the query length gets longer. Experimental results using databases of 1 GBytes show that the size of the n-gram/2L index is reduced by up to 1.9 ~ 2.7 times and, at the same time, the query performance is improved by up to 13.1 times compared with those of the n-gram inverted index.

#index 824679
#* Query translation from XPATH to SQL in the presence of recursive DTDs
#@ Wenfei Fan;Jeffrey Xu Yu;Hongjun Lu;Jianhua Lu;Rajeev Rastogi
#t 2005
#c 4
#% 11797
#% 101623
#% 288766
#% 289315
#% 300166
#% 333935
#% 348183
#% 379484
#% 462039
#% 462235
#% 479956
#% 480489
#% 480657
#% 654493
#% 745478
#% 1016141
#! The interaction between recursion in XPATH and recursion in DTDS makes it challenging to answer XPATH queries on XML data that is stored in an RDBMS via schema-based shredding. We present a new approach to translating XPATH queries into SQL queries with a simple least fixpoint (LFP) operator, which is already supported by most commercial RDBMS. The approach is based on our algorithm for rewriting XPATH queries into regular XPATH expressions, which are capable of capturing both DTD recursion and XPATH queries in a uniform framework. Furthermore, we provide an algorithm for translating regular XPATH queries to SQL queries with LFP, and optimization techniques for minimizing the use of the LFP operator. The novelty of our approach consists in its capability to answer a large class of XPATH queries by means of only low-end RDBMS features already available in most RDBMS. Our experimental results verify the effectiveness of our techniques.

#index 824680
#* Pattern tree algebras: sets or sequences?
#@ Stelios Paparizos;H. V. Jagadish
#t 2005
#c 4
#% 136740
#% 210169
#% 333981
#% 397366
#% 397375
#% 411554
#% 465006
#% 465018
#% 479956
#% 562456
#% 570875
#% 654493
#% 765407
#% 994015
#% 1015274
#! XML and XQuery semantics are very sensitive to the order of the produced output. Although pattern-tree based algebraic approaches are becoming more and more popular for evaluating XML, there is no universally accepted technique which can guarantee both a correct output order and a choice of efficient alternative plans.We address the problem using hybrid collections of trees that can be either sets or sequences or something in between. Each such collection is coupled with an Ordering Specification that describes how the trees are sorted (full, partial or no order). This provides us with a formal basis for developing a query plan having parts that maintain no order and parts with partial or full order.It turns out that duplicate elimination introduces some of the same issues as order maintenance: it is expensive and a single collection type does not always provide all the flexibility required to optimize this properly. To solve this problem we associate with each hybrid collection a Duplicate Specification that describes the presence or absence of duplicate elements in it. We show how to extend an existing bulk tree algebra, TLC [12], to use Ordering and Duplicate specifications and produce correctly ordered results. We also suggest some optimizations enabled by the flexibility of our approach, and experimentally demonstrate the performance increase due to them.

#index 824681
#* Structure and content scoring for XML
#@ Sihem Amer-Yahia;Nick Koudas;Amélie Marian;Divesh Srivastava;David Toman
#t 2005
#c 4
#% 249160
#% 333845
#% 406493
#% 458828
#% 458829
#% 584940
#% 642993
#% 654442
#% 765408
#% 765423
#% 765466
#% 772029
#% 800508
#! XML repositories are usually queried both on structure and content. Due to structural heterogeneity of XML, queries are often interpreted approximately and their answers are returned ranked by scores. Computing answer scores in XML is an active area of research that oscillates between pure content scoring such as the well-known tf*idf and taking structure into account. However, none of the existing proposals fully accounts for structure and combines it with content to score query answers. We propose novel XML scoring methods that are inspired by tf*idf and that account for both structure and content while considering query relaxations. Twig scoring, accounts for the most structure and content and is thus used as our reference method. Path scoring is an approximation that loosens correlations between query nodes hence reducing the amount of time required to manipulate scores during top-k query processing. We propose efficient data structures in order to speed up ranked query processing. We run extensive experiments that validate our scoring methods and that show that path scoring provides very high precision while improving score computation time.

#index 824682
#* Consistently estimating the selectivity of conjuncts of predicates
#@ V. Markl;N. Megiddo;M. Kutsch;T. M. Tran;P. Haas;U. Srivastava
#t 2005
#c 4
#% 102784
#% 137882
#% 169841
#% 210190
#% 273901
#% 314739
#% 333946
#% 333947
#% 333986
#% 397371
#% 411554
#% 427219
#% 464062
#% 479967
#% 480283
#% 480803
#% 482092
#% 632048
#% 760291
#% 765427
#% 765455
#% 815818
#% 1015334
#% 1016225
#! Cost-based query optimizers need to estimate the selectivity of conjunctive predicates when comparing alternative query execution plans. To this end, advanced optimizers use multivariate statistics (MVS) to improve information about the joint distribution of attribute values in a table. The joint distribution for all columns is almost always too large to store completely, and the resulting use of partial distribution information raises the possibility that multiple, non-equivalent selectivity estimates may be available for a given predicate. Current optimizers use ad hoc methods to ensure that selectivities are estimated in a consistent manner. These methods ignore valuable information and tend to bias the optimizer toward query plans for which the least information is available, often yielding poor results. In this paper we present a novel method for consistent selectivity estimation based on the principle of maximum entropy (ME). Our method efficiently exploits all available information and avoids the bias problem. In the absence of detailed knowledge, the ME approach reduces to standard uniformity and independence assumptions. Our implementation using a prototype version of DB2 UDB shows that ME improves the optimizer's cardinality estimates by orders of magnitude, resulting in better plan quality and significantly reduced query execution times.

#index 824683
#* Efficiently processing queries on interval-and-value tuples in relational databases
#@ Jost Enderle;Nicole Schneider;Thomas Seidl
#t 2005
#c 4
#% 64431
#% 86950
#% 86951
#% 102759
#% 137893
#% 213001
#% 225004
#% 286256
#% 287070
#% 332133
#% 411694
#% 427199
#% 452782
#% 462956
#% 464856
#% 480093
#% 480299
#% 480422
#% 527181
#% 527323
#% 527324
#% 765458
#! With the increasing occurrence of temporal and spatial data in present-day database applications, the interval data type is adopted by more and more database systems. For an efficient support of queries that contain selections on interval attributes as well as simple-valued attributes (e.g. numbers, strings) at the same time, special index structures are required supporting both types of predicates in combination. Based on the Relational Interval Tree, we present various indexing schemes that support such combined queries and can be integrated in relational database systems with minimum effort. Experiments on different query types show superior performance for the new techniques in comparison to competing access methods.

#index 824684
#* Selectivity estimation for fuzzy string predicates in large data sets
#@ Liang Jin;Chen Li
#t 2005
#c 4
#% 43163
#% 201889
#% 210189
#% 210190
#% 235941
#% 299984
#% 310546
#% 465160
#% 479648
#% 479958
#% 480465
#% 480488
#% 480654
#% 587758
#% 654467
#% 689389
#% 730018
#% 745472
#% 745489
#% 824717
#% 993968
#% 993980
#! Many database applications have the emerging need to support fuzzy queries that ask for strings that are similar to a given string, such as "name similar to smith" and "telephone number similar to 412-0964." Query optimization needs the selectivity of such a fuzzy predicate, i.e., the fraction of records in the database that satisfy the condition. In this paper, we study the problem of estimating selectivities of fuzzy string predicates. We develop a novel technique, called SEPIA, to solve the problem. It groups strings into clusters, builds a histogram structure for each cluster, and constructs a global histogram for the database. It is based on the following intuition: given a query string q, a preselected string p in a cluster, and a string s in the cluster, based on the proximity between q and p, and the proximity between p and s, we can obtain a probability distribution from a global histogram about the similarity between q and s. We give a full specification of the technique using the edit distance function. We study challenges in adopting this technique, including how to construct the histogram structures, how to use them to do selectivity estimation, and how to alleviate the effect of non-uniform errors in the estimation. We discuss how to extend the techniques to other similarity functions. Our extensive experiments on real data sets show that this technique can accurately estimate selectivities of fuzzy string predicates.

#index 824685
#* Space efficiency in synopsis construction algorithms
#@ Sudipto Guha
#t 2005
#c 4
#% 43163
#% 116390
#% 201921
#% 210190
#% 227883
#% 248822
#% 274152
#% 282942
#% 338425
#% 341100
#% 399763
#% 411554
#% 479648
#% 480306
#% 480628
#% 481266
#% 482123
#% 492932
#% 654460
#% 742562
#% 801684
#% 823333
#% 824686
#% 1016154
#! Histograms and Wavelet synopses have been found to be useful in query optimization, approximate query answering and mining. Over the last few years several good synopsis algorithms have been proposed. These have mostly focused on the running time of the synopsis constructions, optimum or approximate, vis-a-vis their quality. However the space complexity of synopsis construction algorithms has not been investigated as thoroughly. Many of the optimum synopsis construction algorithms (as well as few of the approximate ones) are expensive in space. In this paper, we propose a general technique that reduces space complexity. We show that the notion of "working space" proposed in these contexts is redundant. We believe that our algorithm also generalizes to a broader range of dynamic programs beyond synopsis construction. Our modifications can be easily adapted to existing algorithms. We demonstrate the performance benefits through experiments on real-life and synthetic data.

#index 824686
#* One-pass wavelet synopses for maximum-error metrics
#@ Panagiotis Karras;Nikos Mamoulis
#t 2005
#c 4
#% 116084
#% 248822
#% 257637
#% 273902
#% 397389
#% 435222
#% 572308
#% 575972
#% 578390
#% 801684
#% 1015301
#% 1016153
#% 1016154
#! We study the problem of computing wavelet-based synopses for massive data sets in static and streaming environments. A compact representation of a data set is obtained after a thresholding process is applied on the coefficients of its wavelet decomposition. Existing polynomial-time thresholding schemes that minimize maximum error metrics are disadvantaged by impracticable time and space complexities and are not applicable in a data stream context. This is a cardinal issue, as the problem at hand in its most practically interesting form involves the time-efficient approximation of huge amounts of data, potentially in a streaming environment. In this paper we fill this gap by developing efficient and practicable wavelet thresholding algorithms for maximum-error metrics, for both a static and a streaming case. Our algorithms achieve near-optimal accuracy and superior runtime performance, as our experiments show, under frugal space requirements in both contexts.

#index 824687
#* MDL summarization with holes
#@ Shaofeng Bu;Laks V. S. Lakshmanan;Raymond T. Ng
#t 2005
#c 4
#% 210182
#% 227880
#% 270686
#% 273902
#% 388196
#% 397388
#% 420053
#% 479957
#% 480820
#% 576103
#% 993995
#% 993996
#! Summarization of query results is an important problem for many OLAP applications. The Minimum Description Length principle has been applied in various studies to provide summaries. In this paper, we consider a new approach of applying the MDL principle. We study the problem of finding summaries of the form S Θ H for k-d cubes with tree hierarchies. The S part generalizes the query results, while the H part describes all the exceptions to the generalizations. The optimization problem is to minimize the combined cardinalities of S and H. We first characterize the problem by showing that solving the 1-d problem can be done in time linear to the size of hierarchy, but solving the 2-d problem is NP-hard. We then develop three different heuristics, based on a greedy approach, a dynamic programming approach and a quadratic programming approach. We conduct a comprehensive experimental evaluation. Both the dynamic programming algorithm and the greedy algorithm can be used for different circumstances. Both produce summaries that are significantly shorter than those generated by state-of-the-art alternatives.

#index 824688
#* View matching for outer-join views
#@ Per-Åke Larson;Jingren Zhou
#t 2005
#c 4
#% 172933
#% 201927
#% 210167
#% 220425
#% 264963
#% 300138
#% 333965
#% 464056
#% 465165
#% 479792
#% 480149
#% 481608
#% 482081
#% 564419
#% 765457
#! Prior work on computing queries from materialized views has focused on views defined by expressions consisting of selection, projection, and inner joins, with an optional aggregation on top (SPJG views). This paper provides the first view matching algorithm for views that may also contain outer joins (SPOJG views). The algorithm relies on a normal form for SPOJ expressions and does not use bottom-up syntactic matching of expressions. It handles any combination of inner and outer joins, deals correctly with SQL bag semantics and exploits not-null constraints, uniqueness constraints and foreign key constraints.

#index 824689
#* Caching with "good enough" currency, consistency, and completeness
#@ Hongfei Guo;Per-Åke Larson;Raghu Ramakrishnan
#t 2005
#c 4
#% 102804
#% 230401
#% 333965
#% 333969
#% 333995
#% 397400
#% 397401
#% 397402
#% 480332
#% 480495
#% 480814
#% 480818
#% 481584
#% 481916
#% 565955
#% 571216
#% 745536
#% 765469
#% 1015314
#! SQL extensions that allow queries to explicitly specify data quality requirements in terms of currency and consistency were proposed in an earlier paper. This paper develops a data quality-aware, finer grained cache model and studies cache design in terms of four fundamental properties: presence, consistency, completeness and currency. The model provides an abstract view of the cache to the query processing layer, and opens the door for adaptive cache management. We describe an implementation approach that builds on the MTCache framework for partially materialized views. The optimizer checks most consistency constraints and generates a dynamic plan that includes currency checks and inexpensive checks for dynamic consistency constraints that cannot be validated during optimization. Our solution not only supports transparent caching but also provides fine grained data currency and consistency guarantees.

#index 824690
#* Query caching and view selection for XML databases
#@ Bhushan Mandhani;Dan Suciu
#t 2005
#c 4
#% 70370
#% 378393
#% 397402
#% 480474
#% 481916
#% 650962
#% 1015267
#% 1016134
#% 1016224
#! In this paper, we propose a method for maintaining a semantic cache of materialized XPath views. The cached views include queries that have been previously asked, and additional selected views. The cache can be stored inside or outside the database. We describe a notion of XPath query/view answerability, which allows us to reduce tree operations to string operations for matching a query/view pair. We show how to store and maintain the cached views in relational tables, so that cache lookup is very efficient. We also describe a technique for view selection, given a warm-up workload. We experimentally demonstrate the efficiency of our caching techniques, and performance gains obtained by employing such a cache.

#index 824691
#* Optimizing nested queries with parameter sort orders
#@ Ravindra Guravannavar;H. S. Ramanujam;S. Sudarshan
#t 2005
#c 4
#% 32878
#% 77931
#% 172930
#% 287005
#% 334006
#% 411554
#% 461897
#% 480091
#% 480152
#% 481915
#% 565457
#% 783793
#% 1015318
#! Nested iteration is an important technique for query evaluation. It is the default way of executing nested subqueries in SQL. Although decorrelation often results in cheaper non-nested plans, decorrelation is not always applicable for nested subqueries. Nested iteration, if implemented properly, can also win over decorrelation for several classes of queries. Decorrelation is also hard to apply to nested iteration in user-defined SQL procedures and functions. Recent research has proposed evaluation techniques to speed up execution of nested iteration, but does not address the optimization issue. In this paper, we address the issue of exploiting the ordering of nested iteration/procedure calls to speed up nested iteration. We propose state retention of operators as an important technique to exploit the sort order of parameters/correlation variables. We then show how to efficiently extend an optimizer to take parameter sort orders into consideration. We implemented our evaluation techniques on PostgreSQL, and present performance results that demonstrate significant benefits.

#index 824692
#* Stack-based algorithms for pattern matching on DAGs
#@ Li Chen;Amarnath Gupta;M. Erdem Kurul
#t 2005
#c 4
#% 51391
#% 58365
#% 70370
#% 77943
#% 148890
#% 287288
#% 333981
#% 343807
#% 378391
#% 379484
#% 397360
#% 397375
#% 408396
#% 465018
#% 479806
#% 480489
#% 571040
#% 577358
#% 659999
#% 772025
#! Existing work for query processing over graph data models often relies on pre-computing the transitive closure or path indexes. In this paper, we propose a family of stack-based algorithms to handle path, twig, and dag pattern queries for directed acyclic graphs (DAGs) in particular. Our algorithms do not precompute the transitive closure nor path indexes for a given graph, however they achieve an optimal runtime complexity quadratic in the average size of the query variable bindings. We prove the soundness and completeness of our algorithms and present the experimental results.

#index 824693
#* Bidirectional expansion for keyword search on graph databases
#@ Varun Kacholia;Shashank Pandit;Soumen Chakrabarti;S. Sudarshan;Rushi Desai;Hrishikesh Karambelkar
#t 2005
#c 4
#% 268079
#% 290703
#% 330678
#% 333854
#% 479806
#% 654442
#% 659990
#% 660011
#% 993987
#% 1015325
#% 1016135
#% 1016176
#! Relational, XML and HTML data can be represented as graphs with entities as nodes and relationships as edges. Text is associated with nodes and possibly edges. Keyword search on such graphs has received much attention lately. A central problem in this scenario is to efficiently extract from the data graph a small number of the "best" answer trees. A Backward Expanding search, starting at nodes matching keywords and working up toward confluent roots, is commonly used for predominantly text-driven queries. But it can perform poorly if some keywords match many nodes, or some node has very large degree.In this paper we propose a new search algorithm, Bidirectional Search, which improves on Backward Expanding search by allowing forward search from potential roots towards leaves. To exploit this flexibility, we devise a novel search frontier prioritization technique based on spreading activation. We present a performance study on real data, establishing that Bidirectional Search significantly outperforms Backward Expanding search.

#index 824694
#* Link spam alliances
#@ Zoltán Gyöngyi;Hector Garcia-Molina
#t 2005
#c 4
#% 590524
#% 772018
#% 799632
#! Link spam is used to increase the ranking of certain target web pages by misleading the connectivity-based ranking algorithms in search engines. In this paper we study how web pages can be interconnected in a spam farm in order to optimize rankings. We also study alliances, that is, interconnections of spam farms. Our results identify the optimal structures and quantify the potential gains. In particular, we show that alliances can be synergistic and improve the rankings of all participants. We believe that the insights we gain will be useful in identifying and combating link spam.

#index 824695
#* The SphereSearch engine for unified ranked retrieval of heterogeneous XML and web documents
#@ Jens Graupmann;Ralf Schenkel;Gerhard Weikum
#t 2005
#c 4
#% 144029
#% 169781
#% 340914
#% 345709
#% 345712
#% 397415
#% 458829
#% 464825
#% 479471
#% 479807
#% 480330
#% 481602
#% 482655
#% 541480
#% 631988
#% 643072
#% 643566
#% 654442
#% 654469
#% 660011
#% 665645
#% 754068
#% 765408
#% 766440
#% 769884
#% 799737
#% 800534
#% 801668
#% 844221
#% 994015
#% 1015258
#% 1015325
#% 1016183
#% 1721858
#% 1721869
#! This paper presents the novel SphereSearch Engine that provides unified ranked retrieval on heterogeneous XML and Web data. Its search capabilities include vague structure conditions, text content conditions, and relevance ranking based on IR statistics and statistically quantified ontological relationships. Web pages in HTML or PDF are automatically converted into XML format, with the option of generating semantic tags by means of linguistic annotation tools. For Web data the XML-oriented query engine is leveraged to provide very rich search options that cannot be expressed in traditional Web search engines: concept-aware and link-aware querying that takes into account the implicit structure and context of Web pages. The benefits of the SphereSearch engine are demonstrated by experiments with a large and richly tagged but non-schematic open encyclopedia extended with external documents.

#index 824696
#* Hubble: an advanced dynamic folder technology for XML
#@ Ning Li;Joshua Hui;Hui-I Hsiao;Kevin S. Beyer
#t 2005
#c 4
#% 193325
#% 201921
#% 345714
#% 465747
#% 479956
#% 745518
#% 810036
#% 1016134
#! A significant amount of information is stored in computer systems today, but people are struggling to manage their documents such that the information is easily found. XML is a de-facto standard for content publishing and data exchange. The proliferation of XML documents has created new challenges and opportunities for managing document collections. Existing technologies for automatically organizing document collections are either imprecise or based on only simple criteria. Since XML documents are self describing, it is now possible to automatically categorize XML documents precisely, according to their content. With the availability of the standard XML query languages, e.g. XQuery, much more powerful folder technologies are now feasible. To address this new challenge and exploit this new opportunity, this paper proposes a new and powerful dynamic folder mechanism, called Hubble. Hubble fully exploits the rich data model and semantic information embedded in the XML documents to build folder hierarchies dynamically and to categorize XML collections precisely. Besides supporting basic folder operations, Hubble also provides advanced features such as multi-path navigation and folder traversal across multiple document collections. Our performance study shows that Hubble is both efficient and scalable. Thus, it is an ideal technology for automating the process of organizing and categorizing XML documents.

#index 824697
#* C-store: a column-oriented DBMS
#@ Mike Stonebraker;Daniel J. Abadi;Adam Batkin;Xuedong Chen;Mitch Cherniack;Miguel Ferreira;Edmond Lau;Amerson Lin;Sam Madden;Elizabeth O'Neil;Pat O'Neil;Alex Rasin;Nga Tran;Stan Zdonik
#t 2005
#c 4
#% 43171
#% 114582
#% 115661
#% 116041
#% 136740
#% 146203
#% 193743
#% 201869
#% 201951
#% 208047
#% 227861
#% 227880
#% 273917
#% 322412
#% 403195
#% 420053
#% 442700
#% 480623
#% 481933
#% 993967
#! This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.We present preliminary performance data on a subset of TPC-H and show that the system we are building, C-Store, is substantially faster than popular commercial products. Hence, the architecture looks very encouraging.

#index 824698
#* Fine-grained replication and scheduling with freshness and correctness guarantees
#@ Fuat Akal;Can Türker;Hans-Jörg Schek;Yuri Breitbart;Torsten Grabs;Lourens Veen
#t 2005
#c 4
#% 3645
#% 9241
#% 210179
#% 237197
#% 273894
#% 335454
#% 336201
#% 461887
#% 480310
#% 636005
#% 717164
#% 745516
#% 765469
#% 765503
#% 793894
#% 993994
#% 1180884
#! Lazy replication protocols provide good scalability properties by decoupling transaction execution from the propagation of new values to replica sites while guaranteeing a correct and more efficient transaction processing and replica maintenance. However, they impose several restrictions that are often not valid in practical database settings, e.g., they require that each transaction executes at its initiation site and/or are restricted to full replication schemes. Also, the protocols cannot guarantee that the transactions will always see the freshest available replicas. This paper presents a new lazy replication protocol called PDBREP that is free of these restrictions while ensuring one-copy-serializable executions. The protocol exploits the distinction between read-only and update transactions and works with arbitrary physical data organizations such as partitioning and striping as well as different replica granularities. It does not require that each read-only transaction executes entirely at its initiation site. Hence, each read-only site need not contain a fully replicated database. PDBREP moreover generalizes the notion of freshness to finer data granules than entire databases.

#index 824699
#* Cache-conscious frequent pattern mining on a modern processor
#@ Amol Ghoting;Gregory Buehrer;Srinivasan Parthasarathy;Daehyun Kim;Anthony Nguyen;Yen-Kuang Chen;Pradeep Dubey
#t 2005
#c 4
#% 94539
#% 128287
#% 152934
#% 155362
#% 201894
#% 202503
#% 227919
#% 248791
#% 251477
#% 280409
#% 300120
#% 300194
#% 333949
#% 342643
#% 379325
#% 420063
#% 463903
#% 465003
#% 466664
#% 479484
#% 479819
#% 480119
#% 481290
#% 481754
#% 566122
#% 593968
#% 631926
#% 737328
#% 745501
#! In this paper, we examine the performance of frequent pattern mining algorithms on a modern processor. A detailed performance study reveals that even the best frequent pattern mining implementations, with highly efficient memory managers, still grossly under-utilize a modern processor. The primary performance bottlenecks are poor data locality and low instruction level parallelism (ILP). We propose a cache-conscious prefix tree to address this problem. The resulting tree improves spatial locality and also enhances the benefits from hardware cache line prefetching. Furthermore, the design of this data structure allows the use of a novel tiling strategy to improve temporal locality. The result is an overall speedup of up to 3.2 when compared with state-of-the-art implementations. We then show how these algorithms can be improved further by realizing a non-naive thread-based decomposition that targets simultaneously multi-threaded processors. A key aspect of this decomposition is to ensure cache re-use between threads that are co-scheduled at a fine granularity. This optimization affords an additional speedup of 50%, resulting in an overall speedup of up to 4.8. To the best of our knowledge, this effort is the first to target cache-conscious data mining.

#index 824700
#* Parallel execution of test runs for database application systems
#@ Florian Haftmann;Donald Kossmann;Eric Lo
#t 2005
#c 4
#% 115661
#% 172913
#% 190688
#% 227405
#% 248829
#% 403195
#% 435112
#% 442518
#% 479656
#% 480949
#% 819354
#! In a recent paper [8], it was shown how tests for database application systems can be executed efficiently. The challenge was to control the state of the database during testing and to order the test runs in such a way that expensive reset operations that bring the database into the right state need to be executed as seldom as possible. This work extends that work so that test runs can be executed in parallel. The goal is to achieve linear speed-up and/or exploit the available resources as well as possible. This problem is challenging because parallel testing can involve interference between the execution of concurrent test runs.

#index 824701
#* Query execution assurance for outsourced databases
#@ Radu Sion
#t 2005
#c 4
#% 191721
#% 227956
#% 232761
#% 241787
#% 275836
#% 300184
#% 346714
#% 397367
#% 488310
#% 488637
#% 489157
#% 566385
#% 566390
#% 566391
#% 589250
#% 592726
#% 593711
#% 659992
#% 664665
#% 755175
#% 765447
#% 766200
#% 784400
#% 993943
#% 1394513
#! In this paper we propose and analyze a method for proofs of actual query execution in an outsourced database framework, in which a client outsources its data management needs to a specialized provider. The solution is not limited to simple selection predicate queries but handles arbitrary query types. While this work focuses mainly on read-only, compute-intensive (e.g. data-mining) queries, it also provides preliminary mechanisms for handling data updates (at additional costs). We introduce query execution proofs; for each executed batch of queries the database service provider is required to provide a strong cryptographic proof that provides assurance that the queries were actually executed correctly over their entire target data set. We implement a proof of concept and present experimental results in a real-world data mining application, proving the deployment feasibility of our solution. We analyze the solution and show that its overheads are reasonable and are far outweighed by the added security benefits. For example an assurance level of over 95% can be achieved with less than 25% execution time overhead.

#index 824702
#* Automatic composition of transition-based semantic web services with messaging
#@ Daniela Berardi;Diego Calvanese;Giuseppe De Giacomo;Richard Hull;Massimo Mecella
#t 2005
#c 4
#% 173860
#% 342119
#% 390685
#% 445446
#% 577343
#% 754120
#% 801675
#% 1374383
#% 1561977
#! In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform; (ii) their impact on the "real world" (modeled as a relational database); (iii) their transition-based behavior; and (iv) the messages they can send and receive (from/to other web services and "human" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic.

#index 824703
#* An efficient and versatile query engine for TopX search
#@ Martin Theobald;Ralf Schenkel;Gerhard Weikum
#t 2005
#c 4
#% 169781
#% 212665
#% 228097
#% 333981
#% 340886
#% 340911
#% 340914
#% 345712
#% 397358
#% 397375
#% 397376
#% 397378
#% 458829
#% 479623
#% 480330
#% 480488
#% 504581
#% 631988
#% 642993
#% 643566
#% 654441
#% 654442
#% 659993
#% 659999
#% 763882
#% 765408
#% 765418
#% 765466
#% 800508
#% 993968
#% 1015258
#% 1015265
#% 1015277
#% 1016135
#% 1016183
#! This paper presents a novel engine, coined TopX, for efficient ranked retrieval of XML documents over semistructured but nonschematic data collections. The algorithm follows the paradigm of threshold algorithms for top-k query processing with a focus on inexpensive sequential accesses to index lists and only a few judiciously scheduled random accesses. The difficulties in applying the existing top-k algorithms to XML data lie in 1) the need to consider scores for XML elements while aggregating them at the document level, 2) the combination of vague content conditions with XML path conditions, 3) the need to relax query conditions if too few results satisfy all conditions, and 4) the selectivity estimation for both content and structure conditions and their impact on evaluation strategies. TopX addresses these issues by precomputing score and path information in an appropriately designed index structure, by largely avoiding or postponing the evaluation of expensive path conditions so as to preserve the sequential access pattern on index lists, and by selectively scheduling random accesses when they are cost-beneficial. In addition, TopX can compute approximate top-k results using probabilistic score estimators, thus speeding up queries with a small and controllable loss in retrieval precision.

#index 824704
#* KLEE: a framework for distributed top-k query algorithms
#@ Sebastian Michel;Peter Triantafillou;Gerhard Weikum
#t 2005
#c 4
#% 212665
#% 256883
#% 278831
#% 322884
#% 337285
#% 340886
#% 342828
#% 397376
#% 397378
#% 399762
#% 411758
#% 480819
#% 577361
#% 643566
#% 654443
#% 731409
#% 763882
#% 765466
#% 766671
#% 768521
#% 1015264
#% 1015265
#% 1016183
#% 1016196
#% 1016203
#! This paper addresses the efficient processing of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We present KLEE, a novel algorithmic framework for distributed top-k queries, designed for high performance and flexibility. KLEE makes a strong case for approximate top-k algorithms over widely distributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further, KLEE affords the query-initiating peer the flexibility to trade-off result quality and expected performance and to trade-off the number of communication phases engaged during query execution versus network bandwidth performance. We have implemented KLEE and related algorithms and conducted a comprehensive performance evaluation. Our evaluation employed real-world and synthetic large, web-data collections, and query benchmarks. Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth, query response times, and much lighter peer loads, all with small errors in result precision and other result-quality measures.

#index 824705
#* Scaling and time warping in time series querying
#@ Ada Wai-chee Fu;Eamonn Keogh;Leo Yung Hang Lau;Chotirat Ann Ratanamahatana
#t 2005
#c 4
#% 86950
#% 172949
#% 398429
#% 427199
#% 462231
#% 464994
#% 481956
#% 611616
#% 625052
#% 631923
#% 654456
#% 717294
#% 765481
#% 993965
#% 1016194
#% 1391300
#! The last few years have seen an increasing understanding that Dynamic Time Warping (DTW), a technique that allows local flexibility in aligning time series, is superior to the ubiquitous Euclidean Distance for time series classification, clustering, and indexing. More recently, it has been shown that for some problems, Uniform Scaling (US), a technique that allows global scaling of time series, may just be as important for some problems. In this work, we note that for many real world problems, it is necessary to combine both DTW and US to achieve meaningful results. This is particularly true in domains where we must account for the natural variability of human action, including biometrics, query by humming, motion-capture/animation, and handwriting recognition. We introduce the first technique which can handle both DTW and US simultaneously, and demonstrate its utility and effectiveness on a wide range of problems in industry, medicine, and entertainment.

#index 824706
#* BATON: a balanced tree structure for peer-to-peer networks
#@ H. V. Jagadish;Beng Chin Ooi;Quang Hieu Vu
#t 2005
#c 4
#% 252608
#% 300164
#% 340175
#% 340176
#% 453509
#% 479769
#% 481296
#% 505869
#% 674136
#% 745498
#% 772022
#% 963874
#% 1016166
#% 1711098
#! We propose a balanced tree structure overlay on a peer-to-peer network capable of supporting both exact queries and range queries efficiently. In spite of the tree structure causing distinctions to be made between nodes at different levels in the tree, we show that the load at each node is approximately equal. In spite of the tree structure providing precisely one path between any pair of nodes, we show that sideways routing tables maintained at each node provide sufficient fault tolerance to permit efficient repair. Specifically, in a network with N nodes, we guarantee that both exact queries and range queries can be answered in O(log N) steps and also that update operations (to both data and network) have an amortized cost of O(log N). An experimental assessment validates the practicality of our proposal.

#index 824707
#* Client assignment in content dissemination networks for dynamic data
#@ Shetal Shah;Krithi Ramamritham;Chinya Ravishankar
#t 2005
#c 4
#% 1451
#% 25998
#% 43419
#% 227885
#% 271622
#% 300179
#% 303712
#% 330581
#% 333969
#% 397355
#% 399551
#% 464228
#% 591420
#% 609890
#% 635804
#% 661477
#% 661478
#% 762650
#% 789418
#% 805844
#% 963655
#% 963887
#% 978365
#% 978376
#% 978378
#% 1016145
#! Consider a content distribution network consisting of a set of sources, repositories and clients where the sources and the repositories cooperate with each other for efficient dissemination of dynamic data. In this system, necessary changes are pushed from sources to repositories and from repositories to clients so that they are automatically informed about the changes of interest. Clients and repositories associate coherence requirements with a data item d, denoting the maximum permissible deviation of the value of d known to them from the value at the source. Given a list of served by each repository and a set of requests, we address the following problem: How do we assign clients to the repositories, so that the fidelity, that is, the degree to which client coherence requirements are met, is maximized?In this paper, we first prove that the client assignment problem is NP-Hard. Given the closeness of the client-repository assignment problem and the matching problem in combinatorial optimization, we have tailored and studied two available solutions to the matching problem from the literature: (i) max-flow min-cost and (ii) stable-marriages. Our empirical results using real-world dynamic data show that the presence of coherence requirements adds a new dimension to the client-repository assignment problem. An interesting result is that in update intensive situations a better fidelity can be delivered to the clients by attempting to deliver data to some of the clients at a coherence lower than what they desire. A consequence of this observation is the necessity for quick adaptation of the delivered (vs. desired) data coherence with respect to the changes in the dynamics of the system. We develop techniques for such adaptation and show their impressive performance.

#index 824708
#* Indexing data-oriented overlay networks
#@ Karl Aberer;Anwitaman Datta;Manfred Hauswirth;Roman Schmidt
#t 2005
#c 4
#% 224221
#% 232640
#% 340175
#% 340176
#% 343843
#% 505869
#% 590427
#% 730102
#% 762652
#% 768520
#% 768539
#% 816637
#% 1016166
#% 1710892
#% 1711123
#% 1729837
#% 1740898
#! The application of structured overlay networks to implement index structures for data-oriented applications such as peer-to-peer databases or peer-to-peer information retrieval, requires highly efficient approaches for overlay construction, as changing application requirements frequently lead to re-indexing of the data and hence (re)construction of overlay networks. This problem has so far not been addressed in the literature and thus we describe an approach for the efficient construction of data-oriented, structured overlay networks from scratch in a self-organized way. Standard maintenance algorithms for overlay networks cannot accomplish this efficiently, as they are inherently sequential. Our proposed algorithm is completely decentralized, parallel, and can construct a new overlay network with short latency. At the same time it ensures good load-balancing for skewed data key distributions which result from preserving key order relationships as necessitated by data-oriented applications. We provide both a theoretical analysis of the basic algorithms and a complete system implementation that has been tested on PlanetLab. We use this implementation to support peer-to-peer information retrieval and database applications.

#index 824709
#* Streaming pattern discovery in multiple time-series
#@ Spiros Papadimitriou;Jimeng Sun;Christos Faloutsos
#t 2005
#c 4
#% 80995
#% 120749
#% 210173
#% 211942
#% 310500
#% 342600
#% 345857
#% 378408
#% 379444
#% 379445
#% 397354
#% 578388
#% 654444
#% 654462
#% 654463
#% 654489
#% 654497
#% 726621
#% 729932
#% 729943
#% 729966
#% 745513
#% 769896
#% 800505
#% 800623
#% 810058
#% 853059
#% 993959
#% 993961
#% 1015261
#% 1015280
#% 1015301
#% 1015324
#% 1016153
#% 1756509
#! In this paper, we introduce SPIRIT (Streaming Pattern dIscoveRy in multIple Time-series). Given n numerical data streams, all of whose values we observe at each time tick t, SPIRIT can incrementally find correlations and hidden variables, which summarise the key trends in the entire stream collection. It can do this quickly, with no buffering of stream values and without comparing pairs of streams. Moreover, it is any-time, single pass, and it dynamically detects changes. The discovered trends can also be used to immediately spot potential anomalies, to do efficient forecasting and, more generally, to dramatically simplify further data processing. Our experimental evaluation and case studies show that SPIRIT can incrementally capture correlations and discover trends, efficiently and effectively.

#index 824710
#* Mining compressed frequent-pattern sets
#@ Dong Xin;Jiawei Han;Xifeng Yan;Hong Cheng
#t 2005
#c 4
#% 152934
#% 227919
#% 248791
#% 280409
#% 300120
#% 410276
#% 420063
#% 461909
#% 463903
#% 466664
#% 479484
#% 481290
#% 729933
#% 769876
#! A major challenge in frequent-pattern mining is the sheer size of its mining results. In many cases, a high min_sup threshold may discover only commonsense patterns but a low one may generate an explosive number of output patterns, which severely restricts its usage.In this paper, we study the problem of compressing frequent-pattern sets. Typically, frequent patterns can be clustered with a tightness measure δ (called δ-cluster), and a representative pattern can be selected for each cluster. Unfortunately, finding a minimum set of representative patterns is NP-Hard. We develop two greedy methods, RPglobal and RPlocal. The former has the guaranteed compression bound but higher computational complexity. The latter sacrifices the theoretical bounds but is far more efficient. Our performance study shows that the compression quality using RPlocal is very close to RPglobal, and both can reduce the number of closed frequent patterns by almost two orders of magnitude. Furthermore, RPlocal mines even faster than FPClose[11], a very fast closed frequent-pattern mining method. We also show that RPglobal and RPlocal can be combined together to balance the quality and efficiency.

#index 824711
#* Discovering large dense subgraphs in massive graphs
#@ David Gibson;Ravi Kumar;Andrew Tomkins
#t 2005
#c 4
#% 109307
#% 214673
#% 255137
#% 278835
#% 281214
#% 293720
#% 309749
#% 310514
#% 311808
#% 319876
#% 451536
#% 479969
#% 481290
#% 498852
#% 565488
#% 577318
#% 577360
#% 769116
#% 769512
#% 769897
#% 770307
#% 772018
#% 785130
#% 1279489
#! We present a new algorithm for finding large, dense subgraphs in massive graphs. Our algorithm is based on a recursive application of fingerprinting via shingles, and is extremely efficient, capable of handling graphs with tens of billions of edges on a single machine with modest resources.We apply our algorithm to characterize the large, dense subgraphs of a graph showing connections between hosts on the World Wide Web; this graph contains over 50M hosts and 11B edges, gathered from 2.1B web pages. We measure the distribution of these dense subgraphs and their evolution over time. We show that more than half of these hosts participate in some dense subgraph found by the analysis. There are several hundred giant dense subgraphs of at least ten thousand hosts; two thousand dense subgraphs at least a thousand hosts; and almost 64K dense subgraphs of at least a hundred hosts.Upon examination, many of the dense subgraphs output by our algorithm are link spam, i.e., websites that attempt to manipulate search engine rankings through aggressive interlinking to simulate popular content. We therefore propose dense subgraph extraction as a useful primitive for spam detection, and discuss its incorporation into the workflow of web search engines.

#index 824712
#* General purpose database summarization
#@ Régis Saint-Paul;Guillaume Raschia;Noureddine Mouaddib
#t 2005
#c 4
#% 173000
#% 287794
#% 333954
#% 451052
#% 480124
#% 482049
#% 503731
#% 572305
#% 654500
#% 745494
#% 993996
#! In this paper, a message-oriented architecture for large database summarization is presented. The summarization system takes a database table as input and produces a reduced version of this table through both a rewriting and a generalization process. The resulting table provides tuples with less precision than the original but yet are very informative of the actual content of the database. This reduced form can be used as input for advanced data mining processes as well as some specific application presented in other works. We describe the incremental maintenance of the summarized table, the system capability to directly deal with XML database systems, and finally scalability which allows it to handle very large datasets of a million record.

#index 824713
#* Online estimation for subset-based SQL queries
#@ Christopher Jermaine;Alin Dobra;Abhijit Pol;Shantanu Joshi
#t 2005
#c 4
#% 227883
#% 273910
#% 387855
#% 438135
#% 503719
#% 654463
#! The largest databases in use today are so large that answering a query exactly can take minutes, hours, or even days. One way to address this problem is to make use of approximation algorithms. Previous work on online aggregation has considered how to give online estimates with ever-increasing accuracy for aggregate functions over relational join and selection queries. However, no existing work is applicable to online estimation over subset-based SQL queries-those queries with a correlated subquery linked to an outer query via a NOT EXISTS, NOT IN, EXISTS, or IN clause (other queries such as EXCEPT and INTERSECT can also be seen as subset-based queries). In this paper we develop algorithms for online estimation over such queries, and consider the difficult problem of providing probabilistic accuracy guarantees at all times during query execution.

#index 824714
#* Content-based routing: different plans for different data
#@ Pedro Bizarro;Shivnath Babu;David DeWitt;Jennifer Widom
#t 2005
#c 4
#% 136740
#% 248793
#% 273911
#% 300167
#% 310500
#% 333926
#% 333946
#% 333986
#% 338425
#% 376266
#% 397371
#% 480803
#% 578560
#% 654482
#% 654497
#% 742047
#% 750252
#% 765434
#% 765435
#% 765455
#% 765456
#% 765500
#% 800505
#% 810016
#% 1016178
#! Query optimizers in current database systems are designed to pick a single efficient plan for a given query based on current statistical properties of the data. However, different subsets of the data can sometimes have very different statistical properties. In such scenarios it can be more efficient to process different subsets of the data for a query using different plans. We propose a new query processing technique called content-based routing (CBR) that eliminates the single-plan restriction in current systems. We present low-overhead adaptive algorithms that partition input data based on statistical properties relevant to query execution strategies, and efficiently route individual tuples through customized plans based on their partition. We have implemented CBR as an extension to the Eddies query processor in the TelegraphCQ system, and we present an extensive experimental evaluation showing the significant performance benefits of CBR.

#index 824715
#* REED: robust, efficient filtering and event detection in sensor networks
#@ Daniel J. Abadi;Samuel Madden;Wolfgang Lindner
#t 2005
#c 4
#% 111368
#% 285926
#% 289282
#% 297915
#% 309433
#% 322884
#% 346845
#% 411554
#% 442700
#% 479937
#% 622760
#% 654482
#% 731087
#% 731096
#% 783736
#% 805466
#% 963581
#% 963582
#% 1016271
#% 1394365
#! This paper presents a set of algorithms for efficiently evaluating join queries over static data tables in sensor networks. We describe and evaluate three algorithms that take advantage of distributed join techniques. Our algorithms are capable of running in limited amounts of RAM, can distribute the storage burden over groups of nodes, and are tolerant to dropped packets and node failures. REED is thus suitable for a wide range of event-detection applications that traditional sensor network database and data collection systems cannot be used to implement.

#index 824716
#* Shuffling a stacked deck: the case for partially randomized ranking of search engine results
#@ Sandeep Pandey;Sourashis Roy;Christopher Olston;Junghoo Cho;Soumen Chakrabarti
#t 2005
#c 4
#% 349411
#% 399058
#% 438193
#% 503228
#% 577224
#% 577302
#% 577329
#% 593891
#% 754058
#% 754060
#% 769488
#% 810054
#% 813734
#! In-degree, PageRank, number of visits and other measures of Web page popularity significantly influence the ranking of search results by modern search engines. The assumption is that popularity is closely correlated with quality, a more elusive concept that is difficult to measure directly. Unfortunately, the correlation between popularity and quality is very weak for newly-created pages that have yet to receive many visits and/or in-links. Worse, since discovery of new content is largely done by querying search engines, and because users usually focus their attention on the top few results, newly-created but high-quality pages are effectively "shut out," and it can take a very long time before they become popular.We propose a simple and elegant solution to this problem: the introduction of a controlled amount of randomness into search result ranking methods. Doing so offers new pages a chance to prove their worth, although clearly using too much randomness will degrade result quality and annul any benefits achieved. Hence there is a tradeoff between exploration to estimate the quality of new pages and exploitation of pages already known to be of high quality. We study this tradeoff both analytically and via simulation, in the context of an economic objective function based on aggregate result quality amortized over time. We show that a modest amount of randomness leads to improved search results.

#index 824717
#* Indexing mixed types for approximate retrieval
#@ Liang Jin;Chen Li;Nick Koudas;Anthony K. H. Tung
#t 2005
#c 4
#% 70370
#% 201889
#% 235941
#% 300181
#% 310546
#% 427199
#% 443055
#% 479462
#% 480499
#% 480654
#% 587758
#% 654467
#% 824684
#% 993980
#! In various applications such as data cleansing, being able to retrieve categorical or numerical attributes based on notions of approximate match (e.g., edit distance, numerical distance) is of profound importance. Commonly, approximate match predicates are specified on combinations of attributes in conjunction. Existing database techniques for approximate retrieval, however, limit their applicability to single attribute retrieval through B-trees and their variants. In this paper, we propose a methodology that utilizes known multidimensional indexing structures for the problem of approximate multi-attribute retrieval. Our method enables indexing of a collection of string and/or numeric attributes to facilitate approximate retrieval using edit distance as an approximate match predicate for strings and numeric distance for numeric attributes. The approach presented is based on representing sets of strings at higher levels of the index structure as tries suitably compressed in a way that reasoning about edit distance between a query string and a compressed trie at index nodes is still feasible. We propose and evaluate various techniques to generate the compressed trie representation and fully specify our indexing methodology. Our experimental results show the benefits of our proposal when compared with various alternate strategies for the same problem.

#index 824718
#* Answering queries from statistics and probabilistic views
#@ Nilesh Dalvi;Dan Suciu
#t 2005
#c 4
#% 194290
#% 215225
#% 216970
#% 235023
#% 378409
#% 384978
#% 442830
#% 480102
#% 480645
#% 481923
#% 572314
#% 641044
#% 765455
#% 1016201
#% 1700137
#! Systems integrating dozens of databases, in the scientific domain or in a large corporation, need to cope with a wide variety of imprecisions, such as: different representations of the same object in different sources; imperfect and noisy schema alignments; contradictory information across sources; constraint violations; or insufficient evidence to answer a given query. If standard query semantics were applied to such data, all but the most trivial queries will return an empty answer.

#index 824719
#* Inspector joins
#@ Shimin Chen;Anastassia Ailamaki;Phillip B. Gibbons;Todd C. Mowry
#t 2005
#c 4
#% 3771
#% 13016
#% 18614
#% 36117
#% 136740
#% 248793
#% 322884
#% 479821
#% 480464
#% 480803
#% 566122
#% 745501
#% 765456
#! The key idea behind Inspector Joins is that during the I/O partitioning phase of a hash-based join, we have the opportunity to look at the actual data itself and then use this knowledge in two ways: (1) to create specialized indexes, specific to the given query on the given data, for optimizing the CPU cache performance of the subsequent join phase of the algorithm, and (2) to decide which join phase algorithm best suits this specific query. We show how inspector joins, employing novel statistics and specialized indexes, match or exceed the performance of state-of-the-art cache-friendly hash join algorithms. For example, when run on eight or more processors, our experiments show that inspector joins offer 1.1-1.4X speedups over these previous algorithms, with the speedup increasing as the number of processors increases.

#index 824720
#* Revisiting pipelined parallelism in multi-join query processing
#@ Bin Liu;Elke A. Rundensteiner
#t 2005
#c 4
#% 58352
#% 86929
#% 115661
#% 116040
#% 128452
#% 152915
#% 201885
#% 210199
#% 217052
#% 308351
#% 339717
#% 340663
#% 435116
#% 442698
#% 442700
#% 442903
#% 443065
#% 463108
#% 480424
#% 480595
#% 480943
#% 480954
#% 480966
#% 481289
#% 481784
#% 571084
#% 791180
#% 824781
#! Multi-join queries are the core of any integration service that integrates data from multiple distributed data sources. Due to the large number of data sources and possibly high volumes of data, the evaluation of multi-join queries faces increasing scalability concerns. State-of-the-art parallel multi-join query processing commonly assume that the application of maximal pipelined parallelism leads to superior performance. In this paper, we instead illustrate that this assumption does not generally hold. We investigate how best to combine pipelined parallelism with alternate forms of parallelism to achieve an overall effective processing strategy. A segmented bushy processing strategy is proposed. Experimental studies are conducted on an actual software system over a cluster of high-performance PCs. The experimental results confirm that the proposed solution leads to about 50% improvement in terms of total processing time in comparison to existing state-of-the-art solutions.

#index 824721
#* Early hash join: a configurable algorithm for the efficient and early production of join results
#@ Ramon Lawrence
#t 2005
#c 4
#% 159337
#% 248795
#% 273908
#% 273910
#% 273911
#% 340635
#% 397352
#% 397370
#% 427195
#% 479623
#% 480120
#% 480272
#% 736391
#% 745488
#% 765426
#% 765467
#% 993948
#% 993956
#% 1015278
#% 1015313
#% 1015317
#% 1016179
#! Minimizing both the response time to produce the first few thousand results and the overall execution time is important for interactive querying. Current join algorithms either minimize the execution time at the expense of response time or minimize response time by producing results early without optimizing the total time. We present a hash-based join algorithm, called early hash join, which can be dynamically configured at any point during join processing to tradeoff faster production of results for overall execution time. We demonstrate that varying how inputs are read has a major effect on these two factors and provide formulas that allow an optimizer to calculate the expected rate of join output and the number of I/O operations performed using different input reading strategies. Experimental results show that early hash join performs significantly fewer I/O operations and executes faster than other early join algorithms, especially for one-to-many joins. Its overall execution time is comparable to standard hybrid hash join, but its response time is an order of magnitude faster. Thus, early hash join can replace hybrid hash join in any situation where a fast initial response time is beneficial without the penalty in overall execution time exhibited by other early join algorithms.

#index 824722
#* On map-matching vehicle tracking data
#@ Sotiris Brakatsoulas;Dieter Pfoser;Randall Salas;Carola Wenk
#t 2005
#c 4
#% 443703
#% 527176
#% 733271
#% 765187
#% 800527
#% 1068359
#! Vehicle tracking data is an essential "raw" material for a broad range of applications such as traffic management and control, routing, and navigation. An important issue with this data is its accuracy. The method of sampling vehicular movement using GPS is affected by two error sources and consequently produces inaccurate trajectory data. To become useful, the data has to be related to the underlying road network by means of map matching algorithms. We present three such algorithms that consider especially the trajectory nature of the data rather than simply the current position as in the typical map-matching case. An incremental algorithm is proposed that matches consecutive portions of the trajectory to the road network, effectively trading accuracy for speed of computation. In contrast, the two global algorithms compare the entire trajectory to candidate paths in the road network. The algorithms are evaluated in terms of (i) their running time and (ii) the quality of their matching result. Two novel quality measures utilizing the Fréchet distance are introduced and subsequently used in an experimental evaluation to assess the quality of matching real tracking data to a road network.

#index 824723
#* An efficient and scalable approach to CNN queries in a road network
#@ Hyung-Ju Cho;Chin-Wan Chung
#t 2005
#c 4
#% 86950
#% 413797
#% 465167
#% 729850
#% 729851
#% 1015321
#% 1016199
#! A continuous search in a road network retrieves the objects which satisfy a query condition at any point on a path. For example, return the three nearest restaurants from all locations on my route from point s to point e. In this paper, we deal with NN queries as well as continuous NN queries in the context of moving objects databases. The performance of existing approaches based on the network distance such as the shortest path length depends largely on the density of objects of interest. To overcome this problem, we propose UNICONS (a unique continuous search algorithm) for NN queries and CNN queries performed on a network. We incorporate the use of precomputed NN lists into Dijkstra's algorithm for NN queries. A mathematical rationale is employed to produce the final results of CNN queries. Experimental results for real-life datasets of various sizes show that UNICONS outperforms its competitors by up to 3.5 times for NN queries and 5 times for CNN queries depending on the density of objects and the number of NNs required.

#index 824724
#* Complex spatio-temporal pattern queries
#@ Marios Hadjieleftheriou;George Kollios;Petko Bakalov;Vassilis J. Tsotras
#t 2005
#c 4
#% 201876
#% 252304
#% 287466
#% 315005
#% 333854
#% 427199
#% 452847
#% 458857
#% 480473
#% 480817
#% 495433
#% 503882
#% 527195
#% 527328
#% 745464
#% 765453
#% 769899
#% 771228
#! This paper introduces a novel type of query, what we name Spatio-temporal Pattern Queries (STP). Such a query specifies a spatiotemporal pattern as a sequence of distinct spatial predicates where the predicate temporal ordering (exact or relative) matters. STP queries can use various types of spatial predicates (range search, nearest neighbor, etc.) where each such predicate is associated (1) with an exact temporal constraint (a time-instant or a time-interval), or (2) more generally, with a relative order among the other query predicates. Using traditional spatiotemporal index structures for these types of queries would be either inefficient or not an applicable solution. Alternatively, we propose specialized query evaluation algorithms for STP queries With Time. We also present a novel index structure, suitable for STP queries With Order. Finally, we conduct a comprehensive experimental evaluation to show the merits of our techniques.

#index 824725
#* Distributed privacy preserving information sharing
#@ Nan Zhang;Wei Zhao
#t 2005
#c 4
#% 1714
#% 23638
#% 271185
#% 301569
#% 318401
#% 387427
#% 545454
#% 577289
#% 635215
#% 654448
#% 743280
#% 765478
#% 772829
#% 800559
#! In this paper, we address issues related to sharing information in a distributed system consisting of autonomous entities, each of which holds a private database. Semi-honest behavior has been widely adopted as the model for adversarial threats. However, it substantially underestimates the capability of adversaries in reality. In this paper, we consider a threat space containing more powerful adversaries that includes not only semi-honest but also those malicious adversaries. In particular, we classify malicious adversaries into two widely existing subclasses, called weakly malicious and strongly malicious adversaries, respectively. We define a measure of privacy leakage for information sharing systems and propose protocols that can effectively and efficiently protect privacy against different kinds of malicious adversaries.

#index 824726
#* On k-anonymity and the curse of dimensionality
#@ Charu C. Aggarwal
#t 2005
#c 4
#% 264246
#% 300184
#% 333876
#% 342614
#% 481290
#% 577233
#% 577289
#% 800515
#% 801690
#! In recent years, the wide availability of personal data has made the problem of privacy preserving data mining an important one. A number of methods have recently been proposed for privacy preserving data mining of multidimensional data records. One of the methods for privacy preserving data mining is that of anonymization, in which a record is released only if it is indistinguishable from k other entities in the data. We note that methods such as k-anonymity are highly dependent upon spatial locality in order to effectively implement the technique in a statistically robust way. In high dimensional space the data becomes sparse, and the concept of spatial locality is no longer easy to define from an application point of view. In this paper, we view the k-anonymization problem from the perspective of inference attacks over all possible combinations of attributes. We show that when the data contains a large number of attributes which may be considered quasi-identifiers, it becomes difficult to anonymize the data without an unacceptably high amount of information loss. This is because an exponential number of combinations of dimensions can be used to make precise inference attacks, even when individual attributes are partially specified within a range. We provide an analysis of the effect of dimensionality on k-anonymity methods. We conclude that when a data set contains a large number of attributes which are open to inference attacks, we are faced with a choice of either completely suppressing most of the data or losing the desired level of anonymity. Thus, this paper shows that the curse of high dimensionality also applies to the problem of privacy preserving data mining.

#index 824727
#* Checking for k-anonymity violation by views
#@ Chao Yao;X. Sean Wang;Sushil Jajodia
#t 2005
#c 4
#% 248030
#% 300184
#% 346656
#% 408396
#% 443382
#% 443463
#% 712713
#% 765447
#% 765449
#% 801690
#% 1700133
#% 1700137
#! When a private relational table is published using views, secrecy or privacy may be violated. This paper uses a formally-defined notion of k-anonymity to measure disclosure by views, where k 1 is a positive integer. Intuitively, violation of k-anonymity occurs when a particular attribute value of an entity can be determined to be among less than k possibilities by using the views together with the schema information of the private table. The paper shows that, in general, whether a set of views violates k-anonymity is a computationally hard problem. Subcases are identified and their computational complexities discussed. Especially interesting are those subcases that yield polynomial checking algorithms (in the number of tuples in the views). The paper also provides an efficient conservative algorithm that checks for necessary conditions for k-anonymity violation.

#index 824728
#* Indexing multi-dimensional uncertain data with arbitrary probability density functions
#@ Yufei Tao;Reynold Cheng;Xiaokui Xiao;Wang Kay Ngai;Ben Kao;Sunil Prabhakar
#t 2005
#c 4
#% 86950
#% 137887
#% 213975
#% 237187
#% 295512
#% 300174
#% 410276
#% 464847
#% 481956
#% 527176
#% 654487
#% 772835
#% 1016201
#% 1016202
#! In an "uncertain database", an object o is associated with a multi-dimensional probability density function(pdf), which describes the likelihood that o appears at each position in the data space. A fundamental operation is the "probabilistic range search" which, given a value pq and a rectangular area rq, retrieves the objects that appear in rq with probabilities at least pq. In this paper, we propose the U-tree, an access method designed to optimize both the I/O and CPU time of range retrieval on multi-dimensional imprecise data. The new structure is fully dynamic (i.e., objects can be incrementally inserted/deleted in any order), and does not place any constraints on the data pdfs. We verify the query and update efficiency of U-trees with extensive experiments.

#index 824729
#* A trajectory splitting model for efficient spatio-temporal indexing
#@ Slobodan Rasetic;Jörg Sander;James Elding;Mario A. Nascimento
#t 2005
#c 4
#% 86950
#% 137887
#% 172949
#% 273706
#% 296090
#% 300174
#% 413597
#% 427199
#% 458857
#% 480473
#% 480825
#% 495433
#% 527166
#% 555050
#% 617878
#% 659971
#% 1015320
#% 1180064
#! This paper addresses the problem of splitting trajectories optimally for the purpose of efficiently supporting spatio-temporal range queries using index structures (e.g., R-trees) that use minimum bounding hyper-rectangles as trajectory approximations. We derive a formal cost model for estimating the number of I/Os required to evaluate a spatio-temporal range query with respect to a given query size and an arbitrary split of a trajectory. Based on the proposed model, we introduce a dynamic programming algorithm for splitting a set of trajectories that minimizes the number of expected disk I/Os with respect to an average query size. In addition, we develop a linear time, near optimal solution for this problem to be used in a dynamic case where trajectory points are continuously updated. Our experimental evaluation confirms the effectiveness and efficiency of our proposed splitting policies when embedded into an R-tree structure.

#index 824730
#* On computing top-t most influential spatial sites
#@ Tian Xia;Donghui Zhang;Evangelos Kanoulas;Yang Du
#t 2005
#c 4
#% 201876
#% 235114
#% 300163
#% 465009
#% 480661
#% 495433
#% 527189
#% 527328
#% 730019
#% 800510
#% 993999
#% 1016191
#! Given a set O of weighted objects, a set S of sites, and a query site s, the bichromatic RNN query computes the influence set of s, or the set of objects in O that consider s as the nearest site among all sites in S. The influence of a site s can be defined as the total weight of its RNNs. This paper addresses the new and interesting problem of finding the top-t most influential sites from S, inside a given spatial region Q. A straightforward approach is to find the sites in Q, and compute the RNNs of every such site. This approach is not efficient for two reasons. First, all sites in Q need to be identified whatsoever, and the number may be large. Second, both the site R-tree and the object R-tree need to be browsed a large number of times. For each site in Q, the R-tree of sites is browsed to identify the influence region -- a polygonal region that may contain RNNs, and then the R-tree of objects is browsed to find the RNN set. This paper proposes an algorithm called TopInfluential-Sites, which finds the top-t most influential sites by browsing both trees once systematically. Novel pruning techniques are provided, based on a new metric called minExistDNN. There is no need to compute the influence for all sites in Q, or even to visit all sites in Q. Experimental results verify that our proposed method outperforms the straightforward approach.

#index 824732
#* Efficient implementation of large-scale multi-structural databases
#@ R. Fagin;Ph. Kolaitis;R. Kumar;J. Novak;D. Sivakumar;A. Tomkins
#t 2005
#c 4
#% 210182
#% 480123
#% 503884
#% 667501
#% 667506
#% 770307
#% 787548
#% 809250
#! In earlier work, we defined "multi-structural databases," a data model to support efficient analysis of large, complex data sets over multiple numerical and hierarchical dimensions. We defined three types of queries over this data model, each of which required solving an optimization problem. An example is to find the ten most significant non-overlapping regions of geography crossed with time in which coverage of the Olympics was much stronger in newspapers than online sources.In this paper, we present a general query framework capturing the original three queries as part of a much broader family. We then give efficient algorithms for particular subclasses of this family. Finally, we describe an implementation of these algorithms that operates on a collection of several billion web documents. Using our algorithms in conjunction with random sampling techniques, our system can solve these queries in real time.

#index 824733
#* OLAP over uncertain and imprecise data
#@ Doug Burdick;Prasad M. Deshpande;T. S. Jayram;Raghu Ramakrishnan;Shivakumar Vaithyanathan
#t 2005
#c 4
#% 32879
#% 108510
#% 115608
#% 119816
#% 212999
#% 235023
#% 237198
#% 271717
#% 273687
#% 399792
#% 442830
#% 443030
#% 443455
#% 480102
#% 487853
#% 503731
#% 582130
#% 617864
#% 654487
#% 664844
#% 793254
#! We extend the OLAP data model to represent data ambiguity, specifically imprecision and uncertainty, and introduce an allocation-based approach to the semantics of aggregation queries over such data. We identify three natural query properties and use them to shed light on alternative query semantics. While there is much work on representing and querying ambiguous data, to our knowledge this is the first paper to handle both imprecision and uncertainty in an OLAP setting.

#index 824734
#* Prediction cubes
#@ Bee-Chung Chen;Lei Chen;Yi Lin;Raghu Ramakrishnan
#t 2005
#c 4
#% 129987
#% 136350
#% 210182
#% 280419
#% 280480
#% 290482
#% 347881
#% 376266
#% 400847
#% 420053
#% 420141
#% 480630
#% 566140
#% 717181
#! In this paper, we introduce a new family of tools for exploratory data analysis, called prediction cubes. As in standard OLAP data cubes, each cell in a prediction cube contains a value that summarizes the data belonging to that cell, and the granularity of cells can be changed via operations such as roll-up and drill-down. In contrast to data cubes, in which each cell value is computed by an aggregate function, e.g., SUM or AVG, each cell value in a prediction cube summarizes a predictive model trained on the data corresponding to that cell, and characterizes its decision behavior or predictiveness. In this paper, we propose and motivate prediction cubes, and show that they can be efficiently computed by exploiting the idea of model decomposition.

#index 824735
#* Tuning schema matching software using synthetic scenarios
#@ Mayssam Sayyadian;Yoonkyong Lee;AnHai Doan;Arnon S. Rosenthal
#t 2005
#c 4
#% 22948
#% 278397
#% 331769
#% 333943
#% 333988
#% 333990
#% 348187
#% 379119
#% 438427
#% 479783
#% 480153
#% 480645
#% 488766
#% 529190
#% 551850
#% 571057
#% 572314
#% 637829
#% 641044
#% 654458
#% 654459
#% 659941
#% 660001
#% 765409
#% 765433
#% 765462
#% 790843
#% 790846
#% 790848
#% 800497
#% 800498
#% 810111
#% 824737
#% 993982
#% 1016240
#! Most recent schema matching systems assemble multiple components, each employing a particular matching technique. The domain user must then tune the system: select the right component to be executed and correctly adjust their numerous "knobs" (e.g., thresholds, formula coefficients). Tuning is skill- and time-intensive, but (as we show) without it the matching accuracy is significantly inferior.We describe eTuner, an approach to automatically tune schema matching systems. Given a schema S, we match S against synthetic schemas, for which the ground truth mapping is known, and find a tuning that demonstrably improves the performance of matching S against real schemas. To efficiently search the huge space of tuning configurations, eTuner works sequentially, starting with tuning the lowest level components. To increase the applicability of eTuner, we develop methods to tune a broad range of matching components. While the tuning process is completely automatic, eTuner can also exploit user assistance (whenever available) to further improve the tuning quality. We employed eTuner to tune four recently developed matching systems on several real-world domains. eTuner produced tuned matching systems that achieve higher accuracy than using the systems with currently possible tuning methods, at virtually no cost to the domain user.

#index 824736
#* Semantic adaptation of schema mappings when schemas evolve
#@ Cong Yu;Lucian Popa
#t 2005
#c 4
#% 32903
#% 287733
#% 315025
#% 378409
#% 443527
#% 465057
#% 480134
#% 480822
#% 564416
#% 572311
#% 572314
#% 599549
#% 765432
#% 765540
#% 801676
#% 809249
#% 993981
#% 1015302
#% 1015303
#% 1393677
#! Schemas evolve over time to accommodate the changes in the information they represent. Such evolution causes invalidation of various artifacts depending on the schemas, such as schema mappings. In a heterogenous environment, where cooperation among data sources depends essentially upon them, schema mappings must be adapted to reflect schema evolution. In this study, we explore the mapping composition approach for addressing this mapping adaptation problem. We study the semantics of mapping composition in the context of mapping adaptation and compare our approach with the incremental approach of Velegrakis et al [21]. We show that our method is superior in terms of capturing the semantics of both the original mappings and the evolution. We design and implement a mapping adaptation system based on mapping composition as well as additional mapping pruning techniques that significantly speed up the adaptation. We conduct comprehensive experimental analysis and show that the composition approach is practical in various evolution scenarios. The mapping language that we consider is a nested relational extension of the second-order dependencies of Fagin et al [7]. Our work can also be seen as an implementation of the mapping composition operator of the model management framework.

#index 824737
#* Mapping maintenance for data integration systems
#@ Robert McCann;Bedoor AlShebli;Quoc Le;Hoa Nguyen;Long Vu;AnHai Doan
#t 2005
#c 4
#% 229827
#% 262043
#% 266102
#% 273911
#% 280413
#% 300179
#% 333990
#% 342872
#% 424302
#% 428400
#% 451055
#% 479452
#% 481923
#% 572314
#% 654458
#% 654459
#% 705442
#% 729873
#% 754068
#% 765433
#% 769903
#% 1015303
#% 1271981
#! To answer user queries, a data integration system employs a set of semantic mappings between the mediated schema and the schemas of data sources. In dynamic environments sources often undergo changes that invalidate the mappings. Hence, once the system is deployed, the administrator must monitor it over time, to detect and repair broken mappings. Today such continuous monitoring is extremely labor intensive, and poses a key bottleneck to the widespread deployment of data integration systems in practice.We describe MAVERIC, an automatic solution to detecting broken mappings. At the heart of MAVERIC is a set of computationally inexpensive modules called sensors, which capture salient characteristics of data sources (e.g., value distributions, HTML layout properties). We describe how MAVERIC trains and deploys the sensors to detect broken mappings. Next we develop three novel improvements: perturbation (i.e., injecting artificial changes into the sources) and multi-source training to improve detection accuracy, and filtering to further reduce the number of false alarms. Experiments over 114 real-world sources in six domains demonstrate the effectiveness of our sensor-based approach over existing solutions, as well as the utility of our improvements.

#index 824738
#* Bridging the gap between OLAP and SQL
#@ Jens-Peter Dittrich;Donald Kossmann;Alexander Kreutz
#t 2005
#c 4
#% 191175
#% 201951
#% 227872
#% 378403
#% 442850
#% 462044
#% 462204
#% 464215
#% 503400
#% 503731
#% 654445
#% 765417
#% 765418
#% 810050
#% 1016175
#% 1016212
#% 1016235
#! In the last ten years, database vendors have invested heavily in order to extend their products with new features for decision support. Examples of functionality that has been added are top N [2], ranking [13, 7], spreadsheet computations [19], grouping sets [14], data cube [9], and moving sums [15] in order to name just a few. Unfortunately, many modern OLAP systems do not use that functionality or replicate a great deal of it in addition to other database-related functionality. In fact, the gap between the functionality provided by an OLAP system and the functionality used from the underlying database systems has widened in the past, rather than narrowed. The reasons for this trend are that SQL as a data definition and query language, the relational model, and the client/server architecture of the current generation of database products have fundamental shortcomings for OLAP. This paper lists these deficiencies and presents the BTell OLAP engine as an example on how to bridge these shortcomings. In addition, we discuss how to extend current DBMS to better support OLAP in the future.

#index 824739
#* Optimizing refresh of a set of materialized views
#@ Nathan Folkert;Abhinav Gupta;Andrew Witkowski;Sankar Subramanian;Srikanth Bellamkonda;Shrikanth Shankar;Tolga Bozkaya;Lei Sheng
#t 2005
#c 4
#% 13016
#% 152928
#% 198465
#% 201928
#% 201929
#% 227869
#% 273696
#% 300138
#% 464056
#% 464215
#% 479792
#% 480141
#% 480149
#% 481951
#% 482082
#% 654445
#! In many data warehousing environments, it is common to have materialized views (MVs) at different levels of aggregation of one or more dimensions. The extreme case of this is relational OLAP environments, where, for performance reasons, nearly all levels of aggregation across all dimensions may be computed and stored in MVs. Furthermore, base tables and MVs are usually partitioned for ease and speed of maintenance. In these scenarios, updates to the base table are done using Bulk or Partition operations like add, exchange, truncate and drop partition. If changes to base tables can be tracked at the partition level, join dependencies. functional dependencies and query rewrite can be used to optimize refresh of an individual MV. The refresh optimizer, in the presence of partitioned tables and MVs, may recognize dependencies between base table and the MV partitions leading to the generation of very efficient refresh expressions. Additionally, in the presence of multiple MVs, the refresh subsytem can come up with an optimal refresh schedule such that MVs can be refreshed using query rewrite against previously refreshed MVs. This makes the database server more manageable and user friendly since a single function call can optimally refresh all the MVs in the system.

#index 824740
#* Large scale data warehouses on grid: Oracle database 10g and HP proliant servers
#@ Meikel Poess;Raghunath Othayoth Nambiar
#t 2005
#c 4
#% 191175
#% 208037
#% 275743
#% 322880
#% 411252
#% 479452
#% 480130
#% 571217
#% 572300
#! Grid computing has the potential to drastically change enterprise computing as we know it today. The main concept of grid computing is viewing computing as a utility. It should not matter where data resides, or what computer processes a task. This concept has been applied successfully to academic research. It also has many advantages for commercial data warehouse applications such as virtualization, flexible provisioning, reduced cost due to commodity hardware, high availability and high scale-out. In this paper we show how a large-scale, high-performing and scalable grid-based data warehouse can be implemented using commodity hardware (industry-standard x86-based). Oracle Database 10g and the Linux operating system. We further demonstrate this architecture in a recently published TPC-H benchmark.

#index 824741
#* The integrated microbial genomes (IMG) system: a case study in biological data management
#@ Victor M. Markowitz;Frank Korzeniewski;Krishna Palaniappan;Ernest Szeto;Natalia Ivanova;Nikos C. Kyrpides
#t 2005
#c 4
#% 772132
#% 1016230
#% 1016231
#% 1016236
#! Biological data management includes the traditional areas of data generation, acquisition, modelling, integration, and analysis. Although numerous academic biological data management systems are currently available, employing them effectively remains a significant challenge. We discuss how this challenge was addressed in the course of developing the Integrated Microbial Genomes (IMG) system for comparative analysis of microbial genome data.

#index 824742
#* A heartbeat mechanism and its application in gigascope
#@ Theodore Johnson;S. Muthukrishnan;Vladislav Shkapenyuk;Oliver Spatscheck
#t 2005
#c 4
#% 378388
#% 397414
#% 453512
#% 464215
#% 578391
#% 654497
#% 660004
#% 801694
#% 803602
#% 810007
#% 810033
#% 979303
#% 993949
#% 1015373
#! Data stream management systems often rely on ordering properties of tuple attributes in order to implement non-blocking operators. However, query operators that work with multiple streams, such as stream merge or join, can often still block if one of the input stream is very slow or bursty. In principle, punctuation and heartbeat mechanisms have been proposed to unblock streaming operators. In practice, it is a challenge to incorporate such mechanisms into a high-performance stream management system that is operational in an industrial application.In this paper, we introduce a system for punctuation-carrying heartbeat generation that we developed for Gigascope, a high-performance streaming database for network monitoring, that is operationally used within AT&T's IP backbone. We show how heartbeats can be regularly generated by low-level nodes in query execution plans and propagated upward unblocking all streaming operators on its way. Additionally, our heartbeat mechanism can be used for other applications in distributed settings such as detecting node failures, performance monitoring, and query optimization. A performance evaluation using live data feeds shows that our system is capable of working at multiple Gigabit line speeds in a live, industrial deployment and can significantly decrease the query memory utilization.

#index 824743
#* Using a fuzzy classification query language for customer relationship management
#@ Andreas Meier;Nicolas Werro;Martin Albrecht;Miltiadis Sarakinos
#t 2005
#c 4
#% 70545
#% 212710
#% 410521
#! A key challenge for companies is to manage customer relationships as an asset. To create an effective toolkit for the analysis of customer relationships, a combination of relational databases and fuzzy logic is proposed. The fuzzy Classification Query Language allows marketers to improve customer equity, launch loyalty programs, automate mass customization, and refine marketing campaigns.

#index 824744
#* Flexible database generators
#@ Nicolas Bruno;Surajit Chaudhuri
#t 2005
#c 4
#% 172913
#% 273901
#% 333947
#% 334026
#% 365700
#% 397385
#% 397407
#% 482070
#% 482092
#% 632048
#% 632056
#% 632100
#% 765427
#! Evaluation and applicability of many database techniques, ranging from access methods, histograms, and optimization strategies to data normalization and mining, crucially depend on their ability to cope with varying data distributions in a robust way. However, comprehensive real data is often hard to come by, and there is no flexible data generation framework capable of modelling varying rich data distributions. This has led individual researchers to develop their own ad-hoc data generators for specific tasks. As a consequence, the resulting data distributions and query workloads are often hard to reproduce, analyze, and modify, thus preventing their wider usage. In this paper we present a flexible, easy to use, and scalable framework for database generation. We then discuss how to map several proposed synthetic distributions to our framework and report preliminary results.

#index 824745
#* Recovery principles of MySQL Cluster 5.1
#@ Mikael Ronström;Jonas Oreland
#t 2005
#c 4
#% 735
#% 403195
#% 797999
#! MySQL Cluster is a parallel main memory database. It is using the normal MySQL software with a new storage engine NDB Cluster. MySQL Cluster 5.1 has been adapted to also handle fields on disk. In this work a number of recovery principles of MySQL Cluster had to be adapted to handle very large data sizes. The article presents an efficient algorithm for synchronizing a starting node with very large data sets. It provides reasons for the unorthodox choice of a no-steal algorithm in the buffer manager. It also presents the algorithm to change the data.

#index 824746
#* Getting priorities straight: improving Linux support for database I/O
#@ Christoffer Hall;Philippe Bonnet
#t 2005
#c 4
#% 218154
#% 387173
#% 387563
#% 401436
#% 610217
#% 745471
#% 770369
#! The Linux 2.6 kernel supports asynchronous I/O as a result of propositions from the database industry. This is a positive evolution but is it a panacea? In the context of the Badger project, a collaboration between MySQL AB and University of Copenhagen, we evaluate how MySQL/InnoDB can best take advantage of Linux asynchronous I/O and how Linux can help MySQL/InnoDB best take advantage of the underlying I/O bandwidth. This is a crucial problem for the increasing number of MySQL servers deployed for very large database applications. In this paper, we first show that the conservative I/O submission policy used by InnoDB (as well as Oracle 9.2) leads to an under-utilization of the available I/O bandwidth. We then show that introducing prioritized asynchronous I/O in Linux will allow MySQL/InnoDB and the other Linux databases to fully utilize the available I/O bandwith using a more aggressive I/O submission policy.

#index 824747
#* Temporal management of RFID data
#@ Fusheng Wang;Peiya Liu
#t 2005
#c 4
#% 287631
#% 361445
#% 443289
#% 644230
#% 787130
#% 787131
#% 787174
#% 858997
#% 1016227
#% 1016228
#! RFID technology can be used to significantly improve the efficiency of business processes by providing the capability of automatic identification and data capture. This technology poses many new challenges on current data management systems. RFID data are time-dependent, dynamically changing, in large volumes, and carry implicit semantics. RFID data management systems need to effectively support such large scale temporal data created by RFID applications. These systems need to have an explicit temporal data model for RFID data to support tracking and monitoring queries. In addition, they need to have an automatic method to transform the primitive observations from RFID readers into derived data used in RFID-enabled applications. In this paper, we present an integrated RFID data management system -- Siemens RFID Middleware -- based on an expressive temporal data model for RFID data. Our system enables semantic RFID data filtering and automatic data transformation based on declarative rules, provides powerful query support of RFID object tracking and monitoring, and can be adapted to different RFID-enabled applications.

#index 824748
#* Supporting RFID-based item tracking applications in Oracle DBMS using a bitmap datatype
#@ Ying Hu;Seema Sundara;Timothy Chorma;Jagannathan Srinivasan
#t 2005
#c 4
#% 227861
#% 248814
#% 273904
#% 480458
#% 1016131
#% 1016228
#! Radio Frequency Identification (RFID) based item-level tracking holds the promise of revolutionizing supply-chain, retail store, and asset management applications. However, the high volume of data generated by item-level tracking poses challenges to the applications as well as to backend databases. This paper addresses the problem of efficiently modeling identifier collections occurring in RFID-based item-tracking applications and databases. Specifically, 1) a bitmap datatype is introduced to compactly represent a collection of identifiers, and 2) a set of bitmap access and manipulation routines is provided. The proposed bitmap datatype can model a collection of generic identifiers, including 64-bit, 96-bit, and 256-bit Electronic Product Codes™ (EPCs), and it can be used to represent both transient and persistent identifier collections. Persistent identifier collections can be stored in a table as a column of bitmap datatype. An efficient primary B+- tree-based storage scheme is proposed for such columns. The bitmap datatype can be easily implemented by leveraging the DBMS bitmap index implementation, which typically manages bitmaps of table row identifiers. This paper presents the bitmap datatype and related functionality, illustrates its usage in supporting RFID-based item-tracking applications, describes its prototype implementation in Oracle DBMS, and gives a performance study that characterizes the benefits of the bitmap datatype.

#index 824749
#* SVM in oracle database 10g: removing the barriers to widespread adoption of support vector machines
#@ Boriana L. Milenova;Joseph S. Yarmus;Marcos M. Campos
#t 2005
#c 4
#% 190581
#% 269217
#% 269218
#% 342706
#% 458379
#% 466677
#% 527493
#% 642773
#% 722757
#% 722797
#% 729940
#% 741673
#% 789649
#% 854484
#% 855602
#% 1558464
#! Contemporary commercial databases are placing an increased emphasis on analytic capabilities. Data mining technology has become crucial in enabling the analysis of large volumes of data. Modern data mining techniques have been shown to have high accuracy and good generalization to novel data. However, achieving results of good quality often requires high levels of user expertise. Support Vector Machines (SVM) is a powerful state-of-the-art data mining algorithm that can address problems not amenable to traditional statistical analysis. Nevertheless, its adoption remains limited due to methodological complexities, scalability challenges, and scarcity of production quality SVM implementations. This paper describes Oracle's implementation of SVM where the primary focus lies on ease of use and scalability while maintaining high performance accuracy. SVM is fully integrated within the Oracle database framework and thus can be easily leveraged in a variety of deployment scenarios.

#index 824750
#* Native XML support in DB2 universal database
#@ Matthias Nicola;Bert van der Linden
#t 2005
#c 4
#% 570876
#% 654493
#% 730031
#% 783793
#% 803121
#% 810036
#% 1016134
#% 1016224
#! The major relational database systems have been providing XML support for several years, predominantly by mapping XML to existing concepts such as LOBs or (object-)relational tables. The limitations of these approaches are well known in research and industry. Thus, a forthcoming version of DB2 Universal Database® is enhanced with comprehensive native XML support. "Native" means that XML documents are stored on disk pages in tree structures matching the XML data model. This avoids the mapping between XML and relational structures, and the corresponding limitations. The native XML storage is complemented with XML indexes, full XQuery, SQL/XML, and XML Schema support, as well as utilities such as a parallel high-speed XML bulk loader. This makes DB2 a true hybrid database system which places equal weight on XML and relational data management.

#index 824751
#* XQuery implementation in a relational database system
#@ Shankar Pal;Istvan Cseri;Oliver Seeliger;Michael Rys;Gideon Schaller;Wei Yu;Dragan Tomic;Adrian Baras;Brandon Berg;Denis Churin;Eugene Kogan
#t 2005
#c 4
#% 334006
#% 397366
#% 411759
#% 428146
#% 750045
#% 765488
#% 1015338
#% 1016150
#% 1016224
#! Many enterprise applications prefer to store XML data as a rich data type, i.e. a sequence of bytes, in a relational database system to avoid the complexity of decomposing the data into a large number of tables and the cost of reassembling the XML data. The upcoming release of Microsoft's SQL Server supports XQuery as the query language over such XML data using its relational infrastructure.XQuery is an emerging W3C recommendation for querying XML data. It provides a set of language constructs (FLWOR), the ability to dynamically shape the query result, and a large set of functions and operators. It includes the emerging W3C recommendation XPath 2.0 for path-based navigational access. XQuery's type system is compatible with that of XML Schema and allows static type checking.This paper describes the experiences and the challenges in implementing XQuery in Microsoft's SQL Server 2005. XQuery language constructs are compiled into an enhanced set of relational operators while preserving the semantics of XQuery. The query tree is optimized using relational optimization techniques, such as cost-based decisions, and rewrite rules based on XML schemas. Novel techniques are used for efficiently managing document order and XML hierarchy.

#index 824752
#* CXHist: an on-line classification-based histogram for XML string selectivity estimation
#@ Lipyeow Lim;Min Wang;Jeffrey Scott Vitter
#t 2005
#c 4
#% 92148
#% 210189
#% 273705
#% 273901
#% 299984
#% 333947
#% 340144
#% 397364
#% 397366
#% 397379
#% 458836
#% 461918
#% 465018
#% 479958
#% 480488
#% 480660
#% 482092
#% 571046
#% 659984
#% 729437
#% 745489
#% 810036
#% 993968
#% 993970
#% 1016149
#! Query optimization in IBM's System RX, the first truly relational-XML hybrid data management system, requires accurate selectivity estimation of path-value pairs, i.e., the number of nodes in the XML tree reachable by a given path with the given text value. Previous techniques have been inadequate, because they have focused mainly on the tag-labeled paths (tree structure) of the XML data. For most real XML data, the number of distinct string values at the leaf nodes is orders of magnitude larger than the set of distinct rooted tag paths. Hence, the real challenge lies in accurate selectivity estimation of the string predicates on the leaf values reachable via a given path.In this paper, we present CXHist, a novel workload-aware histogram technique that provides accurate selectivity estimation on a broad class of XML string-based queries. CXHist builds a histogram in an on-line manner by grouping queries into buckets using their true selectivity obtained from query feedback. The set of queries associated with each bucket is summarized into feature distributions. These feature distributions mimic a Bayesian classifier that is used to route a query to its associated bucket during selectivity estimation. We show how CXHist can be used for two general types of path, string queries: exact match queries and substring match queries. Experiments using a prototype show that CXHist provides accurate selectivity estimation for both exact match queries and substring match queries.

#index 824753
#* Consistency for web services applications
#@ Paul Greenfield;Dean Kuo;Surya Nepal;Alan Fekete
#t 2005
#c 4
#% 32897
#% 122904
#% 295410
#% 369768
#% 394417
#% 403195
#% 721517
#! A key challenge facing the designers of service-oriented applications is ensuring that the autonomous services that make up these distributed applications always finish in consistent states despite application-level failures and other exceptional events. This paper addresses this problem by first describing the relationship between internal service states, messages and application protocols and then shows how this relationship transforms the problem of ensuring consistent outcomes into a correctness problem that can be addressed with established protocol verification tools.

#index 824754
#* Query by Excel
#@ Andrew Witkowski;Srikanth Bellamkonda;Tolga Bozkaya;Aman Naimat;Lei Sheng;Sankar Subramanian;Allison Waingold
#t 2005
#c 4
#% 353359
#% 354739
#% 388706
#% 654445
#% 806213
#% 818507
#% 1016212
#! Spreadsheets, and MS Excel in particular, are established analysis tools. They offer an attractive user interface, provide an easy to use computational model, and offer substantial interactivity for what-if analysis. However, as opposed to RDBMS, spreadsheets do not provide a central repository hence they do not provide shareability of models built in Excel and lead to proliferation of multiple copies of the same spreadsheet. Furthermore, spreadsheets do not offer scalable computation, for example, they lack parallelization. To address the shareability, and scalability problems, we propose to automatically translate Excel computation into SQL. An analyst can import the data from a relational system, define computation over it using familiar Excel formulas and then translate and store it as a relational SQL view over the imported data. The Excel computation is then performed by the relational system. To edit the model, the analyst can bring the model back to Excel, modify it in Excel and store it back as an SQL View. We refer to this system as Query by Excel, QBX in short.

#index 824755
#* An efficient SQL-based RDF querying scheme
#@ Eugene Inseok Chong;Souripriya Das;George Eadon;Jagannathan Srinivasan
#t 2005
#c 4
#% 348181
#% 479792
#% 519415
#% 783540
#! Devising a scheme for efficient and scalable querying of Resource Description Framework (RDF) data has been an active area of current research. However, most approaches define new languages for querying RDF data, which has the following shortcomings: 1) They are difficult to integrate with SQL queries used in database applications, and 2) They incur inefficiency as data has to be transformed from SQL to the corresponding language data format. This paper proposes a SQL based scheme that avoids these problems. Specifically, it introduces a SQL table function RDF_MATCH to query RDF data. The results of RDF_MATCH table function can be further processed by SQL's rich querying capabilities and seamlessly combined with queries on traditional relational data. Furthermore, the RDF_MATCH table function invocation is rewritten as a SQL query, thereby avoiding run-time table function procedural overheads. It also enables optimization of rewritten query in conjunction with the rest of the query. The resulting query is executed efficiently by making use of B-tree indexes as well as specialized subject-property materialized views. This paper describes the functionality of the RDF_MATCH table function for querying RDF data, which can optionally include user-defined rulebases, and discusses its implementation in Oracle RDBMS. It also presents an experimental study characterizing the overhead eliminated by avoiding procedural code at runtime, characterizing performance under various input conditions, and demonstrating scalability using 80 million RDF triples from UniProt protein and annotation data.

#index 824756
#* Analyzing plan diagrams of database query optimizers
#@ Naveen Reddy;Jayant R. Haritsa
#t 2005
#c 4
#% 172900
#% 300196
#% 411554
#% 479786
#% 480803
#% 480955
#% 565457
#% 654472
#% 654473
#% 654518
#% 993945
#% 993946
#% 1015318
#% 1016264
#! A "plan diagram" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. In this paper, we present and analyze representative plan diagrams on a suite of popular commercial query optimizers for queries based on the TPC-H benchmark. These diagrams, which often appear similar to cubist paintings, provide a variety of interesting insights, including that current optimizers make extremely fine-grained plan choices, which may often be supplanted by less efficient options without substantively affecting the quality; that the plan optimality regions may have highly intricate patterns and irregular boundaries, indicating strongly non-linear cost models; that non-monotonic cost behavior exists where increasing result cardinalities decrease the estimated cost; and, that the basic assumptions underlying the research literature on parametric query optimization often do not hold in practice.

#index 824757
#* Database publication practices
#@ Philip A. Bernstein;David DeWitt;Andreas Heuer;Zachary Ives;Christian S. Jensen;Holger Meyer;M. Tamer Özsu;Richard T. Snodgrass;Kyu-Young Whang;Jennifer Widom
#t 2005
#c 4
#% 765513
#! There has been a growing interest in improving the publication processes for database research papers. This panel reports on recent changes in those processes and presents an initial cut at historical data for the VLDB Journal and ACM Transactions on Database Systems.

#index 824758
#* PrediCalc: a logical spreadsheet management system
#@ Michael Kassoff;Lee-Ming Zen;Ankit Garg;Michael Genesereth
#t 2005
#c 4
#% 52820
#% 58842
#% 61616
#% 140407
#% 443461
#% 850453
#! Computerized spreadsheets are a great success. They are often touted in newspapers and magazine articles as the first "killer app" for personal computers. Over the years, they have proven their worth time and again. Today, they are used for managing enterprises of all sorts - from one-person projects to multi-institutional conglomerates.

#index 824759
#* Automatic data fusion with HumMer
#@ Alexander Bilke;Jens Bleiholder;Felix Naumann;Christoph Böhm;Karsten Draba;Melanie Weis
#t 2005
#c 4
#% 480825
#% 800498
#% 800550
#% 810044
#% 1720717
#! Heterogeneous and dirty data is abundant. It is stored under different, often opaque schemata, it represents identical real-world objects multiple times, causing duplicates, and it has missing values and conflicting values. The Humboldt Merger (HumMer) is a tool that allows ad-hoc, declarative fusion of such data using a simple extension to SQL.Guided by a query against multiple tables, HumMer proceeds in three fully automated steps: First, instance-based schema matching bridges schematic heterogeneity of the tables by aligning corresponding attributes. Next, duplicate detection techniques find multiple representations of identical real-world objects. Finally, data fusion and conflict resolution merges duplicates into a single, consistent, and clean representation.

#index 824760
#* Querying business processes with BP-QL
#@ Catriel Beeri;Anat Eyal;Simon Kamenkovich;Tova Milo
#t 2005
#c 4
#% 29439
#% 655355
#% 754120
#% 765420
#% 994005
#! A business process consists of a group of business activities undertaken by one or more organizations in pursuit of some particular goal. It usually depends upon various business functions for support, e.g. personnel, accounting, inventory, and interacts with other business processes/activities carried by the same or other organizations. Consequently, the software implementing a business processes typically operates in a cross-organization, distributed environment.

#index 824761
#* StreamGlobe: processing and sharing data streams in grid-based P2P infrastructures
#@ Richard Kuntschke;Bernhard Stegmaier;Alfons Kemper;Angelika Reiser
#t 2005
#c 4
#% 300179
#% 397353
#% 427022
#% 760777
#% 799147
#% 1016148
#% 1016180
#! Data stream processing is currently gaining importance due to the developments in novel application areas like e-science, e-health, and e-business (considering RFID, for example). Focusing on e-science, it can be observed that scientific experiments and observations in many fields, e. g., in physics and astronomy, create huge volumes of data which have to be interchanged and processed. With experimental and observational data coming in particular from sensors, online simulations, etc., the data has an inherently streaming nature. Furthermore, continuing advances will result in even higher data volumes, rendering storing all of the delivered data prior to processing increasingly impractical. Hence, in such e-science scenarios, processing and sharing of data streams will play a decisive role. It will enable new possibilities for researchers, since they will be able to subscribe to interesting data streams of other scientists without having to set up their own devices or experiments. This results in much better utilization of expensive equipment such as telescopes, satellites, etc. Further, processing and sharing data streams on-the-fly in the network helps to reduce network traffic and to avoid network congestion. Thus, even huge streams of data can be handled efficiently by removing unnecessary parts early on, e. g., by early filtering and aggregation, and by sharing previously generated data streams and processing results.

#index 824762
#* MINERVA: collaborative P2P search
#@ Matthias Bender;Sebastian Michel;Peter Triantafillou;Gerhard Weikum;Christian Zimmer
#t 2005
#c 4
#% 169806
#% 278831
#% 287237
#% 340175
#% 340176
#% 451536
#% 505869
#% 630984
#% 730035
#% 793899
#% 818210
#% 963596
#% 1015281
#% 1016183
#% 1180871
#! This paper proposes the live demonstration of a prototype of MINERVA, a novel P2P Web search engine. The search engine is layered on top of a DHT-based overlay network that connects an a-priori unlimited number of peers, each of which maintains a personal local database and a local search facility. Each peer posts a small amount of metadata to a physically distributed directory that is used to efficiently select promising peers from across the peer population that can best locally execute a query. The proposed demonstration serves as a proof of concept for P2P Web search by deploying the project on standard notebook PCs and also invites everybody to join the network by instantly installing a small piece of software from a USB memory stick.

#index 824763
#* HePToX: marrying XML and heterogeneity in your P2P databases
#@ Angela Bonifati;Elaine Qing Chang;Aks V. S. Lakshmanan;Terence Ho;Rachel Pottinger
#t 2005
#c 4
#% 572314
#% 654468
#% 723449
#% 765446
#% 801676
#% 801692
#% 993981
#% 1015302
#! We present HePToX, a full-fledged peer-to-peer database system that efficiently handles XML data heterogeneity. In a highly dynamic P2P network, it is unrealistic for a peer entering the network to be forced to agree on a global mediated schema, or to perform heavy-weight operations for mapping its schema to neighboring schemas. In our demo, we show that to enter the HePToX network a peer user is only asked to draw a simple set of visual annotations to a few other schemas. We show how the mapping rules are then automatically generated and how efficient query translation is performed on top of these mappings.

#index 824764
#* U-DBMS: a database system for managing constantly-evolving data
#@ Reynold Cheng;Sarvjeet Singh;Sunil Prabhakar
#t 2005
#c 4
#% 295512
#% 654487
#! In many systems, sensors are used to acquire information from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e.g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, and it can be processed with probabilistic query operators to produce imprecise (but correct) answers. This demonstration presents a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data.

#index 824765
#* Database change notifications: primitives for efficient database query result caching
#@ Cesar Galindo-Legaria;Torsten Grabs;Christian Kleinerman;Florian Waas
#t 2005
#c 4
#% 397402
#% 745536
#% 993978
#! Many database applications implement caching of data from a back-end database server to avoid repeated round trips to the back-end and to improve response times for end-user requests. For example, consider a web application that caches dynamic web content in the mid-tier [3, 2]. The content of dynamic web pages is usually assembled from data stored in the underlying database system and subject to modification whenever the data sources are modified. The workload is ideal for caching query results: most queries are read-only (browsing sessions) and only a small portion of the queries are actually modifying data. Caching at the mid-tier helps off-load the back-end database servers and can increase scalability of a distributed system drastically.

#index 824766
#* CMS-ToPSS: efficient dissemination of RSS documents
#@ Milenko Petrovic;Haifeng Liu;Hans-Arno Jacobsen
#t 2005
#c 4
#% 271199
#% 333938
#% 338354
#% 342032
#% 480296
#% 519415
#% 640616
#% 793898
#% 805893
#! Recent years have seen a rise in the number of unconventional publishing tools on the Internet. Tools such as wikis, blogs, discussion forums, and web-based content management systems have experienced tremendous rise in popularity and use; primarily because they provide something traditional tools do not: easy of use for non computer-oriented users and they are based on the idea of "collaboration." It is estimated, by pewinternet.org, that 32 million people in the US read blogs (which represents 27% of the estimated 120 million US Internet users) while 8 million people have said that they have created blogs.

#index 824767
#* Interactive schema translation with instance-level mappings
#@ Philip A. Bernstein;Sergey Melnik;Peter Mork
#t 2005
#c 4
#% 458995
#% 800616
#% 809249
#% 810021
#! We demonstrate a prototype that translates schemas from a source metamodel (e.g., OO, relational, XML) to a target metamodel. The prototype is integrated with Microsoft Visual Studio 2005 to generate relational schemas from an object-oriented design. It has four novel features. First, it produces instance mappings to round-trip the data between the source schema and the generated target schema. It compiles the instance mappings into SQL views to reassemble the objects stored in relational tables. Second, it offers interactive editing, i.e., incremental modifications of the source schema yield incremental modifications of the target schema. Third, it incorporates a novel mechanism for mapping inheritance hierarchies to relations, which supports all known strategies and their combinations. Fourth, it is integrated with a commercial product featuring a high-quality user interface. The schema translation process is driven by high-level rules that eliminate constructs that are absent from the target metamodel.

#index 824768
#* AReNA: adaptive distributed catalog infrastructure based on relevance networks
#@ Vladimir Zadorozhny;Avigdor Gal;Louiqa Raschid;Qiang Ye
#t 2005
#c 4
#% 303761
#% 340177
#% 397356
#% 622625
#% 636013
#% 745490
#! Wide area applications (WAAs) utilize a WAN infrastructure (e.g., the Internet) to connect a federation of hundreds of servers with tens of thousands of clients. Earlier generations of WAA relied on Web accessible sources and the http protocol for data delivery. Recent developments such as the PlanetLab [8] testbed is now demonstrating an emerging class of data- and compute- intensive wide area applications.

#index 824769
#* Data sharing in the Hyperion peer database system
#@ Patricia Rodríguez-Gianolli;Anastasios Kementsietsidis;Maddalena Garzetti;Iluju Kiringa;Lei Jiang;Mehedi Masud;Renée J. Miller;John Mylopoulos
#t 2005
#c 4
#% 378409
#% 465057
#% 481923
#% 654468
#% 723448
#% 723449
#% 765432
#% 1016168
#% 1016250
#! This demo presents Hyperion, a prototype system that supports data sharing for a network of independent Peer Relational Database Management Systems (PDBMSs). The nodes of such a network are assumed to be autonomous PDBMSs that form acquaintances at run-time, and manage mapping tables to define value correspondences among different databases. They also use distributed Event-Condition-Action (ECA) rules to enable and coordinate data sharing. Peers perform local querying and update processing, and also propagate queries and updates to their acquainted peers. The demo illustrates the following key functionalities of Hyperion: (1) the use of (data level) mapping tables to infer new metadata as peers dynamically join the network, (2) the ability to answer queries using data in acquaintances, and (3) the ability to coordinate peers through update propagation.

#index 824770
#* Nile-PDT: a phenomenon detection and tracking framework for data stream management systems
#@ M. H. Ali;W. G. Aref;R. Bose;A. K. Elmagarmid;A. Helal;I. Kamel;M. F. Mokbel
#t 2005
#c 4
#% 745434
#% 803662
#% 853011
#% 1831061
#! In this demo, we present Nile-PDT, a Phenomenon Detection and Tracking framework using the Nile data stream management system. A phenomenon is characterized by a group of streams showing similar behavior over a period of time. The functionalities of Nile-PDT is split between the Nile server and the Nile-PDT application client. At the server side, Nile detects phenomenon candidate members and tracks their propagation incrementally through specific sensor network operators. Phenomenon candidate members are processed at the client side to detect phenomena of interest to a particular application. Nile-PDT is scalable in the number of sensors, the sensor data rates, and the number of phenomena. Guided by the detected phenomena, Nile-PDT tunes query processing towards sensors that heavily affect the monitoring of phenomenon propagation.

#index 824771
#* Robust real-time query processing with QStream
#@ Sven Schmidt;Thomas Legler;Sebastian Schär;Wolfgang Lehner
#t 2005
#c 4
#% 639297
#% 654497
#% 810539
#% 1015324
#% 1016272
#! Processing data streams with Quality-of-Service (QoS) guarantees is an emerging area in existing streaming applications. Although it is possible to negotiate the result quality and to reserve the required processing resources in advance, it remains a challenge to adapt the DSMS to data stream characteristics which are not known in advance or are difficult to obtain. Within this paper we present the second generation of our QStream DSMS which addresses the above challenge by using a real-time capable operating system environment for resource reservation and by applying an adaptation mechanism if the data stream characteristics change spontaneously.

#index 824772
#* Loadstar: load shedding in data stream mining
#@ Yun Chi;Haixun Wang;Philip S. Yu
#t 2005
#c 4
#% 726621
#% 729932
#% 745534
#% 1015280
#! In this demo, we show that intelligent load shedding is essential in achieving optimum results in mining data streams under various resource constraints. The Loadstar system introduces load shedding techniques to classifying multiple data streams of large volume and high speed. Loadstar uses a novel metric known as the quality of decision (QoD) to measure the level of uncertainty in classification. Resources are then allocated to sources where uncertainty is high. To make optimum classification decisions and accurate QoD measurement, Loadstar relies on feature prediction to model the data dropped by the load shedding mechanism. Furthermore, Loadstar is able to adapt to the changing data characteristics in data streams. The system thus offers a nice solution to data mining with resource constraints.

#index 824773
#* iMeMex: escapes from the personal information jungle
#@ Jens-Peter Dittrich;Marcos Antonio Vaz Salles;Donald Kossmann;Lukas Blunschi
#t 2005
#c 4
#% 1015345
#! Modern computer work stations provide thousands of applications that store data in 100.000 files on the file system of the underlying OS. To handle these files data processing logic is reinvented inside each application. This results in a jungle of data processing solutions and a jungle of data and file formats. For a user, it is extremely hard to manage information in this jungle. Most of all it is impossible to use data distributed among different files and formats for combined queries, e.g., join and union operations. To solve the problems arising from file based data management, we present a software system called iMeMex as a unified solution to personal information management and integration. iMeMex is designed to integrate seamlessly into existing operating systems like Windows, Linux and Mac OS X. Our system enables existing applications to gradually dispose file based storage. By using iMeMex modern operating systems are enabled to make use of sophisticated DBMS, IR and data integration technologies. The seamless integration of iMeMex into existing operating systems enables new applications that provide concepts of data storage and analysis unseen before.

#index 824774
#* Personalizing XML text search in PIMENT
#@ Sihem Amer-Yahia;Irini Fundulaki;Prateek Jain;Laks Lakshmanan
#t 2005
#c 4
#% 406493
#% 504581
#% 754116
#! A growing number of text-rich XML repositories are being made available. As a result, more efforts have been deployed to provide XML full-text search that combines querying structure with complex conditions on text ranging from simple keyword search to sophisticated proximity search composed with stemming and thesaurus. However, one of the key challenges in full-text search is to match users' expectations and determine the most relevant answers to a full-text query. In this context, we propose query personalization as a way to take user profiles into account in order to customize query answers based on individual users' needs.We present PIMENT, a system that enables query personalization by query rewriting and answer ranking. PIMENT is composed of a profile repository that stores user profiles, a query customizer that rewrites user queries based on user profiles and, a ranking module to rank query answers.

#index 824775
#* WISE-Integrator: a system for extracting and integrating complex web search interfaces of the deep web
#@ Hai He;Weiyi Meng;Clement Yu;Zonghuan Wu
#t 2005
#c 4
#% 307632
#% 480479
#% 654459
#% 745439
#% 765409
#% 765410
#% 765491
#% 777930
#% 1015284
#% 1016163
#! We demonstrate WISE-Integrator - an automatic search interface extraction and integration tool. The basic research issues behind this tool will also be explained.

#index 824776
#* WmXML: a system for watermarking XML data
#@ Xuan Zhou;HweeHwa Pang;Kian-Lee Tan;Dhruv Mangla
#t 2005
#c 4
#% 256418
#% 263455
#% 632213
#% 654449
#% 654516
#% 765432
#! As increasing amount of data is published in the form of XML, copyright protection of XML data is becoming an important requirement for many applications. While digital watermarking is a widely used measure to protect digital data from copyright offences, the complex and flexible construction of XML data poses a number of challenges to digital watermarking, such as re-organization and alteration attacks. To overcome these challenges, the watermarking scheme has to be based on the usability of data and the underlying semantics like key attributes and functional dependencies. In this paper, we describe WmXML, a system for watermarking XML documents. It generates queries from essential semantics to identify the available watermarking bandwidth in XML documents, and integrates query rewriting technique to overcome the threats from data re-organization and alteration. In the demonstration, we will showcase the use of WmXML and its effectiveness in countering various attacks.

#index 824777
#* Pathfinder: XQuery---the relational way
#@ Peter Boncz;Torsten Grust;Maurice van Keulen;Stefan Manegold;Jan Rittinger;Jens Teubner
#t 2005
#c 4
#% 397358
#% 994015
#% 1015298
#% 1016150
#% 1016257
#! Relational query processors are probably the best understood (as well as the best engineered) query engines available today. Although carefully tuned to process instances of the relational model (tables of tuples), these processors can also provide a foundation for the evaluation of "alien" (non-relational) query languages: if a relational encoding of the alien data model and its associated query language is given, the RDBMS may act like a special-purpose processor for the new language.

#index 824778
#* MIX: a meta-data indexing system for XML
#@ SungRan Cho;Nick Koudas;Divesh Srivastava
#t 2005
#c 4
#% 252304
#% 397358
#% 570875
#% 772019
#% 1016204
#! We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.

#index 824779
#* ULoad: choosing the right storage for your XML application
#@ Andrei Arion;Véronique Benzaken;Ioana Manolescu;Ravi Vijay
#t 2005
#c 4
#% 6259
#% 411554
#% 479465
#% 479956
#% 480158
#% 480822
#% 570875
#% 570876
#% 765407
#% 1015271
#! A key factor for the outstanding success of database management systems is physical data independence: queries, and application programs, are able to refer to the data at the logical level, ignoring the details on how the data is physically stored and accessed by the system. The corner stone of implementing physical data independence is an access path selection algorithm: whenever a disk-resident data item can be accessed in several ways, the access path selection algorithm, which is part of the query optimizer, will identify the possible alternatives, and choose the one likely to provide the best performance for a given query [13].

#index 824780
#* A faceted query engine applied to archaeology
#@ Kenneth A. Ross;Angel Janevski;Julia Stoyanovich
#t 2005
#c 4
#% 1711122
#! In this demonstration, we describe a system for storing and querying faceted hierarchies. We have developed a general faceted domain model and a query language for hierarchically classified data. We present here the use of our system on two real archaeological datasets containing thousands of artifacts. Our system is a sharable, evolvable resource that can provide global access to sizeable datasets in queriable format, and can serve as a valuable tool for data analysis and research in many application domains.

#index 824781
#* A dynamically adaptive distributed system for processing complex continuous queries
#@ Bin Liu;Yali Zhu;Mariana Jbantova;Bradley Momberger;Elke A. Rundensteiner
#t 2005
#c 4
#% 86929
#% 217052
#% 378388
#% 674194
#% 765437
#% 824720
#% 1016269
#! Recent years have witnessed rapidly growing research attention on continuous query processing over streams [2, 3]. A continuous query system can easily run out of resources in case of large amount of input stream data. Distributed continuous query processing over a shared nothing architecture, i.e., a cluster of machines, has been recognized as a scalable method to solve this problem [2, 8, 9]. Due to the lack of initial cost information and the fluctuating nature of the streaming data, uneven workload among machines may occur and this may impair the benefits of distributed processing. Thus dynamic adaptation techniques are crucial for a distributed continuous query system.

#index 824782
#* RankSQL: supporting ranking queries in relational database management systems
#@ Chengkai Li;Mohamed A. Soliman;Kevin Chen-Chuan Chang;Ihab F. Ilyas
#t 2005
#c 4
#% 397378
#% 777931
#% 810018
#! Ranking queries (or top-k queries) are dominant in many emerging applications, e.g., similarity queries in multimedia databases, searching Web databases, middleware, and data mining. The increasing importance of top-k queries warrants an efficient support of ranking in the relational database management system (RDBMS) and has recently gained the attention of the research community. Top-k queries aim at providing only the top k query results, according to a user-specified ranking function, which in many cases is an aggregate of multiple criteria. The following is an example top-k query.

#index 824783
#* PSYCHO: a prototype system for pattern management
#@ Barbara Catania;Anna Maddalena;Maurizio Mazza
#t 2005
#c 4
#% 480318
#% 799745
#% 928618
#! Patterns represent in a compact and rich in semantics way huge quantity of heterogeneous data. Due to their characteristics, specific systems are required for pattern management, in order to model and manipulate patterns, with a possibly user-defined structure, in an efficient and effective way. In this demonstration we present PSYCHO, a pattern based management system prototype. PSYCHO allows the user to: (i) use standard pattern types or define new ones; (ii) generate or import patterns, represented according to existing standards; (iii) manipulate possibly heterogeneous patterns under an integrated environment.

#index 824784
#* Answering imprecise queries over web databases
#@ Ullas Nambiar;Subbarao Kambhampati
#t 2005
#c 4
#% 41230
#% 280419
#% 479659
#% 479803
#% 631985
#% 716425
#% 772030
#! The rapid expansion of the World Wide Web has made a large number of databases like bibliographies, scientific databases etc. to become accessible to lay users demanding "instant gratification". Often, these users may not know how to precisely express their needs and may formulate queries that lead to unsatisfactory results.

#index 824785
#* ConQuer: a system for efficient querying over inconsistent databases
#@ Ariel Fuxman;Diego Fuxman;Renée J. Miller
#t 2005
#c 4
#% 273687
#% 644182
#% 810020
#! Although integrity constraints have long been used to maintain data consistency, there are situations in which they may not be enforced or satisfied. In this demo, we showcase ConQuer, a system for efficient and scalable answering of SQL queries on databases that may violate a set of constraints. ConQuer permits users to postulate a set of key constraints together with their queries. The system rewrites the queries to retrieve all (and only) data that is consistent with respect to the constraints. The rewriting is into SQL, so the rewritten queries can be efficiently optimized and executed by commercial database systems.

#index 824786
#* QoS-based data access and placement for federated systems
#@ Wen-Syan Li;Vishal S. Batra;Vijayshankar Raman;Wei Han;Inderpal Narang
#t 2005
#c 4
#% 397393
#% 800595
#! A wide variety of applications require access to multiple heterogeneous, distributed data sources. By transparently integrating such diverse data sources, underlying differences in DBMSs, languages, and data models can be hidden and users can use a single data model and a single high-level query language to access the unified data through a global schema.

#index 824787
#* Approximate joins: concepts and techniques
#@ Nick Koudas;Divesh Srivastava
#t 2005
#c 4
#% 654523
#! The quality of the data residing in information repositories and databases gets degraded due to a multitude of reasons. Such reasons include typing mistakes during insertion (e.g., character transpositions), lack of standards for recording database fields (e.g., addresses), and various errors introduced by poor database design (e.g., missing integrity constraints). Data of poor quality can result in significant impediments to popular business practices: sending products or bills to incorrect addresses, inability to locate customer records during service calls, inability to correlate customers across multiple services, etc.

#index 824788
#* Offline and data stream algorithms for efficient computation of synopsis structures
#@ Sudipto Guha;Kyuseok Shim
#t 2005
#c 4
#! Synopsis and small space representations are important data analysis tools and have long been used OLAP/DSS systems, approximate query answering, query optimization and data mining. These techniques represent the input in terms broader characteristics and improve efficiency of various applications, e.g., learning, classification, event detection, among many others. In a recent past, the synopsis techniques have gained more currency due to the emerging areas like data stream management.In this tutorial, we propose to revisit algorithms for Wavelet and Histogram synopsis construction. In the recent years, a significant number of papers have appeared which has advanced the state-of-the-art in synopsis construction considerably. In particular, we have seen the development of a large number of efficient algorithms which are also guaranteed to be near optimal. Furthermore, these synopsis construction problems have found deep roots in theory and database systems, and have influenced a wide range of problems. In a different level, a large number of the synopsis construction algorithms use a similar set of techniques. It is extremely valuable to discuss and analyze these techniques, and we expect broader pictures and paradigms to emerge. This would allow us to develop algorithms for newer problems with greater ease. Understanding these recurrent themes and intuition behind the development of these algorithms is one of the main thrusts of the tutorial.Our goal will be to cover a wide spectrum of these topics and make the researchers in VLDB community aware of the new algorithms, optimum or approximate, offline or streaming. The tutorial will be self contained and develop most of the mathematical and database backgrounds needed.

#index 824789
#* Personalized systems: models and methods from an IR and DB perspective
#@ Yannis Ioannidis;Georgia Koutrika
#t 2005
#c 4
#! In today's knowledge-driven society, information abundance and personal electronic device ubiquity have made it difficult for users to find the right information at the right time and at the right level of detail. To solve this problem, researchers have developed systems that adapt their behavior to the goals, tasks, interests, and other characteristics of their users. Based on models that capture important user characteristics, these personalized systems maintain their users' profiles and take them into account to customize the content generated or its presentation to the different individuals.

#index 824790
#* Contextual insight in search: enabling technologies and applications
#@ Aleksander Øhrn
#t 2005
#c 4
#! When the fields of efficient and scalable search, data aggregation, XML search and information extraction come together, we get a powerful and exciting mix. Taken together, these are enabling technologies that make it possible to search for information in new ways and that make new types of search applications possible. Using a state-of-the-art enterprise search platform as an example, this tutorial outlines the workings of these technologies and presents some applications of their intersection.Search engines differ from traditional databases in several ways, yet they both address the issue of organizing information and making it retrievable. The anatomy of a modern search engine will be presented. Although search engines are typically associated with web search, some are now equipped with features usually associated with databases and structured data, e.g., the ability to do aggregation of meta data across a full result set.Most search engines are built around a flat document model: A document is seen as a collection of typed fields, but the fields themselves have no particular structure. As such, queries tend to focus on document content, with little or no constraints on document structure. Recently, however, scalable and efficient search engines have appeared that support indexing and retrieval of complex XML. Queries that are posed to a search engine can thus combine content and structure in ways that enable extreme search precision. Furthermore, data aggregation can be restricted to take place on the level of the matching document fragments instead of on the document level, thus providing more contextually relevant statistics.Given XML capabilities, applying text mining and information extraction techniques to the content becomes particularly interesting: By automatically detecting semantic entities, and possibly also relations between these, this information can be searched for in different ways and, e.g., enable applications that have a strong element of discovery to them.

#index 824791
#* Semantic overlay networks
#@ Karl Aberer;Philippe Cudré-Mauroux
#t 2005
#c 4
#! In a handful of years only, Peer-to-Peer (P2P) systems have become an integral part of the Internet. After a few key successes related to music-sharing (e.g., Napster or Gnutella), they rapidly developed and are nowadays firmly established in various contexts, ranging from large-scale content distribution (Bit Torrent) to Internet telephony(Skype) or networking platforms (JXTA). The main idea behind P2P is to leverage on the power of end-computers: Instead of relying on central components (e.g., servers), services are powered by decentralized overlay architectures where end-computers connect to each other dynamically.

#index 824792
#* XML full-text search: challenges and opportunities
#@ Sihem Amer-Yahia;Jayavel Shanmugasundaram
#t 2005
#c 4
#% 86371
#% 194247
#% 333981
#% 458829
#% 642993
#% 654442
#% 754116
#% 772029
#% 1015258
#! An ever growing number of XML repositories are being made available for search. A lot of activity has been deployed in the past few years to query such repositories. In particular, full-text querying of text-rich XML documents has generated a wealth of issues that are being addressed by both the database (DB) and information retrieval (IR) communities. The DB community has traditionally focused on developing query languages and efficient evaluation algorithms for highly structured data. In contrast, the IR community has focused on searching unstructured data, and has developed various techniques for ranking query results and evaluating their effectiveness. Fortunately, recent trends in DB and IR research demonstrate a growing interest in adopting IR techniques in DBs and vice versa [1, 2, 3, 4, 5, 6, 7, 9].

#index 893087
#* Enterprise information mashups: integrating information, simply
#@ Anant Jhingran
#t 2006
#c 4
#! There is a fundamental transformation that is taking place on the web around information composition through mashups. We first describe this transformation and then assert that this will also affect enterprise architectures. Currently the state-of-the-art in enterprises around information composition is federation and other integration technologies. These scale well, and are well worth the upfront investment for enterprise class, long-lived applications. However, there are many information composition tasks that are not currently well served by these architectures. The needs of Situational Applications (i.e. applications that come together for solving some immediate business problems) are one such set of tasks. Augmenting structured data with unstructured information is another such task. Our hypothesis is that a new class of integration technologies will emerge to serve these tasks, and we call it an enterprise information mashup fabric. In the talk, we discuss the information management primitives that are needed for this fabric, the various options that exist for implementation, and pose several, currently unanswered, research questions.

#index 893088
#* Next generation data management in enterprise application platforms
#@ Vishal Sikka
#t 2006
#c 4
#! As a leading provider of applications and application infrastructure software, SAP has been always interested in the entire spectrum of enterprise data management from transactional to analytical, structured and unstructured, as well as high-volatility event data streams. The underlying architecture for enterprise applications has fundamentally changed in the last decade, with the adoption of service-oriented architectures representing the latest shift. However, DBMS architecture has not evolved sufficiently to meet the challenges that these new application characteristics pose. As a result, at SAP we have been rethinking the way enterprise applications manage their data. In this talk, we will present some key aspects of this rethinking. We will start with a description of the shift in application architecture and the challenges that this shift poses on data management. We will then describe the failings of a single overarching DBMS architecture against these needs, and then describe some examples of usage-specific data management in enterprise application platforms. In particular we will focus on our approach to managing analytical, transactional and master data. We will present some results that describe how, with a combination of better utilization of main-memory based data management techniques, addressing the needs of the next generation application infrastructure and advances in the underlying computing and storage infrastructure, we can do significantly more efficient data management.

#index 893089
#* Data integration: the teenage years
#@ Alon Halevy;Anand Rajaraman;Joann Ordille
#t 2006
#c 4
#% 58347
#% 159113
#% 159337
#% 172900
#% 198466
#% 201885
#% 213437
#% 213982
#% 224758
#% 237189
#% 237190
#% 248038
#% 248793
#% 248795
#% 263136
#% 273911
#% 273912
#% 273924
#% 274160
#% 278445
#% 281149
#% 283052
#% 296931
#% 300167
#% 303886
#% 328429
#% 333245
#% 333988
#% 333990
#% 340305
#% 348187
#% 378409
#% 430422
#% 451595
#% 464056
#% 479452
#% 479783
#% 480134
#% 480496
#% 480645
#% 480822
#% 481786
#% 481923
#% 482116
#% 496091
#% 511906
#% 564419
#% 570880
#% 571169
#% 572307
#% 572311
#% 572314
#% 577238
#% 577247
#% 591586
#% 636009
#% 654457
#% 654458
#% 654459
#% 654465
#% 654468
#% 714310
#% 751818
#% 762652
#% 765432
#% 765434
#% 766199
#% 800497
#% 801676
#% 809239
#% 809249
#% 810014
#% 810073
#% 824718
#% 845350
#% 870896
#% 874876
#% 874971
#% 893167
#% 993982
#% 1015302
#% 1015326
#% 1016160
#% 1279212
#% 1499470

#index 893090
#* Safety guarantee of continuous join queries over punctuated data streams
#@ Hua-Gang Li;Songting Chen;Junichi Tatemura;Divyakant Agrawal;K. Selçuk Candan;Wang-Pin Hsiung
#t 2006
#c 4
#% 300167
#% 310488
#% 340635
#% 378388
#% 397414
#% 411554
#% 578391
#% 660004
#% 801694
#% 993949
#% 1015278
#% 1015296
#! Continuous join queries (CJQ) are needed for correlating data from multiple streams. One fundamental problem for processing such queries is that since the data streams are infinite, this would require the join operator to store infinite states and eventually run out of space. Punctuation semantics has been proposed to specifically address this problem. In particular, punctuations explicitly mark the end of a subset of data and, hence, enable purging of the stored data which will not contribute to any new query results. Given a set of available punctuation schemes, if one can identify that a CJQ still requires unbounded storage, then this query can be flagged as unsafe and can be prevented from running. Unfortunately, while punctuation semantics is clearly useful, the mechanisms to identify if and how a particular CJQ could benefit from a given set of punctuation schemes are not yet known. In this paper, we provide sufficient and necessary conditions for checking whether a CJQ can be safely executed under a given set of punctuation schemes or not. In particular, we introduce a novel punctuation graph to aid the analysis of the safety for a given query. We show that the safety checking problem can be done in polynomial time based on this punctuation graph construct. In addition, various issues and challenges related to the safety checking of CJQs are highlighted.

#index 893091
#* Scalable continuous query processing by tracking hotspots
#@ Pankaj K. Agarwal;Junyi Xie;Jun Yang;Hai Yu
#t 2006
#c 4
#% 235114
#% 299982
#% 300167
#% 300179
#% 304590
#% 394417
#% 397353
#% 443298
#% 479648
#% 480649
#% 480774
#% 631962
#% 723366
#% 726622
#% 765282
#% 810025
#% 993949
#% 1698242
#! This paper considers the problem of scalably processing a large number of continuous queries. We propose a flexible framework with novel data structures and algorithms for group-processing and indexing continuous queries by exploiting potential overlaps in query predicates. Our approach partitions the collection of continuous queries into groups based on the clustering patterns of the query ranges, and then applies specialized processing strategies to those heavily-clustered groups (or hotspots). To maintain the partition dynamically, we present efficient algorithms that maintain a nearly optimal partition in nearly amortized logarithmic time. We show how to use the hotspots to scalably process large numbers of continuous select-join and band-join queries, which are much more challenging than simple range selection queries. Experiments demonstrate that this approach can improve the processing throughput by orders of magnitude. As another application of hotspots, we show how to use them to build a high-quality histogram for intervals in linear time.

#index 893092
#* Continuous nearest neighbor monitoring in road networks
#@ Kyriakos Mouratidis;Man Lung Yiu;Dimitris Papadias;Nikos Mamoulis
#t 2006
#c 4
#% 201876
#% 287466
#% 413797
#% 421124
#% 427199
#% 442615
#% 526840
#% 527187
#% 527191
#% 579313
#% 654478
#% 729850
#% 729851
#% 765453
#% 800571
#% 800572
#% 810048
#% 810061
#% 824723
#% 832568
#% 879211
#! Recent research has focused on continuous monitoring of nearest neighbors (NN) in highly dynamic scenarios, where the queries and the data objects move frequently and arbitrarily. All existing methods, however, assume the Euclidean distance metric. In this paper we study k-NN monitoring in road networks, where the distance between a query and a data object is determined by the length of the shortest path connecting them. We propose two methods that can handle arbitrary object and query moving patterns, as well as fluctuations of edge weights. The first one maintains the query results by processing only updates that may invalidate the current NN sets. The second method follows the shared execution paradigm to reduce the processing time. In particular, it groups together the queries that fall in the path between two consecutive intersections in the network, and produces their results by monitoring the NN sets of these intersections. We experimentally verify the applicability of the proposed techniques to continuous monitoring of large data and query sets.

#index 893093
#* Implementing mapping composition
#@ Philip A. Bernstein;Todd J. Green;Sergey Melnik;Alan Nash
#t 2006
#c 4
#% 328429
#% 341233
#% 458607
#% 765446
#% 801676
#% 809249
#% 810021
#% 824736
#% 874971
#% 1015302
#% 1015326
#! Mapping composition is a fundamental operation in metadata driven applications. Given a mapping over schemas σ1 and σ2 and a mapping over schemas σ2 and σ3, the composition problem is to compute an equivalent mapping over σ1 and σ3. We describe a new composition algorithm that targets practical applications. It incorporates view unfolding. It eliminates as many σ2 symbols as possible, even if not all can be eliminated. It covers constraints expressed using arbitrary monotone relational operators and, to a lesser extent, non-monotone operators. And it introduces the new technique of left composition. We describe our implementation, explain how to extend it to support user-defined operators, and present experimental results which validate its effectiveness.

#index 893094
#* Nested mappings: schema mapping reloaded
#@ Ariel Fuxman;Mauricio A. Hernandez;Howard Ho;Renee J. Miller;Paolo Papotti;Lucian Popa
#t 2006
#c 4
#% 281149
#% 283052
#% 287339
#% 300166
#% 378409
#% 479783
#% 480134
#% 480429
#% 481935
#% 654464
#% 762652
#% 765432
#% 777935
#% 801676
#% 809235
#% 809249
#% 810078
#% 824658
#% 824736
#% 824763
#% 824767
#% 826032
#% 874881
#% 993981
#% 1015302
#! Many problems in information integration rely on specifications, called schema mappings, that model the relationships between schemas. Schema mappings for both relational and nested data are well-known. In this work, we present a new formalism for schema mapping that extends these existing formalisms in two significant ways. First, our nested mappings allow for nesting and correlation of mappings. This results in a natural programming paradigm that often yields more accurate specifications. In particular, we show that nested mappings can naturally preserve correlations among data that existing mapping formalisms cannot. We also show that using nested mappings for purposes of exchanging data from a source to a target will result in less redundancy in the target data. The second extension to the mapping formalism is the ability to express, in a declarative way, grouping and data merging semantics. This semantics can be easily changed and customized to the integration task at hand. We present a new algorithm for the automatic generation of nested mappings from schema matchings (that is, simple element-to-element correspondences between schemas). We have implemented this algorithm, along with algorithms for the generation of transformation queries (e.g., XQuery) based on the nested mapping specification. We show that the generation algorithms scale well to large, highly nested schemas. We also show that using nested mappings in data exchange can drastically reduce the execution cost of producing a target instance, particularly over large data sources, and can also dramatically improve the quality of the generated data.

#index 893095
#* Debugging schema mappings with routes
#@ Laura Chiticariu;Wang-Chiew Tan
#t 2006
#c 4
#% 1834
#% 318704
#% 333988
#% 378409
#% 384978
#% 464589
#% 765432
#% 800499
#% 809239
#% 809248
#% 810021
#% 810078
#% 826032
#% 864469
#% 893196
#% 993981
#% 1016204
#! A schema mapping is a high-level declarative specification of the relationship between two schemas; it specifies how data structured under one schema, called the source schema, is to be converted into data structured under a possibly different schema, called the target schema. Schema mappings are fundamental components for both data exchange and data integration. To date, a language for specifying (or programming) schema mappings exists. However, developmental support for programming schema mappings is still lacking. In particular, a tool for debugging schema mappings has not yet been developed. In this paper, we propose to build a debugger for understanding and exploring schema mappings. We present a primary feature of our debugger, called routes, that describes the relationship between source and target data with the schema mapping. We present two algorithms for computing all routes or one route for selected target data. Both algorithms execute in polynomial time in the size of the input. In computing all routes, our algorithm produces a concise representation that factors common steps in the routes. Furthermore, every minimal route for the selected data can, essentially, be found in this representation. Our second algorithm is able to produce one route fast, if there is one, and alternative routes as needed. We demonstrate the feasibility of our route algorithms through a set of experimental results on both synthetic and real datasets.

#index 893096
#* A linear time algorithm for optimal tree sibling partitioning and approximation algorithms in Natix
#@ Carl-Christian Kanne;Guido Moerkotte
#t 2006
#c 4
#% 116057
#% 287716
#% 570876
#% 632058
#% 810036
#% 881735
#% 994015
#% 1721253
#! Document insertion into a native XML Data Store (XDS) requires to partition the document tree into a number of storage units with limited capacity, such as records on disk pages. As intra partition navigation is much faster than navigation between partitions, minimizing the number of partitions has a beneficial effect on query performance.We present a linear time algorithm to optimally partition an ordered, labeled, weighted tree such that each partition does not exceed a fixed weight limit. Whereas traditionally tree partitioning algorithms only allow child nodes to share a partition with their parent node (i.e. a partition corresponds to a subtree), our algorithm also considers partitions containing several subtrees as long as their roots are adjacent siblings. We call this sibling partitioning.Based on our study of the optimal algorithm, we further introduce two novel, near-optimal heuristics. They are easier to implement, do not need to hold the whole document instance in memory, and require much less runtime than the optimal algorithm.Finally, we provide an experimental study comparing our novel and existing algorithms. One important finding is that compared to partitioning that exclusively considers parent-child partitions, including sibling partitioning as well can decrease the total number of partitions by more than 90%, and improve query performance by more than a factor of two.

#index 893097
#* Efficient discovery of XML data redundancies
#@ Cong Yu;H. V. Jagadish
#t 2006
#c 4
#% 10245
#% 205246
#% 287754
#% 322880
#% 330627
#% 398752
#% 400530
#% 413601
#% 458850
#% 479939
#% 630971
#% 733268
#% 742566
#% 771227
#% 1721245
#! As XML becomes widely used, dealing with redundancies in XML data has become an increasingly important issue. Redundantly stored information can lead not just to a higher data storage cost, but also to increased costs for data transfer and data manipulation. Furthermore, such data redundancies can lead to potential update anomalies, rendering the database inconsistent.One way to avoid data redundancies is to employ good schema design based on known functional dependencies. In fact, several recent studies have focused on defining the notion of XML Functional Dependencies (XML FDs) to capture XML data redundancies. We observe further that XML databases are often "casually designed" and XML FDs may not be determined in advance. Under such circumstances, discovering XML data redundancies (in terms of FDs) from the data itself becomes necessary and is an integral part of the schema refinement process.In this paper, we present the design and implementation of the first system, DiscoverXFD, for effcient discovery of XML data redundancies. It employs a novel XML data structure and introduces a new class of partition based algorithms. DiscoverXFD can not only be used for the previous definitions of XML functional dependencies, but also for a more comprehensive notion we develop in this paper, capable of detecting redundancies involving set elements while maintaining clear semantics. Experimental evaluations using real life and benchmark datasets demonstrate that our system is practical and scales well with increasing data size.

#index 893098
#* Inference of concise DTDs from XML data
#@ Geert Jan Bex;Frank Neven;Thomas Schwentick;Karl Tuyls
#t 2006
#c 4
#% 71516
#% 151246
#% 238555
#% 248809
#% 252366
#% 257873
#% 273922
#% 318041
#% 342676
#% 404772
#% 431034
#% 450698
#% 462062
#% 462235
#% 464724
#% 479465
#% 480822
#% 504573
#% 542161
#% 564264
#% 572314
#% 577353
#% 765540
#% 772031
#% 809236
#% 835398
#% 842028
#% 845589
#% 845590
#% 894435
#% 1016148
#% 1673698
#% 1722205
#% 1739920
#% 1740164
#! We consider the problem to infer a concise Document Type Definition (DTD) for a given set of XML-documents, a problem which basically reduces to learning of concise regular expressions from positive example strings. We identify two such classes: single occurrence regular expressions (SOREs) and chain regular expressions (CHAREs). Both classes capture the far majority of the regular expressions occurring in practical DTDs and are succinct by definition. We present the algorithm iDTD (infer DTD) that learns SOREs from strings by first inferring an automaton by known techniques and then translating that automaton to a corresponding SORE, possibly by repairing the automaton when no equivalent SORE can be found. In the process, we introduce a novel automaton to regular expression rewrite technique which is of independent interest. We show that iDTD outperforms existing systems in accuracy, conciseness and speed. In a scenario where only a very small amount of XML data is available, for instance when generated by Web service requests or by answers to queries, iDTD produces regular expressions which are too specific. Therefore, we introduce a novel learning algorithm CRX that directly infers CHAREs (which form a subclass of SOREs) without going through an automaton representation. We show that CRX performs very well within its target class on very small data sets. Finally, we discuss incremental computation, noise, numerical predicates, and the generation of XML Schemas.

#index 893100
#* Anatomy: simple and effective privacy preservation
#@ Xiaokui Xiao;Yufei Tao
#t 2006
#c 4
#% 248030
#% 397385
#% 443463
#% 576761
#% 577239
#% 785363
#% 800514
#% 800515
#% 801690
#% 810011
#% 824726
#% 824727
#% 864406
#% 864412
#% 874988
#% 874989
#% 1700134
#! This paper presents a novel technique, anatomy, for publishing sensitive data. Anatomy releases all the quasi-identifier and sensitive values directly in two separate tables. Combined with a grouping mechanism, this approach protects privacy, and captures a large amount of correlation in the microdata. We develop a linear-time algorithm for computing anatomized tables that obey the l-diversity privacy requirement, and minimize the error of reconstructing the microdata. Extensive experiments confirm that our technique allows significantly more effective data analysis than the conventional publication method based on generalization. Specifically, anatomy permits aggregate reasoning with average error below 10%, which is lower than the error obtained from a generalized table by orders of magnitude.

#index 893101
#* Towards robustness in query auditing
#@ Shubha U. Nabar;Bhaskara Marthi;Krishnaram Kenthapadi;Nina Mishra;Rajeev Motwani
#t 2006
#c 4
#% 3421
#% 67453
#% 277396
#% 287297
#% 287711
#% 289165
#% 300184
#% 576110
#% 576111
#% 605785
#% 630970
#% 723772
#% 743280
#% 809244
#% 809245
#% 810028
#% 874891
#% 1016172
#% 1707132
#% 1732708
#% 1740518
#! We consider the online query auditing problem for statistical databases. Given a stream of aggregate queries posed over sensitive data, when should queries be denied in order to protect the privacy of individuals? We construct efficient auditors for max queries and bags of max and min queries in both the partial and full disclosure settings. Our algorithm for the partial disclosure setting involves a novel application of probabilistic inference techniques that may be of independent interest. We also study for the first time, a particular dimension of the utility of an auditing scheme and obtain initial results for the utility of sum auditing when guarding against full disclosure.The result is positive for large databases, indicating that answers to queries will not be riddled with denials.

#index 893102
#* Adaptive cleaning for RFID data streams
#@ Shawn R. Jeffery;Minos Garofalakis;Michael J. Franklin
#t 2006
#c 4
#% 61309
#% 190611
#% 378388
#% 654508
#% 654510
#% 787130
#% 824747
#% 864527
#% 878299
#% 1016178
#% 1016227
#% 1668029
#! To compensate for the inherent unreliability of RFID data streams, most RFID middleware systems employ a "smoothing filter", a sliding-window aggregate that interpolates for lost readings. In this paper, we propose SMURF, the first declarative, adaptive smoothing filter for RFID data cleaning. SMURF models the unreliability of RFID readings by viewing RFID streams as a statistical sample of tags in the physical world, and exploits techniques grounded in sampling theory to drive its cleaning processes. Through the use of tools such as binomial sampling and π-estimators, SMURF continuously adapts the smoothing window size in a principled manner to provide accurate RFID data to applications.

#index 893103
#* A deferred cleansing method for RFID data analytics
#@ Jun Rao;Sangeeta Doraiswamy;Hetal Thakkar;Latha S. Colby
#t 2006
#c 4
#% 201898
#% 249985
#% 273687
#% 333850
#% 576116
#% 654467
#% 810020
#% 810107
#% 824747
#% 824748
#% 864470
#% 1016178
#% 1016227
#% 1016228
#% 1668029
#! Radio Frequency Identification is gaining broader adoption in many areas. One of the challenges in implementing an RFID-based system is dealing with anomalies in RFID reads. A small number of anomalies can translate into large errors in analytical results. Conventional "eager" approaches cleanse all data upfront and then apply queries on cleaned data. However, this approach is not feasible when several applications define anomalies and corrections on the same data set differently and not all anomalies can be defined beforehand. This necessitates anomaly handling at query time. We introduce a deferred approach for detecting and correcting RFID data anomalies. Each application specifies the detection and the correction of relevant anomalies using declarative sequence-based rules. An application query is then automatically rewritten based on the cleansing rules that the application has specified, to provide answers over cleaned data. We show that a naive approach to deferred cleansing that applies rules without leveraging query information can be prohibitive. We develop two novel rewrite methods, both of which reduce the amount of data to be cleaned, by exploiting predicates in application queries while guaranteeing correct answers. We leverage standardized SQL/OLAP functionality to implement rules specified in a declarative sequence-based language. This allows efficient evaluation of cleansing rules using existing query processing capabilities of a DBMS. Our experimental results show that deferred cleansing is affordable for typical analytic queries over RFID data.

#index 893104
#* Online outlier detection in sensor data using non-parametric models
#@ S. Subramaniam;T. Palpanas;D. Papadopoulos;V. Kalogeraki;D. Gunopulos
#t 2006
#c 4
#% 115608
#% 273906
#% 300136
#% 300183
#% 300193
#% 309385
#% 379444
#% 397385
#% 401165
#% 438354
#% 479648
#% 479791
#% 480306
#% 480628
#% 576113
#% 635986
#% 654488
#% 660003
#% 749406
#% 751027
#% 765134
#% 765402
#% 783740
#% 785418
#% 801695
#% 805466
#% 824652
#% 825637
#% 847115
#% 864393
#% 1016178
#% 1394365
#! Sensor networks have recently found many popular applications in a number of different settings. Sensors at different locations can generate streaming data, which can be analyzed in real-time to identify events of interest. In this paper, we propose a framework that computes in a distributed fashion an approximation of multi-dimensional data distributions in order to enable complex applications in resource-constrained sensor networks.We motivate our technique in the context of the problem of outlier detection. We demonstrate how our framework can be extended in order to identify either distance- or density-based outliers in a single pass over the data, and with limited memory requirements. Experiments with synthetic and real data show that our method is efficient and accurate, and compares favorably to other proposed techniques. We also demonstrate the applicability of our technique to other related problems in sensor networks.

#index 893105
#* Relaxing join and selection queries
#@ Nick Koudas;Chen Li;Anthony K. H. Tung;Rares Vernica
#t 2006
#c 4
#% 2115
#% 86950
#% 287466
#% 288976
#% 300175
#% 333854
#% 397378
#% 427199
#% 445170
#% 465167
#% 479816
#% 480093
#% 480671
#% 654480
#% 731407
#% 769900
#% 806212
#% 810018
#% 824670
#% 824671
#% 824672
#% 993954
#% 993957
#% 1269490
#! Database users can be frustrated by having an empty answer to a query. In this paper, we propose a framework to systematically relax queries involving joins and selections. When considering relaxing a query condition, intuitively one seeks the 'minimal' amount of relaxation that yields an answer. We first characterize the types of answers that we return to relaxed queries. We then propose a lattice based framework in order to aid query relaxation. Nodes in the lattice correspond to different ways to relax queries. We characterize the properties of relaxation at each node and present algorithms to compute the corresponding answer. We then discuss how to traverse this lattice in a way that a non-empty query answer is obtained with the minimum amount of query condition relaxation. We implemented this framework and we present our results of a thorough performance evaluation using real and synthetic data. Our results indicate the practical utility of our framework.

#index 893106
#* Using partial evaluation in distributed query evaluation
#@ Peter Buneman;Gao Cong;Wenfei Fan;Anastasios Kementsietsidis
#t 2006
#c 4
#% 115661
#% 201928
#% 219211
#% 273897
#% 328424
#% 330305
#% 345693
#% 378412
#% 397374
#% 462779
#% 480296
#% 549327
#% 654485
#% 740758
#% 765126
#% 772022
#% 810045
#% 814648
#% 824661
#% 1015275
#% 1016166
#! A basic idea in parallel query processing is that one is prepared to do more computation than strictly necessary at individual sites in order to reduce the elapsed time, the network traffic, or both in the evaluation of the query. We develop this idea for the evaluation of boolean XPath queries over a tree that is fragmented, both horizontally and vertically over a number of sites. The key idea is to send the whole query to each site which partially evaluates, in parallel, the query and sends the results as compact boolean functions to a coordinator which combines these to obtain the result. This approach has several advantages. First, each site is visited only once, even if several fragments of the tree are stored at that site. Second, no prior constraints on how the tree is decomposed are needed, nor is any structural information about the tree required, such as a DTD. Third, there is a satisfactory bound on the total computation performed on all sites and on the total network traffic. We also develop a simple incremental maintenance algorithm that requires communication only with the sites at which changes have taken place; moreover the network traffic depends neither on the data nor on the update. These results, we believe, illustrate the usefulness and potential of partial evaluation in distributed systems as well as centralized xml stores for evaluating XPath queries and beyond.

#index 893107
#* TRAC: toward recency and consistency reporting in a database with distributed data sources
#@ Jiansheng Huang;Jeffrey F. Naughton;Miron Livny
#t 2006
#c 4
#% 286967
#% 296437
#% 458601
#% 462645
#% 479920
#% 480483
#% 765469
#% 993977
#% 1015287
#! Distributed computing environments, including workflows in computational grids, present challenges for monitoring, as the state of the system may be captured only in logs distributed throughout the system. One approach to monitoring such systems is to "sniff" these distributed logs and to store their transformed content in a DBMS. This centralizes the state and exposes it for querying; unfortunately, it also creates uncertainty with respect to the recency and consistency of the data. Previous related work has focused on allowing queries to express currency and consistency constraints, which are then enforced by "pulling" data from the distributed sources on demand, or by requiring synchronous updates of a centralized data store. In some instances this is impossible due to legacy system issues or inefficient as the system scales to large numbers of processors. Accordingly, we propose that instead of enforcing consistency and recency, such monitoring systems should report these properties along with query results, with the hope that this will allow the data to be appropriately interpreted. We present techniques for reporting consistency and recency for queries and evaluate them with respect to efficiency and precision. Finally, we describe our prototype implementation and present experimental results of our techniques.

#index 893108
#* Towards robust indexing for ranked queries
#@ Dong Xin;Chen Chen;Jiawei Han
#t 2006
#c 4
#% 213981
#% 237205
#% 248010
#% 248028
#% 288976
#% 300180
#% 333854
#% 333951
#% 341704
#% 379902
#% 465167
#% 479816
#! Top-k query asks for k tuples ordered according to a specific ranking function that combines the values from multiple participating attributes. The combined score function is usually linear. To efficiently answer top-k queries, preprocessing and indexing the data have been used to speed up the run time performance. Many indexing methods allow the online query algorithms progressively retrieve the data and stop at a certain point. However, in many cases, the number of data accesses is sensitive to the query parameters (i.e., linear weights in the score functions).In this paper, we study the sequentially layered indexing problem where tuples are put into multiple consecutive layers and any top-k query can be answered by at most k layers of tuples. We propose a new criterion for building the layered index. A layered index is robust if for any k, the number of tuples in the top k layers is minimal in comparison with all the other alternatives. The robust index guarantees the worst case performance for arbitrary query parameters. We derive a necessary and sufficient condition for robust index. The problem is shown solvable within O(ndlog n) (where d is the number of dimensions, and n is the number of tuples). To reduce the high complexity of the exact solution, we develop an approximate approach, which has time complexity O(2d n(log n)r(d)-1), where r(d) = ⌈d/2⌉ + ⌊d/2⌋ ⌈d/2⌉. Our experimental results show that our proposed method outperforms the best known previous methods.

#index 893109
#* An incrementally maintainable index for approximate lookups in hierarchical data
#@ Nikolaus Augsten;Michael Böhlen;Johann Gamper
#t 2006
#c 4
#% 66654
#% 121278
#% 397373
#% 397375
#% 480489
#% 480656
#% 654452
#% 659923
#% 659999
#% 765423
#% 766669
#% 768815
#% 800534
#% 806218
#% 810044
#% 810071
#% 824676
#% 993951
#! Several recent papers argue for approximate lookups in hierarchical data and propose index structures that support approximate searches in large sets of hierarchical data. These index structures must be updated if the underlying data changes. Since the performance of a full index reconstruction is prohibitive, the index must be updated incrementally.We propose a persistent and incrementally maintainable index for approximate lookups in hierarchical data. The index is based on small tree patterns, called pq-grams. It supports efficient updates in response to structure and value changes in hierarchical data and is based on the log of tree edit operations. We prove the correctness of the incremental maintenance for sequences of edit operations. Our algorithms identify a small set of pq-grams that must be updated to maintain the index. The experimental results with synthetic and real data confirm the scalability of our approach.

#index 893110
#* FIX: feature-based indexing technique for XML documents
#@ Ning Zhang;M. Tamer Özsu;Ihab F. Ilyas;Ashraf Aboulnaga
#t 2006
#c 4
#% 132779
#% 291299
#% 333981
#% 397360
#% 397375
#% 593696
#% 650962
#% 654450
#% 654452
#% 659999
#% 660000
#% 745461
#% 745477
#% 745518
#% 765429
#% 770338
#% 810046
#% 810072
#% 814010
#% 824662
#% 824663
#% 824667
#% 1015273
#% 1015277
#! Indexing large XML databases is crucial for efficient evaluation of XML twig queries. In this paper, we propose a feature-based indexing technique, called FIX, based on spectral graph theory. The basic idea is that for each twig pattern in a collection of XML documents, we calculate a vector of features based on its structural properties. These features are used as keys for the patterns and stored in a B+tree. Given an XPath query, its feature vector is first calculated and looked up in the index. Then a further refinement phase is performed to fetch the final results. We experimentally study the indexing technique over both synthetic and real data sets. Our experiments show that FIX provides great pruning power and could gain an order of magnitude performance improvement for many XPath queries over existing evaluation techniques.

#index 893111
#* Type-based XML projection
#@ Véronique Benzaken;Giuseppe Castagna;Dario Colazzo;Kim Nguyên
#t 2006
#c 4
#% 125620
#% 487257
#% 722731
#% 771883
#% 824680
#% 827135
#% 994015
#% 1015272
#% 1721253
#% 1722421
#! XML data projection (or pruning) is one of the main optimization techniques recently adopted in the context of main-memory XML query-engines. The underlying idea is quite simple: given a query Q over a document D, the subtrees of D not necessary to evaluate Q are pruned, thus obtaining a smaller document D'. Then Q is executed over D', hence avoiding to allocate and process nodes that will never be reached by navigational specifications in Q.In this article, we propose a new approach, based on types, that greatly improves current solutions. Besides providing comparable or greater precision and far lesser pruning overhead our solution, unlike current approaches, takes into account backward axes, predicates, and can be applied to multiple queries rather than just to single ones. A side contribution is a new type system for XPath able to handle backward axes, which we devise in order to apply our solution.The soundness of our approach is formally proved. Furthermore, we prove that the approach is also complete (i.e., yields the best possible type-driven pruning) for a relevant class of queries and DTDs, which include nearly all the queries used in the XMark and XPathMark benchmarks. These benchmarks are also used to test our implementation and show and gauge the practical benefits of our solution.

#index 893112
#* Twig2Stack: bottom-up processing of generalized-tree-pattern queries over XML documents
#@ Songting Chen;Hua-Gang Li;Junichi Tatemura;Wang-Pin Hsiung;Divyakant Agrawal;K. Selçuk Candan
#t 2006
#c 4
#% 172924
#% 289335
#% 333981
#% 397375
#% 562456
#% 659999
#% 733593
#% 765406
#% 803121
#% 810046
#% 824667
#% 824692
#% 864465
#% 878665
#% 1015272
#% 1015274
#% 1015276
#! Tree pattern matching is one of the most fundamental tasks for XML query processing. Holistic twig query processing techniques [4, 16] have been developed to minimize the intermediate results, namely, those root-to-leaf path matches that are not in the final twig results. However, useless path matches cannot be completely avoided, especially when there is a parent-child relationship in the twig query. Furthermore, existing approaches do not consider the fact that in practice, in order to process XPath or XQuery statements, a more powerful form of twig queries, namely, Generalized-Tree-Pattern (GTP) [8] queries, is required. Most existing works on processing GTP queries generally calls for costly post-processing for eliminating redundant data and/or grouping of the matching results.In this paper, we first propose a novel hierarchical stack encoding scheme to compactly represent the twig results. We introduce Twig2Stack, a bottom-up algorithm for processing twig queries based on this encoding scheme. Then we show how to efficiently enumerate the query results from the encodings for a given GTP query. To our knowledge, this is the first GTP matching solution that avoids any post path-join, sort, duplicate elimination and grouping operations. Extensive performance studies on various data sets and queries show that the proposed Twig2Stack algorithm not only has better twig query processing performance than state-of-the-art algorithms, but is also capable of efficiently processing the more complex GTP queries.

#index 893113
#* An algebraic query model for effective and efficient retrieval of XML fragments
#@ Sujeet Pradhan
#t 2006
#c 4
#% 309726
#% 330678
#% 368248
#% 458829
#% 465155
#% 654441
#% 654442
#% 660011
#% 766415
#% 810052
#% 818242
#% 824703
#% 838492
#% 863389
#% 1015258
#% 1016135
#% 1721863
#! Finding a suitable fragment of interest in a nonschematic XML document with a simple keyword search is a complex task. To deal with this problem, this paper proposes a theoretical framework with a focus on an algebraic query model having a novel query semantics. Based on this semantics, XML fragments that look meaningful to a keyword-based query are effectively retrieved by the operations defined in the model. In contrast to earlier work, our model supports filters for restricting the size of a query result, which otherwise may contain a large number of potentially irrelevant fragments. We introduce a class of filters having a special property that enables significant reduction in query processing cost. Many practically useful filters fall in this class and hence, the proposed model can be efficiently applied to real-world XML documents. Several other issues regarding algebraic manipulation of the operations defined in our query model are also formally discussed.

#index 893114
#* Putting context into schema matching
#@ Philip Bohannon;Eiman Elnahrawy;Wenfei Fan;Michael Flaster
#t 2006
#c 4
#% 50073
#% 169717
#% 300131
#% 307632
#% 332166
#% 333990
#% 344447
#% 384978
#% 398752
#% 443408
#% 479783
#% 480134
#% 480645
#% 481944
#% 564416
#% 572314
#% 641044
#% 654457
#% 654458
#% 765433
#% 783471
#% 800497
#% 810067
#% 810078
#% 810103
#% 810105
#% 824737
#% 993981
#% 993982
#% 1015303
#! Attribute-level schema matching has proven to be an important first step in developing mappings for data exchange, integration, restructuring and schema evolution. In this paper we investigate contextual schema matching, in which selection conditions are associated with matches by the schema matching process in order to improve overall match quality. We define a general space of matching techniques, and within this framework we identify a variety of novel, concrete algorithms for contextual schema matching. Furthermore, we show how common schema mapping techniques can be generalized to take more effective advantage of contextual matches, enabling automatic construction of mappings across certain forms of schema heterogeneity. An experimental study examines a wide variety of quality and performance issues. In addition, it demonstrates that contextual schema matching is an effective and practical technique to further automate the definition of complex data transformations.

#index 893115
#* Schema summarization
#@ Cong Yu;H. V. Jagadish
#t 2006
#c 4
#% 57514
#% 207930
#% 220283
#% 263981
#% 268079
#% 534852
#% 535195
#% 650962
#% 660001
#! Real database systems can often be very complex. A person wishing to access data from an unfamiliar database has the daunting task of understanding its schema before being able to pose a correct query against it. A schema summary can be of great help, providing a succinct overview of the entire schema, and making it possible to explore in depth only the relevant schema components.In this paper we formally define a schema summary and two desirable properties (in addition to minimizing size) of a summary: presenting important schema elements and achieving broad information coverage. We develop algorithms that allow us to automatically generate schema summaries based on these two goals. We further develop an objective metric for assessing the quality of a schema summary using query information. Experimental evaluation using this metric demonstrates that the summaries produced by our algorithms can significantly reduce the amount of user effort required to formulate a query through schema exploration.

#index 893116
#* Multi-column substring matching for database schema translation
#@ Robert H. Warren;Frank Wm. Tompa
#t 2006
#c 4
#% 121278
#% 312726
#% 320062
#% 320220
#% 321332
#% 321635
#% 333988
#% 333990
#% 475222
#% 480645
#% 572314
#% 577309
#% 654467
#% 765433
#% 768937
#% 790844
#% 884607
#% 1016219
#! We describe a method for discovering complex schema translations involving substrings from multiple database columns. The method does not require a training set of instances linked across databases and it is capable of dealing with both fixed-and variable-length field columns. We propose an iterative algorithm that deduces the correct sequence of concatenations of column substrings in order to translate from one database to another. We introduce the algorithm along with examples on common database data values and examine its performance on real-world and synthetic datasets.

#index 893117
#* Querying business processes
#@ Catriel Beeri;Anat Eyal;Simon Kamenkovich;Tova Milo
#t 2006
#c 4
#% 29439
#% 32904
#% 71306
#% 86957
#% 294240
#% 342820
#% 442960
#% 557919
#% 655355
#% 754120
#% 765420
#% 778311
#% 800609
#% 809234
#% 810053
#% 814647
#% 824760
#% 994005
#% 1711212
#! We present in this paper BP-QL, a novel query language for querying business processes. The BP-QL language is based on an intuitive model of business processes, an abstraction of the emerging BPEL (Business Process Execution Language) standard. It allows users to query business processes visually, in a manner very analogous to how such processes are typically specified, and can be employed in a distributed setting, where process components may be provided by distinct providers(peers).We describe here the query language as well as its underlying formal model. We consider the properties of the various language components and explain how they influenced the language design. In particular we distinguish features that can be efficiently supported, and those that incur a prohibitively high cost, or cannot be computed at all. We also present our implementation which complies with real life standards for business process specifications, XML, and Web services, and is used in the BP-QL system.

#index 893118
#* Query optimization over web services
#@ Utkarsh Srivastava;Kamesh Munagala;Jennifer Widom;Rajeev Motwani
#t 2006
#c 4
#% 554
#% 83933
#% 152940
#% 229827
#% 273912
#% 287461
#% 300169
#% 333947
#% 340663
#% 397356
#% 397416
#% 442700
#% 479449
#% 509879
#% 741349
#% 765420
#% 765434
#% 765435
#% 783480
#% 797819
#% 874896
#% 1015278
#% 1015282
#! Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a general-purpose Web Service Management System (WSMS) that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic WSMS problem: query optimization for Select-Project-Join queries spanning multiple web services. Our main result is an algorithm for arranging a query's web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query's total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is NP-hard. We also give an algorithm for determining the optimal granularity of data "chunks" to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques.

#index 893119
#* iDM: a unified and versatile data model for personal dataspace management
#@ Jens-Peter Dittrich;Marcos Antonio Vaz Salles
#t 2006
#c 4
#% 1055
#% 201968
#% 248838
#% 273681
#% 286258
#% 294600
#% 322880
#% 330305
#% 339375
#% 386381
#% 462204
#% 463919
#% 481258
#% 481923
#% 654465
#% 654485
#% 726621
#% 765422
#% 810014
#% 810025
#% 824693
#% 824695
#% 824696
#% 824773
#% 845350
#% 860036
#% 1721851
#! Personal Information Management Systems require a powerful and versatile data model that is able to represent a highly heterogeneous mix of data such as relational data, XML, file content, folder hierarchies, emails and email attachments, data streams, RSS feeds and dynamically computed documents, e.g. ActiveXML [3]. Interestingly, until now no approach was proposed that is able to represent all of the above data in a single, powerful yet simple data model. This paper fills this gap. We present the iMeMex Data Model (iDM) for personal information management. iDM is able to represent unstructured, semi-structured and structured data inside a single model. Moreover, iDM is powerful enough to represent graph-structured data, intensional data as well as infinite data streams. Further, our model enables to represent the structural information available inside files. As a consequence, the artifical boundary between inside and outside a file is removed to enable a new class of queries. As iDM allows the representation of the whole personal dataspace [20] of a user in a single model, it is the foundation of the iMeMex Personal Dataspace Management System (PDSMS) [16, 14, 47]. This paper also presents results of an evaluation of an initial iDM implementation in iMeMex that show that iDM can be efficiently supported in a real PDSMS.

#index 893120
#* CURE for cubes: cubing using a ROLAP engine
#@ Konstantinos Morfonios;Yannis Ioannidis
#t 2006
#c 4
#% 210182
#% 227880
#% 273916
#% 333925
#% 397388
#% 464215
#% 479450
#% 480123
#% 481951
#% 654446
#% 729576
#% 745506
#% 765155
#% 769898
#% 810029
#% 993996
#% 1015294
#% 1016173
#% 1016174
#! Data cube construction has been the focus of much research due to its importance in improving efficiency of OLAP. A significant fraction of this work has been on ROLAP techniques, which are based on relational technology. Existing ROLAP cubing solutions mainly focus on "flat" datasets, which do not include hierarchies in their dimensions. Nevertheless, the nature of hierarchies introduces several complications into cube construction, making existing techniques essentially inapplicable in a significant number of real-world applications. In particular, hierarchies raise three main challenges: (a) The number of nodes in a cube lattice increases dramatically and its shape is more involved. These require new forms of lattice traversal for efficient execution. (b) The number of unique values in the higher levels of a dimension hierarchy may be very small; hence, partitioning data into fragments that fit in memory and include all entries of a particular value may often be impossible. This requires new partitioning schemes. (c) The number of tuples that need to be materialized in the final cube increases dramatically. This requires new storage schemes that remove all forms of redundancy for efficient space utilization. In this paper, we propose CURE, a novel ROLAP cubing method that addresses these issues and constructs complete data cubes over very large datasets with arbitrary hierarchies. CURE contributes a novel lattice traversal scheme, an optimized partitioning method, and a suite of relational storage schemes for all forms of redundancy. We demonstrate the effectiveness of CURE through experiments on both real-world and synthetic datasets. Among the experimental results, we distinguish those that have made CURE the first ROLAP technique to complete the construction of the cube of the highest-density dataset in the APB-1 benchmark (12 GB). CURE was in fact quite efficient on this, showing great promise with respect to the potential of the technique overall.

#index 893121
#* Efficient allocation algorithms for OLAP over imprecise data
#@ Doug Burdick;Prasad M. Deshpande;T. S. Jayram;Raghu Ramakrishnan;Shivakumar Vaithyanathan
#t 2006
#c 4
#% 77979
#% 172952
#% 210182
#% 410276
#% 427199
#% 479450
#% 481951
#% 654487
#% 824733
#% 1016201
#! Recent work proposed extending the OLAP data model to support data ambiguity, specifically imprecision and uncertainty. A process called allocation was proposed to transform a given imprecise fact table into a form, called the Extended Database, that can be readily used to answer OLAP aggregation queries.In this work, we present scalable, efficient algorithms for creating the Extended Database (i.e., performing allocation) for a given imprecise fact table. Many allocation policies require multiple iterations over the imprecise fact table, and the straightforward evaluation approaches introduced earlier can be highly inefficient. Optimizing iterative allocation policies for large datasets presents novel challenges, and has not been considered previously to the best of our knowledge. In addition to developing scalable allocation algorithms, we present a performance evaluation that demonstrates their efficiency and compares their performance with respect to straight-foward approaches.

#index 893122
#* Composite subset measures
#@ Lei Chen;Raghu Ramakrishnan;Paul Barford;Bee-Chung Chen;Vinod Yegneswaran
#t 2006
#c 4
#% 136740
#% 176535
#% 287213
#% 420053
#% 458550
#% 459024
#% 462204
#% 465170
#% 479646
#% 481604
#% 481608
#% 481951
#% 482082
#% 504020
#% 654445
#% 723279
#% 781686
#% 785347
#% 810029
#% 864597
#% 963669
#! Measures are numeric summaries of a collection of data records produced by applying aggregation functions. Summarizing a collection of subsets of a large dataset, by computing a measure for each subset in the (typically, user-specified) collection is a fundamental problem. The multidimensional data model, which treats records as points in a space defined by dimension attributes, offers a natural space of data subsets to be considered as summarization candidates, and traditional SQL and OLAP constructs, such as GROUP BY and CUBE, allow us to compute measures for subsets drawn from this space. However, GROUP BY only allows us to summarize a limited collection of subsets, and CUBE summarizes all subsets in this space. Further, they restrict the measure used to summarize a data subset to be a one-step aggregation, using functions such as SUM, of field-values in the data records.In this paper, we introduce composite subset measures, computed by aggregating not only data records but also the measures of other related subsets. We allow summarization of naturally related regions in the multidimensional space, offering more flexibility than either GROUP BY or CUBE in the choice of what data subsets to summarize. Thus, our framework allows more meaningful summaries to be computed for a targeted collection of data subsets.We propose an algebra called AW-RA and an equivalent pictorial language called aggregation workflows. Aggregation workflows allow for intuitive expression of composite measure queries, and the underlying algebra is designed to facilitate efficient multiscan execution. We describe an evaluation framework based on multiple passes of sorting and scanning over the original dataset. In each pass, several measures are evaluated simultaneously, and dependencies between these measures and containment relationships between the underlying subsets of data are orchestrated to reduce the memory footprint of the computation. We present a performance evaluation that demonstrates the benefits of our approach.

#index 893123
#* Efficient and decentralized PageRank approximation in a peer-to-peer web search network
#@ Josiane Xavier Parreira;Debora Donato;Sebastian Michel;Gerhard Weikum
#t 2006
#c 4
#% 2833
#% 268079
#% 290830
#% 307247
#% 307424
#% 311808
#% 322884
#% 340175
#% 340176
#% 401985
#% 409460
#% 451536
#% 453464
#% 505869
#% 577330
#% 616528
#% 783528
#% 799636
#% 812792
#% 824762
#% 853940
#% 1016164
#% 1394450
#% 1711105
#! PageRank-style (PR) link analyses are a cornerstone of Web search engines and Web mining, but they are computationally expensive. Recently, various techniques have been proposed for speeding up these analyses by distributing the link graph among multiple sites. However, none of these advanced methods is suitable for a fully decentralized PR computation in a peer-to-peer (P2P) network with autonomous peers, where each peer can independently crawl Web fragments according to the user's thematic interests. In such a setting the graph fragments that different peers have locally available or know about may arbitrarily overlap among peers, creating additional complexity for the PR computation.This paper presents the JXP algorithm for dynamically and collaboratively computing PR scores of Web pages that are arbitrarily distributed in a P2P network. The algorithm runs at every peer, and it works by combining locally computed PR scores with random meetings among the peers in the network. It is scalable as the number of peers on the network grows, and experiments as well as theoretical arguments show that JXP scores converge to the true PR scores that one would obtain by a centralized computation.

#index 893124
#* LinkClus: efficient clustering via heterogeneous semantic links
#@ Xiaoxin Yin;Jiawei Han;Philip S. Yu
#t 2006
#c 4
#% 152934
#% 210173
#% 248790
#% 273891
#% 283833
#% 469422
#% 481281
#% 550556
#% 577273
#% 629644
#% 643009
#% 729918
#% 729933
#% 769883
#% 805904
#% 823359
#% 835018
#% 840840
#! Data objects in a relational database are cross-linked with each other via multi-typed links. Links contain rich semantic information that may indicate important relationships among objects. Most current clustering methods rely only on the properties that belong to the objects per se. However, the similarities between objects are often indicated by the links, and desirable clusters cannot be generated using only the properties of objects.In this paper we explore linkage-based clustering, in which the similarity between two objects is measured based on the similarities between the objects linked with them. In comparison with a previous study (SimRank) that computes links recursively on all pairs of objects, we take advantage of the power law distribution of links, and develop a hierarchical structure called SimTree to represent similarities in multi-granularity manner. This method avoids the high cost of computing and storing pairwise similarities but still thoroughly explore relationships among objects. An efficient algorithm is proposed to compute similarities between objects by avoiding pairwise similarity computations through merging computations that go through the same branches in the SimTree. Experiments show the proposed approach achieves high efficiency, scalability, and accuracy in clustering multi-typed linked objects.

#index 893125
#* Link spam detection based on mass estimation
#@ Zoltan Gyongyi;Pavel Berkhin;Hector Garcia-Molina;Jan Pedersen
#t 2006
#c 4
#% 577329
#% 590524
#% 754088
#% 772018
#% 799632
#% 807297
#% 818201
#% 824694
#% 824711
#% 1016177
#! Link spamming intends to mislead search engines and trigger an artificially high link-based ranking of specific target web pages. This paper introduces the concept of spam mass, a measure of the impact of link spamming on a page's ranking. We discuss how to estimate spam mass and how the estimates can help identifying pages that benefit significantly from link spamming. In our experiments on the host-level Yahoo! web graph we use spam mass estimates to successfully identify tens of thousands of instances of heavyweight link spamming.

#index 893126
#* Answering top-k queries using views
#@ Gautam Das;Dimitrios Gunopulos;Nick Koudas;Dimitris Tsirogiannis
#t 2006
#c 4
#% 172913
#% 213981
#% 248010
#% 300180
#% 328424
#% 333854
#% 333951
#% 397378
#% 399762
#% 427199
#% 464726
#% 479462
#% 479816
#% 480330
#% 480819
#% 659990
#% 659993
#% 763882
#% 777931
#! The problem of obtaining efficient answers to top-k queries has attracted a lot of research attention. Several algorithms and numerous variants of the top-k retrieval problem have been introduced in recent years. The general form of this problem requests the k highest ranked values from a relation, using monotone combining functions on (a subset of) its attributes.In this paper we explore space performance tradeoffs related to this problem. In particular we study the problem of answering top-k queries using views. A view in this context is a materialized version of a previously posed query, requesting a number of highest ranked values according to some monotone combining function defined on a subset of the attributes of a relation. Several problems of interest arise in the presence of such views. We start by presenting a new algorithm capable of combining the information from a number of views to answer ad hoc top-k queries. We then address the problem of identifying the most promising (in terms of performance) views to use for query answering in the presence of a collection of views. We formalize both problems and present efficient algorithms for their solution. We also discuss several extensions of the basic problems in this setting.We present the results of a thorough experimental study that deploys our techniques on real and synthetic data sets. Our results indicate that the techniques proposed herein comprise a robust solution to the problem of top-k query answering using views, gracefully exploring the space versus performance tradeoffs in the context of top-k query answering.

#index 893127
#* Answering top-k queries with multi-dimensional selections: the ranking cube approach
#@ Dong Xin;Jiawei Han;Hong Cheng;Xiaolei Li
#t 2006
#c 4
#% 43163
#% 223781
#% 227861
#% 227894
#% 237205
#% 248010
#% 248028
#% 248814
#% 300180
#% 333854
#% 333951
#% 397378
#% 399762
#% 427219
#% 480329
#% 765418
#% 810018
#% 824734
#% 993958
#% 1016173
#! Observed in many real applications, a top-k query often consists of two components to reflect a user's preference: a selection condition and a ranking function. A user may not only propose ad hoc ranking functions, but also use different interesting subsets of the data. In many cases, a user may want to have a thorough study of the data by initiating a multi-dimensional analysis of the top-k query results. Previous work on top-k query processing mainly focuses on optimizing data access according to the ranking function only. The problem of efficient answering top-k queries with multi-dimensional selections has not been well addressed yet.This paper proposes a new computational model, called ranking cube, for efficient answering top-k queries with multi-dimensional selections. We define a rank-aware measure for the cube, capturing our goal of responding to multi-dimensional ranking analysis. Based on the ranking cube, an efficient query algorithm is developed which progressively retrieves data blocks until the top-k results are found. The curse of dimensionality is a well-known challenge for the data cube and we cope with this difficulty by introducing a new technique of ranking fragments. Our experiments on Microsoft's SQL Server 2005 show that our proposed approaches have significant improvement over the previous methods.

#index 893128
#* IO-Top-k: index-access optimized top-k query processing
#@ Holger Bast;Debapriyo Majumdar;Ralf Schenkel;Martin Theobald;Gerhard Weikum
#t 2006
#c 4
#% 169806
#% 212665
#% 213786
#% 227894
#% 300167
#% 397378
#% 397608
#% 643566
#% 763882
#% 765418
#% 765466
#% 766671
#% 768521
#% 777931
#% 800508
#% 810018
#% 824703
#% 1015256
#% 1015265
#% 1016183
#% 1683906
#% 1698862
#! Top-k query processing is an important building block for ranked retrieval, with applications ranging from text and data integration to distributed aggregation of network logs and sensor data. Top-k queries operate on index lists for a query's elementary conditions and aggregate scores for result candidates. One of the best implementation methods in this setting is the family of threshold algorithms, which aim to terminate the index scans as early as possible based on lower and upper bounds for the final scores of result candidates. This procedure performs sequential disk accesses for sorted index scans, but also has the option of performing random accesses to resolve score uncertainty. This entails scheduling for the two kinds of accesses: 1) the prioritization of different index lists in the sequential accesses, and 2) the decision on when to perform random accesses and for which candidates.The prior literature has studied some of these scheduling issues, but only for each of the two access types in isolation. The current paper takes an integrated view of the scheduling issues and develops novel strategies that outperform prior proposals by a large margin. Our main contributions are new, principled, scheduling methods based on a Knapsack-related optimization for sequential accesses and a cost model for random accesses. The methods can be further boosted by harnessing probabilistic estimators for scores, selectivities, and index list correlations. In performance experiments with three different datasets (TREC Terabyte, HTTP server logs, and IMDB), our methods achieved significant performance gains compared to the best previously known methods.

#index 893129
#* Performance tradeoffs in read-optimized databases
#@ Stavros Harizopoulos;Velen Liang;Daniel J. Abadi;Samuel Madden
#t 2006
#c 4
#% 69094
#% 251474
#% 252458
#% 286258
#% 322412
#% 464843
#% 465169
#% 479821
#% 480119
#% 480821
#% 765417
#% 765431
#% 810039
#% 824697
#% 864446
#% 875026
#% 993967
#% 1015332
#% 1016187
#% 1016238
#! Database systems have traditionally optimized performance for write-intensive workloads. Recently, there has been renewed interest in architectures that optimize read performance by using column-oriented data representation and light-weight compression. This previous work has shown that under certain broad classes of workloads, column-based systems can outperform row-based systems. Previous work, however, has not characterized the precise conditions under which a particular query workload can be expected to perform better on a column-oriented database.In this paper we first identify the distinctive components of a read-optimized DBMS and describe our implementation of a high-performance query engine that can operate on both row and column-oriented data. We then use our prototype to perform an in-depth analysis of the tradeoffs between column and row-oriented architectures. We explore these tradeoffs in terms of disk bandwidth, CPU cache latency, and CPU cycles. We show that for most database workloads, a carefully designed column system can outperform a carefully designed row system, sometimes by an order of magnitude. We also present an analytical model to predict whether a given workload on a particular hardware configuration is likely to perform better on a row or column-based system.

#index 893130
#* To tune or not to tune?: a lightweight physical design alerter
#@ Nicolas Bruno;Surajit Chaudhuri
#t 2006
#c 4
#% 248815
#% 411554
#% 480158
#% 482100
#% 631950
#% 632100
#% 810026
#% 810111
#% 820356
#% 1016220
#% 1016221
#% 1688268
#! In recent years there has been considerable research on automating the physical design in database systems. Current techniques provide good recommendations, but are resource intensive. This makes DBAs somewhat conservative when deciding to launch a resource-intensive tuning session. In this paper, we introduce an alerter that helps determining when a physical design tool should be invoked. The alerter is a lightweight mechanism that provides guaranteed lower (and upper bounds) on the improvement that a DBA could expect by invoking a comprehensive physical design tool. Moreover, it produces an accompanying recommendation that serves as a "proof" for the lower bound. We show experimentally that the alerter handles large workloads with little overhead, and help judiciously decide on launching subsequent tuning sessions.

#index 893131
#* Efficient scheduling of heterogeneous continuous queries
#@ Mohamed A. Sharaf;Panos K. Chrysanthis;Alexandros Labrinidis;Kirk Pruhs
#t 2006
#c 4
#% 36117
#% 259634
#% 282614
#% 290747
#% 300167
#% 333854
#% 340635
#% 397352
#% 397353
#% 480642
#% 481131
#% 580692
#% 593936
#% 654462
#% 659996
#% 788215
#% 839187
#% 993949
#% 1015279
#% 1015282
#% 1015324
#! Data Stream Management Systems (DSMS) typically host multiple Continuous Queries (CQ) that process streams of data. In this paper, we examine the problem of how to schedule CQs in a DSMS to optimize for average QoS. We show that unlike standard on-line systems, scheduling policies in DSMSs that optimize for average response time will be different than policies that optimize for average slowdown which is more appropriate metric to use in the presence of a heterogeneous workload. We also propose a hybrid scheduling policy based on slowdown that strikes a fine balance between performance and fairness. We further discuss how our policies can be efficiently implemented and extended to exploit sharing in optimized multi-query plans and multi-stream CQs. Finally, we experimentally show using real data that our policies outperform currently used ones.

#index 893132
#* Indexing for function approximation
#@ Biswanath Panda;Mirek Riedewald;Stephen B. Pope;Johannes Gehrke;L. Paul Chew
#t 2006
#c 4
#% 86950
#% 232764
#% 252304
#% 261358
#% 316524
#% 342828
#% 427199
#% 435141
#% 464195
#% 479649
#% 479973
#% 480093
#% 481956
#% 654481
#% 730197
#% 814646
#% 1386747
#! Simulation is one of the most powerful tools that scientists have at their disposal for studying and understanding real-world physical phenomena. In order to be realistic, the mathematical models which drive simulations are often very complex and run for a very large number of simulation steps. The required computational resources often make it infeasible to evaluate simulation models exactly at each step, and thus scientists trade accuracy for reduced simulation cost.In this paper, we explore function approximation for a combustion simulation. In particular, we model high-dimensional function approximation (HFA) as a storage and retrieval problem, and we show that HFA defines a novel class of applications for high dimensional index structures. The interesting property of HFA is that it imposes a mixed query/update workload on the index which leads to novel tradeoffs between the efficiency of search versus updates. We investigate in detail one specific approach to HFA based on Taylor Series expansions and we analyze tradeoffs in index structure design through a thorough experimental study.

#index 893133
#* Adaptive execution of variable-accuracy functions
#@ Matthew Denny;Michael J. Franklin
#t 2006
#c 4
#% 70370
#% 152940
#% 172931
#% 210206
#% 227883
#% 397353
#% 479636
#% 479967
#% 480332
#% 481915
#% 654487
#% 654488
#% 654508
#% 654510
#% 810062
#% 931796
#% 993949
#% 1016178
#! Many analysis applications require the ability to repeatedly execute sophisticated modeling functions, which can each take minutes or even hours to produce a single answer. Because of this expense, such applications have largely been unable to directly use such models in queries, with either on-demand or continuous query processing technology. Query processors are hindered in their ability to optimize expensive modeling functions due to the "black box" nature of existing user-defined function (UDF) interfaces. In this paper, we address the problem of querying over sophisticated models with the development of VAOs (Variable-Accuracy Operators). VAOs use a new function interface that exposes the trade-off between compute time and accuracy that exists in many modeling functions. Using this interface, VAOs adaptively run each function call in a query only to an accuracy needed to answer the query, thus eliminating unneeded work. In this paper, we present the design of VAOs for a set of common query operations. We show the effectiveness of VAOs using a prototype implementation running financial queries over real bond market data.

#index 893134
#* AFilter: adaptable XML filtering with prefix-caching suffix-clustering
#@ K. Selçuk Candan;Wang-Pin Hsiung;Songting Chen;Junichi Tatemura;Divyakant Agrawal
#t 2006
#c 4
#% 300179
#% 397375
#% 397407
#% 480296
#% 570879
#% 570880
#% 654476
#% 654477
#% 731408
#% 765441
#% 781453
#% 791182
#% 801685
#% 803121
#% 809253
#% 824667
#% 824669
#% 864465
#% 1015276
#% 1016148
#! XML message filtering problem involves searching for instances of a given, potentially large, set of patterns in a continuous stream of XML messages. Since the messages arrive continuously, it is essential that the filtering rate matches the data arrival rate. Therefore, the given set of filter patterns needs to be indexed appropriately to enable real-time processing of the streaming XML data. In this paper, we propose AFilter, an adaptable, and thus scalable, path expression filtering approach. AFilter has a base memory requirement linear in filter expression and data size. Furthermore, when additional memory is available, AFilter can exploit prefix commonalities in the set of filter expressions using a loosely-coupled prefix caching mechanism as opposed to tightly-coupled active state representation of alternative approaches. Unlike existing systems, AFilter can also exploit suffix-commonalities across filter expressions, while simultaneously leveraging the prefix-commonalities through the cache. Finally, AFilter uses a triggering mechanism to prevent excessive consumption of resources by delaying processing until a trigger condition is observed. Experiment results show that AFilter provides significantly better scalability and runtime performance when compared to state of the art filtering systems.

#index 893135
#* Answering tree pattern queries using views
#@ Laks V. S. Lakshmanan;Hui Wang;Zheng Zhao
#t 2006
#c 4
#% 198465
#% 273924
#% 333989
#% 378393
#% 378409
#% 397375
#% 465051
#% 465053
#% 465065
#% 480149
#% 564264
#% 570875
#% 572311
#% 576102
#% 654450
#% 824661
#% 1015260
#% 1015267
#% 1016134
#% 1688312
#% 1721246
#! We study the query answering using views (QAV) problem for tree pattern queries. Given a query and a view, the QAV problem is traditionally formulated in two ways: (i) find an equivalent rewriting of the query using only the view, or (ii) find a maximal contained rewriting using only the view. The former is appropriate for classical query optimization and was recently studied by Xu and Ozsoyoglu for tree pattern queries (TP). However, for information integration, we cannot rely on equivalent rewriting and must instead use maximal contained rewriting as shown by Halevy. Motivated by this, we study maximal contained rewriting for TP, a core subset of XPath, both in the absence and presence of a schema. In the absence of a schema, we show there are queries whose maximal contained rewriting (MCR) can only be expressed as the union of exponentially many TPs. We characterize the existence of a maximal contained rewriting and give a polynomial time algorithm for testing the existence of an MCR. We also give an algorithm for generating the MCR when one exists. We then consider QAV in the presence of a schema. We characterize the existence of a maximal contained rewriting when the schema contains no recursion or union types, and show that it consists of at most one TP. We give an efficient polynomial time algorithm for generating the maximal contained rewriting whenever it exists. Finally, we discuss QAV in the presence of recursive schemas.

#index 893136
#* Maintaining XPath views in loosely coupled systems
#@ Arsany Sawires;Junichi Tatemura;Oliver Po;Divyakant Agrawal;Amr El Abbadi;K. Selçuk Candan
#t 2006
#c 4
#% 13016
#% 59350
#% 200966
#% 273924
#% 397409
#% 462213
#% 465051
#% 479629
#% 481128
#% 564264
#% 572311
#% 733593
#% 775875
#% 800635
#% 805907
#% 810045
#% 824661
#% 824690
#% 838429
#% 839147
#% 864516
#% 875007
#% 994015
#% 1015276
#% 1016134
#! We address the problem of maintaining materialized XPath views in environments where the view maintenance system and the base data system are loosely-coupled. We show that the recently proposed XPath view maintenance techniques require tight coupling, and thus are not practical for loosely-coupled systems. Our solution adapts to loose-coupling by using information that is fully available through standard XPath interfaces. This information consists of the view definition, the update statement, and the current materialized view result. Under this model, incremental maintenance is not always possible; thus, maintaining the consistency of the views requires frequent view recomputations. Our goal is to reduce the frequency of view recomputation by detecting cases where a base update is irrelevant to a view, and cases where a view is self maintainable given a base update. We develop an approach that reduces the irrelevance and self maintainability tests, respectively, to checking the intersection and containment of XPath expressions. We present experimental results showing the effectiveness of the proposed approach in reducing view recomputations.

#index 893137
#* A dip in the reservoir: maintaining sample synopses of evolving datasets
#@ Rainer Gemulla;Wolfgang Lehner;Peter J. Haas
#t 2006
#c 4
#% 327
#% 1331
#% 164368
#% 210190
#% 243299
#% 248812
#% 273908
#% 379444
#% 411355
#% 463285
#% 765424
#% 765426
#% 765455
#% 808428
#% 824653
#% 864393
#% 1015280
#% 1015310
#% 1688270
#% 1729914
#! Perhaps the most flexible synopsis of a database is a random sample of the data; such samples are widely used to speed up processing of analytic queries and data-mining tasks, enhance query optimization, and facilitate information integration. In this paper, we study methods for incrementally maintaining a uniform random sample of the items in a dataset in the presence of an arbitrary sequence of insertions and deletions. For "stable" datasets whose size remains roughly constant over time, we provide a novel sampling scheme, called "random pairing" (RP) which maintains a bounded-size uniform sample by using newly inserted data items to compensate for previous deletions. The RP algorithm is the first extension of the almost 40-year-old reservoir sampling algorithm to handle deletions. Experiments show that, when dataset-size fluctuations over time are not too extreme, RP is the algorithm of choice with respect to speed and sample-size stability. For "growing" datasets, we consider algorithms for periodically "resizing" a bounded-size random sample upwards. We prove that any such algorithm cannot avoid accessing the base data, and provide a novel resizing algorithm that minimizes the time needed to increase the sample size.

#index 893138
#* On biased reservoir sampling in the presence of stream evolution
#@ Charu C. Aggarwal
#t 2006
#c 4
#% 1331
#% 227883
#% 248812
#% 248822
#% 273907
#% 333983
#% 379444
#% 379445
#% 397385
#% 428155
#% 480805
#% 576112
#% 578390
#% 824686
#% 993960
#% 1016200
#! The method of reservoir based sampling is often used to pick an unbiased sample from a data stream. A large portion of the unbiased sample may become less relevant over time because of evolution. An analytical or mining task (eg. query estimation) which is specific to only the sample points from a recent time-horizon may provide a very inaccurate result. This is because the size of the relevant sample reduces with the horizon itself. On the other hand, this is precisely the most important case for data stream algorithms, since recent history is frequently analyzed. In such cases, we show that an effective solution is to bias the sample with the use of temporal bias functions. The maintenance of such a sample is non-trivial, since it needs to be dynamically maintained, without knowing the total number of points in advance. We prove some interesting theoretical properties of a large class of memory-less bias functions, which allow for an efficient implementation of the sampling algorithm. We also show that the inclusion of bias in the sampling process introduces a maximum requirement on the reservoir size. This is a nice property since it shows that it may often be possible to maintain the maximum relevant sample with limited storage requirements. We not only illustrate the advantages of the method for the problem of query estimation, but also show that the approach has applicability to broader data mining problems such as evolution analysis and classification.

#index 893139
#* State-slice: new paradigm of multi-query optimization of window-based stream queries
#@ Song Wang;Elke Rundensteiner;Samrat Ganguly;Sudeept Bhatnagar
#t 2006
#c 4
#% 36117
#% 300166
#% 333962
#% 378388
#% 386207
#% 397353
#% 578391
#% 578560
#% 659996
#% 726621
#% 800502
#% 800517
#% 810032
#% 824714
#% 824770
#% 874999
#% 875022
#% 993948
#% 993949
#% 1015279
#% 1015280
#% 1016156
#% 1016157
#% 1016210
#% 1016269
#! Modern stream applications such as sensor monitoring systems and publish/subscription services necessitate the handling of large numbers of continuous queries specified over high volume data streams. Efficient sharing of computations among multiple continuous queries, especially for the memory- and CPU-intensive window-based operations, is critical. A novel challenge in this scenario is to allow resource sharing among similar queries, even if they employ windows of different lengths. This paper first reviews the existing sharing methods in the literature, and then illustrates the significant performance shortcomings of these methods.This paper then presents a novel paradigm for the sharing of window join queries. Namely we slice window states of a join operator into fine-grained window slices and form a chain of sliced window joins. By using an elaborate pipelining methodology, the number of joins after state slicing is reduced from quadratic to linear. This novel sharing paradigm enables us to push selections down into the chain and flexibly select subsequences of such sliced window joins for computation sharing among queries with different window sizes. Based on the state-slice sharing paradigm, two algorithms are proposed for the chain buildup. One minimizes the memory consumption while the other minimizes the CPU usage. The algorithms are proven to find the optimal chain with respect to memory or CPU usage for a given query workload. We have implemented the slice-share paradigm within the data stream management system CAPE. The experimental results show that our strategy provides the best performance over a diverse range of workload settings among all alternate solutions in the literature.

#index 893140
#* Similarity search: a matching based approach
#@ Anthony K. H. Tung;Rui Zhang;Nick Koudas;Beng Chin Ooi
#t 2006
#c 4
#% 102772
#% 131061
#% 169940
#% 213981
#% 310509
#% 333854
#% 451645
#% 464195
#% 479649
#% 480132
#% 481279
#% 481956
#% 654466
#% 806212
#! Similarity search is a crucial task in multimedia retrieval and data mining. Most existing work has modelled this problem as the nearest neighbor (NN) problem, which considers the distance between the query object and the data objects over a fixed set of features. Such an approach has two drawbacks: 1) it leaves many partial similarities uncovered; 2) the distance is often affected by a few dimensions with high dissimilarity. To overcome these drawbacks, we propose the k-n-match problem in this paper.The k-n-match problem models similarity search as matching between the query object and the data objects in n dimensions, where n is a given integer smaller than dimensionality d and these n dimensions are determined dynamically to make the query object and the data objects returned in the answer set match best. The k-n-match query is expected to be superior to the kNN query in discovering partial similarities, however, it may not be as good in identifying full similarity since a single value of n may only correspond to a particular aspect of an object instead of the entirety. To address this problem, we further introduce the frequent k-n-match problem, which finds a set of objects that appears in the k-n-match answers most frequently for a range of n values. Moreover, we propose search algorithms for both problems. We prove that our proposed algorithm is optimal in terms of the number of individual attributes retrieved, which is especially useful for information retrieval from multiple systems. We can also apply the proposed algorithmic strategy to achieve a disk based algorithm for the (frequent) k-n-match query. By a thorough experimental study using both real and synthetic data sets, we show that: 1) the k-n-match query yields better result than the kNN query in identifying similar objects by partial similarities; 2) our proposed method (for processing the frequent k-n-match query) outperforms existing techniques for similarity search in terms of both effectiveness and efficiency.

#index 893141
#* Progressive computation of the min-dist optimal-location query
#@ Donghui Zhang;Yang Du;Tian Xia;Yufei Tao
#t 2006
#c 4
#% 235114
#% 300163
#% 480661
#% 777263
#% 1016191
#% 1720745
#% 1720751
#! This paper proposes and solves the min-dist optimal-location query in spatial databases. Given a set S of sites, a set O of weighted objects, and a spatial region Q, the min-dist optimal-location query returns a location in Q which, if a new site is built there, minimizes the average distance from each object to its closest site. This query can help a franchise (e.g. McDonald's) decide where to put a new store in order to maximize the benefit to its customers. To solve this problem is challenging, for there are theoretically infinite number of locations in Q, all of which could be candidates. This paper first provides a theorem that limits the number of candidate locations without losing the power to find exact answers. Then it provides a progressive algorithm that quickly suggests a location, tells the maximum error it may have, and keeps refining the result. When the algorithm finishes, the exact answer can be found. The intermediate result of early runs can be used to prune the search space for later runs. Crucial to the pruning technique are novel lower-bound estimators. The proposed algorithm, the effect of several optimizations, and the progressiveness are experimentally evaluated.

#index 893142
#* Bellwether analysis: predicting global aggregates from local regions
#@ Bee-Chung Chen;Raghu Ramakrishnan;Jude W. Shavlik;Pradeep Tamma
#t 2006
#c 4
#% 61792
#% 136350
#% 210182
#% 273916
#% 333925
#% 376266
#% 420053
#% 479787
#% 496116
#% 729926
#% 765155
#% 824734
#% 840922
#% 893122
#% 993958
#! Massive datasets are becoming commonplace in a wide range of domains, and mining them is recognized as a challenging problem with great potential value. Motivated by this challenge, much effort has been concentrated on developing scalable versions of machine learning algorithms. An often overlooked issue is that large datasets are rarely labeled with the outputs that we wish to learn to predict, due to the human labor required. We make the key observation that analysts can often use queries to define labels for cases, which leads to the problem of learning to predict such query-produced labels. Of course, if a dataset is available in its entirety, we can simply run the query again to compute labels. The interesting scenarios are those where, after the predictive model is trained, new data is gathered at significant incremental cost and, perhaps, over time. The challenge is to accurately predict the query-labels for the projected completion of new datasets, based only on certain cost-effective subsets, which we call bellwethers.

#index 893143
#* Efficiently linking text documents with relevant structured information
#@ Venkatesan T. Chakaravarthy;Himanshu Gupta;Prasan Roy;Mukesh Mohania
#t 2006
#c 4
#% 102781
#% 157753
#% 287631
#% 387427
#% 397418
#% 479956
#% 660011
#% 697105
#% 769877
#% 769884
#% 799737
#% 800590
#% 830526
#% 830529
#% 835453
#% 838494
#% 864415
#% 864416
#% 1015325
#! Faced with growing knowledge management needs, enterprises are increasingly realizing the importance of interlinking critical business information distributed across structured and unstructured data sources. We present a novel system, called EROCS, for linking a given text document with relevant structured data. EROCS views the structured data as a predefined set of "entities" and identifies the entities that best match the given document. EROCS also embeds the identified entities in the document, effectively creating links between the structured data and segments within the document. Unlike prior approaches, EROCS identifies such links even when the relevant entity is not explicitly mentioned in the document. EROCS uses an efficient algorithm that performs this task keeping the amount of information retrieved from the database at a minimum. Our evaluation shows that EROCS achieves high accuracy with reasonable overheads.

#index 893144
#* Meaningful labeling of integrated query interfaces
#@ Eduard C. Dragut;Clement Yu;Weiyi Meng
#t 2006
#c 4
#% 22948
#% 85086
#% 158907
#% 273914
#% 442861
#% 458607
#% 481923
#% 529190
#% 587740
#% 654457
#% 765409
#% 765410
#% 765433
#% 769890
#% 783705
#% 783791
#% 810021
#% 824659
#% 864431
#% 864433
#% 1015284
#% 1015326
#% 1683872
#! The contents of Web databases are accessed through queries formulated on complex user interfaces. In many domains of interest (e.g. Auto) users are interested in obtaining information from alternative sources. Thus, they have to access many individual Web databases via query interfaces. We aim to construct automatically a well-designed query interface that integrates a set of interfaces in the same domain. This will permit users to access information uniformly from multiple sources. Earlier research in this area includes matching attributes across multiple query interfaces in the same domain and grouping related attributes. In this paper, we investigate the naming of the attributes in the integrated query interface. We provide a set of properties which are required in order to have consistent labels for the attributes within an integrated interface so that users have no difficulty in understanding it. Based on these properties, we design algorithms to systematically label the attributes. Experimental results on seven domains validate our theoretical study. In the process of naming attributes, a set of logical inference rules among the textual labels is discovered. These inferences are also likely to be applicable to other integration problems sensitive to naming: e.g., HTML forms, HTML tables or concept hierarchies in the semantic Web.

#index 893145
#* GORDIAN: efficient and scalable discovery of composite keys
#@ Yannis Sismanis;Paul Brown;Peter J. Haas;Berthold Reinwald
#t 2006
#c 4
#% 115608
#% 131546
#% 189872
#% 210160
#% 240222
#% 247593
#% 248822
#% 333946
#% 333947
#% 333986
#% 384978
#% 397369
#% 397371
#% 397388
#% 420053
#% 443390
#% 458275
#% 464199
#% 480803
#% 579314
#% 864426
#% 993933
#% 1015256
#% 1015310
#% 1016174
#% 1016225
#% 1016266
#% 1729914
#! Identification of (composite) key attributes is of fundamental importance for many different data management tasks such as data modeling, data integration, anomaly detection, query formulation, query optimization, and indexing. However, information about keys is often missing or incomplete in many real-world database scenarios. Surprisingly, the fundamental problem of automatic key discovery has received little attention in the existing literature. Existing solutions ignore composite keys, due to the complexity associated with their discovery. Even for simple keys, current algorithms take a brute-force approach; the resulting exponential CPU and memory requirements limit the applicability of these methods to small datasets. In this paper, we describe GORDIAN, a scalable algorithm for automatic discovery of keys in large datasets, including composite keys. GORDIAN can provide exact results very efficiently for both real-world and synthetic datasets. GORDIAN can be used to find (composite) key attributes in any collection of entities, e.g., key column-groups in relational data, or key leaf-node sets in a collection of XML documents with a common schema. We show empirically that GORDIAN can be combined with sampling to efficiently obtain high quality sets of approximate keys even in very large datasets.

#index 893146
#* An integrated approach to recovery and high availability in an updatable, distributed data warehouse
#@ Edmond Lau;Samuel Madden
#t 2006
#c 4
#% 4619
#% 55399
#% 107720
#% 114582
#% 116069
#% 201869
#% 317987
#% 403195
#% 411554
#% 411707
#% 427195
#% 464056
#% 466947
#% 481590
#% 602675
#% 602806
#% 617478
#% 717164
#% 824697
#% 875026
#% 963864
#% 1541196
#! Any highly available data warehouse will use some form of data replication to tolerate machine failures. In this paper, we demonstrate that we can leverage this data redundancy to build an integrated approach to recovery and high availability. Our approach, called HARBOR, revives a crashed site by querying remote, online sites for missing updates and uses timestamps to determine which tuples need to be copied or updated. HARBOR does not require a stable log, recovers without quiescing the system, allows replicated data to be stored non-identically, and is simpler than a log-based recovery algorithm.We compare the runtime overhead and recovery performance of HARBOR to those of two-phase commit and ARIES, the gold standard for log-based recovery, on a three-node distributed database system. Our experiments demonstrate that HARBOR suffers lower runtime overhead, has recovery performance comparable to ARIES's, and can tolerate the fault of a worker and efficiently bring it back online.

#index 893147
#* Lazy database replication with snapshot isolation
#@ Khuzaima Daudjee;Kenneth Salem
#t 2006
#c 4
#% 9241
#% 102804
#% 201869
#% 201928
#% 210179
#% 237196
#% 237197
#% 240016
#% 256717
#% 286967
#% 323980
#% 335500
#% 397402
#% 476793
#% 481584
#% 509531
#% 635834
#% 745516
#% 745536
#% 765469
#% 793894
#% 800544
#% 810043
#% 814649
#% 824698
#% 839564
#% 864422
#% 875049
#! Snapshot isolation is a popular transactional isolation level in database systems. Several replication techniques based on snapshot isolation have recently been proposed. These proposals, however, do not fully leverage the local concurrency controls that provide snapshot isolation. Furthermore, guaranteeing snapshot isolation in lazy replicated systems may result in transaction inversions, which happen when transactions see stale data. Strong snapshot isolation, which is provided in centralized database servers, avoids transaction inversions but is expensive to provide in a lazy replicated system. In this paper, we show how snapshot isolation can be maintained in lazy replicated systems while taking full advantage of the local concurrency controls. We propose strong session snapshot isolation, a correctness criterion that prevents transaction inversions. We show how strong session snapshot isolation can be implemented efficiently in a lazy replicated database system. Through performance studies, we quantify the cost of implementing our techniques in lazy replicated systems.

#index 893148
#* Delay aware querying with seaweed
#@ Dushyanth Narayanan;Austin Donnelly;Richard Mortier;Antony Rowstron
#t 2006
#c 4
#% 152947
#% 227883
#% 300167
#% 300179
#% 303701
#% 340297
#% 397353
#% 479984
#% 481296
#% 505869
#% 569762
#% 765444
#% 766991
#% 770901
#% 770903
#% 810008
#% 810073
#% 821989
#% 824706
#% 824708
#% 963460
#% 978767
#% 1015281
#% 1015282
#% 1016165
#% 1016202
#% 1016208
#! Large highly distributed data sets are poorly supported by current query technologies. Applications such as endsystem-based network management are characterized by data stored on large numbers of endsystems, with frequent local updates and relatively infrequent global one-shot queries.The challenges are scale (103 to 109 endsystems)and endsystem unavailability. In such large systems, a significant fraction of endsystems and their data will be unavailable at any given time. Existing methods to provide high data availability despite endsystem unavailability involve centralizing, redistributing or replicating the data. At large scale these methods are not scalable.We advocate a design that trades query delay for completeness, incrementally returning results as endsystems become available. We also introduce the idea of completeness prediction, which provides the user with explicit feedback about this delay/completeness trade-off. Completeness prediction is based on replication of compact data summaries and availability models. This metadata is orders of magnitude smaller than the data.Seaweed is a scalable query infrastructure supporting incremental results, online in-network aggregation and completeness prediction. It is built on a distributed hash table (DHT) but unlike previous DHT based approaches it does not redistribute data across the network. It exploits the DHT infrastructure for failure resilient metadata replication, query dissemination, and result aggregation. We analytically compare Seaweed's scalability against other approaches and also evaluate the Seaweed prototype running on a large-scale network simulator driven by real-world traces.

#index 893149
#* Full disjunctions: polynomial-delay iterators in action
#@ Sara Cohen;Itzhak Fadida;Yaron Kanza;Benny Kimelfeld;Yehoshua Sagiv
#t 2006
#c 4
#% 4279
#% 39702
#% 70370
#% 86947
#% 172933
#% 213983
#% 220425
#% 289425
#% 393907
#% 463276
#% 576099
#% 599549
#% 765457
#% 809242
#! Full disjunctions are an associative extension of the outer-join operator to an arbitrary number of relations. Their main advantage is the ability to maximally combine data from different relations while preserving all the original information. An algorithm for efficiently computing full disjunctions is presented. This algorithm is superior to previous ones in three ways. First, it is the first algorithm that computes a full disjunction with a polynomial delay between tuples. Hence, it can be implemented as an iterator that produces a stream of tuples, which is important in many cases (e.g., pipelined query processing and Web applications). Second, the total runtime is linear in the size of the output. Third, the algorithm employs a novel optimization that divides the relation schemes into biconnected components, uses a separate iterator for each component and applies outerjoins whenever possible. Combining efficiently full disjunctions with standard SQL operators is discussed. Experiments show the superiority of our algorithm over the state of the art.

#index 893150
#* The spatial skyline queries
#@ Mehdi Sharifzadeh;Cyrus Shahabi
#t 2006
#c 4
#% 235114
#% 322381
#% 465167
#% 480671
#% 800555
#% 806212
#% 814650
#% 864453
#% 993954
#% 1704004
#! In this paper, for the first time, we introduce the concept of Spatial Skyline Queries (SSQ). Given a set of data points P and a set of query points Q each data point has a number of derived spatial attributes each of which is the point's distance to a query point. An SSQ retrieves those points of P which are not dominated by any other point in P considering their derived spatial attributes. The main difference with the regular skyline query is that this spatial domination depends on the location of the query points Q SSQ has application in several domains such as emergency response and online maps. The main intuition and novelty behind our approaches is that we exploit the geometric properties of the SSQ problem space to avoid the exhaustive examination of all the point pairs in P and Q. Consequently, we reduce the complexity of SSQ search from O(|P|2|Q|) to O(|S|2|C|+√|P|), where |S| and |C| are the solution size and the number of vertices of the convex hull of Q, respectively.We propose two algorithms, B2S2 and VS2, for static query points and one algorithm, VCS2, for streaming Q whose points change location over time (e.g., are mobile). VCS2 exploits the pattern of change in Q to avoid unnecessary re-computation of the skyline and hence efficiently perform updates. Our extensive experiments using real-world datasets verify that both R-tree-based B2S2 and Voronoi-based VS2 out perform the best competitor approach in terms of processing time by a wide margin (4-6 times better in most cases).

#index 893151
#* The new Casper: query processing for location services without compromising privacy
#@ Mohamed F. Mokbel;Chi-Yin Chow;Walid G. Aref
#t 2006
#c 4
#% 268785
#% 330612
#% 346201
#% 421124
#% 452685
#% 458865
#% 545454
#% 576761
#% 576762
#% 578871
#% 654448
#% 717245
#% 745503
#% 755170
#% 755205
#% 765453
#% 771228
#% 772835
#% 800515
#% 800571
#% 800618
#% 801690
#% 810011
#% 810048
#% 810061
#% 811896
#% 813973
#% 864406
#% 864414
#% 864458
#% 864464
#% 911803
#% 1016188
#% 1016275
#% 1112971
#% 1719090
#% 1720764
#! This paper tackles a major privacy concern in current location-based services where users have to continuously report their locations to the database server in order to obtain the service. For example, a user asking about the nearest gas station has to report her exact location. With untrusted servers, reporting the location information may lead to several privacy threats. In this paper, we present Casper1; a new framework in which mobile and stationary users can entertain location-based services without revealing their location information. Casper consists of two main components, the location anonymizer and the privacy-aware query processor. The location anonymizer blurs the users' exact location information into cloaked spatial regions based on user-specified privacy requirements. The privacy-aware query processor is embedded inside the location-based database server in order to deal with the cloaked spatial areas rather than the exact location information. Experimental results show that Casper achieves high quality location-based services while providing anonymity for both data and queries.

#index 893152
#* Providing resiliency to load variations in distributed stream processing
#@ Ying Xing;Jeong-Hyon Hwang;Uǧur Çetintemel;Stan Zdonik
#t 2006
#c 4
#% 36286
#% 116388
#% 210199
#% 243929
#% 412350
#% 654508
#% 654510
#% 726621
#% 800584
#% 963595
#! Scalability in stream processing systems can be achieved by using a cluster of computing devices. The processing burden can, thus, be distributed among the nodes by partitioning the query graph. The specific operator placement plan can have a huge impact on performance. Previous work has focused on how to move query operators dynamically in reaction to load changes in order to keep the load balanced. Operator movement is too expensive to alleviate short-term bursts; moreover, some systems do not support the ability to move operators dynamically. In this paper, we develop algorithms for selecting an operator placement plan that is resilient to changes in load. In other words, we assume that operators cannot move, therefore, we try to place them in such a way that the resulting system will be able to withstand the largest set of input rate combinations. We call this a resilient placement.This paper first formalizes the problem for operators that exhibit linear load characteristics (e.g., filter, aggregate), and introduces a resilient placement algorithm. We then show how we can extend our algorithm to take advantage of additional workload information (such as known minimum input stream rates). We further show how this approach can be extended to operators that exhibit non-linear load characteristics (e.g., join). Finally, we present prototype- and simulation-based experiments that quantify the benefits of our approach over existing techniques using real network traffic traces.

#index 893153
#* Load shedding in stream databases: a control-based approach
#@ Yi-Cheng Tu;Song Liu;Sunil Prabhakar;Bin Yao
#t 2006
#c 4
#% 102933
#% 188026
#% 304066
#% 393051
#% 397352
#% 397354
#% 430940
#% 654462
#% 654488
#% 654510
#% 659965
#% 726621
#% 745534
#% 772841
#% 864443
#% 993949
#% 1015280
#% 1698928
#% 1848989
#! In Data Stream Management Systems (DSMSs), query processing has to meet various Quality-of-Service (QoS) requirements. In many data stream applications, processing delay is the most critical quality requirement since the value of query results decreases dramatically over time. The ability to remain within a desired level of delay is significantly hampered under situations of overloading, which are common in data stream systems. When overloaded, DSMSs employ load shedding in order to meet quality requirements and keep pace with the high rate of data arrivals. Data stream applications are extremely dynamic due to bursty data arrivals and time-varying data processing costs. Current approaches ignore system status information in decision-making and consequently are unable to achieve desired control of quality under dynamic load. In this paper, we present a quality management framework that leverages well studied feedback control techniques. We discuss the design and implementation of such a framework in a real DSMS - the Borealis stream manager. Our data management framework is built on the advantages of system identification and rigorous controller analysis. Experimental results show that our solution achieves significantly fewer QoS (delay) violations with the same or lower level of data loss, as compared to current strategies utilized in DSMSs. It is also robust and bears negligible computational overhead.

#index 893154
#* Window-aware load shedding for aggregation queries over data streams
#@ Nesime Tatbul;Stan Zdonik
#t 2006
#c 4
#% 227883
#% 333926
#% 397354
#% 480306
#% 480628
#% 480810
#% 578391
#% 578560
#% 654444
#% 654508
#% 654510
#% 726621
#% 742565
#% 745534
#% 765436
#% 799140
#% 800504
#% 810033
#% 810095
#% 864593
#% 993949
#% 1015280
#% 1016156
#% 1016169
#% 1016170
#! Data stream management systems may be subject to higher input rates than their resources can handle. When overloaded, the system must shed load in order to maintain low-latency query results. In this paper, we describe a load shedding technique for queries consisting of one or more aggregate operators with sliding windows. We introduce a new type of drop operator, called a "Window Drop". This operator is aware of the window properties (i.e., window size and window slide) of its downstream aggregate operators in the query plan. Accordingly, it logically divides the input stream into windows and probabilistically decides which windows to drop. This decision is further encoded into tuples by marking the ones that are disallowed from starting new windows. Unlike earlier approaches, our approach preserves integrity of windows throughout a query plan, and always delivers subsets of original query answers with minimal degradation in result quality.

#index 893155
#* Mining frequent closed cubes in 3D datasets
#@ Liping Ji;Kian-Lee Tan;Anthony K. H. Tung
#t 2006
#c 4
#% 300124
#% 342666
#% 420063
#% 463903
#% 465003
#% 479627
#% 481290
#% 631926
#% 729933
#% 729984
#% 765132
#% 769919
#% 810066
#! In this paper, we introduce the concept of frequent closed cube (FCC), which generalizes the notion of 2D frequent closed pattern to 3D context. We propose two novel algorithms to mine FCCs from 3D datasets. The first scheme is a Representative Slice Mining (RSM) framework that can be used to extend existing 2D FCP mining algorithms for FCC mining. The second technique, called CubeMiner, is a novel algorithm that operates on the 3D space directly. We have implemented both schemes, and evaluated their performance on both real and synthetic datasets. The experimental results show that the RSM-based scheme is efficient when one of the dimensions is small, while CubeMiner is superior otherwise.

#index 893156
#* Efficient incremental maintenance of data cubes
#@ Ki Yong Lee;Myoung Ho Kim
#t 2006
#c 4
#% 25998
#% 136740
#% 152928
#% 210182
#% 227868
#% 227869
#% 273916
#% 300199
#% 333925
#% 464215
#% 479450
#% 481288
#% 481951
#% 580225
#% 631946
#% 745506
#% 810029
#! The data cube provides users with aggregated results that are group-bys for all possible combinations of dimension attributes. When the number of dimension attributes is n, the data cube computes 2n group-bys, each of which is called a cuboid. A data cube is often precomputed and stored as materialized views in data warehouses. The data cube needs to be updated when source relations change. The incremental maintenance of a data cube is to compute and propagate only changes of source relations rather than recompute the entire data cube from the source relations.To maintain a data cube incrementally, previous methods compute a delta cube which represents the change of the data cube. We call a cuboid in a delta cube a delta cuboid. For a data cube with 2n cuboids, a delta cube consists of 2n delta cuboids. Thus, as the number of dimension attributes increases, the cost of computing the delta cube increases significantly. In this paper, we propose an incremental maintenance method for data cubes that can maintain a data cube by using only (n ⌈n/2⌉) delta cuboids. As a result, the cost of computing delta cuboids is substantially reduced. Through various experiments, we show the performance advantages of our method over the previous methods.

#index 893157
#* Flowcube: constructing RFID flowcubes for multi-dimensional analysis of commodity flows
#@ Hector Gonzalez;Jiawei Han;Xiaolei Li
#t 2006
#c 4
#% 210182
#% 273916
#% 333925
#% 342666
#% 459021
#% 466593
#% 466716
#% 479646
#% 481588
#% 481758
#% 481951
#% 501226
#% 644230
#% 749034
#% 864470
#% 1015294
#% 1016228
#! With the advent of RFID (Radio Frequency Identication) technology, manufacturers, distributors, and retailers will be able to track the movement of individual objects throughout the supply chain. The volume of data generated by a typical RFID application will be enormous as each item will generate a complete history of all the individual locations that it occupied at every point in time, possibly from a specific production line at a given factory, passing through multiple warehouses, and all the way to a particular checkout counter in a store. The movement trails of such RFID data form gigantic commodity flowgraph representing the locations and durations of the path stages traversed by each item. This commodity flow contains rich multi-dimensional information on the characteristics, trends, changes and outliers of commodity movements.In this paper, we propose a method to construct a warehouse of commodity flows, called flowcube. As in standard OLAP, the model will be composed of cuboids that aggregate item flows at a given abstraction level. The flowcube differs from the traditional data cube in two major ways. First, the measure of each cell will not be a scalar aggregate but a commodity flowgraph that captures the major movement trends and significant deviations of the items aggregated in the cell. Second, each flowgraph itself can be viewed at multiple levels by changing the level of abstraction of path stages. In this paper, we motivate the importance of the model, and present an efficient method to compute it by (1) performing simultaneous aggregation of paths to all interesting abstraction levels, (2) pruning low support path segments along the item and path stage abstraction lattices, and (3) compressing the cube by removing rarely occurring cells, and cells whose commodity flows can be inferred from higher level cells.

#index 893158
#* Approximate encoding for direct access and query processing over compressed bitmaps
#@ Tan Apaydin;Guadalupe Canahuate;Hakan Ferhatosmanoglu;Ali Saman Tosun
#t 2006
#c 4
#% 69791
#% 114577
#% 118767
#% 227861
#% 248814
#% 273904
#% 273905
#% 307424
#% 316523
#% 322884
#% 340164
#% 342735
#% 432578
#% 446419
#% 466953
#% 479808
#% 480329
#% 504155
#% 564096
#% 571294
#% 617842
#% 655987
#% 725365
#% 800526
#% 853041
#% 866981
#% 1016130
#% 1016131
#! Bitmap indices have been widely and successfully used in scientific and commercial databases. Compression techniques based on run-length encoding are used to improve the storage performance. However, these techniques introduce significant overheads in query processing even when only a few rows are queried. We propose a new bitmap encoding scheme based on multiple hashing, where the bitmap is kept in a compressed form, and can be directly accessed without decompression. Any subset of rows and/or columns can be retrieved efficiently by reconstructing and processing only the necessary subset of the bitmap. The proposed scheme provides approximate results with a trade-off between the amount of space and the accuracy. False misses are guaranteed not to occur, and the false positive rate can be estimated and controlled. We show that query execution is significantly faster than WAH-compressed bitmaps, which have been previously shown to achieve the fastest query response times. The proposed scheme achieves accurate results (90%-100%) and improves the speed of query processing from 1 to 3 orders of magnitude compared to WAH.

#index 893159
#* How to wring a table dry: entropy compression of relations and querying of compressed relations
#@ Vijayshankar Raman;Garret Swart
#t 2006
#c 4
#% 38374
#% 115608
#% 193923
#% 268186
#% 286258
#% 333954
#% 408466
#% 464843
#% 480821
#% 481424
#% 571082
#% 824697
#% 1015332
#! We present a method to compress relations close to their entropy while still allowing efficient queries. Column values are encoded into variable length codes to exploit skew in their frequencies. The codes in each tuple are concatenated and the resulting tuplecodes are sorted and delta-coded to exploit the lack of ordering in a relation. Correlation is exploited either by co-coding correlated columns, or by using a sort order that leverages the correlation. We prove that this method leads to near-optimal compression (within 4.3 bits/tuple of entropy), and in practice, we obtain up to a 40 fold compression ratio on vertical partitions tuned for TPC-H queries.We also describe initial investigations into efficient querying over compressed data. We present a novel Huffman coding scheme, called segregated coding, that allows range and equality predicates on compressed data, without accessing the full dictionary. We also exploit the delta coding to speed up scans, by reusing computations performed on nearly identical records. Initial results from a prototype suggest that with these optimizations, we can efficiently scan, tokenize and apply predicates on compressed relations.

#index 893160
#* Compact histograms for hierarchical identifiers
#@ Frederick Reiss;Minos Garofalakis;Joseph M. Hellerstein
#t 2006
#c 4
#% 201921
#% 210190
#% 248822
#% 299982
#% 333947
#% 378404
#% 397414
#% 420053
#% 479648
#% 480465
#% 760987
#% 801684
#% 824685
#% 824686
#% 824687
#! Distributed monitoring applications often involve streams of unique identifiers (UIDs) such as IP addresses or RFID tag IDs. An important class of query for such applications involves partitioning the UIDs into groups using a large lookup table; the query then performs aggregation over the groups. We propose using histograms to reduce bandwidth utilization in such settings, using a histogram partitioning function as a compact representation of the lookup table. We investigate methods for constructing histogram partitioning functions for lookup tables over unique identifiers that form a hierarchy of contiguous groups, as is the case with network addresses and several other types of UID. Each bucket in our histograms corresponds to a subtree of the hierarchy. We develop three novel classes of partitioning functions for this domain, which vary in their structure, construction time, and estimation accuracy.Our approach provides several advantages over previous work. We show that optimal instances of our partitioning functions can be constructed efficiently from large lookup tables. The partitioning functions are also compact, with each partition represented by a single identifier. Finally, our algorithms support minimizing any error metric that can be expressed as a distributive aggregate; and they extend naturally to multiple hierarchical dimensions. In experiments on real-world network monitoring data, we show that our histograms provide significantly higher accuracy per bit than existing techniques.

#index 893161
#* LB_Keogh supports exact indexing of shapes under rotation invariance with arbitrary representations and distance measures
#@ Eamonn Keogh;Li Wei;Xiaopeng Xi;Sang-Hee Lee;Michail Vlachos
#t 2006
#c 4
#% 49238
#% 401848
#% 443600
#% 522710
#% 577221
#% 729931
#% 741404
#% 775619
#% 812562
#% 838404
#% 844343
#% 993965
#% 1016194
#% 1046749
#% 1113090
#% 1378406
#% 1685143
#% 1854406
#% 1858193
#! The matching of two-dimensional shapes is an important problem with applications in domains as diverse as biometrics, industry, medicine and anthropology. The distance measure used must be invariant to many distortions, including scale, offset, noise, partial occlusion, etc. Most of these distortions are relatively easy to handle, either in the representation of the data or in the similarity measure used. However rotation invariance seems to be uniquely difficult. Current approaches typically try to achieve rotation invariance in the representation of the data, at the expense of discrimination ability, or in the distance measure, at the expense of efficiency. In this work we show that we can take the slow but accurate approaches and dramatically speed them up. On real world problems our technique can take current approaches and make them four orders of magnitude faster, without false dismissals. Moreover, our technique can be used with any of the dozens of existing shape representations and with all the most popular distance measures including Euclidean distance, Dynamic Time Warping and Longest Common Subsequence.

#index 893162
#* Distance indexing on road networks
#@ Haibo Hu;Dik Lun Lee;Victor C. S. Lee
#t 2006
#c 4
#% 410276
#% 413797
#% 443105
#% 729850
#% 824723
#% 839701
#% 1015321
#% 1016199
#% 1688257
#% 1688300
#! The processing of kNN and continuous kNN queries on spatial network databases (SNDB) has been intensively studied recently. However, there is a lack of systematic study on the computation of network distances, which is the most fundamental difference between a road network and a Euclidean space. Since the online Dijkstra's algorithm has been shown to be efficient only for short distances, we propose an efficient index, called distance signature, for distance computation and query processing over long distances. Distance signature discretizes the distances between objects and network nodes into categories and then encodes these categories. To minimize the storage and search costs, we present the optimal category partition, and the encoding and compression algorithms for the signatures, based on a simplified network topology. By mathematical analysis and experimental study, we showed that the signature index is efficient and robust for various data distributions, query workloads, parameter settings and network updates.

#index 893163
#* Reference-based indexing of sequence databases
#@ Jayendra Venkateswaran;Deepak Lachwani;Tamer Kahveci;Christopher Jermaine
#t 2006
#c 4
#% 2324
#% 143306
#% 227937
#% 235941
#% 271801
#% 281750
#% 289010
#% 413574
#% 465160
#% 479462
#% 480482
#% 480484
#% 480654
#% 545961
#% 814646
#% 832796
#% 842802
#% 1015330
#! We consider the problem of similarity search in a very large sequence database with edit distance as the similarity measure. Given limited main memory, our goal is to develop a reference-based index that reduces the number of costly edit distance computations in order to answer a query. The idea in reference-based indexing is to select a small set of reference sequences that serve as a surrogate for the other sequences in the database. We consider two novel strategies for selecting references as well as a new strategy for assigning references to database sequences. Our experimental results show that our selection and assignment methods far outperform competitive methods. For example, our methods prune up to 20 times as many sequences as the Omni method, and as many as 30 times as many sequences as frequency vectors. Our methods also scale nicely for databases containing many and/or very long sequences.

#index 893164
#* Efficient exact set-similarity joins
#@ Arvind Arasu;Venkatesh Ganti;Raghav Kaushik
#t 2006
#c 4
#% 201889
#% 214073
#% 248801
#% 273908
#% 281245
#% 387427
#% 479973
#% 480463
#% 480654
#% 569755
#% 577238
#% 632029
#% 654454
#% 654467
#% 765463
#% 810014
#% 810020
#% 864392
#% 993980
#! Given two input collections of sets, a set-similarity join (SSJoin) identifies all pairs of sets, one from each collection, that have high similarity. Recent work has identified SSJoin as a useful primitive operator in data cleaning. In this paper, we propose new algorithms for SSJoin. Our algorithms have two important features: They are exact, i.e., they always produce the correct answer, and they carry precise performance guarantees. We believe our algorithms are the first to have both features; previous algorithms with performance guarantees are only probabilistically approximate. We demonstrate the effectiveness of our algorithms using a thorough experimental evaluation over real-life and synthetic data sets.

#index 893165
#* Analysis of two existing and one new dynamic programming algorithm for the generation of optimal bushy join trees without cross products
#@ Guido Moerkotte;Thomas Neumann
#t 2006
#c 4
#% 210166
#% 315024
#% 410276
#% 411554
#% 480430
#% 705675
#! Two approaches to derive dynamic programming algorithms for constructing join trees are described in the literature. We show analytically and experimentally that these two variants exhibit vastly diverging runtime behaviors for different query graphs. More specifically, each variant is superior to the other for one kind of query graph (chain or clique), but fails for the other. Moreover, neither of them handles star queries well. This motivates us to derive an algorithm that is superior to the two existing algorithms because it adapts to the search space implied by the query graph.

#index 893166
#* Containment of conjunctive object meta-queries
#@ Andrea Cali;Michael Kifer
#t 2006
#c 4
#% 58354
#% 189739
#% 237181
#% 248026
#% 275922
#% 287339
#% 287631
#% 299968
#% 384978
#% 465057
#% 484323
#% 519557
#% 577335
#% 599549
#% 935898
#% 1669512
#% 1720600
#! We consider the problem of query containment over an object data model derived from F-logic. F-logic has generated considerable interest commercially, in the academia, and within various standardization efforts as a means for building ontologies and for reasoning on the Semantic Web. Solution to the containment problem for F-logic queries can help with query optimization as well as the classification problem in information integration systems. An important property of F-logic queries, which sets them apart from database queries, is that they can mix the data-level and the meta-level in simple and useful ways. This means that such queries may refer not only to data but also schema information. To the best of our knowledge, the containment problem for such queries has not been considered in the literature. We show that, even for queries over meta-information together with data, this problem is decidable for non-recursive conjunctive queries. We also provide relevant complexity results.

#index 893167
#* ULDBs: databases with uncertainty and lineage
#@ Omar Benjelloun;Anish Das Sarma;Alon Halevy;Jennifer Widom
#t 2006
#c 4
#% 663
#% 64413
#% 94459
#% 215225
#% 235023
#% 243720
#% 291859
#% 318704
#% 378401
#% 384978
#% 397406
#% 442830
#% 479754
#% 480102
#% 480418
#% 481128
#% 577523
#% 632040
#% 810098
#% 810115
#% 824718
#% 824728
#% 824764
#% 864394
#% 874971
#% 893189
#% 1016201
#% 1016203
#% 1016204
#% 1700137
#! This paper introduces ULDBs, an extension of relational databases with simple yet expressive constructs for representing and manipulating both lineage and uncertainty. Uncertain data and data lineage are two important areas of data management that have been considered extensively in isolation, however many applications require the features in tandem. Fundamentally, lineage enables simple and consistent representation of uncertain data, it correlates uncertainty in query results with uncertainty in the input data, and query processing with lineage and uncertainty together presents computational benefits over treating them separately.We show that the ULDB representation is complete, and that it permits straightforward implementation of many relational operations. We define two notions of ULDB minimality--data-minimal and lineage-minimal--and study minimization of ULDB representations under both notions. With lineage, derived relations are no longer self-contained: their uncertainty depends on uncertainty in the base data. We provide an algorithm for the new operation of extracting a database subset in the presence of interconnected uncertainty. Finally, we show how ULDBs enable a new approach to query processing in probabilistic databases.ULDBs form the basis of the Trio system under development at Stanford.

#index 893168
#* Creating probabilistic databases from information extraction models
#@ Rahul Gupta;Sunita Sarawagi
#t 2006
#c 4
#% 209725
#% 235023
#% 277467
#% 277470
#% 278104
#% 283136
#% 333943
#% 442830
#% 464434
#% 480418
#% 654487
#% 769877
#% 769884
#% 793254
#% 810098
#% 816181
#% 824733
#% 864394
#% 874707
#% 876044
#% 916785
#% 1016201
#% 1814768
#! Many real-life applications depend on databases automatically curated from unstructured sources through imperfect structure extraction tools. Such databases are best treated as imprecise representations of multiple extraction possibli-ties. State-of-the-art statistical models of extraction provide a sound probability distribution over extractions but are not easy to represent and query in a relational framework. In this paper we address the challenge of approximating such distributions as imprecise data models. In particular, we investigate a model that captures both row-level and column-level uncertainty and show that this representation provides significantly better approximation compared to models that use only row or only column level uncertainty. We present efficient algorithms for finding the best approximating parameters for such a model: our algorithm exploits the structure of the model to avoid enumerating the exponential number of extraction possibilities.

#index 893169
#* Quality views: capturing and exploiting the user perspective on data quality
#@ Paolo Missier;Suzanne Embury;Mark Greenwood;Alun Preece;Binling Jin
#t 2006
#c 4
#% 382342
#% 480325
#% 480499
#% 632079
#% 659991
#% 767431
#% 824755
#% 832825
#% 833053
#% 912094
#% 1667791
#! There is a growing awareness among life scientists of the variability in quality of the data in public repositories, and of the threat that poor data quality poses to the validity of experimental results. No standards are available, however, for computing quality levels in this data domain. We argue that data processing environments used by life scientists should feature facilities for expressing and applying quality-based, personal data acceptability criteria.We propose a framework for the specification of users' quality processing requirements, called quality views. These views are compiled and semi-automatically embedded within the data processing environment. The result is a quality management toolkit that promotes rapid prototyping and reuse of quality components. We illustrate the utility of the framework by showing how it can be deployed within Taverna, a scientific workflow management tool, and applied to actual workflows for data analysis in proteomics.

#index 893170
#* Automatic extraction of dynamic record sections from search engine result pages
#@ Hongkun Zhao;Weiyi Meng;Clement Yu
#t 2006
#c 4
#% 120649
#% 248808
#% 271065
#% 273925
#% 275915
#% 322619
#% 322836
#% 330784
#% 344448
#% 397605
#% 480126
#% 480479
#% 480648
#% 480824
#% 577319
#% 654469
#% 660272
#% 705442
#% 724635
#% 729978
#% 754102
#% 754108
#% 766464
#% 783791
#% 805845
#% 805846
#% 838491
#% 1683893
#! A search engine returned result page may contain search results that are organized into multiple dynamically generated sections in response to a user query. Furthermore, such a result page often also contains information irrelevant to the query, such as information related to the hosting site of the search engine. In this paper, we present a method to automatically generate wrappers for extracting search result records from all dynamic sections on result pages returned by search engines. This method has the following novel features: (1) it aims to explicitly identify all dynamic sections, including those that are not seen on sample result pages used to generate the wrapper, and (2) it addresses the issue of correctly differentiating sections and records. Experimental results indicate that this method is very promising. Automatic search result record extraction is critical for applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling.

#index 893171
#* Trustworthy keyword search for regulatory-compliant records retention
#@ Soumyadeb Mitra;Windsor W. Hsu;Marianne Winslett
#t 2006
#c 4
#% 1921
#% 10392
#% 86532
#% 169849
#% 290703
#% 296646
#% 393907
#% 459938
#% 463553
#% 480948
#% 481439
#% 571296
#% 655485
#% 747117
#% 810041
#% 841404
#! Recent litigation and intense regulatory focus on secure retention of electronic records have spurred a rush to introduce Write-Once-Read-Many (WORM) storage devices for retaining business records such as electronic mail. However, simply storing records in WORM storage is insuffcient to ensure that the records are trustworthy, i.e., able to provide irrefutable proof and accurate details of past events. Specifically, some form of index is needed for timely access to the records, but unless the index is maintained securely, the records can in effect be hidden or altered, even if stored in WORM storage. In this paper, we systematically analyze the requirements for establishing a trustworthy inverted index to enable keyword-based search queries. We propose a novel scheme for effcient creation of such an index and demonstrate, through extensive simulations and experiments with an enterprise keyword search engine, that the scheme can achieve online update speeds while maintaining good query performance. In addition, we present a secure index structure for multi-keyword queries that supports insert, lookup and range queries in time logarithmic in the number of documents.

#index 893172
#* Efficient detection of empty-result queries
#@ Gang Luo
#t 2006
#c 4
#% 6804
#% 36117
#% 45495
#% 59350
#% 118773
#% 198465
#% 227883
#% 248032
#% 248807
#% 273901
#% 279164
#% 287497
#% 300166
#% 333947
#% 333951
#% 333965
#% 397371
#% 399762
#% 442712
#% 463696
#% 464056
#% 480149
#% 480158
#% 480463
#% 481128
#% 571169
#% 572311
#% 765427
#% 765467
#% 994017
#% 1015264
#% 1016220
#! Frequently encountered in query processing, empty query results usually do not provide users with much useful information. Yet, users might still have to wait for a long time before they disappointingly realize that their results are empty. To significantly reduce such unfavorable delays, in this paper, we propose a novel method to quickly detect, without actual execution, those queries that will return empty results. Our key idea is to remember and reuse the results from previously-executed, empty-result queries. These results are stored in the form of so-called atomic query parts so that the (partial) results from multiple queries can be combined together to handle a new query without incurring much overhead. To increase our chance of detecting empty-result queries with only a limited storage, our method (1) stores the most "valuable" information about empty-result queries, (2) removes redundant information among different empty-result queries, (3) continuously updates the stored information to adapt to the current query pattern, and (4) utilizes a set of special properties of empty results. We evaluate the efficiency of our method through a theoretical analysis and an initial implementation in PostgreSQL. The results show that our method has low overhead and can often successfully avoid executing empty-result queries.

#index 893173
#* Cost-based query transformation in Oracle
#@ Rafi Ahmed;Allison Lee;Andrew Witkowski;Dinesh Das;Hong Su;Mohamed Zait;Thierry Cruanes
#t 2006
#c 4
#% 32878
#% 43161
#% 58376
#% 86943
#% 86947
#% 116043
#% 152940
#% 169843
#% 172889
#% 210207
#% 220425
#% 287005
#% 387508
#% 458550
#% 463735
#% 480091
#% 481288
#% 481293
#% 481604
#% 481608
#% 482115
#% 564426
#% 565457
#% 654445
#% 765457
#! This paper describes cost-based query transformation in Oracle relational database system, which is a novel phase in query optimization. It discusses a suite of heuristic- and cost-based transformations performed by Oracle. It presents the framework for cost-based query transformation, the need for such a framework, possible interactions among some of the transformation, and efficient algorithms for enumerating the search space of cost-based transformations. It describes a practical technique to combine cost-based transformations with a traditional physical optimizer. Some of the challenges of cost-based transformation are highlighted. Our experience shows that some transformations when performed in a cost-based manner lead to significant execution time improvements.

#index 893174
#* Query processing in the aqualogic data services platform
#@ Vinayak Borkar;Michael Carey;Dmitry Lychagin;Till Westmann;Daniel Engovatov;Nicola Onose
#t 2006
#c 4
#% 123997
#% 286831
#% 770332
#% 781453
#% 800087
#% 810076
#% 864649
#% 875028
#% 1561977
#! BEA recently introduced a new middleware product called the Aqua-Logic Data Services Platform (ALDSP). The purpose of ALDSP is to make it easy to design, develop, deploy, and maintain a data services layer in the world of service-oriented architecture (SOA). ALDSP provides a declarative foundation for building SOA applications and services that need to access and compose information from a range of enterprise data sources; this foundation is based on XML, XML Schema, and XQuery. This paper focuses on query processing in ALDSP, describing its overall query processing architecture, its query compiler and runtime system, its distributed query processing techniques, the translation of XQuery plan fragments into SQL when relational data sources are involved, and the production of lineage information to support updates. Several XQuery extensions that were added in support of requirements related to data services are also covered.

#index 893175
#* The making of TPC-DS
#@ Raghunath Othayoth Nambiar;Meikel Poess
#t 2006
#c 4
#% 328431
#% 377989
#% 385321
#% 397399
#% 741995
#% 1016216
#! For the last decade, the research community and the industry have used TPC-D and its successor TPC-H to evaluate performance of decision support technology. Recognizing a paradigm shift in the industry the Transaction Processing Performance Council has developed a new Decision Support benchmark, TPC-DS, expected to be released this year. From an ease of benchmarking perspective it is similar to past benchmarks. However, it adjusts for new technology and new approaches the industry has embarked on in recent years. This paper describes the main characteristics of TPC-DS, explains why some of the key decisions were made and which performance aspects of decision support system it measures.

#index 893176
#* Data mining with the SAP NetWeaver BI accelerator
#@ Thomas Legler;Wolfgang Lehner;Andrew Ross
#t 2006
#c 4
#% 152934
#% 172892
#% 201075
#% 273898
#% 273916
#% 290703
#% 300120
#% 420053
#% 434348
#% 443085
#% 481290
#% 481754
#% 824697
#! The new SAP NetWeaver Business Intelligence accelerator is an engine that supports online analytical processing. It performs aggregation in memory and in query runtime over large volumes of structured data. This paper first briefly describes the accelerator and its main architectural features, and cites test results that indicate its power. Then it describes in detail how the accelerator may be used for data mining. The accelerator can perform data mining in the same large repositories of data and using the same compact index structures that it uses for analytical processing. A first such implementation of data mining is described and the results of a performance evaluation are presented. Association rule mining in a distributed architecture was implemented with a variant of the BUC iceberg cubing algorithm. Test results suggest that useful online mining should be possible with wait times of less than 60 seconds on business data that has not been preprocessed.

#index 893177
#* Contest of XML lock protocols
#@ Michael Haustein;Theo Härder;Konstantin Luttenberger
#t 2006
#c 4
#% 365700
#% 403195
#% 411708
#% 413565
#% 482657
#% 541480
#% 659999
#% 742049
#% 765488
#% 805077
#% 810036
#% 994015
#! We explore and compare the performance behavior of lock protocols to be used in XML DBMSs (XDBMSs, for short) supporting typical XML document processing interfaces. In this paper, we outline 11 protocols proposed in the literature, highlight essential implementation concepts of our XDBMS and realize all of them in the same DBMS environment using so-called meta-synchronization. We design a framework for XML benchmarks including read and update transactions, run extensive empirical experiments which focus on the locking performance, and compare the results using various performance metrics. As a consequence, we can propose a group of protocols which won this practical contest under identical conditions.

#index 893178
#* Adaptive self-tuning memory in DB2
#@ Adam J. Storm;Christian Garcia-Arellano;Sam S. Lightstone;Yixin Diao;M. Surendra
#t 2006
#c 4
#% 152913
#% 210198
#% 248824
#% 479463
#% 481127
#% 481131
#% 481275
#% 631952
#% 632146
#% 732898
#% 743083
#% 765027
#% 994014
#% 1776656
#! DB2 for Linux, UNIX, and Windows Version 9.1 introduces the Self-Tuning Memory Manager (STMM), which provides adaptive self tuning of both database memory heaps and cumulative database memory allocation. This technology provides state-of-the-art memory tuning combining control theory, runtime simulation modeling, cost-benefit analysis, and operating system resource analysis. In particular, the nove use of cost-benefit analysis and control theory techniques makes STMM a breakthrough technology in database memory management. The cost-benefit analysis allows STMM to tune memory between radically different memory consumers such as compiled statement cache, sort, and buffer pools. These methods allow for the fast convergence of memory settings while also providing stability in the presence of system noise. The tuning mode has been found in numerous experiments to tune memory allocation as well as expert human administrators, including OLTP, DSS, and mixed environments. We believe this is the first known use of cost-benefit analysis and control theory in database memory tuning across heterogeneous memory consumers.

#index 893179
#* Mapping moving landscapes by mining mountains of logs: novel techniques for dependency model generation
#@ Mirko Steinle;Karl Aberer;Sarunas Girdzijauskas;Christian Lovis
#t 2006
#c 4
#% 279911
#% 434361
#% 458786
#% 546061
#% 630337
#% 723282
#% 729999
#% 740900
#% 785405
#% 823352
#% 823375
#% 823414
#% 823418
#% 848846
#! Problem diagnosis for distributed systems is usually difficult. Thus, an automated support is needed to identify root causes of encountered problems such as performance lags or inadequate functioning quickly. The many tools and techniques existing today that perform this task rely usually on some dependency model of the system. However, in complex and fast evolving environments it is practically unfeasible to keep such a model up-to-date manually and it has to be created in an automatic manner. For high level objects this is in itself a challenging and less studied task. In this paper, we propose three different approaches to discover dependencies by mining system logs. Our work is inspired by a recently developed data mining algorithm and techniques for collocation extraction from the natural language processing field. We evaluate the techniques in a case study for Geneva University Hospitals (HUG) and perform large-scale experiments on production data. Results show that all techniques are capable of finding useful dependency information with reasonable precision in a real-world environment.

#index 893180
#* IT839 Policy leading to u-Korea
#@ Jung-Hee Song
#t 2006
#c 4
#! With the strategic early adoption of IT infrastructure technologies such as the broadband and the CDMA wireless communication in Korea, IT industry has been the major contributor to the recent Korean economic growth, accounting for 15.8% of the real GDP in 2005.In 2004, Korean Ministry of Information and Communication has established the so-called IT839 strategy as its new IT initiative. IT839 means 8 new IT services which will be deployed within the next three to four years so that service operators invest on 3 new wireless broadband and secure communication infrastructures to offer high-quality ubiquitous service. For rich user experience, 9 hardware and software component industries are defined as the growth engine. The total twenty industry sectors of the IT839 strategy form the IT industry value chain.In 2005, Korean Ministry of Information and Communication updated its IT839 structure by replacing some of its components to explicitly address the u-Korea framework.This IT839 strategy will contribute not only to IT industry but bring qualitative changes to the economic and social paradigm. It ultimately aims to realize a ubiquitous world by forming a virtuous circle of developing new services, infrastructure, and growth engines.To sustain the current momentum, Korea must become proactive in the global collaboration. Korean Ministry of Information and Communication invests in drawing leading global IT companies into Korea for cooperation with Korea R&D partners. During the past two years, twelve global companies established local R&D laboratories in Korea with partial funding from Korean government.

#index 893181
#* Home network: road to ubiquitous world
#@ In Ryu
#t 2006
#c 4
#! Home Network is considered to be a stepping stone for ubiquitous world. The current home network solution in Korea is highly focused on delivering security, home control, and remote management to high-rise multi-dwelling units. Initial deployment case shows that individual users are fairly satisfied with the current solution sets but still looking for high-end entertainment solution as well. In this talk, we will cover the current status of LG HomNet, LG Electronics' home network solution, deployment cases, and future roadmap.

#index 893182
#* Advances in memory technology
#@ Changhyun Kim
#t 2006
#c 4
#! The continuous growth of the memory market, whose early beginnings in the 70's and 80's were marked by PC and server DRAMs, has experienced a new boost since the beginning of the 21st century due to the emergence of digital consumer & mobile markets such as cellular phone, DSC, and MP3. The join of the nonvolatile and low-power Flash memory has led to a further explosive growth. Ever increasing density and decreasing costs have evoked a tremendous rise in consumer demand.Innovations in memory technology are reflected in the continuous advance in high density, high speed and low power technologies, in the course of which the design rule has shown a transition from micrometer to nanometer scale. Additionally, the development of new materials has given birth to new high-performance nonvolatile memory types (PRAM, RRAM, MRAM, FRAM, etc.), which open even more opportunities for growth of the semiconductor market.The steep increase in technology of today's memories shows itself in the capacity and speed of storing information of everybody's use: a 1cm2 memory chip can store 10Gbit information now, which corresponds to either 80K pages of newspaper, 20 hours of music or 2.5 movie hours. Today's DRAM shows a random access time of 25-50ns and I/O bandwidth of 3-4GHz. Technical innovations will continue to drive the increase of memory density and speed in the future. Higher storage density is expected to be achieved by breakthroughs such as 3D memory stacking technology (cell/chip/package), the use of 3-dimensional transistors or the shrinkage of memory storage nodes to the atomic scale. Memory system performance will possibly be enhanced by the fusion of conventional commodity memories and new memories: several memories like Flash, SRAM, DRAM, new memories will be merged together with logic and software. Thus we expect that semiconductor products will show a larger variety of high performance systems with much higher robustness and persistence.In the 21st century, which has just begun, memory technology will combine with various other fields (IT, BT, NT) and thus open new markets such as massive data & information processing, bio & health care, and humanoid & aerospace. It will contribute to a world with more comfort and stability, where everywhere and anytime people can exchange and share their thoughts, sensations and emotions.

#index 893183
#* Efficient XSLT processing in relational database system
#@ Zhen Hua Liu;Agnuel Novoselsky
#t 2006
#c 4
#% 136740
#% 137701
#% 142228
#% 348183
#% 480657
#% 654484
#% 800601
#% 805908
#% 810036
#% 810050
#% 810083
#% 810118
#% 824751
#% 993940
#! Efficient processing of XQuery, XPath and SQL/XML on XML documents stored and managed in RDBMS has been widely studied. However, much less of such type of work has been done for efficient XSLT processing of XML documents stored and managed by the database. This is partially due to the observation that the rule based template driven XSLT execution model does not fit nicely with the traditional declarative query language processing model which leverages index probing and iterator based pull mode that can be scaled to handle large size data. In this paper, we share our experience of efficient processing of XSLT in Oracle XML DB. We present the technique of processing XSLT efficiently in database by rewriting XSLT stylesheets into highly efficient XQuery through partially evaluating XSLT over the XML documents structural information. Consequently, we can leverage all the work done for efficient XQuery/XPath processing in database to achieve combined optimisations of XSLT with XQuery/XPath and SQL/XML in Oracle XMLDB. This effectively makes XSLT processing scale to large size XML documents using classical declarative query processing techniques in DBMS.

#index 893184
#* On the path to efficient XML queries
#@ Andrey Balmin;Kevin S. Beyer;Fatma Özcan;Matthias Nicola
#t 2006
#c 4
#% 783793
#% 810036
#% 810081
#% 810116
#% 810117
#% 810118
#% 824750
#% 881732
#% 881733
#% 881734
#% 1016134
#! XQuery and SQL/XML are powerful new languages for querying XML data. However, they contain a number of stumbling blocks that users need to be aware of to get the expected results and performance. For example, certain language features make it hard if not impossible to exploit XML indexes.The major database vendors provide XQuery and SQL/XML support in their current or upcoming product releases. In this paper, we identify common pitfalls gleaned from the experiences of early adopters of this functionality. We illustrate these pitfalls through concrete examples, explain the unexpected query behavior, and show alternative formulations of the queries that behave and perform as anticipated. As results we provide guidelines for XQuery and SQL/XML users, feedback on the language standards, and food for thought for emerging languages and APIs.

#index 893185
#* Building conference proceedings requires adaptable workflow and content management
#@ Jutta A. Mülle;Klemens Böhm;Nicolas Röper;Tobias Sünder
#t 2006
#c 4
#% 202098
#% 261267
#% 262989
#% 312784
#% 398149
#% 509525
#% 535366
#% 609451
#% 771255
#% 804369
#% 825657
#% 858061
#% 1719023
#! ProceedingsBuilder is a system that helps the proceedings chair of a scientific conference to carry out his chores. It has features of both workflow management systems (WFMS) and content management systems (CMS), in order to collect the material for the printed proceedings and other products. ProceedingsBuilder has been operational at several conferences, including VLDB 2005. When using Proceedings-Builder, we had a very intense lesson which kinds of work-flow adaptations may become necessary. Existing WFMS do not offer support for most of them. The concern of this article is to describe and classify these various requirements regarding adaptation. ProceedingsBuilder is an example of a broad class of systems, namely editorial systems that collect content in order to publish it. Our findings are of interest to a broader audience, not only to conference organizers.

#index 893186
#* Globalization: challenges to database community
#@ Sang K. Cha;P. Anandan;Meichun Hsu;C. Mohan;Rajeev Rastogi;Vishal Sikka;Honesty Young
#t 2006
#c 4
#! Globalization is flattening the world. As database researcher, we are proud that information technology is a critical enabler of globalization. At the same time, we are seeing that research and development of information technology is also being globalized.In recent years, many R&D labs were established in Asia, especially, in India and China, by global IT companies. Some of our colleagues have moved with globalization to establish new labs or to lead R&D in newly established labs. For those who have not moved physically, it is common to work with colleagues at remote labs with time difference.The objective of this panel is to invite pioneers leading R&D globalization and to share their vision and challenges, and to discuss how globalization impacts the future of database and information management research, education, and industry.

#index 893187
#* NUITS: a novel user interface for efficient keyword search over databases
#@ Shan Wang;Zhaohui Peng;Jun Zhang;Lu Qin;Sheng Wang;Jeffrey Xu Yu;Bolin Ding
#t 2006
#c 4
#% 659990
#% 660011
#% 824693
#% 1015325
#% 1703166
#! The integration of database and information retrieval techniques provides users with a wide range of high quality services. We present a prototype system, called NUITS, for efficiently processing keyword queries on top of a relational database. Our NUITS allows users to issue simple keyword queries as well as advanced keyword queries with conditions. The efficiency of keyword query processing and the user-friendly result display will also be addressed in this paper.

#index 893188
#* IPAC: an interactive approach to access control for semi-structured data
#@ Sriram Mohan;Yuqing Wu
#t 2006
#c 4
#% 340827
#% 664665
#% 755175
#% 765450
#% 808345
#% 838400
#% 838401
#% 838432
#% 864557
#! We propose IPAC(Interactive aPproach to Access Control for semi-structured data), a framework for XML access constraint specification and security view selection. IPAC clearly demarcates access constraint specification, access control strategy and security mechanism (implementation). It features a declarative access constraint specification language, a global access control strategy configuration unit, and an automatic security view generation and ranking tool. IPAC is the first system that assists the DBA in specifying access control strategies and access constraints on XML data, and helps the DBA in choosing the optimal plan that implements the specified strategy and access constraints accurately and efficiently.

#index 893189
#* Trio: a system for data, uncertainty, and lineage
#@ Parag Agrawal;Omar Benjelloun;Anish Das Sarma;Chris Hayworth;Shubha Nabar;Tomoe Sugihara;Jennifer Widom
#t 2006
#c 4
#% 864394
#% 893167

#index 893190
#* SIREN: a similarity retrieval engine for complex data
#@ Maria Camila N. Barioni;Humberto Razente;Agma Traina;Caetano Traina, Jr.
#t 2006
#c 4
#% 342827
#% 428409
#% 443482
#% 629665
#! This paper presents a similarity retrieval engine - SIREN-that allows posing similarity queries in a relational DBMS using an extended syntax that adds the support for such type of queries in the SQL language. It discusses the main architecture of SIREN, describes some key features and provides a description of the demo.

#index 893191
#* AQAX: a system for approximate XML query answers
#@ Joshua Spiegel;Emmanuel Pontikakis;Suratna Budalakoti;Neoklis Polyzotis
#t 2006
#c 4
#% 274152
#% 480810
#% 765423
#% 864450
#! On-line, interactive exploration of large databases becomes prohibitively expensive as the size of the database grows. Approximate query answering offers a cost-effective solution to this problem, by enabling the fast generation of approximate results based on concise data summaries. We apply this paradigm in the context of XML databases, where the increased complexity of data and queries amplifies the challenges behind interactive exploration. We have developed an on-line XML exploration system, termed AQAX that relies on accurate XML summaries in order to enable the rapid exploration of large data sets. To effectively support the exploration of semi-structured query answers, our system employs a tight coupling between the main query processor and the graphical clients that visualize the results. This demonstration will showcase the functionality of our system and the effectiveness of approximate query answering in the context of XML databases.

#index 893192
#* PARAgrab: a comprehensive architecture for web image management and multimodal querying
#@ Dhiraj Joshi;Ritendra Datta;Ziming Zhuang;W. P. Weiss;Marc Friedenberg;Jia Li;James Z. Wang
#t 2006
#c 4
#% 198058
#% 318785
#% 334590
#% 345848
#% 479799
#% 482109
#% 780796
#% 780875
#% 840455
#% 860959
#% 1698382
#! We demonstrate PARAgrab - a scalable Web image archival, retrieval, and annotation system that supports multiple querying modalities. The underlying architecture of our large-scale Web image database is described. Querying and visualization techniques used in the system are explained.

#index 893193
#* Incremental schema matching
#@ Philip A. Bernstein;Sergey Melnik;John E. Churchill
#t 2006
#c 4
#% 572314
#% 790846
#% 790848
#% 801413
#% 810021
#% 824735
#% 1304914
#! The goal of schema matching is to identify correspondences between the elements of two schemas. Most schema matching systems calculate and display the entire set of correspondences in a single shot. Invariably, the result presented to the engineer includes many false positives, especially for large schemas. The user is often overwhelmed by all of the edges, annoyed by the false positives, and frustrated at the inability to see second- and third-best choices. We demonstrate a tool that circumvents these problems by doing the matching interactively. The tool suggests candidate matches for a selected schema element and allows convenient navigation between the candidates. The ranking of match candidates is based on lexical similarity, schema structure, element types, and the history of prior matching actions. The technical challenges are to make the match algorithm fast enough for incremental matching in large schemas and to devise a user interface that avoids overwhelming the user. The tool has been integrated with a prototype version of Microsoft BizTalk Mapper, a visual programming tool for generating XML-to-XML mappings.

#index 893194
#* A semantic information integration tool suite
#@ Jun Yuan;Ali Bahrami;Changzhou Wang;Marie Murray;Anne Hunt
#t 2006
#c 4
#% 790851
#% 810073
#% 1048525
#! We describe a prototype software tool suite for semantic information integration; it has the following features. First, it can import local metadata as well as a domain ontology. Imported metadata is stored persistently in an ontological format. Second, it provides a semantic query facility that allows users to retrieve information across multiple data sources using the domain ontology directly. Third, it has a GUI for users to define mappings between the local metadata and the domain ontology. Fourth, it incorporates a novel mechanism to improve system reliability by dynamically adapting query execution upon detecting various types of environmental changes. In addition, this tool suite is compatible with W3C Semantic Web specifications such as RDF and OWL. It also uses the query engine of Commercial EII products for low level query processing.

#index 893195
#* POP/FED: progressive query optimization for federated queries in DB2
#@ Holger Kache;Wook-Shin Han;Volker Markl;Vijayshankar Raman;Stephan Ewen
#t 2006
#c 4
#% 300138
#% 480803
#% 765456
#% 1016225
#% 1688293
#! Federated queries are regular relational queries accessing data on one or more remote relational or non-relational data sources, possibly combining them with tables stored in the federated DBMS server. Their execution is typically divided between the federated server and the remote data sources. Outdated and incomplete statistics have a bigger impact on federated DBMS than on regular DBMS, as maintenance of federated statistics is unequally more complicated and expensive than the maintenance of the local statistics; consequently bad performance commonly occurs for federated queries due to the selection of a suboptimal query plan. To solve this problem we propose a progressive optimization technique for federated queries called POP/FED by extending the state of the art for progressive reoptimization for local source queries, POP [4]. POP/FED uses (a) an opportunistic, but risk controlled reoptimization technique for federated DBMS, (b) a technique for multiple reoptimizations during federated query processing with a strategy to discover redundant and eliminate partial results, and (c) a mechanism to eagerly procure statistics in a federated environment. In this demonstration we showcase POP/FED implemented in a prototype version of WebSphere Information Integrator for DB2 using the TPC-H benchmark database and its workload. For selected queries of the workload we show unique features including multi-round reoptimizations using both a new graphical reoptimization progress monitor POPMonitor and the DB2 graphical plan explain tool.

#index 893196
#* SPIDER: a schema mapPIng DEbuggeR
#@ Bogdan Alexe;Laura Chiticariu;Wang-Chiew Tan
#t 2006
#c 4
#% 378409
#% 572314
#% 765432
#% 809239
#% 810078
#% 826032
#% 893095
#% 993981
#! A schema mapping is a high-level declarative specification of how data structured under one schema, called the source schema, is to be transformed into data structured under a possibly different schema, called the target schema. We demonstrate SPIDER, a prototype tool for debugging schema mappings, where the language for specifying schema mappings is based on a widely adopted formalism. We have built SPIDER on top of a data exchange system, Clio, from IBM Almaden Research Center. At the heart of SPIDER is a data-driven facility for understanding a schema mapping through the display of routes. A route essentially describes the relationship between source and target data with the schema mapping. In this demonstration, we showcase our route engine, where we can display one or all routes starting from either source or target data, as well as the intermediary data and schema elements involved. In addition, we demonstrate "standard" debugging features for schema mappings that we have also built, such as computing and exploring routes step-by-step, stopping or pausing the computation with breakpoints, performing "guided" computation of routes by taking human input into account, as well as tracking the state of the target instance during the process of computing routes.

#index 893197
#* OntoQuest: exploring ontological data made easy
#@ Li Chen;Maryann Martone;Amarnath Gupta;Lisa Fong;Mona Wong-Barnum
#t 2006
#c 4
#% 824692
#! Recently, there is a large demand by many scientific applications for managing, querying and reasoning ontology concepts and instances. We demonstrate OntoQuest, a system that provides powerful yet easy-to-use query and reasoning utilities by which the ontological data exploration experience is made easy. Even without any knowledge of ontology query languages, one can easily get a hands-on ontological data exploration experience using OntoQuest. The method is to categorize commonly asked queries based on their usage contexts so to prompt the user with context-aware guidance throughout the exploration process. OntoQuest is also designed to offer extended mapping schemes for storing OWL ontologies into back-end databases. Most existing ontology storage systems support mappings only for RDF data. Lastly, OntoQuest supports bulk insertion and updating of instances.In this demonstration, we show how OntoQuest guides a user through the inquiry (querying and reasoning) process and also have a peek at the underlying handling of storing, querying, reasoning, and interacting with the user.

#index 893198
#* HISA: a query system bridging the semantic gap for large image databases
#@ Gang Chen;Xiaoyan Li;Lidan Shou;Jinxiang Dong;Chun Chen
#t 2006
#c 4
#% 318785
#% 479649
#% 642989
#% 721163
#% 722927
#% 771025
#% 784963
#% 818272
#! We propose a novel system called HISA for organizing very large image databases. HISA implements the first known data structure to capture both the ontological knowledge and visual features for effective and effcient retrieval of images by either keywords, image examples, or both. HISA employs automatic image annotation technique, ontology analysis and statistical analysis of domain knowledge to precompute the data structure. Using these techniques, HISA is able to bridge the gap between the image semantics and the visual features, therefore providing more user-friendly and high-performance queries. We demonstrate the novel data structure employed by HISA, the query algorithms, and the pre-computation process.

#index 893199
#* Adaptive density estimation
#@ Arturas Mazeika;Michael H. Böhlen;Andrej Taliun
#t 2006
#c 4
#% 273901
#% 333946
#! This demonstration illustrates the APDF tree: an adaptive tree that supports the effective and effcient computation of continuous density information. The APDF tree allocates more partition points in non-linear areas of the density function and fewer points in linear areas of the density function. This yields not only a bounded, but a tight control of the error. The demonstration explains the core steps of the computation of the APDF tree (split, kernel additions, tree optimization, kernel additions, unsplit) and demos the implementation for different datasets.

#index 893200
#* GMine: a system for scalable, interactive graph visualization and mining
#@ José F. Rodrigues, Jr.;Hanghang Tong;Agma J. M. Traina;Christos Faloutsos;Jure Leskovec
#t 2006
#c 4
#% 281214
#% 769887
#% 1669913
#! Several graph visualization tools exist. However, they are not able to handle large graphs, and/or they do not allow interaction. We are interested on large graphs, with hundreds of thousands of nodes. Such graphs bring two challenges: the first one is that any straightforward interactive manipulation will be prohibitively slow. The second one is sensory overload: even if we could plot and replot the graph quickly, the user would be overwhelmed with the vast volume of information because the screen would be too cluttered as nodes and edges overlap each other.Our GMine system addresses both these issues, by using summarization and multi-resolution. GMine offers multi-resolution graph exploration by partitioning a given graph into a hierarchy of communities-within-communities and storing it into a novel R-treelike structure which we name G-Tree. GMine offers summarization by implementing an innovative subgraph extraction algorithm and then visualizing its output.

#index 893201
#* A middleware for fast and flexible sensor network deployment
#@ Karl Aberer;Manfred Hauswirth;Ali Salehi
#t 2006
#c 4
#% 1112635
#! A key problem in current sensor network technology is the heterogeneity of the available software and hardware platforms which makes deployment and application development a tedious and time consuming task. To minimize the unnecessary and repetitive implementation of identical functionalities for different platforms, we present our Global Sensor Networks (GSN) middleware which supports the flexible integration and discovery of sensor networks and sensor data, enables fast deployment and addition of new platforms, provides distributed querying, filtering, and combination of sensor data, and supports the dynamic adaption of the system configuration during operation. GSN's central concept is the virtual sensor abstraction which enables the user to declaratively specify XML-based deployment descriptors in combination with the possibility to integrate sensor network data through plain SQL queries over local and remote sensor data sources. In this demonstration, we specifically focus on the deployment aspects and allow users to dynamically reconfigure the running system, to add new sensor networks on the fly, and to monitor the effects of the changes via a graphical interface. The GSN implementation is available from http://globalsn.sourceforge.net/.

#index 893202
#* Entirely declarative sensor network systems
#@ David Chu;Arsalan Tavakoli;Lucian Popa;Joseph Hellerstein
#t 2006
#c 4
#% 336865
#% 581040
#% 806214
#% 821939
#% 835186
#% 874978
#% 963582
#! The database and sensor network community have both recognized the utility of SQL for interfacing with sensor network systems. Recently there have been proposals to construct Internet protocols declaratively in variants of Datalog. We take these ideas to their logical extreme, and demonstrate entire distributed sensor network systems built declaratively. Our demo exposes the rapidity, flexibility, and efficiency of our approach by building several fully-functional yet widely-varying sensor network applications and services declaratively. As a result of our declarative construction, we are able to highlight a wealth of previously underexposed similarities between sensor networks and database concepts. In addition, we tackle many database systems challenges in building multiple layers of a declarative database for an embedded, distributed system.

#index 893203
#* R-SOX: runtime semantic query optimization over XML streams
#@ Song Wang;Hong Su;Ming Li;Mingzhu Wei;Shoushen Yang;Drew Ditto;Elke A. Rundensteiner;Murali Mani
#t 2006
#c 4
#% 731408
#% 781453
#% 824674
#% 864654
#% 945868
#! Optimizing queries over XML streams has been an important and non-trivial issue with the emergence of complex XML stream applications such as monitoring sensor networks and online transaction processing. Our system, R-SOX, provides a platform for runtime query optimization based on dynamic schema knowledge embedded in the XML streams. Such information provides refined runtime schema knowledge thus dramatically enlarged the opportunity for schema-based query optimizations. In this demonstration, we focus on the following three aspects: (1) annotation of runtime schema knowledge; (2) incremental maintenance of run-time schema knowledge; (3) dynamic semantic query optimization techniques. The overall framework for runtime semantic query optimization, including several classes of dynamic optimization techniques, will be shown in this demonstration.

#index 893204
#* Using high dimensional indexes to support relevance feedback based interactive images retrieval
#@ Junqi Zhang;Xiangdong Zhou;Wei Wang;Baile Shi;Jian Pei
#t 2006
#c 4
#% 318785
#% 341267
#% 411758
#% 427199
#% 443889
#% 479462
#% 479649
#% 480632
#% 592183
#% 810069
#% 814646
#% 1775156
#! Image retrieval has found more and more applications. Due to the well recognized semantic gap problem, the accuracy and the recall of image similarity search are often still low. As an effective method to improve the quality of image retrieval, the relevance feedback approach actively applies users' feedback to refine the search. As searching a large image database is often costly, to improve the efficiency, high dimensional indexes may help. However, many existing database indexes are not adaptive to updates of distance measures caused by users' feedback. In this paper, we propose a demo to illustrate the relevance feedback based interactive images retrieval procedure, and examine the effectiveness and the efficiency of various indexes. Particularly, audience can interactively investigate the effect of updated distance measures on the data space where the images are supposed to be indexed, and on the distributions of the similar images in the indexes. We also introduce our new B+-tree-like index method based on cluster splitting and iDistance.

#index 893205
#* XML Evolution: a two-phase XML processing model using XML prefiltering techniques
#@ Chia-Hsin Huang;Tyng-Ruey Chuang;James J. Lu;Hahn-Ming Lee
#t 2006
#c 4
#% 340144
#% 413765
#% 570876
#% 650962
#% 782820
#% 783697
#% 803121
#% 814651
#% 835837
#! An implementation based on the two-phase XML processing model introduced in [3] is presented in this paper. The model employs a prefilter to remove uninteresting fragments of an input XML document by approximately executing a user's queries. The refined candidate-set XML document is then returned to the user's DOM- or SAX-based applications for further processing. In this demonstration, it is shown that the technique significantly enhances the performance of existing DOM- and SAX-based XML applications and tools (e.g., XPath/XQuery processors and XML parsers), while reducing computational resource needs. Moreover, the prefilter can be easily integrated into existing applications by adding only one instruction. We also present an enhancement to the indexing scheme of the prefiltering technique to speed up the evaluation of certain axes.

#index 893206
#* MyPortal: robust extraction and aggregation of web content
#@ Marek Kowalkiewicz;Tomasz Kaczmarek;Witold Abramowicz
#t 2006
#c 4
#% 268291
#% 397605
#% 615768
#% 754078
#% 1394469
#% 1684767
#! We demonstrate myPortal - an application for web content block extraction and aggregation. The research issues behind the tool are also explained, with an emphasis on robustness of web content extraction.

#index 893207
#* EOS2: unstoppable stateful PHP
#@ German Shegalov;Gerhard Weikum
#t 2006
#c 4
#% 399766
#% 769240
#% 963662
#% 994023

#index 893208
#* SMOQE: a system for providing secure access to XML
#@ Wenfei Fan;Floris Geerts;Xibei Jia;Anastasios Kementsietsidis
#t 2006
#c 4
#% 480296
#% 654476
#% 731408
#% 765450
#% 1015275
#! XML views have been widely used to enforce access control, support data integration, and speed up query answering. In many applications, e.g., XML security enforcement, it is prohibitively expensive to materialize and maintain a large number of views. Therefore, views are necessarily virtual. An immediate question then is how to answer queries on XML virtual views. A common approach is to rewrite a query on the view to an equivalent one on the underlying document, and evaluate the rewritten query. This is the approach used in the Secure MOdular Query Engine (SMOQE). The demo presents SMOQE, the first system to provide efficient support for answering queries over virtual and possibly recursively defined XML views. We demonstrate a set of novel techniques for the specification of views, the rewriting, evaluation and optimization of XML queries. Moreover, we provide insights into the internals of the engine by a set of visual tools.

#index 893209
#* Crimson: a data management system to support evaluating phylogenetic tree reconstruction algorithms
#@ Yifeng Zheng;Stephen Fisher;Shirley Cohen;Sheng Guo;Junhyong Kim;Susan B. Davidson
#t 2006
#c 4
#% 577353
#% 598374
#% 654493
#% 712090
#% 800491
#% 853050
#! Evolutionary and systems biology increasingly rely on the construction of large phylogenetic trees which represent the relationships between species of interest. As the number and size of such trees increases, so does the need for efficient data storage and query capabilities. Although much attention has been focused on XML as a tree data model, phylogenetic trees differ from document-oriented applications in their size and depth, and their need for structure-based queries rather than path-based queries.This paper focuses on Crimson, a tree storage system for phylogenetic trees used to evaluate phylogenetic tree reconstruction algorithms within the context of the NSF CIPRes project. A goal of the modeling component of the CIPRes project is to construct a huge simulation tree representing a "gold standard" of evolutionary history against which phylogenetic tree reconstruction algorithms can be tested.In this demonstration, we highlight our storage and indexing strategies and show how Crimson is used for benchmarking phylogenetic tree reconstruction algorithms. We also show how our design can be used to support more general queries over phylogenetic trees.

#index 893210
#* HUX: handling updates in XML
#@ Ling Wang;Elke A. Rundensteiner;Murali Mani;Ming Jiang
#t 2006
#c 4
#% 664
#% 102781
#% 286901
#% 287000
#% 291869
#% 411759
#% 479915
#% 480317
#% 570875
#% 839159
#% 848763
#% 891734
#% 994001
#! We demonstrate HUX (Handling Updates in XML) which provides a reliable and efficient solution for the XML view update problem. Given an update over an XML view, our U-Filter subsytem first determines whether the update is translatable or not by examining potential conflicts in both schema and data. If an update is determined to be translatable, our U-Translator subsystem searches potential translations and finds a "good" one. Our demonstration illustrates the working, as well as the performance, of the two sub-systems within HUX for different application scenarios.

#index 893211
#* InteMon: intelligent system monitoring on large clusters
#@ Evan Hoke;Jimeng Sun;Christos Faloutsos
#t 2006
#c 4
#% 311536
#% 576112
#% 591147
#% 824709
#% 1142424
#! InteMon is a prototype monitoring and mining system for large clusters. Currently, it monitors over 100 hosts of a prototype data center at CMU. It uses the SNMP protocol and it stores the monitoring data in an mySQL database. Then, it allows for visualization of the time-series data using a JSP web-based frontend interface for users.What sets it apart from other cluster monitoring systems is its ability to automatically analyze the monitoring data in real time and alert the users for potential anomalies. It uses state of the art stream mining methods, it has a sophisticated definition of anomalies (broken correlations among input streams), and it can also pinpoint the reason of the anomaly. InteMon has a user-friendly GUI, it allows the users to perform interactive mining tasks, and it is fully operational.

#index 893212
#* Simple and realistic data generation
#@ Kenneth Houkjær;Kristian Torp;Rico Wind
#t 2006
#c 4
#% 172913
#% 311963
#% 410276
#% 435112
#% 824744
#! This paper presents a generic, DBMS independent, and highly extensible relational data generation tool. The tool can efficiently generate realistic test data for OLTP, OLAP, and data streaming applications. The tool uses a graph model to direct the data generation. This model makes it very simple to generate data even for large database schemas with complex inter- and intra table relationships. The model also makes it possible to generate data with very accurate characteristics.

#index 893213
#* XCheck: a platform for benchmarking XQuery engines
#@ Loredana Afanasiev;Massimo Franceschet;Maarten Marx
#t 2006
#c 4
#% 342677
#% 489176
#% 541480
#% 745518
#% 875010
#% 994015
#% 1721253
#% 1721254
#! XCheck is a tool for assessing the relative performance of different XQuery engines by means of benchmarks consisting of a set of XQuery queries and a set of XML documents. Given a benchmark and a set of engines, XCheck runs the benchmark on these engines and produces highly informative performance output. The current version of XCheck contains all available XQuery benchmarks which are run against four XQuery engines: Galax, Qizx/open, Saxon and MonetDB/XQuery. XCheck's design makes it easy to include new engines and new benchmarks.

#index 893214
#* GignoMDA: exploiting cross-layer optimization for complex database applications
#@ Dirk Habich;Sebastian Richly;Wolfgang Lehner
#t 2006
#c 4
#% 616086
#% 725887
#! Database Systems are often used as persistent layer for applications. This implies that database schemas are generated out of transient programming class descriptions. The basic idea of the MDA approach generalizes this principle by providing a framework to generate applications (and database schemas) for different programming platforms. Within our GignoMDA project [3]--which is subject of this demo proposal--we have extended classic concepts for code generation. That means, our approach provides a single point of truth describing all aspects of database applications (e.g. database schema, project documentation,...) with great potential for cross-layer optimization. These new cross-layer optimization hints are a novel way for the challenging global optimization issue of multi-tier database applications. The demo at VLDB comprises an in-depth explanation of our concepts and the prototypical implementation by directly demonstrating the modeling and the automatic generation of database applications.

#index 893215
#* LGeDBMS: a small DBMS for embedded system with flash memory
#@ Gye-Jeong Kim;Seung-Cheon Baek;Hyun-Sook Lee;Han-Deok Lee;Moon Jeung Joe
#t 2006
#c 4
#% 107692
#% 566138
#% 829901
#% 978505
#! The ever-increasing requirement of high performance and huge capacity memories of emerging consumer electronics appliances, such as mobile phone, digital camera, MP3, PMP, PDA, etc., has led to the widespread adaptation of flash memory as main data storages, respectively. As a result, managing the data on ash memory has been gaining in significant to satisfy the requirement of mobile embedded applications. However, the read/write/erase behaviors of flash memory are radically different than that of magnetic disks which make traditional database technology irrelevant. In this paper, we introduce LGeDBMS, a scale-downed DBMS engine designed for ash memory and its application. Finally, we demonstrate a PIM(Personal Information Management) application on a mobile phone using LGeDBMS.

#index 893216
#* One platform for mining structured and unstructured data: dream or reality?
#@ Dina Bitton;Franz Faerber;Laura Haas;Jayavel Shanmugasundaram
#t 2006
#c 4
#% 631868
#% 875064
#% 893143

#index 893217
#* Foundations of automated database tuning
#@ Surajit Chaudhuri;Gerhard Weikum
#t 2006
#c 4

#index 893218
#* Streaming in a connected world: querying and tracking distributed data streams
#@ Graham Cormode;Minos Garofalakis
#t 2006
#c 4
#% 1740384

#index 893219
#* Query co-processing on commodity processors
#@ Anastassia Ailamaki;Naga K. Govindaraju;Stavros Harizopoulos;Dinesh Manocha
#t 2006
#c 4
#! The rapid increase in the data volumes for the past few decades has intensified the need for high processing power for database and data mining applications. Researchers have actively sought to design and develop new architectures for improving the performance. Recent research shows that the performance can be significantly improved using either (a) effective utilization of architectural features and memory hierarchies used by the conventional processors, or (b) the high computational power and memory bandwidth in commodity hardware such as network processing units (NPUs), Cell processors and graphics processing units (GPUs). This tutorial will survey the micro-architectural and architectural differences across these processors with data management in mind, and will present previous work and future opportunities for expanding query processing algorithms to other hardware than general-purpose processors. In addition to the database community, we intend to increase awareness in the computer architecture scene about opportunities to construct heterogeneous chips.

#index 893220
#* A decade of progress in indexing and mining large time series databases
#@ Eamonn Keogh
#t 2006
#c 4
#% 172949
#! Time series data is ubiquitous; large volumes of time series data are routinely created in scientific, industrial, entertainment, medical and biological domains. Examples include gene expression data, electrocardiograms, electroencephalograms, gait analysis, stock market quotes, space telemetry etc. Although statisticians have worked with time series for more than a century, many of their techniques hold little utility for researchers working with massive time series databases.A decade ago, a seminal paper by Faloutsos, Ranganathan, Manolopoulos appeared in SIGMOD. The paper, Fast Subsequence Matching in Time-Series Databases, has spawned at least a thousand references and extensions in the database/ data mining and information retrieval communities. This tutorial will summarize the decade of progress since this influential paper appeared.

#index 893221
#* Randomized algorithms for matrices and massive data sets
#@ Petros Drineas;Michael W. Mahoney
#t 2006
#c 4
#% 870224
#% 870225
#% 870226
#% 881488
#! The tutorial will cover randomized sampling algorithms that extract structure from very large data sets modeled as matrices or tensors. Both provable algorithmic results and recent work on applying these methods to large biological and internet data sets will be discussed.

#index 906830
#* Proceedings of the 32nd international conference on Very large data bases
#@ Umeshwar Dayal;Khu-Yong Whang;David Lomet;Gustavo Alonso;Guy Lohman;Martin Kersten;Sang K. Cha;Young-Kuk Kim
#t 2006
#c 4

#index 906831
#* Proceedings of the 32nd international conference on Very large data bases
#@ Umeshwar Dayal;Khu-Yong Whang;David Lomet;Gustavo Alonso;Guy Lohman;Martin Kersten;Sang K. Cha;Young-Kuk Kim
#t 2006
#c 4

#index 993092
#* Proceedings of the 4th workshop on Data management for sensor networks: in conjunction with 33rd International Conference on Very Large Data Bases
#@ Amol Deshpande;Qiong Luo
#t 2007
#c 4
#! Sensor networks enable an unprecedented level of access to the physical world, and hold tremendous potential to revolutionize many application domains. Research on sensor networks spans many areas of computer science, and there are now major conferences, e.g., IPSN and SenSys, devoted to sensor networks. However, there is no exclusive forum for discussion on early and innovative work on data management in sensor networks. The International Workshop on Data Management for Sensor Networks (DMSN), inaugurated in 2004, aims to fill this significant gap in the database and sensor network communities. Building on the successes of the three previous DMSN workshops (2004--6), DMSN 2007 brings together researchers working on all aspects of sensor data management: from data processing in networks of remote, wireless, resource-constrained sensors to managing heterogeneous, noisy, and sometimes sensitive sensor data in databases. The resource-constrained, lossy, noisy, distributed, and remote nature of sensor networks means that traditional database techniques often cannot be applied without significant re-tooling. Challenges associated with acquiring and processing large-scale, heterogeneous sets of live sensor data also call for novel data management techniques. Finally, in many applications, collecting sensor data raises important privacy and security concerns that require new protection and anonymization techniques. As the field of sensor networks continues to develop, we have expanded the scope of DMSN 2007 from previous workshops in the series, by encouraging contributions on a broader sets of topics, including: database languages for sensor tasking; distributed sensor data storage and indexing; data replication and consistency in noisy and lossy environments; energy-efficient data acquisition and dissemination; in-network query processing; networking support for data processing; query optimization and deployment planning in sensor networks; database tech- niques for managing loss, uncertainty, noise, and ambiguity; model-based sensor data processing; challenges and techniques for new types of sensor data, e.g., RFID, images and videos, data from scientific and medical instru- ments; personal, ubiquitous applications of sensor-based infrastructures; integration of sensor data of different modalities and from different sources; integration of sensor data in traditional databases and streaming systems; techniques for secure sensor data collection and processing; and privacy protection techniques for sensor data. As a response to the Call for Papers this year, we received 15 full paper submissions. During the review process, each paper was reviewed by three or four members of the program committee or external reviewers and was also carefully discussed, resulting in the acceptance of 7 papers.

#index 993093
#* Similarity-aware query allocation in sensor networks with multiple base stations
#@ Shili Xiang;Hock-Beng Lim;Kian-Lee Tan;Yongluan Zhou
#t 2007
#c 4
#% 177450
#% 205305
#% 250530
#% 410276
#% 800584
#% 806214
#% 960277
#% 981636
#% 1016167
#% 1016178
#% 1675421
#% 1839749
#! In this paper, we consider a large scale sensor network comprising multiple, say K, base stations and a large number of wireless sensors. Such an infrastructure is expected to be more energy efficient and scale well with the size of the sensor nodes. To support a large number of queries, we examine the problem of allocating queries across the base stations to minimize the total data communication cost among the sensors. In particular, we examine similarity-aware techniques that exploit the similarities among queries when allocating queries, so that queries that require data from a common set of sensor nodes are allocated to the same base stations. We first approximate the problem of allocating queries to K base stations as a max-K-cut problem, and adapts an existing solution to our context. However, the scheme only works in a static context, where all queries are known in advance. In order to operate in a dynamic environment with frequent query arrivals and termination, we further propose a novel similarity-aware strategy that allocates queries to base stations one at a time. We also propose several heuristics to order a batch of queries for incremental allocation. We conducted experiments to evaluate our proposed schemes, and our results show that our similarity-aware query allocation schemes can effectively exploit the sharing among queries to greatly reduce the communication cost.

#index 993094
#* Dynamic balanced storage in wireless sensor networks
#@ Yongxuan Lai;Hong Chen;Yufeng Wang
#t 2007
#c 4
#% 309466
#% 401227
#% 576977
#% 731091
#% 772633
#% 797877
#% 822526
#% 893141
#% 907518
#! Data-centric storage is an effective and important technique in the wireless sensor networks. It stores the sensing data according to their values by mapping them to some point in the network in order to avoid routing all the values outside the network and flooding the queries. However, in most data-centric storage schemes, there is a "hotspot" problem due to the skewness of data and randomness of the mapping functions. Large number of sensor readings (events) may be routed to the same point by the predefined hashed function. In this paper, we propose a new Dynamic BAlanced data-centric Storage (DBAS) scheme, a cooperative strategy between the base station and the in-network processing in wireless sensor network. Our scheme, which utilizes the rich resources in the base station and is aware of the data distributions of the network, dynamically adjusts the mappings from readings to the storage points to balance the storage and workload in the network, as well as to reduce the cost of storing these readings. Moreover, it takes advantage of perimeter routing algorithm of the GPSR routing protocol to store multiple copies of readings to improve the robustness of the network with little overhead. Simulation results show that DBAS is more balanced and energy efficient than the traditional data-centric storage mechanism in wireless sensor network.

#index 993095
#* SenseSwarm: a perimeter-based data acquisition framework for mobile sensor networks
#@ Demetrios Zeinalipour-Yazti;Panayiotis Andreou;Panos K. Chrysanthis;George Samaras
#t 2007
#c 4
#% 31686
#% 309433
#% 336865
#% 410276
#% 427022
#% 576977
#% 654482
#% 720835
#% 783738
#% 783739
#% 805466
#% 879269
#% 907005
#% 907518
#% 963436
#% 966976
#% 1208228
#! This paper assumes a set of n mobile sensors that move in the Euclidean plane as a swarm. Our objectives are to explore a given geographic region by detecting and aggregating spatio-temporal events of interest and to store these events in the network until the user requests them. Such a setting finds applications in environments where the user (i.e., the sink) is infrequently within communication range from the field deployment. Our framework, coined SenseSwarm, dynamically partitions the sensing devices into perimeter and core nodes. Data acquisition is scheduled at the perimeter in order to minimize energy consumption while storage and replication takes place at the core nodes which are physically and logically shielded to threats and obstacles. To efficiently identify the perimeter of the swarm we devise the Perimeter Algorithm (PA), an efficient distributed algorithm with a message complexity of O(p + n), where p denotes the number of nodes on the perimeter and n the overall number of nodes. For storage and replication we devise a spatio-temporal in-network aggregation scheme based on minimum bounding rectangles and minimum bounding cuboids. Our trace-driven experimentation shows that our framework can offer significant energy reductions while maintaining high data availability rates.

#index 993096
#* A graph-based approach to vehicle tracking in traffic camera video streams
#@ Hamid Haidarian Shahri;Galileo Namata;Saket Navlakha;Amol Deshpande;Nick Roussopoulos
#t 2007
#c 4
#% 760805
#% 902308
#% 937552
#! Vehicle tracking has a wide variety of applications from law enforcement to traffic planning and public safety. However, the image resolution of the videos available from most traffic camera systems, make it difficult to track vehicles based on unique identifiers like license plates. In many cases, vehicles with similar attributes are indistinguishable from one another due to image quality issues. Often, network bandwidth and power constraints limit the frame rate, as well. In this paper, we discuss the challenges of performing vehicle tracking queries over video streams from ubiquitous traffic cameras. We identify the limitations of tracking vehicles individually in such conditions and provide a novel graph-based approach using the identity of neighboring vehicles to improve the performance. We evaluate our approach using streaming video feeds from live traffic cameras available on the Internet. The results show that vehicle tracking is feasible, even for low quality and low frame rate traffic cameras. Additionally, exploitation of the attributes of neighboring vehicles significantly improves the performance.

#index 993097
#* Imagers as sensors: correlating plant CO2 uptake with digital visible-light imagery
#@ Josh Hyman;Eric Graham;Mark Hansen;Deborah Estrin
#t 2007
#c 4
#% 219847
#% 400847
#% 437405
#% 837846
#! There exist many natural phenomena where direct measurement is either impossible or extremely invasive. To obtain approximate measurements of these phenomena we can build prediction models based on other sensing modalities such as features extracted from data collected by an imager. These models are derived from controlled experiments performed under laboratory conditions, and can then be applied to the associated event in nature. In this paper we explore various different methods for generating such models and discuss their accuracy, robustness, and computational complexity. Given sufficiently computationally simple models, we can eventually push their computation down towards the sensor nodes themselves to reduce the amount of data required to both flow through the network and be stored in a database. The addition of these models turn in-situ imagers into powerful biological sensors, and image databases into useful records of biological activity.

#index 993098
#* Workflow support for wireless sensor and actor networks: a position paper
#@ Pablo Ezequiel Guerrero;Daniel Jacobi;Alejandro Buchmann
#t 2007
#c 4
#% 143889
#% 297915
#% 336865
#% 394022
#% 401228
#% 414174
#% 429193
#% 451429
#% 453371
#% 565496
#% 581040
#% 755199
#% 781482
#% 783722
#% 799146
#% 805466
#% 806214
#% 807251
#% 815425
#% 818422
#% 863307
#% 865222
#% 867597
#% 870671
#% 879220
#% 903008
#% 951762
#% 963583
#% 983474
#% 1112638
#% 1675409
#% 1863409
#! As initial challenges of wireless sensor and actor networks (WSANs) are overcome, their application possibilities evolve. For these applications to move mainstream, efficient programming methods are required which can be used by domain experts. So far, the question of how can WSANs be efficiently programmed remains unanswered. In this paper we examine proposed middleware approaches, and show that they have focused on data extraction rather than in-network actuation. We thus propose the usage of workflows as a means to define the logic that orchestrates the network activity, and introduce a language to express WSAN interactions. At this time, a concrete system is not given, but the paper discusses the relevant aspects towards one, and poses many questions for future research.

#index 993099
#* Declarative temporal data models for sensor-driven query processing
#@ Yanif Ahmad;Uğur Çetintemel
#t 2007
#c 4
#% 479459
#% 481093
#% 571071
#% 654456
#% 765402
#% 824724
#% 829308
#% 874976
#% 893102
#! Many sensor network applications monitor continuous phenomena by sampling, and fit time-varying models that capture the phenomena's behaviors. We introduce Pulse, a framework for processing continuous queries over these continuous-time data models. Pulse allows users to declaratively specify both their queries and models, and transforms these queries into simultaneous equation systems, which in many cases are significantly cheaper to process than a stream of discrete tuples. Pulse is able to guarantee user-defined error bounds between query results from continuous-time data models and sampled data, including cases of null results. We present a high-level overview of the design and architecture of Pulse and propose several query optimization techniques that are novel to our context, such as the simplification of our equation systems. We also discuss our plans for extending Pulse to support several novel model types, including differential equations and time series, and outline an abstraction to support query processing on these classes of models.

#index 993929
#* Proceedings of the 28th international conference on Very Large Data Bases
#@ 
#t 2002
#c 4

#index 993930
#* Data routing rather than databases: the meaning of the next wave of the web revolution to data management
#@ Adam Bosworth
#t 2002
#c 4
#! What is going to be as important in the next 20 years as relational databases were in the prior 20 years is the management of self-describing extensible messages. The net is undergoing a profound change as it moves from an entirely pull-oriented model into a push model. This latter model is far more biological in nature with an increasing amount of information flowing asynchronously through the system to form an InformationBus. The key challenges for the next 20 years will be storing, routing, querying, filtering, managing, and interacting with this bus in a manner that doesn't lead to total systems degradation. Predictive intelligent filtering and rules engines will become more important than querying. Driving factors for this revolution will be the need for push for portable devices due to their poor latency and intermittent communication, an increasing demand for timely information on fully connected devices, a huge rise in application to application integration through asynchronous messaging based on web services and a concomitant requirement for an entirely new type of message broker, and an increasing desire for intelligent agents to cope with information overload as all information becomes available all the time. The key enabling technology will be XML messages and the various technologies that will develop for handling XML ranging from transformation to compression to indexing to storage to programming languages.

#index 993931
#* Foundation matters
#@ C. J. Date
#t 2002
#c 4
#% 302769
#% 389885
#% 645159
#! This talk is meant as a wake-up call ... The foundation of the database field is, of course, the relational model. Sad to say, however, there are some in the database community--certainly in industry, and to some extent in academia also--who do not seem to be as familiar with that model as they ought to be; there are others who seem to think it is not very interesting or relevant to the day-today business of earning a living; and there are still others who seem to think all of the foundation-level problems have been solved. Indeed, there seems to be a widespread feeling that "the world has moved on," so to speak, and the relational model as such is somehow passé. In my opinion, nothing could be further from the truth! In this talk, I want to sketch the results of some of my own investigations into database foundations over the past twenty years or so; my aim is to convey some of the excitement and abiding interest that is still to be found in those investigations, with a view--I hope--to inspiring others in the field to become involved in such activities. First of all, almost all of the ideas I will be covering either are part of, or else build on top of, The Third Manifesto [1]. The Third Manifesto is a detailed proposal for the future direction of data and DBMSs. Like Codd's original papers on the relational model, it can be seen as an abstract blueprint for the design of a DBMS and the language interface to such a DBMS. Among many other things: • It shows that the relational model--and I do mean the relational model, not SQL--is a necessary and sufficient foundation on which to build "object/relational" DBMSs (sometimes called universal servers). • It also points out certain blunders that can unfortunately be observed in some of today's products (not to mention the SQL:1999 standard). • And it explores in depth the idea that a relational database, along with the relational operators, is really a logical system and shows how that idea leads to a solution to the view updating problem, among other things.

#index 993932
#* Wireless graffiti: data, data everywhere
#@ Tomasz Imielinski;Badri Nath
#t 2002
#c 4
#% 140613
#% 149228
#% 172874
#% 172876
#% 175253
#% 201897
#% 203867
#% 212808
#% 235853
#% 245015
#% 281506
#% 297913
#% 297915
#% 309430
#% 309433
#% 336865
#% 339204
#% 402611
#% 410216
#% 433932
#% 438425
#% 438456
#% 443127
#% 480965
#% 481777
#% 622758
#% 788231
#% 1768468
#% 1830004
#! In this paper, we take a retrospective look at the problem of querying and updating location dependent data in massively distributed mobile environments. Looking forward, we paint our vision of the future dataspace - physical space enhanced with embedded digital information. Finally we describe a few of the applications enabled by dataspace due to the availability of large scale ad-hoc sensor networks, short-range wireless communications, and fine-grain location information.

#index 993933
#* Self-tuning database technology and information services: from wishful thinking to viable engineering
#@ Gerhard Weikum;Axel Moenkeberg;Christof Hasse;Peter Zabback
#t 2002
#c 4
#% 2544
#% 43171
#% 102807
#% 152596
#% 152913
#% 152943
#% 170893
#% 210198
#% 244119
#% 248824
#% 258238
#% 268750
#% 275367
#% 300164
#% 309705
#% 333948
#% 334009
#% 335726
#% 397368
#% 397397
#% 401436
#% 443390
#% 456017
#% 460873
#% 480153
#% 480158
#% 480775
#% 480803
#% 480949
#% 481459
#% 482110
#% 571066
#% 979713
#! Automatic tuning has been an elusive goal for database technology for a long time and is becoming a pressing issue for modern E-services. This paper reviews and assesses the advances that have been made on this important subject during the last ten years. A major conclusion is that self-tuning database technology should be based on the paradigm of a feedback control loop, but is also bound to build on mathematical models and their proper engineering into system components. In addition, the composition of information services into truly self-tuning, higher-level E-services may require a radical departure towards simpler, highly componentized software architectures with narrow interfaces between RISC-style "autonomic" components.

#index 993934
#* REFEREE: an open framework for practical testing of recommender systems using ResearchIndex
#@ Dan Cosley;Steve Lawrence;David M. Pennock
#t 2002
#c 4
#% 124010
#% 173879
#% 202009
#% 202011
#% 220709
#% 260778
#% 266281
#% 283169
#% 304425
#% 340921
#% 342767
#% 397155
#% 420515
#% 438103
#% 445370
#% 465928
#% 466913
#% 495929
#% 528156
#% 528182
#% 529806
#% 1499571
#% 1650569
#! Automated recommendation (e.g., personalized product recommendation on an ecommerce web site) is an increasingly valuable service associated with many databases--typically online retail catalogs and web logs. Currently, a major obstacle for evaluating recommendation algorithms is the lack of any standard, public, real-world testbed appropriate for the task. In an attempt to fill this gap, we have created REFEREE, a framework for building recommender systems using ResearchIndex--a huge online digital library of computer science research papers--so that anyone in the research community can develop, deploy, and evaluate recommender systems relatively easily and quickly. Research Index is in many ways ideal for evaluating recommender systems, especially so-called hybrid recommenders that combine information filtering and collaborative filtering techniques. The documents in the database are associated with a wealth of content information (author, title, abstract, full text) and collaborative information (user behaviors), as well as linkage information via the citation structure. Our framework supports more realistic evaluation metrics that assess user buy-in directly, rather than resorting to offline metrics like prediction accuracy that may have little to do with end user utility. The sheer scale of ResearchIndex (over 500,000 documents with thousands of user accesses per hour) will force algorithm designers to make real-world trade-offs that consider performance, not just accuracy. We present our own tradeoff decisions in building an example hybrid recommender called PD-Live. The algorithm uses content-based similarity information to select a set of documents from which to recommend, and collaborative information to rank the documents. PD-Live performs reasonably well compared to other recommenders in ResearchIndex.

#index 993935
#* Adaptable similarity search using non-relevant information
#@ T. V. Ashwin;Rahul Gupta;Sugata Ghosal
#t 2002
#c 4
#% 136350
#% 194301
#% 223810
#% 227857
#% 227999
#% 232646
#% 309088
#% 333973
#% 420077
#% 421052
#% 437405
#% 479655
#% 479788
#% 480478
#% 1180245
#% 1775090
#! Many modern database applications require content-based similarity search capability in numeric attribute space. Further, users' notion of similarity varies between search sessions. Therefore online techniques for adaptively refining the similarity metric based on relevance feedback from the user are necessary. Existing methods use retrieved items marked relevant by the user to refine the similarity metric, without taking into account the information about non-relevant (or unsatisfactory) items. Consequently items in database close to non-relevant ones continue to be retrieved in further iterations. In this paper a robust technique is proposed to incorporate non-relevant information to efficiently discover the feasible search region. A decision surface is determined to split the attribute space into relevant and nonrelevant regions. The decision surface is composed of hyperplanes, each of which is normal to the minimum distance vector from a nonrelevant point to the convex hull of the relevant points. A similarity metric, estimated using the relevant objects is used to rank and retrieve database objects in the relevant region. Experiments on simulated and benchmark datasets demonstrate robustness and superior performance of the proposed technique over existing adaptive similarity search techniques.

#index 993936
#* Sideway value algebra for object-relational databases
#@ G. Özsoyoǧlu;A. Al-Hamdani;I. S. Altingövde;S. A. Özel;Ö. Ulusoy;Z. M. Özsoyoǧlu
#t 2002
#c 4
#% 55490
#% 136740
#% 227894
#% 248801
#% 300170
#% 333951
#% 390132
#% 397378
#% 442830
#% 479623
#% 479816
#% 480309
#% 584942
#% 602982
#! Using functions in various forms, recent database publications have assigned "scores", "preference values", and "probabilistic values" to object-relational database tuples. We generalize these functions and their evaluations as sideway functions and sideway values, respectively. Sideway values represent the advices (recommendations) of data creators or preferences of users, and are employed for the purposes of ranking query outputs and limiting output sizes during query evaluation as well as for application-dependent querying. This paper introduces SQL extensions and a sideway value algebra (SVA) for object-relational databases. SVA operators modify and propagate sideway values of base relations in automated and generic ways. We define the SVA join, and a recursive SVA closure operator, called TClosure. Output tuples of the SVA join operator are assigned sideway values on the basis of the sideway values and similarities of joined tuples, and the operator returns the highest ranking tuples. TClosure operator recursively expands a given set of objects (as tuples) according to a given regular expression of relationship types, and derives sideway values for the set of newly reached objects. We present evaluation algorithms for SVA join and TClosure operators, and report experimental results on the performance of the operators using the DBLP Bibliography data and synthetic data.

#index 993937
#* Database selection using actual physical and acquired logical collection resources in a massive domain-specific operational environment
#@ Jack G. Conrad;Xi S. Guo;Peter Jackson;Monem Meziou
#t 2002
#c 4
#% 111456
#% 132779
#% 169768
#% 169779
#% 172898
#% 194244
#% 194246
#% 194275
#% 262063
#% 262065
#% 262096
#% 262105
#% 267454
#% 280853
#% 280856
#% 282422
#% 287253
#% 287463
#% 306497
#% 309093
#% 309133
#% 316534
#% 340146
#% 342681
#% 424292
#% 479642
#% 479803
#% 481748
#% 567255
#% 682332
#% 748738
#% 855073
#! The continued growth of very large data environments such as Westlaw, Dialog, and the World Wide Web, increases the importance of effective and efficient database selection and searching. Recent research has focused on autonomous and automatic collection selection, searching, and results merging in distributed environments. These studies often rely on TREC data and queries for experimentation. We have extended this work to West's on-line production environment where thousands of legal, financial and news databases are accessed by up to a quarter-million professional users each day. Using the WIN natural language search engine, a cousin to UMass's INQUERY, along with a collection retrieval inference network (CORI) to provide database scoring, we examine the effect that a set of optimized parameters has on database selection performance. We also compare current language modeling techniques to this approach. Traditionally, West's information has been structured over 15,000 online databases, representing roughly 6 terabytes of textual data. Given the expense of running global searches in this environment, it is usually not practical to perform full document retrieval over the entire collection. It is therefore necessary to create a new infrastructure to support automatic database selection in the service of broader searching. In this research, we represent our operational environment in two distinct ways. First, we characterize the underlying physical databases that serve as a foundation for the entire Westlaw search system. Second, we create a rearchitected set of logical document collections that corresponds to classes of high level organizational concepts such as jurisdiction, practice area, and document-type. Keeping the end-user in mind, we focus on performance issues relating to optimal database selection, where domain experts have provided complete pre-hoc relevance judgments for collections characterized under each of our physical and logical database models.

#index 993938
#* Structural function inlining technique for structurally recursive XML queries
#@ Chang-Won Park;Jun-Ki Min;Chin-Wan Chung
#t 2002
#c 4
#% 178066
#% 287461
#% 428145
#% 462235
#% 481101
#% 504575
#% 571040
#! Structurally recursive XML queries are an important query class that follows the structure of XML data. At present, it is difficult for XQuery to type and optimize structurally recursive queries because of polymorphic recursive functions involved in the queries. In this paper, we propose a new technique called structural function inlining which inlines recursive functions used in a query by making good use of available type information. Based on the technique, we develop a new approach to typing and optimizing structurally recursive queries. The new approach yields a more precise result type for a query. Furthermore, it produces an optimal algebraic expression for the query with respect to the type information. When a structurally recursive query is applied to non-recursive XML data, our approach translates the query into a finitely nested iterations. We conducted several experiments with commonly used real-life and synthetic datasets. The experimental results show that the number of node lookups by our approach is on the average 3.7 times and up to 279.8 times smaller than that by the XQuery core's current approach in evaluating structurally recursive queries.

#index 993939
#* Efficient algorithms for processing XPath queries
#@ Georg Gottlob;Christoph Koch;Reinhard Pichler
#t 2002
#c 4
#% 299942
#% 378389
#% 378391
#% 378393
#% 397375
#% 400361
#% 473117
#! Our experimental analysis of several popular XPath processors reveals a striking fact: Query evaluation in each of the systems requires time exponential in the size of queries in the worst case. We show that XPath can be processed much more efficiently, and propose main-memory algorithms for this problem with polynomial-time combined query evaluation complexity. Moreover, we present two fragments of XPath for which linear-time query processing algorithms exist.

#index 993940
#* Incorporating XSL processing into database engines
#@ Guido Moerkotte
#t 2002
#c 4
#% 136740
#% 397366
#% 462171
#% 464988
#% 479806
#% 479956
#% 480152
#% 504574
#! The two observations that 1) many XML documents are stored in a database or generated from data stored in a database and 2) processing these documents with XSL stylesheet processors is an important, often recurring task justify a closer look at the current situation. Typically, the XML document is retrieved or constructed from the database, exported, parsed, and then processed by a special XSL processor. This cumbersome process clearly sets the goal to incorporate XSL stylesheet processing into the database engine. We describe one way to reach this goal by translating XSL stylesheets into algebraic expressions. Further, we present algorithms to optimize the template rule selection process and the algebraic expression resulting from the translation. Along the way, we present several undecidability results hinting at the complexity of the problem on hand.

#index 993941
#* Optimizing view queries in ROLEX to support navigable result trees
#@ P. Bohannon;S. Ganguly;H. F. Korth;P. P. S. Narayan;P. Shenoy
#t 2002
#c 4
#% 32878
#% 248787
#% 273940
#% 287005
#% 300177
#% 309851
#% 333935
#% 411554
#% 461897
#% 479950
#% 479954
#% 479956
#% 480091
#% 481909
#% 565457
#% 632032
#% 659924
#! An increasing number of applications use XML data published from relational databases. For speed and convenience, such applications routinely cache this XML data locally and access it through standard navigational interfaces such as DOM, sacrificing the consistency and integrity guarantees provided by a DBMS for speed. The ROLEX system is being built to extend the capabilities of relational database systems to deliver fast, consistent and navigable XML views of relational data to an application via a virtual DOM interface. This interface translates navigation operations on a DOM tree into execution-plan actions, allowing a spectrum of possibilities for lazy materialization. The ROLEX query optimizer uses a characterization of the navigation behavior of an application, and optimizes view queries to minimize the expected cost of that navigation. This paper presents the architecture of ROLEX, including its model of query execution and the query optimizer. We demonstrate with a performance study the advantages of the ROLEX approach and the importance of optimizing query execution for navigation.

#index 993942
#* Chip-secured data access: confidential data on untrusted servers
#@ Luc Bouganim;Philippe Pucheral
#t 2002
#c 4
#% 236327
#% 290825
#% 440254
#% 480153
#% 511600
#% 566139
#% 572304
#% 661973
#% 978215
#% 978249
#! The democratization of ubiquitous computing (access data anywhere, anytime, anyhow), the increasing connection of corporate databases to the Internet and the today's natural resort to Web-hosting companies strongly emphasize the need for data confidentiality. Database servers arouse user's suspicion because no one can fully trust traditional security mechanisms against more and more frequent and malicious attacks and no one can be fully confident on an invisible DBA administering confidential data. This paper gives an in-depth analysis of existing security solutions and concludes on the intrinsic weakness of the traditional server-based approach to preserve data confidentiality. With this statement in mind, we propose a solution called C-SDA (Chip-Secured Data Access), which enforces data confidentiality and controls personal privileges thanks to a client-based security component acting as a mediator between a client and an encrypted database. This component is embedded in a smartcard to prevent any tampering to occur. This cooperation of hardware and software security components constitutes a strong guarantee against attacks threatening personal as well as business data.

#index 993943
#* Hippocratic databases
#@ Rakesh Agrawal;Jerry Kiernan;Ramakrishnan Srikant;Yirong Xu
#t 2002
#c 4
#% 149
#% 1868
#% 36683
#% 53706
#% 67453
#% 158832
#% 164560
#% 228355
#% 249181
#% 287297
#% 287298
#% 287794
#% 287795
#% 317991
#% 340475
#% 340827
#% 346931
#% 348144
#% 368248
#% 374401
#% 384014
#% 388487
#% 389077
#% 390132
#% 397367
#% 428401
#% 437974
#% 442709
#% 480496
#% 480499
#% 482049
#% 616923
#% 659992
#% 664705
#% 781216
#% 1393825
#! The Hippocratic Oath has guided the conduct of physicians for centuries. Inspired by its tenet of preserving privacy, we argue that future database systems must include responsibility for the privacy of data they manage as a founding tenet. We enunciate the key privacy principles for such Hippocratic database systems. We propose a strawman design for Hippocratic databases, identify the technical challenges and problems in designing such databases, and suggest some approaches that may lead to solutions. Our hope is that this paper will serve to catalyze a fruitful and exciting direction for future database research.

#index 993944
#* Watermarking relational databases
#@ Rakesh Agrawal;Jerry Kiernan
#t 2002
#c 4
#% 256418
#% 389077
#% 616923
#% 632213
#% 1848706
#! We enunciate the need for watermarking database relations to deter their piracy, identify the unique characteristics of relational data which pose new challenges for watermarking, and provide desirable properties of a watermarking system for relational data. A watermark can be applied to any database relation having attributes which are such that changes in a few of their values do not affect the applications. We then present an effective watermarking technique geared for relational data. This technique ensures that some bit positions of some of the attributes of some of the tuples contain specific values. The tuples, attributes within a tuple, bit positions in an attribute, and specific bit values are all algorithmically determined under the control of a private key known only to the owner of the data. This bit pattern constitutes the watermark. Only if one has access to the private key can the watermark be detected with high probability. Detecting the watermark neither requires access to the original data nor the watermark. The watermark can be detected even in a small subset of a watermarked relation as long as the sample contains some of the marks. Our extensive analysis shows that the proposed technique is robust against various forms of malicious attacks and updates to the data. Using an implementation running on DB2, we also show that the performance of the algorithms allows for their use in real world applications.

#index 993945
#* Parametric query optimization for linear and piecewise linear cost functions
#@ Arvind Hulgeri;S. Sudarshan
#t 2002
#c 4
#% 58375
#% 172900
#% 411554
#% 479786
#% 480955
#% 565457
#% 571088
#! The cost of a query plan depends on many parameters, such as predicate selectivities and available memory, whose values may not be known at optimization time. Parametric query optimization (PQO) optimizes a query into a number of candidate plans, each optimal for some region of the parameter space. We first propose a solution for the PQO problem for the case when the cost functions are linear in the given parameters. This solution is minimally intrusive in the sense that an existing query optimizer can be used with minor modifications: the solution invokes the conventional query optimizer multiple times, with different parameter values. We then propose a solution for the PQO problem for the case when the cost functions are piecewise-linear in the given parameters. The solution is based on modification of an existing query optimizer. This solution is quite general, since arbitrary cost functions can be approximated to piecewise linear form. Both the solutions work for an arbitrary number of parameters.

#index 993946
#* Plan selection based on query clustering
#@ Antara Ghosh;Jignashu Parikh;Vibhuti S. Sengar;Jayant R. Haritsa
#t 2002
#c 4
#% 36117
#% 152902
#% 169337
#% 172900
#% 210173
#% 300166
#% 375388
#% 411554
#% 442996
#% 462025
#% 479786
#! Query optimization is a computationally intensive process, especially for complex queries. We present here a tool, called PLASTIC, that can be used by query optimizers to amortize the optimization cost. Our scheme groups similar queries into clusters and uses the optimizer-generated plan for the cluster representative to execute all future queries assigned to the cluster. Query similarity is evaluated based on a comparison of query structures and the associated table schemas and statistics, and a classifier is employed for efficient cluster assignments. Experiments with a variety of queries on a commercial optimizer show that PLASTIC predicts the correct plan choice in most cases, thereby providing significantly improved query optimization times. Further, when errors are made, the additional execution cost incurred due to the sub-optimal plan choices is marginal.

#index 993947
#* Generic database cost models for hierarchical memory systems
#@ Stefan Manegold;Peter Boncz;Martin L. Kersten
#t 2002
#c 4
#% 68142
#% 136740
#% 210190
#% 248787
#% 300194
#% 443513
#% 479819
#% 479984
#% 480119
#% 480464
#% 480821
#% 566122
#% 571047
#% 571056
#! Accurate prediction of operator execution time is a prerequisite for database query optimization. Although extensively studied for conventional disk-based DBMSs, cost modeling in main-memory DBMSs is still an open issue. Recent database research has demonstrated that memory access is more and more becoming a significant-- if not the major--cost component of database operations. If used properly, fast but small cache memories--usually organized in cascading hierarchy between CPU and main memory--can help to reduce memory access costs. However, they make the cost estimation problem more complex. In this article, we propose a generic technique to create accurate cost functions for database operations. We identify a few basic memory access patterns and provide cost functions that estimate their access costs for each level of the memory hierarchy. The cost functions are parameterized to accommodate various hardware characteristics appropriately. Combining the basic patterns, we can describe the memory access patterns of database operations. The cost functions of database operations can automatically be derived by combining the basic patterns' cost functions accordingly. To validate our approach, we performed experiments using our DBMS prototype Monet. The results presented here confirm the accuracy of our cost models for different operations. Aside from being useful for query optimization, our models provide insight to tune algorithms not only in a main-memory DBMS, but also in a disk-based DBMS with a large main-memory buffer cache.

#index 993948
#* Streaming queries over streaming data
#@ Sirish Chandrasekaran;Michael J. Franklin
#t 2002
#c 4
#% 116082
#% 172950
#% 198467
#% 227861
#% 227941
#% 239969
#% 248795
#% 280429
#% 297191
#% 300167
#% 300179
#% 333850
#% 333926
#% 333938
#% 340635
#% 379445
#% 397353
#% 428155
#% 461923
#% 480296
#% 480500
#% 631962
#% 632057
#% 659962
#% 715191
#% 979303
#% 993949
#! Recent work on querying data streams has focused on systems where newly arriving data is processed and continuously streamed to the user in real-time. In many emerging applications, however, ad hoc queries and/or intermittent connectivity also require the processing of data that arrives prior to query submission or during a period of disconnection. For such applications, we have developed PSoup, a system that combines the processing of ad-hoc and continuous queries by treating data and queries symmetrically, allowing new queries to be applied to old data and new data to be applied to old queries. PSoup also supports intermittent connectivity by separating the computation of query results from the delivery of those results. PSoup builds on adaptive query processing techniques developed in the Telegraph project at UC Berkeley. In this paper, we describe PSoup and present experiments that demonstrate the effectiveness of our approach.

#index 993949
#* Monitoring streams: a new class of data management applications
#@ Don Carney;Uǧur Çetintemel;Mitch Cherniack;Christian Convey;Sangdon Lee;Greg Seidman;Michael Stonebraker;Nesime Tatbul;Stan Zdonik
#t 2002
#c 4
#% 189378
#% 198467
#% 227883
#% 239984
#% 273911
#% 279905
#% 300167
#% 300179
#% 333926
#% 397353
#% 428155
#% 442714
#% 442832
#% 442967
#% 480296
#% 480642
#% 480768
#% 482088
#% 631962
#% 660004
#% 979303
#% 1863831
#! This paper introduces monitoring applications, which we will show differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS that is currently under construction at Brandeis University, Brown University, and M.I.T. We describe the basic system architecture, a stream-oriented set of operators, optimization tactics, and support for real-time operation.

#index 993950
#* A transducer-based XML query processor
#@ Bertram Ludäscher;Pratik Mukhopadhyay;Yannis Papakonstantinou
#t 2002
#c 4
#% 248820
#% 273910
#% 299944
#% 300167
#% 333926
#% 333935
#% 340305
#% 340635
#% 378392
#% 378408
#% 379445
#% 397352
#% 397353
#% 397354
#% 397370
#% 480152
#% 480296
#% 487257
#% 562456
#% 903341
#! The XML Stream Machine (XSM) system is a novel XQuery processing paradigm that is tuned to the efficient processing of sequentially accessed XML data (streams). The system compiles a given XQuery into an XSM, which is an XML stream transducer, i.e., an abstract device that takes as input one or more XML data streams and produces one or more output streams, potentially using internal buffers. We present a systematic way to translate XQueries into efficient XSMs: First the XQuery is translated into a network of XSMs that correspond to the basic operators of the XQuery language and exchange streams. The network is reduced to a single XSM by repeated application of an XSM composition operation that is optimized to reduce the number of tests and actions that the XSM performs as well as the number of intermediate buffers that it uses. Finally, the optimized XSM is compiled into a C program. First empirical results illustrate the performance benefits of the XSM-based processor.

#index 993951
#* Updates for structure indexes
#@ Raghav Kaushik;Philip Bohannon;Jeffrey F. Naughton;Pradeep Shenoy
#t 2002
#c 4
#% 31484
#% 236416
#% 281149
#% 291299
#% 397359
#% 397360
#% 397379
#% 462062
#% 479465
#% 480488
#! The problem of indexing path queries in semistructured/XML databases has received considerable attention recently, and several proposals have advocated the use of structure indexes as supporting data structures for this problem. In this paper, we investigate efficient update algorithms for structure indexes. We study two kinds of updates -- the addition of a subgraph, intended to represent the addition of a new file to the database, and the addition of an edge, to represent a small incremental change. We focus on three instances of structure indexes that are based on the notion of graph bisimilarity. We propose algorithms to update the bisimulation partition for both kinds of updates and show how they extend to these indexes. Our experiments on two real world data sets show that our update algorithms are an order of magnitude faster than dropping and rebuilding the index. To the best of our knowledge, no previous work has addressed updates for structure indexes based on graph bisimilarity.

#index 993952
#* RE-Tree: an efficient index structure for regular expressions
#@ Chee-Yong Chan;Minos Garofalakis;Rajeev Rastogi
#t 2002
#c 4
#% 61792
#% 86950
#% 120649
#% 152938
#% 212287
#% 273922
#% 281764
#% 300157
#% 386377
#% 404772
#% 408396
#% 427199
#% 480296
#% 520221
#% 659987
#% 659995
#! Due to their expressive power, Regular Expressions (REs) are quickly becoming an integral part of language specifications for several important application scenarios. Many of these applications have to manage huge databases of RE specifications and need to provide an effective matching mechanism that, given an input string, quickly identifies the REs in the database that match it. In this paper, we propose the RE-tree, a novel index structure for large databases of RE specifications. Given an input query string, the RE-tree speeds up the retrieval of matching REs by focusing the search and comparing the input string with only a small fraction of REs in the database. Even though the RE-tree is similar in spirit to other tree-based structures that have been proposed for indexing multi-dimensional data, RE indexing is significantly more challenging since REs typically represent infinite sets of strings with no well-defined notion of spatial locality. To address these new challenges, our RE-tree index structure relies on novel measures for comparing the relative sizes of infinite regular languages. We also propose innovative solutions for the various RE-tree operations, including the effective splitting of RE-tree nodes and computing a "tight" bounding RE for a collection of REs. Finally, we demonstrate how sampling-based approximation algorithms can be used to significantly speed up the performance of RE-tree operations. Our experimental results with synthetic data sets indicate that the REtree is very effective in pruning the search space and easily outperforms naive sequential search approaches.

#index 993953
#* Efficient structural joins on indexed XML documents
#@ Shu-Yao Chien;Zografoula Vagena;Donghui Zhang;Vassilis J. Tsotras;Carlo Zaniolo
#t 2002
#c 4
#% 23651
#% 58371
#% 86950
#% 152937
#% 182672
#% 210212
#% 281149
#% 281150
#% 287070
#% 300157
#% 333981
#% 397358
#% 397375
#% 427199
#% 443130
#% 443181
#% 462062
#% 462235
#% 479453
#% 479465
#% 479806
#% 479956
#% 480489
#% 480656
#% 480817
#% 480827
#% 504578
#% 565265
#% 571296
#% 659999
#% 660000
#! Queries on XML documents typically combine selections on element contents, and, via path expressions, the structural relationships between tagged elements. Structural joins are used to find all pairs of elements satisfying the primitive structural relationships specified in the query, namely, parent-child and ancestor-descendant relationships. Efficient support for structural joins is thus the key to efficient implementations of XML queries. Recently proposed node numbering schemes enable the capturing of the XML document structure using traditional indices (such as B+-trees or R-trees). This paper proposes efficient structural join algorithms in the presence of tag indices. We first concentrate on using B+- trees and show how to expedite a structural join by avoiding collections of elements that do not participate in the join. We then introduce an enhancement (based on sibling pointers) that further improves performance. Such sibling pointers are easily implemented and dynamically maintainable. We also present a structural join algorithm that utilizes R-trees. An extensive experimental comparison shows that the B+-tree structural joins are more robust. Furthermore, they provide drastic improvement gains over the current state of the art.

#index 993954
#* Shooting stars in the sky: an online algorithm for skyline queries
#@ Donald Kossmann;Frank Ramsak;Steffen Rost
#t 2002
#c 4
#% 2115
#% 86950
#% 100803
#% 201876
#% 227894
#% 287222
#% 288976
#% 289148
#% 333847
#% 438135
#% 465167
#% 480671
#! Skyline queries ask for a set of interesting points from a potentially large set of data points. If we are traveling, for instance, a restaurant might be interesting if there is no other restaurant which is nearer, cheaper, and has better food. Skyline queries retrieve all such interesting restaurants so that the user can choose the most promising one. In this paper, we present a new online algorithm that computes the Skyline. Unlike most existing algorithms that compute the Skyline in a batch, this algorithm returns the first results immediately, produces more and more results continuously, and allows the user to give preferences during the running time of the algorithm so that the user can control what kind of results are produced next (e.g., rather cheap or rather near restaurants).

#index 993955
#* Continuous nearest neighbor search
#@ Yufei Tao;Dimitris Papadias;Qiongmao Shen
#t 2002
#c 4
#% 86950
#% 201876
#% 237187
#% 248797
#% 248804
#% 273706
#% 287466
#% 300162
#% 300174
#% 397377
#% 427199
#% 443327
#% 461923
#% 479649
#% 480093
#% 480632
#% 481947
#% 495433
#! A continuous nearest neighbor query retrieves the nearestneighbor (NN) of every point on a line segment (e.g., "find all mynearest gas stations during my route from points to pointe. The result contains a set of (point, interval) tuples,such that point is the NN of all points in the correspondinginterval. Existing methods for continuous nearest neighbor searchare based on the repetitive application of simple NN algorithms,which incurs significant overhead. In this paper we proposetechniques that solve the problem by performing a single query forthe whole input segment. As a result the cost, depending on thequery and dataset characteristics, may drop by orders of magnitude.In addition, we propose analytical models for the expected size ofthe output, as well as, the cost of query processing, and extendout techniques to several variations of the problem.

#index 993956
#* Progressive merge join: a generic and non-blocking sort-based join algorithm
#@ Jens-Peter Dittrich;Bernhard Seeger;David Scot Taylor;Peter Widmayer
#t 2002
#c 4
#% 2115
#% 13041
#% 77937
#% 136740
#% 210187
#% 210353
#% 227883
#% 227932
#% 273910
#% 273911
#% 316536
#% 333973
#% 342595
#% 442965
#% 443326
#% 462070
#% 463751
#% 479463
#% 479797
#% 479928
#% 480825
#% 526847
#% 632105
#! Many state-of-the-art join-techniques require the input relations to be almost fully sorted before the actual join processing starts. Thus, these techniques start producing first results only after a considerable time period has passed. This blocking behaviour is a serious problem when consequent operators have to stop processing, in order to wait for first results of the join. Furthermore, this behaviour is not acceptable if the result of the join is visualized or/ and requires user interaction. These are typical scenarios for data mining applications. The, off-time' of existing techniques even increases with growing problem sizes. In this paper, we propose a generic technique called Progressive Merge Join (PMJ) that eliminates the blocking behaviour of sort-based join algorithms. The basic idea behind PMJ is to have the join produce results, as early as the external mergesort generates initial runs. Hence, it is possible for PMJ to return first results very early. This paper provides the basic algorithms and the generic framework of PMJ, as well as use-cases for different types of joins. Moreover, we provide a generic online selectivity estimator with probabilistic quality guarantees. For similarity joins in particular, first non-blocking join algorithms are derived from applying PMJ to the state-of-the-art techniques. We have implemented PMJ as part of an object-relational cursor algebra. A set of experiments shows that a substantial amount of results are produced, even before the input relationas would have been sorted. We observed only a moderate increase in the total runtime compared to the blocking counterparts.

#index 993957
#* Foundations of preferences in database systems
#@ Werner Kießling
#t 2002
#c 4
#% 183909
#% 300170
#% 326523
#% 333854
#% 333951
#% 334012
#% 379207
#% 458873
#% 461211
#% 465167
#% 471123
#% 480330
#% 480671
#% 481291
#% 504581
#% 665415
#% 994017
#! Personalization of e-services poses new challenges to database technology, demanding a powerful and flexible modeling technique for complex preferences. Preference queries have to be answered cooperatively by treating preferences as soft constraints, attempting a best possible match-making. We propose a strict partial order semantics for preferences, which closely matches people's intuition. A variety of natural and of sophisticated preferences are covered by this model. We show how to inductively construct complex preferences by means of various preference constructors. This model is the key to a new discipline called preference engineering and to a preference algebra. Given the Best-Matches-Only (BMO) query model we investigate how complex preference queries can be decomposed into simpler ones, preparing the ground for divide & conquer algorithms. Standard SQL and XPATH can be extended seamlessly by such preferences (presented in detail in the companion paper [15]). We believe that this model is appropriate to extend database technology towards effective support of personalization.

#index 993958
#* Multi-dimensional regression analysis of time-series data streams
#@ Yixin Chen;Guozhu Dong;Jiawei Han;Benjamin W. Wah;Jianyong Wang
#t 2002
#c 4
#% 172949
#% 210182
#% 223781
#% 227880
#% 273916
#% 333925
#% 333926
#% 333931
#% 342600
#% 420053
#% 428155
#% 459025
#% 463903
#% 464851
#% 464994
#% 477968
#% 480156
#% 480628
#% 480630
#% 480820
#% 481609
#% 481611
#% 481951
#% 594012
#! Real-time production systems and other dynamic environments often generate tremendous (potentially infinite) amount of stream data; the volume of data is too huge to be stored on disks or scanned multiple times. Can we perform on-line, multi-dimensional analysis and data mining of such data to alert people about dramatic changes of situations and to initiate timely, high-quality responses? This is a challenging task. In this paper, we investigate methods for on-line, multi-dimensional regression analysis of time-series stream data, with the following contributions: (1) our analysis shows that only a small number of compressed regression measures instead of the complete stream of data need to be registered for multi-dimensional linear regression analysis, (2) to facilitate on-line stream data analysis, a partially materialized data cube model, with regression as measure, and a tilt time frame as its time dimension, is proposed to minimize the amount of data to be retained in memory or stored on disks, and (3) an exception-guided drilling approach is developed for on-line, multi-dimensional exception-based regression analysis. Based on this design, algorithms are proposed for efficient analysis of time-series data streams. Our performance study compares the proposed algorithms and identifies the most memory- and time- efficient one for multi-dimensional stream data analysis.

#index 993959
#* Comparing data streams using Hamming norms (how to zero in)
#@ Graham Cormode;Mayur Datar;Piotr Indyk;S. Muthukrishnan
#t 2002
#c 4
#% 2833
#% 132779
#% 248821
#% 278835
#% 293714
#% 299989
#% 336610
#% 338425
#% 340581
#% 342600
#% 397369
#% 397385
#% 397414
#% 428155
#% 480156
#% 480628
#% 480805
#% 481749
#% 593957
#% 594012
#% 594029
#% 632029
#% 659979
#% 660004
#% 963734
#% 1863141
#! Massive data streams are now fundamental to many data processing applications. For example, Internet routers produce large scale diagnostic data streams. Such streams are rarely stored in traditional databases, and instead must be processed "on the fly" as they are produced. Similarly, sensor networks produce multiple data streams of observations from their sensors. There is growing focus on manipulating data streams, and hence, there is a need to identify basic operations of interest in managing data streams, and to support them efficiently. We propose computation of the Hamming norm as a basic operation of interest. The Hamming norm formalises ideas that are used throughout data processing. When applied to a single stream, the Hamming norm gives the number of distinct items that are present in that data stream, which is a statistic of great interest in databases. When applied to a pair of streams, the Hamming norm gives an important measure of (dis)similarity: the number of unequal item counts in the two streams. Hamming norms have many uses in comparing data streams. We present a novel approximation technique for estimating the Hamming norm for massive data streams; this relies on what we call the "l0 sketch" and we prove its accuracy. We test our approximation method on a large quantity of synthetic and real stream data, and show that the estimation is accurate to within a few percentage points.

#index 993960
#* Approximate frequency counts over data streams
#@ Gurmeet Singh Manku;Rajeev Motwani
#t 2002
#c 4
#% 1331
#% 2833
#% 46803
#% 69273
#% 190611
#% 201894
#% 214073
#% 248812
#% 273898
#% 273907
#% 273916
#% 300120
#% 309747
#% 333925
#% 333926
#% 333931
#% 338425
#% 344400
#% 379445
#% 479795
#% 479804
#% 480465
#% 480628
#% 481290
#% 481754
#% 481779
#% 492912
#% 593957
#! We present algorithms for computing frequency counts exceeding a user-specified threshold over data streams. Our algorithms are simple and have provably small memory footprints. Although the output is approximate, the error is guaranteed not to exceed a user-specified parameter. Our algorithms can easily be deployed for streams of singleton items like those found in IP network monitoring. We can also handle streams of variable sized sets of items exemplified by a sequence of market basket transactions at a retail store. For such streams, we describe an optimized implementation to compute frequent itemsets in a single pass.

#index 993961
#* StatStream: statistical monitoring of thousands of data streams in real time
#@ Yunyue Zhu;Dennis Shasha
#t 2002
#c 4
#% 172949
#% 227857
#% 227924
#% 273907
#% 310488
#% 310500
#% 316560
#% 317468
#% 333926
#% 333931
#% 333941
#% 379445
#% 428155
#% 460862
#% 464196
#% 480146
#% 480628
#% 534183
#% 594012
#% 631920
#% 631923
#% 632090
#! Consider the problem of monitoring tens of thousands of time series data streams in an online fashion and making decisions based on them. In addition to single stream statistics such as average and standard deviation, we also want to find high correlations among all pairs of streams. A stock market trader might use such a tool to spot arbitrage opportunities. This paper proposes efficient methods for solving this problem based on Discrete Fourier Transforms and a three level time interval hierarchy. Extensive experiments on synthetic data and real world financial trading data show that our algorithm beats the direct computation approach by several orders of magnitude. It also improves on previous Fourier Transform approaches by allowing the efficient computation of time-delayed correlation over any size sliding window and any time delay. Correlation also lends itself to an efficient grid-based data structure. The result is the first algorithm that we know of to compute correlations over thousands of data streams in real time. The algorithm is incremental, has fixed response time, and can monitor the pairwise correlations of 10,000 streams on a single PC. The algorithm is embarrassingly parallelizable.

#index 993962
#* Optimizing result prefetching in web search engines with segmented indices
#@ Ronny Lempel;Shlomo Moran
#t 2002
#c 4
#% 183334
#% 248794
#% 249153
#% 268079
#% 295801
#% 298181
#% 306468
#% 330706
#% 339621
#% 340141
#% 356652
#% 504881
#% 593792
#% 616169
#! We study the process in which search engines with segmented indices serve queries. In particular, we investigate the number of result pages which search engines should prepare during the query processing phase. Search engine users have been observed to browse through very few pages of results for queries which they submit. This behavior of users suggests that prefetching many results upon processing an initial query is not efficient, since most of the prefetched results will not be requested by the user who initiated the search. However, a policy which abandons result prefetching in favor of retrieving just the first page of search results might not make optimal use of system resources as well. We argue that for a certain behavior of users, engines should prefetch a constant number of result pages per query. We define a concrete query processing model for search engines with segmented indices, and analyze the cost of such prefetching policies. Based on these costs, we show how to determine the constant which optimizes the prefetching policy. Our results are mostly applicable to local index partitions of the inverted files, but are also applicable to processing of short queries in global index architectures.

#index 993963
#* I/O-conscious data preparation for large-scale web search engines
#@ Maxim Lifantsev;Tzi-cker Chiueh
#t 2002
#c 4
#% 69239
#% 268079
#% 268087
#% 268186
#% 280833
#% 282905
#% 309743
#% 309747
#% 309749
#% 330706
#% 340141
#% 341100
#% 424292
#% 963651
#% 1908455
#! Given that commercial search engines cover billions of web pages, efficiently managing the corresponding volumes of disk-resident data needed to answer user queries quickly is a formidable data manipulation challenge. We present a general technique for efficiently carrying out large sets of simple transformation or querying operations over external-memory data tables. It greatly reduces the number of performed disk accesses and seeks by maximizing the temporal locality of data access and organizing most of the necessary disk accesses into long sequential reads or writes of data that is reused many times while in memory. This technique is based on our experience from building a functionally complete and fully operational web search engine called Yuntis. As such, it is in particular well suited for most data manipulation tasks in a modern web search engine and is employed throughout Yuntis. The key idea of this technique is co-ordinated partitioning of related data tables and corresponding partitioning and delayed batched execution of the transformation and querying operations that work with the data. This data and processing partitioning is naturally compatible with distributed data storage and parallel execution on a cluster of workstations. Empirical measurements on the Yuntis prototype demonstrate that our technique can improve the performance of external-memory data preparation runs by a factor of 100 versus a straightforward implementation.

#index 993964
#* Distributed search over the hidden web: hierarchical database sampling and selection
#@ Panagiotis G. Ipeirotis;Luis Gravano
#t 2002
#c 4
#% 46803
#% 136350
#% 194246
#% 215227
#% 227891
#% 248228
#% 252472
#% 262063
#% 267454
#% 273926
#% 280853
#% 280856
#% 281354
#% 282422
#% 287463
#% 301225
#% 309133
#% 309783
#% 316534
#% 333932
#% 337277
#% 340146
#% 406493
#% 458379
#% 479642
#% 567255
#% 1499571
#! Many valuable text databases on the web have non-crawlable contents that are "hidden" behind search interfaces. Metasearchers are helpful tools for searching over many such databases at once through a unified query interface. A critical task for a metasearcher to process a query efficiently and effectively is the selection of the most promising databases for the query, a task that typically relies on statistical summaries of the database contents. Unfortunately, web-accessible text databases do not generally export content summaries. In this paper, we present an algorithm to derive content summaries from "uncooperative" databases by using "focused query probes," which adaptively zoom in on and extract documents that are representative of the topic coverage of the databases. Our content summaries are the first to include absolute document frequency estimates for the database words. We also present a novel database selection algorithm that exploits both the extracted content summaries and a hierarchical classification of the databases, automatically derived during probing, to compensate for potentially incomplete content summaries. Finally, we evaluate our techniques thoroughly using a variety of databases, including 50 real web-accessible text databases. Our experiments indicate that our new content-summary construction technique is efficient and produces more accurate summaries than those from previously proposed strategies. Also, our hierarchical database selection algorithm exhibits significantly higher precision than its flat counterparts.

#index 993965
#* Exact indexing of dynamic time warping
#@ Eamonn Keogh
#t 2002
#c 4
#% 137711
#% 172949
#% 201876
#% 201893
#% 227924
#% 237204
#% 248797
#% 330932
#% 333941
#% 427199
#% 462231
#% 466260
#% 480146
#% 481609
#% 564263
#% 578400
#% 586837
#% 632088
#% 659971
#! The problem of indexing time series has attracted much research interest in the database community. Most algorithms used to index time series utilize the Euclidean distance or some variation thereof. However is has been forcefully shown that the Euclidean distance is a very brittle distance measure. Dynamic Time Warping (DTW) is a much more robust distance measure for time series, allowing similar shapes to match even if they are out of phase in the time axis. Because of this flexibility, DTW is widely used in science, medicine, industry and finance. Unfortunately however, DTW does not obey the triangular inequality, and thus has resisted attempts at exact indexing. Instead, many researchers have introduced approximate indexing techniques, or abandoned the idea of indexing and concentrated on speeding up sequential search. In this work we introduce a novel technique for the exact indexing of DTW. We prove that our method guarantees no false dismissals and we demonstrate its vast superiority over all competing approaches in the largest and most comprehensive set of time series indexing experiments ever undertaken.

#index 993966
#* Adaptive index structures
#@ Yufei Tao;Dimitris Papadias
#t 2002
#c 4
#% 86950
#% 116084
#% 172902
#% 191595
#% 201921
#% 248812
#% 273887
#% 333947
#% 333983
#% 427199
#% 443327
#% 480093
#% 480471
#% 481956
#! Traditional indexes aim at optimizing the node accesses during query processing, which, however, does not necessarily minimize the total cost due to the possibly large number of random accesses. In this paper, we propose a general framework for adaptive indexes that improve overall query cost. The performance gain is achieved by allowing index nodes to contain a variable number of disk pages. Update algorithms dynamically re-structure adaptive indexes depending on the data and query characteristics. Extensive experiments show that adaptive B- and R-trees significantly outperform their conventional counterparts, while incurring minimal update overhead.

#index 993967
#* A case for fractured mirrors
#@ Ravishankar Ramamurthy;David J. DeWitt;Qi Su
#t 2002
#c 4
#% 25017
#% 43171
#% 86931
#% 152942
#% 152944
#% 172939
#% 227861
#% 244119
#% 251474
#% 286258
#% 287672
#% 300185
#% 390132
#% 464177
#% 464816
#% 480119
#% 480281
#% 480629
#% 480821
#! The Decomposition Storage Model (DSM) vertically partitions all attributes of a given relation. DSM has excellent I/O behavior when the number of attributes touched in the query is small. It also has a better cache footprint than the n N-ary storage model (NSM) that is used by most database system. However, DSM incurs a high cost in reconstructing the original tuple from the partitions. We first revisit some of the performance problems associated with DSM. We suggest a simple indexing strategy and compare different reconstruction algorithms. The paper then proposes a new mirroring scheme, termed fractured mirrors, using both NSM and DSM models. This scheme combines the best aspects of both models, along with the added benefit of mirroring to better serve an ad-hoc query workload. A prototype system has been built using the Shore storage manager and performance is evaluated using queries from the TPC-H workload.

#index 993968
#* XPathLearner: an on-line self-tuning Markov histogram for XML path selectivity estimation
#@ Lipyeow Lim;Min Wang;Sriram Padmanabhan;Jeffrey Scott Vitter;Ronald Parr
#t 2002
#c 4
#% 92148
#% 176024
#% 210189
#% 273705
#% 273901
#% 333947
#% 411554
#% 465018
#% 479806
#% 480488
#! The extensible mark-up language (XML) is gaining widespread use as a format for data exchange and storage on the World Wide Web. Queries over XML data require accurate selectivity estimation of path expressions to optimize query execution plans. Selectivity estimation of XML path expression is usually done based on summary statistics about the structure of the underlying XML repository. All previous methods require an off-line scan of the XML repository to collect the statistics. In this paper, we propose XPathLearner, a method for estimating selectivity of the most commonly used types of path expressions without looking at the XML data. XPathLearner gathers and refines the statistics using query feedback in an on-line manner and is especially suited to queries in Internet scale applications since the underlying XML repository is either inaccessible or too large to be scanned in its entirety. Besides the on-line property, our method also has two other novel features: (a) XPathLearner is workload-aware in collecting the statistics and thus can be more accurate than the more costly off-line method under tight memory constraints, and (b) XPathLearner automatically adjusts the statistics using query feedback when the underlying XML data change. We show empirically the estimation accuracy of our method using several real data sets.

#index 993969
#* How to summarize the universe: dynamic maintenance of quantiles
#@ Anna C. Gilbert;Yannis Kotidis;S. Muthukrishnan;Martin J. Strauss
#t 2002
#c 4
#% 2152
#% 152934
#% 210160
#% 210190
#% 242366
#% 248820
#% 273907
#% 278835
#% 310585
#% 333931
#% 340670
#% 347226
#% 397369
#% 480465
#% 480628
#% 480805
#% 481775
#% 482104
#% 482123
#! Order statistics, i.e., quantiles, are frequently used in databases both at the database server as well as the application level. For example, they are useful in selectivity estimation during query optimization, in partitioning large relations, in estimating query result sizes when building user interfaces, and in characterizing the data distribution of evolving datasets in the process of data mining. We present a new algorithm for dynamically computing quantiles of a relation subject to insert as well as delete operations. The algorithm monitors the operations and maintains a simple, small-space representation (based on random subset sums or RSSs) of the underlying data distribution. Using these RSSs, we can quickly estimate, without having to access the data, all the quantiles, each guaranteed to be accurate to within user-specified precision. Previously-known one-pass quantile estimation algorithms that provide similar quality and performance guarantees can not handle deletions. Other algorithms that can handle delete operations cannot guarantee performance without rescanning the entire database. We present the algorithm, its theoretical performance analysis and extensive experimental results with synthetic and real datasets. Independent of the rates of insertions and deletions, our algorithm is remarkably precise at estimating quantiles in small space, as our experiments demonstrate.

#index 993970
#* Structure and value synopses for XML data graphs
#@ Neoklis Polyzotis;Minos Garofalakis
#t 2002
#c 4
#% 31484
#% 44876
#% 210214
#% 273902
#% 333946
#% 397364
#% 397379
#% 458836
#% 465018
#% 479465
#% 479806
#% 480488
#% 482092
#% 660000
#! All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows (1) path navigation and branching through the label structure of the XML data graph, and (2) predicates on the values of specific path/branch nodes, in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, we extent our earlier work on structural XSKETCH synopses and we propose an (augmented) XSKETCH synopsis model that exploits localized stability and value-distribution summaries (e.g., histograms) to accurately capture the complex correlation patterns that can exist between and across path structure and element values in the data graph. We develop a systematic XSKETCH estimation framework for complex path expressions with value predicates and we propose an efficient heuristic algorithm based on greedy forward selection for building an effective XSKETCH for a given amount of space (which is, in general, an NP-hard optimization problem). Implementation results with both synthetic and real-life data sets verify the effectiveness of our approach.

#index 993971
#* Compressed accessibility map: efficient access control for XML
#@ Ting Yu;Divesh Srivastava;Laks V. S. Lakshmanan;H. V. Jagadish
#t 2002
#c 4
#% 91075
#% 176496
#% 227956
#% 271801
#% 273692
#% 287026
#% 287493
#% 287670
#% 309716
#% 443103
#% 461901
#% 481124
#! XML is widely regarded as a promising means for data representation integration, and exchange. As companies transact business over the Internet, the sensitive nature of the information mandates that access must be provided selectively, using sophisticated access control specifications. Using the specification directly to determine if a user has access to a specific XML data item can hence be extremely inefficient. The alternative of fully materializing, for each data item, the users authorized to access it can be space-inefficient. In this paper, we propose a space- and time-efficient solution to the access control problem for XML data. Our solution is based on a novel notion of a compressed accessibility map (CAM), which compactly identifies the XML data items to which a user has access, by exploiting structural locality of accessibility in tree-structured data. We present a CAM lookup algorithm for determining if a user has access to a data item; it takes time proportional to the product of the depth of the item in the XML data and logarithm of the CAM size.

#index 993972
#* Optimizing the secure evaluation of twig queries
#@ SungRan Cho;Sihem Amer-Yahia;Laks V. S. Lakshmanan;Divesh Srivastava
#t 2002
#c 4
#% 273924
#% 287026
#% 287029
#% 309716
#% 314755
#% 333981
#% 424307
#% 433922
#% 479806
#% 480656
#% 504578
#% 566390
#! The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. In this context, query evaluation engines need to ensure that user queries only use and return XML data the user is allowed to access. These added access control checks can considerably increase query evaluation time. In this paper, we consider the problem of optimizing the secure evaluation of XML twig queries. We focus on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. For this model, secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. Based on security information in the DTD, we devise efficient algorithms that optimally determine when the recursive check can be eliminated, and when it can be simplified to just a local check on the element's attributes, without violating the access control policy. Finally, we experimentally evaluate the performance benefits of our techniques using a variety of XML data and queries.

#index 993973
#* Provisions and obligations in policy management and security applications
#@ Claudio Bettini;Sushil Jajodia;X. Sean Wang;Duminda Wijesekera
#t 2002
#c 4
#% 53388
#% 178047
#% 263982
#% 283126
#% 314755
#% 315363
#% 340827
#% 370758
#% 428360
#% 438511
#% 455634
#% 480620
#% 536330
#% 663868
#% 664539
#% 665419
#! Policies are widely used in many systems and applications. Recently, it has been recognized that a "yes/no" response to every scenario is just not enough for many modern systems and applications. Many policies require certain conditions to be satisfied and actions to be performed before or after a decision is made. To address this need, this paper introduces the notions of provisions and obligations. Provisions are those conditions that need to be satisfied or actions that must be performed before a decision is rendered, while obligations are those conditions or actions that must be fulfilled by either the users or the system after the decision. This paper formalizes a rule-based policy framework that includes provisions and obligations, and investigates a reasoning mechanism within this framework. A policy decision may be supported by more than one derivation, each associated with a potentially different set of provisions and obligations (called a global PO set). The reasoning mechanism can derive all the global PO sets for each specific policy decision, and facilitates the selection of the best one based on numerical weights assigned to provisions and obligations as well as on semantic relationships among them. The paper also shows the use of the proposed policy framework in a security application.

#index 993974
#* Effective change detection using sampling
#@ Junghoo Cho;Alexandros Ntoulas
#t 2002
#c 4
#% 870
#% 77005
#% 124019
#% 176491
#% 227945
#% 268079
#% 273908
#% 300139
#% 330604
#% 333983
#% 397355
#% 424292
#% 463736
#% 480136
#% 481749
#% 593734
#! For a large-scale data-intensive environment, such as the World-Wide Web or data warehousing, we often make local copies of remote data sources. Due to limited network and computational resources, however, it is often difficult to monitor the sources constantly to check for changes and to download changed data items to the copies. In this scenario, our goal is to detect as many changes as we can using the fixed download resources that we have. In this paper we propose three sampling-based download policies that can identify more changed data items effectively. In our sampling-based approach, we first sample a small number of data items from each data source and download more data items from the sources with more changed samples. We analyze the effectiveness of the sampling-based policies and compare our proposed policies to existing ones, including the state-of-the-art frequency-based policy in [8, 11]. Our experiments on synthetic and real-world data will show the relative merits of various policies and the great potential of our sampling-based policy. In certain cases, our sampling-based policy could download twice as many changed items as the best existing policy.

#index 993975
#* Maintaining coherency of dynamic data in cooperating repositories
#@ Shetal Shah;Krithi Ramamritham;Prashant Shenoy
#t 2002
#c 4
#% 70370
#% 77005
#% 227885
#% 271622
#% 300179
#% 303712
#% 330581
#% 348124
#% 397355
#% 399551
#% 443298
#% 464228
#% 615595
#% 635804
#% 661477
#% 963655
#% 963887
#% 978365
#% 978376
#% 979357
#% 1849759
#! In this paper, we consider techniques for disseminating dynamic data--such as stock prices and real-time weather information--from sources to a set of repositories. We focus on the problem of maintaining coherency of dynamic data items in a network of cooperating repositories. We show that cooperation among repositories-- where each repository pushes updates of data items to other repositories--helps reduce system-wide communication and computation overheads for coherency maintenance. However, contrary to intuition, we also show that increasing the degree of cooperation beyond a certain point can, in fact, be detrimental to the goal of maintaining coherency at low communication and computational overheads. We present techniques (i) to derive the "optimal" degree of cooperation among repositories, (ii) to construct an efficient dissemination tree for propagating changes from sources to cooperating repositories, and (iii) to determine when to push an update from one repository to another for coherency maintenance. We evaluate the efficacy of our techniques using real-world traces of dynamically changing data items (specifically, stock prices) and show that careful dissemination of updates through a network of cooperating repositories can substantially lower the cost of coherency maintenance.

#index 993976
#* A bandwidth model for internet search
#@ Axel Uhl
#t 2002
#c 4
#% 173905
#% 192155
#% 674611
#! In this paper a formal model for the domain of Internet search is presented that makes it possible to quantify the relations between important parameters of a distributed search architecture. Among these are physical network parameters, query frequency, required currency of search results, change rate of the data to be searched, logical network topology, and total bandwidth consumption for answering one query. The model is then used to compute many important relations between the various parameters. The results can be used to quantitatively assess, streamline, and optimize distributed Internet search architectures. The results back the general perception that a centralized approach to Internet-scale search will no longer be able to provide the desired coverage and currency, especially given that the Internet's content keeps growing much faster than the bandwidth available to index it. Using a hierarchical distribution approach and using change-based update notications instead of polling for changes allows to address sets of objects that are several orders of magnitude larger than what is possible with a centralized approach. Yet, using such an approach does not signicantly increase the total bandwidth required for a single query per object reached by the search.

#index 993977
#* Using latency-recency profiles for data delivery on the web
#@ Laura Bright;Louiqa Raschid
#t 2002
#c 4
#% 77005
#% 201928
#% 209653
#% 210176
#% 210182
#% 256882
#% 287260
#% 300139
#% 300177
#% 333969
#% 333995
#% 340607
#% 464706
#% 480495
#% 480816
#% 504569
#% 571037
#% 635804
#% 978362
#% 978378
#% 979356
#! An important challenge to web technologies such as proxy caching, web portals, and application servers is keeping cached data up-to-date. Clients may have different preferences for the latency and recency of their data. Some prefer the most recent data, others will accept stale cached data that can be delivered quickly. Existing approaches to maintaining cache consistency do not consider this diversity and may increase the latency of requests, consume excessive bandwidth, or both. Further, this overhead may be unnecessary in cases where clients will tolerate stale data that can be delivered quickly. This paper introduces latency-recency profiles, a set of parameters that allow clients to express preferences for their different applications. A cache or portal uses profiles to determine whether to deliver a cached object to the client or to download a fresh object from a remote server. We present an architecture for profiles that is both scalable and straightforward to implement at a cache. Experimental results using both synthetic and trace data show that profiles can reduce latency and bandwidth consumption compared to existing approaches, while still delivering fresh data in many cases. When there is insufficient bandwidth to answer all requests at once, profiles significantly reduce latencies for all clients.

#index 993978
#* View invalidation for dynamic content caching in multitiered architectures
#@ K. Selçuk Candan;Divyakant Agrawal;Wen-Syan Li;Oliver Po;Wang-Pin Hsiung
#t 2002
#c 4
#% 13015
#% 13016
#% 32914
#% 59350
#% 152928
#% 164364
#% 169844
#% 201928
#% 210176
#% 227947
#% 268764
#% 300141
#% 300177
#% 301084
#% 333995
#% 397357
#% 397402
#% 427218
#% 480495
#% 480814
#% 481128
#% 504584
#% 963904
#% 994021
#! In today's multitiered application architectures, clients do not access data stored in the databases directly. Instead, they use applications which in turn invoke the DBMS to generate the relevant content. Since executing application programs may require significant time and other resources, it is more advantageous to cache application results in a result cache. Various view materialization and update management techniques have been proposed to deal with updates to the underlying data. These techniques guarantee that the cached results are always consistent with the underlying data. Several applications, including e-commerce sites, on the other hand, do not require the caches be consistent all the time. Instead, they require that all outdated pages in the caches are invalidated in a timely fashion. In this paper, we show that invalidation is inherently different from view maintenance. We develop algorithms that benefit from this difference in reducing the cost of update management in certain applications and we present an invalidation framework that benefits from these algorithms.

#index 993979
#* Improving data access of J2EE applications by exploiting asynchronous messaging and caching services
#@ Samuel Kounev;Alejandro Buchmann
#t 2002
#c 4
#% 2544
#% 9241
#% 235084
#% 310239
#% 336201
#% 390132
#% 506004
#! The J2EE platform provides a variety of options for making business data persistent using DBMS technology. However, the integration with existing backend database systems has proven to be of crucial importance for the scalability and performance of J2EE applications, because modern e-business systems are extremely data-intensive. As a result, the data access layer, and the link between the application server and the database server in particular, are very susceptible to turning into a system bottleneck. In this paper we use the ECperf benchmark as an example of a realistic application in order to illustrate the problems mentioned above and discuss how they could be approached and eliminated. In particular, we show how asynchronous, message-based processing could be exploited to reduce the load on the DBMS and improve system performance, scalability and reliability. Furthermore, we discuss the major issues related to the correct use of entity beans (the components provided by J2EE for modelling persistent data) and present a number of methods to optimize their performance utilizing caching mechanisms. We have evaluated the proposed techniques through measurements and have documented the performance gains that they provide.

#index 993980
#* Eliminating fuzzy duplicates in data warehouses
#@ Rohit Ananthakrishna;Surajit Chaudhuri;Venkatesh Ganti
#t 2002
#c 4
#% 169370
#% 189872
#% 201889
#% 248801
#% 255137
#% 280419
#% 333943
#% 387427
#% 464837
#% 480496
#% 480499
#% 480645
#% 480654
#% 631985
#! The duplicate elimination problem of detecting multiple tuples, which describe the same real world entity, is an important data cleaning problem. Previous domain independent solutions to this problem relied on standard textual similarity functions (e.g., edit distance, cosine metric) between multi-attribute tuples. However, such approaches result in large numbers of false positives if we want to identify domain-specific abbreviations and conventions. In this paper, we develop an algorithm for eliminating duplicates in dimensional tables in a data warehouse, which are usually associated with hierarchies. We exploit hierarchies to develop a high quality, scalable duplicate elimination algorithm, and evaluate it on real datasets from an operational data warehouse.

#index 993981
#* Translating web data
#@ Lucian Popa;Yannis Velegrakis;Mauricio A. Hernández;Renée J. Miller;Ronald Fagin
#t 2002
#c 4
#% 6259
#% 198465
#% 281149
#% 285926
#% 287339
#% 333988
#% 378409
#% 462051
#% 479783
#% 480134
#% 480429
#% 572314
#% 993982
#! We present a novel framework for mapping between any combination of XML and relational schemas, in which a high-level, user-specified mapping is translated into semantically meaningful queries that transform source data into the target representation. Our approach works in two phases. In the first phase, the high-level mapping, expressed as a set of inter-schema correspondences, is converted into a set of mappings that capture the design choices made in the source and target schemas (including their hierarchical organization as well as their nested referential constraints). The second phase translates these mappings into queries over the source schemas that produce data satisfying the constraints and structure of the target schema, and preserving the semantic relationships of the source. Nonnull target values may need to be invented in this process. The mapping algorithm is complete in that it produces all mappings that are consistent with the schema constraints. We have implemented the translation algorithm in Clio, a schema mapping tool, and present our experience using Clio on several real schemas.

#index 993982
#* COMA: a system for flexible combination of schema matching approaches
#@ Hong-Hai Do;Erhard Rahm
#t 2002
#c 4
#% 158907
#% 263981
#% 307632
#% 317975
#% 328429
#% 331769
#% 332166
#% 333990
#% 348187
#% 479783
#% 480645
#% 572314
#% 637829
#% 660001
#! Schema matching is the task of finding semantic correspondences between elements of two schemas. It is needed in many database applications, such as integration of web data sources, data warehouse loading and XML message mapping. To reduce the amount of user effort as much as possible, automatic approaches combining several match techniques are required. While such match approaches have found considerable interest recently, the problem of how to best combine different match algorithms still requires further work. We have thus developed the COMA schema matching system as a platform to combine multiple matchers in a flexible way. We provide a large spectrum of individual matchers, in particular a novel approach aiming at reusing results from previous match operations, and several mechanisms to combine the results of matcher executions. We use COMA as a framework to comprehensively evaluate the effectiveness of different matchers and their combinations for real-world schemas. The results obtained so far show the superiority of combined match approaches and indicate the high value of reuse-oriented strategies.

#index 993983
#* Efficient exploration of large scientific databases
#@ Etzard Stolte;Gustavo Alonso
#t 2002
#c 4
#% 198465
#% 227883
#% 227927
#% 248038
#% 248812
#% 252360
#% 259995
#% 273696
#% 273902
#% 280448
#% 300185
#% 434617
#% 436116
#% 458837
#% 464056
#% 471029
#% 479648
#% 479984
#% 480306
#% 480465
#% 481266
#% 570889
#! One of the challenging aspects of scientific data repositories is how to efficiently explore the catalogues that describe the data. We have encountered such a problem while developing HEDC, HESSI Experimental data center, a multi-terabyte repository built for the recently launched HESSI satellite. In HEDC, scientific users will soon be confronted with a catalogue of many million tuples. In this paper we present a novel technique that allows users to efficiently explore such a large data space in an interactive manner. Our approach is to store a copy of relevant fields in segmented and wavelet encoded views that are streamed to specialized clients. These clients use approximated data and adaptive decoding techniques to allow users to quickly visualize the search space. In the paper we describe how this approach reduces from hours to seconds the time needed to generate meaningful visualizations of millions of tuples.

#index 993984
#* Searching on the secondary structure of protein sequences
#@ Laurie Hammel;Jignesh M. Patel
#t 2002
#c 4
#% 43163
#% 172939
#% 201921
#% 213981
#% 287647
#% 333854
#% 359443
#% 411554
#% 443469
#% 479958
#% 480482
#% 480484
#% 480819
#% 481266
#% 586837
#! In spite of the many decades of progress in database research, surprisingly scientists in the life sciences community still struggle with inefficient and awkward tools for querying biological data sets. This work highlights a specific problem involving searching large volumes of protein data sets based on their secondary structure. In this paper we define an intuitive query language that can be used to express queries on secondary structure and develop several algorithms for evaluating these queries. We implement these algorithms both in Periscope, a native system that we have built, and in a commercial ORDBMS. We show that the choice of algorithms can have a significant impact on query performance. As part of the Periscope implementation we have also developed a framework for optimizing these queries and for accurately estimating the costs of the various query evaluation plans. Our performance studies show that the proposed techniques are very efficient in the Periscope system and can provide scientists with interactive secondary structure querying options even on large protein data sets.

#index 993985
#* ProTDB: probabilistic data in XML
#@ Andrew Nierman;H. V. Jagadish
#t 2002
#c 4
#% 209725
#% 215225
#% 228817
#% 232662
#% 235023
#% 271717
#% 340914
#% 345737
#% 348163
#% 442830
#% 442863
#% 458829
#% 480102
#% 562456
#! Where as traditional databases manage only deterministic information, many applications that use databases involve uncertain data. This paper presents a Probabilistic Tree Data Base (ProTDB) to manage probabilistic data, represented in XML. Our approach differs from previous efforts to develop probabilistic relational systems in that we build a probabilistic XML database. This design is driven by application needs that involve data not readily amenable to a relational representation. XML data poses several modeling challenges: due to its structure, due to the possibility of uncertainty association at multiple granularities, and due to the possibility of missing and repeated sub-elements. We present a probabilistic XML model that addresses all of these challenges. We devise an implementation of XML query operations using our probability model, and demonstrate the efficiency of our implementation experimentally. We have used ProTDB to manage data from two application areas: protein chemistry data from the bioinformatics domain, and information extraction data obtained from the web using a natural language analysis system. We present a brief case study of the latter to demonstrate the value of probabilistic XML data management.

#index 993986
#* Fast and accurate text classification via multiple linear discriminant projections
#@ Soumen Chakrabarti;Shourya Roy;Mahesh V. Soundalgekar
#t 2002
#c 4
#% 41374
#% 194283
#% 219052
#% 232764
#% 260001
#% 269217
#% 290482
#% 342598
#% 344595
#% 430887
#% 458379
#% 481945
#! Support vector machines (SVMs) have shown superb performance for text classification tasks. They are accurate, robust, and quick to apply to test instances. Their only potential drawback is their training time and memory requirement. For n training instances held in memory, the best-known SVM implementations take time proportional to na, where a is typically between 1.8 and 2.1. SVMs have been trained on data sets with several thousand instances, but Web directories today contain millions of instances which are valuable for mapping billions of Web pages into Yahoo!-like directories. We present SIMPL, a nearly linear-time classification algorithm which mimics the strengths of SVMs while avoiding the training bottleneck. It uses Fisher's linear discriminant, a classical tool from statistical pattern recognition, to project training instances to a carefully selected low-dimensional subspace before inducing a decision tree on the projected instances. SIMPL uses efficient sequential scans and sorts, and is comparable in speed and memory scalability to widely-used naive Bayes (NB) classifiers, but it beats NB accuracy decisively. It not only approaches and sometimes exceeds SVM accuracy, but also beats SVM running time by orders of magnitude. While developing SIMPL, we also make a detailed experimental analysis of the cache performance of SVMs.

#index 993987
#* Discover: keyword search in relational databases
#@ Vagelis Hristidis;Yannis Papakonstantinou
#t 2002
#c 4
#% 36117
#% 67565
#% 268079
#% 289383
#% 300166
#% 300240
#% 384978
#% 479803
#% 495133
#% 660011
#! DISCOVER operates on relational databases and facilitates information discovery on them by allowing its user to issue keyword queries without any knowledge of the database schema or of SQL. DISCOVER returns qualified joining networks of tuples, that is, sets of tuples that are associated because they join on their primary and foreign keys and collectively contain all the keywords of the query. DISCOVER proceeds in two steps. First the Candidate Network Generator generates all candidate networks of relations, that is, join expressions that generate the joining networks of tuples. Then the Plan Generator builds plans for the efficient evaluation of the set of candidate networks, exploiting the opportunities to reuse common subexpressions of the candidate networks. We prove that DISCOVER finds without redundancy all relevant candidate networks, whose size can be data bound, by exploiting the structure of the schema. We prove that the selection of the optimal execution plan (way to reuse common subexpressions) is NP-complete. We provide a greedy algorithm and we show that it provides near-optimal plan execution time cost. Our experimentation also provides hints on tuning the greedy algorithm.

#index 993988
#* Maintaining data privacy in association rule mining
#@ Shariq J. Rizvi;Jayant R. Haritsa
#t 2002
#c 4
#% 152934
#% 210160
#% 273898
#% 300124
#% 300184
#% 333876
#% 342643
#% 374401
#% 428404
#% 481290
#% 481754
#% 481758
#% 539744
#% 577233
#% 577289
#% 586838
#% 664070
#! Data mining services require accurate input data for their results to be meaningful, but privacy concerns may influence users to provide spurious information. We investigate here, with respect to mining association rules, whether users can be encouraged to provide correct information by ensuring that the mining process cannot, with any reasonable degree of certainty, violate their privacy. We present a scheme, based on probabilistic distortion of user data, that can simultaneously provide a high degree of privacy to the user and retain a high level of accuracy in the mining results. The performance of the scheme is validated against representative real and synthetic datasets.

#index 993989
#* A logical framework for scheduling workflows under resource allocation constraints
#@ Pinar Senkul;Michael Kifer;Ismail H. Toroslu
#t 2002
#c 4
#% 35562
#% 171056
#% 185412
#% 224319
#% 248013
#% 261269
#% 273709
#% 275349
#% 283220
#% 361248
#% 461899
#% 464235
#% 481261
#% 509521
#% 562155
#% 614640
#% 631979
#% 637510
#! A workflow consists of a collection of coordinated tasks designed to carry out a well-defined complex process, such as catalog ordering, trip planning, or a business process in an enterprise. Scheduling of workflows is a problem of finding a correct execution sequence for the workflow tasks, i.e., execution that obeys the constraints that embody the business logic of the workflow. Research on workflow scheduling has largely concentrated on temporal constraints, which specify correct ordering of tasks. Another important class of constraints -- those that arise from resource allocation -- has received relatively little attention in workflow modeling. Since typically resources are not limitless and cannot be shared, scheduling of a workflow execution involves decisions as to which resources to use and when. In this work, we present a framework for workflows whose correctness is given by a set of resource allocation constraints and develop techniques for scheduling such systems. Our framework integrates Concurrent Transaction Logic (CTR) with constraint logic programming (CLP), yielding a new logical formalism, which we call Concurrent Constraint Transaction Logic, or CCTR.

#index 993990
#* An almost-serial protocol for transaction execution in main-memory database systems
#@ Stephen Blott;Henry F. Korth
#t 2002
#c 4
#% 9241
#% 32885
#% 256718
#% 273940
#% 287352
#% 403195
#% 427195
#% 442706
#% 442707
#% 442832
#% 442836
#% 479624
#% 480959
#% 481454
#% 531907
#% 617063
#! Disk-based database systems benefit from concurrency among transactions - usually with marginal overhead. For main-memory database systems, however, locking overhead can have a serious impact on performance. This paper proposes SP, a serial protocol for the execution of transactions in main-memory systems, and evaluates its performance against that of strict two-phase locking. The novelty of SP lies in the use of timestamps and mutexes to allow one transaction to begin before its predecessors' commit records have been written to disk, while also ensuring that no committed transactions read uncommitted data. We demonstrate seven-fold and two-fold increases in maximum throughput for read-and update-intensive workloads, respectively. At fixed loads, we demonstrate ten-fold and two-fold improvements in response time for the same transaction mixes. We show that for a wide range of practical workloads, SP on a single processor outperforms locking on a multiprocessor, and then present a modified SP, that exploits multiprocessor systems.

#index 993991
#* Lightweight flexible isolation for language-based extensible systems
#@ Laurent Daynès;Grzegorz Czajkowski
#t 2002
#c 4
#% 31789
#% 86938
#% 107722
#% 126824
#% 172878
#% 214984
#% 279144
#% 315205
#% 343809
#% 387159
#% 393784
#% 403195
#% 435108
#% 465157
#% 480959
#% 481091
#% 481752
#% 602708
#% 609895
#% 963657
#% 979323
#! Safe programming languages encourage the development of dynamically extensible systems, such as extensible Web servers and mobile agent platforms. Although protection is of utmost importance in these settings, current solutions do not adequately address fault containment. This paper advocates an approach to protection where transactions act as protection domains. This enables direct sharing of objects while protecting against unauthorized accesses and failures of authorized components. The main questions about this approach are what transaction models translate best into protection mechanisms suited for extensible language-based systems and what is the impact of transaction-based protection on performance. A programmable isolation engine has been integrated with the runtime of a safe programming language in order to allow quick experimentation with a variety of isolation models and to answer both questions. This paper reports on the techniques for flexible fine-grained locking and undo devised to meet the functional and performance requirements of transaction-based protection. Performance analysis of a prototype implementation shows that (i) sophisticated concurrency controls do not translate into higher overheads, and (ii) the ability to memoize locking operations is crucial to performance.

#index 993992
#* Processing star queries on hierarchically-clustered fact tables
#@ Nikos Karayannidis;Aris Tsois;Timos Sellis;Roland Pieringer;Volker Markl;Frank Ramsak;Robert Fenk;Klaus Elhardt;Rudolf Bayer
#t 2002
#c 4
#% 68091
#% 191154
#% 223781
#% 227861
#% 227965
#% 245998
#% 248805
#% 248806
#% 248814
#% 252304
#% 273905
#% 285932
#% 334006
#% 342735
#% 462217
#% 463735
#% 464215
#% 464826
#% 481288
#% 481293
#% 481604
#% 481608
#% 482081
#% 482831
#% 637792
#! Star queries are the most prevalent kind of queries in data warehousing, OLAP and business intelligence applications. Thus, there is an imperative need for efficiently processing star queries. To this end, a new class of fact table organizations has emerged that exploits path-based surrogate keys in order to hierarchically cluster the fact table data of a star schema [DRSN98, MRB99, KS01]. In the context of these new organizations, star query processing changes radically. In this paper, we present a complete abstract processing plan that captures all the necessary steps in evaluating such queries over hierarchically clustered fact tables. Furthermore, we present optimizations for surrogate key processing and a novel early grouping transformation for grouping on the dimension hierarchies. Our algorithms have been already implemented in a commercial relational database management system (RDBMS) and the experimental evaluation, as well as customer feedback, indicates speedups of orders of magnitude for typical star queries in real world applications.

#index 993993
#* Exploiting versions for on-line data warehouse maintenance in MOLAP servers
#@ Heum-Geun Kang;Chin-Wan Chung
#t 2002
#c 4
#% 9241
#% 86950
#% 172939
#% 223781
#% 227866
#% 227944
#% 250025
#% 296090
#% 462208
#% 463760
#% 480093
#% 481956
#% 562673
#% 632047
#% 637804
#! A data warehouse is an integrated database whose data is collected from several data sources, and supports on-line analytical processing (OLAP). Typically, a query to the data warehouse tends to be complex and involves a large volume of data. To keep the data at the warehouse consistent with the source data, changes to the data sources should be propagated to the data warehouse periodically. Because the propagation of the changes (maintenance) is batch processing, it takes long time. Since both query transactions and maintenance transactions are long and involve large volumes of data, traditional concurrency control mechanisms such as two-phase locking are not adequate for a data warehouse environment. We propose a multi-version concurrency control mechanism suited for data warehouses which use multi-dimensional OLAP (MOLAP) servers. We call the mechanism multiversion concurrency control for data warehouses (MVCCDW). To our knowledge, our work is the first attempt to exploit versions for online data warehouse maintenance in a MOLAP environment. MVCC-DW guarantees the serializability of concurrent transactions. Transactions running under the mechanism do not block each other and do not need to place locks.

#index 993995
#* The generalized MDL approach for summarization
#@ Laks V. S. Lakshmanan;Raymond T. Ng;Christine Xing Wang;Xiaodong Zhou;Theodore J. Johnson
#t 2002
#c 4
#% 35909
#% 61792
#% 196770
#% 248792
#% 273916
#% 333927
#% 427199
#% 479645
#% 479795
#% 481956
#% 520221
#% 656071
#! There are many applications in OLAP and data analysis where we identify regions of interest. For example, in OLAP, an analysis query involving aggregate sales performance of various products in different locations and seasons could help identify interesting cells, such as cells of a data cube having an aggregate sales higher than a threshold. While a normal answer to such a quiry merely returns all interesting cells, it may be far more informative to the user if the system return summaries or descriptions of regions formed from the identified cells. The minimum Description Length (MDL) principle is a well-known strategy for finding such region descriptions. In this paper, we propose a generalization of the MDL principle, called GMDL, and show that GMDL leads to fewer regions than MDL, and hence more concise "answers" returned to the user. The key idea is that a region may contain "don't care" cells (up to a global maximum), if these "don't care" cells help to form bigger summary regions, leading to a more concise overall summary. We study the problem of generating minimal region descriptions under the GMDL principle for two different scenarios. In the first, all dimensions of the data space are spatial. In the second scenario, all dimentions are categorial and organized in hierarchies. We propose region finding algorithms for both scenarios and evaluate their run time and compression performance using detailed experimentation. Our results show the effectiveness of the GMDL principle and the proposed algorithms.

#index 993996
#* Quotient cube: how to summarize the semantics of a data cube
#@ Laks V. S. Lakshmanan;Jian Pei;Jiawei Han
#t 2002
#c 4
#% 210182
#% 227880
#% 236410
#% 237202
#% 259995
#% 273916
#% 280448
#% 397388
#% 464215
#% 479450
#% 480820
#% 481290
#% 481951
#! Partitioning a data cube into sets of cells with "similar behavior" often better exposes the semantics in the cube. E.g., if we find that average boots sales in the West 10th store of Walmart was the same for winter as for the whole year, it signifies something interesting about the trend of boots sales in that location in that year. In this paper, we are interested in finding succinct summaries of the data cube, exploiting regularities present in the cube, with a clear basis. We would like the summary: (i) to be as concise as possible, (ii) to itself form a lattice preserving the rollup/drilldown semantics of the cube, and (iii) to allow the original cube to be fully recovered. We illustrate the utility of solving this problem and discuss the inherent challenges. We develop techniques for partitioning cube cells for obtaining succinct summaries, and introduce the quotient cube. We give efficient algorithms for computing it from a base table. For monotone aggregate functions (e.g., COUNT, MIN, MAX, SUM on non-negative measures, etc.), our solution is optimal (i.e., quotient cube of the least size). For nonmonotone functions (e.g., AVG), we obtain a locally optimal solution. We experimentally demonstrate the efficacy of our ideas and techniques and the scalability of our algorithms.

#index 993997
#* A one-pass aggregation algorithm with the optimal buffer size in multidimensional OLAP
#@ Young-Koo Lee;Kyu-Young Whang;Yang-Sae Moon;Il-Yeol Song
#t 2002
#c 4
#% 735
#% 43163
#% 69273
#% 86950
#% 136740
#% 152943
#% 223781
#% 227880
#% 248805
#% 252304
#% 285932
#% 308509
#% 318051
#% 404765
#% 404922
#% 435124
#% 443326
#% 479472
#% 479812
#% 480587
#% 481951
#% 581212
#! Aggregation is an operation that plays a key role inmultidimensional OLAP (MOLAP). Existing aggregation methods inMOLAP have been proposed for file structures such asmultidimensional arrays. These file structures are suitable fordata with uniform distributions, but do not work well with skeweddistributions. In this paper, we consider an aggregation methodthat uses dynamic multidimensional files adapting to skeweddistributions. In these multidimensional files, the sizes of pageregions vary according to the data density in these regions, andthe pages that belong to a larger region are accessed multipletimes while computing aggregations. To solve this problem, we firstpresent an aggregation computation model, called theDisjoint-Inclusive Partition (DIP) computation model, that is theformal basis of our approach. Based on this model, we then presentthe one-pass aggregation algorithm. This algorithm computesaggregations using the one-pass buffer size, which is the minimumbuffer size required for guaranteeing one disk access per page. Weprove that our aggregation algorithm is optimal with respect to theone-pass buffer size under our aggregation computation model. Usingthe DIP computation model allows us to correctly predict the orderof accessing data pages in advance. Thus, our algorithm achievesthe optimal one-pass buffer size by using a buffer replacementpolicy, such as Belady's B0 or Toss-Immediate policies,that exploits the page access order computed in advance. Since thepage access order is not known a priori in general, these policieshave been known to lack practicality despite its theoreticsignificance. Nevertheless, in this paper, we show that thesepolicies can be effectively used for aggregation computation.We have conducted extensive experiments. We first demonstratethat the one-pass buffer size theoretically derived is indeedcorrect in real environments. We then compare the performance ofthe one-pass algorithm with those of other ones. Experimentalresults for a real data set show that the one-pass algorithmreduces the number of disk accesses by up to 7.31 times comparedwith a naive algorithm. We also show that the memory requirement ofour algorithm for processing the aggregation in one-pass is verysmall being 0.05%|0.6% of the size of the database. These resultsindicate that our algorithm is practically usable even for a fairlylarge database. We believe our work provides an excellent formalbasis for investigating further issues in computing aggregations inMOLAP.

#index 993998
#* Incremental maintenance for non-distributive aggregate functions
#@ Themistoklis Palpanas;Richard Sidle;Roberta Cochrane;Hamid Pirahesh
#t 2002
#c 4
#% 13015
#% 13016
#% 58377
#% 152928
#% 201883
#% 201928
#% 201929
#% 210208
#% 210210
#% 227869
#% 287249
#% 300141
#% 300199
#% 305947
#% 333962
#% 340301
#% 378062
#% 442767
#% 459016
#% 464215
#% 479770
#% 480623
#! Incremental view maintenance is a well-known topic that has been addressed in the literature as well as implemented in database products. Yet, incremental refresh has been studied in depth only for a subset of the aggregate functions. In this paper we propose a general incremental maintenance mechanism that applies to all aggregate functions, including those that are not distributive over all operations. This class of functions is of great interest, and includes MIN/MAX, STDDEV, correlation, regression, XML constructor, and user defined functions. We optimize the maintenance of such views in two ways. First, by only recomputing the set of affected groups. Second, we extend the incremental infrastructure with work areas to support the maintenance of functions that are algebraic. We further optimize computation when multiple dissimilar aggregate functions are computed in the same view, and for special cases such as the maintenance of MIN/MAX, which are incrementally maintainable over insertions. We also address the important problem of incremental maintenance of views containing super-aggregates, including materialized OLAP cubes. We have implemented our algorithm on a prototype version of IBM DB2 UDB, and an experimental evaluation proves the validity of our approach.

#index 993999
#* Reverse nearest neighbor aggregates over data streams
#@ Flip Korn;S. Muthukrishnan;Divesh Srivastava
#t 2002
#c 4
#% 143040
#% 228299
#% 232768
#% 248820
#% 273898
#% 273900
#% 273907
#% 281557
#% 297915
#% 300163
#% 310500
#% 333926
#% 333931
#% 347226
#% 379445
#% 465009
#% 480628
#% 480661
#% 482104
#% 594012
#% 594029
#% 660004
#% 993959
#% 993969
#! Reverse Nearest Neighbor (RNN) queries have been studied for finite, stored data sets and are of interest for decision support. However, in many applications such as fixed wireless telephony access and sensor-based highway traffic monitoring, the data arrives in a stream and cannot be stored. Exploratory analysis on this data stream can be formalized naturally using the notion of RNN aggregates (RNNAs), which involve the computation of some aggregate (such as C0UNT or MAX DISTANCE) over the set of reverse nearest neighbor "clients" associated with each "server". In this paper, we introduce and investigate the problem of computing three types of RNNA queries over data streams of "client" locations: (i) Max-RNNA: given K servers, return the maximum RNNA over all clients to their closest servers; (ii) List-RNNA: given K servers, return a list of RNNAs over all clients to each of the K servers; and (iii) Opt-RNNA: find a subset of at most K servers for which their RNNAs are below a given threshold. While exact computation of these queries is not possible in the data stream model, we present efficient algorithms to approximately answer these RNNA queries over data streams with error guarantees. We provide analytical proofs of constant factor approximations for many RNNA queries, and complement our analyses with experimental evidence of the accuracy of our techniques.

#index 994000
#* Tree pattern aggregation for scalable XML data dissemination
#@ Chee-Yong Chan;Wenfei Fan;Pascal Felber;Minos Garofalakis;Rajeev Rastogi
#t 2002
#c 4
#% 213981
#% 302816
#% 333989
#% 452841
#% 480296
#% 480488
#% 556654
#% 659995
#! With the rapid growth of XML-document traffic on the Internet, scalable content-based dissemination of XML documents to a large, dynamic group of consumers has become an important research challenge. To indicate the type of content that they are interested in, data consumers typically specify their subscriptions using some XML pattern specification language (e.g., XPath). Given the large volume of subscribers, system scalability and efficiency mandate the ability to aggregate the set of consumer subscriptions to a smaller set of content specifications, so as to both reduce their storage-space requirements as well as speed up the document-subscription matching process. In this paper, we provide the first systematic study of subscription aggregation where subscriptions are specified with tree patterns (an important subclass of XPath expressions). The main challenge is to aggregate an input set of tree patterns into a smaller set of generalized tree patterns such that: (1) a given space constraint on the total size of the subscriptions is met, and (2) the loss in precision (due to aggregation) during document filtering is minimized. We propose an efficient tree-pattern aggregation algorithm that makes effective use of document-distribution statistics in order to compute a precise set of aggregate tree patterns within the allotted space budget. As part of our solution, we also develop several novel algorithms for tree-pattern containment and minimization, as well as "least-upper-bound" computation for a set of tree patterns. These results are of interest in their own right, and can prove useful in other domains, such as XML query optimization. Extensive results from a prototype implementation validate our approach.

#index 994001
#* DTD-directed publishing with attribute translation grammars
#@ Michael Benedikt;Chee Yong Chan;Wenfei Fan;Rajeev Rastogi;Shihui Zheng;Aoying Zhou
#t 2002
#c 4
#% 309851
#% 332166
#% 333855
#% 333935
#% 344425
#% 454629
#% 464716
#% 464988
#% 478931
#% 481125
#% 572305
#% 653704
#! We present a framework for publishing relational data in XML with respect to a fixed DTD. In data exchange on the Web, XML views of relational data are typically required to conform to a predefined DTD. The presence of recursion in a DTD as well as non-determinism makes it challenging to generate DTD-directed, efficient transformations. Our framework provides a language for defining views that are guaranteed to be DTD-conformant, as well as middleware for evaluating these views. It is based on a novel notion of attribute translation grammars (ATGs). An ATG extends a DTD by associating semantic rules via SQL queries. Directed by the DTD, it extracts data from a relational database, and constructs an XML document. We provide algorithms for efficiently evaluating ATGs, along with methods for statically analyzing them. This yields a systematic and effective approach to publishing data with respect to a predefined DTD.

#index 994002
#* A multi-version cache replacement and prefetching policy for hybrid data delivery environments
#@ André Seifert;Marc H. Scholl
#t 2002
#c 4
#% 735
#% 1822
#% 58374
#% 152943
#% 172874
#% 201696
#% 201897
#% 227885
#% 232791
#% 244119
#% 249324
#% 260004
#% 271353
#% 316491
#% 443127
#% 464065
#% 480597
#% 480780
#% 481450
#% 482107
#% 668666
#% 674402
#% 674411
#% 1848595
#! This paper introduces MICP, a novel multiversion integrated cache replacement and prefetching algorithm designed for efficient cache and transaction management in hybrid data delivery networks. MICP takes into account the dynamically and sporadically changing cost/benefit ratios of cached and/or disseminated object versions by making cache replacement and prefetching decisions sensitive to the objects' access probabilities, their position in the broadcast cycle, and their update frequency. Additionally, to eliminate the issue of a newly created or outdated, but re-cacheable, object version replacing a version that may not be reacquired from the server, MICP logically divides the client cache into two variable-sized partitions, namely the REC and the NON-REC partitions for maintaining re-cacheable and nonre-cacheable object versions, respectively. Besides judiciously selecting replacement victims, MICP selectively prefetches popular object versions from the broadcast channel in order to further improve transaction response time. A simulation study compares MICP with one offline and two online cache replacement and prefetching algorithms. Performance results for the workloads and system settings considered demonstrate that MICP improves transaction throughput rates by about 18.9% compared to the best performing online algorithm and it performs only 40.8% worse than an adapted version of the offline algorithm P.

#index 994003
#* Toward recovery-oriented computing
#@ Armando Fox
#t 2002
#c 4
#% 283807
#% 433941
#% 546061
#% 546077
#% 546398
#% 609940
#% 657584
#% 674157
#% 867413
#% 867449
#% 963654
#% 963655
#% 963910
#% 978826
#! Recovery Oriented Computing (ROC) is a joint research effort between Stanford University and the University of California, Berkeley. ROC takes the perspective that hardware faults, software bugs, and operator errors are facts to be coped with, not problems to be solved. This perspective is supported both by historical evidence and by recent studies on the main sources of outages in production systems. By concentrating on reducing Mean Time to Repair (MTTR) rather than increasing Mean Time to Failure (MTTF), ROC reduces recovery time and thus offers higher availability. We describe the principles and philosophy behind the joint Stanford/Berkeley ROC effort and outline some of its research areas and current projects.

#index 994004
#* SMART: making DB2 (more) autonomic
#@ Guy M. Lohman;Sam S. Lightstone
#t 2002
#c 4
#% 248815
#% 248855
#% 342729
#% 397397
#% 480158
#% 480803
#% 632100
#! IBM's SMART (Self-Managing And Resource Tuning) project aims to make DB2 self-managing, i.e. autonomic, to decrease the total cost of ownership and penetrate new markets. Over several releases, increasingly sophisticated SMART features will ease administrative tasks such as initial deployment, database design, system maintenance, problem determination, and ensuring system availability and recovery.

#index 994005
#* Business process cockpit
#@ Mehmet Sayal;Fabio Casati;Umeshwar Dayal;Ming-Chien Shan
#t 2002
#c 4
#% 480666

#index 994006
#* GnatDb: a small-footprint, secure database system
#@ Radek Vingralek
#t 2002
#c 4
#% 107692
#% 109556
#% 151495
#% 381870
#% 440254
#% 523745
#% 524029
#% 536323
#% 566138
#% 566391
#% 657582
#% 963644
#% 963647
#% 963714
#% 963752
#% 978634
#! This paper describes GnatDb, which is an embedded database system that provides protection against both accidental and malicious corruption of data. GnatDb is designed to run on a wide range of appliances, some of which have very limited resources. Therefore, its design is heavily driven by the need to reduce resource consumption. GnatDb employs atomic and durable updates to protect the data against accidental corruption. It prevents malicious corruption of the data using standard cryptographic techniques that leverage the underlying log-structured storage model. We show that the total memory consumption of GnatDb, which includes the code footprint, the stack and the heap, does not exceed 11 KB, while its performance on a typical appliance platform remains at an acceptable level.

#index 994007
#* Experience report: exploiting advanced database optimization features for Large-Scale SAP R/3 installations
#@ Bernhard Zeller;Alfons Kemper
#t 2002
#c 4
#% 43171
#% 43203
#% 83235
#% 102810
#% 227872
#% 248018
#% 317933
#% 334653
#% 355177
#% 381472
#% 411734
#% 442698
#% 442700
#% 452783
#% 465149
#% 479920
#% 480785
#% 481461
#% 571066
#% 571084
#! The database volumes of enterprise resource planning (ERP) systems like SAP R/3 are growing at a tremendous rate and some of them have already reached a size of several Terabytes. OLTP (Online Transaction Processing) databases of this size are hard to maintain and tend to perform poorly. Therefore most database vendors have implemented new features like horizontal partitioning to optimize such mission critical applications. Horizontal partitioning was already investigated in detail in the context of shared nothing distributed database systems but today's ERP systems mostly use a centralized database with a shared everything architecture. In this work, we therefore investigate how an SAP R/3 system performs when the data in the underlying database is partitioned horizontally. Our results show that especially joins, in parallel executed statements, and administrative tasks benefit greatly from horizontal partitioning while the resulting small increase in the execution times of insertions, deletions and updates is tolerable. These positive results have initiated the SAP cooperation partners to pursue a partitioned data layout in some of their largest installed productive systems.

#index 994008
#* Information integration and XML in IBM's DB2
#@ Pat Selinger
#t 2002
#c 4

#index 994009
#* A new passenger support system for public transport using mobile database access
#@ Koichi Goto;Yahiko Kambayashi
#t 2002
#c 4
#% 169835
#% 175253
#% 198038
#% 443127
#% 443263
#% 509886
#% 543364
#% 635895
#% 638404
#! We have been developing a mobile passenger support system for public transport. Passengers can make their travel plans and purchase necessary tickets by accessing databases via the system. After starting the travel, a mobile terminal checks the travel schedule of its user by accessing several databases and gathering various kinds of information. In this application field, many kinds of data must be handled. Examples of such data are route information, fare information, area map, station map, planned operation schedule, real-time operation schedule, vehicle facilities and so on. Depending on the user's situation, different information should be supplied and personalized. In this paper, we propose a new mechanism to support passengers using the multi-channel data communication environments. On the other hand, transport systerns can gather information about situations and demands of users and modify their services offered for the users. We also describe a prototype system developed for visually handicapped passengers and the results of tests in an actual railway station.

#index 994010
#* OBK: an online high energy physics' meta-data repository
#@ I. Alexandrov;A. Amorim;E. Badescu;M. Barczyk;D. Burckhart-Chromek;M. Caprini;M. Dobson;J. Flammer;R. Hart;R. Jones;A. Kazarov;S. Kolos;V. Kotov;D. Liko;L. Lucio
#t 2002
#c 4
#% 482096
#! ATLAS will be one of the four detectors for the LHC (Large Hadron Collider) particle accelerator currently being built at CERN, Geneva. The project is expected to start production in 2006 and during its lifetime (15-20 years) to generate roughly one petabyte per year of particle physics' data. This vast amount of information will require several meta-data repositories which will ease the manipulation and understanding of physics' data by the final users (physicists doing analysis). Metadata repositories and tools at ATLAS may address such problems as the logical organization of the physics data according to data taking sessions, errors and faults during data gathering, data quality or terciary storage meta-information. The OBK (Online Book-Keeper) is a component of ATLAS' Online Software - the system which provides configuration, control and monitoring services to the DAQ (Data AQquisition system). In this paper we will explain the role of the OBK as one of the main collectors and managers of meta-data produced online, how that data is stored and the interfaces that are provided to access it - merging the physics data with the collected metadata will play an essential role for future analysis and interpretion of the physics events observed at ATLAS. We also provide an historical background to the OBK by analysing the several prototypes implemented in the context of our software development process and the results and experience obtained with the various DBMS technologies used.

#index 994011
#* The gRNA: a highly programmable infrastructure for prototyping, developing and deploying genomics-centric applications
#@ Amey V. Laud;Sourav Bhowmick;Pedro Cruz;Dadabhai T. Singh;George Rajesh
#t 2002
#c 4
#% 333981
#% 479956
#% 480489
#% 481614
#% 589353
#! The evolving challenges in lifesciences research cannot be all addressed by off-the-shelf bioinformatics applications. Life scientists need to analyze their data using novel or context-sensitive approaches that might be published in recent journals and publications, or based on their own hypotheses and assumptions. The genomics Research Network Architecture (gRNA) is a highly programmable, modular environment specially designed to invigorate the development of genomics-centric tools for life sciences-research. The gRNA provides the development environment in which new applications can be quickly written, and the deployment environment in which they can systematically avail of computing resources and integrate information from distributed biological data sources.

#index 994012
#* An efficient method for performing record deletions and updates using index scans
#@ C. Mohan
#t 2002
#c 4
#% 77648
#% 83183
#% 83188
#% 114582
#% 116087
#% 194942
#% 320902
#% 328431
#% 336201
#% 462941
#% 463444
#% 464233
#% 466944
#% 479983
#% 481256
#! We present a method for efficiently performing deletions and updates of records when the records to be deleted or updated are chosen by a range scan on an index. The traditional method involves numerous unnecessary lock calls and traversals of the index from root to leaves, especially when the qualifying records' keys span more than one leaf page of the index. Customers have suffered performance losses from these inefficiencies and have complained about them. Our goal was to minimize the number of interactions with the lock manager, and the number of page fixes, comparison operations and, possibly, I/Os. Some of our improvements come from increased synergy between the query planning and data manager components of a DBMS. Our patented method has been implemented in DB2 V7 to address specific customer requirements. It has also been done to improve performance on the TPC-H benchmark.

#index 994013
#* Joining ranked inputs in practice
#@ Ihab F. Ilyas;Walid G. Aref;Ahmed K. Elmagarmid
#t 2002
#c 4
#% 67565
#% 210172
#% 227894
#% 227939
#% 228000
#% 248834
#% 252374
#% 278831
#% 333854
#% 437405
#% 479623
#% 480330
#% 480819
#% 481599
#% 631918
#% 631988
#% 659255
#! Joining ranked inputs is an essential requirement for many database applications, such as ranking search results from multiple search engines and answering multi-feature queries for multimedia retrieval systems. We introduce a new practical pipelined query operator, termed NRA-RJ, that produces a global rank from input ranked streams based on a score function. The output of NRA-RJ can serve as a valid input to other NRA-RJ operators in the query pipeline. Hence, the NRA-RJ operator can support a hierarchy of join operations and can be easily integrated in query processing engines of commercial database systems. The NRA-RJ operator bridges Fagin's optimal aggregation algorithm into a practical implementation and contains several optimizations that address performance issues. We compare the performance of NRA-RJ against recent rank join algorithms. Experimental results demonstrate the performance trade-offs among these algorithms. The experimental results are based on an empirical study applied to a medical video application on top of a prototype database system. The study reveals important design options and shows that the NRA-RJ operator outperforms other pipelined rank join operators when the join condition is an equi-join on key attributes.

#index 994014
#* SQL memory management in Oracle9i
#@ Benoît Dageville;Mohamed Zait
#t 2002
#c 4
#! Complex database queries require the use of memory-intensive operators like sort and hash-join. Those operators need memory, also referred to as SQL memory, to process their input data. For example, a sort operator uses a work area to perform the in-memory sort of a set of rows. The amount of memory allocated by these operators greatly affects their performance. However, there is only a finite amount of memory available in the system, shared by all concurrent operators. The challenge for database systems is to design a fair and efficient strategy to manage this memory. Commercial database systems rely on database administrators (DBA) to supply an optimal setting for configuration parameters that are internally used to decide how much memory to allocate to a given database operator. However, database systems continue to be deployed in new areas, e.g, e-commerce, and the database applications are increasingly complex, e.g, to provide more functionality, and support more users. One important consequence is that the application workload is very hard, if not impossible, to predict. So, expecting a DBA to find an optimal value for memory configuration parameters is not realistic. The values can only be optimal for a limited period of time while the workload is within the assumed range. Ideally, the optimal value should adapt in response to variations in the application workload. Several research projects addressed this problem in the past, but very few commercial systems proposed a comprehensive solution to managing memory used by SQL operators in a database application with a variable workload. This paper presents a new model used in Oracle9i to manage memory for database operators. This approach is automatic, adaptive and robust. We will present the architecture of the memory manager, the internal algorithms, and a performance study showing its superiority.

#index 994015
#* XMark: a benchmark for XML data management
#@ Albrecht Schmidt;Florian Waas;Martin Kersten;Michael J. Carey;Ioana Manolescu;Ralph Busse
#t 2002
#c 4
#% 77672
#% 114578
#% 152904
#% 227875
#% 308463
#% 333981
#% 342677
#% 428147
#% 479656
#% 479956
#% 480463
#% 504574
#% 504578
#% 541480
#% 567011
#% 650962
#! While standardization efforts for XML query languages have been progressing, researchers and users increasingly focus on the database technology that has to deliver on the new challenges that the abundance of XML documents poses to data management: validation, performance evaluation and optimization of XML query processors are the upcoming issues. Following a long tradition in database research, we provide a framework to assess the abilities of an XML database to cope with a broad range of different query types typically encountered in real-world scenarios. The benchmark can help both implementors and users to compare XML databases in a standardized application scenario. To this end, we offer a set of queries where each query is intended to challenge a particular aspect of the query processor. The overall workload we propose consists of a scalable document database and a concise, yet comprehensive set of queries which covers the major aspects of XML query processing ranging from textual features to data analysis queries and ad hoc queries. We complement our research with results we obtained from running the benchmark on several XML database platforms. These results are intended to give a first baseline and illustrate the state of the art.

#index 994016
#* The denodo data integration platform
#@ Alberto Pan;Juan Raposo;Manuel Álvarez;Paula Montoto;Vicente Orjales;Justo Hidalgo;Lucía Ardao;Anastasio Molano;Ángel Viña
#t 2002
#c 4
#% 116303
#% 210176
#% 248801
#% 261741
#% 273923
#% 328424
#% 334002
#% 340302
#% 525667
#% 632051
#! The world today is characterised by the proliferation of information sources available through media such as the WWW, databases, semi-structured files (e.g. XML documents), etc. Nevertheless, this information is usually scattered, heterogeneous and weakly structured, so it is difficult to process it automatically. DENODO Corporation has developed a mediator system for the construction of semi-structured and structured data integration applications. This system has already been used in the construction of several applications on the Internet and in corporate environments, which are currently deployed at several important Internet audience sites and large sized business corporations. In this extended abstract, we present an overview of the system and we put forward some conclusions arising from our experience in building real-world data integration applications, focusing in some challenges we believe require more attention from the research community.

#index 994017
#* Preference SQL: design, implementation, experiences
#@ Werner Kießling;Gerhard Köstler
#t 2002
#c 4
#% 41230
#% 183909
#% 213443
#% 300170
#% 333951
#% 334012
#% 379207
#% 458873
#% 461211
#% 465167
#% 471123
#% 480671
#% 665415
#% 993957
#! Current search engines can hardly cope adequately with fuzzy predicates defined by complex preferences. The biggest problem of search engines implemented with standard SQL is that SQL does not directly understand the notion of preferences. Preference SQL extends SQL by a preference model based on strict partial orders (presented in more detail in the companion paper [Kie02]), where preference queries behave like soft selection constraints. Several built-in base preference types and the powerful Pareto operator, combined with the adherence to declarative SQL programming style, guarantees great programming productivity. The Preference SQL optimizer does an efficient re-writing into standard SQL, including a high-level implementation of the skyline perator for Pareto-optimal sets. This pre-processor approach enables a seamless application integration, making Preference SQL available on all major SQL platforms. Several commercial B2C portals are powered by Preference SQL. Its benefits comprise cooperative query answering and smart customer advice, leading to higher e-customer satisfaction and shorter development times of personalized search engines. We report practical experiences ranging from m-commerce and comparison shopping to a large-scale performance test for a job portal.

#index 994018
#* The Rubicon of smart data
#@ Roger King
#t 2002
#c 4

#index 994019
#* Information management challenges from the aerospace industry
#@ Suryanarayana M. Sripada
#t 2002
#c 4
#% 273951
#% 386111
#% 479622
#! The aerospace industry poses significant challenges to information management unlike any other industry. Data management challenges arising from different segments of the aerospace business are identified through illustrative scenarios. These examples and challenges could provide focus and stimulus to further research in information management.

#index 994020
#* Experiments on query expansion for internet yellow page services using web log mining
#@ Yusuke Ohura;Katsumi Takahashi;Iko Pramudiono;Masaru Kitsuregawa
#t 2002
#c 4
#% 124010
#% 173879
#% 186340
#% 202011
#% 209662
#% 266283
#% 309867
#% 342870
#% 584891
#% 614610
#% 661023
#! Tremendous amount of access log data is accumulated at many web sites. Several efforts to mine the data and apply the results to support end-users or to re-design the Web site's structure have been proposed. This paper describes our trial on access logs utilization from commercial yellow page service called "iTOWNPAGE". Our initial statistical analysis reveals that many users search various categories-even non-sibling ones in the provided hierarchy - together, or finish their search without any results that match their queries. To solve these problems, we first cluster user requests from the access logs using enhanced K-means clustering algorithm and then apply them for query expansion. Our method includes two-steps expansion that 1) recommends similar categories to the request, and 2) suggests related categories although they are nonsimilar in existing category hierarchy. We also report some evaluations that show the effectiveness of the prototype system.

#index 994021
#* Issues and evaluations of caching solutions for web application acceleration
#@ Wen-Syan Li;Wang-Pin Hsiung;Dmitri V. Kalashnikov;Radu Sion;Oliver Po;Divyakant Agrawal;K. Selçuk Candan
#t 2002
#c 4
#% 302484
#% 309705
#% 330682
#% 333995
#% 334052
#% 346714
#% 348124
#% 397357
#% 397402
#% 480495
#% 480634
#% 480814
#% 993978
#! Response time is a key differentiation among electronic commerce (e-commerce) applications. For many e-commerce applications, Web pages are created dynamically based on the current state of a business stored in database systems. Recently, the topic of Web acceleration for database-driven Web applications has drawn a lot of attention in both the research community and commercial arena. In this paper, we analyze the factors that have impacts on the performance and scalability of Web applications. We discuss system architecture issues and describe approaches to deploying caching solutions for accelerating Web applications. We give the performance matrix measurement for network latency and various system architectures. The paper is summarized with a road map for creating high performance Web applications.

#index 994022
#* An automated system for web portal personalization
#@ Charu C. Aggarwal;Philip S. Yu
#t 2002
#c 4
#% 202011
#% 220706
#% 220709
#% 220711
#% 241033
#% 280447
#% 280500
#% 280513
#% 300131
#% 406493
#% 463734
#% 464839
#% 466638
#% 482113
#% 505719
#% 564279
#% 565631
#! This paper proposes a system for personalization of web portals. A specic implementation is discussed in reference to a web portal containing a news feed service. Techniques are proposed for effective categorization, management, and personalization of news feeds obtained from a live news wire service. The process consists of two steps: first manual input is required to build the domain knowledge which could be site-specific; then the automated component uses this domain knowledge in order to perform the personalization, categorization and presentation. Effective schemes for advertising are proposed, where the targeting is done using both the information about the user and the content of the web page on which the advertising icon appears. Automated techniques for identifying sudden variations in news patterns are described; these may be used for supporting news-alerts. A description of a version of this software for our customer web site is provided.

#index 994023
#* EOS: exactly-once E-service middleware
#@ German Shegalov;Gerhard Weikum;Roger Barga;David Lomet
#t 2002
#c 4
#% 660002
#% 665442
#% 665581

#index 994024
#* ServiceGlobe: distributing E-services across the internet
#@ Markus Keidl;Stefan Seltzsam;Konrad Stocker;Alfons Kemper
#t 2002
#c 4
#% 572300

#index 994025
#* SELF-SERV: a platform for rapid composition of web services in a peer-to-peer environment
#@ Quan Z. Sheng;Boualem Benatallah;Marlon Dumas;Eileen Oi-Yan Mak
#t 2002
#c 4
#% 565472
#% 659964

#index 994026
#* Database technologies for electronic commerce
#@ Rakesh Agrawal;Ramakrishnan Srikant;Yirong Xu
#t 2002
#c 4
#% 333854
#% 348164
#% 480629

#index 994027
#* Advanced database technologies in a diabetic healthcare system
#@ Wynne Hsu;Mong Li Lee;Beng Chin Ooi;Pranab Kumar Mohanty;Keng Lik Teo;Chenyi Xia
#t 2002
#c 4
#% 301174
#! With the increased emphasis on healthcare worldwide, the issue of being able to efficiently and effectively manage large amount of patient information in diverse medium becomes critical. In this work, we will demonstrate how advanced database technologies are used in RETINA, an integrated system for the screening and management of diabetic patients. RETINA captures the profile and retinal images of diabetic patients and automatically processes the retina fundus images to extract interesting features. Given the wealth of information acquired, we employ novel techniques to determine the risk profile of patients for better patient care management and to target significant subpopulations for more detailed studies. The results of such studies can be used to introduce effective preventive measures for the targeted sub-populations.

#index 994028
#* RTMonitor: real-time data monitoring using mobile agent technologies
#@ Kam-Yiu Lam;Alan Kwan;Krithi Ramamritham
#t 2002
#c 4
#% 330682
#% 443292
#% 443298
#% 443376
#! RTMonitor is a real-time data management system for traffic navigation applications. In our system, mobile vehicles initiate time-constrained navigation requests and RTMonitor calculates and communicates the best paths for the clients based on the road network and real-time traffic data. The correctness of the suggested routes highly depends on how well the system can maintain temporal consistency of the traffic data. To minimize the overheads of maintaining the real-time data, RTMonitor adopts a cooperative and distributed approach using mobile agents which can greatly reduce the amount of communications and improves the scalability of the system. To minimize the space and message overheads, we have designed a two-level traffic graph scheme to organize the real-time traffic data to support navigation requests. In the framework, the agents use an Adaptive PUSH OR PULL (APoP) scheme to maintain the temporal consistency of the traffic data. Our experiments using synthetic traffic data show that RTMonitor can provide efficient support to serve navigation requests in a timely fashion. Although several agents may be needed to serve a request, the size of each agent is very small (only a few kilobytes) and the resulting communication and processing overheads for data monitoring can be maintained within a reasonable level.

#index 994029
#* Viator: a tool family for graphical networking and data view creation
#@ Stephan Heymann;Katja Tham;Axel Kilian;Gunnar Wegner;Peter Rieger;Dieter Merkel;Johann Christoph Freytag
#t 2002
#c 4
#% 712090
#! Web-based data sources, particularly in Life Sciences, grow in diversity and volume. Most of the data collections are equipped with common document search, hyperlink and retrieval utilities. However, users' wishes often exceed simple document-oriented inquiries. With respect to complex scientific issues it becomes imperative to aid knowledge gain from huge interdependent and thus hard to comprehend data collections more efficiently. Especially data categories that constitute relationships between two each or more items require potent set-oriented content management, visualization and navigation utilities. Moreover, strategies are needed to discover correlations within and between data sets of independent origin. Wherever data sets possess intrinsic graph structure (e.g. of tree, forest or network type) or can be transposed into such, graphical support is considered indispensable. The Viator tool family presented during this demo depicts large graphs on the whole in a hyperbolic geometry and provides means for set-oriented context mining as well as for correlation discovery across distinct data sets at once. Its utility is proven for but not restricted to data from functional genome, transcriptome and proteome research. Viator versions are being operated either as user-end database applications or as template-fed stand-alone solutions for graphical networking.

#index 994030
#* Profiling and internet connectivity in automotive environments
#@ M. Cilia;P. Hasselmeyer;A. P. Buchmann
#t 2002
#c 4
#% 211571
#% 322676
#% 345745
#% 385995
#% 394417
#% 462067
#% 622746
#! This demo combines active DB technology in open, heterogeneous environments with the Web presence requirements of nomadic users. It illustrates these through profiling of users and Internet-enabled vehicles. A scenario is developed in which useful functionality is provided, such as instrument adjustments, maintenance and diagnostic information handling with the corresponding workflows, and convenience features, such as position-dependent language translation support and traffic information. The customization mechanism relies on an active functionality service.

#index 994031
#* GeMBASE: a geometric mediator for brain analysis with surface ensembles
#@ Simone Santini;Amarnath Gupta
#t 2002
#c 4
#% 480130
#% 892556

#index 994032
#* Extending an ORDBMS: the statemachine module
#@ Wolfgang Mahnke;Christian Mathis;Hans-Peter Steiert
#t 2002
#c 4
#% 385828
#! Extensibility is one of the mayor benefits of object-relational database management systems. We have used this system property to implement a StateMachine Module inside an object-relational database management system. The module allows the checking of dynamic integrity constraints as well as the execution of active behavior specified with the UML. Our approach demonstrates that extensibility can effectively be applied to integrate such dynamic aspects specified with UML statecharts into an object-relational database management system.

#index 994033
#* BANKS: browsing and keyword searching in relational databases
#@ B. Aditya;Gaurav Bhalotia;Soumen Chakrabarti;Arvind Hulgeri;Charuta Nakhe;Parag Parag;S. Sudarshan
#t 2002
#c 4
#% 268079
#% 479782
#% 993987
#! The BANKS system enables keyword-based search on databases, together with data and schema browsing. BANKS enables users to extract information in a simple manner without any knowledge of the schema or any need for writing complex queries. A user can get information by typing a few keywords, following hyperlinks, and interacting with controls on the displayed results. Extensive support for answer ranking forms a critical part of the BANKS system.

#index 994034
#* Active XML: peer-to-peer data and web services integration
#@ Serge Abitrboul;Omar Benjellourn;Ioana Manolescu;Tova Milo;Roger Weber
#t 2002
#c 4
#% 333979
#% 333982

#index 994035
#* LegoDB: customizing relational storage for XML documents
#@ Philip Bohannon;Juliana Freire;Jayant R. Haritsa;Prasan Roy;Jérôme Siméon
#t 2002
#c 4
#% 273922
#% 309851
#% 397364
#% 479956
#% 504574
#% 659924

#index 994036
#* enTrans: a system for flexible consistency maintenance in directory applications
#@ Anandi Herlekar;Atul Deopujari;Krithi Ramamritham;Shyamsunder Gopale;Shridhar Shukla
#t 2002
#c 4
#% 172878
#% 481752

#index 994037
#* Champagne: data change propagation for heterogeneous information systems
#@ Ralf Rantzau;Carmen Constantinescu;Uwe Heinkel;Holger Meinecke
#t 2002
#c 4
#% 683721
#! Flexible methods supporting the data interchange between autonomous information systems are important for today's increasingly heterogeneous enterprise IT infrastructures. Updates, insertions, and deletions of data objects in autonomous information systems often have to trigger data changes in other autonomous systems, even if the distributed systems are not integrated into a global schema. We suggest a solution to this problem based on the propagation and transformation of data using several XML technologies. Our prototype manages dependencies between the schemas of distributed data sources and allows to define and process arbitrary actions on changed data by manipulating all dependent data sources. The prototype comprises a propagation engine that interprets scripts based on a workflow specification language, a data dependency specification tool, a system administration tool, and a repository that stores all relevant information for these tools.

#index 994038
#* ALIAS: an active learning led interactive deduplication system
#@ Sunita Sarawagi;Anuradha Bhamidipaty;Alok Kirpal;Chandra Mouli
#t 2002
#c 4
#% 236729
#% 577238
#! Deduplication, a key operation in integrating data from multiple sources, is a time-consuming, labor-intensive and domain-specific operation. We present our design of ALIAS that uses a novel approach to ease this task by limiting the manual effort to inputing simple, domain-specific attribute similarity functions and interactively labeling a small number of record pairs. We describe how active learning is useful in selecting informative examples of duplicates and nonduplicates that can be used to train a deduplication function. ALIAS provides mechanism for efficiently applying the function on large lists of records using a novel cluster-based execution model.

#index 994039
#* A-TOPSS: a publish/subscribe system supporting approximate matching
#@ Haifeng Liu;H.-Arno Jacobsen
#t 2002
#c 4
#% 213981
#% 297191
#% 333938
#% 462223
#% 463734
#% 480296
#% 661478

#index 1015252
#* Proceedings of the 29th international conference on Very large data bases - Volume 29
#@ Johann Christoph Freytag;Peter C. Lockemann;Serge Abiteboul;Michael J. Carey;Patricia G. Selinger;Andreas Heuer
#t 2003
#c 4

#index 1015253
#* A nanotechnology-based approach to data storage
#@ E. Eleftheriou;P. Bächtold;G. Cherubini;A. Dholakia;C. Hagleitner;T. Loeliger;A. Pantazi;H. Pozidis;T. R. Albrecht;G. K. Binnig;M. Despont;U. Drechsler;U. Dürig;B. Gotsmann;D. Jubin;W. Häberle;M. A. Lantz;H. Rothuizen;R. Stutz;P. Vettiger;D. Wiesmann
#t 2003
#c 4
#% 404030
#% 1306834
#% 1770564
#! Ultrahigh storage densities of up to 1 Tb/in2. or more can be achieved by using local-probe techniques to write, read back, and erase data in very thin polymer films. The thermomechanical scanning-probe-based data-storage concept, internally dubbed "millipede", combines ultrahigh density, small form factor, and high data rates. High data rates are achieved by parallel operation of large 2D arrays with thousands micro/nanomechanical cantilevers/tips that can be batch-fabricated by silicon surface-micromachining techniques. The inherent parallelism, the ultrahigh areal densities and the small form factor may open up new perspectives and opportunities for application in areas beyond those envisaged today.

#index 1015254
#* Integrating information for on demand computing
#@ Nelson Mendonça Mattos
#t 2003
#c 4
#% 85086
#% 85089
#% 152980
#% 332162
#% 397402
#% 427030
#% 479449
#% 480152
#% 480657
#% 572314
#% 770328
#% 770330
#% 770332
#% 770334
#% 770336
#% 770338
#! Information integration provides a competitive advantage to businesses and is fundamental to on demand computing. It is strategic area of investment by software companies today whose goal is to provide a unified view of the data regardless of differences in data format, data location and access interfaces, dynamically manage data placement to match availability, currency and performance requirements, and provide autonomic features that reduce the burden on IT staffs for managing complex data architectures. This paper describes the motivation for integrating information for on demand computing, explains its requirements, and illustrates its value through usage scenarios. As shown in the paper, there is still a tremendous amount of research, engineering, and development work needed to make the full information integration vision a reality and it is expected that software companies will continue to heavily invest in aggressively pursing the information integration vision.

#index 1015255
#* The data-centric revolution in networking
#@ Scott Shenker
#t 2003
#c 4
#! Historically, there has been little overlap between the database and networking research communities; they operate on very different levels and focus on very different issues. While this strict separation of concerns has lasted for many years, in this talk I will argue that the gap has recently narrowed to the point where the two fields now have much to say to each other. Networking research has traditionally focused on enabling communication between network hosts. This research program has produced a myriad of specific algorithms and protocols to solve such problems as error recovery, congestion control, routing, multicast and quality-of-service. It has also led to a set of general architectural principles, such as fate sharing and the end-to-end principle, that provide widely applicable guidelines for allocating functionality among network entities.

#index 1015256
#* The history of histograms (abridged)
#@ Yannis Ioannidis
#t 2003
#c 4
#% 28144
#% 43163
#% 102784
#% 152585
#% 152917
#% 172902
#% 201921
#% 210189
#% 210190
#% 227883
#% 248820
#% 248821
#% 248822
#% 259995
#% 273705
#% 273887
#% 273901
#% 273902
#% 273903
#% 273906
#% 273907
#% 273909
#% 273910
#% 282206
#% 299982
#% 300193
#% 333946
#% 333947
#% 333948
#% 333986
#% 347226
#% 397364
#% 397370
#% 397371
#% 397379
#% 397385
#% 397386
#% 397389
#% 427219
#% 437405
#% 453191
#% 461918
#% 465064
#% 479648
#% 479958
#% 479967
#% 479984
#% 480125
#% 480465
#% 480488
#% 480628
#% 480803
#% 481266
#% 481620
#% 481775
#% 482092
#% 482104
#% 482123
#% 492932
#% 511649
#% 572308
#% 588594
#% 632072
#% 659920
#% 689389
#% 922066
#% 993960
#% 993968
#% 993969
#% 993970
#! The history of histograms is long and rich, full of detailed information in every step. It includes the course of histograms in different scientific fields, the successes and failures of histograms in approximating and compressing information, their adoption by industry, and solutions that have been given on a great variety of histogram-related problems. In this paper and in the same spirit of the histogram techniques themselves, we compress their entire history (including their "future history" as currently anticipated) in the given/fixed space budget, mostly recording details for the periods, events, and results with the highest (personally-biased) interest. In a limited set of experiments, the semantic distance between the compressed and the full form of the history was found relatively small!

#index 1015257
#* Complex queries over web repositories
#@ Sriram Raghavan;Hector Garcia-Molina
#t 2003
#c 4
#% 227894
#% 237193
#% 261741
#% 291299
#% 309721
#% 340141
#% 387427
#% 464719
#% 482088
#% 503878
#% 993970
#! Web repositories, such as the Stanford WebBase repository, manage large heterogeneous collections of Web pages and associated indexes. For effective analysis and mining, these repositories must provide a declarative query interface that supports complex expressive Web queries. Such queries have two key characteristics: (i) They view a Web repository simultaneously as a collection of text documents, as a navigable directed graph, and as a set of relational tables storing properties of Web pages (length, URL, title, etc.). (ii) The queries employ application-specific ranking and ordering relationships over pages and links to filter out and retrieve only the "best" query results. In this paper, we model a Web repository in terms of "Web relations" and describe an algebra for expressing complex Web queries. Our algebra extends traditional relational operators as well as graph navigation operators to uniformly handle plain, ranked, and ordered Web relations. In addition, we present an overview of the cost-based optimizer and execution engine that we have developed, to efficiently execute Web queries over large repositories.

#index 1015258
#* XSEarch: a semantic search engine for XML
#@ Sara Cohen;Jonathan Mamou;Yaron Kanza;Yehoshua Sagiv
#t 2003
#c 4
#% 268079
#% 309726
#% 333845
#% 340914
#% 342678
#% 345709
#% 345710
#% 387427
#% 458829
#% 465068
#% 654442
#! XSEarch, a semantic search engine for XML, is presented. XSEarch has a simple query language, suitable for a naive user. It returns semantically related document fragments that satisfy the user's query. Query answers are ranked using extended information-retrieval techniques and are generated in an order similar to the ranking. Advanced indexing techniques were developed to facilitate efficient implementation of XSEarch. The performance of the different techniques as well as the recall and the precision were measured experimentally. These experiments indicate that XSEarch is efficient, scalable and ranks quality results highly.

#index 1015259
#* An efficient and resilient approach to filtering and disseminating streaming data
#@ Shetal Shah;Shyamshankar Dharmarajan;Krithi Ramamritham
#t 2003
#c 4
#% 201922
#% 227885
#% 271622
#% 300179
#% 303712
#% 330581
#% 333969
#% 348124
#% 397355
#% 399551
#% 443298
#% 464228
#% 480816
#% 609890
#% 631962
#% 635804
#% 661477
#% 661478
#% 963655
#% 963887
#% 978365
#% 978376
#% 978378
#% 979357
#% 993975
#! Many web users monitor dynamic data such as stock prices, real-time sensor data and traffic data for making on-line decisions. Instances of such data can be viewed as data streams. In this paper, we consider techniques for creating a resilient and efficient content distribution network for such dynamically changing streaming data. We address the problem of maintaining the coherency of dynamic data items in a network of repositories: data disseminated to one repository is filtered by that repository and disseminated to repositories dependent on it. Our method is resilient to link failures and repository failures. This resiliency implies that data fidelity is not lost even when the repository from which (or a communication path through which) a user obtains data experiences failures. Experimental evaluation, using real world traces of streaming data, demonstrates that (i) the (computational and communication) cost of adding this redundancy is low, and (ii) surprisingly, in many cases, adding resiliency enhancing features actually improves the fidelity provided by the system even in cases when there are no failures. To further enhance fidelity, we also propose efficient techniques for filtering data arriving at one repository and for scheduling the dissemination of filtered data to another repository. Our results show that the combination of resiliency enhancing and efficiency improving techniques in fact help derive the potential that push based systems are said to have in delivering 100% fidelity. Without them, computational and communication delays inherent in dissemination networks can lead to a large fidelity loss even in push based dissemination.

#index 1015260
#* Efficient mining of XML query patterns for caching
#@ Liang Huai Yang;Mong Li Lee;Wynne Hsu
#t 2003
#c 4
#% 114567
#% 130241
#% 316562
#% 378182
#% 397358
#% 397359
#% 397360
#% 397375
#% 397409
#% 443349
#% 466644
#% 480489
#% 481290
#% 481916
#% 571036
#% 577218
#% 587737
#% 629656
#! As XML becomes ubiquitous, the efficient retrieval of XML data becomes critical. Research to improve query response time has been largely concentrated on indexing paths, and optimizing XML queries. An orthogonal approach is to discover frequent XML query patterns and cache their results to improve the performance of XML management systems. In this paper, we present an efficient algorithm called FastXMiner, to discover frequent XML query patterns. We develop theorems to prove that only a small subset of the generated candidate patterns needs to undergo expensive tree containment tests. In addition, we demonstrate how the frequent query patterns can be used to improve caching performance. Experiments results show that FastXMiner is efficient and scalable, and caching the results of frequent patterns significantly improves the query response time.

#index 1015261
#* A framework for clustering evolving data streams
#@ Charu C. Aggarwal;Jiawei Han;Jianyong Wang;Philip S. Yu
#t 2003
#c 4
#% 36672
#% 210173
#% 248790
#% 273890
#% 310488
#% 310500
#% 320942
#% 378388
#% 481281
#% 594012
#% 654489
#% 659972
#! The clustering problem is a difficult problem for the data stream domain. This is because the large volumes of data arriving in a stream renders most traditional algorithms too inefficient. In recent years, a few one-pass clustering algorithms have been developed for the data stream problem. Although such methods address the scalability issues of the clustering problem, they are generally blind to the evolution of the data and do not address the following issues: (1) The quality of the clusters is poor when the data evolves considerably over time. (2) A data stream clustering algorithm requires much greater functionality in discovering and exploring clusters over different portions of the stream. The widely used practice of viewing data stream clustering algorithms as a class of one-pass clustering algorithms is not very useful from an application point of view. For example, a simple one-pass clustering algorithm over an entire data stream of a few years is dominated by the outdated history of the stream. The exploration of the stream over different time windows can provide the users with a much deeper understanding of the evolving behavior of the clusters. At the same time, it is not possible to simultaneously perform dynamic clustering over all possible time horizons for a data stream of even moderately large volume. This paper discusses a fundamentally different philosophy for data stream clustering which is guided by application-centered requirements. The idea is divide the clustering process into an online component which periodically stores detailed summary statistics and an offine component which uses only this summary statistics. The offine component is utilized by the analyst who can use a wide variety of inputs (such as time horizon or number of clusters) in order to provide a quick understanding of the broad clusters in the data stream. The problems of efficient choice, storage, and use of this statistical data for a fast data stream turns out to be quite tricky. For this purpose, we use the concepts of a pyramidal time frame in conjunction with a microclustering approach. Our performance experiments over a number of real and synthetic data sets illustrate the effectiveness, efficiency, and insights provided by our approach.

#index 1015262
#* A regression-based temporal pattern mining scheme for data streams
#@ Wei-Guang Teng;Ming-Syan Chen;Philip S. Yu
#t 2003
#c 4
#% 310500
#% 333926
#% 342600
#% 342689
#% 345857
#% 379445
#% 397354
#% 397383
#% 397426
#% 420063
#% 458843
#% 463903
#% 464204
#% 466506
#% 479785
#% 480156
#% 594012
#% 993948
#% 993958
#% 993960
#! We devise in this paper a regression-based algorithm, called algorithm FTP-DS (Frequent Temporal Patterns of Data Streams), to mine frequent temporal patterns for data streams. While providing a general framework of pattern frequency counting, algorithm FTP-DS has two major features, namely one data scan for online statistics collection and regression-based compact pattern representation.To attain the feature of one data scan, the data segmentation and the pattern growth scenarios are explored for the frequency counting purpose. Algorithm FTP-DS scans online transaction flows and generates candidate frequent patterns in real time. The second important feature of algorithm FTP-DS is on the regression-based compact pattern representation. Specifically, to meet the space constraint, we devise for pattern representation a compact ATF (standing for Accumulated Time and Frequency) form to aggregately comprise all the information required for regression analysis. In addition, we develop the techniques of the segmentation tuning and segment relaxation to enhance the functions of FTP-DS. With these features, algorithm FTP-DS is able to not only conduct mining with variable time intervals but also perform trend detection effectively. Synthetic data and a real dataset which contains net-Permission work alarm logs from a major telecommunication company are utilized to verify the feasibility of algorithm FTP-DS.

#index 1015263
#* On the cost of multilingualism in database systems
#@ A. Kumaran;Jayant R. Haritsa
#t 2003
#c 4
#% 82349
#% 123589
#% 179717
#% 224135
#% 227875
#% 335500
#% 386510
#% 437684
#% 656080
#! Database engines are well-designed for storing and processing text data based on Latin scripts. But in today's global village, databases should ideally support multilingual text data equally efficiently. While current database systems do support management of multilingual data, we are not aware of any prior studies that compare and quantify their performance in this regard. In this paper, we first compare the multilingual functionality provided by a suite of popular database systems. We find that while the systems support most SQL-defined multilingual functionality, some needed features are not yet implemented. We then profile their performance in handling text data in IS0:8859, the standard database character set, and in Unicode, the multilingual character set. Our experimental results indicate significant performance degradation while handling multilingual data in these database systems. Worse, we find that the query optimizer's accuracy is different between standard and multilingual data types. As a first step towards alleviating the above problems, we propose Cuniform, a compressed format that is trivially convertible to Unicode. Our initial experimental results with Cuniform indicate that it largely eliminates the performance degradation for multilingual scripts with small repertoires. Further, the Cuniform format can elegantly support extensions to SQL for multilexical text processing.

#index 1015264
#* Distributed top-N query processing with possibly uncooperative local systems
#@ Clement Yu;George Philip;Weiyi Meng
#t 2003
#c 4
#% 300193
#% 320584
#% 333945
#% 333946
#% 333947
#% 333951
#% 337285
#% 397385
#% 442868
#% 479451
#% 479623
#% 479816
#% 479967
#% 479984
#% 480125
#% 481923
#% 482092
#% 571043
#% 631927
#% 994017
#! We consider the problem of processing top-N queries in a distributed environment with possibly uncooperative local database systems. For a given top-N query, the problem is to find the N tuples that satisfy the query the best but not necessarily completely in an efficient manner. Top-N queries are gaining popularity in relational databases and are expected to be very useful for e-commerce applications. Many companies provide the same type of goods and services to the public on the Web, and relational databases may be employed to manage the data. It is not feasible for a user to query a large number of databases. It is therefore desirable to provide a facility where a user query is accepted at some site, suitable tuples from appropriate sites are retrieved and the results are merged and then presented to the user. In this paper, we present a method for constructing the desired facility. Our method consists of two steps. The first step determines which databases are likely to contain the desired tuples for a given query so that the databases can be ranked based on their desirability with respect to the query. Four different techniques are introduced for this step with one requiring no cooperation from local systems. The second step determines how the ranked databases should be searched and what tuples from the searched databases should be returned. A new algorithm is proposed for this purpose. Experimental results are presented to compare different methods and very promising results are obtained using the method that requires no cooperation from local databases.

#index 1015265
#* Optimized query execution in large search engines with global page ordering
#@ Xiaohui Long;Torsten Suel
#t 2003
#c 4
#% 157880
#% 183334
#% 198335
#% 212665
#% 213981
#% 228097
#% 262099
#% 268073
#% 268079
#% 282905
#% 290703
#% 298181
#% 309779
#% 330609
#% 330706
#% 330707
#% 333854
#% 340141
#% 340886
#% 340887
#% 348173
#% 387427
#% 397608
#% 433941
#% 476484
#% 480330
#% 591565
#% 616528
#% 631988
#% 659255
#% 659993
#% 659994
#% 708700
#% 993962
#! Large web search engines have to answer thousands of queries per second with interactive response times. A major factor in the cost of executing a query is given by the lengths of the inverted lists for the query terms, which increase with the size of the document collection and are often in the range of many megabytes. To address this issue, IR and database researchers have proposed pruning techniques that compute or approximate term-based ranking functions without scanning over the full inverted lists. Over the last few years, search engines have incorporated new types of ranking techniques that exploit aspects such as the hyperlink structure of the web or the popularity of a page to obtain improved results. We focus on the question of how such techniques can be efficiently integrated into query processing. In particular, we study pruning techniques for query execution in large engines in the case where we have a global ranking of pages, as provided by Pagerank or any other method, in addition to the standard term-based approach. We describe pruning schemes for this case and evaluate their efficiency on an experimental cluster-based search engine with million web pages. Our results show that there is significant potential benefit in such techniques.

#index 1015266
#* Path queries on compressed XML
#@ Peter Buneman;Martin Grohe;Christoph Koch
#t 2003
#c 4
#% 3873
#% 145228
#% 273922
#% 287349
#% 300153
#% 365338
#% 462235
#% 464720
#% 464724
#% 479465
#% 479956
#% 480821
#% 551856
#% 587580
#% 659997
#% 993939
#% 994000
#! Central to any XML query language is a path language such as XPath which operates on the tree structure of the XML document. We demonstrate in this paper that the tree structure can be effectively compressed and manipulated using techniques derived from symbolic model checking. Specifically, we show first that succinct representations of document tree structures based on sharing subtrees are highly effective. Second, we show that compressed structures can be queried directly and efficiently through a process of manipulating selections of nodes and partial decompression. We study both the theoretical and experimental properties of this technique and provide algorithms for querying our compressed instances using node-selecting path query languages such as XPath. We believe the ability to store and manipulate large portions of the structure of very large XML documents in main memory is crucial to the development of efficient, scalable native XML databases and query engines.

#index 1015267
#* On the minimization of Xpath queries
#@ S. Flesca;F. Furfaro;E. Masciari
#t 2003
#c 4
#% 237181
#% 248025
#% 248032
#% 248033
#% 333989
#% 378393
#% 397374
#% 465051
#% 465053
#% 564264
#% 599549
#% 993939
#! XML queries are usually expressed by means of XPath expressions identifying portions of the selected documents. An XPath expression defines a way of navigating an XML tree and returns the set of nodes which are reachable from one or more starting nodes through the paths specified by the expression. The problem of efficiently answering XPath queries is very interesting and has recently received increasing attention by the research community. In particular, an increasing effort has been devoted to define effective optimization techniques for XPath queries. One of the main issues related to the optimization of XPath queries is their minimization. The minimization of XPath queries has been studied for limited fragments of XPath, containing only the descendent, the child and the branch operators. In this work, we address the problem of minimizing XPath queries for a more general fragment, containing also the wildcard operator. We characterize the complexity of the minimization of XPath queries, stating that it is NP-hard, and propose an algorithm for computing minimum XPath queries. Moreover, we identify an interesting tractable case and propose an ad hoc algorithm handling the minimization of this kind of queries in polynomial time.

#index 1015268
#* Covering indexes for XML queries: bisimulation - simulation = negation
#@ Prakash Ramanan
#t 2003
#c 4
#% 31484
#% 193212
#% 333989
#% 397360
#% 397374
#% 464724
#% 528124
#% 593696
#% 993939
#! Tree Pattern Queries (TPQ), Branching Path Queries (BPQ), and Core XPath (CXPath) are subclasses of the XML query language XPath, TPQ ⊂ BPQ ⊂ CX Path ⊂ X Path. Let TPQ = TPQ+ ⊂ BPQ+ ⊂ CX Path+ ⊂ X Path+ denote the corresponding subclasses, consisting of queries that do not involve the boolean negation operator not in their predicates. Simulation and bisimulation are two different binary relations on graph vertices that have previously been studied in connection with some of these classes. For instance, TPQ queries can be minimized using simulation. Most relevantly, for an XML document, its bisimulation quotient is the smallest index that covers (i.e., can be used to answer) all BPQ queries. Our results are as follows: • A CXPath+ query can be evaluated on an XML document by computing the simulation of the query tree by the document graph. • For an XML document, its simulation quotient is the smallest covering index for BPQ+. This, together with the previously-known result stated above, leads to the following: For BPQ covering indexes of XML documents, Bisimulation - Simulation = Negation. • For an XML document, its simulation quotient, with the idref edges ignored throughout, is the smallest covering index for TPQ. For any XML document, its simulation quotient is never larger than its bisimulation quotient; in some instances, it is exponentially smaller. Our last two results show that disallowing negation in the queries could substantially reduce the size of the smallest covering index.

#index 1015269
#* Phrase Matching in XML
#@ Sihem Amer-Yahia;Mary Fernández;Divesh Srivastava;Yu Xu
#t 2003
#c 4
#% 194247
#% 309726
#% 339373
#% 406493
#% 458829
#% 458861
#% 541480
#% 654442
#! Phrase matching is a common IR technique to search text and identify relevant documents in a document collection. Phrase matching in XML presents new challenges as text may be interleaved with arbitrary markup, thwarting search techniques that require strict contiguity or close proximity of keywords. We present a technique for phrase matching in XML that permits dynamic specification of both the phrase to be matched and the markup to be ignored. We develop an effective algorithm for our technique that utilizes inverted indices on phrase words and XML tags. We describe experimental results comparing our algorithm to an indexed-nested loop algorithm that illustrate our algorithm's efficiency.

#index 1015270
#* RRXS: redundancy reducing XML storage in relations
#@ Yi Chen;Susan Davidson;Carmem Hara;Yifeng Zheng
#t 2003
#c 4
#% 273922
#% 287295
#% 300168
#% 330627
#% 378395
#% 384978
#% 465061
#% 479465
#% 479956
#% 562455
#% 564264
#% 576092
#% 1394430
#! Current techniques for storing XML using relational technology consider the structure of an XML document but ignore its semantics as expressed by keys or functional dependencies. However, when the semantics of a document are considered redundancy may be reduced, node identifiers removed where value-based keys are available, and semantic constraints validated using relational primary key technology. In this paper, we propose a novel constraint definition called XFDs that capture structural as well as semantic information. We present a set of rewriting rules for XFDs, and use them to design a polynomial time algorithm which, given an input set of XFDs, computes a reduced set of XFDs. Based on this algorithm, we present a redundancy removing storage mapping from XML to relations called RRXS. The effectiveness of the mapping is demonstrated by experiments on three data sets.

#index 1015271
#* MARS: a system for publishing XML from mixed and redundant storage
#@ Alin Deutsch;Val Tannen
#t 2003
#c 4
#% 273922
#% 283052
#% 300168
#% 309851
#% 333935
#% 333989
#% 384978
#% 398263
#% 411554
#% 458836
#% 461897
#% 465018
#% 465053
#% 479956
#% 480317
#% 480488
#% 480657
#% 480822
#% 504580
#% 562454
#% 564416
#% 570875
#% 712339
#% 826031
#% 994015
#! We present a system for publishing as XML data from mixed (relational+XML) proprietary storage, while supporting redundancy in storage for tuning purposes. The correspondence between public and proprietary schemas is given by a combination of LAV-and GAV-style views expressed in XQuery. XML and relational integrity constraints are also taken into consideration. Starting with client XQueries formulated against the public schema the system achieves the combined effect of rewriting-with-views, composition-with-views and query minimization under integrity constraints to obtain optimal reformulations against the proprietary schema. The paper focuses on the engineering and the experimental evaluation of the MARS system.

#index 1015272
#* Projecting XML documents
#@ Amélie Marian;Jérôme Siméon
#t 2003
#c 4
#% 214091
#% 348130
#% 348166
#% 397359
#% 397360
#% 397375
#% 480296
#% 487257
#% 509696
#% 562456
#% 565266
#% 659924
#% 659995
#% 993953
#% 994015
#! XQuery is not only useful to query XML in databases, but also to applications that must process XML documents as files or streams. These applications suffer from the limitations of current main-memory XQuery processors which break for rather small documents. In this paper we propose techniques, based on a notion of projection for XML, which can be used to drastically reduce memory requirements in XQuery processors. The main contribution of the paper is a static analysis technique that can identify at compile time which parts of the input document are needed to answer an arbitrary XQuery. We present a loading algorithm that takes the resulting information to build a projected document, which is smaller than the original document, and on which the query yields the same result. We implemented projection in the Galax XQuery processor. Our experiments show that projection reduces memory requirements by a factor of 20 on average, and is effective for a wide variety of queries. In addition, projection results in some speedup during query evaluation.

#index 1015273
#* Mixed mode XML query processing
#@ Alan Halverson;Josef Burger;Leonidas Galanis;Ameet Kini;Rajasekar Krishnamurthy;Ajith Nagaraja Rao;Feng Tian;Stratis D. Viglas;Yuan Wang;Jeffrey F. Naughton;David J. DeWitt
#t 2003
#c 4
#% 172939
#% 236416
#% 333981
#% 397364
#% 397375
#% 428146
#% 458836
#% 465006
#% 465018
#% 479806
#% 480296
#% 480488
#% 480489
#% 480663
#% 570875
#% 598374
#% 659999
#% 993939
#% 993953
#% 993970
#! Querying XML documents typically involves both tree-based navigation and pattern matching similar to that used in structured information retrieval domains. In this paper, we show that for good performance, a native XML query processing system should support query plans that mix these two processing paradigms. We describe our prototype native XML system, and report on experiments demonstrating that even for simple queries, there are a number of options for how to combine tree-based navigation and structural joins based on information retrieval-style inverted lists, and that these options can have widely varying performance. We present ways of transparently using both techniques in a single system, and provide a cost model for identifying efficient combinations of the techniques. Our preliminary experimental results prove the viability of our approach.

#index 1015274
#* From tree patterns to generalized tree patterns: on efficient evaluation of XQuery
#@ Zhimin Chen;H. V. Jagadish;Laks V. S. Lakshmanan;Stelios Paparizos
#t 2003
#c 4
#% 333981
#% 378393
#% 397366
#% 397375
#% 413564
#% 413650
#% 465006
#% 465051
#% 479956
#% 487290
#% 562456
#% 564264
#% 570875
#% 570876
#% 654493
#! XQuery is the de facto standard XML query language, and it is important to have efficient query evaluation techniques available for it. A core operation in the evaluation of XQuery is the finding of matches for specified tree patterns, and there has been much work towards algorithms for finding such matches efficiently. Multiple XPath expressions can be evaluated by computing one or more tree pattern matches. However, relatively little has been done on efficient evaluation of XQuery queries as a whole. In this paper, we argue that there is much more to XQuery evaluation than a tree pattern match. We propose a structure called generalized tree pattern (GTP) for concise representation of a whole XQuery expression. Evaluating the query reduces to finding matches for its GTP. Using this idea we develop efficient evaluation plans for XQuery expressions, possibly involving join, quantifiers, grouping, aggregation, and nesting. XML data often conforms to a schema. We show that using relevant constraints from the schema, one can optimize queries significantly, and give algorithms for automatically inferring GTP simplifications given a schema. Finally, we show, through a detailed set of experiments using the TIMBER XML database system, that plans via GTPs (with or without schema knowledge) significantly outperform plans based on navigation and straightforward plans obtained directly from the query.

#index 1015275
#* Efficient processing of expressive node-selecting queries on XML data in secondary storage: a tree automata-based approach
#@ Christoph Koch
#t 2003
#c 4
#% 49315
#% 73005
#% 101943
#% 237192
#% 344425
#% 378389
#% 384978
#% 401124
#% 427027
#% 465061
#% 480296
#% 587580
#% 993939
#! We propose a new, highly scalable and efficient technique for evaluating node-selecting queries on XML trees which is based on recent advances in the theory of tree automata. Our query processing techniques require only two linear passes over the XML data on disk, and their main memory requirements are in principle independent of the size of the data. The overall running time is O(m + n), where monly depends on the query and n is the size of the data. The query language supported is very expressive and captures exactly all node-selecting queries answerable with only a bounded amount of memory (thus, all queries that can be answered by any form of finite-state system on XML trees). Visiting each tree node only twice is optimal, and current automata-based approaches to answering path queries on XML streams, which work using one linear scan of the stream, are considerably less expressive. These technical results - which give rise to expressive query engines that deal more efficiently with large amounts of data in secondary storage - are complemented with an experimental evaluation of our work.

#index 1015276
#* Query processing for high-volume XML message brokering
#@ Yanlei Diao;Michael Franklin
#t 2003
#c 4
#% 36117
#% 300166
#% 300179
#% 333938
#% 333982
#% 397353
#% 443298
#% 458847
#% 465061
#% 480268
#% 480296
#% 631962
#% 654476
#% 993950
#! XML filtering solutions developed to date have focused on the matching of documents to large numbers of queries but have not addressed the customization of output needed for emerging distributed information infrastructures. Support for such customization can significantly increase the complexity of the filtering process. In this paper, we show how to leverage an efficient, shared path matching engine to extract the specific XML elements needed to generate customized output in an XML Message Broker. We compare three different approaches that differ in the degree to which they exploit the shared path matching engine. We also present techniques to optimize the post-processing of the path matching engine output, and to enable the sharing of such processing across queries. We evaluate these techniques with a detailed performance study of our implementation.

#index 1015277
#* Holistic twig joins on indexed XML documents
#@ Haifeng Jiang;Wei Wang;Hongjun Lu;Jeffrey Xu Yu
#t 2003
#c 4
#% 210166
#% 333981
#% 397358
#% 397366
#% 397375
#% 411554
#% 458836
#% 479806
#% 479956
#% 480430
#% 480489
#% 654453
#% 659999
#% 993953
#! Finding all the occurrences of a twig pattern specified by a selection predicate on multiple elements in an XML document is a core operation for efficient evaluation of XML queries. Holistic twig join algorithms were proposed recently as an optimal solution when the twig pattern only involves ancestor-descendant relationships. In this paper, we address the problem of efficient processing of holistic twig joins on all/partly indexed XML documents. In particular, we propose an algorithm that utilizes available indices on element sets. While it can be shown analytically that the proposed algorithm is as efficient as the existing state-of-the-art algorithms in terms of worst case I/O and CPU cost, experimental results on various datasets indicate that the proposed index-based algorithm performs significantly better than the existing ones, especially when binary structural joins in the twig pattern have varying join selectivities.

#index 1015278
#* Maximizing the output rate of multi-way join queries over streaming information sources
#@ Stratis D. Viglas;Jeffrey F. Naughton;Josef Burger
#t 2003
#c 4
#% 9240
#% 41903
#% 191154
#% 248795
#% 273911
#% 300167
#% 322884
#% 397352
#% 397353
#% 427195
#% 479617
#% 479963
#! Recently there has been a growing interest in join query evaluation for scenarios in which inputs arrive at highly variable and unpredictable rates. In such scenarios, the focus shifts from completing the computation as soon as possible to producing a prefix of the output as soon as possible. To handle this shift in focus, most solutions to date rely upon some combination of streaming binary operators and "on-the-fly" execution plan reorganization. In contrast, we consider the alternative of extending existing symmetric binary join operators to handle more than two inputs. Toward this end, we have completed a prototype implementation of a multi-way join operator, which we term the "MJoin" operator, and explored its performance. Our results show that in many instances the MJoin produces outputs sooner than any tree of binary operators. Additionally, since MJoins are completely symmetric with respect to their inputs, they can reduce the need for expensive runtime plan reorganization. This suggests that supporting multiway joins in a single, symmetric, streaming operator may be a useful addition to systems that support queries over input streams from remote sites.

#index 1015279
#* Scheduling for shared window joins over data streams
#@ Moustafa A. Hammad;Michael J. Franklin;Walid G. Aref;Ahmed K. Elmagarmid
#t 2003
#c 4
#% 245996
#% 263479
#% 273910
#% 273911
#% 340635
#% 397353
#% 480642
#% 853011
#% 993948
#% 993949
#! Continuous Query (CQ) systems typically exploit commonality among query expressions to achieve improved efficiency through shared processing. Recently proposed CQ systems have introduced window specifications in order to support unbounded data streams. There has been, however, little investigation of sharing for windowed query operators. In this paper, we address the shared execution of windowed joins, a core operator for CQ systems. We show that the strategy used in systems to date has a previously unreported performance flaw that can negatively impact queries with relatively small windows. We then propose two new execution strategies for shared joins. We evaluate the alternatives using both analytical models and implementation in a DBMS. The results show that one strategy, called MQT, provides the best performance over a range of workload settings.

#index 1015280
#* Load shedding in a data stream manager
#@ Nesime Tatbul;Uğur Çetintemel;Stan Zdonik;Mitch Cherniack;Michael Stonebraker
#t 2003
#c 4
#% 333926
#% 480628
#% 654444
#% 654508
#% 726621
#% 993949
#% 1015324
#% 1863831
#! A Data Stream Manager accepts push-based inputs from a set of data sources, processes these inputs with respect to a set of standing queries, and produces outputs based on Quality-of-Service (QoS) specifications. When input rates exceed system capacity, the system will become overloaded and latency will deteriorate. Under these conditions, the system will shed load, thus degrading the answer, in order to improve the observed latency of the results. This paper examines a technique for dynamically inserting and removing drop operators into query plans as required by the current load. We examine two types of drops: the first drops a fraction of the tuples in a randomized fashion, and the second drops tuples based on the importance of their content. We address the problems of determining when load shedding is needed, where in the query plan to insert drops, and how much of the load should be shed at that point in the plan. We describe efficient solutions and present experimental evidence that they can bring the system back into the useful operating range with minimal degradation in answer quality.

#index 1015281
#* Querying the internet with PIER
#@ Ryan Huebsch;Joseph M. Hellerstein;Nick Lanham;Boon Thau Loo;Scott Shenker;Ion Stoica
#t 2003
#c 4
#% 45074
#% 86929
#% 115661
#% 136740
#% 217052
#% 264263
#% 275367
#% 300167
#% 340175
#% 340176
#% 340635
#% 378388
#% 397295
#% 479937
#% 496162
#% 496288
#% 496291
#% 505869
#% 571217
#% 674136
#% 790040
#% 805466
#% 979743
#! The database research community prides itself on scalable technologies. Yet database systems traditionally do not excel on one important scalability dimension: the degree of distribution. This limitation has hampered the impact of database technologies on massively distributed systems like the Internet. In this paper, we present the initial design of PIER, a massively distributed query engine based on overlay networks, which is intended to bring database query processing facilities to new, widely distributed environments. We motivate the need for massively distributed queries, and argue for a relaxation of certain traditional database research goals in the pursuit of scalability and widespread adoption. We present simulation results showing PIER gracefully running relational queries across thousands of machines, and show results from the same software base in actual deployment on a large experimental cluster.

#index 1015282
#* Tuple routing strategies for distributed eddies
#@ Feng Tian;David J. DeWitt
#t 2003
#c 4
#% 230957
#% 248820
#% 248822
#% 300166
#% 300167
#% 300179
#% 338425
#% 340635
#% 378388
#% 378408
#% 379444
#% 379445
#% 397352
#% 397353
#% 397354
#% 443298
#% 480465
#% 654462
#% 659945
#% 835744
#% 993948
#% 993949
#% 993999
#! Many applications that consist of streams of data are inherently distributed. Since input stream rates and other system parameters such as the amount of available computing resources can fluctuate significantly, a stream query plan must be able to adapt to these changes. Routing tuples between operators of a distributed stream query plan is used in several data stream management systems as an adaptive query optimization technique. The routing policy used can have a significant impact on system performance. In this paper, we use a queuing network to model a distributed stream query plan and define performance metrics for response time and system throughput. We also propose and evaluate several practical routing policies for a distributed stream management system. The performance results of these policies are compared using a discrete event simulator. Finally, we study the impact of the routing policy on system throughput and resource allocation when computing resources can be shared between operators.

#index 1015283
#* AQuery: query language for ordered data, optimization techniques, and experiments
#@ Alberto Lerner;Dennis Shasha
#t 2003
#c 4
#% 29142
#% 43162
#% 137862
#% 210184
#% 227894
#% 252608
#% 397414
#% 397592
#% 458559
#% 482088
#% 503878
#% 565457
#% 571045
#% 572268
#% 645385
#! An order-dependent query is one whose result (interpreted as a multiset) changes if the order of the input records is changed. In a stock-quotes database, for instance, retrieving all quotes concerning a given stock for a given day does not depend on order, because the collection of quotes does not depend on order. By contrast, finding a stock's five-price moving-average in a trades table gives a result that depends on the order of the table. Query languages based on the relational data model can handle order-dependent queries only through add-ons. SQL:1999, for instance, has a new "window" mechanism which can sort data in limited parts of a query. Add-ons make order-dependent queries di_cult to write and to optimize. In this paper we show that order can be a natural property of the underlying data model and algebra. We introduce a new query language and algebra, called AQuery, that supports order from-the-ground-up. New order-related query transformations arise in this setting. We show by experiment that this framework - language plus optimization techniques - brings orders-of-magnitude improvement over SQL:1999 systems on many natural order-dependent queries.

#index 1015284
#* Wise-integrator: an automatic integrator of web search interfaces for E-commerce
#@ Hai He;Weiyi Meng;Clement Yu;Zonghuan Wu
#t 2003
#c 4
#% 22948
#% 55294
#% 115462
#% 120649
#% 198058
#% 227992
#% 248801
#% 307632
#% 331769
#% 333990
#% 480479
#% 480645
#% 481923
#% 572314
#% 654459
#% 660001
#% 993982
#! More and more databases are becoming Web accessible through form-based search interfaces, and many of these sources are E-commerce sites. Providing a unified access to multiple E-commerce search engines selling similar products is of great importance in allowing users to search and compare products from multiple sites with ease. One key task for providing such a capability is to integrate the Web interfaces of these E-commerce search engines so that user queries can be submitted against the integrated interface. Currently, integrating such search interfaces is carried out either manually or semi-automatically, which is inefficient and difficult to maintain. In this paper, we present WISE-Integrator - a tool that performs automatic integration of Web Interfaces of Search Engines. WISE-Integrator employs sophisticated techniques to identify matching attributes from different search interfaces for integration. It also resolves domain differences of matching attributes. Our experimental results based on 20 and 50 interfaces in two different domains indicate that WISE-Integrator can achieve high attribute matching accuracy and can produce high-quality integrated search interfaces without human interactions.

#index 1015285
#* SASH: a self-adaptive histogram set for dynamically changing workloads
#@ Lipyeow Lim;Min Wang;Jeffrey Scott Vitter
#t 2003
#c 4
#% 92148
#% 172902
#% 210190
#% 245654
#% 248822
#% 273901
#% 273902
#% 277396
#% 333946
#% 333947
#% 333948
#% 333986
#% 480803
#% 482092
#% 528180
#% 1809531
#! Most RDBMSs maintain a set of histograms for estimating the selectivities of given queries. These selectivities are typically used for cost-based query optimization. While the problem of building an accurate histogram for a given attribute or attribute set has been well-studied, little attention has been given to the problem of building and tuning a set of histograms collectively for multidimensional queries in a self-managed manner based only on query feedback. In this paper, we present SASH, a Self-Adaptive Set of Histograms that addresses the problem of building and maintaining a set of histograms. SASH uses a novel two-phase method to automatically build and maintain itself using query feedback information only. In the online tuning phase, the current set of histograms is tuned in response to the estimation error of each query in an online manner. In the restructuring phase, a new and more accurate set of histograms replaces the current set of histograms. The new set of histograms (attribute sets and memory distribution) is found using information from a batch of query feedback. We present experimental results that show the effectiveness and accuracy of our approach.

#index 1015286
#* VIPAS: virtual link powered authority search in the web
#@ Chi-Chun Lin;Ming-Syan Chen
#t 2003
#c 4
#% 90661
#% 240208
#% 249110
#% 255165
#% 255179
#% 268073
#% 268079
#% 282905
#% 320930
#% 387427
#% 443082
#% 584891
#% 614610
#% 618427
#% 630984
#% 637570
#! With the exponential growth of the World Wide Web, looking for pages with high quality and relevance in the Web has become an important research field. There have been many keyword-based search engines built for this purpose. However, these search engines usually suffer from the problem that a relevant Web page may not contain the keyword in its page text. Algorithms exploiting the link structure of Web documents, such as HITS, have also been proposed to overcome the problems of traditional search engines. Though these algorithms perform better than keyword-based search engines, they still have some defects. Among others, one major problem is that links in Web pages are only able to reflect the view of the page authors on the topic of those pages but not that of the page readers. In this paper, we propose a new algorithm with the idea of using virtual links which are created according to what the user behaves in browsing the output list of the query result. These virtual links are then employed to identify authoritative resources in the Web. Speci fically, the algorithm, referred to as algorithm VIPAS (standing for virtual link powered authority search), is divided into three phases. The first phase performs basic link analysis. The second phase collects statistics by observing the user behavior in browsing pages listed in the query result, and virtual links are then created according to what observed. In the third phase, these virtual links as well as real ones are taken together to produce an updated list of authoritative pages that will be presented to the user when the query with similar keywords is encountered next time. A Web warehouse is built and the algorithm is integrated into the system. By conducting experiments on the system, we have shown that VIPAS is not only very effective but also very adaptive in providing much more valuable information to users.

#index 1015287
#* Balancing performance and data freshness in web database servers
#@ Alexandros Labrinidis;Nick Roussopoulos
#t 2003
#c 4
#% 44638
#% 45094
#% 273917
#% 279164
#% 286991
#% 300177
#% 333962
#% 397357
#% 397402
#% 464706
#% 480474
#% 480818
#% 566126
#% 654503
#% 654504
#% 978365
#! Personalization, advertising, and the sheer volume of online data generate a staggering amount of dynamic web content. In addition to web caching, View Materialization has been shown to accelerate the generation of dynamic web content. View materialization is an attractive solution as it decouples the serving of access requests from the handling of updates. In the context of the Web, selecting which views to materialize must be decided online and needs to consider both performance and data freshness, which we refer to as the Online View Selection problem. In this paper, we define data freshness metrics, provide an adaptive algorithm for the online view selection problem, and present experimental results.

#index 1015288
#* Buffering accesses to memory-resident index structures
#@ Jingren Zhou;Kenneth A. Ross
#t 2003
#c 4
#% 153260
#% 300194
#% 317933
#% 333940
#% 333949
#% 397362
#% 427199
#% 443093
#% 479819
#% 479821
#% 479974
#% 480119
#% 527174
#% 631937
#! Recent studies have shown that cache-conscious indexes outperform conventional main memory indexes. Cache-conscious indexes focus on better utilization of each cache line for improving search performance of a single lookup. None has exploited cache spatial and temporal locality between consecutive lookups. We show that conventional indexes, even "cache-conscious" ones, suffer from significant cache thrashing between accesses. Such thrashing can impact the performance of applications such as stream processing and query operations such as index-nested-loops join. We propose techniques to buffer accesses to memory-resident tree-structured indexes to avoid cache thrashing. We study several alternative designs of the buffering technique, including whether to use fixed-size or variable-sized buffers, whether to buffer at each tree level or only at some of the levels, how to support bulk access while there are concurrent updates happening to the index, and how to preserve the order of the incoming lookups in the output results. Our methods improve cache performance for both cache-conscious and conventional index structures. Our experiments show that buffering techniques enable a probe throughput that is two to three times higher than traditional methods.

#index 1015289
#* Data morphing: an adaptive, cache-conscious storage technique
#@ Richard A. Hankins;Jignesh M. Patel
#t 2003
#c 4
#% 872
#% 210081
#% 262123
#% 271492
#% 275367
#% 286258
#% 320113
#% 321250
#% 390132
#% 479821
#% 480119
#% 480821
#% 483997
#% 566122
#% 632058
#% 802918
#! The number of processor cache misses has a critical impact on the performance of DBMSs running on servers with large main-memory configurations. In turn, the cache utilization of database systems is highly dependent on the physical organization of the records in main-memory. A recently proposed storage model, called PAX, was shown to greatly improve the performance of sequential file-scan operations when compared to the commonly implemented N-ary storage model. However, the PAX storage model can also demonstrate poor cache utilization for other common operations, such as index scans. Under a workload of heterogenous database operations, neither the PAX storage model nor the N-ary storage model is optimal. In this paper, we propose a flexible data storage technique called Data Morphing. Using Data Morphing, a cache-efficient attribute layout, called a partition, is first determined through an analysis of the query workload. This partition is then used as a template for storing data in a cache-efficient way. We present two algorithms for computing partitions, and also present a versatile storage model that accommodates the dynamic reorganization of the attributes in a file. Finally, we experimentally demonstrate that the Data Morphing technique provides a significant performance improvement over both the traditional N-ary storage model and the PAX model.

#index 1015290
#* COMBI-operator - database support for data mining applications
#@ Alexander Hinneburg;Dirk Habich;Wolfgang Lehner
#t 2003
#c 4
#% 227868
#% 227880
#% 248784
#% 248813
#% 273916
#% 300131
#% 300213
#% 333925
#% 342704
#% 397384
#% 397420
#% 464215
#% 464998
#% 479450
#% 479787
#% 479795
#% 479962
#% 479972
#% 480144
#% 480669
#% 481951
#% 481954
#% 659966
#! Database support for data mining has become an important research topic. Especially for large high-dimensional data volumes, comprehensive support from the database side is necessary. In this paper we identify the data intensive subproblem of aggregating high-dimensional data in all possible low-dimensional projections (for instance estimating low-dimensional histograms), which occurs in several established data mining techniques. Second, we show that existing OLAP SQL-extensions are insufficient for high-dimensional data and propose a new SQL-operator, which seamlessly fits into the set of existing OLAP Group By operators. Third, we propose efficient implementations for the operator, which take the limited resources of main memory into account. We demonstrate on a number of real and synthetic data sets that for the identified subproblem our new implementations yield a large speedup (up to factor 10) over existing methods built in commercially available database systems.

#index 1015291
#* A shrinking-based approach for multi-dimensional data analysis
#@ Yong Shi;Yuqing Song;Aidong Zhang
#t 2003
#c 4
#% 70370
#% 119374
#% 210173
#% 245445
#% 248790
#% 248792
#% 248797
#% 273890
#% 273891
#% 296738
#% 466481
#% 479799
#% 481281
#% 566128
#% 631985
#% 664845
#! Existing data analysis techniques have difficulty in handling multi-dimensional data. In this paper, we first present a novel data preprocessing technique called shrinking which optimizes the inner structure of data inspired by the Newton's Universal Law of Gravitation[22] in the real world. This data reorganization concept can be applied in many fields such as pattern recognition, data clustering and signal processing. Then, as an important application of the data shrinking preprocessing, we propose a shrinking-based approach for multi-dimensional data analysis which consists of three steps: data shrinking, cluster detection, and cluster evaluation and selection. The process of data shrinking moves data points along the direction of the density gradient, thus generating condensed, widely-separated clusters. Following data shrinking, clusters are detected by finding the connected components of dense cells. The data-shrinking and cluster-detection steps are conducted on a sequence of grids with different cell sizes. The clusters detected at these scales are compared by a cluster-wise evaluation measurement, and the best clusters are selected as the final result. The experimental results show that this approach can effectively and efficiently detect clusters in both low- and high-dimensional spaces.

#index 1015292
#* Data bubbles for non-vector data: speeding-up hierarchical clustering in arbitrary metric spaces
#@ Jianjun Zhou;Jörg Sander
#t 2003
#c 4
#% 36672
#% 201893
#% 210173
#% 273890
#% 280402
#% 333933
#% 342625
#% 631984
#! To speed-up clustering algorithms, data summarization methods have been proposed, which first summarize the data set by computing suitable representative objects. Then, a clustering algorithm is applied to these representatives only, and a clustering structure for the whole data set is derived, based on the result for the representatives. Most previous methods are, however, limited in their application domain. They are in general based on sufficient statistics such as the linear sum of a set of points, which assumes that the data is from a vector space. On the other hand, in many important applications, the data is from a metric non-vector space, and only distances between objects can be exploited to construct effective data summarizations. In this paper, we develop a new data summarization method based only on distance information that can be applied directly to non-vector data. An extensive performance evaluation shows that our method is very effective in finding the hierarchical clustering structure of non-vector data using only a very small number of data summarizations, thus resulting in a large reduction of runtime while trading only very little clustering quality.

#index 1015293
#* Finding hierarchical heavy hitters in data streams
#@ Graham Cormode;Flip Korn;S. Muthukrishnan;Divesh Srivastava
#t 2003
#c 4
#% 248820
#% 273907
#% 273916
#% 310774
#% 310899
#% 333931
#% 338425
#% 397414
#% 401896
#% 446438
#% 453512
#% 479795
#% 480628
#% 492912
#% 548479
#% 569754
#% 576119
#% 577220
#% 963735
#% 993960
#% 993969
#% 993995
#% 993996
#! Aggregation along hierarchies is a critical summary technique in a large variety of on-line applications including decision support and network management (e.g., IP clustering, denial-of-service attack monitoring). Despite the amount of recent study that has been dedicated to online aggregation on sets (e.g., quantiles, hot items), surprisingly little attention has been paid to summarizing hierarchical structure in stream data. The problem we study in this paper is that of finding Hierarchical Heavy Hitters (HHH): given a hierarchy and a fraction φ, we want to find all HHH nodes that have a total number of descendants in the data stream no smaller than φ of the total number of elements in the data stream, after discounting the descendant nodes that are HHH nodes. The resulting summary gives a topological "cartogram" of the hierarchical data. We present deterministic and randomized algorithms for finding HHHs, which builds upon existing techniques by incorporating the hierarchy into the algorithms. Our experiments demonstrate several factors of improvement in accuracy over the straightforward approach, which is due to making algorithms hierarchy-aware.

#index 1015294
#* Star-cubing: computing iceberg cubes by top-down and bottom-up integration
#@ Dong Xin;Jiawei Han;Xiaolei Li;Benjamin W. Wah
#t 2003
#c 4
#% 210182
#% 227880
#% 236410
#% 248785
#% 259995
#% 273916
#% 280448
#% 300120
#% 333925
#% 397388
#% 420053
#% 420141
#% 459025
#% 462204
#% 464706
#% 479450
#% 479476
#% 479646
#% 481290
#% 654446
#% 993958
#% 993996
#! Data cube computation is one of the most essential but expensive operations in data warehousing. Previous studies have developed two major approaches, top-down vs. bottom-up. The former, represented by the Multi-Way Array Cube (called MultiWay) algorithm [25], aggregates simultaneously on multiple dimensions; however, it cannot take advantage of Apriori pruning [2] when computing iceberg cubes (cubes that contain only aggregate cells whose measure value satisfies a threshold, called iceberg condition). The latter, represented by two algorithms: BUC [6] and H-Cubing[11], computes the iceberg cube bottom-up and facilitates Apriori pruning. BUC explores fast sorting and partitioning techniques; whereas H-Cubing explores a data structure, H-Tree, for shared computation. However, none of them fully explores multi-dimensional simultaneous aggregation. In this paper, we present a new method, Star-Cubing, that integrates the strengths of the previous three algorithms and performs aggregations on multiple dimensions simultaneously. It utilizes a star-tree structure, extends the simultaneous aggregation methods, and enables the pruning of the group-by's that do not satisfy the iceberg condition. Our performance study shows that Star-Cubing is highly efficient and outperforms all the previous methods in almost all kinds of data distributions.

#index 1015295
#* Coarse-grained optimization: techniques for rewriting SQL statement sequences
#@ Tobias Kraft;Holger Schwarz;Ralf Rantzau;Bernhard Mitschang
#t 2003
#c 4
#% 36117
#% 86949
#% 116043
#% 300138
#% 300166
#% 300179
#% 333848
#% 333965
#% 335500
#% 411750
#% 479634
#% 480268
#% 481604
#% 482111
#% 495408
#% 571091
#! Relational OLAP tools and other database applications generate sequences of SQL statements that are sent to the database server as result of a single information request provided by a user. Unfortunately, these sequences cannot be processed efficiently by current database systems because they typically optimize and process each statement in isolation. We propose a practical approach for this optimization problem, called "coarse-grained optimization," complementing the conventional query optimization phase. This new approach exploits the fact that statements of a sequence are correlated since they belong to the same information request. A lightweight heuristic optimizer modifies a given statement sequence using a small set of rewrite rules. Since the optimizer is part of a separate system layer, it is independent of but can be tuned to a specific underlying database system. We discuss implementation details and demonstrate that our approach leads to significant performance improvements.

#index 1015296
#* Processing sliding window multi-joins in continuous queries over data streams
#@ Lukasz Golab;M Tamer Özsu
#t 2003
#c 4
#% 68142
#% 116082
#% 273909
#% 273910
#% 300179
#% 310488
#% 338425
#% 340635
#% 378388
#% 378408
#% 379444
#% 379445
#% 397352
#% 397353
#% 397354
#% 443298
#% 480628
#% 578391
#% 578560
#% 979303
#% 993949
#% 993961
#% 1015278
#% 1015279
#! We study sliding window multi-join processing in continuous queries over data streams. Several algorithms are reported for performing continuous, incremental joins, under the assumption that all the sliding windows fit in main memory. The algorithms include multiway incremental nested loop joins (NLJs) and multi-way incremental hash joins. We also propose join ordering heuristics to minimize the processing cost per unit time. We test a possible implementation of these algorithms and show that, as expected, hash joins are faster than NLJs for performing equi-joins, and that the overall processing cost is influenced by the strategies used to remove expired tuples from the sliding windows.

#index 1015297
#* Continuous K-nearest neighbor queries for continuously moving points with updates
#@ Glenn S. Iwerks;Hanan Samet;Ken Smith
#t 2003
#c 4
#% 68091
#% 74141
#% 152928
#% 201876
#% 282343
#% 287466
#% 299979
#% 300174
#% 378405
#% 397377
#% 461923
#% 481599
#! In recent years there has been an increasing interest in databases of moving objects where the motion and extent of objects are represented as a function of time. The focus of this paper is on the maintenance of continuous K- nearest neighbor (k-NN) queries on moving points when updates are allowed. Updates change the functions describing the motion of the points, causing pending events to change. Events are processed to keep the query result consistent as points move. It is shown that the cost of maintaining a continuous k-NN query result for moving points represented in this way can be significantly reduced with a modest increase in the number of events processed in the presence of updates. This is achieved by introducing a continuous within query to filter the number of objects that must be taken into account when maintaining a continuous k-NN query. This new approach is presented and compared with other recent work. Experimental results are presented showing the utility of this approach.

#index 1015298
#* Staircase join: teach a relational DBMS to watch its (axis) steps
#@ Torsten Grust;Maurice van Keulen;Jens Teubner
#t 2003
#c 4
#% 333981
#% 378397
#% 397358
#% 451767
#% 480299
#% 480489
#% 993953
#% 994015
#! Relational query processors derive much of their effectiveness from the awareness of specific table properties like sort order, size, or absence of duplicate tuples. This text applies (and adapts) this successful principle to database-supported XML and XPath processing: the relational system is made tree aware, i.e., tree properties like subtree size, intersection of paths, inclusion or disjointness of subtrees are made explicit. We propose a local change to the database kernel, the staircase join, which encapsulates the necessary tree knowledge needed to improve XPath performance. Staircase join operates on an XML encoding which makes this knowledge available at the cost of simple integer operations (e.g., +, ≤ ). We finally report on quite promising experiments with a staircase join enhanced main-memory database kernel.

#index 1015299
#* Checks and balances: monitoring data quality problems in network traffic databases
#@ Flip Korn;S. Muthukrishnan;Yunyue Zhu
#t 2003
#c 4
#% 1699
#% 119374
#% 169370
#% 397369
#% 443087
#% 449074
#% 464837
#% 480496
#% 480499
#% 644182
#! Internet Service Providers (ISPs) use real-time data feeds of aggregated traffic in their network to support technical as well as business decisions. A fundamental difficulty with building decision support tools based on aggregated traffic data feeds is one of data quality. Data quality problems stem from network-specific issues (irregular polling caused by UDP packet drops and delays, topological mislabelings, etc.) and make it difficult to distinguish between artifacts and actual phenomena, rendering data analysis based on such data feeds ineffective. In principle, traditional integrity constraints and triggers may be used to enforce data quality. In practice, data cleaning is done outside the database and is ad-hoc. Unfortunately, these approaches are too rigid and limited for the subtle data quality problems arising from network data where existing problems morph with network dynamics, new problems emerge over time, and poor quality data in a local region may itself indicate an important phenomenon in the underlying network. We need a new approach - both in principle and in practice - to face data quality problems in network traffic databases. We propose a continuous data quality monitoring approach based on probabilistic, approximate constraints (PACs). These are simple, user-specified rule templates with open parameters for tolerance and likelihood. We use statistical techniques to instantiate suitable parameter values from the data, and show how to apply them for monitoring data quality.In principle, our PAC-based approach can be applied to data quality problems in any data feed. We present PAC-Man, which is the system that manages PACs for the entire aggregate network traffic database in a large ISP, and show that it is very effective in monitoring data quality problems.

#index 1015300
#* Systematic development of data mining-based data quality tools
#@ Dominik Luebbers;Udo Grimmer;Matthias Jarke
#t 2003
#c 4
#% 136350
#% 201889
#% 223781
#% 242234
#% 260121
#% 269587
#% 300136
#% 301169
#% 446076
#% 449588
#% 465564
#% 488115
#% 535837
#% 541495
#% 572314
#! Data quality problems have been a persistent concern especially for large historically grown databases. If maintained over long periods, interpretation and usage of their schemas often shifts. Therefore, traditional data scrubbing techniques based on existing schema and integrity constraint documentation are hardly applicable. So-called data auditing environments circumvent this problem by using machine learning techniques in order to induce semantically meaningful structures from the actual data, and then classifying outliers that do not fit the induced schema as potential errors. However, as the quality of the analyzed database is a-priori unknown, the design of data auditing environments requires special methods for the calibration of error measurements based on the induced schema. In this paper, we present a data audit test generator that systematically generates and pollutes artificial benchmark databases for this purpose. The test generator has been implemented as part of a data auditing environment based on the well-known machine learning algorithm C4.5. Validation in the partial quality audit of a large service-related database at Daimler-Chrysler shows the usefulness of the approach as a complement to standard data scrubbing.

#index 1015301
#* Adaptive, hands-off stream mining
#@ Spiros Papadimitriou;Anthony Brockwell;Christos Faloutsos
#t 2003
#c 4
#% 1435
#% 13453
#% 160390
#% 300123
#% 315350
#% 333926
#% 336865
#% 359751
#% 378408
#% 379445
#% 397353
#% 397354
#% 397389
#% 458843
#% 480156
#% 480628
#% 632090
#% 660003
#% 993949
#% 993958
#! Sensor devices and embedded processors are becoming ubiquitous. Their limited resources (CPU, memory and/or communication bandwidth and power) pose some interesting challenges. We need both powerful and concise "languages" to represent the important features of the data, which can (a) adapt and handle arbitrary periodic components, including bursts, and (b) require little memory and a single pass over the data. We propose AWSOM (Arbitrary Window Stream mOdeling Method), which allows sensors in remote or hostile environments to efficiently and effectively discover interesting patterns and trends. This can be done automatically, i.e., with no user intervention and expert tuning before or during data gathering. Our algorithms require limited resources and can thus be incorporated in sensors, possibly alongside a distributed query processing engine [9, 5, 22]. Updates are performed in constant time, using logarithmic space. Existing, state of the art forecasting methods (SARIMA, GARCH, etc) fall short on one or more of these requirements. To the best of our knowledge, AWSOM is the first method that has all the above characteristics. Experiments on real and synthetic datasets demonstrate that AWSOM discovers meaningful patterns over long time periods. Thus, the patterns can also be used to make long-range forecasts, which are notoriously difficult to perform. In fact, AWSOM outperforms manually set up auto-regressive models, both in terms of long-term pattern detection and modeling, as well as by at least 10× in resource consumption.

#index 1015302
#* Composing mappings among data sources
#@ Jayant Madhavan;Alon Y. Halevy
#t 2003
#c 4
#% 63354
#% 85086
#% 116303
#% 191611
#% 213982
#% 237184
#% 237190
#% 248038
#% 283052
#% 299967
#% 342389
#% 378409
#% 397351
#% 464717
#% 464727
#% 465057
#% 479783
#% 572311
#% 577359
#% 578668
#% 599549
#% 654457
#% 654468
#% 993981
#! Semantic mappings between data sources play a key role in several data sharing architectures. Mappings provide the relationships between data stored in different sources, and therefore enable answering queries that require data from other nodes in a data sharing network. Composing mappings is one of the core problems that lies at the heart of several optimization methods in data sharing networks, such as caching frequently traversed paths and redundancy analysis. This paper investigates the theoretical underpinnings of mapping composition. We study the problem for a rich mapping language, GLAV, that combines the advantages of the known mapping formalisms globalas-view and local-as-view. We first show that even when composing two simple GLAV mappings, the full composition may be an infinite set of GLAV formulas. Second, we show that if we restrict the set of queries to be in CQk (a common restriction in practice), then we can always encode the infinite set of GLAV formulas using a finite representation. Furthermore, we describe an algorithm that given a query and a finite encoding of an infinite set of GLAV formulas, finds all the certain answers to the query. Consequently, we show that for a commonly occuring class of queries it is possible to pre-compose mappings, thereby potentially offering significant savings in query processing.

#index 1015303
#* Mapping adaptation under evolving schemas
#@ Yannis Velegrakis;Renée J. Miller;Lucian Popa
#t 2003
#c 4
#% 32903
#% 125614
#% 199537
#% 201898
#% 213969
#% 227869
#% 287339
#% 315025
#% 342955
#% 378409
#% 442861
#% 443527
#% 465057
#% 480623
#% 481923
#% 482056
#% 482116
#% 488624
#% 572314
#% 576100
#% 993981
#% 994035
#! To achieve interoperability, modern information systems and e-commerce applications use mappings to translate data from one representation to another. In dynamic environments like the Web, data sources may change not only their data but also their schemas, their semantics, and their query capabilities. Such changes must be reflected in the mappings. Mappings left inconsistent by a schema change have to be detected and updated. As large, complicated schemas become more prevalent, and as data is reused in more applications, manually maintaining mappings (even simple mappings like view definitions) is becoming impractical. We present a novel framework and a tool (ToMAS) for automatically adapting mappings as schemas evolve. Our approach considers not only local changes to a schema, but also changes that may affect and transform many components of a schema. We consider a comprehensive class of mappings for relational and XML schemas with choice types and (nested) constraints. Our algorithm detects mappings affected by a structural or constraint change and generates all the rewritings that are consistent with the semantics of the mapped schemas. Our approach explicitly models mapping choices made by a user and maintains these choices, whenever possible, as the schemas and mappings evolve. We describe an implementation of a mapping management and adaptation tool based on these ideas and compare it with a mapping generation tool.

#index 1015304
#* Locking protocols for materialized aggregate join views
#@ Gang Luo;Jeffrey F. Naughton;Curt J. Ellmann;Michael W. Watzke
#t 2003
#c 4
#% 4618
#% 9241
#% 114583
#% 116087
#% 172217
#% 227864
#% 279164
#% 289224
#% 289399
#% 328431
#% 333926
#% 403195
#% 416025
#% 464705
#% 480288
#% 480589
#% 481256
#! The maintenance of materialized aggregate join views is a well-studied problem. However, to date the published literature has largely ignored the issue of concurrency control. Clearly immediate materialized view maintenance with transactional consistency, if enforced by generic concurrency control mechanisms, can result in low levels of concurrency and high rates of deadlock. While this problem is superficially amenable to well-known techniques such as fine-granularity locking and special lock modes for updates that are associative and commutative, we show that these previous techniques do not fully solve the problem. We extend previous high concurrency locking techniques to apply to materialized view maintenance, and show how this extension can be implemented even in the presence of indices on the materialized view.

#index 1015305
#* Supporting frequent updates in R-trees: a bottom-up approach
#@ Mong Li Lee;Wynne Hsu;Christian S. Jensen;Bin Cui;Keng Lik Teo
#t 2003
#c 4
#% 86950
#% 273706
#% 286237
#% 300174
#% 427199
#% 458857
#% 462218
#% 462240
#% 480093
#% 480473
#% 480817
#% 481455
#% 527166
#% 554884
#% 572263
#% 659961
#% 664835
#! Advances in hardware-related technologies promise to enable new data management applications that monitor continuous processes. In these applications, enormous amounts of state samples are obtained via sensors and are streamed to a database. Further, updates are very frequent and may exhibit locality. While the R-tree is the index of choice for multi-dimensional data with low dimensionality, and is thus relevant to these applications, R-tree updates are also relatively inefficient. We present a bottom-up update strategy for R-trees that generalizes existing update techniques and aims to improve update performance. It has different levels of reorganization--ranging from global to local--during updates, avoiding expensive top-down updates. A compact main-memory summary structure that allows direct access to the R-tree index nodes is used together with efficient bottom-up algorithms. Empirical studies indicate that the bottom-up strategy outperforms the traditional top-down technique, leads to indices with better query performance, achieves higher throughput, and is scalable.

#index 1015306
#* The ND-tree: a dynamic indexing technique for multidimensional non-ordered discrete data spaces
#@ Gang Qian;Qiang Zhu;Qiang Xue;Sakti Pramanik
#t 2003
#c 4
#% 86950
#% 201893
#% 227937
#% 227939
#% 271801
#% 287715
#% 411694
#% 427199
#% 443469
#% 464195
#% 464841
#% 479462
#% 479649
#% 481279
#% 481460
#% 481956
#% 588556
#% 631963
#% 712934
#! Similarity searches in multidimensional Nonordered Discrete Data Spaces (NDDS) are becoming increasingly important for application areas such as genome sequence databases. Existing indexing methods developed for multidimensional (ordered) Continuous Data Spaces (CDS) such as R-tree cannot be directly applied to an NDDS. This is because some essential geometric concepts/properties such as the minimum bounding region and the area of a region in a CDS are no longer valid in an NDDS. On the other hand, indexing methods based on metric spaces such as M-tree are too general to effectively utilize the data distribution characteristics in an NDDS. Therefore, their retrieval performance is not optimized. To support efficient similarity searches in an NDDS, we propose a new dynamic indexing technique, called the ND-tree. The key idea is to extend the relevant geometric concepts as well as some indexing strategies used in CDSs to NDDSs. Efficient algorithms for ND-tree construction are presented. Our experimental results on synthetic and genomic sequence data demonstrate that the performance of the ND-tree is significantly better than that of the linear scan and M-tree in high dimensional NDDSs.

#index 1015307
#* Temporal slicing in the evaluation of XML queries
#@ Dengfeng Gao;Richard T. Snodgrass
#t 2003
#c 4
#% 16028
#% 18615
#% 374530
#% 397349
#% 443136
#% 452818
#% 480129
#% 480659
#% 481928
#% 565265
#% 570878
#% 654455
#% 659923
#% 953154
#! As with relational data, XML data changes over time with the creation, modification, and deletion of XML documents. Expressing queries on time-varying (relational or XML) data is more difficult than writing queries on nontemporal data. In this paper, we present a temporal XML query language, τXQuery, in which we add valid time support to XQuery by minimally extending the syntax and semantics of XQuery. We adopt a stratum approach which maps a τXQuery query to a conventional XQuery. The paper focuses on how to perform this mapping, in particular, on mapping sequenced queries, which are by far the most challenging. The critical issue of supporting sequenced queries (in any query language) is time-slicing the input data while retaining period timestamping. Timestamps are distributed throughout an XML document, rather than uniformly in tuples, complicating the temporal slicing while also providing opportunities for optimization. We propose four optimizations of our initial maximally-fragmented time-slicing approach: selected node slicing, copy-based per-expression slicing, in-place per-expression slicing, and idiomatic slicing, each of which reduces the number of constant periods over which the query is evaluated. While performance tradeoffs clearly depend on the underlying XQuery engine, we argue that there are queries that favor each of the five approaches.

#index 1015308
#* The generalized pre-grouping transformation: aggregate-query optimization in the presence of dependencies
#@ Aris Tsois;Timos Sellis
#t 2003
#c 4
#% 32891
#% 69272
#% 98466
#% 137867
#% 190638
#% 223781
#% 287005
#% 416029
#% 442851
#% 463735
#% 479814
#% 480091
#% 480289
#% 480764
#% 481288
#% 481604
#% 481608
#% 993992
#! One of the recently proposed techniques for the efficient evaluation of OLAP aggregate queries is the usage of clustering access methods. These methods store the fact table of a data warehouse clustered according to the dimension hierarchies using special attributes called hierarchical surrogate keys. In the presence of these access methods new processing and optimization techniques have been recently proposed. One important such optimization technique, called Hierarchical Pre-Grouping, uses the hierarchical surrogate keys in order to aggregate the fact table tuples as early as possible and to avoid redundant joins. In this paper, we study the Pre-Grouping transformation, attempting to generalize its applicability and identify its relationship to other similar transformations. Our results include a general algebraic definition of the Pre-Grouping transformation along with the formal definition of sufficient conditions for applying the transformation. Using a provided theorem we show that Pre-Grouping can be applied in the presence of functional and inclusion dependencies without the explicit usage of hierarchical surrogate keys. An additional result of our study is the definition of the Surrogate-Join transformation that can modify a join condition using a number of dependencies. To our knowledge, Surrogate-Join does not belong to any of the Semantic Query Transformation types discussed in the past.

#index 1015309
#* Estimating the output cardinality of partial preaggregation with a measure of clusteredness
#@ Sven Helmer;Thomas Neumann;Guido Moerkotte
#t 2003
#c 4
#% 735
#% 54023
#% 54047
#% 55312
#% 59351
#% 86750
#% 99463
#% 131062
#% 136740
#% 169238
#% 273691
#% 299989
#% 300195
#% 318024
#% 397354
#% 442975
#% 465162
#% 479795
#% 479963
#% 480125
#% 481288
#% 481608
#% 481951
#% 562280
#% 659917
#% 660003
#% 994000
#! We introduce a new parameter, the clusteredness of data, and show how it can be used for estimating the output cardinality of a partial preaggregation operator. This provides the query optimizer with an important piece of information for deciding whether the application of partial preaggregation is beneficial. Experimental results are very promising, due to the high accuracy of the cardinality estimation based on our measure of clusteredness.

#index 1015310
#* BHUNT: automatic discovery of Fuzzy algebraic constraints in relational data
#@ Paul G. Brown;Peter J. Hass
#t 2003
#c 4
#% 131546
#% 247593
#% 333946
#% 397371
#% 397389
#% 442678
#% 458275
#% 464199
#% 480803
#% 482092
#% 993933
#! We present the BHUNT scheme for automatically discovering algebraic constraints between pairs of columns in relational data. The constraints may be "fuzzy" in that they hold for most, but not all, of the records, and the columns may be in the same table or different tables. Such constraints are of interest in the context of both data mining and query optimization, and the BHUNT methodology can potentially be adapted to discover fuzzy functional dependencies and other useful relationships. BHUNT first identifies candidate sets of column value pairs that are likely to satisfy an algebraic constraint. This discovery process exploits both system catalog information and data samples, and employs pruning heuristics to control processing costs. For each candidate, BHUNT constructs algebraic constraints by applying statistical histogramming, segmentation, or clustering techniques to samples of column values. Using results from the theory of tolerance intervals, the sample sizes can be chosen to control the number of "exception" records that fail to satisfy the discovered constraints. In query-optimization mode, BHUNT can automatically partition the data into normal and exception records. During subsequent query processing, queries can be modified to incorporate the constraints; the optimizer uses the constraints to identify new, more efficient access paths. The results are then combined with the results of executing the original query against the (small) set of exception records. Experiments on a very large database using a prototype implementation of BHUNT show reductions in table accesses of up to two orders of magnitude, leading to speedups in query processing by up to a factor of 6.8.

#index 1015311
#* Tabular placement of relational data on MEMS-based storage devices
#@ Hailing Yu;Divyakant Agrawal;Amr El Abbadi
#t 2003
#c 4
#% 286258
#% 303710
#% 315350
#% 336857
#% 390132
#% 480821
#% 963650
#% 993967
#% 1306834
#! Due to the advances in semiconductor manufacturing, the gap between main memory and secondary storage is constantly increasing. This becomes a significant performance bottleneck for Database Management Systems, which rely on secondary storage heavily to store large datasets. Recent advances in nanotechnology have led to the invention of alternative means for persistent storage. In particular, MicroElectroMechanical Systems (MEMS) based storage technology has emerged as the leading candidate for next generation storage systems. In order to integrate MEMS-based storage into conventional computing platform, new techniques are needed for I/O scheduling and data placement. In the context of relational data, it has been observed that access to relations needs to be enabled in both row-wise as well as in columnwise fashions. In this paper, we exploit the physical characteristics of MEMS-based storage devices to develop a data placement scheme for relational data that enables retrieval in both row-wise and column-wise manner. We demonstrate that this data layout not only improves I/O utilization, but results in better cache performance.

#index 1015312
#* Memory requirements for query execution in highly constrained devices
#@ Nicolas Anciaux;Luc Bouganim;Philippe Pucheral
#t 2003
#c 4
#% 136740
#% 227894
#% 273928
#% 397413
#% 402611
#% 464835
#% 465020
#% 566139
#% 572304
#% 632033
#% 660659
#% 805466
#% 993932
#! Pervasive computing introduces data management requirements that must be tackled in a growing variety of lightweight computing devices. Personal folders on chip, networks of sensors and data hosted by autonomous mobile computers are different illustrations of the need for evaluating queries confined in hardware constrained computing devices. RAM is the most limiting factor in this context. This paper gives a thorough analysis of the RAM consumption problem and makes the following contributions. First, it proposes a query execution model that reaches a lower bound in terms of RAM consumption. Second, it devises a new form of optimization, called iteration filter, that drastically reduces the prohibitive cost incurred by the preceding model, without hurting the RAM lower bound. Third, it analyses how the preceding techniques can benefit from an incremental growth of RAM. This work paves the way for setting up co-design rules helping to calibrate the RAM resource of a hardware platform according to given application's requirements as well as to adapt an application to an existing hardware platform. To the best of our knowledge, this work is the first attempt to devise co-design rules for data centric embedded applications. We illustrate the effectiveness of our techniques through a performance evaluation.

#index 1015313
#* Lachesis: robust database storage management based on device-specific performance characteristics
#@ Jiri Schindler;Anastassia Ailamaki;Gregory R. Ganger
#t 2003
#c 4
#% 107692
#% 114582
#% 136740
#% 172939
#% 201692
#% 210391
#% 220001
#% 236946
#% 240010
#% 271664
#% 291640
#% 300123
#% 397362
#% 411554
#% 442700
#% 459939
#% 459940
#% 479819
#% 479821
#% 480119
#% 480821
#% 521997
#% 571084
#% 645475
#% 993947
#! Database systems work hard to tune I/O performance, but do not always achieve the full performance potential of modern disk systems. Their abstracted view of storage components hides useful device-specific characteristics, such as disk track boundaries and advanced built-in firmware algorithms. This paper presents a new storage manager architecture, called Lachesis, that exploits and adapts to observable device-specific characteristics in order to achieve and sustain high performance. For DSS queries, Lachesis achieves I/O efficiency nearly equivalent to sequential streaming even in the presence of competing random I/O traffic. In addition, Lachesis simplifies manual configuration and restores the optimizer's assumptions about the relative costs of different access patterns expressed in query plans. Experiments using IBM DB2 I/O traces as well as a prototype implementation show that Lachesis improves standalone DSS performance by 10% on average. More importantly, when running concurrently with an on-line transaction processing (OLTP) workload, Lachesis improves DSS performance by up to 3×, while OLTP also exhibits a 7% speedup.

#index 1015314
#* Cache tables: paving the way for an adaptive database cache
#@ Mehmet Altinel;Christof Bornhövd;Sailesh Krishnamurthy;C. Mohan;Hamid Pirahesh;Berthold Reinwald
#t 2003
#c 4
#% 210176
#% 271961
#% 279164
#% 300138
#% 333965
#% 333995
#% 397400
#% 397401
#% 397402
#% 397403
#% 480128
#% 480495
#% 480814
#% 480818
#% 481916
#% 654504
#! We introduce a new database object called Cache Table that enables persistent caching of the full or partial content of a remote database table. The content of a cache table is either defined declaratively and populated in advance at setup time, or determined dynamically and populated on demand at query execution time. Dynamic cache tables exploit the characteristics of typical transactional web applications with a high volume of short transactions, simple equality predicates, and 3-4 way joins. Based on federated query processing capabilities, we developed a set of new technologies for database caching: cache tables, "Janus" (two-headed) query execution plans, cache constraints, and asynchronous cache population methods. Our solution supports transparent caching both at the edge of content-delivery networks and in the middle-tier of an enterprise application infrastructure, improving the response time, throughput and scalability of transactional web applications.

#index 1015315
#* Primitives for workload summarization and implications for SQL
#@ Surajit Chaudhuri;Prasanna Ganesan;Vivek Narasayya
#t 2003
#c 4
#% 119485
#% 203146
#% 213981
#% 248010
#% 248815
#% 273901
#% 300195
#% 333955
#% 334006
#% 341672
#% 397390
#% 408396
#% 413626
#% 465162
#% 465167
#% 479460
#% 480158
#% 480471
#% 480671
#% 480803
#% 581671
#% 632100
#% 654480
#% 654492
#! Workload information has proved to be a crucial component for database-administration tasks as well as for analysis of query logs to understand user behavior and system usage. These tasks require the ability to summarize large SQL workloads. In this paper, we identify primitives that are important to enable many important workload-summarization tasks. These primitives also appear to be useful in a variety of practical scenarios besides workload summarization. Today's SQL is inadequate to express these primitives conveniently. We discuss possible extensions to SQL and the relational engine to efficiently support such summarization primitives.

#index 1015316
#* A dependability benchmark for OLTP application environments
#@ Marco Vieira;Henrique Madeira
#t 2003
#c 4
#% 169857
#% 437689
#% 445858
#% 564976
#% 594200
#% 594330
#% 613898
#% 617418
#% 639946
#% 1505022
#! The ascendance of networked information in our economy and daily lives has increased the awareness of the importance of dependability features. OLTP (On-Line Transaction Processing) systems constitute the kernel of the information systems used today to support the daily operations of most of the business. Although these systems comprise the best examples of complex business-critical systems, no practical way has been proposed so far to characterize the impact of faults in such systems or to compare alternative solutions concerning dependability features. This paper proposes a dependability benchmark for OLTP systems. This dependability benchmark uses the workload of the TPC-C performance benchmark and specifies the measures and all the steps required to evaluate both the performance and key dependability features of OLTP systems, with emphasis on availability. This dependability benchmark is presented through a concrete example of benchmarking the performance and dependability of several different transactional systems configurations. The effort required to run the dependability benchmark is also discussed in detail.

#index 1015317
#* Supporting top-K join queries in relational databases
#@ Ihab F. Ilyas;Walid G. Aref;Ahmed K. Elmagarmid
#t 2003
#c 4
#% 159337
#% 159341
#% 227894
#% 228000
#% 273910
#% 278831
#% 330769
#% 333854
#% 397378
#% 399762
#% 411554
#% 479623
#% 480330
#% 480819
#% 659255
#% 994013
#! Ranking queries produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance.

#index 1015318
#* AniPQO: almost non-intrusive parametric query optimization for nonlinear cost functions
#@ Arvind Hulgeri;S. Sudarshan
#t 2003
#c 4
#% 58375
#% 172900
#% 273694
#% 479786
#% 480955
#% 993945
#% 993946
#! The cost of a query plan depends on many parameters, such as predicate selectivities and available memory, whose values may not be known at optimization time. Parametric query optimization (PQO) optimizes a query into a number of candidate plans, each optimal for some region of the parameter space. We propose a heuristic solution for the PQO problem for the case when the cost functions may be nonlinear in the given parameters. This solution is minimally intrusive in the sense that an existing query optimizer can be used with minor modifications. We have implemented the heuristic and the results of the tests on the TPCD benchmark indicate that the heuristic is very effective. The minimal intrusiveness, generality in terms of cost functions and number of parameters and good performance (up to 4 parameters) indicate that our solution is of significant practical importance.

#index 1015319
#* Efficient approximation of optimization queries under parametric aggregation constraints
#@ Sudipto Guha;Dimitrios Gunopoulos;Nick Koudas;Divesh Srivastava;Michail Vlachos
#t 2003
#c 4
#% 70370
#% 247577
#% 300170
#% 300180
#% 302730
#% 333847
#% 333951
#% 464726
#% 479816
#% 480819
#% 593993
#% 659993
#% 857276
#% 993957
#% 994017
#! We introduce and study a new class of queries that we refer to as OPAC (optimization under parametric aggregation constraints) queries. Such queries aim to identify sets of database tuples that constitute solutions of a large class of optimization problems involving the database tuples. The constraints and the objective function are specified in terms of aggregate functions of relational attributes, and the parameter values identify the constants used in the aggregation constraints. We develop algorithms that preprocess relations and construct indices to efficiently provide answers to OPAC queries. The answers returned by our indices are approximate, not exact, and provide guarantees for their accuracy. Moreover, the indices can be tuned easily to meet desired accuracy levels, providing a graceful tradeoff between answer accuracy and index space. We present the results of a thorough experimental evaluation analyzing the impact of several parameters on the accuracy and performance of our techniques. Our results indicate that our methodology is effective and can be deployed easily, utilizing index structures such as R-trees.

#index 1015320
#* The TPR*-tree: an optimized spatio-temporal access method for predictive queries
#@ Yufei Tao;Dimitris Papadias;Jimeng Sun
#t 2003
#c 4
#% 68091
#% 86950
#% 137887
#% 273706
#% 300174
#% 397377
#% 397386
#% 443327
#% 443444
#% 480817
#% 495433
#! A predictive spatio-temporal query retrieves the set of moving objects that will intersect a query window during a future time interval. Currently, the only access method for processing such queries in practice is the TPR-tree. In this paper we first perform an analysis to determine the factors that affect the performance of predictive queries and show that several of these factors are not considered by the TPR-tree, which uses the insertion/deletion algorithms of the R*-tree designed for static data. Motivated by this, we propose a new index structure called the TPR*- tree, which takes into account the unique features of dynamic objects through a set of improved construction algorithms. In addition, we provide cost models that determine the optimal performance achievable by any data-partition spatio-temporal access method. Using experimental comparison, we illustrate that the TPR*-tree is nearly-optimal and significantly outperforms the TPR-tree under all conditions.

#index 1015321
#* Query processing in spatial network databases
#@ Dimitris Papadias;Jun Zhang;Nikos Mamoulis;Yufei Tao
#t 2003
#c 4
#% 16790
#% 70370
#% 77979
#% 86950
#% 139176
#% 152937
#% 172949
#% 248797
#% 287466
#% 300162
#% 413797
#% 427199
#% 443105
#% 443208
#% 463251
#% 479797
#% 480093
#% 527328
#! Despite the importance of spatial networks in real-life applications, most of the spatial database literature focuses on Euclidean spaces. In this paper we propose an architecture that integrates network and Euclidean information, capturing pragmatic constraints. Based on this architecture, we develop a Euclidean restriction and a network expansion framework that take advantage of location and connectivity to efficiently prune the search space. These frameworks are successfully applied to the most popular spatial queries, namely nearest neighbors, range search, closest pairs and e-distance joins, in the context of spatial network databases.

#index 1015322
#* Multiscale histograms: summarizing topological relations in large spatial datasets
#@ Xuemin Lin;Qing Liu;Yidong Yuan;Xiaofang Zhou
#t 2003
#c 4
#% 82346
#% 227866
#% 232715
#% 242366
#% 248822
#% 252304
#% 273887
#% 300185
#% 458863
#% 498511
#% 632072
#% 632104
#% 659976
#% 993969
#% 1275343
#! Summarizing topological relations is fundamental to many spatial applications including spatial query optimization. In this paper, we present several novel techniques to effectively construct cell density based spatial histograms for range (window) summarizations restricted to the four most important topological relations: contains, contained, overlap, and disjoint. We first present a novel framework to construct a multiscale histogram composed of multiple Euler histograms with the guarantee of the exact summarization results for aligned windows in constant time. Then we present an approximate algorithm, with the approximate ratio 19/12, to minimize the storage spaces of such multiscale Euler histograms, although the problem is generally NP-hard. To conform to a limited storage space where only k Euler histograms are allowed, an effective algorithm is presented to construct multiscale histograms to achieve high accuracy. Finally, we present a new approximate algorithm to query an Euler histogram that cannot guarantee the exact answers; it runs in constant time. Our extensive experiments against both synthetic and real world datasets demonstrated that the approximate multiscale histogram techniques may improve the accuracy of the existing techniques by several orders of magnitude while retaining the cost efficiency, and the exact multiscale histogram technique requires only a storage space linearly proportional to the number of cells for the real datasets.

#index 1015323
#* Avoiding sorting and grouping in processing queries
#@ Xiaoyu Wang;Mitch Cherniack
#t 2003
#c 4
#% 210169
#% 392275
#% 397592
#% 411554
#% 442705
#% 463735
#% 479753
#% 481288
#% 571045
#! Sorting and grouping are amongst the most costly operations performed during query evaluation. System R [6] used simple inference strategies to determine orderings held of intermediate relations to avoid unnecessary sorting, and to influence join plan selection. Since then, others have proposed using integrity constraint information to infer orderings of intermediate query results. However, these proposals do not consider how to avoid grouping operations by inferring groupings, nor do they consider secondary orderings (where records in the same group satisfy some ordering). In this paper, we introduce a formalism for expressing and reasoning about order properties: ordering and grouping constraints that hold of physical representations of relations. In so doing, we can reason about how the relation is ordered or grouped, both in terms of primary and secondary orders. After formally defining order properties, we introduce a plan refinement algorithm that infers order properties for intermediate and final query results on the basis of those known to hold of query inputs, and then exploits these inferences to avoid unnecessary sorting and grouping. We then show empirical results demonstrating the benefits of plan refinement, and show that the overhead that our algorithm adds to query optimization is low.

#index 1015324
#* Operator scheduling in a data stream manager
#@ Don Carney;Uğur Çetintemel;Alex Rasin;Stan Zdonik;Mitch Cherniack;Mike Stonebraker
#t 2003
#c 4
#% 54971
#% 117903
#% 158051
#% 239984
#% 239996
#% 300167
#% 300179
#% 397352
#% 428155
#% 435110
#% 442967
#% 480642
#% 654462
#% 654508
#% 726621
#% 993949
#% 1015280
#! Many stream-based applications have sophisticated data processing requirements and real-time performance expectations that need to be met under high-volume, time-varying data streams. In order to address these challenges, we propose novel operator scheduling approaches that specify (1) which operators to schedule (2) in which order to schedule the operators, and (3) how many tuples to process at each execution step. We study our approaches in the context of the Aurora data stream manager. We argue that a fine-grained scheduling approach in combination with various scheduling techniques (such as batching of operators and tuples) can significantly improve system efficiency by reducing various system overheads. We also discuss application-aware extensions that make scheduling decisions according to per-application Quality of Service (QoS) specifications. Finally, we present prototype-based experimental results that characterize the efficiency and effectiveness of our approaches under various stream workloads and processing scenarios.

#index 1015325
#* Efficient IR-style keyword search over relational databases
#@ Vagelis Hristidis;Luis Gravano;Yannis Papakonstantinou
#t 2003
#c 4
#% 55490
#% 187402
#% 268079
#% 309726
#% 333854
#% 333951
#% 443243
#% 479803
#% 480819
#% 654442
#% 660011
#% 993987
#! Applications in which plain text coexists with structured data are pervasive. Commercial relational database management systems (RDBMSs) generally provide querying capabilities for text attributes that incorporate state-of-the-art information retrieval (IR) relevance ranking strategies, but this search functionality requires that queries specify the exact column or columns against which a given list of keywords is to be matched. This requirement can be cumbersome and inflexible from a user perspective: good answers to a keyword query might need to be "assembled" -in perhaps unforeseen ways- by joining tuples from multiple relations. This observation has motivated recent research on free-form keyword search over RDBMSs. In this paper, we adapt IR-style document-relevance ranking strategies to the problem of processing free-form keyword queries over RDBMSs. Our query model can handle queries with both AND and OR semantics, and exploits the sophisticated single-column text-search functionality often available in commercial RDBMSs. We develop query-processing strategies that build on a crucial characteristic of IR-style keyword search: only the few most relevant matches -according to some definition of "relevance"- are generally of interest. Consequently, rather than computing all matches for a keyword query, which leads to inefficient executions, our techniques focus on the top-k matches for the query, for moderate values of k. A thorough experimental evaluation over real data shows the performance advantages of our approach.

#index 1015326
#* Merging models based on given correspondences
#@ Rachel A. Pottinger;Philip A. Bernstein
#t 2003
#c 4
#% 22948
#% 24408
#% 259646
#% 278445
#% 333990
#% 334025
#% 442861
#% 458607
#% 464717
#% 478260
#% 480645
#% 529190
#% 654457
#% 766116
#! A model is a formal description of a complex application artifact, such as a database schema, an application interface, a UML model, an ontology, or a message format. The problem of merging such models lies at the core of many meta data applications, such as view integration, mediated schema creation for data integration, and ontology merging. This paper examines the problem of merging two models given correspondences between them. It presents requirements for conducting a merge and a specific algorithm that subsumes previous work.

#index 1015327
#* Locating data sources in large distributed systems
#@ Leonidas Galanis;Yuan Wang;Shawn R. Jeffery;David J. DeWitt
#t 2003
#c 4
#% 152946
#% 264263
#% 271951
#% 322884
#% 337046
#% 340175
#% 340176
#% 342374
#% 342375
#% 351979
#% 480488
#% 496155
#% 496291
#% 505869
#% 572300
#% 636008
#% 636009
#% 674136
#% 835744
#% 993970
#% 1388073
#! Querying large numbers of data sources is gaining importance due to increasing numbers of independent data providers. One of the key challenges is executing queries on all relevant information sources in a scalable fashion and retrieving fresh results. The key to scalability is to send queries only to the relevant servers and avoid wasting resources on data sources which will not provide any results. Thus, a catalog service, which would determine the relevant data sources given a query, is an essential component in efficiently processing queries in a distributed environment. This paper proposes a catalog framework which is distributed across the data sources themselves and does not require any central infrastructure. As new data sources become available, they automatically become part of the catalog service infrastructure, which allows scalability to large numbers of nodes. Furthermore, we propose techniques for workload adaptability. Using simulation and real-world data we show that our approach is valid and can scale to thousands of data sources.

#index 1015328
#* Robust estimation with sampling and approximate pre-aggregation
#@ Christopher Jermaine
#t 2003
#c 4
#% 1331
#% 138554
#% 227883
#% 248812
#% 273902
#% 273909
#% 300193
#% 300195
#% 333955
#% 333983
#% 397389
#% 465162
#% 479648
#% 480471
#% 482092
#% 482095
#% 482123
#! The majority of data reduction techniques for approximate query processing (such as wavelets, histograms, kernels, and so on) are not usually applicable to categorical data. There has been something of a disconnect between research in this area and the reality of data-base data; much recent research has focused on approximate query processing over ordered or numerical attributes, but arguably the majority of database attributes are categorical: country, state, job_title, color, sex, department, and so on. This paper considers the problem of approximation of aggregate functions over categorical data, or mixed categorical/numerical data. We propose a method based upon random sampling, called Approximate Pre-Aggregation (APA). The biggest drawback of sampling for aggregate function estimating is the sensitivity of sampling to attribute value skew, and APA uses several techniques to overcome this sensitivity. The increase in accuracy using APA compared to "plain vanilla" sampling is dramatic. For SUM and AVG queries, the relative error for random sampling alone is more than 700% greater than for sampling with APA. Even if stratified sampling techniques are used, the error is still between 28% and 175% greater than for APA.

#index 1015329
#* Controlling access to published data using cryptography
#@ Gerome Miklau;Dan Suciu
#t 2003
#c 4
#% 67453
#% 164560
#% 300237
#% 319849
#% 344639
#% 374401
#% 397367
#% 433922
#% 513068
#% 567234
#% 568960
#% 571217
#% 593711
#% 646047
#% 664705
#! We propose a framework for enforcing access control policies on published XML documents using cryptography. In this framework the owner publishes a single data instance, which is partially encrypted, and which enforces all access control policies. Our contributions include a declarative language for access policies, and the resolution of these policies into a logical "protection model" which protects an XML tree with keys. The data owner enforces an access control policy by granting keys to users. The model is quite powerful, allowing the data owner to describe complex access scenarios, and is also quite elegant, allowing logical optimizations to be described as rewriting rules. Finally, we describe cryptographic techniques for enforcing the protection model on published data, and provide a performance analysis using real datasets.

#index 1015330
#* OASIS: an online and accurate technique for local-alignment searches on biological sequences
#@ Colin Meek;Jignesh M. Patel;Shruti Kasetty
#t 2003
#c 4
#% 269546
#% 288578
#% 289010
#% 300312
#% 443469
#% 468476
#% 480482
#% 480484
#% 480656
#% 495434
#% 544212
#% 564599
#% 571046
#% 589381
#% 723700
#! A common query against large protein and gene sequence data sets is to locate targets that are similar to an input query sequence. The current set of popular search tools, such as BLAST, employ heuristics to improve the speed of such searches. However, such heuristics can sometimes miss targets, which in many cases is undesirable. The alternative to BLAST is to use an accurate algorithm, such as the Smith-Waterman (S-W) algorithm. However, these accurate algorithms are computationally very expensive, which limits their use in practice. This paper takes on the challenge of designing an accurate and efficient algorithm for evaluating local-alignment searches. To meet this goal, we propose a novel search algorithm, called OASIS. This algorithm employs a dynamic programming A*-search driven by a suffix-tree index that is built on the input data set. We experimentally evaluate OASIS and demonstrate that for an important class of searches, in which the query sequence lengths are small, OASIS is more than an order of magnitude faster than S-W. In addition, the speed of OASIS is comparable to BLAST. Furthermore, OASIS returns results in decreasing order of the matching score, making it possible to use OASIS in an online setting. Consequently, we believe that it may now be practically feasible to query large biological sequence data sets using an accurate local-alignment search algorithm.

#index 1015331
#* Privacy-preserving indexing of documents on the network
#@ Mayank Bawa;Roberto J. Bayardo, Jr.;Rakesh Agrawal
#t 2003
#c 4
#% 67453
#% 233480
#% 249181
#% 261357
#% 268079
#% 287463
#% 317991
#% 322884
#% 340175
#% 348157
#% 397367
#% 414382
#% 438481
#% 577361
#% 593711
#% 664654
#% 664705
#% 963875
#! We address the problem of providing privacy-preserving search over distributed access-controlled content. Indexed documents can be easily reconstructed from conventional (inverted) indexes used in search. The need to avoid breaches of access-control through the index requires the index hosting site to be fully secured and trusted by by all participating content providers. This level of trust is impractical in the increasingly common case where multiple competing organizations or individuals wish to selectively share content. We propose a solution that eliminates the need of such a trusted authority. The solution builds a centralized privacy-preserving index in conjunction with a distributed access-control enforcing search protocol. The new index provides strong and quantifiable privacy guarantees that hold even if the entire index is made public. Experiments on a real-life dataset validate performance of the scheme. The appeal of our solution is two-fold: (a) Content providers maintain complete control in defining access groups and ensuring its compliance, and (b) System implementors retain tunable knobs to balance privacy and efficiency concerns for their particular domains.

#index 1015332
#* Data compression in Oracle
#@ Meikel Poess;Dmitry Potapov
#t 2003
#c 4
#% 328431
#! The Oracle RDBMS recently introduced an innovative compression technique for reducing the size of relational tables. By using a compression algorithm specifically designed for relational data, Oracle is able to compress data much more effectively than standard compression techniques. More significantly, unlike other compression techniques, Oracle incurs virtually no performance penalty for SQL queries accessing compressed tables. In fact, Oracle's compression may provide performance gains for queries accessing large amounts of data, as well as for certain data management operations like backup and recovery. Oracle's compression algorithm is particularly well-suited for data warehouses: environments, which contains large volumes of historical data, with heavy query workloads. Compression can enable a data warehouse to store several times more raw data without increasing the total disk storage or impacting query performance.

#index 1015333
#* Improving performance with bulk-inserts in Oracle R-trees
#@ Ning An;Ravi Kanth;V. Kothuri;Siva Ravada
#t 2003
#c 4
#% 86950
#% 260072
#% 260073
#% 273941
#% 427199
#% 527174
#! Spatial indexes play a major role in fast access to spatial and location data. Most commercial applications insert new data in bulk: in batches or arrays. In this paper, we propose a novel bulk insertion technique for R-Trees that is fast and does not compromise on the quality of the resulting index. We present our experiences with incorporating the proposed bulk insertion strategies into Oracle 10i. Experiments with real datasets show that our bulk insertion strategy improves performance of insert operations by 50%-90%.

#index 1015334
#* Statistics on views
#@ César A. Galindo-Legaria;Milind M. Joshi;Florian Waas;Ming-Chuan Wu
#t 2003
#c 4
#% 140389
#% 198465
#% 333965
#% 397371
#% 404922
#% 482115
#% 565457
#! The quality of execution plans generated by a query optimizer is tied to the accuracy of its cardinality estimation. Errors in estimation lead to poor performance, erratic behavior, and user frustration. Traditionally, the optimizer is restricted to use only statistics on base table columns and derive estimates bottom-up. This approach has shortcomings with dealing with complex queries, and with rich languages such as SQL: Errors grow as estimation is done on top of estimation, and some constructs are simply not handled. In this paper we describe the creation and utilization of statistics on views in SQL Server, which provides the optimizer with statistical information on the result of scalar or relational expressions. It opens a new dimension on the data available for cardinality estimation and enables arbitrary correction. We describe the implementation of this feature in the optimizer architecture, and show its impact on the quality of plans generated through a number of examples.

#index 1015335
#* Efficient query processing for multi-dimensionally clustered tables in DB2
#@ Bishwaranjan Bhattacharjee;Sriram Padmanabhan;Timothy Malkemus;Tony Lai;Leslie Cranston;Matthew Huras
#t 2003
#c 4
#% 654495
#! We have introduced a Multi-Dimensional Clustering (MDC) physical layout scheme in DB2 version 8.0 for relational tables. Multi-Dimensional Clustering is based on the definition of one or more orthogonal clustering attributes (or expressions) of a table. The table is organized physically by associating records with similar values for the dimension attributes in a cluster. Each clustering key is allocated one or more blocks of physical storage with the aim of storing the multiple records belonging to the cluster in almost contiguous fashion. Block oriented indexes are created to access these blocks. In this paper, we describe novel techniques for query processing operations that provide significant performance improvements for MDC tables. Current database systems employ a repertoire of access methods including table scans, index scans, index ANDing, and index ORing. We have extended these access methods for efficiently processing the block based MDC tables. One important concept at the core of processing MDC tables is the block oriented access technique. In addition, since MDC tables can include regular record oriented indexes, we employ novel techniques to combine block and record indexes. Block oriented processing is extended to nested loop joins and star joins as well. We show results from experiments using a star-schema database to validate our claims of performance with minimal overhead.

#index 1015336
#* A platform based on the multi-dimensional data modal for analysis of bio-molecular structures
#@ Srinath Srinivasa;Sujit Kumar
#t 2003
#c 4
#% 443514
#% 469403
#% 722796
#% 772128
#% 993984
#! A platform called AnMol for supporting analytical applications over structural data of large biomolecules is described. The term "biomolecular structure" has various connotations and different representations. AnMol reduces these representations into graph structures. Each of these graphs are then stored as one or more vectors in a database. Vectors encapsulate structural features of these graphs. Structural queries like similarity and substructure are transformed into spatial constructs like distance and containment within regions. Query results are based on inexact matches. A refinement mechanism is supported for increasing accuracy of the results. Design and implementation issues of AnMol including schema structure and performance results are discussed in this paper.

#index 1015337
#* Capturing global transactions from multiple recovery log files in a partitioned database system
#@ Chengfei Liu;Bruce G. Lindsay;Serge Bourbonnais;Elizabeth B. Hamel;Tuong C. Truong;Jens Stankiewitz
#t 2003
#c 4
#% 9241
#% 210179
#% 248825
#% 273894
#% 320187
#% 323980
#% 462073
#% 479978
#% 480310
#% 636006
#! DB2 DataPropagator is one of the IBM's solutions for asynchronous replication of relational data by two separate programs Capture and Apply. The Capture program captures changes made to source data from recovery log files into staging tables, while the Apply program applies the changes from the staging tables to target data. Currently the Capture program only supports capturing changes made by local transactions in a single database log file. With the increasing deployment of partitioned database systems in OLTP environments there is a need to replicate the operational data from the partitioned systems. This paper introduces a system called CaptureEEE which extends the Capture program to capture global transactions executed on partitioned databases supported by DB2 Enterprise-Extended Edition. The architecture and the components of CaptureEEE are presented. The algorithm for merging log entries from multiple recovery log files is discussed in detail.

#index 1015338
#* The BEA/XQRL streaming XQuery processor
#@ Daniela Florescu;Chris Hillery;Donald Kossmann;Paul Lucas;Fabio Riccardi;Till Westmann;Michael J. Carey;Arvind Sundararajan;Geetika Agrawal
#t 2003
#c 4
#% 136740
#% 397607
#% 654476
#% 654477
#% 993950
#% 994015
#! In this paper, we describe the design, implementation, and performance characteristics of a complete, industrial-strength XQuery engine, the BEA streaming XQuery processor. The engine was designed to provide very high performance for message processing applications, i.e., for transforming XML data streams, and it is a central component of the 8.1 release of BEA's WebLogic Integration (WLI) product. This XQuery engine is fully compliant with the August 2002 draft of the W3C XML Query Language specification. A goal of this paper is to describe how an efficient, fully compliant XQuery engine can be built from a few relatively simple components and well-understood technologies.

#index 1015339
#* Xml schemas in Oracle XML DB
#@ Ravi Murthy;Sandeepan Banerjee
#t 2003
#c 4
#% 273937
#! The W3C XML Scheme language is becomimg increasingly popular for expressing the data model for XML documents. It is a powerful language that incorporates both strutural and datatype modeling features. There are many benefits to storing XML Schema compliant data in a database system, including better queryability, optimied updates and stronger validation. However, the fidelity of the XML document cannot be sacrificed. Thus, the fundamental problem facing database implementers is: how can XML Schemes be mapped to relational (and object-relational) database without losing schema semantics or data-fidelity? In this paper, we present the Oracle XML DB solution for a flexible mapping of XML Schemas to object-relational database. It preserves document fidelity, including ordering, namespaces, comments, processing instructions etc., and handles all the XML Schema semantics including cyclic definitions, dervations (extension and restriction), and wildcards. We also discuss various query and update optimiations that involve rewriting XPath operations to directly operate on the underlying relational data.

#index 1015340
#* Integrated data management for mobile services in the real world
#@ C. Hage;C. S. Jensen;T. B. Pedersen;L. Speicys;I. Timko
#t 2003
#c 4
#% 300174
#% 325914
#% 421119
#% 495433
#% 993955
#! Market research companies predict a huge market for services to be delivered to mobile users. Services include route guidance, point-of-interest search, metering services such as road pricing and parking payment, traffic monitoring, etc. We believe that no single such service will be the killer service, but that suites of integrated services are called for. Such integrated services reuse integrated content obtained from multiple content providers. This paper describes concepts and techniques underlying the data management system deployed by a Danish mobile content integrator. While georeferencing of content is important, it is even more important to relate content to the transportation infrastructure. The data management system thus relies on several sophisticated, integrated representations of the infrastructure, each of which supports its own kind of use. The paper covers data modeling, querying, and update, as well as the applications using the system.

#index 1015341
#* Web services (industrial session)
#@ L. F. Cabrera
#t 2003
#c 4
#! Web services have become a mainstream developmemt in the industry. Their emergence is based on the mechanisms that made Web Browsing popular: the existence of a pervasive networking infrastructure, the widely deployed availability of communication protocols such as IP, TCP, UDP and HTTP, the standardization of XML documents and their display by browsers, and the emergence of higher level transports such as SOAP. In additiom standard services such as UDDI and description languages such as WSDL established a base on which services could be specified, described, and published. Thus, the begning of the inter Web Service communication was founded.

#index 1015342
#* Grid and applications (industrial session)
#@ Frank Leymann
#t 2003
#c 4

#index 1015343
#* Commercial use of database technology
#@ Harald Schöning
#t 2003
#c 4
#! This session provides insight into two different European products for the E-commerce/database market. First, Martin Meijsen, Software AG, gives an overview on Tamino, Software AG's XML DBMS while the second presentation by Eva Kühn of TECCO AG, Austria discusses technical details of products that provides "zero-delay access to data warehouses",

#index 1015344
#* The zero-delay data warehouse: mobilizing heterogeneous database
#@ Eva Kühn
#t 2003
#c 4
#% 352382
#% 387588
#! "Now is the time... for the real-time enterprise": In spite of this assertion from Gartner Group the heterogeneity of today's IT environments and the increasing demands from mobile users are major obstacles for the creation of this vision. Yet its technical foundation is available: software architectures based on innovative middleware components that offer a level of abstraction superior to conventional middleware solutions, including distributed transactions and the seamless integration of mobile devices using open standards, crossing the borders between heterogeneous platforms and systems. Space based computing is a new middleware paradigm meeting these demands. As an example we present the real time build-up of data warehouses.

#index 1015345
#* A database striptease or how to manage your personal databases
#@ Martin Kersten;Gerhard Weikum;Michael Franklin;Daniel Keim;Alex Buchmann;Surajit Chaudhuri
#t 2003
#c 4

#index 1015346
#* Who needs XML database?
#@ Sophie Cluet
#t 2003
#c 4

#index 1015347
#* Illuminating the dark side of web services
#@ Michael L. Brodie
#t 2003
#c 4

#index 1015348
#* Xcerpt and visXcerpt: from pattern-based to visual querying of XML and semistructured data
#@ Sacha Berger;François Bry;Sebastian Schaffert;Christoph Wieser
#t 2003
#c 4
#% 64902
#% 281149
#% 291299
#% 342436
#% 464926
#% 528056
#% 571040
#% 620053

#index 1015349
#* OrientStore: a schema based native XML storage system
#@ Xiaofeng Meng;Daofeng Luo;Mong Li Lee;Jing An
#t 2003
#c 4
#% 236416
#% 345742
#% 465006
#% 632058
#% 994015

#index 1015350
#* Managing distributed workspaces with active XML
#@ Serge Abiteboul;Jérôme Baumgarten;Angela Bonifati;Grégory Cobéna;Cosmin Cremarenco;Florin Dragan;Ioana Manolescu;Tova Milo;Nicoleta Preda
#t 2003
#c 4
#% 432277
#% 654465
#% 654485
#% 994034

#index 1015351
#* XQueC: pushing queries to compressed XML data
#@ Andrei Arion;Angela Bonifati;Gianni Costa;Sandra D'Aguanno;Ioana Manolescu;Andrea Pugliese
#t 2003
#c 4
#% 300153
#% 322412
#% 333953
#% 479465
#% 571082
#% 659997
#% 994015

#index 1015352
#* A system for keyword proximity search on XML databases
#@ Andrey Balmin;Vagelis Hristidis;Nick Koudas;Yannis Papakonstantinou;Divesh Srivastava;Tianqiu Wang
#t 2003
#c 4
#% 333845
#% 393907
#% 458861
#% 479803
#% 993987

#index 1015353
#* XISS/R: XML indexing and storage system using RDBMS
#@ Philip J. Harding;Quanzhong Li;Bongki Moon
#t 2003
#c 4
#% 273922
#% 397366
#% 480489
#% 659924
#! We demonstrate the XISS/R system, an implementation of the XML Indexing and Storage System (XISS) on top of a relational database. The system is based on the XISS extended preorder numbering scheme, which captures the nesting structure of XML data and provides the opportunity for storage and query processing independent of the particular structure of the data. The system includes a web-based user interface, which enables stored documents to be queried via XPath. The user interface utilizes the XPath Query Engine, which automatically translates XPath queries into efficient SQL statements.

#index 1015354
#* Implementing XQuery 1.0: the Galax experience
#@ Mary Fernández;Jérôme Siméon;Byron Choi;Amélie Marian;Gargi Sur
#t 2003
#c 4
#% 428308
#% 565266
#! Galax is a light-weight, portable, open-source implementation of XQuery 1.0. Started in December 2000 as a small prototype designed to test the XQuery static type system, Galax has now become a solid implementation, aiming at full conformance with the family of XQuery 1.0 specifications. Because of its completeness and open architecture, Galax also turns out to be a very convenient platform for researchers interested in experimenting with XQuery optimization. We demonstrate the Galax system as well as its most advanced features, including support for XPath 2.0, XML Schema and static type-checking. We also present some of our first experiments with optimization. Notably, we demonstrate query rewriting capabilities in the Galax compiler, and the ability to run queries on documents up to a Gigabyte without the need for preindexing. Although early versions of Galax have been shown in industrial conferences over the last two years, this is the first time it is demonstrated in the database community.

#index 1015355
#* Web service composition with O'GRAPE and OSIRIS
#@ Roger Weber;Christoph Schuler;Patrick Neukomm;Heiko Schuldt;Hans-J. Schek
#t 2003
#c 4
#% 345694
#% 485141
#% 528213
#% 665528
#% 1304915

#index 1015356
#* Chameleon: an extensible and customizable tool for web data translation
#@ Riccardo Torlone;Paolo Atzeni
#t 2003
#c 4
#% 291299
#% 322415
#% 328429
#% 458995
#% 665627
#! Chameleon is a tool for the management of Web data according to different formats and models and for the automatic transformation of schemas and instances from one model to another. It handles semistructured data, schema languages for XML, and traditional database models. The system is based on a "metamodel" approach, in the sense that it knows a set of metaconstructs,and allows the definition of models by means of the involved metaconstructs. The system also has a library of basic translations, referring to the known metaconstructs, and builds actual translations by means of suitable combinations of the basic ones. The main functions offered to the user are: (i) definition of a model; (ii)definition and validation of a schema with respect to a given model; (iii)schema translation (from a model to another).

#index 1015357
#* NexusScout: an advanced location-based application on a distributed, open mediation platform
#@ Daniela Nicklas;Matthias Grossmann;Thomas Schwarz
#t 2003
#c 4
#% 281550
#% 516388
#% 527199
#% 559123
#% 640097
#! This demo shows several advanced use cases of location-based services and demonstrates how these use cases are facilitated by a mediation middleware for spatial information, the Nexus Platform. The scenario shows how a mobile user can access location-based information via so called Virtual Information Towers, register spatial events, send and receive geographical messages or find her friends by displaying other mobile users. The platform facilitates these functions by transparently combining spatial data from a dynamically changing set of data providers, tracking mobile objects and observing registered spatial events.

#index 1015358
#* Schema-driven customization of web services
#@ S. Abiteboul;B. Amann;J. Baumgarten;O. Benjelloun;F. Dang Ngoc;T. Milo
#t 2003
#c 4
#% 333990
#% 464706
#% 654465
#% 654485
#% 994034

#index 1015359
#* BibFinder/StatMiner: effectively mining and using coverage and overlap statistics in data integration
#@ Zaiqing Nie;Subbarao Kambhampati;Thomas Hernandez
#t 2003
#c 4
#% 342684
#% 479813
#% 482108
#% 659968
#! Recent work in data integration has shown the importance of statistical information about the coverage and overlap of sources for efficient query processing. Despite this recognition there are no effective approaches for learning the needed statistics. In this paper we present StatMiner, a system for estimating the coverage and overlap statistics while keeping the needed statistics tightly under control. StatMiner uses a hierarchical classification of the queries, and threshold based variants of familiar data mining techniques to dynamically decide the level of resolution at which to learn the statistics. We will demonstrate the major functionalities of StatMiner and the effectiveness of the learned statistics in BibFinder, a publicly available computer science bibliography mediator we developed. The sources that BibFinder integrates are autonomous and can have uncontrolled coverage and overlap. An important focus in BibFinder was thus to mine coverage and overlap statistics about these sources and to exploit them to improve query processing.

#index 1015360
#* S-ToPSS: semantic Toronto publish/subscribe system
#@ Milenko Petrovic;Ioana Burcea;Hans-Arno Jacobsen
#t 2003
#c 4
#% 111922
#% 158908
#% 271199
#% 309678
#% 333938
#% 519428

#index 1015361
#* From focused crawling to expert information: an application framework for web exploration and portal generation
#@ Sergej Sizov;Jens Graupmann;Martin Theobald
#t 2003
#c 4
#% 198058
#% 279755
#% 290830
#% 309141
#% 333932
#% 387427
#% 402289
#% 420077
#% 445448
#% 479807
#% 480479
#% 482655
#% 578668

#index 1015362
#* CachePortal II: acceleration of very large scale data center-hosted database-driven web applications
#@ Wen-Syan Li;Oliver Po;Wang-Pin Hsiung;K. Selçuk Candan;Divyakant Agrawal;Yusuf Akca;Kunihiro Taniguchi
#t 2003
#c 4
#% 333995
#% 480494
#% 577362
#% 993978
#% 994021

#index 1015363
#* ATLAS: a small but complete SQL extension for data mining and data streams
#@ Haixun Wang;Carlo Zaniolo;Chang Richard Luo
#t 2003
#c 4
#% 227883
#% 378388
#% 480144

#index 1015364
#* Business modeling using SQL spreadsheets
#@ Andrew Witkowski;Srikanth Bellamkonda;Tolga Bozkaya;Nathan Folkert;Abhinav Gupta;Lei Sheng;Sankar Subramanian
#t 2003
#c 4
#% 654445
#! One of the critical deficiencies of SQL is the lack of support for array and spreadsheet like calculations which are frequent in OLAP and Business Modeling applications. Applications relying on SQL have to emulate these calculations using joins, UNION operations, Window Functions and complex CASE expressions. The designated place in SQL for algebraic calculations is the SELECT clause, which is extremely limiting and forces applications to generate queries with nested views, subqueries and complex joins. This distributes Business Modeling computations across many query blocks, making applications coded in SQL hard to develop. The limitations of RDBMS have been filled by spreadsheets and specialized MOLAP engines which are good at formulas for mathematical modeling but lack the formalism of the relational model, are difficult to manage, and exhibit scalability problems. This demo presents a scalable, mathematically rigorous, and performant SQL extensions for Relational Business Modeling, called the SQL Spreadsheet. We present examples of typical Business Modeling computations with SQL spreadsheet and compare them with the ones using standard SQL showing performance advantages and ease of programming for the former. We will show a scalability example where data is processed in parallel and will present a new class of query optimizations applicable to SQL spreadsheet.

#index 1015365
#* An interpolated volume data model
#@ Tianqiu Wang;Simone Santini;Amarnath Gupta
#t 2003
#c 4
#% 479459
#% 480459

#index 1015366
#* Efficacious data cube exploration by semantic summarization and compression
#@ Laks V. S. Lakshmanan;Jian Pei;Yan Zhao
#t 2003
#c 4
#% 207552
#% 654446
#% 993996
#! Data cube is the core operator in data warehousing and OLAP. Its efficient computation, maintenance, and utilization for query answering and advanced analysis have been the subjects of numerous studies. However, for many applications, the huge size of the data cube limits its applicability as a means for semantic exploration by the user. Recently, we have developed a systematic approach to achieve efficacious data cube construction and exploration by semantic summarization and compression. Our approach is pivoted on a notion of quotient cube that groups together structurally related data cube cells with common (aggregate) measure values into equivalence classes. The equivalence relation used to partition the cube lattice preserves the roll-up/drill-down semantics of the data cube, in that the same kind of explorations can be conducted in the quotient cube as in the original cube, between classes instead of between cells. We have also developed compact data structures for representing a quotient cube and efficient algorithms for answering queries using a quotient cube for its incremental maintenance against updates. We have implemented SOCQET, a prototype data warehousing system making use of our results on quotient cube. In this demo, we will demonstrate (1) the critical techniques of building a quotient cube; (2) use of a quotient cube to answer various queries and to support advanced OLAP; (3) an empirical study on the effectiveness and efficiency of quotient cube-based data warehouses and OLAP; (4) a user interface for visual and interactive OLAP; and (5) SOCQET, a research prototype data warehousing system integrating all the techniques. The demo reflects our latest research results and may stimulate some interesting future studies.

#index 1015367
#* QUIET: continuous query-driven index tuning
#@ Kai-Uwe Sattler;Ingolf Geist;Eike Schallehn
#t 2003
#c 4
#% 152943
#% 482100
#% 566126
#% 632100
#! Index tuning as part of database tuning is the task of selecting and creating indexes with the goal of reducing query processing times. However, in dynamic environments with various ad-hoc queries it is difficult to identify potential useful indexes in advance. In this demonstration, we present our tool QUIET addressing this problem. This tool "intercepts" queries and - based on a cost model as well as runtime statistics about profits of index configurations - decides about index creation automatically at runtime. In this way, index tuning is driven by queries without explicit actions of the database users.

#index 1015368
#* Chip-secured data access: reconciling access rights with data encryption
#@ Luc Bouganim;François Dang Ngoc;Philippe Pucheral;Lilan Wu
#t 2003
#c 4
#% 397367
#% 440254
#% 572304
#% 661973
#% 978215
#% 993942
#% 993943

#index 1015369
#* IrisNet: an architecture for internet-scale sensing services
#@ Suman Nath;Amol Deshpande;Yan Ke;Phillip B. Gibbons;Brad Karp;Srinivasan Seshan
#t 2003
#c 4
#% 281556
#% 281557
#% 654482
#% 654483
#% 805466
#! We demonstrate the design and an early prototype of IrisNet (Internet-scale Resource-Intensive Sensor Network services), a common, scalable networked infrastructure for deploying wide area sensing services. IrisNet is a potentially global network of smart sensing nodes, with webcams or other monitoring devices, and organizing nodes that provide the means to query recent and historical sensor-based data. IrisNet exploits the fact that high-volume sensor feeds are typically attached to devices with significant computing power and storage, and running a standard operating system. It uses aggressive filtering, smart query routing, and semantic caching to dramatically reduce network bandwidth utilization and improve query response times, as we demonstrate. Our demo will present two services built on Iris-Net, from two very different application domains. The first one, a parking space finder, utilizes webcams that monitor parking spaces to answer queries such as the availability of parking spaces near a user's destination. The second one, a distributed infrastructure monitor, uses measurement tools installed in individual nodes of a large distributed infrastructure to answer queries such as average network bandwidth usage of a set of nodes.

#index 1015370
#* Large-scale, standards-based earth observation imagery and web mapping services
#@ Peter Baumann
#t 2003
#c 4
#% 210184
#% 248863
#% 463760
#% 479459
#% 563948
#% 564097
#% 631967
#! Earth observation (EO) and simulation data share some core characteristics: they resemble raster data of some spatio-temporal dimensionality; the complete objects are extremely large, well into Tera- and Petabyte volumes; data generation and retrieval follow very different access patterns. EO time series additionally share that acquisition/generation happens in time slices. The central standardization body for geo service interfaces is the Open GIS Consortium (OGC). Earlier OGC has issued the Web Map Service (WMS) Interface Specification which addresses 2-D (raster and vector) maps. This year, the Web Coverage Service (WCS) Specification has been added with specific focus on 2-D and 3-D rasters ("coverages"). In this paper we present operational applications offering WMS/WCS services: a 2-D ortho photo maintained by the Bavarian Mapping Agency and a 3-D satellite time series deployed by the German Aerospace Association. All are based on the rasdaman array middleware which extends relational DBMSs with storage and retrieval capabilities for extremely large multidimensional arrays.

#index 1015371
#* Privacy-enhanced data management for next-generation e-commerce
#@ Chris Clifton;Irini Fundulaki;Richard Hull;Bharat Kumar;Daniel Lieuwen;Arnaud Sahuguet
#t 2003
#c 4

#index 1015372
#* The semantic web: semantics for data on the web
#@ Stefan Decker;Vipul Kashyap
#t 2003
#c 4
#! In our tutorial on Semantic Web (SW) technology, we explain the why, the various technology thrusts and the relationship to database technology. The motivation behind presenting this tutorial is discussed and the framework of the tutorial along with the various component technologies and research areas related to the Semantic Web is presented.

#index 1015373
#* Data stream query processing: a tutorial
#@ Nick Koudas;Divesh Srivastava
#t 2003
#c 4

#index 1015374
#* Grid data management systems & services
#@ Arun Jagatheesan;Reagan Moore;Norman W. Paton;Paul Watson
#t 2003
#c 4
#! The Grid is an emerging infrastructure for providing coordinated and consistent access to distributed, heterogeneous computational and information storage resources amongst autonomous organizations. Data grids are being built across the world as the next generation data handling systems for sharing access to data and storage systems within multiple administrative domains. A data grid provides logical name spaces for digital entities and storage resources to create global identifiers that are location independent. Data grid systems provide services on the logical name space for the manipulation, management, and organization of digital entities. Databases are increasingly being used within Grid applications for data and metadata management, and several groups are now developing services for the access and integration of structured data on the Grid. The service-based approach to making data available on the Grid is being encouraged by the adoption of the Open Grid Services Architecture (OGSA), which is bringing about the integration of the Grid with Web Service technologies. The tutorial will introduce the Grid, and examine the requirements, issues and possible solutions for integrating data into the Grid. It will take examples from current systems, in particular the SDSC Storage Resource Broker and the OGSA-Database Access and Integration project.

#index 1015375
#* Constructing and integrating data-centric web applications: methods, tools, and techniques
#@ Stefano Ceri;Ioana Manolescu
#t 2003
#c 4
#% 425200
#! This tutorial deals with the construction of data-centric Web applications, focusing on the modelling of processes and on the integration with Web services. The tutorial describes the standards, methods, and tools that are commonly used for building these applications.

#index 1016126
#* Proceedings of the Thirtieth international conference on Very large data bases - Volume 30
#@ Mario A. Nascimento;M. Tamer Özsu;Donald Kossmann;Renée J. Miller;José A. Blakeley;K. Bernhard Schiefer
#t 2004
#c 4

#index 1016127
#* Databases in a wireless world
#@ David Yach
#t 2004
#c 4
#! The traditional view of distributed databases is based on a number of database servers with regular communication. Today information is stored not only in these central databases, but on a myriad of computers and computer-based devices in addition to the central storage. These range from desktop and laptop computers to PDA's and wireless devices such as cellular phones and BlackBerry's. The combination of large centralized databases with a large number and variety of associated edge databases effectively form a large distributed database, but one where many of the traditional rules and assumptions for distributed databases are no longer true. This keynote will discuss some of the new and challenging attributes of this new environment, particularly focusing on the challenges of wireless and occasionally connected devices. It will look at the new constraints, how these impact the traditional distributed database model, the techniques and heuristics being used to work within these constraints, and identify the potential areas where future research might help tackle these difficult issues.

#index 1016128
#* Structures, semantics and statistics
#@ Alon Y. Halevy
#t 2004
#c 4
#% 279755
#% 333990
#% 378409
#% 464717
#% 572311
#% 572314
#% 654457
#% 654459
#% 1016160
#! At a fundamental level, the key challenge in data integration is to reconcile the semantics of disparate data sets, each expressed with a different database structure. I argue that computing statistics over a large number of structures offers a powerful methodology for producing semantic mappings, the expressions that specify such reconciliation. In essence, the statistics offer hints about the semantics of the symbols in the structures, thereby enabling the detection of semantically similar concepts. The same methodology can be applied to several other data management tasks that involve search in a space of complex structures and in enabling the next-generation on-the-fly data integration systems.

#index 1016129
#* Whither data mining?
#@ Rakesh Agrawal;Ramakrishnan Srikant
#t 2004
#c 4
#! The last decade has witnessed tremendous advances in data mining. We take a retrospective look at these developments, focusing on association rules discovery, and discuss the challenges and opportunities ahead.

#index 1016130
#* Compressing large boolean matrices using reordering techniques
#@ David Johnson;Shankar Krishnan;Jatin Chhugani;Subodh Kumar;Suresh Venkatasubramanian
#t 2004
#c 4
#% 118767
#% 131750
#% 173282
#% 210173
#% 249322
#% 270535
#% 281769
#% 302725
#% 443393
#% 479808
#% 480830
#% 578388
#% 723895
#% 726681
#% 729922
#! Large boolean matrices are a basic representational unit in a variety of applications, with some notable examples being interactive visualization systems, mining large graph structures, and association rule mining. Designing space and time efficient scalable storage and query mechanisms for such large matrices is a challenging problem. We present a lossless compression strategy to store and access such large matrices efficiently on disk. Our approach is based on viewing the columns of the matrix as points in a very high dimensional Hamming space, and then formulating an appropriate optimization problem that reduces to solving an instance of the Traveling Salesman Problem on this space. Finding good solutions to large TSP's in high dimensional Hamming spaces is itself a challenging and little-explored problem -- we cannot readily exploit geometry to avoid the need to examine all N2 inter-city distances and instances can be too large for standard TSP codes to run in main memory. Our multi-faceted approach adapts classical TSP heuristics by means of instance-partitioning and sampling, and may be of independent interest. For instances derived from interactive visualization and telephone call data we obtain significant improvement in access time over standard techniques, and for the visualization application we also make significant improvements in compression.

#index 1016131
#* On the performance of bitmap indices for high cardinality attributes
#@ Kesheng Wu;Ekow Otoo;Arie Shoshani
#t 2004
#c 4
#% 227861
#% 248814
#% 273904
#% 342735
#% 462217
#% 466953
#% 479808
#% 480329
#% 504155
#% 564096
#% 571294
#% 617842
#% 853010
#! It is well established that bitmap indices are efficient for read-only attributes with low attribute cardinalities. For an attribute with a high cardinality, the size of the bitmap index can be very large. To overcome this size problem, specialized compression schemes are used. Even though there are empirical evidences that some of these compression schemes work well, there has not been any systematic analysis of their effectiveness. In this paper, we systematically analyze the two most efficient bitmap compression techniques, the Byte-aligned Bitmap Code (BBC) and the Word-Aligned Hybrid (WAH) code. Our analyses show that both compression schemes can be optimal. We propose a novel strategy to select the appropriate algorithms so that this optimality is achieved in practice. In addition, our analyses and tests show that the compressed indices are relatively small compared with commonly used indices such as B-trees. Given these facts, we conclude that bitmap index is efficient on attributes of low cardinalities as well as on those of high cardinalities.

#index 1016132
#* Practical suffix tree construction
#@ Sandeep Tata;Richard A. Hankins;Jignesh M. Patel
#t 2004
#c 4
#% 194111
#% 235941
#% 289010
#% 300312
#% 317163
#% 352402
#% 480484
#% 488080
#% 587727
#% 737254
#% 745501
#% 745510
#% 1015330
#! Large string datasets are common in a number of emerging text and biological database applications. Common queries over such datasets include both exact and approximate string matches. These queries can be evaluated very efficiently by using a suffix tree index on the string dataset. Although suffix trees can be constructed quickly in memory for small input datasets, constructing persistent trees for large datasets has been challenging. In this paper, we explore suffix tree construction algorithms over a wide spectrum of data sources and sizes. First, we show that on modern processors, a cache-efficient algorithm with O(n2) complexity outperforms the popular O(n) Ukkonen algorithm, even for in-memory construction. For larger datasets, the disk I/O requirement quickly becomes the bottleneck in each algorithm's performance. To address this problem, we present a buffer management strategy for the O(n2) algorithm, creating a new disk-based construction algorithm that scales to sizes much larger than have been previously described in the literature. Our approach far outperforms the best known disk-based construction algorithms.

#index 1016133
#* Answering xpath queries over networks by sending minimal views
#@ Keishi Tajima;Yoshiki Fukui
#t 2004
#c 4
#% 38688
#% 300179
#% 333989
#% 378393
#% 397358
#% 443298
#% 465065
#% 480296
#% 481916
#% 487257
#% 562145
#% 572311
#% 576095
#% 576102
#% 632039
#% 654476
#% 654477
#% 659995
#% 993950
#% 994015
#! When a client submits a set of XPath queries to a XML database on a network, the set of answer sets sent back by the database may include redundancy in two ways: some elements may appear in more than one answer set, and some elements in some answer sets may be subelements of other elements in other (or the same) answer sets. Even when a client submits a single query, the answer can be self-redundant because some elements may be subelements of other elements in that answer. Therefore, sending those answers as they are is not optimal with respect to communication costs. In this paper, we propose a method of minimizing communication costs in XPath processing over networks. Given a single or a set of queries, we compute a minimal-size view set that can answer all the original queries. The database sends this view set to the client, and the client produces answers from it. We show algorithms for computing such a minimal view set for given queries. This view set is optimal; it only includes elements that appear in some of the final answers, and each element appears only once.

#index 1016134
#* A framework for using materialized XPath views in XML query processing
#@ Andrey Balmin;Fatma Özcan;Kevin S. Beyer;Roberta J. Cochrane;Hamid Pirahesh
#t 2004
#c 4
#% 198465
#% 300138
#% 333965
#% 378393
#% 397360
#% 462062
#% 464056
#% 479465
#% 480489
#% 480656
#% 487257
#% 564264
#! XML languages, such as XQuery, XSLT and SQL/XML, employ XPath as the search and extraction language. XPath expressions often define complicated navigation, resulting in expensive query processing, especially when executed over large collections of documents. In this paper, we propose a framework for exploiting materialized XPath views to expedite processing of XML queries. We explore a class of materialized XPath views, which may contain XML fragments, typed data values, full paths, node references or any combination thereof. We develop an XPath matching algorithm to determine when such views can be used to answer a user query containing XPath expressions. We use the match information to identify the portion of an XPath expression in the user query which is not covered by the XPath view. Finally, we construct, possibly multiple, compensation expressions which need to be applied to the view to produce the query result. Experimental evaluation, using our prototype implementation, shows that the matching algorithm is very efficient and usually accounts for a small fraction of the total query compilation time.

#index 1016135
#* Schema-free XQuery
#@ Yunyao Li;Cong Yu;H. V. Jagadish
#t 2004
#c 4
#% 27049
#% 309726
#% 397375
#% 458829
#% 459260
#% 465155
#% 570875
#% 654441
#% 654442
#% 659990
#% 770338
#% 993953
#% 993987
#% 994033
#% 1015258
#! The widespread adoption of XML holds out the promise that document structure can be exploited to specify precise database queries. However, the user may have only a limited knowledge of the XML structure, and hence may be unable to produce a correct XQuery, especially in the context of a heterogeneous information collection. The default is to use keyword-based search and we are all too familiar with how difficult it is to obtain precise answers by these means. We seek to address these problems by introducing the notion of Meaningful Lowest Common Ancestor Structure (MLCAS) for finding related nodes within an XML document. By automatically computing MLCAS and expanding ambiguous tag names, we add new functionality to XQuery and enable users to take full advantage of XQuery in querying XML data precisely and efficiently without requiring (perfect) knowledge of the document structure. Such a Schema-Free XQuery is potentially of value not just to casual users with partial knowledge of schema, but also to experts working in a data integration or data evolution context. In such a context, a schema-free query, once written, can be applied universally to multiple data sources that supply similar content under different schemas, and applied "forever" as these schemas evolve. Our experimental evaluation found that it was possible to express a wide variety of queries in a schema-free manner and have them return correct results over a broad diversity of schemas. Furthermore, the evaluation of a schema-free query is not expensive using a novel stack-based algorithm we develop for computing MLCAS: from 1 to 4 times the execution time of an equivalent schema-aware query.

#index 1016136
#* Client-based access control management for XML documents
#@ Luc Bouganim;François Dang Ngoc;Philippe Pucheral
#t 2004
#c 4
#% 308083
#% 314755
#% 318404
#% 333989
#% 344639
#% 345973
#% 378393
#% 397367
#% 424307
#% 433922
#% 513367
#% 613813
#% 646047
#% 654477
#% 903341
#% 993942
#% 993972
#% 994006
#% 1015266
#% 1015329
#! The erosion of trust put in traditional database servers and in Database Service Providers, the growing interest for different forms of data dissemination and the concern for protecting children from suspicious Internet content are different factors that lead to move the access control from servers to clients. Several encryption schemes can be used to serve this purpose but all suffer from a static way of sharing data. With the emergence of hardware and software security elements on client devices, more dynamic client-based access control schemes can be devised. This paper proposes an efficient client-based evaluator of access control rules for regulating access to XML documents. This evaluator takes benefit from a dedicated index to quickly converge towards the authorized parts of a - potentially streaming - document. Additional security mecanisms guarantee that prohibited data can never be disclosed during the processing and that the input document is protected from any form of tampering. Experiments on synthetic and real datasets demonstrate the effectiveness of the approach.

#index 1016137
#* Secure XML publishing without information leakage in the presence of data inference
#@ Xiaochun Yang;Chen Li
#t 2004
#c 4
#% 241
#% 25470
#% 273692
#% 309716
#% 333989
#% 340827
#% 344639
#% 397367
#% 443120
#% 443382
#% 490495
#% 552137
#% 646047
#% 745436
#% 765449
#% 765450
#% 993971
#% 1015329
#% 1016189
#! Recent applications are seeing an increasing need that publishing XML documents should meet precise security requirements. In this paper, we consider data-publishing applications where the publisher specifies what information is sensitive and should be protected. We show that if a partial document is published carelessly, users can use common knowledge (e.g., "all patients in the same ward have the same disease") to infer more data, which can cause leakage of sensitive information. The goal is to protect such information in the presence of data inference with common knowledge. We consider common knowledge represented as semantic XML constraints. We formulate the process how users can infer data using three types of common XML constraints. Interestingly, no matter what sequences users follow to infer data, there is a unique, maximal document that contains all possible inferred documents. We develop algorithms for finding a partial document of a given XML document without causing information leakage, while allowing publishing as much data as possible. Our experiments on real data sets show that effect of inference on data security, and how the proposed techniques can prevent such leakage from happening.

#index 1016138
#* Limiting disclosure in hippocratic databases
#@ Kristen LeFevre;Rakesh Agrawal;Vuk Ercegovac;Raghu Ramakrishnan;Yirong Xu;David DeWitt
#t 2004
#c 4
#% 67453
#% 69537
#% 151161
#% 164560
#% 204453
#% 238413
#% 252481
#% 346901
#% 606353
#% 993943
#! We present a practical and efficient approach to incorporating privacy policy enforcement into an existing application and database environment, and we explore some of the semantic tradeoffs introduced by enforcing these privacy policy rules at cell-level granularity. Through a comprehensive set of performance experiments, we show that the cost of privacy enforcement is small, and scalable to large databases.

#index 1016139
#* On testing satisfiability of tree pattern queries
#@ Laks V. S. Lakshmanan;Ganesh Ramesh;Hui Wang;Zheng Zhao
#t 2004
#c 4
#% 137871
#% 299944
#% 333989
#% 368248
#% 378393
#% 397366
#% 465051
#% 479956
#% 504368
#% 564264
#% 564339
#% 570875
#% 576108
#% 598376
#% 748604
#% 748803
#! XPath and XQuery (which includes XPath as a sublanguage) are the major query languages for XML. An important issue arising in efficient evaluation of queries expressed in these languages is satisfiability, i.e., whether there exists a database, consistent with the schema if one is available, on which the query has a non-empty answer. Our experience shows satisfiability check can effect substantial savings in query evaluation. We systematically study satisfiability of tree pattern queries (which capture a useful fragment of XPath) together with additional constraints, with or without a schema. We identify cases in which this problem can be solved in polynomial time and develop novel efficient algorithms for this purpose. We also show that in several cases, the problem is NP-complete. We ran a comprehensive set of experiments to verify the utility of satisfiability check as a preprocessing step in query processing. Our results show that this check takes a negligible fraction of the time needed for processing the query while often yielding substantial savings.

#index 1016140
#* Containment of nested XML queries
#@ Xin Dong;Alon Y. Halevy;Igor Tatarinov
#t 2004
#c 4
#% 36181
#% 109995
#% 122396
#% 123118
#% 137865
#% 137867
#% 140410
#% 164364
#% 190638
#% 210214
#% 237181
#% 248025
#% 283052
#% 289266
#% 309851
#% 333989
#% 378393
#% 378409
#% 464717
#% 464724
#% 481128
#% 495632
#% 572311
#% 599549
#% 654468
#% 765446
#% 1499552
#! Query containment is the most fundamental relationship between a pair of database queries: a query Q is said to be contained in a query Q′ if the answer for Q is always a subset of the answer for Q′, independent of the current state of the database. Query containment is an important problem in a wide variety of data management applications, including verification of integrity constraints, reasoning about contents of data sources in data integration, semantic caching, verification of knowledge bases, determining queries independent of updates, and most recently, in query reformulation for peer data management systems. Query containment has been studied extensively in the relational context and for XPath queries, but not for XML queries with nesting. We consider the theoretical aspects of the problem of query containment for XML queries with nesting. We begin by considering conjunctive XML queries (c-XQueries), and show that containment is in polynomial time if we restrict the fanout (number of sibling sub-blocks) to be 1. We prove that for arbitrary fanout, containment is coNP-hard already for queries with nesting depth 2, even if the query does not include variables in the return clauses. We then show that for queries with fixed nesting depth, containment is coNP-complete. Next, we establish the computational complexity of query containment for several practical extensions of c-XQueries, including queries with union and arithmetic comparisons, and queries where the XPath expressions may include descendant edges and negation. Finally, we describe a few heuristics for speeding up query containment checking in practice by exploiting properties of the queries and the underlying schema.

#index 1016141
#* Efficient XML-to-SQL query translation: where to add the intelligence?
#@ Rajasekar Krishnamurthy;Raghav Kaushik;Jeffrey F. Naughton
#t 2004
#c 4
#% 583
#% 32891
#% 137867
#% 190638
#% 289266
#% 333935
#% 333989
#% 348183
#% 397374
#% 416034
#% 443173
#% 480152
#% 480657
#% 480822
#% 564416
#% 599549
#% 654484
#% 1015271
#! We consider the efficiency of queries generated by XML to SQL translation. We first show that published XML-to-SQL query translation algorithms are suboptimal in that they often translate simple path expressions into complex SQL queries even when much simpler equivalent SQL queries exist. There are two logical ways to deal with this problem. One could generate suboptimal SQL queries using a fairly naive translation algorithm, and then attempt to optimize the resulting SQL; or one could use a more intelligent translation algorithm with the hopes of generating efficient SQL directly. We show that optimizing the SQL after it is generated is problematic, becoming intractable even in simple scenarios; by contrast, designing a translation algorithm that exploits information readily available at translation time is a promising alternative. To support this claim, we present a translation algorithm that exploits translation time information to generate efficient SQL for path expression queries over tree schemas.

#index 1016142
#* Taming XPath queries by minimizing wildcard steps
#@ Chee-Yong Chan;Wenfei Fan;Yiming Zeng
#t 2004
#c 4
#% 333989
#% 397374
#% 397375
#% 465065
#% 482653
#% 487257
#% 654450
#% 659999
#% 745461
#% 765450
#% 993939
#% 1015277
#! This paper presents a novel and complementary technique to optimize an XPath query by minimizing its wildcard steps. Our approach is based on using a general composite axis called the layer axis, to rewrite a sequence of XPath steps (all of which are wildcard steps except for possibly the last) into a single layer-axis step. We describe an efficient implementation of the layer axis and present a novel and efficient rewriting algorithm to minimize both non-branching as well as branching wildcard steps in XPath queries. We also demonstrate the usefulness of wildcard-step elimination by proposing an optimized evaluation strategy for wildcard-free XPath queries that enables selective loading of only the relevant input XML data for query evaluation. Our experimental results not only validate the scalability and efficiency of our optimized evaluation strategy, but also demonstrate the effectiveness of our rewriting algorithm for minimizing wildcard steps in XPath queries. To the best of our knowledge, this is the first effort that addresses this new optimization problem.

#index 1016143
#* The NEXT framework for logical XQuery optimization
#@ Alin Deutsch;Yannis Papakonstantinou;Yu Xu
#t 2004
#c 4
#% 32878
#% 116043
#% 237181
#% 248014
#% 287005
#% 333989
#% 346828
#% 368248
#% 378393
#% 384978
#% 395735
#% 397365
#% 397374
#% 480317
#% 480822
#% 481293
#% 487267
#% 562135
#% 562456
#% 570875
#% 599549
#% 765446
#% 1015267
#% 1015274
#% 1016143
#! Classical logical optimization techniques rely on a logical semantics of the query language. The adaptation of these techniques to XQuery is precluded by its definition as a functional language with operational semantics. We introduce Nested XML Tableaux which enable a logical foundation for XQuery semantics and provide the logical plan optimization framework of our XQuery processor. As a proof of concept, we develop and evaluate a minimization algorithm for removing redundant navigation within and across nested subqueries. The rich XQuery features create key challenges that fundamentally extend the prior work on the problems of minimizing conjunctive and tree pattern queries.

#index 1016144
#* Detecting change in data streams
#@ Daniel Kifer;Shai Ben-David;Johannes Gehrke
#t 2004
#c 4
#% 70050
#% 157946
#% 204531
#% 227859
#% 342600
#% 345857
#% 378388
#% 443392
#% 462212
#% 479785
#% 577220
#% 593972
#% 1015261
#! Detecting changes in a data stream is an important area of research with many applications. In this paper, we present a novel method for the detection and estimation of change. In addition to providing statistical guarantees on the reliability of detected changes, our method also provides meaningful descriptions and quantification of these changes. Our approach assumes that the points in the stream are independently generated, but otherwise makes no assumptions on the nature of the generating distribution. Thus our techniques work for both continuous and discrete data. In an experimental study we demonstrate the power of our techniques.

#index 1016145
#* Stochastic consistency, and scalable pull-based caching for erratic data stream sources
#@ Shanzhong Zhu;Chinya V. Ravishankar
#t 2004
#c 4
#% 15245
#% 44983
#% 63223
#% 72600
#% 77005
#% 102804
#% 124019
#% 150431
#% 227885
#% 300139
#% 330581
#% 330682
#% 378388
#% 464847
#% 480136
#% 481777
#% 566135
#% 615595
#% 632053
#% 646360
#% 660004
#% 805478
#% 993961
#% 993975
#! We introduce the notion of stochastic consistency, and propose a novel approach to achieving it for caches of highly erratic data. Erratic data sources, such as stock prices, sensor data, are common and important in practice. However, their erratic patterns of change make caching hard. Stochastic consistency guarantees that errors in cached values of erratic data remain within a user-specified bound, with a user-specified probability. We use a Brownian motion model to capture the behavior of data changes, and use its underlying theory to predict when caches should initiate pulls to refresh cached copies to maintain stochastic consistency. Our approach allows servers to remain totally stateless, thus achieving excellent scalability and reliability. We also discuss a new real-time scheduling approach for servicing pull requests at the server. Our scheduler delivers prompt response whenever possible, and minimizes the aggregate cache-source deviation due to delays during server overload. We conduct extensive experiments to validate our model on real-life datasets, and show that our scheme outperforms current schemes.

#index 1016146
#* False positive or false negative: mining frequent itemsets from high speed transactional data streams
#@ Jeffery Xu Yu;Zhihong Chong;Hongjun Lu;Aoying Zhou
#t 2004
#c 4
#% 1331
#% 2833
#% 214073
#% 379445
#% 481290
#% 481779
#% 492912
#% 548479
#% 576119
#% 593957
#% 654461
#% 993960
#! The problem of finding frequent items has been recently studied over high speed data streams. However, mining frequent itemsets from transactional data streams has not been well addressed yet in terms of its bounds of memory consumption. The main difficulty is due to the nature of the exponential explosion of itemsets. Given a domain of I unique items, the possible number of itemsets can be up to 2I - 1. When the length of data streams approaches to a very large number N, the possibility of an itemset to be frequent becomes larger and difficult to track with limited memory. However, the real killer of effective frequent itemset mining is that most of existing algorithms are false-positive oriented. That is, they control memory consumption in the counting processes by an error parameter ε, and allow items with support below the specified minimum support s but above s-ε counted as frequent ones. Such false-positive items increase the number of false-positive frequent itemsets exponentially, which may make the problem computationally intractable with bounded memory consumption. In this paper, we developed algorithms that can effectively mine frequent item(set)s from high speed transactional data streams with a bound of memory consumption. While our algorithms are false-negative oriented, that is, certain frequent itemsets may not appear in the results, the number of false-negative itemsets can be controlled by a predefined parameter so that desired recall rate of frequent itemsets can be guaranteed. We developed algorithms based on Chernoff bound. Our extensive experimental studies show that the proposed algorithms have high accuracy, require less memory, and consume less CPU time. They significantly outperform the existing false-positive algorithms.

#index 1016147
#* Indexing temporal XML documents
#@ Alberto O. Mendelzon;Flavio Rizzolo;Alejandro Vaisman
#t 2004
#c 4
#% 287070
#% 379484
#% 397359
#% 397360
#% 400361
#% 479465
#% 480489
#% 480656
#% 480827
#% 504576
#% 562809
#% 565265
#% 654452
#% 660000
#% 665630
#% 993951
#% 1015307
#! Different models have been proposed recently for representing temporal data, tracking historical information, and recovering the state of the document as of any given time, in XML documents. We address the problem of indexing temporal XML documents. In particular we show that by indexing continuous paths, i.e. paths that are valid continuously during a certain interval in a temporal XML graph, we can dramatically increase query performance. We describe in detail the indexing scheme, denoted TempIndex, and compare its performance against both a system based on a nontemporal path index, and one based on DOM.

#index 1016148
#* Schema-based scheduling of event processors and buffer minimization for queries on structured data streams
#@ Christoph Koch;Stefanie Scherzinger;Nicole Schweikardt;Bernhard Stegmaier
#t 2004
#c 4
#% 262724
#% 413563
#% 465053
#% 465061
#% 480296
#% 654476
#% 659995
#% 993950
#% 1015266
#% 1015272
#% 1015338
#% 1016148
#! We introduce an extension of the XQuery language, FluX, that supports event-based query processing and the conscious handling of main memory buffers. Purely event-based queries of this language can be executed on streaming XML data in a very direct way. We then develop an algorithm that allows to efficiently rewrite XQueries into the event-based FluX language. This algorithm uses order constraints from a DTD to schedule event handlers and to thus minimize the amount of buffering required for evaluating a query. We discuss the various technical aspects of query optimization and query evaluation within our framework. This is complemented with an experimental evaluation of our approach.

#index 1016149
#* Bloom histogram: path selectivity estimation for XML data with updates
#@ Wei Wang;Haifeng Jiang;Hongjun Lu;Jeffrey Xu Yu
#t 2004
#c 4
#% 13018
#% 248821
#% 322884
#% 333947
#% 347226
#% 397364
#% 397379
#% 397385
#% 413604
#% 465018
#% 479648
#% 480488
#% 482123
#% 654461
#% 993968
#% 993970
#% 1015256
#% 1015285
#! Cost-based XML query optimization calls for accurate estimation of the selectivity of path expressions. Some other interactive and internet applications can also benefit from such estimations. While there are a number of estimation techniques proposed in the literature, almost none of them has any guarantee on the estimation accuracy within a given space limit. In addition, most of them assume that the XML data are more or less static, i.e., with few updates. In this paper, we present a framework for XML path selectivity estimation in a dynamic context. Specifically, we propose a novel data structure, bloom histogram, to approximate XML path frequency distribution within a small space budget and to estimate the path selectivity accurately with the bloom histogram. We obtain the upper bound of its estimation error and discuss the trade-offs between the accuracy and the space limit. To support updates of bloom histograms efficiently when underlying XML data change, a dynamic summary layer is used to keep exact or more detailed XML path information. We demonstrate through our extensive experiments that the new solution can achieve significantly higher accuracy with an even smaller space than the previous methods in both static and dynamic environments.

#index 1016150
#* XQuery on SQL hosts
#@ Torsten Grust;Sherif Sakr;Jens Teubner
#t 2004
#c 4
#% 287005
#% 397358
#% 480489
#% 480657
#% 480822
#% 564207
#% 645385
#% 654493
#% 742563
#% 993953
#% 994015
#% 1015298
#% 1015354
#! Relational database systems may be turned into efficient XML and XPath processors if the system is provided with a suitable relational tree encoding. This paper extends this relational XML processing stack and shows that an RDBMS can also serve as a highly efficient XQuery runtime environment. Our approach is purely relational: XQuery expressions are compiled into SQL code which operates on the tree encoding. The core of the compilation procedure trades XQuery's notions of variable scopes and nested iteration (FLWOR blocks) for equi-joins. The resulting relational XQuery processor closely adheres to the language semantics, e.g., it obeys node identity as well as document and sequence order, and can support XQuery's full axis feature. The system exhibits quite promising performance figures in experiments. Somewhat unexpectedly, we will also see that the XQuery compiler can make good use of SQL's OLAP functionality.

#index 1016151
#* ROX: relational over XML
#@ Alan Halverson;Vanja Josifovski;Guy Lohman;Hamid Pirahesh;Mathias Mörschel
#t 2004
#c 4
#% 285926
#% 397607
#% 480088
#% 480657
#% 572311
#% 632100
#% 643940
#% 745477
#% 803121
#% 820356
#% 994035
#! An increasing percentage of the data needed by business applications is being generated in XML format. Storing the XML in its native format will facilitate new applications that exchange business objects in XML format and query portions of XML documents using XQuery. This paper explores the feasibility of accessing natively-stored XML data through traditional SQL interfaces, called Relational Over XML (ROX), in order to avoid the costly conversion of legacy applications to XQuery. It describes the forces that are driving the industry to evolve toward the ROX scenario as well as some of the issues raised by ROX. The impact of denormalization of data in XML documents is discussed both from a semantic and performance perspective. We also weigh the implications of ROX for manageability and query optimization. We experimentally compared the performance of a prototype of the ROX scenario to today's SQL engines, and found that good performance can be achieved through a combination of utilizing XML's hierarchical storage to store relations "pre-joined" as well as creating indices over the remaining join columns. We have developed an experimental framework using DB2 8.1 for Linux, Unix and Windows, and have gathered initial performance results that validate this approach.

#index 1016152
#* From XML view updates to relational view updates: old solutions to a new problem
#@ Vanessa P. Braganholo;Susan B. Davidson;Carlos A. Heuser
#t 2004
#c 4
#% 286901
#% 287000
#% 291869
#% 333979
#% 397607
#% 411560
#% 411759
#% 480152
#% 480657
#% 482068
#% 570875
#% 576096
#% 632064
#% 654492
#% 993941
#! This paper addresses the question of updating relational databases through XML views. Using query trees to capture the notions of selection, projection, nesting, grouping, and heterogeneous sets found throughout most XML query languages, we show how XML views expressed using query trees can be mapped to a set of corresponding relational views. We then show how updates on the XML view are mapped to updates on the corresponding relational views. Existing work on updating relational views can then be leveraged to determine whether or not the relational views are updatable with respect to the relational updates, and if so, to translate the updates to the underlying relational database.

#index 1016153
#* XWAVE: optimal and approximate extended wavelets
#@ Sudipto Guha;Chulyun Kim;Kyuseok Shim
#t 2004
#c 4
#% 210190
#% 227883
#% 248812
#% 248822
#% 257637
#% 259995
#% 273902
#% 273919
#% 333983
#% 479984
#% 480306
#% 480465
#% 480628
#% 482092
#% 492932
#% 504019
#% 654460
#! Wavelet synopses have been found to be of interest in query optimization and approximate query answering. Recently, extended wavelets were proposed by Deligiannakis and Roussopoulos for data sets containing multiple measures. Extended wavelets optimize the storage utilization by attempting to store the same wavelet coefficient across different measures. This reduces the bookkeeping overhead and more coefficients can be stored. An optimal algorithm for minimizing the error in representation and an approximation algorithm for the complementary problem was provided. However, both their algorithms take linear space. Synopsis structures are often used in environments where space is at a premium and the data arrives as a continuous stream which is too expensive to store. In this paper, we give algorithms for extended wavelets which are space sensitive, i.e., use space which is dependent on the size of the synopsis (and at most on the logarithm of the total data) and operates in a streaming fashion. We present better optimal algorithms based on dynamic programming and a near optimal approximate greedy algorithm. We also demonstrate the performance benefits of our algorithms compared to previous ones through experiments on real-life and synthetic data sets.

#index 1016154
#* REHIST: relative error histogram construction algorithms
#@ Sudipto Guha;Kyuseok Shim;Jungchul Woo
#t 2004
#c 4
#% 201921
#% 248822
#% 273902
#% 274152
#% 299982
#% 333872
#% 338425
#% 347226
#% 378404
#% 397389
#% 399763
#% 479648
#% 481266
#% 492932
#% 660003
#% 742562
#! Histograms and Wavelet synopses provide useful tools in query optimization and approximate query answering. Traditional histogram construction algorithms, such as V-Optimal, optimize absolute error measures for which the error in estimating a true value of 10 by 20 has the same effect of estimating a true value of 1000 by 1010. However, several researchers have recently pointed out the drawbacks of such schemes and proposed wavelet based schemes to minimize relative error measures. None of these schemes provide satisfactory guarantees - and we provide evidence that the difficulty may lie in the choice of wavelets as the representation scheme. In this paper, we consider histogram construction for the known relative error measures. We develop optimal as well as fast approximation algorithms. We provide a comprehensive theoretical analysis and demonstrate the effectiveness of these algorithms in providing significantly more accurate answers through synthetic and real life data sets.

#index 1016155
#* Distributed set-expression cardinality estimation
#@ Abhinandan Das;Sumit Ganguly;Minos Garofalakis;Rajeev Rastogi
#t 2004
#c 4
#% 297915
#% 397443
#% 480805
#% 492912
#% 654443
#% 654463
#% 654482
#% 654488
#% 654497
#% 963734
#% 993969
#! We consider the problem of estimating set-expression cardinality in a distributed streaming environment where rapid update streams originating at remote sites are continually transmitted to a central processing system. At the core of our algorithmic solutions for answering set-expression cardinality queries are two novel techniques for lowering data communication costs without sacrificing answer precision. Our first technique exploits global knowledge of the distribution of certain frequently occurring stream elements to significantly reduce the transmission of element state information to the central site. Our second technical contribution involves a novel way of capturing the semantics of the input set expression in a boolean logic formula, and using models (of the formula) to determine whether an element state change at a remote site can affect the set expression result. Results of our experimental study with real-life as well as synthetic data sets indicate that our distributed set-expression cardinality estimation algorithms achieve substantial reductions in message traffic compared to naive approaches that provide the same accuracy guarantees.

#index 1016156
#* Memory-limited execution of windowed stream joins
#@ Utkarsh Srivastava;Jennifer Widom
#t 2004
#c 4
#% 190611
#% 273682
#% 273908
#% 338425
#% 347226
#% 378388
#% 379444
#% 379445
#% 397354
#% 578560
#% 654444
#% 745534
#% 1015280
#! We address the problem of computing approximate answers to continuous sliding-window joins over data streams when the available memory may be insufficient to keep the entire join state. One approximation scenario is to provide a maximum subset of the result, with the objective of losing as few result tuples as possible. An alternative scenario is to provide a random sample of the join result, e.g., if the output of the join is being aggregated. We show formally that neither approximation can be addressed effectively for a sliding-window join of arbitrary input streams. Previous work has addressed only the maximum-subset problem, and has implicitly used a frequency-based model of stream arrival. We address the sampling problem for this model. More importantly, we point out a broad class of applications for which an age-based model of stream arrival is more appropriate, and we address both approximation scenarios under this new model. Finally, for the case of multiple joins being executed with an overall memory constraint, we provide an algorithm for memory allocation across the joins that optimizes a combined measure of approximation in all scenarios considered. All of our algorithms are implemented and experimental results demonstrate their effectiveness.

#index 1016157
#* Resource sharing in continuous sliding-window aggregates
#@ Arvind Arasu;Jennifer Widom
#t 2004
#c 4
#% 36117
#% 271199
#% 300166
#% 300167
#% 300179
#% 333926
#% 333938
#% 378388
#% 379445
#% 397353
#% 397443
#% 420053
#% 480296
#% 583712
#% 632019
#% 642409
#% 654476
#% 654477
#% 659987
#% 801696
#% 993948
#% 993949
#% 993961
#% 993969
#! We consider the problem of resource sharing when processing large numbers of continuous queries. We specifically address sliding-window aggregates over data streams, an important class of continuous operators for which sharing has not been addressed. We present a suite of sharing techniques that cover a wide range of possible scenarios: different classes of aggregation functions (algebraic, distributive, holistic), different window types (time-based, tuple-based, suffix, historical), and different input models (single stream, multiple substreams). We provide precise theoretical performance guarantees for our techniques, and show their practical effectiveness through experimental study.

#index 1016158
#* Remembrance of streams past: overload-sensitive management of archived streams
#@ Sirish Chandrasekaran;Michael Franklin
#t 2004
#c 4
#% 223781
#% 300179
#% 333977
#% 370597
#% 397353
#% 420114
#% 480951
#% 570884
#% 578560
#% 993948
#% 993949
#% 993960
#% 1015280
#! This paper studies Data Stream Management Systems that combine real-time data streams with historical data, and hence access incoming streams and archived data simultaneously. A significant problem for these systems is the I/O cost of fetching historical data which inhibits processing of the live data streams. Our solution is to reduce the I/O cost for accessing the archive by retrieving only a reduced (summarized or sampled) version of the historical data. This paper does not propose new summarization or sampling techniques, but rather a framework in which multiple resolutions of summarization/sampling can be generated efficiently. The query engine can select the appropriate level of summarization to use depending on the resources currently available. The central research problem studied is whether to generate the multiple representations of archived data eagerly upon data-arrival, lazily at query-time, or in a hybrid fashion. Concrete techniques for each approach are presented, which are tied to a specific data reduction technique (random sampling). The tradeoffs among the three approaches are studied both analytically and experimentally.

#index 1016159
#* WIC: a general-purpose algorithm for monitoring web information sources
#@ Sandeep Pandey;Kedar Dhamdhere;Christopher Olston
#t 2004
#c 4
#% 42408
#% 300139
#% 310775
#% 316563
#% 330604
#% 348137
#% 408680
#% 443298
#% 576112
#% 577369
#% 577370
#! The Web is becoming a universal information dissemination medium, due to a number of factors including its support for content dynamicity. A growing number of Web information providers post near real-time updates in domains such as auctions, stock markets, bulletin boards, news, weather, roadway conditions, sports scores, etc. External parties often wish to capture this information for a wide variety of purposes ranging from online data mining to automated synthesis of information from multiple sources. There has been a great deal of work on the design of systems that can process streams of data from Web sources, but little attention has been paid to how to produce these data streams, given that Web pages generally require "pull-based" access. In this paper we introduce a new general-purpose algorithm for monitoring Web information sources, effectively converting pull-based sources into push-based ones. Our algorithm can be used in conjunction with continuous query systems that assume information is fed into the query engine in a push-based fashion. Ideally, a Web monitoring algorithm for this purpose should achieve two objectives: (1) timeliness and (2) completeness of information captured. However, we demonstrate both analytically and empirically using real-world data that these objectives are fundamentally at odds. When resources available for Web monitoring are limited, and the number of sources to monitor is large, it may be necessary to sacrifice some timeliness to achieve better completeness, or vice versa. To take this fact into account, our algorithm is highly parameterized and targets an application-specified balance between timeliness and completeness. In this paper we formalize the problem of optimizing for a flexible combination of timeliness and completeness, and prove that our parameterized algorithm is a 2- approximation in all cases, and in certain cases is optimal.

#index 1016160
#* Similarity search for web services
#@ Xin Dong;Alon Halevy;Jayant Madhavan;Ema Nemes;Jun Zhang
#t 2004
#c 4
#% 140588
#% 219051
#% 232136
#% 235458
#% 262054
#% 333990
#% 341700
#% 438137
#% 465754
#% 519428
#% 572314
#% 660001
#% 840583
#% 993982
#! Web services are loosely coupled software components, published, located, and invoked across the web. The growing number of web services available within an organization and on the Web raises a new and challenging search problem: locating desired web services. Traditional keyword search is insufficient in this context: the specific types of queries users require are not captured, the very small text fragments in web services are unsuitable for keyword search, and the underlying structure and semantics of the web services are not exploited. We describe the algorithms underlying the Woogle search engine for web services. Woogle supports similarity search for web services, such as finding similar web-service operations and finding operations that compose with a given one. We describe novel techniques to support these types of searches, and an experimental study on a collection of over 1500 web-service operations that shows the high recall and precision of our algorithms.

#index 1016161
#* AWESOME: a data warehouse-based system for adaptive website recommendations
#@ Andreas Thor;Erhard Rahm
#t 2004
#c 4
#% 290482
#% 314933
#% 332648
#% 343132
#% 413553
#% 414514
#% 420121
#% 420134
#% 464215
#% 482231
#% 595145
#% 630984
#% 642616
#% 739634
#% 745705
#% 993934
#! Recommendations are crucial for the success of large websites. While there are many ways to determine recommendations, the relative quality of these recommenders depends on many factors and is largely unknown. We propose a new classification of recommenders and comparatively evaluate their relative quality for a sample web-site. The evaluation is performed with AWESOME (Adaptive website recommendations), a new data warehouse-based recommendation system capturing and evaluating user feedback on presented recommendations. Moreover, we show how AWESOME performs an automatic and adaptive closed-loop website optimization by dynamically selecting the most promising recommenders based on continuously measured recommendation feedback. We propose and evaluate several alternatives for dynamic recommender selection including a powerful machine learning approach.

#index 1016162
#* Accurate and efficient crawling for relevant websites
#@ Martin Ester;Hans-Peter Kriegel;Matthias Schubert
#t 2004
#c 4
#% 248810
#% 262061
#% 268087
#% 280817
#% 281251
#% 282905
#% 312861
#% 348138
#% 458379
#% 466250
#% 479980
#% 480309
#% 577236
#! Focused web crawlers have recently emerged as an alternative to the well-established web search engines. While the well-known focused crawlers retrieve relevant webpages, there are various applications which target whole websites instead of single webpages. For example, companies are represented by websites, not by individual webpages. To answer queries targeted at websites, web directories are an established solution. In this paper, we introduce a novel focused website crawler to employ the paradigm of focused crawling for the search of relevant websites. The proposed crawler is based on a two-level architecture and corresponding crawl strategies with an explicit concept of websites. The external crawler views the web as a graph of linked websites, selects the websites to be examined next and invokes internal crawlers. Each internal crawler views the webpages of a single given website and performs focused (page) crawling within that website. Our experimental evaluation demonstrates that the proposed focused website crawler clearly outperforms previous methods of focused crawling which were adapted to retrieve websites instead of single webpages.

#index 1016163
#* Instance-based schema matching for web databases by domain-specific query probing
#@ Jiying Wang;Ji-Rong Wen;Fred Lochovsky;Wei-Ying Ma
#t 2004
#c 4
#% 22948
#% 261741
#% 273926
#% 330784
#% 333990
#% 387427
#% 408396
#% 443408
#% 480479
#% 480645
#% 480824
#% 481280
#% 533916
#% 572314
#% 577319
#% 654459
#% 654469
#% 721077
#% 993982
#% 1015284
#! In a Web database that dynamically provides information in response to user queries, two distinct schemas, interface schema (the schema users can query) and result schema (the schema users can browse), are presented to users. Each partially reflects the actual schema of the Web database. Most previous work only studied the problem of schema matching across query interfaces of Web databases. In this paper, we propose a novel schema model that distinguishes the interface and the result schema of a Web database in a specific domain. In this model, we address two significant Web database schema-matching problems: intra-site and inter-site. The first problem is crucial in automatically extracting data from Web databases, while the second problem plays a significant role in meta-retrieving and integrating data from different Web databases. We also investigate a unified solution to the two problems based on query probing and instance-based schema matching techniques. Using the model, a cross validation technique is also proposed to improve the accuracy of the schema matching. Our experiments on real Web databases demonstrate that the two problems can be solved simultaneously with high precision and recall.

#index 1016164
#* Computing pagerank in a distributed internet search system
#@ Yuan Wang;David J. DeWitt
#t 2004
#c 4
#% 268079
#% 268087
#% 282905
#% 330609
#% 340175
#% 340176
#% 348173
#% 480479
#% 505869
#% 565488
#% 577328
#% 577329
#% 578337
#% 674136
#% 728195
#! Existing Internet search engines use web crawlers to download data from the Web. Page quality is measured on central servers, where user queries are also processed. This paper argues that using crawlers has a list of disadvantages. Most importantly, crawlers do not scale. Even Google, the leading search engine, indexes less than 1% of the entire Web. This paper proposes a distributed search engine framework, in which every web server answers queries over its own data. Results from multiple web servers will be merged to generate a ranked hyperlink list on the submitting server. This paper presents a series of algorithms that compute PageRank in such framework. The preliminary experiments on a real data set demonstrate that the system achieves comparable accuracy on PageRank vectors to Google's well-known PageRank algorithm and, therefore, high quality of query results.

#index 1016165
#* Enhancing P2P file-sharing with an internet-scale query processor
#@ Boon Thau Loo;Joseph M. Hellerstein;Ryan Huebsch;Scott Shenker;Ion Stoica
#t 2004
#c 4
#% 264263
#% 340175
#% 340176
#% 340933
#% 429749
#% 463917
#% 496291
#% 505869
#% 571217
#% 636008
#% 636009
#% 646239
#% 674136
#% 740863
#% 960186
#% 1015281
#% 1722230
#! In this paper, we address the problem of designing a scalable, accurate query processor for peer-to-peer filesharing and similar distributed keyword search systems. Using a globally-distributed monitoring infrastructure, we perform an extensive study of the Gnutella filesharing network, characterizing its topology, data and query workloads. We observe that Gnutella's query processing approach performs well for popular content, but quite poorly for rare items with few replicas. We then consider an alternate approach based on Distributed Hash Tables (DHTs). We describe our implementation of PIERSearch, a DHT-based system, and propose a hybrid system where Gnutella is used to locate popular items, and PIERSearch for handling rare items. We develop an analytical model of the two approaches, and use it in concert with our Gnutella traces to study the trade-off between query recall and system overhead of the hybrid system. We evaluate a variety of localized schemes for identifying items that are rare and worth handling via the DHT. Lastly, we show in a live deployment on fifty nodes on two continents that it nicely complements Gnutella in its ability to handle rare items.

#index 1016166
#* Online balancing of range-partitioned data with applications to peer-to-peer systems
#@ Prasanna Ganesan;Mayank Bawa;Hector Garcia-Molina
#t 2004
#c 4
#% 43171
#% 115661
#% 116042
#% 213080
#% 300164
#% 340175
#% 340176
#% 342375
#% 384014
#% 397397
#% 453509
#% 460873
#% 466944
#% 479920
#% 480298
#% 481296
#% 505869
#% 612643
#% 708321
#% 765673
#% 770901
#% 772021
#% 963874
#! We consider the problem of horizontally partitioning a dynamic relation across a large number of disks/nodes by the use of range partitioning. Such partitioning is often desirable in large-scale parallel databases, as well as in peer-to-peer (P2P) systems. As tuples are inserted and deleted, the partitions may need to be adjusted, and data moved, in order to achieve storage balance across the participant disks/nodes. We propose efficient, asymptotically optimal algorithms that ensure storage balance at all times, even against an adversarial insertion and deletion of tuples. We combine the above algorithms with distributed routing structures to architect a P2P system that supports efficient range queries, while simultaneously guaranteeing storage balance.

#index 1016167
#* Network-aware query processing for stream-based applications
#@ Yanif Ahmad;Uğur Çetintemel
#t 2004
#c 4
#% 13018
#% 86949
#% 202286
#% 210177
#% 300179
#% 322845
#% 330305
#% 340175
#% 340176
#% 397352
#% 479452
#% 505869
#% 631868
#% 654443
#% 654483
#% 726621
#% 805466
#% 963648
#% 993949
#% 1015281
#% 1502090
#% 1850764
#! This paper investigates the benefits of network awareness when processing queries in widely-distributed environments such as the Internet. We present algorithms that leverage knowledge of network characteristics (e.g., topology, bandwidth, etc.) when deciding on the network locations where the query operators are executed. Using a detailed emulation study based on realistic network models, we analyse and experimentally evaluate the proposed approaches for distributed stream processing. Our results quantify the significant benefits of the network-aware approaches and reveal the fundamental trade-off between bandwidth efficiency and result latency that arises in networked query processing.

#index 1016168
#* Data sharing through query translation in autonomous sources
#@ Anastasios Kementsietsidis;Marcelo Arenas
#t 2004
#c 4
#% 33376
#% 273914
#% 289266
#% 378409
#% 384978
#% 465057
#% 496291
#% 572314
#% 654468
#% 723449
#% 1015281
#% 1015302
#! We consider the problem of data sharing between autonomous data sources in an environment where constraints cannot be placed on the shared contents of sources. Our solutions rely on the use of mapping tables which define how data from different sources are associated. In this setting, the answer to a local query, that is, a query posed against the schema of a single source, is augmented by retrieving related data from associated sources. This retrieval of data is achieved by translating, through mapping tables, the local query into a set of queries that are executed against the associated sources. We consider both sound translations (which only retrieve correct answers) and complete translations (which retrieve all correct answers, and no incorrect answers) and we present algorithms to compute such translations. Our solutions are implemented and tested experimentally and we describe here our key findings.

#index 1016169
#* Linear road: a stream data management benchmark
#@ Arvind Arasu;Mitch Cherniack;Eduardo Galvez;David Maier;Anurag S. Maskey;Esther Ryvkina;Michael Stonebraker;Richard Tibbetts
#t 2004
#c 4
#% 300179
#% 726621
#% 993949
#% 1015324
#! This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora [1] (out of Brandeis University, Brown University and MIT) and STREAM [8] (out of Stanford University) stream systems. Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses "variable tolling" [6, 11, 9]: an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations: one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of 5 on streaming data applications.

#index 1016170
#* Query languages and data models for database sequences and data streams
#@ Yan-Nei Law;Haixun Wang;Carlo Zaniolo
#t 2004
#c 4
#% 116082
#% 172950
#% 227883
#% 245996
#% 248813
#% 300179
#% 333850
#% 361831
#% 378388
#% 384978
#% 397353
#% 420101
#% 443298
#% 464058
#% 480497
#% 481943
#% 481954
#% 578391
#% 578560
#% 654497
#% 993948
#% 993949
#! We study the fundamental limitations of relational algebra (RA) and SQL in supporting sequence and stream queries, and present effective query language and data model enrichments to deal with them. We begin by observing the well-known limitations of SQL in application domains which are important for data streams, such as sequence queries and data mining. Then we present a formal proof that, for continuous queries on data streams, SQL suffers from additional expressive power problems. We begin by focusing on the notion of nonblocking (NB) queries that are the only continuous queries that can be supported on data streams. We characterize the notion of nonblocking queries by showing that they are equivalent to monotonic queries. Therefore the notion of NB-completeness for RA can be formalized as its ability to express all monotonic queries expressible in RA using only the monotonic operators of RA. We show that RA is not NB-complete, and SQL is not more powerful than RA for monotonic queries. To solve these problems, we propose extensions that allow SQL to support all the monotonic queries expressible by a Turing machine using only monotonic operators. We show that these extensions are (i) user-defined aggregates (UDAs) natively coded in SQL (rather than in an external language), and (ii) a generalization of the union operator to support the merging of multiple streams according to their timestamps. These query language extensions require matching extensions to basic relational data model to support sequences explicitly ordered by times-tamps. Along with the formulation of very powerful queries, the proposed extensions entail more efficient expressions for many simple queries. In particular, we show that nonblocking queries are simple to characterize according to their syntactic structure.

#index 1016171
#* Tamper detection in audit logs
#@ Richard T. Snodgrass;Shilong Stanley Yao;Christian Collberg
#t 2004
#c 4
#% 253706
#% 284594
#% 300458
#% 340610
#% 342345
#% 386623
#% 388926
#% 427311
#% 443542
#% 480096
#% 480481
#% 480617
#% 513367
#% 631961
#% 657774
#% 805459
#% 963647
#! Audit logs are considered good practice for business systems, and are required by federal regulations for secure systems, drug approval data, medical information disclosure, financial records, and electronic voting. Given the central role of audit logs, it is critical that they are correct and inalterable. It is not sufficient to say, "our data is correct, because we store all interactions in a separate audit log." The integrity of the audit log itself must also be guaranteed. This paper proposes mechanisms within a database management system (DBMS), based on cryptographically strong one-way hash functions, that prevent an intruder, including an auditor or an employee or even an unknown bug within the DBMS itself, from silently corrupting the audit log. We propose that the DBMS store additional information in the database to enable a separate audit log validator to examine the database along with this extra information and state conclusively whether the audit log has been compromised. We show with an implementation on a high-performance storage engine that the overhead for auditing is low and that the validator can efficiently and correctly determine if the audit log has been compromised.

#index 1016172
#* Auditing compliance with a Hippocratic database
#@ Rakesh Agrawal;Roberto Bayardo;Christos Faloutsos;Jerry Kiernan;Ralf Rantzau;Ramakrishnan Srikant
#t 2004
#c 4
#% 36117
#% 67453
#% 116043
#% 164560
#% 300179
#% 320902
#% 390132
#% 403195
#% 442781
#% 644201
#% 993943
#% 1016138
#! We introduce an auditing framework for determining whether a database system is adhering to its data disclosure policies. Users formulate audit expressions to specify the (sensitive) data subject to disclosure review. An audit component accepts audit expressions and returns all queries (deemed "suspicious") that accessed the specified data during their execution. The overhead of our approach on query processing is small, involving primarily the logging of each query string along with other minor annotations. Database triggers are used to capture updates in a backlog database. At the time of audit, a static analysis phase selects a subset of logged queries for further analysis. These queries are combined and transformed into an SQL audit query, which when run against the backlog database, identifies the suspicious queries efficiently and precisely. We describe the algorithms and data structures used in a DB2-based implementation of this framework. Experimental results reinforce our design choices and show the practicality of the approach.

#index 1016173
#* High-dimensional OLAP: a minimal cubing approach
#@ Xiaolei Li;Jiawei Han;Hector Gonzalez
#t 2004
#c 4
#% 210182
#% 223781
#% 227861
#% 227880
#% 236410
#% 248814
#% 273916
#% 280448
#% 290703
#% 333925
#% 387427
#% 397388
#% 420053
#% 462217
#% 480329
#% 481290
#% 481951
#% 654446
#% 993996
#% 1015294
#! Data cube has been playing an essential role in fast OLAP (online analytical processing) in many multi-dimensional data warehouses. However, there exist data sets in applications like bioinformatics, statistics, and text processing that are characterized by high dimensionality, e.g., over 100 dimensions, and moderate size, e.g., around 106 tuples. No feasible data cube can be constructed with such data sets. In this paper we will address the problem of developing an efficient algorithm to perform OLAP on such data sets. Experience tells us that although data analysis tasks may involve a high dimensional space, most OLAP operations are performed only on a small number of dimensions at a time. Based on this observation, we propose a novel method that computes a thin layer of the data cube together with associated value-list indices. This layer, while being manageable in size, will be capable of supporting flexible and fast OLAP operations in the original high dimensional space. Through experiments we will show that the method has I/O costs that scale nicely with dimensionality. Furthermore, the costs are comparable to that of accessing an existing data cube when full materialization is possible.

#index 1016174
#* The polynomial complexity of fully materialized coalesced cubes
#@ Yannis Sismanis;Nick Roussopoulos
#t 2004
#c 4
#% 210182
#% 227868
#% 227880
#% 259995
#% 273697
#% 273916
#% 300195
#% 316978
#% 397388
#% 462204
#% 464215
#% 479450
#% 481948
#% 481951
#% 482110
#% 654446
#% 729576
#! The data cube operator encapsulates all possible groupings of a data set and has proved to be an invaluable tool in analyzing vast amounts of data. However its apparent exponential complexity has significantly limited its applicability to low dimensional datasets. Recently the idea of the coalesced cube was introduced, and showed that high-dimensional coalesced cubes are orders of magnitudes smaller in size than the original data cubes even when they calculate and store every possible aggregation with 100% precision. In this paper we present an analytical framework for estimating the size of coalesced cubes. By using this framework on uniform coalesced cubes we show that their size and the required computation time scales polynomially with the dimensionality of the data set and, therefore, a full data cube at 100% precision is not inherently cursed by high dimensionality. Additionally, we show that such coalesced cubes scale polynomially (and close to linearly) with the number of tuples on the dataset. We were also able to develop an efficient algorithm for estimating the size of coalesced cubes before actually computing them, based only on metadata about the cubes. Finally, we complement our analytical approach with an extensive experimental evaluation using real and synthetic data sets, and demonstrate that not only uniform but also zipfian and real coalesced cubes scale polynomially.

#index 1016175
#* Relational link-based ranking
#@ Floris Geerts;Heikki Mannila;Evimaria Terzi
#t 2004
#c 4
#% 13742
#% 190611
#% 262061
#% 268079
#% 290830
#% 330707
#% 333854
#% 340932
#% 384978
#% 393907
#% 397169
#% 479659
#% 480819
#% 577273
#% 660011
#% 733373
#% 777931
#% 1016176
#% 1390190
#! Link analysis methods show that the interconnections between web pages have lots of valuable information. The link analysis methods are, however, inherently oriented towards analyzing binary relations. We consider the question of generalizing link analysis methods for analyzing relational databases. To this aim, we provide a generalized ranking framework and address its practical implications. More specically, we associate with each relational database and set of queries a unique weighted directed graph, which we call the database graph. We explore the properties of database graphs. In analogy to link analysis algorithms, which use the Web graph to rank web pages, we use the database graph to rank partial tuples. In this way we can, e.g., extend the PageRank link analysis algorithm to relational databases and give this extension a random querier interpretation. Similarly, we extend the HITS link analysis algorithm to relational databases. We conclude with some preliminary experimental results.

#index 1016176
#* Objectrank: authority-based keyword search in databases
#@ Andrey Balmin;Vagelis Hristidis;Yannis Papakonstantinou
#t 2004
#c 4
#% 67565
#% 70370
#% 129666
#% 190611
#% 262061
#% 268073
#% 268079
#% 290830
#% 291299
#% 333854
#% 348173
#% 413614
#% 577328
#% 577329
#% 654442
#% 659990
#% 993987
#! The ObjectRank system applies authority-based ranking to keyword search in databases modeled as labeled graphs. Conceptually, authority originates at the nodes (objects) containing the keywords and flows to objects according to their semantic connections. Each node is ranked according to its authority with respect to the particular keywords. One can adjust the weight of global importance, the weight of each keyword of the query, the importance of a result actually containing the keywords versus being referenced by nodes containing them, and the volume of authority flow via each type of semantic connection. Novel performance challenges and opportunities are addressed. First, schemas impose constraints on the graph, which are exploited for performance purposes. Second, in order to address the issue of authority ranking with respect to the given keywords (as opposed to Google's global PageRank) we precompute single keyword ObjectRanks and combine them during run time. We conducted user surveys and a set of performance experiments on multiple real and synthetic datasets, to assess the semantic meaningfulness and performance of ObjectRank.

#index 1016177
#* Combating web spam with trustrank
#@ Zoltán Gyöngyi;Hector Garcia-Molina;Jan Pedersen
#t 2004
#c 4
#% 290830
#% 348173
#% 387427
#% 577328
#% 577367
#% 903341
#! Web spam pages use various techniques to achieve higher-than-deserved rankings in a search engine's results. While human experts can identify spam, it is too expensive to manually evaluate a large number of pages. Instead, we propose techniques to semi-automatically separate reputable, good pages from spam. We first select a small set of seed pages to be evaluated by an expert. Once we manually identify the reputable seed pages, we use the link structure of the web to discover other pages that are likely to be good. In this paper we discuss possible ways to implement the seed selection and the discovery of good pages. We present results of experiments run on the World Wide Web indexed by AltaVista and evaluate the performance of our techniques. Our results show that we can effectively filter out spam from a significant fraction of the web, based on a good seed set of less than 200 sites.

#index 1016178
#* Model-driven data acquisition in sensor networks
#@ Amol Deshpande;Carlos Guestrin;Samuel R. Madden;Joseph M. Hellerstein;Wei Hong
#t 2004
#c 4
#% 174161
#% 227883
#% 248812
#% 297915
#% 309433
#% 333946
#% 333969
#% 333986
#% 376266
#% 388024
#% 397355
#% 438135
#% 480805
#% 654482
#% 654487
#% 745442
#% 858997
#% 1650381
#! Declarative queries are proving to be an attractive paradigm for ineracting with networks of wireless sensors. The metaphor that "the sensornet is a database" is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a model of that reality is required to complement the readings. In this paper, we enrich interactive sensor querying with statistical modeling techniques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to compute in both time and energy. Utilizing the combination of a model and live data acquisition raises the challenging optimization problem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an exponential time algorithm for finding the optimal solution to this optimization problem, and a polynomial-time heuristic for identifying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach provides a high-fidelity representation of the real phenomena and leads to significant performance gains versus traditional data acquisition techniques.

#index 1016179
#* GridDB: a data-centric overlay for scientific grids
#@ David T. Liu;Michael J. Franklin
#t 2004
#c 4
#% 2896
#% 116575
#% 227880
#% 228007
#% 286831
#% 300185
#% 318704
#% 384978
#% 397372
#% 397398
#% 411557
#% 420114
#% 480120
#% 497874
#% 503710
#% 503870
#% 504161
#% 562125
#% 569934
#% 610751
#% 654502
#% 678708
#! We present GridDB, a data-centric overlay for scientific grid data analysis. In contrast to currently deployed process-centric middleware, GridDB manages data entities rather than processes. GridDB provides a suite of services important to data analysis: a declarative interface, type-checking, interactive query processing, and memoization. We discuss several elements of GridDB: workflow/data model, query language, software architecture and query processing; and a prototype implementation. We validate GridDB by showing its modeling of real-world physics and astronomy analyses, and measurements on our prototype.

#index 1016180
#* Towards an internet-scale XML dissemination service
#@ Yanlei Diao;Shariq Rizvi;Michael J. Franklin
#t 2004
#c 4
#% 68236
#% 151529
#% 223386
#% 271199
#% 271622
#% 274142
#% 297191
#% 300153
#% 302816
#% 303694
#% 333938
#% 342372
#% 399551
#% 434043
#% 446421
#% 458847
#% 465061
#% 480296
#% 612477
#% 646220
#% 654476
#% 659995
#% 659997
#% 661478
#% 731408
#% 765441
#% 963648
#% 994000
#% 1015259
#% 1015276
#! Publish/subscribe systems have demonstrated the ability to scale to large numbers of users and high data rates when providing content-based data dissemination services on the Internet. However, their services are limited by the data semantics and query expressiveness that they support. On the other hand, the recent work on selective dissemination of XML data has made significant progress in moving from XML filtering to the richer functionality of transformation for result customization, but in general has ignored the challenges of deploying such XML-based services on an Internet-scale. In this paper, we address these challenges in the context of incorporating the rich functionality of XML data dissemination in a highly scalable system. We present the architectural design of ONYX, a system based on an overlay network. We identify the salient technical challenges in supporting XML filtering and transformation in this environment and propose techniques for solving them.

#index 1016181
#* Efficiency-quality tradeoffs for vector score aggregation
#@ Pavan Kumar C. Singitham;Mahathi S. Mahabhashyam;Prabhakar Raghavan
#t 2004
#c 4
#% 24108
#% 86950
#% 150132
#% 169940
#% 200694
#% 227939
#% 232764
#% 238757
#% 249321
#% 278831
#% 303893
#% 330769
#% 394709
#% 427199
#% 435141
#% 464195
#% 481956
#% 580070
#% 643566
#% 654466
#% 656701
#% 840583
#% 1015265
#! Finding the l nearest neighbors to a query in a vector space is an important primitive in text and image retrieval. Here we study an extension of this problem with applications to XML and image retrieval: we have multiple vector spaces, and the query places a weight on each space. Match scores from the spaces are weighted by these weights to determine the overall match between each record and the query; this is a case of score aggregation. We study approximation algorithms that use a small fraction of the computation of exhaustive search through all records, while returning nearly the best matches. We focus on the tradeoff between the computation and the quality of the results. We develop two approaches to retrieval from such multiple vector spaces. The first is inspired by resource allocation. The second, inspired by computational geometry, combines the multiple vector spaces together with all possible query weights into a single larger space. While mathematically elegant, this abstraction is intractable for implementation. We therefore devise an approximation of this combined space. Experiments show that all our approaches (to varying extents) enable retrieval quality comparable to exhaustive search, while avoiding its heavy computational cost.

#index 1016182
#* Merging the results of approximate match operations
#@ Sudipto Guha;Nick Koudas;Amit Marathe;Divesh Srivastava
#t 2004
#c 4
#% 201889
#% 248801
#% 330769
#% 333854
#% 333943
#% 453464
#% 480654
#% 577238
#% 577309
#% 644182
#% 654466
#% 654467
#! Data Cleaning is an important process that has been at the center of research interest in recent years. An important end goal of effective data cleaning is to identify the relational tuple or tuples that are "most related" to a given query tuple. Various techniques have been proposed in the literature for efficiently identifying approximate matches to a query string against a single attribute of a relation. In addition to constructing a ranking (i.e., ordering) of these matches, the techniques often associate, with each match, scores that quantify the extent of the match. Since multiple attributes could exist in the query tuple, issuing approximate match operations for each of them separately will effectively create a number of ranked lists of the relation tuples. Merging these lists to identify a final ranking and scoring, and returning the top-K tuples, is a challenging task. In this paper, we adapt the well-known footrule distance (for merging ranked lists) to effectively deal with scores. We study efficient algorithms to merge rankings, and produce the top-K tuples, in a declarative way. Since techniques for approximately matching a query string against a single attribute in a relation are typically best deployed in a database, we introduce and describe two novel algorithms for this problem and we provide SQL specifications for them. Our experimental case study, using real application data along with a realization of our proposed techniques on a commercial data base system, highlights the benefits of the proposed algorithms and attests to the overall effectiveness and practicality of our approach.

#index 1016183
#* Top-k query evaluation with probabilistic guarantees
#@ Martin Theobald;Gerhard Weikum;Ralf Schenkel
#t 2004
#c 4
#% 79312
#% 187079
#% 213786
#% 268079
#% 278831
#% 287466
#% 337285
#% 340886
#% 340887
#% 342828
#% 397378
#% 399762
#% 410276
#% 411758
#% 458829
#% 479967
#% 480330
#% 480819
#% 570629
#% 643566
#% 659255
#% 670420
#% 719598
#% 730018
#% 731409
#% 763882
#% 766671
#% 1015256
#% 1015265
#! Top-k queries based on ranking elements of multidimensional datasets are a fundamental building block for many kinds of information discovery. The best known general-purpose algorithm for evaluating top-k queries is Fagin's threshold algorithm (TA). Since the user's goal behind top-k queries is to identify one or a few relevant and novel data items, it is intriguing to use approximate variants of TA to reduce run-time costs. This paper introduces a family of approximate top-k algorithms based on probabilistic arguments. When scanning index lists of the underlying multidimensional data space in descending order of local scores, various forms of convolution and derived bounds are employed to predict when it is safe, with high probability, to drop candidate items and to prune the index scans. The precision and the efficiency of the developed methods are experimentally evaluated based on a large Web corpus and a structured data collection.

#index 1016184
#* Steps towards cache-resident transaction processing
#@ Stavros Harizopoulos;Anastassia Ailamaki
#t 2004
#c 4
#% 160425
#% 172939
#% 176360
#% 202154
#% 239252
#% 251473
#% 251474
#% 251477
#% 252458
#% 262154
#% 333949
#% 337067
#% 365700
#% 438537
#% 464987
#% 465169
#% 480119
#% 480821
#% 566122
#% 631323
#% 723308
#% 765417
#% 801322
#% 978740
#! Online transaction processing (OLTP) is a multibillion dollar industry with high-end database servers employing state-of-the-art processors to maximize performance. Unfortunately, recent studies show that CPUs are far from realizing their maximum intended throughput because of delays in the processor caches. When running OLTP, instruction-related delays in the memory subsystem account for 25 to 40% of the total execution time. In contrast to data, instruction misses cannot be overlapped with out-of-order execution, and instruction caches cannot grow as the slower access time directly affects the processor speed. The challenge is to alleviate the instruction-related delays without increasing the cache size. We propose Steps, a technique that minimizes instruction cache misses in OLTP workloads by multiplexing concurrent transactions and exploiting common code paths. One transaction paves the cache with instructions, while close followers enjoy a nearly miss-free execution. Steps yields up to 96.7% reduction in instruction cache misses for each additional concurrent transaction, and at the same time eliminates up to 64% of mispredicted branches by loading a repeating execution pattern into the CPU. This paper (a) describes the design and implementation of Steps, (b) analyzes Steps using microbenchmarks, and (c) shows Steps performance when running TPC-C on top of the Shore storage manager.

#index 1016185
#* Write-optimized B-trees
#@ Goetz Graefe
#t 2004
#c 4
#% 117
#% 43172
#% 57333
#% 64430
#% 86931
#% 114582
#% 122307
#% 131555
#% 153223
#% 159275
#% 216907
#% 287672
#% 287715
#% 317933
#% 319549
#% 397362
#% 463592
#% 480457
#% 480589
#% 481256
#% 571081
#% 604239
#% 616140
#% 1015332
#! Large writes are beneficial both on individual disks and on disk arrays, e.g., RAID-5. The presented design enables large writes of internal B-tree nodes and leaves. It supports both in-place updates and large append-only ("log-structured") write operations within the same storage volume, within the same B-tree, and even at the same time. The essence of the proposal is to make page migration inexpensive, to migrate pages while writing them, and to make such migration optional rather than mandatory as in log-structured file systems. The inexpensive page migration also aids traditional defragmentation as well as consolidation of free space needed for future large writes. These advantages are achieved with a very limited modification to conventional B-trees that also simplifies other B-tree operations, e.g., key range locking and compression. Prior proposals and prototypes implemented transacted B-tree on top of log-structured file systems and added transaction support to log-structured file systems. Instead, the presented design adds techniques and performance characteristics of log-structured file systems to traditional B-trees and their standard transaction support, notably without adding a layer of indirection for locating B-tree nodes on disk. The result retains fine-granularity locking, full transactional ACID guarantees, fast search performance, etc. expected of a modern B-tree implementation, yet adds efficient transacted page relocation and large, high-bandwidth writes.

#index 1016186
#* Cache-conscious radix-decluster projections
#@ Stefan Manegold;Peter Boncz;Niels Nes;Martin Kersten
#t 2004
#c 4
#% 18614
#% 227861
#% 251473
#% 251474
#% 286258
#% 287349
#% 440230
#% 479821
#% 480119
#% 480821
#% 566122
#% 571059
#% 993947
#% 993967
#! As CPUs become more powerful with Moore's law and memory latencies stay constant, the impact of the memory access performance bottleneck continues to grow on relational operators like join, which can exhibit random access on a memory region larger than the hardware caches. While cache-conscious variants for various relational algorithms have been described, previous work has mostly ignored (the cost of) projection columns. However, real-life joins almost always come with projections, such that proper projection column manipulation should be an integral part of any generic join algorithm. In this paper, we analyze cache-conscious hash-join algorithms including projections on two storage schemes: N-ary Storage Model (NSM) and Decomposition Storage Model (DSM). It turns out, that the strategy of first executing the join and only afterwards dealing with the projection columns (i.e., post-projection) on DSM, in combination with a new finely tunable algorithm called Radix-Decluster, outperforms all previously reported projection strategies. To make this result generally applicable, we also outline how DSM Radix-Decluster can be integrated in a NSM-based RDBMS using projection indices.

#index 1016187
#* Clotho: decoupling memory page layout from storage organization
#@ Minglong Shao;Jiri Schindler;Steven W. Schlosser;Anastassia Ailamaki;Gregory R. Ganger
#t 2004
#c 4
#% 43172
#% 114582
#% 172939
#% 286258
#% 336857
#% 386623
#% 402263
#% 459939
#% 479821
#% 480119
#% 480821
#% 835871
#% 993947
#% 993967
#% 1015289
#% 1015311
#% 1015313
#% 1306834
#! As database application performance depends on the utilization of the memory hierarchy, smart data placement plays a central role in increasing locality and in improving memory utilization. Existing techniques, however, do not optimize accesses to all levels of the memory hierarchy and for all the different workloads, because each storage level uses different technology (cache, memory, disks) and each application accesses data using different patterns. Clotho is a new buffer pool and storage management architecture that decouples in-memory page layout from data organization on non-volatile storage devices to enable independent data layout design at each level of the storage hierarchy. Clotho can maximize cache and memory utilization by (a) transparently using appropriate data layouts in memory and non-volatile storage, and (b) dynamically synthesizing data pages to follow application access patterns at each level as needed. Clotho creates in-memory pages individually tailored for compound and dynamically changing workloads, and enables efficient use of different storage technologies (e.g., disk arrays or MEMS-based storage devices). This paper describes the Clotho design and prototype implementation and evaluates its performance under a variety of workloads using both disk arrays and simulated MEMS-based storage devices.

#index 1016188
#* Vision paper: enabling privacy for the paranoids
#@ G. Aggarwal;M. Bawa;P. Ganesan;H. Garcia-Molina;K. Kenthapadi;N. Mishra;R. Motwani;U. Srivastava;D. Thomas;J. Widom;Y. Xu
#t 2004
#c 4
#% 1868
#% 23638
#% 67453
#% 91075
#% 102749
#% 164560
#% 228355
#% 261357
#% 286750
#% 287297
#% 287794
#% 287795
#% 300184
#% 319353
#% 330621
#% 340475
#% 348144
#% 350873
#% 389077
#% 536163
#% 576761
#% 654448
#% 664539
#% 664654
#% 781216
#% 993943
#% 1015331
#! P3P [23, 24] is a set of standards that allow corporations to declare their privacy policies. Hippocratic Databases [6] have been proposed to implement such policies within a corporation's datastore. From an end-user individual's point of view, both of these rest on an uncomfortable philosophy of trusting corporations to protect his/her privacy. Recent history chronicles several episodes when such trust has been willingly or accidentally violated by corporations facing bankruptcy courts, civil subpoenas or lucrative mergers. We contend that data management solutions for information privacy must restore controls in the individual's hands. We suggest that enabling such control will require a radical re-think on modeling, release, and management of personal data.

#index 1016189
#* A privacy-preserving index for range queries
#@ Bijit Hore;Sharad Mehrotra;Gene Tsudik
#t 2004
#c 4
#% 70370
#% 115608
#% 297186
#% 300184
#% 300193
#% 333872
#% 333876
#% 333947
#% 340475
#% 346931
#% 369236
#% 397367
#% 482049
#% 488015
#% 495530
#% 575966
#% 577233
#% 593711
#% 664673
#% 725292
#% 963644
#% 993942
#! Database outsourcing is an emerging data management paradigm which has the potential to transform the IT operations of corporations. In this paper we address privacy threats in database outsourcing scenarios where trust in the service provider is limited. Specifically, we analyze the data partitioning (bucketization) technique and algorithmically develop this technique to build privacy-preserving indices on sensitive attributes of a relational table. Such indices enable an untrusted server to evaluate obfuscated range queries with minimal information leakage. We analyze the worst-case scenario of inference attacks that can potentially lead to breach of privacy (e.g., estimating the value of a data element within a small error margin) and identify statistical measures of data privacy in the context of these attacks. We also investigate precise privacy guarantees of data partitioning which form the basic building blocks of our index. We then develop a model for the fundamental privacy-utility tradeoff and design a novel algorithm for achieving the desired balance between privacy and utility (accuracy of range query evaluation) of the index.

#index 1016190
#* Resilient rights protection for sensor streams
#@ Radu Sion;Mikhail Atallah;Sunil Prabhakar
#t 2004
#c 4
#% 191721
#% 378388
#% 379445
#% 539761
#% 583804
#% 654449
#% 745531
#% 993944
#% 993948
#% 993949
#% 993999
#% 1756704
#% 1810493
#! Today's world of increasingly dynamic computing environments naturally results in more and more data being available as fast streams. Applications such as stock market analysis, environmental sensing, web clicks and intrusion detection are just a few of the examples where valuable data is streamed. Often, streaming information is offered on the basis of a non-exclusive, single-use customer license. One major concern, especially given the digital nature of the valuable stream, is the ability to easily record and potentially "re-play" parts of it in the future. If there is value associated with such future re-plays, it could constitute enough incentive for a malicious customer (Mallory) to duplicate segments of such recorded data, subsequently re-selling them for profit. Being able to protect against such infringements becomes a necessity. In this paper we introduce the issue of rights protection for discrete streaming data through watermarking. This is a novel problem with many associated challenges including: operating in a finite window, single-pass, (possibly) high-speed streaming model, surviving natural domain specific transforms and attacks (e.g.extreme sparse sampling and summarizations), while at the same time keeping data alterations within allowable bounds. We propose a solution and analyze its resilience to various types of attacks as well as some of the important expected domain-specific transforms, such as sampling and summarization. We implement a proof of concept software (wms.*) and perform experiments on real sensor data from the NASA Infrared Telescope Facility at the University of Hawaii, to assess encoding resilience levels in practice. Our solution proves to be well suited for this new domain. For example, we can recover an over 97% confidence watermark from a highly down-sampled (e.g. less than 8%) stream or survive stream summarization (e.g. 20%) and random alteration attacks with very high confidence levels, often above 99%.

#index 1016191
#* Reverse kNN search in arbitrary dimensionality
#@ Yufei Tao;Dimitris Papadias;Xiang Lian
#t 2004
#c 4
#% 86950
#% 201876
#% 213975
#% 235114
#% 237205
#% 261733
#% 287466
#% 300163
#% 427199
#% 480661
#% 481956
#% 495433
#% 993999
#! Given a point q, a reverse k nearest neighbor (RkNN) query retrieves all the data points that have q as one of their k nearest neighbors. Existing methods for processing such queries have at least one of the following deficiencies: (i) they do not support arbitrary values of k (ii) they cannot deal efficiently with database updates, (iii) they are applicable only to 2D data (but not to higher dimensionality), and (iv) they retrieve only approximate results. Motivated by these shortcomings, we develop algorithms for exact processing of RkNN with arbitrary values of k on dynamic multidimensional datasets. Our methods utilize a conventional data-partitioning index on the dataset and do not require any pre-computation. In addition to their flexibility, we experimentally verify that the proposed algorithms outperform the existing ones even in their restricted focus.

#index 1016192
#* Gorder: an efficient method for KNN join processing
#@ Chenyi Xia;Hongjun Lu;Beng Chin Ooi;Jing Hu
#t 2004
#c 4
#% 152937
#% 201876
#% 210186
#% 210187
#% 248804
#% 287466
#% 300136
#% 318703
#% 333973
#% 342595
#% 386623
#% 427199
#% 438137
#% 443326
#% 465000
#% 479453
#% 480307
#% 480632
#% 783643
#! An important but very expensive primitive operation of high-dimensional databases is the K-Nearest Neighbor (KNN) similarity join. The operation combines each point of one dataset with its KNNs in the other dataset and it provides more meaningful query results than the range similarity join. Such an operation is useful for data mining and similarity search. In this paper, we propose a novel KNN-join algorithm, called the Gorder (or the G-ordering KNN) join method. Gorder is a block nested loop join method that exploits sorting, join scheduling and distance computation filtering and reduction to reduce both I/O and CPU costs. It sorts input datasets into the G-order and applied the scheduled block nested loop join on the G-ordered data. The distance computation reduction is employed to further reduce CPU cost. It is simple and yet efficient, and handles high-dimensional data efficiently. Extensive experiments on both synthetic cluster and real life datasets were conducted, and the results illustrate that Gorder is an efficient KNN-join method and outperforms existing methods by a wide margin.

#index 1016193
#* Query and update efficient B+-tree based indexing of moving objects
#@ Christian S. Jensen;Dan Lin;Beng Chin Ooi
#t 2004
#c 4
#% 64431
#% 86950
#% 102808
#% 273706
#% 286237
#% 300174
#% 318051
#% 427199
#% 443397
#% 480327
#% 480473
#% 480632
#% 481759
#% 495433
#% 510512
#% 554884
#% 622692
#% 765454
#% 772839
#% 1015305
#% 1015320
#! A number of emerging applications of data management technology involve the monitoring and querying of large quantities of continuous variables, e.g., the positions of mobile service users, termed moving objects. In such applications, large quantities of state samples obtained via sensors are streamed to a database. Indexes for moving objects must support queries efficiently, but must also support frequent updates. Indexes based on minimum bounding regions (MBRs) such as the R-tree exhibit high concurrency overheads during node splitting, and each individual update is known to be quite costly. This motivates the design of a solution that enables the B+ -tree to manage moving objects. We represent moving-object locations as vectors that are timestamped based on their update time. By applying a novel linearization technique to these values, it is possible to index the resulting values using a single B+-tree that partitions values according to their timestamp and otherwise preserves spatial proximity. We develop algorithms for range and k nearest neighbor queries, as well as continuous queries. The proposal can be grafted into existing database systems cost effectively. An extensive experimental study explores the performance characteristics of the proposal and also shows that it is capable of substantially outperforming the R-tree based TPR-tree for both single and concurrent access scenarios.

#index 1016194
#* Indexing large human-motion databases
#@ Eamonn Keogh;Themistoklis Palpanas;Victor B. Zordan;Dimitrios Gunopulos;Marc Cardle
#t 2004
#c 4
#% 86950
#% 172949
#% 213538
#% 333941
#% 379307
#% 397376
#% 398425
#% 398426
#% 398427
#% 398428
#% 427199
#% 443515
#% 464851
#% 477479
#% 479649
#% 480146
#% 577221
#% 631923
#% 632088
#% 632089
#% 659971
#% 662807
#% 727870
#% 729931
#% 993965
#% 1113090
#! Data-driven animation has become the industry standard for computer games and many animated movies and special effects. In particular, motion capture data recorded from live actors, is the most promising approach offered thus far for animating realistic human characters. However, the manipulation of such data for general use and re-use is not yet a solved problem. Many of the existing techniques dealing with editing motion rely on indexing for annotation, segmentation, and re-ordering of the data. Euclidean distance is inappropriate for solving these indexing problems because of the inherent variability found in human motion. The limitations of Euclidean distance stems from the fact that it is very sensitive to distortions in the time axis. A partial solution to this problem, Dynamic Time Warping (DTW), aligns the time axis before calculating the Euclidean distance. However, DTW can only address the problem of local scaling. As we demonstrate in this paper, global or uniform scaling is just as important in the indexing of human motion. We propose a novel technique to speed up similarity search under uniform scaling, based on bounding envelopes. Our technique is intuitive and simple to implement. We describe algorithms that make use of this technique, we perform an experimental analysis with real datasets, and we evaluate it in the context of a motion capture processing system. The results demonstrate the utility of our approach, and show that we can achieve orders of magnitude of speedup over the brute force approach, the only alternative solution currently available.

#index 1016195
#* On the marriage of Lp-norms and edit distance
#@ Lei Chen;Raymond Ng
#t 2004
#c 4
#% 39145
#% 172949
#% 227924
#% 232122
#% 294634
#% 333941
#% 342827
#% 460862
#% 462231
#% 465160
#% 477479
#% 479462
#% 479649
#% 480146
#% 481611
#% 534183
#% 564263
#% 572265
#% 577221
#% 617886
#% 631923
#% 654456
#% 659936
#% 659971
#% 993965
#! Existing studies on time series are based on two categories of distance functions. The first category consists of the Lp-norms. They are metric distance functions but cannot support local time shifting. The second category consists of distance functions which are capable of handling local time shifting but are nonmetric. The first contribution of this paper is the proposal of a new distance function, which we call ERP ("Edit distance with Real Penalty"). Representing a marriage of L1- norm and the edit distance, ERP can support local time shifting, and is a metric. The second contribution of the paper is the development of pruning strategies for large time series databases. Given that ERP is a metric, one way to prune is to apply the triangle inequality. Another way to prune is to develop a lower bound on the ERP distance. We propose such a lower bound, which has the nice computational property that it can be efficiently indexed with a standard B+- tree. Moreover, we show that these two ways of pruning can be used simultaneously for ERP distances. Specifically, the false positives obtained from the B+-tree can be further minimized by applying the triangle inequality. Based on extensive experimentation with existing benchmarks and techniques, we show that this combination delivers superb pruning power and search time performance, and dominates all existing strategies.

#index 1016196
#* Approximate NN queries on streams with guaranteed error/performance bounds
#@ Nick Koudas;Beng Chin Ooi;Kian-Lee Tan;Rui Zhang
#t 2004
#c 4
#% 86950
#% 145895
#% 227939
#% 249321
#% 249322
#% 252608
#% 264161
#% 282552
#% 378388
#% 397380
#% 415957
#% 443517
#% 479649
#% 480304
#% 480632
#% 481956
#% 482831
#% 527026
#% 745466
#% 993959
#% 993999
#% 1015262
#! In data stream applications, data arrive continuously and can only be scanned once as the query processor has very limited memory (relative to the size of the stream) to work with. Hence, queries on data streams do not have access to the entire data set and query answers are typically approximate. While there have been many studies on the k Nearest Neighbors (kNN) problem in conventional multi-dimensional databases, the solutions cannot be directly applied to data streams for the above reasons. In this paper, we investigate the kNN problem over data streams. We first introduce the e-approximate kNN (ekNN) problem that finds the approximate kNN answers of a query point Q such that the absolute error of the k-th nearest neighbor distance is bounded by e. To support ekNN queries over streams, we propose a technique called DISC (aDaptive Indexing on Streams by space-filling Curves). DISC can adapt to different data distributions to either (a) optimize memory utilization to answer ekNN queries under certain accuracy requirements or (b) achieve the best accuracy under a given memory constraint. At the same time, DISC provide efficient updates and query processing which are important requirements in data stream applications. Extensive experiments were conducted using both synthetic and real data sets and the results confirm the effectiveness and efficiency of DISC.

#index 1016197
#* Object fusion in geographic information systems
#@ Catriel Beeri;Yaron Kanza;Eliyahu Safra;Yehoshua Sagiv
#t 2004
#c 4
#% 287358
#% 413786
#% 421046
#% 481935
#% 519420
#% 529310
#! Given two geographic databases, a fusion algorithm should produce all pairs of corresponding objects (i.e., objects that represent the same real-world entity). Four fusion algorithms, which only use locations of objects, are described and their performance is measured in terms of recall and precision. These algorithms are designed to work even when locations are imprecise and each database represents only some of the real-world entities. Results of extensive experimentation are presented and discussed. The tests show that the performance depends on the density of the data sources and the degree of overlap among them. All four algorithms are much better than the current state of the art (i.e., the one-sided nearest-neighbor join). One of these four algorithms is best in all cases, at a cost of a small increase in the running time compared to the other algorithms.

#index 1016198
#* Maintenance of spatial semijoin queries on moving points
#@ Glenn S. Iwerks;Hanan Samet;Kenneth P. Smith
#t 2004
#c 4
#% 68091
#% 74141
#% 152928
#% 201876
#% 248804
#% 282343
#% 287466
#% 300174
#% 427199
#% 442615
#% 461923
#% 481599
#% 495433
#% 574283
#% 579313
#% 1015297
#! In this paper, we address the maintenance of spatial semijoin queries over continuously moving points, where points are modeled as linear functions of time. This is analogous to the maintenance of a materialized view except, as time advances, the query result may change independently of updates. As in a materialized view, we assume there is no prior knowledge of updates before they occur. We present a new approach, continuous fuzzy sets (CFS), to maintain continuous spatial semijoins efficiently. CFS is compared experimentally to a simple scaling of previous work. The result is significantly better performance of CFS compared to previous work by up to an order of magnitude in some cases.

#index 1016199
#* Voronoi-based K nearest neighbor search for spatial network databases
#@ Mohammad Kolahdouzan;Cyrus Shahabi
#t 2004
#c 4
#% 121114
#% 201876
#% 248797
#% 287466
#% 413797
#% 443533
#% 462239
#% 479462
#% 481279
#% 481947
#% 729850
#% 1015321
#! A frequent type of query in spatial networks (e.g., road networks) is to find the K nearest neighbors (KNN) of a given query object. With these networks, the distances between objects depend on their network connectivity and it is computationally expensive to compute the distances (e.g., shortest paths) between objects. In this paper, we propose a novel approach to efficiently and accurately evaluate KNN queries in spatial network databases using first order Voronoi diagram. This approach is based on partitioning a large network to small Voronoi regions, and then pre-computing distances both within and across the regions. By localizing the precomputation within the regions, we save on both storage and computation and by performing across-the-network computation for only the border points of the neighboring regions, we avoid global pre-computation between every node-pair. Our empirical experiments with several real-world data sets show that our proposed solution outperforms approaches that are based on on-line distance computation by up to one order of magnitude, and provides a factor of four improvement in the selectivity of the filter step as compared to the index-based approaches.

#index 1016200
#* A framework for projected clustering of high dimensional data streams
#@ Charu C. Aggarwal;Jiawei Han;Jianyong Wang;Philip S. Yu
#t 2004
#c 4
#% 36672
#% 210173
#% 248790
#% 248792
#% 273890
#% 273891
#% 302724
#% 310488
#% 310500
#% 320942
#% 378388
#% 481281
#% 594012
#% 654489
#% 659943
#% 659972
#% 740767
#% 1015261
#! The data stream problem has been studied extensively in recent years, because of the great ease in collection of stream data. The nature of stream data makes it essential to use algorithms which require only one pass over the data. Recently, single-scan, stream analysis methods have been proposed in this context. However, a lot of stream data is high-dimensional in nature. High-dimensional data is inherently more complex in clustering, classification, and similarity search. Recent research discusses methods for projected clustering over high-dimensional data sets. This method is however difficult to generalize to data streams because of the complexity of the method and the large volume of the data streams. In this paper, we propose a new, high-dimensional, projected data stream clustering method, called HPStream. The method incorporates a fading cluster structure, and the projection based clustering methodology. It is incrementally updatable and is highly scalable on both the number of dimensions and the size of the data streams, and it achieves better clustering quality in comparison with the previous stream clustering methods. Our performance study with both real and synthetic data sets demonstrates the efficiency and effectiveness of our proposed framework and implementation methods.

#index 1016201
#* Efficient query evaluation on probabilistic databases
#@ Nilesh Dalvi;Dan Suciu
#t 2004
#c 4
#% 41230
#% 144840
#% 215225
#% 216970
#% 219033
#% 234797
#% 235023
#% 248801
#% 265692
#% 333679
#% 333854
#% 387427
#% 397406
#% 442830
#% 480028
#% 480102
#% 564993
#% 654442
#% 659990
#% 993987
#! We describe a system that supports arbitrarily complex SQL queries on probabilistic databases. The query semantics is based on a probabilistic model and the results are ranked, much like in Information Retrieval. Our main focus is efficient query evaluation, a problem that has not received attention in the past. We describe an optimization algorithm that can compute efficiently most queries. We show, however, that the data complexity of some queries is #P-complete, which implies that these queries do not admit any efficient evaluation methods. For these queries we describe both an approximation algorithm and a Monte-Carlo simulation algorithm.

#index 1016202
#* Efficient indexing methods for probabilistic threshold queries over uncertain data
#@ Reynold Cheng;Yuni Xia;Sunil Prabhakar;Rahul Shah;Jeffrey Scott Vitter
#t 2004
#c 4
#% 178068
#% 210355
#% 237205
#% 273714
#% 292029
#% 295512
#% 300174
#% 480299
#% 654487
#% 656697
#% 731478
#% 772835
#! It is infeasible for a sensor database to contain the exact value of each sensor at all points in time. This uncertainty is inherent in these systems due to measurement and sampling errors, and resource limitations. In order to avoid drawing erroneous conclusions based upon stale data, the use of uncertainty intervals that model each data item as a range and associated probability density function (pdf) rather than a single value has recently been proposed. Querying these uncertain data introduces imprecision into answers, in the form of probability values that specify the likeliness the answer satisfies the query. These queries are more expensive to evaluate than their traditional counterparts but are guaranteed to be correct and more informative due to the probabilities accompanying the answers. Although the answer probabilities are useful, for many applications, it is only necessary to know whether the probability exceeds a given threshold - we term these Probabilistic Threshold Queries (PTQ). In this paper we address the efficient computation of these types of queries. In particular, we develop two index structures and associated algorithms to efficiently answer PTQs. The first index scheme is based on the idea of augmenting uncertainty information to an R-tree. We establish the difficulty of this problem by mapping one-dimensional intervals to a two-dimensional space, and show that the problem of interval indexing with probabilities is significantly harder than interval indexing which is considered a well-studied problem. To overcome the limitations of this R-tree based structure, we apply a technique we call variance-based clustering, where data points with similar degrees of uncertainty are clustered together. Our extensive index structure can answer the queries for various kinds of uncertainty pdfs, in an almost optimal sense. We conduct experiments to validate the superior performance of both indexing schemes.

#index 1016203
#* Probabilistic ranking of database query results
#@ Surajit Chaudhuri;Gautam Das;Vagelis Hristidis;Gerhard Weikum
#t 2004
#c 4
#% 41230
#% 144070
#% 232136
#% 248010
#% 248801
#% 248857
#% 324129
#% 324192
#% 333854
#% 334001
#% 387427
#% 399762
#% 458860
#% 480302
#% 480330
#% 480418
#% 591565
#% 719598
#% 993954
#% 993957
#% 993987
#% 1650569
#! We investigate the problem of ranking answers to a database query when many tuples are returned. We adapt and apply principles of probabilistic models from Information Retrieval for structured data. Our proposed solution is domain independent. It leverages data and workload statistics and correlations. Our ranking functions can be further customized for different applications. We present results of preliminary experiments which demonstrate the efficiency as well as the quality of our ranking system.

#index 1016204
#* An annotation management system for relational databases
#@ Deepavali Bhagwat;Laura Chiticariu;Wang-Chiew Tan;Gaurav Vijayvargiya
#t 2004
#c 4
#% 137867
#% 186319
#% 209667
#% 239976
#% 301089
#% 309724
#% 318704
#% 330770
#% 378401
#% 384978
#% 566114
#% 567134
#% 654468
#! We present an annotation management system for relational databases. In this system, every piece of data in a relation is assumed to have zero or more annotations associated with it and annotations are propagated along, from the source to the output, as data is being transformed through a query. Such an annotation management system is important for understanding the provenance and quality of data, especially in applications that deal with integration of scientific and biological data. We present an extension, pSQL, of a fragment of SQL that has three different types of annotation propagation schemes, each useful for different purposes. The default scheme propagates annotations according to where data is copied from. The default-all scheme propagates annotations according to where data is copied from among all equivalent formulations of a given query. The custom scheme allows a user to specify how annotations should propagate. We present a storage scheme for the annotations and describe algorithms for translating a pSQL query under each propagation scheme into one or more SQL queries that would correctly retrieve the relevant annotations according to the specified propagation scheme. For the default-all scheme, we also show how we generate finitely many queries that can simulate the annotation propagation behavior of the set of all equivalent queries, which is possibly infinite. The algorithms are implemented and the feasibility of the system is demonstrated by a set of experiments that we have conducted.

#index 1016205
#* Symmetric relations and cardinality-bounded multisets in database systems
#@ Kenneth A. Ross;Julia Stoyanovich
#t 2004
#c 4
#% 91617
#% 189869
#% 368248
#% 464534
#% 480463
#% 482082
#% 482121
#% 654445
#% 654454
#! In a binary symmetric relationship, A is related to B if and only if B is related to A. Symmetric relationships between k participating entities can be represented as multisets of cardinality k. Cardinality-bounded multisets are natural in several real-world applications. Conventional representations in relational databases suffer from several consistency and performance problems. We argue that the database system itself should provide native support for cardinality-bounded multisets. We provide techniques to be implemented by the database engine that avoid the drawbacks, and allow a schema designer to simply declare a table to be symmetric in certain attributes. We describe a compact data structure, and update methods for the structure. We describe an algebraic symmetric closure operator, and show how it can be moved around in a query plan during query optimization in order to improve performance. We describe indexing methods that allow efficient lookups on the symmetric columns. We show how to perform database normalization in the presence of symmetric relations. We provide techniques for inferring that a view is symmetric. We also describe a syntactic SQL extension that allows the succinct formulation of queries over symmetric relations.

#index 1016206
#* Algebraic manipulation of scientific datasets
#@ Bill Howe;David Maier
#t 2004
#c 4
#% 210184
#% 220616
#% 308500
#% 332162
#% 397391
#% 442705
#% 479459
#% 481428
#% 504017
#% 520991
#% 577222
#% 726088
#% 993983
#! We investigate algebraic processing strategies for large numeric datasets equipped with a possibly irregular grid structure. Such datasets arise, for example, in computational simulations, observation networks, medical imaging, and 2-D and 3-D rendering. Existing approaches for manipulating these datasets are incomplete: The performance of SQL queries for manipulating large numeric datasets is not competitive with specialized tools. Database extensions for processing multidimensional discrete data can only model regular, rectilinear grids. Visualization software libraries are designed to process gridded datasets efficiently, but no algebra has been developed to simplify their use and afford optimization. Further, these libraries are data dependent - physical changes to data representation or organization break user programs. In this paper, we present an algebra of grid-fields for manipulating both regular and irregular gridded datasets, algebraic optimization techniques, and an implementation backed by experimental results. We compare our techniques to those of spatial databases and visualization software libraries, using real examples from an Environmental Observation and Forecasting System. We find that our approach can express optimized plans inaccessible to other techniques, resulting in improved performance with reduced programming effort.

#index 1016207
#* Multi-objective query processing for database systems
#@ Wolf-Tilo Balke;Ulrich Güntzer
#t 2004
#c 4
#% 41230
#% 288976
#% 289148
#% 333847
#% 333854
#% 392343
#% 443243
#% 458873
#% 465167
#% 479813
#% 480330
#% 480671
#% 509871
#% 566111
#% 631988
#% 654480
#% 659993
#% 993954
#% 993957
#! Query processing in database systems has developed beyond mere exact matching of attribute values. Scoring database objects and retrieving only the top k matches or Pareto-optimal result sets (skyline queries) are already common for a variety of applications. Specialized algorithms using either paradigm can avoid naïve linear database scans and thus improve scalability. However, these paradigms are only two extreme cases of exploring viable compromises for each user's objectives. To find the correct result set for arbitrary cases of multi-objective query processing in databases we will present a novel algorithm for computing sets of objects that are nondominated with respect to a set of monotonic objective functions. Naturally containing top k and skyline retrieval paradigms as special cases, this algorithm maintains scalability also for all cases in between. Moreover, we will show the algorithm's correctness and instance-optimality in terms of necessary object accesses and how the response behavior can be improved by progressively producing result objects as quickly as possible, while the algorithm is still running.

#index 1016208
#* Lifting the burden of history from adaptive query processing
#@ Amol Deshpande;Joseph M. Hellerstein
#t 2004
#c 4
#% 86949
#% 248793
#% 248795
#% 273911
#% 300167
#% 397353
#% 397372
#% 479938
#% 715955
#% 742047
#% 993948
#% 1015278
#% 1015296
#! Adaptive query processing schemes attempt to re-optimize query plans during the course of query execution. A variety of techniques for adaptive query processing have been proposed, varying in the granularity at which they can make decisions [8]. The eddy [1] is the most aggressive of these techniques, with the flexibility to choose tuple-by-tuple how to order the application of operators. In this paper we identify and address a fundamental limitation of the original eddies proposal: the burden of history in routing. We observe that routing decisions have long-term effects on the state of operators in the query, and can severely constrain the ability of the eddy to adapt over time. We then propose a mechanism we call STAIRs that allows the query engine to manipulate the state stored inside the operators and undo the effects of past routing decisions. We demonstrate that eddies with STAIRs achieve both high adaptivity and good performance in the face of uncertainty, outperforming prior eddy proposals by orders of magnitude.

#index 1016209
#* A combined framework for grouping and order optimization
#@ Thomas Neumann;Guido Moerkotte
#t 2004
#c 4
#% 210169
#% 383546
#% 411554
#% 481288
#% 745443
#% 1015323
#! Since the introduction of cost-based query optimization by Selinger et al. in their seminal paper, the performance-critical role of interesting orders has been recognized. Some algebraic operators change interesting orders (e.g. sort and select), while others exploit them (e.g. merge join). Likewise, Wang and Cherniack (VLDB 2003) showed that existing groupings should be exploited to avoid redundant grouping operations. Ideally, the reasoning about interesting orderings and groupings should be integrated into one framework. So far, no complete, correct, and efficient algorithm for ordering and grouping inference has been proposed. We fill this gap by proposing a general two-phase approach that efficiently integrates the reasoning about orderings and groupings. Our experimental results show that with a modest increase of the time and space requirements of the preprocessing phase both orderings and groupings can be handled at the same time. More importantly, there is no additional cost for the second phase during which the plan generator changes and exploits orderings and groupings by adding operators to subplans.

#index 1016210
#* The case for precision sharing
#@ Sailesh Krishnamurthy;Michael J. Franklin;Joseph M. Hellerstein;Garrett Jacobson
#t 2004
#c 4
#% 36117
#% 58375
#% 77648
#% 190153
#% 300167
#% 300179
#% 322884
#% 333848
#% 397353
#% 993948
#% 993949
#% 1015282
#! Sharing has emerged as a key idea of static and adaptive stream query processing systems. Inherent in these systems is a tension between sharing common work and avoiding unnecessary work. Increased sharing has generally led to more unnecessary work. Our approach of precision sharing aims to share aggressively without unnecessary work. We show why "adaptive" tuple lineage is more generally applicable and use it for precisely shared static dataflows. We also show how "static" ordering constraints can be used for precision sharing in adaptive systems. Finally, we report an experimental study of precision sharing.

#index 1016211
#* Returning modified rows - select statements with side effects
#@ Andreas Behm;Serge Rielau;Richard Swagerman
#t 2004
#c 4
#% 152905
#% 235084
#% 481952
#! SQL in the IBM® DB2® Universal DatabaseTM for Linux®, UNIX®, and Windows® (DB2 UDB) database management product has been extended to support nested INSERT, UPDATE, and DELETE operations in SELECT statements. This allows database applications additional processing on modified rows. Within a single unit of work, applications can retrieve a result set containing the modified rows from a table or view modified by an SQL data-change operation. This eliminates the need to select the row after an INSERT or UPDATE, or before a DELETE statement. As a result, fewer network round trips, less server CPU time, fewer cursors, and less server memory are required. In addition, deadlocks can be avoided. The proposed approach is integrated with the set semantics of SQL, and does not require any procedural logic or modifications on the underlying relational data model. Pipelining multiple update, insert and delete operations using the same source data provides a very efficient way for multitable data-change statements typically found in ETL (extraction, transformation, load) applications. We demonstrate significant performance benefit with our experiences in the TPC-C benchmark. Experimental results show that the new SQL is more efficient in query execution compared to classic SQL.

#index 1016212
#* PIVOT and UNPIVOT: optimization and execution strategies in an RDBMS
#@ Conor Cunningham;César A. Galindo-Legaria;Goetz Graefe
#t 2004
#c 4
#% 248816
#% 334006
#% 420053
#% 479968
#% 480629
#% 654445
#! PIVOT and UNPIVOT, two operators on tabular data that exchange rows and columns, enable data transformations useful in data modeling, data analysis, and data presentation. They can quite easily be implemented inside a query processor, much like select, project, and join. Such a design provides opportunities for better performance, both during query optimization and query execution. We discuss query optimization and execution implications of this integrated design and evaluate the performance of this approach using a prototype implementation in Microsoft SQL Server.

#index 1016213
#* A multi-purpose implementation of mandatory access control in relational database management systems
#@ Walid Rjaibi;Paul Bird
#t 2004
#c 4
#% 102749
#% 164560
#% 236413
#% 241781
#% 261360
#% 320469
#% 379819
#% 443189
#% 462628
#% 480945
#% 488006
#% 993943
#! Mandatory Access Control (MAC) implementations in Relational Database Management Systems (RDBMS) have focused solely on Multilevel Security (MLS). MLS has posed a number of challenging problems to the database research community, and there has been an abundance of research work to address those problems. Unfortunately, the use of MLS RDBMS has been restricted to a few government organizations where MLS is of paramount importance such as the intelligence community and the Department of Defense. The implication of this is that the investment of building an MLS RDBMS cannot be leveraged to serve the needs of application domains where there is a desire to control access to objects based on the label associated with that object and the label associated with the subject accessing that object, but where the label access rules and the label structure do not necessarily match the MLS two security rules and the MLS label structure. This paper introduces a flexible and generic implementation of MAC in RDBMS that can be used to address the requirements from a variety of application domains, as well as to allow an RDBMS to efficiently take part in an end-to-end MAC enterprise solution. The paper also discusses the extensions made to the SQL compiler component of an RDBMS to incorporate the label access rules in the access plan it generates for an SQL query, and to prevent unauthorized leakage of data that could occur as a result of traditional optimization techniques performed by SQL compilers.

#index 1016214
#* Hardware acceleration in commercial databases: a case study of spatial operations
#@ Nagender Bandi;Chengyu Sun;Divyakant Agrawal;Amr El Abbadi
#t 2004
#c 4
#% 41923
#% 172908
#% 279852
#% 287379
#% 324364
#% 479653
#% 527193
#% 629133
#% 629134
#% 654479
#% 1112720
#! Traditional databases have focused on the issue of reducing I/O cost as it is the bottleneck in many operations. As databases become increasingly accepted in areas such as Geographic Information Systems (GIS) and Bioinformatics, commercial DBMS need to support data types for complex data such as spatial geometries and protein structures. These non-conventional data types and their associated operations present new challenges. In particular, the computational cost of some spatial operations can be orders of magnitude higher than the I/O cost. In order to improve the performance of spatial query processing, innovative solutions for reducing this computational cost are beginning to emerge. Recently, it has been proposed that hard-ware acceleration of an off-the-shelf graphics card can be used to reduce the computational cost of spatial operations. However, this proposal is preliminary in that it establishes the feasibility of the hardware assisted approach in a stand-alone setting but not in a real-world commercial database. In this paper we present an architecture to show how hardware acceleration of an off-the-shelf graphics card can be integrated into a popular commercial database to speed up spatial queries. Extensive experimentation with real-world datasets shows that significant improvement in the performance of spatial operations can be achieved with this integration. The viability of this approach underscores the significance of a tighter integration of hardware acceleration into commercial databases for spatial applications.

#index 1016215
#* P*TIME: highly scalable OLTP DBMS for managing update-intensive stream workload
#@ Sang K. Cha;Changbin Song
#t 2004
#c 4
#% 114582
#% 248864
#% 251477
#% 300194
#% 333940
#% 378388
#% 465019
#% 479821
#% 480153
#% 480821
#% 480829
#% 615164
#% 615245
#! Over the past thirty years since the system R and Ingres projects started to lay the foundation for today's RDBMS implementations, the underlying hardware and software platforms have changed dramatically. However, the fundamental RDBMS architecture, especially, the storage engine architecture, largely remains unchanged. While this conventional architecture may suffices for satisfying most of today's applications, its deliverable performance range is far from meeting the so-called growing "real-time enterprise" demand of acquiring and querying high-volume update data streams cost-effectively. P*TIME is a new, memory-centric light-weight OLTP RDBMS designed and built from scratch to deliver orders of magnitude higher scalability on commodity SMP hardware than existing RDBMS implementations, not only in search but also in update performance. Its storage engine layer incorporates our previous innovations for exploiting engine-level microparallelism such as differential logging and optimistic latch-free index traversal concurrency control protocol. This paper presents the architecture and performance of P*TIME and reports our experience of deploying P*TIME as the stock market database server at one of the largest on-line brokerage firms.

#index 1016216
#* Generating thousand benchmark queries in seconds
#@ Meikel Poess;John M. Stephens, Jr.
#t 2004
#c 4
#% 208037
#% 328431
#% 397399
#% 479656
#% 741995
#! The combination of an exponential growth in the amount of data managed by a typical business intelligence system and the increased competitiveness of a global economy has propelled decision support systems (DSS) from the role of exploratory tools employed by a few visionary companies to become a core requirement for a competitive enterprise. That same maturation has often resulted in a selection process that requires an ever more critical system evaluation and selection to be completed in an increasingly short period of time. While there have been some advances in the generation of data sets for system evaluation (see [3]), the quantification of query performance has often relied on models and methodologies that were developed for systems that were more simplistic, less dynamic, and less central to a successful business. In this paper we present QGEN, a flexible, high-level query generator optimized for decision support system evaluation. QGEN is able to generate arbitrary query sets, which conform to a selected statistical profile without requiring that the queries be statically defined or disclosed prior to testing. Its novel design links query syntax with abstracted data distributions, enabling users to parameterize their query workload to match an emerging access pattern or data set modification. This results in query sets that retain comparability for system comparisons while reflecting the inherent dynamism of operational systems, and which provide a broad range of syntactic and semantic coverage, while remaining focused on appropriate commonalities within a particular evaluation process or business segment.

#index 1016217
#* Supporting ontology-based semantic matching in RDBMS
#@ Souripriya Das;Eugene Inseok Chong;George Eadon;Jaannathan Srinivasan
#t 2004
#c 4
#% 225467
#% 287358
#% 480458
#% 509855
#% 519428
#% 559054
#% 632099
#% 740266
#! Ontologies are increasingly being used to build applications that utilize domain-specific knowledge. This paper addresses the problem of supporting ontology-based semantic matching in RDBMS. Specifically, 1) A set of SQL operators, namely ONT_RELATED, ONT_EXPAND, ONT_DISTANCE, and ONT_PATH, are introduced to perform ontology-based semantic matching, 2) A new indexing scheme ONT_INDEXTYPE is introduced to speed up ontology-based semantic matching operations, and 3) System-defined tables are provided for storing ontologies specified in OWL. Our approach enables users to reference ontology data directly from SQL using the semantic match operators, thereby opening up possibilities of combining with other operations such as joins as well as making the ontology-driven applications easy to develop and efficient. In contrast, other approaches use RDBMS only for storage of ontologies and querying of ontology data is typically done via APIs. This paper presents the ontology-related functionality including inferencing, discusses how it is implemented on top of Oracle RDBMS, and illustrates the usage with several database applications.

#index 1016218
#* BioPatentMiner: an information retrieval system for biomedical patents
#@ Sougata Mukherjea;Bhuvan Bamba
#t 2004
#c 4
#% 240201
#% 248869
#% 262061
#% 268073
#% 268079
#% 281396
#% 282905
#% 289315
#% 330707
#% 332081
#% 348181
#% 413641
#% 577372
#% 730063
#! Before undertaking new biomedical research, identifying concepts that have already been patented is essential. Traditional keyword based search on patent databases may not be sufficient to retrieve all the relevant information, especially for the biomedical domain. More sophisticated retrieval techniques are required. This paper presents BioPatentMiner, a system that facilitates information retrieval from biomedical patents. It integrates information from the patents with knowledge from biomedical ontologies to create a Semantic Web. Besides keyword search and queries linking the properties specified by one or more RDF triples, the system can discover Semantic Associations between the resources. The system also determines the importance of the resources to rank the results of a search and prevent information overload while determining the Semantic Associations.

#index 1016220
#* DB2 design advisor: integrated automatic physical database design
#@ Daniel C. Zilio;Jun Rao;Sam Lightstone;Guy Lohman;Adam Storm;Christian Garcia-Arellano;Scott Fadden
#t 2004
#c 4
#% 36119
#% 201956
#% 248815
#% 248855
#% 300138
#% 397390
#% 397397
#% 479985
#% 480158
#% 632100
#% 654495
#% 765431
#% 820356
#% 994004
#% 1016226
#! The DB2 Design Advisor in IBM® DB2® Universal DatabaseTM (DB2 UDB) Version 8.2 for Linux®, UNIX® and Windows® is a tool that, for a given workload, automatically recommends physical design features that are any subset of indexes, materialized query tables (also called materialized views), shared-nothing database partitionings, and multidimensional clustering of tables. Our work is the very first industrial-strength tool that covers the design of as many as four different features, a significant advance to existing tools, which support no more than just indexes and materialized views. Building such a tool is challenging, because of not only the large search space introduced by the interactions among features, but also the extensibility needed by the tool to support additional features in the future. We adopt a novel "hybrid" approach in the Design Advisor that allows us to take important interdependencies into account as well as to encapsulate design features as separate components to lower the reengineering cost. The Design Advisor also features a built-in module that automatically reduces the given workload, and therefore provides great scalability for the tool. Our experimental results demonstrate that our tool can quickly provide good physical design recommendations that satisfy users' requirements.

#index 1016221
#* Automatic SQL tuning in oracle 10g
#@ Benoit Dageville;Dinesh Das;Karl Dias;Khaled Yagoub;Mohamed Zait;Mohamed Ziauddin
#t 2004
#c 4
#% 116043
#% 480803
#% 482100
#% 632100
#% 770354
#! SQL tuning is a very critical aspect of database performance tuning. It is an inherently complex activity requiring a high level of expertise in several domains: query optimization, to improve the execution plan selected by the query optimizer; access design, to identify missing access structures; and SQL design, to restructure and simplify the text of a badly written SQL statement. Furthermore, SQL tuning is a time consuming task due to the large volume and evolving nature of the SQL workload and its underlying data. In this paper we present the new Automatic SQL Tuning feature of Oracle 10g. This technology is implemented as a core enhancement of the Oracle query optimizer and offers a comprehensive solution to the SQL tuning challenges mentioned above. Automatic SQL Tuning introduces the concept of SQL profiling to transparently improve execution plans. It also generates SQL tuning recommendations by performing cost-based access path and SQL structure "what-if" analyses. This feature is exposed to the user through both graphical and command line interfaces. The Automatic SQL Tuning is an integral part of the Oracle's framework for self-managing databases. The superiority of this new technology is demonstrated by comparing the results of Automatic SQL Tuning to manual tuning using a real customer workload.

#index 1016222
#* High performance index build algorithms for intranet search engines
#@ Marcus Fontoura;Engene Shekita;Jason Y. Zien;Sridhar Rajagopalan;Andreas Neumann
#t 2004
#c 4
#% 69317
#% 70370
#% 131555
#% 134526
#% 255137
#% 268079
#% 322884
#% 330706
#% 330769
#% 340887
#% 340928
#% 340932
#% 348180
#% 387427
#% 453464
#% 481439
#% 577318
#% 577339
#% 643069
#% 655485
#% 754125
#% 1015265
#! There has been a substantial amount of research on high-performance algorithms for constructing an inverted text index. However, constructing the inverted index in a intranet search engine is only the final step in a more complicated index build process. Among other things, this process requires an analysis of all the data being indexed to compute measures like PageRank. The time to perform this global analysis step is significant compared to the time to construct the inverted index, yet it has not received much attention in the research literature. In this paper, we describe how the use of slightly outdated information from global analysis and a fast index construction algorithm based on radix sorting can be combined in a novel way to significantly speed up the index build process without sacrificing search quality.

#index 1016223
#* Query rewrite for XML in Oracle XML DB
#@ Muralidhar Krishnaprasad;Zhen Hua Liu;Anand Manikutty;James W. Warner;Vikas Arora;Susan Kotsovolos
#t 2004
#c 4
#% 333935
#% 396729
#% 480657
#% 562456
#% 1015339
#! Oracle XML DB integrates XML storage and querying using the Oracle relational and object relational framework. It has the capability to physically store XML documents by shredding them as relational or object relational data, and creating logical XML documents using SQL/XML publishing functions. However, querying XML in a relational or object relational database poses several challenges. The biggest challenge is to efficiently process queries against XML in a database whose fundamental storage is table-based and whose fundamental query engine is tuple-oriented. In this paper, we present the 'XML Query Rewrite' technique used in Oracle XML DB. This technique integrates querying XML using XPath embedded inside SQL operators and SQL/XML publishing functions with the object relational and relational algebra. A common set of algebraic rules is used to reduce both XML and object queries into their relational equivalent. This enables a large class of XML queries over XML type tables and views to be transformed into their semantically equivalent relational or object relational queries. These queries are then amenable to classical relational optimisations yielding XML query performance comparable to relational. Furthermore, this rewrite technique lays out a foundation to enable rewrite of XQuery [1] over XML.

#index 1016224
#* Indexing XML data stored in a relational database
#@ Shankar Pal;Istvan Cseri;Oliver Seeliger;Gideon Schaller;Leo Giakoumakis;Vasili Zolotov
#t 2004
#c 4
#% 287715
#% 340144
#% 392275
#% 397358
#% 397366
#% 411759
#% 428146
#% 479465
#% 504574
#% 654442
#% 765488
#% 994015
#! As XML usage grows for both data-centric and document-centric applications, introducing native support for XML data in relational databases brings significant benefits. It provides a more mature platform for the XML data model and serves as the basis for interoperability between relational and XML data. Whereas query processing on XML data shredded into one or more relational tables is well understood, it provides limited support for the XML data model. XML data can be persisted as a byte sequence (BLOB) in columns of tables to support the XML model more faithfully. This introduces new challenges for query processing such as the ability to index the XML blob for good query performance. This paper reports novel techniques for indexing XML data in the upcoming version of Microsoft® SQL ServerTM, and how it ties into the relational framework for query processing.

#index 1016225
#* Automated statistics collection in DB2 UDB
#@ A. Aboulnaga;P. Haas;M. Kandil;S. Lightstone;G. Lohman;V. Markl;I. Popivanov;V. Raman
#t 2004
#c 4
#% 210190
#% 273901
#% 333947
#% 397371
#% 427029
#% 443390
#% 480803
#% 582511
#% 760291
#% 765455
#% 1015285
#! The use of inaccurate or outdated database statistics by the query optimizer in a relational DBMS often results in a poor choice of query execution plans and hence unacceptably long query processing times. Configuration and maintenance of these statistics has traditionally been a time-consuming manual operation, requiring that the database administrator (DBA) continually monitor query performance and data changes in order to determine when to refresh the statistics values and when and how to adjust the set of statistics that the DBMS maintains. In this paper we describe the new Automated Statistics Collection (ASC) component of IBM® DB2® Universal DatabaseTM (DB2 UDB). This autonomic technology frees the DBA from the tedious task of manually supervising the collection and maintenance of database statistics. ASC monitors both the update-delete-insert (UDI) activities on the data as well as query feedback (QF), i.e., the results of the queries that are executed on the data. ASC uses these two sources of information to automatically decide which statistics to collect and when to collect them. This combination of UDI-driven and QF-driven autonomic processes ensures that the system can handle unforeseen queries while also ensuring good performance for frequent and important queries. We present the basic concepts, architecture, and key implementation details of ASC in DB2 UDB, and present a case study showing how the use of ASC can speed up a query workload by orders of magnitude without requiring any DBA intervention.

#index 1016226
#* Automated design of multidimensional clustering tables for relational databases
#@ Sam S. Lightstone;Bishwaranjan Bhattacharjee
#t 2004
#c 4
#% 248815
#% 248855
#% 397397
#% 458523
#% 480158
#% 480298
#% 480662
#% 481749
#% 632100
#% 637792
#% 654495
#% 1015335
#% 1016220
#! The ability to physically cluster a database table on multiple dimensions is a powerful technique that offers significant performance benefits in many OLAP, warehousing, and decision-support systems. An industrial implementation of this technique for the DB2® Universal DatabaseTM (DB2 UDB) product, called multidimensional clustering (MDC), which co-exists with other classical forms of data storage and indexing methods, was described in VLDB 2003. This paper describes the first published model for automating the selection of clustering keys in single-dimensional and multidimensional relational databases that use a cell/block storage structure for MDC. For any significant dimensionality (3 or more), the possible solution space is combinatorially complex. The automated MDC design model is based on what-if query cost modeling, data sampling, and a search algorithm for evaluating a large constellation of possible combinations. The model is effective at trading the benefits of potential combinations of clustering keys against data sparsity and performance. It also effectively selects the granularity at which dimensions should be used for clustering (such as week of year versus month of year). We show results from experiments indicating that the model provides design recommendations of comparable quality to those made by human experts. The model has been implemented in the IBM® DB2 UDB for Linux®, UNIX® and Windows® Version 8.2 release.

#index 1016227
#* Integrating automatic data acquisition with business processes experiences with SAP's auto-ID infrastructure
#@ Christof Bornhövd;Tao Lin;Stephan Haller;Joachim Schaper
#t 2004
#c 4
#% 522881
#% 644230
#! Smart item technologies, like RFID and sensor networks, are considered to be the next big step in business process automation [1]. Through automatic and real-time data acquisition, these technologies can benefit a great variety of industries by improving the efficiency of their operations. SAP's Auto-ID infrastructure enables the integration of RFID and sensor technologies with existing business processes. In this paper we give an overview of the existing infrastructure, discuss lessons learned from successful customer pilots, and point out some of the open research issues.

#index 1016228
#* Managing RFID data
#@ Sudarshan S. Chawathe;Venkat Krishnamurthy;Sridhar Ramachandran;Sanjay Sarma
#t 2004
#c 4
#% 45076
#% 223781
#% 440410
#% 481106
#% 632040
#% 725293
#! Radio-Frequency Identification (RFID) technology enables sensors to efficiently and inexpensively track merchandise and other objects. The vast amount of data resulting from the proliferation of RFID readers and tags poses some interesting challenges for data management. We present a brief introduction to RFID technology and highlight a few of the data management challenges.

#index 1016229
#* Production database systems: making them easy is hard work
#@ David Campbell
#t 2004
#c 4
#% 482100
#! Enterprise capable database products have evolved into incredibly complex systems, some of which present hundreds of configuration parameters to the system administrator. So, while the processing and storage costs for maintaining large volumes of data have plummeted, the human costs associated with maintaining the data have continued to rise. In this presentation, we discuss the framework and approach used by the team who took Microsoft SQL Server from a state where it had several hundred configuration parameters to a system that can configure itself and respond to changes in workload and environment with little human intervention.

#index 1016230
#* Managing data from high-throughput genomic processing: a case study
#@ Toby Bloom;Ted Sharpe
#t 2004
#c 4
#! Genomic data has become the canonical example of very large, very complex data sets. As such, there has been significant interest in ways to provide targeted database support to address issues that arise in genomic processing. Whether genomic data is truly a special case, or just another application area exhibiting problems common to other domains, is an as yet unanswered question. In this abstract, we explore the structure and processing requirements of a large-scale genome sequencing center, as a case study of the issues that arise in genomic data managements, and as a means to compare those issues with those that arise in other domains.

#index 1016231
#* Database challenges in the integration of biomedical data sets
#@ Rakesh Nagarajan;Mushtaq Ahmed;Aditya Phatak
#t 2004
#c 4
#% 772131
#% 772132
#! The clinical and basic science research domains present exciting and difficult data integration issues. Solving these problems is crucial as current research efforts in the field of biomedicine heavily depend upon integrated storage, querying, analysis, and visualization of clinicopathology information, genomic annotation, and large scale functional genomic research data sets. Such large scale experimental analyses are essential to decipher the pathophysiological processes occurring in most human diseases so that they may be effectively treated. In this paper, we discuss the challenges of integration of multiple biomedical data sets not only at the university level but also at the national level and present the data warehousing based solution we have employed at Washington University School of Medicine. We also describe the tools we have developed to store, query, analyze, and visualize these data sets together.

#index 1016232
#* The Bloomba personal content database
#@ Raymie Stata;Patrick Hunt;M. G. Thiruvalluvan
#t 2004
#c 4
#% 168251
#% 290703
#% 387508
#% 403195
#% 642983
#! We believe continued growth in the volume of personal content, together with a shift to a multi-device personal computing environment, will inevitably lead to the development of Personal Content Databases (PCDBs). These databases will make it easier for users to find, use, and replicate large, heterogeneous repositories of personal content. In this paper, we describe the PCDB used to power Bloomba, a commercial personal information manager in broad use. We highlight areas where the special requirements of personal content and personal platforms have influenced the design and implementation of our PCDB. We also discuss what we have and have not been able to leverage from the database community and suggest a few lines of research that would be useful to builders of PCDBs.

#index 1016233
#* Trends in data warehousing: a practitioner's view
#@ William O'Connell
#t 2004
#c 4
#! This talk will present emerging data warehousing reference architectures, and focus on trends and directions that are shaping these enterprise installations. Implications will be highlighted, including both of new and old technology. Stack seamless integration is also pivotal to success, which also has significant implications on things such as Metadata.

#index 1016234
#* Technology challenges in a data warehouse
#@ Ramesh Bhashyam
#t 2004
#c 4
#! This presentation will discuss several database technology challenges that are faced when building a data warehouse. It will touch on the challenges posed by high capacity drives and the mechanisms in Teradata DBMS to address that. It will consider the features and capabilities required of a database in a mixed application environment of a warehouse and some solutions to address that.

#index 1016235
#* Sybase IQ multiplex - designed for analytics
#@ Roger MacNicol;Blaine French
#t 2004
#c 4
#! The internal design of database systems has traditionally given primacy to the needs of transactional data. A radical re-evaluation of the internal design giving primacy to the needs of complex analytics shows clear benefits in large databases for both single servers and in multinode shared-disk grid computing. This design supports the trend to keep more years of more finely grained data online by ameliorating the data explosion problem.

#index 1016236
#* Biological data management: research, practice and opportunities
#@ Thodoros Topaloglou;Susan B. Davidson;H. V. Jagadish;Victor M. Markowitz;Evan W. Steeg;Mike Tyers
#t 2004
#c 4

#index 1016237
#* Where is business intelligence taking today's database systems?
#@ William O'Connell;Andy Witkowski;Ramesh Bhashyam;Surajit Chauduri
#t 2004
#c 4

#index 1016238
#* Database architectures for new hardware
#@ Anastassia Ailamaki
#t 2004
#c 4
#! Thirty years ago, DBMS stored data on disks and cached recently used data in main memory buffer pools, while designers worried about improving I/O performance and maximizing main memory utilization. Today, however, databases live in multi-level memory hierarchies that include disks, main memories, and several levels of processor caches. Four (often correlated) factors have shifted the performance bottleneck of data-intensive commercial workloads from I/O to the processor and memory subsystem. First, storage systems are becoming faster and more intelligent (now disks come complete with their own processors and caches). Second, modern database storage managers aggressively improve locality through clustering, hide I/O latencies using prefetching, and parallelize disk accesses using data striping. Third, main memories have become much larger and often hold the application's working set. Finally, the increasing memory/processor speed gap has pronounced the importance of processor caches to database performance. This tutorial will first survey the computer architecture and database literature on understanding and evaluating database application performance on modern hardware. We will present approaches and methodologies used to produce time breakdowns when executing database workloads on modern processors. We will contrast traditional methods that use system simulation to the more realistic, yet challenging use of hardware event counters. Then, we will survey techniques proposed in the literature to alleviate the problem and their evaluation. We will emphasize the importance and explain the challenges when determining the optimal data placement on all levels of memory hierarchy, and contrast to other approaches such as prefetching data and instructions. Finally, we will discuss open problems and future directions: Is it only the memory subsystem database software architects should worry about? How important are other decisions processors make to database workload behavior? Given the emerging multi-threaded, multi-processor computers with modular, deep cache hierarchies, how feasible is it to create database systems that will adapt to their environment and will automatically take full advantage of the underlying hierarchy?

#index 1016239
#* Security of shared data in large systems: state of the art and research directions
#@ Arnon Rosenthal;Marianne Winslett
#t 2004
#c 4
#! The goals of this tutorial are to enlighten the VLDB research community about the state of the art in data security, especially for enterprise or larger systems, and to engage the community's interest in improving the state of the art. The tutorial includes numerous suggested topics for research and development projects in data security.

#index 1016240
#* Self-managing technology in database management systems
#@ Surajit Chaudhuri;Benoit Dageville;Guy Lohman
#t 2004
#c 4

#index 1016241
#* Architectures and algorithms for internet-scale (p2p) data management
#@ Joseph M. Hellerstein
#t 2004
#c 4
#% 337046
#% 340175
#% 496147
#% 723280
#% 960186
#% 963599
#% 1015281

#index 1016242
#* The continued saga of DB-IR integration
#@ Ricardo Baeza-Yates;Mariano Consens
#t 2004
#c 4

#index 1016243
#* GPX: interactive mining of gene expression data
#@ Daxin Jiang;Jian Pei;Aidong Zhang
#t 2004
#c 4
#% 729972
#% 748010
#% 769919
#! Discovering co-expressed genes and coherent expression patterns in gene expression data is an important data analysis task in bioinformatics research and biomedical applications. Although various clustering methods have been proposed, two tough challenges still remain on how to integrate the users' domain knowledge and how to handle the high connectivity in the data. Recently, we have systematically studied the problem and proposed an effective approach [3]. In this paper, we describe a demonstration of GPX (for Gene Pattern eXplorer), an integrated environment for interactive exploration of coherent expression patterns and co-expressed genes in gene expression data. GPX integrates several novel techniques, including the coherent pattern index graph, a gene annotation panel, and a graphical interface, to adopt users' domain knowledge and support explorative operations in the clustering procedure. The GPX system as well as its techniques will be showcased, and the progress of GPX will be exemplified using several real-world gene expression data sets.

#index 1016244
#* Computing frequent itemsets inside oracle 10G
#@ Wei Li;Ari Mozes
#t 2004
#c 4
#% 152934
#% 248813
#% 300124
#% 443091
#% 481754
#% 994014
#! Frequent itemset counting is the first step for most association rule algorithms and some classification algorithms. It is the process of counting the number of occurrences of a set of items that happen across many transactions. The goal is to find those items which occur together most often. Expressing this functionality in RDBMS engines is difficult for two reasons. First, it leads to extremely inefficient execution when using existing RDBMS operations since they are not designed to handle this type of workload. Second, it is difficult to express the special output type of itemsets. In Oracle 10G, we introduce a new SQL table function which encapsulates the work of frequent itemset counting. It accepts the input dataset along with some user-configurable information, and it directly produces the frequent itemset results. We present examples of typical computations with frequent itemset counting inside Oracle 10G. We also describe how Oracle dynamically adapts during frequent itemset execution as a result of changes in the nature of the data as well as changes in the available system resources.

#index 1016245
#* StreamMiner: a classifier ensemble-based engine to mine concept-drifting data streams
#@ Wei Fan
#t 2004
#c 4
#% 333931
#% 342600
#% 342639
#% 378388
#% 397380
#% 428155
#% 594012
#% 727888
#% 729932
#% 769888
#% 993958
#% 1250172
#! We demonstrate StreamMiner, a random decision-tree ensemble based engine to mine data streams. A fundamental challenge in data stream mining applications (e.g., credit card transaction authorization, security buy-sell transaction, and phone call records, etc) is concept-drift or the discrepancy between the previously learned model and the true model in the new data. The basic problem is the ability to judiciously select data and adapt the old model to accurately match the changed concept of the data stream. StreamMiner uses several techniques to support mining over data streams with possible concept-drifts. We demonstrate the following two key functionalities of StreamMiner: 1. Detecting possible concept-drift on the fly when the trained streaming model is used to classify incoming data streams without knowing the ground truth. 2. Systematic data selection of old data and new data chunks to compute the optimal model that best fits on the changing data streams.

#index 1016246
#* Semantic mining and analysis of gene expression data
#@ Xin Xu;Gao Cong;Beng Chin Ooi;Kian-Lee Tan;Anthony K. H. Tung
#t 2004
#c 4
#% 631970
#% 765413
#! Association rules can reveal biological relevant relationship between genes and environments / categories. However, most existing association rule mining algorithms are rendered impractical on gene expression data, which typically contains thousands or tens of thousands of columns (gene expression levels), but only tens of rows (samples). The main problem is that these algorithms have an exponential dependence on the number of columns. Another shortcoming is evident that too many associations are generated from such kind of data. To this end, we have developed a novel depth-first row-wise algorithm FARMER [2] that is specially designed to efficiently discover and cluster association rules into interesting rule groups (IRGs) that satisfy user-specified minimum support, confidence and chi-square value thresholds on biological datasets as opposed to finding association rules individually. Based on FARMER, we have developed a prototype system that integrates semantic mining and visual analysis of IRGs mined from gene expression data.

#index 1016247
#* Hos-Miner: a system for detecting outlyting subspaces of high-dimensional data
#@ Ji Zhang;Meng Lou;Tok Wang Ling;Hai Wang
#t 2004
#c 4
#% 300136
#% 300183
#% 333929
#% 342625
#% 479791
#% 479986
#% 481956
#! We identify a new and interesting high-dimensional outlier detection problem in this paper, that is, detecting the subspaces in which given data points are outliers. We call the subspaces in which a data point is an outlier as its Outlying Subspaces. In this paper, we will propose the prototype of a dynamic subspace search system, called HOS-Miner (HOS stands for High-dimensional Outlying Subspaces), that utilizes a sample-based learning process to effectively identify the outlying subspaces of a given point.

#index 1016248
#* VizTree: a tool for visually mining and monitoring massive time series databases
#@ Jessica Lin;Eamonn Keogh;Stefano Lonardi;Jeffrey P. Lankford;Daonna M. Nystrom
#t 2004
#c 4
#% 662750
#! Moments before the launch of every space vehicle, engineering discipline specialists must make a critical go/no-go decision. The cost of a false positive, allowing a launch in spite of a fault, or a false negative, stopping a potentially successful launch, can be measured in the tens of millions of dollars, not including the cost in morale and other more intangible detriments. The Aerospace Corporation is responsible for providing engineering assessments critical to the go/no-go decision for every Department of Defense (DoD) launch vehicle. These assessments are made by constantly monitoring streaming telemetry data in the hours before launch. For this demonstration, we will introduce VizTree, a novel time-series visualization tool to aid the Aerospace analysts who must make these engineering assessments. VizTree was developed at the University of California, Riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry. Unlike other time series visualization tools, VizTree can scale to very large databases, giving it the potential to be a generally useful data mining and database tool.

#index 1016249
#* An electronic patient record "on steroids": distributed, peer-to-peer, secure and privacy-conscious
#@ Serge Abiteboul;Bogdan Alexe;Omar Benjelloun;Bogdan Cautis;Irini Fundulaki;Tova Milo;Arnaud Sahuguet
#t 2004
#c 4
#% 344639
#% 379248
#% 654465
#% 765420
#% 765507
#% 993943
#% 994034
#% 1015350
#% 1015358

#index 1016250
#* Queries and updates in the coDB peer to peer database system
#@ Enrico Franconi;Gabriel Kuper;Andrei Lopatenko;Ilya Zaihrayeu
#t 2004
#c 4
#% 360802
#% 378409
#% 465057
#% 465063
#% 654468
#% 723446
#% 801692
#% 1392016
#% 1712581
#! In this short paper we present the coDB P2P DB system. A network of databases, possibly with different schemas, are interconnected by means of GLAV coordination rules, which are inclusions of conjunctive queries, with possibly existential variables in the head; coordination rules may be cyclic. Each node can be queried in its schema for data, which the node can fetch from its neighbours, if a coordination rule is involved.

#index 1016251
#* A-ToPSS: a publish/subscribe system supporting imperfect information processing
#@ Haifeng Liu;Hans-Arno Jacobsen
#t 2004
#c 4
#% 25443
#% 745469
#% 994039

#index 1016252
#* Efficient constraint processing for highly personalized location based services
#@ Zhengdao Xu;Hans-Arno Jacobsen
#t 2004
#c 4
#% 273706
#% 299979
#% 300174
#% 333938
#% 564133
#% 654478
#% 1831231

#index 1016253
#* LH*RS: a highly available distributed data storage
#@ Witold Litwin;Rim Moussa;Thomas J. E. Schwarz
#t 2004
#c 4
#% 43172
#% 54037
#% 237822
#% 300165
#% 496160
#! The ideal storage system is always available and incrementally expandable. Existing storage systems fall far from this ideal. Affordable computers and high-speed networks allow us to investigate storage architectures closer to the ideal. Our demo, present a prototype implementation of LH*RS: a highly available scalable and distributed data structure.

#index 1016254
#* Semantic query optimization in an automata-algebra combined XQuery engine over XML streams
#@ Hong Su;Elke A. Rundensteiner;Murali Mani
#t 2004
#c 4
#% 333989
#% 462235
#% 570880
#% 654476
#% 654477
#% 654513
#% 730045
#% 993950
#% 1015276

#index 1016255
#* ShreX: managing XML documents in relational databases
#@ Fang Du;Sihem Amer-Yahia;Juliana Freire
#t 2004
#c 4
#% 273922
#% 413651
#% 479956
#% 504574
#% 654514
#% 659924
#% 665391
#% 745518
#! We describe ShreX, a freely-available system for shredding, loading and querying XML documents in relational databases. ShreX supports all mapping strategies proposed in the literature as well as strategies available in commercial RDBMSs. It provides generic (mapping-independent) functions for loading shredded documents into relations and for translating XML queries into SQL. ShreX is portable and can be used with any relational database backend.

#index 1016256
#* A uniform system for publishing and maintaining XML data
#@ Byron Choi;Wenfei Fan;Xibei Jia;Arek Kasprzyk
#t 2004
#c 4
#% 411759
#% 653704
#% 654464
#% 765443
#% 993941
#% 994001

#index 1016257
#* An injection with tree awareness: adding staircase join to postgreSQL
#@ Sabine Mayer;Torsten Grust;Maurice van Keulen;Jens Teubner
#t 2004
#c 4
#% 397358
#% 1015298

#index 1016258
#* FluXQuery: an optimizing XQuery processor for streaming XML data
#@ Christoph Koch;Stefanie Scherzinger;Nicole Schweikardt;Bernhard Stegmaier
#t 2004
#c 4
#% 413563
#% 465061
#% 480296
#% 654476
#% 654507
#% 659995
#% 993950
#% 1015266
#% 1015272
#% 1016148

#index 1016259
#* COMPASS: a concept-based web search engine for HTML, XML, and deep web data
#@ Jens Graupmann;Michael Biwer;Christian Zimmer;Patrick Zimmer;Matthias Bender;Martin Theobald;Gerhard Weikum
#t 2004
#c 4
#% 397415
#% 458829
#% 480648
#% 654442
#% 1015284

#index 1016260
#* Discovering and ranking semantic associations over a Large RDF metabase
#@ Chris Halaschek;Boanerges Aleman-Meza;I. Budak Arpinar;Amit P. Sheth
#t 2004
#c 4
#% 413603
#% 434033
#% 452869
#% 577372
#! Information retrieval over semantic metadata has recently received a great amount of interest in both industry and academia. In particular, discovering complex and meaningful relationships among this data is becoming an active research topic. Just as ranking of documents is a critical component of today's search engines, the ranking of relationships will be essential in tomorrow's semantic analytics engines. Building upon our recent work on specifying these semantic relationships, which we refer to as Semantic Associations, we demonstrate a system where these associations are discovered among a large semantic metabase represented in RDF. Additionally we employ ranking techniques to provide users with the most interesting and relevant results.

#index 1016261
#* An automatic data grabber for large web sites
#@ Valter Crescenzi;Giansalvatore Mecca;Paolo Merialdo;Paolo Missier
#t 2004
#c 4
#% 281251
#% 330784
#% 397415
#% 413658
#% 480479
#% 480824
#% 482516
#% 654469
#% 729628
#! We demonstrate a system to automatically grab data from data intensive web sites. The system first infers a model that describes at the intensional level the web site as a collection of classes; each class represents a set of structurally homogeneous pages, and it is associated with a small set of representative pages. Based on the model a library of wrappers, one per class, is then inferred, with the help an external wrapper generator. The model, together with the library of wrappers, can thus be used to navigate the site and extract the data.

#index 1016262
#* WS-CatalogNet: an infrastructure for creating, peering, and querying e-catalog communities
#@ Karim Baïna;Boualem Benatallah;Hye-young Paik;Farouk Toumani;Christophe Rey;Agnieszka Rutkowska;Bryan Harianto
#t 2004
#c 4
#% 800007

#index 1016263
#* Trust-Serv: a lightweight trust negotiation service
#@ Halvard Skogsrud;Boualem Benatallah;Fabio Casati;Manh Q. Dinh
#t 2004
#c 4
#% 342328
#% 722477
#% 754063
#% 994025

#index 1016264
#* Green query optimization: taming query optimization overheads through plan recycling
#@ Parag Sarda;Jayant R. Haritsa
#t 2004
#c 4
#% 654518
#% 993946
#! PLASTIC [1] is a recently-proposed tool to help query optimizers significantly amortize optimization overheads through a technique of plan recycling. The tool groups similar queries into clusters and uses the optimizer-generated plan for the cluster representative to execute all future queries assigned to the cluster. An earlier demo [2] had presented a basic prototype implementation of PLASTIC. We have now significantly extended the scope, useability, and efficiency of PLASTIC, by incorporating a variety of new features, including an enhanced query feature vector, variable-sized clustering and a decision-tree-based query classifier. The demo of the upgraded PLASTIC tool is shown on commercial database platforms (IBM DB2 and Oracle 9i).

#index 1016265
#* Progressive optimization in action
#@ Vijayshankar Raman;Volker Markl;David Simmen;Guy Lohman;Hamid Pirahesh
#t 2004
#c 4
#% 765456
#! Progressive Optimization (POP) is a technique to make query plans robust, and minimize need for DBA intervention, by repeatedly re-optimizing a query during runtime if the cardinalities estimated during optimization prove to be significantly incorrect. POP works by carefully calculating validity ranges for each plan operator under which the overall plan can be optimal. POP then instruments the query plan with checkpoints that validate at runtime that cardinalities do lie within validity ranges, and re-optimizes the query otherwise. In this demonstration we showcase POP implemented for a research prototype version of IBM's DB2 DBMS, using a mix of real-world and synthetic benchmark databases and workloads. For selected queries of the workload we display the query plans with validity ranges as well as the placement of the various kinds of CHECK operators using the DB2 graphical plan explain tool. We also execute the queries, showing how and where re-optimization is triggered through the CHECK operators, the new plan generated upon re-optimization, and the extent to which previously computed intermediate results are reused.

#index 1016266
#* CORDS: automatic generation of correlation statistics in DB2
#@ Ihab F. Ilyas;Volker Markl;Peter J. Haas;Paul G. Brown;Ashraf Aboulnaga
#t 2004
#c 4
#% 765455
#! When query optimizers erroneously assume that database columns are statistically independent, they can underestimate the selectivities of conjunctive predicates by orders of magnitude. Such underestimation often leads to drastically suboptimal query execution plans. We demonstrate cords, an efficient and scalable tool for automatic discovery of correlations and soft functional dependencies between column pairs. We apply cords to real, synthetic, and TPC-H benchmark data, and show that cords discovers correlations in an efficient and scalable manner. The output of cords can be visualized graphically, making cords a useful mining and analysis tool for database administrators. cords ranks the discovered correlated column pairs and recommends to the optimizer a set of statistics to collect for the "most important" of the pairs. Use of these statistics speeds up processing times by orders of magnitude for a wide range of queries.

#index 1016267
#* CHICAGO: a test and evaluation environment for coarse-grained optimization
#@ Tobias Kraft;Holger Schwarz
#t 2004
#c 4
#% 36117
#% 116043
#% 411750
#% 480268
#% 1015295
#! Relational OLAP tools and other database applications generate sequences of SQL statements that are sent to the database server as result of a single information request issued by a user. Coarse-Grained Optimization is a practical approach for the optimization of such statement sequences based on rewrite rules. In this demonstration we present the CHICAGO test and evaluation environment that allows to assess the effectiveness of rewrite rules and control strategies. It includes a lightweight heuristic optimizer that modifies a given statement sequence using a small and variable set of rewrite rules.

#index 1016268
#* SVT: schema validation tool for microsoft SQL-server
#@ Ernest Teniente;Carles Farré;Toni Urpí;Carlos Beltrán;David Gañán
#t 2004
#c 4
#% 459009
#% 572310
#! We present SVT, a tool for validating database schemas in SQL Server. This is done by means of testing desirable properties that a database schema should satisfy. To our knowledge, no commercial relational DBMS provides yet a tool able to perform such kind of validation.

#index 1016269
#* CAPE: continuous query engine with heterogeneous-grained adaptivity
#@ Elke A. Rundensteiner;Luping Ding;Timothy Sutherland;Yali Zhu;Brad Pielech;Nishant Mehta
#t 2004
#c 4
#% 578391
#% 654462
#% 726621
#% 736391
#% 765437

#index 1016270
#* HiFi: a unified architecture for high fan-in systems
#@ Owen Cooper;Anil Edakkunni;Michael J. Franklin;Wei Hong;Shawn R. Jeffery;Sailesh Krishnamurthy;Fredrick Reiss;Shariq Rizvi;Eugene Wu
#t 2004
#c 4
#% 654482
#! Advances in data acquisition and sensor technologies are leading towards the development of "High Fan-in" architectures: widely distributed systems whose edges consist of numerous receptors such as sensor networks and RFID readers and whose interior nodes consist of traditional host computers organized using the principle of successive aggregation. Such architectures pose significant new data management challenges. The HiFi system, under development at UC Berkeley, is aimed at addressing these challenges. We demonstrate an initial prototype of HiFi that uses data stream query processing to acquire, filter, and aggregate data from multiple devices including sensor motes, RFID readers, and low power gateways organized as a High Fan-in system.

#index 1016271
#* An integration framework for sensor networks and data stream management systems
#@ Daniel J. Abadi;Wolfgang Lindner;Samuel Madden;Jörg Schuler
#t 2004
#c 4
#% 297915
#% 309433
#% 654482
#% 726621
#% 1394365
#! This demonstration shows an integrated query processing environment where users can seamlessly query both a data stream management system and a sensor network with one query expression. By integrating the two query processing systems, the optimization goals of the sensor network (primarily power) and server network (primarily latency and quality) can be unified into one quality of service metric. The demo shows various steps of the unified optimization process for a sample query where the effects of each step that the optimizer takes can be directly viewed using a quality of service monitor. Our demo includes sensors deployed in the demo area in a tiny mockup of a factory application.

#index 1016272
#* QStream: deterministic querying of data streams
#@ Sven Schmidt;Henrike Berthold;Wolfgang Lehner
#t 2004
#c 4
#% 611014
#% 639297
#% 654497
#% 1015324
#! Current developments in processing data streams are based on the best-effort principle and therefore not adequate for many application areas. When sensor data is gathered by interface hardware and is used for triggering data-dependent actions, the data has to be queried and processed not only in an efficient but also in a deterministic way. Our streaming system prototype embodies novel data processing techniques. It is based on an operator component model and runs on top of a real-time capable environment. This enables us to provide real Quality-of-Service for data stream queries.

#index 1016273
#* AIDA: an adaptive immersive data analyzer
#@ Mehdi Sharifzadeh;Cyrus Shahabi;Bahareh Navai;Farid Parvini;Albert A. Rizzo
#t 2004
#c 4
#% 762580
#! In this demonstration, we show various querying capabilities of an application called AIDA. AIDA is developed to help the study of attention disorder in kids. In a different study [1], we collected several immresive sensory data streams from kids monitored in an immersive application called the virtual classroom. This dataset, termed immersidata is used to analyze the behavior of kids in the virtual classroom environment. AIDA's database stores all the geometry of the objects in the virtual classroom environment and their spatio-temporal behavior. In addition, it stores all the immersidata collected from the kids experimenting with the application. AIDA's graphical user interface then supports various spatio-temporal queries on these datasets. Moreover, AIDA replays the immersidata streams as if they are collected in real-time and on which supports various continuous queries. This demonstration is a proof-of-concept prototype of a typical design and development of a domain-specific query and analysis application on the users' interaction data with immersive environments.

#index 1016274
#* BilVideo video database management system
#@ Özgür Ulusoy;Uğur Güdükbay;Mehmet Emin Dönderler;Ediz Saykol;Cemil Alper
#t 2004
#c 4
#% 452675
#% 575697
#% 733375
#% 823280
#! A prototype video database management system, which we call BilVideo, is presented. BilVideo provides an integrated support for queries on spatio-temporal, semantic and low-level features (color, shape, and texture) on video data. BilVideo does not target a specific application, and thus, it can be used to support any application with video data. An example application, news archives search system, is presented with some sample queries.

#index 1016275
#* PLACE: a query processor for handling real-time spatio-temporal data streams
#@ Mohamed F. Mokbel;Xiaopeng Xiong;Walid G. Aref;Susanne E. Hambrusch;Sunil Prabhakar;Moustafa A. Hammad
#t 2004
#c 4
#% 245996
#% 300179
#% 567868
#% 729864
#% 745434
#% 765166
#% 765453
#% 993948
#% 1015279
#% 1712547
#! The emergence of location-aware services calls for new real-time spatio-temporal query processing algorithms that deal with large numbers of mobile objects and queries. In this demo, we present PLACE (Pervasive Location-Aware Computing Environments); a scalable location-aware database server developed at Purdue University. The PLACE server addresses scalability by adopting an incremental evaluation mechanism for answering concurrently executing continuous spatio-temporal queries. The PLACE server supports a wide variety of stationery and moving continuous spatio-temporal queries through a set of pipelined spatio-temporal operators. The large numbers of moving objects generate real-time spatio-temporal data streams.

#index 1022199
#* Proceedings of the 33rd international conference on Very large data bases
#@ Wolfgang Klas;Erich J. Neuhold
#t 2007
#c 4
#! It is our pleasure to present to you the Proceedings of the 33rd International Conference on Very Large Data Bases (VLDB), taking place in Vienna, Austria. The annual VLDB Conference is the premier international venue for the dissemination and exchange of ideas in the broad area of data management, both in core database technology and the application of this technology to develop infrastructure for information systems. For the first time this year, the conference takes place on three days instead of three and a half. Each day starts out with a keynote, on Tuesday, September 25 by Werner Vogels, CTO of Amazon.com, on Wednesday, September 26 by Eric Brewer, Professor at the University of California-Berkeley and Director of Intel Research Berkeley, and on Thursday, September 27 with the VLDB 10-year best paper award presentation. The program committee that selected the papers consisted of 74 members for the Core Database Technology track, 65 members for the Infrastructure for Information Systems track, 21 members for the Industrial, Applications, and Experience track, and 15 members for the Demonstrations track. We accepted all papers that the program committee recommended as being of VLDB quality without setting any a priori quotas. The conference received 668 submissions overall, including 594 to the three paper tracks. The Core Database technology track received 263 submissions out of which 46 (17.5%) were accepted; the Infrastructure for Information Systems track received 275 submissions out of which 45 (16.4%) were accepted; the Industrial, Applications, and Experience track received 56 submissions out of which 17 (30.4%) were accepted; and the Demonstrations track received 74 submissions out of which 29 (39.2%) were accepted. Compared to last year this shows that the community's rapid growth seems to be slowing: Submissions to the Core Database Technology track were down 21.3%, to the Infrastructure for Information Systems track 5.8%, to the Industrial, Applications, and Experience track 5.1%, and to the Demonstrations track 11.9%. VLDB 2007 participated in the rollover process from SIGMOD 2007, and we received 28 submissions through this process (the rollover numbers are included in the totals above). Rollover papers were due two weeks after the regular submission deadline to give the authors additional time to address the comments of the SIGMOD reviewers. The rollover papers were re-reviewed by their three original reviewers from SIGMOD 2007 with one additional reviewer from VLDB 2007, resulting in 26 additional rollover reviewers for the Core Database Technology track and 21 additional reviewers for the Infrastructure for Information Systems track. 17 papers out of the rollover submissions were accepted (60.7%). We thank the SIGMOD 2007 PC Chair, Beng Chin Ooi, for his support and advice during the rollover process, and we thank the 47 selected reviewers from SIGMOD 2007 for participating in this additional round of reviews. We offered 36 papers that were rejected from VLDB 2007 the option to participate in the rollover process to SIGMOD 2008.

#index 1022200
#* Data access patterns in the Amazon.com technology platform
#@ Werner Vogels
#t 2007
#c 4
#! The Amazon.com technology platform provides a set of highly advanced business and infrastructure services implemented using ultra-scalable distributed systems technologies. Within this environment we can identify a number of specific data access patterns, each with their own availability, consistency, performance and operational requirements in order to serve a collection of highly diverse business processes. In this presentation we will reviews these different patterns in detail and discuss which technologies are required to support them in an always-on environment.

#index 1022201
#* Technology for developing regions
#@ Eric Brewer
#t 2007
#c 4
#! Moore's Law and the wave of technologies it enabled have led to tremendous improvements in productivity and the quality of life in industrialized nations. Yet, technology has had almost no effect on the other five billion people on our planet. In this talk I argue that decreasing costs of computing and wireless networking make this the right time to spread the benefits of technology, and that the biggest missing piece is a lack of focus on the problems that matter. After covering some example applications that have shown very high impact, I present some our own preliminary results, including the use of novel low-cost telemedicine to improve the vision of real people, with over 20,000 patients examined so far. I conclude with some discussion on the role of database researchers in this new area.

#index 1022202
#* Self-tuning database systems: a decade of progress
#@ Surajit Chaudhuri;Vivek Narasayya
#t 2007
#c 4
#% 36119
#% 170893
#% 172902
#% 248793
#% 248815
#% 248821
#% 273901
#% 300167
#% 333947
#% 346894
#% 397371
#% 397390
#% 411554
#% 458523
#% 459950
#% 480153
#% 480158
#% 480803
#% 481290
#% 481444
#% 482100
#% 566118
#% 631950
#% 632048
#% 632100
#% 745441
#% 750953
#% 765176
#% 765425
#% 765431
#% 765456
#% 765467
#% 765468
#% 778724
#% 800589
#% 810026
#% 810027
#% 810056
#% 810111
#% 820356
#% 864426
#% 875027
#% 875062
#% 893130
#% 893217
#% 919529
#% 960268
#% 993933
#% 994014
#% 1015334
#% 1015367
#% 1016220
#% 1016221
#% 1016225
#% 1016240
#% 1026989
#% 1688268
#% 1688297
#! In this paper we discuss advances in self-tuning database systems over the past decade, based on our experience in the AutoAdmin project at Microsoft Research. This paper primarily focuses on the problem of automated physical database design. We also highlight other areas where research on self-tuning database technology has made significant progress. We conclude with our thoughts on opportunities and open issues.

#index 1022203
#* Probabilistic skylines on uncertain data
#@ Jian Pei;Bin Jiang;Xuemin Lin;Yidong Yuan
#t 2007
#c 4
#% 82346
#% 288976
#% 289148
#% 321455
#% 480671
#% 654480
#% 654487
#% 800555
#% 810024
#% 824670
#% 824671
#% 824672
#% 824728
#% 849816
#% 864394
#% 864452
#% 864453
#% 875011
#% 875012
#% 875025
#% 893150
#% 993954
#% 1016201
#% 1016202
#% 1669490
#% 1688273
#% 1700131
#% 1720764
#! Uncertain data are inherent in some important applications. Although a considerable amount of research has been dedicated to modeling uncertain data and answering some types of queries on uncertain data, how to conduct advanced analysis on uncertain data remains an open problem at large. In this paper, we tackle the problem of skyline analysis on uncertain data. We propose a novel probabilistic skyline model where an uncertain object may take a probability to be in the skyline, and a p-skyline contains all the objects whose skyline probabilities are at least p. Computing probabilistic skylines on large uncertain data sets is challenging. We develop two efficient algorithms. The bottom-up algorithm computes the skyline probabilities of some selected instances of uncertain objects, and uses those instances to prune other instances and uncertain objects effectively. The top-down algorithm recursively partitions the instances of uncertain objects into subsets, and prunes subsets and objects aggressively. Our experimental results on both the real NBA player data set and the benchmark synthetic data sets show that probabilistic skylines are interesting and useful, and our two algorithms are efficient on large data sets, and complementary to each other in performance.

#index 1022204
#* Matching twigs in probabilistic XML
#@ Benny Kimelfeld;Yehoshua Sagiv
#t 2007
#c 4
#% 39702
#% 115964
#% 172933
#% 209725
#% 213983
#% 237180
#% 265692
#% 442830
#% 465044
#% 480102
#% 576099
#% 809242
#% 893149
#% 949368
#% 977012
#% 977013
#% 977014
#% 993985
#% 1016201
#% 1661436
#% 1661448
#% 1688305
#% 1700137
#! Evaluation of twig queries over probabilistic XML is investigated. Projection is allowed and, in particular, a query may be Boolean. It is shown that for a well-known model of probabilistic XML, the evaluation of twigs with projection is tractable under data complexity (whereas in other probabilistic data models, projection is intractable). Under query-and-data complexity, the problem becomes intractable even without projection (and for rather simple twigs and data). In earlier work on probabilistic XML, answers are always complete. However, there is often a need to produce partial answers because XML data may have missing sub-elements and, furthermore, complete answers may be deemed irrelevant if their probabilities are too low. It is shown how to define a semantics that provides partial answers that are maximal with respect to a probability threshold, which is specified by the user. For this semantics, it is shown how to efficiently evaluate twigs, even under query-and-data complexity if there is no projection.

#index 1022205
#* OLAP over imprecise data with domain constraints
#@ Doug Burdick;AnHai Doan;Raghu Ramakrishnan;Shivakumar Vaithyanathan
#t 2007
#c 4
#% 663
#% 11817
#% 77979
#% 172952
#% 223781
#% 341100
#% 378403
#% 384978
#% 420053
#% 430773
#% 477212
#% 503731
#% 582130
#% 667501
#% 729449
#% 783532
#% 824733
#% 864417
#% 879041
#% 893121
#% 893168
#% 1016201
#% 1022235
#% 1673674
#! Several recent papers have focused on OLAP over imprecise data, where each fact can be a region, instead of a point, in a multi-dimensional space. They have provided a multiple-world semantics for such data, and developed efficient ways to answer OLAP aggregation queries over the imprecise facts. These solutions, however, assume that the imprecise facts can be interpreted independently of one another, a key assumption that is often violated in practice. Indeed, imprecise facts in real-world applications are often correlated, and such correlations can be captured as domain integrity constraints (e.g., repairs with the same customer names and models took place in the same city, or a text span can refer to a person or a city, but not both). In this paper we provide a framework for answering OLAP aggregation queries over imprecise data in the presence of such domain constraints. We first describe a relatively simple yet powerful constraint language, and formalize what it means to take into account such constraints in query answering. Next, we prove that OLAP queries can be answered efficiently given a database D* of fact marginals. We then exploit the regularities in the constraint space (captured in a constraint hypergraph) and the fact space to efficiently construct D*. We present extensive experiments over real-world and synthetic data to demonstrate the effectiveness of our approach.

#index 1022206
#* Materialized views in probabilistic databases: for information exchange and query optimization
#@ Christopher Ré;Dan Suciu
#t 2007
#c 4
#% 235023
#% 248038
#% 333965
#% 384978
#% 442830
#% 464056
#% 464717
#% 480670
#% 564416
#% 572307
#% 572311
#% 577523
#% 654487
#% 754068
#% 765449
#% 810098
#% 864394
#% 864417
#% 874893
#% 875015
#% 893168
#% 976984
#% 1016178
#% 1016201
#% 1030782
#% 1250397
#! Views over probabilistic data contain correlations between tuples, and the current approach is to capture these correlations using explicit lineage. In this paper we propose an alternative approach to materializing probabilistic views, by giving conditions under which a view can be represented by a block-independent disjoint (BID) table. Not all views can be represented as BID tables and so we propose a novel partial representation that can represent all views but may not define a unique probability distribution. We then give conditions on when a query's value on a partial representation will be uniquely defined. We apply our theory to two applications: query processing using views and information exchange using views. In query processing on probabilistic data, we can ignore the lineage and use materialized views to more efficiently answer queries. By contrast, if the view has explicit lineage, the query evaluation must reprocess the lineage to compute the query resulting in dramatically slower execution. The second application is information exchange when we do not wish to disclose the entire lineage, which otherwise may result in shipping the entire database. The paper contains several theoretical results that completely solve the problem of deciding whether a conjunctive view can be represented as a BID and whether a query on a partial representation is uniquely determined. We validate our approach experimentally showing that representable views exist in real and synthetic workloads and show over three magnitudes of improvement in query processing versus a lineage based approach.

#index 1022207
#* LCS-TRIM: dynamic programming meets XML indexing and querying
#@ Shirish Tatikonda;Srinivasan Parthasarathy;Matthew Goyder
#t 2007
#c 4
#% 288885
#% 289101
#% 333981
#% 397358
#% 397359
#% 397360
#% 397375
#% 410276
#% 479465
#% 480489
#% 480656
#% 504578
#% 577218
#% 617211
#% 654450
#% 659999
#% 745477
#% 749404
#% 783787
#% 800535
#% 824668
#% 838493
#% 993953
#% 1015277
#% 1669487
#% 1698885
#% 1716933
#! In this article, we propose a new approach for querying and indexing a database of trees with specific applications to XML datasets. Our approach relies on representing both the queries and the data using a sequential encoding and then subsequently employing an innovative variant of the longest common subsequence (LCS) matching algorithm to retrieve the desired results. A key innovation here is the use of a series of inter-linked early pruning steps, coupled with a simple index structure that enable us to reduce the search space and eliminate a large number of false positive matches prior to applying the more expensive LCS matching algorithm. Additionally, we also present mechanisms that enable the user to specify constraints on the retrieved output and show how such constraints can be pushed deep into the retrieval process, leading to improved response times. Mechanisms supporting the retrieval of approximate matches are also supported. When compared with state-of-the-art approaches, the query processing time of our algorithms is shown to be up to two to three orders of magnitude faster on several real datasets on realistic query workloads. Finally, we show that our approach is suitable for emerging multi-core server architectures when retrieving data for more expensive queries.

#index 1022208
#* Extending XQuery with window functions
#@ Irina Botan;Donald Kossmann;Peter M. Fischer;Tim Kraska;Dana Florescu;Rokas Tamosevicius
#t 2007
#c 4
#% 3888
#% 116043
#% 136740
#% 210166
#% 578560
#% 654510
#% 726621
#% 781453
#% 875004
#% 875006
#% 878299
#% 879213
#% 1015283
#% 1016169
#% 1016258
#% 1668029
#% 1688281
#% 1700120
#% 1728691
#! This paper presents two extensions for XQuery. The first extension allows the definition and processing of different kinds of windows over an input sequence; i.e., tumbling, sliding, and landmark windows. The second extension extends the XQuery data model (XDM) to support infinite sequences. This extension makes it possible to use XQuery as a language for continuous queries. Both extensions have been integrated into a Java-based open source XQuery engine. This paper gives details of this implementation and presents the results of running the Linear Road benchmark on the extended XQuery engine.

#index 1022209
#* Structured materialized views for XML queries
#@ Andrei Arion;Véronique Benzaken;Ioana Manolescu;Yannis Papakonstantinou
#t 2007
#c 4
#% 198465
#% 248025
#% 273924
#% 378393
#% 397366
#% 465051
#% 479465
#% 479956
#% 480656
#% 564264
#% 570875
#% 570877
#% 576108
#% 577353
#% 654452
#% 742563
#% 765488
#% 824661
#% 824690
#% 824779
#% 875007
#% 875010
#% 893135
#% 1015271
#% 1015274
#% 1016134
#% 1016140
#% 1034835
#% 1733292
#! The performance of XML database queries can be greatly enhanced by rewriting them using materialized views. We study the problem of rewriting a query using materialized views, where both the query and the views are described by a tree pattern language, appropriately extended to capture a large XQuery subset. The pattern language features optional nodes and nesting, allowing to capture the data needs of nested XQueries. The language also allows describing storage features such as structural identifiers, which enlarge the space of rewritings. We study pattern containment and equivalent rewriting under the constraints expressed in a structural summary, whose enhanced form also entails integrity constraints. Our approach is implemented in the ULoad [7] prototype and we present a performance analysis.

#index 1022210
#* XRPC: interoperable and efficient distributed XQuery
#@ Ying Zhang;Peter Boncz
#t 2007
#c 4
#% 1758
#% 264263
#% 320187
#% 330305
#% 414612
#% 754118
#% 765420
#% 785184
#% 866984
#% 875010
#% 960361
#% 1014242
#% 1015272
#% 1016150
#% 1688304
#% 1712582
#! We propose XRPC, a minimal XQuery extension that enables distributed yet efficient querying of heterogeneous XQuery data sources. XRPC enhances the existing concept of XQuery functions with the Remote Procedure Call (RPC) paradigm. By calling out of an XQuery for-loop to multiple destinations, and by calling functions that themselves perform XRPC calls, complex P2P communication patterns can be achieved. The XRPC extension is orthogonal to all XQuery features, including the XQuery Update Facility (XQUF). We provide formal semantics for XRPC that encompasses execution of both read-only and update queries. XRPC is also a network SOAP sub-protocol, that integrates seamlessly with web services and Service Oriented Architectures (SOA), and AJAX-based GUIs. A crucial feature of the protocol is bulk RPC, that allows remote execution of many different calls to the same procedure, using possibly a single network round-trip. The efficiency potential of XRPC is demonstrated via an open-source implementation in MonetDB/XQuery. We show, however, that XRPC is not system-specific: every XQuery data source can service XRPC calls using a wrapper. Since XQuery is a pure functional language, we can leverage techniques developed for functional query decomposition to rewrite data shipping queries into XRPC-based function shipping queries. Powerful distributed database techniques (such as semi-join optimizations) directly map on bulk RPC, opening up interesting future work opportunities.

#index 1022211
#* Security in outsourcing of association rule mining
#@ W. K. Wong;David W. Cheung;Edward Hung;Ben Kao;Nikos Mamoulis
#t 2007
#c 4
#% 152934
#% 154992
#% 286849
#% 300184
#% 319847
#% 481290
#% 577233
#% 661973
#% 772829
#% 881074
#% 893100
#% 954159
#% 1707965
#! Outsourcing association rule mining to an outside service provider brings several important benefits to the data owner. These include (i) relief from the high mining cost, (ii) minimization of demands in resources, and (iii) effective centralized mining for multiple distributed owners. On the other hand, security is an issue; the service provider should be prevented from accessing the actual data since (i) the data may be associated with private information, (ii) the frequency analysis is meant to be used solely by the owner. This paper proposes substitution cipher techniques in the encryption of transactional data for outsourcing association rule mining. After identifying the non-trivial threats to a straightforward one-to-one item mapping substitution cipher, we propose a more secure encryption scheme based on a one-to-n item mapping that transforms transactions non-deterministically, yet guarantees correct decryption. We develop an effective and efficient encryption algorithm based on this method. Our algorithm performs a single pass over the database and thus is suitable for applications in which data owners send streams of transactions to the service provider. A comprehensive cryptanalysis study is carried out. The results show that our technique is highly secure with a low data transformation cost.

#index 1022212
#* Over-encryption: management of access control evolution on outsourced data
#@ Sabrina De Capitani di Vimercati;Sara Foresti;Sushil Jajodia;Stefano Paraboschi;Pierangela Samarati
#t 2007
#c 4
#% 36033
#% 318404
#% 340827
#% 397367
#% 659992
#% 765448
#% 799891
#% 840668
#% 885729
#% 993942
#% 1015329
#! Data outsourcing is emerging today as a successful paradigm allowing users and organizations to exploit external services for the distribution of resources. A crucial problem to be addressed in this context concerns the enforcement of selective authorization policies and the support of policy updates in dynamic scenarios. In this paper, we present a novel solution to the enforcement of access control and the management of its evolution. Our proposal is based on the application of selective encryption as a means to enforce authorizations. Two layers of encryption are imposed on data: the inner layer is imposed by the owner for providing initial protection, the outer layer is imposed by the server to reflect policy modifications. The combination of the two layers provides an efficient and robust solution. The paper presents a model, an algorithm for the management of the two layers, and an analysis to identify and therefore counteract possible information exposure risks.

#index 1022213
#* CADS: continuous authentication on data streams
#@ Stavros Papadopoulos;Yin Yang;Dimitris Papadias
#t 2007
#c 4
#% 235114
#% 319994
#% 378388
#% 513367
#% 578560
#% 657774
#% 761411
#% 810042
#% 874980
#% 1669498
#! We study processing and authentication of long-running queries on outsourced data streams. In this scenario, a data owner (DO) constantly transmits its data to a service provider (SP), together with additional authentication information. Clients register continuous range queries to the SP. Whenever the data change, the SP must update the results of all affected queries and inform the clients accordingly. The clients can verify the correctness of the results using the authentication information provided by the DO. Compared to conventional databases, stream environments pose new challenges such as the need for fast structure updating, support for continuous query processing and authentication, and provision for temporal completeness. Specifically, in addition to the correctness of individual results, the client must be able to verify that there are no missing results in between updates. We face these challenges through several contributions. Since there is no previous work, we first present a technique, called REF, that achieves correctness and temporal completeness but incurs false transmissions, i.e., the SP has to inform clients whenever there is a data update, even if their results are not affected. Then, we propose CADS, which minimizes the processing and transmission overhead through an elaborate indexing scheme and a virtual caching mechanism. Finally, we extend CADS to the case where multiple owners outsource their data to the same SP. The SP integrates all data in a single authentication process, independently of the number of DOs.

#index 1022214
#* Proof-infused streams: enabling authentication of sliding window queries on streams
#@ Feifei Li;Ke Yi;Marios Hadjieleftheriou;George Kollios
#t 2007
#c 4
#% 39029
#% 235114
#% 321455
#% 333977
#% 378388
#% 379445
#% 427199
#% 513367
#% 552158
#% 576119
#% 657774
#% 659992
#% 745532
#% 761411
#% 772846
#% 810042
#% 824701
#% 838424
#% 858971
#% 863402
#% 874980
#% 993969
#% 997534
#% 1395167
#% 1651365
#% 1664115
#% 1675939
#! As computer systems are essential components of many critical commercial services, the need for secure online transactions is now becoming evident. The demand for such applications, as the market grows, exceeds the capacity of individual businesses to provide fast and reliable services, making outsourcing technologies a key player in alleviating issues of scale. Consider a stock broker that needs to provide a real-time stock trading monitoring service to clients. Since the cost of multicasting this information to a large audience might become prohibitive, the broker could outsource the stock feed to third-party providers, who are in turn responsible for forwarding the appropriate sub-feed to clients. Evidently, in critical applications the integrity of the third-party should not be taken for granted. In this work we study a variety of authentication algorithms for selection and aggregation queries over sliding windows. Our algorithms enable the end-users to prove that the results provided by the third-party are correct, i.e., equal to the results that would have been computed by the original provider. Our solutions are based on Merkle hash trees over a forest of space partitioning data structures, and try to leverage key features, like update, query, signing, and authentication costs. We present detailed theoretical analysis for our solutions and empirically evaluate the proposed techniques.

#index 1022215
#* Staying FIT: efficient load shedding techniques for distributed stream processing
#@ Nesime Tatbul;Uǧur Çetintemel;Stan Zdonik
#t 2007
#c 4
#% 45094
#% 318051
#% 401896
#% 479786
#% 480955
#% 726621
#% 745534
#% 765436
#% 765470
#% 800504
#% 864436
#% 884486
#% 893154
#% 963586
#% 978376
#% 993945
#% 1015259
#% 1015280
#! In distributed stream processing environments, large numbers of continuous queries are distributed onto multiple servers. When one or more of these servers become overloaded due to bursty data arrival, excessive load needs to be shed in order to preserve low latency for the query results. Because of the load dependencies among the servers, load shedding decisions on these servers must be well-coordinated to achieve end-to-end control on the output quality. In this paper, we model the distributed load shedding problem as a linear optimization problem, for which we propose two alternative solution approaches: a solver-based centralized approach, and a distributed approach based on metadata aggregation and propagation, whose centralized implementation is also available. Both of our solutions are based on generating a series of load shedding plans in advance, to be used under certain input load conditions. We have implemented our techniques as part of the Borealis distributed stream processing system. We present experimental results from our prototype implementation showing the performance of these techniques under different input and query workloads.

#index 1022216
#* A simple and efficient estimation method for stream expression cardinalities
#@ Aiyou Chen;Jin Cao;Tian Bu
#t 2007
#c 4
#% 2833
#% 214073
#% 282505
#% 311808
#% 480805
#% 519953
#% 745442
#% 788218
#% 808537
#% 821990
#% 937911
#% 1698257
#! Estimating the cardinality (i.e. number of distinct elements) of an arbitrary set expression defined over multiple distributed streams is one of the most fundamental queries of interest. Earlier methods based on probabilistic sketches have focused mostly on the sketching algorithms. However, the estimators do not fully utilize the information in the sketches and thus are not statistically efficient. In this paper, we develop a novel statistical model and an efficient yet simple estimator for the cardinalities based on a continuous variant of the well known Flajolet-Martin sketches. Specifically, we show that, for two streams, our estimator has almost the same statistical efficiency as the Maximum Likelihood Estimator (MLE), which is known to be optimal in the sense of Cramer-Rao lower bounds under regular conditions. Moreover, as the number of streams gets larger, our estimator is still computationally simple, but the MLE becomes intractable due to the complexity of the likelihood. Let N be the cardinality of the union of all streams, and &verbar;S&verbar; be the cardinality of a set expression S to be estimated. For a given relative standard error δ, the memory requirement of our estimator is O(δ-2&verbar;S&verbar;-1 N log log N), which is superior to state-of-the-art algorithms, especially for large N and small &verbar;S&verbar;/N where the estimation is most challenging.

#index 1022217
#* Ad-hoc top-k query answering for data streams
#@ Gautam Das;Dimitrios Gunopulos;Nick Koudas;Nikos Sarkas
#t 2007
#c 4
#% 62323
#% 144877
#% 213981
#% 235114
#% 248010
#% 260653
#% 300180
#% 333854
#% 333951
#% 347226
#% 378388
#% 427219
#% 465167
#% 479648
#% 875023
#% 893126
#% 893128
#% 1016183
#! A top-k query retrieves the k highest scoring tuples from a data set with respect to a scoring function defined on the attributes of a tuple. The efficient evaluation of top-k queries has been an active research topic and many different instantiations of the problem, in a variety of settings, have been studied. However, techniques developed for conventional, centralized or distributed databases are not directly applicable to highly dynamic environments and on-line applications, like data streams. Recently, techniques supporting top-k queries on data streams have been introduced. Such techniques are restrictive however, as they can only efficiently report top-k answers with respect to a pre-specified (as opposed to ad-hoc) set of queries. In this paper we introduce a novel geometric representation for the top-k query problem that allows us to raise this restriction. Utilizing notions of geometric arrangements, we design and analyze algorithms for incrementally maintaining a data set organized in an arrangement representation under streaming updates. We introduce query evaluation strategies that operate on top of an arrangement data structure that are able to guarantee efficient evaluation for ad-hoc queries. The performance of our core technique is augmented by incorporating tuple pruning strategies, minimizing the number of tuples that need to be stored and manipulated. This results in a main memory indexing technique supporting both efficient incremental updates and the evaluation of ad-hoc top-k queries. A thorough experimental study evaluates the efficiency of the proposed technique.

#index 1022218
#* Extending q-grams to estimate selectivity of string matching with low edit distance
#@ Hongrae Lee;Raymond T. Ng;Kyuseok Shim
#t 2007
#c 4
#% 210189
#% 235941
#% 269546
#% 273705
#% 333679
#% 341144
#% 387427
#% 408353
#% 480488
#% 480654
#% 481290
#% 544203
#% 577238
#% 654467
#% 745489
#% 783499
#% 824684
#% 824717
#% 993968
#! There are many emerging database applications that require accurate selectivity estimation of approximate string matching queries. Edit distance is one of the most commonly used string similarity measures. In this paper, we study the problem of estimating selectivity of string matching with low edit distance. Our framework is based on extending q-grams with wildcards. Based on the concepts of replacement semi-lattice, string hierarchy and a combinatorial analysis, we develop the formulas for selectivity estimation and provide the algorithm BasicEQ. We next develop the algorithm Opt EQ by enhancing BasicEQ with two novel improvements. Finally we show a comprehensive set of experiments using three benchmarks comparing Opt EQ with the state-of-the-art method SEPIA. Our experimental results show that Opt EQ delivers more accurate selectivity estimations.

#index 1022219
#* Fast nGram-based string search over data encoded using algebraic signatures
#@ Witold Litwin;Riad Mokadem;Philippe Rigaux;Thomas Schwarz
#t 2007
#c 4
#% 143306
#% 179696
#% 320454
#% 664705
#% 745517
#% 768815
#% 824678
#% 864587
#% 1015331
#% 1702388
#! We propose a novel string search algorithm for data stored once and read many times. Our search method combines the sublinear traversal of the record (as in Boyer Moore or Knuth-Morris-Pratt) with the agglomeration of parts of the record and search pattern into a single character -- the algebraic signature -- in the manner of Karp-Rabin. Our experiments show that our algorithm is up to seventy times faster for DNA data, up to eleven times faster for ASCII, and up to a six times faster for XML documents compared with an implementation of Boyer-Moore. To obtain this speed-up, we store records in encoded form, where each original character is replaced with an algebraic signature. Our method applies to records stored in databases in general and to distributed implementations of a Database As Service (DAS) in particular. Clients send records for insertion and search patterns already in encoded form and servers never operate on records in clear text. No one at a node can involuntarily discover the content of the stored data.

#index 1022220
#* Effective phrase prediction
#@ Arnab Nandi;H. V. Jagadish
#t 2007
#c 4
#% 82523
#% 96288
#% 131061
#% 210189
#% 237338
#% 262869
#% 273705
#% 289010
#% 297234
#% 301263
#% 308589
#% 333854
#% 345088
#% 593764
#% 742368
#% 752055
#% 766461
#% 768305
#% 781169
#% 832095
#% 939950
#% 993960
#% 1016132
#% 1699616
#! Autocompletion is a widely deployed facility in systems that require user input. Having the system complete a partially typed "word" can save user time and effort. In this paper, we study the problem of autocompletion not just at the level of a single "word", but at the level of a multi-word "phrase". There are two main challenges: one is that the number of phrases (both the number possible and the number actually observed in a corpus) is combinatorially larger than the number of words; the second is that a "phrase", unlike a "word", does not have a well-defined boundary, so that the autocompletion system has to decide not just what to predict, but also how far. We introduce a FussyTree structure to address the first challenge and the concept of a significant phrase to address the second. We develop a probabilistically driven multiple completion choice model, and exploit features such as frequency distributions to improve the quality of our suffix completions. We experimentally demonstrate the practicability and value of our technique for an email composition application and show that we can save approximately a fifth of the keystrokes typed.

#index 1022221
#* Lazy maintenance of materialized views
#@ Jingren Zhou;Per-Ake Larson;Hicham G. Elmongui
#t 2007
#c 4
#% 13016
#% 152928
#% 201869
#% 201928
#% 201929
#% 210210
#% 227869
#% 227944
#% 227947
#% 300138
#% 300141
#% 300199
#% 333965
#% 340300
#% 479792
#% 765472
#% 800562
#% 960278
#! Materialized views can speed up query processing greatly but they have to be kept up to date to be useful. Today, database systems typically maintain views eagerly in the same transaction as the base table updates. This has the effect that updates pay for view maintenance while beneficiaries (queries) get a free ride! View maintenance overhead can be significant and it seems unfair to have updates bear the cost. We present a novel way to lazily maintain materialized views that relieves updates of this overhead. Maintenance of a view is postponed until the system has free cycles or the view is referenced by a query. View maintenance is fully or partly hidden from queries depending on the system load. Ideally, views are maintained entirely on system time at no cost to updates and queries. The efficiency of lazy maintenance is improved by combining updates from several transactions into a single maintenance operation, by condensing multiple updates of the same row into a single update, and by exploiting row versioning. Experiments using a prototype implementation in Microsoft SQL Server show much faster response times for updates and also significant reduction in maintenance cost when combining updates.

#index 1022222
#* Extending dependencies with conditions
#@ Loreto Bravo;Wenfei Fan;Shuai Ma
#t 2007
#c 4
#% 583
#% 6242
#% 213972
#% 224743
#% 296539
#% 384978
#% 665856
#% 752741
#% 809239
#% 810019
#% 810078
#% 814475
#% 833132
#% 857502
#% 879041
#% 893114
#% 1661438
#! This paper introduces a class of conditional inclusion dependencies (CINDs), which extends traditional inclusion dependencies (INDs) by enforcing bindings of semantically related data values. We show that CINDs are useful not only in data cleaning, but are also in contextual schema matching [7]. To make effective use of CINDs in practice, it is often necessary to reason about them. The most important static analysis issue concerns consistency, to determine whether or not a given set of CINDs has conflicts. Another issue concerns implication, i.e., deciding whether a set of CINDs entails another CIND. We give a full treatment of the static analyses of CINDs, and show that CINDs retain most nice properties of traditional INDs: (a) CINDs are always consistent; (b) CINDs are finitely axiomatizable, i.e., there exists a sound and complete inference system for implication of CINDs; and (c) the implication problem for CINDs has the same complexity as its traditional counterpart, namely, PSPACE-complete, in the absence of attributes with a finite domain; but it is EXPTIME-complete in the general setting. In addition, we investigate the interaction between CINDs and conditional functional dependencies (CFDs), an extension of functional dependencies proposed in [9]. We show that the consistency problem for the combination of CINDs and CFDs becomes undecidable. In light of the undecidability, we provide heuristic algorithms for the consistency analysis of CFDs and CINDs, and experimentally verify the effectiveness and efficiency of our algorithms.

#index 1022223
#* Unifying data and domain knowledge using virtual views
#@ Lipyeow Lim;Haixun Wang;Min Wang
#t 2007
#c 4
#% 36683
#% 198473
#% 378409
#% 397607
#% 577305
#% 770326
#% 783540
#% 783793
#% 810036
#% 1016217
#! The database community is on a constant quest for better integration of data management and knowledge management. Recently, with the increasing use of ontology in various applications, the quest has become more concrete and urgent. However, manipulating knowledge along with relational data in DBMSs is not a trivial undertaking. In this paper, we introduce a novel, unified framework for managing data and domain knowledge. We provide the user with a virtual view that unifies the data, the domain knowledge and the knowledge inferable from the data using the domain knowledge. Because the virtual view is in the relational format, users can query the data and the knowledge in a seamlessly integrated manner. To facilitate knowledge representation and inferencing within the database engine, our approach leverages XML support in hybrid relational-XML DBMSs (e.g., Microsoft SQL Server & IBM DB2 9 PureXML). We provide a query rewriting mechanism to bridge the difference between logical and physical data modeling, so that queries on the virtual view can be automatically transformed to components that execute on the hybrid relational-XML engine in a way that is transparent to the user.

#index 1022224
#* Efficient skyline computation over low-cardinality domains
#@ Michael Morse;Jignesh M. Patel;H. V. Jagadish
#t 2007
#c 4
#% 288976
#% 462059
#% 465167
#% 480671
#% 654480
#% 765467
#% 765468
#% 800555
#% 806212
#% 810024
#% 824670
#% 824671
#% 824672
#% 849816
#% 875012
#% 875025
#% 943612
#% 993954
#% 1688273
#! Current skyline evaluation techniques follow a common paradigm that eliminates data elements from skyline consideration by finding other elements in the dataset that dominate them. The performance of such techniques is heavily influenced by the underlying data distribution (i.e. whether the dataset attributes are correlated, independent, or anti-correlated). In this paper, we propose the Lattice Skyline Algorithm (LS) that is built around a new paradigm for skyline evaluation on datasets with attributes that are drawn from low-cardinality domains. LS continues to apply even if one attribute has high cardinality. Many skyline applications naturally have such data characteristics, and previous skyline methods have not exploited this property. We show that for typical dimensionalities, the complexity of LS is linear in the number of input tuples. Furthermore, we show that the performance of LS is independent of the input data distribution. Finally, we demonstrate through extensive experimentation on both real and synthetic databsets that LS can results in a significant performance advantage over existing technqiues.

#index 1022225
#* Approaching the skyline in Z order
#@ Ken C. K. Lee;Baihua Zheng;Huajing Li;Wang-Chien Lee
#t 2007
#c 4
#% 62323
#% 415957
#% 465167
#% 480327
#% 480671
#% 800555
#% 806212
#% 824670
#% 849816
#% 864451
#% 864453
#% 875012
#% 993954
#% 1688253
#! Given a set of multidimensional data points, skyline query retrieves a set of data points that are not dominated by any other points. This query is useful for multi-preference analysis and decision making. By analyzing the skyline query, we observe a close connection between Z-order curve and skyline processing strategies and propose to use a new index structure called ZBtree, to index and store data points based on Z-order curve. We develop a suite of novel and efficient skyline algorithms, which scale very well to data dimensionality and cardinality, including (1) ZSearch, which processes skyline queries and supports progressive result delivery; (2) ZUpdate, which facilitates incremental skyline result maintenance; and (3) k-ZSearch, which answers k-dominant skyline query (a skyline variant that retrieves a representative subset of skyline results). Extensive experiments have been conducted to evaluate our proposed algorithms and compare them against the best available algorithms designed for skyline search, skyline result update, and k-dominant skyline search, respectively. The result shows that our algorithms, developed coherently based on the same ideas and concepts, soundly outperforms the state-of-the-art skyline algorithms in their specialized domains.

#index 1022226
#* Efficient computation of reverse skyline queries
#@ Evangelos Dellis;Bernhard Seeger
#t 2007
#c 4
#% 62323
#% 86950
#% 300163
#% 465009
#% 465167
#% 479648
#% 480825
#% 654480
#% 730019
#% 731407
#% 802243
#% 824671
#% 824672
#% 864451
#% 864452
#% 875013
#% 875025
#% 878300
#% 893150
#% 907528
#% 993957
#% 1016191
#! In this paper, for the first time, we introduce the concept of Reverse Skyline Queries. At first, we consider for a multidimensional data set P the problem of dynamic skyline queries according to a query point q. This kind of dynamic skyline corresponds to the skyline of a transformed data space where point q becomes the origin and all points of P are represented by their distance vector to q. The reverse skyline query returns the objects whose dynamic skyline contains the query object q. In order to compute the reverse skyline of an arbitrary query point, we first propose a Branch and Bound algorithm (called BBRS), which is an improved customization of the original BBS algorithm. Furthermore, we identify a super set of the reverse skyline that is used to bound the search space while computing the reverse skyline. To further reduce the computational cost of determining if a point belongs to the reverse skyline, we propose an enhanced algorithm (called RSSA) that is based on accurate pre-computed approximations of the skylines. These approximations are used to identify whether a point belongs to the reverse skyline or not. Through extensive experiments with both real-world and synthetic datasets, we show that our algorithms can efficiently support reverse skyline queries. Our enhanced approach improves reversed skyline processing by up to an order of magnitude compared to the algorithm without the usage of pre-computed approximations.

#index 1022227
#* VGRAM: improving performance of approximate queries on string collections using variable-length grams
#@ Chen Li;Bin Wang;Xiaochun Yang
#t 2007
#c 4
#% 121278
#% 210189
#% 248801
#% 273705
#% 290703
#% 333679
#% 480654
#% 546257
#% 547438
#% 654467
#% 745489
#% 765463
#% 809460
#% 824678
#% 863382
#% 864392
#% 875066
#% 893164
#% 956506
#% 968433
#% 1682128
#% 1717297
#! Many applications need to solve the following problem of approximate string matching: from a collection of strings, how to find those similar to a given string, or the strings in another (possibly the same) collection of strings? Many algorithms are developed using fixed-length grams, which are substrings of a string used as signatures to identify similar strings. In this paper we develop a novel technique, called VGRAM, to improve the performance of these algorithms. Its main idea is to judiciously choose high-quality grams of variable lengths from a collection of strings to support queries on the collection. We give a full specification of this technique, including how to select high-quality grams from the collection, how to generate variable-length grams for a string based on the preselected grams, and what is the relationship between the similarity of the gram sets of two strings and their edit distance. A primary advantage of the technique is that it can be adopted by a plethora of approximate string algorithms without the need to modify them substantially. We present our extensive experiments on real data sets to evaluate the technique, and show the significant performance improvements on three existing algorithms.

#index 1022228
#* Improving data quality: consistency and accuracy
#@ Gao Cong;Wenfei Fan;Floris Geerts;Xibei Jia;Shuai Ma
#t 2007
#c 4
#% 663
#% 1331
#% 156856
#% 176087
#% 213972
#% 224743
#% 242237
#% 273687
#% 283228
#% 296539
#% 301169
#% 350100
#% 366807
#% 408396
#% 420072
#% 465052
#% 480496
#% 480499
#% 491358
#% 549573
#% 778320
#% 810019
#% 814475
#% 818916
#% 1661438
#! Two central criteria for data quality are consistency and accuracy. Inconsistencies and errors in a database often emerge as violations of integrity constraints. Given a dirty database D, one needs automated methods to make it consistent, i.e., find a repair D' that satisfies the constraints and "minimally" differs from D. Equally important is to ensure that the automatically-generated repair D' is accurate, or makes sense, i.e., D' differs from the "correct" data within a predefined bound. This paper studies effective methods for improving both data consistency and accuracy. We employ a class of conditional functional dependencies (CFDs) proposed in [6] to specify the consistency of the data, which are able to capture inconsistencies and errors beyond what their traditional counterparts can catch. To improve the consistency of the data, we propose two algorithms: one for automatically computing a repair D' that satisfies a given set of CFDs, and the other for incrementally finding a repair in response to updates to a clean database. We show that both problems are intractable. Although our algorithms are necessarily heuristic, we experimentally verify that the methods are effective and efficient. Moreover, we develop a statistical method that guarantees that the repairs found by the algorithms are accurate above a predefined rate without incurring excessive user interaction.

#index 1022229
#* Example-driven design of efficient record matching queries
#@ Surajit Chaudhuri;Bee-Chung Chen;Venkatesh Ganti;Raghav Kaushik
#t 2007
#c 4
#% 201889
#% 217812
#% 248801
#% 309208
#% 310516
#% 314740
#% 333943
#% 350103
#% 465167
#% 480496
#% 480654
#% 577238
#% 577247
#% 577263
#% 654467
#% 674103
#% 729913
#% 765463
#% 769877
#% 864392
#% 875043
#% 875064
#% 875066
#% 893164
#% 915242
#% 980548
#% 1016182
#% 1016219
#% 1250576
#! Record matching is the task of identifying records that match the same real world entity. This is a problem of great significance for a variety of business intelligence applications. Implementations of record matching rely on exact as well as approximate string matching (e.g., edit distances) and use of external reference data sources. Record matching can be viewed as a query composed of a small set of primitive operators. However, formulating such record matching queries is difficult and depends on the specific application scenario. Specifically, the number of options both in terms of string matching operations as well as the choice of external sources can be daunting. In this paper, we exploit the availability of positive and negative examples to search through this space and suggest an initial record matching query. Such queries can be subsequently modified by the programmer as needed. We ensure that the record matching queries our approach produces are (1) efficient: these queries can be run on large datasets by leveraging operations that are well-supported by RDBMSs, and (2) explainable: the queries are easy to understand so that they may be modified by the programmer with relative ease. We demonstrate the effectiveness of our approach on several real-world datasets.

#index 1022230
#* Adaptive aggregation on chip multiprocessors
#@ John Cieslewicz;Kenneth A. Ross
#t 2007
#c 4
#% 115661
#% 172913
#% 201883
#% 248793
#% 300167
#% 356652
#% 443513
#% 765419
#% 765456
#% 824719
#% 850738
#% 873339
#% 873341
#% 875068
#% 893219
#% 1052064
#! The recent introduction of commodity chip multiprocessors requires that the design of core database operations be carefully examined to take full advantage of on-chip parallelism. In this paper we examine aggregation in a multi-core environment, the Sun UltraSPARC T1, a chip multiprocessor with eight cores and a shared L2 cache. Aggregation is an important aspect of query processing that is seemingly easy to understand and implement. Our research, however, demonstrates that a chip multiprocessor adds new dimensions to understanding hash-based aggregation performance---concurrent sharing of aggregation data structures and contentious accesses to frequently used values. We also identify a trade off between private data structures assigned to each thread versus shared data structures for aggregation. Depending on input characteristics, different aggregation strategies are optimal and choosing the wrong strategy can result in a performance penalty of over an order of magnitude. We provide a thorough explanation of the factors affecting aggregation performance on chip multiprocessors and identify three key input characteristics that dictate performance: (1) average run length of identical group-by values, (2) locality of references to the aggregation hash table, and (3) frequency of repeated accesses to the same hash table location. We then introduce an adaptive aggregation operator that performs lightweight sampling of the input to choose the correct aggregation strategy with high accuracy. Our experiments verify that our adaptive algorithm chooses the highest performing aggregation strategy on a number of common input distributions.

#index 1022231
#* To share or not to share?
#@ Ryan Johnson;Stavros Harizopoulos;Nikos Hardavellas;Kivanc Sabirli;Ippokratis Pandis;Anastasia Ailamaki;Naju G. Mancheril;Babak Falsafi
#t 2007
#c 4
#% 36117
#% 172968
#% 262154
#% 273911
#% 286991
#% 300166
#% 300179
#% 333848
#% 340635
#% 397353
#% 465169
#% 479468
#% 479821
#% 480158
#% 481289
#% 506783
#% 765417
#% 810039
#% 843700
#% 893129
#! Intuitively, aggressive work sharing among concurrent queries in a database system should always improve performance by eliminating redundant computation or data accesses. We show that, contrary to common intuition, this is not always the case in practice, especially in the highly parallel world of chip multiprocessors. As the number of cores in the system increases, a trade-off appears between exploiting work sharing opportunities and the available parallelism. To resolve the trade-off, we develop an analytical approach that predicts the effect of work sharing in multi-core systems. Database systems can use the model to determine, statically or at runtime, whether work sharing is beneficial and apply it only when appropriate. The contributions of this paper are as follows. First, we introduce and analyze the effects of the trade-off between work sharing and parallelism on database systems running complex decision-support queries. Second, we propose an intuitive and simple model that can evaluate the trade-off using real-world measurement approximations of the query execution processes. Furthermore, we integrate the model into a prototype database execution engine, and demonstrate that selective work sharing according to the model outperforms never-share static schemes by 20% on average and always-share ones by 2.5x.

#index 1022232
#* Executing stream joins on the cell processor
#@ Buǧra Gedik;Philip S. Yu;Rajesh R. Bordawekar
#t 2007
#c 4
#% 320200
#% 462491
#% 479920
#% 480774
#% 654444
#% 765436
#% 788216
#% 810059
#% 824697
#% 832052
#% 838409
#% 850738
#% 853011
#% 874997
#% 875006
#% 888909
#% 905193
#% 1015278
#% 1015280
#% 1015296
#% 1016156
#! Low-latency and high-throughput processing are key requirements of data stream management systems (DSMSs). Hence, multi-core processors that provide high aggregate processing capacity are ideal matches for executing costly DSMS operators. The recently developed Cell processor is a good example of a heterogeneous multi-core architecture and provides a powerful platform for executing data stream operators with high-performance. On the down side, exploiting the full potential of a multi-core processor like Cell is often challenging, mainly due to the heterogeneous nature of the processing elements, the software managed local memory at the co-processor side, and the unconventional programming model in general. In this paper, we study the problem of scalable execution of windowed stream join operators on multi-core processors, and specifically on the Cell processor. By examining various aspects of join execution flow, we determine the right set of techniques to apply in order to minimize the sequential segments and maximize parallelism. Concretely, we show that basic windows coupled with low-overhead pointer-shifting techniques can be used to achieve efficient join window partitioning, column-oriented join window organization can be used to minimize scattered data transfers, delay-optimized double buffering can be used for effective pipelining, rate-aware batching can be used to balance join throughput and tuple delay, and finally SIMD (single-instruction multiple-data) optimized operator code can be used to exploit data parallelism. Our experimental results show that, following the design guidelines and implementation techniques outlined in this paper, windowed stream joins can achieve high scalability (linear in the number of co-processors) by making efficient use of the extensive hardware parallelism provided by the Cell processor (reaching data processing rates of &ap; 13 GB/sec) and significantly surpass the performance obtained form conventional high-end processors (supporting a combined input stream rate of 2000 tuples/sec using 15 minutes windows and without dropping any tuples, resulting in &ap; 8.3 times higher output rate compared to an SSE implementation on dual 3.2Ghz Intel Xeon).

#index 1022233
#* RankMass crawler: a crawler with high personalized pagerank coverage guarantee
#@ Junghoo Cho;Uri Schonfeld
#t 2007
#c 4
#% 268079
#% 268087
#% 281166
#% 281251
#% 330609
#% 348136
#% 348137
#% 348138
#% 348173
#% 480309
#% 480479
#% 577328
#% 577329
#% 577330
#% 641979
#% 799632
#% 805879
#% 807302
#% 869469
#% 869499
#% 869602
#% 882035
#% 893123
#% 1016162
#% 1016164
#% 1016177
#! Crawling algorithms have been the subject of extensive research and optimizations, but some important questions remain open. In particular, given the unbounded number of pages available on the Web, search-engine operators constantly struggle with the following vexing questions: When can I stop downloading the Web? How many pages should I download to cover "most" of the Web? How can I know I am not missing an important part when I stop? In this paper we provide an answer to these questions by developing, in the context of a system that is given a set of trusted pages, a family of crawling algorithms that (1) provide a theoretical guarantee on how much of the "important" part of the Web it will download after crawling a certain number of pages and (2) give a high priority to important pages during a crawl, so that the search engine can index the most important part of the Web first. We prove the correctness of our algorithms by theoretical analysis and evaluate their performance experimentally based on 141 million URLs obtained from the Web. Our experiments demonstrate that even our simple algorithm is effective in downloading important pages early on and provides high "coverage" of the Web with a relatively small number of pages.

#index 1022234
#* EntityRank: searching entities directly and holistically
#@ Tao Cheng;Xifeng Yan;Kevin Chen-Chuan Chang
#t 2007
#c 4
#% 268079
#% 330616
#% 577318
#% 730022
#% 740900
#% 742102
#% 754068
#% 799737
#% 805883
#% 854668
#% 869535
#% 875001
#% 875061
#% 875064
#% 956501
#% 960235
#% 960356
#! As the Web has evolved into a data-rich repository, with the standard "page view," current search engines are becoming increasingly inadequate for a wide range of query tasks. While we often search for various data "entities" (e.g., phone number, paper PDF, date), today's engines only take us indirectly to pages. While entities appear in many pages, current engines only find each page individually. Toward searching directly and holistically for finding information of finer granularity, we study the problem of entity search, a significant departure from traditional document retrieval. We focus on the core challenge of ranking entities, by distilling its underlying conceptual model Impression Model and developing a probabilistic ranking framework, EntityRank, that is able to seamlessly integrate both local and global information in ranking. We evaluate our online prototype over a 2TB Web corpus, and show that EntityRank performs effectively.

#index 1022235
#* Building structured web community portals: a top-down, compositional, and incremental approach
#@ Pedro DeRose;Warren Shen;Fei Chen;AnHai Doan;Raghu Ramakrishnan
#t 2007
#c 4
#% 268079
#% 316533
#% 321635
#% 341658
#% 348138
#% 476761
#% 495944
#% 571038
#% 577318
#% 731210
#% 770307
#% 779950
#% 782759
#% 788107
#% 830520
#% 864415
#% 869502
#% 874992
#% 875064
#% 875066
#% 893089
#% 1022269
#% 1022288
#! Structured community portals extract and integrate information from raw Web pages to present a unified view of entities and relationships in the community. In this paper we argue that to build such portals, a top-down, compositional, and incremental approach is a good way to proceed. Compared to current approaches that employ complex monolithic techniques, this approach is easier to develop, understand, debug, and optimize. In this approach, we first select a small set of important community sources. Next, we compose plans that extract and integrate data from these sources, using a set of extraction/integration operators. Executing these plans yields an initial structured portal. We then incrementally expand this portal by monitoring the evolution of current data sources, to detect and add new data sources. We describe our initial solutions to the above steps, and a case study of employing these solutions to build DBLife, a portal for the database community. We found that DBLife could be built quickly and achieve high accuracy using simple extraction/integration operators, and that it can be maintained and expanded with little human effort. The initial solutions together with the case study demonstrate the feasibility and potential of our approach.

#index 1022236
#* Scalable semantic web data management using vertical partitioning
#@ Daniel J. Abadi;Adam Marcus;Samuel R. Madden;Kate Hollenbach
#t 2007
#c 4
#% 286258
#% 479956
#% 480629
#% 481599
#% 519567
#% 571056
#% 728100
#% 824697
#% 824755
#% 864445
#% 875026
#% 1016235
#! Efficient management of RDF data is an important factor in realizing the Semantic Web vision. Performance and scalability issues are becoming increasingly pressing as Semantic Web technology is applied to real-world applications. In this paper, we examine the reasons why current data management solutions for RDF data scale poorly, and explore the fundamental scalability limitations of these approaches. We review the state of the art for improving performance for RDF databases and consider a recent suggestion, "property tables." We then discuss practically and empirically why this solution has undesirable features. As an improvement, we propose an alternative solution: vertically partitioning the RDF data. We compare the performance of vertical partitioning with prior art on queries generated by a Web-based RDF browser over a large-scale (more than 50 million triples) catalog of library data. Our results show that a vertical partitioned schema achieves similar performance to the property table technique while being much simpler to design. Further, if a column-oriented DBMS (a database architected specially for the vertically partitioned case) is used instead of a row-oriented DBMS, another order of magnitude performance improvement is observed, with query times dropping from minutes to several seconds.

#index 1022237
#* Ranked subsequence matching in time-series databases
#@ Wook-Shin Han;Jinsoo Lee;Yang-Sae Moon;Haifeng Jiang
#t 2007
#c 4
#% 86950
#% 137711
#% 172949
#% 201876
#% 232122
#% 248796
#% 248797
#% 333941
#% 390132
#% 397381
#% 443369
#% 460862
#% 462231
#% 464994
#% 479649
#% 480146
#% 527026
#% 564263
#% 578400
#% 654456
#% 754411
#% 784537
#% 798319
#% 893220
#% 993965
#% 1669475
#! Existing work on similar sequence matching has focused on either whole matching or range subsequence matching. In this paper, we present novel methods for ranked subsequence matching under time warping, which finds top-k subsequences most similar to a query sequence from data sequences. To the best of our knowledge, this is the first and most sophisticated subsequence matching solution mentioned in the literature. Specifically, we first provide a new notion of the minimum-distance matching-window pair (MDMWP) and formally define the mdmwp-distance, a lower bound between a data subsequence and a query sequence. The mdmwp-distance can be computed prior to accessing the actual subsequence. Based on the mdmwp-distance, we then develop a ranked subsequence matching algorithm to prune unnecessary subsequence accesses. Next, to reduce random disk I/Os and bad buffer utilization, we develop a method of deferred group subsequence retrieval. We then derive another lower bound, the window-group distance, that can be used to effectively prune unnecessary subsequence accesses during deferred group-subsequence retrieval. Through extensive experiments with many data sets, we showcase the superiority of the proposed methods.

#index 1022238
#* Indexable PLA for efficient similarity search
#@ Qiuxia Chen;Lei Chen;Xiang Lian;Yunhao Liu;Jeffrey Xu Yu
#t 2007
#c 4
#% 172949
#% 227924
#% 232122
#% 248798
#% 287466
#% 316560
#% 333941
#% 427199
#% 460862
#% 464851
#% 480146
#% 481611
#% 654456
#% 654497
#% 729943
#% 745513
#% 765403
#% 765451
#% 810049
#% 874982
#% 878302
#% 960281
#% 960283
#% 993961
#% 1016195
#! Similarity-based search over time-series databases has been a hot research topic for a long history, which is widely used in many applications, including multimedia retrieval, data mining, web search and retrieval, and so on. However, due to high dimensionality (i.e. length) of the time series, the similarity search over directly indexed time series usually encounters a serious problem, known as the "dimensionality curse". Thus, many dimensionality reduction techniques are proposed to break such curse by reducing the dimensionality of time series. Among all the proposed methods, only Piecewise Linear Approximation (PLA) does not have indexing mechanisms to support similarity queries, which prevents it from efficiently searching over very large time-series databases. Our initial studies on the effectiveness of different reduction methods, however, show that PLA performs no worse than others. Motivated by this, in this paper, we re-investigate PLA for approximating and indexing time series. Specifically, we propose a novel distance function in the reduced PLA-space, and prove that this function indeed results in a lower bound of the Euclidean distance between the original time series, which can lead to no false dismissals during the similarity search. As a second step, we develop an effective approach to index these lower bounds to improve the search efficiency. Our extensive experiments over a wide spectrum of real and synthetic data sets have demonstrated the efficiency and effectiveness of PLA together with the newly proposed lower bound distance, in terms of both pruning power and wall clock time, compared with two state-of-the-art reduction methods, Adaptive Piecewise Constant Approximation (APCA) and Chebyshev Polynomials (CP).

#index 1022239
#* Mining approximate top-k subspace anomalies in multi-dimensional time-series data
#@ Xiaolei Li;Jiawei Han
#t 2007
#c 4
#% 34077
#% 248792
#% 273916
#% 280501
#% 300131
#% 300183
#% 301165
#% 333929
#% 420141
#% 577221
#% 664842
#% 765518
#% 800496
#% 809264
#% 810065
#% 824705
#% 824710
#% 844310
#% 893120
#% 993958
#% 1016173
#! Market analysis is a representative data analysis process with many applications. In such an analysis, critical numerical measures, such as profit and sales, fluctuate over time and form time-series data. Moreover, the time series data correspond to market segments, which are described by a set of attributes, such as age, gender, education, income level, and product-category, that form a multi-dimensional structure. To better understand market dynamics and predict future trends, it is crucial to study the dynamics of time-series in multi-dimensional market segments. This is a topic that has been largely ignored in time series and data cube research. In this study, we examine the issues of anomaly detection in multi-dimensional time-series data. We propose time-series data cube to capture the multi-dimensional space formed by the attribute structure. This facilitates the detection of anomalies based on expected values derived from higher level, "more general" time-series. Anomaly detection in a time-series data cube poses computational challenges, especially for high-dimensional, large data sets. To this end, we also propose an efficient search algorithm to iteratively select subspaces in the original high-dimensional space and detect anomalies within each one. Our experiments with both synthetic and real-world data demonstrate the effectiveness and efficiency of the proposed solution.

#index 1022240
#* Time series compressibility and privacy
#@ Spiros Papadimitriou;Feifei Li;George Kollios;Philip S. Yu
#t 2007
#c 4
#% 56600
#% 287794
#% 300184
#% 333876
#% 397389
#% 480628
#% 575972
#% 576111
#% 576761
#% 577289
#% 727904
#% 729943
#% 729962
#% 800557
#% 809245
#% 810010
#% 824686
#% 824726
#% 843878
#% 844360
#% 853239
#% 863402
#% 864412
#% 874988
#% 874989
#% 993961
#% 1015301
#% 1740518
#% 1815896
#% 1815965
#% 1818266
#! In this paper we study the trade-offs between time series compressibility and partial information hiding and their fundamental implications on how we should introduce uncertainty about individual values by perturbing them. More specifically, if the perturbation does not have the same compressibility properties as the original data, then it can be detected and filtered out, reducing uncertainty. Thus, by making the perturbation "similar" to the original data, we can both preserve the structure of the data better, while simultaneously making breaches harder. However, as data become more compressible, a fraction of the uncertainty can be removed if true values are leaked, revealing how they were perturbed. We formalize these notions, study the above trade-offs on real data and develop practical schemes which strike a good balance and can also be extended for on-the-fly data hiding in a streaming environment.

#index 1022241
#* A Bayesian method for guessing the extreme values in a data set?
#@ Mingxi Wu;Christopher Jermaine
#t 2007
#c 4
#% 102316
#% 152937
#% 210186
#% 227883
#% 248804
#% 273910
#% 300175
#% 300183
#% 479797
#% 479967
#% 570886
#% 729912
#% 798509
#! For a large number of data management problems, it would be very useful to be able to obtain a few samples from a data set, and to use the samples to guess the largest (or smallest) value in the entire data set. Min/max online aggregation, top-k query processing, outlier detection, and distance join are just a few possible applications. This paper details a statistically rigorous, Bayesian approach to attacking this problem. Just as importantly, we demonstrate the utility of our approach by showing how it can be applied to two specific problems that arise in the context of data management.

#index 1022242
#* Efficient processing of top-k dominating queries on multi-dimensional data
#@ Man Lung Yiu;Nikos Mamoulis
#t 2007
#c 4
#% 213975
#% 287466
#% 333854
#% 333951
#% 333977
#% 427199
#% 462059
#% 480671
#% 527189
#% 800555
#% 806212
#% 810024
#% 824670
#% 824671
#% 824672
#% 864451
#% 864452
#% 864453
#% 874975
#% 875012
#% 875025
#% 993954
#% 1011225
#% 1688273
#! The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages: (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.

#index 1022243
#* Best position algorithms for top-k queries
#@ Reza Akbarinia;Esther Pacitti;Patrick Valduriez
#t 2007
#c 4
#% 212665
#% 278831
#% 333854
#% 397378
#% 411758
#% 631988
#% 643566
#% 654443
#% 659255
#% 659993
#% 731409
#% 763882
#% 766671
#% 768521
#% 800509
#% 824704
#% 864455
#% 864530
#% 870255
#% 874894
#% 875023
#% 893126
#% 893128
#% 894443
#% 960251
#% 1015265
#% 1016196
#% 1099741
#! The general problem of answering top-k queries can be modeled using lists of data items sorted by their local scores. The most efficient algorithm proposed so far for answering top-k queries over sorted lists is the Threshold Algorithm (TA). However, TA may still incur a lot of useless accesses to the lists. In this paper, we propose two new algorithms which stop much sooner. First, we propose the best position algorithm (BPA) which executes top-k queries more efficiently than TA. For any database instance (i.e. set of sorted lists), we prove that BPA stops as early as TA, and that its execution cost is never higher than TA. We show that the position at which BPA stops can be (m-1) times lower than that of TA, where m is the number of lists. We also show that the execution cost of our algorithm can be (m-1) times lower than that of TA. Second, we propose the BPA2 algorithm which is much more efficient than BPA. We show that the number of accesses to the lists done by BPA2 can be about (m-1) times lower than that of BPA. Our performance evaluation shows that over our test databases, BPA and BPA2 achieve significant performance gains in comparison with TA.

#index 1022244
#* Sum-max monotonic ranked joins for evaluating top-k twig queries on weighted data graphs
#@ Yan Qi;K. Selçuk Candan;Maria Luisa Sapino
#t 2007
#c 4
#% 213981
#% 248818
#% 268079
#% 326669
#% 330678
#% 333989
#% 340914
#% 397359
#% 397360
#% 397375
#% 465155
#% 479465
#% 479967
#% 480819
#% 570875
#% 598374
#% 643566
#% 654442
#% 766671
#% 771860
#% 810018
#% 810046
#% 824693
#% 874975
#% 875000
#% 893134
#% 960246
#% 993953
#% 993987
#% 1015258
#% 1015268
#% 1015317
#% 1016176
#% 1180913
#! In many applications, the underlying data (the web, an XML document, or a relational database) can be seen as a graph. These graphs may be enriched with weights, associated with the nodes and edges of the graph, denoting application specific desirability/penalty assessments, such as popularity, trust, or cost. A particular challenge when considering such weights in query processing is that results need to be ranked accordingly. Answering keyword-based queries on weighted graphs is shown to be computationally expensive. In this paper, we first show that answering queries with further structure imposed on them remains NP-hard. We next show that, while the query evaluation task can be viewed in terms of ranked structural-joins along query axes, the monotonicity property, necessary for ranked join algorithms, is violated. Consequently, traditional ranked join algorithms are not directly applicable. Thus, we establish an alternative, sum-max monotonicity property and show how to leverage this for developing a self-punctuating, horizon-based ranked join (HR-Join) operator for ranked twig-query execution on data graphs. We experimentally show the effectiveness of the proposed evaluation schemes and the HR-join operator for merging ranked sub-results under sum-max monotonicity.

#index 1022245
#* Answering aggregation queries in a secure system model
#@ Tingjian Ge;Stan Zdonik
#t 2007
#c 4
#% 146203
#% 199537
#% 223781
#% 227861
#% 248812
#% 354287
#% 397367
#% 664705
#% 765448
#% 799891
#% 818434
#% 824697
#% 874980
#% 875026
#% 1386180
#% 1664117
#% 1706207
#! As more sensitive data is captured in electronic form, security becomes more and more important. Data encryption is the main technique for achieving security. While in the past enterprises were hesitant to implement database encryption because of the very high cost, complexity, and performance degradation, they now have to face the ever-growing risk of data theft as well as emerging legislative requirements. Data encryption can be done at multiple tiers within the enterprise. Different choices on where to encrypt the data offer different security features that protect against different attacks. One class of attack that needs to be taken seriously is the compromise of the database server, its software or administrator. A secure way to address this threat is for a DBMS to directly process queries on the ciphertext, without decryption. We conduct a comprehensive study on answering SUM and AVG aggregation queries in such a system model by using a secure homomorphic encryption scheme in a novel way. We demonstrate that the performance of such a solution is comparable to a traditional symmetric encryption scheme (e.g., DES) in which each value is decrypted and the computation is performed on the plaintext. Clearly this traditional encryption scheme is not a viable solution to the problem because the server must have access to the secret key and the plaintext, which violates our system model and security requirements. We study the problem in the setting of a read-optimized DBMS for data warehousing applications, in which SUM and AVG are frequent and crucial.

#index 1022246
#* The boundary between privacy and utility in data publishing
#@ Vibhor Rastogi;Dan Suciu;Sungho Hong
#t 2007
#c 4
#% 149
#% 300184
#% 576110
#% 576111
#% 576761
#% 751578
#% 809245
#% 810011
#% 810028
#% 864412
#% 874891
#% 874988
#% 893100
#% 993988
#% 1670071
#% 1728031
#% 1740518
#! We consider the privacy problem in data publishing: given a database instance containing sensitive information "anonymize" it to obtain a view such that, on one hand attackers cannot learn any sensitive information from the view, and on the other hand legitimate users can use it to compute useful statistics. These are conflicting goals. In this paper we prove an almost crisp separation of the case when a useful anonymization algorithm is possible from when it is not, based on the attacker's prior knowledge. Our definition of privacy is derived from existing literature and relates the attacker's prior belief for a given tuple t, with the posterior belief for the same tuple. Our definition of utility is based on the error bound on the estimates of counting queries. The main result has two parts. First we show that if the prior beliefs for some tuples are large then there exists no useful anonymization algorithm. Second, we show that when the prior is bounded for all tuples then there exists an anonymization algorithm that is both private and useful. The anonymization algorithm that forms our positive result is novel, and improves the privacy/utility tradeoff of previously known algorithms with privacy/utility guarantees such as FRAPP.

#index 1022247
#* Minimality attack in privacy preserving data publishing
#@ Raymond Chi-Wing Wong;Ada Wai-Chee Fu;Ke Wang;Jian Pei
#t 2007
#c 4
#% 300184
#% 576761
#% 576762
#% 577233
#% 785363
#% 800514
#% 801690
#% 810011
#% 864412
#% 874988
#% 874989
#% 881497
#% 881546
#% 881551
#% 893100
#% 951837
#% 960291
#% 963759
#% 1700134
#! Data publishing generates much concern over the protection of individual privacy. Recent studies consider cases where the adversary may possess different kinds of knowledge about the data. In this paper, we show that knowledge of the mechanism or algorithm of anonymization for data publication can also lead to extra information that assists the adversary and jeopardizes individual privacy. In particular, all known mechanisms try to minimize information loss and such an attempt provides a loophole for attacks. We call such an attack a minimality attack. In this paper, we introduce a model called m-confidentiality which deals with minimality attacks, and propose a feasible solution. Our experiments show that minimality attacks are practical concerns on real datasets and that our algorithm can prevent such attacks with very little overhead and information loss.

#index 1022248
#* On the correctness criteria of fine-grained access control in relational databases
#@ Qihua Wang;Ting Yu;Ninghui Li;Jorge Lobo;Elisa Bertino;Keith Irwin;Ji-Won Byun
#t 2007
#c 4
#% 663
#% 147801
#% 287246
#% 287313
#% 287333
#% 411572
#% 606353
#% 765447
#% 918054
#% 993943
#% 1016138
#! Databases are increasingly being used to store information covered by heterogeneous policies, which require support for access control with great flexibility. This has led to increasing interest in using fine-grained access control, where different cells in a relation may be governed by different access control rules. Although several proposals have been made to support fine-grained access control, there currently does not exist a formal notion of correctness regarding the query answering procedure. In this paper, we propose such a formal notion of correctness in fine-grained database access control, and discuss why existing approaches fall short in some circumstances. We then propose a labeling approach for masking unauthorized information and a query evaluation algorithm which better supports fine-grained access control. Finally, we implement our algorithm using query modification and evaluate its performance.

#index 1022249
#* Modeling and querying vague spatial objects using shapelets
#@ Daniel Zinn;Jim Bosch;Michael Gertz
#t 2007
#c 4
#% 300185
#% 396537
#% 449860
#% 527030
#% 731478
#% 771228
#% 824728
#% 836161
#% 1720765
#! Research in modeling and querying spatial data has primarily focused on traditional "crisp" spatial objects with exact location and spatial extent. More recent work, however, has begun to address the need for spatial data types describing spatial phenomena that cannot be modeled by objects having sharp boundaries. Other work has focused on point objects whose location is not precisely known and is typically described using a probability distribution. In this paper, we present a new technique for modeling and querying vague spatial objects. Using shapelets, an image decomposition technique developed in astronomy, as base data type, we introduce a comprehensive set of low-level operations that provide building blocks for versatile high-level operations on vague spatial objects. In addition, we describe an implementation of this data model as an extension to PostgreSQL, including an indexing technique for shapelet objects. Unlike existing techniques for modeling and querying vague or fuzzy data, our approach is optimized for localized, smoothly varying spatial objects, and as such is more suitable for many real-world datasets.

#index 1022250
#* On efficient spatial matching
#@ Raymond Chi-Wing Wong;Yufei Tao;Ada Wai-Chee Fu;Xiaokui Xiao
#t 2007
#c 4
#% 86950
#% 201876
#% 235114
#% 248804
#% 287466
#% 300162
#% 300163
#% 462059
#% 480661
#% 495427
#% 599803
#% 629665
#% 737337
#% 814646
#% 824730
#% 847167
#% 875013
#% 1669937
#! This paper proposes and solves a new problem called spatial matching (SPM). Let P and O be two sets of objects in an arbitrary metric space, where object distances are defined according to a norm satisfying the triangle inequality. Each object in O represents a customer, and each object in P indicates a service provider, which has a capacity corresponding to the maximum number of customers that can be supported by the provider. SPM assigns each customer to her/his nearest provider, among all the providers whose capacities have not been exhausted in serving other closer customers. We elaborate the applications where SPM is useful, and develop algorithms that settle this problem with a linear number O(|P| + |O|) of nearest neighbor queries. We verify our theoretical findings with extensive experiments, and show that the proposed solutions outperform alternative methods by a factor of orders of magnitude.

#index 1022251
#* Main-memory operation buffering for efficient R-tree update
#@ Laurynas Biveinis;Simonas Šaltenis;Christian S. Jensen
#t 2007
#c 4
#% 86950
#% 295512
#% 300174
#% 427199
#% 443093
#% 458528
#% 480651
#% 481304
#% 485022
#% 503869
#% 527174
#% 567865
#% 800186
#% 838505
#% 857498
#% 864410
#% 870306
#% 870307
#% 1015305
#% 1669479
#! Emerging communication and sensor technologies enable new applications of database technology that require database systems to efficiently support very high rates of spatial-index updates. Previous works in this area require the availability of large amounts of main memory, do not exploit all the main memory that is indeed available, or do not support some of the standard index operations. Assuming a setting where the index updates need not be written to disk immediately, we propose an R-tree-based indexing technique that does not exhibit any of these drawbacks. This technique exploits the buffering of update operations in main memory as well as the grouping of operations to reduce disk I/O. In particular, operations are performed in bulk so that multiple operations are able to share I/O. The paper presents an analytical cost model that is shown to be accurate by empirical studies. The studies also show that, in terms of update I/O performance, the proposed technique improves on state of the art in settings with frequent updates.

#index 1022252
#* Monitoring business processes with queries
#@ Catriel Beeri;Anat Eyal;Tova Milo;Alon Pilberg
#t 2007
#c 4
#% 86944
#% 86957
#% 279905
#% 351041
#% 465061
#% 480938
#% 654476
#% 655355
#% 742565
#% 754120
#% 781453
#% 800633
#% 801685
#% 814647
#% 869489
#% 893117
#% 960349
#% 994005
#% 1015276
#% 1704054
#! Many enterprises nowadays use business processes, based on the BPEL standard, to achieve their goals. These are complex, often distributed, processes. Monitoring the execution of such processes for interesting patterns is critical for enforcing business policies and meeting efficiency and reliability goals. BP-Mon (Business Processes Monitoring) is a novel query language for monitoring business processes, that allows users to visually define monitoring tasks and associated reports, using a simple intuitive interface, similar to those used for designing BPEL processes. We describe here the BP-Mon language and its underlying formal model. We also present the language implementation and describe our novel optimization techniques. An important feature of the implementation is that BP-Mon queries are translated to BPEL processes that run on the same execution engine as the monitored processes. Our experiments indicate that this approach incurs very minimal overhead, hence is a practical and efficient approach to monitoring.

#index 1022253
#* An approach to optimize data processing in business processes
#@ Marko Vrhovnik;Holger Schwarz;Oliver Suhre;Bernhard Mitschang;Volker Markl;Albert Maier;Tobias Kraft
#t 2007
#c 4
#% 36117
#% 98100
#% 116043
#% 300166
#% 330305
#% 333848
#% 397393
#% 458856
#% 480268
#% 765420
#% 825657
#% 845351
#% 893118
#% 903261
#% 1015295
#% 1304915
#% 1709209
#! In order to optimize their revenues and profits, an increasing number of businesses organize their business activities in terms of business processes. Typically, they automate important business tasks by orchestrating a number of applications and data stores. Obviously, the performance of a business process is directly dependent on the efficiency of data access, data processing, and data management. In this paper, we propose a framework for the optimization of data processing in business processes. We introduce a set of rewrite rules that transform a business process in such a way that an improved execution with respect to data management can be achieved without changing the semantics of the original process. These rewrite rules are based on a semi-procedural process graph model that externalizes data dependencies as well as control flow dependencies of a business process. Furthermore, we present a multi-stage control strategy for the optimization process. We illustrate the benefits and opportunities of our approach through a prototype implementation. Our experimental results demonstrate that independent of the underlying database system performance gains of orders of magnitude are achievable by reasoning about data and control in a unified framework.

#index 1022254
#* Reasoning about the behavior of Semantic Web services with concurrent transaction logic
#@ Dumitru Roman;Michael Kifer
#t 2007
#c 4
#% 169697
#% 248013
#% 264853
#% 451429
#% 464235
#% 481261
#% 562155
#% 769364
#% 798290
#% 898163
#% 910587
#% 993989
#! The recent upsurge in the interest in Semantic Web services and the high-profile projects such as the WSMO, OWLS, and SWSL, have drawn attention to the importance of logic-based modeling of the behavior of Web services. In the context of Semantic Web services, the logic-based approach has many applications, including service discovery, service choreography, enactment, and contracting for services. In this paper we propose logic-based methods for reasoning about service behavior, including the aforementioned choreography, contracting, and enactment. The formalism underlying our framework is Concurrent Transaction Logic---a logic for declarative specification, analysis, and execution of database transactions. The new results include reasoning about service behavior under more general sets of constraints and extension of the framework towards conditional control and data flow---two crucial aspect that were missing in previous logical formalizations.

#index 1022255
#* Randomized algorithms for data reconciliation in wide area aggregate query processing
#@ Fei Xu;Christopher Jermaine
#t 2007
#c 4
#% 227883
#% 273687
#% 332166
#% 397354
#% 397393
#% 431103
#% 480417
#% 480805
#% 572314
#% 577238
#% 654459
#% 654467
#% 659991
#% 762652
#% 765463
#% 769884
#% 790848
#% 810020
#% 824769
#% 824787
#% 864415
#% 864444
#% 875066
#% 893089
#! Many aspects of the data integration problem have been considered in the literature: how to match schemas across different data sources, how to decide when different records refer to the same entity, how to efficiently perform the required entity resolution in a batch fashion, and so on. However, what has largely been ignored is a way to efficiently deploy these existing methods in a realistic, distributed enterprise integration environment. The straightforward use of existing methods often requires that all data be shipped to a coordinator for cleaning, which is often unacceptable. We develop a set of randomized algorithms that allow efficient application of existing entity resolution methods to the answering of aggregate queries over data that have been distributed across multiple sites. Using our methods, it is possible to efficiently generate aggregate query results that account for duplicate and inconsistent values scattered across a federated system.

#index 1022256
#* Query processing over incomplete autonomous databases
#@ Garrett Wolf;Hemal Khatri;Bhaumik Chokshi;Jianchun Fan;Yi Chen;Subbarao Kambhampati
#t 2007
#c 4
#% 663
#% 17144
#% 243727
#% 287313
#% 333990
#% 349549
#% 376266
#% 464526
#% 464837
#% 654487
#% 765434
#% 800444
#% 810120
#% 824733
#% 864394
#% 864417
#% 864432
#% 879041
#% 893168
#% 1015317
#% 1269490
#! Incompleteness due to missing attribute values (aka "null values") is very common in autonomous web databases, on which user accesses are usually supported through mediators. Traditional query processing techniques that focus on the strict soundness of answer tuples often ignore tuples with critical missing attributes, even if they wind up being relevant to a user query. Ideally we would like the mediator to retrieve such possible answers and gauge their relevance by accessing their likelihood of being pertinent answers to the query. The autonomous nature of web databases poses several challenges in realizing this objective. Such challenges include the restricted access privileges imposed on the data, the limited support for query patterns, and the bounded pool of database and network resources in the web environment. We introduce a novel query rewriting and optimization framework QPIAD that tackles these challenges. Our technique involves reformulating the user query based on mined correlations among the database attributes. The reformulated queries are aimed at retrieving the relevant possible answers in addition to the certain answers. QPIAD is able to gauge the relevance of such queries allowing tradeoffs in reducing the costs of database query processing and answer transmission. To support this framework, we develop methods for mining attribute correlations (in terms of Approximate Functional Dependencies), value distributions (in the form of Naïve Bayes Classifiers), and selectivity estimates. We present empirical studies to demonstrate that our approach is able to effectively retrieve relevant possible answers with high precision, high recall, and manageable cost.

#index 1022257
#* iTrails: pay-as-you-go information integration in dataspaces
#@ Marcos Antonio Vaz Salles;Jens-Peter Dittrich;Shant Kirakos Karakashian;Olivier René Girard;Lukas Blunschi
#t 2007
#c 4
#% 144029
#% 210182
#% 232656
#% 235023
#% 268079
#% 273914
#% 283052
#% 378393
#% 397608
#% 463919
#% 478258
#% 479465
#% 479813
#% 481923
#% 566136
#% 572311
#% 572314
#% 654465
#% 765408
#% 765446
#% 800577
#% 810073
#% 824703
#% 824773
#% 845350
#% 867054
#% 874876
#% 893089
#% 893119
#% 893167
#% 1015258
#% 1016135
#% 1688265
#% 1721851
#! Dataspace management has been recently identified as a new agenda for information management [17, 22] and information integration [23]. In sharp contrast to standard information integration architectures, a dataspace management system is a data-coexistence approach: it does not require any investments in semantic integration before querying services on the data are provided. Rather, a dataspace can be gradually enhanced over time by defining relationships among the data. Defining those integration semantics gradually is termed pay-as-you-go information integration [17], as time and effort (pay) are needed over time (go) to provide integration semantics. The benefits are better query results (gain). This paper is the first to explore pay-as-you-go information integration in dataspaces. We provide a technique for declarative pay-as-you-go information integration named iTrails. The core idea of our approach is to declaratively add lightweight 'hints' (trails) to a search engine thus allowing gradual enrichment of loosely integrated data sources. Our experiments confirm that iTrails can be efficiently implemented introducing only little overhead during query execution. At the same time iTrails strongly improves the quality of query results. Furthermore, we present rewriting and pruning techniques that allow us to scale iTrails to tens of thousands of trail definitions with minimal growth in the rewritten query size.

#index 1022258
#* Update exchange with mappings and provenance
#@ Todd J. Green;Grigoris Karvounarakis;Zachary G. Ives;Val Tannen
#t 2007
#c 4
#% 152928
#% 201930
#% 237190
#% 273911
#% 283052
#% 378409
#% 465053
#% 481923
#% 654468
#% 715288
#% 801692
#% 809248
#% 826032
#% 857502
#% 874882
#% 874971
#% 893095
#% 893167
#% 976987
#% 976995
#% 993981
#% 1408724
#% 1711274
#! We consider systems for data sharing among heterogeneous peers related by a network of schema mappings. Each peer has a locally controlled and edited database instance, but wants to ask queries over related data from other peers as well. To achieve this, every peer's updates propagate along the mappings to the other peers. However, this update exchange is filtered by trust conditions --- expressing what data and sources a peer judges to be authoritative --- which may cause a peer to reject another's updates. In order to support such filtering, updates carry provenance information. These systems target scientific data sharing applications, and their general principles and architecture have been described in [20]. In this paper we present methods for realizing such systems. Specifically, we extend techniques from data integration, data exchange, and incremental view maintenance to propagate updates along mappings; we integrate a novel model for tracking data provenance, such that curators may filter updates based on trust conditions over this provenance; we discuss strategies for implementing our techniques in conjunction with an RDBMS; and we experimentally demonstrate the viability of our techniques in the ORCHESTRA prototype system.

#index 1022259
#* Data integration with uncertainty
#@ Xin Dong;Alon Y. Halevy;Cong Yu
#t 2007
#c 4
#% 44876
#% 248038
#% 333854
#% 378409
#% 482108
#% 572311
#% 572314
#% 810073
#% 810120
#% 874876
#% 874975
#% 893089
#% 893167
#% 993987
#% 1661428
#% 1661429
#% 1661439
#% 1730010
#! This paper reports our first set of results on managing uncertainty in data integration. We posit that data-integration systems need to handle uncertainty at three levels, and do so in a principled fashion. First, the semantic mappings between the data sources and the mediated schema may be approximate because there may be too many of them to be created and maintained or because in some domains (e.g., bioinformatics) it is not clear what the mappings should be. Second, queries to the system may be posed with keywords rather than in a structured form. Third, the data from the sources may be extracted using information extraction techniques and so may yield imprecise data. As a first step to building such a system, we introduce the concept of probabilistic schema mappings and analyze their formal foundations. We show that there are two possible semantics for such mappings: by-table semantics assumes that there exists a correct mapping but we don't know what it is; by-tuple semantics assumes that the correct mapping may depend on the particular tuple in the source data. We present the query complexity and algorithms for answering queries in the presence of approximate schema mappings, and we describe an algorithm for efficiently computing the top-k answers to queries in such a setting.

#index 1022260
#* Context-aware wrapping: synchronized data extraction
#@ Shui-Lung Chuang;Kevin Chen-Chuan Chang;ChengXiang Zhai
#t 2007
#c 4
#% 210985
#% 269195
#% 278109
#% 397605
#% 464434
#% 480824
#% 572314
#% 654469
#% 660001
#% 765409
#% 765411
#% 769877
#% 805845
#% 805846
#% 840966
#% 864416
#% 1271981
#% 1782837
#! The deep Web presents a pressing need for integrating large numbers of dynamically evolving data sources. To be more automatic yet accurate in building an integration system, we observe two problems: First, across sequential tasks in integration, how can a wrapper (as an extraction task) consider the peer sources to facilitate the subsequent matching task? Second, across parallel sources, how can a wrapper leverage the peer wrappers or domain rules to enhance extraction accuracy? These issues, while seemingly unrelated, both boil down to the lack of "context awareness": Current automatic wrapper induction approaches generate a wrapper for one source at a time, in isolation, and thus inherently lack the awareness of the peer sources or domain knowledge in the context of integration. We propose the concept of context-aware wrappers that are amenable to matching and that can leverage peer wrappers or prior domain knowledge. Such context awareness inspires a synchronization framework to construct wrappers consistently and collaboratively across their mutual context. We draw the insight from turbo codes and develop the turbo syncer to interconnect extraction with matching, which together achieve context awareness in wrapping. Our experiments show that the turbo syncer can, on the one hand, enhance extraction consistency and thus increase matching accuracy (from 17--83% to 78--94% in F-measure) and, on the other hand, incorporate peer wrappers and domain knowledge seamlessly to reduce extraction errors (from 09--60% to 01--11%).

#index 1022261
#* Processing forecasting queries
#@ Songyun Duan;Shivanath Babu
#t 2007
#c 4
#% 227883
#% 411554
#% 420108
#% 632090
#% 727663
#% 729952
#% 814041
#% 823422
#% 859208
#% 874976
#% 875965
#% 878299
#% 892620
#% 926881
#% 993961
#% 1016178
#! Forecasting future events based on historic data is useful in many domains like system management, adaptive query processing, environmental monitoring, and financial planning. We describe the Fa system where users and applications can pose declarative forecasting queries---both one-time queries and continuous queries---and get forecasts in real-time along with accuracy estimates. Fa supports efficient algorithms to generate execution plans automatically for forecasting queries from a novel plan space comprising operators for transforming data, learning statistical models from data, and doing inference using the learned models. In addition, Fa supports adaptive query-processing algorithms that adapt plans for continuous forecasting queries to the time-varying properties of input data streams. We report an extensive experimental evaluation of Fa using synthetic datasets, datasets collected on a testbed, and two real datasets from production settings. Our experiments give interesting insights on plans for forecasting queries, and demonstrate the effectiveness and scalability of our plan-selection algorithms.

#index 1022262
#* Cooperative scans: dynamic bandwidth sharing in a DBMS
#@ Marcin Zukowski;Sándor Héman;Niels Nes;Peter Boncz
#t 2007
#c 4
#% 4683
#% 187411
#% 280521
#% 300166
#% 321683
#% 333848
#% 479630
#% 480607
#% 480821
#% 481092
#% 482086
#% 503721
#% 565469
#% 617842
#% 810039
#% 824697
#% 864446
#% 875026
#% 893129
#% 978976
#% 993385
#! This paper analyzes the performance of concurrent (index) scan operations in both record (NSM/PAX) and column (DSM) disk storage models and shows that existing scheduling policies do not fully exploit data-sharing opportunities and therefore result in poor disk bandwidth utilization. We propose the Cooperative Scans framework that enhances performance in such scenarios by improving data-sharing between concurrent scans. It performs dynamic scheduling of queries and their data requests, taking into account the current system situation. We first present results on top of an NSM/PAX storage layout, showing that it achieves significant performance improvements over traditional policies in terms of both the number of I/Os and overall execution time, as well as latency of individual queries. We provide benchmarks with varying system parameters, data sizes and query loads to confirm the improvement occurs in a wide range of scenarios. Then we extend our proposal to a more complicated DSM scenario, discussing numerous problems related to the two-dimensional nature of disk scheduling in column stores.

#index 1022263
#* Stop-and-restart style execution for long running decision support queries
#@ Surajit Chaudhuri;Raghav Kaushik;Abhijit Pol;Ravi Ramamurthy
#t 2007
#c 4
#% 136740
#% 248793
#% 273917
#% 279164
#% 300127
#% 480128
#% 481916
#% 765456
#% 765467
#% 765468
#% 800589
#% 810016
#% 810056
#% 882036
#! Long running decision support queries can be resource intensive and often lead to resource contention in data warehousing systems. Today, the only real option available to the DBAs when faced with such contention is to carefully select one or more queries and terminate them. However, the work done by such terminated queries is entirely lost even if they were very close to completion and these queries will need to be run in their entirety at a later time. In this paper, we show how instead we can support a Stop-and-Restart style query execution that can leverage partially the work done in the initial query execution. In order to re-execute only the remaining work of the query, a Stop-and-Restart execution would need to save all the previous work. But this approach would clearly incur high overheads which is undesirable. In contrast, we present a technique that can be used to save information selectively from the past execution so that the overhead can be bounded. Despite saving only limited information, our technique is able to reduce the running time of the restarted queries substantially. We show the effectiveness of our approach using real and benchmark data.

#index 1022264
#* K-anonymization as spatial indexing: toward scalable and incremental anonymization
#@ Tochukwu Iwuchukwu;Jeffrey F. Naughton
#t 2007
#c 4
#% 86950
#% 86951
#% 153260
#% 248030
#% 285932
#% 427199
#% 443463
#% 452821
#% 479473
#% 480093
#% 481304
#% 481428
#% 481455
#% 576761
#% 576762
#% 577239
#% 765430
#% 785363
#% 800514
#% 800515
#% 810011
#% 864406
#% 864412
#% 874988
#% 881483
#% 881546
#% 881551
#% 893151
#! In this paper we observe that k-anonymizing a data set is strikingly similar to building a spatial index over the data set, so similar in fact that classical spatial indexing techniques can be used to anonymize data sets. We use this observation to leverage over 20 years of work on database indexing to provide efficient and dynamic anonymization techniques. Experiments with our implementation show that the R-tree index-based approach yields a batch anonymization algorithm that is orders of magnitude more efficient than previously proposed algorithms and has the advantage of supporting incremental updates. Finally, we show that the anonymizations generated by the R-tree approach do not sacrifice quality in their search for efficiency; in fact, by several previously proposed quality metrics, the compact partitioning properties of R-trees generate anonymizations superior to those generated by previously proposed anonymization algorithms.

#index 1022265
#* Fast data anonymization with low information loss
#@ Gabriel Ghinita;Panagiotis Karras;Panos Kalnis;Nikos Mamoulis
#t 2007
#c 4
#% 210182
#% 248030
#% 317313
#% 443397
#% 576761
#% 577239
#% 800515
#% 801690
#% 810011
#% 833130
#% 864406
#% 864412
#% 874892
#% 874988
#% 874989
#% 881483
#% 881551
#% 893100
#! Recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy preserving paradigms of k-anonymity and l-diversity. k-anonymity protects against the identification of an individual's record. l-diversity, in addition, safeguards against the association of an individual with specific sensitive information. However, existing approaches suffer from at least one of the following drawbacks: (i) The information loss metrics are counter-intuitive and fail to capture data inaccuracies inflicted for the sake of privacy. (ii) l-diversity is solved by techniques developed for the simpler k-anonymity problem, which introduces unnecessary inaccuracies. (iii) The anonymization process is inefficient in terms of computation and I/O cost. In this paper we propose a framework for efficient privacy preservation that addresses these deficiencies. First, we focus on one-dimensional (i.e., single attribute) quasi-identifiers, and study the properties of optimal solutions for k-anonymity and l-diversity, based on meaningful information loss metrics. Guided by these properties, we develop efficient heuristics to solve the one-dimensional problems in linear time. Finally, we generalize our solutions to multi-dimensional quasi-identifiers using space-mapping techniques. Extensive experimental evaluation shows that our techniques clearly outperform the state-of-the-art, in terms of execution time and information loss.

#index 1022266
#* Privacy skyline: privacy with multidimensional adversarial knowledge
#@ Bee-Chung Chen;Kristen LeFevre;Raghu Ramakrishnan
#t 2007
#c 4
#% 216970
#% 452821
#% 576761
#% 765449
#% 810011
#% 824727
#% 864406
#% 864412
#% 874893
#% 874988
#% 874989
#% 881483
#% 893100
#% 1700133
#% 1700137
#! Privacy is an important issue in data publishing. Many organizations distribute non-aggregate personal data for research, and they must take steps to ensure that an adversary cannot predict sensitive information pertaining to individuals with high confidence. This problem is further complicated by the fact that, in addition to the published data, the adversary may also have access to other resources (e.g., public records and social networks relating individuals), which we call external knowledge. A robust privacy criterion should take this external knowledge into consideration. In this paper, we first describe a general framework for reasoning about privacy in the presence of external knowledge. Within this framework, we propose a novel multidimensional approach to quantifying an adversary's external knowledge. This approach allows the publishing organization to investigate privacy threats and enforce privacy requirements in the presence of various types and amounts of external knowledge. Our main technical contributions include a multidimensional privacy criterion that is more intuitive and flexible than previous approaches to modeling background knowledge. In addition, we provide algorithms for measuring disclosure and sanitizing data that improve computational efficiency several orders of magnitude over the best known techniques.

#index 1022267
#* Integrity auditing of outsourced data
#@ Min Xie;Haixun Wang;Jian Yin;Xiaofeng Meng
#t 2007
#c 4
#% 319994
#% 362036
#% 374401
#% 397367
#% 513367
#% 551826
#% 566391
#% 593797
#% 745532
#% 765448
#% 810042
#% 824701
#% 874980
#! An increasing number of enterprises outsource their IT services to third parties who can offer these services for a much lower cost due to economy of scale. Quality of service is a major concern in outsourcing. In particular, query integrity, which means that query results returned by the service provider are both correct and complete, must be assured. Previous work requires clients to manage data locally to audit the results sent back by the server, or database engine to be modified for generating authenticated results. In this paper, we introduce a novel integrity audit mechanism that eliminating these costly requirements. In our approach, we insert a small amount of records into an outsourced database so that the integrity of the system can be effectively audited by analyzing the inserted records in the query results. We study both randomized and deterministic approaches for generating the inserted records, as how these records are generated has significant implications for storage and performance. Furthermore, we show that our method is provable secure, which means it can withstand any attacks by an adversary whose computation power is bounded. Our analytical and empirical results demonstrate the effectiveness of our method.

#index 1022268
#* Adaptive fastest path computation on a road network: a traffic mining approach
#@ Hector Gonzalez;Jiawei Han;Xiaolei Li;Margaret Myslinska;John Paul Sondag
#t 2007
#c 4
#% 214769
#% 267476
#% 298270
#% 327432
#% 338580
#% 375388
#% 449588
#% 463903
#% 464223
#% 464996
#% 479787
#% 572923
#% 593917
#% 677994
#% 749474
#% 864397
#% 864470
#% 875476
#% 893157
#% 1272280
#% 1676469
#! Efficient fastest path computation in the presence of varying speed conditions on a large scale road network is an essential problem in modern navigation systems. Factors affecting road speed, such as weather, time of day, and vehicle type, need to be considered in order to select fast routes that match current driving conditions. Most existing systems compute fastest paths based on road Euclidean distance and a small set of predefined road speeds. However, "History is often the best teacher". Historical traffic data or driving patterns are often more useful than the simple Euclidean distance-based computation because people must have good reasons to choose these routes, e.g., they may want to avoid those that pass through high crime areas at night or that likely encounter accidents, road construction, or traffic jams. In this paper, we present an adaptive fastest path algorithm capable of efficiently accounting for important driving and speed patterns mined from a large set of traffic data. The algorithm is based on the following observations: (1) The hierarchy of roads can be used to partition the road network into areas, and different path pre-computation strategies can be used at the area level, (2) we can limit our route search strategy to edges and path segments that are actually frequently traveled in the data, and (3) drivers usually traverse the road network through the largest roads available given the distance of the trip, except if there are small roads with a significant speed advantage over the large ones. Through an extensive experimental evaluation on real road networks we show that our algorithm provides desirable (short and well-supported) routes, and that it is significantly faster than competing methods.

#index 1022269
#* Seeking stable clusters in the blogosphere
#@ Nilesh Bansal;Fei Chiang;Nick Koudas;Frank Wm. Tompa
#t 2007
#c 4
#% 70370
#% 258598
#% 279755
#% 281655
#% 303087
#% 333854
#% 765548
#% 847164
#% 1016219
#% 1022338
#! The popularity of blogs has been increasing dramatically over the last couple of years. As topics evolve in the blogosphere, keywords align together and form the heart of various stories. Intuitively we expect that in certain contexts, when there is a lot of discussion on a specific topic or event, a set of keywords will be correlated: the keywords in the set will frequently appear together (pair-wise or in conjunction) forming a cluster. Note that such keyword clusters are temporal (associated with specific time periods) and transient. As topics recede, associated keyword clusters dissolve, because their keywords no longer appear frequently together. In this paper, we formalize this intuition and present efficient algorithms to identify keyword clusters in large collections of blog posts for specific temporal intervals. We then formalize problems related to the temporal properties of such clusters. In particular, we present efficient algorithms to identify clusters that persist over time. Given the vast amounts of data involved, we present algorithms that are fast (can efficiently process millions of blogs with multiple millions of posts) and take special care to make them efficiently realizable in secondary storage. Although we instantiate our techniques in the context of blogs, our methodology is generic enough to apply equally well to any temporally ordered text source. We present the results of an experimental study using both real and synthetic data sets, demonstrating the efficiency of our algorithms, both in terms of performance and in terms of the quality of the keyword clusters and associated temporal properties we identify.

#index 1022270
#* On dominating your neighborhood profitably
#@ Cuiping Li;Anthony K. H. Tung;Wen Jin;Martin Ester
#t 2007
#c 4
#% 2115
#% 100803
#% 201876
#% 210173
#% 249305
#% 288976
#% 410276
#% 420082
#% 427199
#% 465167
#% 480671
#% 654480
#% 806212
#% 875025
#% 993954
#! Recent research on skyline queries has attracted much interest in the database and data mining community. Given a database, an object belongs to the skyline if it cannot be dominated with respect to the given attributes by any other database object. Current methods have only considered so-called min/max attributes like price and quality which a user wants to minimize or maximize. However, objects can also have spatial attributes like x, y coordinates which can be used to represent relevant constraints on the query results. In this paper, we introduce novel skyline query types taking into account not only min/max attributes but also spatial attributes and the relationships between these different attribute types. Such queries support a micro-economic approach to decision making, considering not only the quality but also the cost of solutions. We investigate two alternative approaches for efficient query processing, a symmetrical one based on off-the-shelf index structures, and an asymmetrical one based on index structures with special purpose extensions. Our experimental evaluation using a real dataset and various synthetic datasets demonstrates that the new query types are indeed meaningful and the proposed algorithms are efficient and scalable.

#index 1022271
#* Detecting attribute dependencies from query feedback
#@ Peter J. Haas;Fabian Hueske;Volker Markl
#t 2007
#c 4
#% 273902
#% 303703
#% 333946
#% 333947
#% 333986
#% 347226
#% 397371
#% 414110
#% 443390
#% 480803
#% 765455
#% 769909
#% 864426
#% 1015285
#% 1015310
#% 1016225
#% 1688310
#! Real-world datasets exhibit a complex dependency structure among the data attributes. Learning this structure is a key task in automatic statistics configuration for query optimizers, as well as in data mining, metadata discovery, and system management. In this paper, we provide a new method for discovering dependent attribute pairs based on query feedback. Our approach avoids the problem of searching through a combinatorially large space of candidate attribute pairs, automatically focusing system resources on those pairs of demonstrable interest to users. Unlike previous methods, our technique combines all of the pertinent feedback for a specified pair of attributes in a principled and robust manner, while being simple and fast enough to be incorporated into current commercial products. The method is similar in spirit to the CORDS algorithm, which proactively collects frequencies of data values and computes a chi-squared statistic from the resulting contingency table. In the reactive query-feedback setting, many entries of the contingency table are missing, and a key contribution of this paper is a variant of classical chi-squared theory that handles this situation. Because we typically discover a large number of dependent attribute pairs, we provide novel methods for ranking the pairs based on degree of dependency. Such ranking information, e.g., enables a database system to avoid exceeding the space budget for the system catalog by storing only the currently most important multivariate statistics. Experiments indicate that our dependency rankings are stable even in the presence of relatively few feedback records.

#index 1022272
#* Suppression and failures in sensor networks: a Bayesian approach
#@ Adam Silberstein;Gavino Puggioni;Alan Gelfand;Kamesh Munagala;Jun Yang
#t 2007
#c 4
#% 235061
#% 333969
#% 342614
#% 654488
#% 731086
#% 731087
#% 745442
#% 765402
#% 783729
#% 783732
#% 783741
#% 810031
#% 864435
#% 873104
#% 874976
#% 874983
#% 893189
#% 1016178
#% 1016201
#% 1668029
#% 1740388
#! Sensor networks allow continuous data collection on unprecedented scales. The primary limiting factor of such networks is energy, of which communication is the dominant consumer. The default strategy of nodes continually reporting their data to the root results in too much messaging. Suppression stands to greatly alleviate this problem. The simplest such scheme is temporal suppression, in which a node transmits its reading only when it has changed beyond some e since last transmitted. In the absence of a report, the root can infer that the value remains within ±ε hence, it is still able to derive the history of readings produced at the node. The critical weakness of suppression is message failure, to which sensor networks are particularly vulnerable. Failure creates ambiguity: a non-report may either be a suppression or a failure. Inferring the correct values for missing data and learning the parameters of the underlying process model become quite challenging. We propose a novel solution, BaySail, that incorporates the knowledge of the suppression scheme and application-level redundancy in Bayesian inference. We investigate several redundancy schemes and evaluate them in terms of in-network transmission costs and out-of-network inference efficacy, and the trade-off between these. Our experimental evaluation shows application-level redundancy outperforms retransmissions and basic sampling in both cost and accuracy of inference. The BaySail framework shows suppression schemes are generally effective for data collection, despite the presence of failures.

#index 1022273
#* MIST: distributed indexing and querying in sensor networks using statistical models
#@ Arnab Bhattacharya;Anand Meka;Ambuj K. Singh
#t 2007
#c 4
#% 78695
#% 174161
#% 189574
#% 292004
#% 333969
#% 427199
#% 429194
#% 479462
#% 765402
#% 798125
#% 800505
#% 812596
#% 864435
#% 874984
#% 895187
#% 938509
#% 1016178
#% 1548548
#! The modeling of high level semantic events from low level sensor signals is important in order to understand distributed phenomena. For such content-modeling purposes, transformation of numeric data into symbols and the modeling of resulting symbolic sequences can be achieved using statistical models---Markov Chains (MCs) and Hidden Markov Models (HMMs). We consider the problem of distributed indexing and semantic querying over such sensor models. Specifically, we are interested in efficiently answering (i) range queries: return all sensors that have observed an unusual sequence of symbols with a high likelihood, (ii) top-1 queries: return the sensor that has the maximum probability of observing a given sequence, and (iii) 1-NN queries: return the sensor (model) which is most similar to a query model. All the above queries can be answered at the centralized base station, if each sensor transmits its model to the base station. However, this is communication-intensive. We present a much more efficient alternative---a distributed index structure, MIST (Model-based Index STructure), and accompanying algorithms for answering the above queries. MIST aggregates two or more constituent models into a single composite model, and constructs an in-network hierarchy over such composite models. We develop two kinds of composite models: the first kind captures the average behavior of the underlying models and the second kind captures the extreme behaviors of the underlying models. Using the index parameters maintained at the root of a subtree, we bound the probability of observation of a query sequence from a sensor in the subtree. We also bound the distance of a query model to a sensor model using these parameters. Extensive experimental evaluation on both real-world and synthetic data sets show that the MIST schemes scale well in terms of network size and number of model states. We also show its superior performance over the centralized schemes in terms of update, query, and total communication costs.

#index 1022274
#* Early profile pruning on XML-aware publish-subscribe systems
#@ Mirella M. Moro;Petko Bakalov;Vassilis J. Tsotras
#t 2007
#c 4
#% 333938
#% 342372
#% 427199
#% 465061
#% 480296
#% 640616
#% 654476
#% 654477
#% 659995
#% 731408
#% 765441
#% 784523
#% 791182
#% 800517
#% 800591
#% 824668
#% 824669
#% 839584
#% 844343
#% 864645
#% 866988
#% 960295
#% 960296
#% 960297
#% 993950
#% 1015277
#% 1016180
#! Publish-subscribe applications are an important class of content-based dissemination systems where the message transmission is defined by the message content, rather than its destination IP address. With the increasing use of XML as the standard format on many Internet-based applications, XML aware pub-sub applications become necessary. In such systems, the messages (generated by publishers) are encoded as XML documents, and the profiles (defined by subscribers) as XML query statements. As the number of documents and query requests grow, the performance and scalability of the matching phase (i.e. matching of queries to incoming documents) become vital. Current solutions have limited or no flexibility to prune out queries in advance. In this paper, we overcome such limitation by proposing a novel early pruning approach called Bounding-based XML Filtering or BoXFilter. The BoXFilter is based on a new tree-like indexing structure that organizes the queries based on their similarity and provides lower and upper bound estimations needed to prune queries not related to the incoming documents. Our experimental evaluation shows that the early profile pruning approach offers drastic performance improvements over the current state-of-the-art in XML filtering.

#index 1022275
#* Value-based notification conditions in large-scale publish/subscribe systems?
#@ Badrish Chandramouli;Jeff M. Phillips;Jun Yang
#t 2007
#c 4
#% 23651
#% 116082
#% 300179
#% 333969
#% 333982
#% 338354
#% 340176
#% 348071
#% 397355
#% 443298
#% 556654
#% 598374
#% 656697
#% 732000
#% 736390
#% 793899
#% 800517
#% 800523
#% 821923
#% 875019
#% 993975
#% 1015259
#% 1016180
#! We address the problem of providing scalable support for subscriptions with personalized value-based notification conditions in wide-area publish/subscribe systems. Notification conditions can be fine-tuned by subscribers, allowing precise and flexible control of when events are delivered to the subscribers. For example, a user may specify that she should be notified if and only if the price of a particular stock moves outside a "radius" around her last notified value. Naive techniques for handling notification conditions are not scalable. It is challenging to share subscription processing and notification dissemination of subscriptions with personalized value-based notification conditions, because two subscriptions may see two completely different sequences of notifications even if they specify the same radius. We develop and experimentally evaluate scalable processing and dissemination techniques for these subscriptions. Our approach uses standard network substrates for notification dissemination, and avoids pushing complex application processing into the network. Compared with other alternatives, our approach generates orders of magnitude lower network traffic, and incurs lower server processing cost.

#index 1022276
#* Efficiently answering top-k typicality queries on large databases
#@ Ming Hua;Jian Pei;Ada W. C. Fu;Xuemin Lin;Ho-Fung Leung
#t 2007
#c 4
#% 22982
#% 271236
#% 281750
#% 333854
#% 450489
#% 599545
#% 743388
#% 765460
#% 803119
#% 841716
#% 875023
#% 881500
#% 890349
#% 893126
#% 893127
#% 942353
#% 1728321
#! Finding typical instances is an effective approach to understand and analyze large data sets. In this paper, we apply the idea of typicality analysis from psychology and cognition science to database query answering, and study the novel problem of answering top-k typicality queries. We model typicality in large data sets systematically. To answer questions like "Who are the top-k most typical NBA players?", the measure of simple typicality is developed. To answer questions like "Who are the top-k most typical guards distinguishing guards from other players?", the notion of discriminative typicality is proposed. Computing the exact answer to a top-k typicality query requires quadratic time which is often too costly for online query answering on large databases. We develop a series of approximation methods for various situations. (1) The randomized tournament algorithm has linear complexity though it does not provide a theoretical guarantee on the quality of the answers. (2) The direct local typicality approximation using VP-trees provides an approximation quality guarantee. (3) A VP-tree can be exploited to index a large set of objects. Then, typicality queries can be answered efficiently with quality guarantees by a tournament method based on a Local Typicality Tree data structure. An extensive performance study using two real data sets and a series of synthetic data sets clearly show that top-k typicality queries are meaningful and our methods are practical.

#index 1022277
#* Depth estimation for ranking query optimization
#@ Karl Schnaitter;Joshua Spiegel;Neoklis Polyzotis
#t 2007
#c 4
#% 102784
#% 136740
#% 145196
#% 210190
#% 248820
#% 248822
#% 273909
#% 333854
#% 479984
#% 480306
#% 765418
#% 777931
#% 810018
#% 864451
#% 874987
#% 912239
#! A relational ranking query uses a scoring function to limit the results of a conventional query to a small number of the most relevant answers. The increasing popularity of this query paradigm has led to the introduction of specialized rank join operators that integrate the selection of top tuples with join processing. These operators access just "enough" of the input in order to generate just "enough" output and can offer significant speed-ups for query evaluation. The number of input tuples that an operator accesses is called the input depth of the operator, and this is the driving cost factor in rank join processing. This introduces the important problem of depth estimation, which is crucial for the costing of rank join operators during query compilation and thus for their integration in optimized physical plans. We introduce an estimation methodology, termed Deep, for approximating the input depths of rank join operators in a physical execution plan. At the core of Deep lies a general, principled framework that formalizes depth computation in terms of the joint distribution of scores in the base tables. This framework results in a systematic estimation methodology that takes the characteristics of the data directly into account and thus enables more accurate estimates. We develop novel estimation algorithms that provide an efficient realization of the formal Deep framework, and describe their integration on top of the statistics module of an existing query optimizer. We validate the performance of Deep with an extensive experimental study on data sets of varying characteristics. The results verify the effectiveness of Deep as an estimation method and demonstrate its advantages over previously proposed techniques.

#index 1022278
#* Anytime measures for top-k algorithms
#@ Benjamin Arai;Gautam Das;Dimitrios Gunopulos;Nick Koudas
#t 2007
#c 4
#% 213981
#% 248010
#% 300180
#% 300193
#% 333854
#% 333951
#% 397378
#% 399762
#% 453464
#% 464726
#% 479462
#% 479816
#% 479967
#% 480330
#% 480819
#% 482092
#% 659993
#% 763882
#% 777931
#% 1016183
#! Top-k queries on large multi-attribute data sets are fundamental operations in information retrieval and ranking applications. In this paper, we initiate research on the anytime behavior of top-k algorithms. In particular, given specific top-k algorithms (TA and TA-Sorted) we are interested in studying their progress toward identification of the correct result at any point during the algorithms' execution. We adopt a probabilistic approach where we seek to report at any point of operation of the algorithm the confidence that the top-k result has been identified. Such a functionality can be a valuable asset when one is interested in reducing the runtime cost of top-k computations. We present a thorough experimental evaluation to validate our techniques using both synthetic and real data sets.

#index 1022279
#* Towards graph containment search and indexing
#@ Chen Chen;Xifeng Yan;Philip S. Yu;Jiawei Han;Dong-Qing Zhang;Xiaohui Gu
#t 2007
#c 4
#% 10419
#% 217812
#% 223567
#% 280409
#% 321327
#% 350323
#% 378391
#% 466644
#% 479465
#% 654452
#% 729938
#% 765429
#% 769951
#% 779470
#% 780860
#% 805893
#% 810072
#% 864425
#% 905193
#% 937108
#% 993958
#% 1717545
#! Given a set of model graphs D and a query graph q, containment search aims to find all model graphs g ε D such that q contains g (q ⊇ g). Due to the wide adoption of graph models, fast containment search of graph data finds many applications in various domains. In comparison to traditional graph search that retrieves all the graphs containing q (q ⊆ g), containment search has its own indexing characteristics that have not yet been examined. In this paper, we perform a systematic study on these characteristics and propose a contrast subgraph-based indexing model, called cIndex. Contrast subgraphs capture the structure differences between model graphs and query graphs, and are thus perfect for indexing due to their high selectivity. Using a redundancy-aware feature selection process, cIndex can sort out a set of significant and distinctive contrast subgraphs and maximize its indexing capability. We show that it is NP-complete to choose the best set of indexing features, and our greedy algorithm can approximate the one-level optimal index within a ratio of 1-- 1/e. Taking this solution as a base indexing model, we further extend it to accommodate hierarchical indexing methodologies and apply data space clustering and sampling techniques to reduce the index construction time. The proposed methodology provides a general solution to containment search and indexing, not only for graphs, but also for any data with transitive relations as well. Experimental results on real test data show that cIndex achieves near-optimal pruning power on various containment search workloads, and confirms its obvious advantage over indices built for traditional graph search in this new scenario.

#index 1022280
#* Graph indexing: tree + delta
#@ Peixiang Zhao;Jeffrey Xu Yu;Philip S. Yu
#t 2007
#c 4
#% 10419
#% 236416
#% 283833
#% 344549
#% 378391
#% 408396
#% 408638
#% 443133
#% 466644
#% 479956
#% 629708
#% 631933
#% 727845
#% 765429
#% 769951
#% 823217
#% 864425
#% 944956
#% 972269
#% 1015336
#! Recent scientific and technological advances have witnessed an abundance of structural patterns modeled as graphs. As a result, it is of special interest to process graph containment queries effectively on large graph databases. Given a graph database G, and a query raph q, the graph containment query is to retrieve all graphs in G which contain q as subgraph(s). Due to the vast number of graphs in G and the nature of complexity for subgraph isomorphism testing, it is desirable to make use of high-quality graph indexing mechanisms to reduce the overall query processing cost. In this paper, we propose a new cost-effective graph indexing method based on frequent tree-features of the graph database. We analyze the effectiveness and efficiency of tree as indexing feature from three critical aspects: feature size, feature selection cost, and pruning power. In order to achieve better pruning ability than existing graph-based indexing methods, we select, in addition to frequent tree-features (Tree), a small number of discriminative graphs (Δ) on demand, without a costly graph mining process beforehand. Our study verifies that (Tree+Δ) is a better choice than graph for indexing purpose, denoted (Tree+Δ ≥Graph), to address the graph containment query problem. It has two implications: (1) the index construction by (Tree+Δ) is efficient, and (2) the graph containment query processing by (Tree+Δ) is efficient. Our experimental studies demonstrate that (Tree+Δ) has a compact index structure, achieves an order of magnitude better performance in index construction, and most importantly, outperforms up-to-date graph-based indexing methods: gIndex and C-Tree, in graph containment query processing.

#index 1022281
#* Multi-probe LSH: efficient indexing for high-dimensional similarity search
#@ Qin Lv;William Josephson;Zhe Wang;Moses Charikar;Kai Li
#t 2007
#c 4
#% 86786
#% 158405
#% 227939
#% 249321
#% 342827
#% 427199
#% 479649
#% 479973
#% 480133
#% 594029
#% 731409
#% 749529
#% 762054
#% 790620
#% 805905
#% 847166
#% 871357
#% 875957
#% 898309
#! Similarity indices for high-dimensional data are very desirable for building content-based search systems for feature-rich data such as audio, images, videos, and other sensor data. Recently, locality sensitive hashing (LSH) and its variations have been proposed as indexing techniques for approximate similarity search. A significant drawback of these approaches is the requirement for a large number of hash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH that overcomes this drawback. Multi-probe LSH is built on the well-known LSH technique, but it intelligently probes multiple buckets that are likely to contain query results in a hash table. Our method is inspired by and improves upon recent theoretical work on entropy-based LSH designed to reduce the space requirement of the basic LSH method. We have implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional datasets. Our evaluation shows that the multi-probe LSH method substantially improves upon previously proposed methods in both space and time efficiency. To achieve the same search quality, multi-probe LSH has a similar time-efficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison with the entropy-based LSH method, to achieve the same search quality, multi-probe LSH uses less query time and 5 to 8 times fewer number of hash tables.

#index 1022282
#* STAR: self-tuning aggregation for scalable monitoring
#@ Navendu Jain;Dmitry Kit;Prince Mahajan;Praveen Yalagandula;Mike Dahlin;Yin Zhang
#t 2007
#c 4
#% 232630
#% 232640
#% 300167
#% 333955
#% 340175
#% 340176
#% 347642
#% 378388
#% 398237
#% 480332
#% 496158
#% 505869
#% 569762
#% 646206
#% 654443
#% 654483
#% 654488
#% 654497
#% 674136
#% 723286
#% 740844
#% 765402
#% 765435
#% 770901
#% 770903
#% 799139
#% 800582
#% 805466
#% 805474
#% 874994
#% 891895
#% 963874
#% 981652
#% 1015281
#! We present STAR, a self-tuning algorithm that adaptively sets numeric precision constraints to accurately and efficiently answer continuous aggregate queries over distributed data streams. Adaptivity and approximation are essential for both robustness to varying workload characteristics and for scalability to large systems. In contrast to previous studies, we treat the problem as a workload-aware optimization problem whose goal is to minimize the total communication load for a multi-level aggregation tree under a fixed error budget. STAR's hierarchical algorithm takes into account the update rate and variance in the input data distribution in a principled manner to compute an optimal error distribution, and it performs cost-benefit throttling to direct error slack to where it yields the largest benefits. Our prototype implementation of STAR in a large-scale monitoring system provides (1) a new distribution mechanism that enables self-tuning error distribution and (2) an optimization to reduce communication overhead in a practical setting by carefully distributing the initial, default error budgets. Through extensive simulations and experiments on a real network monitoring implementation, we show that STAR achieves significant performance benefits compared to existing approaches while still providing high accuracy and incurring low overheads.

#index 1022283
#* SQLB: a query allocation framework for autonomous consumers and providers
#@ J.-A. Quiané-Ruiz;Philippe Lamarre;Patrick Valduriez
#t 2007
#c 4
#% 78513
#% 203572
#% 437045
#% 479449
#% 481617
#% 571217
#% 577335
#% 631919
#% 654813
#% 871762
#% 1016166
#% 1408786
#% 1715394
#! In large-scale distributed information systems, where participants are autonomous and have special interests for some queries, query allocation is a challenge. Much work in this context has focused on distributing queries among providers in a way that maximizes overall performance (typically throughput and response time). However, preserving the participants" interests is also important. In this paper, we make two main contributions. First, we provide a model to define participants' perception of the system w.r.t. their interests and propose metrics to evaluate the quality of query allocation methods. This model facilitates the design and evaluation of new query allocation methods that take into account the participants' interests. Second, we propose a framework for query allocation called Satisfaction-based Query Load Balancing (SQLB). To be fair, SQLB dynamically trades consumers' interests for providers' interests. And it continuously adapts to changes in participants' interests and to the workload. We implemented SQLB and compared it, through experimentation, to two important baseline query allocation methods, namely Capacity based and Mariposa-like. The results demonstrate that SQLB yields high efficiency while satisfying the participants' interests and significantly outperforms the baseline methods.

#index 1022284
#* Peer-to-peer similarity search in metric spaces
#@ Christos Doulkeridis;Akrivi Vlachou;Yannis Kotidis;Michalis Vazirgiannis
#t 2007
#c 4
#% 248017
#% 273901
#% 340175
#% 340176
#% 342827
#% 479648
#% 480632
#% 482109
#% 636008
#% 731409
#% 745498
#% 769459
#% 770901
#% 783519
#% 805905
#% 812756
#% 812772
#% 814646
#% 824706
#% 839352
#% 859778
#% 864421
#% 878642
#% 893140
#% 1016166
#% 1408703
#% 1711096
#% 1851624
#! This paper addresses the efficient processing of similarity queries in metric spaces, where data is horizontally distributed across a P2P network. The proposed approach does not rely on arbitrary data movement, hence each peer joining the network autonomously stores its own data. We present SIMPEER, a novel framework that dynamically clusters peer data, in order to build distributed routing information at super-peer level. SIMPEER allows the evaluation of range and nearest neighbor queries in a distributed manner that reduces communication cost, network latency, bandwidth consumption and computational overhead at each individual peer. SIMPEER utilizes a set of distributed statistics and guarantees that all similar objects to the query are retrieved, without necessarily flooding the network during query processing. The statistics are employed for estimating an adequate query radius for k-nearest neighbor queries, and transform the query to a range query. Our experimental evaluation employs both real-world and synthetic data collections, and our results show that SIMPEER performs efficiently, even in the case of high degree of distribution.

#index 1022285
#* Inferring XML schema definitions from XML data
#@ Geert Jan Bex;Frank Neven;Stijn Vansummeren
#t 2007
#c 4
#% 71516
#% 152835
#% 248809
#% 252366
#% 273922
#% 342676
#% 397364
#% 397407
#% 431034
#% 450698
#% 462062
#% 462235
#% 464724
#% 479465
#% 480822
#% 563261
#% 564264
#% 572314
#% 577353
#% 632058
#% 765540
#% 772031
#% 805911
#% 809236
#% 842028
#% 845589
#% 848763
#% 864650
#% 879213
#% 893098
#% 894435
#% 949370
#% 1016126
#% 1016148
#% 1016255
#% 1699600
#! Although the presence of a schema enables many optimizations for operations on XML documents, recent studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate schemas from sets of sample documents. While previous work in this area has mostly focused on the inference of Document Type Definitions (DTDs for short), we will consider the inference of XML Schema Definitions (XSDs for short) --- the increasingly popular schema formalism that is turning DTDs obsolete. In contrast to DTDs where the content model of an element depends only on the element's name, the content model in an XSD can also depend on the context in which the element is used. Hence, while the inference of DTDs basically reduces to the inference of regular expressions from sets of sample strings, the inference of XSDs also entails identifying from a corpus of sample documents the contexts in which elements bear different content models. Since a seminal result by Gold implies that no inference algorithm can learn the complete class of XSDs from positive examples only, we focus on a class of XSDs that captures most XSDs occurring in practice. For this class, we provide a theoretically complete algorithm that always infers the correct XSD when a sufficiently large corpus of XML documents is available. In addition, we present a variant of this algorithm that works well on real-world (and therefore incomplete) data sets.

#index 1022286
#* Querying complex structured databases
#@ Cong Yu;H. V. Jagadish
#t 2007
#c 4
#% 570875
#% 654441
#% 654442
#% 754116
#% 765407
#% 765408
#% 800508
#% 810052
#% 824681
#% 838492
#% 893115
#% 993987
#% 994033
#% 1015258
#% 1016135
#% 1016176
#% 1688287
#! Correctly generating a structured query (e.g., an XQuery or a SQL query) requires the user to have a full understanding of the database schema, which can be a daunting task. Alternative query models have been proposed to give users the ability to query the database without schema knowledge. Those models, including simple keyword search and labeled keyword search, aim to extract meaningful data fragments that match the structure-free query conditions (e.g., keywords) based on various matching semantics. Typically, the matching semantics are content-based: they are defined on data node inter-relationships and incur significant query evaluation cost. Our first contribution is a novel matching semantics based on analyzing the database schema. We show that query models employing a schema-based matching semantics can reduce query evaluation cost significantly while maintaining or even improving result quality. The adoption of schema-based matching semantics does not change the nature of those query models: they are still schema-ignorant, i.e., users express no schema knowledge (except the labels in labeled keyword search) in the query. While those models work well for some queries on some databases, they often encounter problems when applied to complex queries on databases with complex schemas. Our second contribution is a novel query model that incorporates partial schema knowledge through the use of schema summary. This new summary-aware query model, called Meaningful Summary Query (MSQ), seamlessly integrates summary-based structural conditions and structure-free conditions, and enables ordinary users to query complex databases. We design algorithms for evaluating MSQ queries, and demonstrate that MSQ queries can produce better results against complex databases when compared with previous approaches, and that they can be efficiently evaluated.

#index 1022287
#* Measuring the structural similarity of semistructured documents using entropy
#@ Sven Helmer
#t 2007
#c 4
#% 66654
#% 201889
#% 210212
#% 227859
#% 234905
#% 234979
#% 248809
#% 289193
#% 300157
#% 387427
#% 397406
#% 413582
#% 442886
#% 480645
#% 480648
#% 480824
#% 587841
#% 654464
#% 654469
#% 734500
#% 734653
#% 754108
#% 789009
#% 800590
#% 872017
#% 1809406
#% 1815525
#! We propose a technique for measuring the structural similarity of semistructured documents based on entropy. After extracting the structural information from two documents we use either Ziv-Lempel encoding or Ziv-Merhav crossparsing to determine the entropy and consequently the similarity between the documents. To the best of our knowledge, this is the first true linear-time approach for evaluating structural similarity. In an experimental evaluation we demonstrate that the results of our algorithm in terms of clustering quality are on a par with or even better than existing approaches.

#index 1022288
#* Declarative information extraction using datalog with embedded extraction predicates
#@ Warren Shen;AnHai Doan;Jeffrey F. Naughton;Raghu Ramakrishnan
#t 2007
#c 4
#% 287461
#% 321327
#% 384978
#% 397605
#% 435130
#% 442832
#% 565457
#% 577319
#% 742564
#% 752760
#% 782759
#% 794509
#% 801668
#% 809267
#% 810014
#% 864415
#% 874978
#% 874992
#% 875064
#% 907512
#% 1022235
#% 1672191
#! In this paper we argue that developing information extraction (IE) programs using Datalog with embedded procedural extraction predicates is a good way to proceed. First, compared to current ad-hoc composition using, e.g., Perl or C++, Datalog provides a cleaner and more powerful way to compose small extraction modules into larger programs. Thus, writing IE programs this way retains and enhances the important advantages of current approaches: programs are easy to understand, debug, and modify. Second, once we write IE programs in this framework, we can apply query optimization techniques to them. This gives programs that, when run over a variety of data sets, are more efficient than any monolithic program because they are optimized based on the statistics of the data on which they are invoked. We show how optimizing such programs raises challenges specific to text data that cannot be accommodated in the current relational optimization framework, then provide initial solutions. Extensive experiments over real-world data demonstrate that optimization is indeed vital for IE programs and that we can effectively optimize IE programs written in this proposed framework.

#index 1022289
#* A relational approach to incrementally extracting and querying structure in unstructured data
#@ Eric Chu;Akanksha Baid;Ting Chen;AnHai Doan;Jeffrey Naughton
#t 2007
#c 4
#% 301241
#% 480629
#% 504443
#% 602050
#% 654468
#% 800551
#% 810014
#% 824733
#% 830520
#% 864416
#% 864445
#% 875064
#% 875067
#% 893089
#% 960235
#% 960302
#% 1002142
#% 1022235
#% 1098449
#! There is a growing consensus that it is desirable to query over the structure implicit in unstructured documents, and that ideally this capability should be provided incrementally. However, there is no consensus about what kind of system should be used to support this kind of incremental capability. We explore using a relational system as the basis for a workbench for extracting and querying structure from unstructured data. As a proof of concept, we applied our relational approach to support structured queries over Wikipedia. We show that the data set is always available for some form of querying, and that as it is processed, users can pose a richer set of structured queries. We also provide examples of how we can incrementally evolve our understanding of the data in the context of the relational workbench.

#index 1022290
#* Efficient keyword search over virtual XML views
#@ Feng Shao;Lin Guo;Chavdar Botev;Anand Bhaskar;Muthiah Chettiar;Fan Yang;Jayavel Shanmugasundaram
#t 2007
#c 4
#% 67565
#% 213981
#% 253191
#% 262069
#% 290703
#% 300143
#% 309851
#% 333854
#% 333981
#% 340144
#% 340914
#% 387427
#% 458829
#% 480317
#% 480657
#% 570875
#% 570879
#% 571096
#% 654441
#% 654442
#% 730983
#% 745450
#% 765418
#% 765488
#% 766671
#% 807381
#% 824681
#% 875058
#% 942738
#% 993987
#% 1015272
#% 1015274
#% 1015325
#! Emerging applications such as personalized portals, enterprise search and web integration systems often require keyword search over semi-structured views. However, traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized. In this paper, we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual (unmaterialized) XML views. An interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results. Another feature of the algorithm is that by solely using indices, we can still score the results of queries over the virtual view, and the resulting scores are the same as if the view was materialized. Our performance evaluation using the INEX data set in the Quark [5] open-source XML database system indicates that the proposed approach is scalable and efficient.

#index 1022291
#* A general framework for modeling and processing optimization queries
#@ Michael Gibas;Ning Zheng;Hakan Ferhatosmanoglu
#t 2007
#c 4
#% 201876
#% 212690
#% 237187
#% 252304
#% 300163
#% 300180
#% 316524
#% 333951
#% 342828
#% 479649
#% 479973
#% 527026
#% 643566
#% 757953
#% 875000
#% 1016183
#! An optimization query asks for one or more data objects that maximize or minimize some function over the data set. We propose a general class of queries, model-based optimization queries, in which a generic model is used to define a wide variety of queries involving an optimization objective function and/or a set of constraints on the attributes. This model can be used to define optimization of linear and nonlinear expressions over object attributes as well as many existing query types studied in database research literature. A significant and important subset of this general model relevant to real-world applications include queries where the optimization function and constraints are convex. We cast such queries as members of the convex optimization (CP) model and provide a unified query processing framework for CP queries that I/O optimally accesses data and space partitioning index structures without changing the underlying structures. We perform experiments to show the generality of the technique and where possible, compare to techniques developed for specialized optimization queries. We find that we achieve nearly identical performance to the limited optimization query types with optimal solutions, while providing generic modeling and processing for a much broader class of queries, and while effectively handling problem constraints.

#index 1022292
#* On the production of anorexic plan diagrams
#@ Harish D;Pooja N. Darera;Jayant R. Haritsa
#t 2007
#c 4
#% 13018
#% 172900
#% 214233
#% 248793
#% 256685
#% 273694
#% 378414
#% 408396
#% 411554
#% 463444
#% 479786
#% 480803
#% 480955
#% 654473
#% 824756
#% 993945
#% 1015318
#! A "plan diagram" is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. We have shown recently that, for industrial-strength database engines, these diagrams are often remarkably complex and dense, with a large number of plans covering the space. However, they can often be reduced to much simpler pictures, featuring significantly fewer plans, without materially affecting the query processing quality. Plan reduction has useful implications for the design and usage of query optimizers, including quantifying redundancy in the plan search space, enhancing useability of parametric query optimization, identifying error-resistant and least-expected-cost plans, and minimizing the overheads of multi-plan approaches. We investigate here the plan reduction issue from theoretical, statistical and empirical perspectives. Our analysis shows that optimal plan reduction, w.r.t. minimizing the number of plans, is an NP-hard problem in general, and remains so even for a storage-constrained variant. We then present a greedy reduction algorithm with tight and optimal performance guarantees, whose complexity scales linearly with the number of plans in the diagram for a given resolution. Next, we devise fast estimators for locating the best tradeoff between the reduction in plan cardinality and the impact on query processing quality. Finally, extensive experimentation with a suite of multi-dimensional TPCH-based query templates on industrial-strength optimizers demonstrates that complex plan diagrams easily reduce to "anorexic" (small absolute number of plans) levels incurring only marginal increases in the estimated query processing costs.

#index 1022293
#* Efficient use of the query optimizer for automated physical design
#@ Stratos Papadomanolakis;Debabrata Dash;Anastasia Ailamaki
#t 2007
#c 4
#% 397397
#% 411554
#% 480158
#% 482100
#% 631950
#% 632100
#% 765431
#% 778724
#% 810026
#% 810027
#% 824756
#% 893130
#% 993945
#% 993946
#% 1015318
#% 1016220
#% 1207101
#! State-of-the-art database design tools rely on the query optimizer for comparing between physical design alternatives. Although it provides an appropriate cost model for physical design, query optimization is a computationally expensive process. The significant time consumed by optimizer invocations poses serious performance limitations for physical design tools, causing long running times, especially for large problem instances. So far it has been impossible to remove query optimization overhead without sacrificing cost estimation precision. Inaccuracies in query cost estimation are detrimental to the quality of physical design algorithms, as they increase the chances of "missing" good designs and consequently selecting sub-optimal ones. Precision loss and the resulting reduction in solution quality is particularly undesirable and it is the reason the query optimizer is used in the first place. In this paper we eliminate the tradeoff between query cost estimation accuracy and performance. We introduce the INdex Usage Model (INUM), a cost estimation technique that returns the same values that would have been returned by the optimizer, while being three orders of magnitude faster. Integrating INUM with existing index selection algorithms dramatically improves their running times without precision compromises.

#index 1022294
#* Dynamic workload management for very large data warehouses: juggling feathers and bowling balls
#@ Stefan Krompass;Harumi Kuno;Umeshwar Dayal;Alfons Kemper
#t 2007
#c 4
#% 27242
#% 29089
#% 170893
#% 201925
#% 462482
#% 481131
#% 765467
#% 765468
#% 800589
#% 810056
#% 864540
#% 911761
#% 963918
#% 1661777
#% 1688297
#! Workload management for business intelligence (BI) queries poses different challenges than those addressed in the online transaction processing (OLTP) context. The fundamental problem is that the execution times of BI queries can range from milliseconds to hours, and it is difficult to estimate these times accurately. Key challenges raised by this problem are how to identify queries that are not performing properly and what to do about them. We propose here a workload management system for controlling the execution of individual queries based on realistic customer service level objectives. In order to validate our proposal, we have implemented an experimental system that includes a dynamic execution controller that leverages fuzzy logic. We present results from a number of experiments that we ran using workloads based on actual industrial workloads and customer objectives that we gathered by interviewing industry practitioners. Our experiments show that even a handful of moderately mis-behaving problem queries can have a significant impact on a workload consisting of thousands of queries. We were surprised when our experiments also demonstrated that false positives -- incorrectly identifying a normal query as a problem -- can also have significant consequences. For those reasons, it is very important that an execution controller be as accurate as possible -- avoiding both false positives and false negatives. Our experiments also validate that our execution controller can markedly improve the execution of a workload that includes problem queries.

#index 1022295
#* Tracing lineage beyond relational operators
#@ Mingwu Zhang;Xiangyu Zhang;Xiang Zhang;Sunil Prabhakar
#t 2007
#c 4
#% 48756
#% 82316
#% 152934
#% 286551
#% 386158
#% 430749
#% 462072
#% 504161
#% 527036
#% 592873
#% 632016
#% 763247
#% 803468
#% 825893
#% 853004
#% 1016204
#% 1017302
#! Tracing the lineage of data is an important requirement for establishing the quality and validity of data. Recently, the problem of data provenance has been increasingly addressed in database research. Earlier work has been limited to the lineage of data as it is manipulated using relational operations within an RDBMS. While this captures a very important aspect of scientific data processing, the existing work is incapable of handling the equally important, and prevalent, cases where the data is processed by non-relational operations. This is particularly common in scientific data where sophisticated processing is achieved by programs that are not part of a DBMS. The problem of tracking lineage when non-relational operators are used to process the data is particularly challenging since there is potentially no constraint on the nature of the processing. In this paper we propose a novel technique that overcomes this significant barrier and enables the tracing of lineage of data generated by an arbitrary function. Our technique works directly with the executable code of the function and does not require any high-level description of the function or even the source code. We establish the feasibility of our approach on a typical application and demonstrate that the technique is able to discern the correct lineage. Furthermore, it is shown that the method can help identify limitations in the function itself.

#index 1022296
#* A generic solution for warehousing business process data
#@ Fabio Casati;Malu Castellanos;Umeshwar Dayal;Norman Salazar
#t 2007
#c 4
#% 480134
#% 480666
#% 893094
#! Improving business processes is critical to any corporation. Process improvement requires analysis as its first basic step. Process analysis has many unique challenges: i) companies execute many business processes, and devising ad hoc solutions for each of them is too costly. Hence, generic approaches must be sought; ii) the abstraction level at which processes need to be analyzed is much higher with respect to the information available in the process execution environment; iii) the rapidly increasing need of co-developing the process analysis and the process automation solution and the scale of the problem makes it hard to cope with frequent changes in the sources of process data. To address these problems, we have developed a process warehousing solution, used by HP and its customers. In this paper we describe the solution, the challenges we had to face, and the lessons we learned in implementing and deploying it.

#index 1022297
#* Why you should run TPC-DS: a workload analysis
#@ Meikel Poess;Raghunath Othayoth Nambiar;David Walrath
#t 2007
#c 4
#% 328431
#% 385321
#% 397399
#% 741995
#% 824697
#% 824740
#% 824756
#% 893175
#% 1016216
#! The Transaction Processing Performance Council (TPC) is completing development of TPC-DS, a new generation industry standard decision support benchmark. The TPC-DS benchmark, first introduced in the "The Making of TPC-DS" [9] paper at the 32nd International Conference on Very Large Data Bases (VLDB), has now entered the TPC's "Formal Review" phase for new benchmarks; companies and researchers alike can now download the draft benchmark specification and tools for evaluation. The first paper [9] gave an overview of the TPC-DS data model, workload model, and execution rules. This paper details the characteristics of different phases of the workload, namely: database load, query workload and data maintenance; and also their impact to the benchmark's performance metric. As with prior TPC benchmarks, this workload will be widely used by vendors to demonstrate their capabilities to support complex decision support systems, by customers as a key factor in purchasing servers and software, and by the database community for research and development of optimization techniques.

#index 1022298
#* The end of an architectural era: (it's time for a complete rewrite)
#@ Michael Stonebraker;Samuel Madden;Daniel J. Abadi;Stavros Harizopoulos;Nabil Hachem;Pat Helland
#t 2007
#c 4
#% 27057
#% 75920
#% 114582
#% 148195
#% 286836
#% 287352
#% 300194
#% 346906
#% 411560
#% 442700
#% 479819
#% 750968
#% 800491
#% 824697
#% 878299
#% 893146
#% 1022236
#! In previous papers [SC05, SBC+07], some of us predicted the end of "one size fits all" as a commercial relational DBMS paradigm. These papers presented reasons and experimental evidence that showed that the major RDBMS vendors can be outperformed by 1--2 orders of magnitude by specialized engines in the data warehouse, stream processing, text, and scientific database markets. Assuming that specialized engines dominate these markets over time, the current relational DBMS code lines will be left with the business data processing (OLTP) market and hybrid markets where more than one kind of capability is required. In this paper we show that current RDBMSs can be beaten by nearly two orders of magnitude in the OLTP market as well. The experimental evidence comes from comparing a new OLTP prototype, H-Store, which we have built at M.I.T. to a popular RDBMS on the standard transactional benchmark, TPC-C. We conclude that the current RDBMS code lines, while attempting to be a "one size fits all" solution, in fact, excel at nothing. Hence, they are 25 year old legacy code lines that should be retired in favor of a collection of "from scratch" specialized engines. The DBMS vendors (and the research community) should start with a clean sheet of paper and design systems for tomorrow's requirements, not continue to push code lines and architectures designed for yesterday's needs.

#index 1022299
#* Computer science 2.0: a new world of data management
#@ Michael L. Brodie
#t 2007
#c 4
#! Data management, one of the most successful software technologies, is the bedrock of almost all business, government, and scientific activities, worldwide. Data management continues to grow, more than doubling in data and transaction volumes every two years with a growth in deployments to fuel a $15 billion market. A continuous stream of innovations in capabilities, robustness, and features lead to new, previously infeasible data-intensive applications. Yet forty years of DBMS innovation are pushing DBMSs beyond the complexity barrier where one-size-fits-all DBMSs do not meet the requirements of emerging applications. While data management growth will continue based primarily on the relational data model and conventional DBMSs, a much larger and more challenging data management world is emerging. In the 1990's data under DBMS management reached 10% of the world's data. The six-fold growth of non-relational data in the period 2006--2010 will reduce that number to well below 5%. We are entering the next generation of computing with a fundamentally different computing model and paradigm characterized technologically by multi-core architectures, virtualization, service-oriented computing, and the semantic web. Computer Science 2.0 will mark the end of the Computing Era with its focus on technology and the beginning of the Problem Solving Era with its focus on higher levels of abstraction and automation (i.e., intelligent) tools for real world (i.e., imprecise) domains in which approximate and ever-changing answers are the norm. This confluence of limitations of conventional DBMSs, the explosive growth of previously unimagined applications and data, and the genuine need for problem solving will result in a new world of data management. The data management world should embrace these opportunities and provide leadership for data management in Computer Science 2.0. Two emerging areas that lack such guidance are service-oriented computing and the semantic web. While concepts and standards are evolving for data management in service-oriented architectures, data services or data virtualization has not been a focus of the DBMS research or products communities. Missing this opportunity will be worse than missing the Internet. The semantic web will become the means by which information is accessed and managed with modest projections of 40 billion pages with hundreds of triples per page - the largest distributed system in the world - the only one. Tim Berners-Lee and the World Wide Web Consortium view databases are being nodes that need to be turned inside out. What does the database community think? Semantic web services will constitute a programming model of Computer Science 2.0. How does data management fit into this semantically rich environment? Computer Science 2.0 offers the data management community one of the biggest challenges in its forty-year history and opens up a new world of data management. Key to success in this new world will be collaboration with other disciplines whether at the technical level - partnering with the semantic technologies community to augment their reasoning capabilities with systems support - or at the problem solving level - partnering with real world domains as proposed by the new discipline of Web Science.

#index 1022300
#* RadixZip: linear time compression of token streams
#@ Binh Dao Vo;Gurmeet Singh Manku
#t 2007
#c 4
#% 3244
#% 146203
#% 151541
#% 191154
#% 193923
#% 286258
#% 287664
#% 300153
#% 302725
#% 322412
#% 333953
#% 333954
#% 393784
#% 464843
#% 479808
#% 480329
#% 480821
#% 481424
#% 726681
#% 740539
#% 796723
#% 824697
#% 864446
#% 875026
#% 893129
#% 893159
#% 954300
#% 960266
#% 1014803
#% 1015332
#% 1016235
#% 1829997
#! RadixZip is a block compression technique for token streams. It introduces RadixZip Transform, a linear time algorithm that rearranges bytes using a technique inspired by radix sorting. For appropriate data, RadixZip Transform is analogous to the Burrows-Wheeler Transform used in bzip2, but is both simpler in operation and more effective in compression. In addition, RadixZip Transform can take advantage of correlations between token streams with no computational overhead. Experiments over practical data show that for common token streams, RadixZip is superior to bzip2.

#index 1022301
#* Continuous queries in oracle
#@ Andrew Witkowski;Srikanth Bellamkonda;Hua-Gang Li;Vince Liang;Lei Sheng;Wayne Smith;Sankar Subramanian;James Terry;Tsae-Feng Yu
#t 2007
#c 4
#% 13016
#% 58376
#% 116043
#% 152928
#% 172950
#% 198467
#% 220425
#% 279905
#% 300179
#% 397353
#% 443298
#% 463276
#% 479792
#% 481288
#% 481604
#% 481943
#% 654497
#% 654507
#% 654510
#% 763881
#% 765474
#% 824739
#% 993949
#% 993998
#! This paper describes Continuous Queries (CQ) in Oracle RDBMS, a feature that incorporates stream and complex event processing into an RDBMS, the first such attempt in commercial databases. The feature is based on the concept of query difference and allows us to monitor real time changes to the query as the result of changes to its underlying tables. The result of a continuous query can be deposited into historical tables or queues for further asynchronous de-queuing, or can invoke a synchronous trigger for procedural processing. The main contribution of our CQ engine is that it allows us to react to complex scenarios of changes to data such as mixed INSERT, DELETE and UPDATE changes, unlike the existing stream processing systems that deal with INSERTS only. We support a wide range of query shapes including inner, semi and anti-joins, aggregates and window functions. More details are given to the efficient computation of query difference for general cases and their optimizations based on semantic constraints. They are shown to improve the response time for practical cases by more than an order of magnitude. We also show how delaying CQ re-computation can improve its performance by batch processing the changes to the base tables.

#index 1022302
#* Challenges and experience in prototyping a multi-modal stream analytic and monitoring application on System S
#@ Kun-Lung Wu;Kirsten W. Hildrum;Wei Fan;Philip S. Yu;Charu C. Aggarwal;David A. George;Buǧra Gedik;Eric Bouillet;Xiaohui Gu;Gang Luo;Haixun Wang
#t 2007
#c 4
#% 397353
#% 654507
#% 769927
#% 838409
#% 844301
#% 844373
#% 850523
#% 875006
#% 881469
#% 938461
#% 960275
#% 1016200
#% 1180866
#! In this paper, we describe the challenges of prototyping a reference application on System S, a distributed stream processing middleware under development at IBM Research. With a large number of stream PEs (Processing Elements) implementing various stream analytic algorithms, running on a large-scale, distributed cluster of nodes, and collaboratively digesting several multi-modal source streams with vastly differing rates, prototyping a reference application on System S faces many challenges. Specifically, we focus on our experience in prototyping DAC (Disaster Assistance Claim monitoring), a reference application dealing with multi-modal stream analytic and monitoring. We describe three critical challenges: (1) How do we generate correlated, multi-modal source streams for DAC? (2) How do we design and implement a comprehensive stream application, like DAC, from many divergent stream analytic PEs? (3) How do we deploy DAC in light of source streams with extremely different rates? We report our experience in addressing these challenges, including modeling a disaster claim processing center to generate correlated source streams, constructing the PE flow graph, utilizing programming supports from System S, adopting parallelism, and exploiting resource-adaptive computation.

#index 1022303
#* Efficient bulk deletes for multi dimensional clustered tables in DB2
#@ Bishwaranjan Bhattacharjee;Timothy Malkemus;Sherman Lau;Sean McKeough;Jo-anne Kirton;Robin Von Boeschoten;John P Kennedy
#t 2007
#c 4
#% 154310
#% 182902
#% 210202
#% 287672
#% 465149
#% 479473
#% 481433
#% 481622
#% 481771
#% 654495
#% 1015335
#% 1016226
#! In data warehousing applications, the ability to efficiently delete large chunks of data from a table is very important. This feature is also known as Rollout or Bulk Deletes. Rollout is generally carried out periodically and is often done on more than one dimension or attribute. The ability to efficiently handle the updates of RID indexes while doing Rollouts is a well known problem for database engines and its solution is very important for data warehousing applications. DB2 UDB V8.1 introduced a new physical clustering scheme called Multi Dimensional Clustering (MDC) which allows users to cluster data in a table on multiple attributes or dimensions. This is very useful for query processing and maintenance activities including deletes. Subsequently, an enhancement was incorporated in DB2 UDB Viper 2 which allows for very efficient online rollout of data on dimensional boundaries even when there are a lot of secondary RID indexes defined on the table. This is done by the asynchronous updates of these RID indexes in the background while allowing the delete to commit and the table to be accessed. This paper details the design of MDC Rollout and the challenges that were encountered. It discusses some performance results which show order of magnitude improvements using it and the lessons learnt.

#index 1022304
#* Supporting time-constrained SQL queries in oracle
#@ Ying Hu;Seema Sundara;Jagannathan Srinivasan
#t 2007
#c 4
#% 58348
#% 227883
#% 227894
#% 273908
#% 273909
#% 300195
#% 323772
#% 420114
#% 442995
#% 479623
#% 479816
#% 479967
#% 503719
#% 654486
#% 765418
#% 765467
#% 765468
#% 777931
#% 781787
#% 800589
#% 810056
#% 824755
#% 893173
#! The growing nature of databases, and the flexibility inherent in the SQL query language that allows arbitrarily complex formulations, can result in queries that take inordinate amount of time to complete. To mitigate this problem, strategies that are optimized to return the 'first-few rows' or 'top-k rows' (in case of sorted results) are usually employed. However, both these strategies can lead to unpredictable query processing times. Thus, in this paper we propose supporting time-constrained SQL queries. Specifically, a user issues a SQL query as before but additionally provides nature of constraint (soft or hard), an upper bound for query processing time, and acceptable nature of results (partial or approximate). The DBMS takes the criteria (constraint type, time limit, quality of result) into account in generating the query execution plan, which is expected (guaranteed) to complete in the allocated time for soft (hard) time constraint. If partial results are acceptable then the technique of reducing result set cardinality (i.e. returning first few or top-k rows) is used, whereas if approximate results are acceptable then sampling is used, to compute query results within the specified time limit. For the latter case, we argue that trading off quality of results for predictable response time is quite useful. However, for this case, we provide additional aggregate functions to estimate the aggregate values and to compute the associated confidence interval. This paper presents the notion of time-constrained SQL queries, discusses the challenges in supporting such a construct, describes a framework for supporting such queries, and outlines its implementation in Oracle Database by exploiting Oracle's cost-based optimizer and extensibility capabilities.

#index 1022305
#* Request Window: an approach to improve throughput of RDBMS-based data integration system by utilizing data sharing across concurrent distributed queries
#@ Rubao Lee;Minghong Zhou;Huaming Liao
#t 2007
#c 4
#% 4683
#% 36117
#% 111368
#% 136740
#% 152943
#% 172968
#% 210177
#% 248807
#% 273911
#% 300166
#% 330305
#% 333848
#% 342369
#% 397393
#% 479452
#% 481450
#% 481916
#% 745488
#% 765434
#% 800602
#% 810039
#% 830700
#% 882036
#% 911450
#% 1015278
#% 1408872
#! This paper focuses on the problem of improving distributed query throughput of the RDBMS-based data integration system that has to inherit the query execution model of the underlying RDBMS: execute each query independently and utilize a global buffer pool mechanism to provide disk page sharing across concurrent query execution processes. However, this model is not suitable for processing concurrent distributed queries because the foundation, the memory-disk hierarchy, does not exist for data provided by remote sources. Therefore, the query engine cannot exploit any data sharing so that each process will have to interact with data sources independently: issue data requests and fetch data over the network. This paper presents Request Window, a novel DQP mechanism that can detect and employ data sharing opportunities across concurrent distributed queries. By combining multiple similar data requests issued to the same data source to a common data request, Request Window allows concurrent query executing processes to share the common result data. With the benefits of reduced source burdens and data transfers, the throughput of query engine can be significantly improved. This paper also introduces the IGNITE system, an extended PostgreSQL with DQP support. Our experimental results show that Request Window makes IGNITE achieve a 1.7x speedup over a commercial data integration system when running a workload of distributed TPC-H queries.

#index 1022306
#* Inverse functions in the AquaLogic Data Services Platform
#@ Nicola Onose;Vinayak Borkar;Michael J. Carey
#t 2007
#c 4
#% 13043
#% 204058
#% 462072
#% 464007
#% 750960
#% 800087
#% 875028
#% 893174
#% 1561977
#! When integrating data from heterogeneous sources, it is often necessary to transform both the schemas and the data from the underlying sources in order to present the integrated data in the form desired by its consuming applications. Unfortunately, these transformations---particularly if implemented by custom code---can block query optimization and updates, leading to potentially severe performance and functionality limitations. To circumvent these problems, the BEA AquaLogic Data Services Platform provides support for user-defined inverse functions. This paper describes the motivation, design, user experience, and implementation associated with inverse functions in ALDSP. This functionality debuted in version 2.1 of ALDSP in March 2006.

#index 1022307
#* A genetic approach for random testing of database systems
#@ Hardik Bati;Leo Giakoumakis;Steve Herbert;Aleksandras Surna
#t 2007
#c 4
#% 136740
#% 204004
#% 300196
#% 302788
#% 417872
#% 442850
#% 479656
#% 794867
#% 809116
#% 877670
#% 877676
#% 877677
#% 912483
#% 961365
#! Testing a database engine has been and continues to be a challenging task. The space of possible SQL queries along with their possible access paths is practically unbounded. Moreover, this space is continuously increasing in size as the feature set of modern DBMS systems expands with every product release. To tackle these problems, random query generator tools have been used to create large numbers of test cases. While such test case generators enable the creation of complex and syntactically correct SQL queries, they do not guarantee that the queries produced return results or exercise desired DBMS components. Very often the generated queries contain logical contradictions, which cause "short-circuits" at the query optimization layer, failing to exercise the lower layers of the database engine (query optimization, query execution, access methods, etc.) In this paper we present a random test case generation technique, which provides solutions to the above problems. Our technique utilizes execution feedback, obtained from the DBMS under test, in order to guide the test generation process toward specific DBMS subcomponents and rarely exercised code paths. Test cases are created incrementally using a genetic approach, which synthesizes query characteristics that are of interest for the purposes of test coverage. Our experiments indicate that our technique can outperform other methods of random testing in terms of efficiency and code coverage. We also provide experimental results which show that the use of execution feedback improves code coverage of specific DBMS components. Finally, we share our experiences gained from using this testing approach during the development cycles of Microsoft SQL Server.

#index 1022308
#* Bridging the application and DBMS profiling divide for database application developers
#@ Surajit Chaudhuri;Vivek Narasayya;Manoj Syamala
#t 2007
#c 4
#% 654467
#% 765463
#% 810111
#% 875027
#% 963474
#! In today's world, tools for profiling and tuning application code remain disconnected from the profiling and tuning tools for relational DBMSs. This makes it challenging for developers of database applications to profile, tune and debug their applications, for example, identifying application code that causes deadlocks in the server. We have developed an infrastructure that simultaneously captures both the application context as well as the database context, thereby enabling a rich class of tuning, profiling and debugging tasks that is not possible today. We have built a tool using this infrastructure that enables developers to seamlessly profile, tune and debug ADO.NET applications over Microsoft SQL Server by taking advantage of information across the application and database contexts. We describe and evaluate several tasks that can be accomplished using this tool.

#index 1022309
#* Automating the detection of snapshot isolation anomalies
#@ Sudhir Jorwekar;Alan Fekete;Krithi Ramamritham;S. Sudarshan
#t 2007
#c 4
#% 190645
#% 201869
#% 287230
#% 320902
#% 632091
#% 783784
#% 809252
#% 814649
#! Snapshot isolation (SI) provides significantly improved concurrency over 2PL, allowing reads to be non-blocking. Unfortunately, it can also lead to non-serializable executions in general. Despite this, it is widely used, supported in many commercial databases, and is in fact the highest available level of consistency in Oracle and Post-greSQL. Sufficient conditions for detecting whether SI anomalies could occur in a given set of transactions were presented recently, and extended to necessary conditions for transactions without predicate reads. In this paper we address several issues in extending the earlier theory to practical detection/correction of anomalies. We first show how to mechanically find a set of programs which is large enough so that we ensure that all executions will be free of SI anomalies, by modifying these programs appropriately. We then address the problem of false positives, i.e., transaction programs wrongly identified as possibly leading to anomalies, and present techniques that can significantly reduce such false positives. Unlike earlier work, our techniques are designed to be automated, rather than manually carried out. We describe a tool which we are developing to carry out this task. The tool operates on descriptions of the programs either taken from the application code itself, or taken from SQL query traces. It can be used with any database system. We have used our tool on two real world applications in production use at IIT Bombay, and detected several anomalies, some of which have caused real world problems. We believe such a tool will be invaluable for ensuring safe execution of the large number of applications which are already running under SI.

#index 1022310
#* Optimization of frequent itemset mining on multiple-core processor
#@ Li Liu;Eric Li;Yimin Zhang;Zhizhong Tang
#t 2007
#c 4
#% 94539
#% 129750
#% 152934
#% 201894
#% 227919
#% 280409
#% 300120
#% 420063
#% 463903
#% 465003
#% 466641
#% 466664
#% 479484
#% 481290
#% 481754
#% 631926
#% 789002
#% 824655
#% 824699
#% 1395058
#% 1504621
#! Multi-core processors are proliferated across different domains in recent years. In this paper, we study the performance of frequent pattern mining on a modern multi-core machine. A detailed study shows that, even with the best implementation, current FP-tree based algorithms still under-utilize a multi-core system due to poor data locality and insufficient parallelism expression. We propose two techniques: a cache-conscious FP-array (frequent pattern array) and a lock-free dataset tiling parallelization mechanism to address this problem. The FP-array efficiently improves the data locality performance, and makes use of the benefits from hardware and software prefetching. The result yields an overall 4.0 speedup compared with the state-of-the-art implementation. Furthermore, to unlock the power of multi-core processor, a lock-free parallelization approach is proposed to restructure the FP-tree building algorithm. It not only eliminates the locks in building a single FP-tree with fine-grained threads, but also improves the temporal data locality performance. To summarize, with the proposed cache-conscious FP-array and lock-free parallelization enhancements, the overall FP-tree algorithm achieves a 24 fold speedup on an 8-core machine. Finally, we believe the presented techniques can be applied to other data mining tasks as well with the prevalence of multi-core processor.

#index 1022311
#* CellSort: high performance sorting on the cell processor
#@ Buǧra Gedik;Rajesh R. Bordawekar;Philip S. Yu
#t 2007
#c 4
#% 66928
#% 100554
#% 112212
#% 252608
#% 368123
#% 397361
#% 435159
#% 629126
#% 806985
#% 867058
#% 867945
#% 874997
#% 888909
#% 1504754
#! In this paper we describe the design and implementation of CellSort - a high performance distributed sort algorithm for the Cell processor. We design CellSort as a distributed bitonic merge with a data-parallel bitonic sorting kernel. In order to best exploit the architecture of the Cell processor and make use of all available forms of parallelism to achieve good scalability, we structure CellSort as a three-tiered sort. The first tier is a SIMD (single-instruction multiple data) optimized bitonic sort, which sorts up to 128KB of items that cat fit into one SPE's (a co-processor on Cell) local store. We design a comprehensive SIMDization scheme that employs data parallelism even for the most fine-grained steps of the bitonic sorting kernel. Our results show that, SIMDized bitonic sorting kernel is vastly superior to other alternatives on the SPE and performs up to 1.7 times faster compared to quick sort on 3.2GHz Intel Xeon. The second tier is an in-core bitonic merge optimized for cross-SPE data transfers via asynchronous DMAs, and sorts enough number of items that can fit into the cumulative space available on the local stores of the participating SPEs. We design data transfer and synchronization patters that minimize serial sections of the code by taking advantage of the high aggregate cross-SPE bandwidth available on Cell. Results show that, in-core bitonic sort scales well on the Cell processor with increasing number of SPEs, and performs up to 10 times faster with 16 SPEs compared to parallel quick sort on dual-3.2 GHz Intel Xeon. The third tier is an out-of-core bitonic merge which sorts large number of items stored in the main memory. Results show that, when properly implemented, distributed out-of-core bitonic sort on Cell can significantly outperform the asymptotically (average case) superior quick sort for large number of memory resident items (up to 4 times faster when sorting 0.5GB of data with 16 SPEs, compared to dual-3.2GHz Intel Xeon).

#index 1022312
#* Increasing buffer-locality for multiple index based scans through intelligent placement and index scan speed control
#@ Christian A. Lang;Bishwaranjan Bhattacharjee;Tim Malkemus;Kwai Wong
#t 2007
#c 4
#% 86748
#% 152943
#% 172968
#% 221395
#% 300166
#% 442571
#% 481450
#% 482046
#% 617869
#% 654495
#% 753247
#% 770362
#% 810039
#% 993385
#% 1015335
#% 1016234
#! Decision support systems are characterized by large concurrent scan operations. A significant percentage of these scans are executed as index based scans of the data. This is especially true when the data is physically clustered on the index columns using the various clustering schemes employed by database engines. Common database management systems have only limited ability to reuse buffer content across multiple running queries due to their treatment of queries in isolation. Previous attempts to coordinate scans for better buffer reuse were limited to table scans only. Attempts for index based scan sharing were non existent or were less than satisfactory due to drifting between scans. In this paper, we describe a mechanism to keep scans using the same index closer together on scan position during scanning. This is achieved via intelligent placement of index scans at scan start time based on their scan ranges and speeds. This is then augmented by adaptive throttling of scan speeds based on the index scans runtime behavior during scan execution. We discuss the challenges in doing it for index scans in comparison to the more common table scan sharing. We show that this can be done with minimal changes to an existing database management system as demonstrated in our DB2 UDB prototype. Our experiments show significant gains in end-to-end response times and disk I/O for TPC-H workloads.

#index 1022313
#* SciPort: an adaptable scientific data integration platform for collaborative scientific research
#@ Fusheng Wang;Pierre-Emmanuel Bourgué;Georg Hackenberg;Mo Wang;David Kaltschmidt;Peiya Liu
#t 2007
#c 4
#% 745436
#% 864483
#! Scientific data are posing new challenges to data management due to the large volume, complexity and heterogeneity of the data. Meanwhile, scientific collaboration becomes increasingly important, which relies on integrating and sharing data from distributed institutions. In this demo, we present SciPort, a Web-based platform on supporting scientific data management and integration based on peer-to-peer architectures, where researchers can easily collect, publish, and share their complex scientific data across multi-institutions. SciPort provides a general metadata based data model to capture the context description of experiments and link experiment data into comprehensive metadata documents, and supports a hierarchical organization of the overall data space for data browsing. SciPort takes two alternative "peer"-to-"peer" (or peer-database-to-peer-database) based approaches to integrate scientific data: pure peer-to-peer architecture and central server based peer-to-peer architecture. The later provides a virtual view of all published data from multiple local sites and supports complex queries with XQuery. The system provides a unified framework for adaptable architectures and customizable schemas, and supplies comprehensive tool set to manage and share scientific data. SciPort was first prototyped in Siemens Corporate Research, and now becomes a mature product and has been successfully used in both biomedical research and clinical trials for scientific research communities.

#index 1022314
#* DataScope: viewing database contents in Google Maps' way
#@ Tianyi Wu;Xiaolei Li;Dong Xin;Jiawei Han;Jacob Lee;Ricardo Redder
#t 2007
#c 4
#% 577222
#% 893124
#% 893127
#% 1015294
#% 1016173
#! People have been relying on Google Maps, MapQuest, or other similar services to find desired locations on maps, browse surrounding businesses, get driving directions, etc.. Navigation by clicking and dragging the mouse to browse maps at multiple levels of resolution is one of the most attractive features in Web-based map exploration. Most database systems, though with some graphical user interfaces, are still lack of data-content browsing-based interfaces. Motivated by Google Maps, we develop DataScope, a Web-based data content visualization system, for people to view the desired data easily, interactively, and at multi-resolution.

#index 1022315
#* XBenchMatch: a benchmark for XML schema matching tools
#@ Fabien Duchateau;Zohra Bellahsène;Ela Hunt
#t 2007
#c 4
#% 375017
#% 480645
#% 551850
#% 572314
#% 660001
#% 790848
#% 800550
#% 993982
#% 1705177
#! We present XBenchMatch, a benchmark which uses as input the result of a schema matching algorithm (set of mappings and/or an integrated schema) and generates statistics about the quality of this input and the performance of the matching tool.

#index 1022316
#* GeRoMeSuite: a system for holistic generic model management
#@ David Kensche;Christoph Quix;Xiang Li;Yong Li
#t 2007
#c 4
#% 334025
#% 572314
#% 654457
#% 660001
#% 790848
#% 810103
#% 850730
#% 893094
#% 1393677
#% 1395953
#% 1396152
#% 1409373
#% 1688267
#! Manipulation of models and mappings is a common task in the design and development of information systems. Research in Model Management aims at supporting these tasks by providing a set of operators to manipulate models and mappings. As a framework, GeRoMeSuite provides an environment to simplify the implementation of model management operators. GeRoMeSuite is based on the generic role based metamodel GeRoMe [10], which represents models from different modeling languages (such as XML Schema, OWL, SQL) in a generic way. Thereby, the management of models in a polymorphic fashion is enabled, i.e. the same operator implementations are used regardless of the original modeling language of the schemas. In addition to providing a framework for model management, GeRoMeSuite implements several fundamental operators such as Match, Merge, and Compose.

#index 1022317
#* Semi-automatic schema integration in Clio
#@ Laura Chiticariu;Mauricio A. Hernández;Phokion G. Kolaitis;Lucian Popa
#t 2007
#c 4
#% 22948
#% 39702
#% 442861
#% 458607
#% 480969
#% 572314
#% 993981
#% 1015326
#! Schema integration is the problem of finding a unified representation, called the integrated schema, from a set of source schemas that are related to each other. The relationships between the source schemas can be represented via correspondences between schema elements or via some other forms of schema mappings such as constraints or views. The integrated schema can be viewed as a means for dealing with the heterogeneity in the source schemas, by providing a standard representation of the data. Schema integration has received much of attention in the research literature [1, 2, 6, 8, 10] and still remains a challenge in practice. Existing approaches require substantial amount of human feedback during the integration process and moreover, the outcome of these approaches is a single integrated schema. In general, however, there can be multiple possible schemas that integrate data in different ways and each may be valuable in a given scenario.

#index 1022318
#* XSeek: a semantic XML search engine using keywords
#@ Ziyang Liu;Jeffrey Walker;Yi Chen
#t 2007
#c 4
#% 654442
#% 810052
#% 863389
#% 864456
#% 960261
#% 1015258
#% 1016135
#! We present XSeek, a keyword search engine that enables users to easily access XML data without the need of learning XPath or XQuery and studying possibly complex data schemas. XSeek addresses a challenge in XML keyword search that has been neglected in the literature: how to determine the desired return information, analogous to inferring a "return" clause in XQuery. To infer the search semantics, XSeek recognizes possible entities and attributes in the data, differentiates search predicates and return specifications in the keywords, and generates meaningful search results based on the analysis.

#index 1022319
#* Self-organizing schema mappings in the GridVine peer data management system
#@ Philippe Cudré-Mauroux;Suchit Agarwal;Adriana Budura;Parisa Haghani;Karl Aberer
#t 2007
#c 4
#% 723446
#% 723448
#% 824769
#% 864428
#! GridVine is a Peer Data Management System based on a decentralized access structure. Built following the principle of data independence, it separates a logical layer -- where data, schemas and mappings are managed -- from a physical layer consisting of a structured Peer-to-Peer network supporting efficient routing of messages and index load-balancing. Our system is totally decentralized, yet it fosters semantic interoperability through pairwise schema mappings and query reformulation. In this demonstration, we present a set of algorithms to automatically organize the network of schema mappings. We concentrate on three key functionalities: (1) the sharing of data, schemas and schema mappings in the network, (2) the dynamic creation and deprecation of mappings to foster global interoperability, and (3) the propagation of queries using the mappings. We illustrate these functionalities using bioinformatic schemas and data in a network running on several hundreds of peers simultaneously.

#index 1022320
#* CallAssist: helping call center agents in preference elicitation
#@ Ullas Nambiar;Himanshu Gupta;Mukesh Mohania
#t 2007
#c 4
#% 287631
#% 376266
#% 509533
#% 624479
#% 741441
#% 875003
#% 893143
#! The increasing complexity of products and services being offered by businesses has made providing customers with easy access to technical assistance an important business function. Therefore, most businesses operate call centers to respond to product related queries from consumers. An emerging model is to let a third-party to run the contact center for a business. Preference elicitation - the process of asking queries to determine preferences is a key function performed by call-center agents. In this paper, our focus is on helping call-center agents to efficiently elicit customer's preference.

#index 1022321
#* NS2: networked searchable store with correctness
#@ Radu Sion;Sumeet Bajaj;Bogdan Carbunar;Stefan Katzenbeisser
#t 2007
#c 4
#% 191721
#% 593800
#% 923645
#% 963668
#% 1385024
#! In an outsourced data framework, we introduce and demonstrate mechanisms for securely storing a set of data items (documents) on an un-trusted server, while allowing for subsequent conjunctive keyword searches for matching documents. The protocols provide full computational privacy, query correctness assurances and no leaks: the server either correctly executes client queries or (if it behaves maliciously) is immediately detected. The client is then provided with strong assurances proving the authenticity and completeness of results. This is different from existing secure keyword search research efforts where a cooperating, non-malicious server behavior is assumed. Additionally, not only does the oblivious search protocol conceal the outsourced data (from the un-trusted server) but it also does not leak client access patterns, the queries themselves, the association between different queries or between newly added documents and their corresponding keywords (not even in encrypted form). These assurances come at the expense of additional computation costs which we analyze in the context of today's hardware.

#index 1022322
#* GhostDB: hiding data from prying eyes
#@ Christophe Salperwyck;Nicolas Anciaux;Mehdi Benzine;Luc Bouganim;Philippe Pucheral;Dennis Shasha
#t 2007
#c 4
#% 322884
#% 397367
#% 397395
#% 572304
#% 576761
#% 960290
#% 993943
#% 994006
#! Imagine that you have been entrusted with private data, such as corporate product information, sensitive government information, or symptom and treatment information about hospital patients. You may want to issue queries whose result will combine private and public data, but private data must not be revealed, say, to the prying eyes of some insurance fraudster. GhostDB is an architecture and system to achieve this. You carry private data in a smart USB device (a large Flash persistent store combined with a tamper and snoop-resistant CPU and small RAM). When the key is plugged in, you can issue queries that link private and public data and be sure that the only information revealed to a potential spy is which queries you pose and the public data you access. Queries linking public and private data entail novel distributed processing techniques on extremely unequal devices (standard computer and smart USB device) in which data flows in only one direction: from public to private. This demonstration shows GhostDB's query processing in action.

#index 1022323
#* FuSem: exploring different semantics of data fusion
#@ Jens Bleiholder;Karsten Draba;Felix Naumann
#t 2007
#c 4
#% 480825
#% 514144
#% 591566
#% 641867
#% 810020
#% 824759
#% 1720717
#! Data fusion is the final step of a typical data integration process, after schematic conflicts have been overcome and after duplicates have been correctly identified. We present the relational data fusion system FuSem, which uses schema mappings and information about duplicates to decide what to fuse, i.e., which tuples to merge into one. The aspect emphasized by the demo is how to fuse the duplicates with FuSem. First, it offers several conflict resolution functions to handle data conflicts among duplicates. Furthermore, different fusion semantics proposed in the literature, such as MatchJoin or ConQuer, can be compared and visually explored. Optimized execution allows interactive access to the data and thus to explore the different data fusion procedures.

#index 1022324
#* IndeGS: index supported graphics data server for CFD data postprocessing
#@ Christoph Brochhaus;Thomas Seidl
#t 2007
#c 4
#% 86950
#% 427199
#% 481956
#% 527026
#% 1409339
#! Virtual reality techniques particularly in the field of CFD (computational fluid dynamics) are of growing importance due to their ability to offer comfortable means to interactively explore 3D data sets. The growing accuracy of the simulations brings modern main memory based visualization frameworks to their limits, inducing a limitation on CFD data sizes and an increase in query response times, which are obliged to be very low for efficient interactive exploration. We therefore developed "IndeGS", the index supported graphics data server, to offer efficient dynamic view dependent query processing on secondary storage indexes organized by "IndeGS" offering a high degree of interactivity and mobility in VR environments in the context of CFD postprocessing on arbitrarily sized data sets. Our demonstration setup presents "IndeGS" as an independent network component which can be addressed by arbitrary VR visualization hardware ranging from complex setups (e.g. CAVE, HoloBench) over standard PCs to mobile devices (e.g. PDAs). Our demonstration includes a 2D visualization prototype and a comfortable user interface to simulate view dependent CFD postprocessing performed by an interactive user freely roaming a fully immersive VR environment. Hereby, the effects of the use of different distance functions and query strategies integrated into "IndeGS" are visualized in a comprehensible way.

#index 1022325
#* A STEP towards realizing Codd's vision of rendezvous with the casual user
#@ Michael J. Minock
#t 2007
#c 4
#% 21141
#% 320176
#% 428249
#% 565127
#% 960363
#% 1077427
#% 1698603
#% 1732623
#! This demonstration showcases the STEP system for natural language access to relational databases. In STEP an administrator authors a highly structured semantic grammar through coupling phrasal patterns to elementary expressions within a decidable fragment of tuple relational calculus. The resulting phrasal lexicon serves as a bi-directional grammar, enabling the generation of natural language from tuple relational calculus and the inverse parsing of natural language to tuple calculus. This ability to both understand and generate natural language enables STEP to engage the user in clarification dialogs when the parse of their query is of questionable quality. The STEP system is nearing completion and will soon be field tested in several domains.

#index 1022326
#* TRAX: real-world tracking of moving objects
#@ Christian S. Jensen;Stardas Pakalnis
#t 2007
#c 4
#% 248906
#% 458849
#% 471194
#% 720033
#% 729866
#% 749907
#% 765162
#% 800186
#% 1113048
#! A range of mobile services rely on knowing the current positions of populations of so-called moving objects. In the ideal setting, the positions of all objects are known always and exactly. While this is not possible in practice, it is possible to know each object's position with a certain guaranteed accuracy. This paper presents the TRAX tracking system that supports several techniques capable of tracking the current positions of moving objects with guaranteed accuracies at low update and communication costs in real-world settings. The techniques are readily relevant for practical applications, but they also have implications for continued research. The tracking techniques offer a realistic setting for existing query processing techniques that assume that it is possible to always know the exact positions of moving objects. The techniques enable studies of trade-offs between querying and update, and the accuracy guarantees they offer may be exploited by query processing techniques to offer perfect recall.

#index 1022327
#* Zoom*UserViews: querying relevant provenance in workflow systems
#@ Olivier Biton;Sarah Cohen-Boulakia;Susan B. Davidson
#t 2007
#c 4
#% 832825
#% 893167
#% 1692849
#% 1720925
#! In this demonstration, we present the ZOOM*UserView system, and focus on the module which generates a "user view" based on what tasks the user perceives to be relevant in the workflow specification. We will show how user views can be used to reduce the amount of information returned by provenance queries, while focusing on information the user finds relevant. User views are based on the notion of composite tasks, and induce a higher-level specification of a workflow.

#index 1022328
#* Damia: a data mashup fabric for intranet applications
#@ Mehmet Altinel;Paul Brown;Susan Cline;Rajesh Kartha;Eric Louie;Volker Markl;Louis Mau;Yip-Hing Ng;David Simmen;Ashutosh Singh
#t 2007
#c 4
#% 893087
#! Damia is a lightweight enterprise data integration service where line of business users can create and catalog high value data feeds for consumption by situational applications. Damia is inspired by the Web 2.0 mashup phenomenon. It consists of (1) a browser-based user-interface that allows for the specification of data mashups as data flowfgraphs using a set of operators, (2) a server with an execution engine, as well as (3) APIs for searching, debugging, executing and managing mashups. Damia offers a framework and functionality for dynamic entity resolution, streaming and other higher value features particularly important in the enterprise domain. Damia is currently in perpetual beta in the IBM Intranet. In this demonstration, we showcase the creation and execution of several enterprise data mashups, thereby illustrating the architecture and features of the overall Damia system.

#index 1022329
#* UQLIPS: a real-time near-duplicate video clip detection system
#@ Heng Tao Shen;Xiaofang Zhou;Zi Huang;Jie Shao;Xiangmin Zhou
#t 2007
#c 4
#% 294832
#% 780859
#% 780860
#% 810049
#% 810069
#% 984052
#% 1775378
#% 1857639
#% 1858015
#! Near-duplicate video clip (NDVC) detection is an important problem with a wide range of applications such as TV broadcast monitoring, video copyright enforcement, content-based video clustering and annotation, etc. For a large database with tens of thousands of video clips, each with thousands of frames, can NDVC search be performed in real-time? In addition to considering inter-frame similarity (i.e., spatial information), what is the impact of frame sequence similarity (i.e., temporal information) on search speed and accuracy? UQLIPS is a prototype system for online NDVC detection. The core of UQLIPS comprises two novel complementary schemes for detecting NDVCs. Bounded Coordinate System (BCS), a compact representation model ignoring temporal information, globally summarizes each video to a single vector which captures the dominating content and content changing trends of each clip. The other proposal, named FRAme Symbolization (FRAS), maps each clip to a sequence of symbols, and takes temporal order and sequence context information into consideration. Using a large collection of TV commercials, UQLIPS clearly demonstrates that it is feasible to perform real-time NDVC detection with high accuracy.

#index 1022330
#* The GCX system: dynamic buffer minimization in streaming XQuery evaluation
#@ Christoph Koch;Stefanie Scherzinger;Michael Schmidt
#t 2007
#c 4
#% 413563
#% 473366
#% 801685
#% 824673
#% 824674
#% 827135
#% 893111
#% 912238
#% 982759
#% 1015272
#% 1016148
#! In this demonstration, we present the main-memory based streaming XQuery engine GCX which implements novel buffer management strategies that combine static and dynamic analysis to keep main memory consumption low. Depending on the progress made in query evaluation, memory buffers are dynamically purged and minimized. In this demo, we show the various stages in evaluating a practical fragment of XQuery with GCX. We present the major steps in static analysis and demonstrate the mechanisms of dynamic buffer minimization. We apply our system to XML streams and demonstrate the significant impact of our approach on reducing main memory consumption and running time.

#index 1022331
#* A cost-estimation component for statement sequences
#@ Tobias Kraft
#t 2007
#c 4
#% 479984
#% 495408
#% 1015295
#% 1016267
#% 1406038
#! Query generators producing sequences of SQL statements are embedded in many applications. As the execution time of such sequences is often far from optimal, their optimization is an important issue. Therefore, in [5] we proposed a rule-based optimization approach, which we called CGO (Coarse-Grained Optimization). Our first prototype used a heuristic, priority-based control strategy to choose the rewrite rules that should be applied to a given statement sequence. This worked well but there is still potential for improvements. Thus, in [4] we have introduced an approach to provide cost estimates for statement sequences which is the basis for a cost-based CGO optimizer. It exploits histogram propagation and the optimizer of the underlying database system for this purpose. In this demonstration, we want to showcase the functionality and the effectiveness of our approach. Thereto, we present a prototype of a cost-estimation component for statement sequences which implements this approach. It includes a graphical user interface to explain the histogram-propagation process and to report the results of the cost-estimation process. In the setup for this demonstration, we use a TPC-H benchmark database with an appropriate set of sequences as sample scenario.

#index 1022332
#* Eliminating impedance mismatch in C++
#@ Joseph (Yossi) Gil;Keren Lenz
#t 2007
#c 4
#% 347
#% 33000
#% 33001
#% 38691
#% 88613
#% 287751
#% 308504
#% 388393
#% 390039
#% 427224
#% 763261
#% 807068
#% 840506
#% 875029
#% 979017
#% 1389619
#! Recently, the C# and the VISUAL BASIC communities were tantalized by the advent of LINQ [18]---the Language INtegrated Query technology from Microsoft. LINQ represents a set of language extensions relying on advanced (some say hard to understand) techniques drawn from functional languages such as type inference, λ-expressions and most importantly, monads. The 3rd edition of C# just as the 9th of VISUAL BASIC allow programmer to directly access relational and XML-based databases from within the programming language. We show that very similar capabilities can be achieved in the C++ programming language without relying on any language extensions, compiler modifications, external processing tools, or any other vendor specific machinery: ARATAT is a C++ template library whose objective is type safe generation of SQL statements for access relational database systems. Learning curve is minimal since ARARAT resembles relational algebra, which is at the core of SQL.

#index 1022333
#* Large scale P2P distribution of open-source software
#@ Serge Abiteboul;Itay Dar;Radu Pop;Gabriel Vasile;Dan Vodislav;Nicoleta Preda
#t 2007
#c 4
#% 505869
#% 765420
#% 791020
#% 800621
#% 960296
#% 963598
#! Open-source software communities currently face an increasing complexity in managing and distributing software content among their developers and contributors. This is mainly due to the continuously growing size of the software, of the community, the high frequency of updates, and the heterogeneity of the participants. We propose a large scale P2P distribution system that tackles two main issues in software content management: efficient content dissemination and advanced information system capabilities.

#index 1022334
#* HiSbase: histogram-based P2P main memory data management
#@ Tobias Scholl;Bernhard Bauer;Benjamin Gufler;Richard Kuntschke;Daniel Weber;Angelika Reiser;Alfons Kemper
#t 2007
#c 4
#% 68091
#% 340175
#% 411251
#% 415957
#% 505869
#% 723555
#% 839352
#% 914605
#% 946433
#% 1015281
#! Many e-science communities, e. g., medicine, climatology, and astrophysics, are overwhelmed by the exponentially growing data volumes that need to be accessible by collaborating researchers. Nowadays, new scientific results are often obtained by exploring and cross-correlating data from different distributed sources [3]. However, neither centralized data processing by shipping the data to the processing site on demand nor a centralized data warehouse approach scale sufficiently to handle the huge data volumes and processing demands of future e-science communities and applications. The former suffers from high transmission costs while the latter cannot scale to the large amounts of data in combination with the growing number of queries.

#index 1022335
#* P2P authority analysis for social communities
#@ Josiane Xavier Parreira;Sebastian Michel;Matthias Bender;Tom Crecelius;Gerhard Weikum
#t 2007
#c 4
#% 451536
#% 505869
#% 577367
#% 754098
#% 754107
#% 812792
#% 836146
#% 845353
#% 875063
#% 881493
#% 881523
#% 893123
#% 893125
#% 943040
#% 1016164
#% 1016177
#% 1711388
#! PageRank-style authority analyses of Web graphs are of great importance for Web mining. Such authority analyses also apply to hot "Web 2.0" applications that exhibit a natural graph structure, such as social networks (e.g., MySpace, Facebook) or tagging communities (e.g., Flickr, Del.icio.us). Finding the most trustworthy or most important authorities in such a community is a pressing need, given the huge scale and also the anonymity of social networks. Computing global authority measures in a Peer-to-Peer (P2P) collaboration of autonomous peers is a hot research topic, in particular because of the incomplete local knowledge of the peers, which typically only know about (arbitrarily overlapping) sub-graphs of the complete graph. We demonstrate a self-organizing P2P collaboration that, based on the local sub-graphs, efficiently computes global authority scores. In hand with the loosely-coupled spirit of a P2P system, the computation is carried out in a completely asynchronous manner without any central knowledge or coordinating instance. We demonstrate the applicability of authority analyses to large-scale distributed systems.

#index 1022336
#* SOR: a practical system for ontology storage, reasoning and search
#@ Jing Lu;Li Ma;Lei Zhang;Jean-Sébastien Brunner;Chen Wang;Yue Pan;Yong Yu
#t 2007
#c 4
#% 452641
#% 519567
#% 956569
#% 1015335
#% 1667768
#% 1684010
#% 1713480
#! Ontology, an explicit specification of shared conceptualization, has been increasingly used to define formal data semantics and improve data reusability and interoperability in enterprise information systems. In this paper, we present and demonstrate SOR (Scalable Ontology Repository), a practical system for ontology storage, reasoning, and search. SOR uses Relational DBMS to store ontologies, performs inference over them, and supports SPARQL language for query. Furthermore, a faceted search with relationship navigation is designed and implemented for ontology search. This demonstration shows how to efficiently solve three key problems in practical ontology management in RDBMS, namely storage, reasoning, and search. Moreover, we show how the SOR system is used for semantic master data management.

#index 1022337
#* Periscope/SQ: interactive exploration of biological sequence databases
#@ Sandeep Tata;Willis Lang;Jignesh M. Patel
#t 2007
#c 4
#% 385828
#% 469572
#% 829998
#% 853017
#% 864474
#! Life science laboratories today have to rely on procedural techniques to store and manage large sequence datasets. Procedural techniques are cumbersome to use and are often very inefficient compared to optimized declarative techniques. We have designed and implemented a system called Periscope/SQ that makes it possible to rapidly express complex queries within a declarative framework and take advantage of database-style query optimization. As a result, queries in Periscope/SQ run orders of magnitude faster than typical procedural implementations. We demonstrate the power of Persicope/SQ through an application called Gene-Locator which allows biologists to rapidly explore large genomic sequence databases.

#index 1022338
#* BlogScope: a system for online analysis of high volume text streams
#@ Nilesh Bansal;Nick Koudas
#t 2007
#c 4
#% 279755
#% 643520
#% 748499
#% 823332
#% 956523
#% 960263
#! We present BlogScope (www.blogscope.net), a system for online analysis of temporally ordered streaming text, currently applied to the analysis of the Blogosphere. The system currently tracks over ten million blogs and handles hundreds of thousands of updates daily. BlogScope is an information discovery and text analysis system that offers a set of unique features. Such features include, spatio-temporal analysis of blogs, flexible navigation of the Blogosphere through information bursts, keyword correlations and burst synopsis, as well as enhanced ranking functions for improved query answer relevance. We describe the system, its design and the features of the current version of BlogScope.

#index 1022339
#* FluxCapacitor: efficient time-travel text search
#@ Klaus Berberich;Srikanta Bedathur;Thomas Neumann;Gerhard Weikum
#t 2007
#c 4
#% 466506
#% 867054
#% 956535
#% 987257
#% 1016154
#% 1392437
#% 1404876
#! An increasing number of temporally versioned text collections is available today with Web archives being a prime example. Search on such collections, however, is often not satisfactory and ignores their temporal dimension completely. Time-travel text search solves this problem by evaluating a keyword query on the state of the text collection as of a user-specified time point. This work demonstrates our approach to efficient time-travel text search and its implementation in the FLUXCAPACITOR prototype.

#index 1022340
#* Deadline and QoS aware data warehouse
#@ Wen-Syan Li;Dengfeng Gao;Rafae Bhatti;Inderpal Narang;Hirofumi Matsuzawa;Masayuki Numao;Masahiro Ohkawa;Takeshi Fukuda
#t 2007
#c 4
#% 465007
#% 480158
#% 632100
#% 800595
#% 820356
#% 1016220
#! A data warehouse infrastructure needs to support the requirement of (day time) ad hoc query response time and (night time) batch workload completion time. The following tasks need to be finished in a batch window: (1) Apply one day's delta data to the base tables; (2) refresh MQTs (Materialized Query Tables) for ad hoc queries and batch workloads; (3) run batch queries. Tools are available to optimize each step; however, many factors need to be considered for improving the overall performance of a data warehouse (i.e. meeting batch window deadline and ad hoc query response time). We have prototyped a Data Warehouse Operation Advisor to systematically study each component contributing to the batch window problem, and then perform global optimization to achieve desired results!

#index 1022341
#* Query language support for incomplete information in the MayBMS system
#@ Lyublena Antova;Christoph Koch;Dan Olteanu
#t 2007
#c 4
#% 960293
#% 1661439
#! MayBMS [4, 1, 3, 2] is a data management system for incomplete information developed at Saarland University. Its main features are a simple and compact representation system for incomplete information and a language called I-SQL with explicit operations for handling uncertainty. MayBMS is currently an extension of PostgreSQL and manages both complete and incomplete data and evaluates I-SQL queries.

#index 1022342
#* Adaptive query processing: why, how, when, what next?
#@ Amol Deshpande;Zachary Ives;Vijayshankar Raman
#t 2007
#c 4
#% 1026989
#! Adaptive query processing has been the subject of a great deal of recent work, particularly in emerging data management environments such as data integration and data streams. We provide an overview of the work in this area, identifying its common themes, laying out the space of query plans, and discussing open research problems. We discuss why adaptive query processing is needed, how it is being implemented, where it is most appropriately used, and finally, what next, i.e., open research problems.

#index 1022343
#* Raster databases
#@ Peter Baumann
#t 2007
#c 4
#! Since the launch of Google Earth at the latest it is clear that online services for multi-Terabyte satellite imagery are becoming integral part of our Internet experience. Actually, 2-D imagery is but the tip of the iceberg - the general concept of multi-dimensional spatio-temporal raster data covers 1-D sensor time series, 2-D imagery, 3-D image time series (x/y/t) and exploration data (x/y/z), 4-D climate models (x/y/z/t), and many more.

#index 1022344
#* From data privacy to location privacy: models and algorithms
#@ Ling Liu
#t 2007
#c 4
#% 452685
#% 812799
#% 844360
#% 864412
#% 893151
#% 911803
#% 956531
#% 1034732
#! This tutorial presents the definition, the models and the techniques of location privacy from the data privacy perspective. By reviewing and revising the state of art research in data privacy area, the presenter describes the essential concepts, the alternative models, and the suite of techniques for providing location privacy in mobile and ubiquitous data management systems. The tutorial consists of two main components. First, we will introduce location privacy threats and give an overview of the state of art research in data privacy and analyze the applicability of the existing data privacy techniques to location privacy problems. Second, we will present the various location privacy models and techniques effective in either the privacy policy based framework or the location anonymization based framework. The discussion will address a number of important issues in both data privacy and location privacy research, including the location utility and location privacy trade-offs, the need for a careful combination of policy-based location privacy mechanisms and location anonymization based privacy schemes, as well as the set of safeguards for secure transmission, use and storage of location information, reducing the risks of unauthorized disclosure of location information. The tutorial is designed to be self-contained, and gives the essential background for anyone interested in learning about the concept and models of location privacy, and the principles and techniques for design and development of a secure and customizable architecture for privacy-preserving mobile data management in mobile and pervasive information systems. This tutorial is accessible to data management administrators, mobile location based service developers, and graduate students and researchers who are interested in data management in mobile information syhhhstems, pervasive computing, and data privacy.

#index 1022345
#* Secure data outsourcing
#@ Radu Sion
#t 2007
#c 4
#% 264163
#% 566391
#% 593711
#% 593800
#% 659992
#% 810042
#% 824701
#% 1386192
#% 1386206
#% 1669498
#! The networked and increasingly ubiquitous nature of today's data management services mandates assurances to detect and deter malicious or faulty behavior. This is particularly relevant for outsourced data frameworks in which clients place data management with specialized service providers. Clients are reluctant to place sensitive data under the control of a foreign party without assurances of confidentiality. Additionally, once outsourced, privacy and data access correctness (data integrity and query completeness) become paramount. Today's solutions are fundamentally insecure and vulnerable to illicit behavior, because they do not handle these dimensions. In this tutorial we will explore how to design and build robust, efficient, and scalable data outsourcing mechanisms providing strong security assurances of (1) correctness, (2) confidentiality, and (3) data access privacy. There exists a strong relationship between such assurances; for example, the lack of access pattern privacy usually allows for statistical attacks compromising data confidentiality. Confidentiality can be achieved by data encryption. However, to be practical, outsourced data services should allow expressive client queries (e.g., relational joins with arbitrary predicates) without compromising confidentiality. This is a hard problem because decryption keys cannot be directly provided to potentially untrusted servers. Moreover, if the remote server cannot be fully trusted, protocol correctness become essential. Therefore, solutions that do not address all three dimensions are incomplete and insecure.

#index 1022346
#* Regulatory-compliant data management
#@ Radu Sion;Marianne Winslett
#t 2007
#c 4
#! Digital societies and markets increasingly mandate consistent procedures for the access, processing and storage of information. In the United States alone, over 10,000 such regulations can be found in financial, life sciences, health - care and government sectors, including the Gramm - Leach - Bliley Act, Health Insurance Portability and Accountability Act, and Sarbanes - Oxley Act. A recurrent theme in these regulations is the need for regulatory - compliant data management as an underpinning to ensure data confidentiality, access integrity and authentication; provide audit trails, guaranteed deletion, and data migration; and deliver Write Once Read Many (WORM) assurances, essential for enforcing long - term data retention and life - cycle policies.

#index 1022347
#* Probabilistic graphical models and their role in databases
#@ Amol Deshpande;Sunita Sarawagi
#t 2007
#c 4
#% 333943
#% 333946
#% 333986
#% 464434
#% 466892
#% 496116
#% 531459
#% 677787
#% 727667
#% 731721
#% 765402
#% 765455
#% 769884
#% 827631
#% 864435
#% 876044
#% 893168
#% 976984
#% 1016178
#! Probabilistic graphical models provide a framework for compact representation and efficient reasoning about the joint probability distribution of several interdependent variables. This is a classical topic with roots in statistical physics. In recent years, spurred by several applications in unstructured data integration, sensor networks, image processing, bio-informatics, and code design, the topic has received renewed interest in the machine learning, data mining, and database communities. Techniques from graphical models have also been applied to many topics directly of interest to the database community including information extraction, sensor data analysis, imprecise data representation and querying, selectivity estimation for query optimization, and data privacy. As database research continues to expand beyond the confines of traditional enterprise domains, we expect both the need and applicability of probabilistic graphical models to increase dramatically over the next few years. With this tutorial, we are aiming to provide a foundational overview of probabilistic graphical models to the database community, accompanied by a brief overview of some of the recent research literature on the role of graphical models in databases.

#index 1022348
#* XML Retrieval: DB/IR in theory, web in practice
#@ Sihem Amer-Yahia;Ricardo Baeza-Yates;Mariano P. Consens;Mounia Lalmas
#t 2007
#c 4
#! The world of data has been developed from two main points of view: the structured relational data model and the unstructured text model. The two distinct cultures of databases and information retrieval now have a natural meeting place in the Web with its semi-structured XML model. Data in Digital Libraries and in Enterprise Environments also shares many of the semi-structured characteristics of web data. As web-style searching becomes an ubiquitous tool, the need for integrating these two viewpoints becomes even more important. In particular, we consider the application of DB and IR research to querying Web data in the context of online communities. With Web 2.0, the question arises: how can search interfaces remain simple when users are allowed to contribute content (Wikipedia), share it (Flickr), and rate it (YouTube)? When they can decide who their friends are (del.icio.us), what they like to see, and how they want it to look like (MySpace)? While we want to keep the user interface simple (keyword search), we would like to study the applicability of querying structure and content to a context where new forms of data-driven dynamic web content (e.g. user feed-back, tags, contributed multimedia) are provided. This tutorial will provide an overview of the different issues and approaches put forward by the IR and DB communities and survey the DB-IR integration efforts as they focus in the problem of retrieval from XML content. In particular, the context of querying content in online communities is an excellent example of such an application. Both earlier proposals as well as recent ones will be discussed. A variety of application scenarios for XML Retrieval will be covered, including examples of current tools and techniques.

#index 1022349
#* Model management and schema mappings: theory and practice
#@ Philip A. Bernstein;Howard Ho
#t 2007
#c 4
#% 328429
#% 332166
#% 480134
#% 809239
#% 810078
#% 881740
#% 945790
#% 960233
#% 960272
#! We present an overview of a tutorial on model management---an approach to solving data integration problems, such as data warehousing, e-commerce, object-to-relational mapping, schema evolution and enterprise information integration. Model management defines a small set of operations for manipulating schemas and mappings, such as Match, Compose, Inverse, and Merge. The long-term goal is to build generic implementations of the operations that can be applied to a wide variety of data integration problems.

#index 1022350
#* Performance evaluation and experimental assessment: conscience or curse of database research?
#@ Ioana Manolescu;Stefan Manegold
#t 2007
#c 4
#! Performance, performance and performance used to be the three things that really mattered in database research. Most of our published works indeed include an experimental evaluation of the proposed techniques. However, such evaluations are sometimes seen as a "must-have" eating up the valuable space where one could describe new ideas. The experimental evaluations end up being short, lacking important information to interpret and/or reproduce the results, and often end without clear conclusion.

#index 1022351
#* What does web 2.0 have to do with databases?
#@ Sihem Amer-Yahia;Alon Halevy
#t 2007
#c 4
#! Web 2.0 is a buzzword we have been hearing for over 2 years. According to Wikipedia, it hints at an improved form of the World Wide Web where technologies such as weblogs, social bookmarking, RSS feeds, photo and video sharing, based on an architecture of participation and democracy that encourages users to add value to the application as they use it. Web 2.0 enables social networking on the Web by allowing users to contribute content, share it, rate it, create a network of friends, and decide what they like to see and how they want it to look like.

#index 1083753
#* Proceedings of the 5th workshop on Data management for sensor networks
#@ Magdalena Balazinska;Jun Yang;Yanlei Diao;Christian S. Jensen
#t 2008
#c 4
#! Sensor networks aim to offer unprecedented means of monitoring the physical world, thus enabling entirely new applications. Many areas of science contribute to the research on sensor networks, for which reason many conferences exist that either cover or are devoted solely to sensor networks. The International Workshop on Data Management for Sensor Networks series, which was inaugurated in 2004 and of which this workshop is the fifth edition, stands out as a unique forum devoted to early and innovative work on data management in sensor networks. The workshop thus fills the gap in-between the database and other sensor network research areas. The scope of DMSN'08 covers all important aspects of sensor data management, including data acquisition, processing, and storage in remote wireless networks; the handling of uncertain sensor data; and the management of heterogeneous and sometimes sensitive sensor data in databases. The resource-constrained, lossy, noisy, distributed, and remote nature of wireless sensor networks implies that traditional database techniques often cannot be applied without significant retooling. Challenges associated with acquiring, processing, and archiving large-scale, heterogeneous sets of live sensor data also call for novel data management techniques. The inherently incomplete and noisy nature of sensor data further calls for techniques for data cleaning, inference, approximation. Finally, in many applications, the collecting of sensor data raises important privacy and security concerns that require new protection and anonymization techniques. DMSN'08 received 20 submissions, each of which was assigned to three or four members of the program committee. Based on the reviews and discussions among the program committee members, 8 papers were accepted for inclusion in these proceedings and for presentation at the workshop. The papers are grouped into three sessions, the first of which concerns in-network aggregation. With the purpose of reducing communication costs, Baljeet et al. study the use of tree topologies based on dominating sets for the forwarding and aggregation when computing MAX queries. Next, Cho et al. propose a new, so-called partial ordered tree that is capable of exploiting spatial correlation among sensor readings when performing top-k monitoring. Finally, Kontaki et al. propose a distributed solution for computing the d-hop k-data coverage query that generalizes previously considered queries. The second session concerns query processing trade-offs that involve the use of energy. In particular, Tang and Cao propose a data-driven power management framework, with which data accuracy and communication latency can be traded for improved energy efficiency. Trajcevski et al. study the trading of latency for improved balancing of energy consumption in a sensor network. The third session covers various aspects of complex sensor data processing. First, Mihaylov et al. consider the use of in-network joins for data integration in ad hoc networks and sensor and streams systems. Next, Evers et al. propose to associate sensor readings with time intervals rather than time points and then consider the use of two Hidden Markov Models, where a sensor value may extend across multiple hidden states, in this context. Their focus is on inference algorithms for these models. Karpinski and Cahill end this session by proposing a stream-based language targeted specifically at the programming of wireless networks encompassing both sensors and actuators. The workshop concludes with a panel. In this panel, well-known researchers in the community of sensor data management present and discuss specific new applications of sensor network technologies, as well as the notable technological challenges posed by these applications. As such, the panel contributes to setting new directions for the field.

#index 1127349
#* Is transactional memory an oxymoron?
#@ Mark D. Hill
#t 2008
#c 4
#! Transactional memory (TM) was invented 15 years ago [1]. Recently, however, TM activity has exploded [2], as the proliferation of multicore chips has provoked researchers to revisit support for parallel programming. Since some regard me as a TM expert, most of my talk will summarize TM's goals and implementation options, primarily by developing a taxonomy and using Wisconsin LogTM [3, 4] as a case study. In particular, I will consider TM implementations via software, hardware, and hybrids, as well as important design choices, such as how to buffer TM writes and when to detect TM conflicts. I will conclude with forward-looking comments regarding TM and database transactions. In theory, the two concepts have many similarities. In practice, however, they differ substantially. First, today's TM exclusively targets concurrency, while database transactions seek reliability first. Second, TM implementations focus in operations with caches and memory, while database transactions deal more with the more substantial access gap between memory and disks. Finally, I will speculate on cross-fertilization opportunities between TM and database transactions. I will not provide answers here, in part because you are the database experts. Nevertheless, I hope to encourage us all to ask the right questions.

#index 1127350
#* Databases and the silification of health
#@ Justin Zobel
#t 2008
#c 4
#! Developments in databases and computing are helping to create a revolution in health and in biomedical research. Many aspects of medicine are increasingly data-centric, from basics such as record-keeping to diagnosis and biological discovery. Drivers of change include massive curated biological data sets, consolidations of medical knowledge into systematised online repositories, innovations in data linkage, change of practice in hospitals, and new diagnostic technologies. However, the volumes of data means that new database and computational innovations are required if the data's value is to be fully exploited. This talk reviews the biomedical mechanisms that are creating data and explores achievements and challenges for database researchers in future health.

#index 1127351
#* What's wrong with high-dimensional similarity search?
#@ Stephen Blott;Roger Weber
#t 2008
#c 4
#% 479649
#! Similarity search in high-dimensional vector spaces has been the subject of substantial research, motivated in part by the need to provide query support for images and other complex data types. The paper VLDB 1998 paper "Quantitative Analysis and Performance Study for Similarity-Search Methods in High-Dimensional Spaces" analyses why this search problem can be so tricky, and shows with intuitive yet formal proofs that nearest-neighbour search is fundamentally linear beyond a certain dimensionality. Consequently, the paper proposes a new, linear search structure (the VA-File) which focuses on accelerating the indispensable sequential scan with approximations and computational schemes to reduce both CPU and IO efforts. Experiments with both synthetic and image data showed -- surprisingly, at the time -- that such schemes outperform hierarchical methods in all cases where the dimensionality is greater than five. In this paper, we review that work and identify both what we got right in the paper and its impact, and also (with the benefit of hindsight) those elements of the work for which we were off the mark. The lessons learned are relevant not just to the narrow area of similarity search, but also more broadly across the fields of databases and computing.

#index 1127352
#* Constrained physical design tuning
#@ Nicolas Bruno;Surajit Chaudhuri
#t 2008
#c 4
#% 248815
#% 465167
#% 466374
#% 480158
#% 482100
#% 557649
#% 631950
#% 632100
#% 750953
#% 810026
#% 820356
#% 893130
#% 1016220
#% 1016221
#% 1063540
#% 1207101
#% 1688268
#! Existing solutions to the automated physical design problem in database systems attempt to minimize execution costs of input workloads for a given a storage constraint. In this paper, we argue that this model is not flexible enough to address several real-world situations. To overcome this limitation, we introduce a constraint language that is simple yet powerful enough to express many important scenarios. We build upon an existing transformation-based framework to effectively incorporate constraints in the search space. We then show experimentally that we are able to handle a rich class of constraints and that our proposed technique scales gracefully.

#index 1127353
#* Scalable multi-query optimization for exploratory queries over federated scientific databases
#@ Anastasios Kementsietsidis;Frank Neven;Dieter Van de Craen;Stijn Vansummeren
#t 2008
#c 4
#% 992
#% 36117
#% 44054
#% 54047
#% 264263
#% 286916
#% 286991
#% 289282
#% 300166
#% 300179
#% 330305
#% 384978
#% 442714
#% 443065
#% 565473
#% 654468
#% 762652
#% 772131
#% 1022258
#% 1038848
#% 1044502
#% 1675421
#% 1688291
#% 1688292
#% 1692837
#! The diversity and large volumes of data processed in the Natural Sciences today has led to a proliferation of highly-specialized and autonomous scientific databases with inherent and often intricate relationships. As a user-friendly method for querying this complex, ever-expanding network of sources for correlations, we propose exploratory queries. Exploratory queries are loosely-structured, hence requiring only minimal user knowledge of the source network. Evaluating an exploratory query usually involves the evaluation of many distributed queries. As the number of such distributed queries can quickly become large, we attack the optimization problem for exploratory queries by proposing several multi-query optimization algorithms that compute a global evaluation plan while minimizing the total communication cost, a key bottleneck in distributed settings. The proposed algorithms are necessarily heuristics, as computing an optimal global evaluation plan is shown to be NP-hard. Finally, we present an implementation of our algorithms, along with experiments that illustrate their potential not only for the optimization of exploratory queries, but also for the multiquery optimization of large batches of standard queries.

#index 1127354
#* Clustera: an integrated computation and data management system
#@ David J. DeWitt;Erik Paulson;Eric Robinson;Jeffrey Naughton;Joshua Royalty;Srinath Shankar;Andrew Krioukov
#t 2008
#c 4
#% 24042
#% 115661
#% 479920
#% 599216
#% 874977
#% 963669
#% 983001
#% 983467
#% 990412
#% 1011733
#% 1063553
#! This paper introduces Clustera, an integrated computation and data management system. In contrast to traditional cluster-management systems that target specific types of workloads, Clustera is designed for extensibility, enabling the system to be easily extended to handle a wide variety of job types ranging from computationally-intensive, long-running jobs with minimal I/O requirements to complex SQL queries over massive relational tables. Another unique feature of Clustera is the way in which the system architecture exploits modern software building blocks including application servers and relational database systems in order to realize important performance, scalability, portability and usability benefits. Finally, experimental evaluation suggests that Clustera has good scale-up properties for SQL processing, that Clustera delivers performance comparable to Hadoop for MapReduce processing and that Clustera can support higher job throughput rates than previously published results for the Condor and CondorJ2 batch computing systems.

#index 1127355
#* Performance profiling with EndoScope, an acquisitional software monitoring framework
#@ Alvin Cheung;Samuel Madden
#t 2008
#c 4
#% 127983
#% 239250
#% 239252
#% 315184
#% 393907
#% 411772
#% 603078
#% 654482
#% 800584
#% 809099
#% 878299
#% 960178
#% 963595
#% 963675
#% 978396
#% 1019321
#! We propose EndoScope, a software monitoring framework that allows users to pose declarative queries that monitor the state and performance of running programs. Unlike most existing monitoring tools, EndoScope is acquisitional, meaning that it only instruments the portions of the program that need to be monitored to answer queries. The use of a high level declarative language allows EndoScope to search for efficient physical instantiations of queries by applying a suite of optimizations, including control flow graph analysis, and traditional database query optimization techniques, such as predicate pushdown and join optimization, to minimize the number of program instrumentation points and overhead to the monitored program. Furthermore, a flexible, high level language and the ability to attach to running programs enable developers to build various program analysis and monitoring applications beyond traditional software profilers with EndoScope. We describe a prototype implementation of the EndoScope framework and a simple profiler for Java programs implemented with EndoScope. We show results from using our profiler on a collection of real-world programs, including a TPC-C implementation using the Derby database and the petstore application running on top of Tomcat application server. Our results show the benefit of our optimization framework and demonstrate that our declarative, acquisitional approach can yield program instrumentation overheads that are dramatically lower than conventional profiling tools (for example, when profiling the Derby Database running TPC-C, our system's overhead ranges from 1% to about 25%, whereas the fastest existing profiler we measured imposes a minimum overhead of about 30%.)

#index 1127356
#* Mining search engine query logs via suggestion sampling
#@ Ziv Bar-Yossef;Maxim Gurevich
#t 2008
#c 4
#% 8387
#% 77967
#% 268114
#% 340888
#% 801673
#% 807320
#% 818221
#% 869499
#% 878624
#% 907547
#% 956534
#% 960286
#% 987215
#% 1227446
#! Many search engines and other web applications suggest auto-completions as the user types in a query. The suggestions are generated from hidden underlying databases, such as query logs, directories, and lexicons. These databases consist of interesting and useful information, but they are typically not directly accessible. In this paper we describe two algorithms for sampling suggestions using only the public suggestion interface. One of the algorithms samples suggestions uniformly at random and the other samples suggestions proportionally to their popularity. These algorithms can be used to mine the hidden suggestion databases. Example applications include comparison of popularity of given keywords within a search engine's query log, estimation of the volume of commercially-oriented queries in a query log, and evaluation of the extent to which a search engine exposes its users to negative content. Our algorithms employ Monte Carlo methods in order to obtain unbiased samples from the suggestion database. Empirical analysis using a publicly available query log demonstrates that our algorithms are efficient and accurate. Results of experiments on two major suggestion services are also provided.

#index 1127357
#* Plan-based complex event detection across distributed sources
#@ Mert Akdere;Uǧur Çetintemel;Nesime Tatbul
#t 2008
#c 4
#% 36117
#% 177755
#% 279905
#% 411554
#% 481448
#% 631974
#% 654510
#% 875004
#% 960278
#% 1394395
#! Complex Event Detection (CED) is emerging as a key capability for many monitoring applications such as intrusion detection, sensor-based activity & phenomena tracking, and network monitoring. Existing CED solutions commonly assume centralized availability and processing of all relevant events, and thus incur significant overhead in distributed settings. In this paper, we present and evaluate communication efficient techniques that can efficiently perform CED across distributed event sources. Our techniques are plan-based: we generate multi-step event acquisition and processing plans that leverage temporal relationships among events and event occurrence statistics to minimize event transmission costs, while meeting application-specific latency expectations. We present an optimal but exponential-time dynamic programming algorithm and two polynomial-time heuristic algorithms, as well as their extensions for detecting multiple complex events with common sub-expressions. We characterize the behavior and performance of our solutions via extensive experimentation on synthetic and real-world data sets using our prototype implementation.

#index 1127358
#* Finding relevant patterns in bursty sequences
#@ Alexander Lachmann;Mirek Riedewald
#t 2008
#c 4
#% 310559
#% 316552
#% 329537
#% 459006
#% 463903
#% 464996
#% 481290
#% 577220
#% 577256
#% 783708
#% 798033
#% 884557
#% 1000456
#! Sequence data is ubiquitous and finding frequent sequences in a large database is one of the most common problems when analyzing sequence data. Unfortunately many sources of sequence data, e.g., sensor networks for data-driven science, RFID-based supply chain monitoring, and computing system monitoring infrastructure, produce a challenging workload for sequence mining. It is common to find bursts of events of the same type. Such bursts result in high mining cost, because input sequences are longer. An even greater challenge is that these bursts tend to produce an overwhelming number of irrelevant repetitive sequence patterns with high support. Simply raising the support threshold is not a solution, because at some point interesting sequences will get eliminated. As an alternative we propose a novel transformation of the input sequences. We show that this transformation has several desirable properties. First, the transformed data can still be mined with existing sequence mining algorithms. Second, for a given support threshold the mining result can often be obtained much faster and it is usually much smaller and easier to interpret. Third, and most importantly, we show that the result sequences retain the important characteristics of the sequences that would have been found in the original (not transformed) data. We validate our technique with an experimental study using synthetic and real data.

#index 1127359
#* Constrained locally weighted clustering
#@ Hao Cheng;Kien A. Hua;Khanh Vu
#t 2008
#c 4
#% 36672
#% 273891
#% 296738
#% 300131
#% 413610
#% 413620
#% 443979
#% 464291
#% 464608
#% 464631
#% 464783
#% 466890
#% 765518
#% 770782
#% 770830
#% 780861
#% 875014
#% 881550
#% 1021189
#% 1071085
#% 1250560
#% 1663626
#! Data clustering is a difficult problem due to the complex and heterogeneous natures of multidimensional data. To improve clustering accuracy, we propose a scheme to capture the local correlation structures: associate each cluster with an independent weighting vector and embed it in the subspace spanned by an adaptive combination of the dimensions. Our clustering algorithm takes advantage of the known pairwise instance-level constraints. The data points in the constraint set are divided into groups through inference; and each group is assigned to the feasible cluster which minimizes the sum of squared distances between all the points in the group and the corresponding centroid. Our theoretical analysis shows that the probability of points being assigned to the correct clusters is much higher by the new algorithm, compared to the conventional methods. This is confirmed by our experimental results, indicating that our design indeed produces clusters which are closer to the ground truth than clusters created by the current state-of-the-art algorithms.

#index 1127360
#* Resisting structural re-identification in anonymized social networks
#@ Michael Hay;Gerome Miklau;David Jensen;Don Towsley;Philipp Weis
#t 2008
#c 4
#% 288652
#% 300079
#% 576761
#% 864412
#% 904307
#% 956511
#% 1022246
#% 1029292
#% 1063476
#% 1068392
#% 1206620
#% 1206763
#% 1387896
#% 1415851
#% 1740518
#! We identify privacy risks associated with releasing network data sets and provide an algorithm that mitigates those risks. A network consists of entities connected by links representing relations such as friendship, communication, or shared activity. Maintaining privacy when publishing networked data is uniquely challenging because an individual's network context can be used to identify them even if other identifying information is removed. In this paper, we quantify the privacy risks associated with three classes of attacks on the privacy of individuals in networks, based on the knowledge used by the adversary. We show that the risks of these attacks vary greatly based on network structure and size. We propose a novel approach to anonymizing network data that models aggregate network structure and then allows samples to be drawn from that model. The approach guarantees anonymity for network entities while preserving the ability to estimate a wide variety of network measures with relatively little bias.

#index 1127361
#* Privacy-preserving anonymization of set-valued data
#@ Manolis Terrovitis;Nikos Mamoulis;Panos Kalnis
#t 2008
#c 4
#% 300120
#% 342643
#% 443463
#% 576761
#% 577239
#% 740764
#% 800515
#% 801690
#% 810011
#% 864406
#% 864412
#% 874892
#% 881551
#% 893100
#% 960239
#% 996348
#% 1022265
#% 1066737
#% 1206581
#! In this paper we study the problem of protecting privacy in the publication of set-valued data. Consider a collection of transactional data that contains detailed information about items bought together by individuals. Even after removing all personal characteristics of the buyer, which can serve as links to his identity, the publication of such data is still subject to privacy attacks from adversaries who have partial knowledge about the set. Unlike most previous works, we do not distinguish data as sensitive and non-sensitive, but we consider them both as potential quasi-identifiers and potential sensitive data, depending on the point of view of the adversary. We define a new version of the k-anonymity guarantee, the km-anonymity, to limit the effects of the data dimensionality and we propose efficient algorithms to transform the database. Our anonymization model relies on generalization instead of suppression, which is the most common practice in related works on such data. We develop an algorithm which finds the optimal solution, however, at a high cost which makes it inapplicable for large, realistic problems. Then, we propose two greedy heuristics, which scale much better and in most of the cases find a solution close to the optimal. The proposed algorithms are experimentally evaluated using real datasets.

#index 1127362
#* Authenticating the query results of text search engines
#@ HweeHwa Pang;Kyriakos Mouratidis
#t 2008
#c 4
#% 218978
#% 268079
#% 290830
#% 317933
#% 319994
#% 323131
#% 387427
#% 409005
#% 443396
#% 513367
#% 566391
#% 643566
#% 745532
#% 761411
#% 772846
#% 810042
#% 838424
#% 867054
#% 874980
#% 940830
#% 983653
#% 1022213
#% 1022214
#% 1025681
#% 1181949
#% 1394513
#% 1664115
#! The number of successful attacks on the Internet shows that it is very difficult to guarantee the security of online search engines. A breached server that is not detected in time may return incorrect results to the users. To prevent that, we introduce a methodology for generating an integrity proof for each search result. Our solution is targeted at search engines that perform similarity-based document retrieval, and utilize an inverted list implementation (as most search engines do). We formulate the properties that define a correct result, map the task of processing a text search query to adaptations of existing threshold-based algorithms, and devise an authentication scheme for checking the validity of a result. Finally, we confirm the efficiency and practicality of our solution through an empirical evaluation with real documents and benchmark queries.

#index 1127363
#* Structural signatures for tree data structures
#@ Ashish Kundu;Elisa Bertino
#t 2008
#c 4
#% 32907
#% 104987
#% 121458
#% 342334
#% 342345
#% 354287
#% 362036
#% 471290
#% 536328
#% 552444
#% 737758
#% 761411
#% 765448
#% 772846
#% 805795
#% 824725
#% 840653
#% 893099
#% 898157
#% 1022246
#% 1022247
#% 1389866
#% 1395167
#% 1727483
#! Data sharing with multiple parties over a third-party distribution framework requires that both data integrity and confidentiality be assured. One of the most widely used data organization structures is the tree structure. When such structures encode sensitive information (such as in XML documents), it is crucial that integrity and confidentiality be assured not only for the content, but also for the structure. Digital signature schemes are commonly used to authenticate the integrity of the data. The most widely used such technique for tree structures is the Merkle hash technique, which however is known to be "not hiding", thus leading to unauthorized leakage of information. Most techniques in the literature are based on the Merkle hash technique and thus suffer from the problem of unauthorized information leakages. Assurance of integrity and confidentiality (no leakages) of tree-structured data is an important problem in the context of secure data publishing and content distribution systems. In this paper, we propose a signature scheme for tree structures, which assures both confidentiality and integrity and is also efficient, especially in third-party distribution environments. Our integrity assurance technique, which we refer to as the "Structural signature scheme", is based on the structure of the tree as defined by tree traversals (pre-order, post-order, in-order) and is defined using a randomized notion of such traversal numbers. In addition to formally defining the technique, we prove that it protects against violations of content and structural integrity and information leakages. We also show through complexity and performance analysis that the structural signature scheme is efficient; with respect to the Merkle hash technique, it incurs comparable cost for signing the trees and incurs lower cost for user-side integrity verification.

#index 1127364
#* Maintaining dynamic channel profiles on the web
#@ Haggai Roitman;David Carmel;Elad Yom-Tov
#t 2008
#c 4
#% 46803
#% 300139
#% 344154
#% 397133
#% 397159
#% 640706
#% 643016
#% 729791
#% 731406
#% 754058
#% 754106
#% 765498
#% 805879
#% 829391
#% 871764
#% 893153
#% 960275
#% 960342
#% 963494
#% 982751
#% 989627
#% 1016159
#% 1191519
#% 1206691
#% 1206693
#% 1272286
#! This work addresses a novel problem of maintaining channel proflies on the Web. Such channel maintenance is essential for next generation of Web 2.0 applications that provide sophisticated search and discovery services over Web information channels. Maintaining a fresh channel profile is extremely difficult due to the the dynamic nature of the channel, especially under the constraint of a limited monitoring budget. We propose a novel monitoring scheme that learns the channels' monitoring rates. The monitoring scheme is further extended to consider the content that is published on the channels. We describe a novelty detection filter that refines the monitoring rate according to the expected rate of novel content published on the channels. We further show how inter-channel profile similarities can be utilized to refine the channel monitoring rates. Using real-world data of Web feeds we study the performance of the monitoring scheme. We experiment with several monitoring policies over a large set of Web feeds and show that a policy based on learning the monitoring rate of the channels, combined with novelty detection, outperforms alternative channel monitoring policies. Our results show that the suggested content-based policy is able to maintain high quality channel profiles under limited monitoring resources.

#index 1127365
#* WYSIWYG development of data driven web applications
#@ Fan Yang;Nitin Gupta;Chavdar Botev;Elizabeth F Churchill;George Levchenko;Jayavel Shanmugasundaram
#t 2008
#c 4
#% 6259
#% 55914
#% 115412
#% 309729
#% 725502
#% 864419
#% 960234
#% 960360
#% 990386
#% 1783099
#! An emerging trend in Social Networking sites and Web portals is the opening up of their APIs to external application developers. For example, the Facebook Platform, Google Gadgets and Yahoo! Widgets allow developers to design their own applications, which can then can be integrated with the platform and shared with other users. However, current APIs are targeted towards developers with programming expertise and database knowledge; they are not accessible to a large class of users who do not have a programming/database background, but would nevertheless like to create new applications. To address this need, we have developed the AppForge system, which provides a WYSIWYG application development platform. Users can graphically specify the components of webpages inside a Web browser, and the corresponding database schema and application logic will be automatically generated on the fly by the system. The WYSIWYG interface gives instantaneous feedback on what users have created and allows them to run, test and continuously refine their applications. AppForge has been used to create prototype versions of a variety of applications such as an event planning system, a recruiting system, an item trading system and an online course management system. We have also conducted a small and preliminary user study to identify and fix some of the usability aspects of AppForge.

#index 1127366
#* Web page language identification based on URLs
#@ Eda Baykan;Monika Henzinger;Ingmar Weber
#t 2008
#c 4
#% 769395
#% 810578
#% 884588
#! Given only the URL of a web page, can we identify its language? This is the question that we examine in this paper. Such a language classifier is, for example, useful for crawlers of web search engines, which frequently try to satisfy certain language quotas. To determine the language of uncrawled web pages, they have to download the page, which might be wasteful, if the page is not in the desired language. With URL-based language classifiers these redundant downloads can be avoided. We apply a variety of machine learning algorithms to the language identification task and evaluate their performance in extensive experiments for five languages: English, French, German, Spanish and Italian. Our best methods achieve an F-measure, averaged over all languages, of around .90 for both a random sample of 1,260 web page from a large web crawl and for 25k pages from the ODP directory. For 5k pages of web search engine results we even achieve an F-measure of .96. The achieved recall for these collections is .93, .88 and .95 respectively. Two independent human evaluators performed considerably worse on the task, with an F-measure of .75 and a typical recall of a mere .67. Using only country-code top-level domains, such as .de or .fr yields a good precision, but a typical recall of below .60 and an F-measure of around .68.

#index 1127367
#* Parallelizing query optimization
#@ Wook-Shin Han;Wooseong Kwak;Jinsoo Lee;Guy M. Lohman;Volker Markl
#t 2008
#c 4
#% 43161
#% 43162
#% 86949
#% 169843
#% 198068
#% 393844
#% 397471
#% 398804
#% 411554
#% 411688
#% 442864
#% 443043
#% 444474
#% 480430
#% 481937
#% 566122
#% 654454
#% 654472
#% 765456
#% 765463
#% 838409
#% 893165
#% 911426
#% 960299
#% 960301
#% 961015
#% 1063510
#% 1408766
#! Many commercial RDBMSs employ cost-based query optimization exploiting dynamic programming (DP) to efficiently generate the optimal query execution plan. However, optimization time increases rapidly for queries joining more than 10 tables. Randomized or heuristic search algorithms reduce query optimization time for large join queries by considering fewer plans, sacrificing plan optimality. Though commercial systems executing query plans in parallel have existed for over a decade, the optimization of such plans still occurs serially. While modern microprocessors employ multiple cores to accelerate computations, parallelizing query optimization to exploit multi-core parallelism is not as straightforward as it may seem. The DP used in join enumeration belongs to the challenging nonserial polyadic DP class because of its non-uniform data dependencies. In this paper, we propose a comprehensive and practical solution for parallelizing query optimization in the multi-core processor architecture, including a parallel join enumeration algorithm and several alternative ways to allocate work to threads to balance their load. We also introduce a novel data structure called skip vector array to significantly reduce the generation of join partitions that are infeasible. This solution has been prototyped in PostgreSQL. Extensive experiments using various query graph topologies confirm that our algorithms allocate the work evenly, thereby achieving almost linear speed-up. Our parallel join enumeration algorithm enhanced with our skip vector array outperforms the conventional generate-and-filter DP algorithm by up to two orders of magnitude for star queries-linear speedup due to parallelism and an order of magnitude performance improvement due to the skip vector array.

#index 1127368
#* Hashed samples: selectivity estimators for set similarity selection queries
#@ Marios Hadjieleftheriou;Xiaohui Yu;Nick Koudas;Divesh Srivastava
#t 2008
#c 4
#% 1331
#% 2833
#% 7492
#% 66937
#% 210190
#% 290850
#% 299989
#% 387427
#% 480654
#% 571046
#% 577238
#% 745489
#% 765424
#% 765425
#% 765463
#% 824684
#% 864392
#% 864395
#% 893164
#% 956458
#% 960250
#% 960263
#% 1022218
#% 1024474
#% 1206677
#! We study selectivity estimation techniques for set similarity queries. A wide variety of similarity measures for sets have been proposed in the past. In this work we concentrate on the class of weighted similarity measures (e.g., TF/IDF and BM25 cosine similarity and variants) and design selectivity estimators based on a priori constructed samples. First, we study the pitfalls associated with straightforward applications of random sampling, and argue that care needs to be taken in how the samples are constructed; uniform random sampling yields very low accuracy, while query sensitive realtime sampling is more expensive than exact solutions (both in CPU and I/O cost). We show how to build robust samples a priori, based on existing synopses for distinct value estimation. We prove the accuracy of our technique theoretically, and verify its performance experimentally. Our algorithm is orders of magnitude faster than exact solutions and has very small space overhead.

#index 1127369
#* Tighter estimation using bottom k sketches
#@ Edith Cohen;Haim Kaplan
#t 2008
#c 4
#% 243166
#% 248812
#% 281245
#% 310770
#% 397369
#% 443393
#% 576112
#% 616528
#% 749450
#% 765460
#% 809263
#% 847089
#% 866696
#% 872052
#% 873264
#% 878249
#% 942353
#% 960250
#% 967028
#% 977009
#% 989512
#% 1002032
#% 1014727
#% 1022276
#% 1815547
#! Summaries of massive data sets support approximate query processing over the original data. A basic aggregate over a set of records is the weight of subpopulations specified as a predicate over records' attributes. Bottom-k sketches are a powerful summarization format of weighted items that includes priority sampling [22], and the classic weighted sampling without replacement. They can be computed efficiently for many representations of the data including distributed databases and data streams and support coordinated and all-distances sketches. We derive novel unbiased estimators and confidence bounds for subpopulation weight. Our rank conditioning (RC) estimator is applicable when the total weight of the sketched set cannot be computed by the summarization algorithm without a significant use of additional resources (such as for sketches of network neighborhoods) and the tighter subset conditioning (SC) estimator that is applicable when the total weight is available (sketches of data streams). Our estimators are derived using clever applications of the Horvitz-Thompson estimator (that is not directly applicable to bottom-k sketches). We develop efficient computational methods and conduct performance evaluation using a range of synthetic and real data sets. We demonstrate considerable benefits of the SC estimator on larger subpopulations (over all other estimators); of the RC estimator (over existing estimators for weighted sampling without replacement); and of our confidence bounds (over all previous approaches).

#index 1127370
#* STBenchmark: towards a benchmark for mapping systems
#@ Bogdan Alexe;Wang-Chiew Tan;Yannis Velegrakis
#t 2008
#c 4
#% 96283
#% 315025
#% 342677
#% 378409
#% 397407
#% 480317
#% 489176
#% 541480
#% 572305
#% 572314
#% 745518
#% 800006
#% 800550
#% 809239
#% 810078
#% 814647
#% 824736
#% 875028
#% 881740
#% 893093
#% 893095
#% 893193
#% 895176
#% 903009
#% 960233
#% 993981
#% 994015
#% 1015303
#% 1022315
#% 1044442
#% 1127589
#% 1206614
#% 1661428
#! A fundamental problem in information integration is to precisely specify the relationships, called mappings, between schemas. Designing mappings is a time-consuming process. To alleviate this problem, many mapping systems have been developed to assist the design of mappings. However, a benchmark for comparing and evaluating these systems has not yet been developed. We present STBenchmark, a solution towards a much needed benchmark for mapping systems. We first describe the challenges that are unique to the development of benchmarks for mapping systems. After this, we describe the three components of STBenchmark: (1) a basic suite of mapping scenarios that we believe represents a minimum set of transformations that should be readily supported by any mapping system, (2) a mapping scenario generator as well as an instance generator that can produce complex mapping scenarios and, respectively, instances of varying sizes of a given schema, (3) a simple usability model that can be used as a first-cut measure on the case of use of a mapping system. We use STBenchmark to evaluate four mapping systems and report our results, as well as describe some interesting observations.

#index 1127371
#* Interactive source registration in community-oriented information integration
#@ Yannis Katsis;Alin Deutsch;Yannis Papakonstantinou
#t 2008
#c 4
#% 164364
#% 248038
#% 273687
#% 283052
#% 303884
#% 378409
#% 384978
#% 398263
#% 465053
#% 488620
#% 762652
#% 765432
#% 777935
#% 809248
#% 809261
#% 824736
#% 826032
#% 857502
#% 864417
#% 1015271
#% 1700140
#! Modern Internet communities need to integrate and query structured information. Employing current information integration infrastructure, data integration is still a very costly effort, since source registration is performed by a central authority which becomes a bottleneck. We propose the community-based integration paradigm which pushes the source registration task to the independent community members. This creates new challenges caused by each community member's lack of a global overview on how her data interacts with the application queries of the community and the data from other sources. How can the source owner maximize the visibility of her data to existing applications, while minimizing the clean-up and reformatting cost associated with publishing? Does her data contradict (or could it contradict in the future) the data of other sources? We introduce RIDE, a visual registration tool that extends schema mapping interfaces like that of MS Biz Talk Server and IBM's Clio with a suggestion component that guides the source owner in the autonomous registration, assisting her in answering these questions. RIDE's implementation features efficient procedures for deciding various levels of self-reliance of a GLAV-style source registration for contributing answers to an application query and checking potential and definite inconsistency across sources.

#index 1127372
#* Data exchange with data-metadata translations
#@ Mauricio A. Hernández;Paolo Papotti;Wang-Chiew Tan
#t 2008
#c 4
#% 248800
#% 283052
#% 378409
#% 480134
#% 481944
#% 572314
#% 809239
#% 810021
#% 810078
#% 814652
#% 824736
#% 824763
#% 826032
#% 838519
#% 875028
#% 881740
#% 893094
#% 960233
#% 960273
#% 993981
#% 1409385
#! Data exchange is the process of converting an instance of one schema into an instance of a different schema according to a given specification. Recent data exchange systems have largely dealt with the case where the schemas are given a priori and transformations can only migrate data from the first schema to an instance of the second schema. In particular, the ability to perform data-metadata translations, transformation in which data is converted into metadata or metadata is converted into data, is largely ignored. This paper provides a systematic study of the data exchange problem with data-metadata translation capabilities. We describe the problem, our solution, implementation and experiments. Our solution is a principled and systematic extension of the existing data exchange framework; all the way from the constructs required in the visual interface to specify data-metadata correspondences, which naturally extend the traditional value correspondences, to constructs required for the mapping language to specify data-metadata translations, and algorithms required for generating mappings and queries that perform the exchange.

#index 1127373
#* Out-of-order processing: a new architecture for high-performance stream systems
#@ Jin Li;Kristin Tufte;Vladislav Shkapenyuk;Vassilis Papadimos;Theodore Johnson;David Maier
#t 2008
#c 4
#% 300167
#% 480120
#% 480642
#% 578391
#% 654497
#% 726621
#% 765404
#% 783480
#% 801694
#% 803602
#% 808509
#% 810008
#% 810033
#% 824742
#% 853056
#% 859024
#% 875022
#% 878299
#% 893090
#% 1015278
#% 1015279
#% 1015296
#% 1016156
#% 1016157
#% 1016269
#% 1206600
#! Many stream-processing systems enforce an order on data streams during query evaluation to help unblock blocking operators and purge state from stateful operators. Such in-order processing (IOP) systems not only must enforce order on input streams, but also require that query operators preserve order. This order-preserving requirement constrains the implementation of stream systems and incurs significant performance penalties, particularly for memory consumption. Especially for high-performance, potentially distributed stream systems, the cost of enforcing order can be prohibitive. We introduce a new architecture for stream systems, out-of-order processing (OOP), that avoids ordering constraints. The OOP architecture frees stream systems from the burden of order maintenance by using explicit stream progress indicators, such as punctuation or heartbeats, to unblock and purge operators. We describe the implementation of OOP stream systems and discuss the benefits of this architecture in depth. For example, the OOP approach has proven useful for smoothing workload bursts caused by expensive end-of-window operations, which can overwhelm internal communication paths in IOP approaches. We have implemented OOP in two stream systems, Gigascope and NiagaraST. Our experimental study shows that the OOP approach can significantly outperform IOP in a number of aspects, including memory, throughput and latency.

#index 1127374
#* StreamTX: extracting tuples from streaming XML data
#@ Wook-Shin Han;Haifeng Jiang;Howard Ho;Quanzhong Li
#t 2008
#c 4
#% 333981
#% 397375
#% 480296
#% 598374
#% 803121
#% 810046
#% 810078
#% 814651
#% 824673
#% 864465
#% 881734
#% 893112
#% 956601
#% 1015277
#% 1015338
#% 1016148
#% 1022330
#! We study the problem of extracting flattened tuple data from streaming, hierarchical XML data. Tuple-extraction queries are essentially XML pattern queries with multiple extraction nodes. Their typical applications include mapping-based XML transformation and integrated (set-based) processing of XML and relational data. Holistic twig joins are known for the optimal matching of XML pattern queries on parsed/indexed XML data. Naïve application of the holistic twig joins to streaming XML data incurs unnecessary disk I/Os. We adapt the holistic twig joins for tuple-extraction queries on streaming XML with two novel features: first, we use the block-and-trigger technique to consume streaming XML data in a best-effort fashion without compromising the optimality of holistic matching; second, to reduce peak buffer sizes and overall running times, we apply query-path pruning and existential-match pruning techniques to aggressively filter irrelevant incoming data. We compare our solution with the direct competitor TurboXPath and other alternative approaches that use full-fledged query engines such as XQuery or XSLT engines for tuple extraction. The experiments using real-world XML data and queries demonstrated that our approach 1) outperformed its competitors by up to orders of magnitude, and 2) exhibited almost linear scalability. Our solution has been demonstrated extensively to IBM customers and will be included in customer engagement applications in healthcare.

#index 1127375
#* Sliding-window top-k queries on uncertain streams
#@ Cheqing Jin;Ke Yi;Lei Chen;Jeffrey Xu Yu;Xuemin Lin
#t 2008
#c 4
#% 214073
#% 333854
#% 379445
#% 631988
#% 654443
#% 654487
#% 864394
#% 874904
#% 875023
#% 881500
#% 893126
#% 893127
#% 894646
#% 960257
#% 977008
#% 977013
#% 989511
#% 991156
#% 997534
#% 1016201
#% 1022276
#% 1039656
#% 1061648
#% 1063520
#% 1063531
#% 1206633
#% 1206640
#% 1206645
#% 1206646
#% 1407141
#! Query processing on uncertain data streams has attracted a lot of attentions lately, due to the imprecise nature in the data generated from a variety of streaming applications, such as readings from a sensor network. However, all of the existing works on uncertain data streams study unbounded streams. This paper takes the first step towards the important and challenging problem of answering sliding-window queries on uncertain data streams, with a focus on arguably one of the most important types of queries---top-k queries. The challenge of answering sliding-window top-k queries on uncertain data streams stems from the strict space and time requirements of processing both arriving and expiring tuples in high-speed streams, combined with the difficulty of coping with the exponential blowup in the number of possible worlds induced by the uncertain data model. In this paper, we design a unified framework for processing sliding-window top-k queries on uncertain streams. We show that all the existing top-k definitions in the literature can be plugged into our framework, resulting in several succinct synopses that use space much smaller than the window size, while are also highly efficient in terms of processing time. In addition to the theoretical space and time bounds that we prove for these synopses, we also present a thorough experimental report to verify their practical efficiency on both synthetic and real data.

#index 1127376
#* Conditioning probabilistic databases
#@ Christoph Koch;Dan Olteanu
#t 2008
#c 4
#% 663
#% 3873
#% 58608
#% 94459
#% 215225
#% 265692
#% 288165
#% 314829
#% 341672
#% 386158
#% 864417
#% 893167
#% 976984
#% 992830
#% 1063719
#% 1068580
#% 1206717
#% 1206732
#% 1272349
#% 1272404
#! Past research on probabilistic databases has studied the problem of answering queries on a static database. Application scenarios of probabilistic databases however often involve the conditioning of a database using additional information in the form of new evidence. The conditioning problem is thus to transform a probabilistic database of priors into a posterior probabilistic database which is materialized for subsequent query processing or further refinement. It turns out that the conditioning problem is closely related to the problem of computing exact tuple confidence values. It is known that exact confidence computation is an NP-hard problem. This has led researchers to consider approximation techniques for confidence computation. However, neither conditioning nor exact confidence computation can be solved using such techniques. In this paper we present efficient techniques for both problems. We study several problem decomposition methods and heuristics that are based on the most successful search techniques from constraint satisfaction, such as the Davis-Putnam algorithm. We complement this with a thorough experimental evaluation of the algorithms proposed. Our experiments show that our exact algorithms scale well to realistic database sizes and can in some scenarios compete with the most efficient previous approximation algorithms.

#index 1127377
#* Efficient search for the top-k probable nearest neighbors in uncertain databases
#@ George Beskales;Mohamed A. Soliman;Ihab F. Ilyas
#t 2008
#c 4
#% 103743
#% 201876
#% 333977
#% 397378
#% 458858
#% 479649
#% 527026
#% 643566
#% 654482
#% 772835
#% 824728
#% 836161
#% 864394
#% 900156
#% 907400
#% 983263
#% 1058620
#% 1206716
#% 1408794
#% 1700123
#% 1720764
#! Uncertainty pervades many domains in our lives. Current real-life applications, e.g., location tracking using GPS devices or cell phones, multimedia feature extraction, and sensor data management, deal with different kinds of uncertainty. Finding the nearest neighbor objects to a given query point is an important query type in these applications. In this paper, we study the problem of finding objects with the highest marginal probability of being the nearest neighbors to a query object. We adopt a general uncertainty model allowing for data and query uncertainty. Under this model, we define new query semantics, and provide several efficient evaluation algorithms. We analyze the cost factors involved in query evaluation, and present novel techniques to address the trade-offs among these factors. We give multiple extensions to our techniques including handling dependencies among data objects, and answering threshold queries. We conduct an extensive experimental study to evaluate our techniques on both real and synthetic data.

#index 1127378
#* BayesStore: managing large, uncertain data repositories with probabilistic graphical models
#@ Daisy Zhe Wang;Eirinaios Michelakis;Minos Garofalakis;Joseph M. Hellerstein
#t 2008
#c 4
#% 663
#% 215225
#% 388024
#% 442830
#% 480102
#% 496116
#% 711139
#% 874976
#% 893167
#% 893168
#% 1016201
#% 1206717
#% 1279353
#% 1650403
#! Several real-world applications need to effectively manage and reason about large amounts of data that are inherently uncertain. For instance, pervasive computing applications must constantly reason about volumes of noisy sensory readings for a variety of reasons, including motion prediction and human behavior modeling. Such probabilistic data analyses require sophisticated machine-learning tools that can effectively model the complex spatio/temporal correlation patterns present in uncertain sensory data. Unfortunately, to date, most existing approaches to probabilistic database systems have relied on somewhat simplistic models of uncertainty that can be easily mapped onto existing relational architectures: Probabilistic information is typically associated with individual data tuples, with only limited or no support for effectively capturing and reasoning about complex data correlations. In this paper, we introduce BayesStore, a novel probabilistic data management architecture built on the principle of handling statistical models and probabilistic inference tools as first-class citizens of the database system. Adopting a machine-learning view, BAYESSTORE employs concise statistical relational models to effectively encode the correlation patterns between uncertain data, and promotes probabilistic inference and statistical model manipulation as part of the standard DBMS operator repertoire to support efficient and sound query processing. We present BAYESSTORE's uncertainty model based on a novel, first-order statistical model, and we redefine traditional query processing operators, to manipulate the data and the probabilistic models of the database in an efficient manner. Finally, we validate our approach, by demonstrating the value of exploiting data correlations during query processing, and by evaluating a number of optimizations which significantly accelerate query processing.

#index 1127379
#* Type inference and type checking for queries on execution traces
#@ Daniel Deutch;Tova Milo
#t 2008
#c 4
#% 71306
#% 114677
#% 214091
#% 273702
#% 288513
#% 299942
#% 299944
#% 359443
#% 538153
#% 810053
#% 814648
#% 817690
#% 848048
#% 874885
#% 893111
#% 893117
#% 976990
#% 976998
#% 994005
#% 1022252
#% 1408536
#% 1661443
#! This paper studies, for the first time, the management of type information for an important class of semi-structured data: nested DAGs (Directed Acyclic Graphs) that describe execution traces of business processes (BPs for short). Specifically, we consider here type inference and type checking for queries over BP execution traces. The queries that we consider select portions of the traces that are of interest to the user; the types describe the possible shape of the execution traces in the input/output of the query. We formally define and characterize here three common classes of BP execution traces and their respective notions of type inference and type checking. We study the complexity of the two problems for query languages of varying expressive power and present efficient type inference/checking algorithms whenever possible. Our analysis offers a nearly complete picture of which combinations of trace classes and query features lead to PTIME algorithms and which to NP-complete or undecidable problems.

#index 1127380
#* Taming verification hardness: an efficient algorithm for testing subgraph isomorphism
#@ Haichuan Shang;Ying Zhang;Xuemin Lin;Jeffrey Xu Yu
#t 2008
#c 4
#% 288990
#% 344549
#% 404719
#% 407822
#% 410276
#% 629708
#% 765429
#% 810072
#% 814196
#% 864425
#% 960305
#% 1022280
#% 1034834
#% 1044450
#! Graphs are widely used to model complicated data semantics in many applications. In this paper, we aim to develop efficient techniques to retrieve graphs, containing a given query graph, from a large set of graphs. Considering the problem of testing subgraph isomorphism is generally NP-hard, most of the existing techniques are based on the framework of filtering-and-verification to reduce the precise computation costs; consequently various novel feature-based indexes have been developed. While the existing techniques work well for small query graphs, the verification phase becomes a bottleneck when the query graph size increases. Motivated by this, in the paper we firstly propose a novel and efficient algorithm for testing subgraph isomorphism, QuickSI. Secondly, we develop a new feature-based index technique to accommodate QuickSI in the filtering phase. Our extensive experiments on real and synthetic data demonstrate the efficiency and scalability of the proposed techniques, which significantly improve the existing techniques.

#index 1127381
#* On generating near-optimal tableaux for conditional functional dependencies
#@ Lukasz Golab;Howard Karloff;Flip Korn;Divesh Srivastava;Bei Yu
#t 2008
#c 4
#% 152934
#% 180945
#% 189872
#% 210160
#% 300711
#% 408396
#% 411354
#% 479957
#% 757718
#% 765455
#% 772030
#% 787539
#% 927034
#% 989571
#% 1015310
#% 1022222
#% 1022228
#! Conditional functional dependencies (CFDs) have recently been proposed as a useful integrity constraint to summarize data semantics and identify data inconsistencies. A CFD augments a functional dependency (FD) with a pattern tableau that defines the context (i.e., the subset of tuples) in which the underlying FD holds. While many aspects of CFDs have been studied, including static analysis and detecting and repairing violations, there has not been prior work on generating pattern tableaux, which is critical to realize the full potential of CFDs. This paper is the first to formally characterize a "good" pattern tableau, based on naturally desirable properties of support, confidence and parsimony. We show that the problem of generating an optimal tableau for a given FD is NP-complete but can be approximated in polynomial time via a greedy algorithm. For large data sets, we propose an "on-demand" algorithm providing the same approximation bound, that outperforms the basic greedy algorithm in running time by an order of magnitude. For ordered attributes, we propose the range tableau as a generalization of a pattern tableau, which can achieve even more parsimony. The effectiveness and efficiency of our techniques are experimentally demonstrated on real data.

#index 1127382
#* Propagating functional dependencies with conditions
#@ Wenfei Fan;Shuai Ma;Yanli Hu;Jie Liu;Yinghui Wu
#t 2008
#c 4
#% 23878
#% 213972
#% 224743
#% 286998
#% 287793
#% 289266
#% 289369
#% 289384
#% 296539
#% 374001
#% 378409
#% 384872
#% 384978
#% 408396
#% 491358
#% 783471
#% 809239
#% 826035
#% 1022222
#% 1054480
#% 1703740
#! The dependency propagation problem is to determine, given a view defined on data sources and a set of dependencies on the sources, whether another dependency is guaranteed to hold on the view. This paper investigates dependency propagation for recently proposed conditional functional dependencies (CFDs). The need for this study is evident in data integration, exchange and cleaning since dependencies on data sources often only hold conditionally on the view. We investigate dependency propagation for views defined in various fragments of relational algebra, CFDs as view dependencies, and for source dependencies given as either CFDs or traditional functional dependencies (FDs). (a) We establish lower and upper bounds, all matching, ranging from PTIME to undecidable. These not only provide the first results for CFD propagation, but also extend the classical work of FD propagation by giving new complexity bounds in the presence of finite domains. (b) We provide the first algorithm for computing a minimal cover of all CFDs propagated via SPC views; the algorithm has the same complexity as one of the most efficient algorithms for computing a cover of FDs propagated via a projection view, despite the increased expressive power of CFDs and SPC views. (c) We experimentally verify that the algorithm is efficient.

#index 1127383
#* Simrank++: query rewriting through link analysis of the click graph
#@ Ioannis Antonellis;Hector Garcia Molina;Chi Chao Chang
#t 2008
#c 4
#% 125556
#% 248027
#% 310567
#% 342961
#% 577273
#% 643001
#% 643057
#% 783475
#% 869501
#% 898311
#% 956546
#% 963669
#% 987222
#% 987361
#! We focus on the problem of query rewriting for sponsored search. We base rewrites on a historical click graph that records the ads that have been clicked on in response to past user queries. Given a query q, we first consider Simrank [7] as a way to identify queries similar to q, i.e., queries whose ads a user may be interested in. We argue that Simrank fails to properly identify query similarities in our application, and we present two enhanced versions of Simrank: one that exploits weights on click graph edges and another that exploits "evidence." We experimentally evaluate our new schemes against Simrank, using actual click graphs and queries from Yahoo!, and using a variety of metrics. Our results show that the enhanced methods can yield more and better query rewrites.

#index 1127384
#* Accuracy estimate and optimization techniques for SimRank computation
#@ Dmitry Lizorkin;Pavel Velikhov;Maxim Grinev;Denis Turdakov
#t 2008
#c 4
#% 268079
#% 447948
#% 465914
#% 577273
#% 581661
#% 805904
#% 818218
#% 931290
#% 961582
#% 994015
#% 1016175
#% 1250381
#% 1275012
#! The measure of similarity between objects is a very useful tool in many areas of computer science, including information retrieval. SimRank is a simple and intuitive measure of this kind, based on graph-theoretic model. SimRank is typically computed iteratively, in the spirit of PageRank. However, existing work on SimRank lacks accuracy estimation of iterative computation and has discouraging time complexity. In this paper we present a technique to estimate the accuracy of computing SimRank iteratively. This technique provides a way to find out the number of iterations required to achieve a desired accuracy when computing SimRank. We also present optimization techniques that improve the computational complexity of the iterative algorithm from O(n4) to O(n3) in the worst case. We also introduce a threshold sieving heuristic and its accuracy estimation that further improves the efficiency of the method. As a practical illustration of our techniques we computed SimRank scores on a subset of English Wikipedia corpus, consisting of the complete set of articles and category links.

#index 1127385
#* End-to-end support for joins in large-scale publish/subscribe systems
#@ Badrish Chandramouli;Jun Yang
#t 2008
#c 4
#% 279164
#% 286916
#% 300179
#% 302816
#% 338354
#% 340176
#% 340301
#% 342032
#% 348071
#% 397353
#% 443298
#% 516235
#% 556654
#% 565473
#% 612477
#% 726622
#% 736390
#% 772022
#% 793899
#% 800517
#% 800565
#% 821923
#% 864430
#% 875019
#% 884509
#% 893091
#% 960297
#% 1015281
#% 1016180
#% 1022275
#% 1063586
#% 1849768
#! We address the problem of supporting a large number of select-join subscriptions for wide-area publish/subscribe. Subscriptions are joins over different tables, with varying interests expressed as range selection conditions over table attributes. Naive schemes, such as computing and sending join results from a server, are inefficient because they produce redundant data, and are unable to share dissemination costs across subscribers and events. We propose a novel, scalable scheme that group-processes and disseminates a general mix of multi-way select-join subscriptions. We also propose a simple and application-agnostic extension to content-driven networks (CN), which further improves sharing of dissemination costs. Experimental evaluations show that our schemes can generate orders of magnitude lower network traffic at very low processing cost. Our extension to CN can further reduce traffic by another order of magnitude, with almost no increase in notification latency.

#index 1127386
#* Scalable ranked publish/subscribe
#@ Ashwin Machanavajjhala;Erik Vee;Minos Garofalakis;Jayavel Shanmugasundaram
#t 2008
#c 4
#% 2115
#% 56637
#% 70370
#% 201876
#% 300170
#% 333938
#% 342372
#% 427199
#% 458873
#% 480306
#% 571296
#% 589264
#% 643566
#% 646220
#% 661478
#% 731408
#% 745469
#% 810018
#% 956549
#% 978069
#% 1016180
#% 1058620
#% 1739418
#! Publish/subscribe (pub/sub) systems are designed to efficiently match incoming events (e.g., stock quotes) against a set of subscriptions (e.g., trader profiles specifying quotes of interest). However, current pub/sub systems only support a simple binary notion of matching: an event either matches a subscription or it does not; for instance, a stock quote will either match or not match a trader profile. In this paper, we argue that this simple notion of matching is inadequate for many applications where only the "best" matching subscriptions are of interest. For instance, in targeted Web advertising, an incoming user ("event") may match several different advertiser-specified user profiles ("subscriptions"), but given the limited advertising real-estate, we want to quickly discover the best (e.g., most relevant) ads to display. To address this need, we initiate a study of ranked pub/sub systems. We focus on the case where subscriptions correspond to interval ranges (e.g, age in [25,35] and salary $50, 000), and events are points that match all the intervals that they stab (e.g., age=28, salary = $65,000). In addition, each interval has a score and our goal is to quickly recover the top-scoring matching subscriptions. Unfortunately, adapting existing index structures to solve this problem results in either an unacceptable space overhead or a significant performance degradation. We thus propose two novel index structures that are both compact and efficient. Our experimental evaluation shows that the proposed structures provide a scalable basis for designing ranked pub/sub systems.

#index 1127387
#* Dependable cardinality forecasts for XQuery
#@ Jens Teubner;Torsten Grust;Sebastian Maneth;Sherif Sakr
#t 2008
#c 4
#% 411554
#% 479465
#% 480488
#% 824756
#% 864441
#% 875010
#% 881734
#% 894441
#% 994015
#% 1015272
#% 1016150
#! Though inevitable for effective cost-based query rewriting, the derivation of meaningful cardinality estimates has remained a notoriously hard problem in the context of XQuery. By basing the estimation on a relational representation of the XQuery syntax, we show how existing cardinality estimation techniques for XPath and proven relational estimation machinery can play together to yield dependable forecasts for arbitrary XQuery (sub)expressions. Our approach benefits from a light-weight form of data flow analysis. Abstract domain identifiers guide our query analyzer through the estimation process and allow for informed decisions even in case of deeply nested XQuery expressions. A variant of projection paths [15] provides a versatile interface into which existing techniques for XPath cardinality estimation can be plugged in seamlessly. We demonstrate an implementation of this interface based on data guides. Experiments show how our approach can equally cope with both, structure-and value-based queries. It is robust with respect to intermediate estimation errors, from which we typically found our implementation to recover gracefully.

#index 1127388
#* Hash-base subgraph query processing method for graph-structured XML documents
#@ Hongzhi Wang;Jianzhong Li;Jizhou Luo;Hong Gao
#t 2008
#c 4
#% 58365
#% 333981
#% 397358
#% 397360
#% 397375
#% 480489
#% 577358
#% 654450
#% 745461
#% 745463
#% 772025
#% 824692
#% 838518
#% 864462
#% 993953
#% 993970
#% 994015
#% 1015277
#% 1656305
#% 1688299
#! When XML documents are modeled as graphs, many research issues arise. In particular, there are many new challenges in query processing on graph-structured XML documents because traditional query processing techniques for tree-structured XML documents cannot be directly applied. This paper studies the problem of structural queries on graph-structured XML documents. A hash-based structural join algorithm, HGJoin, is first proposed to handle reachability queries on graph-structured XML documents. Then, it is extended to the algorithms to process structural queries in form of bipartite graphs. Finally, based on these algorithms, a strategy to process subgraph queries in form of general DAGs is proposed. Analysis and experiments show that all the algorithms have high performance. It is notable that all the algorithms above can be slightly modified to process structural queries in form of general graphs.

#index 1127389
#* Generating XML structure using examples and constraints
#@ Sara Cohen
#t 2008
#c 4
#% 8387
#% 292401
#% 397407
#% 435112
#% 598376
#% 745518
#% 809236
#% 824744
#% 861833
#% 875044
#% 893212
#% 957534
#% 994015
#% 1733565
#! This paper presents a framework for automatically generating structural XML documents. The user provides a target DTD and an example of an XML document, called a Generate-XML-By-Example Document, or a GxBE document, for short. GxBE documents use a natural declarative syntax, which includes XPath expressions and the function count. Using GxBE documents, users can express important global and local characteristics for the desired target documents, and can require satisfaction of XPath expressions from a given workload. This paper explores the problem of efficiently generating a document that satisfies a given DTD and GxBE document.

#index 1127390
#* Read-optimized databases, in depth
#@ Allison L. Holloway;David J. DeWitt
#t 2008
#c 4
#% 3771
#% 238413
#% 286258
#% 480821
#% 712361
#% 824697
#% 864446
#% 875026
#% 893129
#% 893159
#% 960266
#% 1015289
#% 1063542
#! Recently, a number of papers have been published showing the benefits of column stores over row stores. However, the research comparing the two in an "apples-to-apples" way has left a number of unresolved questions. In this paper, we first discuss the factors that can affect the relative performance of each paradigm. Then, we choose points within each of the factors to study further. Our study examines five tables with various characteristics and different query workloads in order to obtain a greater understanding and quantification of the relative performance of column stores and row stores. We then add materialized views to the analysis and see how much they can help the performance of row stores. Finally, we examine the performance of hash join operations in column stores and row stores.

#index 1127391
#* Flashing up the storage layer
#@ Ioannis Koltsidas;Stratis D. Viglas
#t 2008
#c 4
#% 131240
#% 136740
#% 319473
#% 951778
#% 957221
#% 960238
#% 985754
#% 985755
#% 1052068
#% 1670337
#% 1820346
#! In the near future, commodity hardware is expected to incorporate both flash and magnetic disks. In this paper we study how the storage layer of a database system can benefit from the presence of both kinds of disk. We propose using the flash and the magnetic disk at the same level of the memory hierarchy and placing a data page to only one of these disks according to the workload of the page. Pages with a read-intensive workload are placed on the flash disk, while pages with a write-intensive workload are placed on the magnetic disk. We present a family of on-line algorithms to decide the optimal placement of a page and study their theoretical properties. Our system is self-tuning, i.e., our algorithms adapt page placement to changing workloads. We also present a buffer replacement policy that takes advantage of the asymmetric I/O properties of the two types of storage media to reduce the total I/O cost. Our experimental evaluation shows remarkable I/O performance improvement over both flash-only and magnetic-only systems. These results, we believe, exhibit both the potential and necessity of such algorithms in future database systems.

#index 1127392
#* Rose: compressed, log-structured replication
#@ Russell Sears;Mark Callaghan;Eric Brewer
#t 2008
#c 4
#% 208047
#% 317987
#% 322412
#% 479483
#% 480821
#% 810035
#% 857498
#% 864446
#% 875026
#% 893159
#% 960266
#% 978392
#% 992831
#! Rose is a database storage engine for high-throughput replication. It targets seek-limited, write-intensive transaction processing workloads that perform near real-time decision support and analytical processing queries. Rose uses log structured merge (LSM) trees to create full database replicas using purely sequential I/O, allowing it to provide orders of magnitude more write throughput than B-tree based replicas. Also, LSM-trees cannot become fragmented and provide fast, predictable index scans. Rose's write performance relies on replicas' ability to perform writes without looking up old values. LSM-tree lookups have performance comparable to B-tree lookups. If Rose read each value that it updated then its write throughput would also be comparable to a B-tree. Although we target replication, Rose provides high write throughput to any application that updates tuples without reading existing data, such as append-only, streaming and versioning databases. We introduce a page compression format that takes advantage of LSM-tree's sequential, sorted data layout. It increases replication throughput by reducing sequential I/O, and enables efficient tree lookups by supporting small page sizes and doubling as an index of the values it stores. Any scheme that can compress data in a single pass and provide random access to compressed values could be used by Rose. Replication environments have multiple readers but only one writer. This allows Rose to provide atomicity, consistency and isolation to concurrent transactions without resorting to rollback, blocking index requests or interfering with maintenance tasks. Rose avoids random I/O during replication and scans, leaving more I/O capacity for queries than existing systems, and providing scalable, real-time replication of seek-bound workloads. Analytical models and experiments show that Rose provides orders of magnitude greater replication bandwidth over larger databases than conventional techniques.

#index 1127393
#* WebTables: exploring the power of tables on the web
#@ Michael J. Cafarella;Alon Halevy;Daisy Zhe Wang;Eugene Wu;Yang Zhang
#t 2008
#c 4
#% 247593
#% 279755
#% 333990
#% 334031
#% 342630
#% 348147
#% 397418
#% 480645
#% 572314
#% 658628
#% 748499
#% 754068
#% 755816
#% 765433
#% 765491
#% 800497
#% 864416
#% 926881
#% 956500
#% 960360
#% 993987
#! The World-Wide Web consists of a huge number of unstructured documents, but it also contains structured data in the form of HTML tables. We extracted 14.1 billion HTML tables from Google's general-purpose web crawl, and used statistical classification techniques to find the estimated 154M that contain high-quality relational data. Because each relational table has its own "schema" of labeled and typed columns, each such table can be considered a small structured database. The resulting corpus of databases is larger than any other corpus we are aware of, by at least five orders of magnitude. We describe the WEBTABLES system to explore two fundamental questions about this collection of databases. First, what are effective techniques for searching for structured data at search-engine scales? Second, what additional power can be derived by analyzing such a huge corpus? First, we develop new techniques for keyword search over a corpus of tables, and show that they can achieve substantially higher relevance than solutions based on a traditional search engine. Second, we introduce a new object derived from the database corpus: the attribute correlation statistics database (AcsDB) that records corpus-wide statistics on co-occurrences of schema elements. In addition to improving search relevance, the AcsDB makes possible several novel applications: schema auto-complete, which helps a database designer to choose schema elements; attribute synonym finding, which automatically computes attribute synonym pairs for schema matching; and join-graph traversal, which allows a user to navigate between extracted schemas using automatically-generated join links.

#index 1127394
#* Scalable query result caching for web applications
#@ Charles Garrod;Amit Manjhi;Anastasia Ailamaki;Bruce Maggs;Todd Mowry;Christopher Olston;Anthony Tomasic
#t 2008
#c 4
#% 397402
#% 401983
#% 481128
#% 505869
#% 789005
#% 805474
#% 805842
#% 808600
#% 874990
#% 875019
#% 956524
#% 1015314
#% 1015362
#% 1849768
#! The backend database system is often the performance bottleneck when running web applications. A common approach to scale the database component is query result caching, but it faces the challenge of maintaining a high cache hit rate while efficiently ensuring cache consistency as the database is updated. In this paper we introduce Ferdinand, the first proxy-based cooperative query result cache with fully distributed consistency management. To maintain a high cache hit rate, Ferdinand uses both a local query result cache on each proxy server and a distributed cache. Consistency management is implemented with a highly scalable publish/subscribe system. We implement a fully functioning Ferdinand prototype and evaluate its performance compared to several alternative query-caching approaches, showing that our high cache hit rate and consistency management are both critical for Ferdinand's performance gains over existing systems.

#index 1127395
#* Optimization of multi-domain queries on the web
#@ Daniele Braga;Stefano Ceri;Florian Daniel;Davide Martinenghi
#t 2008
#c 4
#% 83933
#% 86949
#% 198466
#% 210207
#% 273912
#% 299968
#% 342359
#% 442700
#% 765434
#% 874895
#% 893118
#% 941785
#% 943616
#% 1022328
#% 1090727
#% 1206616
#% 1207210
#! Where can I attend an interesting database workshop close to a sunny beach? Who are the strongest experts on service computing based upon their recent publication record and accepted European projects? Can I spend an April weekend in a city served by a low-cost direct flight from Milano offering a Mahler's symphony? We regard the above queries as multi-domain queries, i.e., queries that can be answered by combining knowledge from two or more domains (such as: seaside locations, flights, publications, accepted projects, conference offerings, and so on). This information is available on the Web, but no general-purpose software system can accept the above queries nor compute the answer. At the most, dedicated systems support specific multi-domain compositions (e.g., Google-local locates information such as restaurants and hotels upon geographic maps). This paper presents an overall framework for multi-domain queries on the Web. We address the following problems: (a) expressing multi-domain queries with an abstract formalism, (b) separating the treatment of "search" services within the model, by highlighting their differences from "exact" Web services, (c) explaining how the same query can be mapped to multiple "query plans", i.e., a well-defined scheduling of service invocations, possibly in parallel, which complies with their access limitations and preserves the ranking order in which search services return results; (d) introducing cross-domain joins as first-class operation within plans; (e) evaluating the query plans against several cost metrics so as to choose the most promising one for execution. This framework adapts to a variety of application contexts, ranging from end-user-oriented mash-up scenarios up to complex application integration scenarios.

#index 1127396
#* Fault-tolerant stream processing using a distributed, replicated file system
#@ YongChul Kwon;Magdalena Balazinska;Albert Greenberg
#t 2008
#c 4
#% 29590
#% 86476
#% 111345
#% 111351
#% 114582
#% 122671
#% 148334
#% 172939
#% 236658
#% 280135
#% 399766
#% 435160
#% 442832
#% 442834
#% 444425
#% 465019
#% 479769
#% 568580
#% 581183
#% 632965
#% 654497
#% 723279
#% 726621
#% 765470
#% 800583
#% 810008
#% 875006
#% 978510
#% 993508
#% 998845
#% 1390305
#% 1734653
#! We present SGuard, a new fault-tolerance technique for distributed stream processing engines (SPEs) running in clusters of commodity servers. SGuard is less disruptive to normal stream processing and leaves more resources available for normal stream processing than previous proposals. Like several previous schemes, SGuard is based on rollback recovery [18]: it checkpoints the state of stream processing nodes periodically and restarts failed nodes from their most recent checkpoints. In contrast to previous proposals, however, SGuard performs checkpoints asynchronously: i.e., operators continue processing streams during the checkpoint thus reducing the potential disruption due to the checkpointing activity. Additionally, SGuard saves the checkpointed state into a new type of distributed and replicated file system (DFS) such as GFS [22] or HDFS [9], leaving more memory resources available for normal stream processing. To manage resource contention due to simultaneous checkpoints by different SPE nodes, SGuard adds a scheduler to the DFS. This scheduler coordinates large batches of write requests in a manner that reduces individual checkpoint times while maintaining good overall resource utilization. We demonstrate the effectiveness of the approach through measurements of a prototype implementation in the Borealis [2] open-source SPE using HDFS [9] as the DFS.

#index 1127397
#* LeeWave: level-wise distribution of wavelet coefficients for processing kNN queries over distributed streams
#@ Mi-Yen Yeh;Kun-Lung Wu;Philip S. Yu;Ming-Syan Chen
#t 2008
#c 4
#% 248822
#% 260020
#% 329453
#% 413606
#% 578390
#% 578400
#% 625767
#% 654443
#% 654488
#% 729943
#% 757960
#% 788216
#% 800582
#% 800900
#% 810009
#% 810031
#% 823333
#% 824686
#% 874995
#% 907519
#% 993961
#% 1016196
#% 1022302
#% 1688247
#! We present LeeWave --- a bandwidth-efficient approach to searching range-specified k-nearest neighbors among distributed streams by LEvEl-wise distribution of WAVElet coefficients. To find the k most similar streams to a range-specified reference one, the relevant wavelet coefficients of the reference stream can be sent to the peer sites to compute the similarities. However, bandwidth can be unnecessarily wasted if the entire relevant coefficients are sent simultaneously. Instead, we present a level-wise approach by leveraging the multi-resolution property of the wavelet coefficients. Starting from the top and moving down one level at a time, the query initiator sends only the single-level coefficients to a progressively shrinking set of candidates. However, there is one difficult challenge in LeeWave: how does the query initiator prune the candidates without knowing all the relevant coefficients? To overcome this challenge, we derive and maintain a similarity range for each candidate and gradually tighten the bounds of this range as we move from one level to the next. The increasingly tightened similarity ranges enable the query initiator to effectively prune the candidates without causing any false dismissal. Extensive experiments with real and synthetic data show that, when compared with prior approaches, LeeWave uses significantly less bandwidth under a wide range of conditions.

#index 1127398
#* A practical scalable distributed B-tree
#@ Marcos K. Aguilera;Wojciech Golab;Mehul A. Shah
#t 2008
#c 4
#% 148195
#% 202995
#% 213080
#% 286836
#% 286929
#% 291855
#% 340175
#% 340176
#% 470844
#% 480589
#% 481296
#% 612643
#% 653821
#% 963656
#% 963667
#% 998842
#% 998845
#% 1002142
#% 1063527
#! Internet applications increasingly rely on scalable data structures that must support high throughput and store huge amounts of data. These data structures can be hard to implement efficiently. Recent proposals have overcome this problem by giving up on generality and implementing specialized interfaces and functionality (e.g., Dynamo [4]). We present the design of a more general and flexible solution: a fault-tolerant and scalable distributed B-tree. In addition to the usual B-tree operations, our B-tree provides some important practical features: transactions for atomically executing several operations in one or more B-trees, online migration of B-tree nodes between servers for load-balancing, and dynamic addition and removal of servers for supporting incremental growth of the system. Our design is conceptually simple. Rather than using complex concurrency and locking protocols, we use distributed transactions to make changes to B-tree nodes. We show how to extend the B-tree and keep additional information so that these transactions execute quickly and efficiently. Our design relies on an underlying distributed data sharing service, Sinfonia [1], which provides fault tolerance and a light-weight distributed atomic primitive. We use this primitive to commit our transactions. We implemented our B-tree and show that it performs comparably to an existing open-source B-tree and that it scales to hundreds of machines. We believe that our approach is general and can be used to implement other distributed data structures easily.

#index 1127399
#* Main-memory scan sharing for multi-core CPUs
#@ Lin Qiao;Vijayshankar Raman;Frederick Reiss;Peter J. Haas;Guy M. Lohman
#t 2008
#c 4
#% 286991
#% 300179
#% 635133
#% 779573
#% 810039
#% 875026
#% 893129
#% 893159
#% 961012
#% 978720
#% 984965
#% 1016210
#% 1022230
#% 1022231
#% 1022262
#% 1022312
#% 1206624
#% 1920314
#! Computer architectures are increasingly based on multi-core CPUs and large memories. Memory bandwidth, which has riot kept pace with the increasing number of cores, has become the primary processing bottleneck, replacing disk I/O as the limiting factor. To address this challenge, we provide novel algorithms for increasing the throughput of Business Intelligence (BI) queries, as well as for ensuring fairness and avoiding starvation among a concurrent set of such queries. To maximize throughput, we propose a novel FullSharing scheme that allows all concurrent queries, when performing base-table I/O, to share the cache belonging to a given core. We then generalize this approach to a BatchSharing scheme that avoids thrashing on "agg-tables" ---hash tables that are used for aggregation processing---caused by execution of too many queries on a core. This scheme partitions queries into batches such that the working-set of agg-table entries for each batch can fit into a cache; an efficient sampling technique is used to estimate selectivities and working-set sizes for purposes of query partitioning. Finally, we use lottery-scheduling techniques to ensure fairness and impose a hard upper bound on staging time to avoid starvation. On our 8-core testbed, we were able to completely remove the memory I/O bottleneck, increasing throughput by a factor of 2 to 2.5, while also maintaining fairness and avoiding starvation.

#index 1127400
#* Row-wise parallel predicate evaluation
#@ Ryan Johnson;Vijayshankar Raman;Richard Sidle;Garret Swart
#t 2008
#c 4
#% 397361
#% 451767
#% 465169
#% 479821
#% 864446
#% 875026
#% 893159
#% 929683
#% 1015332
#% 1016235
#% 1206624
#% 1920314
#! Table scans have become more interesting recently due to greater use of ad-hoc queries and greater availability of multi-core, vector-enabled hardware. Table scan performance is limited by value representation, table layout, and processing techniques. In this paper we propose a new layout and processing technique for efficient one-pass predicate evaluation. Starting with a set of rows with a fixed number of bits per column, we append columns to form a set of banks and then pad each bank to a supported machine word length, typically 16, 32, or 64 bits. We then evaluate partial predicates on the columns of each bank, using a novel evaluation strategy that evaluates column level equality, range tests, IN-list predicates, and conjuncts of these predicates, simultaneously on multiple columns within a bank, and on multiple rows within a machine register. This approach outperforms pure column stores, which must evaluate the partial predicates one column at a time. We evaluate and compare the performance and representation overhead of this new approach and several proposed alternatives.

#index 1127401
#* Dynamic partitioning of the cache hierarchy in shared data centers
#@ Gokul Soundararajan;Jin Chen;Mohamed A. Sharaf;Cristiana Amza
#t 2008
#c 4
#% 51660
#% 152943
#% 202140
#% 210198
#% 267188
#% 342364
#% 348037
#% 459939
#% 481127
#% 481450
#% 521989
#% 723288
#% 778266
#% 808514
#% 830701
#% 893178
#% 896756
#% 960265
#% 963442
#% 963862
#% 966997
#% 978980
#% 979000
#% 981662
#% 983483
#% 993385
#% 1015324
#% 1085317
#% 1390334
#! Due to the imperative need to reduce the management costs of large data centers, operators multiplex several concurrent database applications on a server farm connected to shared network attached storage. Determining and enforcing per-application resource quotas in the resulting cache hierarchy, on the fly, poses a complex resource allocation problem spanning the database server and the storage server tiers. This problem is further complicated by the need to provide strict Quality of Service (QoS) guarantees to hosted applications. In this paper, we design and implement a novel coordinated partitioning technique of the database buffer pool and storage cache between applications for any given cache replacement policy and per-application access pattern. We use statistical regression to dynamically determine the mapping between cache quota settings and the resulting per-application QoS. A resource controller embedded within the database engine actuates the partitioning of the two-level cache, converging towards the configuration with maximum application utility, expressed as the service provider revenue in that configuration, based on a set of latency sample points. Our experimental evaluation, using the MySQL database engine, a server farm with consolidated storage, and two e-commerce benchmarks, shows the effectiveness of our technique in enforcing application QoS, as well as maximizing the revenue of the service provider in shared server farms.

#index 1127402
#* RDF-3X: a RISC-style engine for RDF
#@ Thomas Neumann;Gerhard Weikum
#t 2008
#c 4
#% 136740
#% 210169
#% 287461
#% 322412
#% 397151
#% 411554
#% 480153
#% 481429
#% 481619
#% 481621
#% 519567
#% 651000
#% 745514
#% 824755
#% 853532
#% 867054
#% 874876
#% 893165
#% 946525
#% 956564
#% 956574
#% 956664
#% 960299
#% 960302
#% 1022236
#% 1055731
#% 1269903
#% 1393168
#% 1409918
#% 1409954
#% 1413091
#% 1655429
#! RDF is a data representation format for schema-free structured information that is gaining momentum in the context of Semantic-Web corpora, life sciences, and also Web 2.0 platforms. The "pay-as-you-go" nature of RDF and the flexible pattern-matching capabilities of its query language SPARQL entail efficiency and scalability challenges for complex queries including long join paths. This paper presents the RDF-3X engine, an implementation of SPARQL that achieves excellent performance by pursuing a RISC-style architecture with a streamlined architecture and carefully designed, puristic data structures and operations. The salient points of RDF-3X are: 1) a generic solution for storing and indexing RDF triples that completely eliminates the need for physical-design tuning, 2) a powerful yet simple query processor that leverages fast merge joins to the largest possible extent, and 3) a query optimizer for choosing optimal join orders using a cost model based on statistical synopses for entire join paths. The performance of RDF-3X, in comparison to the previously best state-of-the-art systems, has been measured on several large-scale datasets with more than 50 million RDF triples and benchmark queries that include pattern matching and long join paths in the underlying data graphs.

#index 1127403
#* Multidimensional content eXploration
#@ Alkis Simitsis;Akanksha Baid;Yannis Sismanis;Berthold Reinwald
#t 2008
#c 4
#% 223781
#% 248845
#% 268079
#% 280819
#% 310516
#% 378064
#% 385321
#% 464215
#% 503400
#% 503731
#% 503875
#% 630866
#% 810050
#% 833135
#% 875959
#% 931293
#% 960250
#% 960285
#% 993960
#% 1016176
#% 1022235
#% 1022338
#% 1127586
#! Content Management Systems (CMS) store enterprise data such as insurance claims, insurance policies, legal documents, patent applications, or archival data like in the case of digital libraries. Search over content allows for information retrieval, but does not provide users with great insight into the data. A more analytical view is needed through analysis, aggregations, groupings, trends, pivot tables or charts, and so on. Multidimensional Content eXploration (MCX) is about effectively analyzing and exploring large amounts of content by combining keyword search with OLAP-style aggregation, navigation, and reporting. We focus on unstructured data or generally speaking documents or content with limited metadata, as it is typically encountered in CMS. We formally present how CMS content and metadata should be organized in a well-defined multidimensional structure, so that sophisticated queries can be expressed and evaluated. The CMS metadata provide traditional OLAP static dimensions that are combined with dynamic dimensions discovered from the analyzed keyword search result, as well as measures for document scores based on the link structure between the documents. In addition, we provide means for multidimensional content exploration through traditional OLAP rollupdrilldown operations on the static and dynamic dimensions, solutions for multi-cube analysis and dynamic navigation of the content. We present our prototype, called DBPubs, which stores research publications as documents that can be searched and -most importantly-- analyzed, and explored. Finally, we present experimental results of the efficiency and effectiveness of our approach.

#index 1127404
#* Relaxation in text search using taxonomies
#@ Marcus Fontoura;Vanja Josifovski;Ravi Kumar;Christopher Olston;Andrew Tomkins;Sergei Vassilvitskii
#t 2008
#c 4
#% 160213
#% 198335
#% 249183
#% 268079
#% 309726
#% 330706
#% 387427
#% 387508
#% 420053
#% 452641
#% 461921
#% 480123
#% 643566
#% 655485
#% 659990
#% 730065
#% 763708
#% 765414
#% 770307
#% 770327
#% 808427
#% 809250
#% 810052
#% 824693
#% 824732
#% 824733
#% 838542
#% 874993
#% 875061
#% 960279
#% 1015265
#% 1015325
#% 1016222
#! In this paper we propose a novel document retrieval model in which text queries are augmented with multi-dimensional taxonomy restrictions. These restrictions may be relaxed at a cost to result quality. This new model may be applicable in many arenas, including multifaceted, product, and local search, where documents are augmented with hierarchical metadata such as topic or location. We present efficient algorithms for indexing and query processing in this new retrieval model. We decompose query processing into two sub-problems: first, an online search problem to determine the correct overall level of relaxation cost that must be incurred to generate the top k results; and second, a budgeted relaxation search problem in which all results at a particular relaxation cost must be produced at minimal cost. We show the latter problem is solvable exactly in two hierarchical dimensions, is NP-hard in three or more dimensions, but admits efficient approximation algorithms with provable guarantees. We present experimental results evaluating our algorithms on both synthetic and real data, showing order of magnitude improvements over the baseline algorithm.

#index 1127405
#* Learning to extract form labels
#@ Hoa Nguyen;Thanh Nguyen;Juliana Freire
#t 2008
#c 4
#% 17185
#% 268079
#% 376266
#% 480479
#% 654459
#% 765409
#% 765410
#% 769471
#% 783472
#% 844413
#% 864431
#% 864434
#% 956537
#% 956538
#% 1122829
#! In this paper we describe a new approach to extract element labels from Web form interfaces. Having these labels is a requirement for several techniques that attempt to retrieve and integrate information that is hidden behind form interfaces, such as hidden Web crawlers and metasearchers. However, given the wide variation in form layout, even within a well-defined domain, automatically extracting these labels is a challenging problem. Whereas previous approaches to this problem have relied on heuristics and manually specified extraction rules, our technique makes use of a learning classifier ensemble to identify element-label mappings; and it applies a reconciliation step which leverages the classifier-derived mappings to boost extraction accuracy. We present a detailed experimental evaluation using over three thousand Web forms. Our results show that our approach is effective: it obtains significantly higher accuracy and is more robust to variability in form layout than previous label extraction techniques.

#index 1127406
#* Automated creation of a forms-based database query interface
#@ Magesh Jayapandian;H. V. Jagadish
#t 2008
#c 4
#% 212735
#% 232476
#% 345710
#% 397365
#% 523480
#% 570875
#% 650962
#% 725449
#% 725453
#% 725502
#% 808601
#% 814647
#% 884422
#% 893115
#% 922667
#% 990386
#% 1133562
#! Forms-based query interfaces are widely used to access databases today. The design of a forms-based interface is often a key step in the deployment of a database. Each form in such an interface is capable of expressing only a very limited range of queries. Ideally, the set of forms as a whole must be able to express all possible queries that any user may have. Creating an interface that approaches this ideal is surprisingly hard. In this paper, we seek to maximize the ability of a forms-based interface to support queries a user may ask, while bounding both the number of forms and the complexity of any one form. Given a database schema and content we present an automated technique to generate a good set of forms that meet the above desiderata. While a careful analysis of real or expected query workloads are useful in designing the interface, these query sets are often unavailable or hard to obtain prior to the database even being deployed. Hence generating a good set of forms just using the database itself is a challenging yet important problem. Our experimental analysis shows that our techniques can create a reasonable set of forms, one that can express 60--90% of user queries, without any input from the database administrator. Human experts, without support from software such as ours, are often unable to support as high a fraction of user queries.

#index 1127407
#* Efficient network aware search in collaborative tagging sites
#@ Sihem Amer Yahia;Michael Benedikt;Laks V. S. Lakshmanan;Julia Stoyanovich
#t 2008
#c 4
#% 215225
#% 227894
#% 387427
#% 397608
#% 408396
#% 411762
#% 643566
#% 728195
#% 800508
#% 824704
#% 875004
#% 975041
#% 994013
#% 1013696
#% 1055739
#% 1055743
#! The popularity of collaborative tagging sites presents a unique opportunity to explore keyword search in a context where query results are determined by the opinion of a network of taggers related to a seeker. In this paper, we present the first in-depth study of network-aware search. We investigate efficient top-k processing when the score of an answer is computed as its popularity among members of a seeker's network. We argue that obvious adaptations of top-k algorithms are too space-intensive, due to the dependence of scores on the seeker's network. We therefore develop algorithms based on maintaining score upper-bounds. The global upper-bound approach maintains a single score upper-bound for every pair of item and tag, over the entire collection of users. The resulting bounds are very coarse. We thus investigate clustering seekers based on similar behavior of their networks. We show that finding the optimal clustering of seekers is intractable, but we provide heuristic methods that give substantial time improvements. We then give an optimization that can benefit smaller populations of seekers based on clustering of taggers. Our results are supported by extensive experiments on del.icio.us datasets.

#index 1127408
#* Cleaning uncertain data with quality guarantees
#@ Reynold Cheng;Jinchuan Chen;Xike Xie
#t 2008
#c 4
#% 189744
#% 191616
#% 265692
#% 295512
#% 410276
#% 442830
#% 527176
#% 654487
#% 654488
#% 810553
#% 824728
#% 840577
#% 864417
#% 864455
#% 873104
#% 893167
#% 893189
#% 1016178
#% 1016201
#% 1016202
#% 1022203
#% 1063485
#% 1206646
#% 1206716
#% 1408794
#! Uncertain or imprecise data are pervasive in applications like location-based services, sensor monitoring, and data collection and integration. For these applications, probabilistic databases can be used to store uncertain data, and querying facilities are provided to yield answers with statistical confidence. Given that a limited amount of resources is available to "clean" the database (e.g., by probing some sensor data values to get their latest values), we address the problem of choosing the set of uncertain objects to be cleaned, in order to achieve the best improvement in the quality of query answers. For this purpose, we present the PWS-quality metric, which is a universal measure that quantifies the ambiguity of query answers under the possible world semantics. We study how PWS-quality can be efficiently evaluated for two major query classes: (1) queries that examine the satisfiability of tuples independent of other tuples (e.g., range queries); and (2) queries that require the knowledge of the relative ranking of the tuples (e.g., MAX queries). We then propose a polynomial-time solution to achieve an optimal improvement in PWS-quality. Other fast heuristics are presented as well. Experiments, performed on both real and synthetic datasets, show that the PWS-quality metric can be evaluated quickly, and that our cleaning algorithm provides an optimal solution with high efficiency. To our best knowledge, this is the first work that develops a quality metric for a probabilistic database, and investigates how such a metric can be used for data cleaning purposes.

#index 1127409
#* On the provenance of non-answers to queries over extracted data
#@ Jiansheng Huang;Ting Chen;AnHai Doan;Jeffrey F. Naughton
#t 2008
#c 4
#% 663
#% 462072
#% 480483
#% 480824
#% 810098
#% 810115
#% 864415
#% 874992
#% 875064
#% 893167
#% 976987
#% 993305
#% 1016204
#% 1022235
#% 1022288
#% 1022289
#% 1063547
#% 1206799
#% 1270363
#% 1661426
#! In information extraction, uncertainty is ubiquitous. For this reason, it is useful to provide users querying extracted data with explanations for the answers they receive. Providing the provenance for tuples in a query result partially addresses this problem, in that provenance can explain why a tuple is in the result of a query. However, in some cases explaining why a tuple is not in the result may be just as helpful. In this work we focus on providing provenance-style explanations for non-answers and develop a mechanism for providing this new type of provenance. Our experience with an information extraction prototype suggests that our approach can provide effective provenance information that can help a user resolve their doubts over non-answers to a query.

#index 1127410
#* Dynamic active probing of helpdesk databases
#@ Shenghuo Zhu;Tao Li;Zhiyuan Chen;Dingding Wang;Yihong Gong
#t 2008
#c 4
#% 116165
#% 218978
#% 243728
#% 310509
#% 333854
#% 340942
#% 345271
#% 428435
#% 449588
#% 452641
#% 461610
#% 465167
#% 478258
#% 479816
#% 673043
#% 796912
#% 814646
#% 864512
#% 866948
#% 866951
#% 875002
#% 879671
#% 889107
#% 919460
#% 937682
#% 960285
#% 993954
#% 1016203
#% 1306049
#! Helpdesk databases are used to store past interactions between customers and companies to improve customer service quality. One common scenario of using helpdesk database is to find whether recommendations exist given a new problem from a customer. However, customers often provide incomplete or even inaccurate information. Manually preparing a list of clarification questions does not work for large databases. This paper investigates the problem of automatic generation of a minimal number of questions to reach an appropriate recommendation. This paper proposes a novel dynamic active probing method. Compared to other alternatives such as decision tree and case-based reasoning, this method has two distinctive features. First, it actively probe the customer to get useful information to reach the recommendation, and the information provided by customer will be immediately used by the method to dynamically generate the next questions to probe. This feature ensures that all available information from the customer is used. Second, this method is based on a probabilistic model, and uses a data augmentation method which avoids overfitting when estimating the probabilities in the model. This feature ensures that the method is robust to databases that are incomplete or contain errors. Experimental results verify the effectiveness of our approach.

#index 1127411
#* Graceful database schema evolution: the PRISM workbench
#@ Carlo A. Curino;Hyun J. Moon;Carlo Zaniolo
#t 2008
#c 4
#% 374001
#% 462081
#% 562454
#% 654457
#% 801676
#% 809249
#% 824146
#% 824660
#% 824736
#% 885758
#% 945866
#% 956718
#% 976996
#% 997492
#% 1015271
#% 1015302
#% 1015303
#% 1036084
#% 1127421
#% 1148121
#% 1393677
#% 1699937
#% 1728154
#! Supporting graceful schema evolution represents an unsolved problem for traditional information systems that is further exacerbated in web information systems, such as Wikipedia and public scientific databases: in these projects based on multiparty cooperation the frequency of database schema changes has increased while tolerance for downtimes has nearly disappeared. As of today, schema evolution remains an error-prone and time-consuming undertaking, because the DB Administrator (DBA) lacks the methods and tools needed to manage and automate this endeavor by (i) predicting and evaluating the effects of the proposed schema changes, (ii) rewriting queries and applications to operate on the new schema, and (iii) migrating the database. Our PRISM system takes a big first step toward addressing this pressing need by providing: (i) a language of Schema Modification Operators to express concisely complex schema changes, (ii) tools that allow the DBA to evaluate the effects of such changes, (iii) optimized translation of old queries to work on the new schema version, (iv) automatic data migration, and (v) full documentation of intervened changes as needed to support data provenance, database flash back, and historical queries. PRISM solves these problems by integrating recent theoretical advances on mapping composition and invertibility, into a design that also achieves usability and scalability. Wikipedia and its 170+ schema versions provided an invaluable testbed for validating PRISM tools and their ability to support legacy queries.

#index 1127412
#* Analyzing and revising data integration schemas to improve their matchability
#@ Xiaoyong Chai;Mayssam Sayyadian;AnHai Doan;Arnon Rosenthal;Len Seligman
#t 2008
#c 4
#% 307632
#% 333988
#% 333990
#% 480969
#% 551850
#% 572314
#% 577320
#% 654458
#% 765433
#% 790843
#% 810103
#% 824735
#% 844413
#% 874991
#% 893095
#% 893193
#% 960271
#% 960272
#% 1022319
#! Data integration systems often provide a uniform query interface, called a mediated schema, to a multitude of data sources. To answer user queries, such systems employ a set of semantic matches between the mediated schema and the data-source schemas. Finding such matches is well known to be difficult. Hence much work has focused on developing semi-automatic techniques to efficiently find the matches. In this paper we consider the complementary problem of improving the mediated schema, to make finding such matches easier. Specifically, a mediated schema S will typically be matched with many source schemas. Thus, can the developer of S analyze and revise S in a way that preserves S's semantics, and yet makes it easier to match with in the future? In this paper we provide an affirmative answer to the above question, and outline a promising solution direction, called mSeer. Given a mediated schema S and a matching tool M, mSeer first computes a matchability score that quantifies how well S can be matched against using M. Next, mSeer uses this score to generate a matchability report that identifies the problems in matching S. Finally, mSeer addresses these problems by automatically suggesting changes to S (e.g., renaming an attribute, reformatting data values, etc.) that it believes will preserve the semantics of S and yet make it more amenable to matching. We present extensive experiments over several real-world domains that demonstrate the promise of the proposed approach.

#index 1127413
#* Learning to create data-integrating queries
#@ Partha Pratim Talukdar;Marie Jacob;Muhammad Salman Mehmood;Koby Crammer;Zachary G. Ives;Fernando Pereira;Sudipto Guha
#t 2008
#c 4
#% 28888
#% 248801
#% 273911
#% 348181
#% 387427
#% 408396
#% 428249
#% 572314
#% 577309
#% 654442
#% 660011
#% 715288
#% 745541
#% 763882
#% 810018
#% 824693
#% 874894
#% 913783
#% 961152
#% 976987
#% 993987
#% 1016176
#% 1016201
#% 1022258
#% 1038848
#% 1063579
#% 1065163
#% 1206684
#! The number of potentially-related data resources available for querying --- databases, data warehouses, virtual integrated schemas --- continues to grow rapidly. Perhaps no area has seen this problem as acutely as the life sciences, where hundreds of large, complex, interlinked data resources are available on fields like proteomics, genomics, disease studies, and pharmacology. The schemas of individual databases are often large on their own, but users also need to pose queries across multiple sources, exploiting foreign keys and schema mappings. Since the users are not experts, they typically rely on the existence of pre-defined Web forms and associated query templates, developed by programmers to meet the particular scientists' needs. Unfortunately, such forms are scarce commodities, often limited to a single database, and mismatched with biologists' information needs that are often context-sensitive and span multiple databases. We present a system with which a non-expert user can author new query templates and Web forms, to be reused by anyone with related information needs. The user poses keyword queries that are matched against source relations and their attributes; the system uses sequences of associations (e.g., foreign keys, links, schema mappings, synonyms, and taxonomies) to create multiple ranked queries linking the matches to keywords; the set of queries is attached to a Web query form. Now the user and his or her associates may pose specific queries by filling in parameters in the form. Importantly, the answers to this query are ranked and annotated with data provenance, and the user provides feedback on the utility of the answers, from which the system ultimately learns to assign costs to sources and associations according to the user's specific information need, as a result changing the ranking of the queries used to generate results. We evaluate the effectiveness of our method against "gold standard" costs from domain experts and demonstrate the method's scalability.

#index 1127414
#* Approximate lineage for probabilistic databases
#@ Christopher Ré;Dan Suciu
#t 2008
#c 4
#% 663
#% 22407
#% 73571
#% 148216
#% 156699
#% 176017
#% 190611
#% 201889
#% 211272
#% 215225
#% 265692
#% 273902
#% 297171
#% 303620
#% 333954
#% 333986
#% 384978
#% 388024
#% 464843
#% 731892
#% 742562
#% 769859
#% 782060
#% 810098
#% 824697
#% 875015
#% 942359
#% 956456
#% 960352
#% 976987
#% 993980
#% 1016201
#% 1022206
#% 1036075
#% 1289151
#! In probabilistic databases, lineage is fundamental to both query processing and understanding the data. Current systems s.a. Trio or Mystiq use a complete approach in which the lineage for a tuple t is a Boolean formula which represents all derivations of t. In large databases lineage formulas can become huge: in one public database (the Gene Ontology) we often observed 10MB of lineage (provenance) data for a single tuple. In this paper we propose to use approximate lineage, which is a much smaller formula keeping track of only the most important derivations, which the system can use to process queries and provide explanations. We discuss in detail two specific kinds of approximate lineage: (1) a conservative approximation called sufficient lineage that records the most important derivations for each tuple, and (2) polynomial lineage, which is more aggressive and can provide higher compression ratios, and which is based on Fourier approximations of Boolean expressions. In this paper we define approximate lineage formally, describe algorithms to compute approximate lineage and prove formally their error bounds, and validate our approach experimentally on a real data set.

#index 1127415
#* Exploiting shared correlations in probabilistic databases
#@ Prithviraj Sen;Amol Deshpande;Lise Getoor
#t 2008
#c 4
#% 1675
#% 31484
#% 215225
#% 496116
#% 602816
#% 654487
#% 708752
#% 722914
#% 864417
#% 893167
#% 893168
#% 960292
#% 1000502
#% 1016178
#% 1016201
#% 1022206
#% 1206732
#% 1206735
#% 1279353
#% 1289560
#! There has been a recent surge in work in probabilistic databases, propelled in large part by the huge increase in noisy data sources --- from sensor data, experimental data, data from uncurated sources, and many others. There is a growing need for database management systems that can efficiently represent and query such data. In this work, we show how data characteristics can be leveraged to make the query evaluation process more efficient. In particular, we exploit what we refer to as shared correlations where the same uncertainties and correlations occur repeatedly in the data. Shared correlations occur mainly due to two reasons: (1) Uncertainty and correlations usually come from general statistics and rarely vary on a tuple-to-tuple basis; (2) The query evaluation procedure itself tends to re-introduce the same correlations. Prior work has shown that the query evaluation problem on probabilistic databases is equivalent to a probabilistic inference problem on an appropriately constructed probabilistic graphical model (PGM). We leverage this by introducing a new data structure, called the random variable elimination graph (rv-elim graph) that can be built from the PGM obtained from query evaluation. We develop techniques based on bisimulation that can be used to compress the rv-elim graph exploiting the presence of shared correlations in the PGM, the compressed rv-elim graph can then be used to run inference. We validate our methods by evaluating them empirically and show that even with a few shared correlations significant speed-ups are possible.

#index 1127416
#* Access control over uncertain data
#@ Vibhor Rastogi;Dan Suciu;Evan Welbourne
#t 2008
#c 4
#% 237190
#% 248038
#% 452685
#% 462501
#% 576111
#% 587605
#% 765447
#% 832379
#% 893102
#% 992830
#% 1022206
#% 1022246
#% 1022248
#% 1063523
#% 1670071
#% 1740518
#! Access control is the problem of regulating access to secret information based on certain context information. In traditional applications, context information is known exactly, permitting a simple allow/deny semantics. In this paper, we look at access control when the context is itself uncertain. Our motivating application is RFID data management, in which the location of objects and people, and the associations between them is often uncertain to the system, yet access to private data is strictly defined in terms of these locations and associations. We formalize a natural semantics for access control that allows the release of partial information in the presence of uncertainty and describe an algorithm that uses a provably optimal perturbation function to enforce these semantics. To specify access control policies in practice, we describe UCAL, a new access control language for uncertain data. We then describe an output perturbation algorithm to implement access control policies described by UCAL. We carry out a set of experiments that demonstrate the feasibility of our approach and confirm its superiority over other possible approaches such as thresholding or sampling.

#index 1127417
#* Anonymizing bipartite graph data using safe groupings
#@ Graham Cormode;Divesh Srivastava;Ting Yu;Qing Zhang
#t 2008
#c 4
#% 408396
#% 443463
#% 576761
#% 864412
#% 881546
#% 893100
#% 956511
#% 1022247
#% 1130836
#% 1206581
#% 1206763
#% 1415851
#! Private data often comes in the form of associations between entities, such as customers and products bought from a pharmacy, which are naturally represented in the form of a large, sparse bipartite graph. As with tabular data, it is desirable to be able to publish anonymized versions of such data, to allow others to perform ad hoc analysis of aggregate graph properties. However, existing tabular anonymization techniques do not give useful or meaningful results when applied to graphs: small changes or masking of the edge structure can radically change aggregate graph properties. We introduce a new family of anonymizations, for bipartite graph data, called (k, l)-groupings. These groupings preserve the underlying graph structure perfectly, and instead anonymize the mapping from entities to nodes of the graph. We identify a class of "safe" (k, l)-groupings that have provable guarantees to resist a variety of attacks, and show how to find such safe groupings. We perform experiments on real bipartite graph data to study the utility of the anonymized version, and the impact of publishing alternate groupings of the same graph data. Our experiments demonstrate that (k, l)-groupings offer strong tradeoffs between privacy and utility.

#index 1127418
#* Privacy preserving serial data publishing by role composition
#@ Yingyi Bu;Ada Wai Chee Fu;Raymond Chi Wing Wong;Lei Chen;Jiuyong Li
#t 2008
#c 4
#% 576761
#% 576762
#% 800514
#% 810011
#% 864412
#% 874989
#% 881497
#% 881546
#% 893100
#% 960289
#% 960291
#% 982549
#% 1022247
#% 1044457
#% 1692987
#% 1725659
#! Previous works about privacy preserving serial data publishing on dynamic databases have relied on unrealistic assumptions of the nature of dynamic databases. In many applications, some sensitive values changes freely while others never change. For example, in medical applications, the disease attribute changes with time when patients recover from one disease and develop another disease. However, patients do not recover from some diseases such as HIV. We call such diseases permanent sensitive values. To the best of our knowledge, none of the existing solutions handle these realistic issues. We propose a novel anonymization approach called HD-composition to solve the above problems. Extensive experiments with real data confirm our theoretical results.

#index 1127419
#* Output perturbation with query relaxation
#@ Xiaokui Xiao;Yufei Tao
#t 2008
#c 4
#% 3421
#% 67453
#% 287794
#% 300184
#% 333947
#% 443463
#% 576110
#% 576762
#% 577239
#% 809244
#% 809245
#% 810028
#% 893101
#% 893105
#% 960279
#% 963241
#% 963242
#% 977011
#% 1022246
#% 1206678
#% 1670071
#% 1740518
#! Given a dataset containing sensitive personal information, a statistical database answers aggregate queries in a manner that preserves individual privacy. We consider the problem of constructing a statistical database using output perturbation, which protects privacy by injecting a small noise into each query result. We show that the state-of-the-art approach, ε-differential privacy, suffers from two severe deficiencies: it (i) incurs prohibitive computation overhead, and (ii) can answer only a limited number of queries, after which the statistical database has to be shut down. To remedy the problem, we develop a new technique that enforces ε-different privacy with economical cost. Our technique also incorporates a query relaxation mechanism, which removes the restriction on the number of permissible queries. The effectiveness and efficiency of our solution are verified through experiments with real data.

#index 1127420
#* Transaction time indexing with version compression
#@ David Lomet;Mingsheng Hong;Rimma Nehme;Rui Zhang
#t 2008
#c 4
#% 9241
#% 10392
#% 57958
#% 58371
#% 86953
#% 108499
#% 135384
#% 149632
#% 172370
#% 174220
#% 182672
#% 182700
#% 201869
#% 225004
#% 287070
#% 427199
#% 442967
#% 443257
#% 452782
#% 458857
#% 463749
#% 480096
#% 480481
#% 481126
#% 489211
#% 562290
#% 562809
#% 565265
#% 570888
#% 571296
#% 654455
#% 810114
#% 839178
#% 864422
#% 874998
#% 875049
#% 1019164
#% 1688316
#! Immortal DB is a transaction time database system designed to enable high performance for temporal applications. It is built into a commercial database engine, Microsoft SQL Server. This paper describes how we integrated a temporal indexing technique, the TSB-tree, into Immortal DB to serve as the core access method. The TSB-tree provides high performance access and update for both current and historical data. A main challenge was integrating TSB-tree functionality while preserving original B+tree functionality, including concurrency control and recovery. We discuss the overall architecture, including our unique treatment of index terms, and practical issues such as uncommitted data and log management. Performance is a primary concern. To increase performance, versions are locally delta compressed, exploiting the commonality between adjacent versions of the same record. This technique is also applied to index terms in index pages. There is a tradeoff between query performance and storage space. We discuss optimizing performance regarding this tradeoff throughout the paper. The result of our efforts is a high-performance transaction time database system built into an RDBMS engine, which has not been achieved before. We include a thorough experimental study and analysis that confirms the very good performance that it achieves.

#index 1127421
#* Managing and querying transaction-time databases under schema evolution
#@ Hyun J. Moon;Carlo A. Curino;Alin Deutsch;Chien-Yi Hou;Carlo Zaniolo
#t 2008
#c 4
#% 71562
#% 107764
#% 242856
#% 286986
#% 361445
#% 384978
#% 442967
#% 527786
#% 654457
#% 765432
#% 824736
#% 893093
#% 1015271
#% 1015303
#% 1015307
#% 1016147
#% 1022286
#% 1072638
#% 1126566
#% 1127411
#% 1148121
#! The old problem of managing the history of database information is now made more urgent and complex by fast spreading web information systems, such as Wikipedia. Our PRIMA system addresses this difficult problem by introducing two key pieces of new technology. The first is a method for publishing the history of a relational database in XML, whereby the evolution of the schema and its underlying database are given a unified representation. This temporally grouped representation makes it easy to formulate sophisticated historical queries on any given schema version using standard XQuery. The second key piece of technology is that schema evolution is transparent to the user: she writes queries against the current schema while retrieving the data from one or more schema versions. The system then performs the labor-intensive and error-prone task of rewriting such queries into equivalent ones for the appropriate versions of the schema. This feature is particularly important for historical queries spanning over potentially hundreds of different schema versions and it is realized in PRIMA by (i) introducing Schema Modification Operators (SMOs) to represent the mappings between successive schema versions and (ii) an XML integrity constraint language (XIC) to efficiently rewrite the queries using the constraints established by the SMOs. The scalability of the approach has been tested against both synthetic data and real-world data from the Wikipedia DB schema evolution history.

#index 1127422
#* On efficiently searching trajectories and archival data for historical similarities
#@ Reza Sherkat;Davood Rafiei
#t 2008
#c 4
#% 80995
#% 137711
#% 137887
#% 172949
#% 201893
#% 227857
#% 248797
#% 399763
#% 458857
#% 462231
#% 480146
#% 481609
#% 504158
#% 534183
#% 578407
#% 616530
#% 654456
#% 659971
#% 729931
#% 765451
#% 810049
#% 810068
#% 810069
#% 815984
#% 864407
#% 878300
#% 881459
#% 993965
#% 1016195
#% 1113090
#! We study the problem of efficiently evaluating similarity queries on histories, where a history is a d-dimensional time series for d ≥ 1. While there are some solutions for time-series and spatio-temporal trajectories where typically d ≤ 3, we are not aware of any work that examines the problem for larger values of d. In this paper, we address the problem in its general case and propose a class of summaries for histories with a few interesting properties. First, for commonly used distance functions such as the Lp-norm, LCSS, and DTW, the summaries can be used to efficiently prune some of the histories that cannot be in the answer set of the queries. Second, histories can be indexed based on their summaries, hence the qualifying candidates can be efficiently retrieved. To further reduce the number of unnecessary distance computations for false positives, we propose a finer level approximation of histories, and an algorithm to find an approximation with the least maximum distance estimation error. Experimental results confirm that the combination of our feature extraction approaches and the indexability of our summaries can improve upon existing methods and scales up for larger values of d and database sizes, based on our experiments on real and synthetic datasets of 17-dimensional histories.

#index 1127423
#* Keyword query cleaning
#@ Ken Q. Pu;Xiaohui Yu
#t 2008
#c 4
#% 198058
#% 333854
#% 387427
#% 397418
#% 546101
#% 654442
#% 740410
#% 863389
#% 864416
#% 875017
#% 960243
#% 960284
#% 960285
#% 993987
#% 1015325
#! Unlike traditional database queries, keyword queries do not adhere to predefined syntax and are often dirty with irrelevant words from natural languages. This makes accurate and efficient keyword query processing over databases a very challenging task. In this paper, we introduce the problem of query cleaning for keyword search queries in a database context and propose a set of effective and efficient solutions. Query cleaning involves semantic linkage and spelling corrections of database relevant query words, followed by segmentation of nearby query words such that each segment corresponds to a high quality data term. We define a quality metric of a keyword query, and propose a number of algorithms for cleaning keyword queries optimally. It is demonstrated that the basic optimal query cleaning problem can be solved using a dynamic programming algorithm. We further extend the basic algorithm to address incremental query cleaning and top-k optimal query cleaning. The incremental query cleaning is efficient and memory-bounded, hence is ideal for scenarios in which the keywords are streamed. The top-k query cleaning algorithm is guaranteed to return the best k cleaned keyword queries in ranked order. Extensive experiments are conducted on three real-life data sets, and the results confirm the effectiveness and efficiency of the proposed solutions.

#index 1127424
#* Reasoning and identifying relevant matches for XML keyword search
#@ Ziyang Liu;Yi Cher
#t 2008
#c 4
#% 214528
#% 342678
#% 654442
#% 810052
#% 863389
#% 864456
#% 956599
#% 960261
#% 1015258
#% 1016135
#% 1019060
#% 1063493
#% 1206722
#! Keyword search is a user-friendly mechanism for retrieving XML data in web and scientific applications. An intuitively compelling but vaguely defined goal is to identify matches to query keywords that are relevant to the user. However, it is hard to directly evaluate the relevance of query results due to the inherent ambiguity of search semantics. In this work, we investigate an axiomatic framework that includes two intuitive and non-trivial properties that an XML keyword search technique should ideally satisfy: monotonicity and consistency, with respect to data and query. This is the first work that reasons about keyword search strategies from a formal perspective. Then we propose a novel semantics for identifying relevant matches, which, to the best of our knowledge, is the only existing algorithm that satisfies both properties. An efficient algorithm is designed for realizing this semantics. Extensive experimental studies have verified the intuition of the properties and shown the effectiveness of the proposed algorithm.

#index 1127425
#* Ed-Join: an efficient algorithm for similarity joins with edit distance constraints
#@ Chuan Xiao;Wei Wang;Xuemin Lin
#t 2008
#c 4
#% 235941
#% 255137
#% 284111
#% 288885
#% 333679
#% 333973
#% 347225
#% 420072
#% 453529
#% 479973
#% 480463
#% 480654
#% 482121
#% 544203
#% 547409
#% 567254
#% 569755
#% 577238
#% 654454
#% 765463
#% 864392
#% 867054
#% 870896
#% 879600
#% 893164
#% 956506
#% 960263
#% 1022218
#% 1022227
#% 1022229
#% 1055684
#! There has been considerable interest in similarity join in the research community recently. Similarity join is a fundamental operation in many application areas, such as data integration and cleaning, bioinformatics, and pattern recognition. We focus on efficient algorithms for similarity join with edit distance constraints. Existing approaches are mainly based on converting the edit distance constraint to a weaker constraint on the number of matching q-grams between pair of strings. In this paper, we propose the novel perspective of investigating mismatching q-grams. Technically, we derive two new edit distance lower bounds by analyzing the locations and contents of mismatching q-grams. A new algorithm, Ed-Join, is proposed that exploits the new mismatch-based filtering methods; it achieves substantial reduction of the candidate sizes and hence saves computation time. We demonstrate experimentally that the new algorithm outperforms alternative methods on large-scale real datasets under a wide range of parameter settings.

#index 1127426
#* Scalable ad-hoc entity extraction from text collections
#@ Sanjay Agrawal;Kaushik Chakrabarti;Surajit Chaudhuri;Venkatesh Ganti
#t 2008
#c 4
#% 290703
#% 321327
#% 344447
#% 464434
#% 478258
#% 654471
#% 754068
#% 765463
#% 769884
#% 805883
#% 864415
#% 874992
#% 893164
#% 1063530
#! Supporting entity extraction from large document collections is important for enabling a variety of important data analysis tasks. In this paper, we introduce the "ad-hoc" entity extraction task where entities of interest are constrained to be from a list of entities that is specific to the task. In such scenarios, traditional entity extraction techniques that process all the documents for each ad-hoc entity extraction task can be significantly expensive. In this paper, we propose an efficient approach that leverages the inverted index on the documents to identify the subset of documents relevant to the task and processes only those documents. We demonstrate the efficiency of our techniques on real datasets.

#index 1127427
#* Scheduling shared scans of large data files
#@ Parag Agrawal;Daniel Kifer;Christopher Olston
#t 2008
#c 4
#% 172968
#% 281805
#% 419400
#% 495281
#% 531962
#% 593936
#% 810039
#% 950948
#% 959575
#% 963669
#% 983467
#% 1022262
#! We study how best to schedule scans of large data files, in the presence of many simultaneous requests to a common set of files. The objective is to maximize the overall rate of processing these files, by sharing scans of the same file as aggressively as possible, without imposing undue wait time on individual jobs. This scheduling problem arises in batch data processing environments such as Map-Reduce systems, some of which handle tens of thousands of processing requests daily, over a shared set of files. As we demonstrate, conventional scheduling techniques such as shortest-job-first do not perform well in the presence of cross-job sharing opportunities. We derive a new family of scheduling policies specifically targeted to sharable workloads. Our scheduling policies revolve around the notion that, all else being equal, it is good to schedule nonsharable scans ahead of ones that can share IO work with future jobs, if the arrival rate of sharable future jobs is expected to be high. We evaluate our policies via simulation over varied synthetic and real workloads, and demonstrate significant performance gains compared with conventional scheduling approaches.

#index 1127428
#* Online maintenance of very large random samples on flash storage
#@ Suman Nath;Phillip B. Gibbons
#t 2008
#c 4
#% 1331
#% 18658
#% 69316
#% 86955
#% 208047
#% 336610
#% 341100
#% 465162
#% 479974
#% 654486
#% 729852
#% 765426
#% 906949
#% 951778
#% 957221
#% 960238
#% 963436
#% 978722
#% 996053
#% 1053488
#% 1085291
#! Recent advances in flash media have made it an attractive alternative for data storage in a wide spectrum of computing devices, such as embedded sensors, mobile phones, PDA's, laptops, and even servers. However, flash media has many unique characteristics that make existing data management/analytics algorithms designed for magnetic disks perform poorly with flash storage. For example, while random (page) reads are as fast as sequential reads, random (page) writes and in-place data updates are orders of magnitude slower than sequential writes. In this paper, we consider an important fundamental problem that would seem to be particularly challenging for flash storage: efficiently maintaining a very large (100 MBs or more) random sample of a data stream (e.g., of sensor readings). First, we show that previous algorithms such as reservoir sampling and geometric file are not readily adapted to flash. Second, we propose B-FILE, an energy-efficient abstraction for flash media to store self-expiring items, and show how a B-FILE can be used to efficiently maintain a large sample in flash. Our solution is simple, has a small (RAM) memory footprint, and is designed to cope with flash constraints in order to reduce latency and energy consumption. Third, we provide techniques to maintain biased samples with a B-FILE and to query the large sample stored in a B-FILE for a subsample of an arbitrary size. Finally, we present an evaluation with flash media that shows our techniques are several orders of magnitude faster and more energy-efficient than (flash-friendly versions of) reservoir sampling and geometric file. A key finding of our study, of potential use to many flash algorithms beyond sampling, is that "semi-random" writes (as defined in the paper) on flash cards are over two orders of magnitude faster and more energy-efficient than random writes.

#index 1127429
#* A skip-list approach for efficiently processing forecasting queries
#@ Tingjian Ge;Stan Zdonik
#t 2008
#c 4
#% 69316
#% 102756
#% 124562
#% 172949
#% 334038
#% 453509
#% 481093
#% 534183
#% 745513
#% 768511
#% 818434
#% 874976
#% 875024
#% 915316
#% 921008
#% 1022261
#% 1727069
#% 1740388
#! Time series data is common in many settings including scientific and financial applications. In these applications, the amount of data is often very large. We seek to support prediction queries over time series data. Prediction relies on model building which can be too expensive to be practical if it is based on a large number of data points. We propose to use statistical tests of hypotheses to choose a proper subset of data points to use for a given prediction query interval. This involves two steps: choosing a proper history length and choosing the number of data points to use within this history. Further, we use an I/O conscious skip list data structure to provide samples of the original data set. Based on the statistics collected for a query workload, which we model as a probability mass function (PMF) over query intervals, we devise a randomized algorithm that selects a set of pre-built models (PM's) to construct, subject to some maintenance cost constraint when there are updates. Given this set of PM's, we discuss interesting query processing strategies for not only point queries, but also range, aggregation, and JOIN queries. We conduct a comprehensive empirical study on real world datasets to verify the effectiveness of our approaches and algorithms.

#index 1127430
#* A request-routing framework for SOA-based enterprise computing
#@ Thomas Phan;Wen-Syan Li
#t 2008
#c 4
#% 114994
#% 369236
#% 465701
#% 577344
#% 615063
#% 729456
#% 793903
#% 820402
#% 820427
#% 893364
#% 931104
#% 949165
#% 998845
#% 1044449
#% 1044482
#% 1206730
#! Enterprises may use a service-oriented architecture (SOA) to provide a streamlined interface to their business processes. To scale up the system, each tier in a composite service usually deploys multiple servers for load distribution and fault tolerance. Such load distribution across multiple servers within the same tier can be viewed as horizontal load distribution. One limitation of this approach is that load cannot be further distributed when all servers in the same tier are fully loaded. In complex multi-tiered systems, a single business process may actually be implemented by multiple different computation pathways among the tiers, each with different components, in order to provide resiliency and scalability. Such SOA-based enterprise computing with multiple implementation options gives opportunities for vertical load distribution across tiers. In this paper, we propose a requestrouting framework for SOA-based enterprise computing that takes into consideration both horizontal and vertical load distribution. Through experimentation we show that our algorithm and methodology scale well up to a large system configuration comprising up to 1000 workflow requests to a complex composite service with multiple implementations. We also show that a combination of both horizontal and vertical load distributions gives the maximum flexibility to improve performance and fault tolerance.

#index 1127431
#* Hexastore: sextuple indexing for semantic web data management
#@ Cathrin Weiss;Panagiotis Karras;Abraham Bernstein
#t 2008
#c 4
#% 82355
#% 139176
#% 172952
#% 330710
#% 345871
#% 519567
#% 571056
#% 769356
#% 783540
#% 805893
#% 823646
#% 824697
#% 824755
#% 851283
#% 864445
#% 875026
#% 960268
#% 1022236
#% 1055731
#% 1409924
#% 1409926
#% 1684016
#% 1702418
#! Despite the intense interest towards realizing the Semantic Web vision, most existing RDF data management schemes are constrained in terms of efficiency and scalability. Still, the growing popularity of the RDF format arguably calls for an effort to offset these drawbacks. Viewed from a relational-database perspective, these constraints are derived from the very nature of the RDF data model, which is based on a triple format. Recent research has attempted to address these constraints using a vertical-partitioning approach, in which separate two-column tables are constructed for each property. However, as we show, this approach suffers from similar scalability drawbacks on queries that are not bound by RDF property value. In this paper, we propose an RDF storage scheme that uses the triple nature of RDF as an asset. This scheme enhances the vertical partitioning idea and takes it to its logical conclusion. RDF data is indexed in six possible ways, one for each possible ordering of the three RDF elements. Each instance of an RDF element is associated with two vectors; each such vector gathers elements of one of the other types, along with lists of the third-type resources attached to each vector element. Hence, a sextuple-indexing scheme emerges. This format allows for quick and scalable general-purpose query processing; it confers significant advantages (up to five orders of magnitude) compared to previous approaches for RDF data management, at the price of a worst-case five-fold increase in index space. We experimentally document the advantages of our approach on real-world and synthetic data sets with practical queries.

#index 1127432
#* Indexing land surface for efficient kNN query
#@ Cyrus Shahabi;Lu-An Tang;Songhua Xing
#t 2008
#c 4
#% 86822
#% 121114
#% 201876
#% 300163
#% 413797
#% 427199
#% 594774
#% 745459
#% 814650
#% 864466
#% 993955
#% 1015321
#% 1016199
#% 1058620
#% 1072648
#% 1704006
#! The class of k Nearest Neighbor (kNN) queries is frequently used in geospatial applications. Many studies focus on processing kNN in Euclidean and road network spaces. Meanwhile, with the recent advances in remote sensory devices that can acquire detailed elevation data, the new geospatial applications heavily operate on this third dimension, i.e., land surface. Hence, for the field of databases to stay relevant, it should be able to efficiently process spatial queries given this constrained third dimension. However, online processing of the surface k Nearest Neighbor (skNN) queries is quite challenging due to the huge size of land surface models which renders any accurate distance computation on the surface extremely slow. In this paper, for the first time, we propose an index structure on land surface that enables exact and fast responses to skNN queries. Two complementary indexing schemes, namely Tight Surface Index (TSI) and Loose Surface Index (LSI), are constructed and stored collectively on a single novel data structure called Surface Index R-tree (SIR-tree). With those indexes, we can process skNN query efficiently by localizing the search and minimizing the invocation of the costly surface distance computation and hence incurring low I/O and computation costs. Our algorithm does not need to know the value of k a priori and can incrementally expand the search region using SIR-tree and report the query result progressively. It also reports the exact shortest surface paths to the query results. We show through experiments with real world data sets that our algorithm has better performance than the competitors in both efficiency and accuracy.

#index 1127433
#* Efficient skyline querying with variable user preferences on nominal attributes
#@ Raymond Chi-Wing Wong;Ada Wai-Chee Fu;Jian Pei;Yip Sing Ho;Tai Wong;Yubao Liu
#t 2008
#c 4
#% 287414
#% 288976
#% 289148
#% 465167
#% 480671
#% 654480
#% 800512
#% 800555
#% 806212
#% 810024
#% 824670
#% 824671
#% 824672
#% 864451
#% 875011
#% 912241
#% 915803
#% 989652
#% 992635
#% 993954
#% 1022224
#% 1022226
#% 1408811
#% 1712421
#! Current skyline evaluation techniques assume a fixed ordering on the attributes. However, dynamic preferences on nominal attributes are more realistic in known applications. In order to generate online response for any such preference issued by a user, one obvious solution is to enumerate all possible preferences and materialize all results of these preferences. However, the pre-processing and storage requirements of a full materialization are typically prohibitive. Instead, we propose a semi-materialization method called the IPO-tree Search which stores partial useful results only. With these partial results, the result of each possible preference can be returned efficiently. We have also conducted experiments to show the efficiency of our proposed algorithm.

#index 1127434
#* Efficient top-k processing over query-dependent functions
#@ Lin Guo;Sihem Amer Yahia;Raghu Ramakrishnan;Jayavel Shanmugasundaram;Utkarsh Srivastava;Erik Vee
#t 2008
#c 4
#% 222
#% 201921
#% 213981
#% 227894
#% 273694
#% 321455
#% 333854
#% 333938
#% 333951
#% 394417
#% 399762
#% 406493
#% 479967
#% 615077
#% 810018
#% 860455
#! We study the efficient evaluation of top-k queries over data items, where the score of each item is dynamically computed by applying an item-specific function whose parameter value is specified in the query. For example, online retail stores rank items by price, which may be a function of the quantity being queried: "Stay 3 nights, get a 15% discount on double-bed rooms." Similarly, while ranking possible routes in online maps by predicted congestion level, the score (congestion) is a function of the time being queried, e.g., "At 5PM on a Friday in Palo Alto, the congestion level on 101 North is high." Since the parameter---the number of nights or the time the online map is queried, in the above examples---is only known at query time, and online applications have stringent response-time requirements, it is infeasible to evaluate every item-specific function to determine the item scores, especially when the number of items is large. Further, space considerations make it infeasible to pre-compute and store the score of each item for each value of the input parameter. In this paper, we develop a novel technique that compresses the (large) set of item scores for all parameter values by dividing the parameter range into intervals, taking into account the expected query workload. This compressed representation is then used to do top-k pruning of query results. Our experiments show that the proposed techniques are scalable and efficient.

#index 1127435
#* FINCH: evaluating reverse k-Nearest-Neighbor queries on location data
#@ Wei Wu;Fei Yang;Chee-Yong Chan;Kian-Lee Tan
#t 2008
#c 4
#% 201876
#% 261733
#% 287466
#% 300163
#% 300174
#% 427199
#% 465009
#% 480661
#% 728933
#% 730019
#% 863390
#% 864464
#% 875013
#% 879211
#% 889094
#% 1016191
#% 1058620
#% 1080128
#! A Reverse k-Nearest-Neighbor (RkNN) query finds the objects that take the query object as one of their k nearest neighbors. In this paper we propose new solutions for evaluating RkNN queries and its variant bichromatic RkNN queries on 2-dimensional location data. We present an algorithm named INCH that can compute a RkNN query's search region (from which the query result candidates are drawn). In our RkNN evaluation algorithm called FINCH, the search region restricts the search space, and the search region is tightened each time a new result candidate is found. We also propose a method that enables us to apply any RkNN algorithm on bichromatic RkNN queries. With that, our FINCH algorithm is also used to evaluate bichromatic RkNN queries. Experiments show that our solutions are more efficient than existing algorithms.

#index 1127436
#* Discovery of convoys in trajectory databases
#@ Hoyoung Jeung;Man Lung Yiu;Xiaofang Zhou;Christian S. Jensen;Heng Tao Shen
#t 2008
#c 4
#% 462231
#% 659971
#% 769946
#% 784297
#% 810049
#% 836158
#% 836177
#% 864473
#% 879210
#% 881538
#% 907380
#% 957731
#% 960283
#% 1016195
#% 1112737
#% 1206688
#% 1720762
#! As mobile devices with positioning capabilities continue to proliferate, data management for so-called trajectory databases that capture the historical movements of populations of moving objects becomes important. This paper considers the querying of such databases for convoys, a convoy being a group of objects that have traveled together for some time. More specifically, this paper formalizes the concept of a convoy query using density-based notions, in order to capture groups of arbitrary extents and shapes. Convoy discovery is relevant for real-life applications in throughput planning of trucks and carpooling of vehicles. Although there has been extensive research on trajectories in the literature, none of this can be applied to retrieve correctly exact convoy result sets. Motivated by this, we develop three efficient algorithms for convoy discovery that adopt the well-known filter-refinement framework. In the filter step, we apply line-simplification techniques on the trajectories and establish distance bounds between the simplified trajectories. This permits efficient convoy discovery over the simplified trajectories without missing any actual convoys. In the refinement step, the candidate convoys are further processed to obtain the actual convoys. Our comprehensive empirical study offers insight into the properties of the paper's proposals and demonstrates that the proposals are effective and efficient on real-world trajectory data.

#index 1127437
#* TraClass: trajectory classification using hierarchical region-based and trajectory-based clustering
#@ Jae-Gil Lee;Jiawei Han;Xiaolei Li;Hector Gonzalez
#t 2008
#c 4
#% 190581
#% 479817
#% 621551
#% 818916
#% 876074
#% 881545
#% 960283
#% 1206639
#% 1558464
#% 1856449
#! Trajectory classification, i.e., model construction for predicting the class labels of moving objects based on their trajectories and other features, has many important, real-world applications. A number of methods have been reported in the literature, but due to using the shapes of whole trajectories for classification, they have limited classification capability when discriminative features appear at parts of trajectories or are not relevant to the shapes of trajectories. These situations are often observed in long trajectories spreading over large geographic areas. Since an essential task for effective classification is generating discriminative features, a feature generation framework TraClass for trajectory data is proposed in this paper, which generates a hierarchy of features by partitioning trajectories and exploring two types of clustering: (1) region-based and (2) trajectory-based. The former captures the higher-level region-based features without using movement patterns, whereas the latter captures the lower-level trajectory-based features using movement patterns. The proposed framework overcomes the limitations of the previous studies because trajectory partitioning makes discriminative parts of trajectories identifiable, and the two types of clustering collaborate to find features of both regions and sub-trajectories. Experimental results demonstrate that TraClass generates high-quality features and achieves high classification accuracy from real trajectory data.

#index 1127438
#* The V*-Diagram: a query-dependent approach to moving KNN queries
#@ Sarana Nutanong;Rui Zhang;Egemen Tanin;Lars Kulik
#t 2008
#c 4
#% 86950
#% 121114
#% 201876
#% 287466
#% 397377
#% 427199
#% 527026
#% 527187
#% 654478
#% 772839
#% 800571
#% 810048
#% 814646
#% 824723
#% 839701
#% 879211
#% 993955
#% 1015321
#% 1016199
#% 1063472
#% 1409341
#% 1720746
#% 1726326
#! The moving k nearest neighbor (MkNN) query finds the k nearest neighbors of a moving query point continuously. The high potential of reducing the query processing cost as well as the large spectrum of associated applications have attracted considerable attention to this query type from the database community. This paper presents an incremental safe-region-based technique for answering MkNN queries, called the V*-Diagram. In general, a safe region is a set of points where the query point can move without changing the query answer. Traditional safe-region approaches compute a safe region based on the data objects but independent of the query location. Our approach exploits the current knowledge of the query point and the search space in addition to the data objects. As a result, the V*-Diagram has much smaller IO and computation costs than existing methods. The experimental results show that the V*-Diagram outperforms the best existing technique by two orders of magnitude.

#index 1127439
#* Rewriting procedures for batched bindings
#@ Ravindra Guravannavar;S. Sudarshan
#t 2008
#c 4
#% 32878
#% 58367
#% 97771
#% 116050
#% 257642
#% 287005
#% 334006
#% 461897
#% 480091
#% 565689
#! Queries, or calls to stored procedures/user-defined functions are often invoked multiple times, either from within a loop in an application program, or from the where/select clause of an outer query. When the invoked query/procedure/function involves database access, a naive implementation can result in very poor performance, due to random I/O. Query decorrelation addresses this problem in the special case of nested sub-queries, but is not applicable otherwise. This problem is traditionally addressed by manually rewriting the application to make it set-oriented, by creating a batch of parameters, and by rewriting the query/procedure to work on the batch instead of one parameter at a time. Such manual rewriting is time-consuming and error prone. In this paper, we propose techniques that can be used to do the following, (a) Automatically rewrite programs to replace multiple calls to a query by a batched call to a correspondingly rewritten query, (b) Rewrite a stored procedure/function to accept a batch of bindings, instead of a single binding. Thereby, for example, a query which would have been invoked many times from different invocations of a stored procedure would be automatically replaced by one (or a few) invocations of a batched version of the query. Our techniques can be applied to code written in any language, such as procedural versions of SQL, or Java. We have implemented the proposed rewriting techniques for a subset of Java, where database operations are performed using an API over JDBC. We demonstrate the benefits due to our rewrites with three cases from real-world applications, which faced significant performance problems due to repeated invocations of queries/procedures.

#index 1127440
#* Identifying robust plans through plan diagram reduction
#@ Harish D.;Pooja N. Darera;Jayant R. Haritsa
#t 2008
#c 4
#% 13018
#% 102784
#% 164748
#% 214233
#% 248793
#% 256685
#% 273694
#% 273901
#% 378414
#% 480803
#% 765456
#% 810016
#% 810017
#% 824756
#% 993945
#% 1015318
#% 1022292
#% 1026989
#% 1050774
#% 1127564
#! Estimates of predicate selectivities by database query optimizers often differ significantly from those actually encountered during query execution, leading to poor plan choices and inflated response times. In this paper, we investigate mitigating this problem by replacing selectivity error-sensitive plan choices with alternative plans that provide robust performance. Our approach is based on the recent observation that even the complex and dense "plan diagrams" associated with industrial-strength optimizers can be efficiently reduced to "anorexic" equivalents featuring only a few plans, without materially impacting query processing quality. Extensive experimentation with a rich set of TPC-H and TPC-DS-based query templates in a variety of database environments indicate that plan diagram reduction typically retains plans that are substantially resistant to selectivity errors on the base relations. However, it can sometimes also be severely counter-productive, with the replacements performing much worse. We address this problem through a generalized mathematical characterization of plan cost behavior over the parameter space, which lends itself to efficient criteria of when it is safe to reduce. Our strategies are fully non-invasive and have been implemented in the Picasso optimizer visualization tool.

#index 1127441
#* A pay-as-you-go framework for query execution feedback
#@ Surajit Chaudhuri;Vivek Narasayya;Ravi Ramamurthy
#t 2008
#c 4
#% 172902
#% 248793
#% 273901
#% 397371
#% 463444
#% 480803
#% 632048
#% 765427
#% 765435
#% 765456
#% 810016
#% 810017
#% 864426
#% 993387
#% 1015334
#% 1016225
#% 1206720
#! Past work has suggested that query execution feedback can be useful in improving the quality of plans by correcting cardinality estimation errors in the query optimizer. The state-of-the-art approach for obtaining execution feedback is "passive" monitoring which records the cardinality of each operator in the execution plan. We observe that there are many cases where even after repeated executions of the same query with use of feedback from passive monitoring, suboptimal choices in the execution plan cannot be corrected. We present a novel "pay-as-you-go" framework in which a query potentially incurs a small overhead on each execution but obtains cardinality information that is not available with passive monitoring alone. Such a framework can significantly extend the reach of query execution feedback in obtaining better plans. We have implemented our techniques in Microsoft SQL Server, and our evaluation on real world and synthetic queries suggests that plan quality can improve significantly compared to passive monitoring even at low overheads.

#index 1127442
#* Evita raced: metacompilation for declarative networks
#@ Tyson Condie;David Chu;Joseph M. Hellerstein;Petros Maniatis
#t 2008
#c 4
#% 32889
#% 43162
#% 116043
#% 204182
#% 248793
#% 300167
#% 316575
#% 368248
#% 411554
#% 440476
#% 476452
#% 495283
#% 565457
#% 723446
#% 809234
#% 821939
#% 835186
#% 874978
#% 938093
#% 939889
#% 960236
#% 978402
#% 997709
#% 998834
#% 1019705
#% 1022288
#% 1043812
#% 1050850
#% 1072057
#! Declarative languages have recently been proposed for many new applications outside of traditional data management. Since these are relatively early research efforts, it is important that the architectures of these declarative systems be extensible, in order to accommodate unforeseen needs in these new domains. In this paper, we apply the lessons of declarative systems to the internals of a declarative engine. Specifically, we describe our design and implementation of Evita Raced, an extensible compiler for the OverLog language used in our declarative networking system, P2. Evita Raced is a metacompiler: an OverLog compiler written in OverLog. We describe the minimalist architecture of Evita Raced, including its extensibility interfaces and its reuse of P2's data model and runtime engine. We demonstrate that a declarative language like OverLog is well-suited to expressing traditional and novel query optimizations as well as other query manipulations, in a compact and natural fashion. Finally, we present initial results of Evita Raced extended with various optimization programs, running on both Internet overlay networks and wireless sensor networks.

#index 1127443
#* Discovering data quality rules
#@ Fei Chiang;Renée J. Miller
#t 2008
#c 4
#% 152934
#% 189872
#% 224743
#% 227917
#% 273899
#% 310494
#% 350100
#% 397369
#% 399793
#% 443393
#% 464837
#% 480124
#% 480496
#% 480499
#% 481290
#% 745494
#% 998770
#% 1022222
#% 1022228
#! Dirty data is a serious problem for businesses leading to incorrect decision making, inefficient daily operations, and ultimately wasting both time and money. Dirty data often arises when domain constraints and business rules, meant to preserve data consistency and accuracy, are enforced incompletely or not at all in application code. In this work, we propose a new data-driven tool that can be used within an organization's data quality management process to suggest possible rules, and to identify conformant and non-conformant records. Data quality rules are known to be contextual, so we focus on the discovery of context-dependent rules. Specifically, we search for conditional functional dependencies (CFDs), that is, functional dependencies that hold only over a portion of the data. The output of our tool is a set of functional dependencies together with the context in which they hold (for example, a rule that states for CS graduate courses, the course number and term functionally determines the room and instructor). Since the input to our tool will likely be a dirty database, we also search for CFDs that almost hold. We return these rules together with the non-conformant records (as these are potentially dirty records). We present effective algorithms for discovering CFDs and dirty values in a data instance. Our discovery algorithm searches for minimal CFDs among the data values and prunes redundant candidates. No universal objective measures of data quality or data quality rules are known. Hence, to avoid returning an unnecessarily large number of CFDs and only those that are most interesting, we evaluate a set of interest metrics and present comparative results using real datasets. We also present an experimental study showing the scalability of our techniques.

#index 1127444
#* Mining non-redundant high order correlations in binary data
#@ Xiang Zhang;Feng Pan;Wei Wang;Andrew Nobel
#t 2008
#c 4
#% 115608
#% 376266
#% 385564
#% 452846
#% 727869
#% 729918
#% 769909
#% 770799
#% 793250
#% 814023
#% 823343
#% 881478
#% 881479
#% 915343
#% 989606
#% 1117701
#% 1274940
#! Many approaches have been proposed to find correlations in binary data. Usually, these methods focus on pair-wise correlations. In biology applications, it is important to find correlations that involve more than just two features. Moreover, a set of strongly correlated features should be non-redundant in the sense that the correlation is strong only when all the interacting features are considered together. Removing any feature will greatly reduce the correlation. In this paper, we explore the problem of finding non-redundant high order correlations in binary data. The high order correlations are formalized using multi-information, a generalization of pairwise mutual information. To reduce the redundancy, we require any subset of a strongly correlated feature subset to be weakly correlated. Such feature subsets are referred to as Non-redundant Interacting Feature Subsets (NIFS). Finding all NIFSs is computationally challenging, because in addition to enumerating feature combinations, we also need to check all their subsets for redundancy. We study several properties of NIFSs and show that these properties are useful in developing efficient algorithms. We further develop two sets of upper and lower bounds on the correlations, which can be incorporated in the algorithm to prune the search space. A simple and effective pruning strategy based on pair-wise mutual information is also developed to further prune the search space. The efficiency and effectiveness of our approach are demonstrated through extensive experiments on synthetic and real-life datasets.

#index 1127445
#* Keyword search on external memory data graphs
#@ Bhavana Bharat Dalvi;Meghana Kshirsagar;S. Sudarshan
#t 2008
#c 4
#% 202287
#% 303048
#% 303087
#% 348615
#% 443516
#% 527029
#% 654442
#% 818328
#% 824693
#% 824695
#% 839172
#% 875017
#% 960243
#% 960259
#% 993987
#% 1015325
#% 1016176
#% 1081216
#! Keyword search on graph structured data has attracted a lot of attention in recent years. Graphs are a natural "lowest common denominator" representation which can combine relational, XML and HTML data. Responses to keyword queries are usually modeled as trees that connect nodes matching the keywords. In this paper we address the problem of keyword search on graphs that may be significantly larger than memory. We propose a graph representation technique that combines a condensed version of the graph (the "supernode graph") which is always memory resident, along with whatever parts of the detailed graph are in a cache, to form a multi-granular graph representation. We propose two alternative approaches which extend existing search algorithms to exploit multigranular graphs; both approaches attempt to minimize IO by directing search towards areas of the graph that are likely to give good results. We compare our algorithms with a virtual memory approach on several real data sets. Our experimental results show significant benefits in terms of reduction in IO due to our algorithms.

#index 1127446
#* Sorting hierarchical data in external memory for archiving
#@ Ioannis Koltsidas;Heiko Müller;Stratis D. Viglas
#t 2008
#c 4
#% 136740
#% 210212
#% 252608
#% 330627
#% 410276
#% 443033
#% 742561
#% 745445
#% 1063581
#! Sorting hierarchical data in external memory is necessary for a wide variety of applications including archiving scientific data and dealing with large XML datasets. The topic of sorting hierarchical data, however, has received little attention from the research community so far. In this paper we focus on sorting arbitrary hierarchical data that far exceed the size of physical memory. We propose HErMeS, an algorithm that generalizes the most widely-used techniques for sorting flat data in external memory. HErMeS efficiently exploits the hierarchical structure to minimize the number of disk accesses and optimize the use of available memory. We extract the theoretical bounds of the algorithm with respect to the structure of the hierarchical dataset. We then show how the algorithm can be used to support efficient archiving. We have conducted an experimental study using several workloads and comparing HErMeS to the state-of-the-art approaches. Our results show that our algorithm (a) meets its theoretical expectations, (b) allows for scalable database archiving, and (c) outperforms the competition by a significant factor. These results, we believe, prove our technique to be a viable and scalable solution to the problem of sorting hierarchical data in external memory.

#index 1127555
#* SLEUTH: Single-pubLisher attack dEtection Using correlaTion Hunting
#@ Ahmed Metwally;Fatih Emekçi;Divyakant Agrawal;Amr El Abbadi
#t 2008
#c 4
#% 281144
#% 281145
#% 317813
#% 322884
#% 399993
#% 492912
#% 548479
#% 568291
#% 569754
#% 576119
#% 642409
#% 730046
#% 765414
#% 805840
#% 824665
#% 894443
#% 956518
#% 963693
#% 978241
#% 981649
#% 993960
#% 1015293
#! Several data management challenges arise in the context of Internet advertising networks, where Internet advertisers pay Internet publishers to display advertisements on their Web sites and drive traffic to the advertisers from surfers' clicks. Although advertisers can target appropriate market segments, the model allows dishonest publishers to defraud the advertisers by simulating fake traffic to their own sites to claim more revenue. This paper addresses the case of publishers launching fraud attacks from numerous machines, which is the most widespread scenario. The difficulty of uncovering these attacks is proportional to the number of machines and resources exploited by the fraudsters. In general, detecting this class of fraud entails solving a new data mining problem, which is finding correlations in multidimensional data. Since the dimensions have large cardinalities, the search space is huge, which has long allowed dishonest publishers to inflate their traffic, and deplete the advertisers' advertising budgets. We devise the approximate SLEUTH algorithms to solve the problem efficiently, and uncover single-publisher frauds. We demonstrate the effectiveness of SLEUTH both analytically and by reporting some of its results on the Fastclick network, where numerous fraudsters were discovered.

#index 1127556
#* Energy cost, the key challenge of today's data centers: a power consumption analysis of TPC-C results
#@ Meikel Poess;Raghunath Othayoth Nambiar
#t 2008
#c 4
#% 328431
#% 824740
#% 834884
#% 960264
#% 963135
#! Historically, performance and price-performance of computer systems have been the key purchasing arguments for customers. With rising energy costs and increasing power use due to the ever-growing demand for computing power (servers, storage, networks), electricity bills have become a significant expense for today's data centers. In the very near future, energy efficiency is expected to be one of the key purchasing arguments. Some performance organizations, such as SPEC, have developed power benchmarks for single servers (SPECpower_ssj2008), but so far, no benchmark exists that measures the power consumption of transaction processing systems. In this paper, we develop a power consumption model based on data readily available in the TPC-C full disclosure report of published benchmarks. We verify our model with measurements taken from three fully scaled and optimized TPC-C configurations including client (middle-tier) systems, database server, and storage subsystem. By applying this model to a subset of 7 years of TPC-C results, we identify the most power-intensive components and demonstrate the existing power consumption trends over time. Assuming similar trends in the future, the hardware enhancements alone will not be able to satisfy the demand for energy efficiency. In its outlook, this paper looks at potential hardware and software enhancements to meet the energy efficiency demands of future systems. Realizing the importance of energy efficiency, the Transaction Processing Performance Council (TPC) has formed a working group to look into adding energy efficiency metrics to all its benchmarks. This paper is expected to complement this initiative.

#index 1127557
#* Google's Deep Web crawl
#@ Jayant Madhavan;David Ko;Łucja Kot;Vignesh Ganapathy;Alex Rasmussen;Alon Halevy
#t 2008
#c 4
#% 198466
#% 333990
#% 340146
#% 406493
#% 447946
#% 480479
#% 765409
#% 800497
#% 809418
#% 864434
#% 866989
#% 955762
#% 993964
#% 1016163
#! The Deep Web, i.e., content hidden behind HTML forms, has long been acknowledged as a significant gap in search engine coverage. Since it represents a large portion of the structured data on the Web, accessing Deep-Web content has been a long-standing challenge for the database community. This paper describes a system for surfacing Deep-Web content, i.e., pre-computing submissions for each HTML form and adding the resulting HTML pages into a search engine index. The results of our surfacing have been incorporated into the Google search engine and today drive more than a thousand queries per second to Deep-Web content. Surfacing the Deep Web poses several challenges. First, our goal is to index the content behind many millions of HTML forms that span many languages and hundreds of domains. This necessitates an approach that is completely automatic, highly scalable, and very efficient. Second, a large number of forms have text inputs and require valid inputs values to be submitted. We present an algorithm for selecting input values for text search inputs that accept keywords and an algorithm for identifying inputs which accept only values of a specific type. Third, HTML forms often have more than one input and hence a naive strategy of enumerating the entire Cartesian product of all possible inputs can result in a very large number of URLs being generated. We present an algorithm that efficiently navigates the search space of possible input combinations to identify only those that generate URLs suitable for inclusion into our web search index. We present an extensive experimental evaluation validating the effectiveness of our algorithms.

#index 1127558
#* Industry-scale duplicate detection
#@ Melanie Weis;Felix Naumann;Ulrich Jehle;Jens Lufter;Holger Schuster
#t 2008
#c 4
#% 201889
#% 387427
#% 420072
#% 729913
#% 766199
#% 810044
#% 871766
#% 913783
#% 967272
#% 993980
#% 1022229
#! Duplicate detection is the process of identifying multiple representations of a same real-world object in a data source. Duplicate detection is a problem of critical importance in many applications, including customer relationship management, personal information management, or data mining. In this paper, we present how a research prototype, namely DogmatiX, which was designed to detect duplicates in hierarchical XML data, was successfully extended and applied on a large scale industrial relational database in cooperation with Schufa Holding AG. Schufa's main business line is to store and retrieve credit histories of over 60 million individuals. Here, correctly identifying duplicates is critical both for individuals and companies: On the one hand, an incorrectly identified duplicate potentially results in a false negative credit history for an individual, who will then not be granted credit anymore. On the other hand, it is essential for companies that Schufa detects duplicates of a person that deliberately tries to create a new identity in the database in order to have a clean credit history. Besides the quality of duplicate detection, i.e., its effectiveness, scalability cannot be neglected, because of the considerable size of the database. We describe our solution to coping with both problems and present a comprehensive evaluation based on large volumes of real-world data.

#index 1127559
#* SCOPE: easy and efficient parallel processing of massive data sets
#@ Ronnie Chaiken;Bob Jenkins;Per-Åke Larson;Bill Ramsey;Darren Shakib;Simon Weaver;Jingren Zhou
#t 2008
#c 4
#% 394617
#% 581186
#% 723279
#% 954300
#% 960326
#% 963669
#% 983467
#% 1002142
#% 1063553
#! Companies providing cloud-scale services have an increasing need to store and analyze massive data sets such as search logs and click streams. For cost and performance reasons, processing is typically done on large clusters of shared-nothing commodity machines. It is imperative to develop a programming model that hides the complexity of the underlying system but provides flexibility by allowing users to extend functionality to meet a variety of requirements. In this paper, we present a new declarative and extensible scripting language, SCOPE (Structured Computations Optimized for Parallel Execution), targeted for this type of massive data analysis. The language is designed for ease of use with no explicit parallelism, while being amenable to efficient parallel execution on large clusters. SCOPE borrows several features from SQL. Data is modeled as sets of rows composed of typed columns. The select statement is retained with inner joins, outer joins, and aggregation allowed. Users can easily define their own functions and implement their own versions of operators: extractors (parsing and constructing rows from a file), processors (row-wise processing), reducers (group-wise processing), and combiners (combining rows from two inputs). SCOPE supports nesting of expressions but also allows a computation to be specified as a series of steps, in a manner often preferred by programmers. We also describe how scripts are compiled into efficient, parallel execution plans and executed on large clusters.

#index 1127560
#* PNUTS: Yahoo!'s hosted data serving platform
#@ Brian F. Cooper;Raghu Ramakrishnan;Utkarsh Srivastava;Adam Silberstein;Philip Bohannon;Hans-Arno Jacobsen;Nick Puz;Daniel Weaver;Ramana Yerneni
#t 2008
#c 4
#% 9241
#% 115661
#% 232771
#% 240016
#% 330305
#% 340175
#% 342375
#% 403195
#% 479978
#% 505869
#% 723279
#% 893147
#% 911470
#% 963667
#% 963669
#% 978404
#% 978411
#% 993515
#% 998842
#% 998845
#% 1015281
#% 1063527
#% 1063553
#! We describe PNUTS, a massively parallel and geographically distributed database system for Yahoo!'s web applications. PNUTS provides data storage organized as hashed or ordered tables, low latency for large numbers of concurrent requests including updates and queries, and novel per-record consistency guarantees. It is a hosted, centrally managed, and geographically distributed service, and utilizes automated load-balancing and failover to reduce operational complexity. The first version of the system is currently serving in production. We describe the motivation for PNUTS and the design and implementation of its table storage and replication layers, and then present experimental results.

#index 1127561
#* Relational support for flexible schema scenarios
#@ Srini Acharya;Peter Carlin;Cesar Galindo-Legaria;Krzysztof Kozielczyk;Pawel Terlecki;Peter Zabback
#t 2008
#c 4
#% 64791
#% 318705
#% 397366
#% 397371
#% 442875
#% 463917
#% 480258
#% 480629
#% 765427
#% 765472
#% 765488
#% 864445
#% 938982
#% 960302
#% 1015334
#% 1016212
#% 1022289
#% 1206806
#! Efficient support for applications that deal with data heterogeneity, hierarchies and schema evolution is an important challenge for relational engines. In this paper we show how this flexibility can be handled in Microsoft SQL Server. For this purpose, the engine has been equipped in an integrated package of relational extensions. The package includes sparse storage, column set operations, filtered indices, filtered statistics and hierarchy querying with OrdPath labeling. In addition, economical loading of metadata allow us to answer queries independently of the number of columns in a table and drastically improve scaling capabilities. The design of a prototypical content and collaboration application based on a wide table is described, along with experiments validating its performance.

#index 1127562
#* Oracle SecureFiles System
#@ Niloy Mukherjee;Bharath Aleti;Amit Ganesh;Krishna Kunchithapadam;Scott Lynn;Sujatha Muthulingam;Kam Shergill;Shaoyu Wang;Wei Zhang
#t 2008
#c 4
#% 244119
#% 443257
#% 480831
#% 481425
#% 723279
#% 951778
#% 978404
#% 1022298
#! Over the last decade, the nature of content stored on computer storage systems has evolved from being relational to being semi-structured, i.e., unstructured data accompanied by relational metadata. Average data volumes have increased from a few hundred megabytes to hundreds of terabytes. Simultaneously, data feed rates have also increased with increase in processor, storage and network bandwidths. Data growth trends seem to be following Moore's law and thereby imply an exponential explosion in content volumes and rates in the years to come. The near future poses requirements for data management systems to provide solutions that provide unlimited scalability in execution, availability, recoverability and storage usage of semi-structured content. Traditionally, filesystems have been preferred over database management systems for providing storage solutions for unstructured data, while databases have been the preferred choice to manage relational data. Lack of consolidated semi-structured content management architecture compromises security, availability, recoverability, and manageability among other features. We introduce a system without compromises, the Oracle SecureFiles System, designed to provide highly scalable storage and access execution of unstructured and structured content as first-class objects within the Oracle relational database management system. Oracle SecureFiles breaks the performance barrier that has kept such content out of databases. The architecture provides capability to maximize utilization of storage usage through compression and de-duplication and achieves robustness by preserving transactional atomicity, durability, availability, read-consistent query-ability and security of the database management system.

#index 1127563
#* Efficient implementation of sorting on multi-core SIMD CPU architecture
#@ Jatin Chhugani;Anthony D. Nguyen;Victor W. Lee;William Macy;Mostafa Hagog;Yen-Kuang Chen;Akram Baransi;Sanjeev Kumar;Pradeep Dubey
#t 2008
#c 4
#% 46617
#% 55459
#% 58185
#% 104044
#% 252608
#% 317797
#% 500284
#% 862731
#% 874997
#% 920035
#% 988651
#% 1002570
#% 1022311
#% 1050225
#% 1504754
#! Sorting a list of input numbers is one of the most fundamental problems in the field of computer science in general and high-throughput database applications in particular. Although literature abounds with various flavors of sorting algorithms, different architectures call for customized implementations to achieve faster sorting times. This paper presents an efficient implementation and detailed analysis of MergeSort on current CPU architectures. Our SIMD implementation with 128-bit SSE is 3.3X faster than the scalar version. In addition, our algorithm performs an efficient multiway merge, and is not constrained by the memory bandwidth. Our multi-threaded, SIMD implementation sorts 64 million floating point numbers in less than0.5 seconds on a commodity 4-core Intel processor. This measured performance compares favorably with all previously published results. Additionally, the paper demonstrates performance scalability of the proposed sorting algorithm with respect to certain salient architectural features of modern chip multiprocessor (CMP) architectures, including SIMD width and core-count. Based on our analytical models of various architectural configurations, we see excellent scalability of our implementation with SIMD width scaling up to 16X wider than current SSE width of 128-bits, and CMP core-count scaling well beyond 32 cores. Cycle-accurate simulation of Intel's upcoming x86 many-core Larrabee architecture confirms scalability of our proposed algorithm.

#index 1127564
#* Efficiently approximating query optimizer plan diagrams
#@ Atreyee Dey;Sourjya Bhaumik;Harish D;Jayant R. Haritsa
#t 2008
#c 4
#% 172900
#% 248793
#% 273694
#% 299989
#% 378414
#% 392851
#% 463444
#% 481749
#% 765456
#% 824756
#% 835018
#% 993945
#% 993946
#% 1015318
#% 1022292
#% 1026989
#! Given a parametrized n-dimensional SQL query template and a choice of query optimizer, a plan diagram is a color-coded pictorial enumeration of the execution plan choices of the optimizer over the query parameter space. These diagrams have proved to be a powerful metaphor for the analysis and redesign of modern optimizers, and are gaining currency in diverse industrial and academic institutions. However, their utility is adversely impacted by the impractically large computational overheads incurred when standard brute-force exhaustive approaches are used for producing fine-grained diagrams on high-dimensional query templates. In this paper, we investigate strategies for efficiently producing close approximations to complex plan diagrams. Our techniques are customized to the features available in the optimizer's API, ranging from the generic optimizers that provide only the optimal plan for a query, to those that also support costing of sub-optimal plans and enumerating rank-ordered lists of plans. The techniques collectively feature both random and grid sampling, as well as inference techniques based on nearest-neighbor classifiers, parametric query optimization and plan cost monotonicity. Extensive experimentation with a representative set of TPC-H and TPC-DS-based query templates on industrial-strength optimizers indicates that our techniques are capable of delivering 90% accurate diagrams while incurring less than 15% of the computational overheads of the exhaustive approach. In fact, for full-featured optimizers, we can guarantee zero error with less than 10% overheads. These approximation techniques have been implemented in the publicly available Picasso optimizer visualization tool.

#index 1127565
#* Brighthouse: an analytic data warehouse for ad-hoc queries
#@ Dominik Ślȩzak;Jakub Wróblewski;Victoria Eastwood;Piotr Synak
#t 2008
#c 4
#% 193923
#% 286258
#% 333947
#% 366687
#% 376266
#% 427307
#% 451767
#% 824697
#% 864446
#% 893148
#% 893158
#% 903014
#% 960250
#% 960266
#% 1015256
#% 1022202
#% 1022297
#% 1022298
#% 1022300
#% 1026989
#% 1063507
#% 1063549
#% 1063727
#% 1127968
#% 1206595
#% 1206624
#% 1698970
#% 1914458
#! Brighthouse is a column-oriented data warehouse with an automatically tuned, ultra small overhead metadata layer called Knowledge Grid, that is used as an alternative to classical indexes. The advantages of column-oriented data storage, as well as data compression have already been well-documented, especially in the context of analytic, decision support querying. This paper demonstrates additional benefits resulting from Knowledge Grid for compressed, column-oriented databases. In particular, we explain how it assists in query optimization and execution, by minimizing the need of data reads and data decompression.

#index 1127566
#* Optimizer plan change management: improved stability and performance in Oracle 11g
#@ Mohamed Ziauddin;Dinesh Das;Hong Su;Yali Zhu;Khaled Yagoub
#t 2008
#c 4
#% 172900
#% 248793
#% 248795
#% 397404
#% 464233
#% 480803
#% 480955
#% 571294
#% 715955
#% 765456
#% 770354
#% 960322
#% 1016221
#% 1063549
#! Execution plans for SQL statements have a significant impact on the overall performance of database systems. New optimizer statistics, configuration parameter changes, software upgrades and hardware resource utilization are among a multitude of factors that may cause the query optimizer to generate new plans. While most of these plan changes are beneficial or benign, a few rogue plans can potentially wreak havoc on system performance or availability, affecting critical and time-sensitive business application needs. The normally desirable ability of a query optimizer to adapt to system changes may sometimes cause it to pick a sub-optimal plan compromising the stability of the system. In this paper, we present the new SQL Plan Management feature in Oracle 11g. It provides a comprehensive solution for managing plan changes to provide stable and optimal performance for a set of SQL statements. Two of its most important goals are preventing sub-optimal plans from being executed while allowing new plans to be used if they are verifiably better than previous plans. This feature is tightly integrated with Oracle's query optimizer. SQL Plan Management is available to users via both command-line and graphical interfaces. We describe the feature and then, using an industrial-strength application suite, present experimental results that show that SQL Plan Management provides stable and optimal performance for SQL statements with no performance regressions.

#index 1127567
#* Towards a physical XML independent XQuery/SQL/XML engine
#@ Zhen Hua Liu;Sivasankaran Chandrasekar;Thomas Baby;Hui J. Chang
#t 2008
#c 4
#% 136740
#% 227962
#% 273937
#% 287005
#% 341233
#% 345742
#% 385828
#% 396733
#% 397366
#% 397375
#% 413562
#% 413650
#% 464007
#% 479956
#% 480657
#% 485023
#% 562456
#% 570875
#% 731408
#% 745529
#% 781453
#% 810036
#% 810083
#% 810116
#% 810118
#% 824661
#% 824751
#% 864401
#% 875010
#% 893174
#% 942738
#% 945868
#% 960317
#% 994015
#% 1015274
#% 1015339
#% 1016134
#% 1016143
#% 1016150
#% 1016223
#% 1016224
#% 1022209
#! There has been a lot of research and industrial effort on building XQuery engines with different kinds of XML storage and index models. However, most of these efforts focus on building either an efficient XQuery engine with one kind of XML storage, index, view model in mind or a general XQuery engine without any consideration of the underlying XML storage, index and view model. We need an underlying framework to build an XQuery engine that can work with and provide optimization for different XML storage, index and view models. Besides XQuery, RDBMSs also support SQL/XML, a standard language that integrates XML and relational processing. There are industrial efforts for building hybrid XQuery and SQL/XML engines that support both languages so that users can manage and query both relational and XML data on one platform. However, we need a theoretical framework to optimize both SQL/XML and XQuery languages in one RDBMS. In this paper, we show our industrial work of building a combined XQuery and SQL/XML engine that is able to work and provide optimization for different kinds of XML storage and index models in Oracle XMLDB. This work is based on XML extended relational algebra as the underlying tuple-based logical algebra and incorporates tree and automata based physical algebra into the logical tuple-based algebra so as to provide optimization for different physical XML formulations. This results in logical and physical rewrite techniques to optimize XQuery and SQL/XML over a variety of physical XML storage, index and view models, including schema aware object relational XML storage with relational indexes, binary XML storage with schema agnostic path-value-order key XMLIndex, SQL/XML view over relational data and relational view over XML. Furthermore, we show the approach of leveraging cost based XML physical rewrite strategy to evaluate different physical rewrite plans.

#index 1127568
#* Closing the query processing loop in Oracle 11g
#@ Allison W. Lee;Mohamed Zait
#t 2008
#c 4
#% 172900
#% 248793
#% 300167
#% 480803
#% 765456
#% 893173
#% 994014
#% 1016221
#% 1063549
#! The role of a query optimizer in a database system is to find the best execution plan for a given SQL statement based on statistics about the objects related to the tables referenced in the statement. These statistics include the tables themselves, their indexes, and other derived objects. Statistics include the number of rows, space utilization on disk, distribution of column values, etc. Optimization also relies on system statistics, such as the I/O bandwidth of the storage sub-system. All of this information is fed into a cost model. The cost model is used to compute the cost whenever the query optimizer needs to make a decision for an access path, join method, join order, or query transformation. The optimizer picks the alternative that yields the lowest cost. The quality of the final execution plan depends primarily on the quality of the information fed into the cost model as well as the cost model itself. In this paper, we discuss two of the problems that affect the quality of execution plans generated by the query optimizer: the cardinality of intermediate results and host variable values. We will give details of the solutions we introduced in Oracle 11g. Our approach establishes a bridge from the SQL execution engine to the SQL compiler. The bridge brings valuable information to help the query optimizer assess the impact of its decisions and make better decisions for future executions of the SQL statement. We illustrate the merits of our solutions based on experiments using the Oracle E-Business Suite workload.

#index 1127569
#* Towards a streaming SQL standard
#@ Namit Jain;Shailendra Mishra;Anand Srinivasan;Johannes Gehrke;Jennifer Widom;Hari Balakrishnan;Uǧur Çetintemel;Mitch Cherniack;Richard Tibbetts;Stan Zdonik
#t 2008
#c 4
#% 116082
#% 481933
#% 654462
#% 726621
#% 763881
#% 875004
#% 878299
#% 907520
#% 993948
#% 1015324
#% 1016169
#% 1112740
#! This paper describes a unification of two different SQL extensions for streams and its associated semantics. We use the data models from Oracle and StreamBase as our examples. Oracle uses a time-based execution model while StreamBase uses a tuple-based execution model. Time-based execution provides a way to model simultaneity while tuple-based execution provides a way to react to primitive events as soon as they are seen by the system. The result is a new model that gives the user control over the granularity at which one can express simultaneity. Of course, it is possible to ignore simultaneity altogether. The proposed model captures ordering and simultaneity through partial orders on batches of tuples. The batching and the ordering are encapsulated in and can be modified by means of a powerful new operator that we call SPREAD. This paper describes the semantics of SPREAD and gives several examples of its use.

#index 1127570
#* eXtract: a snippet generation system for XML search
#@ Yu Huang;Ziyang Liu;Yi Chen
#t 2008
#c 4
#% 654442
#% 810052
#% 960261
#% 1015258
#% 1016135
#% 1019060
#% 1063493
#! Snippets are used by almost every text search engine to complement ranking schemes in order to effectively handle user keyword search. Despite the fact that XML is a standard representation format of web data, research on generating result snippets for XML search remains untouched. In this work, we present eXtract, a system that efficiently generates self-contained result snippets within a given size bound which effectively summarize the query results and differentiate them from one another, according to which users can quickly assess the relevance of the query results.

#index 1127571
#* Language-integrated querying of XML data in SQL server
#@ James F. Terwilliger;Sergey Melnik;Philip A. Bernstein
#t 2008
#c 4
#% 805866
#% 875029
#% 960309
#% 1088126
#% 1408047
#! Developers need to access persistent XML data programmatically. Object-oriented access is often the preferred method. Translating XML data into objects or vice-versa is a hard problem due to the data model mismatch and the difficulty of query translation. Our prototype addresses this problem by transforming object-based queries and updates into queries and updates on XML using declarative mappings between classes and XML schema types. Our prototype extends the ADO.NET Entity Framework and leverages its object-relational mapping capabilities. We demonstrate how a developer can interact with stored relational and XML data using the Language Integrated Query (LINQ) feature of .NET. We show how LINQ queries are translated into a combination of SQL and XQuery. Finally, we illustrate how explicit mappings facilitate data independence upon database refactoring.

#index 1127572
#* XTCcmp: XQuery compilation on XTC
#@ Christian Mathis;Andreas M. Weiner;Theo Härder;Caesar Ralf Franz Hoppen
#t 2008
#c 4
#% 413564
#% 659999
#% 765407
#% 838542
#% 864401
#% 875009
#% 1408834
#% 1728670
#! XTCcmp, the XQuery Compiler of a native XML database system, extends Starburst's well-known Query Graph Model to serve as an internal representation and basis for query restructuring of XQuery expressions. Furthermore, XTCcmp is able to generate execution plans supporting a wide range of both well-known and newly developed variants of core XML processing algorithms and indexes. Our demo visualizes all rule-based transformation stages, i. e., simplification, algebraic rewriting, and plan generation. Furthermore, via an interface to the extensible rule configuration, it allows interaction with the query compiler to vary configuration parameters and to control the compilation outcome.

#index 1127573
#* Periscope/GQ: a graph querying toolkit
#@ Yuanyuan Tian;Jignesh M. Patel;Viji Nair;Sebastian Martini;Matthias Kretzler
#t 2008
#c 4
#% 378391
#% 481434
#% 711970
#% 765429
#% 810072
#% 937108
#% 1063512
#% 1206703
#! Real life data can often be modeled as graphs, in which nodes represent objects and edges between nodes indicate their relationships. Large graph datasets are common in many emerging applications. Examples span from social networks, biological networks to computer networks. To fully exploit the wealth of information encoded in graphs, systems for managing and analyzing graph data are critical. To address this need, we have designed and developed a graph querying toolkit, called Periscope/GQ. This toolkit is built on top of a traditional RDBMS. It provides a uniform schema for storing graphs in the database and supports various graph query operations, especially sophisticated operations, such as approximate graph matching, large graph alignment and graph summarization. Users can easily combine several operations to perform complex analysis on graphs. In addition, Periscope/GQ employs several novel indexing techniques to speed up query execution. This demonstration will highlight the use of Periscope/GQ in two application domains: life science and social networking.

#index 1127574
#* SEDA: a system for search, exploration, discovery, and analysis of XML Data
#@ Andrey Balmin;Latha Colby;Emiran Curtmola;Quanzhong Li;Fatma Özcan;Sharath Srinivas;Zografoula Vagena
#t 2008
#c 4
#% 107462
#% 333854
#% 397366
#% 1915874
#! Keyword search in XML repositories is a powerful tool for interactive data exploration. Much work has recently been done on making XML search aware of relationship information embedded in XML document structure, but without a clear winner in all data and query scenarios. Furthermore, due to its imprecise nature, search results cannot easily be analyzed and summarized to gain more insights into the data. We address these shortcomings with SEDA: a system for Search, Exploration, Discovery, and Analysis of XML Data. SEDA is based on a paradigm of search and user interaction to help users start with simple keyword-style querying and perform rich analysis of XML data by leveraging both the content and structure of the data. SEDA is an interactive system that allows the user to refine her query iteratively to explore the XML data and discover interesting relationships. SEDA first employs a top-k algorithm to compute the most relevant top-k answers fast, and returns tuples of nodes ranked by relevance. SEDA provides several novel data structures and techniques for efficient top-k computation over graph-structured XML data. SEDA also computes all the contexts in which the query terms are found and all the connection paths that connect the query terms in the XML data. These two summaries enable the user to refine her query by disambiguating the contexts and connections relevant to her query. With the user feedback, the system has enough information to compute all query results, not just the top-k. From the complete results, SEDA automatically deduces a star schema, which is then instantiated with the query results and augmented with additional values required for a well-defined data cube. The tables computed at this step are input into an OLAP engine for further analysis.

#index 1127575
#* Process spaceship: discovering and exploring process views from event logs in data spaces
#@ Hamid Motahari;Boualem Benatallah;Regis Saint-Paul;Fabio Casati;Periklis Andritsos
#t 2008
#c 4
#% 420062
#% 733140
#% 845350
#% 960349
#% 1388891
#! Business processes (BPs) are central to the operation of both public and private organizations. A business process is a set of coordinated tasks and activities to achieve a business objective or goal. Given the importance of BPs to overall efficiency and effectiveness, the competitiveness of organizations hinges on continuous BP improvement. In the nineties, the focus of BP improvement was on automation: workflow management systems (WfMSs) and other middleware technologies were used to reduce cost and improve efficiency by providing better system integration and automated enactment of operational business processes. Recently, the focus of business process has expanded to monitoring, analysis and understanding of business processes, and such techniques are incorporated in business process management systems (BPMSs).

#index 1127576
#* P3N: profiling the potential of a peer-based data management system
#@ Mihai Lupu;Y. C. Tay
#t 2008
#c 4
#% 340175
#% 340176
#% 401980
#% 646237
#% 723453
#% 824706
#% 839329
#% 874970
#% 874978
#% 963874
#% 1027399
#% 1063489
#% 1700057
#! A large number of peer-to-peer (P2P) networks have been introduced in the literature since their popular advent in the late 1990s. In particular, structured P2P overlays have gained much attention since 2001. They are noted mainly for their theoretical properties such as balancing of communication, storage and processing load, as well as elegance of design.

#index 1127577
#* P2P logging and timestamping for reconciliation
#@ Mounir Tlili;W. Kokou Dedzoe;Esther Pacitti;Patrick Valduriez;Reza Akbarinia;Pascal Molli;Gérôme Canals;Stéphane Laurière
#t 2008
#c 4
#% 340175
#% 730961
#% 803470
#% 810034
#% 845563
#% 960251
#% 1703432
#! In this paper, we address data reconciliation in peer-to-peer (P2P) collaborative applications. We propose P2P-LTR (Logging and Timestamping for Reconciliation) which provides P2P logging and timestamping services for P2P reconciliation over a distributed hash table (DHT). While updating at collaborating peers, updates are timestamped and stored in a highly available P2P log. During reconciliation, these updates are retrieved in total order to enforce eventual consistency. In this paper, we first give an overview of P2P-LTR with its model and its main procedures. We then present our prototype used to validate P2P-LTR. To demonstrate P2P-LTR, we propose several scenarios that test our solutions and measure performance. In particular, we demonstrate how P2P-LTR handles the dynamic behavior of peers with respect to the DHT.

#index 1127578
#* AlvisP2P: scalable peer-to-peer text retrieval in a structured P2P network
#@ Toan Luu;Gleb Skobeltsyn;Fabius Klemm;Maroje Puh;Ivana Podnar Žarko;Martin Rajman;Karl Aberer
#t 2008
#c 4
#% 839358
#% 888182
#% 907459
#% 987277
#% 1008878
#% 1055120
#! In this paper we present the AlvisP2P IR engine, which enables efficient retrieval with multi-keyword queries from a global document collection available in a P2P network. In such a network, each peer publishes its local index and invests a part of its local computing resources (storage, CPU, bandwidth) to maintain a fraction of a global P2P index. This investment is rewarded by the network-wide accessibility of the local documents via the global search facility. The AlvisP2P engine uses an optimized overlay network and relies on novel indexing/retrieval mechanisms that ensure low bandwidth consumption, thus enabling unlimited network growth. Our demonstration shows how an easy-to-install AlvisP2P client can be used to join an existing P2P network, index local (text or even multimedia) documents with collection-specific indexing mechanisms, and control access rights to them.

#index 1127579
#* WebContent: efficient P2P Warehousing of web data
#@ S. Abiteboul;T. Allard;P. Chatalic;G. Gardarin;A. Ghitescu;F. Goasdoué;I. Manolescu;B. Nguyen;M. Ouazara;A. Somani;N. Travers;G. Vasile;S. Zoupanos
#t 2008
#c 4
#% 264263
#% 654485
#% 765420
#% 809267
#% 952999
#% 960285
#% 1015327
#% 1116555
#% 1206773
#% 1206796
#% 1396154
#% 1408856
#! We present the WebContent platform for managing distributed repositories of XML and semantic Web data. The platform allows integrating various data processing building blocks (crawling, translation, semantic annotation, full-text search, structured XML querying, and semantic querying), presented as Web services, into a large-scale efficient platform. Calls to various services are combined inside ActiveXML [8] documents, which are XML documents including service calls. An ActiveXML optimizer is used to: (i) efficiently distribute computations among sites; (ii) perform XQuery-specific optimizations by leveraging an algebraic XQuery optimizer; and (iii) given an XML query, chose among several distributed indices the most appropriate in order to answer the query.

#index 1127580
#* DObjects: enabling distributed data services for metacomputing platforms
#@ Pawel Jurczyk;Li Xiong
#t 2008
#c 4
#% 85086
#% 111913
#% 115657
#% 330305
#% 437406
#% 569762
#% 614579
#% 631868
#% 770874
#% 864428
#! Many contemporary applications rely heavily on large scale distributed and heterogeneous data sources. The key constraints for building a distributed data query infrastructure for such applications are: scalability, consistency, heterogeneity, and network and resource dynamics. We designed and developed DObjects, a general-purpose query and data operations infrastructure that can be integrated with metacomputing middleware. This demo proposal describes the architecture and the dynamic query processing functionalities of our data services and shows how they are integrated with a metacomputing framework offering users an open platform for building distributed applications that require access to data integrated from multiple data sources.

#index 1127581
#* EasyTicket: a ticket routing recommendation engine for enterprise problem resolution
#@ Qihong Shao;Yi Chen;Shu Tao;Xifeng Yan;Nikos Anerousis
#t 2008
#c 4
#% 730082
#% 1083691
#! Managing problem tickets is a key issue in IT service industry. A large service provider may handle thousands of problem tickets from its customers on a daily basis. The efficiency of processing these tickets highly depends on ticket routing---transferring problem tickets among expert groups in search of the right resolver to the ticket. Despite that many ticket management systems are available, ticket routing in these systems is still manually operated by support personnel. In this demo, we introduce EasyTicket, a ticket routing recommendation engine that helps automate this process. By mining ticket history data, we model an enterprise social network that represents the functional relationships among various expert groups in ticket routing. Based on this network, our system then provides routing recommendations to new tickets. Our experimental studies on 1.4 million real-world problem tickets show that on average, EasyTicket can improve the efficiency of ticket routing by 35%.

#index 1127582
#* AJAXSearch: crawling, indexing and searching web 2.0 applications
#@ Cristian Duda;Gianni Frey;Donald Kossmann;Chong Zhou
#t 2008
#c 4
#% 387427
#% 397418
#% 654442
#% 875018
#% 993987
#! Current search engines such as Google and Yahoo! are prevalent for searching the Web. Search in dynamic pages, however, is either inexistent or far from perfect. AJAX and Rich Internet Application are such applications. They are increasingly frequent on the Web (in YouTube, Amazon, GMail, Yahoo!Mail) or mobile devices and are offering a high degree of interactivity to the user, by seamlessly loading content from the server without the need to refresh the page. Current search engines cannot correctly index AJAX applications. This produces false positives and false negatives, because search engines do not understand the application logic that loads content dynamically. Crawling an AJAX application is a difficult problem. Since the user invokes events on the page, crawling must identify the different application states generated by the client-side logic. This demo sets the stage for this new type of search and shows that a search engine for AJAX can be built. Among others, the challenges, as opposed to traditional search engines, are: automatically identifying states by triggering events, efficiently crawling application states, avoiding the invocation of potentially very numerous events, scalability in the number of events, duplicate elimination of states, result presentation and aggregation, ranking. The demo presents the AJAX search engine: crawler, indexer and query processor, applied on a real application and showcases challenges and solutions.

#index 1127583
#* ManyAspects: a system for highlighting diverse concepts in documents
#@ Kun Liu;Evimaria Terzi;Tyrone Grandison
#t 2008
#c 4
#% 280835
#% 316520
#% 387791
#% 754078
#% 987208
#% 1077150
#% 1250423
#! We demonstrate ManyAspects -- a document-summarization system that ingests a document and automatically highlights a small set of sentences that are expected to cover the different aspects of the document. The sentences are picked using simple coverage and orthogonality criteria. With ManyAspects, you get a concise yet comprehensive overview of the document without having to spend lots of time drilling down into the details. The system can handle both plain text and syndication feeds (RSS and Atom). It can run either as a stand-alone application or be integrated with Web 2.0 forums to pinpoint different opinions on online discussions for blogs, products, movies, etc. For comparative analysis and exploratory flexibility, the system includes other off-the-shelf text-summarization methods, e.g. k-median clustering and singular value decomposition. Thus, the system allows the user to explore the content of the input document in many different ways.

#index 1127584
#* XTreeNet: democratic community search
#@ Emiran Curtmola;Alin Deutsch;Dionysios Logothetis;K. K. Ramakrishnan;Divesh Srivastava;Kenneth Yocum
#t 2008
#c 4
#% 307424
#% 322884
#% 805475
#% 824762
#% 875018
#% 878916
#% 1022333
#% 1085319
#% 1721852
#% 1849768
#! We describe XTreeNet, a distributed query dissemination engine which facilitates democratization of publishing and efficient data search among members of online communities with powerful full-text queries. This demonstration shows XTreeNet in full action. XTreeNet serves as a proof of concept for democratic community search by proposing a distributed novel infrastructure in which data resides only with the publishers owning it. Expressive user queries are disseminated to publishers. Given the virtual nature of the global data collection (e.g., the union of all local data published in the community) our infrastructure efficiently locates the publishers that contain matching documents with a specified query, processes the complex full-text query at the publisher and returns all relevant documents to querier.

#index 1127585
#* An effective and versatile keyword search engine on heterogenous data sources
#@ Guoliang Li;Jianhua Feng;Jianyong Wang;Lizhu Zhou
#t 2008
#c 4
#% 654442
#% 659990
#% 660011
#% 810052
#% 960259
#% 1015325
#% 1019060
#% 1055787
#% 1055795
#% 1063537
#% 1148419
#% 1397450
#! We present EASE, an effective and versatile keyword search engine that enables users to easily access the heterogenous data composed of unstructured, semi-structured and structured data, without the need of learning XPath/XQuery or SQL languages. EASE addresses a challenge in keyword search that has been neglected in the literature: how to efficiently and adaptively process keyword queries on the heterogenous data. To provide such capability, EASE models unstructured, semi-structured and structured data as graphs, summarizes the graphs, and constructs graph indices instead of using traditional inverted indices for effective keyword search. EASE adopts an extended inverted index to facilitate keyword-based search, and employs a novel ranking mechanism for enhancing search effectiveness.

#index 1127586
#* DBPubs: multidimensional exploration of database publications
#@ Akanksha Baid;Andrey Balmin;Heasoo Hwang;Erik Nijkamp;Jun Rao;Berthold Reinwald;Alkis Simitsis;Yannis Sismanis;Frank van Ham
#t 2008
#c 4
#% 268079
#% 577329
#% 1013546
#% 1016176
#% 1127403
#! DBPubs is a system for effectively analyzing and exploring the content of database publications by combining keyword search with OLAP-style aggregations, navigation, and reporting. DBPubs starts with keyword search over the content of publications. The publications' metadata such as title, authors, venues, year, and so on, provide traditional OLAP static dimensions, which are combined with dynamic dimensions discovered from the content of the publications in the search result, such as frequent phrases, relevant phrases, and topics. We compute publication ranks based on the link structure between documents, i.e., citations, and aggregate them to find seminal papers, discover trends, and rank authors. We deploy an OLAP tool for multidimensional content exploration through traditional OLAP rollup-drilldown operations on the static and dynamic dimensions, solutions for multi-cube analysis, dynamic navigation of the content, and highlighting of interesting dices of the multidimensional content dataspace.

#index 1127587
#* Semandaq: a data quality system based on conditional functional dependencies
#@ Wenfei Fan;Floris Geerts;Xibei Jia
#t 2008
#c 4
#% 480496
#% 480499
#% 752741
#% 810019
#% 810020
#% 1022228
#% 1054480
#% 1661438
#! We present Semandaq, a prototype system for improving the quality of relational data. Based on the recently proposed conditional functional dependencies (CFDs), it detects and repairs errors and inconsistencies that emerge as violations of these constraints. We demonstrate the following functionalities supported by Semandaq: (a) an interface for specifying CFDs; (b) a visual tool for automated detection of CFD violations in relational data, leveraging efficient SQL-based techniques; (c) extensive visual data exploration capabilities that provide the user with various measures of the quality of the data; (d) repair (cleaning) functionality without excess human interaction, built upon CFD-based cleaning algorithms; we show how Semandaq allows for a natural exploration of the quality of the obtained repairs. Semandaq is a promising tool that provides easy access and user-friendly data quality facilities for any relational database system.

#index 1127588
#* RIDE: a tool for interactive source registration in community-oriented information integration
#@ Yannis Katsis;Alin Deutsch;Yannis Papakonstantinou;Keliang Zhao
#t 2008
#c 4
#% 765432
#% 826032
#% 1127371
#! Modern Internet communities need to integrate and query structured information. Employing current information integration infrastructure, data integration is still a very costly effort, since source registration is performed by a central authority which becomes a bottleneck. We propose the community-based integration paradigm which pushes the source registration task to the independent community members. This creates new challenges caused by each member's lack of a global overview on how her data interacts with the application queries of the community and the data from other sources. How can the source owner maximize the visibility of her data to existing applications, while minimizing the clean-up and reformatting cost associated with publishing? Does her data contradict (or could it contradict in the future) the data of other sources?

#index 1127589
#* Comparing and evaluating mapping systems with STBenchmark
#@ Bogdan Alexe;Wang-Chiew Tan;Yannis Velegrakis
#t 2008
#c 4
#% 745518
#% 800550
#% 810078
#% 814647
#% 903009
#% 993981
#% 994015
#% 1044442
#% 1206614
#% 1206803
#! Schema mappings are fundamental building blocks in many information integration applications. Designing mappings is a time-consuming process and for that reason many mapping systems have been developed to assist in the task of designing mappings. However, to the best of our knowledge, a benchmark for comparing and evaluating these systems has not yet been developed. We demonstrate STBenchmark, a benchmark that we have developed for evaluating mapping systems. Our demonstration will showcase the different aspects of mapping systems that STBenchmark evaluates, highlight the results of our comparison and evaluation of four mapping systems, as well as make a case for the need for a standard specification input mechanism to mapping systems in order to make progress towards the development of a uniform testbed or repository for schema mappings and data exchange tasks.

#index 1127590
#* Ad-hoc data processing in the cloud
#@ Dionysios Logothetis;Kenneth Yocum
#t 2008
#c 4
#% 115661
#% 794132
#% 805475
#% 810008
#% 960326
#% 963669
#% 983467
#% 993998
#% 1085319
#! Ad-hoc data processing has proven to be a critical paradigm for Internet companies processing large volumes of unstructured data. However, the emergence of cloud-based computing, where storage and CPU are outsourced to multiple third-parties across the globe, implies large collections of highly distributed and continuously evolving data. Our demonstration combines the power and simplicity of the MapReduce abstraction with a wide-scale distributed stream processor, Mortar. While our incremental MapReduce operators avoid data re-processing, the stream processor manages the placement and physical data flow of the operators across the wide area. We demonstrate a distributed web indexing engine against which users can submit and deploy continuous MapReduce jobs. A visualization component illustrates both the incremental indexing and index searches in real time.

#index 1127591
#* Large-scale collaborative analysis and extraction of web data
#@ Felix Weigel;Biswanath Panda;Mirek Riedewald;Johannes Gehrke;Manuel Calimlim
#t 2008
#c 4
#% 954300
#% 963669
#% 983467
#% 1063553
#! Archived web data is a great resource for scientific research, but poses serious challenges in data processing and management. We demonstrate the Web Lab Collaboration Server, a platform and service for large-scale collaborative web data analysis in a distributed computing environment, and show how it seamlessly supports non-technical users during search, data extraction and analysis.

#index 1127592
#* Making SENSE: socially enhanced search and exploration
#@ Tom Crecelius;Mouna Kacimi;Sebastian Michel;Thomas Neumann;Josiane Xavier Parreira;Ralf Schenkel;Gerhard Weikum
#t 2008
#c 4
#% 169781
#% 643566
#% 768903
#% 818232
#% 855601
#% 879611
#% 956544
#% 967452
#% 1006349
#% 1019186
#% 1074116
#! Online communities like Flickr, del.icio.us and YouTube have established themselves as very popular and powerful services for publishing and searching contents, but also for identifying other users who share similar interests. In these communities, data are usually annotated with carefully selected and often semantically meaningful tags, collaboratively chosen by the user who uploaded an item and other users who came across the item. Items like urls or videos are typically retrieved by issueing queries that consist of a set of tags, returning items that have been frequently annotated with these tags. However, users often prefer a more personalized way of searching over such a 'global' search, exploiting preferences of and connections between users. The SENSE system presented in this demo supports hybrid personalization along two dimensions: in the social dimension, a search process is focused towards items tagged by users explicitly selected as friends by the querying user, whereas in the spiritual dimension, users that share preferences with the querying user are preferred. Orthorgonal to this, the system additionally integrates semantic expansion of query tags to improve search results. SENSE provides an efficient top-k algorithm that dynamically expands the search to related users and tags. It is based on principles of threshold algorithms, folding related users and tags into the search space in an incremental on-demand manner, thus visiting only a small fraction of the social network when evaluating a query. The demonstration uses three different real-world datasets: a large set of urls from del.icio.us, a large set of pictures from Flickr, and a large set of books from librarything, each together with a large fraction of the corresponding social network of these sites.

#index 1127593
#* AuditGuard: a system for database auditing under retention restrictions
#@ Wentian Lu;Gerome Miklau
#t 2008
#c 4
#% 663
#% 43028
#% 94459
#% 287246
#% 287268
#% 442781
#% 449857
#% 479621
#% 480934
#% 664915
#! Auditing the changes to a database is critical for identifying malicious behavior, maintaining data quality, and improving system performance. But an accurate audit log is a historical record of the past that can also pose a serious threat to privacy. In many domains, retention policies govern how long data can be preserved by an institution. Regulations like FERPA and HIPAA (in the U.S.) or the Directive of Data Protection (in the EU), require strict retention periods to be observed, mandating the disposal of past data. In addition, institutions often adopt their own retention policies, choosing to remove sensitive data after a period of time to avoid its unintended release, or to avoid disclosure that could be forced by subpeona.

#index 1127594
#* QueryScope: visualizing queries for repeatable database tuning
#@ Ling Hu;Kenneth A. Ross;Yuan-Chi Chang;Christian A. Lang;Donghui Zhang
#t 2008
#c 4
#% 68659
#% 401436
#% 480670
#% 810026
#% 820356
#% 893130
#% 1016220
#% 1022202
#! Reading and perceiving complex SQL queries has been a time consuming task in traditional database applications for decades. When it comes to decision support systems with automatically generated and sometimes highly nested SQL queries, human understanding or tuning of these workloads becomes even more challenging. This demonstration explores visualization methods to represent queries as graphs. We developed the QueryScope tool to help visualize and understand critical elements of a query, thereby cutting down the learning curve. We show how the tool allows the user to drill down on particular queries or to find similarly structured queries that may exhibit similar tuning opportunities. The queries shown in the demonstration are taken from real tuning engagements.

#index 1127595
#* When is it time to rethink the aggregate configuration of your OLAP server?
#@ Katja Hose;Daniel Klan;Matthias Marx;Kai-Uwe Sattler
#t 2008
#c 4
#% 210182
#% 480153
#% 480158
#% 482100
#% 893130
#% 1015367
#% 1016220
#% 1207102
#! OLAP servers based on relational backends typically exploit materialized aggregate tables to improve response times of complex analytical queries. One of the key problems in this context is the view selection problem: choosing the optimal set of aggregation tables (called configuration) for a given workload. In this paper, we present a system that continuously monitors the workload and raises a quantified alert, when a better configuration is available. We address the tasks of query monitoring and view selection at the OLAP level instead of the SQL level, which simplifies the containment checks as well as rewriting and in this way helps to reduce the complexity of the backend system. At the demo we plan to show how our system works, i.e., how the system reacts upon arbitrary (interactive) workloads and how the user is alerted that a better configuration is available.

#index 1127596
#* H-store: a high-performance, distributed main memory transaction processing system
#@ Robert Kallman;Hideaki Kimura;Jonathan Natkins;Andrew Pavlo;Alexander Rasin;Stanley Zdonik;Evan P. C. Jones;Samuel Madden;Michael Stonebraker;Yang Zhang;John Hugg;Daniel J. Abadi
#t 2008
#c 4
#% 287647
#% 340663
#% 469794
#% 479920
#% 481454
#% 800491
#% 824697
#% 864486
#% 1022298
#! Our previous work has shown that architectural and application shifts have resulted in modern OLTP databases increasingly falling short of optimal performance [10]. In particular, the availability of multiple-cores, the abundance of main memory, the lack of user stalls, and the dominant use of stored procedures are factors that portend a clean-slate redesign of RDBMSs. This previous work showed that such a redesign has the potential to outperform legacy OLTP databases by a significant factor. These results, however, were obtained using a bare-bones prototype that was developed just to demonstrate the potential of such a system. We have since set out to design a more complete execution platform, and to implement some of the ideas presented in the original paper. Our demonstration presented here provides insight on the development of a distributed main memory OLTP database and allows for the further study of the challenges inherent in this operating environment.

#index 1127597
#* Organizing and indexing non-convex regions
#@ Eric Perlman;Randal Burns;Michael Kazhdan
#t 2008
#c 4
#% 818938
#! We demonstrate data indexing and query processing techniques that improve the efficiency of comparing, correlating, and joining data contained in non-convex regions. We use computational geometry techniques to automatically characterize the region of space from which data are drawn, partition the region based on that characterization, and create an index from the partitions. Our motivating application performs distributed data analysis queries among federated database sites that store scientific data sets from the Chesapeake Bay. Our preliminary findings indicate that these techniques often reduce the number of I/Os needed to serve a query by a factor of five---depending on the geometry of the query region.

#index 1127598
#* Capri/MR: exploring protein databases from a structural and physicochemical point of view
#@ Eric Paquet;Herna L Viktor
#t 2008
#c 4
#% 883401
#% 905646
#! With the advent of high throughput systems to experimentally determine the three-dimensional (3-D) structure of proteins, molecular biologists are in urgent need of systems to automatically store, maintain and explore the vast structural databases that are thus being created. We have designed and implemented the Capri/MR system which makes it possible to identify families of protein structures, as contained in such very large 3-D protein structure databases. Our system is able to automatically index and search a database of proteins by three-dimensional shape, structural and/or physicochemical properties. For each of these diverse protein structure representations, we create a compact rotation and translation invariant index (or signature) which is placed in a database for future querying. A similarity search algorithm performs an exhaustive search against the entire database. Our search algorithm takes advantage of the compact signatures to rapidly find protein structures that are similar in 3-D shape and/or two-dimensional (2-D) properties. As a result, queries in our Capri/MR system run within a fraction of a second, and we are able to accurately group protein structures into the correct families, with very high precision and recall. In addition, our system dynamically processes new protein structures as they become available. We demonstrate the power of Capri/MR against the Protein Data Bank, which contains all known, experimentally determined, 3-D protein structures (48.000 as of January 2008). The main applications of our Capri/MR system lie in structural proteomics, protein evolution and mutation, as well as in drug design, in particular for studying the docking problem and the computer aided design of non-toxic drugs.

#index 1127599
#* C-DEM: a multi-modal query system for Drosophila Embryo databases
#@ Fan Guo;Lei Li;Christos Faloutsos;Eric P. Xing
#t 2008
#c 4
#% 169940
#% 769952
#% 881496
#% 881536
#% 915344
#% 1016175
#% 1016176
#! The amount of biological data publicly available has experienced an exponential growth as the technology advances. Online databases are now playing an important role as information repositories as well as easily accessible platforms for researchers to communicate and contribute. Recent research projects in image bioinformatics produce a number of databases of images, which visualize the spatial expression pattern of a gene (eg. "fj"), and most of which also have one or several annotation keywords (eg., "embryonic hindgut"). C-DEM is an online system for Drosophila (= fruit-fly) Embryo images Mining. It supports queries from all three modalities to all three, namely, (a) genes, (b) images of gene expression, and (c) annotation keywords of the images. Thus, it can find images that are similar to a given image, and/or related to the desirable annotation keywords, and/or related to specific genes. Typical queries are what are most suitable keywords to assign to image insitu28465.jpg or find images that are related to gene "fj", and to the keyword "embryonic hindgut". C-DEM uses state-of-the-art feature extraction methods for images (wavelets and principal component analysis). It envisions the whole database as a tri-partite graph (one type for each modality), and it uses fast and flexible proximity measures, namely, random walk with restarts (RWR). In addition to flexible querying, C-DEM allows for navigation: the user can click on the results of an earlier query (image thumbnails and/or keywords and/or genes), and the system will report the most related images (and keywords, and genes). The demo is on a real Drosophila Embryo database, with 10,204 images, 2,969 distinct genes, and 113 annotation keywords. The query response time is below one second on a commodity desktop.

#index 1127600
#* Querying and monitoring distributed business processes
#@ Tova Milo;Daniel Deutch
#t 2008
#c 4
#% 86957
#% 114677
#% 263438
#% 297770
#% 345693
#% 442960
#% 479806
#% 557919
#% 654476
#% 654485
#% 655355
#% 762652
#% 800609
#% 809234
#% 809267
#% 810053
#% 814648
#% 824673
#% 824692
#% 824702
#% 878299
#% 893117
#% 994005
#% 1015373
#% 1022252
#% 1022253
#% 1022254
#% 1127379
#% 1408536
#% 1711212
#! A business process (BP for short) consists of a group of business activities undertaken by one or more organizations in pursuit of some particular goal. It usually operates in a cross-organization, distributed environment and the software implementing it is fairly complex. Standards facilitate the design, deployment, and execution of BPs. In particular, the recent BPEL standard (Business Process Execution Language), provides an XML-based language to describe the interface between the participants in a process, as well as the full operational logic of the process and its execution flow. BPEL specifications are automatically compiled into executable code that implements the described BP and runs on a BPEL application server. Processes execution is traced, and their run-time behavior can be recorded in standard XML formats.

#index 1127601
#* A first tutorial on dataspaces
#@ Michael Franklin;Alon Halevy;David Maier
#t 2008
#c 4
#% 435615
#% 730829
#% 824741
#% 845350
#% 874876
#% 1022257
#% 1022259
#% 1022289
#% 1063533
#% 1063534
#% 1127393
#% 1127557
#% 1207213
#! Dataspace systems offer services on data without requiring upfront semantic integration. In sharp contrast with existing information-integration systems, dataspaces systems offer best-effort answers even before semantic mappings are provided to the system. Dataspaces offer a pay-as-you-go approach to data management. Users (or administrators) of the system decide where and when it is worthwhile to invest more effort in identifying semantic relationships. As such, dataspaces offer services on the data in place, without losing the context surrounding the data.

#index 1127602
#* Ontologies and databases: myths and challenges
#@ Enrico Franconi
#t 2008
#c 4
#% 237189
#% 248026
#% 303884
#% 665856
#% 809238
#% 992962
#! After an introduction where the notion of ontology will be introduced in a rigorous way as a set of constraints over legal database instances playing the role of a conceptual model or of a set of dependencies, the tutorial will be divided in three parts. In the first part, we will discuss, mostly driven by examples, the role of ontologies in information systems design and the advantages and the challenges when adopting a formal approach based on logics. In the second part, a real-world tool for ontology design will be used to see the "logic" in action. In the third and most important part, we will discuss the use for information access of ontologies in data intensive scenarios based on database technologies, based on a scenario where a logic-based ontology mediates between the user information need and the data structured in the source database. The tutorial will emphasise the advantages of adopting a logic-based approach to the use of ontologies in data intensive applications, and the challenges that the research should still face to make this approach feasible and scalable in association with current database technology. The audience can be of both database researchers and practitioners, since I will try to explain all the concepts through examples and the central demo, although the concepts that the audience will at the end get will be non-trivial ones. The main goal of this tutorial is to let the audience understand haw these novel technologies are non-trivial, but useful in perspective and worthwhile researching; I will also show that there are already some data intensive scenarios where a rigorous ontology-based approach is already applicable with success; at the same time, I will warn about the wrong usages of ontology-based technologies.

#index 1127603
#* Systems aspects of probabilistic data management
#@ Magdalena Balazinska;Christopher Ré;Dan Suciu
#t 2008
#c 4
#% 654487
#% 864417
#% 893102
#% 893121
#% 893168
#% 991156
#% 1016178
#% 1016201
#% 1022203
#% 1022205
#% 1022206
#% 1022259
#% 1036075
#% 1063520
#% 1063523
#% 1206732
#% 1206772
#! There has been a wide interest recently in managing probabilistic data [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]. But in order to follow the rich literature on probabilistic databases one is often required to take a detour into probability theory, correlations, conditionals, Monte Carlo simulations, error bounds, topics that have been studied extensively in several areas of Computer Science and Mathematics. Because of that, it is often difficult to get to the algorithmic and systems level aspects of probabilistic data management. In this tutorial, we will distill these aspects from the, often theory-heavy literature on probabilistic databases. We will start by describing a real application at the University of Washington, using the RFID Ecosystem; we will show how probabilities arise naturally, and why we need to cope with them. We will then describe what an implementor needs to know to process SQL queries on probabilistic databases. In the second half of the tutorial, we will discuss more advanced issues, such as event processing over probabilistic streams, and views over probabilistic data.

#index 1127604
#* A revival of integrity constraints for data cleaning
#@ Wenfei Fan;Floris Geerts;Xibei Jia
#t 2008
#c 4
#% 273687
#% 491695
#% 903332
#% 1022222
#% 1022228
#% 1054480
#% 1127587
#% 1206764
#% 1661426
#! Integrity constraints, a.k.a. data dependencies, are being widely used for improving the quality of schema. Recently constraints have enjoyed a revival for improving the quality of data. The tutorial aims to provide an overview of recent advances in constraint-based data cleaning.

#index 1127605
#* XML Structural Summaries
#@ Mirella M. Moro;Zografoula Vagena;Vassilis J. Tsotras
#t 2008
#c 4
#% 289010
#% 397359
#% 462062
#% 462235
#% 464724
#% 479465
#% 783546
#% 810046
#% 824662
#% 869671
#% 1016180
#% 1915871
#! This tutorial introduces the concept of XML Structural Summaries and describes their role within XML retrieval. It covers the usage of those summaries for Database-style query processing and Information Retrieval-style search tasks in the context of both centralized and distributed environments. Finally, it discusses new retrieval scenarios that can potentially be favorably supported by those summaries.

#index 1127606
#* Scheduling continuous queries in data stream management systems
#@ Mohamed A. Sharaf;Alexandros Labrinidis;Panos K. Chrysanthis
#t 2008
#c 4
#% 1026964
#! Recently, several policies have been proposed for scheduling multiple Continuous Queries (CQs) in a Data Stream Management System (DSMS). The decision on which policy to use plays an important role in shaping the percieved online performance provided by the DSMS. In this tutorial, we provide an overview of different policies employed by current CQ schedulers and the performance goals optimized by these policies. Further, we discuss the salient properties of CQs conisdered by current policies as well as the efficent implementation of such policies into CQ schedulers. Finally, we present future research directions and open problems in CQ scheduling.

#index 1127607
#* Detecting clusters in moderate-to-high dimensional data: subspace clustering, pattern-based clustering, and correlation clustering
#@ Hans-Peter Kriegel;Peer Kr&#246/ger;Arthur Zimek
#t 2008
#c 4
#% 248792
#% 273891
#% 300131
#% 397382
#% 469422
#% 480132
#% 727908
#% 765439
#% 765518
#% 778215
#% 844313
#% 881456
#% 1408779
#! As a prolific research area in data mining, subspace clustering and related problems induced a vast amount of proposed solutions. However, many publications compare a new proposition -- if at all -- with one or two competitors or even with a so called "naïve" ad hoc solution but fail to clarify the exact problem definition. As a consequence, even if two solutions are thoroughly compared experimentally, it will often remain unclear whether both solutions tackle the same problem or, if they do, whether they agree in certain tacit assumptions and how such assumptions may influence the outcome of an algorithm. In this tutorial, we try to clarify (i) the different problem definitions related to subspace clustering in general, (ii) the specific difficulties encountered in this field of research, (iii) the varying assumptions, heuristics, and intuitions forming the basis of different approaches, and (iv) how several prominent solutions essentially tackle different problems.

#index 1127608
#* Finding frequent items in data streams
#@ Graham Cormode;Marios Hadjieleftheriou
#t 2008
#c 4
#% 214073
#% 303036
#% 333931
#% 379445
#% 404598
#% 492912
#% 548479
#% 569754
#% 749505
#% 765404
#% 783740
#% 801696
#% 816392
#% 847112
#% 874903
#% 874906
#% 954300
#% 960249
#% 960254
#% 977008
#% 991154
#% 993960
#% 993969
#% 1019786
#% 1044487
#% 1206633
#% 1207124
#% 1700144
#% 1713039
#! The frequent items problem is to process a stream of items and find all items occurring more than a given fraction of the time. It is one of the most heavily studied problems in data stream mining, dating back to the 1980s. Many applications rely directly or indirectly on finding the frequent items, and implementations are in use in large scale industrial systems. However, there has not been much comparison of the different methods under uniform experimental conditions. It is common to find papers touching on this topic in which important related work is mischaracterized, overlooked, or reinvented. In this paper, we aim to present the most important algorithms for this problem in a common framework. We have created baseline implementations of the algorithms, and used these to perform a thorough experimental study of their properties. We give empirical evidence that there is considerable variation in the performance of frequent items algorithms. The best methods can be implemented to find frequent items with high accuracy using only tens of kilobytes of memory, at rates of millions of items per second on cheap modern hardware.

#index 1127609
#* Querying and mining of time series data: experimental comparison of representations and distance measures
#@ Hui Ding;Goce Trajcevski;Peter Scheuermann;Xiaoyue Wang;Eamonn Keogh
#t 2008
#c 4
#% 172949
#% 227924
#% 316560
#% 333941
#% 420065
#% 462231
#% 480146
#% 564263
#% 618611
#% 643518
#% 654456
#% 659971
#% 729954
#% 765451
#% 795273
#% 810049
#% 818916
#% 835018
#% 853064
#% 876074
#% 878302
#% 893220
#% 960281
#% 992857
#% 993965
#% 1016195
#% 1022238
#% 1046749
#% 1290045
#! The last decade has witnessed a tremendous growths of interests in applications that deal with querying and mining of time series data. Numerous representation methods for dimensionality reduction and similarity measures geared towards time series have been introduced. Each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive set of time series experiments re-implementing 8 different representation methods and 9 similarity measures and their variants, and testing their effectiveness on 38 time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. Our experiments have provided both a unified validation of some of the existing achievements, and in some cases, suggested that certain claims in the literature may be unduly optimistic.

#index 1127610
#* Column-store support for RDF data management: not all swans are white
#@ Lefteris Sidirourgos;Romulo Goncalves;Martin Kersten;Niels Nes;Stefan Manegold
#t 2008
#c 4
#% 519567
#% 824697
#% 824755
#% 1022236
#! This paper reports on the results of an independent evaluation of the techniques presented in the VLDB 2007 paper "Scalable Semantic Web Data Management Using Vertical Partitioning", authored by D. Abadi, A. Marcus, S. R. Madden, and K. Hollenbach [1]. We revisit the proposed benchmark and examine both the data and query space coverage. The benchmark is extended to cover a larger portion of the query space in a canonical way. Repeatability of the experiments is assessed using the code base obtained from the authors. Inspired by the proposed vertically-partitioned storage solution for RDF data and the performance figures using a column-store, we conduct a complementary analysis of state-of-the-art RDF storage solutions. To this end, we employ MonetDB/SQL, a fully-functional open source column-store, and a well-known -- for its performance -- commercial row-store DBMS. We implement two relational RDF storage solutions -- triple-store and vertically-partitioned -- in both systems. This allows us to expand the scope of [1] with the performance characterization along both dimensions -- triple-store vs. vertically-partitioned and row-store vs. column-store -- individually, before analyzing their combined effects. A detailed report of the experimental test-bed, as well as an in-depth analysis of the parameters involved, clarify the scope of the solution originally presented and position the results in a broader context by covering more systems.

#index 1127611
#* Prefix based numbering schemes for XML: techniques, applications and performances
#@ Virginie Sans;Dominique Laurent
#t 2008
#c 4
#% 39637
#% 51391
#% 58365
#% 378412
#% 397358
#% 397366
#% 480489
#% 487270
#% 598374
#% 654450
#% 730080
#% 736016
#% 745479
#% 765488
#% 778943
#% 783594
#% 800523
#% 823655
#% 858178
#% 882474
#% 917698
#% 961712
#% 1712557
#% 1721248
#! Commonly used in network routing, programming, classification and knowledge representation systems, labeling schemes have also interested the XML community. We thus motivate and describe numbering schemes, their applications, and the trade off between storage capacities and runtime performance. We present a taxonomy of numbering schemes for XML based on the types of supported queries (ancestor, adjacent, etc), the encoding technique, and whether the scheme offers robustness properties according to updates. We describe some of the numbering techniques proposed for XML. We focus on prefix-based schemes. We give a qualitative comparison of the existing numbering schemes, discussing their advantages and drawbacks. Then, we compare their storage requirement and performances. Finally, we consider the new research directions that are likely to benefit from numbering scheme techniques.

#index 1127612
#* A benchmark for evaluating moving object indexes
#@ Su Chen;Christian S. Jensen;Dan Lin
#t 2008
#c 4
#% 68091
#% 86950
#% 300174
#% 421124
#% 427199
#% 554884
#% 554918
#% 765454
#% 767990
#% 864410
#% 870306
#% 1015305
#% 1015320
#% 1016193
#% 1022251
#% 1046510
#% 1063471
#% 1072641
#% 1669479
#! Progress in science and engineering relies on the ability to measure, reliably and in detail, pertinent properties of artifacts under design. Progress in the area of database-index design thus relies on empirical studies based on prototype implementations of indexes. This paper proposes a benchmark that targets techniques for the indexing of the current and near-future positions of moving objects. This benchmark enables the comparison of existing and future indexing techniques. It covers important aspects of such indexes that have not previously been covered by any benchmark. Notable aspects covered include update efficiency, query efficiency, concurrency control, and storage requirements. Next, the paper applies the benchmark to half a dozen notable moving-object indexes, thus demonstrating the viability of the benchmark and offering new insight into the performance properties of the indexes.

#index 1127613
#* Dwarfs in the rearview mirror: how big are they really?
#@ Jens Dittrich;Lukas Blunschi;Marcos Antonio Vaz Salles
#t 2008
#c 4
#% 172913
#% 199537
#% 201951
#% 223781
#% 252608
#% 273916
#% 286258
#% 287349
#% 397388
#% 462044
#% 462204
#% 464215
#% 479450
#% 654446
#% 729576
#% 765154
#% 824738
#% 893176
#% 1016174
#% 1698965
#! Online-Analytical Processing (OLAP) has been a field of competing technologies for the past ten years. One of the still unsolved challenges of OLAP is how to provide quick response times on any Terabyte-sized business data problem. Recently, a very clever multi-dimensional index structure termed Dwarf [26] has been proposed offering excellent query response times as well as unmatched index compression rates. The proposed index seems to scale well for both large data sets as well as high dimensions. Motivated by these surprisingly excellent results, we take a look into the rearview mirror. We have re-implemented the Dwarf index from scratch and make three contributions. First, we successfully repeat several of the experiments of the original paper. Second, we substantially correct some of the experimental results reported by the inventors. Some of our results differ by orders of magnitude. To better understand these differences, we provide additional experiments that better explain the behavior of the Dwarf index. Third, we provide missing experiments comparing Dwarf to baseline query processing strategies. This should give practitioners a better guideline to understand for which cases Dwarf indexes could be useful in practice.

#index 1127614
#* Challenges and techniques for effective and efficient similarity search in large video databases
#@ Jie Shao;Heng Tao Shen;Xiaofang Zhou
#t 2008
#c 4
#% 36117
#% 172949
#% 342828
#% 479649
#% 479973
#% 480307
#% 481956
#% 632042
#% 659971
#% 722798
#% 765163
#% 782526
#% 810069
#% 814646
#% 900255
#% 993965
#% 997096
#% 1016195
#% 1022329
#% 1712711
#% 1727326
#% 1775212
#% 1775726
#% 1858015
#% 1858257
#% 1858486
#! Searching relevant visual information based on content features in large databases is an interesting and changeling topic that has drawn lots of attention from both the research community and industry. This paper gives an overview of our investigations on effective and efficient video similarity search. We briefly introduce some novel techniques developed for two specific tasks studied in this PhD project: video retrieval in a large collection of segmented video clips, and video subsequence identification from a long unsegmented stream. The proposed methods for processing these two types of similarity queries have shown encouraging performance and are being incorporated into our prototype system of video search named UQLIPS, which has demonstrated some marketing potentials for commercialisation.

#index 1127615
#* Studying interaction methodologies in video retrieval
#@ Frank Hopfgartner
#t 2008
#c 4
#% 157521
#% 248225
#% 270279
#% 308739
#% 320432
#% 609107
#% 731615
#% 763590
#% 766454
#% 840424
#% 879567
#% 903632
#% 987212
#% 996182
#% 1033720
#% 1415725
#! So far, several approaches have been studied to bridge the problem of the Semantic Gap, the bottleneck in image and video retrieval. However, no approach is successful enough to increase retrieval performances significantly. One reason is the lack of understanding the user's interest, a major condition towards adapting results to a user. This is partly due to the lack of appropriate interfaces and the missing knowledge of how to interpret user's actions with these interfaces. In this paper, we propose to study the importance of various implicit indicators of relevance. Furthermore, we propose to investigate how this implicit feedback can be combined with static user profiles towards an adaptive video retrieval model.

#index 1127616
#* Mining patterns and rules for software specification discovery
#@ David Lo;Siau-Cheng Khoo
#t 2008
#c 4
#% 4614
#% 68696
#% 258498
#% 297770
#% 302788
#% 339034
#% 343052
#% 347810
#% 374518
#% 420063
#% 433476
#% 463903
#% 464996
#% 477985
#% 577243
#% 655134
#% 745515
#% 807078
#% 810060
#% 868127
#% 902158
#% 906081
#% 989617
#% 1422708
#% 1707663
#% 1707664
#! Software specifications are often lacking, incomplete and outdated in the industry. Lack and incomplete specifications cause various software engineering problems. Studies have shown that program comprehension takes up to 45% of software development costs. One of the root causes of the high cost is the lack-of documented specification. Also, outdated and incomplete specification might potentially cause bugs and compatibility issues. In this paper, we describe novel data mining techniques to mine or reverse engineer these specifications from the pool of software engineering data. A large amount of software data is available for analysis. One form of software data is program execution traces. A program trace can be viewed as a sequence of events collected when a program is run. A set of program traces in turn can be viewed as a sequence database. In this paper, we present some novel work in mining software specifications by employing novel pattern mining and rule mining techniques. Performance studies show the scalability of our technique. Case studies on traces of a real industrial application show the utility of our technique in recovering program specifications from execution traces.

#index 1127617
#* Towards efficient main-memory use for optimum tree index update
#@ Laurynas Biveinis;Simonas Šaltenis
#t 2008
#c 4
#% 4683
#% 86950
#% 208047
#% 287672
#% 300174
#% 427199
#% 479769
#% 480100
#% 480119
#% 480651
#% 481304
#% 481599
#% 503869
#% 527174
#% 567865
#% 580978
#% 593968
#% 800186
#% 810025
#% 838505
#% 857498
#% 864410
#% 866987
#% 870307
#% 1015305
#% 1022251
#! An emerging class of database applications is characterized by frequent updates of low-dimensional data, e.g. coming from sensors that sample continuous real world phenomena. Traditional persistency requirements can be weakened in this setting of frequent updates, emphasizing a role of the main-memory in external storage index structures and enabling a higher update throughput. Moreover, in order for an index to be suitable for practical applications, efficient past-state queries should be supported without significantly penalizing other operations. These issues are not adequately addressed in the database research. We report on the RR-tree---our first step towards resolving them. Based on this, we outline a number of concrete short-term and more abstract longer-term future research directions.

#index 1127618
#* Implementing filesystems by tree-aware DBMSs
#@ Alexander Holupirek;Marc H. Scholl
#t 2008
#c 4
#% 107693
#% 172939
#% 267218
#% 345714
#% 397358
#% 726086
#% 735989
#% 810108
#% 824777
#% 845350
#% 869609
#% 874876
#% 875010
#% 881741
#% 960318
#% 960362
#% 978809
#% 1016150
#! With the rise of XML, the database community has been challenged by semi-structured data processing. Since the data type behind XML is the tree, state-of-the-art RDBMSs have learned to deal with such data (e. g., [18, 5, 6, 16]). This paper introduces a Ph.D. project focused on the question in how far the tree-awareness of recent RDBMSs can be used to, once again, try to implement filesystems using database technology. Our main goal is to provide means to query the data stored in filesystems and to find ways to enhance/combine the data storage and query capabilities of operating systems using semi-structured database technology. Two DBMSs with relational XML storage, built on top of the XPath accelerator numbering scheme [14], are the foundations for our work. With BaseX, an XML database, we establish a link between user, database and filesystem content. BaseX allows visual access to filesystem data stored in the database. An integrated query interface allows users to filter query results in interactive response time. Second, we establish a link between DBMS and OS. We implement a filesystem in userspace backed by the MonetDB/XQuery system, a well-known relational database system, which integrates the Pathfinder XQuery compiler [5] and the MonetDB kernel [4]. As a result, the DBMS is mounted as a conventional filesystem by the operating system kernel. Consequently, access via the established (virtual) filesystem interface as well as database enhanced access to the same data is provided.

#index 1127619
#* Adaptive workflow scheduling under resource allocation constraints and network dynamics
#@ Artin Avanes;Johann-Christoph Freytag
#t 2008
#c 4
#% 171056
#% 248013
#% 273709
#% 275349
#% 345694
#% 461899
#% 481261
#% 509536
#% 562155
#% 571867
#% 631985
#% 637510
#% 720899
#% 897347
#% 993989
#% 1181012
#! Workflow concepts are well suited for scenarios where many distributed entities work collaboratively together to achieve a common goal. Today, workflows are mostly used as computerized model for business processes executed in instances in commercial Workflow Management Systems. However, there are many other application domains where computer-supported cooperative work can be captured and organized by workflows. In this paper, we investigate the task of scheduling workflows in self-organizing wireless networks for disaster scenarios. Most research work in the field of workflow scheduling has been driven by temporal and causality constraints. We present an adaptive scheduling algorithm that finds a suitable execution sequence for workflow activities by additionally considering resource allocation constraints and dynamic topology changes. Our approach utilizes a multi-stage distribution algorithm which we extend with techniques to cope with network dynamics.

#index 1127620
#* Privacy preserving document indexing infrastructure for a distributed environment
#@ Sergej Zerr;Wolfgang Nejdl
#t 2008
#c 4
#% 151495
#% 340175
#% 397367
#% 433922
#% 577239
#% 664705
#% 800514
#% 800515
#% 810254
#% 830694
#% 864406
#% 864412
#% 963446
#% 1015329
#% 1015331
#% 1044459
#! To carry out work assignments, small groups distributed within a larger enterprise or collaborative community often need to share documents among themselves while shielding those documents from others' eyes. In this situation, users need an indexing facility that can quickly locate relevant documents that they are allowed to access, without (1) leaking information about the remaining documents, (2) imposing a large management burden as users, groups, and documents evolve, or (3) requiring users to agree on a central completely trusted authority. In order to achieve this aim user access levels and access control have to be reflected in the index structures and/or retrieval algorithms as well as in ranking the search results. My Ph.D. work focuses on building up an indexing infrastructure which supports confidential indexing, sharing and retrieval of unstructured information which is spread over a number of distributed access-controlled collections. In order to allow for effective and efficient indexing and retrieval in these settings, it considers aspects of confidentiality preservation within an outsourced inverted index, a DHT index structure in P2P networks as well as confidential top-k information retrieval.

#index 1127621
#* GS-TMS: a global stream-based threat monitor system
#@ Jiajia Miao
#t 2008
#c 4
#% 378388
#% 535187
#% 654497
#% 654510
#% 824652
#% 824737
#% 824767
#% 864443
#% 874994
#% 963629
#% 963799
#% 1033628
#% 1279214
#! Computer networks have become ubiquitous and integral part of the nation's critical infrastructure. How to grasp the real-time overall situation of the network security is very noteworthy to study. Current network security systems make great contributions in enhancing the network security. Nevertheless, these products are independent and autonomous, so they fail to share the results of the detected attacks. Consequently, such solutions cannot figure out an overview of the network security situation. In another perspective, building a new global monitoring system from scratch will suffer from redundant construction, more cost, and longer deploying time. To address the dilemma, we propose a novel solution called GS-TMS which reuses the log data generated by the existing widely-spread security systems. By introducing the data stream and data integration technologies, GS-TMS provides a desirable capability of quickly building a large-scale distributed network monitoring system. Furthermore, GS-TMS has additional notable advantages over current monitoring systems in scalability and flexibility.

#index 1127622
#* Incompleteness in information integration
#@ Evgeny Kharlamov;Werner Nutt
#t 2008
#c 4
#% 663
#% 68199
#% 94459
#% 116303
#% 198465
#% 237190
#% 248038
#% 378409
#% 384978
#% 464717
#% 465057
#% 531450
#% 572311
#% 576116
#% 729889
#% 850730
#% 874882
#% 1661428
#% 1661439
#% 1661452
#! Information integration is becoming a critical problem for both businesses and individuals. The data, especially the one that comes from the Web, is naturally incomplete, that is, some data values may be unknown or lost because of communication problems, hidden due to privacy considerations. At the same time research in (virtual) integration in the community focusses on null-free sources and addresses limited forms of incompleteness only. In our work we aim to extend current results on virtual integration by considering various forms of incompleteness at the level of the sources, the integrated database and the queries (we call this Incomplete Information Integration, or III). More specifically, we aim to extend current query answering techniques for local-, and global-as-view integration to integration of tables with SQL nulls, Codd tables, etc. We also aim to consider incomplete answers as a natural extension of the classical approach. Our main research issues are (i) semantics of III, (ii) semantics of query answering in III, (iii) complexity of query answering, and (iv) algorithms (possibly approximate) to compute the answers.

#index 1127623
#* Querying web-based applications under models of uncertainty
#@ Daniel Deutch;Tova Milo
#t 2008
#c 4
#% 71306
#% 214091
#% 273702
#% 288513
#% 487125
#% 496116
#% 749039
#% 810175
#% 893111
#% 893117
#% 976998
#% 994005
#% 1016201
#% 1022252
#% 1030782
#% 1127379
#% 1279354
#% 1408536
#% 1688305
#% 1810819
#! Many businesses offer their services to customers via Web-based application interfaces. Reasoning about execution flows of such applications is extremely valuable for companies. Such reasoning must often operate under terms of uncertainty and partial information, due to partial tracing, effects of unknown external parameters, and more. The objectives of this research are (1) to define models for capturing Web application executions, with partial information and uncertainly of various flavors, (2) to design algorithms that allow for efficient reasoning over applications/execution traces under these models, and (3) to provide practical implementations that exploit these sound theoretical foundations for effective optimization of Web applications. We identify a restricted class of models that capture realistic scenarios, while allowing for an efficient query-based applications analysis. Hardness results indicate the necessity of such restricted models. We describe these results, highlight open problems, and consider directions for future research.

#index 1127624
#* XML-document-filtering automaton
#@ Panu Silvasti;Seppo Sippu;Eljas Soisalon-Soininen
#t 2008
#c 4
#% 321327
#% 342372
#% 462235
#% 464724
#% 480296
#% 570879
#% 654476
#% 730053
#% 731408
#% 791182
#% 824669
#% 835263
#% 910581
#! In a publish-subscribe system based on filtering of XML documents subscribers specify their interests with profiles expressed in the XPath language. The system processes a stream of XML documents and delivers to subscribers a notification or content of documents that match the profiles. We present a new XML-document-filtering algorithm that is based on the classic Aho-Corasick pattern-matching automaton. The automaton has a size linear in the sum of the sizes of the filters. We assume that the XML documents all conform to a given DTD; our algorithm utilizes the DTD in the preprocessing phase of the automaton to prune out descendant axes (//) and wildcards (*) from the XPath filters. The XPath subset currently supported consists of linear XPath expressions without predicates. In the case of a 683 MB protein-sequence database, we obtained a throughput of 18.8 MB/sec for 50 000 filters and 17.0 MB/sec for 500 000 filters, using a SAX parser with a throughput of 27 MB/sec.

#index 1127625
#* Community-driven data grids
#@ Tobias Scholl;Alfons Kemper
#t 2008
#c 4
#% 68091
#% 210190
#% 340175
#% 340176
#% 397398
#% 415957
#% 505869
#% 572314
#% 867051
#% 914605
#% 960252
#% 964497
#% 982557
#% 1022334
#% 1028502
#% 1065610
#% 1688254
#! Beyond already existing huge data volumes, e-science communities face major challenges in managing the anticipated data deluge of forthcoming projects. Community-driven data grids target at domain-specific federations and provide a distributed, collaborative data management by employing dominant data characteristics (e. g., data skew) and query patterns to optimize the overall throughput. By combining well-established techniques for data partitioning and replication with Peer-to-Peer (P2P) technologies we can address several challenging problems: data load balancing, handling of query hot spots, and the adaption to short-term burst as well as long-term load redistributions.

#index 1328050
#* Declarative management in Microsoft SQL server
#@ Hongfei Guo;Dan Jones;Jennifer Beckmann;Praveen Seshadri
#t 2009
#c 4
#% 210198
#% 275367
#% 397397
#% 480153
#% 480158
#% 482100
#% 640616
#% 875062
#% 978227
#% 978459
#% 993933
#% 1015316
#% 1022202
#% 1063558
#! This paper describes the principles and practice of Declarative Management --- a new approach to the management of database systems. The standard approach to database systems management involves a brittle coupling of interactive operations and procedural scripts. Such ad hoc approach results in incorrect administration, which leads to increased management costs. In the Declarative Management paradigm, a user specifies "what" the desired state is, and the system figures out "how" to get there and stay there. Declarative Management represents a fundamental step towards the goal of a self-managing database system. It also has the potential to significantly lower both administrative error and cost. An initial implementation of Declarative Management has been released with the Microsoft SQL Server 2008 database product, and the paper covers the implementation design as well.

#index 1328051
#* StatAdvisor: recommending statistical views
#@ Amr El-Helw;Ihab F. Ilyas;Calisto Zuzarte
#t 2009
#c 4
#% 102784
#% 248793
#% 273908
#% 273909
#% 300138
#% 397371
#% 443390
#% 465007
#% 479931
#% 480158
#% 480803
#% 765427
#% 765455
#% 765456
#% 810016
#% 810017
#% 864426
#% 960248
#% 1015334
#% 1016220
#% 1016225
#! Database statistics are crucial to cost-based optimizers for estimating the execution cost of a query plan. Using traditional basic statistics on base tables requires adopting unrealistic assumptions to estimate the cardinalities of intermediate results, which usually causes large estimation errors that can be several orders of magnitude. Modern commercial database systems support statistical or sample views, which give more accurate statistics on intermediate results and query sub-expressions. While previous research focused on creating and maintaining these advanced statistics, only little effort has been done towards automatically recommending the most beneficial statistical views to construct. In this paper, we present StatAdvisor, a system for recommending statistical views for a given SQL workload. The StatAdvisor addresses the special characteristics of statistical views with respect to view matching and benefit estimation, and introduces a novel plan-based candidate enumeration method, and a benefit-based analysis to determine the most useful statistical views. We present the basic concepts, architecture, and key features of StatAdvisor, and demonstrate its validity and benefits through an extensive experimental study using a prototype that we built in the IBM® DB2® database system as part of the DB2 Design Advisor tools.

#index 1328052
#* An object placement advisor for DB2 using solid state storage
#@ Mustafa Canim;George A. Mihaila;Bishwaranjan Bhattacharjee;Kenneth A. Ross;Christian A. Lang
#t 2009
#c 4
#% 281422
#% 321317
#% 402709
#% 410276
#% 443553
#% 459939
#% 480821
#% 562524
#% 571084
#% 810111
#% 820356
#% 875027
#% 951778
#% 958201
#% 960238
#% 985755
#% 1016221
#% 1052068
#% 1063551
#% 1127391
#% 1129952
#% 1129953
#% 1700130
#! Solid state disks (SSDs) provide much faster random access to data compared to conventional hard disk drives. Therefore, the response time of a database engine could be improved by moving the objects that are frequently accessed in a random fashion to the SSD. Considering the price and limited storage capacity of solid state disks, the database administrator needs to determine which objects (tables, indexes, materialized views, etc.), if placed on the SSD, would most improve the performance of the system. In this paper we propose a tool called "Object Placement Advisor" for making a wise decision for the object placement problem. By collecting profile inputs from workload runs, the advisor utility provides a list of objects to be placed on the SSD by applying heuristics like the greedy knapsack technique or dynamic programming. To show that the proposed approach is effective in conventional database management systems, we have conducted experiments on IBM DB2 with queries and schemas based on the TPC-H and TPC-C benchmarks. The results indicate that using a relatively small amount of SSD storage, the response time of the system can be reduced significantly by considering the recommendation of the advisor.

#index 1328053
#* XPEDIA: XML processing for data integration
#@ Manish Bhide;Manoj K. Agarwal;Amir Bar-Or;Sriram Padmanabhan;Srinivas K. Mittapalli;Girish Venkatachaliah
#t 2009
#c 4
#% 464852
#% 654524
#% 765432
#% 810083
#% 810119
#% 824750
#% 962360
#% 1179197
#% 1206578
#! Data Integration engines increasingly need to provide sophisticated processing options for XML data. In the past, it was adequate for these engines to support basic shredding and XML generation capabilities. However, with the steady growth of XML in applications and databases, integration platforms need to provide more direct operations on XML as well as improve the scalability and efficiency of these operations. In this paper, we describe a robust and comprehensive framework for performing Extract-Transform-Load (ETL) of XML. This includes (i) full computational model and engine capabilities to perform these operations in an ETL flow, (ii) an approach to pushing down XML operations into a database engine capable of supporting XML processing, and (iii) methods to apply partitioning techniques to provide scalable, parallel processing for large XML documents. We describe experimental results showing the effectiveness of these techniques.

#index 1328054
#* XQuery reloaded
#@ Roger Bamford;Vinayak Borkar;Matthias Brantner;Peter M. Fischer;Daniela Florescu;David Graf;Donald Kossmann;Tim Kraska;Dan Muresan;Sorin Nasoi;Markos Zacharioudakis
#t 2009
#c 4
#% 3888
#% 136740
#% 361445
#% 397366
#% 731408
#% 745484
#% 754092
#% 754118
#% 765488
#% 781453
#% 810036
#% 824777
#% 875010
#% 875045
#% 875065
#% 893213
#% 926886
#% 960317
#% 994015
#% 1015272
#% 1022208
#% 1044434
#% 1063488
#% 1063561
#% 1190156
#% 1206803
#% 1684601
#! This paper describes a number of XQuery-related projects. Its goal is to show that XQuery is a useful tool for many different application scenarios. In particular, this paper tries to correct a common myth that XQuery is merely a query language and that SQL is the better query language. Instead, XQuery is a full-fledged programming language for Web applications and services. Furthermore, this paper tries to correct a second myth that XQuery is slow. This paper gives an overview of the state-of-the-art in XQuery implementation and optimization techniques and discusses one particular open-source XQuery processor, Zorba, in more detail. Among others, this paper presents an XQuery Benchmark Service which helps practitioners and XQuery processor vendors to find performance problems in an XQuery processor.

#index 1328055
#* Binary XML storage and query processing in Oracle 11g
#@ Ning Zhang;Nipun Agarwal;Sivasankaran Chandrasekar;Sam Idicula;Vijay Medi;Sabina Petride;Balasubramanyam Sthanikam
#t 2009
#c 4
#% 172939
#% 333981
#% 570876
#% 632058
#% 650962
#% 731408
#% 745477
#% 770330
#% 781453
#% 800577
#% 800601
#% 803121
#% 810036
#% 810119
#% 824750
#% 824751
#% 875010
#% 956602
#% 1015275
#% 1016224
#% 1127562
#% 1127567
#! Oracle RDBMS has supported XML data management for more than six years since version 9i. Prior to 11g, text-centric XML documents can be stored as-is in a CLOB column and schema-based data-centric documents can be shredded and stored in object-relational (OR) tables mapped from their XML Schema. However, both storage formats have intrinsic limitations---XML/CLOB has unacceptable query and update performance, and XML/OR requires XML schema. To tackle this problem, Oracle 11g introduces a native Binary XML storage format and a complete stack of data management operations. Binary XML was designed to address a wide range of real application problems encountered in XML data management---schema flexibility, amenability to XML indexes, update performance, schema evolution, just to name a few. In this paper, we introduce the Binary XML storage format based on Oracle SecureFiles System[21]. We propose a lightweight navigational index on top of the storage and an NFA-based navigational algorithm to provide efficient streaming processing. We further optimize query processing by exploiting XML structural and schema information that are collected in database dictionary. We conducted extensive experiments to demonstrate high performance of the native Binary XML in query processing, update, and space consumption.

#index 1328056
#* Enhanced subquery optimizations in Oracle
#@ Srikanth Bellamkonda;Rafi Ahmed;Andrew Witkowski;Angela Amor;Mohamed Zait;Chun-Chieh Lin
#t 2009
#c 4
#% 116043
#% 287005
#% 480091
#% 481288
#% 481608
#% 564426
#% 654445
#% 654498
#% 742057
#% 893173
#% 960323
#! This paper describes enhanced subquery optimizations in Oracle relational database system. It discusses several techniques -- subquery coalescing, subquery removal using window functions, and view elimination for group-by queries. These techniques recognize and remove redundancies in query structures and convert queries into potentially more optimal forms. The paper also discusses novel parallel execution techniques, which have general applicability and are used to improve the scalability of queries that have undergone some of these transformations. It describes a new variant of antijoin for optimizing subqueries involved in the universal quantifier with columns that may have nulls. It then presents performance results of these optimizations, which show significant execution time improvements.

#index 1328057
#* Sort vs. Hash revisited: fast join implementation on modern multi-core CPUs
#@ Changkyu Kim;Tim Kaldewey;Victor W. Lee;Eric Sedlar;Anthony D. Nguyen;Nadathur Satish;Jatin Chhugani;Andrea Di Blas;Pradeep Dubey
#t 2009
#c 4
#% 5179
#% 156406
#% 172911
#% 172913
#% 190611
#% 397361
#% 442918
#% 443513
#% 444151
#% 444245
#% 479769
#% 479821
#% 480424
#% 480464
#% 480590
#% 480608
#% 566122
#% 629126
#% 745501
#% 874997
#% 993387
#% 1022230
#% 1022232
#% 1050225
#% 1063508
#% 1068011
#% 1127563
#% 1129954
#% 1139191
#% 1305908
#% 1504754
#% 1739419
#! Join is an important database operation. As computer architectures evolve, the best join algorithm may change hand. This paper re-examines two popular join algorithms -- hash join and sort-merge join -- to determine if the latest computer architecture trends shift the tide that has favored hash join for many years. For a fair comparison, we implemented the most optimized parallel version of both algorithms on the latest Intel Core i7 platform. Both implementations scale well with the number of cores in the system and take advantages of latest processor features for performance. Our hash-based implementation achieves more than 100M tuples per second which is 17X faster than the best reported performance on CPUs and 8X faster than that reported for GPUs. Moreover, the performance of our hash join implementation is consistent over a wide range of input data sizes from 64K to 128M tuples and is not affected by data skew. We compare this implementation to our highly optimized sort-based implementation that achieves 47M to 80M tuples per second. We developed analytical models to study how both algorithms would scale with upcoming processor architecture trends. Our analysis projects that current architectural trends of wider SIMD, more cores, and smaller memory bandwidth per core imply better scalability potential for sort-merge join. Consequently, sort-merge join is likely to outperform hash join on upcoming chip multiprocessors. In summary, we offer multicore implementations of hash join and sort-merge join which consistently outperform all previously reported results. We further conclude that the tide that favors the hash join algorithm has not changed yet, but the change is just around the corner.

#index 1328058
#* Efficient outer join data skew handling in parallel DBMS
#@ Yu Xu;Pekka Kostamaa
#t 2009
#c 4
#% 115661
#% 152924
#% 286417
#% 340595
#% 442923
#% 444245
#% 453661
#% 463095
#% 464078
#% 480596
#% 480608
#% 480761
#% 480966
#% 511189
#% 581485
#% 609707
#% 824688
#% 903011
#% 1063548
#! Large enterprises have been relying on parallel database management systems (PDBMS) to process their ever-increasing data volume and complex queries. The scalability and performance of a PDBMS comes from load balancing on all nodes in the system. Skewed processing will significantly slow down query response time and degrade the overall system performance. Business intelligence tools used by enterprises frequently generate a large number of outer joins and require high performance from the underlying database systems. Although extensive research has been done on handling skewed processing for inner joins in PDBMS, there is no known research on data skew handling for parallel outer joins. We propose a simple and efficient outer join algorithm called OJSO (Outer Join Skew Optimization) to improve the performance and scalability of parallel outer joins. Our experimental results show that the OJSO algorithm significantly speeds up query elapsed time in the presence of data skew.

#index 1328059
#* SQL/MapReduce: a practical approach to self-describing, polymorphic, and parallelizable user-defined functions
#@ Eric Friedman;Peter Pawlowski;John Cieslewicz
#t 2009
#c 4
#% 22947
#% 108508
#% 111368
#% 115661
#% 152940
#% 210206
#% 248816
#% 287461
#% 333931
#% 442705
#% 464007
#% 479972
#% 963669
#% 983467
#% 1063553
#% 1127559
#! A user-defined function (UDF) is a powerful database feature that allows users to customize database functionality. Though useful, present UDFs have numerous limitations, including install-time specification of input and output schema and poor ability to parallelize execution. We present a new approach to implementing a UDF, which we call SQL/MapReduce (SQL/MR), that overcomes many of these limitations. We leverage ideas from the MapReduce programming paradigm to provide users with a straightforward API through which they can implement a UDF in the language of their choice. Moreover, our approach allows maximum flexibility as the output schema of the UDF is specified by the function itself at query plan-time. This means that a SQL/MR function is polymorphic. It can process arbitrary input because its behavior as well as output schema are dynamically determined by information available at query plan-time, such as the function's input schema and arbitrary user-provided parameters. This also increases reusability as the same SQL/MR function can be used on inputs with many different schemas or with different user-specified parameters. In this paper we describe the motivation for this new approach to UDFs as well as the implementation within Aster Data Systems' nCluster database. We demonstrate that in the context of massively parallel, shared-nothing database systems, this model of computation facilitates highly scalable computation within the database. We also include examples of new applications that take advantage of this novel UDF framework.

#index 1328060
#* Building a high-level dataflow system on top of Map-Reduce: the Pig experience
#@ Alan F. Gates;Olga Natkovich;Shubham Chopra;Pradeep Kamath;Shravan M. Narayanamurthy;Christopher Olston;Benjamin Reed;Santhosh Srinivasan;Utkarsh Srivastava
#t 2009
#c 4
#% 346845
#% 420053
#% 428410
#% 442850
#% 480966
#% 734592
#% 864411
#% 954300
#% 963669
#% 1063553
#% 1127559
#% 1216713
#% 1217165
#% 1468421
#! Increasingly, organizations capture, transform and analyze enormous data sets. Prominent examples include internet companies and e-science. The Map-Reduce scalable dataflow paradigm has become popular for these applications. Its simple, explicit dataflow programming model is favored by some over the traditional high-level declarative approach: SQL. On the other hand, the extreme simplicity of Map-Reduce leads to much low-level hacking to deal with the many-step, branching dataflows that arise in practice. Moreover, users must repeatedly code standard operations such as join by hand. These practices waste time, introduce bugs, harm readability, and impede optimizations. Pig is a high-level dataflow system that aims at a sweet spot between SQL and Map-Reduce. Pig offers SQL-style high-level data manipulation constructs, which can be assembled in an explicit dataflow and interleaved with custom Map- and Reduce-style functions or executables. Pig programs are compiled into sequences of Map-Reduce jobs, and executed in the Hadoop Map-Reduce environment. Both Pig and Hadoop are open-source projects administered by the Apache Software Foundation. This paper describes the challenges we faced in developing Pig, and reports performance comparisons between Pig execution and raw Map-Reduce execution.

#index 1328061
#* PLANET: massively parallel learning of tree ensembles with MapReduce
#@ Biswanath Panda;Joshua S. Herbach;Sugato Basu;Roberto J. Bayardo
#t 2009
#c 4
#% 190581
#% 209021
#% 273900
#% 273907
#% 342628
#% 400847
#% 420091
#% 459008
#% 479787
#% 481945
#% 631969
#% 660537
#% 729437
#% 729973
#% 785340
#% 875965
#% 949205
#% 963669
#% 1073884
#% 1214754
#% 1417922
#! Classification and regression tree learning on massive datasets is a common data mining task at Google, yet many state of the art tree learning algorithms require training data to reside in memory on a single machine. While more scalable implementations of tree learning have been proposed, they typically require specialized parallel computing architectures. In contrast, the majority of Google's computing infrastructure is based on commodity hardware. In this paper, we describe PLANET: a scalable distributed framework for learning tree models over large datasets. PLANET defines tree learning as a series of distributed computations, and implements each one using the MapReduce model of distributed computation. We show how this framework supports scalable construction of classification and regression trees, as well as ensembles of such models. We discuss the benefits and challenges of using a MapReduce compute cluster for tree learning, and demonstrate the scalability of this approach by applying it to a real world learning task from the domain of computational advertising.

#index 1328062
#* Robust and distributed top-n frequent-pattern mining with SAP BW accelerator
#@ Thomas Legler;Wolfgang Lehner;Jan Schaffner;Jens Krüger
#t 2009
#c 4
#% 280456
#% 300120
#% 429421
#% 434348
#% 468927
#% 481290
#% 481754
#% 501207
#% 629644
#% 678196
#% 729942
#% 768521
#% 824704
#% 893176
#% 994925
#% 1016183
#% 1072635
#! Mining for association rules and frequent patterns is a central activity in data mining. However, most existing algorithms are only moderately suitable for real-world scenarios. Most strategies use parameters like minimum support, for which it can be very difficult to define a suitable value for unknown datasets. Since most untrained users are unable or unwilling to set such technical parameters, we address the problem of replacing the minimum-support parameter with top-n strategies. In our paper, we start by extending a top-n implementation of the ECLAT algorithm to improve its performance by using heuristic search strategy optimizations. Also, real-world datasets are often distributed and modern database architectures are switching from expensive SMPs to cheaper shared-nothing blade servers. Thus, most mining queries require distribution handling. Since partitioning can be forced by user-defined semantics, it is often forbidden to transform the data. Therefore, we developed an adaptive top-n frequent-pattern mining algorithm that simplifies the mining process on real distributions by relaxing some requirements on the results. We first combine the PARTITION and the TPUT algorithms to handle distributed top-n frequent-pattern mining. Then, we extend this new algorithm for distributions with real-world data characteristics. For frequent-pattern mining algorithms, equal distributions are important conditions, and tiny partitions can cause performance bottlenecks. Hence, we implemented an approach called MAST that defines a minimum absolute-support threshold. MAST prunes patterns with low chances of reaching the global top-n result set and high computing costs. In total, our approach simplifies the process of frequent-pattern mining for real customer scenarios and data sets. This may make frequent-pattern mining accessible for very new user groups. Finally, we present results of our algorithms when run on the SAP NetWeaver BW Acceleratorwith standard and real business datasets.

#index 1328063
#* 1,000 tables under the form
#@ Nicolas Dieu;Adrian Dragusanu;Françoise Fabret;François Llirbat;Eric Simon
#t 2009
#c 4
#% 198466
#% 220425
#% 273912
#% 289282
#% 464078
#% 465165
#% 509879
#% 765457
#% 810073
#! The goal of operational Business Intelligence (BI) is to help organizations improve the efficiency of their business by giving every "operational worker" insights needed to make better operational decisions, and aligning day-to-day operations with strategic goals. Operational BI reporting contributes to this goal by embedding analytics and reporting information into workflow applications so that the business user has all required information (contextual and business data) in order to make good decisions. EII systems facilitate the construction of operational BI reports by enabling the creation and querying of customized virtual database schemas over a set of distributed and heterogeneous data sources with a low TCO. Queries over these virtual databases feed the operational BI reports. We describe the characteristics of operational BI reporting applications and show that they increase the complexity of the source to target mapping defined between source data and virtual databases. We show that this complexity yields the execution of "mega queries", i.e., queries with possible a 1,000 tables in their FROM clause. We present some key optimization methods that have been successfully implemented in SAP Business Objects Data Federator system to deal with mega queries.

#index 1328064
#* Efficient index compression in DB2 LUW
#@ Bishwaranjan Bhattacharjee;Lipyeow Lim;Timothy Malkemus;George Mihaila;Kenneth Ross;Sherman Lau;Cathy McArthur;Zoltan Toth;Reza Sherkat
#t 2009
#c 4
#% 36684
#% 38697
#% 67454
#% 68236
#% 287715
#% 290703
#% 453323
#% 464843
#% 479808
#% 480458
#% 481424
#% 571082
#% 617869
#% 654495
#% 864446
#% 875026
#% 893159
#% 905068
#% 1015332
#% 1016235
#% 1022303
#! In database systems, the cost of data storage and retrieval are important components of the total cost and response time of the system. A popular mechanism to reduce the storage footprint is by compressing the data residing in tables and indexes. Compressing indexes efficiently, while maintaining response time requirements, is known to be challenging. This is especially true when designing for a workload spectrum covering both data warehousing and transaction processing environments. DB2 Linux, UNIX, Windows (LUW) recently introduced index compression for use in both environments. This uses techniques that are able to compress index data efficiently while incurring virtually no performance penalty for query processing. On the contrary, for certain operations, the performance is actually better. In this paper, we detail the design of index compression in DB2 LUW and discuss the challenges that were encountered in meeting the design goals. We also demonstrate its effectiveness by showing performance results on typical customer scenarios.

#index 1328065
#* Storing scientific workflows in a database
#@ Zoé Lacroix;Christophe Legendre;Spyro Mousses
#t 2009
#c 4
#% 832825
#% 879809
#% 1153638
#% 1165335
#% 1165337
#% 1407060
#! The use of workflow models to integrate intelligently complex experimental and analytical processes is becoming more and more critical to support scientific discovery. Storing and providing querying capabilities to retrieve, import, re-use, adapt, and reason about workflows are becoming necessary components to workflow architectures supporting collaborative and translational research. We report on the evaluation of ProtocolDB a database that supports workflow design and storage conducted at the Translational Genomics Research Institute (TGen).

#index 1328066
#* MAD skills: new analysis practices for big data
#@ Jeffrey Cohen;Brian Dolan;Mark Dunlap;Joseph M. Hellerstein;Caleb Welton
#t 2009
#c 4
#% 86929
#% 171746
#% 227883
#% 300185
#% 420053
#% 464007
#% 479449
#% 481950
#% 818987
#% 824697
#% 845350
#% 928682
#% 963669
#! As massive data acquisition and storage becomes increasingly affordable, a wide variety of enterprises are employing statisticians to engage in sophisticated data analysis. In this paper we highlight the emerging practice of Magnetic, Agile, Deep (MAD) data analysis as a radical departure from traditional Enterprise Data Warehouses and Business Intelligence. We present our design philosophy, techniques and experience providing MAD analytics for one of the world's largest advertising networks at Fox Audience Network, using the Greenplum parallel database system. We describe database design methodologies that support the agile working style of analysts in these settings. We present dataparallel algorithms for sophisticated statistical techniques, with a focus on density methods. Finally, we reflect on database system features that enable agile design and flexible algorithm development using both SQL and MapReduce interfaces over a variety of storage mechanisms.

#index 1328067
#* DBLP: some lessons learned
#@ Michael Ley
#t 2009
#c 4
#% 1069293
#% 1120910
#! The DBLP Computer Science Bibliography evolved from an early small experimental Web server to a popular service for the computer science community. Many design decisions and details of the public XML-records behind DBLP never were documented. This paper is a review of the evolution of DBLP. The main perspective is data modeling. In DBLP persons play a central role, our discussion of person names may be applicable to many other data bases. All DBLP data are available for your own experiments. You may either download the complete set, or use a simple XML-based API described in an online appendix.

#index 1328068
#* Oracle SecureFiles: prepared for the digital deluge
#@ Niloy Mukherjee;Amit Ganesh;Vinayagam Djegaradjane;Sujatha Muthulingam;Wei Zhang;Krishna Kunchithapadam;Scott Lynn;Bharath Aleti;Kam Shergill;Shaoyu Wang
#t 2009
#c 4
#% 479468
#% 479641
#% 480831
#% 1022299
#% 1055928
#% 1127562
#! Digital unstructured data volumes across enterprise, Internet and multimedia applications are predicted to surpass 6.023x1023 (Avogadro's number) bits a year in the next fifteen years. This poses tremendous scalability challenges for data management solutions in the coming decades. Filesystems seem to be preferred by data management application designers for providing storage solutions for such unstructured data volumes. Oracle SecureFiles is emerging as the database solution to break the performance barrier that has kept unstructured content out of database management systems and to provide advanced filesystem functionality, while letting applications fully leverage the strengths of the RDBMS from transactions to partitioning to rollforward recovery. A set of preliminary performance results was presented at the 34th International Conference on Very Large Data Bases (VLDB 2008). It was claimed that SecureFiles would scale maximally as physical storage systems scale up. We legitimize our claims on SecureFiles scalability through this paper, presenting the scalability aspects of SecureFiles through a performance evaluation of I/O bound filesystem like operations on one of the latest high performance cluster of servers and storage. We are presenting benchmark results that we believe represent a world record database insertion rate for any published result - at over 4.4GB/S using a cluster of seven servers. For 100 byte rows, that represents an insertion rate of 45 billion records a second in relational terms. In terms of unstructured data storage, the scale represents an insertion rate of more than 3.7 million 100 MB high-resolution multimedia videos a day.

#index 1328069
#* Scalable web data extraction for online market intelligence
#@ Robert Baumgartner;Georg Gottlob;Marcus Herzog
#t 2009
#c 4
#% 287202
#% 309793
#% 378389
#% 397605
#% 480648
#% 551870
#% 869105
#% 938578
#% 956500
#% 1006618
#% 1114343
#! Online market intelligence (OMI), in particular competitive intelligence for product pricing, is a very important application area for Web data extraction. However, OMI presents non-trivial challenges to data extraction technology. Sophisticated and highly parameterized navigation and extraction tasks are required. On-the-fly data cleansing is necessary in order two identify identical products from different suppliers. It must be possible to smoothly define data flow scenarios that merge and filter streams of extracted data stemming from several Web sites and store the resulting data into a data warehouse, where the data is subjected to market intelligence analytics. Finally, the system must be highly scalable, in order to be able to extract and process massive amounts of data in a short time. Lixto (www.lixto.com), a company offering data extraction tools and services, has been providing OMI solutions for several customers. In this paper we show how Lixto has tackled each of the above challenges by improving and extending its original data extraction software. Most importantly, we show how high scalability is achieved through cloud computing. This paper also features a case study from the computers and electronics market.

#index 1328070
#* Kosmix: high-performance topic exploration using the deep web
#@ Anand Rajaraman
#t 2009
#c 4
#% 198466
#% 268079
#% 333990
#% 447946
#% 480479
#% 481923
#% 590523
#% 765409
#% 809418
#% 864434
#% 993964
#% 1016163
#% 1082158
#% 1127557
#! Kosmix lies at the intersection of two important trends: topic exploration and the Deep Web. Topic exploration is a new approach to information discovery on the web that satisfies certain use cases not served well by conventional web search. The Deep Web, an inhospitable region for web crawlers, is emerging as a significant information resource. We describe the anatomy of Kosmix, the first general-purpose topic exploration engine to harness the Deep Web using a federated search approach. We focus in particular on the Kosmix approach to query tranformation and caching, which is essential to ensure reasonable performance.

#index 1328071
#* Query mesh: multi-route query processing technology
#@ Rimma V. Nehme;Karen E. Works;Elke A. Rundensteiner;Elisa Bertino
#t 2009
#c 4
#% 376266
#% 390132
#% 421124
#% 1016269
#% 1026989
#% 1030737
#% 1181281
#% 1206727
#! We propose to demonstrate a practical alternative approach to the current state-of-the-art query processing techniques, called the "Query Mesh" (or QM, for short). The main idea of QM is to compute multiple routes (i.e., query plans), each designed for a particular subset of data with distinct statistical properties. Based on the execution routes and the data characteristics, a classifier model is induced and is used to partition new data tuples to assign the best routes for their processing. We propose to demonstrate the QM framework in the streaming context using our demo application, called the "Ubi-City". We will illustrate the innovative features of QM, including: the QM optimization with the integrated machine learning component, the QM execution using the efficient "Self-Routing Fabric" infrastructure, and finally, the QM adaptive component that performs the online adaptation of QM with near-zero runtime overhead.

#index 1328072
#* A demonstration of SciDB: a science-oriented DBMS
#@ P. Cudre-Mauroux;H. Kimura;K.-T. Lim;J. Rogers;R. Simakov;E. Soroush;P. Velikhov;D. L. Wang;M. Balazinska;J. Becla;D. DeWitt;B. Heath;D. Maier;S. Madden;J. Patel;M. Stonebraker;S. Zdonik
#t 2009
#c 4
#% 58352
#% 300185
#% 427199
#% 445771
#% 479920
#% 480096
#% 481428
#% 893189
#% 982557
#% 998070
#! In CIDR 2009, we presented a collection of requirements for SciDB, a DBMS that would meet the needs of scientific users. These included a nested-array data model, science-specific operations such as regrid, and support for uncertainty, lineage, and named versions. In this paper, we present an overview of SciDB's key features and outline a demonstration of the first version of SciDB on data and operations from one of our lighthouse users, the Large Synoptic Survey Telescope (LSST).

#index 1328073
#* MOIR/MT: monitoring large-scale road network traffic in real-time
#@ Kuien Liu;Ke Deng;Zhiming Ding;Mingshu Li;Xiaofang Zhou
#t 2009
#c 4
#% 745534
#% 1015280
#% 1080142
#% 1206625
#% 1211643
#% 1217191
#! Floating Car Data (FCD) provides an economic complement to infrastructure-based traffic monitoring systems. Based on our previous MOIR platform [5], we use FCD as the data source for large-scale real-time traffic monitoring. This new function brings a challenge of efficiently handling of streaming data from a very large number of moving objects. Server overload problems can occur when a system fails to process data and queries in real-tme, which can lead to critical issues such as unbounded delay accumulation, lost monitoring accuracy or lack of spontaneity. These problems can be addressed by adopting suitable load dropping decisions. In this work, we demonstrate several load shedding techniques, focusing on decision-making based on data attributes. With the end results being quantified and visualized using real data for a large city, this proof-of-concept system provides a convincing way of validating our ideas.

#index 1328074
#* Oracle Database Replay
#@ Romain Colle;Leonidas Galanis;Supiti Buranawatanachoke;Stratos Papadomanolakis;Yujun Wang
#t 2009
#c 4
#% 1063558
#! This demonstration presents Oracle Database Replay, a novel approach to testing changes to the relational database management system component of an information system (software upgrades, hardware changes etc). Database Replay makes it possible to subject a test system to a real production workload, which helps identify all potential problems before implementing the planned changes on the production system. Any interesting workload period of a production database system can be captured with minimal overhead. The captured workload is used to drive a test system while maintaining the concurrency and load characteristics of the real production workload. The demonstration showcases how important maintaining the concurrency and load characteristics of the real workload is. The current testing solutions do not allow for synchronization based on data dependencies. Without proper synchronization the demonstration workload does not perform the work required and does not exercise the test system appropriately, leading to poor coverage and inadequate load. Thus many issues remain undetected. Database replay with its data based synchronization makes testing realistic and leads to the discovery of potential problems.

#index 1328075
#* DIADS: a problem diagnosis tool for databases and storage area networks
#@ Nedyalko Borisov;Shivnath Babu;Sandeep Uttamchandani;Ramani Routray;Aameek Singh
#t 2009
#c 4
#% 1189341
#! Many enterprise environments have databases running on network-attached storage infrastructure (referred to as Storage Area Networks or SANs). Both the database and the SAN are complex subsystems that are managed by separate teams of administrators. As often as not, database administrators have limited understanding of SAN configuration and behavior, and limited visibility into the SAN's run-time performance; and vice versa for the SAN administrators. Diagnosing the cause of performance problems is a challenging exercise in these environments. We propose to remedy the situation through a novel tool, called Diads, for database and SAN problem diagnosis. This demonstration proposal summarizes the technical innovations in Diads: (i) a powerful abstraction called Annotated Plan Graphs (APGs) that ties together the execution path of queries in the database and the SAN using low-overhead monitoring data, and (ii) a diagnosis workflow that combines domain-specific knowledge with machine-learning techniques. The scenarios presented in the demonstration are also described.

#index 1328076
#* Artemis: a system for analyzing missing answers
#@ Melanie Herschel;Mauricio A. Hernández;Wang-Chiew Tan
#t 2009
#c 4
#% 663
#% 315853
#% 430773
#% 459273
#% 1127409
#% 1223207
#! A central feature of relational database management systems is the ability to define multiple different views over an underlying database schema. Views provide a method of defining access control to the underlying database, since a view exposes a part of the database and hides the rest. Views also provide logical data independence to application programs that access the database. For most cases, the process of specifying the desired views in SQL is typically tedious and error-prone. While numerous tools exist to support developers in debugging program code, we are not aware of any tool that supports developers in verifying the correctness of their views defined in SQL.

#index 1328077
#* Demonstration of the TrajStore system
#@ Eugene Wu;Philippe Cudre-Mauroux;Samuel Madden
#t 2009
#c 4
#% 427199
#% 480473
#% 555050
#% 824729
#% 1046418
#% 1065023
#! The proliferation of GPS devices has led to a substantial interest in location based services. In particular, modern vehicles can generate an incredible amount of drive data. However, current storage systems are not optimized for storing and querying such large spatial-temporal data sets. In this demonstration, we show the performance of the TrajStore system, a dynamic storage system optimized for quickly accessing data in a particular spatial-temporal region. In particular, TrajStore uses a novel adaptive indexing technique that dynamically adjusts itself to co-locate spatially close trajectories on disk, as well as a number of compression techniques in the storage layer that significantly reduce access time for a given index cell. In this demonstration, we will store a set of real world taxi cab drive traces in TrajStore, and users will be able to query the data through a map based interface.

#index 1328078
#* Microsoft CEP server and online behavioral targeting
#@ M. H. Ali;C. Gerea;B. S. Raman;B. Sezgin;T. Tarnavski;T. Verona;P. Wang;P. Zabback;A. Ananthanarayan;A. Kirilov;M. Lu;A. Raizman;R. Krishnan;R. Schindlauer;T. Grabs;S. Bjeletich;B. Chandramouli;J. Goldstein;S. Bhat;Ying Li;V. Di Nicola;X. Wang;David Maier;S. Grell;O. Nano;I. Santos
#t 2009
#c 4
#% 463105
#% 801694
#% 1071380
#! In this demo, we present the Microsoft Complex Event Processing (CEP) Server, Microsoft CEP for short. Microsoft CEP is an event stream processing system featured by its declarative query language and its multiple consistency levels of stream query processing. Query composability, query fusing, and operator sharing are key features in the Microsoft CEP query processor. Moreover, the debugging and supportability tools of Microsoft CEP provide visibility of system internals to users. Web click analysis has been crucial to behavior-based online marketing. Streams of web click events provide a typical workload for a CEP server. Meanwhile, a CEP server with its processing capabilities plays a key role in web click analysis. This demo highlights the features of Microsoft CEP under a workload of web click events.

#index 1328079
#* A testbed for managing dynamic mixed workloads
#@ Stefan Krompass;Harumi Kuno;Janet L. Wiener;Kevin Wilkinson;Umeshwar Dayal;Alfons Kemper
#t 2009
#c 4
#% 442968
#% 1022294
#% 1181224
#% 1207105
#! Workload management for operational business intelligence (BI) databases is difficult. Queries vary widely in length and objectives. Resource contention is difficult to predict and to control as dynamically-arriving, long, analyst queries compete for resources with ongoing online-transaction processing (OLTP) queries and batch report queries. Currently, administrators struggle to choose workload management policies and set their thresholds manually. The goal of our project is a software framework to make the management of such mixed workloads easier. Our framework includes a policy controller that tunes workload management policies automatically to meet workload objectives. This demonstration of our system illustrates (1) the difficulty of managing a BI database workload and (2) the benefits of tuning policies automatically and individually for each service class of queries in a workload. In addition, our demonstrator is a useful research tool for understanding how policies and a policy controller adapt as the system state changes under a mixed workload. In our demo, the participant plays the administrator and tunes the policies for a variety of difficult-to-manage workloads as they execute. These policies include admission control, scheduling, and execution control policies. We visualize the policies, the user objectives, and the load on the system components (CPUs, memory, disks) during execution, which helps the participant see whether objectives are being met and make appropriate policy decisions. At the end of each workload, the participant is given the opportunity to compare how their policies met workload objectives versus policies determined by our automatic policy controller.

#index 1328080
#* DBToaster: a SQL compiler for high-performance delta processing in main-memory databases
#@ Yanif Ahmad;Christoph Koch
#t 2009
#c 4
#% 1063555
#! We present DBToaster, a novel query compilation framework for producing high performance compiled query executors that incrementally and continuously answer standing aggregate queries using in-memory views. DBToaster targets applications that require efficient main-memory processing of standing queries (views) fed by high-volume data streams, recursively compiling view maintenance (VM) queries into simple C++ functions for evaluating database updates (deltas). While today's VM algorithms consider the impact of single deltas on view queries to produce maintenance queries, we recursively consider deltas of maintenance queries and compile to thoroughly transform queries into code. Recursive compilation successively elides certain scans and joins, and eliminates significant query plan interpreter overheads. In this demonstration, we walk through our compilation algorithm, and show the significant performance advantages of our compiled executors over other query processors. We are able to demonstrate 1--3 orders of magnitude improvements in processing times for a financial application and a data warehouse loading application, both implemented across a wide range of database systems, including PostgreSQL, HSQLDB, a commercial DBMS 'A', the Stanford STREAM engine, and a commercial stream processor 'B'.

#index 1328081
#* ANGIE: active knowledge for interactive exploration
#@ Nicoleta Preda;Fabian M. Suchanek;Gjergji Kasneci;Thomas Neumann;Maya Ramanath;Gerhard Weikum
#t 2009
#c 4
#% 198466
#% 229827
#% 480149
#% 481923
#% 482116
#% 765432
#% 864418
#% 956573
#% 1055735
#% 1055897
#% 1063559
#% 1072645
#% 1092530
#% 1127402
#% 1131140
#% 1131145
#% 1206702
#% 1275182
#% 1409954
#! We present ANGIE, a system that can answer user queries by combining knowledge from a local database with knowledge retrieved from Web services. If a user poses a query that cannot be answered by the local database alone, ANGIE calls the appropriate Web services to retrieve the missing information. This information is integrated seamlessly and transparently into the local database, so that the user can query and browse the knowledge base while appropriate Web services are called automatically in the background.

#index 1328082
#* Comparative evaluation of entity resolution approaches with FEVER
#@ Hanna Köpcke;Andreas Thor;Erhard Rahm
#t 2009
#c 4
#% 288197
#% 875066
#% 881575
#% 893164
#% 913783
#% 1022229
#% 1127425
#! We present FEVER, a new evaluation platform for entity resolution approaches. The modular structure of the FEVER framework supports the incorporation or reconstruction of many previously proposed approaches for entity resolution. A distinctive feature of FEVER is that it not only evaluates traditional measures such as precision and recall but also the effort for configuring (e.g., parameter tuning, training) a good entity resolution approach. FEVER thus strives for a fair comparative evaluation of different approaches by considering both the effectiveness and configuration effort.

#index 1328083
#* RankIE: document retrieval on ranked entity graphs
#@ Falk Brauer;Wojciech Barczynski;Gregor Hackenbroich;Marcus Schramm;Adrian Mocan;Felix Förster
#t 2009
#c 4
#% 590523
#% 875064
#% 893143
#% 1022353
#% 1206687
#% 1206702
#! Developer communities built around software products, like the SAP Community Network, provide a knowledge base for reocurring problems and their solutions. Due to the large amount of content maintained in such communities, e.g., in forums, finding relevant solutions is a major challenge beyond the scope of common keyword-based search engines. In fact, it is measured that around 50% of the forum questions of our particular scenario have already been answered at the time they are posted. We target this challenge by an entity aware search, which exploits structured knowledge, such as domain-specific ontologies, for both query interpretation and creation of document indexes. The system takes a natural language query as input, interprets it as an entity graph, matches this graph with pre-processed content and supports the user in refining his query based on the top-k relevant entities. Results are presented in a user interface that supports faceted search based on entities. Additionally, the user interface is structured according to possible search intentions of users. The evaluation of our system on the SCN scenario yields that the top 5 entities in user queries are recognized with a precision of 83% compared to 61% of state of the art algorithms.

#index 1328084
#* Concise and expressive mappings with +Spicy
#@ Giansalvatore Mecca;Paolo Papotti;Salvatore Raunich;Marcello Buoncristiano
#t 2009
#c 4
#% 806215
#% 893094
#% 993981
#% 1039063
#% 1044442
#% 1127589
#% 1217196
#! We introduce the +Spicy mapping system. The system is based on a number of novel algorithms that contribute to increase the quality and expressiveness of mappings. +Spicy integrates the computation of core solutions in the mapping generation process in a highly efficient way, based on a natural rewriting of the given mappings. This allows for an efficient implementation of core computations using common runtime languages like SQL or XQuery and guarantees very good performances, orders of magnitude better than those of previous algorithms. The rewriting algorithm can be applied both to mappings generated by the system, or to pre-defined mappings provided as part of the input. To do this, the system was enriched with a set of expressive primitives, so that +Spicy is the first mapping system that brings together a sophisticate and expressive mapping generation algorithm with an efficient strategy to compute core solutions.

#index 1328085
#* AgreementMaker: efficient matching for large real-world schemas and ontologies
#@ Isabel F. Cruz;Flavio Palandri Antonelli;Cosmin Stroe
#t 2009
#c 4
#% 334025
#% 413809
#% 572314
#% 660001
#% 810103
#% 941135
#% 958206
#% 1705177
#! We present the AgreementMaker system for matching real-world schemas and ontologies, which may consist of hundreds or even thousands of concepts. The end users of the system are sophisticated domain experts whose needs have driven the design and implementation of the system: they require a responsive, powerful, and extensible framework to perform, evaluate, and compare matching methods. The system comprises a wide range of matching methods addressing different levels of granularity of the components being matched (conceptual vs. structural), the amount of user intervention that they require (manual vs. automatic), their usage (stand-alone vs. composed), and the types of components to consider (schema only or schema and instances). Performance measurements (recall, precision, and runtime) are supported by the system, along with the weighted combination of the results provided by those methods. The AgreementMaker has been used and tested in practical applications and in the Ontology Alignment Evaluation Initiative (OAEI) competition. We report here on some of its most advanced features, including its extensible architecture that facilitates the integration and performance tuning of a variety of matching methods, its capability to evaluate, compare, and combine matching results, and its user interface with a control panel that drives all the matching methods and evaluation strategies.

#index 1328086
#* Linkage Query Writer
#@ Oktie Hassanzadeh;Reynold Xin;Renée J. Miller;Anastasios Kementsietsidis;Lipyeow Lim;Min Wang
#t 2009
#c 4
#% 853532
#% 913783
#% 1016217
#! We present Linkage Query Writer (LinQuer), a system for generating SQL queries for semantic link discovery over relational data. The LinQuer framework consists of (a) LinQL, a language for specification of linkage requirements; (b) a web interface and an API for translating LinQL queries to standard SQL queries; (c) an interface that assists users in writing LinQL queries. We discuss the challenges involved in the design and implementation of a declarative and easy to use framework for discovering links between different data items in a single data source or across different data sources. We demonstrate different steps of the linkage requirements specification and discovery process in several real world scenarios and show how the LinQuer system can be used to create high-quality linked data sources.

#index 1328087
#* SMDM: enhancing enterprise-wide master data management using semantic web technologies
#@ Xiaoyuan Wang;Xingzhi Sun;Feng Cao;Li Ma;Nick Kanellos;Kang Zhang;Yue Pan;Yong Yu
#t 2009
#c 4
#% 73005
#% 1022336
#% 1063570
#% 1189611
#! Motivated by evolving business requirements and novel enterprise applications, we propose and implement the Semantic Master Data Management (SMDM), a semantics-level enhancement to the existing MDM solutions. The SMDM system publishes relational-based master data as virtual RDF store, and injects instantaneous reasoning capabilities into semantic queries. Two kinds of ontologies are introduced to the system, the core MDM ontology and the external imported domain ontology. SMDM enables data linking among multi-domains, implicit relationship discovery, and declarative definition and extension of business policies and entities. Based on these functions, modern companies can customize their applications and services on demand within the MDM hub. In the demonstration, we build the system environment based on IBM's MDM solution, and run the use cases on the master data of an insurance company.

#index 1328088
#* IBM UFO repository: object-oriented data integration
#@ Michael N. Gubanov;Lucian Popa;Howard Ho;Hamid Pirahesh;Jeng-Yih Chang;Shr-Chang Chen
#t 2009
#c 4
#% 32981
#% 411768
#% 654457
#% 770332
#% 810078
#% 881740
#% 960237
#% 960271
#% 960352
#% 983614
#% 1022223
#% 1022286
#% 1063559
#% 1063570
#% 1206580
#% 1390329
#% 1720618
#! Currently, WWW, large enterprises, and desktop users suffer from an inability to efficiently access and manage differently structured data. The same data objects (e.g. Product) stored by different databases, repositories, distributed web storage systems, etc are named, referenced, and combined internally into schemas or data structures differently. This leads to structural mismatch of data that often consists of the same semantic objects (e.g. EBay and Yahoo! online auction offers).

#index 1328089
#* Mashup by surfing a web of data APIs
#@ Huajun Chen;Bin Lu;Yuan Ni;Guotong Xie;Chunying Zhou;Jinhua Mi;Zhaohui Wu
#t 2009
#c 4
#% 893087
#! We present sMash, a system for facilitating users to mashup Web data. The aspects emphasized by the demo are: (1) how to help novice users master data APIs and relationships amongst them easily; (2) how to inspire various users to build more amazing Web data mashups. First, a real-life data API network is constructed and visualized to enable users to surf and mashup. Second, two kinds of recommendations are generated dynamically based on a comprehensive analysis of the network, user's traces and a repository of mashups to provide navigation.

#index 1328090
#* DEMo: data exchange modeling tool
#@ Reinhard Pichler;Vadim Savenkov
#t 2009
#c 4
#% 806215
#% 809247
#% 826032
#% 874880
#% 893196
#% 1063578
#% 1217196
#! Minimality is an important optimization criterion for solutions of data exchange problems, well captured by the notion of the core. Though tractability of core computation has been proved, it has not yet become a part of any industrial-strength system, still being highly computationally expensive. In this demonstration, we show how core computation can be used in a data exchange modeling tool, allowing data engineers to design more robust data transfer scenarios and better understand the sources of redundancy in the target database.

#index 1328091
#* Lahar demonstration: warehousing Markovian streams
#@ Julie Letchner;Christopher Ré;Magdalena Balazinska;Matthai Philipose
#t 2009
#c 4
#% 1063523
#% 1147402
#% 1206877
#! Lahar is a warehousing system for Markovian streams---a common class of uncertain data streams produced via inference on probabilistic models. Example Markovian streams include text inferred from speech, location streams inferred from GPS or RFID readings, and human activity streams inferred from sensor data. Lahar supports OLAP-style queries on Markovian stream archives by leveraging novel approximation and indexing techniques that efficiently manipulate stream probabilities. This demonstration allows users to interactively query a warehouse of imprecise text streams inferred automatically from audio podcasts. Through this interaction, the demo introduces users to the challenges of Markovian stream processing as well as technical contributions developed to address these challenges.

#index 1328092
#* WOLVES: achieving correct provenance analysis by detecting and resolving unsound workflow views
#@ Peng Sun;Ziyang Liu;Sivaramakrishnan Natarajan;Susan B. Davidson;Yi Chen
#t 2009
#c 4
#% 765129
#% 1063593
#% 1206750
#% 1217188
#! Workflow views abstract groups of tasks in a workflow into composite tasks, and are used for simplifying provenance analysis, workflow sharing and reuse. An unsound view does not preserve the dataflow between tasks in the workflow, and can therefore cause incorrect provenance analysis. In this demo we present WOLVES, a system that efficiently identifies and corrects unsound workflow views with minimal changes (view correction). Since the view correction problem is NP-hard, WOLVES allows the user to choose between two forms of local optimality, strong and weak. Efficient time algorithms achieving these optimalities are implemented in WOLVES.

#index 1328093
#* TIAMAT: a tool for interactive analysis of microdata anonymization techniques
#@ Chenyun Dai;Gabriel Ghinita;Elisa Bertino;Ji-Won Byun;Ninghui Li
#t 2009
#c 4
#% 443463
#% 576761
#% 577239
#% 864406
#% 864412
#% 881551
#% 960291
#% 1022265
#% 1408782
#% 1725659
#! Releasing detailed data (microdata) about individuals poses a privacy threat, due to the presence of quasi-identifier (QID) attributes such as age or zip code. Several privacy paradigms have been proposed that preserve privacy by placing constraints on the value of released QIDs. However, in order to enforce these paradigms, data publishers need tools to assist them in selecting a suitable anonymization method and choosing the right system parameters. We developed TIAMAT, a tool for analysis of anonymization techniques which allows data publishers to assess the accuracy and overhead of existing anonymization techniques. The tool performs interactive, head-to-head comparison of anonymization techniques, as well as QID change-impact analysis. Other features include collection of attribute statistics, support for multiple information loss metrics and compatibility with commercial database engines.

#index 1328094
#* iNextCube: information network-enhanced text cube
#@ Yintao Yu;Cindy X. Lin;Yizhou Sun;Chen Chen;Jiawei Han;Binbin Liao;Tianyi Wu;ChengXiang Zhai;Duo Zhang;Bo Zhao
#t 2009
#c 4
#% 210182
#% 280819
#% 340146
#% 893143
#% 1019106
#% 1176884
#% 1181261
#% 1214701
#% 1314743
#% 1715634
#! Nowadays, most business, administration, and/or scientific databases contain both structured attributes and text attributes. We call a database that consists of both multidimensional structured data and narrative text data as multidimensional text database. Searching, OLAP, and mining such databases pose many research challenges. To enhance the power of data analysis, interesting entities and relationships can be extracted from such databases to derive heterogeneous information networks, which in turn will substantially increase the power and flexibility of data exploration in such databases. Based on our previous studies on TextCube [1], TopicCube [2], and information network analysis, such as RankClus [3] and NetClus [4], we construct iNextCube, an information-Network-enhanced text Cube. In this demo, we show the power of iNextCube in the search and analysis of two multidimensional text databases: (i) a DBLP-based CS bibliographic database, and (ii) an online news database.

#index 1328095
#* Hive: a warehousing solution over a map-reduce framework
#@ Ashish Thusoo;Joydeep Sen Sarma;Namit Jain;Zheng Shao;Prasad Chakka;Suresh Anthony;Hao Liu;Pete Wyckoff;Raghotham Murthy
#t 2009
#c 4
#% 1127559
#% 1217159
#! The size of data sets being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hadoop [3] is a popular open-source map-reduce implementation which is being used as an alternative to store and process extremely large data sets on commodity hardware. However, the map-reduce programming model is very low level and requires developers to write custom programs which are hard to maintain and reuse.

#index 1328096
#* Tolkien: an event based storytelling system
#@ Arjun Satish;Ramesh Jain;Amarnath Gupta
#t 2009
#c 4
#% 451595
#% 874935
#% 1037554
#% 1698202
#! Since the dawn of human civilization, stories have been a popular medium of communication, both synchronously and asynchronously. Technically, a story is a time-ordered coherent sequence of events. In many applications, heterogeneous data is collected and organized so appropriate stories could be told. In this paper, we present a system that helps in generation of stories using a large database of events with associated multimodal data, called eventbase. We define storytelling as a two step process in which a storyteller can retrieve appropriate events and associated data, and then those are further filtered using preferences of the viewer. We develop this model using a measure of interestingness based on attributes of selected events and the preferences. Using an event system developed in our laboratory, we demonstrate the story telling process in Tolkien as one that generates multiple queries to select coherent interesting events to form a story.

#index 1328097
#* Enabling social networking in ad hoc networks of mobile phones
#@ Emre Sarigöl;Oriana Riva;Patrick Stuedi;Gustavo Alonso
#t 2009
#c 4
#% 1012
#% 622728
#! This demonstration presents AdSocial, a software platform supporting social network applications in ad hoc networks. AdSocial targets small-scale scenarios such as friends playing a game on the train or co-workers sharing calendar information. Moreover, AdSocial is specifically designed to run on resource-constrained mobile devices, such as mobile phones. By using a simple and efficient data piggybacking mechanism, AdSocial applications share data by using any of the many existing routing protocols for ad hoc networks and without requiring any modification to the protocols themselves. The goal of this demonstration is to show the functionality that AdSocial can support with a very low overhead in an ad hoc network of 10--15 Nokia N810 handhelds. Conference participants will be able to establish voice-video calls, chat, or play games while moving around thus configuring a mobile and multi-hop ad hoc network.

#index 1328098
#* PDiffView: viewing the difference in provenance of workflow results
#@ Zhuowei Bao;Sarah Cohen-Boulakia;Susan B. Davidson;Pierrick Girard
#t 2009
#c 4
#% 600539
#% 1063593
#% 1063709
#% 1207025
#! Scientific workflow systems are becoming increasingly important for managing in-silico experiments. Such experiments are typically specified as directed flow graphs, in which the nodes represent modules and edges represent data flow between the modules. Each execution (a.k.a. run) of an experiment may vary the parameters and data inputs to the modules in the specification; furthermore, alternative paths of the workflow may be followed. In this process, the scientist's goal is to identify parameter settings and approaches which lead to good final results. Comparing workflow executions of the same specification and understanding the difference between them is thus of paramount importance for understanding the provenance of final results [4].

#index 1328099
#* Goal-oriented web-site navigation for on-line shoppers
#@ Daniel Deutch;Tova Milo;Tom Yam
#t 2009
#c 4
#% 330687
#% 813966
#% 874885
#% 893117
#% 1217203
#! Web-sites for on-line shopping typically offer a vast number of product options and combinations thereof. While this is very useful, it often makes the navigation in the site and the identification of the "ideal" purchase (where the notion of ideal differs among users) a confusing, non-trivial experience. This demonstration presents ShopIT (ShoppIng assitanT), a system that assists on-line shoppers by suggesting the most effective navigation paths for their specified criteria and preferences. The suggestions are continually adapted to choices/decisions taken by the users while navigating. ShopITis based on a set of novel, adaptive, provably optimal algorithms for TOP-K query evaluation.

#index 1328100
#* Answering web questions using structured data: dream or reality?
#@ Fernando Pereira;Anand Rajaraman;Sunita Sarawagi;William Tunstall-Pedoe;Gerhard Weikum;Alon Halevy
#t 2009
#c 4
#% 1127393
#% 1127557
#% 1206702
#% 1270363
#% 1328133
#! The question of which role structured data can play in Web search has been raised from the early days of the Web. On the one hand, structured data can be used to answer factual queries. On the other, large amounts of structured data can be used to better organize web-content and therefore to improve search on a wide range of queries.

#index 1328101
#* How best to build web-scale data managers?
#@ Philip A. Bernstein;Daniel J. Abadi;Michael J. Cafarella;Joseph M. Hellerstein;Donald Kossmann;Samuel Madden
#t 2009
#c 4
#% 963669
#% 983467
#% 998845
#% 1054227
#% 1127560
#! Many of the largest database-driven web sites use custom web-scale data managers (WDMs). On the surface, these WDMs are being applied to problems that are well-suited for relational database systems. Some examples are the following: • Map-Reduce [5], Hadoop [7], and Dryad [9] are used to process queries on large data sets using sequential scan and aggregation. Hive [8] is a data warehouse built on Hadoop. • Google's Bigtable [3] is used to store a replicated table of rows of semi-structured data. • Amazon's Dynamo [6] is used to store partitioned, replicated databases of key-value pairs. Cassandra [2] is similar. • Object caching systems are used instead of a persistent store, such as memcached [10], Oracle's Coherence, and Microsoft's Velocity project.

#index 1328102
#* Database architecture evolution: mammals flourished long before dinosaurs became extinct
#@ Stefan Manegold;Martin L. Kersten;Peter Boncz
#t 2009
#c 4
#% 18614
#% 227861
#% 275367
#% 286258
#% 300194
#% 443513
#% 479819
#% 479821
#% 480119
#% 480464
#% 480821
#% 566122
#% 765417
#% 810039
#% 824697
#% 864446
#% 875010
#% 983261
#% 993947
#% 993967
#% 1015288
#% 1015289
#% 1016184
#% 1016186
#% 1016187
#% 1016220
#% 1021950
#% 1022262
#% 1022298
#% 1127610
#% 1129957
#% 1138538
#% 1180960
#% 1181240
#% 1217169
#% 1217170
#% 1217193
#% 1222049
#% 1372642
#! The holy grail for database architecture research is to find a solution that is Scalable & Speedy, to run on anything from small ARM processors up to globally distributed compute clusters, Stable & Secure, to service a broad user community, Small & Simple, to be comprehensible to a small team of programmers, Self-managing, to let it run out-of-the-box without hassle. In this paper, we provide a trip report on this quest, covering both past experiences, ongoing research on hardware-conscious algorithms, and novel ways towards self-management specifically focused on column store solutions.

#index 1328103
#* Data fusion: resolving data conflicts for integration
#@ Xin Luna Dong;Felix Naumann
#t 2009
#c 4
#% 172933
#% 514144
#% 572311
#% 572314
#% 591566
#% 806215
#% 893089
#% 913783
#% 960233
#% 989682
#% 1129527
#% 1328155
#% 1328156
#% 1661428
#! The amount of information produced in the world increases by 30% every year and this rate will only go up. With advanced network technology, more and more sources are available either over the Internet or in enterprise intranets. Modern data management applications, such as setting up Web portals, managing enterprise data, managing community data, and sharing scientific data, often require integrating available data sources and providing a uniform interface for users to access data from different sources; such requirements have been driving fruitful research on data integration over the last two decades [11, 13].

#index 1328104
#* Data visualization and social data analysis
#@ Jeffrey Heer;Joseph M. Hellerstein
#t 2009
#c 4
#! Analysts in all areas of human knowledge, from science and engineering to economics, social science and journalism are drowning in data. New technologies for sensing, simulation, and communication are helping people to both collect and produce data at exponential rates.

#index 1328105
#* Keyword querying and ranking in databases
#@ Surajit Chaudhuri;Gautam Das
#t 2009
#c 4
#! With the proliferation of data sources exposed through web interfaces to consumers, simple ways of exploring contents of such databases are of increasing importance. Examples include users wishing to search catalogs of homes, cars, cameras, restaurants, and photographs. One approach that has been explored is to allow users to query such databases in the same ways as they explore web documents. Thus, it is desirable to be able to use the paradigm of keyword querying and automated result ranking over contents of databases. However, the rich relationships and schema information present in databases makes a direct adaptation of information retrieval techniques inappropriate. This problem has attracted much attention in research as it presents a rich set of challenges from defining semantics of such querying model to developing algorithms that ensure adequate performance. In this tutorial, we focus on the highlights of research progress in this field.

#index 1328106
#* Efficient approximate search on string collections
#@ Marios Hadjieleftheriou;Chen Li
#t 2009
#c 4
#% 333679
#% 480654
#% 765463
#% 800590
#% 824684
#% 824717
#% 864392
#% 875066
#% 893105
#% 893164
#% 956458
#% 960270
#% 1022218
#% 1022227
#% 1022229
#% 1055684
#% 1063496
#% 1063530
#% 1063566
#% 1127368
#% 1127425
#% 1181283
#% 1190092
#% 1206665
#% 1206677
#% 1206985
#% 1217179
#% 1217199
#% 1217200
#! This tutorial provides a comprehensive overview of recent research progress on the important problem of approximate search in string collections. We identify existing indexes, search algorithms, filtering strategies, selectivity-estimation techniques and other work, and comment on their respective merits and limitations.

#index 1328107
#* Information theory for data management
#@ Divesh Srivastava;Suresh Venkatasubramanian
#t 2009
#c 4
#% 300711
#% 333876
#% 576092
#% 576111
#% 577233
#% 654458
#% 765462
#% 804840
#% 853038
#% 915256
#% 1181220
#% 1206637
#! We are awash in data. The explosion in computing power and computing infrastructure allows us to generate multitudes of data, in differing formats, at different scales, and in inter-related areas. Data management is fundamentally about the harnessing of this data to extract information, discovering good representations of the information, and analyzing information sources to glean structure. Data management generally presents us with cost-benefit tradeoffs. If we store more information, we get better answers to queries, but we pay the price in terms of increased storage. Conversely, reducing the amount of information we store improves performance at the cost of decreased accuracy for query results. The ability to quantify information gain or loss can only improve our ability to design good representations, storage mechanisms, and analysis tools for data.

#index 1328108
#* Column-oriented database systems
#@ Daniel J. Abadi;Peter A. Boncz;Stavros Harizopoulos
#t 2009
#c 4
#% 286258
#% 824697
#% 864446
#% 875026
#% 893129
#% 1022262
#% 1063542
#% 1217151
#! Column-oriented database systems (column-stores) have attracted a lot of attention in the past few years. Column-stores, in a nutshell, store each database table column separately, with attribute values belonging to the same column stored contiguously, compressed, and densely packed, as opposed to traditional database systems that store entire records (rows) one after the other. Reading a subset of a table's columns becomes faster, at the potential expense of excessive disk-head seeking from column to column for scattered reads or updates. After several dozens of research papers and at least a dozen of new column-store start-ups, several questions remain. Are these a new breed of systems or simply old wine in new bottles? How easily can a major row-based system achieve column-store performance? Are column-stores the answer to effortlessly support large-scale data-intensive applications? What are the new, exciting system research problems to tackle? What are the new applications that can be potentially enabled by column-stores? In this tutorial, we present an overview of column-oriented database system technology and address these and other related questions.

#index 1328109
#* Believe it or not: adding belief annotations to databases
#@ Wolfgang Gatterbauer;Magdalena Balazinska;Nodira Khoussainova;Dan Suciu
#t 2009
#c 4
#% 13036
#% 116625
#% 188086
#% 264858
#% 273687
#% 654468
#% 782175
#% 810020
#% 810115
#% 864394
#% 864469
#% 874971
#% 875015
#% 893167
#% 960267
#% 960293
#% 960352
#% 976984
#% 993374
#% 1058254
#% 1134504
#% 1206792
#% 1206962
#% 1408533
#% 1661440
#! We propose a database model that allows users to annotate data with belief statements. Our motivation comes from scientific database applications where a community of users is working together to assemble, revise, and curate a shared data repository. As the community accumulates knowledge and the database content evolves over time, it may contain conflicting information and members can disagree on the information it should store. For example, Alice may believe that a tuple should be in the database, whereas Bob disagrees. He may also insert the reason why he thinks Alice believes the tuple should be in the database, and explain what he thinks the correct tuple should be instead. We propose a formal model for Belief Databases that interprets users' annotations as belief statements. These annotations can refer both to the base data and to other annotations. We give a formal semantics based on a fragment of multi-agent epistemic logic and define a query language over belief databases. We then prove a key technical result, stating that every belief database can be encoded as a canonical Kripke structure. We use this structure to describe a relational representation of belief databases, and give an algorithm for translating queries over the belief database into standard relational queries. Finally, we report early experimental results with our prototype implementation on synthetic data.

#index 1328110
#* Similarity search on Bregman divergence: towards non-metric indexing
#@ Zhenjie Zhang;Beng Chin Ooi;Srinivasan Parthasarathy;Anthony K. H. Tung
#t 2009
#c 4
#% 86950
#% 264161
#% 316524
#% 317313
#% 427199
#% 479649
#% 635694
#% 724154
#% 748465
#% 810120
#% 814646
#% 871398
#% 875957
#% 876008
#% 893140
#% 916785
#% 1035129
#% 1073886
#% 1269761
#% 1775636
#% 1775752
#! In this paper, we examine the problem of indexing over non-metric distance functions. In particular, we focus on a general class of distance functions, namely Bregman Divergence [6], to support nearest neighbor and range queries. Distance functions such as KL-divergence and Itakura-Saito distance, are special cases of Bregman divergence, with wide applications in statistics, speech recognition and time series analysis among others. Unlike in metric spaces, key properties such as triangle inequality and distance symmetry do not hold for such distance functions. A direct adaptation of existing indexing infrastructure developed for metric spaces is thus not possible. We devise a novel solution to handle this class of distance measures by expanding and mapping points in the original space to a new extended space. Subsequently, we show how state-of-the-art tree-based indexing methods, for low to moderate dimensional datasets, and vector approximation file (VA-file) methods, for high dimensional datasets, can be adapted on this extended space to answer such queries efficiently. Improved distance bounding techniques and distribution-based index optimization are also introduced to improve the performance of query answering and index construction respectively, which can be applied on both the R-trees and VA files. Extensive experiments are conducted to validate our approach on a variety of datasets and a range of Bregman divergence functions.

#index 1328111
#* Comparing stars: on approximating graph edit distance
#@ Zhiping Zeng;Anthony K. H. Tung;Jianyong Wang;Jianhua Feng;Lizhu Zhou
#t 2009
#c 4
#% 236299
#% 251403
#% 260974
#% 309749
#% 316784
#% 335005
#% 344549
#% 378391
#% 403934
#% 404719
#% 443133
#% 443663
#% 466644
#% 629603
#% 629708
#% 765429
#% 810071
#% 824676
#% 824693
#% 833120
#% 864425
#% 885512
#% 893109
#% 912244
#% 918558
#% 937108
#% 960259
#% 960304
#% 1015336
#% 1022279
#% 1022280
#% 1664003
#! Graph data have become ubiquitous and manipulating them based on similarity is essential for many applications. Graph edit distance is one of the most widely accepted measures to determine similarities between graphs and has extensive applications in the fields of pattern recognition, computer vision etc. Unfortunately, the problem of graph edit distance computation is NP-Hard in general. Accordingly, in this paper we introduce three novel methods to compute the upper and lower bounds for the edit distance between two graphs in polynomial time. Applying these methods, two algorithms AppFull and AppSub are introduced to perform different kinds of graph search on graph databases. Comprehensive experimental studies are conducted on both real and synthetic datasets to examine various aspects of the methods for bounding graph edit distance. Result shows that these methods achieve good scalability in terms of both the number of graphs and the size of graphs. The effectiveness of these algorithms also confirms the usefulness of using our bounds in filtering and searching of graphs.

#index 1328112
#* Indexing Boolean expressions
#@ Steven Euijong Whang;Hector Garcia-Molina;Chad Brower;Jayavel Shanmugasundaram;Sergei Vassilvitskii;Erik Vee;Ramana Yerneni
#t 2009
#c 4
#% 158911
#% 194247
#% 271199
#% 290703
#% 333938
#% 385833
#% 536330
#% 576214
#% 631962
#% 640616
#% 646220
#% 726620
#% 730065
#% 742564
#% 840664
#% 867054
#% 875004
#% 960342
#% 987262
#% 1127386
#! We consider the problem of efficiently indexing Disjunctive Normal Form (DNF) and Conjunctive Normal Form (CNF) Boolean expressions over a high-dimensional multi-valued attribute space. The goal is to rapidly find the set of Boolean expressions that evaluate to true for a given assignment of values to attributes. A solution to this problem has applications in online advertising (where a Boolean expression represents an advertiser's user targeting requirements, and an assignment of values to attributes represents the characteristics of a user visiting an online page) and in general any publish/subscribe system (where a Boolean expression represents a subscription, and an assignment of values to attributes represents an event). All existing solutions that we are aware of can only index a specialized sub-set of conjunctive and/or disjunctive expressions, and cannot efficiently handle general DNF and CNF expressions (including NOTs) over multi-valued attributes. In this paper, we present a novel solution based on the inverted list data structure that enables us to index arbitrarily complex DNF and CNF Boolean expressions over multi-valued attributes. An interesting aspect of our solution is that, by virtue of leveraging inverted lists traditionally used for ranked information retrieval, we can efficiently return the top-N matching Boolean expressions. This capability enables emerging applications such as ranked publish/subscribe systems [16], where only the top subscriptions that match an event are desired. For example, in online advertising there is a limit on the number of advertisements that can be shown on a given page and only the "best" advertisements can be displayed. We have evaluated our proposed technique based on data from an online advertising application, and the results show a dramatic performance improvement over prior techniques.

#index 1328113
#* Scalable delivery of stream query result
#@ Yongluan Zhou;Ali Salehi;Karl Aberer
#t 2009
#c 4
#% 248026
#% 248032
#% 248034
#% 271199
#% 289266
#% 300179
#% 338354
#% 397353
#% 452841
#% 465190
#% 572311
#% 599549
#% 646220
#% 745534
#% 765436
#% 800517
#% 800584
#% 810032
#% 875019
#% 875022
#% 1015280
#% 1016157
#% 1016167
#% 1016180
#% 1127385
#% 1164978
#% 1164991
#% 1180904
#% 1206856
#% 1208230
#! Continuous queries over data streams typically produce large volumes of result streams. To scale up the system, one should carefully study the problem of delivering the result streams to the end users, which, unfortunately, is often overlooked in existing systems. In this paper, we leverage Distributed Publish/Subscribe System (DPSS), a scalable data dissemination infrastructure, for efficient stream query result delivery. To take advantage of DPSS's multicast-like data dissemination architecture, one has to exploit the common contents among different result streams and maximize the sharing of their delivery. Hence, we propose to merge the user queries into a few representative queries whose results subsume those of the original ones, and disseminate the result streams of these representative queries through the DPSS. To realize this approach, we study the stream query containment theories and propose efficient query grouping and merging algorithms. The proposed approach is non-intrusive and hence can be easily implemented as a middleware to be incorporated into existing stream processing systems. A prototype is developed on top of an open-source stream processing system and results of an extensive performance study on real datasets verify the efficacy of the proposed techniques.

#index 1328114
#* Schema-based independence analysis for XML updates
#@ Michael Benedikt;James Cheney
#t 2009
#c 4
#% 654475
#% 799999
#% 893111
#% 894404
#% 942354
#% 975035
#% 994015
#% 1015272
#% 1091021
#% 1092015
#% 1180010
#% 1670117
#% 1688277
#% 1718234
#% 1721253
#! Query-update independence analysis is the problem of determining whether an update affects the results of a query. Query-update independence is useful for avoiding recomputation of materialized views and may have applications to access control and concurrency control. This paper develops static analysis techniques for query-update independence problems involving core XQuery queries and updates with a snapshot semantics (based on the W3C XQuery Update Facility proposal). Our approach takes advantage of schema information, in contrast to previous work on this problem. We formalize our approach, sketch a proof of correctness, and report on the performance and accuracy of our implementation.

#index 1328115
#* Tagging stream data for rich real-time services
#@ Rimma V. Nehme;Elke A. Rundensteiner;Elisa Bertino
#t 2009
#c 4
#% 393907
#% 421124
#% 578391
#% 654477
#% 654507
#% 654508
#% 654510
#% 765421
#% 783480
#% 824673
#% 841639
#% 855601
#% 858102
#% 864469
#% 881054
#% 955010
#% 956515
#% 960267
#% 1016204
#% 1016258
#% 1016269
#% 1026953
#% 1036075
#% 1065406
#% 1074117
#% 1083730
#% 1127373
#% 1131211
#% 1156304
#% 1206727
#! In recent years, data streams have become ubiquitous as technology is improving and the prices of portable devices are falling, e.g., sensor networks, location-based services. Most data streams transmit only data tuples based on which continuous queries are evaluated. In this paper, we propose to enrich data streams with a new type of metadata called streaming tags or short tick-tags. The fundamental premise of tagging is that users can label data using uncontrolled vocabulary, and these tags can be exploited in a wide variety of applications, such as data exploration, data search, and to produce "enriched" with additional semantics, thus more informative query results. In this paper we focus primarily on the problem of continuous query processing with streaming tags and tagged objects, and address the tick-tag semantic issues as well as efficiency concerns. Our main contributions are as follows. First, we specify a general and flexible Stream Tag Framework (or short STF) that supports a stream-centric approach to tagging, and where tick-tags, attached to streaming objects are treated as first-class citizens. Second, under STF, users can query tags explicitly as well as implicitly by outputting the tags of the base data together with query results. Finally, we have implemented STF in a prototype Data Stream Management System, and through a set of performance experiments, we show that the cost of stream tagging is small and the approach is scalable to a large percentage of tagged objects.

#index 1328116
#* Randomized multi-pass streaming skyline algorithms
#@ Atish Das Sarma;Ashwin Lall;Danupon Nanongkai;Jun Xu
#t 2009
#c 4
#% 1331
#% 100803
#% 288976
#% 333931
#% 465167
#% 480671
#% 593909
#% 656697
#% 761484
#% 800512
#% 800555
#% 806212
#% 810024
#% 849816
#% 864451
#% 903013
#% 976988
#% 993954
#% 1022203
#% 1022224
#% 1068351
#% 1074714
#% 1127433
#% 1147647
#% 1177872
#% 1206819
#% 1206852
#% 1206866
#% 1206998
#% 1207004
#! We consider external algorithms for skyline computation without pre-processing. Our goal is to develop an algorithm with a good worst case guarantee while performing well on average. Due to the nature of disks, it is desirable that such algorithms access the input as a stream (even if in multiple passes). Using the tools of randomness, proved to be useful in many applications, we present an efficient multi-pass streaming algorithm, RAND, for skyline computation. As far as we are aware, RAND is the first randomized skyline algorithm in the literature. RAND is near-optimal for the streaming model, which we prove via a simple lower bound. Additionally, our algorithm is distributable and can handle partially ordered domains on each attribute. Finally, we demonstrate the robustness of RAND via extensive experiments on both real and synthetic datasets. RAND is comparable to the existing algorithms in average case and additionally tolerant to simple modifications of the data, while other algorithms degrade considerably with such variation.

#index 1328117
#* Managing massive time series streams with multi-scale compressed trickles
#@ Galen Reeves;Jie Liu;Suman Nath;Feng Zhao
#t 2009
#c 4
#% 249321
#% 333881
#% 342617
#% 397385
#% 397389
#% 480156
#% 578390
#% 723282
#% 810058
#% 823413
#% 824709
#% 878302
#% 963135
#% 993961
#% 1062478
#% 1063529
#% 1072067
#% 1083693
#% 1217205
#% 1815965
#% 1816250
#! We present Cypress, a novel framework to archive and query massive time series streams such as those generated by sensor networks, data centers, and scientific computing. Cypress applies multi-scale analysis to decompose time series and to obtain sparse representations in various domains (e.g. frequency domain and time domain). Relying on the sparsity, the time series streams can be archived with reduced storage space. We then show that many statistical queries such as trend, histogram and correlations can be answered directly from compressed data rather than from reconstructed raw data. Our evaluation with server utilization data collected from real data centers shows significant benefit of our framework.

#index 1328118
#* Promotion analysis in multi-dimensional space
#@ Tianyi Wu;Dong Xin;Qiaozhu Mei;Jiawei Han
#t 2009
#c 4
#% 273902
#% 273916
#% 333854
#% 420053
#% 420082
#% 765155
#% 875003
#% 875025
#% 956547
#% 1016203
#% 1063474
#% 1063475
#% 1075132
#% 1206698
#! Promotion is one of the key ingredients in marketing. It is often desirable to find merit in an object (e.g., product, person, organization, or service) and promote it in an appropriate community. In this paper, we propose a novel functionality, called promotion analysis through ranking, for promoting a given object by leveraging highly ranked results. Since the object may not be highly ranked in the global space, our goal is to discover promotive subspaces in which the object becomes prominent. To achieve this goal, the notion of promotiveness is formulated. We show that this functionality is practical and useful in a wide variety of applications such as business intelligence. However, computing promotive subspaces is challenging due to the explosion of search space and high aggregation cost. For efficient computation, we propose a PromoRank framework, and develop three efficient optimization techniques, namely subspace pruning, object pruning, and promotion cube, which are seamlessly integrated into the framework. Our empirical evaluation on two real data sets confirms the effectiveness of promotion analysis, and that our proposed algorithms significantly outperform baseline solutions.

#index 1328119
#* Measure-driven keyword-query expansion
#@ Nikos Sarkas;Nilesh Bansal;Gautam Das;Nick Koudas
#t 2009
#c 4
#% 223781
#% 227919
#% 333854
#% 342597
#% 482095
#% 528023
#% 757953
#% 824682
#% 832569
#% 838540
#% 878207
#% 1022269
#% 1035573
#% 1127356
#% 1127403
#! User generated content has been fueling an explosion in the amount of available textual data. In this context, it is also common for users to express, either explicitly (through numerical ratings) or implicitly, their views and opinions on products, events, etc. This wealth of textual information necessitates the development of novel searching and data exploration paradigms. In this paper we propose a new searching model, similar in spirit to faceted search, that enables the progressive refinement of a keyword-query result. However, in contrast to faceted search which utilizes domain-specific and hard-to-extract document attributes, the refinement process is driven by suggesting interesting expansions of the original query with additional search terms. Our query-driven and domain-neutral approach employs surprising word co-occurrence patterns and (optionally) numerical user ratings in order to identify meaningful top-k query expansions and allow one to focus on a particularly interesting subset of the original result set. The proposed functionality is supported by a framework that is computationally efficient and nimble in terms of storage requirements. Our solution is grounded on Convex Optimization principles that allow us to exploit the pruning opportunities offered by the natural top-k formulation of our problem. The performance benefits offered by our solution are verified using both synthetic data and large real data sets comprised of blog posts.

#index 1328120
#* Using trees to depict a forest
#@ Bin Liu;H. V. Jagadish
#t 2009
#c 4
#% 210173
#% 248790
#% 248792
#% 300132
#% 333854
#% 408396
#% 443397
#% 443531
#% 527022
#% 765518
#% 844324
#% 875957
#% 879565
#% 894444
#% 900234
#% 960244
#% 1018390
#% 1022276
#% 1022314
#% 1047501
#% 1066736
#% 1206662
#% 1206925
#% 1217262
#% 1348355
#% 1720745
#% 1861495
#! When a database query has a large number of results, the user can only be shown one page of results at a time. One popular approach is to rank results such that the "best" results appear first. However, standard database query results comprise a set of tuples, with no associated ranking. It is typical to allow users the ability to sort results on selected attributes, but no actual ranking is defined. An alternative approach to the first page is not to try to show the best results, but instead to help users learn what is available in the whole result set and direct them to finding what they need. In this paper, we demonstrate through a user study that a page comprising one representative from each of k clusters (generated through a k-medoid clustering) is superior to multiple alternative candidate methods for generating representatives of a data set. Users often refine query specifications based on returned results. Traditional clustering may lead to completely new representatives after a refinement step. Furthermore, clustering can be computationally expensive. We propose a tree-based method for efficiently generating the representatives, and smoothly adapting them with query refinement. Experiments show that our algorithms outperform the state-of-the-art in both result quality and efficiency.

#index 1328121
#* Online piece-wise linear approximation of numerical streams with precision guarantees
#@ Hazem Elmeleegy;Ahmed K. Elmagarmid;Emmanuel Cecchet;Walid G. Aref;Willy Zwaenepoel
#t 2009
#c 4
#% 19618
#% 300179
#% 466506
#% 546061
#% 654488
#% 654508
#% 657671
#% 745434
#% 745513
#% 765402
#% 765403
#% 765445
#% 805466
#% 1015280
#% 1058620
#% 1394366
#! Continuous "always-on" monitoring is beneficial for a number of applications, but potentially imposes a high load in terms of communication, storage and power consumption when a large number of variables need to be monitored. We introduce two new filtering techniques, swing filters and slide filters, that represent within a prescribed precision a time-varying numerical signal by a piecewise linear function, consisting of connected line segments for swing filters and (mostly) disconnected line segments for slide filters. We demonstrate the effectiveness of swing and slide filters in terms of their compression power by applying them to a real-life data set plus a variety of synthetic data sets. For nearly all combinations of signal behavior and precision requirements, the proposed techniques outperform the earlier approaches for online filtering in terms of data reduction. The slide filter, in particular, consistently dominates all other filters, with up to twofold improvement over the best of the previous techniques.

#index 1328122
#* A wavelet transform for efficient consolidation of sensor relations with quality guarantees
#@ Mirco Stern;Erik Buchmann;Klemens Böhm
#t 2009
#c 4
#% 36672
#% 237203
#% 248822
#% 257637
#% 261353
#% 273902
#% 333969
#% 397389
#% 480306
#% 480465
#% 480628
#% 654482
#% 745442
#% 751027
#% 765402
#% 799142
#% 800503
#% 801684
#% 805466
#% 824686
#% 862569
#% 864435
#% 893582
#% 956456
#% 983200
#% 1016178
#% 1201875
#% 1387822
#% 1394366
#% 1703162
#% 1740388
#% 1855064
#! Answering queries with a low selectivity in wireless sensor networks is a challenging problem. A simple tree-based data collection is communication-intensive and costly in terms of energy. Prior work has addressed the problem by approximating query results based on models of sensor readings. This cuts communication effort if the accuracy requirements are loose, e.g., if the temperature is required within ±0.5°C. For more accuracy, the models need frequent updates, and the communication costs quickly increase. In addition, sophisticated models incur substantial training costs. We propose a query-processing scheme that efficiently consolidates sensor data based on wavelet synopses. The difficulty is that the synopsis has to be constructed incrementally during data collection to ensure efficiency. Our core contribution is to show how to distribute the construction of wavelet synopses in sensor networks. In addition, our approach provides strict error guarantees. We evaluate our distributed wavelet compaction on real-world and on synthetic sensor data. Our solution reduces communication costs by more than a factor of five compared to state-of-the-art approaches. Further, our error guarantees for which efficient data consolidation is possible are better than theirs by more than an order of magnitude.

#index 1328123
#* Enabling ε-approximate querying in sensor networks
#@ Liu Yu;Jianzhong Li;Hong Gao;Xiaolin Fang
#t 2009
#c 4
#% 227883
#% 332133
#% 443397
#% 449869
#% 480306
#% 731091
#% 751027
#% 765445
#% 800505
#% 805466
#% 837835
#% 839454
#% 850727
#% 863307
#% 864435
#% 864457
#% 893104
#% 938509
#% 981632
#% 1016178
#% 1206599
#% 1217205
#% 1672728
#! Data approximation is a popular means to support energy-efficient query processing in sensor networks. Conventional data approximation methods require users to specify fixed error bounds a prior to address the trade-off between result accuracy and energy efficiency of queries. We argue that this can be infeasible and inefficient when, as in many real-world scenarios, users are unable to determine in advance what error bounds can lead to affordable cost in query processing. We envision ε-approximate querying (EAQ) to bridge the gap. EAQ is a uniform data access scheme underlying various queries in sensor networks. It allows users or query executors to incrementally 'refine' previously obtained approximate data to reach arbitrary accuracy. EAQ not only grants more flexibility to in-network query processing, but also minimizes energy consumption through communicating data upto a just-sufficient level. To enable the EAQ scheme, we propose a novel data shuffling algorithm. The algorithm converts sensed datasets into special representations called multi-version array (MVA). From prefixes of MVA, we can recover approximate versions of the entire dataset, where all individual data items have guaranteed error bounds. The EAQ scheme supports efficient and flexible processing of various queries including spatial window query, value range query, and queries with QoS constraints. The effectiveness and efficiency of the EAQ scheme are evaluated in a real sensor network testbed.

#index 1328124
#* HAMSTER: using search clicklogs for schema and taxonomy matching
#@ Arnab Nandi;Philip A. Bernstein
#t 2009
#c 4
#% 248801
#% 325683
#% 330767
#% 333990
#% 333997
#% 342961
#% 378409
#% 480645
#% 572314
#% 660001
#% 790852
#% 800497
#% 810014
#% 810103
#% 824735
#% 830529
#% 864573
#% 869564
#% 875039
#% 893193
#% 924747
#% 1055706
#% 1089471
#% 1127412
#% 1206613
#% 1206636
#% 1712595
#! We address the problem of unsupervised matching of schema information from a large number of data sources into the schema of a data warehouse. The matching process is the first step of a framework to integrate data feeds from third-party data providers into a structured-search engine's data warehouse. Our experiments show that traditional schema-based and instance-based schema matching methods fall short. We propose a new technique based on the search engine's clicklogs. Two schema elements are matched if the distribution of keyword queries that cause click-throughs on their instances are similar. We present experiments on large commercial datasets that show the new technique has much better accuracy than traditional techniques.

#index 1328125
#* Cooperative update exchange in the Youtopia system
#@ Łucja Kot;Christoph Koch
#t 2009
#c 4
#% 287336
#% 287339
#% 320902
#% 333988
#% 336201
#% 411708
#% 564416
#% 800006
#% 800551
#% 801692
#% 824769
#% 826032
#% 874876
#% 874971
#% 912245
#% 938575
#% 1015302
#% 1022258
#% 1063534
#% 1063724
#% 1127371
#! Youtopia is a platform for collaborative management and integration of relational data. At the heart of Youtopia is an update exchange abstraction: changes to the data propagate through the system to satisfy user-specified mappings. We present a novel change propagation model that combines a deterministic chase with human intervention. The process is fundamentally cooperative and gives users significant control over how mappings are repaired. An additional advantage of our model is that mapping cycles can be permitted without compromising correctness. We investigate potential harmful interference between updates in our model; we introduce two appropriate notions of serializability that avoid such interference if enforced. The first is very general and related to classical final-state serializability; the second is more restrictive but highly practical and related to conflict-serializability. We present an algorithm to enforce the latter notion. Our algorithm is an optimistic one, and as such may sometimes require updates to be aborted. We develop techniques for reducing the number of aborts and we test these experimentally.

#index 1328126
#* Reference-based alignment in large sequence databases
#@ Panagiotis Papapetrou;Vassilis Athitsos;George Kollios;Dimitrios Gunopulos
#t 2009
#c 4
#% 2324
#% 211061
#% 269546
#% 287434
#% 320454
#% 435476
#% 674116
#% 731324
#% 768815
#% 824678
#% 842802
#% 893163
#% 1015330
#% 1022219
#% 1022227
#% 1044404
#% 1063496
#% 1063497
#% 1206665
#% 1716925
#! This paper introduces a novel method, called Reference-Based String Alignment (RBSA), that speeds up retrieval of optimal subsequence matches in large databases of sequences under the edit distance and the Smith-Waterman similarity measure. RBSA operates using the assumption that the optimal match deviates by a relatively small amount from the query, an amount that does not exceed a prespecified fraction of the query length. RBSA has an exact version that guarantees no false dismissals and can handle large queries efficiently. An approximate version of RBSA is also described, that achieves significant additional improvements over the exact version, with negligible losses in retrieval accuracy. RBSA performs filtering of candidate matches using precomputed alignment scores between the database sequence and a set of fixed-length reference sequences. At query time, the query sequence is partitioned into segments of length equal to that of the reference sequences. For each of those segments, the alignment scores between the segment and the reference sequences are used to efficiently identify a relatively small number of candidate subsequence matches. An alphabet collapsing technique is employed to improve the pruning power of the filter step. In our experimental evaluation, RBSA significantly outperforms state-of-the-art biological sequence alignment methods, such as q-grams, BLAST, and BWT.

#index 1328127
#* Thread cooperation in multicore architectures for frequency counting over multiple data streams
#@ Sudipto Das;Shyam Antony;Divyakant Agrawal;Amr El Abbadi
#t 2009
#c 4
#% 4618
#% 401957
#% 654443
#% 654497
#% 765499
#% 800496
#% 800582
#% 810059
#% 858329
#% 875023
#% 879400
#% 894443
#% 981649
#% 993960
#% 1022217
#% 1022232
#% 1052066
#% 1063508
#% 1063543
#% 1127608
#% 1207012
#! Many real-world data stream analysis applications such as network monitoring, click stream analysis, and others require combining multiple streams of data arriving from multiple sources. This is referred to as multi-stream analysis. To deal with high stream arrival rates, it is desirable that such systems be capable of supporting very high processing throughput. The advent of multicore processors and powerful servers driven by these processors calls for efficient parallel designs that can effectively utilize the parallelism of the multicores, since performance improvement is possible only through effective parallelism. In this paper, we address the problem of parallelizing multi-stream analysis in the context of multicore processors. Specifically, we concentrate on parallelizing frequent elements, top-k, and frequency counting over multiple streams. We discuss the challenges in designing an efficient parallel system for multi-stream processing. Our evaluation and analysis reveals that traditional "contention" based locking results in excessive overhead and wait, which in turn leads to severe performance degradation in modern multicore architectures. Based on our analysis, we propose a "cooperation" based locking paradigm for efficient parallelization of frequency counting. The proposed "cooperation" based paradigm removes waits associated with synchronization, and allows replacing locks by much cheaper atomic synchronization primitives. Our implementation of the proposed paradigm to parallelize a well known frequency counting algorithm shows the benefits of the proposed "cooperation" based locking paradigm when compared to the traditional "contention" based locking paradigm. In our experiments, the proposed "cooperation" based design outperforms the traditional "contention" based design by a factor of 2--5.5X for synthetic zipfian data sets.

#index 1328128
#* Streams on wires: a query compiler for FPGAs
#@ Rene Mueller;Jens Teubner;Gustavo Alonso
#t 2009
#c 4
#% 334006
#% 464215
#% 471385
#% 850738
#% 874997
#% 960254
#% 1011733
#% 1015280
#% 1022232
#% 1157687
#! Taking advantage of many-core, heterogeneous hardware for data processing tasks is a difficult problem. In this paper, we consider the use of FPGAs for data stream processing as coprocessors in many-core architectures. We present Glacier, a component library and compositional compiler that transforms continuous queries into logic circuits by composing library components on an operator-level basis. In the paper we consider selection, aggregation, grouping, as well as windowing operators, and discuss their design as modular elements. We also show how significant performance improvements can be achieved by inserting the FPGA into the system's data path (e.g., between the network interface and the host CPU). Our experiments show that queries on the FPGA can process streams at more than one million tuples per second and that they can do this directly from the network, removing much of the overhead of transferring the data to a conventional CPU.

#index 1328129
#* On-the-fly progress detection in iterative stream queries
#@ Badrish Chandramouli;Jonathan Goldstein;David Maier
#t 2009
#c 4
#% 13026
#% 322955
#% 480602
#% 578391
#% 745434
#% 801694
#% 810033
#% 821939
#% 824742
#% 835186
#% 874978
#% 875004
#% 938093
#% 1022301
#% 1063480
#% 1127373
#% 1127442
#% 1206641
#% 1206880
#% 1328078
#% 1700120
#! Multiple researchers have proposed cyclic query plans for evaluating iterative queries over streams or rapidly changing input. The Declarative Networking community uses cyclic plans to evaluate Datalog programs that track reachability and other graph traversals on networks. Cyclic query plans can also evaluate pattern-matching and other queries based on event sequences. An issue with cyclic queries over dynamic inputs is knowing when the query result has progressed to a certain point in the input, since the number of iterations is data dependent. One option is a "strictly staged" computation, where the query plan quiesces between inputs. This option introduces significant latency, and may also "underload" inter-operator buffers. An alternative is to settle for soft guarantees, such as "eventual consistency". Such imprecision can make it difficult, for example, to know when to purge state from stateful operators. We propose a third option in which cyclic queries run continuously, but detect progress "on the fly" by means of a Flying Fixed-Point (FFP) operator. FFP sits on the cyclic loop and circulates speculative predictions on forward progress, which it then validates. FFP is always able to track progress for a class of queries we term strongly convergent. A key advantage of FFP is that it works with existing algebra operators, thereby inheriting their capabilities, such as windowing and dealing with out-of-order input. Also, for stream systems that explicitly model input-event lifetimes, we know exactly which values are in the query result at each point in time. A key implementation decision is the method for speculating. Using the high-water mark of data events minimizes the number of speculative punctuations. Probing operators on the cyclic loop to determine their external progress circulates many more speculative messages, but tracks actual output progress more closely. We show how a hybrid approach limits predictions while coming close the progress-tracking ability of Probing.

#index 1328130
#* Consistency rationing in the cloud: pay only when it matters
#@ Tim Kraska;Martin Hentschel;Gustavo Alonso;Donald Kossmann
#t 2009
#c 4
#% 4618
#% 9241
#% 153305
#% 172913
#% 201869
#% 264263
#% 333969
#% 336201
#% 392578
#% 403195
#% 435134
#% 571217
#% 577348
#% 610967
#% 745536
#% 762650
#% 765469
#% 824689
#% 963655
#% 1002142
#% 1022200
#% 1022298
#% 1061893
#% 1063488
#% 1118177
#% 1127560
#% 1127596
#% 1217223
#! Cloud storage solutions promise high scalability and low cost. Existing solutions, however, differ in the degree of consistency they provide. Our experience using such systems indicates that there is a non-trivial trade-off between cost, consistency and availability. High consistency implies high cost per transaction and, in some situations, reduced availability. Low consistency is cheaper but it might result in higher operational cost because of, e.g., overselling of products in a Web shop. In this paper, we present a new transaction paradigm, that not only allows designers to define the consistency guarantees on the data instead at the transaction level, but also allows to automatically switch consistency guarantees at runtime. We present a number of techniques that let the system dynamically adapt the consistency level by monitoring the data and/or gathering temporal statistics of the data. We demonstrate the feasibility and potential of the ideas through extensive experiments on a first prototype implemented on Amazon's S3 and running the TPC-W benchmark. Our experiments indicate that the adaptive strategies presented in the paper result in a significant reduction in response time and costs including the cost penalties of inconsistencies.

#index 1328131
#* Locking key ranges with unbundled transaction services
#@ David Lomet;Mohamed F. Mokbel
#t 2009
#c 4
#% 9241
#% 235084
#% 317988
#% 320902
#% 393844
#% 393907
#% 403195
#% 480589
#% 481256
#% 750829
#% 1087253
#% 1087317
#! To adapt database technology to new environments like cloud platforms or multi-core hardware, or to try anew to provide an extensible database platform, it is useful to separate transaction services from data management elements that need close physical proximity to data. With "generic" transactional services of concurrency control and recovery in a separate transactional component (TC), indexing, cache and disk management, now in a data component (DC), can be simplified and tailored more easily to the platform or to a data type extension with a special purpose index. This decomposition requires that details of the DC's management of data be hidden from the TC. Thus, locking and logging need to be "logical", which poses a number of problems. One problem is the handling of locking for ranges of keys. Locks need to be taken at the TC prior to the records and their keys being known to the TC. We describe generic two approaches for dealing with this. (1) Make a "speculative" visit" to the DC to learn key values. (2) Lock a "covering resource" first, then learn and lock key values and ultimately release the covering resource lock. The "table" is the only logical (and hence known to the TC) covering resourse in the traditional locking hierarchy, but using it limits concurrency. Concurrency is improved with the introduction of new partition resources. We show how partitions as covering resources combine high concurrency with low locking overhead. Using partitions is sufficiently effective to consider adapting it for a traditional database kernel.

#index 1328132
#* A scalable, predictable join operator for highly concurrent data warehouses
#@ George Candea;Neoklis Polyzotis;Radek Vingralek
#t 2009
#c 4
#% 36117
#% 172968
#% 210182
#% 300167
#% 300179
#% 397353
#% 654510
#% 765435
#% 810039
#% 875026
#% 1022230
#% 1022262
#% 1022298
#% 1063479
#% 1063481
#% 1063717
#% 1127399
#% 1206624
#! Conventional data warehouses employ the query-at-a-time model, which maps each query to a distinct physical plan. When several queries execute concurrently, this model introduces contention, because the physical plans---unaware of each other---compete for access to the underlying I/O and computation resources. As a result, while modern systems can efficiently optimize and evaluate a single complex data analysis query, their performance suffers significantly when multiple complex queries run at the same time. We describe an augmentation of traditional query engines that improves join throughput in large-scale concurrent data warehouses. In contrast to the conventional query-at-a-time model, our approach employs a single physical plan that can share I/O, computation, and tuple storage across all in-flight join queries. We use an "always-on" pipeline of non-blocking operators, coupled with a controller that continuously examines the current query mix and performs run-time optimizations. Our design allows the query engine to scale gracefully to large data sets, provide predictable execution times, and reduce contention. In our empirical evaluation, we found that our prototype outperforms conventional commercial systems by an order of magnitude for tens to hundreds of concurrent queries.

#index 1328133
#* Answering table augmentation queries from unstructured lists on the web
#@ Rahul Gupta;Sunita Sarawagi
#t 2009
#c 4
#% 235941
#% 283180
#% 330784
#% 348146
#% 577238
#% 654467
#% 769877
#% 788090
#% 805846
#% 864415
#% 870896
#% 893168
#% 915340
#% 1023488
#% 1117028
#% 1166537
#% 1264778
#% 1272213
#% 1328199
#! We present the design of a system for assembling a table from a few example rows by harnessing the huge corpus of information-rich but unstructured lists on the web. We developed a totally unsupervised end to end approach which given the sample query rows --- (a) retrieves HTML lists relevant to the query from a pre-indexed crawl of web lists, (b) segments the list records and maps the segments to the query schema using a statistical model, (c) consolidates the results from multiple lists into a unified merged table, (d) and presents to the user the consolidated records ranked by their estimated membership in the target relation. The key challenges in this task include construction of new rows from very few examples, and an abundance of noisy and irrelevant lists that swamp the consolidation and ranking of rows. We propose modifications to statistical record segmentation models, and present novel consolidation and ranking techniques that can process input tables of arbitrary schema without requiring any human supervision. Experiments with Wikipedia target tables and 16 million unstructured lists show that even with just three sample rows, our system is very effective at recreating Wikipedia tables, with a mean runtime of around 20s.

#index 1328134
#* Efficient rewriting of XPath queries using Query Set Specifications
#@ Bogdan Cautis;Alin Deutsch;Nicola Onose;Vasilis Vassalos
#t 2009
#c 4
#% 278830
#% 459241
#% 570877
#% 733593
#% 765145
#% 765450
#% 824661
#% 824690
#% 826029
#% 1016134
#% 1022209
#% 1180003
#% 1206683
#! We study the problem of querying XML data sources that accept only a limited set of queries, such as sources accessible by Web services which can implement very large (potentially infinite) families of XPath queries. To compactly specify such families of queries we adopt the Query Set Specifications [14], a formalism close to context-free grammars. We say that query Q is expressible by the specification P if it is equivalent to some expansion of P. Q is supported by P if it has an equivalent rewriting using some finite set of P's expansions. We study the complexity of expressibility and support and identify large classes of XPath queries for which there are efficient (PTIME) algorithms. Our study considers both the case in which the XML nodes in the results of the queries lose their original identity and the one in which the source exposes persistent node ids.

#index 1328135
#* Structured search result differentiation
#@ Ziyang Liu;Peng Sun;Yi Chen
#t 2009
#c 4
#% 590523
#% 654442
#% 659990
#% 810052
#% 824693
#% 863389
#% 874894
#% 875003
#% 956599
#% 960243
#% 960261
#% 993987
#% 1015325
#% 1016135
#% 1019060
#% 1063493
#% 1063537
#% 1063539
#% 1127424
#% 1181282
#% 1206698
#! Studies show that about 50% of web search is for information exploration purpose, where a user would like to investigate, compare, evaluate, and synthesize multiple relevant results. Due to the absence of general tools that can effectively analyze and differentiate multiple results, a user has to manually read and comprehend potentially large results in an exploratory search. Such a process is time consuming, labor intensive and error prone. With meta information embedded, keyword search on structured data provides the potential for automating or semi-automating the comparison of multiple results. In this paper we present an approach for differentiating search results on structured data. We define the differentiability of query results and quantify the degree of difference. Then we define the problem of identifying a limited number of valid features in a result that can maximally differentiate this result from the others, which is proved to be NP-hard. We propose two local optimality conditions, namely single-swap and multi-swap. Efficient algorithms are designed to achieve local optimality. To show the applicability of our approach, we implemented a system XRed for XML result differentiation. Our empirical evaluation verifies the effectiveness and efficiency of the proposed approach.

#index 1328136
#* A hierarchical approach to model web query interfaces for web source integration
#@ Eduard C. Dragut;Thomas Kabisch;Clement Yu;Ulf Leser
#t 2009
#c 4
#% 330782
#% 331011
#% 376266
#% 480479
#% 577319
#% 660001
#% 732669
#% 765409
#% 765410
#% 769890
#% 777930
#% 783791
#% 800498
#% 810110
#% 864433
#% 864434
#% 893144
#% 956537
#% 956538
#% 993982
#% 1016163
#% 1061893
#% 1683872
#! Much data in the Web is hidden behind Web query interfaces. In most cases the only means to "surface" the content of a Web database is by formulating complex queries on such interfaces. Applications such as Deep Web crawling and Web database integration require an automatic usage of these interfaces. Therefore, an important problem to be addressed is the automatic extraction of query interfaces into an appropriate model. We hypothesize the existence of a set of domain-independent "commonsense design rules" that guides the creation of Web query interfaces. These rules transform query interfaces into schema trees. In this paper we describe a Web query interface extraction algorithm, which combines HTML tokens and the geometric layout of these tokens within a Web page. Tokens are classified into several classes out of which the most significant ones are text tokens and field tokens. A tree structure is derived for text tokens using their geometric layout. Another tree structure is derived for the field tokens. The hierarchical representation of a query interface is obtained by iteratively merging these two trees. Thus, we convert the extraction problem into an integration problem. Our experiments show the promise of our algorithm: it outperforms the previous approaches on extracting query interfaces on about 6.5% in accuracy as evaluated over three corpora with more than 500 Deep Web interfaces from 15 different domains.

#index 1328137
#* Efficient retrieval of the top-k most relevant spatial web objects
#@ Gao Cong;Christian S. Jensen;Dingming Wu
#t 2009
#c 4
#% 86950
#% 201876
#% 212665
#% 227939
#% 249989
#% 262096
#% 287466
#% 318437
#% 330677
#% 340886
#% 387427
#% 408396
#% 427199
#% 458517
#% 464195
#% 480467
#% 617846
#% 643566
#% 659993
#% 750863
#% 766441
#% 818229
#% 836096
#% 838407
#% 867054
#% 874993
#% 982560
#% 1074109
#% 1206801
#% 1206997
#% 1720754
#! The conventional Internet is acquiring a geo-spatial dimension. Web documents are being geo-tagged, and geo-referenced objects such as points of interest are being associated with descriptive text documents. The resulting fusion of geo-location and documents enables a new kind of top-k query that takes into account both location proximity and text relevancy. To our knowledge, only naive techniques exist that are capable of computing a general web information retrieval query while also taking location into account. This paper proposes a new indexing framework for location-aware top-k text retrieval. The framework leverages the inverted file for text retrieval and the R-tree for spatial proximity querying. Several indexing approaches are explored within the framework. The framework encompasses algorithms that utilize the proposed indexes for computing the top-k query, thus taking into account both text relevancy and location proximity to prune the search space. Results of empirical studies with an implementation of the framework demonstrate that the paper's proposal offers scalability and is capable of excellent performance.

#index 1328138
#* Stop word and related problems in web interface integration
#@ Eduard Dragut;Fang Fang;Prasad Sistla;Clement Yu;Weiyi Meng
#t 2009
#c 4
#% 69498
#% 288780
#% 387427
#% 572314
#% 654459
#% 660001
#% 747738
#% 756964
#% 765409
#% 786515
#% 786523
#% 794511
#% 824659
#% 864433
#% 869500
#% 893144
#% 956537
#% 956570
#% 956571
#% 975019
#% 993982
#% 1016163
#% 1328136
#! The goal of recent research projects on integrating Web databases has been to enable uniform access to the large amount of data behind query interfaces. Among the tasks addressed are: source discovery, query interface extraction, schema matching, etc. There are also a number of tasks that are commonly ignored or assumed to be apriori solved either manually or by some oracle. These tasks include (1) finding the set of stop words and (2) handling occurrences of "semantic enrichment words" within labels. These two subproblems have a direct impact on determining the synonymy and hyponymy relationships between labels. In (1), a word like "from" is a stop word in general but it is a content word in domains such as Airline and Real Estate. We formulate the stop word problem, prove its complexity and provide an approximation algorithm. In (2), we study the impact of words like AND and OR on establishing semantic relationships between labels (e.g. "departure date and time" is a hypernym of "departure date"). In addition, we develop a theoretical framework to differentiate synonymy relationship from hyponymy relationship among labels involving multiple words. We scrutinize its strength and limitations both analytically and experimentally. We use real data from the Web in our experiments. We analyze over 2300 labels of 220 user interfaces in 9 distinct domains.

#index 1328139
#* Lazy-Adaptive Tree: an optimized index structure for flash devices
#@ Devesh Agrawal;Deepak Ganesan;Ramesh Sitaraman;Yanlei Diao;Shashi Singh
#t 2009
#c 4
#% 261358
#% 481599
#% 862585
#% 903035
#% 951778
#% 957221
#% 960238
#% 985755
#% 996050
#% 1015288
#% 1052068
#% 1063551
#% 1085291
#% 1092670
#% 1127391
#% 1207002
#! Flash memories are in ubiquitous use for storage on sensor nodes, mobile devices, and enterprise servers. However, they present significant challenges in designing tree indexes due to their fundamentally different read and write characteristics in comparison to magnetic disks. In this paper, we present the Lazy-Adaptive Tree (LA-Tree), a novel index structure that is designed to improve performance by minimizing accesses to flash. The LA-tree has three key features: 1) it amortizes the cost of node reads and writes by performing update operations in a lazy manner using cascaded buffers, 2) it dynamically adapts buffer sizes to workload using an online algorithm, which we prove to be optimal under the cost model for raw NAND flashes, and 3) it optimizes index parameters, memory management, and storage reclamation to address flash constraints. Our performance results on raw NAND flashes show that the LA-Tree achieves 2x to 12x gains over the best of alternate schemes across a range of workloads and memory constraints. Initial results on SSDs are also promising, with 3x to 6x gains in most cases.

#index 1328140
#* MCC-DB: minimizing cache conflicts in multi-core processors for databases
#@ Rubao Lee;Xiaoning Ding;Feng Chen;Qingda Lu;Xiaodong Zhang
#t 2009
#c 4
#% 248793
#% 251477
#% 291643
#% 300194
#% 348037
#% 463759
#% 479821
#% 480119
#% 481450
#% 580978
#% 765417
#% 765467
#% 765468
#% 824655
#% 916765
#% 960148
#% 960157
#% 979000
#% 983261
#% 993385
#% 993947
#% 1002624
#% 1015288
#% 1022230
#% 1054482
#% 1127399
#% 1185786
#% 1285684
#% 1794939
#! In a typical commercial multi-core processor, the last level cache (LLC) is shared by two or more cores. Existing studies have shown that the shared LLC is beneficial to concurrent query processes with commonly shared data sets. However, the shared LLC can also be a performance bottleneck to concurrent queries, each of which has private data structures, such as a hash table for the widely used hash join operator, causing serious cache conflicts. We show that cache conflicts on multi-core processors can significantly degrade overall database performance. In this paper, we propose a hybrid system method called MCC-DB for accelerating executions of warehouse-style queries, which relies on the DBMS knowledge of data access patterns to minimize LLC conflicts in multi-core systems through an enhanced OS facility of cache partitioning. MCC-DB consists of three components: (1) a cacheaware query optimizer carefully selects query plans in order to balance the numbers of cache-sensitive and cache-insensitive plans; (2) a query execution scheduler makes decisions to co-run queries with an objective of minimizing LLC conflicts; and (3) an enhanced OS kernel facility partitions the shared LLC according to each query's cache capacity need and locality strength. We have implemented MCC-DB by patching the three components in PostgreSQL and Linux kernel. Our intensive measurements on an Intel multi-core system with warehouse-style queries show that MCC-DB can reduce query execution times by up to 33%.

#index 1328141
#* SIMD-scan: ultra fast in-memory table scan using on-chip vector processing units
#@ Thomas Willhalm;Nicolae Popovici;Yazan Boshmaf;Hasso Plattner;Alexander Zeier;Jan Schaffner
#t 2009
#c 4
#% 69072
#% 146203
#% 322412
#% 397361
#% 440215
#% 464843
#% 645827
#% 864446
#% 893129
#% 960266
#% 1052066
#% 1127399
#% 1127400
#! The availability of huge system memory, even on standard servers, generated a lot of interest in main memory database engines. In data warehouse systems, highly compressed column-oriented data structures are quite prominent. In order to scale with the data volume and the system load, many of these systems are highly distributed with a shared-nothing approach. The fundamental principle of all systems is a full table scan over one or multiple compressed columns. Recent research proposed different techniques to speedup table scans like intelligent compression or using an additional hardware such as graphic cards or FPGAs. In this paper, we show that utilizing the embedded Vector Processing Units (VPUs) found in standard superscalar processors can speed up the performance of mainmemory full table scan by factors. This is achieved without changing the hardware architecture and thereby without additional power consumption. Moreover, as on-chip VPUs directly access the system's RAM, no additional costly copy operations are needed for using the new SIMD-scan approach in standard main memory database engines. Therefore, we propose this scan approach to be used as the standard scan operator for compressed column-oriented main memory storage. We then discuss how well our solution scales with the number of processor cores; consequently, to what degree it can be applied in multi-threaded environments. To verify the feasibility of our approach, we implemented the proposed techniques on a modern Intel multi-core processor using Intel® Streaming SIMD Extensions (Intel® SSE). In addition, we integrated the new SIMD-scan approach into SAP® Netweaver® Business Warehouse Accelerator. We conclude with describing the performance benefits of using our approach for processing and scanning compressed data using VPUs in column-oriented main memory database systems.

#index 1328142
#* Mining document collections to facilitate accurate approximate entity matching
#@ Surajit Chaudhuri;Venkatesh Ganti;Dong Xin
#t 2009
#c 4
#% 279755
#% 300120
#% 321327
#% 464434
#% 747738
#% 748463
#% 769884
#% 810014
#% 864392
#% 864415
#% 875066
#% 1023420
#% 1063530
#% 1074395
#% 1127426
#% 1127559
#% 1190070
#% 1206615
#% 1328152
#! Many entity extraction techniques leverage large reference entity tables to identify entities in documents. Often, an entity is referenced in document collections differently from that in the reference entity tables. Therefore, we study the problem of determining whether or not a substring "approximately" matches with a reference entity. Similarity measures which exploit the correlation between candidate substrings and reference entities across a large number of documents are known to be more robust than traditional stand alone string-based similarity functions. However, such an approach has significant efficiency challenges. In this paper, we adopt a new architecture and propose new techniques to address these efficiency challenges. We mine document collections and expand a given reference entity table with variations of each of its entities. Thus, the problem of approximately matching an input string against reference entities reduces to that of exact match against the expanded reference table, which can be implemented efficiently. In an extensive experimental evaluation, we demonstrate the accuracy and scalability of our techniques.

#index 1328143
#* Reasoning about record matching rules
#@ Wenfei Fan;Xibei Jia;Jianzhong Li;Shuai Ma
#t 2009
#c 4
#% 201889
#% 203623
#% 287295
#% 312166
#% 384978
#% 480496
#% 577238
#% 577263
#% 765433
#% 778320
#% 836134
#% 903332
#% 913783
#% 960270
#% 993980
#% 1016182
#% 1022229
#% 1063725
#% 1127558
#% 1206615
#% 1206834
#% 1206990
#% 1269495
#% 1669513
#% 1673578
#! To accurately match records it is often necessary to utilize the semantics of the data. Functional dependencies (FDs) have proven useful in identifying tuples in a clean relation, based on the semantics of the data. For all the reasons that FDs and their inference are needed, it is also important to develop dependencies and their reasoning techniques for matching tuples from unreliable data sources. This paper investigates dependencies and their reasoning for record matching. (a) We introduce a class of matching dependencies (MDs) for specifying the semantics of data in unreliable relations, defined in terms of similarity metrics and a dynamic semantics. (b) We identify a special case of MDs, referred to as relative candidate keys (RCKs), to determine what attributes to compare and how to compare them when matching records across possibly different relations. (c) We propose a mechanism for inferring MDs, a departure from traditional implication analysis, such that when we cannot match records by comparing attributes that contain errors, we may still find matches by using other, more reliable attributes. (d) We provide an O(n2) time algorithm for inferring MDs, and an effective algorithm for deducing a set of RCKs from MDs. (e) We experimentally verify that the algorithms help matching tools efficiently identify keys at compile time for matching, blocking or windowing, and that the techniques effectively improve both the quality and efficiency of various record matching methods.

#index 1328144
#* Turbo-charging estimate convergence in DBO
#@ Alin Dobra;Chris Jermaine;Florin Rusu;Fei Xu
#t 2009
#c 4
#% 102316
#% 102786
#% 227883
#% 273910
#% 300195
#% 397370
#% 479821
#% 481749
#% 503719
#% 824713
#% 875050
#% 960294
#% 1015278
#% 1022241
#% 1063564
#% 1092009
#! DBO is a database system that utilizes randomized algorithms to give statistically meaningful estimates for the final answer to a multi-table, disk-based query from start to finish during query execution. However, DBO's "time 'til utility" (or "TTU"; that is, the time until DBO can give a useful estimate) can be overly large, particularly in the case that many database tables are joined in a query, or in the case that a join query includes a very selective predicate on one or more of the tables, or when the data are skewed. In this paper, we describe Turbo DBO, which is a prototype database system that can answer multi-table join queries in a scalable fashion, just like DBO. However, Turbo DBO often has a much lower TTU than DBO. The key innovation of Turbo DBO is that it makes use of novel algorithms that look for and remember "partial match" tuples in a randomized fashion. These are tuples that satisfy some of the boolean predicates associated with the query, and can possibly be grown into tuples that actually contribute to the final query result at a later time.

#index 1328145
#* Composable, scalable, and accurate weight summarization of unaggregated data sets
#@ Edith Cohen;Nick Duffield;Haim Kaplan;Carsten Lund;Mikkel Thorup
#t 2009
#c 4
#% 1331
#% 243299
#% 248812
#% 273908
#% 446438
#% 719203
#% 725367
#% 810007
#% 977009
#% 989512
#% 1002032
#% 1014727
#% 1062477
#% 1127369
#% 1164953
#% 1404794
#% 1815547
#! Many data sets occur as unaggregated data sets, where multiple data points are associated with each key. In the aggregate view of the data, the weight of a key is the sum of the weights of data points associated with the key. Examples are measurements of IP packet header streams, distributed data streams produced by events registered by sensor networks, and Web page or multimedia requests to context distribution servers. We aim to combine sampling and aggregation to provide accurate and efficient summaries of the aggregate view. However, data points are scattered in time or across multiple servers and hence aggregation is subject to resource constraints on the size of summaries that can be stored or transmitted. We develop a summarization framework for unaggregated data where summarization is a scalable and composable operator, and as such, can be tailored to meet resource constraints. Our summaries support unbiased estimates of the weight of subpopulations of keys specified using arbitrary selection predicates. While we prove that under such scenarios there is no variance optimal scheme, our estimators have the desirable properties that the variance is progressively closer to the minimum possible when applied to a "more" aggregated data set. An extensive evaluation using synthetic and real data sets shows that our summarization framework outperforms all existing schemes for this fundamental problem, even for the special and well-studied case of data streams.

#index 1328146
#* Distributed online aggregations
#@ Sai Wu;Shouxu Jiang;Beng Chin Ooi;Kian-Lee Tan
#t 2009
#c 4
#% 39379
#% 213080
#% 227883
#% 273909
#% 273910
#% 309748
#% 449870
#% 453194
#% 463285
#% 479976
#% 641013
#% 785665
#% 793890
#% 799141
#% 810055
#% 823842
#% 847499
#% 864429
#% 873998
#% 981647
#% 983467
#% 1015281
#% 1023420
#% 1063490
#% 1116991
#! In many decision making applications, users typically issue aggregate queries. To evaluate these computationally expensive queries, online aggregation has been developed to provide approximate answers (with their respective confidence intervals) quickly, and to continuously refine the answers. In this paper, we extend the online aggregation technique to a distributed context where sites are maintained in a DHT (Distributed Hash Table) network. Our Distributed Online Aggregation (DoA) scheme iteratively and progressively produces approximate aggregate answers as follows: in each iteration, a small set of random samples are retrieved from the data sites and distributed to the processing sites; at each processing site, a local aggregate is computed based on the allocated samples; at a coordinator site, these local aggregates are combined into a global aggregate. DoA adaptively grows the number of processing nodes as the sample size increases. To further reduce the sampling overhead, the samples are retained as a precomputed synopsis over the network to be used for processing future queries. We also study how these synopsis can be maintained incrementally. We have conducted extensive experiments on PlanetLab. The results show that our DoA scheme reduces the initial waiting time significantly and provides high quality approximate answers with running confidence intervals progressively.

#index 1328147
#* A recall-based cluster formation game in peer-to-peer systems
#@ Georgia Koloniari;Evaggelia Pitoura
#t 2009
#c 4
#% 340175
#% 576969
#% 643013
#% 653861
#% 823385
#% 878251
#% 938091
#% 981628
#% 1002007
#% 1008867
#% 1022269
#% 1851624
#! In many large-scale content sharing applications, participants or peers are grouped together forming clusters based on their content or interests. In this paper, we deal with the maintenance of such clusters in the presence of updates. We model the evolution of the system as a strategic game, where peers determine their cluster membership based on a utility function of the query recall. Peers are guided either by selfish or altruistic motives: selfish peers aim at improving the recall of their own queries, whereas altruistic peers aim at improving the recall of the queries of other peers. We study the evolution of such clusters both theoretically and experimentally under a variety of conditions. We show that, in general, local decisions made independently by each peer enable the system to adapt to changes and maintain the overall recall of the query workload.

#index 1328148
#* Quantifying isolation anomalies
#@ Alan Fekete;Shirley N. Goldrei;Jorge Pérez Asenjo
#t 2009
#c 4
#% 3951
#% 14204
#% 27057
#% 116087
#% 201869
#% 210179
#% 247071
#% 320902
#% 336201
#% 401436
#% 403195
#% 479602
#% 480589
#% 481256
#% 814649
#% 1022309
#% 1063524
#% 1127968
#% 1183366
#! Choosing a weak isolation level such as Read Committed is understood as a trade-off, where less isolation means that higher performance is gained but there is an increased possibility that data integrity will be lost. Previously, one side of this trade-off has been carefully studied quantitatively -- there are well-known metrics for performance such as transactions per minute, standardized benchmarks that measure these in a controlled way, and analytic models that can predict how performance is influenced by system parameters like multiprogramming level. This paper contributes to quantifying the other aspect of the trade-off. We define a novel microbenchmark that measures how rapidly integrity violations are produced at different isolation levels, for a simple set of transactions. We explore how this rate is impacted by configuration factors such as multiprogramming level, or contention frequency. For the isolation levels in multi-version platforms (Snapshot Isolation and the multiversion variant of Read Committed), we offer a simple probabilistic model that predicts the rate of integrity violations in our microbenchmark from configuration parameters. We validate the predictive model against measurements from the microbenchmark. The model identifies a region of the configuration space where a surprising inversion occurs: for these parameter settings, more integrity violations happen with Snapshot Isolation than with multi-version Read Committed, even though the latter is considered a lower isolation level.

#index 1328149
#* Improving OLTP scalability using speculative lock inheritance
#@ Ryan Johnson;Ippokratis Pandis;Anastasia Ailamaki
#t 2009
#c 4
#% 172939
#% 286245
#% 403195
#% 479468
#% 480767
#% 1022298
#% 1022309
#% 1063543
#% 1129955
#% 1181215
#! Transaction processing workloads provide ample request level concurrency which highly parallel architectures can exploit. However, the resulting heavy utilization of core database services also causes resource contention within the database engine itself and limits scalability. Meanwhile, many database workloads consist of short transactions which access only a few database records each, often with stringent response time requirements. Performance of these short transactions is determined largely by the amount of overhead the database engine imposes for services such as logging, locking, and transaction management. This paper highlights the negative scalability impact of database locking, an effect which is especially severe for short transactions running on highly concurrent multicore hardware. We propose and evaluate Speculative Lock Inheritance, a technique where hot database locks pass directly from transaction to transaction, bypassing the lock manager bottleneck. We implement SLI in the Shore-MT storage manager and show that lock inheritance fundamentally improves scalability by decoupling the number of simultaneous requests for popular locks from the number of threads in the system, eliminating contention within the lock manager even as core counts continue to increase. We achieve this effect with only minor changes to the lock manager and without changes to consistency or other application-visible effects.

#index 1328150
#* Segment-based recovery: write-ahead logging revisited
#@ Russell Sears;Eric Brewer
#t 2009
#c 4
#% 117
#% 9241
#% 114582
#% 151539
#% 158914
#% 213079
#% 239969
#% 268755
#% 273932
#% 286929
#% 319549
#% 445788
#% 465019
#% 480589
#% 742706
#% 769155
#% 963667
#% 978390
#% 978392
#% 978404
#% 998842
#% 998852
#% 1063488
#% 1189372
#% 1307049
#! Although existing write-ahead logging algorithms scale to conventional database workloads, their communication and synchronization overheads limit their usefulness for modern applications and distributed systems. We revisit write-ahead logging with an eye toward finer-grained concurrency and an increased range of workloads, then remove two core assumptions: that pages are the unit of recovery and that times-tamps (LSNs) should be stored on each page. Recovering individual application-level objects (rather than pages) simplifies the handing of systems with object sizes that differ from the page size. We show how to remove the need for LSNs on the page, which in turn enables DMA or zero-copy I/O for large objects, increases concurrency, and reduces communication between the application, buffer manager and log manager. Our experiments show that the looser coupling significantly reduces the impact of latency among the components. This makes the approach particularly applicable to large scale distributed systems, and enables a "cross pollination" of ideas from distributed systems and transactional storage. However, these advantages come at a cost; segments are incompatible with physiological redo, preventing a number of important optimizations. We show how allocation enables (or prevents) mixing of ARIES pages (and physiological redo) with segments. We present an allocation policy that avoids undesirable interactions that complicate other combinations of ARIES and LSN-free pages, and then present a proof that both approaches and our combination are correct. Many optimizations presented here were proposed in the past. However, we believe this is the first unified approach.

#index 1328151
#* A unified approach to ranking in probabilistic databases
#@ Jian Li;Barna Saha;Amol Deshpande
#t 2009
#c 4
#% 215225
#% 235023
#% 330769
#% 453464
#% 577224
#% 654487
#% 840846
#% 864394
#% 864417
#% 893168
#% 976984
#% 976987
#% 1016201
#% 1022259
#% 1063520
#% 1075132
#% 1127375
#% 1127376
#% 1127413
#% 1206646
#% 1206893
#% 1207234
#% 1217141
#% 1291123
#% 1728680
#! The dramatic growth in the number of application domains that naturally generate probabilistic, uncertain data has resulted in a need for efficiently supporting complex querying and decision-making over such data. In this paper, we present a unified approach to ranking and top-k query processing in probabilistic databases by viewing it as a multi-criteria optimization problem, and by deriving a set of features that capture the key properties of a probabilistic dataset that dictate the ranked result. We contend that a single, specific ranking function may not suffice for probabilistic databases, and we instead propose two parameterized ranking functions, called PRFω and PRFe, that generalize or can approximate many of the previously proposed ranking functions. We present novel generating functions-based algorithms for efficiently ranking large datasets according to these ranking functions, even if the datasets exhibit complex correlations modeled using probabilistic and/xor trees or Markov networks. We further propose that the parameters of the ranking function be learned from user preferences, and we develop an approach to learn those parameters. Finally, we present a comprehensive experimental study that illustrates the effectiveness of our parameterized ranking functions, especially PRFe, at approximating other ranking functions and the scalability of our proposed algorithms for exact or approximate ranking.

#index 1328152
#* Learning string transformations from examples
#@ Arvind Arasu;Surajit Chaudhuri;Raghav Kaushik
#t 2009
#c 4
#% 300157
#% 577238
#% 729923
#% 810014
#% 817577
#% 844321
#% 875066
#% 913783
#% 934581
#% 1022229
#% 1206615
#% 1289646
#! "Robert" and "Bob" refer to the same first name but are textually far apart. Traditional string similarity functions do not allow a flexible way to account for such synonyms, abbreviations and aliases. Recently, string transformations have been proposed as a mechanism to make matching robust to such variations. However, in many domains, identifying an appropriate set of transformations is challenging as the space of possible transformations is large. In this paper, we investigate the problem of leveraging examples of matching strings to learn string transformations. We formulate an optimization problem where we are required to learn a concise set of transformations that explain most of the differences. We propose a greedy approximation algorithm for this NP-hard problem. Our experiments over real-life data illustrate the benefits of our approach.

#index 1328153
#* Probabilistic histograms for probabilistic data
#@ Graham Cormode;Antonios Deligiannakis;Minos Garofalakis;Andrew McGregor
#t 2009
#c 4
#% 2115
#% 214073
#% 479648
#% 482092
#% 566132
#% 604653
#% 722934
#% 810098
#% 866990
#% 960257
#% 976984
#% 977008
#% 991156
#% 1015256
#% 1016154
#% 1016201
#% 1063521
#% 1179162
#% 1206717
#% 1206735
#% 1206892
#! There is a growing realization that modern database management systems (DBMSs) must be able to manage data that contains uncertainties that are represented in the form of probabilistic relations. Consequently, the design of each core DBMS component must be revisited in the presence of uncertain and probabilistic information. In this paper, we study how to build histogram synopses for probabilistic relations, for the purposes of enabling both DBMS-internal decisions (such as indexing and query planning), and (possibly, user-facing) approximate query processing tools. In contrast to initial work in this area, our probabilistic histograms retain the key possible-worlds semantics of probabilistic data, allowing for more accurate, yet concise, representation of the uncertainty characteristics of data and query results. We present a variety of techniques for building optimal probabilistic histograms, each one tuned to a different choice of approximation-error metric. We show that these can be incorporated into a general Dynamic Programming (DP) framework, which generalizes that used for existing histogram constructions. The end result is a histogram where each "bucket" is approximately represented by a compact probability distribution function (PDF), which can be used as the basis for query planning and approximate query answering. We present novel, polynomial-time algorithms to find optimal probabilistic histograms for a variety of PDF-error metrics (including variation distance, sum squared error, max error and EMD1). Our experimental study shows that our probabilistic histogram synopses can accurately capture the key statistical properties of uncertain data, while being much more compact to store and work with than the original uncertain relations.

#index 1328154
#* Autocompletion for mashups
#@ Ohad Greenshpan;Tova Milo;Neoklis Polyzotis
#t 2009
#c 4
#% 308589
#% 643566
#% 813966
#% 824702
#% 831888
#% 893087
#% 955063
#% 960347
#% 960351
#% 1022220
#% 1055749
#% 1063559
#% 1063738
#% 1131156
#% 1145301
#% 1206840
#! A mashup is a Web application that integrates data, computation and UI elements provided by several components into a single tool. The concept originated from the understanding that there is an increasing number of applications available on the Web and a growing need to combine them in order to meet user requirements. This paper presents MatchUp, a system that supports rapid, on-demand, intuitive development of mashups, based on a novel autocompletion mechanism. The key observation guiding the development of MatchUp is that mashups developed by different users typically share common characteristics; they use similar classes of mashup components and glue them together in a similar manner. MatchUp exploits these similarities to recommend useful completions (missing components and connections between them) for a user's partial mashup specification. The user is presented with a ranking of the recommendations from which she can choose and refine according to her needs. This paper presents the data model and ranking metric underlying our novel autocompletion mechanism. It introduces an efficient top-k ranking algorithm that is at the core of the MatchUp system and that is formally proved to be optimal in some natural sense. We also experimentally demonstrate the efficiency of our algorithm and the effectiveness of our proposal for rapid mashup construction.

#index 1328155
#* Integrating conflicting data: the role of source dependence
#@ Xin Luna Dong;Laure Berti-Equille;Divesh Srivastava
#t 2009
#c 4
#% 268079
#% 282905
#% 577367
#% 654447
#% 722144
#% 799636
#% 989682
#% 1063709
#! Many data management applications, such as setting up Web portals, managing enterprise data, managing community data, and sharing scientific data, require integrating data from multiple sources. Each of these sources provides a set of values and different sources can often provide conflicting values. To present quality data to users, it is critical that data integration systems can resolve conflicts and discover true values. Typically, we expect a true value to be provided by more sources than any particular false one, so we can take the value provided by the majority of the sources as the truth. Unfortunately, a false value can be spread through copying and that makes truth discovery extremely tricky. In this paper, we consider how to find true values from conflicting information when there are a large number of sources, among which some may copy from others. We present a novel approach that considers dependence between data sources in truth discovery. Intuitively, if two data sources provide a large number of common values and many of these values are rarely provided by other sources (e.g., particular false values), it is very likely that one copies from the other. We apply Bayesian analysis to decide dependence between sources and design an algorithm that iteratively detects dependence and discovers truth from conflicting information. We also extend our model by considering accuracy of data sources and similarity between values. Our experiments on synthetic data as well as real-world data show that our algorithm can significantly improve accuracy of truth discovery and is scalable when there are a large number of data sources.

#index 1328156
#* Truth discovery and copying detection in a dynamic world
#@ Xin Luna Dong;Laure Berti-Equille;Divesh Srivastava
#t 2009
#c 4
#% 300139
#% 777934
#% 824689
#% 960365
#% 989682
#% 1328103
#% 1328155
#! Modern information management applications often require integrating data from a variety of data sources, some of which may copy or buy data from other sources. When these data sources model a dynamically changing world (e.g., people's contact information changes over time, restaurants open and go out of business), sources often provide out-of-date data. Errors can also creep into data when sources are updated often. Given out-of-date and erroneous data provided by different, possibly dependent, sources, it is challenging for data integration systems to provide the true values. Straightforward ways to resolve such inconsistencies (e.g., voting) may lead to noisy results, often with detrimental consequences. In this paper, we study the problem of finding true values and determining the copying relationship between sources, when the update history of the sources is known. We model the quality of sources over time by their coverage, exactness and freshness. Based on these measures, we conduct a probabilistic analysis. First, we develop a Hidden Markov Model that decides whether a source is a copier of another source and identifies the specific moments at which it copies. Second, we develop a Bayesian model that aggregates information from the sources to decide the true value for a data item, and the evolution of the true values over time. Experimental results on both real-world and synthetic data show high accuracy and scalability of our techniques.

#index 1328157
#* Sequential dependencies
#@ Lukasz Golab;Howard Karloff;Flip Korn;Avishek Saha;Divesh Srivastava
#t 2009
#c 4
#% 189872
#% 209634
#% 368351
#% 411733
#% 416005
#% 420063
#% 453493
#% 463903
#% 771230
#% 790534
#% 800491
#% 824709
#% 957605
#% 991153
#% 993961
#% 1015299
#% 1022222
#% 1127381
#% 1127443
#% 1206961
#! We study sequential dependencies that express the semantics of data with ordered domains and help identify quality problems with such data. Given an interval g, we write X →g Y to denote that the difference between the Y -attribute values of any two consecutive records, when sorted on X, must be in g. For example, time →(0,∞) sequence_number indicates that sequence numbers are strictly increasing over time, whereas sequence_number →[4, 5] time means that the time "gaps" between consecutive sequence numbers are between 4 and 5. Sequential dependencies express relationships between ordered attributes, and identify missing (gaps too large), extraneous (gaps too small) and out-of-order data. To make sequential dependencies applicable to real-world data, we relax their requirements and allow them to hold approximately (with some exceptions) and conditionally (on various subsets of the data). This paper proposes the notion of conditional approximate sequential dependencies and provides an efficient framework for discovering pattern tableaux, which are compact representations of the subsets of the data (i.e., ranges of values of the ordered attributes) that satisfy the underlying dependency. We present analyses of our proposed algorithms, and experiments on real data demonstrating the efficiency and utility of our framework.

#index 1328158
#* SHARC: framework for quality-conscious web archiving
#@ Dimitar Denev;Arturas Mazeika;Marc Spaniol;Gerhard Weikum
#t 2009
#c 4
#% 32915
#% 227945
#% 268087
#% 300139
#% 330609
#% 397355
#% 438251
#% 640706
#% 754058
#% 785872
#% 917970
#% 956525
#% 993974
#% 1022233
#% 1055714
#% 1055715
#% 1066743
#% 1089473
#% 1166533
#! Web archives preserve the history of born-digital content and offer great potential for sociologists, business analysts, and legal experts on intellectual property and compliance issues. Data quality is crucial for these purposes. Ideally, crawlers should gather sharp captures of entire Web sites, but the politeness etiquette and completeness requirement mandate very slow, long-duration crawling while Web sites undergo changes. This paper presents the SHARC framework for assessing the data quality in Web archives and for tuning capturing strategies towards better quality with given resources. We define quality measures, characterize their properties, and derive a suite of quality-conscious scheduling strategies for archive crawling. It is assumed that change rates of Web pages can be statistically predicted based on page types, directory depths, and URL names. We develop a stochastically optimal crawl algorithm for the offline case where all change rates are known. We generalize the approach into an online algorithm that detect information on a Web site while it is crawled. For dating a site capture and for assessing its quality, we propose several strategies that revisit pages after their initial downloads in a judiciously chosen order. All strategies are fully implemented in a testbed, and shown to be effective by experiments with both synthetically generated sites and a daily crawl series for a medium-sized site.

#index 1328159
#* Modeling and querying possible repairs in duplicate detection
#@ George Beskales;Mohamed A. Soliman;Ihab F. Ilyas;Shai Ben-David
#t 2009
#c 4
#% 663
#% 32879
#% 36672
#% 248790
#% 273687
#% 310546
#% 480496
#% 629097
#% 727901
#% 800547
#% 800590
#% 864417
#% 913783
#% 915307
#% 937552
#% 960270
#% 992830
#% 1675764
#! One of the most prominent data quality problems is the existence of duplicate records. Current duplicate elimination procedures usually produce one clean instance (repair) of the input data, by carefully choosing the parameters of the duplicate detection algorithms. Finding the right parameter settings can be hard, and in many cases, perfect settings do not exist. Furthermore, replacing the input dirty data with one possible clean instance may result in unrecoverable errors, for example, identification and merging of possible duplicate records in health care systems. In this paper, we treat duplicate detection procedures as data processing tasks with uncertain outcomes. We concentrate on a family of duplicate detection algorithms that are based on parameterized clustering. We propose a novel uncertainty model that compactly encodes the space of possible repairs corresponding to different parameter settings. We show how to efficiently support relational queries under our model, and to allow new types of queries on the set of possible repairs. We give an experimental study illustrating the scalability and the efficiency of our techniques in different configurations.

#index 1328160
#* Discovering relative importance of skyline attributes
#@ Denis Mindolin;Jan Chomicki
#t 2009
#c 4
#% 465167
#% 529348
#% 578692
#% 731407
#% 823654
#% 834611
#% 903013
#% 912241
#% 993957
#% 994017
#% 1022224
#% 1083667
#% 1650354
#! Querying databases with preferences is an important research problem. Among various approaches to querying with preferences, the skyline framework is one of the most popular. A well known deficiency of that framework is that all attributes are of the same importance in skyline preference relations. Consequently, the size of the results of skyline queries may grow exponentially with the number of skyline attributes. Here we propose the framework called p-skylines which enriches skylines with the notion of attribute importance. It turns out that incorporating relative attribute importance in skylines allows for reduction in the corresponding query result sizes. We propose an approach to discovering importance relationships of attributes, based on user-selected sets of superior and inferior examples. We show that the problem of checking the existence of and the problem of computing an optimal p-skyline preference relation covering a given set of examples are NP-complete and FNP-complete, respectively. However, we also show that a restricted version of the discovery problem -- using only superior examples to discover attribute importance -- can be solved efficiently in polynomial time. Our experiments show that the proposed importance discovery algorithm has high accuracy and good scalability.

#index 1328161
#* A particle-and-density based evolutionary clustering method for dynamic networks
#@ Min-Soo Kim;Jiawei Han
#t 2009
#c 4
#% 479658
#% 810066
#% 823342
#% 823344
#% 881460
#% 881514
#% 881523
#% 960283
#% 989586
#% 989640
#% 989643
#% 989654
#% 989663
#% 1022269
#% 1030883
#% 1055740
#% 1083675
#% 1083699
#% 1914479
#! Recently, dynamic networks are attracting increasing interest due to their high potential in capturing natural and social phenomena over time. Discovery of evolutionary communities in dynamic networks has become a critical task. The previous evolutionary clustering methods usually adopt the temporal smoothness framework, which has a desirable feature of controlling the balance between temporal noise and true concept drift of communities. They, however, have some major drawbacks: (1) assuming only a fixed number of communities over time; and (2) not allowing arbitrary start/stop of community over time. The forming of new communities and dissolving of existing communities are very common phenomena in real dynamic networks. In this paper, we propose a new particle-and-density based evolutionary clustering method that efficiently discovers a variable number of communities of arbitrary forming and dissolving. We first model a dynamic network as a collection of lots of particles called nano-communities, and a community as a densely connected subset of particles, called a quasi l-clique-by-clique (shortly, l-KK). Each particle contains a small amount of information about the evolution of data or patterns, and the quasi l-KKs inherent in a given dynamic network provide us with guidance on how to find a variable number of communities of arbitrary forming and dissolving. We propose a density-based clustering method that efficiently finds temporally smoothed local clusters of high quality by using a cost embedding technique and optimal modularity. We also propose a mapping method based on information theory that makes sequences of smoothed local clusters as close as possible to data-inherent quasi l-KKs. The result of the mapping method allows us to easily identify the stage of each community among the three stages: evolving, forming, and dissolving. Experimental studies, by using various data sets, demonstrate that our method improves the clustering accuracy, and at the same time, the time performance by an order of magnitude compared with the current state-of-the art method.

#index 1328162
#* Summarizing relational databases
#@ Xiaoyan Yang;Cecilia M. Procopiuc;Divesh Srivastava
#t 2009
#c 4
#% 115608
#% 190611
#% 220283
#% 765408
#% 893115
#% 960234
#% 989646
#% 994033
#% 1022286
#% 1127406
#! Complex databases are challenging to explore and query by users unfamiliar with their schemas. Enterprise databases often have hundreds of inter-linked tables, so even when extensive documentation is available, new users must spend a considerable amount of time understanding the schema before they can retrieve any information from the database. The problem is aggravated if the documentation is missing or outdated, which may happen with legacy databases. In this paper we identify limitations of previous approaches to address this vexing problem, and propose a principled approach to summarizing the contents of a relational database, so that a user can determine at a glance the type of information it contains, and the main tables in which that information resides. Our approach has three components: First, we define the importance of each table in the database as its stable state value in a random walk over the schema graph, where the transition probabilities depend on the entropies of table attributes. This ensures that the importance of a table depends both on its information content, and on how that content relates to the content of other tables in the database. Second, we define a metric space over the tables in a database, such that the distance function is consistent with an intuitive notion of table similarity. Finally, we use a Weighted k-Center algorithm under this distance function to cluster all tables in the database around the most relevant tables, and return the result as our summary. We conduct an extensive experimental study on a benchmark database, comparing our approach with previous methods, as well as with several hybrid models. We show that our approach not only achieves significantly higher accuracy than the previous state of the art, but is also faster and scales linearly with the size of the schema graph.

#index 1328163
#* Coordinated weighted sampling for estimating aggregates over multiple weight assignments
#@ Edith Cohen;Haim Kaplan;Subhabrata Sen
#t 2009
#c 4
#% 1331
#% 243166
#% 243299
#% 307424
#% 322884
#% 336610
#% 345087
#% 347225
#% 397369
#% 480805
#% 593957
#% 616528
#% 646233
#% 654447
#% 725373
#% 766524
#% 769944
#% 781702
#% 808507
#% 809263
#% 851889
#% 866696
#% 872052
#% 874986
#% 942353
#% 956507
#% 960250
#% 978157
#% 989512
#% 1014727
#% 1039659
#% 1083641
#% 1084473
#% 1127368
#% 1127369
#% 1164953
#% 1213391
#% 1404794
#! Many data sources are naturally modeled by multiple weight assignments over a set of keys: snapshots of an evolving database at multiple points in time, measurements collected over multiple time periods, requests for resources served at multiple locations, and records with multiple numeric attributes. Over such vector-weighted data we are interested in aggregates with respect to one set of weights, such as weighted sums, and aggregates over multiple sets of weights such as the L1 difference. Sample-based summarization is highly effective for data sets that are too large to be stored or manipulated. The summary facilitates approximate processing queries that may be specified after the summary was generated. Current designs, however, are geared for data sets where a single scalar weight is associated with each key. We develop a sampling framework based on coordinated weighted samples that is suited for multiple weight assignments and obtain estimators that are orders of magnitude tighter than previously possible. We demonstrate the power of our methods through an extensive empirical evaluation on diverse data sets ranging from IP network to stock quotes data.

#index 1328164
#* Power-law based estimation of set similarity join size
#@ Hongrae Lee;Raymond T. Ng;Kyuseok Shim
#t 2009
#c 4
#% 201889
#% 243166
#% 299984
#% 300160
#% 397369
#% 481290
#% 616528
#% 765463
#% 810020
#% 864392
#% 867053
#% 869500
#% 874992
#% 893164
#% 956506
#% 956518
#% 1022218
#% 1072644
#% 1127368
#% 1176857
#% 1181256
#% 1181283
#% 1206586
#% 1206677
#% 1300556
#! We propose a novel technique for estimating the size of set similarity join. The proposed technique relies on a succinct representation of sets using Min-Hash signatures. We exploit frequent patterns in the signatures for the Set Similarity Join (SSJoin) size estimation by counting their support. However, there are overlaps among the counts of signature patterns and we need to use the set Inclusion-Exclusion (IE) principle. We develop a novel lattice-based counting method for efficiently evaluating the IE principle. The proposed counting technique is linear in the lattice size. To make the mining process very light-weight, we exploit a recently discovered Power-law relationship of pattern count and frequency. Extensive experimental evaluations show the proposed technique is capable of accurate and efficient estimation.

#index 1328165
#* Optimality and scalability in lattice histogram construction
#@ Panagiotis Karras
#t 2009
#c 4
#% 43163
#% 201921
#% 210190
#% 248822
#% 273901
#% 273902
#% 273909
#% 282942
#% 333947
#% 399763
#% 411355
#% 465064
#% 479648
#% 479984
#% 480125
#% 481266
#% 482092
#% 572308
#% 575972
#% 742562
#% 803119
#% 810547
#% 824686
#% 850727
#% 866990
#% 893160
#% 982754
#% 989609
#% 1015256
#% 1016154
#% 1046507
#% 1070891
#% 1072632
#% 1126563
#% 1181277
#% 1206664
#% 1702985
#% 1817010
#! The Lattice Histogram is a recently proposed data summarization technique that achieves approximation quality preferable to that of an optimal plain histogram. Like other hierarchical synopsis methods, a lattice histogram (LH) aims to approximate data using a hierarchical structure. Still, this structure is not defined a priori; it consists an unknown, not a given, of the problem. Past work has defined the properties that an LH needs to obey and developed general-purpose approximation algorithms for the construction thereof. Still, two major issues remain unaddressed: First, the construction of an optimal LH for a given error metric is a problem unsolved to date. Second, the proposed algorithms suffer from too high space and time complexities that render their application in real-world settings problematic. In this paper, we address both these questions, focusing on the case that the target error metric is a maximum error metric. Our algorithms treat both the error-bounded LH construction problem, in which the space occupied by an LH is minimized under an error constraint, as well as the classic space-bounded problem. First, we develop a dynamic-programming scheme that detects an optimal LH under a given maximum-error bound. Second, we propose an efficient, practical, greedy algorithm that solves the same problem with much lower time and space requirements. Then, we show how both our algorithms can be applied to the classic space-bounded problem, aiming at minimizing error under a bound on space. Our experimental study with real-world data sets shows the effectiveness of our methods compared to competing summarization techniques. Moreover, our findings show that our greedy heuristic performs almost as well as the optimal solution in terms of accuracy.

#index 1328166
#* Adaptively parallelizing distributed range queries
#@ Ymir Vigfusson;Adam Silberstein;Brian F. Cooper;Rodrigo Fonseca
#t 2009
#c 4
#% 115661
#% 201956
#% 227914
#% 275000
#% 299983
#% 303086
#% 442698
#% 442700
#% 631955
#% 808595
#% 824697
#% 960165
#% 963669
#% 978404
#% 998845
#% 1016166
#% 1127560
#! We consider the problem of how to best parallelize range queries in a massive scale distributed database. In traditional systems the focus has been on maximizing parallelism, for example by laying out data to achieve the highest throughput. However, in a massive scale database such as our PNUTS system [11] or BigTable [10], maximizing parallelism is not necessarily the best strategy: the system has more than enough servers to saturate a single client by returning results faster than the client can consume them, and when there are multiple concurrent queries, maximizing parallelism for all of them will cause disk contention, reducing everybody's performance. How can we find the right parallelism level for each query in order to achieve high, consistent throughput for all queries? We propose an adaptive approach with two aspects. First, we adaptively determine the ideal parallelism for a single query execution, which is the minimum number of parallel scanning servers needed to satisfy the client, depending on query selectivity, client load, client-server bandwidth, and so on. Second, we adaptively schedule which servers will be assigned to different query executions, to minimize disk contention on servers and ensure that all queries receive good performance. Our scheduler can be tuned based on different policies, such as favoring short versus long queries or high versus low priority queries. An experimental study demonstrates the effectiveness of our techniques in the PNUTS system.

#index 1328167
#* Mining tree-structured data on multicore systems
#@ Shirish Tatikonda;Srinivasan Parthasarathy
#t 2009
#c 4
#% 66288
#% 100621
#% 154636
#% 189880
#% 230772
#% 232122
#% 288885
#% 300120
#% 434348
#% 629708
#% 630984
#% 631969
#% 637298
#% 638157
#% 727909
#% 729766
#% 729941
#% 737334
#% 745461
#% 745511
#% 785428
#% 788217
#% 813989
#% 844409
#% 853020
#% 881464
#% 907532
#% 915243
#% 944956
#% 983468
#% 1022207
#% 1051704
#% 1126298
#% 1127399
#% 1476274
#% 1669921
#! Mining frequent subtrees in a database of rooted and labeled trees is an important problem in many domains, ranging from phylogenetic analysis to biochemistry and from linguistic parsing to XML data analysis. In this work we revisit this problem and develop an architecture conscious solution targeting emerging multicore systems. Specifically we identify a sequence of memory related optimizations that significantly improve the spatial and temporal locality of a state-of-the-art sequential algorithm -- alleviating the effects of memory latency. Additionally, these optimizations are shown to reduce the pressure on the front-side bus, an important consideration in the context of large-scale multicore architectures. We then demonstrate that these optimizations while necessary are not sufficient for efficient parallelization on multicores, primarily due to parametric and data-driven factors which make load balancing a significant challenge. To address this challenge, we present a methodology that adaptively and automatically modulates the type and granularity of the work being shared among different cores. The resulting algorithm achieves near perfect parallel efficiency on up to 16 processors on challenging real world applications. The optimizations we present have general purpose utility and a key out-come is the development of a general purpose scheduling service for moldable task scheduling on emerging multicore systems.

#index 1328168
#* Predictable performance for unpredictable workloads
#@ P. Unterbrunner;G. Giannikis;G. Alonso;D. Fauser;D. Kossmann
#t 2009
#c 4
#% 2833
#% 36117
#% 69273
#% 114582
#% 195379
#% 201869
#% 333938
#% 378397
#% 427199
#% 442700
#% 442835
#% 479821
#% 480774
#% 480821
#% 481256
#% 742564
#% 810039
#% 993948
#% 1022262
#% 1127399
#% 1206624
#! This paper introduces Crescando: a scalable, distributed relational table implementation designed to perform large numbers of queries and updates with guaranteed access latency and data freshness. To this end, Crescando leverages a number of modern query processing techniques and hardware trends. Specifically, Crescando is based on parallel, collaborative scans in main memory and so-called "query-data" joins known from data-stream processing. While the proposed approach is not always optimal for a given workload, it provides latency and freshness guarantees for all workloads. Thus, Crescando is particularly attractive if the workload is unknown, changing, or involves many different queries. This paper describes the design, algorithms, and implementation of a Crescando storage node, and assesses its performance on modern multi-core hardware.

#index 1328169
#* Graph clustering based on structural/attribute similarities
#@ Yang Zhou;Hong Cheng;Jeffrey Xu Yu
#t 2009
#c 4
#% 248792
#% 249110
#% 280819
#% 313959
#% 481281
#% 577273
#% 769887
#% 769967
#% 868092
#% 881496
#% 915344
#% 989640
#% 989654
#% 1063501
#% 1063512
#% 1064098
#% 1176892
#% 1181261
#! The goal of graph clustering is to partition vertices in a large graph into different clusters based on various criteria such as vertex connectivity or neighborhood similarity. Graph clustering techniques are very useful for detecting densely connected groups in a large graph. Many existing graph clustering methods mainly focus on the topological structure for clustering, but largely ignore the vertex properties which are often heterogenous. In this paper, we propose a novel graph clustering algorithm, SA-Cluster, based on both structural and attribute similarities through a unified distance measure. Our method partitions a large graph associated with attributes into k clusters so that each cluster contains a densely connected subgraph with homogeneous attribute values. An effective method is proposed to automatically learn the degree of contributions of structural similarity and attribute similarity. Theoretical analysis is provided to show that SA-Cluster is converging. Extensive experimental results demonstrate the effectiveness of SA-Cluster through comparison with the state-of-the-art graph clustering and summarization methods.

#index 1328170
#* Output space sampling for graph patterns
#@ Mohammad Al Hasan;Mohammed J. Zaki
#t 2009
#c 4
#% 125556
#% 190611
#% 288990
#% 299985
#% 342604
#% 374580
#% 466644
#% 481779
#% 577214
#% 577261
#% 629708
#% 727845
#% 729938
#% 742493
#% 769940
#% 769951
#% 823356
#% 824710
#% 881542
#% 1063502
#% 1072518
#% 1072809
#% 1083668
#% 1083676
#% 1126603
#% 1176857
#% 1176911
#% 1176985
#% 1180019
#% 1663621
#! Recent interest in graph pattern mining has shifted from finding all frequent subgraphs to obtaining a small subset of frequent subgraphs that are representative, discriminative or significant. The main motivation behind that is to cope with the scalability problem that the graph mining algorithms suffer when mining databases of large graphs. Another motivation is to obtain a succinct output set that is informative and useful. In the same spirit, researchers also proposed sampling based algorithms that sample the output space of the frequent patterns to obtain representative subgraphs. In this work, we propose a generic sampling framework that is based on Metropolis-Hastings algorithm to sample the output space of frequent subgraphs. Our experiments on various sampling strategies show the versatility, utility and efficiency of the proposed sampling approach.

#index 1328171
#* Mining graph patterns efficiently via randomized summaries
#@ Chen Chen;Cindy X. Lin;Matt Fredrikson;Mihai Christodorescu;Xifeng Yan;Jiawei Han
#t 2009
#c 4
#% 300120
#% 342604
#% 431105
#% 466644
#% 480810
#% 481290
#% 481779
#% 629708
#% 745477
#% 765429
#% 769940
#% 813990
#% 823342
#% 823347
#% 841960
#% 867050
#% 869492
#% 881466
#% 894441
#% 994157
#% 1063501
#% 1063502
#% 1063512
#% 1117006
#% 1117010
#% 1127358
#% 1176876
#! Graphs are prevalent in many domains such as Bioinformatics, social networks, Web and cyber-security. Graph pattern mining has become an important tool in the management and analysis of complexly structured data, where example applications include indexing, clustering and classification. Existing graph mining algorithms have achieved great success by exploiting various properties in the pattern space. Unfortunately, due to the fundamental role subgraph isomorphism plays in these methods, they may all enter into a pitfall when the cost to enumerate a huge set of isomorphic embeddings blows up, especially in large graphs. The solution we propose for this problem resorts to reduction on the data space. For each graph, we build a summary of it and mine this shrunk graph instead. Compared to other data reduction techniques that either reduce the number of transactions or compress between transactions, this new framework, called Summarize-Mine, suggests a third path by compressing within transactions. Summarize-Mine is effective in cutting down the size of graphs, thus decreasing the embedding enumeration cost. However, compression might lose patterns at the same time. We address this issue by generating randomized summaries and repeating the process for multiple rounds, where the main idea is that true patterns are unlikely to miss from all rounds. We provide strict probabilistic guarantees on pattern loss likelihood. Experiments on real malware trace data show that Summarize-Mine is very efficient, which can find interesting malware fingerprints that were not revealed previously.

#index 1328172
#* Group recommendation: semantics and efficiency
#@ Sihem Amer-Yahia;Senjuti Basu Roy;Ashish Chawlat;Gautam Das;Cong Yu
#t 2009
#c 4
#% 227894
#% 333854
#% 397608
#% 411762
#% 800508
#% 813966
#% 955930
#% 1396104
#! We study the problem of group recommendation. Recommendation is an important information exploration paradigm that retrieves interesting items for users based on their profiles and past activities. Single user recommendation has received significant attention in the past due to its extensive use in Amazon and Netflix. How to recommend to a group of users who may or may not share similar tastes, however, is still an open problem. The need for group recommendation arises in many scenarios: a movie for friends to watch together, a travel destination for a family to spend a holiday break, and a good restaurant for colleagues to have a working lunch. Intuitively, items that are ideal for recommendation to a group may be quite different from those for individual members. In this paper, we analyze the desiderata of group recommendation and propose a formal semantics that accounts for both item relevance to a group and disagreements among group members. We design and implement algorithms for efficiently computing group recommendations. We evaluate our group recommendation method through a comprehensive user study conducted on Amazon Mechanical Turk and demonstrate that incorporating disagreements is critical to the effectiveness of group recommendation. We further evaluate the efficiency and scalability of our algorithms on the MovieLens data set with 10M ratings.

#index 1328173
#* Class-based graph anonymization for social network data
#@ Smriti Bhagat;Graham Cormode;Balachander Krishnamurthy;Divesh Srivastava
#t 2009
#c 4
#% 67453
#% 443463
#% 576761
#% 864412
#% 881546
#% 893100
#% 956511
#% 960289
#% 1022247
#% 1063476
#% 1080080
#% 1083631
#% 1127360
#% 1127417
#% 1130836
#% 1206763
#% 1415851
#! The recent rise in popularity of social networks, such as Facebook and MySpace, has created large quantities of data about interactions within these networks. Such data contains many private details about individuals so anonymization is required prior to attempts to make the data more widely available for scientific research. Prior work has considered simple graph data to be anonymized by removing all non-graph information and adding or deleting some edges. Since social network data is richer in details about the users and their interactions, loss of details due to anonymization limits the possibility for analysis. We present a new set of techniques for anonymizing social network data based on grouping the entities into classes, and masking the mapping between entities and the nodes that represent them in the anonymized graph. Our techniques allow queries over the rich data to be evaluated with high accuracy while guaranteeing resilience to certain types of attack. To prevent inference of interactions, we rely on a critical "safety condition" when forming these classes. We demonstrate utility via empirical data from social networking settings. We give examples of complex queries that may be posed and show that they can be answered over the anonymized data efficiently and accurately.

#index 1328174
#* Improved search for socially annotated data
#@ Nikos Sarkas;Gautam Das;Nick Koudas
#t 2009
#c 4
#% 268079
#% 279755
#% 280850
#% 287253
#% 333854
#% 389155
#% 577224
#% 757953
#% 855601
#% 956515
#% 956544
#% 956579
#% 956589
#% 1035588
#% 1051482
#% 1077150
#% 1655692
#% 1667787
#! Social annotation is an intuitive, on-line, collaborative process through which each element of a collection of resources (e.g., URLs, pictures, videos, etc.) is associated with a group of descriptive keywords, widely known as tags. Each such group is a concise and accurate summary of the relevant resource's content and is obtained via aggregating the opinion of individual users, as expressed in the form of short tag sequences. The availability of this information gives rise to a new searching paradigm where resources are retrieved and ranked based on the similarity of a keyword query to their accompanying tags. In this paper, we present a principled and efficient search and resource ranking methodology that utilizes exclusively the user-assigned tag sequences. Ranking is based on solid probabilistic foundations and our growing understanding of the dynamics and structure of the social annotation process, which we capture by employing powerful interpolated n-gram models on the tag sequences. The efficiency and applicability of the proposed solution to large data sets is guaranteed through the introduction of a novel and highly scalable constrained optimization framework, employed both for training and incrementally maintaining the n-gram models. We experimentally validate the efficiency and effectiveness of our solutions compared to other applicable approaches. Our evaluation is based on a large crawl of del.icio.us, numbering hundreds of thousands of users and millions of resources, thus demonstrating the applicability of our solutions to real-life, large scale systems. In particular, we demonstrate that the use of interpolated n-grams for modeling tag sequences results in superior ranking effectiveness, while the proposed optimization framework is superior in terms of performance both for obtaining ranking parameters and incrementally maintaining them.

#index 1328175
#* Data publishing against realistic adversaries
#@ Ashwin Machanavajjhala;Johannes Gehrke;Michaela Götz
#t 2009
#c 4
#% 209373
#% 576111
#% 722904
#% 765449
#% 800515
#% 810011
#% 810028
#% 864390
#% 864406
#% 864412
#% 874989
#% 1022266
#% 1206582
#% 1206678
#% 1670071
#% 1740518
#! Privacy in data publishing has received much attention recently. The key to defining privacy is to model knowledge of the attacker -- if the attacker is assumed to know too little, the published data can be easily attacked, if the attacker is assumed to know too much, the published data has little utility. Previous work considered either quite ignorant adversaries or nearly omniscient adversaries. In this paper, we introduce a new class of adversaries that we call realistic adversaries who live in the unexplored space in between. Realistic adversaries have knowledge from external sources with an associated stubbornness indicating the strength of their knowledge. We then introduce a novel privacy framework called epsilon-privacy that allows us to guard against realistic adversaries. We also show that prior privacy definitions are instantiations of our framework. In a thorough experimental study with real census data we show that e-privacy allows us to publish data with high utility while defending against strong adversaries.

#index 1328176
#* Scalable verification for outsourced dynamic databases
#@ HweeHwa Pang;Jilian Zhang;Kyriakos Mouratidis
#t 2009
#c 4
#% 296857
#% 317933
#% 319994
#% 322884
#% 513367
#% 657774
#% 745532
#% 761411
#% 772846
#% 810042
#% 874980
#% 881074
#% 1022213
#% 1181949
#% 1201868
#% 1414468
#% 1664115
#% 1669498
#% 1706190
#% 1726204
#! Query answers from servers operated by third parties need to be verified, as the third parties may not be trusted or their servers may be compromised. Most of the existing authentication methods construct validity proofs based on the Merkle hash tree (MHT). The MHT, however, imposes severe concurrency constraints that slow down data updates. We introduce a protocol, built upon signature aggregation, for checking the authenticity, completeness and freshness of query answers. The protocol offers the important property of allowing new data to be disseminated immediately, while ensuring that outdated values beyond a pre-set age can be detected. We also propose an efficient verification technique for ad-hoc equijoins, for which no practical solution existed. In addition, for servers that need to process heavy query workloads, we introduce a mechanism that significantly reduces the proof construction time by caching just a small number of strategically chosen aggregate signatures. The efficiency and efficacy of our proposed mechanisms are confirmed through extensive experiments.

#index 1328177
#* Optimal random perturbation at multiple privacy levels
#@ Xiaokui Xiao;Yufei Tao;Minghua Chen
#t 2009
#c 4
#% 245771
#% 300184
#% 443463
#% 576111
#% 577233
#% 729962
#% 800513
#% 810010
#% 810011
#% 810028
#% 864412
#% 874988
#% 881497
#% 893100
#% 960291
#% 977011
#% 993988
#% 1022246
#% 1022247
#% 1029084
#% 1061644
#% 1083653
#% 1206574
#% 1206582
#% 1206678
#% 1217125
#% 1217156
#% 1670071
#% 1725659
#% 1740518
#! Random perturbation is a popular method of computing anonymized data for privacy preserving data mining. It is simple to apply, ensures strong privacy protection, and permits effective mining of a large variety of data patterns. However, all the existing studies with good privacy guarantees focus on perturbation at a single privacy level. Namely, a fixed degree of privacy protection is imposed on all anonymized data released by the data holder. This drawback seriously limits the applicability of random perturbation in scenarios where the holder has numerous recipients to which different privacy levels apply. Motivated by this, we study the problem of multi-level perturbation, whose objective is to release multiple versions of a dataset anonymized at different privacy levels. The challenge is that various recipients may collude by sharing their data to infer privacy beyond their permitted levels. Our solution overcomes this obstacle, and achieves two crucial properties. First, collusion is useless, meaning that the colluding recipients cannot learn anything more than what the most trustable recipient (among the colluding recipients) already knows alone. Second, the data each recipient receives can be regarded (and hence, analyzed in the same way) as the output of conventional uniform perturbation. Besides its solid theoretical foundation, the proposed technique is both space economical and computationally efficient. It requires O (n+m) expected space, and produces a new anonymized version in O (n + log m) expected time, where n is the cardinality of the original dataset, and m the number of versions released previously. Both bounds are optimal under the realistic assumption that n » m.

#index 1328178
#* Anticipatory DTW for efficient similarity search in time series databases
#@ Ira Assent;Marc Wichterich;Ralph Krieger;Hardy Kremer;Thomas Seidl
#t 2009
#c 4
#% 248797
#% 333941
#% 359751
#% 462231
#% 564263
#% 578400
#% 654456
#% 729931
#% 809264
#% 864398
#% 893161
#% 903632
#% 993965
#% 1016195
#% 1056068
#% 1063497
#% 1127609
#% 1206572
#! Time series arise in many different applications in the form of sensor data, stocks data, videos, and other time-related information. Analysis of this data typically requires searching for similar time series in a database. Dynamic Time Warping (DTW) is a widely used high-quality distance measure for time series. As DTW is computationally expensive, efficient algorithms for fast computation are crucial. In this paper, we propose a novel filter-and-refine DTW algorithm called Anticipatory DTW. Existing algorithms aim at efficiently finding similar time series by filtering the database and computing the DTW in the refinement step. Unlike these algorithms, our approach exploits previously unused information from the filter step during the refinement, allowing for faster rejection of false candidates. We characterize a class of applicable filters for our approach, which comprises state-of-the-art lower bounds of the DTW. Our novel anticipatory pruning incurs hardly any over-head and no false dismissals. We demonstrate substantial efficiency improvements in thorough experiments on synthetic and real world time series databases and show that our technique is highly scalable to multivariate, long time series and wide DTW bands.

#index 1328179
#* Improving the performance of list intersection
#@ Dimitris Tsirogiannis;Sudipto Guha;Nick Koudas
#t 2009
#c 4
#% 77937
#% 172911
#% 172913
#% 248820
#% 303072
#% 340670
#% 379411
#% 387427
#% 480119
#% 566122
#% 801695
#% 824719
#% 874903
#% 960298
#% 993947
#% 1022230
#% 1022232
#% 1022338
#% 1113132
#% 1667821
#% 1739410
#! List intersection is a central operation, utilized excessively for query processing on text and databases. We present list intersection algorithms for an arbitrary number of sorted and unsorted lists tailored to the characteristics of modern hardware architectures. Two new list intersection algorithms are presented for sorted lists. The first algorithm, termed Dynamic Probes, dynamically decides the probing order on the lists exploiting information from previous probes at runtime. This information is utilized as a cache-resident microindex. The second algorithm, termed Quantile-based, deduces in advance a good probing order, thus avoiding the overhead of adaptivity and is based on detecting lists with non-uniform distribution of document identifiers. For unsorted lists, we present a novel hash-based algorithm that avoids the overhead of sorting. A detailed experimental evaluation is presented based on real and synthetic data using existing chip multiprocessor architectures with eight cores, validating the efficiency and efficacy of the proposed algorithms.

#index 1328180
#* Consistent histograms in the presence of distinct value counts
#@ Raghav Kaushik;Dan Suciu
#t 2009
#c 4
#% 172902
#% 201921
#% 210190
#% 210353
#% 214073
#% 273682
#% 273901
#% 273908
#% 299982
#% 333947
#% 378404
#% 397371
#% 397385
#% 479648
#% 480125
#% 480803
#% 492932
#% 824682
#% 864426
#% 866990
#% 956456
#% 960248
#% 1015256
#% 1015285
#% 1206720
#% 1206892
#% 1676483
#! Self-tuning histograms have been proposed in the past as an attempt to leverage feedback from query execution. However, the focus thus far has been on histograms that only store cardinalities. In this paper, we study consistent histogram construction from query feedback that also takes distinct value counts into account. We first show how the entropy maximization (EM) principle can be leveraged to identify a distribution that approximates the data given the execution feedback making the least additional assumptions. This EM model that takes both distinct value counts and cardinalities into account. However, we find that it is computationally prohibitively expensive. We thus consider an alternative formulation for consistency -- for a given query workload, the goal is to minimize the L2 distance between the true and estimated cardinalities. This approach also handles both cardinalities and distinct values counts. We propose an efficient one-pass algorithm with several theoretical properties modeling this formulation. Our experiments show that this approach produces similar improvements in accuracy as the EM based approach while being computationally significantly more efficient.

#index 1328181
#* GConnect: a connectivity index for massive disk-resident graphs
#@ Charu Aggarwal;Yan Xie;Philip S. Yu
#t 2009
#c 4
#% 122671
#% 281214
#% 283833
#% 283980
#% 299941
#% 656282
#% 729938
#% 729941
#% 798044
#% 810072
#% 824711
#% 850729
#% 864462
#% 894441
#% 989575
#% 1063501
#% 1063502
#! The problem of connectivity is an extremely important one in the context of massive graphs. In many large communication networks, social networks and other graphs, it is desirable to determine the minimum-cut between any pair of nodes. The problem is well solved in the classical literature, since it is related to the maximum-flow problem, which is efficiently solvable. However, large graphs may often be disk resident, and such graphs cannot be efficiently processed for connectivity queries. This is because the minimum-cut problem is typically solved with the use of a variety of combinatorial and flow-based techniques which require random access to the underlying edges in the graph. In this paper, we propose to develop a connectivity index for massive-disk resident graphs. We will use an edge-sampling based approach to create compressed representations of the underlying graphs. Since these compressed representations can be held in main memory, they can be used to derive efficient approximations for the minimum-cut problem. These compressed representations are then organized into a disk-resident index structure. We present experimental results which show that the resulting approach provides between two and three orders of magnitude more efficient query processing than a disk-resident approach at the expense of a small amount of accuracy.

#index 1328182
#* A shared execution strategy for multiple pattern mining requests over streaming data
#@ Di Yang;Elke A. Rundensteiner;Matthew O. Ward
#t 2009
#c 4
#% 210173
#% 479658
#% 576113
#% 824671
#% 875022
#% 878299
#% 893139
#% 989584
#% 1015261
#% 1015279
#% 1016157
#% 1016210
#% 1181258
#! In diverse applications ranging from stock trading to traffic monitoring, popular data streams are typically monitored by multiple analysts for patterns of interest. These analysts may submit similar pattern mining requests, such as cluster detection queries, yet customized with different parameter settings. In this work, we present an efficient shared execution strategy for processing a large number of density-based cluster detection queries with arbitrary parameter settings. Given the high algorithmic complexity of the clustering process and the real-time responsiveness required by streaming applications, serving multiple such queries in a single system is extremely resource intensive. The naive method of detecting and maintaining clusters for different queries independently is often in-feasible in practice, as its demands on system resources increase dramatically with the cardinality of the query workload. To overcome this, we analyze the interrelations between the cluster sets identified by queries with different parameters settings, including both pattern-specific and window-specific parameters. We introduce the notion of the growth property among the cluster sets identified by different queries, and characterize the conditions under which it holds. By exploiting this growth property we propose a uniform solution, called Chandi, which represents identified cluster sets as one single compact structure and performs integrated maintenance on them -- resulting in significant sharing of computational and memory resources. Our comprehensive experimental study, using real data streams from domains of stock trades and moving object monitoring, demonstrates that Chandi is on average four times faster than the best alternative methods, while using 85% less memory space in our test cases. It also shows that Chandi scales in handling large numbers of queries on the order of hundreds or even thousands under high input data rates.

#index 1328183
#* Distance-join: pattern match query in a large graph database
#@ Lei Zou;Lei Chen;M. Tamer Özsu
#t 2009
#c 4
#% 152937
#% 316709
#% 333973
#% 378391
#% 443208
#% 588504
#% 657739
#% 722530
#% 765429
#% 814646
#% 864425
#% 864462
#% 937108
#% 960304
#% 960305
#% 976785
#% 989645
#% 1022280
#% 1063513
#% 1206685
#% 1206699
#% 1206703
#! The growing popularity of graph databases has generated interesting data management problems, such as subgraph search, shortest-path query, reachability verification, and pattern match. Among these, a pattern match query is more flexible compared to a subgraph search and more informative compared to a shortest-path or reachability query. In this paper, we address pattern match problems over a large data graph G. Specifically, given a pattern graph (i.e., query Q), we want to find all matches (in G) that have the similar connections as those in Q. In order to reduce the search space significantly, we first transform the vertices into points in a vector space via graph embedding techniques, coverting a pattern match query into a distance-based multi-way join problem over the converted vector space. We also propose several pruning strategies and a join order selection method to process join processing efficiently. Extensive experiments on both real and synthetic datasets show that our method outperforms existing ones by orders of magnitude.

#index 1328184
#* Creating competitive products
#@ Qian Wan;Raymond Chi-Wing Wong;Ihab F. Ilyas;M. Tamer Özsu;Yu Peng
#t 2009
#c 4
#% 288976
#% 289148
#% 465167
#% 480671
#% 800512
#% 806212
#% 810024
#% 824670
#% 824671
#% 824672
#% 875011
#% 989652
#% 993954
#% 1083667
#% 1127433
#% 1133032
#% 1207004
#! The importance of dominance and skyline analysis has been well recognized in multi-criteria decision making applications. Most previous works study how to help customers find a set of "best" possible products from a pool of given products. In this paper, we identify an interesting problem, creating competitive products, which has not been studied before. Given a set of products in the existing market, we want to study how to create a set of "best" possible products such that the newly created products are not dominated by the products in the existing market. We refer such products as competitive products. A straightforward solution is to generate a set of all possible products and check for dominance relationships. However, the whole set is quite large. In this paper, we propose a solution to generate a subset of this set effectively. An extensive performance study using both synthetic and real datasets is reported to verify its effectiveness and efficiency.

#index 1328185
#* Data processing on FPGAs
#@ Rene Mueller;Jens Teubner;Gustavo Alonso
#t 2009
#c 4
#% 252608
#% 397361
#% 571047
#% 726621
#% 765419
#% 810039
#% 850738
#% 874997
#% 878299
#% 1002570
#% 1011733
#% 1022311
#% 1127563
#% 1139191
#% 1157687
#% 1328128
#% 1468424
#! Computer architectures are quickly changing toward heterogeneous many-core systems. Such a trend opens up interesting opportunities but also raises immense challenges since the efficient use of heterogeneous many-core systems is not a trivial problem. In this paper, we explore how to program data processing operators on top of field-programmable gate arrays (FPGAs). FPGAs are very versatile in terms of how they can be used and can also be added as additional processing units in standard CPU sockets. In the paper, we study how data processing can be accelerated using an FPGA. Our results indicate that efficient usage of FPGAs involves non-trivial aspects such as having the right computation model (an asynchronous sorting network in this case); a careful implementation that balances all the design constraints in an FPGA; and the proper integration strategy to link the FPGA to the rest of the system. Once these issues are properly addressed, our experiments show that FPGAs exhibit performance figures competitive with those of modern general-purpose CPUs while offering significant advantages in terms of power consumption and parallel stream evaluation.

#index 1328186
#* HadoopDB: an architectural hybrid of MapReduce and DBMS technologies for analytical workloads
#@ Azza Abouzeid;Kamil Bajda-Pawlikowski;Daniel Abadi;Avi Silberschatz;Alexander Rasin
#t 2009
#c 4
#% 479905
#% 479920
#% 723288
#% 824697
#% 963669
#% 1063553
#% 1127559
#% 1217159
#! The production environment for analytical data management applications is rapidly changing. Many enterprises are shifting away from deploying their analytical databases on high-end proprietary machines, and moving towards cheaper, lower-end, commodity hardware, typically arranged in a shared-nothing MPP architecture, often in a virtualized environment inside public or private "clouds". At the same time, the amount of data that needs to be analyzed is exploding, requiring hundreds to thousands of machines to work in parallel to perform the analysis. There tend to be two schools of thought regarding what technology to use for data analysis in such an environment. Proponents of parallel databases argue that the strong emphasis on performance and efficiency of parallel databases makes them well-suited to perform such analysis. On the other hand, others argue that MapReduce-based systems are better suited due to their superior scalability, fault tolerance, and flexibility to handle unstructured data. In this paper, we explore the feasibility of building a hybrid system that takes the best features from both technologies; the prototype we built approaches parallel databases in performance and efficiency, yet still yields the scalability, fault tolerance, and flexibility of MapReduce-based systems.

#index 1328187
#* Anonymization of set-valued data via top-down, local generalization
#@ Yeye He;Jeffrey F. Naughton
#t 2009
#c 4
#% 285932
#% 300184
#% 317313
#% 342643
#% 427199
#% 449588
#% 481290
#% 481758
#% 576761
#% 577233
#% 740764
#% 800515
#% 801690
#% 810011
#% 864406
#% 864412
#% 874988
#% 879635
#% 881546
#% 881551
#% 956557
#% 960291
#% 993988
#% 1019163
#% 1063477
#% 1066737
#% 1083709
#% 1117733
#% 1127361
#% 1176943
#% 1206581
#% 1415847
#! Set-valued data, in which a set of values are associated with an individual, is common in databases ranging from market basket data, to medical databases of patients' symptoms and behaviors, to query engine search logs. Anonymizing this data is important if we are to reconcile the conflicting demands arising from the desire to release the data for study and the desire to protect the privacy of individuals represented in the data. Unfortunately, the bulk of existing anonymization techniques, which were developed for scenarios in which each individual is associated with only one sensitive value, are not well-suited for set-valued data. In this paper we propose a top-down, partition-based approach to anonymizing set-valued data that scales linearly with the input size and scores well on an information-loss data quality metric. We further note that our technique can be applied to anonymize the infamous AOL query logs, and discuss the merits and challenges in anonymizing query logs using our approach.

#index 1328188
#* k-automorphism: a general framework for privacy preserving network publication
#@ Lei Zou;Lei Chen;M. Tamer Özsu
#t 2009
#c 4
#% 248030
#% 576761
#% 841960
#% 864412
#% 881460
#% 881523
#% 893100
#% 904307
#% 956511
#% 960291
#% 1063476
#% 1127360
#% 1127417
#% 1127418
#% 1206581
#% 1206763
#! The growing popularity of social networks has generated interesting data management and data mining problems. An important concern in the release of these data for study is their privacy, since social networks usually contain personal information. Simply removing all identifiable personal information (such as names and social security number) before releasing the data is insufficient. It is easy for an attacker to identify the target by performing different structural queries. In this paper we propose k-automorphism to protect against multiple structural attacks and develop an algorithm (called KM) that ensures k-automorphism. We also discuss an extension of KM to handle "dynamic" releases of the data. Extensive experiments show that the algorithm performs well in terms of protection it provides.

#index 1328189
#* Distribution based microdata anonymization
#@ Nick Koudas;Divesh Srivastava;Ting Yu;Qing Zhang
#t 2009
#c 4
#% 443463
#% 576762
#% 810011
#% 864412
#% 893100
#% 960291
#% 1083631
#% 1127360
#% 1127417
#% 1206763
#% 1725659
#! Before sharing to support ad hoc aggregate analyses, microdata often need to be anonymized to protect the privacy of individuals. A variety of privacy models have been proposed for microdata anonymization. Many of these models (e.g., t-closeness) essentially require that, after anonymization, groups of sensitive attribute values follow specified distributions. To support such models, in this paper we study the problem of transforming a group of sensitive attribute values to follow a certain target distribution with minimal data distortion. Specifically, we develop and evaluate a novel methodology that combines the use of sensitive attribute permutation and generalization with the addition of fake sensitive attribute values to achieve this transformation. We identify metrics related to accuracy of aggregate query answers over the transformed data, and develop efficient anonymization algorithms to optimize these accuracy metrics. Using a variety of data sets, we experimentally demonstrate the effectiveness of our techniques.

#index 1328190
#* On chase termination beyond stratification
#@ Michael Meier;Michael Schmidt;Georg Lausen
#t 2009
#c 4
#% 583
#% 287336
#% 378409
#% 411569
#% 416034
#% 465053
#% 572311
#% 826032
#% 857502
#% 912245
#% 993478
#% 1039063
#% 1063724
#% 1179997
#% 1206987
#! We study the termination problem of the chase algorithm, a central tool in various database problems such as the constraint implication problem, Conjunctive Query optimization, rewriting queries using views, data exchange, and data integration. The basic idea of the chase is, given a database instance and a set of constraints as input, to fix constraint violations in the database instance. It is well-known that, for an arbitrary set of constraints, the chase does not necessarily terminate (in general, it is even undecidable if it does or not). Addressing this issue, we review the limitations of existing sufficient termination conditions for the chase and develop new techniques that allow us to establish weaker sufficient conditions. In particular, we introduce two novel termination conditions called safety and inductive restriction, and use them to define the so-called T-hierarchy of termination conditions. We then study the interrelations of our termination conditions with previous conditions and the complexity of checking our conditions. This analysis leads to an algorithm that checks membership in a level of the T-hierarchy and accounts for the complexity of termination conditions. As another contribution, we study the problem of data-dependent chase termination and present sufficient termination conditions w.r.t. fixed instances. They might guarantee termination although the chase does not terminate in the general case. As an application of our techniques beyond those already mentioned, we transfer our results into the field of query answering over knowledge bases where the chase on the underlying database may not terminate, making existing algorithms applicable to broader classes of constraints.

#index 1328191
#* Preventing bad plans by bounding the impact of cardinality estimation errors
#@ Guido Moerkotte;Thomas Neumann;Gabriele Steidl
#t 2009
#c 4
#% 554
#% 102784
#% 210190
#% 248822
#% 299989
#% 479648
#% 479938
#% 480125
#% 480805
#% 571094
#% 824756
#% 850727
#% 1015256
#! Query optimizers rely on accurate estimations of the sizes of intermediate results. Wrong size estimations can lead to overly expensive execution plans. We first define the q-error to measure deviations of size estimates from actual sizes. The q-error enables the derivation of two important results: (1) We provide bounds such that if the q-error is smaller than this bound, the query optimizer constructs an optimal plan. (2) If the q-error is bounded by a number q, we show that the cost of the produced plan is at most a factor of q4 worse than the optimal plan. Motivated by these findings, we next show how to find the best approximation under the q-error. These techniques can then be used to build synopsis for size estimates. Finally, we give some experimental results where we apply the developed techniques.

#index 1328192
#* Exact cardinality query optimization for optimizer testing
#@ Surajit Chaudhuri;Vivek Narasayya;Ravi Ramamurthy
#t 2009
#c 4
#% 36117
#% 82346
#% 102784
#% 172902
#% 248793
#% 300166
#% 341672
#% 397371
#% 408396
#% 443390
#% 480158
#% 480803
#% 643570
#% 765456
#% 1015256
#% 1016220
#% 1016225
#% 1026989
#! The accuracy of cardinality estimates is crucial for obtaining a good query execution plan. Today's optimizers make several simplifying assumptions during cardinality estimation that can lead to large errors and hence poor plans. In a scenario such as query optimizer testing it is very desirable to obtain the "best" plan, i.e., the plan produced when the cardinality of each relevant expression is exact. Such a plan serves as a baseline against which plans produced by using the existing cardinality estimation module in the query optimizer can be compared. However, obtaining all exact cardinalities by executing appropriate subexpressions can be prohibitively expensive. In this paper, we present a set of techniques that makes exact cardinality query optimization a viable option for a significantly larger set of queries than previously possible. We have implemented this functionality in Microsoft SQL Server and we present results using the TPC-H benchmark queries that demonstrate their effectiveness.

#index 1328193
#* Laconic schema mappings: computing the core with SQL queries
#@ Balder ten Cate;Laura Chiticariu;Phokion Kolaitis;Wang-Chiew Tan
#t 2009
#c 4
#% 287733
#% 378409
#% 572307
#% 599549
#% 806215
#% 810078
#% 824763
#% 826032
#% 893094
#% 1039063
#% 1063712
#% 1180001
#% 1217196
#! A schema mapping is a declarative specification of the relationship between instances of a source schema and a target schema. The data exchange (or data translation) problem asks: given an instance over the source schema, materialize an instance (or solution) over the target schema that satisfies the schema mapping. In general, a given source instance may have numerous different solutions. Among all the solutions, universal solutions and core universal solutions have been singled out and extensively studied. A universal solution is a most general one and also represents the entire space of solutions, while a core universal solution is the smallest universal solution and is unique up to isomorphism (hence, we can talk about the core). The problem of designing efficient algorithms for computing the core has attracted considerable attention in recent years. In this paper, we present a method for directly computing the core by SQL queries, when schema mappings are specified by source-to-target tuple-generating dependencies (s-t tgds). Unlike prior methods that, given a source instance, first compute a target instance and then recursively minimize that instance to the core, our method avoids the construction of such intermediate instances. This is done by rewriting the schema mapping into a laconic schema mapping that is specified by first-order s-t tgds with a linear order in the active domain of the source instances. A laconic schema mapping has the property that a "direct translation" of the source instance according to the laconic schema mapping produces the core. Furthermore, a laconic schema mapping can be easily translated into SQL, hence it can be optimized and executed by a database system to produce the core. We also show that our results are optimal: the use of the linear order is inevitable and, in general, schema mappings with constraints over the target schema cannot be rewritten to a laconic schema mapping.

#index 1328194
#* Inverting schema mappings: bridging the gap between theory and practice
#@ Marcelo Arenas;Jorge Pérez;Juan Reutter;Cristian Riveros
#t 2009
#c 4
#% 663
#% 237190
#% 289384
#% 378409
#% 562454
#% 572307
#% 572311
#% 762652
#% 765540
#% 801691
#% 809249
#% 810021
#% 826032
#% 850730
#% 893094
#% 960233
#% 976996
#% 976997
#% 997492
#% 1015302
#% 1063710
#% 1063712
#% 1180001
#! The inversion of schema mappings has been identified as one of the fundamental operators for the development of a general framework for metadata management. In fact, during the last years three alternative notions of inversion for schema mappings have been proposed (Fagin-inverse [10], quasi-inverse [14] and maximum recovery [2]). However, the procedures that have been developed for computing these operators have some features that limit their practical applicability. First, these algorithms work in exponential time and produce inverse mappings of exponential size. Second, these algorithms express inverses in some mappings languages which include features that are difficult to use in practice. A typical example is the use of disjunction in the conclusion of the mapping rules, which makes the process of exchanging data much more complicated. In this paper, we propose solutions for the two problems mentioned above. First, we provide a polynomial time algorithm that computes the three inverse operators mentioned above given a mapping specified by a set of tuple-generating dependencies (tgds). This algorithm uses an output mapping language that can express these three operators in a compact way and, in fact, can compute inverses for a much larger class of mappings. Unfortunately, it has already been proved that this type of mapping languages has to include some features that are difficult to use in practice and, hence, this is also the case for our output mapping language. Thus, as our second contribution, we propose a new and natural notion of inversion that overcomes this limitation. In particular, every mapping specified by a set of tgds admits an inverse under this new notion that can be expressed in a mapping language that slightly extends tgds, and that has the same good properties for data exchange as tgds. Finally, as our last contribution, we provide an algorithm for computing such inverses.

#index 1328195
#* Full-fidelity flexible object-oriented XML access
#@ James F. Terwilliger;Philip A. Bernstein;Sergey Melnik
#t 2009
#c 4
#% 805866
#% 869472
#% 875029
#% 948929
#% 957975
#% 960272
#% 960309
#% 994015
#% 1127571
#% 1408047
#! Developers need to programmatically access persistent XML data. Object-oriented access is often the preferred method. Translating XML data into objects or vice-versa is a hard problem due to the data model mismatch and the difficulty of query translation. We propose a framework that addresses this problem by transforming object-based queries and updates into queries and updates on XML using flexible, declarative mappings between classes and XML schema types. The same mappings are used to shred XML fragments from query results into client-side objects. Information in the XML store that is not mapped using the mapping language, such as comments and processing instructions, are also made available in the object representation.

#index 1328196
#* Privacy-aware mobile services over road networks
#@ Ting Wang;Ling Liu
#t 2009
#c 4
#% 114573
#% 259642
#% 452685
#% 539417
#% 579686
#% 745980
#% 755205
#% 810048
#% 812797
#% 824723
#% 893092
#% 893151
#% 911803
#% 948609
#% 956531
#% 981620
#% 1015321
#% 1016199
#% 1055695
#% 1206712
#% 1207071
#% 1720757
#! Consider a mobile client who travels over roads and wishes to receive location-based services (LBS) from untrusted service providers. How might the user obtain such services without exposing her private position information? Meanwhile, how could the privacy protection mechanism incur no disincentive, e.g., excessive computation or communication cost, for any service provider or mobile user to participate in such a scheme? We detail this problem and present a general model for privacy-aware mobile services. A series of key features distinguish our solution from existing ones: a) it adopts the network-constrained mobility model (instead of the conventional random-waypoint model) to capture the privacy vulnerability of mobile users; b) it regards the attack resilience (for mobile users) and the query-processing cost (for service providers) as two critical measures for designing location privatization solutions, and provides corresponding analytical models; c) it proposes a robust and scalable location anonymization model, XStar, which best leverages the two measures; d) it introduces multi-folded optimizations in implementing XStar, which lead to further performance improvement. A comprehensive experimental evaluation is conducted to validate the analytical models and the efficacy of XStar.

#index 1328197
#* A fair assignment algorithm for multiple preference queries
#@ Leong Hou U;Nikos Mamoulis;Kyriakos Mouratidis
#t 2009
#c 4
#% 59697
#% 122671
#% 300162
#% 300180
#% 410276
#% 443396
#% 465167
#% 480671
#% 643566
#% 806212
#% 824670
#% 875023
#% 919829
#% 941785
#% 993954
#% 1022250
#% 1063470
#% 1092017
#% 1127433
#% 1147660
#% 1206656
#% 1206975
#% 1570410
#% 1698321
#! Consider an internship assignment system, where at the end of each academic year, interested university students search and apply for available positions, based on their preferences (e.g., nature of the job, salary, office location, etc). In a variety of facility, task or position assignment contexts, users have personal preferences expressed by different weights on the attributes of the searched objects. Although individual preference queries can be evaluated by selecting the object in the database with the highest aggregate score, in the case of multiple simultaneous requests, a single object cannot be assigned to more than one users. The challenge is to compute a fair 1--1 matching between the queries and the objects. We model this as a stable-marriage problem and propose an efficient method for its processing. Our algorithm iteratively finds stable query-object pairs and removes them from the problem. At its core lies a novel skyline maintenance technique, which we prove to be I/O optimal. We conduct an extensive experimental evaluation using real and synthetic data, which demonstrates that our approach outperforms adaptations of previous methods by several orders of magnitude.

#index 1328198
#* Pangea: an eager database replication middleware guaranteeing snapshot isolation without modification of database servers
#@ Takeshi Mishima;Hiroshi Nakamura
#t 2009
#c 4
#% 9241
#% 201869
#% 210179
#% 307360
#% 745536
#% 793894
#% 810043
#% 814649
#% 824698
#% 850309
#% 902008
#% 916318
#% 938074
#% 960202
#% 993994
#% 1063525
#% 1066740
#% 1180884
#% 1180907
#! Recently, several middleware-based approaches have been proposed. If we implement all functionalities of database replication only in a middleware layer, we can avoid the high cost of modifying existing database servers or scratch-building. However, it is a big challenge to propose middleware which can enhance performance and scalability without modification of database servers because the restriction may cause extra overhead. Unfortunately, many existing middleware-based approaches suffer from several shortcomings, i.e., some cause a hidden deadlock, some provide only table-level locking, some rely on total order communication tools, and others need to modify existing database servers. In this paper, we propose Pangea, a new eager database replication middleware guaranteeing snapshot isolation that solves the drawbacks of existing middleware by exploiting the property of the first updater wins rule. We have implemented the prototype of Pangea on top of PostgreSQL servers without modification. An advantage of Pangea is that it uses less than 2000 lines of C code. Our experimental results with the TPC-W benchmark reveal that, compared to an existing middleware guaranteeing snapshot isolation without modification of database servers, Pangea provides better performance in terms of throughput and scalability.

#index 1328199
#* Harvesting relational tables from lists on the web
#@ Hazem Elmeleegy;Jayant Madhavan;Alon Halevy
#t 2009
#c 4
#% 273925
#% 326303
#% 330784
#% 333943
#% 348146
#% 480824
#% 577319
#% 654469
#% 765411
#% 805846
#% 976684
#% 1022353
#% 1127393
#% 1127557
#% 1264778
#! A large number of web pages contain data structured in the form of "lists". Many such lists can be further split into multi-column tables, which can then be used in more semantically meaningful tasks. However, harvesting relational tables from such lists can be a challenging task. The lists are manually generated and hence need not have well defined templates -- they have inconsistent delimiters (if any) and often have missing information. We propose a novel technique for extracting tables from lists. The technique is domain-independent and operates in a fully unsupervised manner. We first use multiple sources of information to split individual lines into multiple fields, and then compare the splits across multiple lines to identify and fix incorrect splits and bad alignments. In particular, we exploit a corpus of HTML tables, also extracted from the Web, to identify likely fields and good alignments. For each extracted table, we compute an extraction score that reflects our confidence in the table's quality. We conducted an extensive experimental study using both real web lists and lists derived from tables on the Web. The experiments demonstrate the ability of our technique to extract tables with high accuracy. In addition, we applied our technique on a large sample of about 100,000 lists crawled from the Web. The analysis of the extracted tables have led us to believe that there are likely to be tens of millions of useful and query-able relational tables extractable from lists on the Web.

#index 1328200
#* Data integration for the relational web
#@ Michael J. Cafarella;Alon Halevy;Nodira Khoussainova
#t 2009
#c 4
#% 283052
#% 322884
#% 333990
#% 480496
#% 480499
#% 572314
#% 754068
#% 810014
#% 936926
#% 955063
#% 1022235
#% 1127393
#% 1328199
#! The Web contains a vast amount of structured information such as HTML tables, HTML lists and deep-web databases; there is enormous potential in combining and re-purposing this data in creative ways. However, integrating data from this relational web raises several challenges that are not addressed by current data integration systems or mash-up tools. First, the structured data is usually not published cleanly and must be extracted (say, from an HTML list) before it can be used. Second, due to the vastness of the corpus, a user can never know all of the potentially-relevant databases ahead of time (much less write a wrapper or mapping for each one); the source databases must be discovered during the integration process. Third, some of the important information regarding the data is only present in its enclosing web page and needs to be extracted appropriately. This paper describes Octopus, a system that combines search, extraction, data cleaning and integration, and enables users to create new data sets from those found on the Web. The key idea underlying Octopus is to offer the user a set of best-effort operators that automate the most labor-intensive tasks. For example, the Search operator takes a search-style keyword query and returns a set of relevance-ranked and similarity-clustered structured data sources on the Web; the Context operator helps the user specify the semantics of the sources by inferring attribute values that may not appear in the source itself, and the Extend operator helps the user find related sources that can be joined to add new attributes to a table. Octopus executes some of these operators automatically, but always allows the user to provide feedback and correct errors. We describe the algorithms underlying each of these operators and experiments that demonstrate their efficacy.

#index 1328201
#* Normalization and optimization of schema mappings
#@ Georg Gottlob;Reinhard Pichler;Vadim Savenkov
#t 2009
#c 4
#% 583
#% 663
#% 289266
#% 289384
#% 378409
#% 582130
#% 599549
#% 801691
#% 806215
#% 809239
#% 826032
#% 874882
#% 893089
#% 960233
#% 976995
#% 1036084
#% 1063712
#% 1063722
#% 1063723
#! Schema mappings are high-level specifications that describe the relationship between two database schemas. They are an important tool in several areas of database research, notably in data integration and data exchange. However, a concrete theory of schema mapping optimization including the formulation of optimality criteria and the construction of algorithms for computing optimal schema mappings is completely lacking to date. The goal of this work is to fill this gap. We start by presenting a system of rewrite rules to minimize sets of source-to-target tuple-generating dependencies (st-tgds, for short). Moreover, we show that the result of this minimization is unique up to variable renaming. Hence, our optimization also yields a schema mapping normalization. By appropriately extending our rewrite rule system, we also provide a normalization of schema mappings containing equality-generating target-dependencies (egds). An important application of such a normalization is in the area of defining the semantics of query answering in data exchange, since several definitions in this area depend on the concrete syntactic representation of the st-tgds. This is, in particular, the case for queries with negated atoms and for aggregate queries. The normalization of schema mappings allows us to eliminate the effect of the concrete syntactic representation of the st-tgds from the semantics of query answering. We discuss in detail how our results can be fruitfully applied to aggregate queries.

#index 1328202
#* Continuous monitoring of nearest neighbors on land surface
#@ Songhua Xing;Cyrus Shahabi;Bei Pan
#t 2009
#c 4
#% 86822
#% 201876
#% 300163
#% 397377
#% 413797
#% 427199
#% 800571
#% 818938
#% 841649
#% 864466
#% 893092
#% 993955
#% 1015321
#% 1016191
#% 1016199
#% 1063472
#% 1072648
#% 1127432
#! As geo-realistic rendering of land surfaces is becoming commonplace in geographical information systems (GIS), games and online Earth visualization platforms, a new type of k Nearest Neighbor (kNN) queries, "surface" k Nearest Neighbor (skNN) queries, has emerged and been investigated recently, which extends the traditional kNN queries to a constrained third dimension (i.e., land surface). All existing techniques, however, assume a static environment, limiting their utility in emerging applications (e.g., Location-based Services) where objects move. In this paper, for the first time, we propose two exact methods that can continuously answer skNN queries in a highly dynamic environment which allows for arbitrary movements of data objects. The first method, inspired by the existing techniques in monitoring kNN in road networks [7] maintains an analogous counterpart of the Dijkstra Expansion Tree on land surface, called Surface Expansion Tree (SE-Tree). However, we show the concept of expansion tree for land surface does not work as SE-tree suffers from intrinsic defects: it is fat and short, and hence does not improve the query efficiency. Therefore, we propose a superior approach that partitions SE-Tree into hierarchical chunks of pre-computed surface distances, called Angular Surface Index Tree (ASI-Tree). Unlike SE-tree, ASI-Tree is a well balanced thin and tall tree. With ASI-Tree, we can continuously monitor skNN queries efficiently with low CPU and I/O overheads by both speeding up the surface shortest path computations and localizing the searches. We experimentally verify the applicability and evaluate the efficiency of the proposed methods with both real world and synthetic data sets. ASI-Tree consistently and significantly outperforms SE-Tree in all cases.

#index 1328203
#* Efficient method for maximizing bichromatic reverse nearest neighbor
#@ Raymond Chi-Wing Wong;M. Tamer Özsu;Philip S. Yu;Ada Wai-Chee Fu;Lian Liu
#t 2009
#c 4
#% 6788
#% 11274
#% 86950
#% 235114
#% 300163
#% 303067
#% 480661
#% 824730
#% 893141
#% 1022250
#% 1063470
#% 1328203
#% 1720751
#! Bichromatic reverse nearest neighbor (BRNN) has been extensively studied in spatial database literature. In this paper, we study a related problem called MaxBRNN: find an optimal region that maximizes the size of BRNNs. Such a problem has many real life applications, including the problem of finding a new server point that attracts as many customers as possible by proximity. A straightforward approach is to determine the BRNNs for all possible points that are not feasible since there are a large (or infinite) number of possible points. To the best of our knowledge, the fastest known method has exponential time complexity on the data size. Based on some interesting properties of the problem, we come up with an efficient algorithm called MaxOverlap. Extensive experiments are conducted to show that our algorithm is many times faster than the best-known technique.

#index 1328204
#* Lazy updates: an efficient technique to continuously monitoring reverse kNN
#@ Muhammad Aamir Cheema;Xuemin Lin;Ying Zhang;Wei Wang;Wenjie Zhang
#t 2009
#c 4
#% 237205
#% 300163
#% 421124
#% 458853
#% 465009
#% 495433
#% 730019
#% 800510
#% 800571
#% 800572
#% 810048
#% 810061
#% 832568
#% 864464
#% 889094
#% 975024
#% 993955
#% 1015297
#% 1016191
#% 1080128
#% 1127435
#! In this paper, we study the problem of continuous monitoring of reverse k nearest neighbor queries. Existing continuous reverse nearest neighbor monitoring techniques are sensitive towards objects and queries movement. For example, the results of a query are to be recomputed whenever the query changes its location. We present a framework for continuous reverse k nearest neighbor queries by assigning each object and query with a rectangular safe region such that the expensive recomputation is not required as long as the query and objects remain in their respective safe regions. This significantly improves the computation cost. As a by-product, our framework also reduces the communication cost in client-server architectures because an object does not report its location to the server unless it leaves its safe region or the server sends a location update request. We also conduct a rigid cost analysis to guide an effective selection of such rectangular safe regions. The extensive experiments demonstrate that our techniques outperform the existing techniques by an order of magnitude in terms of computation cost and communication cost.

#index 1328205
#* NEAR-Miner: mining evolution associations of web site directories for efficient maintenance of web archives
#@ Ling Chen;Sourav S. Bhowmick;Wolfgang Nejdl
#t 2009
#c 4
#% 227859
#% 227919
#% 268087
#% 330604
#% 340924
#% 411375
#% 464603
#% 480136
#% 640706
#% 731406
#% 754058
#% 799740
#% 805879
#% 835018
#% 879600
#% 917970
#% 943880
#% 1034725
#% 1055715
#% 1099785
#% 1190280
#! Web archives preserve the history of autonomous Web sites and are potential gold mines for all kinds of media and business analysts. The most common Web archiving technique uses crawlers to automate the process of collecting Web pages. However, (re)downloading entire collection of pages periodically from a large Web site is unfeasible. In this paper, we take a step towards addressing this problem. We devise a data mining-driven policy for selectively (re)downloading Web pages that are located in hierarchical directory structures which are believed to have changed significantly (e.g., a substantial percentage of pages are inserted to/removed from the directory). Consequently, there is no need to download and maintain pages that have not changed since the last crawl as they can be easily retrieved from the archive. In our approach, we propose an off-line data mining algorithm called near-Miner that analyzes the evolution history of Web directory structures of the original Web site stored in the archive and mines negatively correlated association rules (near) between ancestor-descendant Web directories. These rules indicate the evolution correlations between Web directories. Using the discovered rules, we propose an efficient Web archive maintenance algorithm called warm that optimally skips the subdirectories (during the next crawl) which are negatively correlated with it in undergoing significant changes. Our experimental results with real data show that our approach improves the efficiency of the archive maintenance process significantly while sacrificing slightly in keeping the "freshness" of the archives. Furthermore, our experiments demonstrate that it is not necessary to discover nears frequently as the mining rules can be utilized effectively for archive maintenance over multiple versions.

#index 1328206
#* An audit environment for outsourcing of frequent itemset mining
#@ W. K. Wong;David W. Cheung;Edward Hung;Ben Kao;Nikos Mamoulis
#t 2009
#c 4
#% 227917
#% 481290
#% 576118
#% 810042
#% 824701
#% 839173
#% 874980
#% 993960
#% 1022211
#% 1022213
#% 1022214
#% 1022267
#% 1705491
#! Finding frequent itemsets is the most costly task in association rule mining. Outsourcing this task to a service provider brings several benefits to the data owner such as cost relief and a less commitment to storage and computational resources. Mining results, however, can be corrupted if the service provider (i) is honest but makes mistakes in the mining process, or (ii) is lazy and reduces costly computation, returning incomplete results, or (iii) is malicious and contaminates the mining results. We address the integrity issue in the outsourcing process, i.e., how the data owner verifies the correctness of the mining results. For this purpose, we propose and develop an audit environment, which consists of a database transformation method and a result verification method. The main component of our audit environment is an artificial itemset planting (AIP) technique. We provide a theoretical foundation on our technique by proving its appropriateness and showing probabilistic guarantees about the correctness of the verification process. Through analytical and experimental studies, we show that our technique is both effective and efficient.

#index 1328207
#* Publishing naive Bayesian classifiers: privacy without accuracy loss
#@ Barzan Mozafari;Carlo Zaniolo
#t 2009
#c 4
#% 246831
#% 297186
#% 299970
#% 300184
#% 443382
#% 576111
#% 576761
#% 662761
#% 727904
#% 729962
#% 742048
#% 765449
#% 769943
#% 788044
#% 801690
#% 805092
#% 809244
#% 810010
#% 810011
#% 824727
#% 843878
#% 844353
#% 904305
#% 937550
#% 1022246
#% 1181951
#% 1206785
#% 1661432
#% 1700133
#% 1725656
#! We address the problem of publishing a Naïve Bayesian Classifier (NBC) or, equivalently, publishing the necessary views for building an NBC, while protecting privacy of the individuals who provided the training data. Our approach completely preserves the accuracy of the original classifier, and thus significantly improves on current approaches, such as randomization or anonymization, which typically degrade accuracy to preserve privacy. Current query-view security checkers address the question of 'Is the view safe to publish?' and are computationally expensive (often Πp2-complete). Here instead, we tackle the question of 'How to make a view safe to publish?' and propose a linear-time algorithm to publish safe NBC-enabling views. We first show that a simple measure that restricts the ratios between the published NBC statistics is sufficient to prevent any breach of privacy. Then, we propose a linear-time algorithm to enforce this measure by producing perturbed statistics that assure both (i) individuals' privacy, and (ii) a classifier that behaves in the same way as the NBC trained on the original data. By carefully expressing the derived statistics using rational numbers, we can easily produce synthetic (sanitized) datasets. Thus, for any given dataset, we produce another dataset that is secure to publish (w.r.t. a uniform prior) and achieves the same classification accuracy. Finally, we extend our results by providing sufficient conditions to cope with arbitrary (non-uniform prior) distributions, and we validate their effectiveness in practice through experiments on real-world data.

#index 1328208
#* Workload-aware indexing of continuously moving objects
#@ Kostas Tzoumas;Man Lung Yiu;Christian S. Jensen
#t 2009
#c 4
#% 213975
#% 287466
#% 300174
#% 322381
#% 333969
#% 554884
#% 772839
#% 800186
#% 800539
#% 810025
#% 810048
#% 810645
#% 839702
#% 879211
#% 960268
#% 1015305
#% 1015320
#% 1016178
#% 1016193
#% 1022251
#% 1046510
#% 1063471
#% 1116025
#% 1127612
#% 1206768
#! The increased deployment of sensors and data communication networks yields data management workloads with update loads that are intense, skewed, and highly bursty. Query loads resulting from location-based services are expected to exhibit similar characteristics. In such environments, index structures can easily become performance bottlenecks. We address the need for indexing that is adaptive to the workload characteristics, called workload-aware, in order to cover the space in between maintaining an accurate index, and having no index at all. Our proposal, QU-Trade, extends R-tree type indexing and achieves workload-awareness by controlling the underlying index's filtering quality. QU-Trade safely drops index updates, increasing the overlap in the index when the workload is update-intensive, and it restores the filtering capabilities of the index when the workload becomes query-intensive. This is done in a non-uniform way in space so that the quality of the index remains high in frequently queried regions, while it deteriorates in frequently updated regions. The adaptation occurs online, without the need for a learning phase. We apply QU-Trade to the R-tree and the TPR-tree, and we offer analytical and empirical studies. In the presence of substantial workload skew, QU-Trade can achieve index update costs close to zero and can also achieve virtually the same query cost as the underlying index.

#index 1328209
#* Effectively indexing uncertain moving objects for predictive queries
#@ Meihui Zhang;Su Chen;Christian S. Jensen;Beng Chin Ooi;Zhenjie Zhang
#t 2009
#c 4
#% 102808
#% 227864
#% 300174
#% 481759
#% 654487
#% 765452
#% 771228
#% 772835
#% 814349
#% 818434
#% 870306
#% 945789
#% 1015320
#% 1016193
#% 1063471
#% 1127377
#% 1127612
#% 1206625
#% 1206716
#% 1408794
#% 1669479
#! Moving object indexing and query processing is a well studied research topic, with applications in areas such as intelligent transport systems and location-based services. While much existing work explicitly or implicitly assumes a deterministic object movement model, real-world objects often move in more complex and stochastic ways. This paper investigates the possibility of a marriage between moving-object indexing and probabilistic object modeling. Given the distributions of the current locations and velocities of moving objects, we devise an efficient inference method for the prediction of future locations. We demonstrate that such prediction can be seamlessly integrated into existing index structures designed for moving objects, thus improving the meaningfulness of range and nearest neighbor query results in highly dynamic and uncertain environments. The paper reports on extensive experiments on the Bx-tree that offer insights into the properties of the paper's proposal.

#index 1328210
#* Path oracles for spatial networks
#@ Jagan Sankaranarayanan;Hanan Samet;Houman Alborzi
#t 2009
#c 4
#% 281738
#% 338382
#% 443208
#% 813718
#% 818938
#% 824723
#% 1015321
#% 1016199
#% 1063472
#% 1138502
#% 1206994
#% 1217191

#index 1328211
#* Correlation maps: a compressed access method for exploiting soft functional dependencies
#@ Hideaki Kimura;George Huo;Alexander Rasin;Samuel Madden;Stanley B. Zdonik
#t 2009
#c 4
#% 32891
#% 287715
#% 299989
#% 334007
#% 465163
#% 479814
#% 480805
#% 765455
#% 893158
#% 993478
#% 993498
#% 1015310
#% 1206647
#! In relational query processing, there are generally two choices for access paths when performing a predicate lookup for which no clustered index is available. One option is to use an unclustered index. Another is to perform a complete sequential scan of the table. Many analytical workloads do not benefit from the availability of unclustered indexes; the cost of random disk I/O becomes prohibitive for all but the most selective queries. It has been observed that a secondary index on an unclustered attribute can perform well under certain conditions if the unclustered attribute is correlated with a clustered index attribute [4]. The clustered index will co-locate values and the correlation will localize access through the unclustered attribute to a subset of the pages. In this paper, we show that in a real application (SDSS) and widely used benchmark (TPC-H), there exist many cases of attribute correlation that can be exploited to accelerate queries. We also discuss a tool that can automatically suggest useful pairs of correlated attributes. It does so using an analytical cost model that we developed, which is novel in its awareness of the effects of clustering and correlation. Furthermore, we propose a data structure called a Correlation Map (CM) that expresses the mapping between the correlated attributes, acting much like a secondary index. The paper also discusses how bucketing on the domains of both attributes in the correlated attribute pair can dramatically reduce the size of the CM to be potentially orders of magnitude smaller than that of a secondary B+Tree index. This reduction in size allows us to create a large number of CMs that improve performance for a wide range of queries. The small size also reduces maintenance costs as we demonstrate experimentally.

#index 1328212
#* Index interactions in physical design tuning: modeling, analysis, and applications
#@ Karl Schnaitter;Neoklis Polyzotis;Lise Getoor
#t 2009
#c 4
#% 36119
#% 169305
#% 458523
#% 480158
#% 482100
#% 778724
#% 781735
#% 810026
#% 993459
#% 1016220
#% 1022293
#% 1063540
#% 1127352
#% 1207102
#% 1207103
#% 1700123
#! One of the key tasks of a database administrator is to optimize the set of materialized indices with respect to the current workload. To aid administrators in this challenging task, commercial DBMSs provide advisors that recommend a set of indices based on a sample workload. It is left for the administrator to decide which of the recommended indices to materialize and when. This decision requires some knowledge of how the indices benefit the workload, which may be difficult to understand if there are any dependencies or interactions among indices. Unfortunately, advisors do not provide this crucial information as part of the recommendation. Motivated by this shortcoming, we propose a framework and associated tools that can help an administrator understand the interactions within the recommended set of indices. We formalize the notion of index interactions and develop a novel algorithm to identify the interaction relationships that exist within a set of indices. We present experimental results with a prototype implementation over IBM DB2 that demonstrate the efficiency of our approach. We also describe two new database tuning tools that utilize information about index interactions. The first tool visualizes interactions based on a partitioning of the index-set into non-interacting subsets, and the second tool computes a schedule that materializes the indices over several maintenance windows with maximal overall benefit. In both cases, we provide strong analytical results showing that index interactions can enable enhanced functionality.

#index 1328213
#* Tuning database configuration parameters with iTuned
#@ Songyun Duan;Vamsidhar Thummala;Shivnath Babu
#t 2009
#c 4
#% 227883
#% 248815
#% 397390
#% 411780
#% 754086
#% 764032
#% 765425
#% 893178
#% 926881
#% 993933
#% 1016178
#% 1044516
#% 1063541
#% 1207186
#% 1247399
#! Database systems have a large number of configuration parameters that control memory distribution, I/O optimization, costing of query plans, parallelism, many aspects of logging, recovery, and other behavior. Regular users and even expert database administrators struggle to tune these parameters for good performance. The wave of research on improving database manageability has largely overlooked this problem which turns out to be hard to solve. We describe iTuned, a tool that automates the task of identifying good settings for database configuration parameters. iTuned has three novel features: (i) a technique called Adaptive Sampling that proactively brings in appropriate data through planned experiments to find high-impact parameters and high-performance parameter settings, (ii) an executor that supports online experiments in production database environments through a cycle-stealing paradigm that places near-zero overhead on the production workload; and (iii) portability across different database systems. We show the effectiveness of iTuned through an extensive evaluation based on different types of workloads, database systems, and usage scenarios.

#index 1328214
#* An evaluation of checkpoint recovery for massively multiplayer online games
#@ Marcos Vaz Salles;Tuan Cao;Benjamin Sowell;Alan Demers;Johannes Gehrke;Christoph Koch;Walker White
#t 2009
#c 4
#% 12638
#% 114582
#% 172913
#% 287741
#% 346821
#% 354745
#% 427195
#% 442834
#% 462497
#% 480085
#% 481590
#% 531907
#% 800545
#% 806969
#% 848329
#% 864486
#% 893146
#% 895869
#% 918225
#% 960236
#% 993493
#% 1021194
#% 1022298
#% 1063517
#% 1504576
#% 1795009
#! Massively multiplayer online games (MMOs) have emerged as an exciting new class of applications for database technology. MMOs simulate long-lived, interactive virtual worlds, which proceed by applying updates in frames or ticks, typically at 30 or 60 Hz. In order to sustain the resulting high update rates of such games, game state is kept entirely in main memory by the game servers. Nevertheless, durability in MMOs is usually achieved by a standard DBMS implementing ARIES-style recovery. This architecture limits scalability, forcing MMO developers to either invest in high-end hardware or to over-partition their virtual worlds. In this paper, we evaluate the applicability of existing checkpoint recovery techniques developed for main-memory DBMS to MMO workloads. Our thorough experimental evaluation uses a detailed simulation model fed with update traces generated synthetically and from a prototype game server. Based on our results, we recommend MMO developers to adopt a copy-on-update scheme with a double-backup disk organization to checkpoint game state. This scheme outperforms alternatives in terms of the latency introduced in the game as well the time necessary to recover after a crash.

#index 1328215
#* Evaluating clustering in subspace projections of high dimensional data
#@ Emmanuel Müller;Stephan Günnemann;Ira Assent;Thomas Seidl
#t 2009
#c 4
#% 248792
#% 273891
#% 300120
#% 300131
#% 316709
#% 397384
#% 469422
#% 481290
#% 727868
#% 785355
#% 844313
#% 871026
#% 893161
#% 915305
#% 926881
#% 1083683
#% 1083750
#% 1116995
#% 1117035
#% 1176982
#! Clustering high dimensional data is an emerging research field. Subspace clustering or projected clustering group similar objects in subspaces, i.e. projections, of the full space. In the past decade, several clustering paradigms have been developed in parallel, without thorough evaluation and comparison between these paradigms on a common basis. Conclusive evaluation and comparison is challenged by three major issues. First, there is no ground truth that describes the "true" clusters in real world data. Second, a large variety of evaluation measures have been used that reflect different aspects of the clustering result. Finally, in typical publications authors have limited their analysis to their favored paradigm only, while paying other paradigms little or no attention. In this paper, we take a systematic approach to evaluate the major paradigms in a common framework. We study representative clustering algorithms to characterize the different aspects of each paradigm and give a detailed comparison of their properties. We provide a benchmark set of results on a large variety of real world and synthetic data sets. Using different evaluation measures, we broaden the scope of the experimental analysis and create a common baseline for future developments and comparable evaluations in the field. For repeatability, all implementations, data sets and evaluation measures are available on our website.

#index 1328216
#* Framework for evaluating clustering algorithms in duplicate detection
#@ Oktie Hassanzadeh;Fei Chiang;Hyun Chul Lee;Renée J. Miller
#t 2009
#c 4
#% 36672
#% 53085
#% 70370
#% 288780
#% 296738
#% 420072
#% 430746
#% 466425
#% 593966
#% 748636
#% 749492
#% 755402
#% 765463
#% 765548
#% 800590
#% 805798
#% 824711
#% 827333
#% 850014
#% 893164
#% 896856
#% 934857
#% 956506
#% 976994
#% 992320
#% 1022227
#% 1022269
#% 1291119
#% 1861495
#! The presence of duplicate records is a major data quality concern in large databases. To detect duplicates, entity resolution also known as duplication detection or record linkage is used as a part of the data cleaning process to identify records that potentially refer to the same real-world entity. We present the Stringer system that provides an evaluation framework for understanding what barriers remain towards the goal of truly scalable and general purpose duplication detection algorithms. In this paper, we use Stringer to evaluate the quality of the clusters (groups of potential duplicates) obtained from several unconstrained clustering algorithms used in concert with approximate join techniques. Our work is motivated by the recent significant advancements that have made approximate join algorithms highly scalable. Our extensive evaluation reveals that some clustering algorithms that have never been considered for duplicate detection, perform extremely well in terms of both accuracy and scalability.

#index 1488674
#* Generating efficient execution plans for vertically partitioned XML databases
#@ Patrick Kling;M. Tamer Özsu;Khuzaima Daudjee
#t 2010
#c 4
#% 264263
#% 397375
#% 479956
#% 570880
#% 654485
#% 659999
#% 733593
#% 745477
#% 800577
#% 810070
#% 884458
#% 893106
#% 960276
#% 960361
#% 994015
#% 1022210
#% 1072645
#% 1728671
#! Experience with relational systems has shown that distribution is an effective way of improving the scalability of query evaluation. In this paper, we show how distributed query evaluation can be performed in a vertically partitioned XML database system. We propose a novel technique for constructing distributed execution plans that is independent of local query evaluation strategies. We then present a number of optimizations that allow us to further improve the performance of distributed query execution. Finally, we present a response time-based cost model that allows us to pick the best execution plan for a given query and database instance. Based on an implementation of our techniques within a native XML database system, we verify that our execution plans take advantage of the parallelism in a distributed system and that our cost model is effective at identifying the most advantageous plans.

#index 1488675
#* A generic framework for handling uncertain data with local correlations
#@ Xiang Lian;Lei Chen
#t 2010
#c 4
#% 86950
#% 201876
#% 251459
#% 287466
#% 333951
#% 654487
#% 659937
#% 893151
#% 992830
#% 1022203
#% 1022226
#% 1036083
#% 1063485
#% 1127377
#% 1127378
#% 1127415
#% 1129527
#% 1206646
#% 1206781
#% 1206877
#% 1207010
#% 1217181
#% 1290947
#% 1328151
#% 1328155
#% 1669490
#! Data uncertainty is ubiquitous in many real-world applications such as sensor/RFID data analysis. In this paper, we investigate uncertain data that exhibit local correlations, that is, each uncertain object is only locally correlated with a small subset of data, while being independent of others. We propose a generic framework for dealing with this kind of uncertain and locally correlated data, in which we investigate a classical spatial query, nearest neighbor query, on uncertain data with local correlations (namely LC-PNN). Most importantly, to enable fast LC-PNN query processing, we propose a novel filtering technique via offline pre-computations to reduce the query search space. We demonstrate through extensive experiments the efficiency and effectiveness of our approaches.

#index 1488676
#* SnipSuggest: context-aware autocompletion for SQL
#@ Nodira Khoussainova;YongChul Kwon;Magdalena Balazinska;Dan Suciu
#t 2010
#c 4
#% 96288
#% 256685
#% 284796
#% 387427
#% 590523
#% 960234
#% 960360
#% 1016203
#% 1022220
#% 1130852
#% 1218714
#% 1275193
#% 1328162
#! In this paper, we present SnipSuggest, a system that provides on-the-go, context-aware assistance in the SQL composition process. SnipSuggest aims to help the increasing population of non-expert database users, who need to perform complex analysis on their large-scale datasets, but have difficulty writing SQL queries. As a user types a query, SnipSuggest recommends possible additions to various clauses in the query using relevant snippets collected from a log of past queries. SnipSuggest's current capabilities include suggesting tables, views, and table-valued functions in the FROM clause, columns in the SELECT clause, predicates in the WHERE clause, columns in the GROUP BY clause, aggregates, and some support for sub-queries. SnipSuggest adjusts its recommendations according to the context: as the user writes more of the query, it is able to provide more accurate suggestions. We evaluate SnipSuggest over two query logs: one from an undergraduate database class and another from the Sloan Digital Sky Survey database. We show that SnipSuggest is able to recommend useful snippets with up to 93.7% average precision, at interactive speed. We also show that SnipSuggest outperforms naïve approaches, such as recommending popular snippets.

#index 1488677
#* The complexity of causality and responsibility for query answers and non-answers
#@ Alexandra Meliou;Wolfgang Gatterbauer;Katherine F. Moore;Dan Suciu
#t 2010
#c 4
#% 318704
#% 339937
#% 384978
#% 449788
#% 464891
#% 528334
#% 873942
#% 976984
#% 976987
#% 1043798
#% 1063711
#% 1127409
#% 1217176
#% 1217186
#% 1231247
#% 1272043
#% 1328076
#% 1426503
#! An answer to a query has a well-defined lineage expression (alternatively called how-provenance) that explains how the answer was derived. Recent work has also shown how to compute the lineage of a non-answer to a query. However, the cause of an answer or non-answer is a more subtle notion and consists, in general, of only a fragment of the lineage. In this paper, we adapt Halpern, Pearl, and Chockler's recent definitions of causality and responsibility to define the causes of answers and non-answers to queries, and their degree of responsibility. Responsibility captures the notion of degree of causality and serves to rank potentially many causes by their relative contributions to the effect. Then, we study the complexity of computing causes and responsibilities for conjunctive queries. It is known that computing causes is NP-complete in general. Our first main result shows that all causes to conjunctive queries can be computed by a relational query which may involve negation. Thus, causality can be computed in PTIME, and very efficiently so. Next, we study computing responsibility. Here, we prove that the complexity depends on the conjunctive query and demonstrate a dichotomy between PTIME and NP-complete cases. For the PTIME cases, we give a non-trivial algorithm, consisting of a reduction to the max-flow computation problem. Finally, we prove that, even when it is in PTIME, responsibility is complete for LOGSPACE, implying that, unlike causality, it cannot be computed by a relational query.

#index 1523792
#* Enabling real time data analysis
#@ Divesh Srivastava;Lukasz Golab;Rick Greer;Theodore Johnson;Joseph Seidel;Vladislav Shkapenyuk;Oliver Spatscheck;Jennifer Yates
#t 2010
#c 4
#% 273943
#% 654497
#% 1217211
#! Network-based services have become a ubiquitous part of our lives, to the point where individuals and businesses have often come to critically rely on them. Building and maintaining such reliable, high performance network and service infrastructures requires the ability to rapidly investigate and resolve complex service and performance impacting issues. To achieve this, it is important to collect, correlate and analyze massive amounts of data from a diverse collection of data sources in real time. We have designed and implemented a variety of data systems at AT&T Labs-Research to build highly scalable databases that support real time data collection, correlation and analysis, including (a) the Daytona data management system, (b) the DataDepot data warehousing system, (c) the GS tool data stream management system, and (d) the Bistro data feed manager. Together, these data systems have enabled the creation and maintenance of a data warehouse and data analysis infrastructure for troubleshooting complex issues in the network. We describe these data systems and their key research contributions in this paper.

#index 1523793
#* High-end biological imaging generates very large 3D+ and dynamic datasets
#@ Paul Matsudaira
#t 2010
#c 4
#! Biological imaging collects data over time and length scales that range from atoms and molecules to cells and tissues with a frequent goal of molecular detection in live tissues and organs. There are two generic approaches employed in microscopy to study the structure and function of molecules, cells, and organisms. Both collect large numbers of images and generate TB-size datasets but for different purposes. In high-content screening applications drugs are applied to live cells grown in wells and imaged in automated microscopes. A thorough study will examine the effects of a pharmacological agent at several dilutions by collecting images of cells at different time-points and generate TB datasets from millions of cells. After image processing and analysis, critical concentrations of drug activity are identified and mechanism of action is extracted from downstream information. The experiment is convoluted by marking various structures or molecules with a signature color, thus allowing for specific objects to be localized in space and tracked in time. The second approach has the goal of a 3D structure. Datasets consist of either serial slices or projections through the object, a cell, tissue, or organism and are acquired by light or electron microscopy. These methods are also linked in a time series to generate the dynamics of the structure from which mechanical and kinetic parameters are extracted. A significant problem is that these approaches can easily generate TBs of image data in minutes to hour periods. Thus, in biological imaging it is easy to generate data but the pressing problems are how to manage and organize image datasets, relate image datasets obtained by different microscopy methods, extract information from images, and to generate dynamic and 3D models of biological structures. These downstream steps contribute to the total dataset for the experiment.

#index 1523794
#* Dealing with web data: history and look ahead
#@ Junghoo Cho;Hector Garcia-Molina
#t 2010
#c 4
#! The high rate of change and the unprecedented scale of the Web pose enormous challenges to search engines who wish to provide the most up-to-date and highly relevant information to its users. The VLDB 2000 paper "The Evolution of the Web and Implications for an Incremental Crawler" tried to address part of this challenge by collecting and analyzing the Web history data and by describing the architecture and the associated algorithms for an incremental Web crawler that can provide more up-to-date data to users in a timely manner. Experiments and theoretical analysis showed --- surprisingly at the time --- that a policy that allocates more resources to more frequently changing items does not necessarily lead to better performance. In this paper, we discuss what has happened in the 10 years since and talk about the challenges that lie head.

#index 1523795
#* Database replication: a tale of research across communities
#@ Bettina Kemme;Gustavo Alonso
#t 2010
#c 4
#% 9241
#% 204913
#% 210179
#% 251359
#% 307360
#% 335454
#% 336201
#% 360802
#% 415975
#% 451431
#% 480310
#% 565252
#% 635834
#% 635932
#% 636006
#% 648931
#% 793894
#% 800544
#% 810043
#% 820280
#% 839564
#% 850309
#% 866984
#% 893147
#% 902010
#% 938074
#% 960202
#% 998835
#% 1063525
#% 1066740
#% 1180907
#% 1183366
#% 1200331
#% 1237219
#% 1426589
#% 1706111
#! Replication is a key mechanism to achieve scalability and fault-tolerance in databases. Its importance has recently been further increased because of the role it plays in achieving elasticity at the database layer. In database replication, the biggest challenge lies in the trade-off between performance and consistency. A decade ago, performance could only be achieved through lazy replication at the expense of transactional guarantees. The strong consistency of eager approaches came with a high cost in terms of reduced performance and limited scalability. Postgres-R combined results from distributed systems and databases to develop a replication solution that provided both scalability and strong consistency. The use of group communication primitives with strong ordering and delivery guarantees together with optimized transaction handling (tailored locking, transferring logs instead of re-executing updates, keeping the message overhead per transaction constant) were a drastic departure from the state-of-the-art at the time. Ten years later, these techniques are widely used in a variety of contexts but particularly in cloud computing scenarios. In this paper we review the original motivation for Postgres-R and discuss how the ideas behind the design have evolved over the years.

#index 1523796
#* Building disclosure risk aware query optimizers for relational databases
#@ Mustafa Canim;Murat Kantarcioglu;Bijit Hore;Sharad Mehrotra
#t 2010
#c 4
#% 284594
#% 320113
#% 393844
#% 960241
#% 960290
#% 963825
#% 1016171
#% 1092016
#% 1172777
#% 1268480
#! Many DBMS products in the market provide built in encryption support to deal with the security concerns of the organizations. This solution is quite effective in preventing data leakage from compromised/stolen storage devices. However, recent studies show that a significant part of the leaked records have been done so by using specialized malwares that can access the main memory of systems. These malwares can easily capture the sensitive information that are decrypted in the memory including the cryptographic keys used to decrypt them. This can further compromise the security of data residing on disk that are encrypted with the same keys. In this paper we quantify the disclosure risk of encrypted data in a relational DBMS for main memory-based attacks and propose modifications to the standard query processing mechanism to minimize such risks. Specifically, we propose query optimization techniques and disclosure models to design a data-sensitivity aware query optimizer. We implemented a prototype DBMS by modifying both the storage engine and optimizer of MySQL-InnoDB server. The experimental results show that the disclosure risk of such attacks can be reduced dramatically while incurring a small performance overhead in most cases.

#index 1523797
#* Secure personal data servers: a vision paper
#@ Tristan Allard;Nicolas Anciaux;Luc Bouganim;Yanli Guo;Lionel Le Folgoc;Benjamin Nguyen;Philippe Pucheral;Indrajit Ray;Indrakshi Ray;Shaoyi Yin
#t 2010
#c 4
#% 67453
#% 264249
#% 322884
#% 451767
#% 571059
#% 571094
#% 576761
#% 960238
#% 960290
#% 963801
#% 993943
#% 1100317
#% 1101436
#% 1127419
#% 1181263
#% 1328139
#% 1381029
#% 1426631
#! An increasing amount of personal data is automatically gathered and stored on servers by administrations, hospitals, insurance companies, etc. Citizen themselves often count on internet companies to store their data and make them reliable and highly available through the internet. However, these benefits must be weighed against privacy risks incurred by centralization. This paper suggests a radically different way of considering the management of personal data. It builds upon the emergence of new portable and secure devices combining the security of smart cards and the storage capacity of NAND Flash chips. By embedding a full-fledged Personal Data Server in such devices, user control of how her sensitive data is shared by others (by whom, for how long, according to which rule, for which purpose) can be fully reestablished and convincingly enforced. To give sense to this vision, Personal Data Servers must be able to interoperate with external servers and must provide traditional database services like durability, availability, query facilities, transactions. This paper proposes an initial design for the Personal Data Server approach, identifies the main technical challenges associated with it and sketches preliminary solutions. We expect that this paper will open exciting perspectives for future database research.

#index 1523798
#* PolicyReplay: misconfiguration-response queries for data breach reporting
#@ Daniel Fabbri;Kristen LeFevre;Qiang Zhu
#t 2010
#c 4
#% 248807
#% 300166
#% 300458
#% 408396
#% 442781
#% 765447
#% 800604
#% 874998
#% 875015
#% 960240
#% 993943
#% 1016138
#% 1016172
#% 1016204
#% 1047469
#% 1181274
#% 1206679
#% 1206828
#! Recent legislation has increased the requirements of organizations to report data breaches, or unauthorized access to data. While access control policies are used to restrict access to a database, these policies are complex and difficult to configure. As a result, misconfigurations sometimes allow users access to unauthorized data. In this paper, we consider the problem of reporting data breaches after such a misconfiguration is detected. To locate past SQL queries that may have revealed unauthorized information, we introduce the novel idea of a misconfiguration response (MR) query. The MR-query cleanly addresses the challenges of information propagation within the database by replaying the log of operations and returning all logged queries for which the result has changed due to the misconfiguration. A strawman implementation of the MR-query would go back in time and replay all the operations that occurred in the interim, with the correct policy. However, re-executing all operations is inefficient. Instead, we develop techniques to improve reporting efficiency by reducing the number of operations that must be re-executed and reducing the cost of replaying the operations. An extensive evaluation shows that our method can reduce the total runtime by up to an order of magnitude.

#index 1523799
#* Schism: a workload-driven approach to database replication and partitioning
#@ Carlo Curino;Evan Jones;Yang Zhang;Sam Madden
#t 2010
#c 4
#% 102745
#% 115661
#% 136350
#% 215403
#% 274612
#% 397397
#% 480442
#% 708321
#% 765431
#% 767229
#% 789956
#% 978404
#% 1022298
#% 1127560
#% 1200322
#% 1247399
#% 1269378
#% 1301004
#% 1426489
#! We present Schism, a novel workload-aware approach for database partitioning and replication designed to improve scalability of shared-nothing distributed databases. Because distributed transactions are expensive in OLTP settings (a fact we demonstrate through a series of experiments), our partitioner attempts to minimize the number of distributed transactions, while producing balanced partitions. Schism consists of two phases: i) a workload-driven, graph-based replication/partitioning phase and ii) an explanation and validation phase. The first phase creates a graph with a node per tuple (or group of tuples) and edges between nodes accessed by the same transaction, and then uses a graph partitioner to split the graph into k balanced partitions that minimize the number of cross-partition transactions. The second phase exploits machine learning techniques to find a predicate-based explanation of the partitioning strategy (i.e., a set of range predicates that represent the same replication/partitioning scheme produced by the partitioner). The strengths of Schism are: i) independence from the schema layout, ii) effectiveness on n-to-n relations, typical in social network databases, iii) a unified and fine-grained approach to replication and partitioning. We implemented and tested a prototype of Schism on a wide spectrum of test cases, ranging from classical OLTP workloads (e.g., TPC-C and TPC-E), to more complex scenarios derived from social network websites (e.g., Epinions.com), whose schema contains multiple n-to-n relationships, which are known to be hard to partition. Schism consistently outperforms simple partitioning schemes, and in some cases proves superior to the best known manual partitioning, reducing the cost of distributed transactions up to 30%.

#index 1523800
#* Ten thousand SQLs: parallel keyword queries computing
#@ Lu Qin;Jeffrey Xu Yu;Lijun Chang
#t 2010
#c 4
#% 1758
#% 114577
#% 136740
#% 245788
#% 248014
#% 300166
#% 394617
#% 659990
#% 660011
#% 824693
#% 874894
#% 875017
#% 960243
#% 960259
#% 960278
#% 960284
#% 993987
#% 1007728
#% 1015325
#% 1016176
#% 1026960
#% 1063537
#% 1063539
#% 1127353
#% 1127445
#% 1206930
#% 1206998
#% 1207007
#% 1209634
#% 1217198
#% 1328167
#% 1467763
#! Keyword search in relational databases has been extensively studied. Given a relational database, a keyword query finds a set of interconnected tuple structures connected by foreign key references. On rdbms, a keyword query is processed in two steps, namely, candidate networks (CNs) generation and CNs evaluation, where a CN is an sql. In common, a keyword query needs to be processed using over 10,000 sqls. There are several approaches to process a keyword query on rdbms, but there is a limit to achieve high performance on a uniprocessor architecture. In this paper, we study parallel computing keyword queries on a multicore architecture. We give three observations on keyword query computing, namely, a large number of sqls that needs to be processed, high sharing possibility among sqls, and large intermediate results with small number of final results. All make it challenging for parallel keyword queries computing. We investigate three approaches. We first study the query level parallelism, where each sql is processed by one core. We distribute the sqls into different cores based on three objectives, regarding minimizing workload skew, minimizing intercore sharing and maximizing intra-core sharing respectively. Such an approach has the potential risk of load unbalancing through accumulating errors of cost estimation. We then study the operation level parallelism, where each operation of an sql is processed by one core. All operations are processed in stages, where in each stage the costs of operations are re-estimated to reduce the accumulated error. Such operation level parallelism still has drawbacks of workload skew when large operations are involved and a large number of cores are used. Finally, we propose a new algorithm that partitions relations adaptively in order to minimize the extra cost of partitioning and at the same time reduce workload skew. We conducted extensive performance studies using two large real datasets, DBLP and IMDB, and we report the efficiency of our approaches in this paper.

#index 1523801
#* The case for determinism in database systems
#@ Alexander Thomson;Daniel J. Abadi
#t 2010
#c 4
#% 1723
#% 9241
#% 86476
#% 91620
#% 167264
#% 210179
#% 273894
#% 287352
#% 317987
#% 442832
#% 479978
#% 480310
#% 617455
#% 793894
#% 865304
#% 875020
#% 893146
#% 938074
#% 993515
#% 993994
#% 1022298
#% 1426552
#! Replication is a widely used method for achieving high availability in database systems. Due to the nondeterminism inherent in traditional concurrency control schemes, however, special care must be taken to ensure that replicas don't diverge. Log shipping, eager commit protocols, and lazy synchronization protocols are well-understood methods for safely replicating databases, but each comes with its own cost in availability, performance, or consistency. In this paper, we propose a distributed database system which combines a simple deadlock avoidance technique with concurrency control schemes that guarantee equivalence to a predetermined serial ordering of transactions. This effectively removes all nondeterminism from typical OLTP workloads, allowing active replication with no synchronization overhead whatsoever. Further, our system eliminates the requirement for two-phase commit for any kind of distributed transaction, even across multiple nodes within the same replica. By eschewing deadlock detection and two-phase commit, our system under many workloads outperforms traditional systems that allow nondeterministic transaction reordering.

#index 1523802
#* MapMerge: correlating independent schema mappings
#@ Bogdan Alexe;Mauricio Hernández;Lucian Popa;Wang-Chiew Tan
#t 2010
#c 4
#% 583
#% 172933
#% 213983
#% 287339
#% 378409
#% 413119
#% 572314
#% 800563
#% 809239
#% 809249
#% 810021
#% 824736
#% 824763
#% 826032
#% 850730
#% 893094
#% 993981
#% 1015302
#% 1015303
#% 1206578
#% 1217116
#% 1232194
#! One of the main steps towards integration or exchange of data is to design the mappings that describe the (often complex) relationships between the source schemas or formats and the desired target schema. In this paper, we introduce a new operator, called MapMerge, that can be used to correlate multiple, independently designed schema mappings of smaller scope into larger schema mappings. This allows a more modular construction of complex mappings from various types of smaller mappings such as schema correspondences produced by a schema matcher or pre-existing mappings that were designed by either a human user or via mapping tools. In particular, the new operator also enables a new "divide-and-merge" paradigm for mapping creation, where the design is divided (on purpose) into smaller components that are easier to create and understand, and where MapMerge is used to automatically generate a meaningful overall mapping. We describe our MapMerge algorithm and demonstrate the feasibility of our implementation on several real and synthetic mapping scenarios. In our experiments, we make use of a novel similarity measure between two database instances with different schemas that quantifies the preservation of data associations. We show experimentally that MapMerge improves the quality of the schema mappings, by significantly increasing the similarity between the input source instance and the generated target instance.

#index 1523803
#* Chase termination: a constraints rewriting approach
#@ Sergio Greco;Francesca Spezzano
#t 2010
#c 4
#% 101623
#% 289384
#% 378409
#% 384978
#% 806215
#% 826032
#% 874879
#% 879041
#% 976997
#% 1063724
#% 1217115
#% 1328190
#% 1661426
#! Several database areas such as data exchange and integration share the problem of fixing database instance violations with respect to a set of constraints. The chase algorithm solves such violations by inserting tuples and setting the value of nulls. Unfortunately, the chase algorithm may not terminate and the problem of deciding whether the chase process terminates is undecidable. Recently there has been an increasing interest in the identification of sufficient structural properties of constraints which guarantee that the chase algorithm terminates [8, 10, 14, 15]. In this paper we propose an original technique which allows to improve current conditions detecting chase termination. Our proposal consists in rewriting the original set of constraints Σ into an 'equivalent' set Σα and verifying the structural properties for chase termination on Σα. The rewriting of constraints allows to recognize larger classes of constraints for which chase termination is guaranteed. In particular, we show that if Σ satisfies chase termination conditions T, then the rewritten set Σα satisfies T as well, but the vice versa is not true, that is there are significant classes of constraints for which Σα satisfies T and Σ does not.

#index 1523804
#* Scalable data exchange with functional dependencies
#@ Bruno Marnette;Giansalvatore Mecca;Paolo Papotti
#t 2010
#c 4
#% 583
#% 384978
#% 480134
#% 749088
#% 806215
#% 826032
#% 960272
#% 993981
#% 1022258
#% 1039063
#% 1063534
#% 1063712
#% 1127589
#% 1129527
#% 1153332
#% 1181235
#% 1206578
#% 1217115
#% 1217196
#% 1304601
#% 1328084
#% 1328193
#% 1328201
#! The recent literature has provided a solid theoretical foundation for the use of schema mappings in data-exchange applications. Following this formalization, new algorithms have been developed to generate optimal solutions for mapping scenarios in a highly scalable way, by relying on SQL. However, these algorithms suffer from a serious drawback: they are not able to handle key constraints and functional dependencies on the target, i.e., equality generating dependencies (egds). While egds play a crucial role in the generation of optimal solutions, handling them with first-order languages is a difficult problem. In fact, we start from a negative result: it is not always possible to compute solutions for scenarios with egds using an SQL script. Then, we identify many practical cases in which this is possible, and develop a best-effort algorithm to do this. Experimental results show that our algorithm produces solutions of better quality with respect to those produced by previous algorithms, and scales nicely to large databases.

#index 1523805
#* Interactive route search in the presence of order constraints
#@ Yaron Kanza;Roy Levin;Eliyahu Safra;Yehoshua Sagiv
#t 2010
#c 4
#% 1066744
#% 1135142
#% 1135143
#% 1230825
#% 1265149
#% 1298905
#% 1704004
#% 1720752
#% 1720757
#! A route search is an enhancement of an ordinary geographic search. Instead of merely returning a set of entities, the result is a route that goes via entities that are relevant to the search. The input to the problem consists of several search queries, and each query defines a type of geographical entities. When visited, some of the entities succeed in satisfying the user while others fail to do so; however, only the probability of success is known prior to arrival. The main task is to find a route that visits at least one satisfying entity of each type. In an interactive search, the route is computed in steps. In each step, only the next entity of the route is given to the user, and after visiting that entity, the user provides a feedback specifying whether the entity satisfies her. This paper investigates interactive route search in the presence of order constraints that specify that some types of entities should be visited before others. We present heuristic algorithms for interactive route search for two cases, depending on whether the constraints define a complete order or a partial one. The main challenge is to utilize the feedback in order to compute a route that is shorter and has a higher degree of success, compared to routes that are computed non-interactively. We also discuss how to compare the results of the algorithms and introduce suitable measures for doing so. Experiments on real-world data illustrate the efficiency and effectiveness of our algorithms.

#index 1523806
#* Energy management for MapReduce clusters
#@ Willis Lang;Jignesh M. Patel
#t 2010
#c 4
#% 342368
#% 582212
#% 723279
#% 764396
#% 784363
#% 808595
#% 870120
#% 873244
#% 884945
#% 960264
#% 963628
#% 963669
#% 995880
#% 1034471
#% 1034473
#% 1068004
#% 1068005
#% 1174227
#% 1217159
#% 1426521
#% 1433985
#% 1468269
#% 1468291
#% 1480466
#! The area of cluster-level energy management has attracted significant research attention over the past few years. One class of techniques to reduce the energy consumption of clusters is to selectively power down nodes during periods of low utilization to increase energy efficiency. One can think of a number of ways of selectively powering down nodes, each with varying impact on the workload response time and overall energy consumption. Since the MapReduce framework is becoming "ubiquitous", the focus of this paper is on developing a framework for systematically considering various MapReduce node power down strategies, and their impact on the overall energy consumption and workload response time. We closely examine two extreme techniques that can be accommodated in this framework. The first is based on a recently proposed technique called "Covering Set" (CS) that keeps only a small fraction of the nodes powered up during periods of low utilization. At the other extreme is a technique that we propose in this paper, called the All-In Strategy (AIS). AIS uses all the nodes in the cluster to run a workload and then powers down the entire cluster. Using both actual evaluation and analytical modeling we bring out the differences between these two extreme techniques and show that AIS is often the right energy saving strategy.

#index 1523807
#* Toward scalable keyword search over relational data
#@ Akanksha Baid;Ian Rae;Jiexing Li;AnHai Doan;Jeffrey Naughton
#t 2010
#c 4
#% 397418
#% 824693
#% 864512
#% 875017
#% 875061
#% 960243
#% 960259
#% 993987
#% 994033
#% 1015325
#% 1058253
#% 1063536
#% 1063537
#% 1063539
#% 1127406
#% 1181246
#% 1206817
#% 1206911
#% 1206926
#% 1217173
#% 1217198
#% 1217199
#% 1217235
#% 1217262
#! Keyword search (KWS) over relational databases has recently received significant attention. Many solutions and many prototypes have been developed. This task requires addressing many issues, including robustness, accuracy, reliability, and privacy. An emerging issue, however, appears to be performance related: current KWS systems have unpredictable running times. In particular, for certain queries it takes too long to produce answers, and for others the system may even fail to return (e.g., after exhausting memory). In this paper we argue that as today's users have been "spoiled" by the performance of Internet search engines, KWS systems should return whatever answers they can produce quickly and then provide users with options for exploring any portion of the answer space not covered by these answers. Our basic idea is to produce answers that can be generated quickly as in today's KWS systems, then to show users query forms that characterize the unexplored portion of the answer space. Combining KWS systems with forms allows us to bypass the performance problems inherent to KWS without compromising query coverage. We provide a proof of concept for this proposed approach, and discuss the challenges encountered in building this hybrid system. Finally, we present experiments over real-world datasets to demonstrate the feasibility of the proposed solution.

#index 1523808
#* From regular expressions to nested words: unifying languages and query execution for relational and XML sequences
#@ Barzan Mozafari;Kai Zeng;Carlo Zaniolo
#t 2010
#c 4
#% 161294
#% 333850
#% 480497
#% 654477
#% 763881
#% 765274
#% 838512
#% 864437
#% 874910
#% 875004
#% 875010
#% 977007
#% 994015
#% 1024476
#% 1131014
#% 1206641
#% 1217239
#% 1266913
#% 1326584
#% 1328054
#% 1328078
#% 1426608
#% 1661435
#% 1688281
#% 1733511
#! There is growing interest in query language extensions for pattern matching over event streams and stored database sequences, due to the many important applications that such extensions make possible. The push for such extensions has led DBMS vendors and DSMS venture companies to propose Kleene-closure extensions of SQL standards, building on seminal research that demonstrated the effectiveness and amenability to efficient implementation of such constructs. These extensions, however powerful, suffer from limitations that severely impair their effectiveness in many real-world applications. To overcome these problems, we have designed the K*SQL language and system, based on our investigation of the nested words, which are recent models that generalize both words and trees. K*SQL extends the existing relational sequence languages, and also enables applications from other domains such as genomics, software analysis, and XML processing. At the same time, K*SQL remains extremely efficient, using our powerful optimizations for pattern search over nested words. Furthermore, we show that other sequence languages and XPath can be automatically translated into K*SQL, allowing for K*SQL to be also used as a high-performance query execution back-end for those languages. Therefore, K*SQL is a unifying SQL-based engine for sequence and XML queries, which provides novel optimization techniques for both.

#index 1523809
#* Avalanche-safe LINQ compilation
#@ Torsten Grust;Jan Rittinger;Tom Schreiber
#t 2010
#c 4
#% 5379
#% 80488
#% 116050
#% 290811
#% 332909
#% 431899
#% 765488
#% 875029
#% 960309
#% 960362
#% 961238
#% 994015
#% 997039
#% 1001176
#% 1001187
#% 1016150
#% 1180960
#% 1217249
#% 1328195
#% 1372694
#% 1404050
#% 1523937
#! We report on a query compilation technique that enables the construction of alternative efficient query providers for Microsoft's Language Integrated Query (LINQ) framework. LINQ programs are mapped into an intermediate algebraic form, suitable for execution on any SQL:1999-capable relational database system. This compilation technique leads to query providers that (1) faithfully preserve list order and nesting, both being core features of the LINQ data model, (2) support the complete family of LINQ's Standard Query Operators, (3) bring database support to LINQ to XML where the original provider performs in-memory query evaluation, and, most importantly, (4) emit SQL statement sequences whose size is only determined by the input query's result type (and thus independent of the database size). A sample query scenario uses this LINQ provider to marry database-resident TPC-H and XMark data---resulting in a unique query experience that exhibits quite promising performance characteristics, especially for large data instances.

#index 1523810
#* Towards certain fixes with editing rules and master data
#@ Wenfei Fan;Jianzhong Li;Shuai Ma;Nan Tang;Wenyuan Yu
#t 2010
#c 4
#% 39702
#% 242237
#% 273687
#% 341672
#% 394417
#% 408396
#% 509761
#% 654467
#% 810019
#% 833132
#% 1022222
#% 1022228
#% 1054480
#% 1063725
#% 1127381
#% 1127443
#% 1130461
#% 1180000
#% 1196287
#% 1328143
#% 1347336
#! A variety of integrity constraints have been studied for data cleaning. While these constraints can detect the presence of errors, they fall short of guiding us to correct the errors. Indeed, data repairing based on these constraints may not find certain fixes that are absolutely correct, and worse, may introduce new errors when repairing the data. We propose a method for finding certain fixes, based on master data, a notion of certain regions, and a class of editing rules. A certain region is a set of attributes that are assured correct by the users. Given a certain region and master data, editing rules tell us what attributes to fix and how to update them. We show how the method can be used in data monitoring and enrichment. We develop techniques for reasoning about editing rules, to decide whether they lead to a unique fix and whether they are able to fix all the attributes in a tuple, relative to master data and a certain region. We also provide an algorithm to identify minimal certain regions, such that a certain fix is warranted by editing rules and master data as long as one of the regions is correct. We experimentally verify the effectiveness and scalability of the algorithm.

#index 1523811
#* Explaining missing answers to SPJUA queries
#@ Melanie Herschel;Mauricio A. Hernández
#t 2010
#c 4
#% 663
#% 13016
#% 238413
#% 315853
#% 318704
#% 384978
#% 430773
#% 459273
#% 806215
#% 826032
#% 865765
#% 1127409
#% 1217186
#% 1223207
#% 1231247
#% 1328076
#! This paper addresses the problem of explaining missing answers in queries that include selection, projection, join, union, aggregation and grouping (SPJUA). Explaining missing answers of queries is useful in various scenarios, including query understanding and debugging. We present a general framework for the generation of these explanations based on source data. We describe the algorithms used to generate a correct, finite, and, when possible, minimal set of explanations. These algorithms are part of Artemis, a system that assists query developers in analyzing queries by, for instance, allowing them to ask why certain tuples are not in the query results. Experimental results demonstrate that Artemis generates explanations of missing tuples at a pace that allows developers to effectively use them for query analysis.

#index 1523812
#* Sampling the repairs of functional dependency violations under hard constraints
#@ George Beskales;Ihab F. Ilyas;Lukasz Golab
#t 2010
#c 4
#% 273687
#% 384978
#% 465052
#% 783532
#% 810019
#% 833132
#% 893167
#% 949372
#% 1063521
#% 1148408
#% 1179998
#% 1180000
#% 1347336
#% 1661438
#! Violations of functional dependencies (FDs) are common in practice, often arising in the context of data integration or Web data extraction. Resolving these violations is known to be challenging for a variety of reasons, one of them being the exponential number of possible "repairs". Previous work has tackled this problem either by producing a single repair that is (nearly) optimal with respect to some metric, or by computing consistent answers to selected classes of queries without explicitly generating the repairs. In this paper, we propose a novel data cleaning approach that is not limited to finding a single repair or to a particular class of queries, namely, sampling from the space of possible repairs. We give several motivating scenarios where sampling from the space of FD repairs is desirable, propose a new class of useful repairs, and present an algorithm that randomly samples from this space. We also show how to restrict the space of generated repairs based on user-defined hard constraints that define an immutable trusted subset of the input relation, and we experimentally evaluate our algorithm against previous approaches. While this paper focuses on repairing FDs, we envision the proposed sampling approach to be applicable to other integrity constraints with large repair spaces.

#index 1523813
#* Evaluating entity resolution results
#@ David Menestrina;Steven Euijong Whang;Hector Garcia-Molina
#t 2010
#c 4
#% 201889
#% 310516
#% 328186
#% 587758
#% 599921
#% 729913
#% 783704
#% 800590
#% 810014
#% 840907
#% 993980
#% 1077150
#% 1090229
#% 1201863
#% 1217163
#% 1328216
#% 1663664
#! Entity Resolution (ER) is the process of identifying groups of records that refer to the same real-world entity. Various measures (e.g., pairwise F1, cluster F1) have been used for evaluating ER results. However, ER measures tend to be chosen in an ad-hoc fashion without careful thought as to what defines a good result for the specific application at hand. In this paper, our contributions are twofold. First, we conduct an analysis on existing ER measures, showing that they can often conflict with each other by ranking the results of ER algorithms differently. Second, we explore a new distance measure for ER (called "generalized merge distance" or GMD) inspired by the edit distance of strings, using cluster splits and merges as its basic operations. A significant advantage of GMD is that the cost functions for splits and merges can be configured, enabling us to clearly understand the characteristics of a defined GMD measure. Surprisingly, a state-of-the-art clustering measure called Variation of Information is a special case of our configurable GMD measure, and the widely used pairwise F1 measure can be directly computed using GMD. We present an efficient linear-time algorithm that correctly computes the GMD measure for a large class of cost functions that satisfy reasonable properties.

#index 1523814
#* High-performance dynamic pattern matching over disordered streams
#@ Badrish Chandramouli;Jonathan Goldstein;David Maier
#t 2010
#c 4
#% 378388
#% 397352
#% 404772
#% 578391
#% 731408
#% 771230
#% 801694
#% 864528
#% 875004
#% 1063480
#% 1063481
#% 1207016
#% 1214642
#% 1217161
#% 1328078
#% 1328129
#% 1688281
#% 1700120
#! Current pattern-detection proposals for streaming data recognize the need to move beyond a simple regular-expression model over strictly ordered input. We continue in this direction, relaxing restrictions present in some models, removing the requirement for ordered input, and permitting stream revisions (modification of prior events). Further, recognizing that patterns of interest in modern applications may change frequently over the lifetime of a query, we support updating of a pattern specification without blocking input or restarting the operator. Our new pattern operator (called AFA) is a streaming adaptation of a non-deterministic finite automaton (NFA) where additional schema-based user-defined information, called a register, is accessible to NFA transitions during execution. AFAs support dynamic patterns, where the pattern itself can change over time. We propose clean order-agnostic pattern-detection semantics for AFAs, with new algorithms that allow a very efficient implementation, while retaining significant expressiveness and supporting native handling of out-of-order input, stream revisions, dynamic patterns, and several optimizations. Experiments on Microsoft StreamInsight show that we achieve event rates of more than 200K events/sec (up to 5x better than simpler schemes). Our dynamic patterns give up to orders-of-magnitude better throughput than solutions such as operator restart, and our other optimizations are very effective, incurring low memory and latency.

#index 1523815
#* SECRET: a model for analysis of the execution semantics of stream processing systems
#@ Irina Botan;Roozbeh Derakhshan;Nihal Dindar;Laura Haas;Renée J. Miller;Nesime Tatbul
#t 2010
#c 4
#% 726621
#% 801694
#% 810033
#% 878299
#% 1063555
#% 1127569
#% 1174745
#% 1700120
#% 1728691
#! There are many academic and commercial stream processing engines (SPEs) today, each of them with its own execution semantics. This variation may lead to seemingly inexplicable differences in query results. In this paper, we present SECRET, a model of the behavior of SPEs. SECRET is a descriptive model that allows users to analyze the behavior of systems and understand the results of window-based queries for a broad range of heterogeneous SPEs. The model is the result of extensive analysis and experimentation with several commercial and academic engines. In the paper, we describe the types of heterogeneity found in existing engines, and show with experiments on real systems that our model can explain the key differences in windowing behavior.

#index 1523816
#* Recognizing patterns in streams with imprecise timestamps
#@ Haopeng Zhang;Yanlei Diao;Neil Immerman
#t 2010
#c 4
#% 259487
#% 763881
#% 801694
#% 875004
#% 963677
#% 977010
#% 992830
#% 1043817
#% 1063480
#% 1063518
#% 1063523
#% 1112740
#% 1127357
#% 1206571
#% 1206879
#% 1207016
#% 1217161
#% 1328078
#! Large-scale event systems are becoming increasingly popular in a variety of domains. Event pattern evaluation plays a key role in monitoring applications in these domains. Existing work on pattern evaluation, however, assumes that the occurrence time of each event is known precisely and the events from various sources can be merged into a single stream with a total or partial order. We observe that in real-world applications event occurrence times are often unknown or imprecise. Therefore, we propose a temporal model that assigns a time interval to each event to represent all of its possible occurrence times and revisit pattern evaluation under this model. In particular, we propose the formal semantics of such pattern evaluation, two evaluation frameworks, and algorithms and optimizations in these frameworks. Our evaluation results using both real traces and synthetic systems show that the event-based framework always outperforms the point-based framework and with optimizations, it achieves high efficiency for a wide range of workloads tested.

#index 1523817
#* x-RDF-3X: fast querying, high update rates, and consistency for RDF databases
#@ Thomas Neumann;Gerhard Weikum
#t 2010
#c 4
#% 86953
#% 208047
#% 287070
#% 318392
#% 336201
#% 410276
#% 480096
#% 632091
#% 824755
#% 864422
#% 1022236
#% 1055732
#% 1127402
#% 1127431
#% 1127610
#% 1217194
#% 1269903
#% 1333435
#% 1366460
#% 1409918
#! The RDF data model is gaining importance for applications in computational biology, knowledge sharing, and social communities. Recent work on RDF engines has focused on scalable performance for querying, and has largely disregarded updates. In addition to incremental bulk loading, applications also require online updates with flexible control over multi-user isolation levels and data consistency. The challenge lies in meeting these requirements while retaining the capability for fast querying. This paper presents a comprehensive solution that is based on an extended deferred-indexing method with integrated versioning. The version store enables time-travel queries that are efficiently processed without adversely affecting queries on the current data. For flexible consistency, transactional concurrency control is provided with options for either snapshot isolation or full serializability. All methods are integrated in an extension of the RDF-3X system, and their very good performance for both queries and updates is demonstrated by measurements of multi-user workloads with real-life data as well as stress-test synthetic loads.

#index 1523818
#* Graph pattern matching: from intractable to polynomial time
#@ Wenfei Fan;Jianzhong Li;Shuai Ma;Nan Tang;Yinghui Wu;Yunpeng Wu
#t 2010
#c 4
#% 142231
#% 205419
#% 211658
#% 288990
#% 300176
#% 378391
#% 397375
#% 464883
#% 593696
#% 722530
#% 765442
#% 824692
#% 835906
#% 864462
#% 881509
#% 976785
#% 989645
#% 1026963
#% 1044451
#% 1063500
#% 1187181
#% 1189225
#% 1206699
#% 1206916
#% 1217208
#% 1328183
#% 1407271
#% 1523898
#! Graph pattern matching is typically defined in terms of subgraph isomorphism, which makes it an np-complete problem. Moreover, it requires bijective functions, which are often too restrictive to characterize patterns in emerging applications. We propose a class of graph patterns, in which an edge denotes the connectivity in a data graph within a predefined number of hops. In addition, we define matching based on a notion of bounded simulation, an extension of graph simulation. We show that with this revision, graph pattern matching can be performed in cubic-time, by providing such an algorithm. We also develop algorithms for incrementally finding matches when data graphs are updated, with performance guarantees for dag patterns. We experimentally verify that these algorithms scale well, and that the revised notion of graph pattern matching allows us to identify communities commonly found in real-world networks.

#index 1523819
#* GRAIL: scalable reachability index for large graphs
#@ Hilmi Yildirim;Vineet Chaoji;Mohammed J. Zaki
#t 2010
#c 4
#% 58365
#% 88051
#% 410276
#% 449322
#% 593971
#% 598374
#% 722530
#% 765272
#% 800534
#% 838518
#% 864462
#% 960304
#% 1044451
#% 1055756
#% 1058337
#% 1063514
#% 1206685
#% 1215236
#% 1217208
#% 1218741
#! Given a large directed graph, rapidly answering reachability queries between source and target nodes is an important problem. Existing methods for reachability trade-off indexing time and space versus query time performance. However, the biggest limitation of existing methods is that they simply do not scale to very large real-world graphs. We present a very simple, but scalable reachability index, called GRAIL, that is based on the idea of randomized interval labeling, and that can effectively handle very large graphs. Based on an extensive set of experiments, we show that while more sophisticated methods work better on small graphs, GRAIL is the only index that can scale to millions of nodes and edges. GRAIL has linear indexing time and space, and the query time ranges from constant time to being linear in the graph order and size.

#index 1523820
#* HaLoop: efficient iterative data processing on large clusters
#@ Yingyi Bu;Bill Howe;Magdalena Balazinska;Michael D. Ernst
#t 2010
#c 4
#% 13014
#% 115661
#% 290830
#% 442938
#% 963669
#% 983467
#% 1063553
#% 1157462
#% 1217159
#% 1328186
#% 1426513
#! The growing demand for large-scale data mining and data analysis applications has led both industry and academia to design new types of highly scalable data-intensive computing platforms. MapReduce and Dryad are two popular platforms in which the dataflow takes the form of a directed acyclic graph of operators. These platforms lack built-in support for iterative programs, which arise naturally in many applications including data mining, web ranking, graph analysis, model fitting, and so on. This paper presents HaLoop, a modified version of the Hadoop MapReduce framework that is designed to serve these applications. HaLoop not only extends MapReduce with programming support for iterative applications, it also dramatically improves their efficiency by making the task scheduler loop-aware and by adding various caching mechanisms. We evaluated HaLoop on real queries and real datasets. Compared with Hadoop, on average, HaLoop reduces query runtimes by 1.85, and shuffles only 4% of the data between mappers and reducers.

#index 1523821
#* The impact of virtual views on containment
#@ Michael Benedikt;Georg Gottlob
#t 2010
#c 4
#% 36181
#% 101922
#% 140410
#% 164364
#% 164371
#% 205851
#% 248025
#% 248037
#% 289266
#% 342829
#% 427161
#% 464717
#% 481128
#% 572311
#% 576097
#% 591778
#% 599549
#% 630963
#% 647506
#% 801697
#% 826030
#% 874884
#% 938789
#% 1426445
#! Virtual views are a mechanism that facilitates re-use and makes queries easier to express. However the use of iterative view definitions makes very simple query evaluation and analysis problems more complex. In this paper we study classical containment and equivalence problems for queries built up through simple unions of conjunctive queries and view definitions. More precisely, we determine the complexity of containment and equivalence for non-recursive Datalog. We show that the problem is much harder than its classical counterpart -- complete for co-NEXPTIME. We then show that this remains true even with restrictions on the schema and queries in place. Finally, we isolate subcases that are more tractable, ranging from NP to PSPACE.

#index 1523822
#* Updatable and evolvable transforms for virtual databases
#@ James F. Terwilliger;Lois M. L. Delcambre;David Maier;Jeremy Steinhauer;Scott Britell
#t 2010
#c 4
#% 287000
#% 488624
#% 810114
#% 824736
#% 838519
#% 874911
#% 945790
#% 960272
#% 1053873
#% 1063710
#% 1127372
#% 1127411
#% 1232194
#% 1349272
#% 1350916
#% 1914446
#! Applications typically have some local understanding of a database schema, a virtual database that may differ significantly from the actual schema of the data where it is stored. Application engineers often support a virtual database using custom-built middleware because the available solutions, including updatable views, are unable to express necessary capabilities. We propose an alternative means of mapping a virtual database to a physical database that guarantees they remain synchronized under data or schema updates against the virtual schema. One constructs a mapping by composing channel transformations (CTs) that encapsulate atomic transformations --- including complex transformations such as pivoting --- with known updatability properties. Applications, query interfaces, and any other services can behave as if the virtual database is the implemented schema. We describe how CTs translate queries, DML, and DDL, and the properties that are necessary for such translation to be correct. We describe two example CTs in detail, and evaluate an implementation of channels for completeness and performance.

#index 1523823
#* Navigating in complex mashed-up applications
#@ Daniel Deutch;Ohad Greenshpan;Tova Milo
#t 2010
#c 4
#% 114677
#% 424283
#% 449224
#% 643566
#% 810053
#% 920378
#% 955063
#% 956587
#% 960347
#% 1022204
#% 1081275
#% 1180017
#% 1190201
#% 1328099
#% 1328154
#% 1354747
#% 1431793
#! Mashups integrate a set of Web-services and data sources, often referred to as mashlets. We study in this paper a common scenario where these mashlets are components of larger Web-Applications. In this case, integration of mash-lets yields a set of inter-connected applications, referred to as Mashed-up Applications (abbr. MashAPP). While interactions between the mashlets enrich the individual applications, they also render navigation within them more intricate for the user, as actions in one application may affect others. To assist users in their navigation through MashAPPs we provide a solution based on a simple, generic model for MashAPPs and navigation flows within them. Queries over the model allow users to describe navigation flows of interest, and an effective query evaluation algorithm provides users with recommendations on how to navigate within the MashAPP. The model and algorithms serve as a basis for the COMPASS system, built on top of the Mashup Server.

#index 1523824
#* Dremel: interactive analysis of web-scale datasets
#@ Sergey Melnik;Andrey Gubarev;Jing Jing Long;Geoffrey Romer;Shiva Shivakumar;Matt Tolton;Theo Vassilakis
#t 2010
#c 4
#% 58367
#% 300153
#% 384978
#% 519953
#% 723279
#% 765488
#% 954300
#% 963669
#% 978404
#% 1063553
#% 1127559
#% 1166469
#% 1278123
#% 1278124
#% 1290542
#% 1328108
#% 1328186
#% 1426210
#% 1468421
#! Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layout, it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.

#index 1523825
#* On graph query optimization in large networks
#@ Peixiang Zhao;Jiawei Han
#t 2010
#c 4
#% 288990
#% 378391
#% 404719
#% 410276
#% 641398
#% 660011
#% 723439
#% 765429
#% 772884
#% 798044
#% 824693
#% 864425
#% 937108
#% 960259
#% 960304
#% 960305
#% 1022280
#% 1063500
#% 1063514
#% 1108856
#% 1127380
#% 1181229
#% 1217208
#% 1328183
#% 1333435
#! The dramatic proliferation of sophisticated networks has resulted in a growing need for supporting effective querying and mining methods over such large-scale graph-structured data. At the core of many advanced network operations lies a common and critical graph query primitive: how to search graph structures efficiently within a large network? Unfortunately, the graph query is hard due to the NP-complete nature of subgraph isomorphism. It becomes even challenging when the network examined is large and diverse. In this paper, we present a high performance graph indexing mechanism, SPath, to address the graph query problem on large networks. SPath leverages decomposed shortest paths around vertex neighborhood as basic indexing units, which prove to be both effective in graph search space pruning and highly scalable in index construction and deployment. Via SPath, a graph query is processed and optimized beyond the traditional vertex-at-a-time fashion to a more efficient path-at-a-time way: the query is first decomposed to a set of shortest paths, among which a subset of candidates with good selectivity is picked by a query plan optimizer; Candidate paths are further joined together to help recover the query graph to finalize the graph query processing. We evaluate SPath with the state-of-the-art GraphQL on both real and synthetic data sets. Our experimental studies demonstrate the effectiveness and scalability of SPath, which proves to be a more practical and efficient indexing method in addressing graph queries on large networks.

#index 1523826
#* Proximity rank join
#@ Davide Martinenghi;Marco Tagliasacchi
#t 2010
#c 4
#% 248804
#% 643566
#% 755898
#% 777931
#% 907703
#% 1022277
#% 1063713
#% 1075132
#% 1127377
#% 1206876
#% 1206972
#% 1217178
#% 1733680
#! We introduce the proximity rank join problem, where we are given a set of relations whose tuples are equipped with a score and a real-valued feature vector. Given a target feature vector, the goal is to return the K combinations of tuples with high scores that are as close as possible to the target and to each other, according to some notion of distance. The setting closely resembles that of traditional rank join, but the geometry of the vector space plays a distinctive role in the computation of the overall score of a combination. Also, the input relations typically return their results either by distance from the target or by score. Because of these aspects, it turns out that traditional rank join algorithms, such as the well-known HRJN, have shortcomings in solving the proximity rank join problem, as they may read more input than needed. To overcome this weakness, we define a tight bound (used as a stopping criterion) that guarantees instance optimality, i.e., an I/O cost is achieved that is always within a constant factor of optimal. The tight bound can also be used to drive an adaptive pulling strategy, deciding at each step which relation to access next. For practically relevant classes of problems, we show how to compute the tight bound efficiently. An extensive experimental study validates our results and demonstrates significant gains over existing solutions.

#index 1523827
#* Identifying the most influential data objects with reverse top-k queries
#@ Akrivi Vlachou;Christos Doulkeridis;Kjetil Nørvåg;Yannis Kotidis
#t 2010
#c 4
#% 300163
#% 465167
#% 479816
#% 806212
#% 824730
#% 875025
#% 941785
#% 1022226
#% 1206698
#% 1328118
#% 1328184
#% 1328203
#% 1372687
#! Top-k queries are widely applied for retrieving a ranked set of the k most interesting objects based on the individual user preferences. As an example, in online marketplaces, customers (users) typically seek a ranked set of products (objects) that satisfy their needs. Reversing top-k queries leads to a query type that instead returns the set of customers that find a product appealing (it belongs to the top-k result set of their preferences). In this paper, we address the challenging problem of processing queries that identify the top-m most influential products to customers, where influence is defined as the cardinality of the reverse top-k result set. This definition of influence is useful for market analysis, since it is directly related to the number of customers that value a particular product and, consequently, to its visibility and impact in the market. Existing techniques require processing a reverse top-k query for each object in the database, which is prohibitively expensive even for databases of moderate size. In contrast, we propose two algorithms, SB and BB, for identifying the most influential objects: SB restricts the candidate set of objects that need to be examined, while BB is a branch-and-bound algorithm that retrieves the result incrementally. Furthermore, we propose meaningful variations of the query for most influential objects that are supported by our algorithms. Our experiments demonstrate the efficiency of our algorithms both for synthetic and real-life datasets.

#index 1523828
#* Retrieving top-k prestige-based relevant spatial web objects
#@ Xin Cao;Gao Cong;Christian S. Jensen
#t 2010
#c 4
#% 86950
#% 268079
#% 287466
#% 411762
#% 577329
#% 643566
#% 818241
#% 836096
#% 838407
#% 867054
#% 874993
#% 956551
#% 1016176
#% 1055877
#% 1074109
#% 1130835
#% 1206801
#% 1206997
#% 1328137
#! The location-aware keyword query returns ranked objects that are near a query location and that have textual descriptions that match query keywords. This query occurs inherently in many types of mobile and traditional web services and applications, e.g., Yellow Pages and Maps services. Previous work considers the potential results of such a query as being independent when ranking them. However, a relevant result object with nearby objects that are also relevant to the query is likely to be preferable over a relevant object without relevant nearby objects. The paper proposes the concept of prestige-based relevance to capture both the textual relevance of an object to a query and the effects of nearby objects. Based on this, a new type of query, the Location-aware top-k Prestige-based Text retrieval (LkPT) query, is proposed that retrieves the top-k spatial web objects ranked according to both prestige-based relevance and location proximity. We propose two algorithms that compute LkPT queries. Empirical studies with real-world spatial data demonstrate that LkPT queries are more effective in retrieving web objects than a previous approach that does not consider the effects of nearby objects; and they show that the proposed algorithms are scalable and outperform a baseline approach significantly.

#index 1523829
#* Parsimonious linear fingerprinting for time series
#@ Lei Li;B. Aditya Prakash;Christos Faloutsos
#t 2010
#c 4
#% 80995
#% 172949
#% 224113
#% 227857
#% 248027
#% 273706
#% 334059
#% 394984
#% 466507
#% 480628
#% 765402
#% 765452
#% 810030
#% 824705
#% 824709
#% 878305
#% 893092
#% 930938
#% 986139
#% 993965
#% 1016178
#% 1016194
#% 1022326
#% 1214672
#% 1328117
#! We study the problem of mining and summarizing multiple time series effectively and efficiently. We propose PLiF, a novel method to discover essential characteristics ("fingerprints"), by exploiting the joint dynamics in numerical sequences. Our fingerprinting method has the following benefits: (a) it leads to interpretable features; (b) it is versatile: PLiF enables numerous mining tasks, including clustering, compression, visualization, forecasting, and segmentation, matching top competitors in each task; and (c) it is fast and scalable, with linear complexity on the length of the sequences. We did experiments on both synthetic and real datasets, including human motion capture data (17MB of human motions), sensor data (166 sensors), and network router traffic data (18 million raw updates over 2 years). Despite its generality, PLiF outperforms the top clustering methods on clustering; the top compression methods on compression (3 times better reconstruction error, for the same compression ratio); it gives meaningful visualization and at the same time, enjoys a linear scale-up.

#index 1523830
#* The HV-tree: a memory hierarchy aware version index
#@ Rui Zhang;Martin Stradling
#t 2010
#c 4
#% 8826
#% 10392
#% 58371
#% 86953
#% 100621
#% 275367
#% 287020
#% 300194
#% 312199
#% 320719
#% 427199
#% 479819
#% 480817
#% 566122
#% 571296
#% 580978
#% 1127420
#% 1348386
#! The huge amount of temporal data generated from many important applications call for a highly efficient and scalable version index. The TSB-tree has the potential of large scalability due to its unique feature of progressive migration of data to larger mediums. However, its traditional design optimized for two levels of the memory hierarchy (the main memory and the hard disk) undermines its potential for high efficiency in face of today's advances in hardware, especially CPU/cache speed and memory size. We propose a novel version index structure called the HV-tree. Different from all previous version index structures, the HV-tree has nodes of different sizes, each optimized for a level of the memory hierarchy. As data migrates to different levels of the memory hierarchy, the HV-tree will adjust the node size automatically to exploit the best performance of all levels of the memory hierarchy. Moreover, the HV-tree has a unique chain mechanism to maximally keep recent data in higher levels of the memory hierarchy. As a result, HV-tree is several times faster than the TSB-tree for point queries (query with single key and single time value), and up to 1000 times faster than the TSB-tree for key-range and time-range queries.

#index 1523831
#* Transforming range queries to equivalent box queries to optimize page access
#@ Sakti Pramanik;Alok Watve;Chad R. Meiners;Alex Liu
#t 2010
#c 4
#% 86950
#% 190581
#% 227939
#% 248798
#% 340309
#% 399763
#% 411694
#% 427199
#% 442868
#% 464195
#% 479462
#% 479649
#% 481956
#% 731409
#% 875014
#% 1075132
#! Range queries based on L1 distance are a common type of queries in multimedia databases containing feature vectors. We propose a novel approach that transforms the feature space into a new feature space such that range queries in the original space are mapped into equivalent box queries in the transformed space. Since box queries are axes aligned, there are several implementational advantages that can be exploited to speed up the retrieval of query results. For two dimensional data the transformation is precise. For greater than two dimensions we propose a space transformation scheme based on disjoint planer rotation, and along with pruning query box the results are precise. Experimental results with large synthetic databases and some real databases show the effectiveness of the proposed transformation scheme. These experimental results have been corroborated with appropriate mathematical models.

#index 1523832
#* Record linkage with uniqueness constraints and erroneous values
#@ Songtao Guo;Xin Luna Dong;Divesh Srivastava;Remi Zajac
#t 2010
#c 4
#% 16103
#% 32926
#% 282435
#% 464890
#% 572314
#% 729913
#% 765548
#% 770782
#% 810014
#% 810019
#% 875066
#% 913783
#% 937552
#% 960270
#% 1016266
#% 1053052
#% 1063725
#% 1206990
#% 1328103
#% 1328143
#% 1328155
#% 1684627
#! Many data-management applications require integrating data from a variety of sources, where different sources may refer to the same real-world entity in different ways and some may even provide erroneous data. An important task in this process is to recognize and merge the various references that refer to the same entity. In practice, some attributes satisfy a uniqueness constraint---each real-world entity (or most entities) has a unique value for the attribute (e.g., business contact phone, address, and email). Traditional techniques tackle this case by first linking records that are likely to refer to the same real-world entity, and then fusing the linked records and resolving conflicts if any. Such methods can fall short for three reasons: first, erroneous values from sources may prevent correct linking; second, the real world may contain exceptions to the uniqueness constraints and always enforcing uniqueness can miss correct values; third, locally resolving conflicts for linked records may overlook important global evidence. This paper proposes a novel technique to solve this problem. The key component of our solution is to reduce the problem into a k-partite graph clustering problem and consider in clustering both similarity of attribute values and the sources that associate a pair of values in the same record. Thus, we perform global linkage and fusion simultaneously, and can identify incorrect values and differentiate them from alternative representations of the correct value from the beginning. In addition, we extend our algorithm to be tolerant to a few violations of the uniqueness constraints. Experimental results show accuracy and scalability of our technique.

#index 1523833
#* On-the-fly entity-aware query processing in the presence of linkage
#@ Ekaterini Ioannou;Wolfgang Nejdl;Claudia Niederée;Yannis Velegrakis
#t 2010
#c 4
#% 278397
#% 420072
#% 577238
#% 644182
#% 655349
#% 766199
#% 810014
#% 830529
#% 853532
#% 864417
#% 871766
#% 874876
#% 875066
#% 893189
#% 913783
#% 960237
#% 976984
#% 992830
#% 993980
#% 993981
#% 1022259
#% 1022285
#% 1103296
#% 1111110
#% 1217114
#% 1217163
#% 1333840
#% 1400073
#! Entity linkage is central to almost every data integration and data cleaning scenario. Traditional techniques use some computed similarity among data structure to perform merges and then answer queries on the merged data. We describe a novel framework for entity linkage with uncertainty. Instead of using the linkage information to merge structures a-priori, possible linkages are stored alongside the data with their belief value. A new probabilistic query answering technique is used to take the probabilistic linkage into consideration. The framework introduces a series of novelties: (i) it performs merges at run time based not only on existing linkages but also on the given query; (ii) it allows results that may contain structures not explicitly represented in the data, but generated as a result of a reasoning on the linkages; and (iii) enables an evaluation of the query conditions that spans across linked structures, offering a functionality not currently supported by any traditional probabilistic databases. We formally define the semantics, describe an efficient implementation and report on the findings of our experimental evaluation.

#index 1523834
#* Behavior based record linkage
#@ Mohamed Yakout;Ahmed K. Elmagarmid;Hazem Elmeleegy;Mourad Ouzzani;Alan Qi
#t 2010
#c 4
#% 59697
#% 91027
#% 253755
#% 307109
#% 310516
#% 420072
#% 766199
#% 810014
#% 823348
#% 875066
#% 879567
#% 913783
#% 937552
#% 960270
#% 1010488
#! In this paper, we present a new record linkage approach that uses entity behavior to decide if potentially different entities are in fact the same. An entity's behavior is extracted from a transaction log that records the actions of this entity with respect to a given data source. The core of our approach is a technique that merges the behavior of two possible matched entities and computes the gain in recognizing behavior patterns as their matching score. The idea is that if we obtain a well recognized behavior after merge, then most likely, the original two behaviors belong to the same entity as the behavior becomes more complete after the merge. We present the necessary algorithms to model entities' behavior and compute a matching score for them. To improve the computational efficiency of our approach, we precede the actual matching phase with a fast candidate generation that uses a "quick and dirty" matching method. Extensive experiments on real data show that our approach can significantly enhance record linkage quality while being practical for large transaction logs.

#index 1523835
#* iGraph: a framework for comparisons of disk-based graph indexing techniques
#@ Wook-Shin Han;Jinsoo Lee;Minh-Duc Pham;Jeffrey Xu Yu
#t 2010
#c 4
#% 288990
#% 378391
#% 393844
#% 443133
#% 480821
#% 629708
#% 727845
#% 765429
#% 772884
#% 810072
#% 823217
#% 850729
#% 864425
#% 960305
#% 1022280
#% 1044450
#% 1127380
#% 1206703
#% 1673586
#% 1835568
#! Graphs are of growing importance in modeling complex structures such as chemical compounds, proteins, images, and program dependence. Given a query graph Q, the subgraph isomorphism problem is to find a set of graphs containing Q from a graph database, which is NP-complete. Recently, there have been a lot of research efforts to solve the subgraph isomorphism problem for a large graph database by utilizing graph indexes. By using a graph index as a filter, we prune graphs that are not real answers at an inexpensive cost. Then, we need to use expensive subgraph isomorphism tests to verify filtered candidates only. This way, the number of disk I/Os and subgraph isomorphism tests can be significantly minimized. The current practice for experiments in graph indexing techniques is that the author of a newly proposed technique does not implement existing indexes on his own code base, but instead uses the original authors' binary executables and reports only the wall clock time. However, we observe this practice may result in several problems. In order to address these problems, we have made significant efforts in implementing all representative indexing methods on a common framework called iGraph. Unlike existing implementations which either use (full or partial) in-memory representations or rely on OS file system cache without guaranteeing real disk I/Os, we have implemented these indexes on top of a storage engine that guarantees real disk I/Os. Through extensive experiments using many synthetic and real datasets, we also provide new empirical findings in the performance of the full disk-based implementations of these methods.

#index 1523836
#* Runtime measurements in the cloud: observing, analyzing, and reducing variance
#@ Jörg Schad;Jens Dittrich;Jorge-Arnulfo Quiané-Ruiz
#t 2010
#c 4
#% 963669
#% 1153144
#% 1194111
#% 1217159
#% 1221120
#% 1221121
#% 1247791
#% 1298793
#% 1426550
#% 1528466
#! One of the main reasons why cloud computing has gained so much popularity is due to its ease of use and its ability to scale computing resources on demand. As a result, users can now rent computing nodes on large commercial clusters through several vendors, such as Amazon and rackspace. However, despite the attention paid by Cloud providers, performance unpredictability is a major issue in Cloud computing for (1) database researchers performing wall clock experiments, and (2) database applications providing service-level agreements. In this paper, we carry out a study of the performance variance of the most widely used Cloud infrastructure (Amazon EC2) from different perspectives. We use established microbenchmarks to measure performance variance in CPU, I/O, and network. And, we use a multi-node MapReduce application to quantify the impact on real dataintensive applications. We collected data for an entire month and compare it with the results obtained on a local cluster. Our results show that EC2 performance varies a lot and often falls into two bands having a large performance gap in-between --- which is somewhat surprising. We observe in our experiments that these two bands correspond to the different virtual system types provided by Amazon. Moreover, we analyze results considering different availability zones, points in time, and locations. This analysis indicates that, among others, the choice of availability zone also influences the performance variability. A major conclusion of our work is that the variance on EC2 is currently so high that wall clock experiments may only be performed with considerable care. To this end, we provide some hints to users.

#index 1523837
#* The performance of MapReduce: an in-depth study
#@ Dawei Jiang;Beng Chin Ooi;Lei Shi;Sai Wu
#t 2010
#c 4
#% 893129
#% 960326
#% 963669
#% 1063553
#% 1127354
#% 1127390
#% 1127559
#% 1217159
#% 1278123
#% 1278124
#% 1328095
#% 1328186
#% 1426488
#% 1468423
#! MapReduce has been widely used for large-scale data analysis in the Cloud. The system is well recognized for its elastic scalability and fine-grained fault tolerance although its performance has been noted to be suboptimal in the database context. According to a recent study [19], Hadoop, an open source implementation of MapReduce, is slower than two state-of-the-art parallel database systems in performing a variety of analytical tasks by a factor of 3.1 to 6.5. MapReduce can achieve better performance with the allocation of more compute nodes from the cloud to speed up computation; however, this approach of "renting more nodes" is not cost effective in a pay-as-you-go environment. Users desire an economical elastically scalable data processing system, and therefore, are interested in whether MapReduce can offer both elastic scalability and efficiency. In this paper, we conduct a performance study of MapReduce (Hadoop) on a 100-node cluster of Amazon EC2 with various levels of parallelism. We identify five design factors that affect the performance of Hadoop, and investigate alternative but known methods for each factor. We show that by carefully tuning these factors, the overall performance of Hadoop can be improved by a factor of 2.5 to 3.5 for the same benchmark used in [19], and is thus more comparable to that of parallel database systems. Our results show that it is therefore possible to build a cloud data processing system that is both elastically scalable and efficient.

#index 1523838
#* Evaluation of entity resolution approaches on real-world match problems
#@ Hanna Köpcke;Andreas Thor;Erhard Rahm
#t 2010
#c 4
#% 201889
#% 310516
#% 659991
#% 729913
#% 810014
#% 810044
#% 838435
#% 874457
#% 875066
#% 903332
#% 993980
#% 1022229
#% 1055684
#% 1070283
#% 1071633
#% 1250576
#% 1314445
#% 1328216
#% 1459821
#% 1673578
#% 1693724
#! Despite the huge amount of recent research efforts on entity resolution (matching) there has not yet been a comparative evaluation on the relative effectiveness and efficiency of alternate approaches. We therefore present such an evaluation of existing implementations on challenging real-world match tasks. We consider approaches both with and without using machine learning to find suitable parameterization and combination of similarity functions. In addition to approaches from the research community we also consider a state-of-the-art commercial entity resolution implementation. Our results indicate significant quality and efficiency differences between different approaches. We also find that some challenging resolution tasks such as matching product entities from online shops are not sufficiently solved with conventional approaches based on the similarity of attribute values.

#index 1523839
#* MRShare: sharing across multiple queries in MapReduce
#@ Tomasz Nykiel;Michalis Potamias;Chaitanya Mishra;George Kollios;Nick Koudas
#t 2010
#c 4
#% 36117
#% 287461
#% 411750
#% 810039
#% 954300
#% 960278
#% 960326
#% 963669
#% 1022262
#% 1063553
#% 1085307
#% 1127399
#% 1127427
#% 1127559
#% 1217159
#% 1328059
#% 1328060
#% 1328066
#% 1328095
#% 1328132
#% 1328186
#% 1468421
#! Large-scale data analysis lies in the core of modern enterprises and scientific research. With the emergence of cloud computing, the use of an analytical query processing infrastructure (e.g., Amazon EC2) can be directly mapped to monetary value. MapReduce has been a popular framework in the context of cloud computing, designed to serve long running queries (jobs) which can be processed in batch mode. Taking into account that different jobs often perform similar work, there are many opportunities for sharing. In principle, sharing similar work reduces the overall amount of work, which can lead to reducing monetary charges incurred while utilizing the processing infrastructure. In this paper we propose a sharing framework tailored to MapReduce. Our framework, MRShare, transforms a batch of queries into a new batch that will be executed more efficiently, by merging jobs into groups and evaluating each group as a single query. Based on our cost model for MapReduce, we define an optimization problem and we provide a solution that derives the optimal grouping of queries. Experiments in our prototype, built on top of Hadoop, demonstrate the overall effectiveness of our approach and substantial savings.

#index 1523840
#* Towards elastic transactional cloud storage with range query support
#@ Hoang Tam Vo;Chun Chen;Beng Chin Ooi
#t 2010
#c 4
#% 108515
#% 210179
#% 273901
#% 300164
#% 397295
#% 824706
#% 998845
#% 1016166
#% 1022726
#% 1044435
#% 1063488
#% 1063524
#% 1076985
#% 1127398
#% 1127560
#% 1328130
#% 1328131
#% 1426492
#% 1468392
#! Cloud storage is an emerging infrastructure that offers Platforms as a Service (PaaS). On such platforms, storage and compute power are adjusted dynamically, and therefore it is important to build a highly scalable and reliable storage that can elastically scale on-demand with minimal startup cost. In this paper, we propose ecStore -- an elastic cloud storage system that supports automated data partitioning and replication, load balancing, efficient range query, and transactional access. In ecStore, data objects are distributed and replicated in a cluster of commodity computer nodes located in the cloud. Users can access data via transactions which bundle read and write operations on multiple data items stored on possibly different cluster nodes. The architecture of ecStore follows a stratum design that leverages an underlying distributed index with a replication layer in the middle and a transaction management layer on top. ecStore provides adaptive read consistency on replicated data. We also enhance the system with an effective load balancing scheme using a self-tuning replication technique that is specially designed for large-scale data. Furthermore, a multi-version optimistic concurrency control scheme matches well with the characteristics of data in cloud storages. To validate the performance of the system, we have conducted extensive experiments on various platforms including a commercial cloud (Amazon's EC2), an in-house cluster, and PlanetLab.

#index 1523841
#* Hadoop++: making a yellow elephant run like a cheetah (without it even noticing)
#@ Jens Dittrich;Jorge-Arnulfo Quiané-Ruiz;Alekh Jindal;Yagiz Kargin;Vinay Setty;Jörg Schad
#t 2010
#c 4
#% 287222
#% 479819
#% 581803
#% 960326
#% 963669
#% 983467
#% 1063553
#% 1127559
#% 1217159
#% 1278123
#% 1278124
#% 1328060
#% 1328066
#% 1328095
#% 1328186
#% 1372690
#% 1468411
#% 1471595
#% 1523836
#! MapReduce is a computing paradigm that has gained a lot of attention in recent years from industry and research. Unlike parallel DBMSs, MapReduce allows non-expert users to run complex analytical tasks over very large data sets on very large clusters and clouds. However, this comes at a price: MapReduce processes tasks in a scan-oriented fashion. Hence, the performance of Hadoop --- an open-source implementation of MapReduce --- often does not match the one of a well-configured parallel DBMS. In this paper we propose a new type of system named Hadoop++: it boosts task performance without changing the Hadoop framework at all (Hadoop does not even 'notice it'). To reach this goal, rather than changing a working system (Hadoop), we inject our technology at the right places through UDFs only and affect Hadoop from inside. This has three important consequences: First, Hadoop++ significantly outperforms Hadoop. Second, any future changes of Hadoop may directly be used with Hadoop++ without rewriting any glue code. Third, Hadoop++ does not need to change the Hadoop interface. Our experiments show the superiority of Hadoop++ over both Hadoop and HadoopDB for tasks related to indexing and join processing.

#index 1523842
#* Slicing long-running queries
#@ Nicolas Bruno;Vivek Narasayya;Ravi Ramamurthy
#t 2010
#c 4
#% 115661
#% 116040
#% 411554
#% 465167
#% 481597
#% 481608
#% 632048
#% 960280
#% 1022263
#% 1022294
#% 1180004
#% 1181224
#% 1217217
#! The ability to decompose a complex, long-running query into simpler queries that produce the same result is useful for many scenarios, such as admission control, resource management, fault tolerance, and load balancing. In this paper we propose query slicing as a novel mechanism to do such decomposition. We study different ways to extend a traditional query optimizer to enable query slicing and experimentally evaluate the benefits of each approach.

#index 1523843
#* Sharing-aware horizontal partitioning for exploiting correlations during query processing
#@ Kostas Tzoumas;Amol Deshpande;Christian S. Jensen
#t 2010
#c 4
#% 115661
#% 300167
#% 333946
#% 333986
#% 464700
#% 479938
#% 715289
#% 742047
#% 800505
#% 824714
#% 838534
#% 1016208
#% 1026989
#% 1181281
#% 1328071
#% 1417383
#! Optimization of join queries based on average selectivities is suboptimal in highly correlated databases. In such databases, relations are naturally divided into partitions, each partition having substantially different statistical characteristics. It is very compelling to discover such data partitions during query optimization and create multiple plans for a given query, one plan being optimal for a particular combination of data partitions. This scenario calls for the sharing of state among plans, so that common intermediate results are not recomputed. We study this problem in a setting with a routing-based query execution engine based on eddies [1]. Eddies naturally encapsulate horizontal partitioning and maximal state sharing across multiple plans. We define the notion of a conditional join plan, a novel representation of the search space that enables us to address the problem in a principled way. We present a low-overhead greedy algorithm that uses statistical summaries based on graphical models. Experimental results suggest an order of magnitude faster execution time over traditional optimization for high correlations, while maintaining the same performance for low correlations.

#index 1523844
#* Advanced processing for ontological queries
#@ Andrea Calì;Georg Gottlob;Andreas Pieris
#t 2010
#c 4
#% 191611
#% 265104
#% 287339
#% 342829
#% 384978
#% 465053
#% 490909
#% 576116
#% 599549
#% 736407
#% 826032
#% 874914
#% 992962
#% 1063724
#% 1217122
#% 1269632
#% 1305397
#% 1333830
#% 1416180
#% 1500877
#% 1511857
#! Ontology-based data access is a powerful form of extending database technology, where a classical extensional database (EDB) is enhanced by an ontology that generates new intensional knowledge which may contribute to answer a query. The ontological integrity constraints for generating this intensional knowledge can be specified in description logics such as DL-Lite. It was recently shown that these formalisms allow for very efficient query-answering. They are, however, too weak to express simple and useful integrity constraints that involve joins. In this paper we introduce a more expressive formalism that takes joins into account, while still enjoying the same low query-answering complexity. In our framework, ontological constraints are expressed by sets of rules that are so-called tuple-generating dependencies (TGDs). We propose the language of sticky sets of TGDs, which are sets of TGDs with a restriction on multiple occurrences of variables (including joins) in the rule bodies. We establish complexity results for answering conjunctive queries under sticky sets of TGDs, showing, in particular, that ontological conjunctive queries can be compiled into first-order and thus SQL queries over the given EDB instance. We also show how sticky sets of TGDs can be combined with functional dependencies. In summary, we obtain a highly expressive and effective ontological modeling language that unifies and generalizes both classical database constraints and important features of the most widespread tractable description logics.

#index 1523845
#* Towards the web of concepts: extracting concepts from large datasets
#@ Aditya Parameswaran;Hector Garcia-Molina;Anand Rajaraman
#t 2010
#c 4
#% 279755
#% 280849
#% 320937
#% 342667
#% 463903
#% 481290
#% 504443
#% 534291
#% 543588
#% 565289
#% 641956
#% 729418
#% 742368
#% 748700
#% 756964
#% 869501
#% 878624
#% 1035588
#% 1047347
#% 1217114
#% 1705326
#! Concepts are sequences of words that represent real or imaginary entities or ideas that users are interested in. As a first step towards building a web of concepts that will form the backbone of the next generation of search technology, we develop a novel technique to extract concepts from large datasets. We approach the problem of concept extraction from corpora as a market-basket problem, adapting statistical measures of support and confidence. We evaluate our concept extraction algorithm on datasets containing data from a large number of users (e.g., the AOL query log data set), and we show that a high-precision concept set can be extracted.

#index 1523846
#* Exploiting content redundancy for web information extraction
#@ Pankaj Gulhane;Rajeev Rastogi;Srinivasan H. Sengamedu;Ashwin Tengli
#t 2010
#c 4
#% 235411
#% 248801
#% 301241
#% 333943
#% 431536
#% 480824
#% 504443
#% 577238
#% 577309
#% 654467
#% 729913
#% 769877
#% 805846
#% 807298
#% 850430
#% 864392
#% 875066
#% 881505
#% 913783
#% 1077150
#% 1190073
#% 1190153
#! We propose a novel extraction approach that exploits content redundancy on the web to extract structured data from template-based web sites. We start by populating a seed database with records extracted from a few initial sites. We then identify values within the pages of each new site that match attribute values contained in the seed set of records. To match attribute values with diverse representations across sites, we define a new similarity metric that leverages the templatized structure of attribute content. Specifically, our metric discovers the matching pattern between attribute values from two sites, and uses this to ignore extraneous portions of attribute values when computing similarity scores. Further, to filter out noisy attribute value matches, we exploit the fact that attribute values occur at fixed positions within template-based sites. We develop an efficient Apriori-style algorithm to systematically enumerate attribute position configurations with sufficient matching values across pages. Finally, we conduct an extensive experimental study with real-life web data to demonstrate the effectiveness of our extraction approach.

#index 1523847
#* Automatic rule refinement for information extraction
#@ Bin Liu;Laura Chiticariu;Vivian Chu;H. V. Jagadish;Frederick R. Reiss
#t 2010
#c 4
#% 301241
#% 338728
#% 464434
#% 465919
#% 466231
#% 677332
#% 814976
#% 817933
#% 855020
#% 855108
#% 864474
#% 939383
#% 976987
#% 1022288
#% 1063547
#% 1127409
#% 1183368
#% 1206687
#% 1206861
#% 1206862
#% 1217186
#% 1231247
#% 1234486
#% 1264720
#% 1265135
#% 1426568
#% 1471192
#% 1523811
#! Rule-based information extraction from text is increasingly being used to populate databases and to support structured queries on unstructured text. Specification of suitable information extraction rules requires considerable skill and standard practice is to refine rules iteratively, with substantial effort. In this paper, we show that techniques developed in the context of data provenance, to determine the lineage of a tuple in a database, can be leveraged to assist in rule refinement. Specifically, given a set of extraction rules and correct and incorrect extracted data, we have developed a technique to suggest a ranked list of rule modifications that an expert rule specifier can consider. We implemented our technique in the SystemT information extraction system developed at IBM Research -- Almaden and experimentally demonstrate its effectiveness.

#index 1523848
#* Embellishing text search queries to protect user privacy
#@ HweeHwa Pang;Xuhua Ding;Xiaokui Xiao
#t 2010
#c 4
#% 144029
#% 198058
#% 218978
#% 306504
#% 319353
#% 387427
#% 443396
#% 569795
#% 593800
#% 664705
#% 864882
#% 867054
#% 938705
#% 963801
#% 1127261
#% 1127362
#% 1310070
#% 1386180
#% 1532593
#% 1707129
#! Users of text search engines are increasingly wary that their activities may disclose confidential information about their business or personal profiles. It would be desirable for a search engine to perform document retrieval for users while protecting their intent. In this paper, we identify the privacy risks arising from semantically related search terms within a query, and from recurring high-specificity query terms in a search session. To counter the risks, we propose a solution for a similarity text retrieval system to offer anonymity and plausible deniability for the query terms, and hence the user intent, without degrading the system's precision-recall performance. The solution comprises a mechanism that embellishes each user query with decoy terms that exhibit similar specificity spread as the genuine terms, but point to plausible alternative topics. We also provide an accompanying retrieval scheme that enables the search engine to compute the encrypted document relevance scores from only the genuine search terms, yet remain oblivious to their distinction from the decoys. Empirical evaluation results are presented to substantiate the effectiveness of our solution.

#index 1523849
#* Small domain randomization: same privacy, more utility
#@ Rhonda Chaytor;Ke Wang
#t 2010
#c 4
#% 300184
#% 443463
#% 576111
#% 576762
#% 602040
#% 729962
#% 800513
#% 810028
#% 864412
#% 893100
#% 952890
#% 1022246
#% 1022265
#% 1022266
#% 1206574
#% 1206581
#% 1206582
#% 1206814
#% 1217148
#% 1328177
#! Random perturbation is a promising technique for privacy preserving data mining. It retains an original sensitive value with a certain probability and replaces it with a random value from the domain with the remaining probability. If the replacing value is chosen from a large domain, the retention probability must be small to protect privacy. For this reason, previous randomization-based approaches have poor utility. In this paper, we propose an alternative way to randomize sensitive values, called small domain randomization. First, we partition the given table into sub-tables that have smaller domains of sensitive values. Then, we randomize the sensitive values within each sub-table independently. Since each sub-table has a smaller domain, a larger retention probability is permitted. We propose this approach as an alternative to classical partition-based approaches to privacy preserving data publishing. There are two key issues: ensure the published sub-tables do not disclose more private information than what is permitted on the original table, and partition the table so that utility is maximized. We present an effective solution.

#index 1523850
#* Nearest neighbor search with strong location privacy
#@ Stavros Papadopoulos;Spiridon Bakiras;Dimitris Papadias
#t 2010
#c 4
#% 443397
#% 460810
#% 593711
#% 593800
#% 765448
#% 810061
#% 812799
#% 843877
#% 893151
#% 956531
#% 1013611
#% 1063478
#% 1206712
#% 1217157
#% 1409349
#% 1675991
#% 1719090
#% 1721054
#% 1726236
#% 1729021
#! The tremendous growth of the Internet has significantly reduced the cost of obtaining and sharing information about individuals, raising many concerns about user privacy. Spatial queries pose an additional threat to privacy because the location of a query may be sufficient to reveal sensitive information about the querier. In this paper we focus on k nearest neighbor (kNN) queries and define the notion of strong location privacy, which renders a query indistinguishable from any location in the data space. We argue that previous work fails to support this property for arbitrary kNN search. Towards this end, we introduce methods that offer strong location privacy, by integrating private information retrieval (PIR) functionality. Specifically, we employ secure hardware-aided PIR, which has been proven very efficient and is currently considered as a practical mechanism for PIR. Initially, we devise a benchmark solution building upon an existing PIR-based technique. Subsequently, we identify its drawbacks and present a novel scheme called AHG to tackle them. Finally, we demonstrate the performance superiority of AHG over our competitor, and its viability in applications demanding the highest level of privacy.

#index 1523851
#* UPI: a primary index for uncertain databases
#@ Hideaki Kimura;Samuel Madden;Stanley B. Zdonik
#t 2010
#c 4
#% 208047
#% 824728
#% 893167
#% 992831
#% 996377
#% 1016202
#% 1072436
#% 1200291
#% 1217128
#% 1328067
#% 1328211
#% 1338734
#% 1464054
#! Uncertain data management has received growing attention from industry and academia. Many efforts have been made to optimize uncertain databases, including the development of special index data structures. However, none of these efforts have explored primary (clustered) indexes for uncertain databases, despite the fact that clustering has the potential to offer substantial speedups for non-selective analytic queries on large uncertain databases. In this paper, we propose a new index called a UPI (Uncertain Primary Index) that clusters heap files according to uncertain attributes with both discrete and continuous uncertainty distributions. Because uncertain attributes may have several possible values, a UPI on an uncertain attribute duplicates tuple data once for each possible value. To prevent the size of the UPI from becoming unmanageable, its size is kept small by placing low-probability tuples in a special Cutoff Index that is consulted only when queries for low-probability values are run. We also propose several other optimizations, including techniques to improve secondary index performance and techniques to reduce maintenance costs and fragmentation by buffering changes to the table and writing updates in sequential batches. Finally, we develop cost models for UPIs to estimate query performance in various settings to help automatically select tuning parameters of a UPI. We have implemented a prototype UPI and experimented on two real datasets. Our results show that UPIs can significantly (up to two orders of magnitude) improve the performance of uncertain queries both over clustered and unclustered attributes. We also show that our buffering techniques mitigate table fragmentation and keep the maintenance cost as low as or even lower than using an unclustered heap file.

#index 1523852
#* Ranking continuous probabilistic datasets
#@ Jian Li;Amol Deshpande
#t 2010
#c 4
#% 190611
#% 395135
#% 411762
#% 593740
#% 837639
#% 864396
#% 1016178
#% 1035577
#% 1063520
#% 1074083
#% 1075132
#% 1127377
#% 1181270
#% 1206646
#% 1206716
#% 1206893
#% 1206905
#% 1217141
#% 1217175
#% 1328151
#% 1408794
#% 1426515
#! Ranking is a fundamental operation in data analysis and decision support, and plays an even more crucial role if the dataset being explored exhibits uncertainty. This has led to much work in understanding how to rank uncertain datasets in recent years. In this paper, we address the problem of ranking when the tuple scores are uncertain, and the uncertainty is captured using continuous probability distributions (e.g. Gaussian distributions). We present a comprehensive solution to compute the values of a parameterized ranking function (PRF) [18] for arbitrary continuous probability distributions (and thus rank the uncertain dataset); PRF can be used to simulate or approximate many other ranking functions proposed in prior work. We develop exact polynomial time algorithms for some continuous probability distribution classes, and efficient approximation schemes with provable guarantees for arbitrary probability distributions. Our algorithms can also be used for exact or approximate evaluation of k-nearest neighbor queries over uncertain objects, whose positions are modeled using continuous probability distributions. Our experimental evaluation over several datasets illustrates the effectiveness of our approach at efficiently ranking uncertain datasets with continuous attribute uncertainty.

#index 1523853
#* Set similarity join on probabilistic data
#@ Xiang Lian;Lei Chen
#t 2010
#c 4
#% 242237
#% 479462
#% 616528
#% 765463
#% 810098
#% 824764
#% 864392
#% 893164
#% 893167
#% 893168
#% 907562
#% 956506
#% 992830
#% 1054481
#% 1055684
#% 1063521
#% 1127378
#% 1190673
#% 1206781
#% 1290947
#% 1426529
#% 1669490
#! Set similarity join has played an important role in many real-world applications such as data cleaning, near duplication detection, data integration, and so on. In these applications, set data often contain noises and are thus uncertain and imprecise. In this paper, we model such probabilistic set data on two uncertainty levels, that is, set and element levels. Based on them, we investigate the problem of probabilistic set similarity join (PS2J) over two probabilistic set databases, under the possible worlds semantics. To efficiently process the PS2J operator, we first reduce our problem by condensing the possible worlds, and then propose effective pruning techniques, including Jaccard distance pruning, probability upper bound pruning, and aggregate pruning, which can filter out false alarms of probabilistic set pairs, with the help of indexes and our designed synopses. We demonstrate through extensive experiments the PS2J processing performance on both real and synthetic data.

#index 1523854
#* Complex event detection at wire speed with FPGAs
#@ Louis Woods;Jens Teubner;Gustavo Alonso
#t 2010
#c 4
#% 289365
#% 323495
#% 390964
#% 779420
#% 804256
#% 1020757
#% 1063480
#% 1148075
#% 1217239
#% 1426609
#% 1523931
#! Complex event detection is an advanced form of data stream processing where the stream(s) are scrutinized to identify given event patterns. The challenge for many complex event processing (CEP) systems is to be able to evaluate event patterns on high-volume data streams while adhering to real-time constraints. To solve this problem, in this paper we present a hardware-based complex event detection system implemented on field-programmable gate arrays (FPGAs). By inserting the FPGA directly into the data path between the network interface and the CPU, our solution can detect complex events at gigabit wire speed with constant and fully predictable latency, independently of network load, packet size, or data distribution. This is a significant improvement over CPU-based systems and an architectural approach that opens up interesting opportunities for hybrid stream engines that combine the flexibility of the CPU with the parallelism and processing power of FPGAs.

#index 1523855
#* Database compression on graphics processors
#@ Wenbin Fang;Bingsheng He;Qiong Luo
#t 2010
#c 4
#% 480608
#% 765419
#% 824697
#% 864446
#% 874997
#% 875026
#% 893129
#% 911437
#% 1015332
#% 1016214
#% 1051712
#% 1063508
#% 1063542
#% 1092379
#% 1127390
#% 1127550
#% 1190097
#% 1206754
#% 1217168
#% 1222050
#% 1270566
#% 1328141
#% 1426530
#! Query co-processing on graphics processors (GPUs) has become an effective means to improve the performance of main memory databases. However, this co-processing requires the data transfer between the main memory and the GPU memory via a low-bandwidth PCI-E bus. The overhead of such data transfer becomes an important factor, even a bottleneck, for query co-processing performance on the GPU. In this paper, we propose to use compression to alleviate this performance problem. Specifically, we implement nine lightweight compression schemes on the GPU and further study the combinations of these schemes for a better compression ratio. We design a compression planner to find the optimal combination. Our experiments demonstrate that the GPU-based compression and decompression achieved a processing speed up to 45 and 56 GB/s respectively. Using partial decompression, we were able to significantly improve GPU-based query co-processing performance. As a side product, we have integrated our GPU-based compression into MonetDB, an open source column-oriented DBMS, and demonstrated the feasibility of offloading compression and decompression to the GPU.

#index 1523856
#* Aether: a scalable approach to logging
#@ Ryan Johnson;Ippokratis Pandis;Radu Stoica;Manos Athanassoulis;Anastasia Ailamaki
#t 2010
#c 4
#% 114582
#% 172939
#% 401958
#% 427195
#% 464699
#% 466947
#% 480589
#% 480831
#% 765692
#% 816654
#% 1022298
#% 1063543
#% 1063551
#% 1181215
#% 1217152
#% 1328149
#! The shift to multi-core hardware brings new challenges to database systems, as the software parallelism determines performance. Even though database systems traditionally accommodate simultaneous requests, a multitude of synchronization barriers serialize execution. Write-ahead logging is a fundamental, omnipresent component in ARIES-style concurrency and recovery, and one of the most important yet-to-be addressed potential bottlenecks, especially in OLTP workloads making frequent small changes to data. In this paper, we identify four logging-related impediments to database system scalability. Each issue challenges different level in the software architecture: (a) the high volume of small-sized I/O requests may saturate the disk, (b) transactions hold locks while waiting for the log flush, (c) extensive context switching overwhelms the OS scheduler with threads executing log I/Os, and (d) contention appears as transactions serialize accesses to in-memory log data structures. We demonstrate these problems and address them with techniques that, when combined, comprise a holistic, scalable approach to logging. Our solution achieves a 20%-69% speedup over a modern database system when running log-intensive workloads, such as the TPC-B and TATP benchmarks. Moreover, it achieves log insert throughput over 1.8GB/s for small log records on a single socket server, an order of magnitude higher than the traditional way of accessing the log using a single mutex.

#index 1523857
#* Scalable discovery of best clusters on large graphs
#@ Kathy Macropol;Ambuj Singh
#t 2010
#c 4
#% 255137
#% 282505
#% 313959
#% 479973
#% 755402
#% 847347
#% 868469
#% 898311
#% 1013696
#% 1211811
#% 1214695
#% 1279802
#% 1384246
#% 1404181
#% 1504829
#! The identification of clusters, well-connected components in a graph, is useful in many applications from biological function prediction to social community detection. However, finding these clusters can be difficult as graph sizes increase. Most current graph clustering algorithms scale poorly in terms of time or memory. An important insight is that many clustering applications need only the subset of best clusters, and not all clusters in the entire graph. In this paper we propose a new technique, Top Graph Clusters (TopGC), which probabilistically searches large, edge weighted, directed graphs for their best clusters in linear time. The algorithm is inherently parallelizable, and is able to find variable size, overlapping clusters. To increase scalability, a parameter is introduced that controls memory use. When compared with three other state-of-the art clustering techniques, TopGC achieves running time speedups of up to 70% on large scale real world datasets. In addition, the clusters returned by TopGC are consistently found to be better both in calculated score and when compared on real world benchmarks.

#index 1523858
#* An architecture for parallel topic models
#@ Alexander Smola;Shravan Narayanamurthy
#t 2010
#c 4
#% 722904
#% 757953
#% 1214715
#% 1229386
#% 1809993
#! This paper describes a high performance sampling architecture for inference of latent topic models on a cluster of workstations. Our system is faster than previous work by over an order of magnitude and it is capable of dealing with hundreds of millions of documents and thousands of topics. The algorithm relies on a novel communication structure, namely the use of a distributed (key, value) storage for synchronizing the sampler state between computers. Our architecture entirely obviates the need for separate computation and synchronization phases. Instead, disk, CPU, and network are used simultaneously to achieve high performance. We show that this architecture is entirely general and that it can be extended easily to more sophisticated latent variable models such as n-grams and hierarchies.

#index 1523859
#* Keyword++: a framework to improve keyword search over entity databases
#@ Venkatesh Ganti;Yeye He;Dong Xin
#t 2010
#c 4
#% 660011
#% 718437
#% 869535
#% 993987
#% 1022229
#% 1026825
#% 1063536
#% 1190070
#% 1190105
#% 1217173
#% 1217187
#% 1217265
#% 1426537
#% 1426566
#! Keyword search over entity databases (e.g., product, movie databases) is an important problem. Current techniques for keyword search on databases may often return incomplete and imprecise results. On the one hand, they either require that relevant entities contain all (or most) of the query keywords, or that relevant entities and the query keywords occur together in several documents from a known collection. Neither of these requirements may be satisfied for a number of user queries. Hence results for such queries are likely to be incomplete in that highly relevant entities may not be returned. On the other hand, although some returned entities contain all (or most) of the query keywords, the intention of the keywords in the query could be different from that in the entities. Therefore, the results could also be imprecise. To remedy this problem, in this paper, we propose a general framework that can improve an existing search interface by translating a keyword query to a structured query. Specifically, we leverage the keyword to attribute value associations discovered in the results returned by the original search interface. We show empirically that the translated structured queries alleviate the above problems.

#index 1523860
#* Swarm: mining relaxed temporal moving object clusters
#@ Zhenhui Li;Bolin Ding;Jiawei Han;Roland Kays
#t 2010
#c 4
#% 462231
#% 481290
#% 518854
#% 659971
#% 729418
#% 729933
#% 784297
#% 810049
#% 875505
#% 907380
#% 957731
#% 960283
#% 1016195
#% 1127436
#% 1176982
#% 1206688
#% 1426623
#% 1720762
#! Recent improvements in positioning technology make massive moving object data widely available. One important analysis is to find the moving objects that travel together. Existing methods put a strong constraint in defining moving object cluster, that they require the moving objects to stick together for consecutive timestamps. Our key observation is that the moving objects in a cluster may actually diverge temporarily and congregate at certain timestamps. Motivated by this, we propose the concept of swarm which captures the moving objects that move within arbitrary shape of clusters for certain timestamps that are possibly non-consecutive. The goal of our paper is to find all discriminative swarms, namely closed swarm. While the search space for closed swarms is prohibitively huge, we design a method, ObjectGrowth, to efficiently retrieve the answer. In ObjectGrowth, two effective pruning strategies are proposed to greatly reduce the search space and a novel closure checking rule is developed to report closed swarms on-the-fly. Empirical studies on the real data as well as large synthetic data demonstrate the effectiveness and efficiency of our methods.

#index 1523861
#* An adaptive updating protocol for reducing moving object database workload
#@ Su Chen;Beng Chin Ooi;Zhenjie Zhang
#t 2010
#c 4
#% 295512
#% 300174
#% 421124
#% 765452
#% 765453
#% 800571
#% 800572
#% 800575
#% 810048
#% 814349
#% 1015320
#% 1016193
#% 1046510
#% 1063471
#% 1117735
#% 1127612
#% 1217184
#% 1328209
#! In the last decade, spatio-temporal database research focuses on the design of effective and efficient indexing structures in support of location-based queries such as predictive range queries and nearest neighbor queries. While a variety of indexing techniques have been proposed to accelerate the processing of updates and queries, not much attention has been paid to the updating protocol, which is another important factor affecting system performance. In this paper, we propose a generic and adaptive updating protocol for moving object databases with less number of updating messages between the objects and database server, thereby reducing the overall workload of the system. In contrast to the approach adopted by most conventional moving object database systems where the exact locations and velocities last disclosed are used to predict their motions, we propose the concept of Spatio-Temporal Safe Region to approximate possible future locations. Spatio-temporal safe regions provide larger space of tolerance for moving objects, freeing them from location and velocity updates as long as the errors remain predictable in the database. To answer predictive queries accurately, the server is allowed to probe the latest status of some moving objects when their safe regions are inadequate in returning the exact query results. Spatio-temporal safe regions are calculated and optimized by the database server with two contradictory objectives: reducing update workload while guaranteeing query accuracy and efficiency. To achieve this, we propose a cost model that estimates the composition of active and passive updates based on historical motion records and query distribution. We have conducted extensive experiments to evaluate our proposal on a variety of popular indexing structures. The results confirm the viability, robustness, accuracy and efficiency of our proposed protocol.

#index 1523862
#* Shortest path computation on air indexes
#@ Georgios Kellaris;Kyriakos Mouratidis
#t 2010
#c 4
#% 443127
#% 443208
#% 443533
#% 787177
#% 813718
#% 937067
#% 1063472
#% 1260399
#% 1263928
#% 1270569
#! Shortest path computation is one of the most common queries in location-based services that involve transportation networks. Motivated by scalability challenges faced in the mobile network industry, we propose adopting the wireless broadcast model for such location-dependent applications. In this model the data are continuously transmitted on the air, while clients listen to the broadcast and process their queries locally. Although spatial problems have been considered in this environment, there exists no study on shortest path queries in road networks. We develop the first framework to compute shortest paths on the air, and demonstrate the practicality and efficiency of our techniques through experiments with real road networks and actual device specifications.

#index 1523863
#* Efficient and effective similarity search over probabilistic data based on earth mover's distance
#@ Jia Xu;Zhenjie Zhang;Anthony K. H. Tung;Ge Yu
#t 2010
#c 4
#% 235023
#% 248797
#% 325683
#% 643566
#% 718437
#% 771228
#% 824764
#% 864398
#% 893167
#% 893189
#% 975141
#% 976984
#% 983259
#% 1016201
#% 1039615
#% 1063484
#% 1063520
#% 1070886
#% 1206706
#% 1206893
#% 1217128
#% 1328110
#% 1328151
#! Probabilistic data is coming as a new deluge along with the technical advances on geographical tracking, multimedia processing, sensor network and RFID. While similarity search is an important functionality supporting the manipulation of probabilistic data, it raises new challenges to traditional relational database. The problem stems from the limited effectiveness of the distance metric supported by the existing database system. On the other hand, some complicated distance operators have proven their values for better distinguishing ability in the probabilistic domain. In this paper, we discuss the similarity search problem with the Earth Mover's Distance, which is the most successful distance metric on probabilistic histograms and an expensive operator with cubic complexity. We present a new database approach to answer range queries and k-nearest neighbor queries on probabilistic data, on the basis of Earth Mover's Distance. Our solution utilizes the primal-dual theory in linear programming and deploys B+ tree index structures for effective candidate pruning. Extensive experiments show that our proposal dramatically improves the scalability of probabilistic databases.

#index 1523864
#* Probabilistic XML via Markov Chains
#@ Michael Benedikt;Evgeny Kharlamov;Dan Olteanu;Pierre Senellart
#t 2010
#c 4
#% 241166
#% 265692
#% 587580
#% 733595
#% 776152
#% 778122
#% 786577
#% 817690
#% 824798
#% 939555
#% 1022204
#% 1039062
#% 1063523
#% 1131403
#% 1134141
#% 1217138
#% 1291113
#% 1291118
#% 1291120
#% 1424592
#% 1707651
#! We show how Recursive Markov Chains (RMCs) and their restrictions can define probabilistic distributions over XML documents, and study tractability of querying over such models. We show that RMCs subsume several existing probabilistic XML models. In contrast to the latter, RMC models (i) capture probabilistic versions of XML schema languages such as DTDs, (ii) can be exponentially more succinct, and (iii) do not restrict the domain of probability distributions to be finite. We investigate RMC models for which tractability can be achieved, and identify several tractable fragments that subsume known tractable probabilistic XML models. We then look at the space of models between existing probabilistic XML formalisms and RMCs, giving results on the expressiveness and succinctness of RMC subclasses, both with each other and with prior formalisms.

#index 1523865
#* MCDB-R: risk analysis in the database
#@ Subi Arumugam;Fei Xu;Ravi Jampani;Christopher Jermaine;Luis L. Perez;Peter J. Haas
#t 2010
#c 4
#% 874976
#% 1063521
#% 1063529
#% 1127376
#% 1200291
#% 1290557
#% 1291115
#% 1328066
#! Enterprises often need to assess and manage the risk arising from uncertainty in their data. Such uncertainty is typically modeled as a probability distribution over the uncertain data values, specified by means of a complex (often predictive) stochastic model. The probability distribution over data values leads to a probability distribution over database query results, and risk assessment amounts to exploration of the upper or lower tail of a query-result distribution. In this paper, we extend the Monte Carlo Database System to efficiently obtain a set of samples from the tail of a query-result distribution by adapting recent "Gibbs cloning" ideas from the simulation literature to a database setting.

#index 1523866
#* Scalable probabilistic databases with factor graphs and MCMC
#@ Michael Wick;Andrew McCallum;Gerome Miklau
#t 2010
#c 4
#% 13016
#% 215225
#% 464434
#% 810098
#% 850430
#% 864417
#% 1016201
#% 1063521
#% 1127378
#% 1127415
#% 1201871
#% 1260410
#% 1269815
#! Incorporating probabilities into the semantics of incomplete databases has posed many challenges, forcing systems to sacrifice modeling power, scalability, or treatment of relational algebra operators. We propose an alternative approach where the underlying relational database always represents a single world, and an external factor graph encodes a distribution over possible worlds; Markov chain Monte Carlo (MCMC) inference is then used to recover this uncertainty to a desired level of fidelity. Our approach allows the efficient evaluation of arbitrary queries over probabilistic databases with arbitrary dependencies expressed by graphical models with structure that changes during inference. MCMC sampling provides efficiency by hypothesizing modifications to possible worlds rather than generating entire worlds from scratch. Queries are then run over the portions of the world that change, avoiding the onerous cost of running full queries over each sampled world. A significant innovation of this work is the connection between MCMC sampling and materialized view maintenance techniques: we find empirically that using view maintenance techniques is several orders of magnitude faster than naively querying each sampled world. We also demonstrate our system's ability to answer relational queries with aggregation, and demonstrate additional scalability through the use of parallelization on a real-world complex model of information extraction. This framework is sufficiently expressive to support probabilistic inference not only for answering queries, but also for inferring missing database content from raw evidence.

#index 1523867
#* On multi-column foreign key discovery
#@ Meihui Zhang;Marios Hadjieleftheriou;Beng Chin Ooi;Cecilia M. Procopiuc;Divesh Srivastava
#t 2010
#c 4
#% 58641
#% 333931
#% 397369
#% 410276
#% 416036
#% 427873
#% 616528
#% 654458
#% 727923
#% 893145
#% 960250
#% 1016144
#% 1166724
#% 1174747
#% 1213391
#! A foreign/primary key relationship between relational tables is one of the most important constraints in a database. From a data analysis perspective, discovering foreign keys is a crucial step in understanding and working with the data. Nevertheless, more often than not, foreign key constraints are not specified in the data, for various reasons; e.g., some associations are not known to designers but are inherent in the data, while others become invalid due to data inconsistencies. This work proposes a robust algorithm for discovering single-column and multi-column foreign keys. Previous work concentrated mostly on discovering single-column foreign keys using a variety of rules, like inclusion dependencies, column names, and minimum/maximum values. We first propose a general rule, termed Randomness, that subsumes a variety of other rules. We then develop efficient approximation algorithms for evaluating randomness, using only two passes over the data. Finally, we validate our approach via extensive experiments using real and synthetic datasets.

#index 1523868
#* Explore or exploit?: effective strategies for disambiguating large databases
#@ Reynold Cheng;Eric Lo;Xuan S. Yang;Ming-Hay Luk;Xiang Li;Xike Xie
#t 2010
#c 4
#% 442830
#% 654487
#% 654488
#% 797077
#% 810553
#% 864417
#% 873104
#% 893167
#% 913783
#% 993982
#% 1016178
#% 1016201
#% 1022203
#% 1103008
#% 1127408
#% 1328159
#! Data ambiguity is inherent in applications such as data integration, location-based services, and sensor monitoring. In many situations, it is possible to "clean", or remove, ambiguities from these databases. For example, the GPS location of a user is inexact due to measurement errors, but context information (e.g., what a user is doing) can be used to reduce the imprecision of the location value. In order to obtain a database with a higher quality, we study how to disambiguate a database by appropriately selecting candidates to clean. This problem is challenging because cleaning involves a cost, is limited by a budget, may fail, and may not remove all ambiguities. Moreover, the statistical information about how likely database objects can be cleaned may not be precisely known. We tackle these challenges by proposing two types of algorithms. The first type makes use of greedy heuristics to make sensible decisions; however, these algorithms do not make use of cleaning information and require user input for parameters to achieve high cleaning effectiveness. We propose the Explore-Exploit (or EE) algorithm, which gathers valuable information during the cleaning process to determine how the remaining cleaning budget should be invested. We also study how to fine-tune the parameters of EE in order to achieve optimal cleaning effectiveness. Experimental evaluations on real and synthetic datasets validate the effectiveness and efficiency of our approaches.

#index 1523869
#* Building ranked mashups of unstructured sources with uncertain information
#@ Mohamed A. Soliman;Ihab F. Ilyas;Mina Saleeb
#t 2010
#c 4
#% 480819
#% 480824
#% 777931
#% 810018
#% 864473
#% 1063520
#% 1063559
#% 1063713
#% 1075132
#% 1131145
#% 1206893
#% 1206905
#% 1207234
#% 1217172
#% 1217264
#% 1456853
#% 1523852
#! Mashups are situational applications that join multiple sources to better meet the information needs of Web users. Web sources can be huge databases behind query interfaces, which triggers the need of ranking mashup results based on some user preferences. We present MashRank, a mashup authoring and processing system building on concepts from rank-aware processing, probabilistic databases, and information extraction to enable ranked mashups of (unstructured) sources with uncertain ranking attributes. MashRank is based on new semantics, formulations and processing techniques to handle uncertain preference scores, represented as intervals enclosing possible score values. MashRank integrates information extraction with query processing by asynchronously pushing extracted data on-the-fly into pipelined rank-aware query plans, and using ranking early-out requirements to limit extraction cost. To the best of our knowledge, both the technical problems and target applications of MashRank have not been addressed before.

#index 1523870
#* Computing closed skycubes
#@ Chedy Raïssi;Jian Pei;Thomas Kister
#t 2010
#c 4
#% 288976
#% 289148
#% 384416
#% 464873
#% 465167
#% 480671
#% 729933
#% 824671
#% 824672
#% 864452
#% 875011
#% 912241
#! In this paper, we tackle the problem of efficient skycube computation. We introduce a novel approach significantly reducing domination tests for a given subspace and the number of subspaces searched. Technically, we identify two types of skyline points that can be directly derived without using any domination tests. Moreover, based on formal concept analysis, we introduce two closure operators that enable a concise representation of skyline cubes. We show that this concise representation is easy to compute and develop an efficient algorithm, which only needs to search a small portion of the huge search space. We show with empirical results the merits of our approach.

#index 1523871
#* Generating databases for query workloads
#@ Eric Lo;Nick Cheng;Wing-Kai Hon
#t 2010
#c 4
#% 11813
#% 122671
#% 172913
#% 404719
#% 464700
#% 874977
#% 902467
#% 903012
#% 960262
#% 1063507
#% 1217165
#% 1400782
#% 1718227
#! To evaluate the performance of database applications and DBMSs, we usually execute workloads of queries on generated databases of different sizes and measure the response time. This paper introduces MyBenchmark, an offline data generation tool that takes a set of queries as input and generates database instances for which the users can control the characteristics of the resulting workload. Applications of MyBenchmark include database testing, database application testing, and application-driven benchmarking. We present the architecture and the implementation algorithms of MyBenchmark. We also present the evaluation results of MyBenchmark using TPC workloads.

#index 1523872
#* Processing top-k join queries
#@ Minji Wu;Laure Berti-Équille;Amélie Marian;Cecilia M. Procopiuc;Divesh Srivastava
#t 2010
#c 4
#% 480819
#% 643566
#% 763882
#% 976984
#% 977014
#% 1015317
#% 1016183
#% 1021954
#% 1068580
#% 1206646
#% 1206835
#% 1328151
#! We consider the problem of efficiently finding the top-k answers for join queries over web-accessible databases. Classical algorithms for finding top-k answers use branch-and-bound techniques to avoid computing scores of all candidates in identifying the top-k answers. To be able to apply such techniques, it is critical to efficiently compute (lower and upper) bounds and expected scores of candidate answers in an incremental fashion during the evaluation. In this paper, we describe novel techniques for these problems. The first contribution of this paper is a method to efficiently compute bounds for the score of a query result when tuples in tables from the "FROM" clause are discovered incrementally, through either sorted or random access. Our second contribution is an algorithm that, given a set of partially evaluated candidate answers, determines a good order in which to access the tables to minimize wasted efforts in the computation of top-k answers. We evaluate our algorithms on a variety of queries and data sets and demonstrate the significant benefits they provide.

#index 1523873
#* Two-way replacement selection
#@ Xavier Martinez-Palau;David Dominguez-Sal;Josep Lluis Larriba-Pey
#t 2010
#c 4
#% 14204
#% 248824
#% 252608
#% 326257
#% 443033
#% 641970
#% 867058
#% 946439
#% 1127446
#! The performance of external sorting using merge sort is highly dependent on the length of the runs generated. One of the most commonly used run generation strategies is Replacement Selection (RS) because, on average, it generates runs that are twice the size of the memory available. However, the length of the runs generated by RS is downsized for data with certain characteristics, like inputs sorted inversely with respect to the desired output order. The goal of this paper is to propose and analyze two-way replacement selection (2WRS), which is a generalization of RS obtained by implementing two heaps instead of the single heap implemented by RS. The appropriate management of these two heaps allows generating runs larger than the memory available in a stable way, i.e. independent from the characteristics of the datasets. Depending on the changing characteristics of the input dataset, 2WRS assigns a new data record to one or the other heap, and grows or shrinks each heap, accommodating to the growing or decreasing tendency of the dataset. On average, 2WRS creates runs of at least the length generated by RS, and longer for datasets that combine increasing and decreasing data subsets. We tested both algorithms on large datasets with different characteristics and 2WRS achieves speedups at least similar to RS, and over 2.5 when RS fails to generate large runs.

#index 1523874
#* XPath whole query optimization
#@ Sebastian Maneth;Kim Nguyen
#t 2010
#c 4
#% 178069
#% 212287
#% 401124
#% 516520
#% 722732
#% 791182
#% 804841
#% 874142
#% 993939
#% 994015
#% 1015275
#% 1015298
#% 1135431
#% 1673670
#% 1721253
#! Previous work reports about SXSI, a fast XPath engine which executes tree automata over compressed XML indexes. Here, reasons are investigated why SXSI is so fast. It is shown that tree automata can be used as a general framework for fine grained XML query optimization. We define the "relevant nodes" of a query as those nodes that a minimal automaton must touch in order to answer the query. This notion allows to skip many subtrees during execution, and, with the help of particular tree indexes, even allows to skip internal nodes of the tree. We efficiently approximate runs over relevant nodes by means of on-the-fly removal of alternation and non-determinism of (alternating) tree automata. We also introduce many implementation techniques which allows us to efficiently evaluate tree automata, even in the absence of special indexes. Through extensive experiments, we demonstrate the impact of the different optimization techniques.

#index 1523875
#* Fast optimal twig joins
#@ Nils Grimsmo;Truls A. Bjørklund;Magnus Lie Hetland
#t 2010
#c 4
#% 333981
#% 397360
#% 397375
#% 745461
#% 745477
#% 783546
#% 810046
#% 893112
#% 1013630
#% 1015277
#% 1107581
#% 1125880
#% 1206666
#% 1408834
#% 1917621
#! In XML search systems twig queries specify predicates on node values and on the structural relationships between nodes, and a key operation is to join individual query node matches into full twig matches. Linear time twig join algorithms exist, but many non-optimal algorithms with better average-case performance have been introduced recently. These use somewhat simpler data structures that are faster in practice, but have exponential worst-case time complexity. In this paper we explore and extend the solution space spanned by previous approaches. We introduce new data structures and improved strategies for filtering out useless data nodes, yielding combinations that are both worst-case optimal and faster in practice. An experimental study shows that our best algorithm outperforms previous approaches by an average factor of three on common benchmarks. On queries with at least one unselective leaf node, our algorithm can be an order of magnitude faster, and it is never more than 20% slower on any tested benchmark query.

#index 1523876
#* Destabilizers and independence of XML updates
#@ Michael Benedikt;James Cheney
#t 2010
#c 4
#% 59350
#% 442767
#% 443138
#% 481128
#% 565149
#% 654493
#% 809236
#% 839147
#% 893111
#% 893136
#% 963218
#% 980990
#% 994015
#% 1015272
#% 1092015
#% 1266686
#% 1270570
#% 1328114
#% 1397990
#% 1415274
#% 1688277
#% 1721253
#! Independence analysis is the problem of determining whether an update affects the result of a query, e.g. a constraint or materialized view. We develop a new, modular framework for static independence analysis that decomposes the problem into two orthogonal subproblems: approximating the destabilizer, that is, a finite representation of the set of updates that can change the result of the query, and testing whether the update and destabilizer overlap via an intersection analysis. Focusing on XML queries as the view language and the XQuery Update Facility as the update language, we present a syntactic query rewriting algorithm for translating queries to destabilizers, and show that intersection checking can be reduced to satisfiability problems for which efficient checkers already exist. We present an implementation based on an expressive tree satisfiability checker and a Satisfiability Modulo Order package, and give experiments confirming that the resulting analysis is both fast and effective.

#index 1523877
#* Searching workflows with hierarchical views
#@ Ziyang Liu;Qihong Shao;Yi Chen
#t 2010
#c 4
#% 463910
#% 765129
#% 810052
#% 825657
#% 825659
#% 832825
#% 893117
#% 901912
#% 914569
#% 960243
#% 960259
#% 960261
#% 1016179
#% 1022327
#% 1060024
#% 1063539
#% 1063545
#% 1063571
#% 1121279
#% 1127424
#% 1206750
#% 1206807
#% 1206843
#% 1206957
#% 1207025
#% 1217188
#% 1232681
#% 1370256
#% 1692849
#! Workflows are prevalent in diverse applications, which can be scientific experiments, business processes, web services, or recipes. With the dramatically growing number of workflows, there is an increasing need for people to search a workflow repository using keywords and to retrieve the relevant ones. A workflow hierarchy is a three dimensional object containing multiple abstraction views of different granularity on the same workflow. This unique structure poses a new set of challenges compared to keyword search on tree or graph structures typically found in relational or XML data. In this paper, we define an informative, self-contained and concise search result on workflows to be a projection of a workflow hierarchy on a two dimensional viewing plane inferred from user queries. We then design and develop an efficient keyword search engine for workflows. Experimental evaluation demonstrates the effectiveness of our approach.

#index 1523878
#* Data-oriented transaction execution
#@ Ippokratis Pandis;Ryan Johnson;Nikos Hardavellas;Anastasia Ailamaki
#t 2010
#c 4
#% 14204
#% 32897
#% 114582
#% 172939
#% 239255
#% 403195
#% 442700
#% 480767
#% 480831
#% 789387
#% 801348
#% 810039
#% 824657
#% 998845
#% 1022298
#% 1063543
#% 1063551
#% 1181215
#% 1213672
#% 1213682
#% 1222048
#% 1328149
#% 1426552
#% 1523799
#! While hardware technology has undergone major advancements over the past decade, transaction processing systems have remained largely unchanged. The number of cores on a chip grows exponentially, following Moore's Law, allowing for an ever-increasing number of transactions to execute in parallel. As the number of concurrently-executing transactions increases, contended critical sections become scalability burdens. In typical transaction processing systems the centralized lock manager is often the first contended component and scalability bottleneck. In this paper, we identify the conventional thread-to-transaction assignment policy as the primary cause of contention. Then, we design DORA, a system that decomposes each transaction to smaller actions and assigns actions to threads based on which data each action is about to access. DORA's design allows each thread to mostly access thread-local data structures, minimizing interaction with the contention-prone centralized lock manager. Built on top of a conventional storage engine, DORA maintains all the ACID properties. Evaluation of a prototype implementation of DORA on a multicore system demonstrates that DORA attains up to 4.8x higher throughput than a state-of-the-art storage engine when running a variety of synthetic and real-world OLTP workloads.

#index 1523879
#* Optimal top-k query evaluation for weighted business processes
#@ Daniel Deutch;Tova Milo;Neoklis Polyzotis;Tom Yam
#t 2010
#c 4
#% 1722
#% 71306
#% 424283
#% 643566
#% 874885
#% 920378
#% 1015373
#% 1022204
#% 1075132
#% 1127379
#% 1180016
#% 1206929
#% 1217226
#% 1688305
#% 1707651
#! A Business Process (BP for short) consists of a set of activities that achieve some business goal when combined in a flow. Among all the (maybe infinitely many) possible execution flows of a BP, analysts are often interested in identifying flows that are "most important", according to some weight metric. This paper studies the following problem: given a specification of such a BP, a weighting function over BP execution flows, a query, and a number k, identify the k flows with the highest weight among those satisfying the query. We provide here, for the first time, a provably optimal algorithm for identifying the top-k weighted flows of a given BP, and use it for efficient top-k query evaluation.

#index 1523880
#* Behavioral simulations in MapReduce
#@ Guozhang Wang;Marcos Vaz Salles;Benjamin Sowell;Xun Wang;Tuan Cao;Alan Demers;Johannes Gehrke;Walker White
#t 2010
#c 4
#% 4694
#% 86786
#% 86929
#% 135541
#% 150280
#% 175780
#% 189868
#% 204928
#% 215376
#% 235072
#% 266457
#% 277323
#% 399766
#% 442700
#% 464540
#% 511175
#% 659919
#% 809241
#% 810061
#% 827316
#% 874466
#% 874978
#% 938304
#% 954300
#% 960236
#% 960326
#% 963669
#% 1013914
#% 1083582
#% 1157462
#% 1171329
#% 1230827
#% 1322180
#% 1346430
#% 1351116
#% 1386046
#! In many scientific domains, researchers are turning to large-scale behavioral simulations to better understand real-world phenomena. While there has been a great deal of work on simulation tools from the high-performance computing community, behavioral simulations remain challenging to program and automatically scale in parallel environments. In this paper we present BRACE (Big Red Agent-based Computation Engine), which extends the MapReduce framework to process these simulations efficiently across a cluster. We can leverage spatial locality to treat behavioral simulations as iterated spatial joins and greatly reduce the communication between nodes. In our experiments we achieve nearly linear scale-up on several realistic simulations. Though processing behavioral simulations in parallel as iterated spatial joins can be very efficient, it can be much simpler for the domain scientists to program the behavior of a single agent. Furthermore, many simulations include a considerable amount of complex computation and message passing between agents, which makes it important to optimize the performance of a single node and the communication across nodes. To address both of these challenges, BRACE includes a high-level language called BRASIL (the Big Red Agent SImulation Language). BRASIL has object-oriented features for programming simulations, but can be compiled to a dataflow representation for automatic parallelization and optimization. We show that by using various optimization techniques, we can achieve both scalability and single-node performance similar to that of a hand-coded simulation.

#index 1523881
#* A*-tree: a structure for storage and modeling of uncertain multidimensional arrays
#@ Tingjian Ge;Stan Zdonik
#t 2010
#c 4
#% 248863
#% 410276
#% 461922
#% 572268
#% 572308
#% 654487
#% 818434
#% 846094
#% 874976
#% 891559
#% 893167
#% 1016201
#% 1063521
#% 1127378
#% 1206717
#% 1206735
#% 1206770
#% 1650778
#! Multidimensional array database systems are suited for scientific and engineering applications. Data in these applications is often uncertain and imprecise due to errors in the instruments and observations, etc. There are often correlations exhibited in the distribution of values among the cells of an array. Typically, the correlation is stronger for cells that are close to each other and weaker for cells that are far away. We devise a novel data structure, called the A*-tree (multidimensional Array tree), demonstrating that by taking advantage of the predictable and structured correlations of multidimensional data, we can have a more efficient way of modeling and answering queries on large-scale array data. An A*-tree is a unified model for storage and inference. The graphical model that is assumed in an A*-tree is essentially a Bayesian Network. We analyze and experimentally verify the accuracy of an A*-tree encoding of the underlying joint distribution. We also study the efficiency of query processing over A*-trees, comparing it to an alternative graphical model.

#index 1523882
#* On dense pattern mining in graph streams
#@ Charu C. Aggarwal;Yao Li;Philip S. Yu;Ruoming Jin
#t 2010
#c 4
#% 443393
#% 498852
#% 729938
#% 785339
#% 823347
#% 824711
#% 844308
#% 956459
#% 1016146
#% 1063501
#% 1117010
#% 1207028
#% 1372657
#! Many massive web and communication network applications create data which can be represented as a massive sequential stream of edges. For example, conversations in a telecommunication network or messages in a social network can be represented as a massive stream of edges. Such streams are typically very large, because of the large amount of underlying activity in such networks. An important application in these domains is to determine frequently occurring dense structures in the underlying graph stream. In general, we would like to determine frequent and dense patterns in the underlying interactions. We introduce a model for dense pattern mining and propose probabilistic algorithms for determining such structural patterns effectively and efficiently. The purpose of the probabilistic approach is to create a summarization of the graph stream, which can be used for further pattern mining. We show that this summarization approach leads to effective and efficient results for stream pattern mining over a number of real and synthetic data sets.

#index 1523883
#* Efficient proximity detection among mobile users via self-tuning policies
#@ Man Lung Yiu;Leong Hou U;Simonas Šaltenis;Kostas Tzoumas
#t 2010
#c 4
#% 213975
#% 295512
#% 300174
#% 333969
#% 421124
#% 579313
#% 654478
#% 765453
#% 800186
#% 810048
#% 810061
#% 832568
#% 871761
#% 879211
#% 893605
#% 960282
#% 1127438
#% 1206682
#% 1328097
#% 1328208
#! Given a set of users, their friend relationships, and a distance threshold per friend pair, the proximity detection problem is to find each pair of friends such that the Euclidean distance between them is within the given threshold. This problem plays an essential role in friend-locator applications and massively multiplayer online games. Existing proximity detection solutions either incur substantial location update costs or their performance does not scale well to a large number of users. Motivated by this, we present a centralized proximity detection solution that assigns each mobile client with a mobile region. We then design a self-tuning policy to adjust the radius of the region automatically, in order to minimize communication cost. In addition, we analyze the communication cost of our solutions, and provide valuable insights on their behaviors. Extensive experiments suggest that our proposed solution is efficient and robust with respect to various parameters.

#index 1523884
#* k-nearest neighbors in uncertain graphs
#@ Michalis Potamias;Francesco Bonchi;Aristides Gionis;George Kollios
#t 2010
#c 4
#% 342596
#% 729923
#% 730089
#% 740507
#% 821926
#% 891559
#% 893189
#% 983330
#% 1013691
#% 1016201
#% 1063520
#% 1063521
#% 1063568
#% 1063719
#% 1063728
#% 1073984
#% 1127375
#% 1127378
#% 1127573
#% 1147652
#% 1147662
#% 1181270
#% 1206717
#% 1206893
#% 1206905
#% 1214624
#% 1214633
#% 1217175
#% 1291123
#% 1328151
#% 1692830
#! Complex networks, such as biological, social, and communication networks, often entail uncertainty, and thus, can be modeled as probabilistic graphs. Similar to the problem of similarity search in standard graphs, a fundamental problem for probabilistic graphs is to efficiently answer k-nearest neighbor queries (k-NN), which is the problem of computing the k closest nodes to some specific node. In this paper we introduce a framework for processing k-NN queries in probabilistic graphs. We propose novel distance functions that extend well-known graph concepts, such as shortest paths. In order to compute them in probabilistic graphs, we design algorithms based on sampling. During k-NN query processing we efficiently prune the search space using novel techniques. Our experiments indicate that our distance functions outperform previously used alternatives in identifying true neighbors in real-world biological data. We also demonstrate that our algorithms scale for graphs with tens of millions of edges.

#index 1523885
#* Mining significant semantic locations from GPS data
#@ Xin Cao;Gao Cong;Christian S. Jensen
#t 2010
#c 4
#% 262061
#% 273890
#% 290830
#% 340932
#% 411762
#% 661923
#% 723186
#% 834177
#% 835018
#% 870366
#% 908603
#% 939968
#% 1016176
#% 1077150
#% 1190134
#% 1207110
#% 1328137
#% 1913977
#! With the increasing deployment and use of GPS-enabled devices, massive amounts of GPS data are becoming available. We propose a general framework for the mining of semantically meaningful, significant locations, e.g., shopping malls and restaurants, from such data. We present techniques capable of extracting semantic locations from GPS data. We capture the relationships between locations and between locations and users with a graph. Significance is then assigned to locations using random walks over the graph that propagates significance among the locations. In doing so, mutual reinforcement between location significance and user authority is exploited for determining significance, as are aspects such as the number of visits to a location, the durations of the visits, and the distances users travel to reach locations. Studies using up to 100 million GPS records from a confined spatio-temporal region demonstrate that the proposal is effective and is capable of outperforming baseline methods and an extension of an existing proposal.

#index 1523886
#* Boosting the accuracy of differentially private histograms through consistency
#@ Michael Hay;Vibhor Rastogi;Gerome Miklau;Dan Suciu
#t 2010
#c 4
#% 977011
#% 1061644
#% 1198224
#% 1217148
#% 1318624
#% 1414540
#% 1426322
#% 1426328
#% 1478165
#% 1489408
#% 1740518
#! We show that it is possible to significantly improve the accuracy of a general class of histogram queries while satisfying differential privacy. Our approach carefully chooses a set of queries to evaluate, and then exploits consistency constraints that should hold over the noisy output. In a post-processing phase, we compute the consistent input most likely to have produced the noisy output. The final output is differentially-private and consistent, but in addition, it is often much more accurate. We show, both theoretically and experimentally, that these techniques can be used for estimating the degree sequence of a graph very precisely, and for computing a histogram that can support arbitrary range queries accurately.

#index 1523887
#* ρ-uncertainty: inference-proof transaction anonymization
#@ Jianneng Cao;Panagiotis Karras;Chedy Raïssi;Kian-Lee Tan
#t 2010
#c 4
#% 152934
#% 248791
#% 251693
#% 300120
#% 342643
#% 348155
#% 428404
#% 443350
#% 443463
#% 481290
#% 481754
#% 481758
#% 539744
#% 576111
#% 577233
#% 577239
#% 740764
#% 769910
#% 874989
#% 893100
#% 913785
#% 937550
#% 956509
#% 993988
#% 1022247
#% 1083709
#% 1127361
#% 1154063
#% 1200329
#% 1206581
#% 1206883
#% 1217156
#% 1328187
#! The publication of transaction data, such as market basket data, medical records, and query logs, serves the public benefit. Mining such data allows for the derivation of association rules that connect certain items to others with measurable confidence. Still, this type of data analysis poses a privacy threat; an adversary having partial information on a person's behavior may confidently associate that person to an item deemed to be sensitive. Ideally, an anonymization of such data should lead to an inference-proof version that prevents the association of individuals to sensitive items, while otherwise allowing for truthful associations to be derived. Original approaches to this problem were based on value perturbation, damaging data integrity. Recently, value generalization has been proposed as an alternative; still, approaches based on it have assumed either that all items are equally sensitive, or that some are sensitive and can be known to an adversary only by association, while others are non-sensitive and can be known directly. Yet in reality there is a distinction between sensitive and non-sensitive items, but an adversary may possess information on any of them. Most critically, no antecedent method aims at a clear inference-proof privacy guarantee. In this paper, we propose ρ-uncertainty, the first, to our knowledge, privacy concept that inherently safeguards against sensitive associations without constraining the nature of an adversary's knowledge and without falsifying data. The problem of achieving ρ-uncertainty with low information loss is challenging because it is natural. A trivial solution is to suppress all sensitive items. We develop more sophisticated schemes. In a broad experimental study, we show that the problem is solved non-trivially by a technique that combines generalization and suppression, which also achieves favorable results compared to a baseline perturbation-based scheme.

#index 1523888
#* Minimizing minimality and maximizing utility: analyzing method-based attacks on anonymized data
#@ Graham Cormode;Divesh Srivastava;Ninghui Li;Tiancheng Li
#t 2010
#c 4
#% 443463
#% 800514
#% 810011
#% 864406
#% 864412
#% 881546
#% 881551
#% 893100
#% 1015140
#% 1022247
#% 1200328
#% 1206745
#% 1217156
#% 1217237
#% 1370254
#% 1700134
#! The principle of anonymization for data sharing has become a very popular paradigm for the preservation of privacy of the data subjects. Since the introduction of k-anonymity, dozens of methods and enhanced privacy definitions have been proposed. However, over-eager attempts to minimize the information lost by the anonymization potentially allow private information to be inferred. Proof-of-concept of this "minimality attack" has been demonstrated for a variety of algorithms and definitions [16]. In this paper, we provide a comprehensive analysis and study of this attack, and demonstrate that with care its effect can be almost entirely countered. The attack allows an adversary to increase his (probabilistic) belief in certain facts about individuals over the data. We show that (a) a large class of algorithms are not affected by this attack, (b) for a class of algorithms that have a "symmetric" property, the attacker's belief increases by at most a small constant, and (c) even for an algorithm chosen to be highly susceptible to the attack, the attacker's belief when using the attack increases by at most a small constant factor. We also provide a series of experiments that show in all these cases that the confidence about the sensitive value of any individual remains low in practice, while the published data is still useful for its intended purpose. From this, we conclude that the impact of such method-based attacks can be minimized.

#index 1523889
#* Querying probabilistic information extraction
#@ Daisy Zhe Wang;Michael J. Franklin;Minos Garofalakis;Joseph M. Hellerstein
#t 2010
#c 4
#% 464434
#% 765418
#% 874976
#% 875064
#% 893167
#% 893168
#% 939889
#% 1016201
#% 1022288
#% 1063547
#% 1127378
#% 1206687
#% 1206717
#% 1217154
#% 1250184
#% 1269815
#% 1279275
#% 1299522
#! Recently, there has been increasing interest in extending relational query processing to include data obtained from unstructured sources. A common approach is to use stand-alone Information Extraction (IE) techniques to identify and label entities within blocks of text; the resulting entities are then imported into a standard database and processed using relational queries. This two-part approach, however, suffers from two main drawbacks. First, IE is inherently probabilistic, but traditional query processing does not properly handle probabilistic data, resulting in reduced answer quality. Second, performance inefficiencies arise due to the separation of IE from query processing. In this paper, we address these two problems by building on an in-database implementation of a leading IE model---Conditional Random Fields using the Viterbi inference algorithm. We develop two different query approaches on top of this implementation. The first uses deterministic queries over maximum-likelihood extractions, with optimizations to push the relational operators into the Viterbi algorithm. The second extends the Viterbi algorithm to produce a set of possible extraction "worlds", from which we compute top-k probabilistic query answers. We describe these approaches and explore the trade-offs of efficiency and effectiveness between them using two datasets.

#index 1523890
#* Read-once functions and query evaluation in probabilistic databases
#@ Prithviraj Sen;Amol Deshpande;Lise Getoor
#t 2010
#c 4
#% 1675
#% 215225
#% 288984
#% 546123
#% 810098
#% 893167
#% 976984
#% 977013
#% 1016201
#% 1063521
#% 1063719
#% 1111133
#% 1133732
#% 1179162
#% 1206987
#% 1217176
#% 1291116
#% 1291123
#% 1343550
#% 1343724
#% 1372709
#% 1426461
#! Probabilistic databases hold promise of being a viable means for large-scale uncertainty management, increasingly needed in a number of real world applications domains. However, query evaluation in probabilistic databases remains a computational challenge. Prior work on efficient exact query evaluation in probabilistic databases has largely concentrated on query-centric formulations (e.g., safe plans, hierarchical queries), in that, they only consider characteristics of the query and not the data in the database. It is easy to construct examples where a supposedly hard query run on an appropriate database gives rise to a tractable query evaluation problem. In this paper, we develop efficient query evaluation techniques that leverage characteristics of both the query and the data in the database. We focus on tuple-independent databases where the query evaluation problem is equivalent to computing marginal probabilities of Boolean formulas associated with the result tuples. This latter task is easy if the Boolean formulas can be factorized into a form that has every variable appearing at most once (called read-once). However, a naive approach that directly uses previously developed Boolean formula factorization algorithms is inefficient, because those algorithms require the input formulas to be in the disjunctive normal form (DNF). We instead develop novel, more efficient factorization algorithms that directly construct the read-once expression for a result tuple Boolean formula (if one exists), for a large subclass of queries (specifically, conjunctive queries without self-joins). We empirically demonstrate that (1) our proposed techniques are orders of magnitude faster than generic inference algorithms for queries where the result Boolean formulas can be factorized into read-once expressions, and (2) for the special case of hierarchical queries, they rival the efficiency of prior techniques specifically designed to handle such queries.

#index 1523891
#* Foundations of uncertain-data integration
#@ Parag Agrawal;Anish Das Sarma;Jeffrey Ullman;Jennifer Widom
#t 2010
#c 4
#% 663
#% 32879
#% 36683
#% 115608
#% 119792
#% 195432
#% 197233
#% 235023
#% 384978
#% 464727
#% 482108
#% 572311
#% 707146
#% 810098
#% 824718
#% 824764
#% 836134
#% 893089
#% 893167
#% 1016201
#% 1022259
#% 1063534
#% 1424593
#% 1661428
#% 1720904
#% 1728680
#! There has been considerable past work studying data integration and uncertain data in isolation. We develop the foundations for local-as-view (LAV) data integration when the sources being integrated are uncertain. We motivate two distinct settings for uncertain-data integration. We then define containment of uncertain databases in these settings, which allows us to express uncertain sources as views over a virtual mediated uncertain database. Next, we define consistency of a set of uncertain sources and show intractability of consistency-checking. We identify an interesting special case for which consistency-checking is polynomial. Finally, the notion of certain answers from traditional LAV data integration does not generalize to the uncertain setting, so we define a corresponding notion of correct answers.

#index 1523892
#* Identifying, attributing and describing spatial bursts
#@ Michael Mathioudakis;Nilesh Bansal;Nick Koudas
#t 2010
#c 4
#% 210347
#% 279755
#% 410276
#% 413869
#% 479957
#% 577220
#% 729943
#% 824666
#% 847161
#% 881458
#% 989601
#% 1055707
#% 1077150
#! User generated content that appears on weblogs, wikis and social networks has been increasing at an unprecedented rate. The wealth of information produced by individuals from different geographical locations presents a challenging task of intelligent processing. In this paper, we introduce a methodology to identify notable geographically focused events out of this collection of user generated information. At the heart of our proposal lie efficient algorithms that identify geographically focused information bursts, attribute them to demographic factors and identify sets of descriptive keywords. We present the results of a prototype evaluation of our algorithms on BlogScope, a large-scale social media warehousing platform. We demonstrate the scalability and practical utility of our proposal running on top of a multi-terabyte text collection.

#index 1523893
#* CORADD: correlation aware database designer for materialized views and indexes
#@ Hideaki Kimura;George Huo;Alexander Rasin;Samuel Madden;Stanley B. Zdonik
#t 2010
#c 4
#% 299989
#% 480158
#% 480805
#% 482100
#% 631950
#% 765455
#% 959539
#% 991230
#% 993992
#% 1015310
#% 1206647
#% 1207101
#% 1328211
#! We describe an automatic database design tool that exploits correlations between attributes when recommending materialized views (MVs) and indexes. Although there is a substantial body of related work exploring how to select an appropriate set of MVs and indexes for a given workload, none of this work has explored the effect of correlated attributes (e.g., attributes encoding related geographic information) on designs. Our tool identifies a set of MVs and secondary indexes such that correlations between the clustered attributes of the MVs and the secondary indexes are enhanced, which can dramatically improve query performance. It uses a form of Integer Linear Programming (ILP) called ILP Feedback to pick the best set of MVs and indexes for given database size constraints. We compare our tool with a state-of-the-art commercial database designer on two workloads, APB-1 and SSB (Star Schema Benchmark---similar to TPC-H). Our results show that a correlation-aware database designer can improve query performance up to 6 times within the same space budget when compared to a commercial database designer.

#index 1523894
#* Regret-minimizing representative databases
#@ Danupon Nanongkai;Atish Das Sarma;Ashwin Lall;Richard J. Lipton;Jun Xu
#t 2010
#c 4
#% 1436
#% 124159
#% 289148
#% 300180
#% 333951
#% 382586
#% 875012
#% 903013
#% 1075132
#% 1114539
#% 1131307
#% 1181269
#% 1206643
#% 1206819
#% 1211644
#% 1328160
#% 1400780
#% 1671751
#% 1688273
#% 1914457
#! We propose the k-representative regret minimization query (k-regret) as an operation to support multi-criteria decision making. Like top-k, the k-regret query assumes that users have some utility or scoring functions; however, it never asks the users to provide such functions. Like skyline, it filters out a set of interesting points from a potentially large database based on the users' criteria; however, it never overwhelms the users by outputting too many tuples. In particular, for any number k and any class of utility functions, the k-regret query outputs k tuples from the database and tries to minimize the maximum regret ratio. This captures how disappointed a user could be had she seen k representative tuples instead of the whole database. We focus on the class of linear utility functions, which is widely applicable. The first challenge of this approach is that it is not clear if the maximum regret ratio would be small, or even bounded. We answer this question affirmatively. Theoretically, we prove that the maximum regret ratio can be bounded and this bound is independent of the database size. Moreover, our extensive experiments on real and synthetic datasets suggest that in practice the maximum regret ratio is reasonably small. Additionally, algorithms developed in this paper are practical as they run in linear time in the size of the database and the experiments show that their running time is small when they run on top of the skyline operation which means that these algorithm could be integrated into current database systems.

#index 1523895
#* An access cost-aware approach for object retrieval over multiple sources
#@ Benjamin Arai;Gautam Das;Dimitrios Gunopulos;Vagelis Hristidis;Nick Koudas
#t 2010
#c 4
#% 172898
#% 194246
#% 229827
#% 230432
#% 262063
#% 277483
#% 282422
#% 333854
#% 337046
#% 429747
#% 481748
#% 654443
#% 674200
#% 720198
#% 745472
#% 772028
#% 778475
#% 800509
#% 830692
#% 960286
#% 1002142
#% 1016183
#% 1206832
#% 1206906
#! Source and object selection and retrieval from large multi-source data sets are fundamental operations in many applications. In this paper, we initiate research on efficient source (e.g., database) and object selection algorithms on large multi-source data sets. Specifically, in order to acquire a specified number of satisfying objects with minimum cost over multiple databases, the query engine needs to determine the access overhead for individual data sources, the overhead of retrieving objects from each source, and possibly other statistics such as estimating the frequency of finding a satisfying object in order to determine how many objects to retrieve from each data source. We adopt a probabilistic approach to source selection utilizing a cost structure and a dynamic programming model for computing the optimal number of objects to retrieve from each data source. Such a structure can be a valuable asset where there is a monetary or time related cost associated with accessing large distributed databases. We present a thorough experimental evaluation to validate our techniques using real-world data sets.

#index 1523896
#* On the stability of plan costs and the costs of plan stability
#@ M. Abhirama;Sourjya Bhaumik;Atreyee Dey;Harsh Shrimal;Jayant R. Haritsa
#t 2010
#c 4
#% 248793
#% 273694
#% 411554
#% 465167
#% 480803
#% 765456
#% 810016
#% 810017
#% 1015318
#% 1022292
#% 1196764
#! Predicate selectivity estimates are subject to considerable run-time variation relative to their compile-time estimates, often leading to poor plan choices that cause inflated response times. We present here a parametrized family of plan generation and selection algorithms that replace, whenever feasible, the optimizer's solely cost-conscious choice with an alternative plan that is (a) guaranteed to be near-optimal in the absence of selectivity estimation errors, and (b) likely to deliver comparatively stable performance in the presence of arbitrary errors. These algorithms have been implemented within the PostgreSQL optimizer, and their performance evaluated on a rich spectrum of TPC-H and TPC-DS-based query templates in a variety of database environments. Our experimental results indicate that it is indeed possible to identify robust plan choices that substantially curtail the adverse effects of erroneous selectivity estimates. In fact, the plan selection quality provided by our algorithms is often competitive with those obtained through apriori knowledge of the plan search and optimality spaces. The additional computational overheads incurred by the replacement approach are miniscule in comparison to the expected savings in query execution times. We also demonstrate that with appropriate parameter choices, it is feasible to directly produce anorexic plan diagrams, a potent objective in query optimizer design.

#index 1523897
#* Xplus: a SQL-tuning-aware query optimizer
#@ Herodotos Herodotou;Shivnath Babu
#t 2010
#c 4
#% 32889
#% 172902
#% 300167
#% 411554
#% 480803
#% 480811
#% 481121
#% 571294
#% 765456
#% 810016
#% 810017
#% 903014
#% 1026989
#% 1127441
#% 1190272
#% 1206942
#% 1206952
#% 1247794
#% 1328213
#! The need to improve a suboptimal execution plan picked by the query optimizer for a repeatedly run SQL query arises routinely. Complex expressions, skewed or correlated data, and changing conditions can cause the optimizer to make mistakes. For example, the optimizer may pick a poor join order, overlook an important index, use a nested-loop join when a hash join would have done better, or cause an expensive, but avoidable, sort to happen. SQL tuning is also needed while tuning multi-tier services to meet service-level objectives. The difficulty of SQL tuning can be lessened considerably if users and higher-level tuning tools can tell the optimizer: "I am not satisfied with the performance of the plan p being used for the query Q that runs repeatedly. Can you generate a (δ%) better plan?" This paper designs, implements, and evaluates Xplus which, to our knowledge, is the first query optimizer to provide this feature. Xplus goes beyond the traditional plan-first-execute-next approach: Xplus runs some (sub)plans proactively, collects monitoring data from the runs, and iterates. A nontrivial challenge is in choosing a small set of plans to run. Xplus guides this process efficiently using an extensible architecture comprising SQL-tuning experts with different goals, and a policy to arbitrate among the experts. We show the effectiveness of Xplus on real-life tuning scenarios created using TPC-H queries on a PostgreSQL database.

#index 1523898
#* Graph homomorphism revisited for graph matching
#@ Wenfei Fan;Jianzhong Li;Shuai Ma;Hongzhi Wang;Yinghui Wu
#t 2010
#c 4
#% 156856
#% 175642
#% 255137
#% 281245
#% 300176
#% 319876
#% 341672
#% 378391
#% 408396
#% 410276
#% 502766
#% 593696
#% 660001
#% 729974
#% 787098
#% 810072
#% 824692
#% 844425
#% 881567
#% 1019453
#% 1026963
#% 1206699
#% 1206703
#% 1328183
#! In a variety of emerging applications one needs to decide whether a graph G matches another Gp, i.e., whether G has a topological structure similar to that of Gp. The traditional notions of graph homomorphism and isomorphism often fall short of capturing the structural similarity in these applications. This paper studies revisions of these notions, providing a full treatment from complexity to algorithms. (1) We propose p-homomorphism (p-hom) and 1-1 p-hom, which extend graph homomorphism and subgraph isomorphism, respectively, by mapping edges from one graph to paths in another, and by measuring the similarity of nodes. (2) We introduce metrics to measure graph similarity, and several optimization problems for p-hom and 1-1 p-hom. (3) We show that the decision problems for p-hom and 1-1 p-hom are NP-complete even for DAGs, and that the optimization problems are approximation-hard. (4) Nevertheless, we provide approximation algorithms with provable guarantees on match quality. We experimentally verify the effectiveness of the revised notions and the efficiency of our algorithms in Web site matching, using real-life and synthetic data.

#index 1523899
#* SigMatch: fast and scalable multi-pattern matching
#@ Ramakrishnan Kandhan;Nikhil Teletia;Jignesh M. Patel
#t 2010
#c 4
#% 195379
#% 320454
#% 321327
#% 322884
#% 490588
#% 571888
#% 909019
#% 910581
#% 963786
#% 1016219
#% 1022227
#% 1049084
#% 1063481
#% 1063530
#% 1079039
#% 1084475
#% 1111952
#% 1206665
#! Multi-pattern matching involves matching a data item against a large database of "signature" patterns. Existing algorithms for multi-pattern matching do not scale well as the size of the signature database increases. In this paper, we present sigMatch -- a fast, versatile, and scalable technique for multi-pattern signature matching. At its heart, sigMatch organizes the signature database into a (processor) cache-efficient q-gram index structure, called the sigTree. The sigTree groups patterns based on common sub-patterns, such that signatures that don't match can be quickly eliminated from the matching process. The sigTree also uses parallel Bloom filters and a technique to reduce imbalances across groups, for improved performance. Using extensive empirical evaluation across three diverse domains, we show that sigMatch often outperforms existing methods by an order of magnitude or more.

#index 1523900
#* SAPPER: subgraph indexing and approximate matching in large graphs
#@ Shijie Zhang;Jiong Yang;Wei Jin
#t 2010
#c 4
#% 76635
#% 288990
#% 300120
#% 322884
#% 481290
#% 749435
#% 765429
#% 772884
#% 833960
#% 864425
#% 960305
#% 1127380
#% 1181229
#% 1181231
#% 1181253
#% 1206703
#% 1249012
#% 1387890
#% 1717545
#! With the emergence of new applications, e.g., computational biology, new software engineering techniques, social networks, etc., more data is in the form of graphs. Locating occurrences of a query graph in a large database graph is an important research topic. Due to the existence of noise (e.g., missing edges) in the large database graph, we investigate the problem of approximate subgraph indexing, i.e., finding the occurrences of a query graph in a large database graph with (possible) missing edges. The SAPPER method is proposed to solve this problem. Utilizing the hybrid neighborhood unit structures in the index, SAPPER takes advantage of pre-generated random spanning trees and a carefully designed graph enumeration order. Real and synthetic data sets are employed to demonstrate the efficiency and scalability of our approximate subgraph indexing method.

#index 1523901
#* Tree indexing on solid state drives
#@ Yinan Li;Bingsheng He;Robin Jun Yang;Qiong Luo;Ke Yi
#t 2010
#c 4
#% 131555
#% 208047
#% 317933
#% 479470
#% 479483
#% 479974
#% 481304
#% 829901
#% 951778
#% 960238
#% 1016185
#% 1063551
#% 1092670
#% 1092673
#% 1127968
#% 1207002
#% 1213385
#% 1217151
#% 1217152
#% 1328139
#! Large flash disks, or solid state drives (SSDs), have become an attractive alternative to magnetic hard disks, due to their high random read performance, low energy consumption and other features. However, writes, especially small random writes, on flash disks are inherently much slower than reads because of the erase-before-write mechanism. To address this asymmetry of read-write speeds in tree indexing on the flash disk, we propose FD-tree, a tree index designed with the logarithmic method and fractional cascading techniques. With the logarithmic method, an FD-tree consists of the head tree -- a small B+-tree on the top, and a few levels of sorted runs of increasing sizes at the bottom. This design is write-optimized for the flash disk; in particular, an index search will potentially go through more levels or visit more nodes, but random writes are limited to a small area -- the head tree, and are subsequently transformed into sequential ones through merging into the lower runs. With the fractional cascading technique, we store pointers, called fences, in lower level runs to speed up the search. Given an FD-tree of n entries, we analytically show that it performs an update in O(logB n) sequential I/Os and completes a search in O(logB n) random I/Os, where B is the flash page size. We evaluate FD-tree in comparison with representative B+-tree variants under a variety of workloads on three commodity flash SSDs. Our results show that FD-tree has a similar search performance to the standard B+-tree, and a similar update performance to the write-optimized B+-tree variant. As a result, FD-tree dominates the other B+-tree index variants on the overall performance on flash disks as well as on magnetic disks.

#index 1523902
#* Efficient B-tree based indexing for cloud data processing
#@ Sai Wu;Dawei Jiang;Beng Chin Ooi;Kun-Lung Wu
#t 2010
#c 4
#% 115661
#% 280510
#% 337046
#% 340176
#% 723279
#% 723445
#% 805457
#% 824706
#% 874970
#% 960252
#% 978404
#% 998842
#% 998845
#% 1127398
#% 1426551
#% 1449177
#! A Cloud may be seen as a type of flexible computing infrastructure consisting of many compute nodes, where resizable computing capacities can be provided to different customers. To fully harness the power of the Cloud, efficient data management is needed to handle huge volumes of data and support a large number of concurrent end users. To achieve that, a scalable and high-throughput indexing scheme is generally required. Such an indexing scheme must not only incur a low maintenance cost but also support parallel search to improve scalability. In this paper, we present a novel, scalable B+-tree based indexing scheme for efficient data processing in the Cloud. Our approach can be summarized as follows. First, we build a local B+-tree index for each compute node which only indexes data residing on the node. Second, we organize the compute nodes as a structured overlay and publish a portion of the local B+-tree nodes to the overlay for efficient query processing. Finally, we propose an adaptive algorithm to select the published B+-tree nodes according to query patterns. We conduct extensive experiments on Amazon's EC2, and the results demonstrate that our indexing scheme is dynamic, efficient and scalable.

#index 1523903
#* Trie-join: efficient trie-based string similarity joins with edit-distance constraints
#@ Jiannan Wang;Jianhua Feng;Guoliang Li
#t 2010
#c 4
#% 95721
#% 333679
#% 480654
#% 654467
#% 765463
#% 824678
#% 864392
#% 893164
#% 956506
#% 1022227
#% 1055684
#% 1127425
#% 1190092
#% 1206665
#% 1206677
#% 1217179
#% 1217200
#! A string similarity join finds similar pairs between two collections of strings. It is an essential operation in many applications, such as data integration and cleaning, and has attracted significant attention recently. In this paper, we study string similarity joins with edit-distance constraints. Existing methods usually employ a filter-and-refine framework and have the following disadvantages: (1) They are inefficient for the data sets with short strings (the average string length is no larger than 30); (2) They involve large indexes; (3) They are expensive to support dynamic update of data sets. To address these problems, we propose a novel framework called trie-join, which can generate results efficiently with small indexes. We use a trie structure to index the strings and utilize the trie structure to efficiently find the similar string pairs based on subtrie pruning. We devise efficient trie-join algorithms and pruning techniques to achieve high performance. Our method can be easily extended to support dynamic update of data sets efficiently. Experimental results show that our algorithms outperform state-of-the-art methods by an order of magnitude on three real data sets with short strings.

#index 1523904
#* VoR-tree: R-trees with Voronoi diagrams for efficient processing of spatial nearest neighbor queries
#@ Mehdi Sharifzadeh;Cyrus Shahabi
#t 2010
#c 4
#% 121114
#% 201876
#% 287466
#% 300163
#% 427199
#% 462239
#% 465055
#% 465167
#% 479473
#% 527191
#% 714255
#% 714622
#% 784513
#% 806212
#% 814650
#% 893150
#% 1016191
#% 1016199
#! A very important class of spatial queries consists of nearest-neighbor (NN) query and its variations. Many studies in the past decade utilize R-trees as their underlying index structures to address NN queries efficiently. The general approach is to use R-tree in two phases. First, R-tree's hierarchical structure is used to quickly arrive to the neighborhood of the result set. Second, the R-tree nodes intersecting with the local neighborhood (Search Region) of an initial answer are investigated to find all the members of the result set. While R-trees are very efficient for the first phase, they usually result in the unnecessary investigation of many nodes that none or only a small subset of their including points belongs to the actual result set. On the other hand, several recent studies showed that the Voronoi diagrams are extremely efficient in exploring an NN search region, while due to lack of an efficient access method, their arrival to this region is slow. In this paper, we propose a new index structure, termed VoR-Tree that incorporates Voronoi diagrams into R-tree, benefiting from the best of both worlds. The coarse granule rectangle nodes of R-tree enable us to get to the search region in logarithmic time while the fine granule polygons of Voronoi diagram allow us to efficiently tile or cover the region and find the result. Utilizing VoR-Tree, we propose efficient algorithms for various Nearest Neighbor queries, and show that our algorithms have better I/O complexity than their best competitors.

#index 1523905
#* Efficient RkNN retrieval with arbitrary non-metric similarity measures
#@ P. Deepak;Prasad M. Deshpande
#t 2010
#c 4
#% 300163
#% 321455
#% 427199
#% 451645
#% 465009
#% 479462
#% 487887
#% 643566
#% 730019
#% 838510
#% 875013
#% 893128
#% 975024
#% 1016191
#% 1022226
#% 1044465
#% 1068965
#% 1107566
#% 1123924
#% 1181288
#% 1206695
#% 1218744
#% 1727538
#! A RkNN query returns all objects whose nearest k neighbors contain the query object. In this paper, we consider RkNN query processing in the case where the distances between attribute values are not necessarily metric. Dissimilarities between objects could then be a monotonic aggregate of dissimilarities between their values, such aggregation functions being specified at query time. We outline real world cases that motivate RkNN processing in such scenarios. We consider the AL-Tree index and its applicability in RkNN query processing. We develop an approach that exploits the group level reasoning enabled by the AL-Tree in RkNN processing. We evaluate our approach against a Naive approach that performs sequential scans on contiguous data and an improved block-based approach that we provide. We use real-world datasets and synthetic data with varying characteristics for our experiments. This extensive empirical evaluation shows that our approach is better than existing methods in terms of computational and disk access costs, leading to significantly better response times.

#index 1523906
#* Efficient skyline evaluation over partially ordered domains
#@ Shiming Zhang;Nikos Mamoulis;David W. Cheung;Ben Kao
#t 2010
#c 4
#% 86950
#% 277097
#% 480671
#% 731407
#% 806212
#% 810024
#% 824697
#% 864452
#% 903013
#% 993954
#% 1022203
#% 1022225
#% 1022226
#% 1044463
#% 1063485
#% 1063487
#% 1077150
#% 1083667
#% 1092017
#% 1207004
#% 1217145
#% 1217183
#% 1217185
#% 1221417
#% 1272026
#% 1328160
#% 1650354
#% 1688273
#! Although there has been a considerable body of work on skyline evaluation in multidimensional data with totally ordered attribute domains, there are only a few methods that consider attributes with partially ordered domains. Existing work maps each partially ordered domain to a total order and then adapts algorithms for totally-ordered domains to solve the problem. Nevertheless these methods either use stronger notions of dominance, which generate false positives, or require expensive dominance checks. In this paper, we propose two new methods, which do not have these drawbacks. The first method uses an appropriate mapping of a partial order to a total order, inspired by the lattice theorem and an off-the-shelf skyline algorithm. The second technique uses an appropriate storage and indexing approach, inspired by column stores, which enables efficient verification of whether a pair of objects are incompatible. We demonstrate that both our methods are up to an order of magnitude more efficient than previous work and scale well with different problem parameters, such as complexity of partial orders.

#index 1523907
#* Achieving high output quality under limited resources through structure-based spilling in XML streams
#@ Mingzhu Wei;Elke A. Rundensteiner;Murali Mani
#t 2010
#c 4
#% 263479
#% 397407
#% 413563
#% 462235
#% 480822
#% 570880
#% 573639
#% 654476
#% 654477
#% 659999
#% 730045
#% 745488
#% 788325
#% 824721
#% 993950
#% 1015276
#% 1015280
#% 1016258
#% 1044484
#% 1055757
#! Because of high volumes and unpredictable arrival rates, stream processing systems are not always able to keep up with input data - resulting in buffer overflow and uncontrolled loss of data. To produce eventually complete results, load spilling, which pushes some fractions of data to disks temporarily, is commonly employed in relational stream engines. In this work, we now introduce "structure-based spilling", a spilling technique customized for XML streams by considering the partial spillage of possibly complex XML elements. Such structure-based spilling brings new challenges. When a path is spilled, multiple paths may be affected. We analyze possible spilling effects on the query paths and how to execute the "reduced" query to produce partial results. To select the reduced query that maximizes output quality, we develop three optimization strategies, namely, OptR, OptPrune and ToX. We also examine the clean-up stage to guarantee that an entire result set is eventually generated by producing supplementary results. Our experimental study demonstrates that our proposed solutions consistently achieve higher quality results compared to the state-of-the-art techniques.

#index 1523908
#* Dynamic join optimization in multi-hop wireless sensor networks
#@ Svilen R. Mihaylov;Marie Jacob;Zachary G. Ives;Sudipto Guha
#t 2010
#c 4
#% 462207
#% 505869
#% 654482
#% 720835
#% 731096
#% 731480
#% 809256
#% 822531
#% 824715
#% 864436
#% 870326
#% 878299
#% 960277
#% 982571
#% 1022258
#% 1083759
#% 1394365
#% 1716961
#! To enable smart environments and self-tuning data centers, we are developing the Aspen system for integrating physical sensor data, as well as stream data coming from machine logical state, and database or Web data from the Internet. A key component of this system is a query processor optimized for limited-bandwidth, possibly battery-powered devices with multiple hop wireless radio communications. This query processor is given a portion of a data integration query, possibly including joins among sensors, to execute. Several recent papers have developed techniques for computing joins in sensors, but these techniques are static and are only appropriate for specific join selectivity ratios. We consider the problem of dynamic join optimization for sensor networks, developing solutions that employ cost modeling, as well as adaptive learning and self-tuning heuristics to choose the best algorithm under real and variable selectivity values. We focus on in-network join computation, but our architecture extends to other approaches (and we compare against these). We develop basic techniques assuming selectivities are uniform and known in advance, and optimization can be done on a pairwise basis; we then extend the work to handle joins between multiple pairs, when selectivities are not fully known. We experimentally validate our work at scale using standard datasets.

#index 1523909
#* Database-support for continuous prediction queries over streaming data
#@ Mert Akdere;Uǧur Çetintemel;Eli Upfal
#t 2010
#c 4
#% 44876
#% 351595
#% 458550
#% 479821
#% 481288
#% 541077
#% 716892
#% 824476
#% 926881
#% 947436
#% 960292
#% 1022261
#% 1127434
#% 1206772
#% 1206877
#% 1272302
#% 1650691
#! Prediction is emerging as an essential ingredient for real-time monitoring, planning and decision support applications such as intrusion detection, e-commerce pricing and automated resource management. This paper presents a system that efficiently supports continuous prediction queries (CPQs) over streaming data using seamlessly-integrated probabilistic models. Specifically, we describe how to execute and optimize CPQs using discrete (Dynamic) Bayesian Networks as the underlying predictive model. Our primary contribution is a novel cost-based optimization framework that employs materialization, sharing, and model-specific optimization techniques to enable highly-efficient point- and range-based CPQ execution. Furthermore, we support efficient execution of top-k and threshold-based high probability queries. We characterize the behavior of our system and demonstrate significant performance gains using a prototype implementation operating on real-world network intrusion data and deployed as part of a real-time software-performance monitoring system.

#index 1523910
#* Conditioning and aggregating uncertain data streams: going beyond expectations
#@ Thanh T. L. Tran;Andrew McGregor;Yanlei Diao;Liping Peng;Anna Liu
#t 2010
#c 4
#% 893167
#% 960257
#% 991156
#% 992830
#% 1016178
#% 1063521
#% 1092012
#% 1206735
#% 1206770
#% 1206772
#% 1206879
#% 1291116
#% 1426515
#% 1655707
#! Uncertain data streams are increasingly common in real-world deployments and monitoring applications require the evaluation of complex queries on such streams. In this paper, we consider complex queries involving conditioning (e.g., selections and group by's) and aggregation operations on uncertain data streams. To characterize the uncertainty of answers to these queries, one generally has to compute the full probability distribution of each operation used in the query. Computing distributions of aggregates given conditioned tuple distributions is a hard, unsolved problem. Our work employs a new evaluation framework that includes a general data model, approximation metrics, and approximate representations. Within this framework we design fast data-stream algorithms, both deterministic and randomized, for returning approximate distributions with bounded errors as answers to those complex queries. Our experimental results demonstrate the accuracy and efficiency of our approximation techniques and offer insights into the strengths and limitations of deterministic and randomized algorithms.

#index 1523911
#* TRAMP: understanding the behavior of schema mappings through provenance
#@ Boris Glavic;Gustavo Alonso;Renée J. Miller;Laura M. Haas
#t 2010
#c 4
#% 333988
#% 378409
#% 384978
#% 480134
#% 572314
#% 632016
#% 800499
#% 825661
#% 826032
#% 893094
#% 893095
#% 910590
#% 976987
#% 1127370
#% 1206612
#% 1206823
#% 1206861
#% 1217186
#% 1217196
#% 1231247
#% 1328076
#% 1328193
#! Though partially automated, developing schema mappings remains a complex and potentially error-prone task. In this paper, we present TRAMP (TRAnsformation Mapping Provenance), an extensive suite of tools supporting the debugging and tracing of schema mappings and transformation queries. TRAMP combines and extends data provenance with two novel notions, transformation provenance and mapping provenance, to explain the relationship between transformed data and those transformations and mappings that produced that data. In addition we provide query support for transformations, data, and all forms of provenance. We formally define transformation and mapping provenance, present an efficient implementation of both forms of provenance, and evaluate the resulting system through extensive experiments.

#index 1523912
#* Entity resolution with evolving rules
#@ Steven Euijong Whang;Hector Garcia-Molina
#t 2010
#c 4
#% 201889
#% 210182
#% 232768
#% 296738
#% 310516
#% 328186
#% 329790
#% 464056
#% 766199
#% 800590
#% 913783
#% 1015261
#% 1077150
#% 1201863
#% 1217163
#% 1309312
#% 1333820
#! Entity resolution (ER) identifies database records that refer to the same real world entity. In practice, ER is not a one-time process, but is constantly improved as the data, schema and application are better understood. We address the problem of keeping the ER result up-to-date when the ER logic "evolves" frequently. A naïve approach that re-runs ER from scratch may not be tolerable for resolving large datasets. This paper investigates when and how we can instead exploit previous "materialized" ER results to save redundant work with evolved logic. We introduce algorithm properties that facilitate evolution, and we propose efficient rule evolution techniques for two clustering ER models: match-based clustering and distance-based clustering. Using real data sets, we illustrate the cost of materializations and the potential gains over the naïve approach.

#index 1523913
#* Annotating and searching web tables using entities, types and relationships
#@ Girija Limaye;Sunita Sarawagi;Soumen Chakrabarti
#t 2010
#c 4
#% 122671
#% 406493
#% 577318
#% 829043
#% 869535
#% 870896
#% 939944
#% 956564
#% 1019082
#% 1022234
#% 1127393
#% 1130858
#% 1166537
#% 1214667
#% 1328133
#% 1417383
#! Tables are a universal idiom to present relational data. Billions of tables on Web pages express entity references, attributes and relationships. This representation of relational world knowledge is usually considerably better than completely unstructured, free-format text. At the same time, unlike manually-created knowledge bases, relational information mined from "organic" Web tables need not be constrained by availability of precious editorial time. Unfortunately, in the absence of any formal, uniform schema imposed on Web tables, Web search cannot take advantage of these high-quality sources of relational information. In this paper we propose new machine learning techniques to annotate table cells with entities that they likely mention, table columns with types from which entities are drawn for cells in the column, and relations that pairs of table columns seek to express. We propose a new graphical model for making all these labeling decisions for each table simultaneously, rather than make separate local decisions for entities, types and relations. Experiments using the YAGO catalog, DB-Pedia, tables from Wikipedia, and over 25 million HTML tables from a 500 million page Web crawl uniformly show the superiority of our approach. We also evaluate the impact of better annotations on a prototype relational Web search tool. We demonstrate clear benefits of our annotations beyond indexing tables in a purely textual manner.

#index 1523914
#* Interesting-phrase mining for ad-hoc text analytics
#@ Srikanta Bedathur;Klaus Berberich;Jens Dittrich;Nikos Mamoulis;Gerhard Weikum
#t 2010
#c 4
#% 300120
#% 329537
#% 577220
#% 809250
#% 814952
#% 824732
#% 844306
#% 857482
#% 867054
#% 967452
#% 1019106
#% 1022338
#% 1035573
#% 1077150
#% 1127403
#% 1130807
#% 1166508
#% 1176884
#% 1206650
#% 1214671
#% 1390329
#! Large text corpora with news, customer mail and reports, or Web 2.0 contributions offer a great potential for enhancing business-intelligence applications. We propose a framework for performing text analytics on such data in a versatile, efficient, and scalable manner. While much of the prior literature has emphasized mining keywords or tags in blogs or social-tagging communities, we emphasize the analysis of interesting phrases. These include named entities, important quotations, market slogans, and other multi-word phrases that are prominent in a dynamically derived ad-hoc subset of the corpus, e.g., being frequent in the subset but relatively infrequent in the overall corpus. We develop preprocessing and indexing methods for phrases, paired with new search techniques for the top-k most interesting phrases in ad-hoc subsets of the corpus. Our framework is evaluated using a large-scale real-world corpus of New York Times news articles.

#index 1523915
#* Global detection of complex copying relationships between sources
#@ Xin Luna Dong;Laure Berti-Equille;Yifan Hu;Divesh Srivastava
#t 2010
#c 4
#% 115608
#% 654447
#% 875066
#% 913783
#% 989682
#% 990305
#% 1328155
#% 1328156
#% 1491640
#% 1523915
#! Web technologies have enabled data sharing between sources but also simplified copying (and often publishing without proper attribution). The copying relationships can be complex: some sources copy from multiple sources on different subsets of data; some co-copy from the same source, and some transitively copy from another. Understanding such copying relationships is desirable both for business purposes and for improving many key components in data integration, such as resolving conflicts across various sources, reconciling distinct references to the same real-world entity, and efficiently answering queries over multiple sources. Recent works have studied how to detect copying between a pair of sources, but the techniques can fall short in the presence of complex copying relationships. In this paper we describe techniques that discover global copying relationships between a set of structured sources. Towards this goal we make two contributions. First, we propose a global detection algorithm that identifies co-copying and transitive copying, returning only source pairs with direct copying. Second, global detection requires accurate decisions on copying direction; we significantly improve over previous techniques on this by considering various types of evidence for copying and correlation of copying on different data items. Experimental results on real-world data and synthetic data show high effectiveness and efficiency of our techniques.

#index 1523916
#* Fragments and loose associations: respecting privacy in data publishing
#@ Sabrina De Capitani di Vimercati;Sara Foresti;Sushil Jajodia;Stefano Paraboschi;Pierangela Samarati
#t 2010
#c 4
#% 443463
#% 659992
#% 864406
#% 864412
#% 874988
#% 893100
#% 1127417
#% 1425698
#! We propose a modeling of the problem of privacy-compliant data publishing that captures confidentiality constraints on one side and visibility requirements on the other side. Confidentiality constraints express the fact that some attributes, or associations among them, are sensitive and cannot be released. Visibility requirements express requests for views over data that should be provided. We propose a solution based on data fragmentation to split sensitive associations while ensuring visibility. In addition, we show how sensitive associations broken by fragmentation can be released in a sanitized form as loose associations formed in a way to guarantee a specified degree of privacy.

#index 1523917
#* NET-FLi: on-the-fly compression, archiving and indexing of streaming network traffic
#@ Francesco Fusco;Marc Ph. Stoecklin;Michail Vlachos
#t 2010
#c 4
#% 479973
#% 571294
#% 654497
#% 654510
#% 762054
#% 786632
#% 793798
#% 799465
#% 800526
#% 801255
#% 821934
#% 824697
#% 864446
#% 866981
#% 982572
#% 1016131
#% 1022281
#% 1022302
#% 1054227
#% 1055819
#% 1089604
#% 1131088
#% 1207030
#% 1307165
#% 1372701
#! The ever-increasing number of intrusions in public and commercial networks has created the need for high-speed archival solutions that continuously store streaming network data to enable forensic analysis and auditing. However, "turning back the clock" for post-attack analyses is not a trivial task. The first major challenge is that the solution has to sustain data archiving under extremely high-speed insertion rates. Moreover, the archives created need to be stored in a format that is compressed but still amenable to indexing. The above requirements make general-purpose databases unsuitable for this task, and, thus, dedicated solutions are required. In this paper, we describe a prototype solution that satisfies all requirements for high-speed archival storage, indexing and data querying on network flow information. The superior performance of our approach is attributed to the on-the-fly compression and indexing scheme, which is based on compressed bitmap principles. Typical commercial solutions can currently process 20,000--60,000 flows per second. An evaluation of our prototype implementation on current commodity hardware using real-world traffic traces shows its ability to sustain insertion rates ranging from 500,000 to more than 1 million records per second. The system offers interactive query response times that enable administrators to perform complex analysis tasks on-the-fly. Our technique is directly amenable to parallel execution, allowing its application in domains that are challenged by large volumes of historical measurement data, such as network auditing, traffic behavior analysis and large-scale data visualization in service provider networks.

#index 1523918
#* From a stream of relational queries to distributed stream processing
#@ Qiong Zou;Huayong Wang;Robert Soulé;Martin Hirzel;Henrique Andrade;Buǧra Gedik;Kun-Lung Wu
#t 2010
#c 4
#% 115661
#% 136740
#% 211087
#% 248014
#% 273945
#% 286967
#% 378391
#% 397393
#% 398631
#% 442700
#% 442832
#% 461922
#% 783783
#% 787552
#% 803668
#% 875006
#% 878299
#% 995806
#% 1022302
#% 1023420
#% 1044492
#% 1063555
#% 1292005
#% 1292551
#% 1301044
#% 1372716
#% 1401643
#% 1719564
#! Applications from several domains are now being written to process live data originating from hardware and software-based streaming sources. Many of these applications have been written relying solely on database and data warehouse technologies, despite their lack of need for transactional support and ACID properties. In several extreme high-load cases, this approach does not scale to the processing speeds that these applications demand. In this paper we demonstrate an application acceleration approach whereby a regular ODBC-based application is converted into a true streaming application with minimal disruption from a software engineering standpoint. We showcase our approach on three real-world applications. We experimentally demonstrate the substantial performance improvements that can be observed when contrasting the accelerated implementation with the original database-oriented implementation.

#index 1523919
#* UASMAs (universal automated SNP mapping algorithms): a set of algorithms to instantaneously map SNPs in real time to aid functional SNP discovery
#@ James T. L. Mah;Danny C. C. Poo;Shaojiang Cai
#t 2010
#c 4
#% 287434
#% 289010
#% 325324
#% 593970
#% 751623
#% 754755
#% 835496
#% 1044404
#% 1130873
#% 1231157
#! Currently, submission of new SNP entries into SNP repositories such as dbSNP by NCBI is done by manual curation. This gives rise to errors and ambiguities in SNP data entries. Due to the exponential increase in SNP discovery, there is a necessity to create algorithms to accurately and rapidly map SNPs as they are discovered in real time and depositing these entries automatically into a central SNP database. UASMAs are a set of algorithms to instantaneously map SNPs efficiently and accurately by their unique chromosome position in real time. It is the result of integration of structures and algorithms in state of the art alignment methods MAQ, BWT-SW, Bowtie, SOAP2 and BWA. Using BLAST employed by NCBI as benchmark where recall was at most 91%, recall performance of components Bowtie and BWA were much better at up to 99% for longer reads. Similarly, Bowtie and BWA performed better in terms of precision at greater than 91% whereas BLAST was only 78--88%. BLAST performed poorly in terms of recall and precision for longer reads. Bowtie and BWA algorithms in UASMAs were superior in terms of performances in alignment of longer sequences and locating the precise chromosome position of any SNP with respect to the NCBI reference assembly. Results obtained are fast, instantaneous and accurate. Using UASMAs prove to be fast and optimal in mapping new variants onto the genome in view of depositing these entries accurately into a central database. Because it is done in real-time and with increased accuracy, recall and precision, the database created will be complete, up-to-date and devoid of ambiguities and redundancies.

#index 1523920
#* FlashStore: high throughput persistent key-value store
#@ Biplob Debnath;Sudipta Sengupta;Jin Li
#t 2010
#c 4
#% 131555
#% 252608
#% 340175
#% 764572
#% 829901
#% 922958
#% 951778
#% 963436
#% 978505
#% 985754
#% 998845
#% 1053488
#% 1053490
#% 1068006
#% 1085291
#% 1091961
#% 1127391
#% 1127428
#% 1174228
#% 1174229
#% 1189345
#% 1213385
#% 1217152
#% 1278373
#% 1468512
#% 1468535
#! We present FlashStore, a high throughput persistent key-value store, that uses flash memory as a non-volatile cache between RAM and hard disk. FlashStore is designed to store the working set of key-value pairs on flash and use one flash read per key lookup. As the working set changes over time, space is made for the current working set by destaging recently unused key-value pairs to hard disk and recycling pages in the flash store. FlashStore organizes key-value pairs in a log-structure on flash to exploit faster sequential write performance. It uses an in-memory hash table to index them, with hash collisions resolved by a variant of cuckoo hashing. The in-memory hash table stores compact key signatures instead of full keys so as to strike tradeoffs between RAM usage and false flash read operations. FlashStore can be used as a high throughput persistent key-value storage layer for a broad range of server class applications. We compare FlashStore with BerkeleyDB, an embedded key-value store application, running on hard disk and flash separately, so as to bring out the performance gain of FlashStore in not only using flash as a cache above hard disk but also in its use of flash aware algorithms. We use real-world data traces from two data center applications, namely, Xbox LIVE Primetime online multi-player game and inline storage deduplication, to drive and evaluate the design of FlashStore on traditional and low power server platforms. FlashStore outperforms BerkeleyDB by up to 60x on throughput (ops/sec), up to 50x on energy efficiency (ops/Joule), and up to 85x on cost efficiency (ops/sec/dollar) on the evaluated datasets.

#index 1523921
#* MEET DB2: automated database migration evaluation
#@ Reynold S. Xin;William McLaren;Patrick Dantressangle;Steve Schormann;Sam Lightstone;Maria Schwenger
#t 2010
#c 4
#% 85568
#% 189630
#% 278619
#% 318040
#% 322880
#% 526811
#% 607454
#! Commercial databases compete for market share, which is composed of not only net-new sales to those purchasing a database for the first time, but also competitive "win-backs" and migrations. Database migration, or the act of moving both application code and its underlying database platform from one database to another, presents a serious administrative and application development challenge fraught with large manual costs. Migration is typically a high cost effort due to incompatibilities between database platforms. Incompatibilities are caused most often by product specific extensions to language support, procedural logic, DDL, and administrative interfaces. The migration evaluation is the first step in any competitive database migration process. Historically this has been a manual process, with the high costs and subjective results. This has led us to reexamine traditional practices and explore an automatic, innovative solution. We have designed and implemented the Migration Evaluation and Enablement Tool for DB2 for Linux Unix and Windows, or MEET DB2, a tool for automatically evaluating database migration projects. Encapsulated in a simple one-click interface, MEET DB2 is able to provide detailed evaluation of migration complexity based on its deep analysis on the source database. In this paper, we present MEET DB2, and discuss many aspects of our design, and report measurements from real-world use cases. In particular, we show a novel way to use XML and XQuery in this domain for better extensibility and interoperability. We have evaluated MEET DB2 on 18 source code samples, covering nearly 1 million lines of code. The utility has provided benefits in several dimensions including: dramatically reduced time for evaluation, consistency, improved accuracy over human analysis, improved reporting, reduced skill requirements for migration analysis, and clear analytics for product planning.

#index 1523922
#* SSD bufferpool extensions for database systems
#@ Mustafa Canim;George A. Mihaila;Bishwaranjan Bhattacharjee;Kenneth A. Ross;Christian A. Lang
#t 2010
#c 4
#% 237690
#% 281422
#% 343083
#% 393844
#% 402709
#% 562524
#% 654495
#% 753247
#% 753279
#% 808514
#% 830700
#% 884456
#% 913748
#% 963442
#% 1053457
#% 1213385
#% 1306948
#% 1328052
#! High-end solid state disks (SSDs) provide much faster access to data compared to conventional hard disk drives. We present a technique for using solid-state storage as a caching layer between RAM and hard disks in database management systems. By caching data that is accessed frequently, disk I/O is reduced. For random I/O, the potential performance gains are particularly significant. Our system continuously monitors the disk access patterns to identify hot regions of the disk. Temperature statistics are maintained at the granularity of an extent, i.e., 32 pages, and are kept current through an aging mechanism. Unlike prior caching methods, once the SSD is populated with pages from warm regions cold pages are not admitted into the cache, leading to low levels of cache pollution. Simulations based on DB2 I/O traces, and a prototype implementation within DB2 both show substantial performance improvements.

#index 1523923
#* DataGarage: warehousing massive performance data on commodity servers
#@ Charles Loboz;Slawek Smyl;Suman Nath
#t 2010
#c 4
#% 464177
#% 875026
#% 963669
#% 978404
#% 983467
#% 1063553
#% 1127559
#% 1217159
#% 1328117
#% 1328186
#% 1426516
#! Contemporary datacenters house tens of thousands of servers. The servers are closely monitored for operating conditions and utilizations by collecting their performance data (e.g., CPU utilization). In this paper, we show that existing database and file-system solutions are not suitable for warehousing performance data collected from a large number of servers because of the scale and the complexity of performance data. We describe the design and implementation of DataGarage, a performance data warehousing system that we have developed at Microsoft. DataGarage is a hybrid solution that combines benefits of DBMSs, file-systems, and MapReduce systems to address unique challenges of warehousing performance data. We describe how DataGarage allows efficient storage and analysis of years of historical performance data collected from many tens of thousands of servers---on commodity servers. We also report DataGarage's performance with a real dataset and a 32-node, 256-core shared-nothing cluster and our experience of using DataGarage at Microsoft for the last one year.

#index 1523924
#* Cheetah: a high performance, custom data warehouse on top of MapReduce
#@ Songting Chen
#t 2010
#c 4
#% 397353
#% 464215
#% 480821
#% 572311
#% 576761
#% 659996
#% 875022
#% 875026
#% 893159
#% 960326
#% 963669
#% 978404
#% 1022248
#% 1063542
#% 1328060
#% 1328095
#% 1426584
#! Large-scale data analysis has become increasingly important for many enterprises. Recently, a new distributed computing paradigm, called MapReduce, and its open source implementation Hadoop, has been widely adopted due to its impressive scalability and flexibility to handle structured as well as unstructured data. In this paper, we describe our data warehouse system, called Cheetah, built on top of MapReduce. Cheetah is designed specifically for our online advertising application to allow various simplifications and custom optimizations. First, we take a fresh look at the data warehouse schema design. In particular, we define a virtual view on top of the common star or snowflake data warehouse schema. This virtual view abstraction not only allows us to design a SQL-like but much more succinct query language, but also makes it easier to support many advanced query processing features. Next, we describe a stack of optimization techniques ranging from data compression and access method to multi-query optimization and exploiting materialized views. In fact, each node with commodity hardware in our cluster is able to process raw data at 1GBytes/s. Lastly, we show how to seamlessly integrate Cheetah into any ad-hoc MapReduce jobs. This allows MapReduce developers to fully leverage the power of both MapReduce and data warehouse technologies.

#index 1523925
#* Distance-based outlier detection: consolidation and renewed bearing
#@ Gustavo H. Orair;Carlos H. C. Teixeira;Wagner Meira, Jr.;Ye Wang;Srinivasan Parthasarathy
#t 2010
#c 4
#% 201876
#% 201893
#% 210173
#% 248790
#% 300136
#% 300183
#% 338606
#% 420081
#% 478624
#% 479986
#% 481281
#% 577250
#% 727871
#% 729912
#% 1019141
#% 1022241
#% 1023422
#% 1051998
#% 1267765
#! Detecting outliers in data is an important problem with interesting applications in a myriad of domains ranging from data cleaning to financial fraud detection and from network intrusion detection to clinical diagnosis of diseases. Over the last decade of research, distance-based outlier detection algorithms have emerged as a viable, scalable, parameter-free alternative to the more traditional statistical approaches. In this paper we assess several distance-based outlier detection approaches and evaluate them. We begin by surveying and examining the design landscape of extant approaches, while identifying key design decisions of such approaches. We then implement an outlier detection framework and conduct a factorial design experiment to understand the pros and cons of various optimizations proposed by us as well as those proposed in the literature, both independently and in conjunction with one another, on a diverse set of real-life datasets. To the best of our knowledge this is the first such study in the literature. The outcome of this study is a family of state of the art distance-based outlier detection algorithms. Our detailed empirical study supports the following observations. The combination of optimization strategies enables significant efficiency gains. Our factorial design study highlights the important fact that no single optimization or combination of optimizations (factors) always dominates on all types of data. Our study also allows us to characterize when a certain combination of optimizations is likely to prevail and helps provide interesting and useful insights for moving forward in this domain.

#index 1523926
#* Adaptive logging for mobile device
#@ Young-Seok Kim;Heegyu Jin;Kyoung-Gu Woo
#t 2010
#c 4
#% 114582
#% 287725
#% 319549
#% 340700
#% 386623
#% 403195
#% 445788
#% 893215
#% 951778
#% 978392
#% 1085305
#% 1249221
#% 1328150
#! Nowadays, due to the increased user requirements of the fast and reliable data management operation for mobile applications, major device vendors use embedded DBMS for their mobile devices such as MP3 players, mobile phones, digital cameras and PDAs. However, database logging is the major bottleneck against the fast response time. There has been a lot of work minimizing logging overhead but no single recovery method provides the best performance to a variety of database workloads. In this paper, we present a novel recovery method called adaptive logging which can switch the logging method from ARIES to shadow paging adaptively at a page level according to the update state of each page on run time. Also, we propose a log compaction method called deferred logging which removes redundant logs by deferring to create log records until the updated data page is flushed or until the transaction commits. Deferred logging is coupled with adaptive logging seamlessly so that it boosts the performance of adaptive logging by reducing the typical overhead of hybrid methods. We have implemented the proposed approaches to our embedded DBMS which was deployed to more than 10 million mobile devices and evaluated them through a real world application on a mobile device. The result shows that our approaches can reduce logging overhead significantly and consequently can improve the response time of both small update transaction and large update transaction effectively.

#index 1523927
#* RoadTrack: scaling location updates for mobile clients on road networks with query awareness
#@ Peter Pesti;Ling Liu;Bhuvan Bamba;Arun Iyengar;Matt Weber
#t 2010
#c 4
#% 442615
#% 800186
#% 810048
#% 871045
#% 889142
#% 1015321
#% 1016199
#! Mobile commerce and location based services (LBS) are some of the fastest growing IT industries in the last five years. Location update of mobile clients is a fundamental capability in mobile commerce and all types of LBS. Higher update frequency leads to higher accuracy, but incurs unacceptably high cost of location management at the location servers. We propose RoadTrack -- a road-network based, query-aware location update framework with two unique features. First, we introduce the concept of precincts to control the granularity of location update resolution for mobile clients that are not of interest to any active location query services. Second, we define query encounter points for mobile objects that are targets of active location query services, and utilize these encounter points to define the adequate location update schedule for each mobile. The RoadTrack framework offers three unique advantages. First, encounter points as a fundamental query awareness mechanism enable us to control and differentiate location update strategies for mobile clients in the vicinity of active location queries, while meeting the needs of location query evaluation. Second, we employ system-defined precincts to manage the desired spatial resolution of location updates for different mobile clients and to control the scope of query awareness to be capitalized by a location update strategy. Third, our road-network based check-free interval optimization further enhances the effectiveness of the Road-Track query-aware location update scheduling algorithm. This optimization provides significant cost reduction for location update management at both mobile clients and location servers. We evaluate the RoadTrack location update approach using a real world road-network based mobility simulator. Our experimental results demonstrate that the RoadTrack query aware location update approach outperforms existing representative location update strategies in terms of both client energy efficiency and server processing load.

#index 1523928
#* Confucius and its intelligent disciples: integrating social with search
#@ Xiance Si;Edward Y. Chang;Zoltán Gyöngyi;Maosong Sun
#t 2010
#c 4
#% 290830
#% 340932
#% 348163
#% 722904
#% 815902
#% 838398
#% 854791
#% 985829
#% 987235
#% 989659
#% 1035587
#% 1074110
#% 1251646
#% 1399976
#% 1558463
#% 1650298
#! Q&A sites continue to flourish as a large number of users rely on them as useful substitutes for incomplete or missing search results. In this paper, we present our experience with developing Confucius, a Google Q&A service launched in 21 countries and four languages by the end of 2009. Confucius employs six data mining subroutines to harness synergy between web search and social networks. We present these subroutines' design goals, algorithms, and their effects on service quality. We also describe techniques for and experience with scaling the subroutines to mine massive data sets.

#index 1523929
#* The Picasso database query optimizer visualizer
#@ Jayant R. Haritsa
#t 2010
#c 4
#% 248793
#% 273694
#% 378414
#% 463444
#% 480803
#% 824756
#% 993945
#% 1015318
#% 1022292
#% 1026989
#% 1523896
#! Modern database systems employ a query optimizer module to automatically identify the most efficient strategies for executing the declarative SQL queries submitted by users. The efficiency of these strategies, called "plans", is measured in terms of "costs" that are indicative of query response times. Optimization is a mandatory exercise since the difference between the costs of the best execution plan, and a random choice, could be in orders of magnitude. The role of query optimizers has become especially critical during this decade due to the high degree of processing complexity characterizing current data warehousing and mining applications, as exemplified by the TPC-H and TPC-DS decision support benchmarks [20, 21].

#index 1523930
#* CODS: evolving data efficiently and scalably in column oriented databases
#@ Ziyang Liu;Sivaramakrishnan Natarajan;Bin He;Hui-I Hsiao;Yi Chen
#t 2010
#c 4
#% 824697
#% 866981
#% 875026
#% 1127411
#% 1127421
#! Database evolution is the process of updating the schema of a database or data warehouse (schema evolution) and evolving the data to the updated schema (data evolution). Database evolution is often necessitated in relational databases due to the changes of data or workload, the suboptimal initial schema design, or the availability of new knowledge of the database. It involves two steps: updating the database schema, and evolving the data to the new schema. Despite the capability of commercial RDBMSs to well optimize query processing, evolving the data during a database evolution through SQL queries is shown to be prohibitively costly. We designed and developed CODS, a platform for efficient data level data evolution in column oriented databases, which evolves the data to the new schema without materializing query results or unnecessary compression/decompression as occurred in traditional query level approaches. CODS ameliorates the efficiency of data evolution by orders of magnitude compared with commercial or open source RDBMSs.

#index 1523931
#* Efficient event processing through reconfigurable hardware for algorithmic trading
#@ Mohammad Sadoghi;Martin Labrecque;Harsh Singh;Warren Shum;Hans-Arno Jacobsen
#t 2010
#c 4
#% 271199
#% 333938
#% 974551
#% 1282628
#% 1328128
#% 1328185
#! In this demo, we present fpga-ToPSS (Toronto Publish/Subscribe System Family), an efficient event processing platform for high-frequency and low-latency algorithmic trading. Our event processing platform is built over reconfigurable hardware---FPGAs---to achieve line-rate processing. Furthermore, our event processing engine supports Boolean expression matching with an expressive predicate language that models complex financial strategies to autonomously buy and sell stocks based on real-time financial data.

#index 1523932
#* CareDB: a context and preference-aware location-based database system
#@ Justin J. Levandoski;Mohamed F. Mokbel;Mohamed E. Khalefa
#t 2010
#c 4
#% 342767
#% 479816
#% 875012
#% 1022242
#% 1688273
#! We demonstrate CareDB, a context and preference-aware database system. CareDB provides scalable personalized location-based services to users based on their preferences and current surrounding context. Unlike existing location-based database systems that answer queries based solely on proximity in distance, CareDB considers user preferences and various types of context in determining the answer to location-based queries. To this end, CareDB does not aim to define new location-based queries, instead, it aims to redefine the answer of existing location-based queries. To achieve its goals, CareDB has several distinguishing characteristics that revolve around a generic and extensible preference and context-aware query processing framework that addresses (a) scalable, efficient preference joins, (b) gracefully handling contextual attributes that are expensive to derive, and (c) support for uncertain attributes.

#index 1523933
#* Cloudy: a modular cloud storage system
#@ Donald Kossmann;Tim Kraska;Simon Loesing;Stephan Merkli;Raman Mittal;Flavio Pfaffhauser
#t 2010
#c 4
#% 998845
#% 1016169
#% 1328130
#! This demonstration presents Cloudy, a modular cloud storage system. Cloudy provides a highly flexible architecture for distributed data storage and is designed to operate with multiple workloads. Based on a generic data model, Cloudy can be customized to meet application requirements. The goal of this demonstration is to show the ability of Cloudy to efficiently process different query languages, and to automatically adapt to varying load scenarios.

#index 1523934
#* Geospatial stream query processing using Microsoft SQL Server StreamInsight
#@ Seyed Jalal Kazemitabar;Ugur Demiryurek;Mohamed Ali;Afsin Akdogan;Cyrus Shahabi
#t 2010
#c 4
#% 1328078
#% 1328129
#! Microsoft SQL Server spatial libraries contain several components that handle geometrical and geographical data types. With advances in geo-sensing technologies, there has been an increasing demand for geospatial streaming applications. Microsoft SQL Server StreamInsight (StreamInsight, for brevity) is a platform for developing and deploying streaming applications that run continuous queries over high-rate streaming events. With its extensibility infrastructure, StreamInsight enables developers to integrate their domain expertise within the query pipeline in the form of user defined modules. This demo utilizes the extensibility infrastructure in Microsoft StreamInsight to leverage its continuous query processing capabilities in two directions. The first direction integrates SQL spatial libraries into the continuous query pipeline of StreamInsight. StreamInsight provides a well-defined temporal model over incoming events while SQL spatial libraries cover the spatial properties of events to deliver a solution for spatiotemporal stream query processing. The second direction extends the system with an analytical refinement and prediction layer. This layer analyzes historical data that has been accumulated and summarized over the years to refine, smooth and adjust the current query output as well as predict the output in the near future. The demo scenario is based on transportation data in Los Angeles County.

#index 1523935
#* Using XMorph to transform XML data
#@ Curtis E. Dyreson;Sourav S. Bhowmick;Kirankanth Mallampalli
#t 2010
#c 4
#% 322880
#% 333845
#% 458861
#% 460247
#% 476281
#% 479465
#% 810052
#% 869473
#% 1015258
#% 1016135
#% 1019058
#% 1022318
#! XMorph is a new, shape polymorphic, domain-specific XML query language. A query in a shape polymorphic language adapts to the shape of the input, freeing the user from having to know the input's shape and making the query applicable to a wide variety of differently shaped inputs. An XMorph query specifies the shape of the output. The XMorph query engine transforms the input to the desired shape by shredding an XML document to a graph of closest relationships, and performing a closeness preserving transformation. We plan to demonstrate XMorph using a Java applet, which can also be used by the audience during the demonstration, to evaluate various XMorph queries. The applet will show the output, the shapes generated by the query, and report on potential data loss in a transformation.

#index 1523936
#* Active complex event processing: applications in real-time health care
#@ Di Wang;Elke A. Rundensteiner;Han Wang;Richard T. Ellison, III
#t 2010
#c 4
#% 481448
#% 875004
#% 1217161
#! Our analysis of many real-world event based applications has revealed that existing Complex Event Processing technology (CEP), while effective for efficient pattern matching on event stream, is limited in its capability of reacting in realtime to opportunities and risks detected or environmental changes. We are the first to tackle this problem by providing active rule support embedded directly within the CEP engine, henceforth called Active Complex Event Processing technology, or short, Active CEP. We design the Active CEP model and associated rule language that allows rules to be triggered by CEP system state changes and correctly executed during the continuous query process. Moreover we design an Active CEP infrastructure, that integrates the active rule component into the CEP kernel, allowing fine-grained and optimized rule processing. We demonstrate the power of Active CEP by applying it to the development of a collaborative project with UMass Medical School, which detects potential threads of infection and reminds healthcare workers to perform hygiene precautions in real-time.

#index 1523937
#* Thirteen new players in the team: a FERRY-based LINQ to SQL provider
#@ Tom Schreiber;Simone Bonetti;Torsten Grust;Manuel Mayr;Jan Rittinger
#t 2010
#c 4
#% 5379
#% 431899
#% 997039
#% 1001176
#% 1180960
#% 1217249
#% 1523809
#! We demonstrate an efficient LINQ to SQL provider and its significant impact on the runtime performance of LINQ programs that process large data volumes. This alternative provider is based on Ferry, compilation technology that lets relational database systems participate in the evaluation of first-order functional programs over nested, ordered data structures. The Ferry-based provider seamlessly hooks into the .NET LINQ framework and generates SQL code that strictly adheres to the semantics of the LINQ data model. Ferry comes with strong code size guarantees and complete support for the LINQ Standard Query Operator family, enabling a truly interactive and compelling LINQ demonstration. A variety of inspection holes may be opened to learn about the internals of the Ferry-based LINQ to SQL provider.

#index 1523938
#* AXART: enabling collaborative work with AXML artifacts
#@ Serge Abiteboul;Pierre Bourhis;Bogdan Marinoiu;Alban Galland
#t 2010
#c 4
#% 309729
#% 770373
#% 1072645
#% 1153042
#% 1181304
#! The workflow models have been essentially operation-centric for many years, ignoring almost completely the data aspects. Recently, a new paradigm of data-centric workflows, called business artifacts, has been introduced by Nigam and Caswell. We follow this approach and propose a model where artifacts are XML documents that evolve in time due to interactions with their environment, i.e. human users or Web services. This paper proposes the AXART system as a distributed platform for collaborative work that harnesses the power of our model. We will illustrate AXART with an example taken from the movie industry. Indeed, applying for a role in a film is a typical collaborative process that involves various participants, inside and outside the film company. The demonstration scenario considers both standard workflow process and dynamic workflow modifications, based on two extension mechanisms: workflow specialization and workflow exception. The workflows, modeled using artifacts, are supported by the AXART system by combining techniques specific to active documents, like view maintenance, with security techniques to manage access rights.

#index 1523939
#* iFlow: an approach for fast and reliable Internet-scale stream processing utilizing detouring and replication
#@ Christopher McConnell;Fan Ping;Jeong-Hyon Hwang
#t 2010
#c 4
#% 241952
#% 378388
#% 575096
#% 594324
#% 765470
#% 800583
#% 810008
#% 864436
#% 993949
#% 1127396
#% 1206600
#! We propose to demonstrate iFlow, our replication-based system that supports both fast and reliable processing of data streams over the Internet. iFlow uses a low degree of replication in conjunction with detouring techniques to overcome network outages. iFlow also deploys replicas in a manner that improves performance and availability at the same time, and can cope with varying system conditions by continually migrating replicas. Based on a live network monitoring application, our demonstration will substantiate the strengths of iFlow. During the demonstration, various visual tools will provide graphical evidence of improvements with regards to availability, performance, and resource usage. To show iFlow's adaptivity, these tools will also allow us to control the demonstration situation including injecting different types of failures.

#index 1523940
#* Peer coordination through distributed triggers
#@ Verena Kantere;Maher Manoubi;Iluju Kiringa;Timos Sellis;John Mylopoulos
#t 2010
#c 4
#% 264263
#% 461859
#% 723449
#% 824769
#% 1688306
#% 1715393
#! This is a demonstration of data coordination in a peer data management system through the employment of distributed triggers. The latter express in a declarative manner individual security and consistency requirements of peers, that cannot be ensured by default in the P2P environment. Peers achieve to handle in a transparent way data changes that come from local and remote actions and events. The distributed triggers are implemented as an extension of the active functionality of a centralized commercial DBMS. The language and execution semantics of distributed triggers are integrated in the kernel of the DBMS such that the latter handles transparently and simultaneously both centralized and distributed triggers. Moreover, the management of distributed triggers is associated with a set of peer acquaintance and termination protocols which are incorporated in the centralized DBMS.

#index 1523941
#* Seaform: search-as-you-type in forms
#@ Hao Wu;Guoliang Li;Chen Li;Lizhu Zhou
#t 2010
#c 4
#% 1127406
#% 1190092
#% 1217199
#% 1217235
#! Form-style interfaces have been widely used to allow users to access information. In this demonstration paper, we develop a new search paradigm in form-style query interfaces, called Seaform (which stands for Search-As-You-Type in Forms), which computes answers on-the-fly as a user types in a query letter by letter and gives the user instant feedback. Seaform provides better user experiences compared with traditional form-based query systems by reducing the efforts for a user to compose a high-quality query to find relevant answers. Seaform can also enhance faceted search and allow users to on-the-fly explore the underlying data. This search paradigm requires high performance to achieve an interactive speed. We develop efficient techniques and use them to implement two systems on real datasets. We demonstrate the features of these systems.

#index 1523942
#* TimeTrails: a system for exploring spatio-temporal information in documents
#@ Jannik Strötgen;Michael Gertz
#t 2010
#c 4
#% 1024551
#% 1120965
#% 1358020
#% 1358036
#% 1472167
#! Spatial and temporal data have become ubiquitous in many application domains such as the Geosciences or life sciences. Sophisticated database management systems are employed to manage such structured data. However, an important source of spatio-temporal information that has not been fully utilized are unstructured text documents. In documents, combinations of temporal and spatial expressions form events, which can be mapped to a database structure and organized into trajectories that can be explored. In this context, the coupling of information retrieval techniques with spatio-temporal database concepts leads to new ways for managing and exploring document collections. In this demonstration, we present TimeTrails, a system for the extraction, querying, storage, and exploration of spatio-temporal information embedded in text documents. The user can query a document collection, and TimeTrails visualizes the spatio-temporal information extracted from relevant documents as document trajectories, resulting in a map-based view of documents. This view helps the user to explore the temporal and spatial content of documents in a meaningful way and to further restrict search results using spatial and temporal predicates.

#index 1523943
#* QUICK: expressive and flexible search over knowledge bases and text collections
#@ Jeffrey Pound;Ihab F. Ilyas;Grant Weddell
#t 2010
#c 4
#% 956564
#% 1127393
#% 1426537
#! Recent work on Web-extracted data sets has produced an interesting new source of structured Web data. These data sets can be viewed as knowledge bases (KB) -- large heterogeneous linked entity collections with millions of unique edge and node labels, often encoding rich semantic information over entities. For example, YAGO [5] and ExDB [2] have fact collections numbering in the tens and hundreds of millions respectfully, and WebTables [1] contains over one hundred million extracted relations. In terms of schema information, the ExDB, YAGO, and WebTables data sets all have schema items numbering in the millions.

#index 1523944
#* Transforming XML documents as schemas evolve
#@ Marcin Kwietniewski;Jarek Gryz;Stephanie Hazlewood;Paul Van Run
#t 2010
#c 4
#% 299942
#% 342438
#% 413582
#% 801670
#% 967465
#% 1071882
#% 1705177
#! Database systems often use XML schema to describe the format of valid XML documents. Usually, this format is determined when the system is designed. Sometimes, in an already functioning system, a need arises to change the XML schemas. In such a situation, the system has to transform the old XML documents so that they conform to the new format and that as little information as possible is lost in the process. This process is called schema evolution. We have implemented an XML schema transformation toolkit within IBM Master Data Management Server (MDM). MDM uses XML documents to describe products that an enterprise may be offering to its clients. In this work we focus on evolving schemas rather than on integrating separate or heterogeneous data sources. Our solution includes an extendible schema matching algorithm that was designed with evolving XML schemas in mind and takes advantage of hierarchical structure of XML. It also includes a data transformation and migration method appropriate for environments where migration is performed in an abstraction layer above the DBMS. Finally, we describe a novel way of extending an XSLT editor with an XSLT visualization feature to allow the user's input and evaluation of the transformation.

#index 1523945
#* XSACT: a comparison tool for structured search results
#@ Ziyang Liu;Sivaramakrishnan Natarajan;Peng Sun;Stephen Booher;Tim Meehan;Robert Winkler;Yi Chen
#t 2010
#c 4
#% 590523
#% 960261
#% 1063493
#% 1127424
#% 1328135
#! Studies show that about 50% of web search is for information exploration purpose, where a user would like to investigate, compare, evaluate, and synthesize multiple relevant results. Due to the absence of general tools that can effectively analyze and differentiate multiple results, a user has to manually read and comprehend potentially large results in an exploratory search. Such a process is time consuming, labor intensive and error prone. With meta information embedded, keyword search on structured data provides the potential for automating or semi-automating the comparison of multiple results. In this demo we present a system XSACT for differentiating search results on structured data. XSACT takes as input a set of structured results, and outputs a Differentiation Feature Set (DFS) for each result to highlight their differences within a size bound. The problem of generating DFSs with maximal differences is proved to be NP-hard. XSACT adopts efficient algorithms for DFS generation, and features a user-friendly interface that effectively interacts with the users to help them compare search results.

#index 1523946
#* ObjectRunner: lightweight, targeted extraction and querying of structured web data
#@ Talel Abdessalem;Bogdan Cautis;Nora Derouiche
#t 2010
#c 4
#% 330784
#% 480824
#% 577319
#% 654469
#% 756964
#% 788941
#% 801668
#% 805846
#% 805847
#% 881505
#% 889107
#% 956564
#% 1022260
#% 1131145
#% 1200332
#% 1394469
#! We present in this paper ObjectRunner, a system for extracting, integrating and querying structured data from the Web. Our system harvests real-world items from template-based HTML pages (the so-called structured Web). It illustrates a two-phase querying of the Web, in which an intentional description of the targeted data is first provided, in a flexible and widely applicable manner. ObjectRunner follows then a lightweight, best-effort approach, leveraging both the input description and the source structure. This process is domain-independent, in the sense that it applies to any relation, either flat or nested, describing real-world items. We advocate via our prototype that fully automatic extraction and integration of structured data can be done fast and effectively, when the redundancy of the Web meets knowledge over the to-be-extracted data. We present the technical details and the overall platform through several application scenarios on real-life Web sources.

#index 1523947
#* ROXXI: Reviving witness dOcuments to eXplore eXtracted Information
#@ Shady Elbassuoni;Katja Hose;Steffen Metzger;Ralf Schenkel
#t 2010
#c 4
#% 397130
#% 504443
#% 754068
#% 1092530
#% 1190118
#% 1206702
#% 1328083
#% 1409954
#! In recent years, there has been considerable research on information extraction and constructing RDF knowledge bases. In general, the goal is to extract all relevant information from a corpus of documents, store it into an ontology, and answer future queries based only on the created knowledge base. Thus, the original documents become dispensable. On the one hand, an ontology is a convenient and non-redundant structured source of information, based on which specific queries can be answered efficiently. On the other hand, many users doubt the correctness of facts and ontology subgraphs presented to them as query results without proof. Instead, users often wish to verify the obtained facts or subgraphs by reading about them in context, i.e., in a document relating the facts and providing background information. In this demo, we present ROXXI, a system operating on top of an existing knowledge base and reviving the abandoned witness documents. In doing so, it goes the opposite way of information extraction approaches -- starting with ontological facts and tracing their way back to the documents they were extracted from. ROXXI offers interfaces for expert users (SPARQL) as well as for non-experts (ontology browser) and provides a ranked list of documents each associated with a content snippet highlighting the queried facts in context. At the demonstration site, we will show the advantages of this novel approach towards document retrieval and illustrate the benefits of reviving the documents that information extraction approaches neglect.

#index 1523948
#* EXTRUCT: using deep structural information in XML keyword search
#@ Arash Termehchy;Marianne Winslett
#t 2010
#c 4
#% 654442
#% 810052
#% 1015258
#% 1019060
#% 1077150
#% 1127424
#% 1206957
#% 1292476
#% 1372727
#! Users who are unfamiliar with database query languages can search XML data sets using keyword queries. Previous work has shown that current XML keyword search methods, although intuitive, do not effectively use the data's structural information and provide poor precision, recall, and ranking for most queries. Based on an extension of the concept of information theory, we have developed principled frameworks called normalized total correlation (NTC) and normalized term presence correlation (NTPC) to measure the relevance of candidate answers to keyword queries. We demonstrate EXTRUCT, an XML keyword search interface that uses NTC and NTPC. An extensive empirical evaluation over two real-world XML DBs has shown that EX-TRUCT has better precision and recall and provides better ranking than all previous approaches. We demonstrate EXTRUCT, along with seven other keyword search systems for four real-world XML data sets, using prepared queries as well as queries from the audience. The demonstration shows that using deep structural information increases the effectiveness of XML keyword search systems considerably.

#index 1523949
#* SQL QueRIE recommendations
#@ Javad Akbarnejad;Gloria Chatzopoulou;Magdalini Eirinaki;Suju Koshy;Sarika Mittal;Duc On;Neoklis Polyzotis;Jothi S. Vindhiya Varman
#t 2010
#c 4
#% 243166
#% 800588
#% 1218714
#% 1267869
#% 1296950
#% 1535185
#! This demonstration presents QueRIE, a recommender system that supports interactive database exploration. This system aims at assisting non-expert users of scientific databases by tracking their querying behavior and generating personalized query recommendations. The system is supported by two recommendation engines and the underlying recommendation algorithms. The first identifies potentially "interesting" parts of the database related to the corresponding data analysis task by locating those database parts that were accessed by similar users in the past. The second identifies structurally similar queries to the ones posted by the current user. Both approaches result in a recommendation set of SQL queries that is provided to the user to modify, or directly post to the database. The demonstrated system will enable users to query and get real-time recommendations from the SkyServer database, using user traces collected from the SkyServer query log.

#index 1523950
#* P2PDocTagger: content management through automated P2P collaborative tagging
#@ Hock Hee Ang;Vivekanand Gopalkrishnan;Wee Keong Ng;Steven C. H. Hoi
#t 2010
#c 4
#% 860036
#% 1083902
#% 1178452
#% 1268038
#% 1697219
#! As the amount of user generated content grows, personal information management has become a challenging problem. Several information management approaches, such as desktop search, document organization and (collaborative) document tagging have been proposed to address this, however they are either inappropriate or inefficient. Automated collaborative document tagging approaches mitigate the problems of manual tagging, but they are usually based on centralized settings which are plagued by problems such as scalability, privacy, etc. To resolve these issues, we present P2PDocTagger, an automated and distributed document tagging system based on classification in P2P networks. P2P-DocTagger minimizes the efforts of individual peers and reduces computation and communication cost while providing high tagging accuracy, and eases of document organization/retrieval. In addition, we provide a realistic and flexible simulation toolkit -- P2PDMT, to facilitate the development and testing of P2P data mining algorithms.

#index 1523951
#* InZeit: efficiently identifying insightful time points
#@ Vinay Setty;Srikanta Bedathur;Klaus Berberich;Gerhard Weikum
#t 2010
#c 4
#% 333854
#% 577220
#% 766444
#% 818215
#% 987257
#% 1022338
#% 1058620
#% 1077150
#% 1292475
#! Web archives are useful resources to find out about the temporal evolution of persons, organizations, products, or other topics. However, even when advanced text search functionality is available, gaining insights into the temporal evolution of a topic can be a tedious task and often requires sifting through many documents. The demonstrated system named InZeit (pronounced "insight") assists users by determining insightful time points for a given query. These are the time points at which the top-k time-travel query result changes substantially and for which the user should therefore inspect query results. InZeit determines the m most insightful time points efficiently using an extended segment tree for in-memory bookkeeping.

#index 1523952
#* iAVATAR: an interactive tool for finding and visualizing visual-representative tags in image search
#@ Aixin Sun;Sourav S. Bhowmick;Yao Liu
#t 2010
#c 4
#% 397161
#% 760805
#% 967244
#% 967452
#% 1055808
#% 1074094
#% 1279647
#% 1292880
#! Tags associated with social images are valuable information source for superior image search and retrieval experiences. Due to the nature of tagging, many tags associated with images are not visually descriptive. Consequently, presence of these noisy tags may reduce the effectiveness of tags' role in image retrieval. To address this problem, we demonstrate iAvatar (interActive VisuAl-representative TAgs Relationship) system that uses the notion of Normalized Image Tag Clarity (nitc) to find visual-representative tags. A visual-representative tag effectively describes the visual content of the images. Further, we visually demonstrate relationships between popular tags and visual-representative tags as well as co-occurrence likelihood of a pair of tags associated with a search tag or image using tag relationship graph (trg). We demonstrate various innovative features of iAvatar with a real-world dataset and show that it enriches users' understanding of various important tag features during image search.

#index 1523953
#* Deep web integration with VisQI
#@ Thomas Kabisch;Eduard C. Dragut;Clement Yu;Ulf Leser
#t 2010
#c 4
#% 765409
#% 765410
#% 864433
#% 1328136
#% 1328138
#% 1683872
#! In this paper, we present VisQI (VISual Query interface Integration system), a Deep Web integration system. VisQI is capable of (1) transforming Web query interfaces into hierarchically structured representations, (2) of classifying them into application domains and (3) of matching the elements of different interfaces. Thus VisQI contains solutions for the major challenges in building Deep Web integration systems. The system comes along with a full-fledged evaluation system that automatically compares generated data structures against a gold standard. VisQI has a framework-like architecture such that other developers can reuse its components easily.

#index 1523954
#* SOLOMON: seeking the truth via copying detection
#@ Xin Luna Dong;Laure Berti-Equille;Yifan Hu;Divesh Srivastava
#t 2010
#c 4
#% 572314
#% 654447
#% 875066
#% 989682
#% 1022323
#% 1063593
#% 1063709
#% 1328103
#% 1328155
#% 1523915
#! We live in the Information Era, with access to a huge amount of information from a variety of data sources. However, data sources are of different qualities, often providing conflicting, out-of-date and incomplete data. Data sources can also easily copy, reformat and modify data from other sources, propagating erroneous data. These issues make the identification of high quality information and sources non-trivial. We demonstrate the Solomon system, whose core is a module that detects copying between sources. We demonstrate that we can effectively detect copying relationship between data sources, leverage the results in truth discovery, and provide a user-friendly interface to facilitate users in identifying sources that best suit their information needs.

#index 1523955
#* Just-in-time data integration in action
#@ Martin Hentschel;Laura Haas;Renée J. Miller
#t 2010
#c 4
#% 116303
#% 201928
#% 218197
#% 273911
#% 300167
#% 334025
#% 479452
#% 479629
#% 480499
#% 481923
#% 1022235
#% 1127370
#% 1328067
#! Today's data integration systems must be flexible enough to support the typical iterative and incremental process of integration, and may need to scale to hundreds of data sources. In this work we present a novel data integration system that offers great flexibility and scalability. Our approach to data integration is unique in that it executes mapping rules at query runtime using annotations. On top, we have built the People People People application. It allows users to search for people, display information about people, and browse through a network of related people, where the data is integrated from local and remote data sources. The demo presents all features of our underlying data integration engine through a set of motivating scenarios.

#index 1523956
#* Massively parallel data analysis with PACTs on Nephele
#@ Alexander Alexandrov;Max Heimel;Volker Markl;Dominic Battré;Fabian Hueske;Erik Nijkamp;Stephan Ewen;Odej Kao;Daniel Warneke
#t 2010
#c 4
#% 411554
#% 963669
#% 983467
#% 1292903
#% 1426486
#! Large-scale data analysis applications require processing and analyzing of Terabytes or even Petabytes of data, particularly in the areas of web analysis or scientific data management. This trend has been discussed as "web-scale data management" in a panel at VLDB 2009. Formerly, parallel data processing was the domain of parallel database systems. Today, novel requirements like scaling out to thousands of machines, improved fault-tolerance, and schema free processing have made a case for new approaches.

#index 1523957
#* Using sentinel technology in the TARGIT BI suite
#@ Morten Middelfart;Torben Bach Pedersen
#t 2010
#c 4
#% 227917
#% 463903
#% 993961
#% 1267410
#! This paper demonstrates so-called sentinels in the TARGIT BI Suite. Sentinels are a novel type of rules that can warn a user if one or more measure changes in a multi-dimensional data cube are expected to cause a change to another measure critical to the user. We present the concept of sentinels, and we explain how sentinels represent stronger and more specific rules than sequential patterns and correlation techniques. In addition, we present the algorithm, implementation, and data warehouse setup that are prerequisites for our demo. In the demo we present a dialogue where users, without any prior technical knowledge, are able to select a critical measure, a number of cubes, and a time dimension, and subsequently mine and schedule sentinels for early warnings.

#index 1523958
#* CoDA: interactive cluster based concept discovery
#@ Stephan Günnemann;Ines Färber;Hardy Kremer;Thomas Seidl
#t 2010
#c 4
#% 464291
#% 466890
#% 483342
#% 726032
#% 1117008
#% 1165480
#% 1292599
#% 1328215
#! Large data resources are ubiquitous in science and business. For these domains, an intuitive view on the data is essential to fully exploit the hidden knowledge. Often, these data can be semantically structured by concepts. Since the determination of concepts requires a thorough analysis of the data, data mining methods have to be applied. In the field of subspace clustering, some techniques have recently shown to be effective for this task. Although these methods generate concept-based patterns, the user has to provide domain knowledge to gain reasonable concepts out of the data. Our demonstration CoDA (Concept Determination and Analysis) is a tool that supports the user in the final step of concept definition. More concretely, the user is guided through an iterative, interactive process in which concepts are suggested, analyzed, and potentially refined. The core aspect of CoDA is an intuitive, concept-driven presentation of subspace clusters such that concepts can be visually captured.

#index 1523959
#* Keymantic: semantic keyword-based searching in data integration systems
#@ Sonia Bergamaschi;Elton Domnori;Francesco Guerra;Mirko Orsini;Raquel Trillo Lado;Yannis Velegrakis
#t 2010
#c 4
#% 323104
#% 1022318
#% 1174127
#% 1467763
#! We propose the demonstration of Keymantic, a system for keyword-based searching in relational databases that does not require a-priori knowledge of instances held in a database. It finds numerous applications in situations where traditional keyword-based searching techniques are inapplicable due to the unavailability of the database contents for the construction of the required indexes.

#index 1523960
#* Data Auditor: exploring data quality and semantics using pattern tableaux
#@ Lukasz Golab;Howard Karloff;Flip Korn;Divesh Srivastava
#t 2010
#c 4
#% 397369
#% 480496
#% 480499
#% 873104
#% 1022222
#% 1054480
#% 1127381
#% 1127587
#% 1328157
#! We present Data Auditor, a tool for exploring data quality and data semantics. Given a rule or an integrity constraint and a target relation, Data Auditor computes pattern tableaux, which concisely summarize subsets of the relation that (mostly) satisfy or (mostly) fail the constraint. This paper describes 1) the architecture and user interface of Data Auditor, 2) the supported constraints for testing data consistency and completeness, 3) the heuristics used by Data Auditor to "tune" a given constraint or its associated parameters for better fit with the data, and 4) several demonstration scenarios. using real data sets.

#index 1523961
#* Distributed caching platforms
#@ Anil K. Nori
#t 2010
#c 4
#! With the advances in processing, memory, and connectivity technologies, applications are becoming increasingly distributed, data-centric, and web based. These applications demand support for large number of users at high performance, requiring extreme scale, high availability, and low latency data access. Such applications are known as Extreme Transaction Processing (XTP) applications. Distributed Caching Platforms exploit the advances in the memory and networking technologies to fuse memory on multiple machines into a single unified global memory providing data access at low latencies. These Distributed Caching Platforms are evolving into in-memory data platforms for XTP applications. This tutorial provides a comprehensive overview of Distributed Caching Platform technologies and their usage.

#index 1523962
#* Big data and cloud computing: new wine or just new bottles?
#@ Divyakant Agrawal;Sudipto Das;Amr El Abbadi
#t 2010
#c 4
#% 998845
#% 1002142
#% 1063488
#% 1127560
#% 1217216
#% 1217217
#% 1328130
#% 1328131
#% 1426492
#% 1468219
#! Cloud computing is an extremely successful paradigm of service oriented computing and has revolutionized the way computing infrastructure is abstracted and used. Three most popular cloud paradigms include: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). The concept however can also be extended to Database as a Service and many more. Elasticity, pay-per-use, low upfront investment, low time to market, and transfer of risks are some of the major enabling features that make cloud computing a ubiquitous paradigm for deploying novel applications which were not economically feasible in a traditional enterprise infrastructure settings. This has seen a proliferation in the number of applications which leverage various cloud platforms, resulting in a tremendous increase in the scale of the data generated as well as consumed by such applications. Scalable database management systems (DBMS) -- both for update intensive application workloads, as well as decision support systems for descriptive and deep analytics -- are thus a critical part of cloud infrastructures.

#index 1523963
#* Techniques for similarity searching in multimedia databases
#@ Hanan Samet
#t 2010
#c 4
#% 68089
#% 68091
#% 287466
#% 578407
#% 731409
#% 800570
#% 818938
#% 1063472
#! Techniques for similarity searching in multimedia databases are reviewed. This includes a discussion of the curse of dimensionality, as well as multidimensional indexing, distance-based indexing, and the actual search process which is realized by nearest neighbor finding.

#index 1523964
#* Event processing: past, present and future
#@ Opher Etzion
#t 2010
#c 4
#% 286516
#% 394417
#% 445881
#% 654507
#% 753711
#% 788216
#% 800959
#% 1209497
#% 1209764
#% 1248250
#% 1269313
#% 1501222
#! Analysts have marked Event Processing as the most growing segment in enterprise computing during years 2008 and 2009, furthermore, this trend is expected to continue. Many of the large and medium software companies (IBM, Oracle, Microsoft, Sybase, Progress Software, Software AG and TIBCO) are now offering event processing products as well as a collection of smaller companies. Other indications for the emerging nature of this area are: extensive coverage by analysts as a separate area, the establishment of a dedicated research community with an annual conference (DEBS), and the establishment of a consortium that includes vendors and academic people as EPTS (Event Processing Technical Society) http://www.ep-ts.com/. The early event processing commercial products were mostly descendents of research projects rooted in multiple disciplines, some of them are data management disciplines (active databases, data stream management, temporal databases) and some are rooted in other areas (discrete event simulation, distributed computing, formal verification). The tutorial is intended for a technical audience that is interested in deep dive into understanding event processing. The audience will gain insights about event processing: What it really means? Where does it come from? How does it relate to research concepts (e.g. stream computing) as well as enterprise computing terms (e.g. Business Rules Management Systems)? The audience will also gain insights into the current state of the art, the leading architectures, the basic building blocks of event processing, and the various programming styles exemplified by code examples. Last but not least, the audience will gain insights about the current trends, and the research challenges that exist, this part will be based on the discussions in the Event Processing Dagstuhl seminar that was held in May 2010 http://www.dagstuhl.de/de/programm/kalender/semhp/?semnr=10201. The tutorial slides are available for public viewing on slideshare http://www.slideshare.net/search/slideshow?q=opher+etzion+%2B+vldb2010+tutorial The current generation of event processing products [14] has been preceded by several research projects in the 1990-ies: Rapide in Stanford [15], Infospheres in Cal Tech [2], Apama in Cambridge University [6] and Amit in IBM Haifa Research Lab [1]. In later phase there were some research project that have taken the stream oriented approach such as the Stanford Stream project [3] and Aurora [5]. A collection of start-up companies, many of them descendents of these projects have emerged, some survived, and some did not. From those who survived some were acquired by bigger companies. Event processing as a research discipline has multiple ancestors. In the database area it is a descendent of work done in active databases [19], temporal databases [12], and data stream management systems [9]. Other ancestors are inference rules, discrete event simulation, and distributed computing (pub/sub).

#index 1523965
#* Similarity search and mining in uncertain databases
#@ Matthias Renz;Reynold Cheng;Hans-Peter Kriegel
#t 2010
#c 4
#% 32879
#% 654487
#% 823402
#% 824728
#% 864396
#% 885364
#% 893189
#% 915307
#% 992830
#% 1016202
#% 1022203
#% 1030777
#% 1030778
#% 1063485
#% 1194584
#% 1196011
#% 1206717
#% 1206893
#% 1206905
#% 1211651
#% 1214633
#% 1217181
#% 1328151
#% 1380967
#% 1393138
#% 1408794
#% 1411036
#% 1669490
#! Managing, searching and mining uncertain data has achieved much attention in the database community recently due to new sensor technologies and new ways of collecting data. There is a number of challenges in terms of collecting, modelling, representing, querying, indexing and mining uncertain data. In its scope, the diversity of approaches addressing these topics is very high because the underlying assumptions of uncertainty are different across different papers. This tutorial provides a comprehensive and comparative overview of general techniques for the key topics in the fields of querying, indexing and mining uncertain data. In particular, it identifies the most generic types of probabilistic similarity queries and discusses general algorithmic methods to answer such queries efficiently. In addition, the tutorial sketches probabilistic methods for important data mining applications in the context of uncertain data with special emphasis on probabilistic clustering and probabilistic pattern mining. The intended audience of this tutorial ranges from novice researchers to advanced experts as well as practitioners from any application domain dealing with uncertain data retrieval and mining.

#index 1523966
#* Data management and mining in internet ad systems
#@ S. Muthukrishnan
#t 2010
#c 4
#% 868445
#% 1105335
#% 1336429
#! New systems produce data that often present new challenges for data management and mining problems. For example, inventory and sales data led to emphasis on data mining problems such as association rule mining; analysis of Internet Packet traffic logs led to data stream management systems; and, growing markup publication systems led to challenges addressed by semi-structured data management.

#index 1523967
#* Cloud databases: what's new?
#@ Daniel J. Abadi;Michael Carey;Surajit Chaudhuri;Hector Garcia-Molina;Jignesh M. Patel;Raghu Ramakrishnan
#t 2010
#c 4
#! The panelists will discuss what characterizes data management in the cloud, and how this differs from the broad range of applications that conventional database management systems have supported over the past few decades. They will examine whether we need to develop new technologies to address demonstrably new challenges, or whether we can largely re-position existing systems and approaches. The discussion will cover data analysis in the cloud using Map-Reduce based systems such as Hadoop, and cloud data serving (and so-called "No SQL" systems).

#index 1523968
#* Time for our field to grow up
#@ Anastassia Ailamaki;Laura Haas;H. V. Jagadish;David Maier;Tamer Özsu;Marianne Winslett
#t 2010
#c 4
#! Compared to centuries of physics and millennia of mathematics, the 50-year-history of computer science and information management research makes us the toddlers of the scientific community. Yet during our brief existence, we've revolutionized the world and, not content with that, gone on to build and study virtual worlds. We have justly taken pride in our accomplishments, and developed our own unique way of conducting research, unlike other scientific and engineering fields. But cracks have appeared in this edifice we have built. The conference system that served us so well for our first 50 years is falling apart. Our ever-increasing population competes ever more energetically for a finite set of resources. Other scientific and engineering disciplines still think that our field equates to programming, and look down on us. While we may also look down on them, it is undeniably true that high-energy physicists get many more research dollars per capita than we do, and our computer science colleagues wonder whether all the data management problems haven't already been solved. Other departments have started to teach courses that overlap our turf. Are we our own worst enemies? Why doesn't everyone understand how important our research is? Do we have to abandon the conference system? Must we become more like the stodgy old fields of science and engineering? Or can we find our own way?

#index 1523969
#* Distributed threshold querying of general functions by a difference of monotonic representation
#@ Guy Sagy;Daniel Keren;Izchak Sharfman;Assaf Schuster
#t 2010
#c 4
#% 333854
#% 333987
#% 397378
#% 465167
#% 480671
#% 501207
#% 654488
#% 659993
#% 763708
#% 768521
#% 769909
#% 770890
#% 785340
#% 800509
#% 806212
#% 818916
#% 824704
#% 874907
#% 874994
#% 875000
#% 903013
#% 907501
#% 949205
#% 960242
#% 993954
#% 997490
#% 1063467
#% 1063739
#% 1180848
#% 1328116
#% 1698862
#! The goal of a threshold query is to detect all objects whose score exceeds a given threshold. This type of query is used in many settings, such as data mining, event triggering, and top-k selection. Often, threshold queries are performed over distributed data. Given database relations that are distributed over many nodes, an object's score is computed by aggregating the value of each attribute, applying a given scoring function over the aggregation, and thresholding the function's value. However, joining all the distributed relations to a central database might incur prohibitive overheads in bandwidth, CPU, and storage accesses. Efficient algorithms required to reduce these costs exist only for monotonic aggregation threshold queries and certain specific scoring functions. We present a novel approach for efficiently performing general distributed threshold queries. To the best of our knowledge, this is the first solution to the problem of performing such queries with general scoring functions. We first present a solution for monotonic functions, and then introduce a technique to solve for other functions by representing them as a difference of monotonic functions. Experiments with real-world data demonstrate the method's effectiveness in achieving low communication and access costs.

#index 1523970
#* On triangulation-based dense neighborhood graph discovery
#@ Nan Wang;Jingbo Zhang;Kian-Lee Tan;Anthony K. H. Tung
#t 2010
#c 4
#% 431105
#% 498852
#% 501226
#% 824711
#% 833120
#% 875468
#% 1063503
#% 1083625
#% 1124590
#% 1719564
#! This paper introduces a new definition of dense subgraph pattern, the DN -graph. DN -graph considers both the size of the substructure and the minimum level of interactions between any pair of the vertices. The mining of DN -graphs inherits the difficulty of finding clique, the fully-connected subgraphs. We thus opt for approximately locating the DN -graphs using the state-of-the-art graph triangulation methods. Our solution consists of a family of algorithms, each of which targets a different problem setting. These algorithms are iterative, and utilize repeated scans through the triangles in the graph to approximately locate the DN -graphs. Each scan on the graph triangles improves the results. Since the triangles are not physically materialized, the algorithms have small memory footprint. With our solution, the users can adopt a "pay as you go" approach. They have the flexibility to terminate the mining process once they are satisfied with the quality of the results. As a result, our algorithms can cope with semi-streaming environment where the graph edges cannot fit into main memory. Results of extensive performance study confirmed our claims.

#index 1523971
#* Graph indexing of road networks for shortest path queries with label restrictions
#@ Michael Rice;Vassilis J. Tsotras
#t 2010
#c 4
#% 415004
#% 548627
#% 813718
#% 985951
#% 1091917
#% 1102215
#% 1108060
#% 1396410
#% 1396411
#% 1412884
#% 1412885
#! The current widespread use of location-based services and GPS technologies has revived interest in very fast and scalable shortest path queries. We introduce a new shortest path query type in which dynamic constraints may be placed on the allowable set of edges that can appear on a valid shortest path (e.g., dynamically restricting the type of roads or modes of travel which may be considered in a multimodal transportation network). We formalize this problem as a specific variant of formal language constrained shortest path problems, which we call the Kleene Language Constrained Shortest Paths problem. To efficiently support this type of dynamically constrained shortest path query for large-scale datasets, we extend the hierarchical graph indexing technique known as Contraction Hierarchies. Our experimental evaluation using the North American road network dataset (with over 50 million edges) shows an average query speed and search space improvement of over 3 orders of magnitude compared to the naïve adaptation of the standard Dijkstra's algorithm to support this query type. We also show an improvement of over 2 orders of magnitude compared to the only previously-existing indexing technique which could solve this problem without additional preprocessing.

#index 1523972
#* CRIUS: user-friendly database design
#@ Li Qian;Kristen LeFevre;H. V. Jagadish
#t 2010
#c 4
#% 5379
#% 10245
#% 42401
#% 43885
#% 58367
#% 114580
#% 205246
#% 273689
#% 287754
#% 332909
#% 400530
#% 434617
#% 458762
#% 479956
#% 527113
#% 765176
#% 824697
#% 893097
#% 960234
#% 985976
#% 1127421
#! Non-technical users are increasingly adding structures to their data. This gives rise to the need for database design. However, traditional database design is deliberate and heavy-weight, requiring technical expertise that everyday users may not possess. For this reason, we propose that users of personal data management applications should be able to create and refine data structures in an ad-hoc way over time, thereby "organically" growing their schemas. For this purpose, we develop a spreadsheet-like direct manipulation interface. We show how integrity constraints can still provide value, even in this scenario of frequent schema and data modifications. We also develop a back-end database implementation to support this interface, with a design that permits schema changes at a low cost. We have folded these ideas into a system, called CRIUS, which supports a nested data model and a graphical user interface. From the user's perspective, the chief advantages of CRIUS are its support for simple schema definition and modification through an intuitive drag-and-drop interface, as well as its guidance towards user data entry based on incrementally updated data integrity. We have evaluated CRIUS by means of user studies and performance studies. The user studies indicate that 1) CRIUS makes it much easier for users to design a database, as compared to state-of-the-art GUI database design tools, and 2) CRIUS makes user data entry more efficient and less error-prone. The performance experiments show that 1) the incremental integrity update in CRIUS is very efficient, making the data entry guidance applicable and 2) the backend database implementation in CRIUS significantly improves the performance of schema update tasks, without a significant impact on other operations.

#index 1523973
#* Efficient processing of top-k spatial preference queries
#@ João B. Rocha-Junior;Akrivi Vlachou;Christos Doulkeridis;Kjetil Nørvåg
#t 2010
#c 4
#% 300163
#% 318051
#% 427199
#% 465167
#% 527189
#% 643566
#% 806212
#% 824730
#% 864451
#% 875023
#% 983263
#% 1217185
#% 1537100
#% 1720751
#! Top-k spatial preference queries return a ranked set of the k best data objects based on the scores of feature objects in their spatial neighborhood. Despite the wide range of location-based applications that rely on spatial preference queries, existing algorithms incur non-negligible processing cost resulting in high response time. The reason is that computing the score of a data object requires examining its spatial neighborhood to find the feature object with highest score. In this paper, we propose a novel technique to speed up the performance of top-k spatial preference queries. To this end, we propose a mapping of pairs of data and feature objects to a distance-score space, which in turn allows us to identify and materialize the minimal subset of pairs that is sufficient to answer any spatial preference query. Furthermore, we present a novel algorithm that improves query processing performance by avoiding examining the spatial neighborhood of the data objects during query execution. In addition, we propose an efficient algorithm for materialization and we describe useful properties that reduce the cost of maintenance. We show through extensive experiments that our approach significantly reduces the number of I/Os and execution time compared to the state-of-the-art algorithms for different setups.

#index 1523974
#* HYRISE: a main memory hybrid storage engine
#@ Martin Grund;Jens Krüger;Hasso Plattner;Alexander Zeier;Philippe Cudre-Mauroux;Samuel Madden
#t 2010
#c 4
#% 872
#% 42276
#% 69094
#% 258598
#% 286258
#% 411562
#% 442705
#% 479821
#% 480119
#% 480821
#% 765431
#% 824697
#% 990389
#% 993947
#% 993967
#% 1015289
#% 1016215
#% 1022298
#% 1129957
#% 1217145
#! In this paper, we describe a main memory hybrid database system called HYRISE, which automatically partitions tables into vertical partitions of varying widths depending on how the columns of the table are accessed. For columns accessed as a part of analytical queries (e.g., via sequential scans), narrow partitions perform better, because, when scanning a single column, cache locality is improved if the values of that column are stored contiguously. In contrast, for columns accessed as a part of OLTP-style queries, wider partitions perform better, because such transactions frequently insert, delete, update, or access many of the fields of a row, and co-locating those fields leads to better cache locality. Using a highly accurate model of cache misses, HYRISE is able to predict the performance of different partitionings, and to automatically select the best partitioning using an automated database design algorithm. We show that, on a realistic workload derived from customer applications, HYRISE can achieve a 20% to 400% performance improvement over pure all-column or all-row designs, and that it is both more scalable and produces better designs than previous vertical partitioning approaches for main memory systems.

#index 1523975
#* Update rewriting and integrity constraint maintenance in a schema evolution support system: PRISM++
#@ Carlo A. Curino;Hyun Jin Moon;Alin Deutsch;Carlo Zaniolo
#t 2010
#c 4
#% 166203
#% 248038
#% 273687
#% 286901
#% 287000
#% 303884
#% 334025
#% 378409
#% 384978
#% 480969
#% 654457
#% 824736
#% 850730
#% 864389
#% 874911
#% 945866
#% 976996
#% 1015271
#% 1015302
#% 1015303
#% 1036084
#% 1063724
#% 1127411
#% 1127421
#% 1179998
#% 1217116
#% 1699937
#% 1728154
#! Supporting legacy applications when the database schema evolves represents a long-standing challenge of practical and theoretical importance. Recent work has produced algorithms and systems that automate the process of data migration and query adaptation; however, the problems of evolving integrity constraints and supporting legacy updates under schema and integrity constraints evolution are significantly more difficult and have thus far remained unsolved. In this paper, we address this issue by introducing a formal evolution model for the database schema structure and its integrity constraints, and use it to derive update mapping techniques akin to the rewriting techniques used for queries. Thus, we (i) propose a new set of Integrity Constraints Modification Operators (ICMOs), (ii) characterize the impact on integrity constraints of structural schema changes, (iii) devise representations that enable the rewriting of updates, and (iv) develop a unified approach for query and update rewriting under constraints. We then describe the implementation of these techniques provided by our PRISM++ system. The effectiveness of PRISM++ and its enabling technology has been verified on a testbed containing evolution histories of several scientific databases and web information systems, including the Genetic DB Ensembl (410+ schema versions in 9 years), and Wikipedia (240+ schema versions in 6 years).

#index 1523976
#* SXPath: extending XPath towards spatial querying on web documents
#@ Ermelinda Oro;Massimo Ruffolo;Steffen Staab
#t 2010
#c 4
#% 93308
#% 319244
#% 331772
#% 443284
#% 582016
#% 778122
#% 801668
#% 814648
#% 824798
#% 837605
#% 889107
#% 894242
#% 902460
#% 956500
#% 993939
#% 1129529
#% 1197991
#% 1217120
#% 1273793
#% 1328069
#% 1414318
#! Querying data from presentation formats like HTML, for purposes such as information extraction, requires the consideration of tree structures as well as the consideration of spatial relationships between laid out elements. The underlying rationale is that frequently the rendering of tree structures is very involved and undergoing more frequent updates than the resulting layout structure. Therefore, in this paper, we present Spatial XPath (SXPath), an extension of XPath 1.0 that allows for inclusion of spatial navigation primitives into the language resulting in conceptually simpler queries on Web documents. The SXPath language is based on a combination of a spatial algebra with formal descriptions of XPath navigation, and maintains polynomial time combined complexity. Practical experiments demonstrate the usability of SXPath.

#index 1523977
#* Personalized privacy protection in social networks
#@ Mingxuan Yuan;Lei Chen;Philip S. Yu
#t 2010
#c 4
#% 729983
#% 801690
#% 893100
#% 956511
#% 1030876
#% 1063476
#% 1127360
#% 1127417
#% 1200862
#% 1206748
#% 1206763
#% 1318676
#% 1328173
#% 1328188
#% 1415851
#! Due to the popularity of social networks, many proposals have been proposed to protect the privacy of the networks. All these works assume that the attacks use the same background knowledge. However, in practice, different users have different privacy protect requirements. Thus, assuming the attacks with the same background knowledge does not meet the personalized privacy requirements, meanwhile, it looses the chance to achieve better utility by taking advantage of differences of users' privacy requirements. In this paper, we introduce a framework which provides privacy preserving services based on the user's personal privacy requests. Specifically, we define three levels of protection requirements based on the gradually increasing attacker's background knowledge and combine the label generalization protection and the structure protection techniques (i.e. adding noise edge or nodes) together to satisfy different users' protection requirements. We verify the effectiveness of the framework through extensive experiments.

#index 1531273
#* A probabilistic approach for automatically filling form-based web interfaces
#@ Guilherme A. Toda;Eli Cortez;Altigran S. da Silva;Edleno de Moura
#t 2010
#c 4
#% 219047
#% 333943
#% 413551
#% 447947
#% 458379
#% 464434
#% 659990
#% 740768
#% 810101
#% 864416
#% 948374
#% 1016163
#% 1250184
#% 1426569
#! In this paper we present a proposal for the implementation and evaluation of a novel method for automatically using data-rich text for filling form-based input interfaces. Our solution takes a text as input, extracts implicit data values from it and fills appropriate fields. For this task, we rely on knowledge obtained from values of previous submissions for each field, which are freely obtained from the usage of the interfaces. Our approach, called iForm, exploits features related to the content and the style of these values, which are combined through a Bayesian framework. Through extensive experimentation, we show that our approach is feasible and effective, and that it works well even when only a few previous submissions to the input interface are available.

#index 1531274
#* Output URL bidding
#@ Panagiotis Papadimitriou;Hector Garcia-Molina;Ali Dasdan;Santanu Kolay
#t 2010
#c 4
#% 281214
#% 302747
#% 818584
#% 1055677
#% 1077150
#% 1130910
#% 1190098
#% 1253021
#% 1399951
#% 1399958
#% 1407391
#% 1735205
#! Output URL bidding is a new bidding mechanism for sponsored search, where advertisers bid on search result URLs, as opposed to keywords in the input query. For example, an advertiser may want his ad to appear whenever the search result includes the sites www.imdb.com and en.wikipedia.org, instead of bidding on keywords that lead to these sites, e.g., movie titles or actor names. In this paper we study the tradeoff between the simplicity and the specification power of output bids and we explore their utility for advertisers. We first present a model to derive output bids from existing keyword bids. Then, we use the derived bids to experimentally study output bids and contrast them to input query bids. Our main results are the following: (1) Compact output bids that mix both URLs and hosts have the same specification power as more lengthy input bids; (2) Output bidding can increase the recall of relevant queries; and (3) Output and input biding can be combined into a hybrid mechanism that combines the benefits of both.

#index 1531275
#* Fast incremental and personalized PageRank
#@ Bahman Bahmani;Abdur Chowdhury;Ashish Goel
#t 2010
#c 4
#% 290830
#% 340147
#% 340932
#% 348173
#% 577328
#% 577329
#% 730089
#% 769460
#% 799636
#% 805897
#% 808358
#% 869492
#% 956521
#% 983330
#% 1019074
#% 1063716
#% 1077150
#! In this paper, we analyze the efficiency of Monte Carlo methods for incremental computation of PageRank, personalized PageRank, and similar random walk based methods (with focus on SALSA), on large-scale dynamically evolving social networks. We assume that the graph of friendships is stored in distributed shared memory, as is the case for large social networks such as Twitter. For global PageRank, we assume that the social network has n nodes, and m adversarially chosen edges arrive in a random order. We show that with a reset probability of ε, the expected total work needed to maintain an accurate estimate (using the Monte Carlo method) of the PageRank of every node at all times is [EQUATION]. This is significantly better than all known bounds for incremental PageRank. For instance, if we naively recompute the PageRanks as each edge arrives, the simple power iteration method needs [EQUATION] total time and the Monte Carlo method needs O(mn/ε) total time; both are prohibitively expensive. We also show that we can handle deletions equally efficiently. We then study the computation of the top k personalized PageRanks starting from a seed node, assuming that personalized PageRanks follow a power-law with exponent α R q ln n random walks starting from every node for large enough constant q (using the approach outlined for global PageRank), then the expected number of calls made to the distributed social network database is O(k/(R(1-α)/α)). We also present experimental results from the social networking site, Twitter, verifying our assumptions and analyses. The overall result is that this algorithm is fast enough for real-time queries over a dynamic social network.

#index 1531276
#* QSkycube: efficient skycube computation using point-based space partitioning
#@ Jongwuk Lee;Seung-won Hwang
#t 2010
#c 4
#% 465167
#% 480671
#% 654480
#% 824671
#% 824672
#% 864452
#% 875011
#% 903013
#% 912241
#% 993954
#% 1217183
#% 1372698
#% 1442464
#% 1523870
#! Skyline queries have gained considerable attention for multi-criteria analysis of large-scale datasets. However, the skyline queries are known to return too many results for high-dimensional data. To address this problem, a skycube is introduced to efficiently provide users with multiple skylines with different strengths. For efficient skycube construction, state-of-the-art algorithms amortized redundant computation among subspace skylines, or cuboids, either (1) in a bottom-up fashion with the principle of sharing result or (2) in a top-down fashion with the principle of sharing structure. However, we observed further room for optimization in both principles. This paper thus aims to design a more efficient skycube algorithm that shares multiple cuboids using more effective structures. Specifically, we first develop each principle by leveraging multiple parents and a skytree, representing recursive point-based space partitioning. We then design an efficient algorithm exploiting these principles. Experimental results demonstrate that our proposed algorithm is significantly faster than state-of-the-art skycube algorithms in extensive datasets.

#index 1531277
#* ZINC: efficient indexing for skyline computation
#@ Bin Liu;Chee-Yong Chan
#t 2010
#c 4
#% 148890
#% 654480
#% 810024
#% 1022224
#% 1022225
#% 1063487
#% 1127433
#% 1207004
#% 1697240
#! We present a new indexing method named ZINC (for Z-order Indexing with Nested Code) that supports efficient skyline computation for data with both totally and partially ordered attribute domains. The key innovation in ZINC is based on combining the strengths of the ZB-tree, which is the state-of-the-art index method for computing skylines involving totally ordered domains, with a novel, nested coding scheme that succinctly maps partial orders into total orders. An extensive performance evaluation demonstrates that ZINC significantly outperforms the state-of-the-art TSS indexing scheme for skyline queries.

#index 1538763
#* Large-scale collective entity matching
#@ Vibhor Rastogi;Nilesh Dalvi;Minos Garofalakis
#t 2011
#c 4
#% 310516
#% 457973
#% 810014
#% 810017
#% 915340
#% 937552
#% 993980
#% 1083658
#% 1206834
#% 1217163
#! There have been several recent advancements in Machine Learning community on the Entity Matching (EM) problem. However, their lack of scalability has prevented them from being applied in practical settings on large real-life datasets. Towards this end, we propose a principled framework to scale any generic EM algorithm. Our technique consists of running multiple instances of the EM algorithm on small neighborhoods of the data and passing messages across neighborhoods to construct a global solution. We prove formal properties of our framework and experimentally demonstrate the effectiveness of our approach in scaling EM algorithms.

#index 1538764
#* Automatic wrappers for large scale web extraction
#@ Nilesh Dalvi;Ravi Kumar;Mohamed Soliman
#t 2011
#c 4
#% 209021
#% 275915
#% 312860
#% 428148
#% 465919
#% 479807
#% 480824
#% 531458
#% 654469
#% 730038
#% 754068
#% 823368
#% 1127393
#% 1131145
#% 1217172
#% 1328199
#! We present a generic framework to make wrapper induction algorithms tolerant to noise in the training data. This enables us to learn wrappers in a completely unsupervised manner from automatically and cheaply obtained noisy training data, e.g., using dictionaries and regular expressions. By removing the site-level supervision that wrapper-based techniques require, we are able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques. Our system is used in production at Yahoo! and powers live applications.

#index 1538765
#* Fast sparse matrix-vector multiplication on GPUs: implications for graph mining
#@ Xintian Yang;Srinivasan Parthasarathy;P. Sadayappan
#t 2011
#c 4
#% 274612
#% 290830
#% 379325
#% 410276
#% 777354
#% 794132
#% 1047785
#% 1080078
#% 1178640
#% 1211172
#% 1213679
#% 1299131
#% 1333238
#% 1333239
#! Scaling up the sparse matrix-vector multiplication kernel on modern Graphics Processing Units (GPU) has been at the heart of numerous studies in both academia and industry. In this article we present a novel non-parametric, self-tunable, approach to data representation for computing this kernel, particularly targeting sparse matrices representing power-law graphs. Using real web graph data, we show how our representation scheme, coupled with a novel tiling algorithm, can yield significant benefits over the current state of the art GPU efforts on a number of core data mining algorithms such as PageRank, HITS and Random Walk with Restart.

#index 1538766
#* Using Paxos to build a scalable, consistent, and highly available datastore
#@ Jun Rao;Eugene J. Shekita;Sandeep Tata
#t 2011
#c 4
#% 251359
#% 289235
#% 307360
#% 411707
#% 427195
#% 462779
#% 480310
#% 723279
#% 793894
#% 938074
#% 989488
#% 998845
#% 1002142
#% 1053487
#% 1063525
#% 1127560
#% 1237219
#% 1247821
#% 1278059
#% 1278373
#% 1426589
#% 1468530
#! Spinnaker is an experimental datastore that is designed to run on a large cluster of commodity servers in a single datacenter. It features key-based range partitioning, 3-way replication, and a transactional get-put API with the option to choose either strong or timeline consistency on reads. This paper describes Spinnaker's Paxos-based replication protocol. The use of Paxos ensures that a data partition in Spinnaker will be available for reads and writes as long a majority of its replicas are alive. Unlike traditional master-slave replication, this is true regardless of the failure sequence that occurs. We show that Paxos replication can be competitive with alternatives that provide weaker consistency guarantees. Compared to an eventually consistent datastore, we show that Spinnaker can be as fast or even faster on reads and only 5% to 10% slower on writes.

#index 1538767
#* Fast set intersection in memory
#@ Bolin Ding;Arnd Christian König
#t 2011
#c 4
#% 82437
#% 250012
#% 289178
#% 290703
#% 303072
#% 333854
#% 379411
#% 510483
#% 730065
#% 1221039
#% 1224602
#% 1227711
#% 1328179
#% 1407167
#% 1667821
#% 1675941
#% 1739410
#! Set intersection is a fundamental operation in information retrieval and database systems. This paper introduces linear space data structures to represent sets such that their intersection can be computed in a worst-case efficient way. In general, given k (preprocessed) sets, with totally n elements, we will show how to compute their intersection in expected time [EQUATION], where r is the intersection size and w is the number of bits in a machine-word. In addition, we introduce a very simple version of this algorithm that has weaker asymptotic guarantees but performs even better in practice; both algorithms outperform the state of the art techniques for both synthetic and real data sets and workloads.

#index 1550748
#* Human-assisted graph search: it's okay to ask questions
#@ Aditya Parameswaran;Anish Das Sarma;Hector Garcia-Molina;Neoklis Polyzotis;Jennifer Widom
#t 2011
#c 4
#% 280840
#% 313719
#% 404719
#% 577238
#% 765409
#% 874693
#% 891559
#% 1047347
#% 1063533
#% 1063545
#% 1065099
#% 1150163
#% 1217114
#% 1264744
#% 1279817
#% 1424594
#% 1550748
#% 1692849
#! We consider the problem of human-assisted graph search: given a directed acyclic graph with some (unknown) target node(s), we consider the problem of finding the target node(s) by asking an omniscient human questions of the form "Is there a target node that is reachable from the current node?". This general problem has applications in many domains that can utilize human intelligence, including curation of hierarchies, debugging workflows, image segmentation and categorization, interactive search and filter synthesis. To our knowledge, this work provides the first formal algorithmic study of the optimization of human computation for this problem. We study various dimensions of the problem space, providing algorithms and complexity results. We also compare the performance of our algorithm against other algorithms, for the problem of webpage categorization on a real taxonomy. Our framework and algorithms can be used in the design of an optimizer for crowd-sourcing platforms such as Mechanical Turk.

#index 1550749
#* Guided data repair
#@ Mohamed Yakout;Ahmed K. Elmagarmid;Jennifer Neville;Mourad Ouzzani;Ihab F. Ilyas
#t 2011
#c 4
#% 301169
#% 342611
#% 400847
#% 480499
#% 576214
#% 577238
#% 722797
#% 810019
#% 903332
#% 1022222
#% 1022228
#% 1063533
#% 1063725
#% 1127381
#% 1206961
#% 1274894
#% 1328143
#% 1347336
#% 1426628
#% 1523810
#! In this paper we present GDR, a Guided Data Repair framework that incorporates user feedback in the cleaning process to enhance and accelerate existing automatic repair techniques while minimizing user involvement. GDR consults the user on the updates that are most likely to be beneficial in improving data quality. GDR also uses machine learning methods to identify and apply the correct updates directly to the database without the actual involvement of the user on these specific updates. To rank potential updates for consultation by the user, we first group these repairs and quantify the utility of each group using the decision-theory concept of value of information (VOI). We then apply active learning to order updates within a group based on their ability to improve the learned model. User feedback is used to repair the database and to adaptively refine the training set for the model. We empirically evaluate GDR on a real-world dataset and show significant improvement in data quality using our user guided repairing process. We also, assess the trade-off between the user efforts and the resulting data quality.

#index 1550750
#* Hyper-local, directions-based ranking of places
#@ Petros Venetis;Hector Gonzalez;Christian S. Jensen;Alon Halevy
#t 2011
#c 4
#% 330769
#% 411762
#% 615785
#% 643566
#% 661923
#% 723186
#% 728195
#% 730051
#% 784284
#% 818256
#% 836101
#% 838407
#% 870366
#% 874993
#% 948048
#% 964080
#% 1055707
#% 1089478
#% 1125717
#% 1190103
#% 1190134
#% 1523885
#% 1728806
#! Studies find that at least 20% of web queries have local intent; and the fraction of queries with local intent that originate from mobile properties may be twice as high. The emergence of standardized support for location providers in web browsers, as well as of providers of accurate locations, enables so-called hyper-local web querying where the location of a user is accurate at a much finer granularity than with IP-based positioning. This paper addresses the problem of determining the importance of points of interest, or places, in local-search results. In doing so, the paper proposes techniques that exploit logged directions queries. A query that asks for directions from a location a to a location b is taken to suggest that a user is interested in traveling to b and thus is a vote that location b is interesting. Such user-generated directions queries are particularly interesting because they are numerous and contain precise locations. Specifically, the paper proposes a framework that takes a user location and a collection of near-by places as arguments, producing a ranking of the places. The framework enables a range of aspects of directions queries to be exploited for the ranking of places, including the frequency with which places have been referred to in directions queries. Next, the paper proposes an algorithm and accompanying data structures capable of ranking places in response to hyper-local web queries. Finally, an empirical study with very large directions query logs offers insight into the potential of directions queries for the ranking of places and suggests that the proposed algorithm is suitable for use in real web search engines.

#index 1550751
#* Incrementally maintaining classification using an RDBMS
#@ M. Levent Koc;Christopher Ré
#t 2011
#c 4
#% 13016
#% 165663
#% 197394
#% 269217
#% 287432
#% 420077
#% 425047
#% 464204
#% 478625
#% 481945
#% 577238
#% 659966
#% 722797
#% 771229
#% 810014
#% 818916
#% 824749
#% 874976
#% 915700
#% 924022
#% 961152
#% 961202
#% 983905
#% 1016201
#% 1036075
#% 1055735
#% 1063521
#% 1063523
#% 1206717
#% 1206772
#% 1291123
#% 1669944
#! The proliferation of imprecise data has motivated both researchers and the database industry to push statistical techniques into relational database management systems (RDBMSes). We study strategies to maintain model-based views for a popular statistical technique, classification, inside an RDBMS in the presence of updates (to the set of training examples). We make three technical contributions: (1) A strategy that incrementally maintains classification inside an RDBMS. (2) An analysis of the above algorithm that shows that our algorithm is optimal among all deterministic algorithms (and asymptotically within a factor of 2 of a non-deterministic optimal strategy). (3) A novel hybrid-architecture based on the technical ideas that underlie the above algorithm which allows us to store only a fraction of the entities in memory. We apply our techniques to text processing, and we demonstrate that our algorithms provide an order of magnitude improvement over non-incremental approaches to classification on a variety of data sets, such as the Citeseer and DBLife.

#index 1550752
#* High-throughput transaction executions on graphics processors
#@ Bingsheng He;Jeffrey Xu Yu
#t 2011
#c 4
#% 393844
#% 765419
#% 814649
#% 874997
#% 1016214
#% 1022298
#% 1054482
#% 1063508
#% 1063543
#% 1270566
#% 1426530
#% 1426552
#% 1523855
#% 1523878
#! OLTP (On-Line Transaction Processing) is an important business system sector in various traditional and emerging online services. Due to the increasing number of users, OLTP systems require high throughput for executing tens of thousands of transactions in a short time period. Encouraged by the recent success of GPGPU (General-Purpose computation on Graphics Processors), we propose GPUTx, an OLTP engine performing high-throughput transaction executions on the GPU for in-memory databases. Compared with existing GPGPU studies usually optimizing a single task, transaction executions require handling many small tasks concurrently. Specifically, we propose the bulk execution model to group multiple transactions into a bulk and to execute the bulk on the GPU as a single task. The transactions within the bulk are executed concurrently on the GPU. We study three basic execution strategies (one with locks and the other two lock-free), and optimize them with the GPU features including the hardware support of atomic operations, the massive thread parallelism and the SPMD (Single Program Multiple Data) execution. We evaluate GPUTx on a recent NVIDIA GPU in comparison with its counterpart on a quad-core CPU. Our experimental results show that optimizations on GPUTx significantly improve the throughput, and the optimized GPUTx achieves 4-10 times higher throughput than its CPU-based counterpart on public transaction processing benchmarks.

#index 1550753
#* Distributed inference and query processing for RFID tracking and monitoring
#@ Zhao Cao;Charles Sutton;Yanlei Diao;Prashant Shenoy
#t 2011
#c 4
#% 135968
#% 751052
#% 798419
#% 824747
#% 878299
#% 879222
#% 893103
#% 951747
#% 992830
#% 1036083
#% 1063480
#% 1063523
#% 1065043
#% 1206747
#% 1206879
#% 1426506
#! In this paper, we present the design of a scalable, distributed stream processing system for RFID tracking and monitoring. Since RFID data lacks containment and location information that is key to query processing, we propose to combine location and containment inference with stream query processing in a single architecture, with inference as an enabling mechanism for high-level query processing. We further consider challenges in instantiating such a system in large distributed settings and design techniques for distributed inference and query processing. Our experimental results, using both real-world data and large synthetic traces, demonstrate the accuracy, efficiency, and scalability of our proposed techniques.

#index 1573234
#* Similarity join size estimation using locality sensitive hashing
#@ Hongrae Lee;Raymond T. Ng;Kyuseok Shim
#t 2011
#c 4
#% 190611
#% 210188
#% 249321
#% 273682
#% 273908
#% 347225
#% 616528
#% 765463
#% 864392
#% 893164
#% 956506
#% 1127368
#% 1328164
#! Similarity joins are important operations with a broad range of applications. In this paper, we study the problem of vector similarity join size estimation (VSJ). It is a generalization of the previously studied set similarity join size estimation (SSJ) problem and can handle more interesting cases such as TF-IDF vectors. One of the key challenges in similarity join size estimation is that the join size can change dramatically depending on the input similarity threshold. We propose a sampling based algorithm that uses Locality-Sensitive-Hashing (LSH). The proposed algorithm LSH-SS uses an LSH index to enable effective sampling even at high thresholds. We compare the proposed technique with random sampling and the state-of-the-art technique for SSJ (adapted to VSJ) and demonstrate LSH-SS offers more accurate estimates throughout the similarity threshold range and small variance using real-world data sets.

#index 1573235
#* Query expansion based on clustered results
#@ Ziyang Liu;Sivaramakrishnan Natarajan;Yi Chen
#t 2011
#c 4
#% 218978
#% 326522
#% 578875
#% 590523
#% 765464
#% 861248
#% 987193
#% 1063493
#% 1074081
#% 1127356
#% 1181246
#% 1181284
#% 1227584
#% 1227594
#% 1328119
#% 1328120
#% 1328135
#% 1399998
#% 1450851
#% 1482250
#% 1719582
#% 1726012
#% 1740005
#! Query expansion is a functionality of search engines that suggests a set of related queries for a user-issued keyword query. Typical corpus-driven keyword query expansion approaches return popular words in the results as expanded queries. Using these approaches, the expanded queries may correspond to a subset of possible query semantics, and thus miss relevant results. To handle ambiguous queries and exploratory queries, whose result relevance is difficult to judge, we propose a new framework for keyword query expansion: we start with clustering the results according to user specified granularity, and then generate expanded queries, such that one expanded query is generated for each cluster whose result set should ideally be the corresponding cluster. We formalize this problem and show its APX-hardness. Then we propose two efficient algorithms named iterative single-keyword refinement and partial elimination based convergence, respectively, which effectively generate a set of expanded queries from clustered results that provides a classification of the original query results. We believe our study of generating an optimal query based on the ground truth of the query results not only has applications in query expansion, but has significance for studying keyword search quality in general.

#index 1573236
#* CoPhy: a scalable, portable, and interactive index advisor for large workloads
#@ Debabrata Dash;Neoklis Polyzotis;Anastasia Ailamaki
#t 2011
#c 4
#% 757953
#% 778724
#% 810026
#% 810111
#% 824756
#% 959055
#% 1016220
#% 1022293
#% 1044461
#% 1063540
#% 1127352
#% 1206953
#% 1207101
#% 1484144
#! Index tuning, i.e., selecting the indexes appropriate for a workload, is a crucial problem in database system tuning. In this paper, we solve index tuning for large problem instances that are common in practice, e.g., thousands of queries in the workload, thousands of candidate indexes and several hard and soft constraints. Our work is the first to reveal that the index tuning problem has a well structured space of solutions, and this space can be explored efficiently with well known techniques from linear optimization. Experimental results demonstrate that our approach outperforms state-of-the-art commercial and research techniques by a significant margin (up to an order of magnitude).

#index 1573237
#* Tuffy: scaling up statistical inference in Markov logic networks using an RDBMS
#@ Feng Niu;Christopher Ré;AnHai Doan;Jude Shavlik
#t 2011
#c 4
#% 244570
#% 420495
#% 496116
#% 560768
#% 785089
#% 850430
#% 874976
#% 915340
#% 983845
#% 983882
#% 1016201
#% 1036075
#% 1055735
#% 1063521
#% 1063523
#% 1127378
#% 1206717
#% 1206772
#% 1249506
#% 1250224
#% 1250579
#% 1250584
#% 1269815
#% 1270258
#% 1270261
#% 1291123
#% 1305601
#% 1467732
#% 1499987
#% 1573237
#% 1650403
#% 1672978
#! Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems including information extraction, entity resolution, and text mining. Current implementations of MLNs do not scale to large real-world data sets, which is preventing their widespread adoption. We present Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that allows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outperforms state-of-the-art implementations in both quality and speed on several publicly available datasets.

#index 1573238
#* Automatic optimization for MapReduce programs
#@ Eaman Jahani;Michael J. Cafarella;Christopher Ré
#t 2011
#c 4
#% 336756
#% 479821
#% 824697
#% 864401
#% 875026
#% 875029
#% 903261
#% 960326
#% 963669
#% 1063553
#% 1159972
#% 1217159
#% 1278391
#% 1328186
#% 1372690
#% 1373695
#% 1468423
#% 1471595
#% 1523820
#% 1523841
#! The MapReduce distributed programming framework has become popular, despite evidence that current implementations are inefficient, requiring far more hardware than a traditional relational databases to complete similar tasks. MapReduce jobs are amenable to many traditional database query optimizations (B+Trees for selections, column-store-style techniques for projections, etc), but existing systems do not apply them, substantially because free-form user code obscures the true data operation being performed. For example, a selection in SQL is easily detected, but a selection in a MapReduce program is embedded in Java code along with lots of other program logic. We could ask the programmer to provide explicit hints about the program's data semantics, but one of MapReduce's attractions is precisely that it does not ask the user for such information. This paper covers Manimal, which automatically analyzes MapReduce programs and applies appropriate data-aware optimizations, thereby requiring no additional help at all from the programmer. We show that Manimal successfully detects optimization opportunities across a range of data operations, and that it yields speedups of up to 1,121% on previously-written MapReduce programs.

#index 1573239
#* On social-temporal group query with acquaintance constraint
#@ De-Nian Yang;Yi-Ling Chen;Wang-Chien Lee;Ming-Syan Chen
#t 2011
#c 4
#% 881460
#% 1047412
#% 1183090
#% 1214668
#% 1224056
#% 1406405
#% 1451234
#% 1512399
#% 1558532
#% 1694247
#! Three essential criteria are important for activity planning, including: (1) finding a group of attendees familiar with the initiator, (2) ensuring each attendee in the group to have tight social relations with most of the members in the group, and (3) selecting an activity period available for all attendees. Therefore, this paper proposes Social-Temporal Group Query to find the activity time and attendees with the minimum total social distance to the initiator. Moreover, this query incorporates an acquaintance constraint to avoid finding a group with mutually unfamiliar attendees. Efficient processing of the social-temporal group query is very challenging. We show that the problem is NP-hard via a proof and formulate the problem with Integer Programming. We then propose two efficient algorithms, SGSelect and STGSelect, which include effective pruning techniques and employ the idea of pivot time slots to substantially reduce the running time, for finding the optimal solutions. Experimental results indicate that the proposed algorithms are much more efficient and scalable. In the comparison of solution quality, we show that STGSelect outperforms the algorithm that represents manual coordination by the initiator.

#index 1581406
#* Synthesizing products for online catalogs
#@ Hoa Nguyen;Ariel Fuxman;Stelios Paparizos;Juliana Freire;Rakesh Agrawal
#t 2011
#c 4
#% 17185
#% 279755
#% 333990
#% 572314
#% 654458
#% 765433
#% 786511
#% 800498
#% 866989
#% 939759
#% 939794
#% 993982
#% 1083704
#% 1108847
#! A comprehensive product catalog is essential to the success of Product Search engines and shopping sites such as Yahoo! Shopping, Google Product Search, and Bing Shopping. Given the large number of products and the speed at which they are released to the market, keeping catalogs up-to-date becomes a challenging task, calling for the need of automated techniques. In this paper, we introduce the problem of product synthesis, a key component of catalog creation and maintenance. Given a set of offers advertised by merchants, the goal is to identify new products and add them to the catalog, together with their (structured) attributes. A fundamental challenge in product synthesis is the scale of the problem. A Product Search engine receives data from thousands of merchants about millions of products; the product taxonomy contains thousands of categories, where each category has a different schema; and merchants use representations for products that are different from the ones used in the catalog of the Product Search engine. We propose a system that provides an end-to-end solution to the product synthesis problem, and addresses issues involved in data extraction from offers, schema reconciliation, and data fusion. For the schema reconciliation component, we developed a novel and scalable technique for schema matching which leverages knowledge about previously-known instance-level associations between offers and products; and it is trained using automatically created training sets (no manually-labeled data is needed). We present an experimental evaluation using data from Bing Shopping for more than 800K offers, a thousand merchants, and 400 categories. The evaluation confirms that our approach is able to automatically generate a large number of accurate product specifications. Furthermore, the evaluation shows that our schema reconciliation component outperforms state-of-the-art schema matching techniques in terms of precision and recall.

#index 1581407
#* Column-oriented storage techniques for MapReduce
#@ Avrilia Floratou;Jignesh M. Patel;Eugene J. Shekita;Sandeep Tata
#t 2011
#c 4
#% 69316
#% 480821
#% 875026
#% 1023420
#% 1063542
#% 1063553
#% 1085307
#% 1217159
#% 1217169
#% 1270449
#% 1278124
#% 1328186
#% 1490163
#% 1523824
#% 1523837
#% 1523841
#% 1523924
#% 1573238
#% 1594639
#! Users of MapReduce often run into performance problems when they scale up their workloads. Many of the problems they encounter can be overcome by applying techniques learned from over three decades of research on parallel DBMSs. However, translating these techniques to a Map-Reduce implementation such as Hadoop presents unique challenges that can lead to new design choices. This paper describes how column-oriented storage techniques can be incorporated in Hadoop in a way that preserves its popular programming APIs. We show that simply using binary storage formats in Hadoop can provide a 3x performance boost over the naive use of text files. We then introduce a column-oriented storage format that is compatible with the replication and scheduling constraints of Hadoop and show that it can speed up MapReduce jobs on real workloads by an order of magnitude. We also show that dealing with complex column types such as arrays, maps, and nested records, which are common in MapReduce jobs, can incur significant CPU overhead. Finally, we introduce a novel skip list column format and lazy record construction strategy that avoids deserializing unwanted records to provide an additional 1.5x performance boost. Experiments on a real intranet crawl are used to show that our column-oriented storage techniques can improve the performance of the map phase in Hadoop by as much as two orders of magnitude.

#index 1581408
#* Implementing performance competitive logical recovery
#@ David Lomet;Kostas Tzoumas;Michael Zwilling
#t 2011
#c 4
#% 2618
#% 47623
#% 114582
#% 116061
#% 116062
#% 116063
#% 172879
#% 403195
#% 442706
#% 654474
#% 927567
#% 1328131
#% 1328214
#! New hardware platforms, e.g. cloud, multi-core, etc., have led to a reconsideration of database system architecture. Our Deuteronomy project separates transactional functionality from data management functionality, enabling a flexible response to exploiting new platforms. This separation requires, however, that recovery is described logically. In this paper, we extend current recovery methods to work in this logical setting. While this is straightforward in principle, performance is an issue. We show how ARIES style recovery optimizations can work for logical recovery where page information is not captured on the log. In side-by-side performance experiments using a common log, we compare logical recovery with a state-of-the art ARIES style recovery implementation and show that logical redo performance can be competitive.

#index 1581409
#* Personalized social recommendations: accurate or private
#@ Ashwin Machanavajjhala;Aleksandra Korolova;Atish Das Sarma
#t 2011
#c 4
#% 543555
#% 730089
#% 809424
#% 830010
#% 977011
#% 1029084
#% 1055691
#% 1072045
#% 1080074
#% 1095897
#% 1206678
#% 1214684
#% 1227602
#% 1318624
#% 1328175
#% 1399997
#% 1426571
#% 1451190
#% 1595667
#% 1668087
#% 1670071
#% 1740518
#! With the recent surge of social networks such as Facebook, new forms of recommendations have become possible -- recommendations that rely on one's social connections in order to make personalized recommendations of ads, content, products, and people. Since recommendations may use sensitive information, it is speculated that these recommendations are associated with privacy risks. The main contribution of this work is in formalizing trade-offs between accuracy and privacy of personalized social recommendations. We study whether "social recommendations", or recommendations that are solely based on a user's social network, can be made without disclosing sensitive links in the social graph. More precisely, we quantify the loss in utility when existing recommendation algorithms are modified to satisfy a strong notion of privacy, called differential privacy. We prove lower bounds on the minimum loss in utility for any recommendation algorithm that is differentially private. We then adapt two privacy preserving algorithms from the differential privacy literature to the problem of social recommendations, and analyze their performance in comparison to our lower bounds, both analytically and experimentally. We show that good private social recommendations are feasible only for a small subset of the users in the social network or for a lenient setting of privacy parameters.

#index 1581410
#* Efficient diversification of web search results
#@ Gabriele Capannini;Franco Maria Nardini;Raffaele Perego;Fabrizio Silvestri
#t 2011
#c 4
#% 262112
#% 281186
#% 411762
#% 642975
#% 805841
#% 805863
#% 879686
#% 1019076
#% 1073970
#% 1074133
#% 1096052
#% 1130868
#% 1130878
#% 1166473
#% 1190093
#% 1202162
#% 1206662
#% 1227709
#% 1280748
#% 1348342
#% 1400011
#% 1400017
#% 1400021
#% 1400099
#% 1482296
#% 1560154
#% 1697422
#! In this paper we analyze the efficiency of various search results diversification methods. While efficacy of diversification approaches has been deeply investigated in the past, response time and scalability issues have been rarely addressed. A unified framework for studying performance and feasibility of result diversification solutions is thus proposed. First we define a new methodology for detecting when, and how, query results need to be diversified. To this purpose, we rely on the concept of "query refinement" to estimate the probability of a query to be ambiguous. Then, relying on this novel ambiguity detection method, we deploy and compare on a standard test set, three different diversification methods: IASelect, xQuAD, and OptSelect. While the first two are recent state-of-the-art proposals, the latter is an original algorithm introduced in this paper. We evaluate both the efficiency and the effectiveness of our approach against its competitors by using the standard TREC Web diversification track testbed. Results shown that OptSelect is able to run two orders of magnitude faster than the two other state-of-the-art approaches and to obtain comparable figures in diversification effectiveness.

#index 1581411
#* Social content matching in MapReduce
#@ Gianmarco De Francisci Morales;Aristides Gionis;Mauro Sozio
#t 2011
#c 4
#% 142683
#% 205849
#% 261369
#% 320150
#% 341672
#% 604754
#% 1023420
#% 1035587
#% 1064986
#% 1211745
#% 1245882
#% 1318636
#% 1399956
#% 1426655
#% 1431676
#% 1433951
#% 1467704
#% 1484141
#% 1535356
#% 1665186
#! Matching problems are ubiquitous. They occur in economic markets, labor markets, internet advertising, and elsewhere. In this paper we focus on an application of matching for social media. Our goal is to distribute content from information suppliers to information consumers. We seek to maximize the overall relevance of the matched content from suppliers to consumers while regulating the overall activity, e.g., ensuring that no consumer is overwhelmed with data and that all suppliers have chances to deliver their content. We propose two matching algorithms, GreedyMR and StackMR, geared for the MapReduce paradigm. Both algorithms have provable approximation guarantees, and in practice they produce high-quality solutions. While both algorithms scale extremely well, we can show that Stack-MR requires only a poly-logarithmic number of MapReduce steps, making it an attractive option for applications with very large datasets. We experimentally show the trade-offs between quality and efficiency of our solutions on two large datasets coming from real-world social-media web sites.

#index 1592311
#* Recovering semantics of tables on the web
#@ Petros Venetis;Alon Halevy;Jayant Madhavan;Marius Paşca;Warren Shen;Fei Wu;Gengxin Miao;Chung Wu
#t 2011
#c 4
#% 116149
#% 144023
#% 376266
#% 742092
#% 756964
#% 878454
#% 939515
#% 939601
#% 956564
#% 1055735
#% 1127393
#% 1176941
#% 1227594
#% 1264778
#% 1275182
#% 1289516
#% 1305622
#% 1328133
#% 1328199
#% 1328200
#% 1328353
#% 1330553
#% 1338552
#% 1523913
#% 1544114
#! The Web offers a corpus of over 100 million tables [6], but the meaning of each table is rarely explicit from the table itself. Header rows exist in few cases and even when they do, the attribute names are typically useless. We describe a system that attempts to recover the semantics of tables by enriching the table with additional annotations. Our annotations facilitate operations such as searching for tables and finding related tables. To recover semantics of tables, we leverage a database of class labels and relationships automatically extracted from the Web. The database of classes and relationships has very wide coverage, but is also noisy. We attach a class label to a column if a sufficient number of the values in the column are identified with that label in the database of class labels, and analogously for binary relationships. We describe a formal model for reasoning about when we have seen sufficient evidence for a label, and show that it performs substantially better than a simple majority scheme. We describe a set of experiments that illustrate the utility of the recovered semantics for table search and show that it performs substantially better than previous approaches. In addition, we characterize what fraction of tables on the Web can be annotated using our approach.

#index 1592312
#* Efficiently compiling efficient query plans for modern hardware
#@ Thomas Neumann
#t 2011
#c 4
#% 378397
#% 465169
#% 481753
#% 565457
#% 571047
#% 745074
#% 864411
#% 1022230
#% 1206624
#% 1328102
#% 1328141
#% 1594617
#! As main memory grows, query performance is more and more determined by the raw CPU costs of query processing itself. The classical iterator style query processing technique is very simple and exible, but shows poor performance on modern CPUs due to lack of locality and frequent instruction mispredictions. Several techniques like batch oriented processing or vectorized tuple processing have been proposed in the past to improve this situation, but even these techniques are frequently out-performed by hand-written execution plans. In this work we present a novel compilation strategy that translates a query into compact and efficient machine code using the LLVM compiler framework. By aiming at good code and data locality and predictable branch layout the resulting code frequently rivals the performance of hand-written C++ code. We integrated these techniques into the HyPer main memory database system and show that this results in excellent query performance while requiring only modest compilation time.

#index 1592313
#* Distance-constraint reachability computation in uncertain graphs
#@ Ruoming Jin;Lin Liu;Bolin Ding;Haixun Wang
#t 2011
#c 4
#% 277018
#% 330620
#% 370988
#% 576214
#% 656792
#% 674440
#% 1080074
#% 1094920
#% 1179162
#% 1217208
#% 1372711
#% 1451203
#% 1523819
#% 1523884
#% 1697228
#! Driven by the emerging network applications, querying and mining uncertain graphs has become increasingly important. In this paper, we investigate a fundamental problem concerning uncertain graphs, which we call the distance-constraint reachability (DCR) problem: Given two vertices s and t, what is the probability that the distance from s to t is less than or equal to a user-defined threshold d in the uncertain graph? Since this problem is #P-Complete, we focus on efficiently and accurately approximating DCR online. Our main results include two new estimators for the probabilistic reachability. One is a Horvitz-Thomson type estimator based on the unequal probabilistic sampling scheme, and the other is a novel recursive sampling estimator, which effectively combines a deterministic recursive computational procedure with a sampling process to boost the estimation accuracy. Both estimators can produce much smaller variance than the direct sampling estimator, which considers each trial to be either 1 or 0. We also present methods to make these estimators more computationally efficient. The comprehensive experiment evaluation on both real and synthetic datasets demonstrates the efficiency and accuracy of our new estimators.

#index 1592314
#* iCBS: incremental cost-based scheduling under piecewise linear SLAs
#@ Yun Chi;Hyun Jin Moon;Hakan Hacigümüş
#t 2011
#c 4
#% 2115
#% 130338
#% 302486
#% 435110
#% 750148
#% 785216
#% 1022748
#% 1181272
#% 1206909
#% 1206984
#% 1463413
#% 1549847
#! In a cloud computing environment, it is beneficial for the cloud service provider to offer differentiated services among different customers, who often have different cost profiles. Therefore, cost-aware scheduling of queries is important. A practical cost-aware scheduling algorithm must be able to handle the highly demanding query volumes in the scheduling queues to make online scheduling decisions very quickly. We develop such a highly efficient cost-aware query scheduling algorithm, called iCBS. iCBS takes the query costs derived from the service level agreements (SLAs) between the service provider and its customers into account to make cost-aware scheduling decisions. iCBS is an incremental variation of an existing scheduling algorithm, CBS. Although CBS exhibits an exceptionally good cost performance, it has a prohibitive time complexity. Our main contributions are (1) to observe how CBS behaves under piecewise linear SLAs, which are very common in cloud computing systems, and (2) to efficiently leverage these observations and to reduce the online time complexity from O(N) for the original version CBS to O(log2 N) for iCBS.

#index 1592315
#* CoHadoop: flexible data placement and its exploitation in Hadoop
#@ Mohamed Y. Eltabakh;Yuanyuan Tian;Fatma Özcan;Rainer Gemulla;Aljoscha Krettek;John McPherson
#t 2011
#c 4
#% 86929
#% 480966
#% 963669
#% 1059993
#% 1063553
#% 1085307
#% 1217159
#% 1278123
#% 1328095
#% 1328186
#% 1426584
#% 1523837
#% 1523841
#% 1523924
#! Hadoop has become an attractive platform for large-scale data analytics. In this paper, we identify a major performance bottleneck of Hadoop: its lack of ability to colocate related data on the same set of nodes. To overcome this bottleneck, we introduce CoHadoop, a lightweight extension of Hadoop that allows applications to control where data are stored. In contrast to previous approaches, CoHadoop retains the flexibility of Hadoop in that it does not require users to convert their data to a certain format (e.g., a relational database or a specific file format). Instead, applications give hints to CoHadoop that some set of files are related and may be processed jointly; CoHadoop then tries to colocate these files for improved efficiency. Our approach is designed such that the strong fault tolerance properties of Hadoop are retained. Colocation can be used to improve the efficiency of many operations, including indexing, grouping, aggregation, columnar storage, joins, and sessionization. We conducted a detailed study of joins and sessionization in the context of log processing---a common use case for Hadoop---, and propose efficient map-only algorithms that exploit colocated data partitions. In our experiments, we observed that CoHadoop outperforms both plain Hadoop and previous work. In particular, our approach not only performs better than repartition-based algorithms, but also outperforms map-only algorithms that do exploit data partitioning but not colocation. 8.

#index 1592316
#* Merging what's cracked, cracking what's merged: adaptive indexing in main-memory column-stores
#@ Stratos Idreos;Stefan Manegold;Harumi Kuno;Goetz Graefe
#t 2011
#c 4
#% 36119
#% 64791
#% 326335
#% 463917
#% 544469
#% 960268
#% 997495
#% 1022202
#% 1207102
#% 1217169
#% 1372713
#% 1426413
#% 1545227
#! Adaptive indexing is characterized by the partial creation and refinement of the index as side effects of query execution. Dynamic or shifting workloads may benefit from preliminary index structures focused on the columns and specific key ranges actually queried --- without incurring the cost of full index construction. The costs and benefits of adaptive indexing techniques should therefore be compared in terms of initialization costs, the overhead imposed upon queries, and the rate at which the index converges to a state that is fully-refined for a particular workload component. Based on an examination of database cracking and adaptive merging, which are two techniques for adaptive indexing, we seek a hybrid technique that has a low initialization cost and also converges rapidly. We find the strengths and weaknesses of database cracking and adaptive merging complementary. One has a relatively high initialization cost but converges rapidly. The other has a low initialization cost but converges relatively slowly. We analyze the sources of their respective strengths and explore the space of hybrid techniques. We have designed and implemented a family of hybrid algorithms in the context of a column-store database system. Our experiments compare their behavior against database cracking and adaptive merging, as well as against both traditional full index lookup and scan of unordered data. We show that the new hybrids significantly improve over past methods while at least two of the hybrids come very close to the "ideal performance" in terms of both overhead per query and convergence to a final state.

#index 1592339
#* Efficient parallel lists intersection and index compression algorithms using graphics processing units
#@ Naiyong Ao;Fan Zhang;Di Wu;Douglas S. Stones;Gang Wang;Xiaoguang Liu;Jing Liu;Sheng Lin
#t 2011
#c 4
#% 69316
#% 70370
#% 131062
#% 268079
#% 290703
#% 303072
#% 320181
#% 397151
#% 434086
#% 510483
#% 570319
#% 656274
#% 737340
#% 786632
#% 864446
#% 867054
#% 988651
#% 1077150
#% 1190095
#% 1190097
#% 1227711
#% 1228291
#% 1328179
#% 1667821
#% 1739410
#! Major web search engines answer thousands of queries per second requesting information about billions of web pages. The data sizes and query loads are growing at an exponential rate. To manage the heavy workload, we consider techniques for utilizing a Graphics Processing Unit (GPU). We investigate new approaches to improve two important operations of search engines -- lists intersection and index compression. For lists intersection, we develop techniques for efficient implementation of the binary search algorithm for parallel computation. We inspect some representative real-world datasets and find that a sufficiently long inverted list has an overall linear rate of increase. Based on this observation, we propose Linear Regression and Hash Segmentation techniques for contracting the search range. For index compression, the traditional d-gap based compression schemata are not well-suited for parallel computation, so we propose a Linear Regression Compression schema which has an inherent parallel structure. We further discuss how to efficiently intersect the compressed lists on a GPU. Our experimental results show significant improvements in the query processing throughput on several datasets.

#index 1592340
#* gStore: answering SPARQL queries via subgraph matching
#@ Lei Zou;Jinghui Mo;Lei Chen;M. Tamer Özsu;Dongyan Zhao
#t 2011
#c 4
#% 227783
#% 318437
#% 378391
#% 410276
#% 427870
#% 519567
#% 577309
#% 728100
#% 765429
#% 956564
#% 1022236
#% 1044450
#% 1055731
#% 1127402
#% 1127431
#% 1190676
#% 1206978
#% 1217194
#% 1223424
#% 1269903
#% 1288161
#% 1366460
#% 1409918
#% 1523817
#! Due to the increasing use of RDF data, efficient processing of SPARQL queries over RDF datasets has become an important issue. However, existing solutions suffer from two limitations: 1) they cannot answer SPARQL queries with wildcards in a scalable manner; and 2) they cannot handle frequent updates in RDF repositories efficiently. Thus, most of them have to reprocess the dataset from scratch. In this paper, we propose a graph-based approach to store and query RDF data. Rather than mapping RDF triples into a relational database as most existing methods do, we store RDF data as a large graph. A SPARQL query is then converted into a corresponding subgraph matching query. In order to speed up query processing, we develop a novel index, together with some effective pruning rules and efficient search algorithms. Our method can answer exact SPARQL queries and queries with wildcards in a uniform manner. We also propose an effective maintenance algorithm to handle online updates over RDF repositories. Extensive experiments confirm the efficiency and effectiveness of our solution.

#index 1592341
#* Albatross: lightweight elasticity in shared storage databases for the cloud using live data migration
#@ Sudipto Das;Shoji Nishimura;Divyakant Agrawal;Amr El Abbadi
#t 2011
#c 4
#% 286836
#% 531907
#% 963628
#% 966956
#% 1002142
#% 1127560
#% 1202159
#% 1210567
#% 1217217
#% 1328130
#% 1426489
#% 1426492
#% 1468219
#% 1581871
#% 1594596
#% 1594649
#! Database systems serving cloud platforms must serve large numbers of applications (or tenants). In addition to managing tenants with small data footprints, different schemas, and variable load patterns, such multitenant data platforms must minimize their operating costs by efficient resource sharing. When deployed over a pay-per-use infrastructure, elastic scaling and load balancing, enabled by low cost live migration of tenant databases, is critical to tolerate load variations while minimizing operating cost. However, existing databases---relational databases and Key-Value stores alike---lack low cost live migration techniques, thus resulting in heavy performance impact during elastic scaling. We present Albatross, a technique for live migration in a multitenant database serving OLTP style workloads where the persistent database image is stored in a network attached storage. Albatross migrates the database cache and the state of active transactions to ensure minimal impact on transaction execution while allowing transactions active during migration to continue execution. It also guarantees serializability while ensuring correctness during failures. Our evaluation using two OLTP benchmarks shows that Albatross can migrate a live tenant database with no aborted transactions, negligible impact on transaction latency and throughput both during and after migration, and an unavailability window as low as 300 ms.

#index 1592342
#* An incremental Hausdorff distance calculation algorithm
#@ Sarana Nutanong;Edwin H. Jacox;Hanan Samet
#t 2011
#c 4
#% 86950
#% 97042
#% 103743
#% 127524
#% 159792
#% 201876
#% 248804
#% 287466
#% 300162
#% 300175
#% 302748
#% 346322
#% 427199
#% 443698
#% 579233
#% 659971
#% 664822
#% 795273
#% 810049
#% 814650
#% 818938
#% 1016192
#% 1016195
#% 1089822
#% 1164194
#% 1190134
#% 1217207
#! The Hausdorff distance is commonly used as a similarity measure between two point sets. Using this measure, a set X is considered similar to Y iff every point in X is close to at least one point in Y. Formally, the Hausdorff distance HausDist(X, Y) can be computed as the Max-Min distance from X to Y, i.e., find the maximum of the distance from an element in X to its nearest neighbor (NN) in Y. Although this is similar to the closest pair and farthest pair problems, computing the Hausdorff distance is a more challenging problem since its Max-Min nature involves both maximization and minimization rather than just one or the other. A traditional approach to computing HausDist(X, Y) performs a linear scan over X and utilizes an index to help compute the NN in Y for each x in X. We present a pair of basic solutions that avoid scanning X by applying the concept of aggregate NN search to searching for the element in X that yields the Hausdorff distance. In addition, we propose a novel method which incrementally explores the indexes of the two sets X and Y simultaneously. As an example application of our techniques, we use the Hausdorff distance as a measure of similarity between two trajectories (represented as point sets). We also use this example application to compare the performance of our proposed method with the traditional approach and the basic solutions. Experimental results show that our proposed method outperforms all competitors by one order of magnitude in terms of the tree traversal cost and total response time.

#index 1592343
#* Surrogate parenthood: protected and informative graphs
#@ Barbara Blaustein;Adriane Chapman;Len Seligman;M. David Allen;Arnon Rosenthal
#t 2011
#c 4
#% 956511
#% 1014464
#% 1063476
#% 1121279
#% 1164737
#% 1189363
#% 1206763
#% 1207207
#% 1268481
#% 1328173
#% 1415851
#% 1524388
#% 1564179
#% 1581832
#% 1728180
#! Many applications, including provenance and some analyses of social networks, require path-based queries over graphstructured data. When these graphs contain sensitive information, paths may be broken, resulting in uninformative query results. This paper presents innovative techniques that give users more informative graph query results; the techniques leverage a common industry practice of providing what we call surrogates: alternate, less sensitive versions of nodes and edges releasable to a broader community. We describe techniques for interposing surrogate nodes and edges to protect sensitive graph components, while maximizing graph connectivity and giving users as much information as possible. In this work, we formalize the problem of creating a protected account G' of a graph G. We provide a utility measure to compare the informativeness of alternate protected accounts and an opacity measure for protected accounts, which indicates the likelihood that an attacker can recreate the topology of the original graph from the protected account. We provide an algorithm to create a maximally useful protected account of a sensitive graph, and show through evaluation with the PLUS prototype that using surrogates and protected accounts adds value for the user, with no significant impact on the time required to generate results for graph queries.

#index 1606342
#* On pruning for top-k ranking in uncertain databases
#@ Chonghai Wang;Li Yan Yuan;Jia-Huai You;Osmar R. Zaiane;Jian Pei
#t 2011
#c 4
#% 663
#% 480418
#% 810020
#% 864455
#% 1022203
#% 1063520
#% 1147662
#% 1206893
#% 1217141
#% 1565405
#! Top-k ranking for an uncertain database is to rank tuples in it so that the best k of them can be determined. The problem has been formalized under the unified approach based on parameterized ranking functions (PRFs) and the possible world semantics. Given a PRF, one can always compute the ranking function values of all the tuples to determine the top-k tuples, which is a formidable task for large databases. In this paper, we present a general approach to pruning for the framework based on PRFs. We show a mathematical manipulation of possible worlds which reveals key insights in the part of computation that may be pruned and how to achieve it in a systematic fashion. This leads to concrete pruning methods for a wide range of ranking functions. We show experimentally the effectiveness of our approach.

#index 1606343
#* PLP: page latch-free shared-everything OLTP
#@ Ippokratis Pandis;Pinar Tözün;Ryan Johnson;Anastasia Ailamaki
#t 2011
#c 4
#% 116087
#% 300164
#% 307360
#% 442700
#% 480589
#% 480831
#% 570884
#% 789387
#% 801348
#% 816654
#% 866983
#% 1022298
#% 1063543
#% 1123799
#% 1181215
#% 1213672
#% 1213682
#% 1217152
#% 1328127
#% 1328149
#% 1369339
#% 1426552
#% 1523799
#% 1523856
#% 1523878
#% 1581960
#! Scaling the performance of shared-everything transaction processing systems to highly-parallel multicore hardware remains a challenge for database system designers. Recent proposals alleviate locking and logging bottlenecks in the system, leaving page latching as the next potential problem. To tackle the page latching problem, we propose physiological partitioning (PLP). The PLP design applies logical-only partitioning, maintaining the desired properties of shared-everything designs, and introduces a multi-rooted B+Tree index structure (MRBTree) which enables the partitioning of the accesses at the physical page level. Logical partitioning and MRBTrees together ensure that all accesses to a given index page come from a single thread and, hence, can be entirely latch-free; an extended design makes heap page accesses thread-private as well. Eliminating page latching allows us to simplify key code paths in the system such as B+Tree operations leading to more efficient and maintainable code. Profiling a prototype PLP system running on different multicore machines shows that it acquires 85% and 68% fewer contentious critical sections, respectively, than an optimized conventional design and one based on logical-only partitioning. PLP also improves performance up to 40% and 18%, respectively, over the existing systems.

#index 1606344
#* Entity matching: how similar is similar
#@ Jiannan Wang;Guoliang Li;Jeffrey Xu Yu;Jianhua Feng
#t 2011
#c 4
#% 201889
#% 217812
#% 310516
#% 463445
#% 480496
#% 577238
#% 577247
#% 577263
#% 729913
#% 864392
#% 875066
#% 893164
#% 913783
#% 1022229
#% 1328143
#! Entity matching that finds records referring to the same entity is an important operation in data cleaning and integration. Existing studies usually use a given similarity function to quantify the similarity of records, and focus on devising index structures and algorithms for efficient entity matching. However it is a big challenge to define "how similar is similar" for real applications, since it is rather hard to automatically select appropriate similarity functions. In this paper we attempt to address this problem. As there are a large number of similarity functions, and even worse thresholds may have infinite values, it is rather expensive to find appropriate similarity functions and thresholds. Fortunately, we have an observation that different similarity functions and thresholds have redundancy, and we have an opportunity to prune inappropriate similarity functions. To this end, we propose effective optimization techniques to eliminate such redundancy, and devise efficient algorithms to find the best similarity functions. The experimental results on both real and synthetic datasets show that our method achieves high accuracy and outperforms the baseline algorithms.

#index 1606345
#* Active complex event processing over event streams
#@ Di Wang;Elke A. Rundensteiner;Richard T. Ellison, III
#t 2011
#c 4
#% 37972
#% 116045
#% 279905
#% 403195
#% 801694
#% 875004
#% 1207016
#% 1217161
#% 1217239
#% 1255307
#% 1263930
#% 1328078
#% 1328129
#% 1523936
#! State-of-the-art Complex Event Processing technology (CEP), while effective for pattern query execution, is limited in its capability of reacting to opportunities and risks detected by pattern queries. Especially reactions that affect the query results in turn have not been addressed in the literature. We propose to tackle these unsolved problems by embedding active rule support within the CEP engine, henceforth called Active CEP (ACEP). Active rules in ACEP allow us to specify a pattern query's dynamic condition and real-time actions. The technical challenge is to handle interactions between queries and reactions to queries in the high-volume stream execution. We hence introduce a novel stream-oriented transactional model along with a family of stream transaction scheduling algorithms that ensure the correctness of concurrent stream execution. We demonstrate the power of ACEP technology by applying it to the development of a healthcare system being deployed in UMass Medical School hospital. Through extensive performance experiments using real data streams, we show that our unique Low-Water-Mark stream transaction scheduler, customized for streaming environments, successfully achieves near-real-time system responsiveness and gives orders-of-magnitude better throughput than our alternative schedulers.

#index 1606346
#* Structural trend analysis for online social networks
#@ Ceren Budak;Divyakant Agrawal;Amr El Abbadi
#t 2011
#c 4
#% 350859
#% 379443
#% 496159
#% 641137
#% 729923
#% 801412
#% 874902
#% 889651
#% 990216
#% 1077150
#% 1083625
#% 1135150
#% 1176970
#% 1214671
#% 1214705
#% 1219788
#% 1298864
#% 1399992
#% 1399993
#% 1400019
#% 1407359
#% 1464648
#% 1477791
#% 1560421
#% 1560424
#% 1561562
#% 1682599
#! The identification of popular and important topics discussed in social networks is crucial for a better understanding of societal concerns. It is also useful for users to stay on top of trends without having to sift through vast amounts of shared information. Trend detection methods introduced so far have not used the network topology and has thus not been able to distinguish viral topics from topics that are diffused mostly through the news media. To address this gap, we propose two novel structural trend definitions we call coordinated and uncoordinated trends that use friendship information to identify topics that are discussed among clustered and distributed users respectively. Our analyses and experiments show that structural trends are significantly different from traditional trends and provide new insights into the way people share information online. We also propose a sampling technique for structural trend detection and prove that the solution yields in a gain in efficiency and is within an acceptable error bound. Experiments performed on a Twitter data set of 41.7 million nodes and 417 million posts show that even with a sampling rate of 0.005, the average precision is 0.93 for coordinated trends and 1 for uncoordinated trends.

#index 1606347
#* Compression aware physical database design
#@ Hideaki Kimura;Vivek Narasayya;Manoj Syamala
#t 2011
#c 4
#% 273909
#% 299989
#% 480158
#% 481424
#% 482100
#% 810111
#% 875026
#% 1015332
#% 1016220
#% 1328064
#! Modern RDBMSs support the ability to compress data using methods such as null suppression and dictionary encoding. Data compression offers the promise of significantly reducing storage requirements and improving I/O performance for decision support queries. However, compression can also slow down update and query performance due to the CPU costs of compression and decompression. In this paper, we study how data compression affects choice of appropriate physical database design, such as indexes, for a given workload. We observe that approaches that decouple the decision of whether or not to choose an index from whether or not to compress the index can result in poor solutions. Thus, we focus on the novel problem of integrating compression into physical database design in a scalable manner. We have implemented our techniques by modifying Microsoft SQL Server and the Database Engine Tuning Advisor (DTA) physical design tool. Our techniques are general and are potentially applicable to DBMSs that support other compression methods. Our experimental results on real world as well as TPC-H benchmark workloads demonstrate the effectiveness of our techniques.

#index 1606348
#* Efficient probabilistic reverse nearest neighbor query processing on uncertain data
#@ Thomas Bernecker;Tobias Emrich;Hans-Peter Kriegel;Matthias Renz;Stefan Zankl;Andreas Züfle
#t 2011
#c 4
#% 32879
#% 86950
#% 300163
#% 321455
#% 427199
#% 772835
#% 893189
#% 1016191
#% 1022203
#% 1063568
#% 1127377
#% 1194584
#% 1206716
#% 1206893
#% 1211651
#% 1217141
#% 1328151
#% 1380967
#% 1426505
#% 1464053
#% 1523852
#% 1594641
#! Given a query object q, a reverse nearest neighbor (RNN) query in a common certain database returns the objects having q as their nearest neighbor. A new challenge for databases is dealing with uncertain objects. In this paper we consider probabilistic reverse nearest neighbor (PRNN) queries, which return the uncertain objects having the query object as nearest neighbor with a sufficiently high probability. We propose an algorithm for efficiently answering PRNN queries using new pruning mechanisms taking distance dependencies into account. We compare our algorithm to state-of-the-art approaches recently proposed. Our experimental evaluation shows that our approach is able to significantly outperform previous approaches. In addition, we show how our approach can easily be extended to PRkNN (where k 1) query processing for which there is currently no efficient solution.

#index 1606349
#* Keyword search in graphs: finding r-cliques
#@ Mehdi Kargar;Aijun An
#t 2011
#c 4
#% 660011
#% 824693
#% 960259
#% 1063537
#% 1063539
#% 1207007
#% 1214668
#% 1467763
#! Keyword search over a graph finds a substructure of the graph containing all or some of the input keywords. Most of previous methods in this area find connected minimal trees that cover all the query keywords. Recently, it has been shown that finding subgraphs rather than trees can be more useful and informative for the users. However, the current tree or graph based methods may produce answers in which some content nodes (i.e., nodes that contain input keywords) are not very close to each other. In addition, when searching for answers, these methods may explore the whole graph rather than only the content nodes. This may lead to poor performance in execution time. To address the above problems, we propose the problem of finding r-cliques in graphs. An r-clique is a group of content nodes that cover all the input keywords and the distance between each two nodes is less than or equal to r. An exact algorithm is proposed that finds all r-cliques in the input graph. In addition, an approximation algorithm that produces r-cliques with 2-approximation in polynomial delay is proposed. Extensive performance studies using two large real data sets confirm the efficiency and accuracy of finding r-cliques in graphs.

#index 1628170
#* Explanation-based auditing
#@ Daniel Fabbri;Kristen LeFevre
#t 2011
#c 4
#% 204453
#% 248813
#% 300120
#% 463903
#% 464891
#% 481290
#% 765447
#% 1016172
#% 1202160
#% 1206679
#% 1217186
#% 1231247
#% 1523798
#% 1543127
#% 1558875
#% 1581904
#% 1861495
#! To comply with emerging privacy laws and regulations, it has become common for applications like electronic health records systems (EHRs) to collect access logs, which record each time a user (e.g., a hospital employee) accesses a piece of sensitive data (e.g., a patient record). Using the access log, it is easy to answer simple queries (e.g., Who accessed Alice's medical record?), but this often does not provide enough information. In addition to learning who accessed their medical records, patients will likely want to understand why each access occurred. In this paper, we introduce the problem of generating explanations for individual records in an access log. The problem is motivated by user-centric auditing applications, and it also provides a novel approach to misuse detection. We develop a framework for modeling explanations which is based on a fundamental observation: For certain classes of databases, including EHRs, the reason for most data accesses can be inferred from data stored elsewhere in the database. For example, if Alice has an appointment with Dr. Dave, this information is stored in the database, and it explains why Dr. Dave looked at Alice's record. Large numbers of data accesses can be explained using general forms called explanation templates. Rather than requiring an administrator to manually specify explanation templates, we propose a set of algorithms for automatically discovering frequent templates from the database (i.e., those that explain a large number of accesses). We also propose techniques for inferring collaborative user groups, which can be used to enhance the quality of the discovered explanations. Finally, we have evaluated our proposed techniques using an access log and data from the University of Michigan Health System. Our results demonstrate that in practice we can provide explanations for over 94% of data accesses in the log.

#index 1628171
#* Human-powered sorts and joins
#@ Adam Marcus;Eugene Wu;David Karger;Samuel Madden;Robert Miller
#t 2011
#c 4
#% 1252624
#% 1452857
#% 1477559
#% 1477589
#% 1477591
#% 1581851
#! Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible to task people with small jobs, such as labeling images or looking up phone numbers, via a programmatic interface. MTurk tasks for processing datasets with humans are currently designed with significant reimplementation of common workflows and ad-hoc selection of parameters such as price to pay per task. We describe how we have integrated crowds into a declarative workflow engine called Qurk to reduce the burden on workflow designers. In this paper, we focus on how to use humans to compare items for sorting and joining data, two of the most common operations in DBMSs. We describe our basic query interface and the user interface of the tasks we post to MTurk. We also propose a number of optimizations, including task batching, replacing pairwise comparisons with numerical ratings, and pre-filtering tables before joining them, which dramatically reduce the overall cost of running sorts and joins on the crowd. In an experiment joining two sets of images, we reduce the overall cost from $67 in a naive implementation to about $3, without substantially affecting accuracy or latency. In an end-to-end experiment, we reduced cost by a factor of 14.5.

#index 1628172
#* Verifying computations with streaming interactive proofs
#@ Graham Cormode;Justin Thaler;Ke Yi
#t 2011
#c 4
#% 2833
#% 8912
#% 42976
#% 115557
#% 131259
#% 139860
#% 238182
#% 246590
#% 278835
#% 289270
#% 688670
#% 812080
#% 816392
#% 894646
#% 998845
#% 1015141
#% 1022214
#% 1061593
#% 1201036
#% 1211647
#% 1217515
#% 1223423
#% 1232259
#% 1400777
#% 1489744
#% 1489745
#% 1583726
#! When computation is outsourced, the data owner would like to be assured that the desired computation has been performed correctly by the service provider. In theory, proof systems can give the necessary assurance, but prior work is not sufficiently scalable or practical. In this paper, we develop new proof protocols for verifying computations which are streaming in nature: the verifier (data owner) needs only logarithmic space and a single pass over the input, and after observing the input follows a simple protocol with a prover (service provider) that takes logarithmic communication spread over a logarithmic number of rounds. These ensure that the computation is performed correctly: that the service provider has not made any errors or missed out some data. The guarantee is very strong: even if the service provider deliberately tries to cheat, there is only vanishingly small probability of doing so undetected, while a correct computation is always accepted. We first observe that some theoretical results can be modified to work with streaming verifiers, showing that there are efficient protocols for problems in the complexity classes NP and NC. Our main results then seek to bridge the gap between theory and practice by developing usable protocols for a variety of problems of central importance in streaming and database processing. All these problems require linear space in the traditional streaming model, and therefore our protocols demonstrate that adding a prover can exponentially reduce the effort needed by the verifier. Our experimental results show that our protocols are practical and scalable.

#index 1628173
#* A moving-object index for efficient query processing with peer-wise location privacy
#@ Dan Lin;Christian S. Jensen;Rui Zhang;Lu Xiao;Jiaheng Lu
#t 2011
#c 4
#% 300174
#% 341916
#% 443397
#% 452686
#% 576762
#% 765454
#% 772839
#% 808340
#% 864410
#% 884582
#% 893151
#% 904296
#% 911803
#% 1015320
#% 1016193
#% 1046510
#% 1055695
#% 1063478
#% 1127612
#% 1192197
#% 1206881
#% 1208239
#% 1214142
#% 1328137
#% 1369273
#% 1409349
#% 1555383
#% 1562973
#% 1572969
#% 1703973
#% 1719090
#! With the growing use of location-based services, location privacy attracts increasing attention from users, industry, and the research community. While considerable effort has been devoted to inventing techniques that prevent service providers from knowing a user's exact location, relatively little attention has been paid to enabling so-called peer-wise privacy---the protection of a user's location from unauthorized peer users. This paper identifies an important efficiency problem in existing peer-privacy approaches that simply apply a filtering step to identify users that are located in a query range, but that do not want to disclose their location to the querying peer. To solve this problem, we propose a novel, privacy-policy enabled index called the PEB-tree that seamlessly integrates location proximity and policy compatibility. We propose efficient algorithms that use the PEB-tree for processing privacy-aware range and kNN queries. Extensive experiments suggest that the PEB-tree enables efficient query processing.

#index 1628174
#* ERA: efficient serial and parallel suffix tree construction for very long strings
#@ Essam Mansour;Amin Allam;Spiros Skiadopoulos;Panos Kalnis
#t 2011
#c 4
#% 235941
#% 289010
#% 451770
#% 759204
#% 823468
#% 829998
#% 954623
#% 956437
#% 956505
#% 960303
#% 1016132
#% 1211602
#% 1217209
#% 1292611
#% 1299174
#% 1315616
#% 1520205
#! The suffix tree is a data structure for indexing strings. It is used in a variety of applications such as bioinformatics, time series analysis, clustering, text editing and data compression. However, when the string and the resulting suffix tree are too large to fit into the main memory, most existing construction algorithms become very inefficient. This paper presents a disk-based suffix tree construction method, called Elastic Range (ERa), which works efficiently with very long strings that are much larger than the available memory. ERa partitions the tree construction process horizontally and vertically and minimizes I/Os by dynamically adjusting the horizontal partitions independently for each vertical partition, based on the evolving shape of the tree and the available memory. Where appropriate, ERa also groups vertical partitions together to amortize the I/O cost. We developed a serial version; a parallel version for shared-memory and shared-disk multi-core systems; and a parallel version for shared-nothing architectures. ERa indexes the entire human genome in 19 minutes on an ordinary desktop computer. For comparison, the fastest existing method needs 15 minutes using 1024 CPUs on an IBM BlueGene supercomputer.

#index 1628175
#* Fast updates on read-optimized databases using multi-core CPUs
#@ Jens Krueger;Changkyu Kim;Martin Grund;Nadathur Satish;David Schwalb;Jatin Chhugani;Hasso Plattner;Pradeep Dubey;Alexander Zeier
#t 2011
#c 4
#% 872
#% 5179
#% 46617
#% 114577
#% 201951
#% 286258
#% 300194
#% 305872
#% 765431
#% 824697
#% 993947
#% 993967
#% 1016235
#% 1022230
#% 1127563
#% 1217145
#% 1328057
#% 1328141
#% 1426530
#% 1426547
#% 1514603
#% 1523974
#% 1581942
#% 1697286
#! Read-optimized columnar databases use differential updates to handle writes by maintaining a separate write-optimized delta partition which is periodically merged with the read-optimized and compressed main partition. This merge process introduces significant overheads and unacceptable downtimes in update intensive systems, aspiring to combine transactional and analytical workloads into one system. In the first part of the paper, we report data analyses of 12 SAP Business Suite customer systems. In the second half, we present an optimized merge process reducing the merge overhead of current systems by a factor of 30. Our linear-time merge algorithm exploits the underlying high compute and bandwidth resources of modern multi-core CPUs with architecture-aware optimizations and efficient parallelization. This enables compressed in-memory column stores to handle the transactional update rate required by enterprise applications, while keeping properties of read-optimized databases for analytic-style queries.

#index 1628176
#* A data-based approach to social influence maximization
#@ Amit Goyal;Francesco Bonchi;Laks V. S. Lakshmanan
#t 2011
#c 4
#% 342596
#% 408396
#% 729923
#% 956512
#% 989613
#% 1107420
#% 1214641
#% 1355040
#% 1355042
#% 1451243
#% 1476461
#% 1535254
#% 1535380
#% 1536509
#% 1663638
#! Influence maximization is the problem of finding a set of users in a social network, such that by targeting this set, one maximizes the expected spread of influence in the network. Most of the literature on this topic has focused exclusively on the social graph, overlooking historical data, i.e., traces of past action propagations. In this paper, we study influence maximization from a novel data-based perspective. In particular, we introduce a new model, which we call credit distribution, that directly leverages available propagation traces to learn how influence flows in the network and uses this to estimate expected influence spread. Our approach also learns the different levels of influence-ability of users, and it is time-aware in the sense that it takes the temporal nature of influence into account. We show that influence maximization under the credit distribution model is NP-hard and that the function that defines expected spread under our model is submodular. Based on these, we develop an approximation algorithm for solving the influence maximization problem that at once enjoys high accuracy compared to the standard approach, while being several orders of magnitude faster and more scalable.

#index 1654042
#* On predictive modeling for optimizing transaction execution in parallel OLTP systems
#@ Andrew Pavlo;Evan P. C. Jones;Stanley Zdonik
#t 2011
#c 4
#% 119485
#% 248815
#% 427195
#% 463416
#% 487677
#% 581671
#% 929722
#% 993990
#% 998842
#% 1016310
#% 1022298
#% 1063543
#% 1108673
#% 1119400
#% 1127596
#% 1247792
#% 1272228
#% 1301004
#% 1426552
#% 1473345
#% 1523799
#% 1663458
#! A new emerging class of parallel database management systems (DBMS) is designed to take advantage of the partitionable workloads of on-line transaction processing (OLTP) applications [23, 20]. Transactions in these systems are optimized to execute to completion on a single node in a shared-nothing cluster without needing to coordinate with other nodes or use expensive concurrency control measures [18]. But some OLTP applications cannot be partitioned such that all of their transactions execute within a single-partition in this manner. These distributed transactions access data not stored within their local partitions and subsequently require more heavy-weight concurrency control protocols. Further difficulties arise when the transaction's execution properties, such as the number of partitions it may need to access or whether it will abort, are not known beforehand. The DBMS could mitigate these performance issues if it is provided with additional information about transactions. Thus, in this paper we present a Markov model-based approach for automatically selecting which optimizations a DBMS could use, namely (1) more efficient concurrency control schemes, (2) intelligent scheduling, (3) reduced undo logging, and (4) speculative execution. To evaluate our techniques, we implemented our models and integrated them into a parallel, main-memory OLTP DBMS to show that we can improve the performance of applications with diverse workloads.

#index 1654043
#* View selection in Semantic Web databases
#@ François Goasdoué;Konstantinos Karanasos;Julien Leblay;Ioana Manolescu
#t 2011
#c 4
#% 384978
#% 392740
#% 393844
#% 397432
#% 451768
#% 572311
#% 599549
#% 960278
#% 992962
#% 1022236
#% 1127402
#% 1127431
#% 1127610
#% 1217194
#% 1269903
#% 1396154
#% 1482481
#% 1523817
#% 1529012
#% 1552660
#% 1560420
#% 1594602
#% 1603793
#! We consider the setting of a Semantic Web database, containing both explicit data encoded in RDF triples, and implicit data, implied by the RDF semantics. Based on a query workload, we address the problem of selecting a set of views to be materialized in the database, minimizing a combination of query processing, view storage, and view maintenance costs. Starting from an existing relational view selection method, we devise new algorithms for recommending view sets, and show that they scale significantly beyond the existing relational ones when adapted to the RDF context. To account for implicit triples in query answers, we propose a novel RDF query reformulation algorithm and an innovative way of incorporating it into view selection in order to avoid a combinatorial explosion in the complexity of the selection process. The interest of our techniques is demonstrated through a set of experiments.

#index 1654044
#* Building wavelet histograms on large data in MapReduce
#@ Jeffrey Jestes;Ke Yi;Feifei Li
#t 2011
#c 4
#% 210190
#% 214073
#% 248822
#% 397389
#% 479648
#% 480465
#% 480628
#% 570889
#% 572308
#% 577227
#% 643566
#% 768521
#% 823333
#% 824704
#% 874907
#% 963669
#% 1063553
#% 1127559
#% 1127608
#% 1217159
#% 1328060
#% 1328066
#% 1328095
#% 1328186
#% 1372690
#% 1426543
#% 1426601
#% 1468411
#% 1523837
#% 1523841
#% 1688247
#! MapReduce is becoming the de facto framework for storing and processing massive data, due to its excellent scalability, reliability, and elasticity. In many MapReduce applications, obtaining a compact accurate summary of data is essential. Among various data summarization tools, histograms have proven to be particularly important and useful for summarizing data, and the wavelet histogram is one of the most widely used histograms. In this paper, we investigate the problem of building wavelet histograms efficiently on large datasets in MapReduce. We measure the efficiency of the algorithms by both end-to-end running time and communication cost. We demonstrate straightforward adaptations of existing exact and approximate methods for building wavelet histograms to MapReduce clusters are highly inefficient. To that end, we design new algorithms for computing exact and approximate wavelet histograms and discuss their implementation in MapReduce. We illustrate our techniques in Hadoop, and compare to baseline solutions with extensive experiments performed in a heterogeneous Hadoop cluster of 16 nodes, using large real and synthetic datasets, up to hundreds of gigabytes. The results suggest significant (often orders of magnitude) performance improvement achieved by our new algorithms.

#index 1654045
#* Summarization and matching of density-based clusters in streaming environments
#@ Di Yang;Elke A. Rundensteiner;Matthew O. Ward
#t 2011
#c 4
#% 210173
#% 479658
#% 878299
#% 889089
#% 989584
#% 1015261
#% 1019121
#% 1035559
#% 1181258
#% 1318615
#% 1328182
#% 1426610
#% 1664003
#! Density-based cluster mining is known to serve a broad range of applications ranging from stock trade analysis to moving object monitoring. Although methods for efficient extraction of density-based clusters have been studied in the literature, the problem of summarizing and matching of such clusters with arbitrary shapes and complex cluster structures remains unsolved. Therefore, the goal of our work is to extend the state-of-art of density-based cluster mining in streams from cluster extraction only to now also support analysis and management of the extracted clusters. Our work solves three major technical challenges. First, we propose a novel multi-resolution cluster summarization method, called Skeletal Grid Summarization (SGS), which captures the key features of density-based clusters, covering both their external shape and internal cluster structures. Second, in order to summarize the extracted clusters in real-time, we present an integrated computation strategy C-SGS, which piggybacks the generation of cluster summarizations within the online clustering process. Lastly, we design a mechanism to efficiently execute cluster matching queries, which identify similar clusters for given cluster of analyst's interest from clusters extracted earlier in the stream history. Our experimental study using real streaming data shows the clear superiority of our proposed methods in both efficiency and effectiveness for cluster summarization and cluster matching queries to other potential alternatives.

#index 1654046
#* Multilingual schema matching for Wikipedia infoboxes
#@ Thanh Nguyen;Viviane Moreira;Huong Nguyen;Hoa Nguyen;Juliana Freire
#t 2011
#c 4
#% 411762
#% 572314
#% 660001
#% 810103
#% 866989
#% 885759
#% 1022259
#% 1077150
#% 1155717
#% 1166514
#% 1206702
#% 1227999
#% 1269155
#% 1310628
#% 1337101
#% 1409954
#% 1415756
#% 1432220
#% 1470635
#% 1471589
#% 1482334
#% 1494792
#% 1494794
#% 1688251
#% 1916065
#! Recent research has taken advantage of Wikipedia's multi-lingualism as a resource for cross-language information retrieval and machine translation, as well as proposed techniques for enriching its cross-language structure. The availability of documents in multiple languages also opens up new opportunities for querying structured Wikipedia content, and in particular, to enable answers that straddle different languages. As a step towards supporting such queries, in this paper, we propose a method for identifying mappings between attributes from infoboxes that come from pages in different languages. Our approach finds mappings in a completely automated fashion. Because it does not require training data, it is scalable: not only can it be used to find mappings between many language pairs, but it is also effective for languages that are under-represented and lack sufficient training samples. Another important benefit of our approach is that it does not depend on syntactic similarity between attribute names, and thus, it can be applied to language pairs that have distinct morphologies. We have performed an extensive experimental evaluation using a corpus consisting of pages in Portuguese, Vietnamese, and English. The results show that not only does our approach obtain high precision and recall, but it also outperforms state-of-the-art techniques. We also present a case study which demonstrates that the multilingual mappings we derive lead to substantial improvements in answer quality and coverage for structured queries over Wikipedia content.

#index 1654047
#* Controlling false positives in association rule mining
#@ Guimei Liu;Haojun Zhang;Limsoon Wong
#t 2011
#c 4
#% 152934
#% 227919
#% 420126
#% 464873
#% 577214
#% 729942
#% 867057
#% 941039
#% 976826
#% 1058703
#% 1217126
#! Association rule mining is an important problem in the data mining area. It enumerates and tests a large number of rules on a dataset and outputs rules that satisfy user-specified constraints. Due to the large number of rules being tested, rules that do not represent real systematic effect in the data can satisfy the given constraints purely by random chance. Hence association rule mining often suffers from a high risk of false positive errors. There is a lack of comprehensive study on controlling false positives in association rule mining. In this paper, we adopt three multiple testing correction approaches---the direct adjustment approach, the permutation-based approach and the holdout approach---to control false positives in association rule mining, and conduct extensive experiments to study their performance. Our results show that (1) Numerous spurious rules are generated if no correction is made. (2) The three approaches can control false positives effectively. Among the three approaches, the permutation-based approach has the highest power of detecting real association rules, but it is very computationally expensive. We employ several techniques to reduce its cost effectively.

#index 1654048
#* PARIS: probabilistic alignment of relations, instances, and schema
#@ Fabian M. Suchanek;Serge Abiteboul;Pierre Senellart
#t 2011
#c 4
#% 344361
#% 754068
#% 810103
#% 881539
#% 913783
#% 937552
#% 956564
#% 960271
#% 1055897
#% 1129527
#% 1152454
#% 1190116
#% 1206834
#% 1229097
#% 1246170
#% 1269721
#% 1269899
#% 1288166
#% 1333469
#% 1400146
#% 1409921
#% 1409954
#% 1471586
#% 1540301
#% 1540311
#% 1560363
#% 1597483
#% 1719976
#! One of the main challenges that the Semantic Web faces is the integration of a growing number of independently designed ontologies. In this work, we present paris, an approach for the automatic alignment of ontologies. paris aligns not only instances, but also relations and classes. Alignments at the instance level cross-fertilize with alignments at the schema level. Thereby, our system provides a truly holistic solution to the problem of ontology alignment. The heart of the approach is probabilistic, i.e., we measure degrees of matchings based on probability estimates. This allows paris to run without any parameter tuning. We demonstrate the efficiency of the algorithm and its precision through extensive experiments. In particular, we obtain a precision of around 90% in experiments with some of the world's largest ontologies.

#index 1654049
#* Answering top-k queries over a mixture of attractive and repulsive dimensions
#@ Sayan Ranu;Ambuj K. Singh
#t 2011
#c 4
#% 213981
#% 235114
#% 300180
#% 333854
#% 333951
#% 480330
#% 481956
#% 631988
#% 893108
#% 893126
#% 941785
#% 960242
#% 1206766
#! In this paper, we formulate a top-k query that compares objects in a database to a user-provided query object on a novel scoring function. The proposed scoring function combines the idea of attractive and repulsive dimensions into a general framework to overcome the weakness of traditional distance or similarity measures. We study the properties of the proposed class of scoring functions and develop efficient and scalable index structures that index the isolines of the function. We demonstrate various scenarios where the query finds application. Empirical evaluation demonstrates a performance gain of one to two orders of magnitude on querying time over existing state-of-the-art top-k techniques. Further, a qualitative analysis is performed on a real dataset to highlight the potential of the proposed query in discovering hidden data characteristics.

#index 1654050
#* PIQL: success-tolerant query processing in the cloud
#@ Michael Armbrust;Kristal Curtis;Tim Kraska;Armando Fox;Michael J. Franklin;David A. Patterson
#t 2011
#c 4
#% 136740
#% 227894
#% 300169
#% 333854
#% 479938
#% 777931
#% 998845
#% 1002142
#% 1063488
#% 1075132
#% 1127560
#% 1206984
#% 1237169
#% 1426487
#% 1426550
#% 1428155
#% 1557845
#! Newly-released web applications often succumb to a "Success Disaster," where overloaded database machines and resulting high response times destroy a previously good user experience. Unfortunately, the data independence provided by a traditional relational database system, while useful for agile development, only exacerbates the problem by hiding potentially expensive queries under simple declarative expressions. As a result, developers of these applications are increasingly abandoning relational databases in favor of imperative code written against distributed key/value stores, losing the many benefits of data independence in the process. Instead, we propose PIQL, a declarative language that also provides scale independence by calculating an upper bound on the number of key/value store operations that will be performed for any query. Coupled with a service level objective (SLO) compliance prediction model and PIQL's scalable database architecture, these bounds make it easy for developers to write success-tolerant applications that support an arbitrarily large number of users while still providing acceptable performance. In this paper, we present the PIQL query processing system and evaluate its scale independence on hundreds of machines using two benchmarks, TPC-W and SCADr.

#index 1654051
#* gSketch: on query estimation in graph streams
#@ Peixiang Zhao;Charu C. Aggarwal;Min Wang
#t 2011
#c 4
#% 1331
#% 36698
#% 214073
#% 283833
#% 293720
#% 379443
#% 397354
#% 754117
#% 798044
#% 809258
#% 816392
#% 866773
#% 874902
#% 894646
#% 918001
#% 935763
#% 993960
#% 1063716
#% 1127369
#% 1127608
#% 1214643
#% 1372657
#% 1428692
#% 1523882
#% 1567510
#% 1734153
#! Many dynamic applications are built upon large network infrastructures, such as social networks, communication networks, biological networks and the Web. Such applications create data that can be naturally modeled as graph streams, in which edges of the underlying graph are received and updated sequentially in a form of a stream. It is often necessary and important to summarize the behavior of graph streams in order to enable effective query processing. However, the sheer size and dynamic nature of graph streams present an enormous challenge to existing graph management techniques. In this paper, we propose a new graph sketch method, gSketch, which combines well studied synopses for traditional data streams with a sketch partitioning technique, to estimate and optimize the responses to basic queries on graph streams. We consider two different scenarios for query estimation: (1) A graph stream sample is available; (2) Both a graph stream sample and a query workload sample are available. Algorithms for different scenarios are designed respectively by partitioning a global sketch to a group of localized sketches in order to optimize the query estimation accuracy. We perform extensive experimental studies on both real and synthetic data sets and demonstrate the power and robustness of gSketch in comparison with the state-of-the-art global sketch method.

#index 1654052
#* Indexing the earth mover's distance using normal distributions
#@ Brian E. Ruttenberg;Ambuj K. Singh
#t 2011
#c 4
#% 130021
#% 265420
#% 325683
#% 333854
#% 333941
#% 480146
#% 672641
#% 818938
#% 829312
#% 840964
#% 864398
#% 1063484
#% 1206681
#% 1213188
#% 1523863
#% 1688294
#% 1840050
#! Querying uncertain data sets (represented as probability distributions) presents many challenges due to the large amount of data involved and the difficulties comparing uncertainty between distributions. The Earth Mover's Distance (EMD) has increasingly been employed to compare uncertain data due to its ability to effectively capture the differences between two distributions. Computing the EMD entails finding a solution to the transportation problem, which is computationally intensive. In this paper, we propose a new lower bound to the EMD and an index structure to significantly improve the performance of EMD based K-- nearest neighbor (K--NN) queries on uncertain databases. We propose a new lower bound to the EMD that approximates the EMD on a projection vector. Each distribution is projected onto a vector and approximated by a normal distribution, as well as an accompanying error term. We then represent each normal as a point in a Hough transformed space. We then use the concept of stochastic dominance to implement an efficient index structure in the transformed space. We show that our method significantly decreases K--NN query time on uncertain databases. The index structure also scales well with database cardinality. It is well suited for heterogeneous data sets, helping to keep EMD based queries tractable as uncertain data sets become larger and more complex.

#index 1654053
#* Generating exact- and ranked partially-matched answers to questions in advertisements
#@ Rani Qumsiyeh;Maria S. Pera;Yiu-Kai Ng
#t 2011
#c 4
#% 308574
#% 570627
#% 864432
#% 875003
#% 987236
#% 990597
#% 993957
#% 1077150
#% 1174569
#% 1181094
#% 1190157
#% 1224724
#% 1251589
#% 1288603
#% 1451789
#% 1463522
#% 1693254
#! Taking advantage of the Web, many advertisements (ads for short) websites, which aspire to increase client's transactions and thus profits, offer searching tools which allow users to (i) post keyword queries to capture their information needs or (ii) invoke form-based interfaces to create queries by selecting search options, such as a price range, filled-in entries, check boxes, or drop-down menus. These search mechanisms, however, are inadequate, since they cannot be used to specify a natural-language query with rich syntactic and semantic content, which can only be handled by a question answering (QA) system. Furthermore, existing ads websites are incapable of evaluating arbitrary Boolean queries or retrieving partially-matched answers that might be of interest to the user whenever a user's search yields only a few or no results at all. In solving these problems, we present a QA system for ads, called CQAds, which (i) allows users to post a natural-language question Q for retrieving relevant ads, if they exist, (ii) identifies ads as answers that partially-match the requested information expressed in Q, if insufficient or no answers to Q can be retrieved, which are ordered using a similarity-ranking approach, and (iii) analyzes incomplete or ambiguous questions to perform the "best guess" in retrieving answers that "best match" the selection criteria specified in Q. CQAds is also equipped with a Boolean model to evaluate Boolean operators that are either explicitly or implicitly specified in Q, i.e., with or without Boolean operators specified by the users, respectively. CQAds is easy to use, scalable to all ads domains, and more powerful than search tools provided by existing ads websites, since its query-processing strategy retrieves relevant ads of higher quality and quantity. We have verified the accuracy of CQAds in retrieving ads on eight ads domains and compared its ranking strategy with other well-known ranking approaches.

#index 1654054
#* Size-l object summaries for relational keyword search
#@ Georgios J. Fakas;Zhi Cai;Nikos Mamoulis
#t 2011
#c 4
#% 262036
#% 268079
#% 333854
#% 654442
#% 660011
#% 864456
#% 875017
#% 960243
#% 960245
#% 987208
#% 993987
#% 994033
#% 1015325
#% 1016176
#% 1021948
#% 1063493
#% 1206684
#% 1206902
#% 1206911
#% 1207235
#% 1525745
#! A previously proposed keyword search paradigm produces, as a query result, a ranked list of Object Summaries (OSs). An OS is a tree structure of related tuples that summarizes all data held in a relational database about a particular Data Subject (DS). However, some of these OSs are very large in size and therefore unfriendly to users that initially prefer synoptic information before proceeding to more comprehensive information about a particular DS. In this paper, we investigate the effective and efficient retrieval of concise and informative OSs. We argue that a good size-l OS should be a stand-alone and meaningful synopsis of the most important information about the particular DS. More precisely, we define a size-l OS as a partial OS composed of l important tuples. We propose three algorithms for the efficient generation of size-l OSs (in addition to the optimal approach which requires exponential time). Experimental evaluation on DBLP and TPC-H databases verifies the effectiveness and efficiency of our approach.

#index 1654055
#* REX: explaining relationships between entity pairs
#@ Lujun Fang;Anish Das Sarma;Cong Yu;Philip Bohannon
#t 2011
#c 4
#% 408396
#% 411762
#% 629708
#% 654441
#% 659990
#% 660011
#% 729938
#% 769887
#% 818143
#% 824693
#% 853538
#% 881496
#% 956550
#% 960243
#% 960259
#% 960305
#% 993987
#% 1015325
#% 1016176
#% 1127413
#% 1268040
#% 1288160
#% 1292627
#% 1292670
#% 1411112
#% 1451202
#! Knowledge bases of entities and relations (either constructed manually or automatically) are behind many real world search engines, including those at Yahoo!, Microsoft, and Google. Those knowledge bases can be viewed as graphs with nodes representing entities and edges representing (primary) relationships, and various studies have been conducted on how to leverage them to answer entity seeking queries. Meanwhile, in a complementary direction, analyses over the query logs have enabled researchers to identify entity pairs that are statistically correlated. Such entity relationships are then presented to search users through the "related searches" feature in modern search engines. However, entity relationships thus discovered can often be "puzzling" to the users because why the entities are connected is often indescribable. In this paper, we propose a novel problem called entity relationship explanation, which seeks to explain why a pair of entities are connected, and solve this challenging problem by integrating the above two complementary approaches, i.e., we leverage the knowledge base to "explain" the connections discovered between entity pairs. More specifically, we present REX, a system that takes a pair of entities in a given knowledge base as input and efficiently identifies a ranked list of relationship explanations. We formally define relationship explanations and analyze their desirable properties. Furthermore, we design and implement algorithms to efficiently enumerate and rank all relationship explanations based on multiple measures of "interestingness." We perform extensive experiments over real web-scale data gathered from DBpedia and a commercial search engine, demonstrating the efficiency and scalability of REX. We also perform user studies to corroborate the effectiveness of explanations generated by REX.

#index 1654056
#* Pass-join: a partition-based method for similarity joins
#@ Guoliang Li;Dong Deng;Jiannan Wang;Jianhua Feng
#t 2011
#c 4
#% 333679
#% 480654
#% 654467
#% 765463
#% 864392
#% 893164
#% 956506
#% 1022218
#% 1054481
#% 1055684
#% 1063530
#% 1127368
#% 1127425
#% 1127426
#% 1206665
#% 1206677
#% 1206821
#% 1217179
#% 1217204
#% 1328164
#% 1426543
#% 1426578
#% 1523903
#% 1581890
#% 1594614
#! As an essential operation in data cleaning, the similarity join has attracted considerable attention from the database community. In this paper, we study string similarity joins with edit-distance constraints, which find similar string pairs from two large sets of strings whose edit distance is within a given threshold. Existing algorithms are efficient either for short strings or for long strings, and there is no algorithm that can efficiently and adaptively support both short strings and long strings. To address this problem, we propose a partition-based method called Pass-Join. Pass-Join partitions a string into a set of segments and creates inverted indices for the segments. Then for each string, Pass-Join selects some of its substrings and uses the selected substrings to find candidate pairs using the inverted indices. We devise efficient techniques to select the substrings and prove that our method can minimize the number of selected substrings. We develop novel pruning techniques to efficiently verify the candidate pairs. Experimental results show that our algorithms are efficient for both short strings and long strings, and outperform state-of-the-art methods on real datasets.

#index 1654057
#* Relative Lempel-Ziv factorization for efficient storage and retrieval of web collections
#@ Christopher Hoobin;Simon J. Puglisi;Justin Zobel
#t 2011
#c 4
#% 57849
#% 143306
#% 197695
#% 262036
#% 290703
#% 311799
#% 346149
#% 393450
#% 397151
#% 399892
#% 438325
#% 584840
#% 588042
#% 786632
#% 864446
#% 931332
#% 987208
#% 1019138
#% 1054227
#% 1077150
#% 1089600
#% 1128591
#% 1181094
#% 1195871
#% 1355055
#% 1389976
#% 1403800
#% 1418499
#% 1480887
#% 1529947
#% 1598469
#! Compression techniques that support fast random access are a core component of any information system. Current state-of-the-art methods group documents into fixed-sized blocks and compress each block with a general-purpose adaptive algorithm such as gzip. Random access to a specific document then requires decompression of a block. The choice of block size is critical: it trades between compression effectiveness and document retrieval times. In this paper we present a scalable compression method for large document collections that allows fast random access. We build a representative sample of the collection and use it as a dictionary in a LZ77-like encoding of the rest of the collection, relative to the dictionary. We demonstrate on large collections, that using a dictionary as small as 0.1% of the collection size, our algorithm is dramatically faster than previous methods, and in general gives much better compression.

#index 1668633
#* Towards cost-effective storage provisioning for DBMSs
#@ Ning Zhang;Junichi Tatemura;Jignesh M. Patel;Hakan Hacigümüş
#t 2011
#c 4
#% 810026
#% 875027
#% 960238
#% 998845
#% 1022202
#% 1052068
#% 1063541
#% 1063551
#% 1127391
#% 1129952
#% 1129953
#% 1217151
#% 1328052
#% 1328139
#% 1426580
#% 1523901
#% 1523922
#% 1668633
#! Data center operators face a bewildering set of choices when considering how to provision resources on machines with complex I/O subsystems. Modern I/O subsystems often have a rich mix of fast, high performing, but expensive SSDs sitting alongside with cheaper but relatively slower (for random accesses) traditional hard disk drives. The data center operators need to determine how to provision the I/O resources for specific workloads so as to abide by existing Service Level Agreements (SLAs), while minimizing the total operating cost (TOC) of running the workload, where the TOC includes the amortized hardware costs and the run time energy costs. The focus of this paper is on introducing this new problem of TOC-based storage allocation, cast in a framework that is compatible with traditional DBMS query optimization and query processing architecture. We also present a heuristic-based solution to this problem, called DOT. We have implemented DOT in PostgreSQL, and experiments using TPC-H and TPC-C demonstrate significant TOC reduction by DOT in various settings.

#index 1668634
#* B+-tree index optimization by exploiting internal parallelism of flash-based solid state drives
#@ Hongchan Roh;Sanghyun Park;Sungho Kim;Mincheol Shin;Sang-Won Lee
#t 2011
#c 4
#% 114582
#% 286929
#% 729743
#% 960238
#% 985755
#% 1085291
#% 1092673
#% 1213385
#% 1217213
#% 1292628
#% 1426532
#% 1446648
#% 1523901
#% 1586527
#% 1601162
#! Previous research addressed the potential problems of the hard-disk oriented design of DBMSs of flashSSDs. In this paper, we focus on exploiting potential benefits of flashSSDs. First, we examine the internal parallelism issues of flashSSDs by conducting benchmarks to various flashSSDs. Then, we suggest algorithm-design principles in order to best benefit from the internal parallelism. We present a new I/O request concept, called psync I/O that can exploit the internal parallelism of flashSSDs in a single process. Based on these ideas, we introduce B+-tree optimization methods in order to utilize internal parallelism. By integrating the results of these methods, we present a B+-tree variant, PIO B-tree. We confirmed that each optimization method substantially enhances the index performance. Consequently, PIO B-tree enhanced B+-tree's insert performance by a factor of up to 16.3, while improving point-search performance by a factor of 1.2. The range search of PIO B-tree was up to 5 times faster than that of the B+-tree. Moreover, PIO B-tree outperformed other flash-aware indexes in various synthetic workloads. We also confirmed that PIO B-tree outperforms B+-tree in index traces collected inside the Postgresql DBMS with TPC-C benchmark.

#index 1668635
#* High-performance concurrency control mechanisms for main-memory databases
#@ Per-Åke Larson;Spyros Blanas;Cristian Diaconu;Craig Freedman;Jignesh M. Patel;Mike Zwilling
#t 2011
#c 4
#% 9241
#% 25214
#% 58379
#% 194928
#% 201869
#% 229843
#% 247071
#% 285934
#% 286836
#% 463261
#% 480941
#% 1021832
#% 1270565
#% 1328149
#% 1523878
#% 1594687
#! A database system optimized for in-memory storage can support much higher transaction rates than current systems. However, standard concurrency control methods used today do not scale to the high transaction rates achievable by such systems. In this paper we introduce two efficient concurrency control methods specifically designed for main-memory databases. Both use multiversioning to isolate read-only transactions from updates but differ in how atomicity is ensured: one is optimistic and one is pessimistic. To avoid expensive context switching, transactions never block during normal processing but they may have to wait before commit to ensure correct serialization ordering. We also implemented a main-memory optimized version of single-version locking. Experimental results show that while single-version locking works well when transactions are short and contention is low performance degrades under more demanding conditions. The multiversion schemes have higher overhead but are much less sensitive to hotspots and the presence of long-running transactions.

#index 1668636
#* Capturing topology in graph pattern matching
#@ Shuai Ma;Yang Cao;Wenfei Fan;Jinpeng Huai;Tianyu Wo
#t 2011
#c 4
#% 288990
#% 291299
#% 330305
#% 369768
#% 384978
#% 410276
#% 447958
#% 479465
#% 502766
#% 593696
#% 641958
#% 772884
#% 835906
#% 881567
#% 960276
#% 963669
#% 989645
#% 1063515
#% 1069524
#% 1206703
#% 1328183
#% 1372657
#% 1424586
#% 1426513
#% 1506210
#% 1523818
#% 1594585
#! Graph pattern matching is often defined in terms of subgraph isomorphism, an np-complete problem. To lower its complexity, various extensions of graph simulation have been considered instead. These extensions allow pattern matching to be conducted in cubic-time. However, they fall short of capturing the topology of data graphs, i.e., graphs may have a structure drastically different from pattern graphs they match, and the matches found are often too large to understand and analyze. To rectify these problems, this paper proposes a notion of strong simulation, a revision of graph simulation, for graph pattern matching. (1) We identify a set of criteria for preserving the topology of graphs matched. We show that strong simulation preserves the topology of data graphs and finds a bounded number of matches. (2) We show that strong simulation retains the same complexity as earlier extensions of simulation, by providing a cubic-time algorithm for computing strong simulation. (3) We present the locality property of strong simulation, which allows us to effectively conduct pattern matching on distributed graphs. (4) We experimentally verify the effectiveness and efficiency of these algorithms, using real-life data and synthetic data.

#index 1668637
#* Probabilistic management of OCR data using an RDBMS
#@ Arun Kumar;Christopher Ré
#t 2011
#c 4
#% 277396
#% 287055
#% 387427
#% 443781
#% 480926
#% 504885
#% 654487
#% 660009
#% 741066
#% 765262
#% 864394
#% 874976
#% 891559
#% 893168
#% 903341
#% 1016201
#% 1063523
#% 1127378
#% 1127414
#% 1166535
#% 1178694
#% 1196821
#% 1206732
#% 1206877
#% 1206987
#% 1207010
#% 1217181
#% 1275583
#% 1402057
#% 1426444
#% 1523851
#! The digitization of scanned forms and documents is changing the data sources that enterprises manage. To integrate these new data sources with enterprise data, the current state-of-the-art approach is to convert the images to ASCII text using optical character recognition (OCR) software and then to store the resulting ASCII text in a relational database. The OCR problem is challenging, and so the output of OCR often contains errors. In turn, queries on the output of OCR may fail to retrieve relevant answers. State-of-the-art OCR programs, e.g., the OCR powering Google Books, use a probabilistic model that captures many alternatives during the OCR process. Only when the results of OCR are stored in the database, do these approaches discard the uncertainty. In this work, we propose to retain the probabilistic models produced by OCR process in a relational database management system. A key technical challenge is that the probabilistic data produced by OCR software is very large (a single book blows up to 2GB from 400kB as ASCII). As a result, a baseline solution that integrates these models with an RDBMS is over 1000x slower versus standard text processing for single table select-project queries. However, many applications may have quality-performance needs that are in between these two extremes of ASCII and the complete model output by the OCR software. Thus, we propose a novel approximation scheme called Staccato that allows a user to trade recall for query performance. Additionally, we provide a formal analysis of our scheme's properties, and describe how we integrate our scheme with standard-RDBMS text indexing.

#index 1668638
#* RTED: a robust algorithm for the tree edit distance
#@ Mateusz Pawlik;Nikolaus Augsten
#t 2011
#c 4
#% 53369
#% 66654
#% 121462
#% 210212
#% 289193
#% 303066
#% 349489
#% 397373
#% 399634
#% 480126
#% 547947
#% 625871
#% 659923
#% 754108
#% 766669
#% 806218
#% 810071
#% 826007
#% 893109
#% 1030420
#% 1154681
#% 1206601
#% 1290919
#% 1312537
#% 1349285
#% 1500845
#% 1916648
#! We consider the classical tree edit distance between ordered labeled trees, which is defined as the minimum-cost sequence of node edit operations that transform one tree into another. The state-of-the-art solutions for the tree edit distance are not satisfactory. The main competitors in the field either have optimal worst-case complexity, but the worst case happens frequently, or they are very efficient for some tree shapes, but degenerate for others. This leads to unpredictable and often infeasible runtimes. There is no obvious way to choose between the algorithms. In this paper we present RTED, a robust tree edit distance algorithm. The asymptotic complexity of RTED is smaller or equal to the complexity of the best competitors for any input instance, i.e., RTED is both efficient and worst-case optimal. We introduce the class of LRH (Left-Right-Heavy) algorithms, which includes RTED and the fastest tree edit distance algorithms presented in literature. We prove that RTED outperforms all previously proposed LRH algorithms in terms of runtime complexity. In our experiments on synthetic and real world data we empirically evaluate our solution and compare it to the state-of-the-art.

#index 1668639
#* Putting lipstick on pig: enabling database-style workflow provenance
#@ Yael Amsterdamer;Susan B. Davidson;Daniel Deutch;Tova Milo;Julia Stoyanovich;Val Tannen
#t 2011
#c 4
#% 189868
#% 464891
#% 504161
#% 942360
#% 976987
#% 1022258
#% 1022327
#% 1036075
#% 1042649
#% 1063553
#% 1063593
#% 1063736
#% 1092014
#% 1153448
#% 1153476
#% 1180021
#% 1231247
#% 1372707
#% 1426581
#% 1428464
#% 1468482
#% 1468496
#% 1581829
#! Workflow provenance typically assumes that each module is a "black-box", so that each output depends on all inputs (coarse-grained dependencies). Furthermore, it does not model the internal state of a module, which can change between repeated executions. In practice, however, an output may depend on only a small subset of the inputs (fine-grained dependencies) as well as on the internal state of the module. We present a novel provenance framework that marries database-style and workflow-style provenance, by using Pig Latin to expose the functionality of modules, thus capturing internal state and fine-grained dependencies. A critical ingredient in our solution is the use of a novel form of provenance graph that models module invocations and yields a compact representation of fine-grained workflow provenance. It also enables a number of novel graph transformation operations, allowing to choose the desired level of granularity in provenance querying (ZoomIn and ZoomOut), and supporting "what-if" workflow analytic queries. We implemented our approach in the Lipstick system and developed a benchmark in support of a systematic performance evaluation. Our results demonstrate the feasibility of tracking and querying fine-grained workflow provenance.

#index 1668640
#* Relational approach for shortest path discovery over large graphs
#@ Jun Gao;Ruoming Jin;Jiashuai Zhou;Jeffrey Xu Yu;Xiao Jiang;Tengjiao Wang
#t 2011
#c 4
#% 379482
#% 450545
#% 479956
#% 765441
#% 769907
#% 813718
#% 960304
#% 963669
#% 994001
#% 1124990
#% 1206916
#% 1292553
#% 1328181
#% 1392256
#% 1426508
#% 1426510
#% 1426539
#% 1523818
#% 1581927
#% 1669944
#% 1710569
#! With the rapid growth of large graphs, we cannot assume that graphs can still be fully loaded into memory, thus the disk-based graph operation is inevitable. In this paper, we take the shortest path discovery as an example to investigate the technique issues when leveraging existing infrastructure of relational database (RDB) in the graph data management. Based on the observation that a variety of graph search queries can be implemented by iterative operations including selecting frontier nodes from visited nodes, making expansion from the selected frontier nodes, and merging the expanded nodes into the visited ones, we introduce a relational FEM framework with three corresponding operators to implement graph search tasks in the RDB context. We show new features such as window function and merge statement introduced by recent SQL standards can not only simplify the expression but also improve the performance of the FEM framework. In addition, we propose two optimization strategies specific to shortest path discovery inside the FEM framework. First, we take a bi-directional set Dijkstra's algorithm in the path finding. The bi-directional strategy can reduce the search space, and set Dijkstra's algorithm finds the shortest path in a set-at-a-time fashion. Second, we introduce an index named SegTable to preserve the local shortest segments, and exploit SegTable to further improve the performance. The final extensive experimental results illustrate our relational approach with the optimization strategies achieves high scalability and performance.

#index 1668641
#* Mining flipping correlations from large datasets with taxonomies
#@ Marina Barsky;Sangkyum Kim;Tim Weninger;Jiawei Han
#t 2011
#c 4
#% 152934
#% 227919
#% 280433
#% 300120
#% 300477
#% 392618
#% 464822
#% 478298
#% 481588
#% 481758
#% 577214
#% 767654
#% 799740
#% 835018
#% 1465175
#! In this paper we introduce a new type of pattern -- a flipping correlation pattern. The flipping patterns are obtained from contrasting the correlations between items at different levels of abstraction. They represent surprising correlations, both positive and negative, which are specific for a given abstraction level, and which "flip" from positive to negative and vice versa when items are generalized to a higher level of abstraction. We design an efficient algorithm for finding flipping correlations, the Flipper algorithm, which outperforms naïve pattern mining methods by several orders of magnitude. We apply Flipper to real-life datasets and show that the discovered patterns are non-redundant, surprising and actionable. Flipper finds strong contrasting correlations in itemsets with low-to-medium support, while existing techniques cannot handle the pattern discovery in this frequency range.

#index 1668642
#* A statistical approach towards robust progress estimation
#@ Arnd Christian König;Bolin Ding;Surajit Chaudhuri;Vivek Narasayya
#t 2011
#c 4
#% 344447
#% 476622
#% 765467
#% 765468
#% 800589
#% 810056
#% 840846
#% 960323
#% 960334
#% 1174742
#% 1206984
#% 1426544
#% 1581874
#% 1688297
#! The need for accurate SQL progress estimation in the context of decision support administration has led to a number of techniques proposed for this task. Unfortunately, no single one of these progress estimators behaves robustly across the variety of SQL queries encountered in practice, meaning that each technique performs poorly for a significant fraction of queries. This paper proposes a novel estimator selection framework that uses a statistical model to characterize the sets of conditions under which certain estimators outperform others, leading to a significant increase in estimation robustness. The generality of this framework also enables us to add a number of novel "special purpose" estimators which increase accuracy further. Most importantly, the resulting model generalizes well to queries very different from the ones used to train it. We validate our findings using a large number of industrial real-life and benchmark workloads.

#index 1707456
#* Relation strength-aware clustering of heterogeneous information networks with incomplete attributes
#@ Yizhou Sun;Charu C. Aggarwal;Jiawei Han
#t 2012
#c 4
#% 36672
#% 722902
#% 769881
#% 881460
#% 881514
#% 989586
#% 989618
#% 989636
#% 995140
#% 1055681
#% 1055741
#% 1117695
#% 1186295
#% 1214655
#% 1214701
#% 1214714
#% 1289267
#% 1318691
#% 1328161
#% 1328169
#% 1650298
#! With the rapid development of online social media, online shopping sites and cyber-physical systems, heterogeneous information networks have become increasingly popular and content-rich over time. In many cases, such networks contain multiple types of objects and links, as well as different kinds of attributes. The clustering of these objects can provide useful insights in many applications. However, the clustering of such networks can be challenging since (a) the attribute values of objects are often incomplete, which implies that an object may carry only partial attributes or even no attributes to correctly label itself; and (b) the links of different types may carry different kinds of semantic meanings, and it is a difficult task to determine the nature of their relative importance in helping the clustering for a given purpose. In this paper, we address these challenges by proposing a model-based clustering algorithm. We design a probabilistic model which clusters the objects of different types into a common hidden space, by using a user-specified set of attributes, as well as the links from different relations. The strengths of different types of links are automatically learned, and are determined by the given purpose of clustering. An iterative algorithm is designed for solving the clustering problem, in which the strengths of different types of links and the quality of clustering results mutually enhance each other. Our experimental results on real and synthetic data sets demonstrate the effectiveness and efficiency of the algorithm.

#index 1707457
#* Shortest path and distance queries on road networks: an experimental evaluation
#@ Lingkun Wu;Xiaokui Xiao;Dingxiong Deng;Gao Cong;Andy Diwen Zhu;Shuigeng Zhou
#t 2012
#c 4
#% 443208
#% 443533
#% 801679
#% 813718
#% 836179
#% 881686
#% 1063472
#% 1230568
#% 1328210
#% 1357841
#% 1412885
#% 1464046
#% 1484129
#% 1523971
#% 1676469
#! Computing the shortest path between two given locations in a road network is an important problem that finds applications in various map services and commercial navigation products. The state-of-the-art solutions for the problem can be divided into two categories: spatial-coherence-based methods and vertex-importance-based approaches. The two categories of techniques, however, have not been compared systematically under the same experimental framework, as they were developed from two independent lines of research that do not refer to each other. This renders it difficult for a practitioner to decide which technique should be adopted for a specific application. Furthermore, the experimental evaluation of the existing techniques, as presented in previous work, falls short in several aspects. Some methods were tested only on small road networks with up to one hundred thousand vertices; some approaches were evaluated using distance queries (instead of shortest path queries), namely, queries that ask only for the length of the shortest path; a state-of-the-art technique was examined based on a faulty implementation that led to incorrect query results. To address the above issues, this paper presents a comprehensive comparison of the most advanced spatial-coherence-based and vertex-importance-based approaches. Using a variety of real road networks with up to twenty million vertices, we evaluated each technique in terms of its preprocessing time, space consumption, and query efficiency (for both shortest path and distance queries). Our experimental results reveal the characteristics of different techniques, based on which we provide guidelines on selecting appropriate methods for various scenarios.

#index 1707458
#* The filter-placement problem and its application to minimizing information multiplicity
#@ Dóra Erdös;Vatche Ishakian;Andrei Lapets;Evimaria Terzi;Azer Bestavros
#t 2012
#c 4
#% 342371
#% 342596
#% 349696
#% 453525
#% 516255
#% 577217
#% 631923
#% 729923
#% 737972
#% 816494
#% 989613
#% 1102905
#% 1147666
#% 1214671
#% 1245030
#% 1269936
#% 1292553
#% 1396874
#% 1399992
#% 1858015
#! In many information networks, data items -- such as updates in social networks, news flowing through interconnected RSS feeds and blogs, measurements in sensor networks, route updates in ad-hoc networks -- propagate in an uncoordinated manner: nodes often relay information they receive to neighbors, independent of whether or not these neighbors received the same information from other sources. This uncoordinated data dissemination may result in significant, yet unnecessary communication and processing overheads, ultimately reducing the utility of information networks. To alleviate the negative impacts of this information multiplicity phenomenon, we propose that a subset of nodes (selected at key positions in the network) carry out additional information filtering functionality. Thus, nodes are responsible for the removal (or significant reduction) of the redundant data items relayed through them. We refer to such nodes as filters. We formally define the Filter Placement problem as a combinatorial optimization problem, and study its computational complexity for different types of graphs. We also present polynomial-time approximation algorithms and scalable heuristics for the problem. Our experimental results, which we obtained through extensive simulations on synthetic and real-world information flow networks, suggest that in many settings a relatively small number of filters are fairly effective in removing a large fraction of redundant information.

#index 1707459
#* Bayesian locality sensitive hashing for fast similarity search
#@ Venu Satuluri;Srinivasan Parthasarathy
#t 2012
#c 4
#% 117446
#% 249238
#% 249321
#% 255137
#% 347225
#% 479973
#% 762054
#% 763708
#% 879600
#% 939408
#% 955712
#% 956506
#% 956507
#% 1002007
#% 1022281
#% 1023422
#% 1055684
#% 1217189
#% 1354495
#% 1399992
#% 1400000
#% 1549865
#% 1581906
#% 1581929
#% 1590535
#% 1642162
#% 1707459
#! Given a collection of objects and an associated similarity measure, the all-pairs similarity search problem asks us to find all pairs of objects with similarity greater than a certain user-specified threshold. Locality-sensitive hashing (LSH) based methods have become a very popular approach for this problem. However, most such methods only use LSH for the first phase of similarity search - i.e. efficient indexing for candidate generation. In this paper, we present BayesLSH, a principled Bayesian algorithm for the subsequent phase of similarity search - performing candidate pruning and similarity estimation using LSH. A simpler variant, BayesLSH-Lite, which calculates similarities exactly, is also presented. Our algorithms are able to quickly prune away a large majority of the false positive candidate pairs, leading to significant speedups over baseline approaches. For BayesLSH, we also provide probabilistic guarantees on the quality of the output, both in terms of accuracy and recall. Finally, the quality of BayesLSH's output can be easily tuned and does not require any manual setting of the number of hashes to use for similarity estimation, unlike standard approaches. For two state-of-the-art candidate generation algorithms, AllPairs and LSH, BayesLSH enables significant speedups, typically in the range 2x-20x for a wide variety of datasets.

#index 1707460
#* Fast and exact top-k search for random walk with restart
#@ Yasuhiro Fujiwara;Makoto Nakatsuji;Makoto Onizuka;Masaru Kitsuregawa
#t 2012
#c 4
#% 258598
#% 730089
#% 769952
#% 780688
#% 844334
#% 871315
#% 881496
#% 915344
#% 976785
#% 1022280
#% 1055877
#% 1085164
#% 1127383
#% 1127384
#% 1206699
#% 1227601
#% 1265149
#% 1328169
#% 1355043
#% 1581921
#% 1607321
#! Graphs are fundamental data structures and have been employed for centuries to model real-world systems and phenomena. Random walk with restart (RWR) provides a good proximity score between two nodes in a graph, and it has been successfully used in many applications such as automatic image captioning, recommender systems, and link prediction. The goal of this work is to find nodes that have top-k highest proximities for a given node. Previous approaches to this problem find nodes efficiently at the expense of exactness. The main motivation of this paper is to answer, in the affirmative, the question, 'Is it possible to improve the search time without sacrificing the exactness?'. Our solution, K-dash, is based on two ideas: (1) It computes the proximity of a selected node efficiently by sparse matrices, and (2) It skips unnecessary proximity computations when searching for the top-k nodes. Theoretical analyses show that K-dash guarantees result exactness. We perform comprehensive experiments to verify the efficiency of K-dash. The results show that K-dash can find top-k nodes significantly faster than the previous approaches while it guarantees exactness.

#index 1707461
#* Densest subgraph in streaming and MapReduce
#@ Bahman Bahmani;Ravi Kumar;Sergei Vassilvitskii
#t 2012
#c 4
#% 175732
#% 379443
#% 379482
#% 511151
#% 674497
#% 713033
#% 751684
#% 771386
#% 824711
#% 866773
#% 881523
#% 894646
#% 956521
#% 956540
#% 963669
#% 1035579
#% 1083625
#% 1173140
#% 1217208
#% 1232289
#% 1372657
#% 1399956
#% 1442067
#% 1484141
#% 1560415
#% 1581411
#% 1581996
#% 1594588
#% 1699388
#% 1715022
#% 1847975
#! The problem of finding locally dense components of a graph is an important primitive in data analysis, with wide-ranging applications from community mining to spam detection and the discovery of biological network modules. In this paper we present new algorithms for finding the densest subgraph in the streaming model. For any ε 0, our algorithms make O(log1+ε n) passes over the input and find a subgraph whose density is guaranteed to be within a factor 2(1 + ε) of the optimum. Our algorithms are also easily parallelizable and we illustrate this by realizing them in the MapReduce model. In addition we perform extensive experimental evaluation on massive real-world graphs showing the performance and scalability of our algorithms in practice.

#index 1707462
#* Mining attribute-structure correlated patterns in large attributed graphs
#@ Arlei Silva;Wagner Meira, Jr.;Mohammed J. Zaki
#t 2012
#c 4
#% 152934
#% 443350
#% 823347
#% 824711
#% 864460
#% 881553
#% 956459
#% 1063629
#% 1083624
#% 1108882
#% 1133030
#% 1328169
#% 1426574
#% 1446960
#% 1482409
#% 1581924
#! In this work, we study the correlation between attribute sets and the occurrence of dense subgraphs in large attributed graphs, a task we call structural correlation pattern mining. A structural correlation pattern is a dense subgraph induced by a particular attribute set. Existing methods are not able to extract relevant knowledge regarding how vertex attributes interact with dense subgraphs. Structural correlation pattern mining combines aspects of frequent itemset and quasi-clique mining problems. We propose statistical significance measures that compare the structural correlation of attribute sets against their expected values using null models. Moreover, we evaluate the interestingness of structural correlation patterns in terms of size and density. An efficient algorithm that combines search and pruning strategies in the identification of the most relevant structural correlation patterns is presented. We apply our method for the analysis of three real-world attributed graphs: a collaboration, a music, and a citation network, verifying that it provides valuable knowledge in a feasible time.

#index 1707463
#* Semi-automatic index tuning: keeping DBAs in the loop
#@ Karl Schnaitter;Neoklis Polyzotis
#t 2012
#c 4
#% 131240
#% 152943
#% 261358
#% 480158
#% 632100
#% 875027
#% 1022293
#% 1063540
#% 1127352
#% 1127594
#% 1206953
#% 1207102
#% 1207103
#% 1218723
#% 1328212
#! To obtain a high level of system performance, a database administrator (DBA) must choose a set of indices that is appropriate for the workload. The system can aid in this challenging task by providing recommendations for the index configuration. We propose a new index recommendation technique, termed semi-automatic tuning, that keeps the DBA "in the loop" by generating recommendations that use feedback about the DBA's preferences. The technique also works online, which avoids the limitations of commercial tools that require the workload to be known in advance. The foundation of our approach is the Work Function Algorithm, which can solve a wide variety of online optimization problems with strong competitive guarantees. We present an experimental analysis that validates the benefits of semi-automatic tuning in a wide variety of conditions.

#index 1707464
#* Aggregation in probabilistic databases via knowledge compilation
#@ Robert Fink;Larisa Han;Dan Olteanu
#t 2012
#c 4
#% 430773
#% 903016
#% 960365
#% 976987
#% 1063521
#% 1070611
#% 1070886
#% 1111133
#% 1127376
#% 1272349
#% 1291116
#% 1424591
#% 1426451
#% 1581829
#% 1581914
#% 1581916
#% 1595897
#% 1615075
#! This paper presents a query evaluation technique for positive relational algebra queries with aggregates on a representation system for probabilistic data based on the algebraic structures of semiring and semimodule. The core of our evaluation technique is a procedure that compiles semimodule and semiring expressions into so-called decomposition trees, for which the computation of the probability distribution can be done in time linear in the product of the sizes of the probability distributions represented by its nodes. We give syntactic characterisations of tractable queries with aggregates by exploiting the connection between query tractability and polynomial-time decomposition trees. A prototype of the technique is incorporated in the probabilistic database engine SPROUT. We report on performance experiments with custom datasets and TPC-H data.

#index 1730730
#* Stochastic database cracking: towards robust adaptive indexing in main-memory column-stores
#@ Felix Halim;Stratos Idreos;Panagiotis Karras;Roland H. C. Yap
#t 2012
#c 4
#% 36119
#% 235242
#% 480153
#% 482100
#% 544469
#% 571047
#% 601017
#% 800491
#% 810111
#% 824697
#% 875062
#% 893130
#% 960268
#% 1016220
#% 1207102
#% 1217169
#% 1372713
#% 1545227
#% 1592316
#% 1594693
#! Modern business applications and scientific databases call for inherently dynamic data storage environments. Such environments are characterized by two challenging features: (a) they have little idle system time to devote on physical design; and (b) there is little, if any, a priori workload knowledge, while the query and data workload keeps changing dynamically. In such environments, traditional approaches to index building and maintenance cannot apply. Database cracking has been proposed as a solution that allows on-the-fly physical data reorganization, as a collateral effect of query processing. Cracking aims to continuously and automatically adapt indexes to the workload at hand, without human intervention. Indexes are built incrementally, adaptively, and on demand. Nevertheless, as we show, existing adaptive indexing methods fail to deliver workload-robustness; they perform much better with random workloads than with others. This frailty derives from the inelasticity with which these approaches interpret each query as a hint on how data should be stored. Current cracking schemes blindly reorganize the data within each query's range, even if that results into successive expensive operations with minimal indexing benefit. In this paper, we introduce stochastic cracking, a significantly more resilient approach to adaptive indexing. Stochastic cracking also uses each query as a hint on how to reorganize data, but not blindly so; it gains resilience and avoids performance bottlenecks by deliberately applying certain arbitrary choices in its decision-making. Thereby, we bring adaptive indexing forward to a mature formulation that confers the workload-robustness previous approaches lacked. Our extensive experimental study verifies that stochastic cracking maintains the desired properties of original database cracking while at the same time it performs well with diverse realistic workloads.

#index 1730731
#* An adaptive mechanism for accurate query answering under differential privacy
#@ Chao Li;Gerome Miklau
#t 2012
#c 4
#% 757953
#% 977011
#% 1198224
#% 1214684
#% 1217148
#% 1426328
#% 1426454
#% 1478165
#% 1496267
#% 1521654
#% 1523886
#% 1581864
#% 1581865
#% 1595893
#% 1732708
#% 1740518
#% 1846816
#! We propose a novel mechanism for answering sets of counting queries under differential privacy. Given a workload of counting queries, the mechanism automatically selects a different set of "strategy" queries to answer privately, using those answers to derive answers to the workload. The main algorithm proposed in this paper approximates the optimal strategy for any workload of linear counting queries. With no cost to the privacy guarantee, the mechanism improves significantly on prior approaches and achieves near-optimal error for many workloads, when applied under (ε, δ)-differential privacy. The result is an adaptive mechanism which can help users achieve good utility without requiring that they reason carefully about the best formulation of their task.

#index 1730732
#* SharedDB: killing one thousand queries with one stone
#@ Georgios Giannikis;Gustavo Alonso;Donald Kossmann
#t 2012
#c 4
#% 3771
#% 36117
#% 58377
#% 172968
#% 300167
#% 333848
#% 335726
#% 411554
#% 411750
#% 481916
#% 482100
#% 482121
#% 731408
#% 800593
#% 810039
#% 993956
#% 1022262
#% 1022298
#% 1052066
#% 1127399
#% 1206624
#% 1217170
#% 1328057
#% 1328132
#% 1328168
#% 1426545
#% 1563004
#% 1565407
#! Traditional database systems are built around the query-at-a-time model. This approach tries to optimize performance in a best-effort way. Unfortunately, best effort is not good enough for many modern applications. These applications require response time guarantees in high load situations. This paper describes the design of a new database architecture that is based on batching queries and shared computation across possibly hundreds of concurrent queries and updates. Performance experiments with the TPC-W benchmark show that the performance of our implementation, SharedDB, is indeed robust across a wide range of dynamic workloads.

#index 1730733
#* Pushing the boundaries of crowd-enabled databases with query-driven schema expansion
#@ Joachim Selke;Christoph Lofi;Wolf-Tilo Balke
#t 2012
#c 4
#% 466263
#% 734592
#% 768632
#% 871574
#% 922925
#% 960279
#% 961195
#% 1047347
#% 1074060
#% 1083692
#% 1227643
#% 1246173
#% 1357698
#% 1384112
#% 1428419
#% 1450886
#% 1472273
#% 1476454
#% 1480225
#% 1581851
#% 1605920
#% 1625382
#! By incorporating human workers into the query execution process crowd-enabled databases facilitate intelligent, social capabilities like completing missing data at query time or performing cognitive operators. But despite all their flexibility, crowd-enabled databases still maintain rigid schemas. In this paper, we extend crowd-enabled databases by flexible query-driven schema expansion, allowing the addition of new attributes to the database at query time. However, the number of crowd-sourced mini-tasks to fill in missing values may often be prohibitively large and the resulting data quality is doubtful. Instead of simple crowd-sourcing to obtain all values individually, we leverage the usergenerated data found in the Social Web: By exploiting user ratings we build perceptual spaces, i.e., highly-compressed representations of opinions, impressions, and perceptions of large numbers of users. Using few training samples obtained by expert crowd sourcing, we then can extract all missing data automatically from the perceptual space with high quality and at low costs. Extensive experiments show that our approach can boost both performance and quality of crowd-enabled databases, while also providing the flexibility to expand schemas in a query-driven fashion.

#index 1730734
#* A Bayesian approach to discovering truth from conflicting sources for data integration
#@ Bo Zhao;Benjamin I. P. Rubinstein;Jim Gemmell;Jiawei Han
#t 2012
#c 4
#% 273687
#% 290830
#% 482108
#% 989682
#% 1313373
#% 1328155
#% 1328156
#% 1355029
#% 1484339
#% 1491640
#% 1536551
#% 1560376
#% 1560377
#% 1826432
#! In practical data integration systems, it is common for the data sources being integrated to provide conflicting information about the same entity. Consequently, a major challenge for data integration is to derive the most complete and accurate integrated records from diverse and sometimes conflicting sources. We term this challenge the truth finding problem. We observe that some sources are generally more reliable than others, and therefore a good model of source quality is the key to solving the truth finding problem. In this work, we propose a probabilistic graphical model that can automatically infer true records and source quality without any supervision. In contrast to previous methods, our principled approach leverages a generative process of two types of errors (false positive and false negative) by modeling two different aspects of source quality. In so doing, ours is also the first approach designed to merge multi-valued attribute types. Our method is scalable, due to an efficient sampling-based inference algorithm that needs very few iterations in practice and enjoys linear time complexity, with an even faster incremental variant. Experiments on two real world datasets show that our new method outperforms existing state-of-the-art approaches to the truth finding problem.

#index 1730735
#* How to price shared optimizations in the cloud
#@ Prasang Upadhyaya;Magdalena Balazinska;Dan Suciu
#t 2012
#c 4
#% 115568
#% 203572
#% 571217
#% 571287
#% 713046
#% 715028
#% 723934
#% 771659
#% 963595
#% 1000451
#% 1065616
#% 1206951
#% 1426493
#% 1475072
#% 1475073
#% 1478632
#% 1486236
#% 1581873
#% 1594595
#! Data-management-as-a-service systems are increasingly being used in collaborative settings, where multiple users access common datasets. Cloud providers have the choice to implement various optimizations, such as indexing or materialized views, to accelerate queries over these datasets. Each optimization carries a cost and may benefit multiple users. This creates a major challenge: how to select which optimizations to perform and how to share their cost among users. The problem is especially challenging when users are selfish and will only report their true values for different optimizations if doing so maximizes their utility. In this paper, we present a new approach for selecting and pricing shared optimizations by using Mechanism Design. We first show how to apply the Shapley Value Mechanism to the simple case of selecting and pricing additive optimizations, assuming an offline game where all users access the service for the same time-period. Second, we extend the approach to online scenarios where users come and go. Finally, we consider the case of substitutive optimizations. We show analytically that our mechanisms induce truthfulness and recover the optimization costs. We also show experimentally that our mechanisms yield higher utility than the state-of-the-art approach based on regret accumulation.

#index 1730736
#* Dense subgraph maintenance under streaming edge weight updates for real-time story identification
#@ Albert Angel;Nikos Sarkas;Nick Koudas;Divesh Srivastava
#t 2012
#c 4
#% 232768
#% 300120
#% 310514
#% 379443
#% 479658
#% 498852
#% 578388
#% 674497
#% 732477
#% 824711
#% 881514
#% 992089
#% 1022269
#% 1063503
#% 1181258
#% 1217245
#% 1232289
#% 1332132
#% 1349835
#% 1426611
#% 1491310
#% 1581911
#% 1667812
#% 1730736
#! Recent years have witnessed an unprecedented proliferation of social media. People around the globe author, every day, millions of blog posts, micro-blog posts, social network status updates, etc. This rich stream of information can be used to identify, on an ongoing basis, emerging stories, and events that capture popular attention. Stories can be identified via groups of tightly-coupled real-world entities, namely the people, locations, products, etc., that are involved in the story. The sheer scale, and rapid evolution of the data involved necessitate highly efficient techniques for identifying important stories at every point of time. The main challenge in real-time story identification is the maintenance of dense subgraphs (corresponding to groups of tightly-coupled entities) under streaming edge weight updates (resulting from a stream of user-generated content). This is the first work to study the efficient maintenance of dense subgraphs under such streaming edge weight updates. For a wide range of definitions of density, we derive theoretical results regarding the magnitude of change that a single edge weight update can cause. Based on these, we propose a novel algorithm, DynDens, which outperforms adaptations of existing techniques to this setting, and yields meaningful results. Our approach is validated by a thorough experimental evaluation on large-scale real and synthetic datasets.

#index 1730737
#* ReStore: reusing results of MapReduce jobs
#@ Iman Elghandour;Ashraf Aboulnaga
#t 2012
#c 4
#% 572311
#% 810111
#% 963669
#% 1063553
#% 1127427
#% 1217212
#% 1328060
#% 1328095
#% 1426588
#% 1523839
#% 1770398
#! Analyzing large scale data has emerged as an important activity for many organizations in the past few years. This large scale data analysis is facilitated by the MapReduce programming and execution model and its implementations, most notably Hadoop. Users of MapReduce often have analysis tasks that are too complex to express as individual MapReduce jobs. Instead, they use high-level query languages such as Pig, Hive, or Jaql to express their complex tasks. The compilers of these languages translate queries into workflows of MapReduce jobs. Each job in these workflows reads its input from the distributed file system used by the MapReduce system and produces output that is stored in this distributed file system and read as input by the next job in the workflow. The current practice is to delete these intermediate results from the distributed file system at the end of executing the workflow. One way to improve the performance of workflows of MapReduce jobs is to keep these intermediate results and reuse them for future workflows submitted to the system. In this paper, we present ReStore, a system that manages the storage and reuse of such intermediate results. ReStore can reuse the output of whole MapReduce jobs that are part of a workflow, and it can also create additional reuse opportunities by materializing and storing the output of query execution operators that are executed within a MapReduce job. We have implemented ReStore as an extension to the Pig dataflow system on top of Hadoop, and we experimentally demonstrate significant speedups on queries from the PigMix benchmark.

#index 1741027
#* PerfXplain: debugging MapReduce job performance
#@ Nodira Khoussainova;Magdalena Balazinska;Dan Suciu
#t 2012
#c 4
#% 136350
#% 248815
#% 465758
#% 480158
#% 482100
#% 810111
#% 926881
#% 954300
#% 963669
#% 983467
#% 1016221
#% 1022202
#% 1063553
#% 1127559
#% 1206851
#% 1206984
#% 1318701
#% 1328213
#% 1354118
#% 1426210
#% 1426488
#% 1426544
#% 1426630
#% 1468271
#% 1468421
#% 1523836
#% 1523837
#% 1523841
#% 1523897
#% 1573238
#! While users today have access to many tools that assist in performing large scale data analysis tasks, understanding the performance characteristics of their parallel computations, such as MapReduce jobs, remains difficult. We present PerfXplain, a system that enables users to ask questions about the relative performances (i.e., runtimes) of pairs of MapReduce jobs. PerfXplain provides a new query language for articulating performance queries and an algorithm for generating explanations from a log of past MapReduce job executions. We formally define the notion of an explanation together with three metrics, relevance, precision, and generality, that measure explanation quality. We present the explanation-generation algorithm based on techniques related to decision-tree building. We evaluate the approach on a log of past executions on Amazon EC2, and show that our approach can generate quality explanations, outperforming two naïve explanation-generation methods.

#index 1741028
#* Uncertain centroid based partitional clustering of uncertain data
#@ Francesco Gullo;Andrea Tagarelli
#t 2012
#c 4
#% 36672
#% 771228
#% 823402
#% 833970
#% 844385
#% 915307
#% 1030777
#% 1063728
#% 1111126
#% 1176860
#% 1179162
#% 1291115
#% 1464052
#% 1497994
#% 1535394
#% 1669893
#! Clustering uncertain data has emerged as a challenging task in uncertain data management and mining. Thanks to a computational complexity advantage over other clustering paradigms, partitional clustering has been particularly studied and a number of algorithms have been developed. While existing proposals differ mainly in the notions of cluster centroid and clustering objective function, little attention has been given to an analysis of their characteristics and limits. In this work, we theoretically investigate major existing methods of partitional clustering, and alternatively propose a well-founded approach to clustering uncertain data based on a novel notion of cluster centroid. A cluster centroid is seen as an uncertain object defined in terms of a random variable whose realizations are derived based on all deterministic representations of the objects to be clustered. As demonstrated theoretically and experimentally, this allows for better representing a cluster of uncertain objects, thus supporting a consistently improved clustering performance while maintaining comparable efficiency with existing partitional clustering algorithms.

#index 1741029
#* Scalable k-means++
#@ Bahman Bahmani;Benjamin Moseley;Andrea Vattani;Ravi Kumar;Sergei Vassilvitskii
#t 2012
#c 4
#% 210173
#% 248790
#% 296738
#% 320942
#% 334990
#% 552172
#% 578388
#% 766665
#% 771247
#% 785121
#% 803601
#% 845220
#% 871356
#% 898281
#% 956521
#% 963669
#% 991230
#% 1023380
#% 1183591
#% 1335135
#% 1354118
#% 1399956
#% 1400102
#% 1484141
#% 1567038
#% 1581927
#% 1581996
#% 1605989
#% 1707461
#! Over half a century old and showing no signs of aging, k-means remains one of the most popular data processing algorithms. As is well-known, a proper initialization of k-means is crucial for obtaining a good final solution. The recently proposed k-means++ initialization algorithm achieves this, obtaining an initial set of centers that is provably close to the optimum solution. A major downside of the k-means++ is its inherent sequential nature, which limits its applicability to massive data: one must make k passes over the data to find a good initial set of centers. In this work we show how to drastically reduce the number of passes needed to obtain, in parallel, a good initialization. This is unlike prevailing efforts on parallelizing k-means that have mostly focused on the post-initialization phases of k-means. We prove that our proposed initialization algorithm k-means|| obtains a nearly optimal solution after a logarithmic number of passes, and then show that in practice a constant number of passes suffices. Experimental evaluation on real-world large-scale data demonstrates that k-means|| outperforms k-means++ in both sequential and parallel settings.

#index 1741030
#* Querying schemas with access restrictions
#@ Michael Benedikt;Pierre Bourhis;Clemens Ley
#t 2012
#c 4
#% 101955
#% 198466
#% 321054
#% 342359
#% 368248
#% 384978
#% 630964
#% 726626
#% 801698
#% 942360
#% 943616
#% 1180015
#% 1180017
#% 1538776
#% 1581835
#! We study verification of systems whose transitions consist of accesses to a Web-based data-source. An access is a lookup on a relation within a relational database, fixing values for a set of positions in the relation. For example, a transition can represent access to a Web form, where the user is restricted to filling in values for a particular set of fields. We look at verifying properties of a schema describing the possible accesses of such a system. We present a language where one can describe the properties of an access path, and also specify additional restrictions on accesses that are enforced by the schema. Our main property language, AccLTL, is based on a first-order extension of linear-time temporal logic, interpreting access paths as sequences of relational structures. We also present a lower-level automaton model, A-automata, which AccLTL specifications can compile into. We show that AccLTL and A-automata can express static analysis problems related to "querying with limited access patterns" that have been studied in the database literature in the past, such as whether an access is relevant to answering a query, and whether two queries are equivalent in the accessible data they can return. We prove decidability and complexity results for several restrictions and variants of AccLTL, and explain which properties of paths can be expressed in each restriction.

#index 1741031
#* Definition, detection, and recovery of single-page failures, a fourth class of database failures
#@ Goetz Graefe;Harumi Kuno
#t 2012
#c 4
#% 117
#% 114582
#% 286929
#% 287715
#% 317933
#% 317988
#% 403195
#% 464055
#% 531907
#% 967021
#% 1016185
#% 1053487
#% 1581869
#% 1680296
#% 1745137
#! The three traditional failure classes are system, media, and transaction failures. Sometimes, however, modern storage exhibits failures that differ from all of those. In order to capture and describe such cases, single-page failures are introduced as a fourth failure class. This class encompasses all failures to read a data page correctly and with plausible contents despite all correction attempts in lower system levels. Efficient recovery seems to require a new data structure called the page recovery index. Its transactional maintenance can be accomplished writing the same number of log records as today's efficient implementations of logging and recovery. Detection and recovery of a single-page failure can be sufficiently fast that the affected data access is merely delayed, without the need to abort the transaction.

#index 1741032
#* Concurrency control for adaptive indexing
#@ Goetz Graefe;Felix Halim;Stratos Idreos;Harumi Kuno;Stefan Manegold
#t 2012
#c 4
#% 117
#% 36119
#% 64791
#% 114582
#% 287672
#% 287715
#% 403195
#% 463917
#% 544469
#% 875062
#% 960268
#% 997495
#% 1022202
#% 1207102
#% 1217169
#% 1372713
#% 1417228
#% 1426413
#% 1545227
#% 1574740
#% 1592316
#% 1680296
#% 1730730
#% 1745137
#! Adaptive indexing initializes and optimizes indexes incrementally, as a side effect of query processing. The goal is to achieve the benefits of indexes while hiding or minimizing the costs of index creation. However, index-optimizing side effects seem to turn read-only queries into update transactions that might, for example, create lock contention. This paper studies concurrency control in the context of adaptive indexing. We show that the design and implementation of adaptive indexing rigorously separates index structures from index contents; this relaxes the constraints and requirements during adaptive indexing compared to those of traditional index updates. Our design adapts to the fact that an adaptive index is refined continuously, and exploits any concurrency opportunities in a dynamic way. A detailed experimental analysis demonstrates that (a) adaptive indexing maintains its adaptive properties even when running concurrent queries, (b) adaptive indexing can exploit the opportunity for parallelism due to concurrent queries, (c) the number of concurrency conflicts and any concurrency administration overheads follow an adaptive behavior, decreasing as the workload evolves and adapting to the workload needs.

#index 1741033
#* Comments on "Stack-based Algorithms for Pattern Matching on DAGs"
#@ Qiang Zeng;Hai Zhuge
#t 2012
#c 4
#% 58365
#% 397375
#% 824692
#% 994015
#% 1126664
#% 1127388
#% 1494958
#% 1583282
#! The paper "Stack-based Algorithms for Pattern Matching on DAGs" generalizes the classical holistic twig join algorithms and proposes PathStackD, TwigStackD and DagStackD to respectively evaluate path, twig and DAG pattern queries on directed acyclic graphs. In this paper, we investigate the major results of that paper, pointing out several discrepancies and proposing solutions to resolving them. We show that the original algorithms do not find particular types of query solutions that are common in practice. We also analyze the effect of an underlying assumption on the correctness of the algorithms and discuss the pre-filtering process that the original work proposes to prune redundant nodes. Our experimental study on both real and synthetic data substantiates our conclusions.

#index 1741034
#* An analysis of structured data on the web
#@ Nilesh Dalvi;Ashwin Machanavajjhala;Bo Pang
#t 2012
#c 4
#% 480824
#% 654469
#% 715251
#% 754068
#% 1019175
#% 1022235
#% 1044500
#% 1127393
#% 1131145
#% 1176941
#% 1217114
#% 1275182
#% 1328133
#% 1328199
#% 1355036
#% 1355303
#% 1523846
#% 1536549
#% 1538764
#! In this paper, we analyze the nature and distribution of structured data on the Web. Web-scale information extraction, or the problem of creating structured tables using extraction from the entire web, is gathering lots of research interest. We perform a study to understand and quantify the value of Web-scale extraction, and how structured information is distributed amongst top aggregator websites and tail sites for various interesting domains. We believe this is the first study of its kind, and gives us new insights for information extraction over the Web.

#index 1769263
#* Shortest path computation with no information leakage
#@ Kyriakos Mouratidis;Man Lung Yiu
#t 2012
#c 4
#% 443533
#% 452685
#% 576761
#% 593711
#% 810042
#% 813718
#% 874980
#% 893151
#% 907397
#% 937067
#% 956531
#% 1013611
#% 1063472
#% 1063478
#% 1206712
#% 1208246
#% 1217157
#% 1217357
#% 1292535
#% 1326692
#% 1328196
#% 1409349
#% 1442468
#% 1523850
#% 1523862
#% 1563208
#% 1581880
#% 1643292
#% 1672385
#% 1719090
#! Shortest path computation is one of the most common queries in location-based services (LBSs). Although particularly useful, such queries raise serious privacy concerns. Exposing to a (potentially untrusted) LBS the client's position and her destination may reveal personal information, such as social habits, health condition, shopping preferences, lifestyle choices, etc. The only existing method for privacy-preserving shortest path computation follows the obfuscation paradigm; it prevents the LBS from inferring the source and destination of the query with a probability higher than a threshold. This implies, however, that the LBS still deduces some information (albeit not exact) about the client's location and her destination. In this paper we aim at strong privacy, where the adversary learns nothing about the shortest path query. We achieve this via established private information retrieval techniques, which we treat as black-box building blocks. Experiments on real, large-scale road networks assess the practicality of our schemes.

#index 1769264
#* V-SMART-join: a scalable mapreduce framework for all-pair similarity joins of multisets and vectors
#@ Ahmed Metwally;Christos Faloutsos
#t 2012
#c 4
#% 249321
#% 255137
#% 347225
#% 616528
#% 718437
#% 723279
#% 765463
#% 864392
#% 879600
#% 893164
#% 954300
#% 956506
#% 956518
#% 963669
#% 1026845
#% 1055684
#% 1063553
#% 1117074
#% 1127555
#% 1206665
#% 1214695
#% 1215321
#% 1269775
#% 1328095
#% 1426543
#% 1467704
#% 1468421
#% 1535356
#% 1605940
#% 1693964
#! This work proposes V-SMART-Join, a scalable MapReduce-based framework for discovering all pairs of similar entities. The V-SMART-Join framework is applicable to sets, multisets, and vectors. V-SMART-Join is motivated by the observed skew in the underlying distributions of Internet traffic, and is a family of 2-stage algorithms, where the first stage computes and joins the partial results, and the second stage computes the similarity exactly for all candidate pairs. The V-SMART-Join algorithms are very efficient and scalable in the number of entities, as well as their cardinalities. They were up to 30 times faster than the state of the art algorithm, VCL, when compared on a real dataset of a small size. We also established the scalability of the proposed algorithms by running them on a dataset of a realistic size, on which VCL never succeeded to finish. Experiments were run using real datasets of IPs and cookies, where each IP is represented as a multiset of cookies, and the goal is to discover similar IPs to identify Internet proxies.

#index 1769265
#* Distributed GraphLab: a framework for machine learning and data mining in the cloud
#@ Yucheng Low;Danny Bickson;Joseph Gonzalez;Carlos Guestrin;Aapo Kyrola;Joseph M. Hellerstein
#t 2012
#c 4
#% 51999
#% 102917
#% 193442
#% 258598
#% 269195
#% 321546
#% 602821
#% 703555
#% 963669
#% 983467
#% 1019798
#% 1030869
#% 1102242
#% 1318636
#% 1417076
#% 1426513
#% 1426603
#% 1468231
#% 1475077
#% 1492395
#% 1523858
#% 1526993
#% 1560415
#% 1581996
#% 1621137
#! While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.

#index 1769266
#* Adding logical operators to tree pattern queries on graph-structured data
#@ Qiang Zeng;Xiaorui Jiang;Hai Zhuge
#t 2012
#c 4
#% 58365
#% 333989
#% 379482
#% 397374
#% 397375
#% 654450
#% 745463
#% 765406
#% 824667
#% 824692
#% 864462
#% 893112
#% 994015
#% 1013630
#% 1015274
#% 1063500
#% 1063515
#% 1127388
#% 1206666
#% 1206685
#% 1217208
#% 1328183
#% 1523818
#% 1531325
#% 1537098
#% 1583282
#% 1669487
#% 1716933
#% 1741033
#% 1903461
#! As data are increasingly modeled as graphs for expressing complex relationships, the tree pattern query on graph-structured data becomes an important type of queries in real-world applications. Most practical query languages, such as XQuery and SPARQL, support logical expressions using logical-AND/OR/NOT operators to define structural constraints of tree patterns. In this paper, (1) we propose generalized tree pattern queries (GTPQs) over graph-structured data, which fully support propositional logic of structural constraints. (2) We make a thorough study of fundamental problems including satisfiability, containment and minimization, and analyze the computational complexity and the decision procedures of these problems. (3) We propose a compact graph representation of intermediate results and a pruning approach to reduce the size of intermediate results and the number of join operations -- two factors that often impair the efficiency of traditional algorithms for evaluating tree pattern queries. (4) We present an efficient algorithm for evaluating GTPQs using 3-hop as the underlying reachability index. (5) Experiments on both real-life and synthetic data sets demonstrate the effectiveness and efficiency of our algorithm, from several times to orders of magnitude faster than state-of-the-art algorithms in terms of evaluation time, even for traditional tree pattern queries with only conjunctive operations.

#index 1769267
#* Learning semantic string transformations from examples
#@ Rishabh Singh;Sumit Gulwani
#t 2012
#c 4
#% 524027
#% 720012
#% 765433
#% 875066
#% 893116
#% 913783
#% 1206615
#% 1217187
#% 1328152
#% 1424594
#% 1426357
#% 1451380
#% 1528293
#% 1584741
#% 1792729
#% 1890766
#! We address the problem of performing semantic transformations on strings, which may represent a variety of data types (or their combination) such as a column in a relational table, time, date, currency, etc. Unlike syntactic transformations, which are based on regular expressions and which interpret a string as a sequence of characters, semantic transformations additionally require exploiting the semantics of the data type represented by the string, which may be encoded as a database of relational tables. Manually performing such transformations on a large collection of strings is error prone and cumbersome, while programmatic solutions are beyond the skill-set of end-users. We present a programming by example technology that allows end-users to automate such repetitive tasks. We describe an expressive transformation language for semantic manipulation that combines table lookup operations and syntactic manipulations. We then present a synthesis algorithm that can learn all transformations in the language that are consistent with the user-provided set of input-output examples. We have implemented this technology as an add-in for the Microsoft Excel Spreadsheet system and have evaluated it successfully over several benchmarks picked from various Excel help-forums.

#index 1769268
#* Cologne: a declarative distributed constraint optimization platform
#@ Changbin Liu;Lu Ren;Boon Thau Loo;Yun Mao;Prithwish Basu
#t 2012
#c 4
#% 718308
#% 761357
#% 777791
#% 820335
#% 874978
#% 1131646
#% 1171818
#% 1206880
#% 1246358
#% 1246527
#% 1301611
#% 1468213
#% 1468393
#% 1468409
#% 1621150
#! This paper presents Cologne, a declarative optimization platform that enables constraint optimization problems (COPs) to be declaratively specified and incrementally executed in distributed systems. Cologne integrates a declarative networking engine with an off-the-shelf constraint solver. We have developed the Colog language that combines distributed Datalog used in declarative networking with language constructs for specifying goals and constraints used in COPs. Cologne uses novel query processing strategies for processing Colog programs, by combining the use of bottom-up distributed Datalog evaluation with top-down goal-oriented constraint solving. Using case studies based on cloud and wireless network optimizations, we demonstrate that Cologne (1) can flexibly support a wide range of policy-based optimizations in distributed systems, (2) results in orders of magnitude less code compared to imperative implementations, and (3) is highly efficient with low overhead and fast convergence times.

#index 1769269
#* Optimizing I/O for big array analytics
#@ Yi Zhang;Jun Yang
#t 2012
#c 4
#% 13742
#% 36117
#% 100621
#% 154255
#% 176391
#% 288520
#% 300166
#% 481290
#% 779564
#% 810039
#% 893867
#% 1022262
#% 1062575
#% 1394345
#% 1426545
#% 1426582
#% 1495665
#% 1736370
#% 1741949
#! Big array analytics is becoming indispensable in answering important scientific and business questions. Most analysis tasks consist of multiple steps, each making one or multiple passes over the arrays to be analyzed and generating intermediate results. In the big data setting, I/O optimization is a key to efficient analytics. In this paper, we develop a framework and techniques for capturing a broad range of analysis tasks expressible in nested-loop forms, representing them in a declarative way, and optimizing their I/O by identifying sharing opportunities. Experiment results show that our optimizer is capable of finding execution plans that exploit nontrivial I/O sharing opportunities with significant savings.

#index 1769270
#* Probabilistically bounded staleness for practical partial quorums
#@ Peter Bailis;Shivaram Venkataraman;Michael J. Franklin;Joseph M. Hellerstein;Ion Stoica
#t 2012
#c 4
#% 3083
#% 18612
#% 35764
#% 69312
#% 230631
#% 251230
#% 251359
#% 320187
#% 344920
#% 397295
#% 398237
#% 476793
#% 480332
#% 480431
#% 480798
#% 602675
#% 636091
#% 717164
#% 816490
#% 859470
#% 889932
#% 998845
#% 1111848
#% 1213944
#% 1216351
#% 1468530
#% 1584976
#% 1625057
#% 1625058
#% 1654050
#% 1666581
#% 1703664
#% 1725404
#% 1733706
#% 1862759
#% 1889760
#! Data store replication results in a fundamental trade-off between operation latency and data consistency. In this paper, we examine this trade-off in the context of quorum-replicated data stores. Under partial, or non-strict quorum replication, a data store waits for responses from a subset of replicas before answering a query, without guaranteeing that read and write replica sets intersect. As deployed in practice, these configurations provide only basic eventual consistency guarantees, with no limit to the recency of data returned. However, anecdotally, partial quorums are often "good enough" for practitioners given their latency benefits. In this work, we explain why partial quorums are regularly acceptable in practice, analyzing both the staleness of data they return and the latency benefits they offer. We introduce Probabilistically Bounded Staleness (PBS) consistency, which provides expected bounds on staleness with respect to both versions and wall clock time. We derive a closed-form solution for versioned staleness as well as model real-time staleness for representative Dynamo-style systems under internet-scale production workloads. Using PBS, we measure the latency-consistency trade-off for partial quorum systems. We quantitatively demonstrate how eventually consistent systems frequently return consistent data within tens of milliseconds while offering significant latency benefits.

#index 1848107
#* Efficient subgraph matching on billion node graphs
#@ Zhao Sun;Hongzhi Wang;Haixun Wang;Bin Shao;Jianzhong Li
#t 2012
#c 4
#% 140385
#% 224082
#% 288990
#% 341704
#% 387508
#% 750711
#% 772884
#% 810072
#% 1014225
#% 1063500
#% 1181229
#% 1206699
#% 1206702
#% 1328183
#% 1366460
#% 1372657
#% 1380974
#% 1399937
#% 1426513
#% 1523825
#% 1523900
#% 1529319
#% 1770359
#% 1770371
#% 1848108
#! The ability to handle large scale graph data is crucial to an increasing number of applications. Much work has been dedicated to supporting basic graph operations such as subgraph matching, reachability, regular expression matching, etc. In many cases, graph indices are employed to speed up query processing. Typically, most indices require either super-linear indexing time or super-linear indexing space. Unfortunately, for very large graphs, super-linear approaches are almost always infeasible. In this paper, we study the problem of subgraph matching on billion-node graphs. We present a novel algorithm that supports efficient subgraph matching for graphs deployed on a distributed memory store. Instead of relying on super-linear indices, we use efficient graph exploration and massive parallel computing for query processing. Our experimental results demonstrate the feasibility of performing subgraph matching on web-scale graph data.

#index 1848108
#* Efficient subgraph similarity search on large probabilistic graph databases
#@ Ye Yuan;Guoren Wang;Lei Chen;Haixun Wang
#t 2012
#c 4
#% 408396
#% 729938
#% 730089
#% 754098
#% 765429
#% 772884
#% 810072
#% 810120
#% 818434
#% 821926
#% 864425
#% 906419
#% 976984
#% 1022236
#% 1179162
#% 1332386
#% 1372657
#% 1372711
#% 1451203
#% 1464049
#% 1581859
#% 1592313
#% 1697228
#% 1846694
#% 1848107
#! Many studies have been conducted on seeking the efficient solution for subgraph similarity search over certain (deterministic) graphs due to its wide application in many fields, including bioinformatics, social network analysis, and Resource Description Framework (RDF) data management. All these works assume that the underlying data are certain. However, in reality, graphs are often noisy and uncertain due to various factors, such as errors in data extraction, inconsistencies in data integration, and privacy preserving purposes. Therefore, in this paper, we study subgraph similarity search on large probabilistic graph databases. Different from previous works assuming that edges in an uncertain graph are independent of each other, we study the uncertain graphs where edges' occurrences are correlated. We formally prove that subgraph similarity search over probabilistic graphs is #P-complete, thus, we employ a filter-and-verify framework to speed up the search. In the filtering phase, we develop tight lower and upper bounds of subgraph similarity probability based on a probabilistic matrix index, PMI. PMI is composed of discriminative subgraph features associated with tight lower and upper bounds of subgraph isomorphism probability. Based on PMI, we can sort out a large number of probabilistic graphs and maximize the pruning capability. During the verification phase, we develop an efficient sampling algorithm to validate the remaining candidates. The efficiency of our proposed solutions has been verified through extensive experiments.

#index 1848109
#* Truss decomposition in massive networks
#@ Jia Wang;James Cheng
#t 2012
#c 4
#% 41684
#% 268040
#% 322619
#% 498852
#% 823347
#% 1124590
#% 1245882
#% 1426539
#% 1523970
#% 1560415
#% 1594586
#% 1605988
#% 1625106
#% 1872379
#% 1907284
#! The k-truss is a type of cohesive subgraphs proposed recently for the study of networks. While the problem of computing most cohesive subgraphs is NP-hard, there exists a polynomial time algorithm for computing k-truss. Compared with k-core which is also efficient to compute, k-truss represents the "core" of a k-core that keeps the key information of, while filtering out less important information from, the k-core. However, existing algorithms for computing k-truss are inefficient for handling today's massive networks. We first improve the existing in-memory algorithm for computing k-truss in networks of moderate size. Then, we propose two I/O-efficient algorithms to handle massive networks that cannot fit in main memory. Our experiments on real datasets verify the efficiency of our algorithms and the value of k-truss.

#index 1848110
#* Seal: spatio-textual similarity search
#@ Ju Fan;Guoliang Li;Lizhu Zhou;Shanshan Chen;Jun Hu
#t 2012
#c 4
#% 464876
#% 480654
#% 765463
#% 818938
#% 838407
#% 864392
#% 874993
#% 893164
#% 956506
#% 982560
#% 1206801
#% 1206821
#% 1206997
#% 1328137
#% 1523828
#% 1523903
#% 1581875
#% 1581876
#% 1581877
#% 1594674
#% 1654056
#% 1846749
#! Location-based services (LBS) have become more and more ubiquitous recently. Existing methods focus on finding relevant points-of-interest (POIs) based on users' locations and query keywords. Nowadays, modern LBS applications generate a new kind of spatio-textual data, regions-of-interest (ROIs), containing region-based spatial information and textual description, e.g., mobile user profiles with active regions and interest tags. To satisfy search requirements on ROIs, we study a new research problem, called spatio-textual similarity search: Given a set of ROIs and a query ROI, we find the similar ROIs by considering spatial overlap and textual similarity. Spatio-textual similarity search has many important applications, e.g., social marketing in location-aware social networks. It calls for an efficient search method to support large scales of spatio-textual data in LBS systems. To this end, we introduce a filter-and-verification framework to compute the answers. In the filter step, we generate signatures for the ROIs and the query, and utilize the signatures to generate candidates whose signatures are similar to that of the query. In the verification step, we verify the candidates and identify the final answers. To achieve high performance, we generate effective high-quality signatures, and devise efficient filtering algorithms as well as pruning techniques. Experimental results on real and synthetic datasets show that our method achieves high performance.

#index 1848111
#* On the spatiotemporal burstiness of terms
#@ Theodoros Lappas;Marcos R. Vieira;Dimitrios Gunopulos;Vassilis J. Tsotras
#t 2012
#c 4
#% 30042
#% 210347
#% 333854
#% 469401
#% 527319
#% 542891
#% 577220
#% 729943
#% 745458
#% 765412
#% 769899
#% 823409
#% 824666
#% 869516
#% 869596
#% 878300
#% 956682
#% 1117049
#% 1127436
#% 1190134
#% 1206639
#% 1214669
#% 1298864
#% 1298889
#% 1372716
#% 1400018
#% 1400104
#% 1426523
#% 1523860
#% 1523892
#% 1669956
#! Thousands of documents are made available to the users via the web on a daily basis. One of the most extensively studied problems in the context of such document streams is burst identification. Given a term t, a burst is generally exhibited when an unusually high frequency is observed for t. While spatial and temporal burstiness have been studied individually in the past, our work is the first to simultaneously track and measure spatiotemporal term burstiness. In addition, we use the mined burstiness information toward an efficient document-search engine: given a user's query of terms, our engine returns a ranked list of documents discussing influential events with a strong spatiotemporal impact. We demonstrate the efficiency of our methods with an extensive experimental evaluation on real and synthetic datasets.

#index 1848112
#* Efficient reachability query evaluation in large spatiotemporal contact datasets
#@ Houtan Shirani-Mehr;Farnoush Banaei-Kashani;Cyrus Shahabi
#t 2012
#c 4
#% 397472
#% 548494
#% 772440
#% 814352
#% 864473
#% 994341
#% 1001880
#% 1061832
#% 1063472
#% 1074714
#% 1214770
#% 1314379
#% 1480795
#% 1523819
#% 1618256
#% 1633183
#! With the advent of reliable positioning technologies and prevalence of location-based services, it is now feasible to accurately study the propagation of items such as infectious viruses, sensitive information pieces, and malwares through a population of moving objects, e.g., individuals, mobile devices, and vehicles. In such application scenarios, an item passes between two objects when the objects are sufficiently close (i.e., when they are, so-called, in contact), and hence once an item is initiated, it can penetrate the object population through the evolving network of contacts among objects, termed contact network. In this paper, for the first time we define and study reachability queries in large (i.e., disk-resident) contact datasets which record the movement of a (potentially large) set of objects moving in a spatial environment over an extended time period. A reachability query verifies whether two objects are "reachable" through the evolving contact network represented by such contact datasets. We propose two contact-dataset indexes that enable efficient evaluation of such queries despite the potentially humongous size of the contact datasets. With the first index, termed ReachGrid, at the query time only a small necessary portion of the contact network which is required for reachability evaluation is constructed and traversed. With the second approach, termed ReachGraph, we precompute reachability at different scales and leverage these precalculations at the query time for efficient query processing. We optimize the placement of both indexes on disk to enable efficient index traversal during query processing. We study the pros and cons of our proposed approaches by performing extensive experiments with both real and synthetic data. Based on our experimental results, our proposed approaches outperform existing reachability query processing techniques in contact networks by 76% on average.

#index 1848113
#* Boosting moving object indexing through velocity partitioning
#@ Thi Nguyen;Zhen He;Rui Zhang;Phillip Ward
#t 2012
#c 4
#% 86950
#% 273706
#% 299979
#% 300174
#% 427199
#% 480307
#% 745466
#% 765452
#% 765454
#% 794880
#% 803125
#% 870306
#% 878301
#% 1015320
#% 1016193
#% 1046510
#% 1063471
#% 1127438
#% 1127612
#% 1230827
#% 1264027
#% 1328208
#% 1456650
#% 1878271
#! There have been intense research interests in moving object indexing in the past decade. However, existing work did not exploit the important property of skewed velocity distributions. In many real world scenarios, objects travel predominantly along only a few directions. Examples include vehicles on road networks, flights, people walking on the streets, etc. The search space for a query is heavily dependent on the velocity distribution of the objects grouped in the nodes of an index tree. Motivated by this observation, we propose the velocity partitioning (VP) technique, which exploits the skew in velocity distribution to speed up query processing using moving object indexes. The VP technique first identifies the "dominant velocity axes (DVAs)" using a combination of principal components analysis (PCA) and k-means clustering. Then, a moving object index (e.g., a TPR-tree) is created based on each DVA, using the DVA as an axis of the underlying coordinate system. An object is maintained in the index whose DVA is closest to the object's current moving direction. Thus, all the objects in an index are moving in a near 1-dimensional space instead of a 2-dimensional space. As a result, the expansion of the search space with time is greatly reduced, from a quadratic function of the maximum speed (of the objects in the search range) to a near linear function of the maximum speed. The VP technique can be applied to a wide range of moving object index structures. We have implemented the VP technique on two representative ones, the TPR*-tree and the Bx-tree. Extensive experiments validate that the VP technique consistently improves the performance of those index structures.

#index 1848114
#* Type-based detection of XML query-update independence
#@ Nicole Bidoit-Tollu;Dario Colazzo;Federico Ulliana
#t 2012
#c 4
#% 338753
#% 378411
#% 893111
#% 894404
#% 994015
#% 1015272
#% 1091021
#% 1092015
#% 1266686
#% 1328114
#% 1370257
#% 1523876
#% 1549862
#% 1661444
#% 1688277
#% 1721253
#! This paper presents a novel static analysis technique to detect XML query-update independence, in the presence of a schema. Rather than types, our system infers chains of types. Each chain represents a path that can be traversed on a valid document during query/update evaluation. The resulting independence analysis is precise, although it raises a challenging issue: recursive schemas may lead to inference of infinitely many chains. A sound and complete approximation technique ensuring a finite analysis in any case is presented, together with an efficient implementation performing the chain-based analysis in polynomial space and time.

#index 1848115
#* Minuet: a scalable distributed multiversion B-tree
#@ Benjamin Sowell;Wojciech Golab;Mehul A. Shah
#t 2012
#c 4
#% 748
#% 56081
#% 58371
#% 102809
#% 208047
#% 286929
#% 289207
#% 435156
#% 470844
#% 480475
#% 570884
#% 571296
#% 824706
#% 830695
#% 960252
#% 963667
#% 978174
#% 1016185
#% 1022298
#% 1022727
#% 1054227
#% 1127398
#% 1127560
#% 1278059
#% 1426413
#% 1426489
#% 1523840
#% 1523902
#% 1594617
#% 1594632
#! Data management systems have traditionally been designed to support either long-running analytics queries or short-lived transactions, but an increasing number of applications need both. For example, online games, socio-mobile apps, and e-commerce sites need to not only maintain operational state, but also analyze that data quickly to make predictions and recommendations that improve user experience. In this paper, we present Minuet, a distributed, main-memory B-tree that supports both transactions and copy-on-write snapshots for in-situ analytics. Minuet uses main-memory storage to enable low-latency transactional operations as well as analytics queries without compromising transaction performance. In addition to supporting read-only analytics queries on snapshots, Minuet supports writable clones, so that users can create branching versions of the data. This feature can be quite useful, e.g. to support complex "what-if" analysis or to facilitate wide-area replication. Our experiments show that Minuet outperforms a commercial main-memory database in many ways. It scales to hundreds of cores and TBs of memory, and can process hundreds of thousands of B-tree operations per second while executing long-running scans.

#index 1848116
#* Challenging the long tail recommendation
#@ Hongzhi Yin;Bin Cui;Jing Li;Junjie Yao;Chen Chen
#t 2012
#c 4
#% 348173
#% 420539
#% 766448
#% 832334
#% 860672
#% 915310
#% 918842
#% 963350
#% 1083671
#% 1190123
#% 1214623
#% 1214687
#% 1287227
#% 1355025
#% 1426509
#% 1476448
#% 1500471
#% 1541728
#% 1581924
#! The success of "infinite-inventory" retailers such as Amazon.com and Netflix has been largely attributed to a "long tail" phenomenon. Although the majority of their inventory is not in high demand, these niche products, unavailable at limited-inventory competitors, generate a significant fraction of total revenue in aggregate. In addition, tail product availability can boost head sales by offering consumers the convenience of "one-stop shopping" for both their mainstream and niche tastes. However, most of existing recommender systems, especially collaborative filter based methods, can not recommend tail products due to the data sparsity issue. It has been widely acknowledged that to recommend popular products is easier yet more trivial while to recommend long tail products adds more novelty yet it is also a more challenging task. In this paper, we propose a novel suite of graph-based algorithms for the long tail recommendation. We first represent user-item information with undirected edge-weighted graph and investigate the theoretical foundation of applying Hitting Time algorithm for long tail item recommendation. To improve recommendation diversity and accuracy, we extend Hitting Time and propose efficient Absorbing Time algorithm to help users find their favorite long tail items. Finally, we refine the Absorbing Time algorithm and propose two entropy-biased Absorbing Cost algorithms to distinguish the variation on different user-item rating pairs, which further enhances the effectiveness of long tail recommendation. Empirical experiments on two real life datasets show that our proposed algorithms are effective to recommend long tail items and outperform state-of-the-art recommendation techniques.

#index 1869827
#* Answering table queries on the web using column keywords
#@ Rakesh Pimplikar;Sunita Sarawagi
#t 2012
#c 4
#% 325295
#% 344568
#% 572314
#% 643004
#% 830529
#% 889176
#% 1127393
#% 1328133
#% 1328200
#% 1338594
#% 1338626
#% 1417383
#% 1426537
#% 1426566
#% 1536559
#% 1730598
#! We present the design of a structured search engine which returns a multi-column table in response to a query consisting of keywords describing each of its columns. We answer such queries by exploiting the millions of tables on the Web because these are much richer sources of structured knowledge than free-format text. However, a corpus of tables harvested from arbitrary HTML web pages presents huge challenges of diversity and redundancy not seen in centrally edited knowledge bases. We concentrate on one concrete task in this paper. Given a set of Web tables T1,..., Tn, and a query Q with q sets of keywords Q1,..., Qq, decide for each Ti if it is relevant to Q and if so, identify the mapping between the columns of Ti and query columns. We represent this task as a graphical model that jointly maps all tables by incorporating diverse sources of clues spanning matches in different parts of the table, corpus-wide co-occurrence statistics, and content overlap across table columns. We define a novel query segmentation model for matching keywords to table columns, and a robust mechanism of exploiting content overlap across table columns. We design efficient inference algorithms based on bipartite matching and constrained graph cuts to solve the joint labeling task. Experiments on a workload of 59 queries over a 25 million web table corpus shows significant boost in accuracy over baseline IR methods.

#index 1869828
#* Efficient verification of web-content searching through authenticated web crawlers
#@ Michael T. Goodrich;Charalampos Papamanthou;Duy Nguyen;Roberto Tamassia;Cristina Videira Lopes;Olga Ohrimenko;Nikos Triandopoulos
#t 2012
#c 4
#% 314746
#% 342345
#% 512615
#% 513367
#% 552444
#% 657774
#% 761411
#% 772846
#% 810042
#% 867054
#% 874980
#% 978647
#% 1127362
#% 1128860
#% 1206573
#% 1217147
#% 1414468
#% 1492037
#% 1563740
#% 1616405
#% 1669498
#% 1675939
#% 1706190
#% 1707082
#! We consider the problem of verifying the correctness and completeness of the result of a keyword search. We introduce the concept of an authenticated web crawler and present its design and prototype implementation. An authenticated web crawler is a trusted program that computes a specially-crafted signature over the web contents it visits. This signature enables (i) the verification of common Internet queries on web pages, such as conjunctive keyword searches---this guarantees that the output of a conjunctive keyword search is correct and complete; (ii) the verification of the content returned by such Internet queries---this guarantees that web data is authentic and has not been maliciously altered since the computation of the signature by the crawler. In our solution, the search engine returns a cryptographic proof of the query result. Both the proof size and the verification time are proportional only to the sizes of the query description and the query result, but do not depend on the number or sizes of the web pages over which the search is performed. As we experimentally demonstrate, the prototype implementation of our system provides a low communication overhead between the search engine and the user, and fast verification of the returned results by the user.

#index 1869829
#* SODA: generating SQL for business users
#@ Lukas Blunschi;Claudio Jossen;Donald Kossmann;Magdalini Mori;Kurt Stockinger
#t 2012
#c 4
#% 208037
#% 287268
#% 397398
#% 458860
#% 659990
#% 660011
#% 875017
#% 960259
#% 993987
#% 997497
#% 1021948
#% 1063536
#% 1215248
#% 1217198
#% 1328162
#% 1488676
#% 1494946
#% 1581893
#% 1619378
#% 1642295
#% 1846786
#! The purpose of data warehouses is to enable business analysts to make better decisions. Over the years the technology has matured and data warehouses have become extremely successful. As a consequence, more and more data has been added to the data warehouses and their schemas have become increasingly complex. These systems still work great in order to generate pre-canned reports. However, with their current complexity, they tend to be a poor match for non tech-savvy business analysts who need answers to ad-hoc queries that were not anticipated. This paper describes the design, implementation, and experience of the SODA system (Search over DAta Warehouse). SODA bridges the gap between the business needs of analysts and the technical complexity of current data warehouses. SODA enables a Google-like search experience for data warehouses by taking keyword queries of business users and automatically generating executable SQL. The key idea is to use a graph pattern matching algorithm that uses the metadata model of the data warehouse. Our results with real data from a global player in the financial services industry show that SODA produces queries with high precision and recall, and makes it much easier for business users to interactively explore highly-complex data warehouses.

#index 1869830
#* Privacy preservation by disassociation
#@ Manolis Terrovitis;Nikos Mamoulis;John Liagouris;Spiros Skiadopoulos
#t 2012
#c 4
#% 287285
#% 342643
#% 443463
#% 481588
#% 576761
#% 810011
#% 824726
#% 864406
#% 864412
#% 893100
#% 996348
#% 1016201
#% 1066737
#% 1083709
#% 1117733
#% 1127361
#% 1127417
#% 1181219
#% 1190072
#% 1206581
#% 1328187
#% 1425698
#% 1523848
#% 1523887
#% 1538422
#% 1604970
#% 1692267
#% 1740518
#! In this work, we focus on protection against identity disclosure in the publication of sparse multidimensional data. Existing multidimensional anonymization techniques (a) protect the privacy of users either by altering the set of quasi-identifiers of the original data (e.g., by generalization or suppression) or by adding noise (e.g., using differential privacy) and/or (b) assume a clear distinction between sensitive and non-sensitive information and sever the possible linkage. In many real world applications the above techniques are not applicable. For instance, consider web search query logs. Suppressing or generalizing anonymization methods would remove the most valuable information in the dataset: the original query terms. Additionally, web search query logs contain millions of query terms which cannot be categorized as sensitive or non-sensitive since a term may be sensitive for a user and non-sensitive for another. Motivated by this observation, we propose an anonymization technique termed disassociation that preserves the original terms but hides the fact that two or more different terms appear in the same record. We protect the users' privacy by disassociating record terms that participate in identifying combinations. This way the adversary cannot associate with high probability a record with a rare combination of terms. To the best of our knowledge, our proposal is the first to employ such a technique to provide protection against identity disclosure. We propose an anonymization algorithm based on our approach and evaluate its performance on real and synthetic datasets, comparing it against other state-of-the-art methods based on generalization and differential privacy.

#index 1869831
#* Supercharging recommender systems using taxonomies for learning user purchase behavior
#@ Bhargav Kanagal;Amr Ahmed;Sandeep Pandey;Vanja Josifovski;Jeff Yuan;Lluis Garcia-Pueyo
#t 2012
#c 4
#% 44876
#% 277396
#% 463903
#% 481290
#% 481758
#% 783531
#% 985041
#% 1083671
#% 1145214
#% 1189164
#% 1214623
#% 1214666
#% 1260273
#% 1355024
#% 1400014
#% 1417104
#% 1605925
#% 1605928
#% 1625364
#! Recommender systems based on latent factor models have been effectively used for understanding user interests and predicting future actions. Such models work by projecting the users and items into a smaller dimensional space, thereby clustering similar users and items together and subsequently compute similarity between unknown user-item pairs. When user-item interactions are sparse (sparsity problem) or when new items continuously appear (cold start problem), these models perform poorly. In this paper, we exploit the combination of taxonomies and latent factor models to mitigate these issues and improve recommendation accuracy. We observe that taxonomies provide structure similar to that of a latent factor model: namely, it imposes human-labeled categories (clusters) over items. This leads to our proposed taxonomy-aware latent factor model (TF) which combines taxonomies and latent factors using additive models. We develop efficient algorithms to train the TF models, which scales to large number of users/items and develop scalable inference/recommendation algorithms by exploiting the structure of the taxonomy. In addition, we extend the TF model to account for the temporal dynamics of user interests using high-order Markov chains. To deal with large-scale data, we develop a parallel multi-core implementation of our TF model. We empirically evaluate the TF model for the task of predicting user purchases using a real-world shopping dataset spanning more than a million users and products. Our experiments demonstrate the benefits of using our TF models over existing approaches, in terms of both prediction accuracy and running time.

#index 1869832
#* DBToaster: higher-order delta processing for dynamic, frequently fresh views
#@ Yanif Ahmad;Oliver Kennedy;Christoph Koch;Milos Nikolic
#t 2012
#c 4
#% 13016
#% 98469
#% 152928
#% 182421
#% 201929
#% 210208
#% 210210
#% 227945
#% 262860
#% 287324
#% 300141
#% 342955
#% 397352
#% 411736
#% 464056
#% 480158
#% 562293
#% 726629
#% 788999
#% 806217
#% 820356
#% 875022
#% 912495
#% 960278
#% 993998
#% 1015280
#% 1022221
#% 1312534
#% 1328080
#% 1426451
#% 1809993
#! Applications ranging from algorithmic trading to scientific data analysis require realtime analytics based on views over databases that change at very high rates. Such views have to be kept fresh at low maintenance cost and latencies. At the same time, these views have to support classical SQL, rather than window semantics, to enable applications that combine current with aged or historical data. In this paper, we present viewlet transforms, a recursive finite differencing technique applied to queries. The viewlet transform materializes a query and a set of its higher-order deltas as views. These views support each other's incremental maintenance, leading to a reduced overall view maintenance cost. The viewlet transform of a query admits efficient evaluation, the elimination of certain expensive query operations, and aggressive parallelization. We develop viewlet transforms into a workable query execution technique, present a heuristic and cost-based optimization framework, and report on experiments with a prototype dynamic data management system that combines viewlet transforms with an optimizing compilation technique. The system supports tens of thousands of complete view refreshes a second for a wide range of queries.

#index 1869833
#* Real time discovery of dense clusters in highly dynamic graphs: identifying real world events in highly dynamic environments
#@ Manoj K. Agarwal;Krithi Ramamritham;Manish Bhide
#t 2012
#c 4
#% 190611
#% 243166
#% 492905
#% 577360
#% 729968
#% 823347
#% 881460
#% 1022269
#% 1399992
#% 1400018
#% 1426611
#% 1432574
#% 1581969
#! Due to their real time nature, microblog streams are a rich source of dynamic information, for example, about emerging events. Existing techniques for discovering such events from a microblog stream in real time (such as Twitter trending topics), have several lacunae when used for discovering emerging events; extant graph based event detection techniques are not practical in microblog settings due to their complexity; and conventional techniques, which have been developed for blogs, web-pages, etc., involving the use of keyword search, are only useful for finding information about known events. Hence, in this paper, we present techniques to discover events that are unraveling in microblog message streams in real time so that such events can be reported as soon as they occur. We model the problem as discovering dense clusters in highly dynamic graphs. Despite many recent advances in graph analysis, ours is the first technique to identify dense clusters in massive and highly dynamic graphs in real time. Given the characteristics of microblog streams, in order to find clusters without missing any events, we propose and exploit a novel graph property which we call short-cycle property. Our algorithms find these clusters efficiently in spite of rapid changes to the microblog streams. Further we present a novel ranking function to identify the important events. Besides proving the correctness of our algorithms we show their practical utility by evaluating them using real world microblog data. These demonstrate our technique's ability to discover, with high precision and recall, emerging events in high intensity data streams in real time. Many recent web applications create data which can be represented as massive dynamic graphs. Our technique can be easily extended to discover, in real time, interesting patterns in such graphs.

#index 1869834
#* Sketch-based querying of distributed sliding-window data streams
#@ Odysseas Papapetrou;Minos Garofalakis;Antonios Deligiannakis
#t 2012
#c 4
#% 278835
#% 333931
#% 397443
#% 414993
#% 464786
#% 480805
#% 492912
#% 576119
#% 654488
#% 810009
#% 816392
#% 853012
#% 874995
#% 878246
#% 981652
#% 1054483
#% 1128091
#% 1349610
#% 1392293
#% 1415461
#% 1504028
#% 1711934
#% 1863150
#! While traditional data-management systems focus on evaluating single, ad-hoc queries over static data sets in a centralized setting, several emerging applications require (possibly, continuous) answers to queries on dynamic data that is widely distributed and constantly updated. Furthermore, such query answers often need to discount data that is "stale", and operate solely on a sliding window of recent data arrivals (e.g., data updates occurring over the last 24 hours). Such distributed data streaming applications mandate novel algorithmic solutions that are both time- and space-efficient (to manage high-speed data streams), and also communication-efficient (to deal with physical data distribution). In this paper, we consider the problem of complex query answering over distributed, high-dimensional data streams in the sliding-window model. We introduce a novel sketching technique (termed ECM-sketch) that allows effective summarization of streaming data over both time-based and count-based sliding windows with probabilistic accuracy guarantees. Our sketch structure enables point as well as inner-product queries, and can be employed to address a broad range of problems, such as maintaining frequency statistics, finding heavy hitters, and computing quantiles in the sliding-window model. Focusing on distributed environments, we demonstrate how ECM-sketches of individual, local streams can be composed to generate a (low-error) ECM-sketch summary of the order-preserving aggregation of all streams; furthermore, we show how ECM-sketches can be exploited for continuous monitoring of sliding-window queries over distributed streams. Our extensive experimental study with two real-life data sets validates our theoretical claims and verifies the effectiveness of our techniques. To the best of our knowledge, ours is the first work to address efficient, guaranteed-error complex query answering over distributed data streams in the sliding-window model.

#index 1869835
#* LogBase: a scalable log-structured database system in the cloud
#@ Hoang Tam Vo;Sheng Wang;Divyakant Agrawal;Gang Chen;Beng Chin Ooi
#t 2012
#c 4
#% 13043
#% 43172
#% 58371
#% 114582
#% 131555
#% 201869
#% 208047
#% 286929
#% 317988
#% 443192
#% 570884
#% 814649
#% 1002142
#% 1015289
#% 1063524
#% 1127560
#% 1400975
#% 1426489
#% 1426492
#% 1523799
#% 1523840
#% 1573340
#% 1625033
#% 1765839
#% 1770337
#! Numerous applications such as financial transactions (e.g., stock trading) are write-heavy in nature. The shift from reads to writes in web applications has also been accelerating in recent years. Write-ahead-logging is a common approach for providing recovery capability while improving performance in most storage systems. However, the separation of log and application data incurs write overheads observed in write-heavy environments and hence adversely affects the write throughput and recovery time in the system. In this paper, we introduce LogBase -- a scalable log-structured database system that adopts log-only storage for removing the write bottleneck and supporting fast system recovery. It is designed to be dynamically deployed on commodity clusters to take advantage of elastic scaling property of cloud environments. LogBase provides in-memory multiversion indexes for supporting efficient access to data maintained in the log. LogBase also supports transactions that bundle read and write operations spanning across multiple records. We implemented the proposed system and compared it with HBase and a disk-based log-structured record-oriented system modeled after RAMCloud. The experimental results show that LogBase is able to provide sustained write throughput, efficient data access out of the cache, and effective system recovery.

#index 1869836
#* Efficient processing of k nearest neighbor joins using MapReduce
#@ Wei Lu;Yanyan Shen;Su Chen;Beng Chin Ooi
#t 2012
#c 4
#% 300136
#% 465000
#% 479791
#% 479973
#% 480632
#% 731409
#% 783643
#% 814646
#% 943214
#% 963669
#% 1016192
#% 1426543
#% 1523837
#% 1532900
#% 1581925
#% 1769264
#% 1798378
#% 1846758
#! k nearest neighbor join (kNN join), designed to find k nearest neighbors from a dataset S for every object in another dataset R, is a primitive operation widely adopted by many data mining applications. As a combination of the k nearest neighbor query and the join operation, kNN join is an expensive operation. Given the increasing volume of data, it is difficult to perform a kNN join on a centralized machine efficiently. In this paper, we investigate how to perform kNN join using MapReduce which is a well-accepted framework for data-intensive applications over clusters of computers. In brief, the mappers cluster objects into groups; the reducers perform the kNN join on each group of objects separately. We design an effective mapping mechanism that exploits pruning rules for distance filtering, and hence reduces both the shuffling and computational costs. To reduce the shuffling cost, we propose two approximate algorithms to minimize the number of replicas. Extensive experiments on our in-house cluster demonstrate that our proposed methods are efficient, robust and scalable.

#index 1869837
#* Early accurate results for advanced analytics on MapReduce
#@ Nikolay Laptev;Kai Zeng;Carlo Zaniolo
#t 2012
#c 4
#% 227883
#% 503535
#% 577261
#% 723279
#% 765425
#% 810057
#% 954300
#% 978976
#% 1063553
#% 1328095
#% 1335135
#% 1426601
#% 1523820
#% 1581928
#% 1594630
#% 1846687
#! Approximate results based on samples often provide the only way in which advanced analytical applications on very massive data sets can satisfy their time and resource constraints. Unfortunately, methods and tools for the computation of accurate early results are currently not supported in MapReduce-oriented systems although these are intended for 'big data'. Therefore, we proposed and implemented a non-parametric extension of Hadoop which allows the incremental computation of early results for arbitrary work-flows, along with reliable on-line estimates of the degree of accuracy achieved so far in the computation. These estimates are based on a technique called bootstrapping that has been widely employed in statistics and can be applied to arbitrary functions and data distributions. In this paper, we describe our Early Accurate Result Library (EARL) for Hadoop that was designed to minimize the changes required to the MapReduce framework. Various tests of EARL of Hadoop are presented to characterize the frequent situations where EARL can provide major speed-ups over the current version of Hadoop.

#index 1869838
#* CDAS: a crowdsourcing data analytics system
#@ Xuan Liu;Meiyu Lu;Beng Chin Ooi;Yanyan Shen;Sai Wu;Meihui Zhang
#t 2012
#c 4
#% 1047347
#% 1081621
#% 1150163
#% 1328155
#% 1350334
#% 1375847
#% 1432722
#% 1452857
#% 1478106
#% 1478124
#% 1478126
#% 1478132
#% 1550748
#% 1581851
#% 1581980
#% 1584789
#% 1598354
#! Some complex problems, such as image tagging and natural language processing, are very challenging for computers, where even state-of-the-art technology is yet able to provide satisfactory accuracy. Therefore, rather than relying solely on developing new and better algorithms to handle such tasks, we look to the crowdsourcing solution -- employing human participation -- to make good the shortfall in current technology. Crowdsourcing is a good supplement to many computer tasks. A complex job may be divided into computer-oriented tasks and human-oriented tasks, which are then assigned to machines and humans respectively. To leverage the power of crowdsourcing, we design and implement a Crowdsourcing Data Analytics System, CDAS. CDAS is a framework designed to support the deployment of various crowdsourcing applications. The core part of CDAS is a quality-sensitive answering model, which guides the crowdsourcing engine to process and monitor the human tasks. In this paper, we introduce the principles of our quality-sensitive model. To satisfy user required accuracy, the model guides the crowdsourcing query engine for the design and processing of the corresponding crowdsourcing jobs. It provides an estimated accuracy for each generated result based on the human workers' historical performances. When verifying the quality of the result, the model employs an online strategy to reduce waiting time. To show the effectiveness of the model, we implement and deploy two analytics jobs on CDAS, a twitter sentiment analytics job and an image tagging job. We use real Twitter and Flickr data as our queries respectively. We compare our approaches with state-of-the-art classification and image annotation techniques. The results show that the human-assisted methods can indeed achieve a much higher accuracy. By embedding the quality-sensitive model into crowdsourcing query engine, we effectively reduce the processing cost while maintaining the required query answer quality.

#index 1869839
#* Mining statistically significant substrings using the chi-square statistic
#@ Mayank Sachan;Arnab Bhattacharya
#t 2012
#c 4
#% 235941
#% 331608
#% 386161
#% 452860
#% 481562
#% 717353
#% 785333
#% 805093
#% 837639
#% 995576
#% 1480884
#% 1737790
#% 1784497
#! The problem of identification of statistically significant patterns in a sequence of data has been applied to many domains such as intrusion detection systems, financial models, web-click records, automated monitoring systems, computational biology, cryptology, and text analysis. An observed pattern of events is deemed to be statistically significant if it is unlikely to have occurred due to randomness or chance alone. We use the chi-square statistic as a quantitative measure of statistical significance. Given a string of characters generated from a memoryless Bernoulli model, the problem is to identify the substring for which the empirical distribution of single letters deviates the most from the distribution expected from the generative Bernoulli model. This deviation is captured using the chi-square measure. The most significant substring (MSS) of a string is thus defined as the substring having the highest chi-square value. Till date, to the best of our knowledge, there does not exist any algorithm to find the MSS in better than O(n2) time, where n denotes the length of the string. In this paper, we propose an algorithm to find the most significant substring, whose running time is O(n3/2) with high probability. We also study some variants of this problem such as finding the top-t set, finding all substrings having chi-square greater than a fixed threshold and finding the MSS among substrings greater than a given length. We experimentally demonstrate the asymptotic behavior of the MSS on varying the string size and alphabet size. We also describe some applications of our algorithm on cryptology and real world data from finance and sports. Finally, we compare our technique with the existing heuristics for finding the MSS.

#index 1869840
#* Massively parallel sort-merge joins in main memory multi-core database systems
#@ Martina-Cezara Albutiu;Alfons Kemper;Thomas Neumann
#t 2012
#c 4
#% 235242
#% 442700
#% 443513
#% 463759
#% 479821
#% 480966
#% 571045
#% 983261
#% 1063508
#% 1127563
#% 1180004
#% 1222049
#% 1328057
#% 1328102
#% 1581849
#% 1581898
#% 1592312
#% 1594617
#% 1647981
#% 1667313
#% 1694381
#! Two emerging hardware trends will dominate the database system technology in the near future: increasing main memory capacities of several TB per server and massively parallel multi-core processing. Many algorithmic and control techniques in current database technology were devised for disk-based systems where I/O dominated the performance. In this work we take a new look at the well-known sort-merge join which, so far, has not been in the focus of research in scalable massively parallel multi-core data processing as it was deemed inferior to hash joins. We devise a suite of new massively parallel sort-merge (MPSM) join algorithms that are based on partial partition-based sorting. Contrary to classical sort-merge joins, our MPSM algorithms do not rely on a hard to parallelize final merge step to create one complete sort order. Rather they work on the independently created runs in parallel. This way our MPSM algorithms are NUMA-affine as all the sorting is carried out on local memory partitions. An extensive experimental evaluation on a modern 32-core machine with one TB of main memory proves the competitive performance of MPSM on large main memory databases with billions of objects. It scales (almost) linearly in the number of employed cores and clearly outperforms competing hash join proposals -- in particular it outperforms the "cutting-edge" Vectorwise parallel query engine by a factor of four.

#index 1869841
#* hStorage-DB: heterogeneity-aware data management to exploit the full capability of hybrid storage systems
#@ Tian Luo;Rubao Lee;Michael Mesnier;Feng Chen;Xiaodong Zhang
#t 2012
#c 4
#% 348037
#% 393844
#% 442918
#% 963685
#% 1063551
#% 1189358
#% 1328052
#% 1434005
#% 1523922
#% 1568645
#% 1581941
#% 1586520
#% 1601162
#% 1601163
#% 1625035
#! As storage systems become increasingly heterogeneous and complex, it adds burdens on DBAs, causing suboptimal performance even after a lot of human efforts have been made. In addition, existing monitoring-based storage management by access pattern detections has difficulties to handle workloads that are highly dynamic and concurrent. To achieve high performance by best utilizing heterogeneous storage devices, we have designed and implemented a heterogeneity-aware software framework for DBMS storage management called hStorage-DB, where semantic information that is critical for storage I/O is identified and passed to the storage manager. According to the collected semantic information, requests are classified into different types. Each type is assigned a proper QoS policy supported by the underlying storage system, so that every request will be served with a suitable storage device. With hStorage-DB, we can well utilize semantic information that cannot be detected through data access monitoring but is particularly important for a hybrid storage system. To show the effectiveness of hStorage-DB, we have implemented a system prototype that consists of an I/O request classification enabled DBMS, and a hybrid storage system that is organized into a two-level caching hierarchy. Our performance evaluation shows that hStorage-DB can automatically make proper decisions for data allocation in different storage devices and make substantial performance improvements in a cost-efficient way.

#index 1880430
#* A scalable algorithm for maximizing range sum in spatial databases
#@ Dong-Wan Choi;Chin-Wan Chung;Yufei Tao
#t 2012
#c 4
#% 3147
#% 333977
#% 527189
#% 566144
#% 589264
#% 813802
#% 824730
#% 847100
#% 893141
#% 943213
#% 1081215
#% 1228145
#% 1328203
#% 1523973
#% 1581825
#% 1594573
#% 1594580
#% 1720751
#! This paper investigates the MaxRS problem in spatial databases. Given a set O of weighted points and a rectangular region r of a given size, the goal of the MaxRS problem is to find a location of r such that the sum of the weights of all the points covered by r is maximized. This problem is useful in many location-based applications such as finding the best place for a new franchise store with a limited delivery range and finding the most attractive place for a tourist with a limited reachable range. However, the problem has been studied mainly in theory, particularly, in computational geometry. The existing algorithms from the computational geometry community are in-memory algorithms which do not guarantee the scalability. In this paper, we propose a scalable external-memory algorithm (ExactMaxRS) for the MaxRS problem, which is optimal in terms of the I/O complexity. Furthermore, we propose an approximation algorithm (ApproxMaxCRS) for the MaxCRS problem that is a circle version of the MaxRS problem. We prove the correctness and optimality of the ExactMaxRS algorithm along with the approximation bound of the ApproxMaxCRS algorithm. From extensive experimental results, we show that the ExactMaxRS algorithm is two orders of magnitude faster than methods adapted from existing algorithms, and the approximation bound in practice is much better than the theoretical bound of the ApproxMaxCRS algorithm.

#index 1880431
#* Spatial queries with two kNN predicates
#@ Ahmed M. Aly;Walid G. Aref;Mourad Ouzzani
#t 2012
#c 4
#% 86950
#% 201876
#% 387508
#% 427199
#% 765453
#% 800572
#% 818938
#% 893092
#% 951485
#% 975051
#% 1016192
#% 1206820
#% 1309309
#% 1426633
#% 1442466
#% 1483643
#! The widespread use of location-aware devices has led to countless location-based services in which a user query can be arbitrarily complex, i.e., one that embeds multiple spatial selection and join predicates. Amongst these predicates, the k-Nearest-Neighbor (kNN) predicate stands as one of the most important and widely used predicates. Unlike related research, this paper goes beyond the optimization of queries with single kNN predicates, and shows how queries with two kNN predicates can be optimized. In particular, the paper addresses the optimization of queries with: (i) two kNN-select predicates, (ii) two kNN-join predicates, and (iii) one kNN-join predicate and one kNN-select predicate. For each type of queries, conceptually correct query evaluation plans (QEPs) and new algorithms that optimize the query execution time are presented. Experimental results demonstrate that the proposed algorithms outperform the conceptually correct QEPs by orders of magnitude.

#index 1880432
#* Optimal algorithms for crawling a hidden database in the web
#@ Cheng Sheng;Nan Zhang;Yufei Tao;Xin Jin
#t 2012
#c 4
#% 340146
#% 480479
#% 654459
#% 659993
#% 765410
#% 769890
#% 809418
#% 893144
#% 956534
#% 993964
#% 1127356
#% 1127557
#% 1130961
#% 1328136
#% 1372752
#% 1423051
#% 1426573
#% 1581892
#! A hidden database refers to a dataset that an organization makes accessible on the web by allowing users to issue queries through a search interface. In other words, data acquisition from such a source is not by following static hyper-links. Instead, data are obtained by querying the interface, and reading the result page dynamically generated. This, with other facts such as the interface may answer a query only partially, has prevented hidden databases from being crawled effectively by existing search engines. This paper remedies the problem by giving algorithms to extract all the tuples from a hidden database. Our algorithms are provably efficient, namely, they accomplish the task by performing only a small number of queries, even in the worst case. We also establish theoretical results indicating that these algorithms are asymptotically optimal -- i.e., it is impossible to improve their efficiency by more than a constant factor. The derivation of our upper and lower bound results reveals significant insight into the characteristics of the underlying problem. Extensive experiments confirm the proposed techniques work very well on all the real datasets examined.

#index 1880433
#* Diversifying top-k results
#@ Lu Qin;Jeffrey Xu Yu;Lijun Chang
#t 2012
#c 4
#% 262112
#% 278831
#% 333854
#% 397133
#% 408396
#% 571763
#% 643566
#% 659255
#% 874894
#% 983263
#% 1063539
#% 1075132
#% 1166473
#% 1207007
#% 1450870
#% 1451241
#% 1560358
#% 1581911
#% 1606349
#! Top-k query processing finds a list of k results that have largest scores w.r.t the user given query, with the assumption that all the k results are independent to each other. In practice, some of the top-k results returned can be very similar to each other. As a result some of the top-k results returned are redundant. In the literature, diversified top-k search has been studied to return k results that take both score and diversity into consideration. Most existing solutions on diversified top-k search assume that scores of all the search results are given, and some works solve the diversity problem on a specific problem and can hardly be extended to general cases. In this paper, we study the diversified top-k search problem. We define a general diversified top-k search problem that only considers the similarity of the search results themselves. We propose a framework, such that most existing solutions for top-k query processing can be extended easily to handle diversified top-k search, by simply applying three new functions, a sufficient stop condition sufficient(), a necessary stop condition necessary(), and an algorithm for diversified top-k search on the current set of generated results, div-search-current(). We propose three new algorithms, namely, div-astar, div-dp, and div-cut to solve the div-search-current() problem. div-astar is an A* based algorithm, div-dp is an algorithm that decomposes the results into components which are searched using div-astar independently and combined using dynamic programming. div-cut further decomposes the current set of generated results using cut points and combines the results using sophisticated operations. We conducted extensive performance studies using two real datasets, enwiki and reuters. Our div-cut algorithm finds the optimal solution for diversified top-k search problem in seconds even for k as large as 2, 000.

#index 1880434
#* Keyword-aware optimal route search
#@ Xin Cao;Lisi Chen;Gao Cong;Xiaokui Xiao
#t 2012
#c 4
#% 327432
#% 408396
#% 1066744
#% 1135142
#% 1135143
#% 1230825
#% 1265149
#% 1298905
#% 1328137
#% 1426420
#% 1482236
#% 1484412
#% 1523805
#% 1523828
#% 1531117
#% 1581877
#% 1594572
#% 1594578
#% 1594591
#% 1643294
#% 1667211
#% 1720757
#% 1818833
#! Identifying a preferable route is an important problem that finds applications in map services. When a user plans a trip within a city, the user may want to find "a most popular route such that it passes by shopping mall, restaurant, and pub, and the travel time to and from his hotel is within 4 hours." However, none of the algorithms in the existing work on route planning can be used to answer such queries. Motivated by this, we define the problem of keyword-aware optimal route query, denoted by KOR, which is to find an optimal route such that it covers a set of user-specified keywords, a specified budget constraint is satisfied, and an objective score of the route is optimal. The problem of answering KOR queries is NP-hard. We devise an approximation algorithm OSScaling with provable approximation bounds. Based on this algorithm, another more efficient approximation algorithm BucketBound is proposed. We also design a greedy approximation algorithm. Results of empirical studies show that all the proposed algorithms are capable of answering KOR queries efficiently, while the BucketBound and Greedy algorithms run faster. The empirical studies also offer insight into the accuracy of the proposed algorithms.

#index 1880435
#* Answering queries using views over probabilistic XML: complexity and tractability
#@ Bogdan Cautis;Evgeny Kharlamov
#t 2012
#c 4
#% 464434
#% 570877
#% 572314
#% 733593
#% 824661
#% 824690
#% 875007
#% 889107
#% 977012
#% 977013
#% 993985
#% 1015260
#% 1015271
#% 1016134
#% 1022209
#% 1129529
#% 1181227
#% 1190673
#% 1200291
#% 1206683
#% 1291113
#% 1291120
#% 1328134
#% 1424591
#% 1531323
#% 1552647
#% 1562967
#% 1581975
#% 1594601
#! We study the complexity of query answering using views in a probabilistic XML setting, identifying large classes of XPath queries -- with child and descendant navigation and predicates -- for which there are efficient (PTime) algorithms. We consider this problem under the two possible semantics for XML query results: with persistent node identifiers and in their absence. Accordingly, we consider rewritings that can exploit a single view, by means of compensation, and rewritings that can use multiple views, by means of intersection. Since in a probabilistic setting queries return answers with probabilities, the problem of rewriting goes beyond the classic one of retrieving XML answers from views. For both semantics of XML queries, we show that, even when XML answers can be retrieved from views, their probabilities may not be computable. For rewritings that use only compensation, we describe a PTime decision procedure, based on easily verifiable criteria that distinguish between the feasible cases -- when probabilistic XML results are computable -- and the unfeasible ones. For rewritings that can use multiple views, with compensation and intersection, we identify the most permissive conditions that make probabilistic rewriting feasible, and we describe an algorithm that is sound in general, and becomes complete under fairly permissive restrictions, running in PTime modulo worst-case exponential time equivalence tests. This is the best we can hope for since intersection makes query equivalence intractable already over deterministic data. Our algorithm runs in PTime whenever deterministic rewritings can be found in PTime.

#index 1880436
#* Probabilistic databases with MarkoViews
#@ Abhay Jha;Dan Suciu
#t 2012
#c 4
#% 92787
#% 285330
#% 810098
#% 840890
#% 850430
#% 915340
#% 992830
#% 1111133
#% 1127376
#% 1206717
#% 1206987
#% 1217176
#% 1250579
#% 1269496
#% 1269815
#% 1272404
#% 1343690
#% 1426461
#% 1426558
#% 1467732
#% 1471209
#% 1523866
#% 1538784
#% 1573237
#% 1581889
#% 1615075
#% 1675285
#! Most of the work on query evaluation in probabilistic databases has focused on the simple tuple-independent data model, where tuples are independent random events. Several efficient query evaluation techniques exists in this setting, such as safe plans, algorithms based on OBDDs, tree-decomposition and a variety of approximation algorithms. However, complex data analytics tasks often require complex correlations, and query evaluation then is significantly more expensive, or more restrictive. In this paper, we propose MVDB as a framework both for representing complex correlations and for efficient query evaluation. An MVDB specifies correlations by views, called MarkoViews, on the probabilistic relations and declaring the weights of the view's outputs. An MVDB is a (very large) Markov Logic Network. We make two sets of contributions. First, we show that query evaluation on an MVDB is equivalent to evaluating a Union of Conjunctive Query(UCQ) over a tuple-independent database. The translation is exact (thus allowing the techniques developed for tuple independent databases to be carried over to MVDB), yet it is novel and quite non-obvious (some resulting probabilities may be negative!). This translation in itself though may not lead to much gain since the translated query gets complicated as we try to capture more correlations. Our second contribution is to propose a new query evaluation strategy that exploits offline compilation to speed up online query evaluation. Here we utilize and extend our prior work on compilation of UCQ. We validate experimentally our techniques on a large probabilistic database with MarkoViews inferred from the DBLP data.

#index 1880437
#* The complexity of social coordination
#@ Konstantinos Mamouras;Sigal Oren;Lior Seeman;Lucja Kot;Johannes Gehrke
#t 2012
#c 4
#% 32897
#% 274912
#% 295410
#% 452556
#% 1472962
#% 1581902
#! Coordination is a challenging everyday task; just think of the last time you organized a party or a meeting involving several people. As a growing part of our social and professional life goes online, an opportunity for an improved coordination process arises. Recently, Gupta et al. proposed entangled queries as a declarative abstraction for data-driven coordination, where the difficulty of the coordination task is shifted from the user to the database. Unfortunately, evaluating entangled queries is very hard, and thus previous work considered only a restricted class of queries that satisfy safety (the coordination partners are fixed) and uniqueness (all queries need to be satisfied). In this paper we significantly extend the class of feasible entangled queries beyond uniqueness and safety. First, we show that we can simply drop uniqueness and still efficiently evaluate a set of safe entangled queries. Second, we show that as long as all users coordinate on the same set of attributes, we can give an efficient algorithm for coordination even if the set of queries does not satisfy safety. In an experimental evaluation we show that our algorithms are feasible for a wide spectrum of coordination scenarios.

#index 1880438
#* Efficient multi-way theta-join processing using MapReduce
#@ Xiaofei Zhang;Lei Chen;Min Wang
#t 2012
#c 4
#% 126338
#% 137867
#% 256685
#% 443409
#% 761413
#% 1023420
#% 1127427
#% 1265149
#% 1426486
#% 1426492
#% 1426543
#% 1468411
#% 1523837
#% 1523839
#% 1523841
#% 1581925
#% 1581937
#% 1583140
#% 1594632
#% 1594639
#% 1601116
#% 1602032
#% 1621136
#! Multi-way Theta-join queries are powerful in describing complex relations and therefore widely employed in real practices. However, existing solutions from traditional distributed and parallel databases for multi-way Theta-join queries cannot be easily extended to fit a shared-nothing distributed computing paradigm, which is proven to be able to support OLAP applications over immense data volumes. In this work, we study the problem of efficient processing of multi-way Theta-join queries using MapReduce from a cost-effective perspective. Although there have been some works using the (key, value) pair-based programming model to support join operations, efficient processing of multi-way Theta-join queries has never been fully explored. The substantial challenge lies in, given a number of processing units (that can run Map or Reduce tasks), mapping a multi-way Theta-join query to a number of MapReduce jobs and having them executed in a well scheduled sequence, such that the total processing time span is minimized. Our solution mainly includes two parts: 1) cost metrics for both single MapReduce job and a number of MapReduce jobs executed in a certain order; 2) the efficient execution of a chain-typed Theta-join with only one MapReduce job. Comparing with the query evaluation strategy proposed in [23] and the widely adopted Pig Latin and Hive SQL solutions, our method achieves significant improvement of the join processing efficiency.

#index 1880439
#* Stubby: a transformation-based optimizer for MapReduce workflows
#@ Harold Lim;Herodotos Herodotou;Shivnath Babu
#t 2012
#c 4
#% 32889
#% 482082
#% 580970
#% 800563
#% 963669
#% 1085307
#% 1217159
#% 1278390
#% 1328060
#% 1426210
#% 1468421
#% 1523839
#% 1573238
#% 1581866
#% 1601116
#% 1621135
#% 1621136
#! There is a growing trend of performing analysis on large datasets using workflows composed of MapReduce jobs connected through producer-consumer relationships based on data. This trend has spurred the development of a number of interfaces---ranging from program-based to query-based interfaces---for generating MapReduce workflows. Studies have shown that the gap in performance can be quite large between optimized and unoptimized workflows. However, automatic cost-based optimization of MapReduce workflows remains a challenge due to the multitude of interfaces, large size of the execution plan space, and the frequent unavailability of all types of information needed for optimization. We introduce a comprehensive plan space for MapReduce workflows generated by popular workflow generators. We then propose Stubby, a cost-based optimizer that searches selectively through the subspace of the full plan space that can be enumerated correctly and costed based on the information available in any given setting. Stubby enumerates the plan space based on plan-to-plan transformations and an efficient search algorithm. Stubby is designed to be extensible to new interfaces and new types of optimizations, which is a desirable feature given how rapidly MapReduce systems are evolving. Stubby's efficiency and effectiveness have been evaluated using representative workflows from many domains.

#index 1880440
#* Labeling workflow views with fine-grained dependencies
#@ Zhuowei Bao;Susan B. Davidson;Tova Milo
#t 2012
#c 4
#% 271250
#% 378412
#% 379483
#% 593971
#% 765129
#% 765450
#% 765488
#% 893117
#% 1063545
#% 1153476
#% 1174012
#% 1206750
#% 1217188
#% 1217201
#% 1217208
#% 1426561
#% 1523819
#% 1538770
#% 1581829
#% 1581887
#% 1581922
#! This paper considers the problem of efficiently answering reachability queries over views of provenance graphs, derived from executions of workflows that may include recursion. Such views include composite modules and model fine-grained dependencies between module inputs and outputs. A novel view-adaptive dynamic labeling scheme is developed for efficient query evaluation, in which view specifications are labeled statically (i.e. as they are created) and data items are labeled dynamically as they are produced during a workflow execution. Although the combination of fine-grained dependencies and recursive workflows entail, in general, long (linear-size) data labels, we show that for a large natural class of workflows and views, labels are compact (logarithmic-size) and reachability queries can be evaluated in constant time. Experimental results demonstrate the benefit of this approach over the state-of-the-art technique when applied for labeling multiple views.

#index 1880441
#* Fundamentals of order dependencies
#@ Jaroslaw Szlichta;Parke Godfrey;Jarek Gryz
#t 2012
#c 4
#% 36683
#% 210169
#% 287295
#% 287677
#% 300382
#% 342360
#% 393641
#% 411554
#% 411733
#% 479814
#% 637806
#% 800596
#% 824691
#% 1549878
#! Dependencies have played a significant role in database design for many years. They have also been shown to be useful in query optimization. In this paper, we discuss dependencies between lexicographically ordered sets of tuples. We introduce formally the concept of order dependency and present a set of axioms (inference rules) for them. We show how query rewrites based on these axioms can be used for query optimization. We present several interesting theorems that can be derived using the inference rules. We prove that functional dependencies are subsumed by order dependencies and that our set of axioms for order dependencies is sound and complete.

#index 1880442
#* FDB: a query engine for factorised relational databases
#@ Nurzhan Bakibayev;Dan Olteanu;Jakub Závodný
#t 2012
#c 4
#% 6259
#% 102787
#% 384978
#% 479821
#% 481904
#% 765431
#% 768808
#% 847068
#% 875026
#% 976987
#% 1015332
#% 1083337
#% 1111133
#% 1523974
#% 1615075
#% 1623070
#% 1818427
#% 1826142
#% 1914832
#! Factorised databases are relational databases that use compact factorised representations at the physical layer to reduce data redundancy and boost query performance. This paper introduces FDB, an in-memory query engine for select-project-join queries on factorised databases. Key components of FDB are novel algorithms for query optimisation and evaluation that exploit the succinctness brought by data factorisation. Experiments show that for data sets with many-to-many relationships FDB can outperform relational engines by orders of magnitude.

#index 1880443
#* Optimization of analytic window functions
#@ Yu Cao;Chee-Yong Chan;Jie Li;Kian-Lee Tan
#t 2012
#c 4
#% 136740
#% 210169
#% 319789
#% 411554
#% 481951
#% 482082
#% 654498
#% 810029
#% 1015323
#% 1016209
#% 1328056
#% 1853526
#! Analytic functions represent the state-of-the-art way of performing complex data analysis within a single SQL statement. In particular, an important class of analytic functions that has been frequently used in commercial systems to support OLAP and decision support applications is the class of window functions. A window function returns for each input tuple a value derived from applying a function over a window of neighboring tuples. However, existing window function evaluation approaches are based on a naive sorting scheme. In this paper, we study the problem of optimizing the evaluation of window functions. We propose several efficient techniques, and identify optimization opportunities that allow us to optimize the evaluation of a set of window functions. We have integrated our scheme into PostgreSQL. Our comprehensive experimental study on the TPC-DS datasets as well as synthetic datasets and queries demonstrate significant speedup over existing approaches.

#index 1880444
#* Opening the black boxes in data flow optimization
#@ Fabian Hueske;Mathias Peters;Matthias J. Sax;Astrid Rheinländer;Rico Bergmann;Aljoscha Krettek;Kostas Tzoumas
#t 2012
#c 4
#% 249985
#% 287461
#% 411554
#% 481288
#% 903261
#% 963669
#% 983467
#% 1023420
#% 1063510
#% 1063553
#% 1127559
#% 1217249
#% 1292903
#% 1328059
#% 1328095
#% 1426486
#% 1468421
#% 1566972
#% 1573238
#% 1594583
#% 1594630
#! Many systems for big data analytics employ a data flow abstraction to define parallel data processing tasks. In this setting, custom operations expressed as user-defined functions are very common. We address the problem of performing data flow optimization at this level of abstraction, where the semantics of operators are not known. Traditionally, query optimization is applied to queries with known algebraic semantics. In this work, we find that a handful of properties, rather than a full algebraic specification, suffice to establish reordering conditions for data processing operators. We show that these properties can be accurately estimated for black box operators by statically analyzing the general-purpose code of their user-defined functions. We design and implement an optimizer for parallel data flows that does not assume knowledge of semantics or algebraic properties of operators. Our evaluation confirms that the optimizer can apply common rewritings such as selection reordering, bushy join-order enumeration, and limited forms of aggregation push-down, hence yielding similar rewriting power as modern relational DBMS optimizers. Moreover, it can optimize the operator order of nonrelational data flows, a unique feature among today's systems.

#index 1880445
#* Spinning fast iterative data flows
#@ Stephan Ewen;Kostas Tzoumas;Moritz Kaufmann;Volker Markl
#t 2012
#c 4
#% 13014
#% 23902
#% 58352
#% 58527
#% 101938
#% 411554
#% 462000
#% 470818
#% 479905
#% 479920
#% 565457
#% 754117
#% 963669
#% 983467
#% 1127559
#% 1181242
#% 1318636
#% 1426486
#% 1426513
#% 1464950
#% 1475077
#% 1523820
#% 1549835
#% 1566972
#% 1567910
#% 1594630
#% 1880444
#! Parallel dataflow systems are a central part of most analytic pipelines for big data. The iterative nature of many analysis and machine learning algorithms, however, is still a challenge for current systems. While certain types of bulk iterative algorithms are supported by novel dataflow frameworks, these systems cannot exploit computational dependencies present in many algorithms, such as graph algorithms. As a result, these algorithms are inefficiently executed and have led to specialized systems based on other paradigms, such as message passing or shared memory. We propose a method to integrate incremental iterations, a form of workset iterations, with parallel dataflows. After showing how to integrate bulk iterations into a dataflow system and its optimizer, we present an extension to the programming model for incremental iterations. The extension alleviates for the lack of mutable state in dataflows and allows for exploiting the sparse computational dependencies inherent in many iterative algorithms. The evaluation of a prototypical implementation shows that those aspects lead to up to two orders of magnitude speedup in algorithm runtime, when exploited. In our experiments, the improved dataflow system is highly competitive with specialized systems while maintaining a transparent and unified dataflow abstraction.

#index 1880446
#* REX: recursive, delta-based data-centric computation
#@ Svilen R. Mihaylov;Zachary G. Ives;Sudipto Guha
#t 2012
#c 4
#% 152928
#% 152940
#% 159337
#% 300179
#% 479937
#% 481288
#% 565457
#% 578391
#% 963669
#% 983467
#% 1015281
#% 1063553
#% 1206880
#% 1328186
#% 1354118
#% 1426513
#% 1464950
#% 1468411
#% 1468421
#% 1475077
#% 1523820
#% 1594630
#% 1621131
#! In today's Web and social network environments, query workloads include ad hoc and OLAP queries, as well as iterative algorithms that analyze data relationships (e.g., link analysis, clustering, learning). Modern DBMSs support ad hoc and OLAP queries, but most are not robust enough to scale to large clusters. Conversely, "cloud" platforms like MapReduce execute chains of batch tasks across clusters in a fault tolerant way, but have too much overhead to support ad hoc queries. Moreover, both classes of platform incur significant overhead in executing iterative data analysis algorithms. Most such iterative algorithms repeatedly refine portions of their answers, until some convergence criterion is reached. However, general cloud platforms typically must reprocess all data in each step. DBMSs that support recursive SQL are more efficient in that they propagate only the changes in each step --- but they still accumulate each iteration's state, even if it is no longer useful. User-defined functions are also typically harder to write for DBMSs than for cloud platforms. We seek to unify the strengths of both styles of platforms, with a focus on supporting iterative computations in which changes, in the form of deltas, are propagated from iteration to iteration, and state is efficiently updated in an extensible way. We present a programming model oriented around deltas, describe how we execute and optimize such programs in our REX runtime system, and validate that our platform also handles failures gracefully. We experimentally validate our techniques, and show speedups over the competing methods ranging from 2.5 to nearly 100 times.

#index 1880447
#* K-reach: who is in your small world
#@ James Cheng;Zechao Shang;Hong Cheng;Haixun Wang;Jeffrey Xu Yu
#t 2012
#c 4
#% 47573
#% 58365
#% 88051
#% 341704
#% 379482
#% 824692
#% 864462
#% 960304
#% 960305
#% 1044451
#% 1055756
#% 1063514
#% 1181254
#% 1181255
#% 1194484
#% 1194592
#% 1206685
#% 1217208
#% 1426510
#% 1426512
#% 1426539
#% 1484129
#% 1523819
#% 1581922
#% 1585382
#% 1592313
#% 1594607
#% 1618133
#% 1625106
#% 1669515
#% 1688299
#% 1770333
#% 1770357
#! We study the problem of answering k-hop reachability queries in a directed graph, i.e., whether there exists a directed path of length k, from a source query vertex to a target query vertex in the input graph. The problem of k-hop reachability is a general problem of the classic reachability (where k = ∞). Existing indexes for processing classic reachability queries, as well as for processing shortest path queries, are not applicable or not efficient for processing k-hop reachability queries. We propose an index for processing k-hop reachability queries, which is simple in design and efficient to construct. Our experimental results on a wide range of real datasets show that our index is more efficient than the state-of-the-art indexes even for processing classic reachability queries, for which these indexes are primarily designed. We also show that our index is efficient in answering k-hop reachability queries.

#index 1880448
#* Performance guarantees for distributed reachability queries
#@ Wenfei Fan;Xin Wang;Yinghui Wu
#t 2012
#c 4
#% 77943
#% 219211
#% 264263
#% 330305
#% 345693
#% 346045
#% 548463
#% 722530
#% 834423
#% 893106
#% 937549
#% 960276
#% 998845
#% 1023420
#% 1054227
#% 1111124
#% 1127560
#% 1127600
#% 1151709
#% 1164236
#% 1335136
#% 1372690
#% 1413162
#% 1426513
#% 1725392
#! In the real world a graph is often fragmented and distributed across different sites. This highlights the need for evaluating queries on distributed graphs. This paper proposes distributed evaluation algorithms for three classes of queries: reachability for determining whether one node can reach another, bounded reachability for deciding whether there exists a path of a bounded length between a pair of nodes, and regular reachability for checking whether there exists a path connecting two nodes such that the node labels on the path form a string in a given regular expression. We develop these algorithms based on partial evaluation, to explore parallel computation. When evaluating a query Q on a distributed graph G, we show that these algorithms possess the following performance guarantees, no matter how G is fragmented and distributed: (1) each site is visited only once; (2) the total network traffic is determined by the size of Q and the fragmentation of G, independent of the size of G; and (3) the response time is decided by the largest fragment of G rather than the entire G. In addition, we show that these algorithms can be readily implemented in the MapReduce framework. Using synthetic and real-life data, we experimentally verify that these algorithms are scalable on large graphs, regardless of how the graphs are distributed.

#index 1880449
#* Efficient indexing and querying over syntactically annotated trees
#@ Pirooz Chubak;Davood Rafiei
#t 2012
#c 4
#% 333981
#% 340144
#% 342398
#% 378391
#% 397375
#% 504160
#% 659999
#% 765429
#% 817472
#% 864439
#% 878624
#% 1013630
#% 1275182
#% 1471191
#% 1478182
#! Natural language text corpora are often available as sets of syntactically parsed trees. A wide range of expressive tree queries are possible over such parsed trees that open a new avenue in searching over natural language text. They not only allow for querying roles and relationships within sentences, but also improve search effectiveness compared to flat keyword queries. One major drawback of current systems supporting querying over parsed text is the performance of evaluating queries over large data. In this paper we propose a novel indexing scheme over unique subtrees as index keys. We also propose a novel root-split coding scheme that stores subtree structural information only partially, thus reducing index size and improving querying performance. Our extensive set of experiments show that root-split coding reduces the index size of any interval coding which stores individual node numbers by a factor of 50% to 80%, depending on the sizes of subtrees indexed. Moreover, We show that our index using root-split coding, outperforms previous approaches by at least an order of magnitude in terms of the response time of queries.

#index 1880450
#* Queries with guarded negation
#@ Vince Bárány;Balder ten Cate;Martin Otto
#t 2012
#c 4
#% 36181
#% 54225
#% 190638
#% 230142
#% 342829
#% 343623
#% 384978
#% 582129
#% 587508
#% 598376
#% 733595
#% 822573
#% 826032
#% 1108958
#% 1348452
#% 1585244
#% 1611372
#% 1661437
#% 1724574
#% 1826177
#! A well-established and fundamental insight in database theory is that negation (also known as complementation) tends to make queries difficult to process and difficult to reason about. Many basic problems are decidable and admit practical algorithms in the case of unions of conjunctive queries, but become difficult or even undecidable when queries are allowed to contain negation. Inspired by recent results in finite model theory, we consider a restricted form of negation, guarded negation. We introduce a fragment of SQL, called GN-SQL, as well as a fragment of Datalog with stratified negation, called GN-Datalog, that allow only guarded negation, and we show that these query languages are computationally well behaved, in terms of testing query containment, query evaluation, open-world query answering, and boundedness. GN-SQL and GN-Datalog subsume a number of well known query languages and constraint languages, such as unions of conjunctive queries, monadic Datalog, and frontier-guarded tgds. In addition, an analysis of standard benchmark workloads shows that many uses of negation in SQL in practice are guarded.

#index 1880451
#* PrivBasis: frequent itemset mining with differential privacy
#@ Ninghui Li;Wahbeh Qardaji;Dong Su;Jianneng Cao
#t 2012
#c 4
#% 227919
#% 322619
#% 481290
#% 576110
#% 576761
#% 729418
#% 809245
#% 824726
#% 956509
#% 963241
#% 977011
#% 1029084
#% 1061644
#% 1066737
#% 1083709
#% 1127361
#% 1190072
#% 1206581
#% 1214684
#% 1328187
#% 1372692
#% 1451189
#% 1451190
#% 1523886
#% 1523887
#% 1595893
#% 1605968
#% 1670071
#% 1692264
#% 1740518
#! The discovery of frequent itemsets can serve valuable economic and research purposes. Releasing discovered frequent itemsets, however, presents privacy challenges. In this paper, we study the problem of how to perform frequent itemset mining on transaction databases while satisfying differential privacy. We propose an approach, called PrivBasis, which leverages a novel notion called basis sets. A θ-basis set has the property that any itemset with frequency higher than θ is a subset of some basis. We introduce algorithms for privately constructing a basis set and then using it to find the most frequent itemsets. Experiments show that our approach greatly outperforms the current state of the art.

#index 1880452
#* Low-rank mechanism: optimizing batch queries under differential privacy
#@ Ganzhao Yuan;Zhenjie Zhang;Marianne Winslett;Xiaokui Xiao;Yin Yang;Zhifeng Hao
#t 2012
#c 4
#% 230264
#% 416706
#% 576110
#% 1029084
#% 1061644
#% 1073906
#% 1214684
#% 1300083
#% 1426322
#% 1426454
#% 1426563
#% 1451189
#% 1451190
#% 1464628
#% 1521653
#% 1523886
#% 1581864
#% 1581865
#% 1605968
#% 1606359
#% 1627567
#% 1740518
#% 1846816
#% 1846817
#! Differential privacy is a promising privacy-preserving paradigm for statistical query processing over sensitive data. It works by injecting random noise into each query result, such that it is provably hard for the adversary to infer the presence or absence of any individual record from the published noisy results. The main objective in differentially private query processing is to maximize the accuracy of the query results, while satisfying the privacy guarantees. Previous work, notably the matrix mechanism [16], has suggested that processing a batch of correlated queries as a whole can potentially achieve considerable accuracy gains, compared to answering them individually. However, as we point out in this paper, the matrix mechanism is mainly of theoretical interest; in particular, several inherent problems in its design limit its accuracy in practice, which almost never exceeds that of naïve methods. In fact, we are not aware of any existing solution that can effectively optimize a query batch under differential privacy. Motivated by this, we propose the Low-Rank Mechanism (LRM), the first practical differentially private technique for answering batch queries with high accuracy, based on a low rank approximation of the workload matrix. We prove that the accuracy provided by LRM is close to the theoretical lower bound for any mechanism to answer a batch of queries under differential privacy. Extensive experiments using real data demonstrate that LRM consistently outperforms state-of-the-art query processing solutions under differential privacy, by large margins.

#index 1880453
#* Functional mechanism: regression analysis under differential privacy
#@ Jun Zhang;Zhenjie Zhang;Xiaokui Xiao;Yin Yang;Marianne Winslett
#t 2012
#c 4
#% 302906
#% 963241
#% 977011
#% 1029084
#% 1190072
#% 1198225
#% 1214684
#% 1426454
#% 1426563
#% 1451189
#% 1451190
#% 1464628
#% 1523886
#% 1581864
#% 1584903
#% 1606359
#% 1627567
#% 1692264
#% 1740518
#% 1818428
#% 1846816
#% 1846817
#! ε-differential privacy is the state-of-the-art model for releasing sensitive information while protecting privacy. Numerous methods have been proposed to enforce ε-differential privacy in various analytical tasks, e.g., regression analysis. Existing solutions for regression analysis, however, are either limited to non-standard types of regression or unable to produce accurate regression results. Motivated by this, we propose the Functional Mechanism, a differentially private method designed for a large class of optimization-based analyses. The main idea is to enforce ε-differential privacy by perturbing the objective function of the optimization problem, rather than its results. As case studies, we apply the functional mechanism to address two most widely used regression models, namely, linear regression and logistic regression. Both theoretical analysis and thorough experimental evaluations show that the functional mechanism is highly effective and efficient, and it significantly outperforms existing solutions.

#index 1880454
#* Injecting uncertainty in graphs for identity obfuscation
#@ Paolo Boldi;Francesco Bonchi;Aristides Gionis;Tamir Tassa
#t 2012
#c 4
#% 190330
#% 243166
#% 248030
#% 576762
#% 577219
#% 956511
#% 1063476
#% 1127360
#% 1127417
#% 1195272
#% 1206713
#% 1206763
#% 1259854
#% 1328173
#% 1328188
#% 1366214
#% 1372691
#% 1415851
#% 1451203
#% 1464049
#% 1523884
#% 1524264
#% 1524388
#% 1560417
#% 1580227
#% 1592313
#% 1594593
#% 1606039
#% 1688517
#% 1984487
#! Data collected nowadays by social-networking applications create fascinating opportunities for building novel services, as well as expanding our understanding about social structures and their dynamics. Unfortunately, publishing social-network graphs is considered an ill-advised practice due to privacy concerns. To alleviate this problem, several anonymization methods have been proposed, aiming at reducing the risk of a privacy breach on the published data, while still allowing to analyze them and draw relevant conclusions. In this paper we introduce a new anonymization approach that is based on injecting uncertainty in social graphs and publishing the resulting uncertain graphs. While existing approaches obfuscate graph data by adding or removing edges entirely, we propose using a finer-grained perturbation that adds or removes edges partially: this way we can achieve the same desired level of obfuscation with smaller changes in the data, thus maintaining higher utility. Our experiments on real-world networks confirm that at the same level of identity obfuscation our method provides higher usefulness than existing randomized methods that publish standard graphs.

#index 1880455
#* Publishing microdata with a robust privacy guarantee
#@ Jianneng Cao;Panagiotis Karras
#t 2012
#c 4
#% 325683
#% 443397
#% 443463
#% 576111
#% 796201
#% 800513
#% 801690
#% 810011
#% 864406
#% 893100
#% 937550
#% 960289
#% 1022246
#% 1022247
#% 1063505
#% 1070891
#% 1083631
#% 1083653
#% 1181277
#% 1200329
#% 1206582
#% 1214673
#% 1217156
#% 1370254
#% 1446819
#% 1513038
#% 1523849
#% 1523888
#% 1538423
#% 1581862
#% 1581865
#% 1606068
#% 1670071
#! Today, the publication of microdata poses a privacy threat. Vast research has striven to define the privacy condition that microdata should satisfy before it is released, and devise algorithms to anonymize the data so as to achieve this condition. Yet, no method proposed to date explicitly bounds the percentage of information an adversary gains after seeing the published data for each sensitive value therein. This paper introduces β-likeness, an appropriately robust privacy model for microdata anonymization, along with two anonymization schemes designed therefore, the one based on generalization, and the other based on perturbation. Our model postulates that an adversary's confidence on the likelihood of a certain sensitive-attribute (SA) value should not increase, in relative difference terms, by more than a predefined threshold. Our techniques aim to satisfy a given β threshold with little information loss. We experimentally demonstrate that (i) our model provides an effective privacy guarantee in a way that predecessor models cannot, (ii) our generalization scheme is more effective and efficient in its task than methods adapting algorithms for the k-anonymity model, and (iii) our perturbation method outperforms a baseline approach. Moreover, we discuss in detail the resistance of our model and methods to attacks proposed in previous research.

#index 1880456
#* Measuring two-event structural correlations on graphs
#@ Ziyu Guan;Xifeng Yan;Lance M. Kaplan
#t 2012
#c 4
#% 86786
#% 220708
#% 268079
#% 466644
#% 629708
#% 765430
#% 818916
#% 1328169
#% 1399940
#% 1426574
#% 1446960
#% 1475163
#% 1562244
#% 1581924
#! Real-life graphs usually have various kinds of events happening on them, e.g., product purchases in online social networks and intrusion alerts in computer networks. The occurrences of events on the same graph could be correlated, exhibiting either attraction or repulsion. Such structural correlations can reveal important relationships between different events. Unfortunately, correlation relationships on graph structures are not well studied and cannot be captured by traditional measures. In this work, we design a novel measure for assessing two-event structural correlations on graphs. Given the occurrences of two events, we choose uniformly a sample of "reference nodes" from the vicinity of all event nodes and employ the Kendall's τ rank correlation measure to compute the average concordance of event density changes. Significance can be efficiently assessed by τ's nice property of being asymptotically normal under the null hypothesis. In order to compute the measure in large scale networks, we develop a scalable framework using different sampling strategies. The complexity of these strategies is analyzed. Experiments on real graph datasets with both synthetic and real events demonstrate that the proposed framework is not only efficacious, but also efficient and scalable.

#index 1880457
#* Ranking large temporal data
#@ Jeffrey Jestes;Jeff M. Phillips;Feifei Li;Mingwang Tang
#t 2012
#c 4
#% 227866
#% 466506
#% 480299
#% 480756
#% 548489
#% 656697
#% 723366
#% 726629
#% 745485
#% 745513
#% 881459
#% 1022238
#% 1054486
#% 1072636
#% 1075132
#% 1083693
#% 1194585
#% 1206852
#% 1426548
#% 1483642
#! Ranking temporal data has not been studied until recently, even though ranking is an important operator (being promoted as a first-class citizen) in database systems. However, only the instant top-k queries on temporal data were studied in, where objects with the k highest scores at a query time instance t are to be retrieved. The instant top-k definition clearly comes with limitations (sensitive to outliers, difficult to choose a meaningful query time t). A more flexible and general ranking operation is to rank objects based on the aggregation of their scores in a query interval, which we dub the aggregate top-k query on temporal data. For example, return the top-10 weather stations having the highest average temperature from 10/01/2010 to 10/07/2010; find the top-20 stocks having the largest total transaction volumes from 02/05/2011 to 02/07/2011. This work presents a comprehensive study to this problem by designing both exact and approximate methods (with approximation quality guarantees). We also provide theoretical analysis on the construction cost, the index size, the update and the query costs of each approach. Extensive experiments on large real datasets clearly demonstrate the efficiency, the effectiveness, and the scalability of our methods compared to the baseline methods.

#index 1880458
#* Compacting transactional data in hybrid OLTP&OLAP databases
#@ Florian Funke;Alfons Kemper;Thomas Neumann
#t 2012
#c 4
#% 286258
#% 287725
#% 322412
#% 323271
#% 333953
#% 461896
#% 480464
#% 750095
#% 864446
#% 875026
#% 960266
#% 963628
#% 1127596
#% 1206624
#% 1217168
#% 1426547
#% 1581459
#% 1581868
#% 1586199
#% 1592312
#% 1594617
#% 1628175
#% 1667313
#% 1697286
#! Growing main memory sizes have facilitated database management systems that keep the entire database in main memory. The drastic performance improvements that came along with these in-memory systems have made it possible to reunite the two areas of online transaction processing (OLTP) and online analytical processing (OLAP): An emerging class of hybrid OLTP and OLAP database systems allows to process analytical queries directly on the transactional data. By offering arbitrarily current snapshots of the transactional data for OLAP, these systems enable real-time business intelligence. Despite memory sizes of several Terabytes in a single commodity server, RAM is still a precious resource: Since free memory can be used for intermediate results in query processing, the amount of memory determines query performance to a large extent. Consequently, we propose the compaction of memory-resident databases. Compaction consists of two tasks: First, separating the mutable working set from the immutable "frozen" data. Second, compressing the immutable data and optimizing it for efficient, memory-consumption-friendly snapshotting. Our approach reorganizes and compresses transactional data online and yet hardly affects the mission-critical OLTP throughput. This is achieved by unburdening the OLTP threads from all additional processing and performing these tasks asynchronously.

#index 1880459
#* Processing a trillion cells per mouse click
#@ Alexander Hall;Olaf Bachmann;Robert Büssow;Silviu Gănceanu;Marc Nunkesser
#t 2012
#c 4
#% 2833
#% 223781
#% 232657
#% 415958
#% 479630
#% 481450
#% 519953
#% 547976
#% 723279
#% 753247
#% 824697
#% 875026
#% 1016130
#% 1023420
#% 1054227
#% 1063542
#% 1127565
#% 1138538
#% 1166469
#% 1217169
#% 1328108
#% 1551293
#% 1560641
#% 1599938
#! Column-oriented database systems have been a real game changer for the industry in recent years. Highly tuned and performant systems have evolved that provide users with the possibility of answering ad hoc queries over large datasets in an interactive manner. In this paper we present the column-oriented datastore developed as one of the central components of PowerDrill. It combines the advantages of columnar data layout with other known techniques (such as using composite range partitions) and extensive algorithmic engineering on key data structures. The main goal of the latter being to reduce the main memory footprint and to increase the efficiency in processing typical user queries. In this combination we achieve large speed-ups. These enable a highly interactive Web UI where it is common that a single mouse click leads to processing a trillion values in the underlying dataset.

#index 1880460
#* OLTP on hardware islands
#@ Danica Porobic;Ippokratis Pandis;Miguel Branco;Pınar Tözün;Anastasia Ailamaki
#t 2012
#c 4
#% 172939
#% 202152
#% 251473
#% 286836
#% 287230
#% 307360
#% 480119
#% 480831
#% 789387
#% 801348
#% 1022298
#% 1063543
#% 1111848
#% 1181215
#% 1213682
#% 1278375
#% 1328149
#% 1426552
#% 1467093
#% 1523799
#% 1523856
#% 1523878
#% 1563004
#% 1581849
#% 1589961
#% 1594617
#% 1606343
#% 1654042
#% 1668635
#% 1968412
#! Modern hardware is abundantly parallel and increasingly heterogeneous. The numerous processing cores have nonuniform access latencies to the main memory and to the processor caches, which causes variability in the communication costs. Unfortunately, database systems mostly assume that all processing cores are the same and that microarchitecture differences are not significant enough to appear in critical database execution paths. As we demonstrate in this paper, however, hardware heterogeneity does appear in the critical path and conventional database architectures achieve suboptimal and even worse, unpredictable performance. We perform a detailed performance analysis of OLTP deployments in servers with multiple cores per CPU (multicore) and multiple CPUs per server (multisocket). We compare different database deployment strategies where we vary the number and size of independent database instances running on a single server, from a single shared-everything instance to fine-grained shared-nothing configurations. We quantify the impact of non-uniform hardware on various deployments by (a) examining how efficiently each deployment uses the available hardware resources and (b) measuring the impact of distributed transactions and skewed requests on different workloads. Finally, we argue in favor of shared-nothing deployments that are topology- and workload-aware and take advantage of fast on-chip communication between islands of cores on the same socket.

#index 1880461
#* Serializability, not serial: concurrency control and availability in multi-datacenter datastores
#@ Stacy Patterson;Aaron J. Elmore;Faisal Nawab;Divyakant Agrawal;Amr El Abbadi
#t 2012
#c 4
#% 9241
#% 251359
#% 723279
#% 791021
#% 978404
#% 989488
#% 1192730
#% 1426489
#% 1426492
#% 1468219
#% 1468530
#% 1538766
#% 1592341
#% 1594649
#% 1625041
#! We present a framework for concurrency control and availability in multi-datacenter datastores. While we consider Google's Megastore as our motivating example, we define general abstractions for key components, making our solution extensible to any system that satisfies the abstraction properties. We first develop and analyze a transaction management and replication protocol based on a straightforward implementation of the Paxos algorithm. Our investigation reveals that this protocol acts as a concurrency prevention mechanism rather than a concurrency control mechanism. We then propose an enhanced protocol called Paxos with Combination and Promotion (Paxos-CP) that provides true transaction concurrency while requiring the same per instance message complexity as the basic Paxos protocol. Finally, we compare the performance of Paxos and Paxos-CP in a multi-datacenter experimental study, and we demonstrate that Paxos-CP results in significantly fewer aborted transactions than basic Paxos.

#index 1880462
#* Automatic partitioning of database applications
#@ Alvin Cheung;Samuel Madden;Owen Arden;Andrew C. Myers
#t 2012
#c 4
#% 19622
#% 46576
#% 126531
#% 158821
#% 267206
#% 397397
#% 398238
#% 604457
#% 616965
#% 765431
#% 793318
#% 809977
#% 875029
#% 956528
#% 998833
#% 1124078
#% 1127439
#% 1216361
#% 1266688
#% 1394333
#% 1404050
#% 1432720
#% 1528269
#% 1563023
#% 1589880
#% 1594644
#% 1628699
#% 1736360
#! Database-backed applications are nearly ubiquitous in our daily lives. Applications that make many small accesses to the database create two challenges for developers: increased latency and wasted resources from numerous network round trips. A well-known technique to improve transactional database application performance is to convert part of the application into stored procedures that are executed on the database server. Unfortunately, this conversion is often difficult. In this paper we describe Pyxis, a system that takes database-backed applications and automatically partitions their code into two pieces, one of which is executed on the application server and the other on the database server. Pyxis profiles the application and server loads, statically analyzes the code's dependencies, and produces a partitioning that minimizes the number of control transfers as well as the amount of data sent during each transfer. Our experiments using TPC-C and TPC-W show that Pyxis is able to generate partitions with up to 3x reduction in latency and 1.7x improvement in throughput when compared to a traditional non-partitioned implementation and has comparable performance to that of a custom stored procedure implementation.

#index 1880463
#* CrowdER: crowdsourcing entity resolution
#@ Jiannan Wang;Tim Kraska;Michael J. Franklin;Jianhua Feng
#t 2012
#c 4
#% 216424
#% 577238
#% 729913
#% 864392
#% 913783
#% 956506
#% 1063533
#% 1070283
#% 1206636
#% 1426567
#% 1432722
#% 1452857
#% 1477589
#% 1523838
#% 1526538
#% 1581851
#% 1590535
#% 1628171
#% 1890006
#! Entity resolution is central to data integration and data cleaning. Algorithmic approaches have been improving in quality, but remain far from perfect. Crowdsourcing platforms offer a more accurate but expensive (and slow) way to bring human insight into the process. Previous work has proposed batching verification tasks for presentation to human workers but even with batching, a human-only approach is infeasible for data sets of even moderate size, due to the large numbers of matches to be tested. Instead, we propose a hybrid human-machine approach in which machines are used to do an initial, coarse pass over all the data, and people are used to verify only the most likely matching pairs. We show that for such a hybrid system, generating the minimum number of verification tasks of a given size is NP-Hard, but we develop a novel two-tiered heuristic approach for creating batched tasks. We describe this method, and present the results of extensive experiments on real data sets using a popular crowdsourcing platform. The experiments show that our hybrid approach achieves both good efficiency and high accuracy compared to machine-only or human-only alternatives.

#index 1880464
#* Whom to ask?: jury selection for decision making tasks on micro-blog services
#@ Caleb Chen Cao;Jieying She;Yongxin Tong;Lei Chen
#t 2012
#c 4
#% 662755
#% 730082
#% 818434
#% 956516
#% 1150163
#% 1190108
#% 1214668
#% 1279817
#% 1355029
#% 1355041
#% 1394202
#% 1400018
#% 1425621
#% 1426462
#% 1452857
#% 1472273
#% 1550748
#% 1560422
#% 1581851
#% 1587203
#! It is universal to see people obtain knowledge on micro-blog services by asking others decision making questions. In this paper, we study the Jury Selection Problem(JSP) by utilizing crowdsourcing for decision making tasks on micro-blog services. Specifically, the problem is to enroll a subset of crowd under a limited budget, whose aggregated wisdom via Majority Voting scheme has the lowest probability of drawing a wrong answer(Jury Error Rate-JER). Due to various individual error-rates of the crowd, the calculation of JER is non-trivial. Firstly, we explicitly state that JER is the probability when the number of wrong jurors is larger than half of the size of a jury. To avoid the exponentially increasing calculation of JER, we propose two efficient algorithms and an effective bounding technique. Furthermore, we study the Jury Selection Problem on two crowdsourcing models, one is for altruistic users(AltrM) and the other is for incentive-requiring users(PayM) who require extra payment when enrolled into a task. For the AltrM model, we prove the monotonicity of JER on individual error rate and propose an efficient exact algorithm for JSP. For the PayM model, we prove the NP-hardness of JSP on PayM and propose an efficient greedy-based heuristic algorithm. Finally, we conduct a series of experiments to investigate the traits of JSP, and validate the efficiency and effectiveness of our proposed algorithms on both synthetic and real micro-blog data.

#index 1880465
#* ALAE: accelerating local alignment with affine gap exactly in biosequence databases
#@ Xiaochun Yang;Honglei Liu;Bin Wang
#t 2012
#c 4
#% 143306
#% 593970
#% 778470
#% 801064
#% 823464
#% 1015330
#% 1044404
#% 1374787
#! We study the problem of local alignment, which is finding pairs of similar subsequences with gaps. The problem exists in biosequence databases. BLAST is a typical software for finding local alignment based on heuristic, but could miss results. Using the Smith-Waterman algorithm, we can find all local alignments in O(mn) time, where m and n are lengths of a query and a text, respectively. A recent exact approach BWT-SW improves the complexity of the Smith-Waterman algorithm under constraints, but still much slower than BLAST. This paper takes on the challenge of designing an accurate and efficient algorithm for evaluating local-alignment searches, especially for long queries. In this paper, we propose an efficient software called ALAE to speed up BWT-SW using a compressed suffix array. ALAE utilizes a family of filtering techniques to prune meaningless calculations and an algorithm for reusing score calculations. We also give a mathematical analysis and show that the upper bound of the total number of calculated entries using ALAE could vary from 4.50mn0.520 to 9.05mn0.896 for random DNA sequences and vary from 8.28mn0.364 to 7.49mn0.723 for random protein sequences. We demonstrate the significant performance improvement of ALAE on BWT-SW using a thorough experimental study on real biosequences. ALAE guarantees correctness and accelerates BLAST for most of parameters.

#index 1880466
#* sDTW: computing DTW distances using locally relevant constraints based on salient feature alignments
#@ K. Selçuk Candan;Rosaria Rossini;Xiaolan Wang;Maria Luisa Sapino
#t 2012
#c 4
#% 310545
#% 481609
#% 635689
#% 760805
#% 824956
#% 993965
#% 1056068
#% 1127609
#% 1872261
#! Many applications generate and consume temporal data and retrieval of time series is a key processing step in many application domains. Dynamic time warping (DTW) distance between time series of size N and M is computed relying on a dynamic programming approach which creates and fills an N x M grid to search for an optimal warp path. Since this can be costly, various heuristics have been proposed to cut away the potentially unproductive portions of the DTW grid. In this paper, we argue that time series often carry structural features that can be used for identifying locally relevant constraints to eliminate redundant work. Relying on this observation, we propose salient feature based sDTW algorithms which first identify robust salient features in the given time series and then find a consistent alignment of these to establish the boundaries for the warp path search. More specifically, we propose alternative fixed core&adaptive width, adaptive core&fixed width, and adaptive core&adaptive width strategies which enforce different constraints reflecting the high level structural characteristics of the series in the data set. Experiment results show that the proposed sDTW algorithms help achieve much higher accuracy in DTW computation and time series retrieval than fixed core & fixed width algorithms that do not leverage local features of the given time series.

#index 1880467
#* SCOUT: prefetching for latent structure following queries
#@ Farhan Tauheed;Thomas Heinis;Felix Schürmann;Henry Markram;Anastasia Ailamaki
#t 2012
#c 4
#% 197016
#% 213501
#% 252304
#% 261875
#% 262121
#% 303775
#% 330333
#% 462059
#% 463903
#% 485312
#% 489375
#% 543067
#% 721139
#% 738545
#% 745501
#% 875016
#% 1142312
#% 1142428
#% 1379583
#% 1441239
#% 1476091
#% 1486250
#% 1720757
#% 1846821
#! Today's scientists are quickly moving from in vitro to in silico experimentation: they no longer analyze natural phenomena in a petri dish, but instead they build models and simulate them. Managing and analyzing the massive amounts of data involved in simulations is a major task. Yet, they lack the tools to efficiently work with data of this size. One problem many scientists share is the analysis of the massive spatial models they build. For several types of analysis they need to interactively follow the structures in the spatial model, e.g., the arterial tree, neuron fibers, etc., and issue range queries along the way. Each query takes long to execute, and the total time for executing a sequence of queries significantly delays data analysis. Prefetching the spatial data reduces the response time considerably, but known approaches do not prefetch with high accuracy. We develop SCOUT, a structure-aware method for prefetching data along interactive spatial query sequences. SCOUT uses an approximate graph model of the structures involved in past queries and attempts to identify what particular structure the user follows. Our experiments with neuro-science data show that SCOUT prefetches with an accuracy from 71% to 92%, which translates to a speedup of 4x-15x. SCOUT also improves the prefetching accuracy on datasets from other scientific domains, such as medicine and biology.

#index 1880468
#* Accelerating pathology image data cross-comparison on CPU-GPU hybrid systems
#@ Kaibo Wang;Yin Huai;Rubao Lee;Fusheng Wang;Xiaodong Zhang;Joel H. Saltz
#t 2012
#c 4
#% 227934
#% 235114
#% 336390
#% 356568
#% 481455
#% 654479
#% 765419
#% 810039
#% 835018
#% 874997
#% 1038149
#% 1063508
#% 1219782
#% 1426530
#% 1434033
#% 1455916
#% 1550752
#% 1591699
#% 1592339
#% 1601173
#% 1730862
#! As an important application of spatial databases in pathology imaging analysis, cross-comparing the spatial boundaries of a huge amount of segmented micro-anatomic objects demands extremely data- and compute-intensive operations, requiring high throughput at an affordable cost. However, the performance of spatial database systems has not been satisfactory since their implementations of spatial operations cannot fully utilize the power of modern parallel hardware. In this paper, we provide a customized software solution that exploits GPUs and multi-core CPUs to accelerate spatial cross-comparison in a cost-effective way. Our solution consists of an efficient GPU algorithm and a pipelined system framework with task migration support. Extensive experiments with real-world data sets demonstrate the effectiveness of our solution, which improves the performance of spatial cross-comparison by over 18 times compared with a parallelized spatial database approach.

#index 1880469
#* Robust estimation of resource consumption for SQL queries using statistical techniques
#@ Jiexing Li;Arnd Christian König;Vivek Narasayya;Surajit Chaudhuri
#t 2012
#c 4
#% 136740
#% 273901
#% 333947
#% 476622
#% 480803
#% 824675
#% 960323
#% 1206984
#% 1549874
#% 1581872
#% 1581874
#% 1846730
#! The ability to estimate resource consumption of SQL queries is crucial for a number of tasks in a database system such as admission control, query scheduling and costing during query optimization. Recent work has explored the use of statistical techniques for resource estimation in place of the manually constructed cost models used in query optimization. Such techniques, which require as training data examples of resource usage in queries, offer the promise of superior estimation accuracy since they can account for factors such as hardware characteristics of the system or bias in cardinality estimates. However, the proposed approaches lack robustness in that they do not generalize well to queries that are different from the training examples, resulting in significant estimation errors. Our approach aims to address this problem by combining knowledge of database query processing with statistical models. We model resource-usage at the level of individual operators, with different models and features for each operator type, and explicitly model the asymptotic behavior of each operator. This results in significantly better estimation accuracy and the ability to estimate resource usage of arbitrary plans, even when they are very different from the training instances. We validate our approach using various large scale real-life and benchmark workloads on Microsoft SQL Server.

#index 1880470
#* Who tags what?: an analysis framework
#@ Mahashweta Das;Saravanan Thirumuruganathan;Sihem Amer-Yahia;Gautam Das;Cong Yu
#t 2012
#c 4
#% 27618
#% 46803
#% 71901
#% 205305
#% 249321
#% 347225
#% 479973
#% 722904
#% 1206904
#% 1217225
#% 1429407
#% 1429408
#% 1429409
#% 1429421
#% 1429422
#! The rise of Web 2.0 is signaled by sites such as Flickr, del.icio.us, and YouTube, and social tagging is essential to their success. A typical tagging action involves three components, user, item (e.g., photos in Flickr), and tags (i.e., words or phrases). Analyzing how tags are assigned by certain users to certain items has important implications in helping users search for desired information. In this paper, we explore common analysis tasks and propose a dual mining framework for social tagging behavior mining. This framework is centered around two opposing measures, similarity and diversity, being applied to one or more tagging components, and therefore enables a wide range of analysis scenarios such as characterizing similar users tagging diverse items with similar tags, or diverse users tagging similar items with diverse tags, etc. By adopting different concrete measures for similarity and diversity in the framework, we show that a wide range of concrete analysis problems can be defined and they are NP-Complete in general. We design efficient algorithms for solving many of those problems and demonstrate, through comprehensive experiments over real data, that our algorithms significantly out-perform the exact brute-force approach without compromising analysis result quality.

#index 1880471
#* A generic framework for efficient and effective subsequence retrieval
#@ Haohan Zhu;George Kollios;Vassilis Athitsos
#t 2012
#c 4
#% 172949
#% 281750
#% 294634
#% 310545
#% 397381
#% 464994
#% 479462
#% 749529
#% 824678
#% 875957
#% 893163
#% 993965
#% 1016195
#% 1022219
#% 1022227
#% 1022237
#% 1044404
#% 1063496
#% 1063497
#% 1072633
#% 1231157
#% 1328126
#% 1328178
#% 1535445
#% 1581883
#% 1581884
#% 1619582
#% 1872261
#! This paper proposes a general framework for matching similar subsequences in both time series and string databases. The matching results are pairs of query subsequences and database subsequences. The framework finds all possible pairs of similar subsequences if the distance measure satisfies the "consistency" property, which is a property introduced in this paper. We show that most popular distance functions, such as the Euclidean distance, DTW, ERP, the Frechét distance for time series, and the Hamming distance and Levenshtein distance for strings, are all "consistent". We also propose a generic index structure for metric spaces named "reference net". The reference net occupies O(n) space, where n is the size of the dataset and is optimized to work well with our framework. The experiments demonstrate the ability of our method to improve retrieval performance when combined with diverse distance measures. The experiments also illustrate that the reference net scales well in terms of space overhead and query time.

#index 1880472
#* Only aggressive elephants are fast elephants
#@ Jens Dittrich;Jorge-Arnulfo Quiané-Ruiz;Stefan Richter;Stefan Schuh;Alekh Jindal;Jörg Schad
#t 2012
#c 4
#% 300194
#% 340175
#% 480821
#% 778724
#% 1217159
#% 1278124
#% 1366463
#% 1386049
#% 1426584
#% 1426588
#% 1471595
#% 1523806
#% 1523836
#% 1523837
#% 1523839
#% 1523841
#% 1523924
#% 1542029
#% 1573236
#% 1573238
#% 1581407
#% 1586685
#% 1591706
#% 1592315
#% 1594684
#% 1621145
#% 1895102
#! Yellow elephants are slow. A major reason is that they consume their inputs entirely before responding to an elephant rider's orders. Some clever riders have trained their yellow elephants to only consume parts of the inputs before responding. However, the teaching time to make an elephant do that is high. So high that the teaching lessons often do not pay off. We take a different approach. We make elephants aggressive; only this will make them very fast. We propose HAIL (Hadoop Aggressive Indexing Library), an enhancement of HDFS and Hadoop MapReduce that dramatically improves runtimes of several classes of MapReduce jobs. HAIL changes the upload pipeline of HDFS in order to create different clustered indexes on each data block replica. An interesting feature of HAIL is that we typically create a win-win situation: we improve both data upload to HDFS and the runtime of the actual Hadoop MapReduce job. In terms of data upload, HAIL improves over HDFS by up to 60% with the default replication factor of three. In terms of query execution, we demonstrate that HAIL runs up to 68x faster than Hadoop. In our experiments, we use six clusters including physical and EC2 clusters of up to 100 nodes. A series of scalability experiments also demonstrates the superiority of HAIL.

#index 1880473
#* Multiple location profiling for users and relationships from social network and content
#@ Rui Li;Shengjie Wang;Kevin Chen-Chuan Chang
#t 2012
#c 4
#% 480467
#% 766441
#% 881498
#% 956510
#% 987205
#% 1055707
#% 1083684
#% 1083687
#% 1117695
#% 1190131
#% 1355044
#% 1399939
#% 1482254
#% 1650403
#% 1872354
#! Users' locations are important for many applications such as personalized search and localized content delivery. In this paper, we study the problem of profiling Twitter users' locations with their following network and tweets. We propose a multiple location profiling model (MLP), which has three key features: 1) it formally models how likely a user follows another user given their locations and how likely a user tweets a venue given his location, 2) it fundamentally captures that a user has multiple locations and his following relationships and tweeted venues can be related to any of his locations, and some of them are even noisy, and 3) it novelly utilizes the home locations of some users as partial supervision. As a result, MLP not only discovers users' locations accurately and completely, but also "explains" each following relationship by revealing users' true locations in the relationship. Experiments on a large-scale data set demonstrate those advantages. Particularly, 1) for predicting users' home locations, MLP successfully places 62% users and out-performs two state-of-the-art methods by 10% in accuracy, 2) for discovering users' multiple locations, MLP improves the baseline methods by 14% in recall, and 3) for explaining following relationships, MLP achieves 57% accuracy.

#index 1880474
#* Flash-based extended cache for higher throughput and faster recovery
#@ Woon-Hak Kang;Sang-Won Lee;Bongki Moon
#t 2012
#c 4
#% 227871
#% 323285
#% 403195
#% 523890
#% 1063551
#% 1092670
#% 1127391
#% 1181215
#% 1217213
#% 1328052
#% 1523922
#% 1581941
#% 1586198
#% 1601162
#! Considering the current price gap between disk and flash memory drives, for applications dealing with large scale data, it will be economically more sensible to use flash memory drives to supplement disk drives rather than to replace them. This paper presents FaCE, which is a new low-overhead caching strategy that uses flash memory as an extension to the DRAM buffer. FaCE aims at improving the transaction throughput as well as shortening the recovery time from a system failure. To achieve the goals, we propose two novel algorithms for flash cache management, namely, Multi-Version FIFO replacement and Group Second Chance. One striking result from FaCE is that using a small flash memory drive as a caching device could deliver even higher throughput than using a large flash memory drive to store the entire database tables. This was possible due to flash write optimization as well as disk access reduction obtained by the FaCE caching methods. In addition, FaCE takes advantage of the non-volatility of flash memory to fully support database recovery by extending the scope of a persistent database to include the data pages stored in the flash cache. We have implemented FaCE in the PostgreSQL open source database server and demonstrated its effectiveness for TPC-C benchmarks.

#index 1880475
#* Don't thrash: how to cache your hash on flash
#@ Michael A. Bender;Martin Farach-Colton;Rob Johnson;Russell Kraner;Bradley C. Kuszmaul;Dzejla Medjedovic;Pablo Montes;Pradeep Shetty;Richard P. Spillane;Erez Zadok
#t 2012
#c 4
#% 69791
#% 84047
#% 307424
#% 322884
#% 961010
#% 985941
#% 1002142
#% 1012267
#% 1053490
#% 1121304
#% 1172471
#% 1193080
#% 1373690
#% 1580638
#% 1601099
#% 1639572
#! This paper presents new alternatives to the well-known Bloom filter data structure. The Bloom filter, a compact data structure supporting set insertion and membership queries, has found wide application in databases, storage systems, and networks. Because the Bloom filter performs frequent random reads and writes, it is used almost exclusively in RAM, limiting the size of the sets it can represent. This paper first describes the quotient filter, which supports the basic operations of the Bloom filter, achieving roughly comparable performance in terms of space and time, but with better data locality. Operations on the quotient filter require only a small number of contiguous accesses. The quotient filter has other advantages over the Bloom filter: it supports deletions, it can be dynamically resized, and two quotient filters can be efficiently merged. The paper then gives two data structures, the buffered quotient filter and the cascade filter, which exploit the quotient filter advantages and thus serve as SSD-optimized alternatives to the Bloom filter. The cascade filter has better asymptotic I/O performance than the buffered quotient filter, but the buffered quotient filter outperforms the cascade filter on small to medium data sets. Both data structures significantly outperform recently-proposed SSD-optimized Bloom filter variants, such as the elevator Bloom filter, buffered Bloom filter, and forest-structured Bloom filter. In experiments, the cascade filter and buffered quotient filter performed insertions 8.6--11 times faster than the fastest Bloom filter variant and performed lookups 0.94--2.56 times faster.

#index 1880476
#* Learning expressive linkage rules using genetic programming
#@ Robert Isele;Christian Bizer
#t 2012
#c 4
#% 124073
#% 197394
#% 350103
#% 465702
#% 577238
#% 577247
#% 659991
#% 723484
#% 729913
#% 874457
#% 913783
#% 924747
#% 1022823
#% 1052859
#% 1314445
#% 1426567
#% 1692269
#% 1892363
#! A central problem in data integration and data cleansing is to find entities in different data sources that describe the same real-world object. Many existing methods for identifying such entities rely on explicit linkage rules which specify the conditions that entities must fulfill in order to be considered to describe the same real-world object. In this paper, we present the GenLink algorithm for learning expressive linkage rules from a set of existing reference links using genetic programming. The algorithm is capable of generating linkage rules which select discriminative properties for comparison, apply chains of data transformations to normalize property values, choose appropriate distance measures and thresholds and combine the results of multiple comparisons using non-linear aggregation functions. Our experiments show that the GenLink algorithm outperforms the state-of-the-art genetic programming approach to learning linkage rules recently presented by Carvalho et. al. and is capable of learning linkage rules which achieve a similar accuracy as human written rules for the same problem.

#index 1880477
#* Mining frequent itemsets over uncertain databases
#@ Yongxin Tong;Lei Chen;Yurong Cheng;Philip S. Yu
#t 2012
#c 4
#% 152934
#% 300120
#% 443350
#% 466490
#% 481290
#% 772835
#% 810049
#% 818434
#% 1016195
#% 1063531
#% 1166600
#% 1179162
#% 1189215
#% 1214624
#% 1214633
#% 1290947
#% 1393138
#% 1411036
#% 1411089
#% 1451166
#% 1464052
#% 1482221
#% 1535367
#% 1556865
#% 1594654
#% 1737806
#% 1846710
#! In recent years, due to the wide applications of uncertain data, mining frequent itemsets over uncertain databases has attracted much attention. In uncertain databases, the support of an itemset is a random variable instead of a fixed occurrence counting of this itemset. Thus, unlike the corresponding problem in deterministic databases where the frequent itemset has a unique definition, the frequent itemset under uncertain environments has two different definitions so far. The first definition, referred as the expected support-based frequent itemset, employs the expectation of the support of an itemset to measure whether this itemset is frequent. The second definition, referred as the probabilistic frequent itemset, uses the probability of the support of an itemset to measure its frequency. Thus, existing work on mining frequent itemsets over uncertain databases is divided into two different groups and no study is conducted to comprehensively compare the two different definitions. In addition, since no uniform experimental platform exists, current solutions for the same definition even generate inconsistent results. In this paper, we firstly aim to clarify the relationship between the two different definitions. Through extensive experiments, we verify that the two definitions have a tight connection and can be unified together when the size of data is large enough. Secondly, we provide baseline implementations of eight existing representative algorithms and test their performances with uniform measures fairly. Finally, according to the fair tests over many different benchmark data sets, we clarify several existing inconsistent conclusions and discuss some new findings.

#index 1880478
#* Uncertain time-series similarity: return to the basics
#@ Michele Dallachiesa;Besmira Nushi;Katsiaryna Mirylenka;Themis Palpanas
#t 2012
#c 4
#% 172949
#% 201876
#% 397381
#% 460862
#% 464994
#% 631923
#% 837836
#% 992857
#% 1022240
#% 1083693
#% 1127609
#% 1179162
#% 1181271
#% 1206714
#% 1218743
#% 1292552
#% 1381029
#% 1426515
#% 1445746
#% 1451177
#% 1451534
#% 1482191
#% 1615902
#! In the last years there has been a considerable increase in the availability of continuous sensor measurements in a wide range of application domains, such as Location-Based Services (LBS), medical monitoring systems, manufacturing plants and engineering facilities to ensure efficiency, product quality and safety, hydrologic and geologic observing systems, pollution management, and others. Due to the inherent imprecision of sensor observations, many investigations have recently turned into querying, mining and storing uncertain data. Uncertainty can also be due to data aggregation, privacy-preserving transforms, and error-prone mining algorithms. In this study, we survey the techniques that have been proposed specifically for modeling and processing uncertain time series, an important model for temporal data. We provide an analytical evaluation of the alternatives that have been proposed in the literature, highlighting the advantages and disadvantages of each approach, and further compare these alternatives with two additional techniques that were carefully studied before. We conduct an extensive experimental evaluation with 17 real datasets, and discuss some surprising results, which suggest that a fruitful research direction is to take into account the temporal correlations in the time series. Based on our evaluations, we also provide guidelines useful for the practitioners in the field.

#index 1880479
#* Statistical distortion: consequences of data cleaning
#@ Tamraparni Dasu;Ji Meng Loh
#t 2012
#c 4
#% 344898
#% 989512
#% 1014727
#% 1054480
#% 1202161
#% 1426508
#% 1426628
#% 1594567
#% 1605984
#! We introduce the notion of statistical distortion as an essential metric for measuring the effectiveness of data cleaning strategies. We use this metric to propose a widely applicable yet scalable experimental framework for evaluating data cleaning strategies along three dimensions: glitch improvement, statistical distortion and cost-related criteria. Existing metrics focus on glitch improvement and cost, but not on the statistical impact of data cleaning strategies. We illustrate our framework on real world data, with a comprehensive suite of experiments and analyses.

#index 1880480
#* Towards energy-efficient database cluster design
#@ Willis Lang;Stavros Harizopoulos;Jignesh M. Patel;Mehul A. Shah;Dimitris Tsirogiannis
#t 2012
#c 4
#% 115661
#% 776773
#% 814356
#% 893129
#% 1034471
#% 1039029
#% 1070440
#% 1217159
#% 1247895
#% 1278373
#% 1290542
#% 1328185
#% 1328186
#% 1373699
#% 1373701
#% 1414208
#% 1426497
#% 1426521
#% 1433985
#% 1468269
#% 1468291
#% 1480466
#% 1523806
#% 1586203
#% 1730839
#! Energy is a growing component of the operational cost for many "big data" deployments, and hence has become increasingly important for practitioners of large-scale data analysis who require scale-out clusters or parallel DBMS appliances. Although a number of recent studies have investigated the energy efficiency of DBMSs, none of these studies have looked at the architectural design space of energy-efficient parallel DBMS clusters. There are many challenges to increasing the energy efficiency of a DBMS cluster, including dealing with the inherent scaling inefficiency of parallel data processing, and choosing the appropriate energy-efficient hardware. In this paper, we experimentally examine and analyze a number of key parameters related to these challenges for designing energy-efficient database clusters. We explore the cluster design space using empirical results and propose a model that considers the key bottlenecks to energy efficiency in a parallel DBMS. This paper represents a key first step in designing energy-efficient database clusters, which is increasingly important given the trend toward parallel database appliances.

#index 1895048
#* Data management on the spatial web
#@ Christian S. Jensen
#t 2012
#c 4
#! Due in part to the increasing mobile use of the web and the proliferation of geo-positioning, the web is fast acquiring a significant spatial aspect. Content and users are being augmented with locations that are used increasingly by location-based services. Studies suggest that each week, several billion web queries are issued that have local intent and target spatial web objects. These are points of interest with a web presence, and they thus have locations as well as textual descriptions. This development has given prominence to spatial web data management, an area ripe with new and exciting opportunities and challenges. The research community has embarked on inventing and supporting new query functionality for the spatial web. Different kinds of spatial web queries return objects that are near a location argument and are relevant to a text argument. To support such queries, it is important to be able to rank objects according to their relevance to a query. And it is important to be able to process the queries with low latency. The talk offers an overview of key aspects of the spatial web. Based on recent results obtained by the speaker and his colleagues, the talk explores new query functionality enabled by the setting. Further, the talk offers insight into the data management techniques capable of supporting such functionality.

#index 1895049
#* Data analytics opportunities in a smarter planet
#@ Brenda Dietrich
#t 2012
#c 4
#! New applications of computing are being enabled by instrumentation of physical entities, aggregation of data, and the analysis of the data. The resulting integration of information and control permits efficient and effective management of complex man-made systems. Examples include transportation systems, buildings, electrical grids, health care systems, governments, and supply chains. Achieving this vision requires extensive data integration and analysis, over diverse, rapidly changing, and often uncertain data. There are many challenges, requiring both new data management techniques as well as new mathematics, forcing new collaborations as the basis of the new "Data Science". Needs and opportunities will be discussed in the context of specific pilots and projects.

#index 1895050
#* Challenges in economic massive content storage and management (MCSAM) in the era of self-organizing, self-expanding and self-linking data clusters
#@ Kenan Şahin
#t 2012
#c 4
#! Rapid spread of social networks, global on-line shopping, post 9/11 security oriented linking of data bases and foremost the global adoption of smart phones/devices, among other phenomena, are transforming data clusters into dynamic and almost uncontrollable entities that have their own local intelligence, clients and objectives. The scale and rapidity of change is such that large scale innovations in content storage and management are urgently needed if the diseconomies of scale and complexity are to be mitigated. The field needs to reinvent itself. Istanbul, a city that has reinvented itself many times is an excellent venue to engage in such a discussion and for me to offer suggestions and proposals that derive from personal experiences that span academia, start ups, R&D firms and Bell Labs as well my early years spent in Istanbul.

#index 1895051
#* Approximate frequency counts over data streams
#@ Gurmeet Singh Manku;Rajeev Motwani
#t 2012
#c 4
#% 993960
#! Research in data stream algorithms has blossomed since late 90s. The talk will trace the history of the Approximate Frequency Counts paper, how it was conceptualized and how it influenced data stream research. The talk will also touch upon a recent development: analysis of personal data streams for improving our quality of lives.

#index 1895052
#* The MADlib analytics library: or MAD skills, the SQL
#@ Joseph M. Hellerstein;Christoper Ré;Florian Schoppmann;Daisy Zhe Wang;Eugene Fratkin;Aleksander Gorajek;Kee Siong Ng;Caleb Welton;Xixuan Feng;Kun Li;Arun Kumar
#t 2012
#c 4
#% 116376
#% 300213
#% 333679
#% 464434
#% 757953
#% 818500
#% 845220
#% 935763
#% 963195
#% 991230
#% 1173394
#% 1183591
#% 1217159
#% 1251443
#% 1328066
#% 1379583
#% 1426513
#% 1512993
#% 1523889
#% 1581889
#% 1594623
#% 1594630
#% 1615854
#% 1770346
#% 1813854
#! MADlib is a free, open-source library of in-database analytic methods. It provides an evolving suite of SQL-based algorithms for machine learning, data mining and statistics that run at scale within a database engine, with no need for data import/export to other tools. The goal is for MADlib to eventually serve a role for scalable database systems that is similar to the CRAN library for R: a community repository of statistical methods, this time written with scale and parallelism in mind. In this paper we introduce the MADlib project, including the background that led to its beginnings, and the motivation for its open-source nature. We provide an overview of the library's architecture and design patterns, and provide a description of various statistical methods in that context. We include performance and speedup results of a core design pattern from one of those methods over the Greenplum parallel DBMS on a modest-sized test cluster. We then report on two initial efforts at incorporating academic research into MADlib, which is one of the project's goals. MADlib is freely available at http://madlib.net, and the project is open for contributions of both new methods, and ports to additional database platforms.

#index 1895053
#* Can the elephants handle the NoSQL onslaught?
#@ Avrilia Floratou;Nikhil Teletia;David J. DeWitt;Jignesh M. Patel;Donghui Zhang
#t 2012
#c 4
#% 1217159
#% 1426489
#% 1581407
#% 1592315
#% 1594639
#% 1798376
#! In this new era of "big data", traditional DBMSs are under attack from two sides. At one end of the spectrum, the use of document store NoSQL systems (e.g. MongoDB) threatens to move modern Web 2.0 applications away from traditional RDBMSs. At the other end of the spectrum, big data DSS analytics that used to be the domain of parallel RDBMSs is now under attack by another class of NoSQL data analytics systems, such as Hive on Hadoop. So, are the traditional RDBMSs, aka "big elephants", doomed as they are challenged from both ends of this "big data" spectrum? In this paper, we compare one representative NoSQL system from each end of this spectrum with SQL Server, and analyze the performance and scalability aspects of each of these approaches (NoSQL vs. SQL) on two workloads (decision support analysis and interactive data-serving) that represent the two ends of the application spectrum. We present insights from this evaluation and speculate on potential trends for the future.

#index 1895054
#* Solving big data challenges for enterprise application performance management
#@ Tilmann Rabl;Sergio Gómez-Villamor;Mohammad Sadoghi;Victor Muntés-Mulero;Hans-Arno Jacobsen;Serge Mankovskii
#t 2012
#c 4
#% 723279
#% 998845
#% 1002142
#% 1127560
#% 1127596
#% 1400975
#% 1426489
#% 1468530
#% 1573340
#% 1621133
#% 1637911
#% 1642260
#! As the complexity of enterprise systems increases, the need for monitoring and analyzing such systems also grows. A number of companies have built sophisticated monitoring tools that go far beyond simple resource utilization reports. For example, based on instrumentation and specialized APIs, it is now possible to monitor single method invocations and trace individual transactions across geographically distributed systems. This high-level of detail enables more precise forms of analysis and prediction but comes at the price of high data rates (i.e., big data). To maximize the benefit of data monitoring, the data has to be stored for an extended period of time for ulterior analysis. This new wave of big data analytics imposes new challenges especially for the application performance monitoring systems. The monitoring data has to be stored in a system that can sustain the high data rates and at the same time enable an up-to-date view of the underlying infrastructure. With the advent of modern key-value stores, a variety of data storage systems have emerged that are built with a focus on scalability and high data rates as predominant in this monitoring use case. In this work, we present our experience and a comprehensive performance evaluation of six modern (open-source) data stores in the context of application performance monitoring as part of CA Technologies initiative. We evaluated these systems with data and workloads that can be found in application performance monitoring, as well as, on-line advertisement, power monitoring, and many other use cases. We present our insights not only as performance results but also as lessons learned and our experience relating to the setup and configuration complexity of these data stores in an industry setting.

#index 1895055
#* M3R: increased performance for in-memory Hadoop jobs
#@ Avraham Shinnar;David Cunningham;Vijay Saraswat;Benjamin Herta
#t 2012
#c 4
#% 723279
#% 834429
#% 1023420
#% 1063553
#% 1328095
#% 1354118
#% 1464950
#% 1468411
#% 1523820
#% 1581937
#% 1594623
#! Main Memory Map Reduce (M3R) is a new implementation of the Hadoop Map Reduce (HMR) API targeted at online analytics on high mean-time-to-failure clusters. It does not support resilience, and supports only those workloads which can fit into cluster memory. In return, it can run HMR jobs unchanged -- including jobs produced by compilers for higher-level languages such as Pig, Jaql, and SystemML and interactive front-ends like IBM BigSheets -- while providing significantly better performance than the Hadoop engine on several workloads (e.g. 45x on some input sizes for sparse matrix vector multiply). M3R also supports extensions to the HMR API which can enable Map Reduce jobs to run faster on the M3R engine, while not affecting their performance under the Hadoop engine.

#index 1895056
#* A storage advisor for hybrid-store databases
#@ Philipp Rösch;Lars Dannecker;Franz Färber;Gregor Hackenbroich
#t 2012
#c 4
#% 464706
#% 479821
#% 480158
#% 480821
#% 482100
#% 765431
#% 810111
#% 993967
#% 1016220
#% 1206624
#% 1217145
#% 1490163
#% 1523974
#% 1594639
#% 1667313
#! With the SAP HANA database, SAP offers a high-performance in-memory hybrid-store database. Hybrid-store databases---that is, databases supporting row- and column-oriented data management---are getting more and more prominent. While the columnar management offers high-performance capabilities for analyzing large quantities of data, the row-oriented store can handle transactional point queries as well as inserts and updates more efficiently. To effectively take advantage of both stores at the same time the novel question whether to store the given data row- or column-oriented arises. We tackle this problem with a storage advisor tool that supports database administrators at this decision. Our proposed storage advisor recommends the optimal store based on data and query characteristics; its core is a cost model to estimate and compare query execution times for the different stores. Besides a per-table decision, our tool also considers to horizontally and vertically partition the data and manage the partitions on different stores. We evaluated the storage advisor for the use in the SAP HANA database; we show the recommendation quality as well as the benefit of having the data in the optimal store with respect to increased query performance.

#index 1895057
#* From cooperative scans to predictive buffer management
#@ Michał Świtakowski;Peter Boncz;Marcin Zukowski
#t 2012
#c 4
#% 86929
#% 172968
#% 201869
#% 480607
#% 481092
#% 602601
#% 810039
#% 993385
#% 1022262
#% 1022312
#% 1306948
#% 1328168
#% 1426547
#% 1426629
#% 1647981
#! In analytical applications, database systems often need to sustain workloads with multiple concurrent scans hitting the same table. The Cooperative Scans (CScans) framework, which introduces an Active Buffer Manager (ABM) component into the database architecture, has been the most effective and elaborate response to this problem, and was initially developed in the X100 research prototype. We now report on the the experiences of integrating Cooperative Scans into its industrial-strength successor, the Vectorwise database product. During this implementation we invented a simpler optimization of concurrent scan buffer management, called Predictive Buffer Management (PBM). PBM is based on the observation that in a workload with long-running scans, the buffer manager has quite a bit of information on the workload in the immediate future, such that an approximation of the ideal OPT algorithm becomes feasible. In the evaluation on both synthetic benchmarks as well as a TPC-H throughput run we compare the benefits of naive buffer management (LRU) versus CScans, PBM and OPT; showing that PBM achieves benefits close to Cooperative Scans, while incurring much lower architectural impact.

#index 1895058
#* The unified logging infrastructure for data analytics at Twitter
#@ George Lee;Jimmy Lin;Chuang Liu;Andrew Lorek;Dmitriy Ryaboy
#t 2012
#c 4
#% 78171
#% 279755
#% 342651
#% 463903
#% 740900
#% 815879
#% 935763
#% 959277
#% 963669
#% 989668
#% 1063553
#% 1213622
#% 1328060
#% 1328186
#% 1426588
#% 1468530
#% 1523841
#% 1573822
#% 1581926
#% 1581937
#% 1581946
#% 1586685
#% 1594588
#% 1594639
#% 1621145
#% 1770415
#! In recent years, there has been a substantial amount of work on large-scale data analytics using Hadoop-based platforms running on large clusters of commodity machines. A less-explored topic is how those data, dominated by application logs, are collected and structured to begin with. In this paper, we present Twitter's production logging infrastructure and its evolution from application-specific logging to a unified "client events" log format, where messages are captured in common, well-formatted, flexible Thrift messages. Since most analytics tasks consider the user session as the basic unit of analysis, we pre-materialize "session sequences", which are compact summaries that can answer a large class of common queries quickly. The development of this infrastructure has streamlined log collection and data analysis, thereby improving our ability to rapidly experiment and iterate on various aspects of the service.

#index 1895059
#* Transaction log based application error recovery and point in-time query
#@ Tomas Talius;Robin Dhamankar;Andrei Dumitrache;Hanuma Kodavalla
#t 2012
#c 4
#% 114582
#% 571296
#% 800545
#% 810114
#% 978445
#% 1063517
#% 1594649
#% 1625041
#! Database backups have traditionally been used as the primary mechanism to recover from hardware and user errors. High availability solutions maintain redundant copies of data that can be used to recover from most failures except user or application errors. Database backups are neither space nor time efficient for recovering from user errors which typically occur in the recent past and affect a small portion of the database. Moreover periodic full backups impact user workload and increase storage costs. In this paper we present a scheme that can be used for both user and application error recovery starting from the current state and rewinding the database back in time using the transaction log. While we provide a consistent view of the entire database as of a point in time in the past, the actual prior versions are produced only for data that is accessed. We make the as of data accessible to arbitrary point in time queries by integrating with the database snapshot feature in Microsoft SQL Server.

#index 1895060
#* The vertica analytic database: C-store 7 years later
#@ Andrew Lamb;Matt Fuller;Ramakrishna Varadarajan;Nga Tran;Ben Vandiver;Lyric Doshi;Chuck Bear
#t 2012
#c 4
#% 287005
#% 393641
#% 403195
#% 479630
#% 480623
#% 481749
#% 481933
#% 800491
#% 824697
#% 963669
#% 998845
#% 1127565
#% 1328095
#% 1400975
#% 1667313
#! This paper describes the system architecture of the Vertica Analytic Database (Vertica), a commercialization of the design of the C-Store research prototype. Vertica demonstrates a modern commercial RDBMS system that presents a classical relational interface while at the same time achieving the high performance expected from modern "web scale" analytic systems by making appropriate architectural choices. Vertica is also an instructive lesson in how academic systems research can be directly commercialized into a successful product.

#index 1895061
#* Interactive analytical processing in big data systems: a cross-industry study of MapReduce workloads
#@ Yanpei Chen;Sara Alspaugh;Randy Katz
#t 2012
#c 4
#% 160390
#% 172913
#% 196088
#% 236757
#% 290464
#% 787639
#% 820340
#% 963612
#% 963669
#% 1022294
#% 1053487
#% 1084463
#% 1132715
#% 1217159
#% 1246354
#% 1278391
#% 1328060
#% 1328095
#% 1400550
#% 1426544
#% 1464623
#% 1523806
#% 1523820
#% 1523824
#% 1523841
#% 1526991
#% 1563014
#% 1563022
#% 1567923
#% 1573238
#% 1581937
#% 1589964
#% 1604232
#% 1625034
#% 1639648
#% 1730839
#% 1783392
#! Within the past few years, organizations in diverse industries have adopted MapReduce-based systems for large-scale data processing. Along with these new users, important new workloads have emerged which feature many small, short, and increasingly interactive jobs in addition to the large, long-running batch jobs for which MapReduce was originally designed. As interactive, large-scale query processing is a strength of the RDBMS community, it is important that lessons from that field be carried over and applied where possible in this new domain. However, these new workloads have not yet been described in the literature. We fill this gap with an empirical analysis of MapReduce traces from six separate business-critical deployments inside Facebook and at Cloudera customers in e-commerce, telecommunications, media, and retail. Our key contribution is a characterization of new MapReduce workloads which are driven in part by interactive analysis, and which make heavy use of query-like programming frameworks on top of MapReduce. These workloads display diverse behaviors which invalidate prior assumptions about MapReduce such as uniform data access, regular diurnal patterns, and prevalence of large jobs. A secondary contribution is a first step towards creating a TPC-like data processing benchmark for MapReduce.

#index 1895062
#* Muppet: MapReduce-style processing of fast data
#@ Wang Lam;Lu Liu;Sts Prasad;Anand Rajaraman;Zoheb Vacheri;AnHai Doan
#t 2012
#c 4
#% 378388
#% 654510
#% 963669
#% 1053947
#% 1063555
#% 1468411
#% 1535212
#% 1581928
#% 1581938
#% 1621131
#% 1874962
#! MapReduce has emerged as a popular method to process big data. In the past few years, however, not just big data, but fast data has also exploded in volume and availability. Examples of such data include sensor data streams, the Twitter Firehose, and Facebook updates. Numerous applications must process fast data. Can we provide a MapReduce-style framework so that developers can quickly write such applications and execute them over a cluster of machines, to achieve low latency and high scalability? In this paper we report on our investigation of this question, as carried out at Kosmix and WalmartLabs. We describe MapUpdate, a framework like MapReduce, but specifically developed for fast data. We describe Muppet, our implementation of MapUpdate. Throughout the description we highlight the key challenges, argue why MapReduce is not well suited to address them, and briefly describe our current solutions. Finally, we describe our experience and lessons learned with Muppet, which has been used extensively at Kosmix and WalmartLabs to power a broad range of applications in social media and e-commerce.

#index 1895063
#* Building user-defined runtime adaptation routines for stream processing applications
#@ Gabriela Jacques-Silva;Buğra Gedik;Rohit Wagle;Kun-Lung Wu;Vibhore Kumar
#t 2012
#c 4
#% 425372
#% 633902
#% 875062
#% 878299
#% 995806
#% 1016220
#% 1022215
#% 1022302
#% 1127569
#% 1180877
#% 1183368
#% 1354118
#% 1383817
#% 1393287
#% 1431827
#% 1591788
#% 1693812
#! Stream processing applications are deployed as continuous queries that run from the time of their submission until their cancellation. This deployment mode limits developers who need their applications to perform runtime adaptation, such as algorithmic adjustments, incremental job deployment, and application-specific failure recovery. Currently, developers do runtime adaptation by using external scripts and/or by inserting operators into the stream processing graph that are unrelated to the data processing logic. In this paper, we describe a component called orchestrator that allows users to write routines for automatically adapting the application to runtime conditions. Developers build an orchestrator by registering and handling events as well as specifying actuations. Events can be generated due to changes in the system state (e.g., application component failures), built-in system metrics (e.g., throughput of a connection), or custom application metrics (e.g., quality score). Once the orchestrator receives an event, users can take adaptation actions by using the orchestrator actuation APIs. We demonstrate the use of the orchestrator in IBM's System S in the context of three different applications, illustrating application adaptation to changes on the incoming data distribution, to application failures, and on-demand dynamic composition.

#index 1895064
#* MOIST: a scalable and parallel moving object indexer with school tracking
#@ Junchen Jiang;Hongji Bao;Edward Y. Chang;Yuqian Li
#t 2012
#c 4
#% 300174
#% 442615
#% 764226
#% 765402
#% 769946
#% 800186
#% 810048
#% 814349
#% 871045
#% 1015320
#% 1016193
#% 1022251
#% 1046510
#% 1054227
#% 1063471
#% 1112737
#% 1127612
#% 1206768
#% 1328208
#% 1328209
#% 1523861
#% 1523927
#% 1610248
#! Location-Based Service (LBS) is rapidly becoming the next ubiquitous technology for a wide range of mobile applications. To support applications that demand nearest-neighbor and history queries, an LBS spatial indexer must be able to efficiently update, query, archive and mine location records, which can be in contention with each other. In this work, we propose MOIST, whose baseline is a recursive spatial partitioning indexer built upon BigTable. To reduce update and query contention, MOIST groups nearby objects of similar trajectory into the same school, and keeps track of only the history of school leaders. This dynamic clustering scheme can eliminate redundant updates and hence reduce update latency. To improve history query processing, MOIST keeps some history data in memory, while it flushes aged data onto parallel disks in a locality-preserving way. Through experimental studies, we show that MOIST can support highly efficient nearest-neighbor and history queries and can scale well with an increasing number of users and update frequency.

#index 1895065
#* Serializable snapshot isolation in PostgreSQL
#@ Dan R. K. Ports;Kevin Grittner
#t 2012
#c 4
#% 13043
#% 201869
#% 286836
#% 320902
#% 374538
#% 480096
#% 480589
#% 481599
#% 632091
#% 709864
#% 783784
#% 814649
#% 1022309
#% 1063524
#% 1526992
#% 1594664
#! This paper describes our experience implementing PostgreSQL's new serializable isolation level. It is based on the recently-developed Serializable Snapshot Isolation (SSI) technique. This is the first implementation of SSI in a production database release as well as the first in a database that did not previously have a lock-based serializable isolation level. We reflect on our experience and describe how we overcame some of the resulting challenges, including the implementation of a new lock manager, a technique for ensuring memory usage is bounded, and integration with other PostgreSQL features. We also introduce an extension to SSI that improves performance for read-only transactions. We evaluate PostgreSQL's serializable isolation level using several benchmarks and show that it achieves performance only slightly below that of snapshot isolation, and significantly outperforms the traditional two-phase locking approach on read-intensive workloads.

#index 1895066
#* Exploiting evidence from unstructured data to enhance master data management
#@ Karin Murthy;Prasad M. Deshpande;Atreyee Dey;Ramanujam Halasipuram;Mukesh Mohania;P. Deepak;Jennifer Reed;Scott Schumacher
#t 2012
#c 4
#% 769884
#% 864416
#% 875032
#% 893143
#% 913783
#% 939392
#% 1166537
#% 1201863
#% 1217204
#% 1261595
#% 1471192
#% 1482323
#% 1484336
#% 1581890
#% 1618131
#! Master data management (MDM) integrates data from multiple structured data sources and builds a consolidated 360-degree view of business entities such as customers and products. Today's MDM systems are not prepared to integrate information from unstructured data sources, such as news reports, emails, call-center transcripts, and chat logs. However, those unstructured data sources may contain valuable information about the same entities known to MDM from the structured data sources. Integrating information from unstructured data into MDM is challenging as textual references to existing MDM entities are often incomplete and imprecise and the additional entity information extracted from text should not impact the trustworthiness of MDM data. In this paper, we present an architecture for making MDM text-aware and showcase its implementation as IBM Info-Sphere MDM Extension for Unstructured Text Correlation, an add-on to IBM InfoSphere Master Data Management Standard Edition. We highlight how MDM benefits from additional evidence found in documents when doing entity resolution and relationship discovery. We experimentally demonstrate the feasibility of integrating information from unstructured data sources into MDM.

#index 1895067
#* Avatara: OLAP for web-scale analytics products
#@ Lili Wu;Roshan Sumbaly;Chris Riccomini;Gordon Koo;Hyung Jin Kim;Jay Kreps;Sam Shah
#t 2012
#c 4
#% 283833
#% 393641
#% 438504
#% 453194
#% 963669
#% 998845
#% 1107620
#% 1127560
#% 1354118
#% 1594588
#% 1765838
#! Multidimensional data generated by members on websites has seen massive growth in recent years. OLAP is a well-suited solution for mining and analyzing this data. Providing insights derived from this analysis has become crucial for these websites to give members greater value. For example, LinkedIn, the largest professional social network, provides its professional members rich analytics features like "Who's Viewed My Profile?" and "Who's Viewed This Job?" The data behind these features form cubes that must be efficiently served at scale, and can be neatly sharded to do so. To serve our growing 160 million member base, we built a scalable and fast OLAP serving system called Avatara to solve this many, small cubes problem. At LinkedIn, Avatara has been powering several analytics features on the site for the past two years.

#index 1895068
#* Dedoop: efficient deduplication with Hadoop
#@ Lars Kolb;Andreas Thor;Erhard Rahm
#t 2012
#c 4
#% 1314445
#% 1523838
#% 1694380
#% 1846778
#! We demonstrate a powerful and easy-to-use tool called Dedoop (Deduplication with Hadoop) for MapReduce-based entity resolution (ER) of large datasets. Dedoop supports a browser-based specification of complex ER workflows including blocking and matching steps as well as the optional use of machine learning for the automatic generation of match classifiers. Specified workflows are automatically translated into MapReduce jobs for parallel execution on different Hadoop clusters. To achieve high performance Dedoop supports several advanced load balancing strategies.

#index 1895069
#* MapReduce-based dimensional ETL made easy
#@ Xiufeng Liu;Christian Thomsen;Torben Bach Pedersen
#t 2012
#c 4
#% 963669
#% 1278123
#% 1296946
#% 1616905
#! This paper demonstrates ETLMR, a novel dimensional Extract--Transform--Load (ETL) programming framework that uses Map-Reduce to achieve scalability. ETLMR has built-in native support of data warehouse (DW) specific constructs such as star schemas, snowflake schemas, and slowly changing dimensions (SCDs). This makes it possible to build MapReduce-based dimensional ETL flows very easily. The ETL process can be configured with only few lines of code. We will demonstrate the concrete steps in using ETLMR to load data into a (partly snowflaked) DW schema. This includes configuration of data sources and targets, dimension processing schemes, fact processing, and deployment. In addition, we also present the scalability on large data sets.

#index 1895070
#* CloudVista: interactive and economical visual cluster analysis for big data in the cloud
#@ Huiqi Xu;Zhen Li;Shumin Guo;Keke Chen
#t 2012
#c 4
#% 210173
#% 823086
#% 963669
#% 1615874
#% 1635071
#! Analysis of big data has become an important problem for many business and scientific applications, among which clustering and visualizing clusters in big data raise some unique challenges. This demonstration presents the CloudVista prototype system to address the problems with big data caused by using existing data reduction approaches. It promotes a whole-big-data visualization approach that preserves the details of clustering structure. The prototype system has several merits. (1) Its visualization model is naturally parallel, which guarantees the scalability. (2) The visual frame structure minimizes the data transferred between the cloud and the client. (3) The RandGen algorithm is used to achieve a good balance between interactivity and batch processing. (4) This approach is also designed to minimize the financial cost of interactive exploration in the cloud. The demonstration will highlight the problems with existing approaches and show the advantages of the CloudVista approach. The viewers will have the chance to play with the CloudVista prototype system and compare the visualization results generated with different approaches.

#index 1895071
#* Myriad: scalable and expressive data generation
#@ Alexander Alexandrov;Kostas Tzoumas;Volker Markl
#t 2012
#c 4
#% 893212
#% 985979
#% 1545218
#% 1673564
#% 1903831
#! The current research focus on Big Data systems calls for a rethinking of data generation methods. The traditional sequential data generation approach is not well suited to large-scale systems as generating a terabyte of data may require days or even weeks depending on the number of constraints imposed on the generated model. We demonstrate Myriad, a new data generation toolkit that enables the specification of semantically rich data generator programs that can scale out linearly in a shared-nothing environment. Data generation programs built on top of Myriad implement an efficient parallel execution strategy leveraged by the extensive use of pseudo-random number generators with random access support.

#index 1895072
#* A demonstration of DBWipes: clean as you query
#@ Eugene Wu;Samuel Madden;Michael Stonebraker
#t 2012
#c 4
#% 480499
#% 763701
#% 879803
#% 1573738
#% 1581916
#! As data analytics becomes mainstream, and the complexity of the underlying data and computation grows, it will be increasingly important to provide tools that help analysts understand the underlying reasons when they encounter errors in the result. While data provenance has been a large step in providing tools to help debug complex workflows, its current form has limited utility when debugging aggregation operators that compute a single output from a large collection of inputs. Traditional provenance will return the entire input collection, which has very low precision. In contrast, users are seeking precise descriptions of the inputs that caused the errors. We propose a Ranked Provenance System, which identifies subsets of inputs that influenced the output error, describes each subset with human readable predicates and orders them by contribution to the error. In this demonstration, we will present DBWipes, a novel data cleaning system that allows users to execute aggregate queries, and interactively detect, understand, and clean errors in the query results. Conference attendees will explore anomalies in campaign donations from the current US presidential election and in readings from a 54-node sensor deployment.

#index 1895073
#* ASTERIX: an open source system for "Big Data" management and analysis (demo)
#@ Sattam Alsubaiee;Yasser Altowim;Hotham Altwaijry;Alexander Behm;Vinayak Borkar;Yingyi Bu;Michael Carey;Raman Grover;Zachary Heilbron;Young-Seok Kim;Chen Li;Nicola Onose;Pouria Pirzadeh;Rares Vernica;Jian Wen
#t 2012
#c 4
#% 393844
#% 1063553
#% 1426543
#% 1566972
#% 1594630
#! At UC Irvine, we are building a next generation parallel database system, called ASTERIX, as our approach to addressing today's "Big Data" management challenges. ASTERIX aims to combine time-tested principles from parallel database systems with those of the Web-scale computing community, such as fault tolerance for long running jobs. In this demo, we present a whirlwind tour of ASTERIX, highlighting a few of its key features. We will demonstrate examples of our data definition language to model semi-structured data, and examples of interesting queries using our declarative query language. In particular, we will show the capabilities of ASTERIX for answering geo-spatial queries and fuzzy queries, as well as ASTERIX' data feed construct for continuously ingesting data.

#index 1895074
#* Blink and it's done: interactive queries on very large data
#@ Sameer Agarwal;Anand P. Iyer;Aurojit Panda;Samuel Madden;Barzan Mozafari;Ion Stoica
#t 2012
#c 4
#% 480810
#% 1770395
#% 1770416
#% 1783374
#% 1783393
#! In this demonstration, we present BlinkDB, a massively parallel, sampling-based approximate query processing framework for running interactive queries on large volumes of data. The key observation in BlinkDB is that one can make reasonable decisions in the absence of perfect answers. BlinkDB extends the Hive/HDFS stack and can handle the same set of SPJA (selection, projection, join and aggregate) queries as supported by these systems. BlinkDB provides real-time answers along with statistical error guarantees, and can scale to petabytes of data and thousands of machines in a fault-tolerant manner. Our experiments using the TPC-H benchmark and on an anonymized real-world video content distribution workload from Conviva Inc. show that BlinkDB can execute a wide range of queries up to 150x faster than Hive on MapReduce and 10--150x faster than Shark (Hive on Spark) over tens of terabytes of data stored across 100 machines, all with an error of 2--10%.

#index 1895075
#* Massive genomic data processing and deep analysis
#@ Abhishek Roy;Yanlei Diao;Evan Mauceli;Yiping Shen;Bai-Lin Wu
#t 2012
#c 4
#% 338580
#% 1174585
#% 1231157
#% 1393138
#% 1581883
#% 1586686
#! Today large sequencing centers are producing genomic data at the rate of 10 terabytes a day and require complicated processing to transform massive amounts of noisy raw data into biological information. To address these needs, we develop a system for end-to-end processing of genomic data, including alignment of short read sequences, variation discovery, and deep analysis. We also employ a range of quality control mechanisms to improve data quality and parallel processing techniques for performance. In the demo, we will use real genomic data to show details of data transformation through the workflow, the usefulness of end results (ready for use as testable hypotheses), the effects of our quality control mechanisms and improved algorithms, and finally performance improvement.

#index 1895076
#* MonetDB/DataCell: online analytics in a streaming column-store
#@ Erietta Liarou;Stratos Idreos;Stefan Manegold;Martin Kersten
#t 2012
#c 4
#% 13016
#% 201929
#% 300179
#% 317871
#% 480768
#% 654497
#% 654510
#% 788215
#% 788216
#% 1026964
#% 1181240
#% 1217169
#% 1328078
#% 1426598
#% 1490168
#! In DataCell, we design streaming functionalities in a modern relational database kernel which targets big data analytics. This includes exploitation of both its storage/execution engine and its optimizer infrastructure. We investigate the opportunities and challenges that arise with such a direction and we show that it carries significant advantages for modern applications in need for online analytics such as web logs, network monitoring and scientific data management. The major challenge then becomes the efficient support for specialized stream features, e.g., multi-query processing and incremental window-based processing as well as exploiting standard DBMS functionalities in a streaming environment such as indexing. This demo presents DataCell, an extension of the MonetDB open-source column-store for online analytics. The demo gives users the opportunity to experience the features of DataCell such as processing both stream and persistent data and performing window based processing. The demo provides a visual interface to monitor the critical system components, e.g., how query plans transform from typical DBMS query plans to online query plans, how data flows through the query plans as the streams evolve, how DataCell maintains intermediate results in columnar form to avoid repeated evaluation of the same stream portions, etc. The demo also provides the ability to interactively set the test scenarios and various DataCell knobs.

#index 1895077
#* SWORS: a system for the efficient retrieval of relevant spatial web objects
#@ Xin Cao;Gao Cong;Christian S. Jensen;Jun Jie Ng;Beng Chin Ooi;Nhan-Tue Phan;Dingming Wu
#t 2012
#c 4
#% 1206801
#% 1328137
#% 1555383
#% 1581877
#! Spatial web objects that possess both a geographical location and a textual description are gaining in prevalence. This gives prominence to spatial keyword queries that exploit both location and textual arguments. Such queries are used in many web services such as yellow pages and maps services. We present SWORS, the Spatial Web Object Retrieval System, that is capable of efficiently retrieving spatial web objects that satisfy spatial keyword queries. Specifically, SWORS supports two types of queries: a) the location-aware top-k text retrieval (LkT) query that retrieves k individual spatial web objects taking into account query location proximity and text relevancy; b) the spatial keyword group (SKG) query that retrieves a group of objects that cover the query keywords and are nearest to the query location and have the shortest inter-object distances. SWORS provides browser-based interfaces for desktop and laptop computers and provides a client application for mobile devices. The interfaces and the client enable users to formulate queries and view the query results on a map. The server side stores the data and processes the queries. We use three real-life data sets to demonstrate the functionality and performance of SWORS.

#index 1895078
#* CyLog/Crowd4U: a declarative platform for complex data-centric crowdsourcing
#@ Atsuyuki Morishima;Norihide Shinagawa;Tomomi Mitsuishi;Hideto Aoki;Shun Fukusumi
#t 2012
#c 4
#% 1065099
#% 1065101
#% 1252621
#% 1291114
#% 1472960
#% 1581980
#! This demo presents a principled approach to the problems of data-centric human/machine computations with Crowd4U, a crowdsourcing platform equipped with a suite of tools for rapid development of crowdsourcing applications. Using the demo, we show that declarative database abstraction can be used as a powerful tool to design, implement, and analyze data-centric crowdsourcing applications. The power of Crowd4U comes from CyLog, a database abstraction that handles complex data-centric human/machine computations. CyLog is a Datalog-like language that incorporates a principled feedback system for humans at the language level so that the semantics of the computation not closed in machines can be defined based on the game theory. We believe that the demo clearly shows that database abstraction can be a promising basis for designing complex data-centric applications requiring human/machine computations.

#index 1895079
#* Exploiting database similarity joins for metric spaces
#@ Yasin N. Silva;Spencer Pearson
#t 2012
#c 4
#% 731409
#% 864392
#% 1054481
#% 1387564
#% 1426633
#! Similarity Joins are recognized among the most useful data processing and analysis operations and are extensively used in multiple application domains. They retrieve all data pairs whose distances are smaller than a predefined threshold ε. Multiple Similarity Join algorithms and implementation techniques have been proposed. They range from out-of-database approaches for only in-memory and external memory data to techniques that make use of standard database operators to answer similarity joins. Recent work has shown that this operation can be efficiently implemented as a physical database operator. However, the proposed operator only support 1D numeric data. This paper presents DBSimJoin, a physical Similarity Join database operator for datasets that lie in any metric space. DBSimJoin is a non-blocking operator that prioritizes the early generation of results. We implemented the proposed operator in PostgreSQL, an open source database system. We show how this operator can be used in multiple real-world data analysis scenarios with multiple data types and distance functions. Particularly, we show the use of DBSimJoin to identify similar images represented as feature vectors, and similar publications in a bibliographic database. We also show that DBSimJoin scales very well when important parameters, e.g., e, data size, increase.

#index 1895080
#* Stethoscope: a platform for interactive visual analysis of query execution plans
#@ Mrunal Gawade;Martin Kersten
#t 2012
#c 4
#% 832188
#! Searching for the performance bottleneck in an execution trace is an error prone and time consuming activity. Existing tools offer some comfort by providing a visual representation of trace for analysis. In this paper we present the Stethoscope, an interactive visual tool to inspect and analyze columnar database query performance, both online and offline. It's unique interactive animated interface capitalizes the large data-flow graph representation of a query execution plan, augmented with query execution trace information. We demonstrate features of Stethoscope for both online and offline analysis of long running queries. It helps in understanding where time goes, how optimizers perform, and how parallel processing on multi-core systems is exploited.

#index 1895081
#* Hum-a-song: a subsequence matching with gaps-range-tolerances query-by-humming system
#@ Alexios Kotsifakos;Panagiotis Papapetrou;Jaakko Hollmén;Dimitrios Gunopulos;Vassilis Athitsos;George Kollios
#t 2012
#c 4
#% 286744
#% 654456
#% 751601
#% 849877
#% 1767360
#! We present "Hum-a-song", a system built for music retrieval, and particularly for the Query-By-Humming (QBH) application. According to QBH, the user is able to hum a part of a song that she recalls and would like to learn what this song is, or find other songs similar to it in a large music repository. We present a simple yet efficient approach that maps the problem to time series subsequence matching. The query and the database songs are represented as 2-dimensional time series conveying information about the pitch and the duration of the notes. Then, since the query is a short sequence and we want to find its best match that may start and end anywhere in the database, subsequence matching methods are suitable for this task. In this demo, we present a system that employs and exposes to the user a variety of state-of-the-art dynamic programming methods, including a newly proposed efficient method named SMBGT that is robust to noise and considers all intrinsic problems in QBH; it allows variable tolerance levels when matching elements, where tolerances are defined as functions of the compared sequences, gaps in both the query and target sequences, and bounds the matching length and (optionally) the minimum number of matched elements. Our system is intended to become open source, which is to the best of our knowledge the first non-commercial effort trying to solve QBH with a variety of methods, and that also approaches the problem from the time series perspective.

#index 1895082
#* SkewTune in action: mitigating skew in MapReduce applications
#@ YongChul Kwon;Magdalena Balazinska;Bill Howe;Jerome Rolia
#t 2012
#c 4
#% 268079
#% 963669
#% 983467
#% 1127354
#% 1426481
#% 1468411
#% 1468423
#% 1486236
#% 1523820
#% 1532861
#% 1770321
#% 1798411
#% 1846759
#! We demonstrate SkewTune, a system that automatically mitigates skew in user-defined MapReduce programs and is a drop-in replacement for Hadoop. The demonstration has two parts. First, we demonstrate how SkewTune mitigates skew in real MapReduce applications at runtime by running a real application in a public cloud. Second, through an interactive graphical interface, we demonstrate the details of the skew mitigation process using both real and synthetic workloads that represent various skew configurations.

#index 1895083
#* Playful query specification with DataPlay
#@ Azza Abouzied;Joseph M. Hellerstein;Avi Silberschatz
#t 2012
#c 4
#% 236416
#% 285926
#% 317981
#% 415958
#% 1167474
#! DataPlay is a query tool that encourages a trial-and-error approach to query specification. DataPlay uses a graphical query language to make a particularly challenging query specification task - quantification - easier. It constrains the relational data model to enable the presentation of non-answers, in addition to answers, to aid query interpretation. Two novel features of DataPlay are suggesting semantic variations to a query and correcting queries by example. We introduce DataPlay as a sophisticated query specification tool and demonstrate its unique interaction models.

#index 1895084
#* NoDB in action: adaptive query processing on raw data
#@ Ioannis Alagiannis;Renata Borovica;Miguel Branco;Stratos Idreos;Anastasia Ailamaki
#t 2012
#c 4
#% 845351
#% 960234
#% 1022202
#% 1375919
#% 1770339
#! As data collections become larger and larger, users are faced with increasing bottlenecks in their data analysis. More data means more time to prepare the data, to load the data into the database and to execute the desired queries. Many applications already avoid using traditional database systems, e.g., scientific data analysis and social networks, due to their complexity and the increased data-to-query time, i.e. the time between getting the data and retrieving its first useful results. For many applications data collections keep growing fast, even on a daily basis, and this data deluge will only increase in the future, where it is expected to have much more data than what we can move or store, let alone analyze. In this demonstration, we will showcase a new philosophy for designing database systems called NoDB. NoDB aims at minimizing the data-to-query time, most prominently by removing the need to load data before launching queries. We will present our prototype implementation, PostgresRaw, built on top of PostgreSQL, which allows for efficient query execution over raw data files with zero initialization overhead. We will visually demonstrate how PostgresRaw incrementally and adaptively touches, parses, caches and indexes raw data files autonomously and exclusively as a side-effect of user queries.

#index 1895085
#* Complex preference queries supporting spatial applications for user groups
#@ Florian Wenzel;Markus Endres;Stefan Mandl;Werner Kießling
#t 2012
#c 4
#% 893150
#% 993957
#% 1080130
#% 1590539
#% 1806269
#! Our demo application demonstrates a personalized location-based web application using Preference SQL that allows single users as well as groups of users to find accommodations in Istanbul that satisfy both hard constraints and user preferences. The application assists in defining spatial, numerical, and categorical base preferences and composes complex preference statements in an intuitive fashion. Unlike existing location-based services, the application considers spatial queries as soft instead of hard constraints to determine the best matches which are finally presented on a map. The underlying Preference SQL framework is implemented on top of a database, therefore enabling a seamless application integration with standard SQL back-end systems as well as efficient and extensible preference query processing.

#index 1895086
#* Demonstration of the FDB query engine for factorised databases
#@ Nurzhan Bakibayev;Dan Olteanu;Jakub Závodný
#t 2012
#c 4
#% 102787
#% 1615075
#% 1818427
#% 1826142
#% 1880442
#! FDB is an in-memory query engine for factorised databases, which are relational databases that use compact factorised representations at the physical layer to reduce data redundancy and boost query performance. We demonstrate FDB using real data sets from IMDB, DBLP, and the NELL repository of facts learned from Web pages. The users can inspect factorisations as well as plans used by FDB to compute factorised results of select-project-join queries on factorised databases.

#index 1895087
#* PET: reducing database energy cost via query optimization
#@ Zichen Xu;Yi-Cheng Tu;Xiaorui Wang
#t 2012
#c 4
#% 1124990
#% 1127556
#% 1426521
#! Energy conservation is a growing important issue in designing modern database management system (DBMS). This requires a deep thinking about the tradeoffs between energy and performance. Despite the significant amount of efforts at the hardware level to make the major components consume less energy, we argue for a revisit of the DBMS query processing mechanism to identify and harvest the potential of energy saving. However, the state-of-art architecture of DBMS does not take energy usage into consideration in its design. A major challenge in developing an energy-aware DBMS is to design and implement a cost-based query optimizer that evaluates query plans by both performance and energy costs. By following such a strategy, our previous work revealed the fact that energy-efficient query plans do not necessarily have the shortest processing time. This demo proposal introduces PET -- an energy-aware query optimization framework that is built as a part of the PostgreSQL kernel. PET, via its power cost estimation module and plan evaluation model, enables the database system to run under a DBA-specified energy/performance tradeoff level. PET contains a power cost estimator that can accurately estimate the power cost of query plans at compile time, and a query evaluation engine that the DBA could configure key PET parameters towards the desired tradeoff. The software to be demonstrated will also include workload engine for producing large quantities of queries and data sets. Our demonstration will show how PET functions via a comprehensive set of views from its graphical user interface named PET Viewer. Through such interfaces, a user can achieve a good understanding of the energy-related query optimization and cost-based plan generation. Users are also allowed to interact with PET to experience the different energy/performance tradeoffs by changing PET and workload parameters at query runtime.

#index 1895088
#* SPAM: a SPARQL analysis and manipulation tool
#@ Andrés Letelier;Jorge Pérez;Reinhard Pichler;Sebastian Skritek
#t 2012
#c 4
#% 1022236
#% 1127431
#% 1206875
#% 1223424
#% 1424588
#% 1560420
#% 1581837
#% 1581858
#% 1589318
#% 1770125
#% 1846781
#! SQL developers are used to having elaborate tools which help them in writing queries. In contrast, the creation of tools to assist users in the development of SPARQL queries is still in its infancy. In this system demo, we present the SPARQL Analysis and Manipulation (SPAM) tool, which provides help for the development of SPARQL queries. The main features of the SPAM tool comprise an editor with both text and graphical interface, as well as various functions for the static and dynamic analysis of SPARQL queries.

#index 1895089
#* QueryMarket demonstration: pricing for online data markets
#@ Paraschos Koutris;Prasang Upadhyaya;Magdalena Balazinska;Bill Howe;Dan Suciu
#t 2012
#c 4
#% 1770132
#! Increasingly data is being bought and sold online. To facilitate such transactions, online data market-places have emerged to provide a service for sellers to price views on their data, and buyers to buy such views. These marketplaces neither support the sale of ad-hoc queries (that are not one of the specified views), nor do they support queries that join datasets. We present QueryMarket, a prototype data marketplace that automatically extrapolates prices to ad-hoc queries, including those with joins, from the manually priced views. We call this capability "query-based pricing" and describe how it is superior to existing pricing methods, and how it provides more flexible pricing for the sellers. We then show how QueryMarket implements query-based pricing and how it generates explanations for the prices it computes.

#index 1895090
#* DISKs: a system for distributed spatial group keyword search on road networks
#@ Siqiang Luo;Yifeng Luo;Shuigeng Zhou;Gao Cong;Jihong Guan
#t 2012
#c 4
#% 274612
#% 660011
#% 960259
#% 1555383
#% 1581877
#! Query (e.g., shortest path) on road networks has been extensively studied. Although most of the existing query processing approaches are designed for centralized environments, there is a growing need to handle queries on road networks in distributed environments due to the increasing query workload and the challenge of querying large networks. In this demonstration, we showcase a distributed system called DISKs (DIstributed Spatial Keyword search) that is capable of efficiently supporting spatial group keyword search (S-GKS) on road networks. Given a group of keywords X and a distance r, an SGKS returns locations on a road network, such that for each returned location p, there exists a set of nodes (on the road network), which are located within a network distance r from p and collectively contains X. We will demonstrate the innovative modules, performance and interactive user interfaces of DISKs.

#index 1895091
#* WETSUIT: an efficient mashup tool for searching and fusing web entities
#@ Stefan Endrullis;Andreas Thor;Erhard Rahm
#t 2012
#c 4
#% 1063559
#% 1190113
#% 1215807
#% 1611822
#% 1846819
#! We demonstrate a new powerful mashup tool called WETSUIT (Web EnTity Search and fUsIon Tool) to search and integrate web data from diverse sources and domain-specific entity search engines. WETSUIT supports adaptive search strategies to query sets of relevant entities with a minimum of communication overhead. Mashups can be composed using a set of high-level operators based on the Java-compatible language Scala. The operator implementation supports a high degree of parallel processing, in particular a streaming of entities between all data transformation operations facilitating a fast presentation of intermediate results. WETSUIT has already been applied to solve challenging integration tasks from different domains.

#index 1895092
#* Model-based integration of past & future in TimeTravel
#@ Mohamed E. Khalefa;Ulrike Fischer;Torben Bach Pedersen;Wolfgang Lehner
#t 2012
#c 4
#% 404849
#% 461885
#% 873056
#% 874976
#% 1063529
#% 1083693
#% 1206570
#% 1615889
#% 1846736
#! We demonstrate TimeTravel, an efficient DBMS system for seamless integrated querying of past and (forecasted) future values of time series, allowing the user to view past and future values as one joint time series. This functionality is important for advanced application domain like energy. The main idea is to compactly represent time series as models. By using models, the TimeTravel system answers queries approximately on past and future data with error guarantees (absolute error and confidence) one order of magnitude faster than when accessing the time series directly. In addition, it efficiently supports exact historical queries by only accessing relevant portions of the time series. This is unlike existing approaches, which access the entire time series to exactly answer the query. To realize this system, we propose a novel hierarchical model index structure. As real-world time series usually exhibits seasonal behavior, models in this index incorporate seasonality. To construct a hierarchical model index, the user specifies seasonality period, error guarantees levels, and a statistical forecast method. As time proceeds, the system incrementally updates the index and utilizes it to answer approximate and exact queries. TimeTravel is implemented into PostgreSQL, thus achieving complete user transparency at the query level. In the demo, we show the easy building of a hierarchical model index for a real-world time series and the effect of varying the error guarantees on the speed up of approximate and exact queries.

#index 1895093
#* DrillBeyond: enabling business analysts to explore the web of open data
#@ Julian Eberius;Maik Thiele;Katrin Braunschweig;Wolfgang Lehner
#t 2012
#c 4
#% 1581851
#% 1581893
#! Following the Open Data trend, governments and public agencies have started making their data available on the Web and established platforms such as data.gov or data.un.org. These Open Data platforms provide a huge amount of data for various topics such as demographics, transport, finance or health in various data formats. One typical usage scenario for this kind of data is their integration into a database or data warehouse in order to apply data analytics. However, in today's business intelligence tools there is an evident lack of support for so-called situational or ad-hoc data integration. In this demonstration we will therefore present DrillBeyond, a novel database and information retrieval engine which allows users to query a local database as well as the Web of Open Data in a seamless and integrated way with standard SQL. The audience will be able to pose queries to our DrillBeyond system which will be answered partly from local data in the database and partly from datasets that originate from the Web of Data. We will show how such queries are divided into known and unknown parts and how missing attributes are mapped to open datasets. We will demonstrate the integration of the open datasets back into the DBMS in order to apply its analytical features.

#index 1895094
#* Discovering and exploring relations on the web
#@ Ndapandula Nakashole;Gerhard Weikum;Fabian Suchanek
#t 2012
#c 4
#% 956564
#% 1355026
#% 1536527
#% 1654055
#% 1711857
#% 1711858
#% 1711865
#% 1733680
#% 1770359
#% 1913673
#! We propose a demonstration of PATTY, a system for learning semantic relationships from the Web. PATTY is a collection of relations learned automatically from text. It aims to be to patterns what WordNet is to words. The semantic types of PATTY relations enable advanced search over subject-predicate-object data. With the ongoing trends of enriching Web data (both text and tables) with entity-relationship-oriented semantic annotations, we believe a demo of the PATTY system will be of interest to the database community.

#index 1895095
#* MapRat: meaningful explanation, interactive exploration and geo-visualization of collaborative ratings
#@ Saravanan Thirumuruganathan;Mahashweta Das;Shrikant Desai;Sihem Amer-Yahia;Gautam Das;Cong Yu
#t 2012
#c 4
#% 420053
#! Collaborative rating sites such as IMDB and Yelp have become rich resources that users consult to form judgments about and choose from among competing items. Most of these sites either provide a plethora of information for users to interpret all by themselves or a simple overall aggregate information. Such aggregates (e.g., average rating over all users who have rated an item, aggregates along pre-defined dimensions, etc.) can not help a user quickly decide the desirability of an item. In this paper, we build a system MapRat that allows a user to explore multiple carefully chosen aggregate analytic details over a set of user demographics that meaningfully explain the ratings associated with item(s) of interest. MapRat allows a user to systematically explore, visualize and understand user rating patterns of input item(s) so as to make an informed decision quickly. In the demo, participants are invited to explore collaborative movie ratings for popular movies.

#index 1895096
#* Deco: a system for declarative crowdsourcing
#@ Hyunjung Park;Hector Garcia-Molina;Richard Pang;Neoklis Polyzotis;Aditya Parameswaran;Jennifer Widom
#t 2012
#c 4
#% 13016
#% 1526538
#% 1573506
#% 1581851
#! Deco is a system that enables declarative crowdsourcing: answering SQL queries posed over data gathered from the crowd as well as existing relational data. Deco implements a novel push-pull hybrid execution model in order to support a flexible data model and a precise query semantics, while coping with the combination of latency, monetary cost, and uncertainty of crowdsourcing. We demonstrate Deco using two crowdsourcing platforms: Amazon Mechanical Turk and an in-house platform, to show how Deco provides a convenient means of collecting and querying crowdsourced data.

#index 1895097
#* Developing and analyzing XSDs through BonXai
#@ Wim Martens;Matthias Niewerth;Frank Neven;Thomas Schwentick
#t 2012
#c 4
#% 299944
#% 772031
#% 894435
#% 976999
#% 1021195
#% 1217202
#% 1370257
#% 1552663
#! BonXai is a versatile schema specification language expressively equivalent to XML Schema. It is not intended as a replacement for XML Schema but it can serve as an additional, user-friendly front-end. It offers a simple way and a lightweight syntax to specify the context of elements based on regular expressions rather than on types. In this demo we show the front-end capabilities of BonXai and exemplify its potential to offer a novel way to view existing XML Schema Definitions. In particular, we present several usage scenarios specifically targeted to showcase the ease of specifying, modifying, and understanding XML Schema Definitions through BonXai.

#index 1895098
#* InfoPuzzle: exploring group decision making in mobile peer-to-peer databases
#@ Aaron J. Elmore;Sudipto Das;Divyakant Agrawal;Amr El Abbadi
#t 2012
#c 4
#% 88091
#% 839567
#% 884479
#% 990826
#% 1044487
#% 1214770
#! As Internet-based services and mobile computing devices, such as smartphones and tablets, become ubiquitous, society's reliance on them to accomplish critical and time-sensitive tasks, such as information dissemination and collaborative decision making, also increases. Dependence on these media magnifies the damage caused by their disruption, whether malicious or natural. For instance, a natural disaster disrupting cellular and Internet infrastructures impedes information spread, which in turn leads to chaos, both among the victims as well as the aid providers. Decentralized and ad-hoc mechanisms for information dissemination and decision making are paramount to help restore order. We demonstrate InfoPuzzle, a mobile peer-to-peer database that utilizes direct device communication to enable group decision making, or consensus, without reliance on centralized communication services. InfoPuzzle minimizes the system's resource consumption, to prolong the lifetime of the power constrained devices by minimizing communication overhead, computational complexity, and persistent storage size. Due to user mobility and the limited range of point-to-point communication, knowing the exact number of participants is impossible, and therefore traditional consensus or quorum protocols cannot be used. We rely of distinct counting techniques, probabilistic thresholds, and bounded time based approaches to reach agreement. In this demo, we will explore various challenges and heuristics in estimating group participation to aid users in reconciling consensus without centralized services.

#index 1895099
#* Manage and query generic moving objects in SECONDO
#@ Jianqiu Xu;Ralf Hartmut Güting
#t 2012
#c 4
#% 800618
#% 1181297
#% 1302862
#% 1329594
#% 1524237
#% 1643318
#% 1667196
#% 1930409
#! In this demonstration, we introduce a system that is able to manage moving objects in all real world environments, e.g., road network, bus network and indoor. The complete trip of a person is managed by the system such as Walk, Car, Walk, and Indoor, where the precise locations of both outdoor and indoor movements are represented. Trajectories located in several environments are integrated into the same framework. The system supports the shortest path searching for start and end locations being in different environments, for example, from a room to a bus stop. A comprehensive and scalable set of moving objects is generated to simulate human movement in practice. Optimization methods are developed to efficiently answer novel queries regarding transportation modes and mobile environments. Most of these queries are not supported by existing methods because of the limitation of data representation.

#index 1895100
#* Chronos: facilitating history discovery by linking temporal records
#@ Pei Li;Christina Tziviskou;Haidong Wang;Xin Luna Dong;Xiaoguang Liu;Andrea Maurino;Divesh Srivastava
#t 2012
#c 4
#% 810108
#% 1063592
#% 1217238
#% 1328216
#% 1523951
#% 1581965
#% 1581978
#! Many data sets contain temporal records over a long period of time; each record is associated with a time stamp and describes some aspects of a real-world entity at that particular time. From such data, users often wish to search for entities in a particular period and understand the history of one entity or all entities in the data set. A major challenge for enabling such search and exploration is to identify records that describe the same real-world entity over a long period of time; however, linking temporal records is hard given that the values that describe an entity can evolve over time (e.g., a person can move from one affiliation to another). We demonstrate the Chronos system which offers users the useful tool for finding real-world entities over time and understanding history of entities in the bibliography domain. The core of Chronos is a temporal record-linkage algorithm, which is tolerant to value evolution over time. Our algorithm can obtain an F-measure of over 0.9 in linking author records and fix errors made by DBLP. We show how Chronos allows users to explore the history of authors, and how it helps users understand our linkage results by comparing our results with those of existing systems, highlighting differences in the results, explaining our decisions to users, and answering "what-if" questions.

#index 1895101
#* TELEIOS: a database-powered virtual earth observatory
#@ Manolis Koubarakis;Mihai Datcu;Charalambos Kontoes;Ugo Di Giammatteo;Stefan Manegold;Eva Klien
#t 2012
#c 4
#% 1333474
#% 1652704
#% 1737610
#% 1882103
#! TELEIOS is a recent European project that addresses the need for scalable access to petabytes of Earth Observation data and the discovery and exploitation of knowledge that is hidden in them. TELEIOS builds on scientific database technologies (array databases, SciQL, data vaults) and Semantic Web technologies (stRDF and stSPARQL) implemented on top of a state of the art column store database system (MonetDB). We demonstrate a first prototype of the TELEIOS Virtual Earth Observatory (VEO) architecture, using a forest fire monitoring application as example.

#index 1895102
#* Efficient big data processing in Hadoop MapReduce
#@ Jens Dittrich;Jorge-Arnulfo Quiané-Ruiz
#t 2012
#c 4
#% 723279
#% 983467
#% 1217159
#% 1278124
#% 1328060
#% 1328108
#% 1372690
#% 1426479
#% 1426488
#% 1426584
#% 1426588
#% 1468423
#% 1523837
#% 1523841
#% 1573238
#% 1581407
#% 1581925
#% 1581926
#% 1586685
#% 1594684
#% 1621136
#% 1621145
#% 1880472
#! This tutorial is motivated by the clear need of many organizations, companies, and researchers to deal with big data volumes efficiently. Examples include web analytics applications, scientific applications, and social networks. A popular data processing engine for big data is Hadoop MapReduce. Early versions of Hadoop MapReduce suffered from severe performance problems. Today, this is becoming history. There are many techniques that can be used with Hadoop MapReduce jobs to boost performance by orders of magnitude. In this tutorial we teach such techniques. First, we will briefly familiarize the audience with Hadoop MapReduce and motivate its use for big data processing. Then, we will focus on different data management techniques, going from job optimization to physical data organization like data layouts and indexes. Throughout this tutorial, we will highlight the similarities and differences between Hadoop MapReduce and Parallel DBMS. Furthermore, we will point out unresolved research problems and open issues.

#index 1895103
#* MapReduce algorithms for big data analysis
#@ Kyuseok Shim
#t 2012
#c 4
#% 248792
#% 310516
#% 956521
#% 963669
#% 1127463
#% 1176961
#% 1190074
#% 1215321
#% 1229386
#% 1328061
#% 1365393
#% 1400001
#% 1426543
#% 1426584
#% 1507740
#% 1535356
#% 1566936
#% 1581925
#% 1607936
#% 1688496
#% 1688787
#% 1746886
#% 1769264
#% 1846758
#! There is a growing trend of applications that should handle big data. However, analyzing big data is a very challenging problem today. For such applications, the MapReduce framework has recently attracted a lot of attention. Google's MapReduce or its open-source equivalent Hadoop is a powerful tool for building such applications. In this tutorial, we will introduce the MapReduce framework based on Hadoop, discuss how to design efficient MapReduce algorithms and present the state-of-the-art in MapReduce algorithms for data mining, machine learning and similarity joins. The intended audience of this tutorial is professionals who plan to design and develop MapReduce algorithms and researchers who should be aware of the state-of-the-art in MapReduce algorithms available today for big data analysis.

#index 1895104
#* Entity resolution: theory, practice & open challenges
#@ Lise Getoor;Ashwin Machanavajjhala
#t 2012
#c 4
#% 201889
#% 251405
#% 310516
#% 577238
#% 654467
#% 777329
#% 788090
#% 800590
#% 810014
#% 875066
#% 915242
#% 915340
#% 937552
#% 1201863
#% 1217163
#% 1250576
#% 1269495
#% 1426567
#% 1523813
#% 1523838
#% 1538763
#! This tutorial brings together perspectives on ER from a variety of fields, including databases, machine learning, natural language processing and information retrieval, to provide, in one setting, a survey of a large body of work. We discuss both the practical aspects and theoretical underpinnings of ER. We describe existing solutions, current challenges, and open research problems.

#index 1895105
#* I/O characteristics of NoSQL databases
#@ Jiri Schindler
#t 2012
#c 4
#! The advent of the so-called NoSQL databases has brought about a new model of using storage systems. While traditional relational database systems took advantage of features offered by centrally-managed, enterprise-class storage arrays, the new generation of database systems with weaker data consistency models is content with using and managing locally attached individual storage devices and providing data reliability and availability through high-level software features and protocols. This work aims to review the architecture of several existing NoSQL DBs with an emphasis on how they organize and access data in the shared-nothing locally-attached storage model. It shows how these systems operate under typical workloads (new inserts and point and range queries), what access characteristics they exhibit to storage systems. Finally, it examines how several recently developed key/value stores, schema-free document storage systems, and extensible column stores organize data on local filesystems on top of directly-attached disks and what system features they must (re)implement in order to provide the expected data reliability.

#index 1895106
#* Mining knowledge from interconnected data: a heterogeneous information network analysis approach
#@ Yizhou Sun;Jiawei Han;Xifeng Yan;Philip S. Yu
#t 2012
#c 4
#% 1063512
#% 1081580
#% 1176876
#% 1181261
#% 1214701
#% 1451159
#% 1495579
#% 1581917
#% 1606073
#% 1635098
#% 1693927
#% 1707456
#% 1730734
#% 1872391
#! Most objects and data in the real world are interconnected, forming complex, heterogeneous but often semi-structured information networks. However, most people consider a database merely as a data repository that supports data storage and retrieval rather than one or a set of heterogeneous information networks that contain rich, inter-related, multi-typed data and information. Most network science researchers only study homogeneous networks, without distinguishing the different types of objects and links in the networks. In this tutorial, we view database and other interconnected data as heterogeneous information networks, and study how to leverage the rich semantic meaning of types of objects and links in the networks. We systematically introduce the technologies that can effectively and efficiently mine useful knowledge from such information networks.

#index 1895107
#* Understanding and managing cascades on large graphs
#@ B. Aditya Prakash;Christos Faloutsos
#t 2012
#c 4
#% 324817
#% 729923
#% 868469
#% 989613
#% 991977
#% 1214671
#% 1425621
#% 1429734
#% 1451246
#% 1496777
#% 1535434
#% 1535470
#% 1536522
#% 1597390
#% 1628176
#% 1688538
#% 1746901
#% 1872229
#% 1872281
#! How do contagions spread in population networks? Which group should we market to, for maximizing product penetration? Will a given YouTube video go viral? Who are the best people to vaccinate? What happens when two products compete? The objective of this tutorial is to provide an intuitive and concise overview of most important theoretical results and algorithms to help us understand and manipulate such propagation-style processes on large networks. The tutorial contains three parts: (a) Theoretical results on the behavior of fundamental models; (b) Scalable Algorithms for changing the behavior of these processes e.g., for immunization, marketing etc.; and (c) Empirical Studies of diffusion on blogs and on-line websites like Twitter. The problems we focus on are central in surprisingly diverse areas: from computer science and engineering, epidemiology and public health, product marketing to information dissemination. Our emphasis is on intuition behind each topic, and guidelines for the practitioner.

#index 1895108
#* Interoperability in eHealth systems
#@ Asuman Dogac
#t 2012
#c 4
#% 854831
#% 1868059
#! Interoperability in eHealth systems is important for delivering quality healthcare and reducing healthcare costs. Some of the important use cases include coordinating the care of chronic patients by enabling the co-operation of many different eHealth systems such as Electronic Health Record Systems (EHRs), Personal Health Record Systems (PHRs) and wireless medical sensor devices; enabling secondary use of EHRs for clinical research; being able to share life long EHRs among different healthcare providers. Although achieving eHealth interoperability is quite a challenge both because there are competing standards and clinical information itself is very complex, there have been a number of successful industry initiatives such as Integrating the Healthcare Enterprise (IHE) Profiles, as well as large scale deployments such as the National Health Information System of Turkey and the epSOS initiative for sharing Electronic Health Records and ePrescriptions in Europe. This article briefly describes the subjects discussed in the VLDB 2012 tutorial to provide an overview of the issues in eHealth interoperability describing the key technologies and standards, identifying important use cases and the associated research challenges and also describing some of the large scale deployments. The aim is to foster further interest in this area.

#index 1895109
#* Secure and privacy-preserving data services in the cloud: a data centric view
#@ Divyakant Agrawal;Amr El Abbadi;Shiyuan Wang
#t 2012
#c 4
#% 88391
#% 264163
#% 397367
#% 593800
#% 725292
#% 765448
#% 1016189
#% 1022245
#% 1298793
#% 1309521
#% 1386180
#% 1486143
#% 1581863
#% 1601106
#% 1619666
#% 1625037
#% 1745624
#! Cloud computing becomes a successful paradigm for data computing and storage. Increasing concerns about data security and privacy in the cloud, however, have emerged. Ensuring security and privacy for data management and query processing in the cloud is critical for better and broader uses of the cloud. This tutorial covers some common cloud security and privacy threats and the relevant research, while focusing on the works that protect data confidentiality and query access privacy for sensitive data being stored and queried in the cloud. We provide a comprehensive study of state-of-the-art schemes and techniques for protecting data confidentiality and access privacy, which make different tradeoffs in the multidimensional space of security, privacy, functionality and performance.

#index 1895110
#* Graph synopses, sketches, and streams: a survey
#@ Sudipto Guha;Andrew McGregor
#t 2012
#c 4
#% 866773
#% 1426513
#% 1668207
#% 1770116
#! Massive graphs arise in any application where there is data about both basic entities and the relationships between these entities, e.g., web-pages and hyperlinks; neurons and synapses; papers and citations; IP addresses and network flows; people and their friendships. Graphs have also become the de facto standard for representing many types of highly structured data. However, the sheer size of many of these graphs renders classical algorithms inapplicable when it comes to analyzing such graphs. In addition, these existing algorithms are typically ill-suited to processing distributed or stream data. Various platforms have been developed for processing large data sets. At the same time, there is the need to develop new algorithmic ideas and paradigms. In the case of graph processing, a lot of recent work has focused on understanding the important algorithmic issues. An central aspect of this is the question of how to construct and leverage small-space synopses in graph processing. The goal of this tutorial is to survey recent work on this question and highlight interesting directions for future research.

#index 1895111
#* Challenges and opportunities with big data
#@ Alexandros Labrinidis;H. V. Jagadish
#t 2012
#c 4
#! The promise of data-driven decision-making is now being recognized broadly, and there is growing enthusiasm for the notion of "Big Data," including the recent announcement from the White House about new funding initiatives across different agencies, that target research for Big Data. While the promise of Big Data is real -- for example, it is estimated that Google alone contributed 54 billion dollars to the US economy in 2009 -- there is no clear consensus on what is Big Data. In fact, there have been many controversial statements about Big Data, such as "Size is the only thing that matters." In this panel we will try to explore the controversies and debunk the myths surrounding Big Data.

#index 1895112
#* Panel discussion on social networks and mobility in the cloud
#@ Amr El Abbadi;Mohamed F. Mokbel
#t 2012
#c 4
#! Social networks, mobility and the cloud represent special and unique opportunities for synergy among several existing and emerging communities that are now often evolving in isolated silos. All three areas hold much promise for the future of computing, and represent significant challenges for large scale data management. As these three areas evolve, their direct influence on significant decisions on each other becomes evident and critical. This panel will bring together a set of renowned researchers who will explore and discuss the synergy and tensions among critical and often intertwined research and application issues that arise in the context of social networks and mobility in a cloud infrastructure setting.

#index 1944321
#* Spatio-textual similarity joins
#@ Panagiotis Bouros;Shen Ge;Nikos Mamoulis
#t 2012
#c 4
#% 13041
#% 152937
#% 249989
#% 287466
#% 333973
#% 427199
#% 480467
#% 480654
#% 527189
#% 641964
#% 765463
#% 766441
#% 838407
#% 864392
#% 874993
#% 893164
#% 913783
#% 956506
#% 982560
#% 1206801
#% 1206821
#% 1206997
#% 1328137
#% 1523828
#% 1555383
#% 1581877
#% 1590535
#% 1618262
#% 1667259
#% 1720754
#! Given a collection of objects that carry both spatial and textual information, a spatio-textual similarity join retrieves the pairs of objects that are spatially close and textually similar. As an example, consider a social network with spatially and textually tagged persons (i.e., their locations and profiles). A useful task (for friendship recommendation) would be to find pairs of persons that are spatially close and their profiles have a large overlap (i.e., they have common interests). Another application is data de-duplication (e.g., finding photographs which are spatially close to each other and high overlap in their descriptive tags). Despite the importance of this operation, there is very little previous work that studies its efficient evaluation and in fact under a different definition; only the best match for each object is identified. In this paper, we combine ideas from state-of-the-art spatial distance join and set similarity join methods and propose efficient algorithms that take into account both spatial and textual constraints. Besides, we propose a batch processing technique which boosts the performance of our approaches. An experimental evaluation using real and synthetic datasets shows that our optimized techniques are orders of magnitude faster than base-line solutions.

#index 1944322
#* DisC diversity: result diversification based on dissimilarity and coverage
#@ Marina Drosou;Evaggelia Pitoura
#t 2012
#c 4
#% 95568
#% 142547
#% 177422
#% 408396
#% 443482
#% 805841
#% 975092
#% 996262
#% 1074133
#% 1114818
#% 1126951
#% 1127465
#% 1166473
#% 1181244
#% 1190093
#% 1206662
#% 1206819
#% 1328120
#% 1472964
#% 1550142
#% 1581911
#% 1594636
#% 1598392
#% 1641999
#% 1693888
#% 1770131
#% 1770354
#% 1798393
#! Recently, result diversification has attracted a lot of attention as a means to improve the quality of results retrieved by user queries. In this paper, we propose a new, intuitive definition of diversity called DisC diversity. A DisC diverse subset of a query result contains objects such that each object in the result is represented by a similar object in the diverse subset and the objects in the diverse subset are dissimilar to each other. We show that locating a minimum DisC diverse subset is an NP-hard problem and provide heuristics for its approximation. We also propose adapting DisC diverse subsets to a different degree of diversification. We call this operation zooming. We present efficient implementations of our algorithms based on the M-tree, a spatial index structure, and experimentally evaluate their performance.

#index 1944323
#* On differentially private frequent itemset mining
#@ Chen Zeng;Jeffrey F. Naughton;Jin-Yi Cai
#t 2012
#c 4
#% 300120
#% 320944
#% 342643
#% 481290
#% 599545
#% 740764
#% 1066737
#% 1127361
#% 1198224
#% 1328187
#% 1426563
#% 1451190
#% 1464628
#% 1523887
#% 1595893
#% 1670071
#% 1740518
#% 1880451
#! We consider differentially private frequent itemset mining. We begin by exploring the theoretical difficulty of simultaneously providing good utility and good privacy in this task. While our analysis proves that in general this is very difficult, it leaves a glimmer of hope in that our proof of difficulty relies on the existence of long transactions (that is, transactions containing many items). Accordingly, we investigate an approach that begins by truncating long transactions, trading off errors introduced by the truncation with those introduced by the noise added to guarantee privacy. Experimental results over standard benchmark databases show that truncating is indeed effective. Our algorithm solves the "classical" frequent itemset mining problem, in which the goal is to find all itemsets whose support exceeds a threshold. Related work has proposed differentially private algorithms for the top-k itemset mining problem ("find the k most frequent itemsets".) An experimental comparison with those algorithms show that our algorithm achieves better F-score unless k is small.

#index 1959784
#* Less is more: selecting sources wisely for integration
#@ Xin Luna Dong;Barna Saha;Divesh Srivastava
#t 2012
#c 4
#% 960344
#% 989682
#% 1090724
#% 1129527
#% 1166519
#% 1194683
#% 1232630
#% 1247814
#% 1328103
#% 1328155
#% 1355029
#% 1441531
#% 1484339
#% 1560376
#% 1730734
#% 1826432
#% 1959789
#! We are often thrilled by the abundance of information surrounding us and wish to integrate data from as many sources as possible. However, understanding, analyzing, and using these data are often hard. Too much data can introduce a huge integration cost, such as expenses for purchasing data and resources for integration and cleaning. Furthermore, including low-quality data can even deteriorate the quality of integration results instead of bringing the desired quality gain. Thus, "the more the better" does not always hold for data integration and often "less is more". In this paper, we study how to select a subset of sources before integration such that we can balance the quality of integrated data and integration cost. Inspired by the Marginalism principle in economic theory, we wish to integrate a new source only if its marginal gain, often a function of improved integration quality, is higher than the marginal cost, associated with data-purchase expense and integration resources. As a first step towards this goal, we focus on data fusion tasks, where the goal is to resolve conflicts from different sources. We propose a randomized solution for selecting sources for fusion and show empirically its effectiveness and scalability on both real-world data and synthetic data.

#index 1959785
#* Distributed time-aware provenance
#@ Wenchao Zhou;Suyog Mapara;Yiqing Ren;Yang Li;Andreas Haeberlen;Zachary Ives;Boon Thau Loo;Micah Sherr
#t 2012
#c 4
#% 464891
#% 770889
#% 771779
#% 795391
#% 978444
#% 1022258
#% 1063593
#% 1127414
#% 1231247
#% 1246527
#% 1278054
#% 1426553
#% 1426581
#% 1464044
#% 1567906
#% 1581982
#% 1625051
#! The ability to reason about changes in a distributed system's state enables network administrators to better diagnose protocol misconfigurations, detect intrusions, and pinpoint performance bottlenecks. We propose a novel provenance model called Distributed Time-aware Provenance (DTaP) that aids forensics and debugging in distributed systems by explicitly representing time, distributed state, and state changes. Using a distributed Datalog abstraction for modeling distributed protocols, we prove that the DTaP model provides a sound and complete representation that correctly captures dependencies among events in a distributed system. We additionally introduce DistTape, an implementation of the DTaP model that uses novel distributed storage structures, query processing, and cost-based optimization techniques to efficiently query time-aware provenance in a distributed setting. Using two example systems (declarative network routing and Hadoop MapReduce), we demonstrate that DistTape can efficiently maintain and query time-aware provenance at low communication and computation cost.

#index 1959786
#* Query processing under GLAV mappings for relational and graph databases
#@ Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini;Moshe Y. Vardi
#t 2012
#c 4
#% 73005
#% 188853
#% 198465
#% 210214
#% 237191
#% 248038
#% 268708
#% 283052
#% 291299
#% 299968
#% 303884
#% 378409
#% 384978
#% 464720
#% 464867
#% 490489
#% 576097
#% 576102
#% 587566
#% 598376
#% 630963
#% 632039
#% 731485
#% 801691
#% 806215
#% 809239
#% 826032
#% 850730
#% 893089
#% 924747
#% 943614
#% 1015302
#% 1022349
#% 1054485
#% 1063723
#% 1217116
#% 1267123
#% 1270567
#% 1305397
#% 1416180
#% 1424597
#% 1424599
#% 1424604
#% 1426463
#% 1523844
#% 1538780
#% 1541335
#% 1675602
#% 1711120
#! Schema mappings establish a correspondence between data stored in two databases, called source and target respectively. Query processing under schema mappings has been investigated extensively in the two cases where each target atom is mapped to a query over the source (called GAV, global-as-view), and where each source atom is mapped to a query over the target (called LAV, local-as-view). The general case, called GLAV, in which queries over the source are mapped to queries over the target, has attracted a lot of attention recently, especially for data exchange. However, query processing for GLAV mappings has been considered only for the basic service of query answering, and mainly in the context of conjunctive queries (CQs) in relational databases. In this paper we study query processing for GLAV mappings in a wider sense, considering not only query answering, but also query rewriting, perfectness (the property of a rewriting to compute exactly the certain answers), and query containment relative to a mapping. We deal both with the relational case, and with graph databases, where the basic querying mechanism is that of regular path queries. Query answering in GLAV can be smoothly reduced to a combination of the LAV and GAV cases, and for CQs this reduction can be exploited also for the remaining query processing tasks. In contrast, as we show, GLAV query processing for graph databases is non-trivial and requires new insights and techniques. We obtain upper bounds for answering, rewriting, and perfectness, and show decidability of relative containment.

#index 1959787
#* Computing immutable regions for subspace top-k queries
#@ Kyriakos Mouratidis;HweeHwa Pang
#t 2012
#c 4
#% 278831
#% 300180
#% 387427
#% 397378
#% 442615
#% 465167
#% 527187
#% 643566
#% 654478
#% 733373
#% 766671
#% 777931
#% 812421
#% 824672
#% 864452
#% 875023
#% 941785
#% 982766
#% 1058620
#% 1063520
#% 1075132
#% 1127438
#% 1523852
#% 1581913
#! Given a high-dimensional dataset, a top-k query can be used to shortlist the k tuples that best match the user's preferences. Typically, these preferences regard a subset of the available dimensions (i.e., attributes) whose relative significance is expressed by user-specified weights. Along with the query result, we propose to compute for each involved dimension the maximal deviation to the corresponding weight for which the query result remains valid. The derived weight ranges, called immutable regions, are useful for performing sensitivity analysis, for finetuning the query weights, etc. In this paper, we focus on top-k queries with linear preference functions over the queried dimensions. We codify the conditions under which changes in a dimension's weight invalidate the query result, and develop algorithms to compute the immutable regions. In general, this entails the examination of numerous non-result tuples. To reduce processing time, we introduce a pruning technique and a thresholding mechanism that allow the immutable regions to be determined correctly after examining only a small number of non-result tuples. We demonstrate empirically that the two techniques combine well to form a robust and highly resource-efficient algorithm. We verify the generality of our findings using real high-dimensional data from different domains (documents, images, etc) and with different characteristics.

#index 1959788
#* Large scale cohesive subgraphs discovery for social network visual analysis
#@ Feng Zhao;Anthony K. H. Tung
#t 2012
#c 4
#% 109538
#% 122797
#% 1063503
#% 1083734
#% 1183091
#% 1523970
#% 1594586
#% 1605988
#% 1661246
#% 1746850
#% 1846696
#% 1848109
#! Graphs are widely used in large scale social network analysis nowadays. Not only analysts need to focus on cohesive subgraphs to study patterns among social actors, but also normal users are interested in discovering what happening in their neighborhood. However, effectively storing large scale social network and efficiently identifying cohesive subgraphs is challenging. In this work we introduce a novel subgraph concept to capture the cohesion in social interactions, and propose an I/O efficient approach to discover cohesive subgraphs. Besides, we propose an analytic system which allows users to perform intuitive, visual browsing on large scale social networks. Our system stores the network as a social graph in the graph database, retrieves a local cohesive subgraph based on the input keywords, and then hierarchically visualizes the subgraph out on orbital layout, in which more important social actors are located in the center. By summarizing textual interactions between social actors as tag cloud, we provide a way to quickly locate active social communities and their interactions in a unified view.

#index 1959789
#* Truth finding on the deep web: is the problem solved?
#@ Xian Li;Xin Luna Dong;Kenneth Lyons;Weiyi Meng;Divesh Srivastava
#t 2012
#c 4
#% 282905
#% 1081580
#% 1129527
#% 1328103
#% 1328107
#% 1328155
#% 1328156
#% 1355029
#% 1484339
#% 1491640
#% 1497992
#% 1523915
#% 1560376
#% 1730734
#% 1741034
#% 1826432
#% 1959784
#% 1959789
#! The amount of useful information available on the Web has been growing at a dramatic pace in recent years and people rely more and more on the Web to fulfill their information needs. In this paper, we study truthfulness of Deep Web data in two domains where we believed data are fairly clean and data quality is important to people's lives: Stock and Flight. To our surprise, we observed a large amount of inconsistency on data from different sources and also some sources with quite low accuracy. We further applied on these two data sets state-of-the-art data fusion methods that aim at resolving conflicts and finding the truth, analyzed their strengths and limitations, and suggested promising research directions. We wish our study can increase awareness of the seriousness of conflicting data on the Web and in turn inspire more research in our community to tackle this problem.

#index 1959790
#* Counting with the crowd
#@ Adam Marcus;David Karger;Samuel Madden;Robert Miller;Sewoong Oh
#t 2012
#c 4
#% 496159
#% 835045
#% 1452857
#% 1477589
#% 1478137
#% 1581851
#% 1628026
#% 1628028
#% 1628171
#% 1711598
#% 1770351
#! In this paper, we address the problem of selectivity estimation in a crowdsourced database. Specifically, we develop several techniques for using workers on a crowdsourcing platform like Amazon's Mechanical Turk to estimate the fraction of items in a dataset (e.g., a collection of photos) that satisfy some property or predicate (e.g., photos of trees). We do this without explicitly iterating through every item in the dataset. This is important in crowd-sourced query optimization to support predicate ordering and in query evaluation, when performing a GROUP BY operation with a COUNT or AVG aggregate. We compare sampling item labels, a traditional approach, to showing workers a collection of items and asking them to estimate how many satisfy some predicate. Additionally, we develop techniques to eliminate spammers and colluding attackers trying to skew selectivity estimates when using this count estimation approach. We find that for images, counting can be much more effective than sampled labeling, reducing the amount of work necessary to arrive at an estimate that is within 1% of the true fraction by up to an order of magnitude, with lower worker latency. We also find that sampled labeling outperforms count estimation on a text processing task, presumably because people are better at quickly processing large batches of images than they are at reading strings of text. Our spammer detection technique, which is applicable to both the label- and count-based approaches, can improve accuracy by up to two orders of magnitude.

#index 1959791
#* ClouDiA: a deployment advisor for public clouds
#@ Tao Zou;Ronan Le Bras;Marcos Vaz Salles;Alan Demers;Johannes Gehrke
#t 2012
#c 4
#% 170893
#% 281790
#% 288990
#% 316826
#% 482100
#% 570430
#% 600815
#% 625932
#% 660587
#% 730098
#% 736538
#% 772884
#% 864436
#% 874692
#% 875021
#% 981650
#% 998845
#% 1022200
#% 1046313
#% 1213375
#% 1216353
#% 1246353
#% 1278391
#% 1351116
#% 1400975
#% 1426484
#% 1426488
#% 1442446
#% 1449177
#% 1468226
#% 1468421
#% 1468423
#% 1487826
#% 1523836
#% 1523880
#% 1524053
#% 1527840
#% 1566953
#% 1573238
#% 1604232
#% 1604244
#% 1621144
#% 1635036
#% 1641833
#% 1783399
#% 1913807
#% 1913812
#! An increasing number of distributed data-driven applications are moving into shared public clouds. By sharing resources and operating at scale, public clouds promise higher utilization and lower costs than private clusters. To achieve high utilization, however, cloud providers inevitably allocate virtual machine instances noncontiguously, i.e., instances of a given application may end up in physically distant machines in the cloud. This allocation strategy can lead to large differences in average latency between instances. For a large class of applications, this difference can result in significant performance degradation, unless care is taken in how application components are mapped to instances. In this paper, we propose ClouDiA, a general deployment advisor that selects application node deployments minimizing either (i) the largest latency between application nodes, or (ii) the longest critical path among all application nodes. ClouDiA employs mixed-integer programming and constraint programming techniques to efficiently search the space of possible mappings of application nodes to instances. Through experiments with synthetic and real applications in Amazon EC2, we show that our techniques yield a 15% to 55% reduction in time-to-solution or service response time, without any need for modifying application code.

#index 1959792
#* An in-depth comparison of subgraph isomorphism algorithms in graph databases
#@ Jinsoo Lee;Wook-Shin Han;Romans Kasperovics;Jeong-Hoon Lee
#t 2012
#c 4
#% 288990
#% 378391
#% 629708
#% 641398
#% 672054
#% 765429
#% 772884
#% 864425
#% 960305
#% 1022280
#% 1044450
#% 1063500
#% 1127380
#% 1181229
#% 1206703
#% 1523825
#% 1523835
#% 1581921
#! Finding subgraph isomorphisms is an important problem in many applications which deal with data modeled as graphs. While this problem is NP-hard, in recent years, many algorithms have been proposed to solve it in a reasonable time for real datasets using different join orders, pruning rules, and auxiliary neighborhood information. However, since they have not been empirically compared one another in most research work, it is not clear whether the later work outperforms the earlier work. Another problem is that reported comparisons were often done using the original authors' binaries which were written in different programming environments. In this paper, we address these serious problems by re-implementing five state-of-the-art subgraph isomorphism algorithms in a common code base and by comparing them using many real-world datasets and their query loads. Through our in-depth analysis of experimental results, we report surprising empirical findings.

#index 1959793
#* Lightweight locking for main memory database systems
#@ Kun Ren;Alexander Thomson;Daniel J. Abadi
#t 2012
#c 4
#% 9241
#% 27057
#% 152596
#% 194948
#% 286836
#% 317987
#% 403195
#% 452756
#% 480604
#% 480767
#% 480959
#% 531907
#% 692767
#% 1022298
#% 1063543
#% 1328149
#% 1426552
#% 1523801
#% 1523878
#% 1668635
#% 1770319
#! Locking is widely used as a concurrency control mechanism in database systems. As more OLTP databases are stored mostly or entirely in memory, transactional throughput is less and less limited by disk IO, and lock managers increasingly become performance bottlenecks. In this paper, we introduce very lightweight locking (VLL), an alternative approach to pessimistic concurrency control for main-memory database systems that avoids almost all overhead associated with traditional lock manager operations. We also propose a protocol called selective contention analysis (SCA), which enables systems implementing VLL to achieve high transactional throughput under high contention workloads. We implement these protocols both in a traditional single-machine multi-core database server setting and in a distributed database where data is partitioned across many commodity machines in a shared-nothing cluster. Our experiments show that VLL dramatically reduces locking overhead and thereby increases transactional throughput in both settings.

#index 1959794
#* Lightweight privacy-preserving peer-to-peer data integration
#@ Ye Zhang;Wai-Kit Wong;S. M. Yiu;Nikos Mamoulis;David W. Cheung
#t 2013
#c 4
#% 300184
#% 381870
#% 576761
#% 577289
#% 593711
#% 723448
#% 743280
#% 765446
#% 772829
#% 824769
#% 864412
#% 980502
#% 1016250
#% 1022247
#% 1063478
#% 1098373
#% 1206751
#% 1217156
#% 1400778
#% 1426565
#% 1523850
#% 1813183
#! Peer Data Management Systems (PDMS) are an attractive solution for managing distributed heterogeneous information. When a peer (client) requests data from another peer (server) with a different schema, translations of the query and its answer are done by a sequence of intermediate peers (translators). There are two privacy issues in this P2P data integration process: (i) answer privacy: no unauthorized parties (including the translators) should learn the query result; (ii) mapping privacy: the schema and the value mappings used by the translators to perform the translation should not be revealed to other peers. Elmeleegy and Ouzzani proposed the PPP protocol that is the first to support privacy-preserving querying in PDMS. However, PPP suffers from several shortcomings. First, PPP does not satisfy the requirement of answer privacy, because it is based on commutative encryption; we show that this issue can be fixed by adopting another cryptographic technique called oblivious transfer. Second, PPP adopts a weaker notion for mapping privacy, which allows the client peer to observe certain mappings done by translators. In this paper, we develop a lightweight protocol, which satisfies mapping privacy and extend it to a more complex one that facilitates parallel translation by peers. Furthermore, we consider a stronger adversary model where there may be collusions among peers and propose an efficient protocol that guards against collusions. We conduct an experimental study on the performance of the proposed protocols using both real and synthetic data. The results show that the proposed protocols not only achieve a better privacy guarantee than PPP, but they are also more efficient.

#index 1959795
#* Memory efficient minimum substring partitioning
#@ Yang Li;Pegah Kamousi;Fangqiu Han;Shengqi Yang;Xifeng Yan;Subhash Suri
#t 2013
#c 4
#% 90818
#% 287222
#% 341704
#% 729913
#% 768815
#% 805840
#% 823464
#% 832959
#% 905815
#% 913783
#% 1446984
#% 1556716
#! Massively parallel DNA sequencing technologies are revolutionizing genomics research. Billions of short reads generated at low costs can be assembled for reconstructing the whole genomes. Unfortunately, the large memory footprint of the existing de novo assembly algorithms makes it challenging to get the assembly done for higher eukaryotes like mammals. In this work, we investigate the memory issue of constructing de Bruijn graph, a core task in leading assembly algorithms, which often consumes several hundreds of gigabytes memory for large genomes. We propose a disk-based partition method, called Minimum Substring Partitioning (MSP), to complete the task using less than 10 gigabytes memory, without runtime slowdown. MSP breaks the short reads into multiple small disjoint partitions so that each partition can be loaded into memory, processed individually and later merged with others to form a de Bruijn graph. By leveraging the overlaps among the k-mers (substring of length k), MSP achieves astonishing compression ratio: The total size of partitions is reduced from Θ(kn) to Θ(n), where n is the size of the short read database, and k is the length of a k-mer. Experimental results show that our method can build de Bruijn graphs using a commodity computer for any large-volume sequence dataset.

#index 1959796
#* NeMa: fast graph search with label similarity
#@ Arijit Khan;Yinghui Wu;Charu C. Aggarwal;Xifeng Yan
#t 2013
#c 4
#% 273924
#% 288990
#% 464720
#% 601159
#% 660001
#% 743922
#% 772884
#% 906561
#% 907535
#% 937108
#% 960259
#% 989645
#% 1016217
#% 1127380
#% 1131124
#% 1206703
#% 1217197
#% 1318714
#% 1399108
#% 1426513
#% 1431710
#% 1440734
#% 1523818
#% 1523898
#% 1523900
#% 1581833
#% 1581921
#% 1592340
#% 1606349
#% 1668636
#% 1846822
#% 1848107
#% 1894283
#! It is increasingly common to find real-life data represented as networks of labeled, heterogeneous entities. To query these networks, one often needs to identify the matches of a given query graph in a (typically large) network modeled as a target graph. Due to noise and the lack of fixed schema in the target graph, the query graph can substantially differ from its matches in the target graph in both structure and node labels, thus bringing challenges to the graph querying tasks. In this paper, we propose NeMa (Network Match), a neighborhood-based subgraph matching technique for querying real-life networks. (1) To measure the quality of the match, we propose a novel subgraph matching cost metric that aggregates the costs of matching individual nodes, and unifies both structure and node label similarities. (2) Based on the metric, we formulate the minimum cost subgraph matching problem. Given a query graph and a target graph, the problem is to identify the (top-k) matches of the query graph with minimum costs in the target graph. We show that the problem is NP-hard, and also hard to approximate. (3) We propose a heuristic algorithm for solving the problem based on an inference model. In addition, we propose optimization techniques to improve the efficiency of our method. (4) We empirically verify that NeMa is both effective and efficient compared to the keyword search and various state-of-the-art graph querying techniques.

#index 1959797
#* PARAS: a parameter space framework for online association mining
#@ Xika Lin;Abhishek Mukherji;Elke A. Rundensteiner;Carolina Ruiz;Matthew O. Ward
#t 2013
#c 4
#% 227919
#% 274146
#% 280454
#% 300120
#% 443427
#% 466646
#% 481290
#% 481588
#% 727669
#% 1066795
#% 1100188
#% 1328213
#% 1426546
#! Association rule mining is known to be computationally intensive, yet real-time decision-making applications are increasingly intolerant to delays. In this paper, we introduce the parameter space model, called PARAS. PARAS enables efficient rule mining by compactly maintaining the final rulesets. The PARAS model is based on the notion of stable region abstractions that form the coarse granularity ruleset space. Based on new insights on the redundancy relationships among rules, PARAS establishes a surprisingly compact representation of complex redundancy relationships while enabling efficient redundancy resolution at query-time. Besides the classical rule mining requests, the PARAS model supports three novel classes of exploratory queries. Using the proposed PSpace index, these exploratory query classes can all be answered with near real-time responsiveness. Our experimental evaluation using several benchmark datasets demonstrates that PARAS achieves 2 to 5 orders of magnitude improvement over state-of-the-art approaches in online association rule mining.

#index 1959798
#* Actively soliciting feedback for query answers in keyword search-based data integration
#@ Zhepeng Yan;Nan Zheng;Zachary G. Ives;Partha Pratim Talukdar;Cong Yu
#t 2013
#c 4
#% 333990
#% 572314
#% 577309
#% 643566
#% 654442
#% 660001
#% 660011
#% 824693
#% 843651
#% 845350
#% 913783
#% 957170
#% 960259
#% 961152
#% 993987
#% 1015317
#% 1016176
#% 1035578
#% 1063533
#% 1100699
#% 1127413
#% 1190056
#% 1264829
#% 1269476
#% 1426534
#% 1426567
#% 1456651
#% 1550749
#% 1581893
#% 1581894
#% 1693894
#! The problem of scaling up data integration, such that new sources can be quickly utilized as they are discovered, remains elusive: global schemas for integrated data are difficult to develop and expand, and schema and record matching techniques are limited by the fact that data and metadata are often under-specified and must be disambiguated by data experts. One promising approach is to avoid using a global schema, and instead to develop keyword search-based data integration--where the system lazily discovers associations enabling it to join together matches to keywords, and return ranked results. The user is expected to understand the data domain and provide feedback about answers' quality. The system generalizes such feedback to learn how to correctly integrate data. A major open challenge is that under this model, the user only sees and offers feedback on a few "top-k" results: this result set must be carefully selected to include answers of high relevance and answers that are highly informative when feedback is given on them. Existing systems merely focus on predicting relevance, by composing the scores of various schema and record matching algorithms. In this paper we show how to predict the uncertainty associated with a query result's score, as well as how informative feedback is on a given result. We build upon these foundations to develop an active learning approach to keyword search-based data integration, and we validate the effectiveness of our solution over real data from several very different domains.

#index 1959799
#* Spatial keyword query processing: an experimental evaluation
#@ Lisi Chen;Gao Cong;Christian S. Jensen;Dingming Wu
#t 2013
#c 4
#% 86950
#% 318437
#% 427199
#% 838407
#% 874993
#% 982560
#% 1190095
#% 1206801
#% 1206997
#% 1292663
#% 1328137
#% 1486233
#% 1490145
#% 1555383
#% 1581875
#% 1581877
#% 1594674
#% 1618262
#% 1641963
#% 1720754
#% 1846749
#% 1918428
#% 1940426
#% 1943522
#! Geo-textual indices play an important role in spatial keyword querying. The existing geo-textual indices have not been compared systematically under the same experimental framework. This makes it difficult to determine which indexing technique best supports specific functionality. We provide an all-around survey of 12 state-of-the-art geo-textual indices. We propose a benchmark that enables the comparison of the spatial keyword query performance. We also report on the findings obtained when applying the benchmark to the indices, thus uncovering new insights that may guide index selection as well as further research.

#index 1992366
#* Partitioning and ranking tagged data sources
#@ Milad Eftekhar;Nick Koudas
#t 2013
#c 4
#% 289280
#% 577220
#% 727838
#% 729918
#% 956579
#% 956589
#% 1035588
#% 1298864
#% 1328174
#% 1399992
#% 1523892
#% 1606346
#% 1655692
#! Online types of expression in the form of social networks, micro-blogging, blogs and rich content sharing platforms have proliferated in the last few years. Such proliferation contributed to the vast explosion in online data sharing we are experiencing today. One unique aspect of online data sharing is tags manually inserted by content generators to facilitate content description and discovery (e.g., hashtags in tweets). In this paper we focus on these tags and we study and propose algorithms that make use of tags in order to automatically organize and categorize this vast collection of socially contributed and tagged information. In particular, we take a holistic approach in organizing such tags and we propose algorithms to partition as well as rank this information collection. Our partitioning algorithms aim to segment the entire collection of tags (and the associated content) into a specified number of partitions for specific problem constraints. In contrast our ranking algorithms aim to identify few partitions fast, for suitably defined ranking functions. We present a detailed experimental study utilizing the full twitter firehose (set of all tweets in the Twitter service) that attests to the practical utility and effectiveness of our overall approach. We also present a detailed qualitative study of our results.

#index 1992367
#* Efficient implementation of generalized quantification in relational query languages
#@ Antonio Badia;Bin Cao
#t 2013
#c 4
#% 32878
#% 63293
#% 66208
#% 112496
#% 189638
#% 210183
#% 220425
#% 334006
#% 384978
#% 463894
#% 465170
#% 480463
#% 481604
#% 654454
#% 654498
#% 745529
#% 765457
#% 809240
#% 810018
#% 810023
#% 864409
#% 960278
#! We present research aimed at improving our understanding of the use and implementation of quantification in relational query languages in general and SQL in particular. In order to make our results as general as possible, we use the framework of Generalized Quantification. Generalized Quantifiers (GQs) are high-level, declarative logical operators that in the past have been studied from a theoretical perspective. In this paper we focus on their practical use, showing how to incorporate a dynamic set of GQs in relational query languages, how to implement them efficiently and use them in the context of SQL. We present experimental evidence of the performance of the approach, showing that it improves over traditional (relational) approaches.

#index 1992368
#* DAX: a widely distributed multitenant storage service for DBMS hosting
#@ Rui Liu;Ashraf Aboulnaga;Kenneth Salem
#t 2013
#c 4
#% 602675
#% 978404
#% 998842
#% 998845
#% 1063488
#% 1063543
#% 1127560
#% 1426489
#% 1426492
#% 1468219
#% 1526990
#% 1538766
#% 1541196
#% 1592341
#% 1594649
#% 1625057
#% 1769270
#% 1770412
#% 1880461
#% 1901412
#% 1911326
#! Many applications hosted on the cloud have sophisticated data management needs that are best served by a SQL-based relational DBMS. It is not difficult to run a DBMS in the cloud, and in many cases one DBMS instance is enough to support an application's workload. However, a DBMS running in the cloud (or even on a local server) still needs a way to persistently store its data and protect it against failures. One way to achieve this is to provide a scalable and reliable storage service that the DBMS can access over a network. This paper describes such a service, which we call DAX. DAX relies on multi-master replication and Dynamo-style flexible consistency, which enables it to run in multiple data centers and hence be disaster tolerant. Flexible consistency allows DAX to control the consistency level of each read or write operation, choosing between strong consistency at the cost of high latency or weak consistency with low latency. DAX makes this choice for each read or write operation by applying protocols that we designed based on the storage tier usage characteristics of database systems. With these protocols, DAX provides a storage service that can host multiple DBMS tenants, scaling with the number of tenants and the required storage capacity and bandwidth. DAX also provides high availability and disaster tolerance for the DBMS storage tier. Experiments using the TPC-C benchmark show that DAX provides up to a factor of 4 performance improvement over baseline solutions that do not exploit flexible consistency.

#index 1992369
#* A distributed graph engine for web scale RDF data
#@ Kai Zeng;Jiacheng Yang;Haixun Wang;Bin Shao;Zhongyuan Wang
#t 2013
#c 4
#% 289282
#% 519567
#% 728100
#% 824755
#% 864462
#% 1055731
#% 1063500
#% 1127402
#% 1127431
#% 1190676
#% 1206699
#% 1217194
#% 1328183
#% 1366460
#% 1374374
#% 1399937
#% 1409918
#% 1409954
#% 1426513
#% 1540693
#% 1602034
#% 1676087
#% 1702418
#% 1770359
#% 1770362
#% 1770371
#% 1848107
#! Much work has been devoted to supporting RDF data. But state-of-the-art systems and methods still cannot handle web scale RDF data effectively. Furthermore, many useful and general purpose graph-based operations (e.g., random walk, reachability, community discovery) on RDF data are not supported, as most existing systems store and index data in particular ways (e.g., as relational tables or as a bitmap matrix) to maximize one particular operation on RDF data: SPARQL query processing. In this paper, we introduce Trinity. RDF, a distributed, memory-based graph engine for web scale RDF data. Instead of managing the RDF data in triple stores or as bitmap matrices, we store RDF data in its native graph form. It achieves much better (sometimes orders of magnitude better) performance for SPARQL queries than the state-of-the-art approaches. Furthermore, since the data is stored in its native graph form, the system can support other operations (e.g., random walks, reachability) on RDF graphs as well. We conduct comprehensive experimental studies on real life, web scale RDF data to demonstrate the effectiveness of our approach.

#index 1992370
#* Upper and lower bounds on the cost of a map-reduce computation
#@ Anish Das Sarma;Foto N. Afrati;Semih Salihoglu;Jeffrey D. Ullman
#t 2013
#c 4
#% 323048
#% 481289
#% 847068
#% 963669
#% 1141493
#% 1426543
#% 1484141
#% 1542029
#% 1560415
#% 1581836
#% 1581925
#% 1594623
#% 1602032
#% 1693954
#% 1770321
#% 1846750
#! In this paper we study the tradeoff between parallelism and communication cost in a map-reduce computation. For any problem that is not "embarrassingly parallel," the finer we partition the work of the reducers so that more parallelism can be extracted, the greater will be the total communication between mappers and reducers. We introduce a model of problems that can be solved in a single round of map-reduce computation. This model enables a generic recipe for discovering lower bounds on communication cost as a function of the maximum number of inputs that can be assigned to one reducer. We use the model to analyze the tradeoff for three problems: finding pairs of strings at Hamming distance d, finding triangles and other patterns in a larger graph, and matrix multiplication. For finding strings of Hamming distance 1, we have upper and lower bounds that match exactly. For triangles and many other graphs, we have upper and lower bounds that are the same to within a constant factor. For the problem of matrix multiplication, we have matching upper and lower bounds for one-round map-reduce algorithms. We are also able to explore two-round map-reduce algorithms for matrix multiplication and show that these never have more communication, for a given reducer size, than the best one-round algorithm, and often have significantly less.

#index 1992371
#* Processing analytical queries over encrypted data
#@ Stephen Tu;M. Frans Kaashoek;Samuel Madden;Nickolai Zeldovich
#t 2013
#c 4
#% 397367
#% 480158
#% 664705
#% 956557
#% 1022245
#% 1063542
#% 1195803
#% 1198205
#% 1207101
#% 1386180
#% 1431619
#% 1581863
#% 1581871
#% 1616432
#% 1625037
#% 1716928
#! MONOMI is a system for securely executing analytical workloads over sensitive data on an untrusted database server. MONOMI works by encrypting the entire database and running queries over the encrypted data. MONOMI introduces split client/server query execution, which can execute arbitrarily complex queries over encrypted data, as well as several techniques that improve performance for such workloads, including per-row precomputation, space-efficient encryption, grouped homomorphic addition, and pre-filtering. Since these optimizations are good for some queries but not others, MONOMI introduces a designer for choosing an efficient physical design at the server for a given workload, and a planner to choose an efficient execution plan for a given query at runtime. A prototype of MONOMI running on top of Postgres can execute most of the queries from the TPC-H benchmark with a median overhead of only 1.24× (ranging from 1.03×to 2.33×) compared to an un-encrypted Postgres database where a compromised server would reveal all data.

#index 1992372
#* Practical differential privacy via grouping and smoothing
#@ Georgios Kellaris;Stavros Papadopoulos
#t 2013
#c 4
#% 248030
#% 874989
#% 881497
#% 937550
#% 963241
#% 1029084
#% 1206678
#% 1426322
#% 1426328
#% 1426454
#% 1426563
#% 1478165
#% 1496267
#% 1523886
#% 1581865
#% 1595893
#% 1604968
#% 1627567
#% 1670071
#% 1692264
#% 1732708
#% 1740518
#% 1818428
#% 1846817
#% 1880452
#! We address one-time publishing of non-overlapping counts with ε-differential privacy. These statistics are useful in a wide and important range of applications, including transactional, traffic and medical data analysis. Prior work on the topic publishes such statistics with prohibitively low utility in several practical scenarios. Towards this end, we present GS, a method that pre-processes the counts by elaborately grouping and smoothing them via averaging. This step acts as a form of preliminary perturbation that diminishes sensitivity, and enables GS to achieve ε-differential privacy through low Laplace noise injection. The grouping strategy is dictated by a sampling mechanism, which minimizes the smoothing perturbation. We demonstrate the superiority of GS over its competitors, and confirm its practicality, via extensive experiments on real datasets.

#index 1992373
#* On scaling up sensitive data auditing
#@ Raghav Kaushik;Yupeng Fu;Ravishankar Ramamurthy
#t 2013
#c 4
#% 461897
#% 765449
#% 864394
#% 864469
#% 874893
#% 976987
#% 1016172
#% 1016204
#% 1041196
#% 1206679
#% 1231247
#% 1246515
#% 1581829
#% 1581904
#% 1615075
#% 1730732
#! This paper studies the following problem: given (1) a query and (2) a set of sensitive records, find the subset of records "accessed" by the query. The notion of a query accessing a single record is adopted from prior work. There are several scenarios where the number of sensitive records is large (in the millions). The novel challenge addressed in this work is to develop a general-purpose solution for complex SQL that scales in the number of sensitive records. We propose efficient techniques that improves upon straightforward alternatives by orders of magnitude. Our empirical evaluation over the TPC-H benchmark data illustrates the benefits of our techniques.

#index 1992374
#* XORing elephants: novel erasure codes for big data
#@ Maheswaran Sathiamoorthy;Megasthenis Asteris;Dimitris Papailiopoulos;Alexandros G. Dimakis;Ramkumar Vadali;Scott Chen;Dhruba Borthakur
#t 2013
#c 4
#% 362897
#% 612167
#% 1084464
#% 1095874
#% 1164236
#% 1246354
#% 1350166
#% 1474041
#% 1475091
#% 1526977
#% 1591740
#% 1604232
#% 1625041
#% 1706230
#% 1765840
#% 1815569
#% 1816288
#% 1865583
#! Distributed storage systems for large clusters typically use replication to provide reliability. Recently, erasure codes have been used to reduce the large storage overhead of three-replicated systems. Reed-Solomon codes are the standard design choice and their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability. This paper shows how to overcome this limitation. We present a novel family of erasure codes that are efficiently repairable and offer higher reliability compared to Reed-Solomon codes. We show analytically that our codes are optimal on a recently identified tradeoff between locality and minimum distance. We implement our new codes in Hadoop HDFS and compare to a currently deployed HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation shows a reduction of approximately 2× on the repair disk I/O and repair network traffic. The disadvantage of the new coding scheme is that it requires 14% more storage compared to Reed-Solomon codes, an overhead shown to be information theoretically optimal to obtain locality. Because the new codes repair failures faster, this provides higher reliability, which is orders of magnitude higher compared to replication.

#index 1992375
#* Scaling factorization machines to relational data
#@ Steffen Rendle
#t 2013
#c 4
#% 727834
#% 850432
#% 1073982
#% 1083671
#% 1190066
#% 1211829
#% 1214666
#% 1232028
#% 1355024
#% 1451223
#% 1476452
#% 1535439
#% 1730808
#! The most common approach in predictive modeling is to describe cases with feature vectors (aka design matrix). Many machine learning methods such as linear regression or support vector machines rely on this representation. However, when the underlying data has strong relational patterns, especially relations with high cardinality, the design matrix can get very large which can make learning and prediction slow or even infeasible. This work solves this issue by making use of repeating patterns in the design matrix which stem from the underlying relational structure of the data. It is shown how coordinate descent learning and Bayesian Markov Chain Monte Carlo inference can be scaled for linear regression and factorization machine models. Empirically, it is shown on two large scale and very competitive datasets (Netflix prize, KDDCup 2012), that (1) standard learning algorithms based on the design matrix representation cannot scale to relational predictor variables, (2) the proposed new algorithms scale and (3) the predictive quality of the proposed generic feature-based approach is as good as the best specialized models that have been tailored to the respective tasks.

#index 2030461
#* Query optimization over crowdsourced data
#@ Hyunjung Park;Jennifer Widom
#t 2013
#c 4
#% 86947
#% 227894
#% 248014
#% 300169
#% 411554
#% 479452
#% 480788
#% 565457
#% 632048
#% 659918
#% 1426546
#% 1480225
#% 1526538
#% 1581851
#% 1628171
#% 1746876
#% 1746898
#% 1895078
#% 1895096
#% 1919728
#! Deco is a comprehensive system for answering declarative queries posed over stored relational data together with data obtained on-demand from the crowd. In this paper we describe Deco's cost-based query optimizer, building on Deco's data model, query language, and query execution engine presented earlier. Deco's objective in query optimization is to find the best query plan to answer a query, in terms of estimated monetary cost. Deco's query semantics and plan execution strategies require several fundamental changes to traditional query optimization. Novel techniques incorporated into Deco's query optimizer include a cost model distinguishing between "free" existing data versus paid new data, a cardinality estimation algorithm coping with changes to the database state during query execution, and a plan enumeration algorithm maximizing reuse of common subplans in a setting that makes reuse challenging. We experimentally evaluate Deco's query optimizer, focusing on the accuracy of cost estimation and the efficiency of plan enumeration.

#index 2030462
#* A data-adaptive and dynamic segmentation index for whole matching on time series
#@ Yang Wang;Peng Wang;Jian Pei;Wei Wang;Sheng Huang
#t 2013
#c 4
#% 172949
#% 248798
#% 333941
#% 427199
#% 460862
#% 480146
#% 577221
#% 631923
#% 662750
#% 765451
#% 844343
#% 876074
#% 881545
#% 893220
#% 1022238
#% 1083693
#% 1234328
#% 1535372
#! Similarity search on time series is an essential operation in many applications. In the state-of-the-art methods, such as the R-tree based methods, SAX and iSAX, time series are by default divided into equi-length segments globally, that is, all time series are segmented in the same way. Those methods then focus on how to approximate or symbolize the segments and construct indexes. In this paper, we make an important observation: global segmentation of all time series may incur unnecessary cost in space and time for indexing time series. We develop DSTree, a data adaptive and dynamic segmentation index on time series. In addition to savings in space and time, our new index can provide tight upper and lower bounds on distances between time series. An extensive empirical study shows that our new index DSTree supports time series similarity search effectively and efficiently.

#index 2030463
#* Extraction and integration of partially overlapping web sources
#@ Mirko Bronzi;Valter Crescenzi;Paolo Merialdo;Paolo Papotti
#t 2013
#c 4
#% 301241
#% 654458
#% 654469
#% 889107
#% 1022260
#% 1063547
#% 1077150
#% 1125351
#% 1127393
#% 1131164
#% 1265149
#% 1272181
#% 1275182
#% 1327643
#% 1328199
#% 1328200
#% 1333430
#% 1380965
#% 1409523
#% 1491640
#% 1523846
#% 1538764
#% 1594640
#% 1598411
#% 1741034
#% 1826065
#% 1959789
#! We present an unsupervised approach for harvesting the data exposed by a set of structured and partially overlapping data-intensive web sources. Our proposal comes within a formal framework tackling two problems: the data extraction problem, to generate extraction rules based on the input websites, and the data integration problem, to integrate the extracted data in a unified schema. We introduce an original algorithm, WEIR, to solve the stated problems and formally prove its correctness. WEIR leverages the overlapping data among sources to make better decisions both in the data extraction (by pruning rules that do not lead to redundant information) and in the data integration (by reflecting local properties of a source over the mediated schema). Along the way, we characterize the amount of redundancy needed by our algorithm to produce a solution, and present experimental results to show the benefits of our approach with respect to existing solutions.

#index 2030464
#* The Yin and Yang of processing data warehousing queries on GPU devices
#@ Yuan Yuan;Rubao Lee;Xiaodong Zhang
#t 2013
#c 4
#% 190611
#% 765419
#% 874997
#% 875026
#% 1016214
#% 1063508
#% 1063542
#% 1206754
#% 1268643
#% 1270566
#% 1328132
#% 1426531
#% 1523855
#% 1550752
#% 1581849
#% 1592339
#% 1601116
#% 1789651
#% 1789653
#% 1853900
#% 1880468
#% 1967103
#% 2010440
#! Database community has made significant research efforts to optimize query processing on GPUs in the past few years. However, we can hardly find that GPUs have been truly adopted in major warehousing production systems. Preparing to merge GPUs to the warehousing systems, we have identified and addressed several critical issues in a three-dimensional study of warehousing queries on GPUs by varying query characteristics, software techniques, and GPU hardware configurations. We also propose an analytical model to understand and predict the query performance on GPUs. Based on our study, we present our performance insights for warehousing query execution on GPUs. The objective of our work is to provide a comprehensive guidance for GPU architects, software system designers, and database practitioners to narrow the speed gap between the GPU kernel execution (the fast mode) and data transfer to prepare GPU execution (the slow mode) for high performance in processing data warehousing queries. The GPU query engine developed in this work is open source to the public.

#index 2030465
#* Mining and indexing graphs for supergraph search
#@ Dayu Yuan;Prasenjit Mitra;C. Lee Giles
#t 2013
#c 4
#% 397371
#% 629708
#% 765429
#% 960305
#% 1022279
#% 1127380
#% 1174743
#% 1181230
#% 1292522
#% 1466321
#% 1486251
#% 1523835
#% 1615774
#% 1846701
#% 1968410
#! We study supergraph search (SPS), that is, given a query graph q and a graph database G that contains a collection of graphs , return graphs that have q as a supergraph from G. SPS has broad applications in bioinformatics, cheminformatics and other scientific and commercial fields. Determining whether a graph is a subgraph (or supergraph) of another is an NP-complete problem. Hence, it is intractable to compute SPS for large graph databases. Two separate indexing methods, a "filter + verify"-based method and a "prefix-sharing"-based method, have been studied to efficiently compute SPS. To implement the above two methods, subgraph patterns are mined from the graph database to build an index. Those subgraphs are mined to optimize either the filtering gain or the prefix-sharing gain. However, no single subgraph-mining algorithm considers both gains. This work is the first one to mine subgraphs to optimize both the filtering gain and the prefix-sharing gain while processing SPS queries. First, we show that the subgraph-mining problem is NP-hard. Then, we propose two polynomial-time algorithms to solve the problem with an approximation ratio of 1-1/e and 1/4 respectively. In addition, we construct a lattice-like index, LW-index, to organize the selected subgraph patterns for fast index-lookup. Our experiments show that our approach improves the query processing time for SPS queries by a factor of 3 to 10.

#index 2030466
#* Efficient recovery of missing events
#@ Jianmin Wang;Shaoxu Song;Xiaochen Zhu;Xuemin Lin
#t 2013
#c 4
#% 102787
#% 106829
#% 184552
#% 273687
#% 443493
#% 810019
#% 833660
#% 960293
#% 1050674
#% 1063545
#% 1180000
#% 1194938
#% 1206571
#% 1207025
#% 1217186
#% 1217188
#% 1304656
#% 1382156
#% 1426561
#% 1588231
#% 1826156
#% 1905337
#% 1932118
#! For various entering and transmission issues raised by human or system, missing events often occur in event data, which record execution logs of business processes. Without recovering these missing events, applications such as provenance analysis or complex event processing built upon event data are not reliable. Following the minimum change discipline in improving data quality, it is also rational to find a recovery that minimally differs from the original data. Existing recovery approaches fall short of efficiency owing to enumerating and searching over all the possible sequences of events. In this paper, we study the efficient techniques for recovering missing events. According to our theoretical results, the recovery problem is proved to be NP-hard. Nevertheless, we are able to concisely represent the space of event sequences in a branching framework. Advanced indexing and pruning techniques are developed to further improve the recovery efficiency. Our proposed efficient techniques make it possible to find top-k recoveries. The experimental results demonstrate that our minimum recovery approach achieves high accuracy, and significantly outperforms the state-of-the-art technique for up to 5 orders of magnitudes improvement in time performance.

#index 2030467
#* Hadoop's adolescence: an analysis of Hadoop usage in scientific workloads
#@ Kai Ren;YongChul Kwon;Magdalena Balazinska;Bill Howe
#t 2013
#c 4
#% 963669
#% 1063553
#% 1217159
#% 1217165
#% 1318636
#% 1426488
#% 1426513
#% 1459262
#% 1526991
#% 1583710
#% 1621127
#% 1639648
#% 1741027
#% 1770321
#% 1783392
#% 1798411
#% 1813854
#% 1895061
#! We analyze Hadoop workloads from three di?erent research clusters from a user-centric perspective. The goal is to better understand data scientists' use of the system and how well the use of the system matches its design. Our analysis suggests that Hadoop usage is still in its adolescence. We see underuse of Hadoop features, extensions, and tools. We see significant diversity in resource usage and application styles, including some interactive and iterative workloads, motivating new tools in the ecosystem. We also observe significant opportunities for optimizations of these workloads. We find that job customization and configuration are used in a narrow scope, suggesting the future pursuit of automatic tuning systems. Overall, we present the first user-centered measurement study of Hadoop and find significant opportunities for improving its efficient use for data scientists.

#index 2030468
#* RACE: a scalable and elastic parallel system for discovering repeats in very long sequences
#@ Essam Mansour;Ahmed El-Roby;Panos Kalnis;Aron Ahmadia;Ashraf Aboulnaga
#t 2013
#c 4
#% 55284
#% 235941
#% 289010
#% 451770
#% 546453
#% 751138
#% 960303
#% 1210834
#% 1217209
#% 1267641
#% 1411895
#% 1491241
#% 1628174
#% 1692217
#% 1734184
#% 1939320
#! A wide range of applications, including bioinformatics, time series, and log analysis, depend on the identification of repetitions in very long sequences. The problem of finding maximal pairs subsumes most important types of repetition-finding tasks. Existing solutions require both the input sequence and its index (typically an order of magnitude larger than the input) to fit in memory. Moreover, they are serial algorithms with long execution time. Therefore, they are limited to small datasets, despite the fact that modern applications demand orders of magnitude longer sequences. In this paper we present RACE, a parallel system for finding maximal pairs in very long sequences. RACE supports parallel execution on stand-alone multicore systems, in addition to scaling to thousands of nodes on clusters or supercomputers. RACE does not require the input or the index to fit in memory; therefore, it supports very long sequences with limited memory. Moreover, it uses a novel array representation that allows for cache-efficient implementation. RACE is particularly suitable for the cloud (e.g., Amazon EC2) because, based on availability, it can scale elastically to more or fewer machines during its execution. Since scaling out introduces overheads, mainly due to load imbalance, we propose a cost model to estimate the expected speedup, based on statistics gathered through sampling. The model allows the user to select the appropriate combination of cloud resources based on the provider's prices and the required deadline. We conducted extensive experimental evaluation with large real datasets and large computing infrastructures. In contrast to existing methods, RACE can handle the entire human genome on a typical desktop computer with 16GB RAM. Moreover, for a problem that takes 10 hours of serial execution, RACE finishes in 28 seconds using 2,048 nodes on an IBM BlueGene/P supercomputer.

#index 2030469
#* LLAMA: a cache/storage subsystem for modern hardware
#@ Justin Levandoski;David Lomet;Sudipta Sengupta
#t 2013
#c 4
#% 735
#% 47623
#% 116087
#% 131555
#% 152943
#% 208047
#% 287647
#% 287797
#% 317933
#% 480119
#% 567865
#% 960238
#% 1063543
#% 1196583
#% 1581848
#% 1606343
#% 1668635
#% 1770319
#% 1770337
#% 1971522
#% 2010397
#! LLAMA is a subsystem designed for new hardware environments that supports an API for page-oriented access methods, providing both cache and storage management. Caching (CL) and storage (SL) layers use a common mapping table that separates a page's logical and physical location. CL supports data updates and management updates (e.g., for index re-organization) via latch-free compare-and-swap atomic state changes on its mapping table. SL uses the same mapping table to cope with page location changes produced by log structuring on every page flush. To demonstrate LLAMA's suitability, we tailored our latch-free Bw-tree implementation to use LLAMA. The Bw-tree is a B-tree style index. Layered on LLAMA, it has higher performance and scalability using real workloads compared with BerkeleyDB's B-tree, which is known for good performance.

#index 2030470
#* Revisiting co-processing for hash joins on the coupled CPU-GPU architecture
#@ Jiong He;Mian Lu;Bingsheng He
#t 2013
#c 4
#% 58352
#% 115661
#% 152915
#% 479821
#% 480119
#% 480464
#% 480943
#% 566122
#% 824697
#% 850738
#% 983261
#% 993387
#% 993947
#% 1052063
#% 1054482
#% 1063508
#% 1270566
#% 1328057
#% 1467054
#% 1523855
#% 1550752
#% 1581849
#% 1581898
#% 1641810
#% 1644839
#% 1789653
#% 1847302
#% 1912162
#% 2010440
#! Query co-processing on graphics processors (GPUs) has become an effective means to improve the performance of main memory databases. However, the relatively low bandwidth and high latency of the PCI-e bus are usually bottleneck issues for co-processing. Recently, coupled CPU-GPU architectures have received a lot of attention, e.g. AMD APUs with the CPU and the GPU integrated into a single chip. That opens up new opportunities for optimizing query co-processing. In this paper, we experimentally revisit hash joins, one of the most important join algorithms for main memory databases, on a coupled CPU-GPU architecture. Particularly, we study the fine-grained co-processing mechanisms on hash joins with and without partitioning. The co-processing outlines an interesting design space. We extend existing cost models to automatically guide decisions on the design space. Our experimental results on a recent AMD APU show that (1) the coupled architecture enables fine-grained co-processing and cache reuses, which are inefficient on discrete CPU-GPU architectures; (2) the cost model can automatically guide the design and tuning knobs in the design space; (3) fine-grained co-processing achieves up to 53%, 35% and 28% performance improvement over CPU-only, GPU-only and conventional CPU-GPU co-processing, respectively. We believe that the insights and implications from this study are initial yet important for further research on query co-processing on coupled CPU-GPU architectures.

#index 2030471
#* Top-K nearest keyword search on large graphs
#@ Miao Qiao;Lu Qin;Hong Cheng;Jeffrey Xu Yu;Wentao Tian
#t 2013
#c 4
#% 338382
#% 498538
#% 660011
#% 824693
#% 874993
#% 960259
#% 993987
#% 1016199
#% 1063472
#% 1063537
#% 1063539
#% 1207007
#% 1211643
#% 1355056
#% 1464046
#% 1581895
#% 1606349
#% 1611383
#% 1641963
#% 1667211
#% 1746838
#% 1880434
#! It is quite common for networks emerging nowadays to have labels or textual contents on the nodes. On such networks, we study the problem of top-k nearest keyword (k-NK) search. In a network G modeled as an undirected graph, each node is attached with zero or more keywords, and each edge is assigned with a weight measuring its length. Given a query node q in G and a keyword λ, a k-NK query seeks k nodes which contain λ and are nearest to q. k-NK is not only useful as a stand-alone query but also as a building block for tackling complex graph pattern matching problems. The key to an accurate k-NK result is a precise shortest distance estimation in a graph. Based on the latest distance oracle technique, we build a shortest path tree for a distance oracle and use the tree distance as a more accurate estimation. With such representation, the original k-NK query on a graph can be reduced to answering the query on a set of trees and then assembling the results obtained from the trees. We propose two efficient algorithms to report the exact k-NK result on a tree. One is query time optimized for a scenario when a small number of result nodes are of interest to users. The other handles k-NK queries for an arbitrarily large k efficiently. In obtaining a k-NK result on a graph from that on trees, a global storage technique is proposed to further reduce the index size and the query time. Extensive experimental results conform with our theoretical findings, and demonstrate the effectiveness and efficiency of our k-NK algorithms on large real graphs.

#index 2030472
#* A general framework for geo-social query processing
#@ Nikos Armenatzoglou;Stavros Papadopoulos;Dimitris Papadias
#t 2013
#c 4
#% 810061
#% 814650
#% 996335
#% 1002007
#% 1015321
#% 1281823
#% 1281998
#% 1475162
#% 1478995
#% 1523883
#% 1524237
#% 1572969
#% 1606049
#% 1806273
#% 1846818
#% 1872344
#% 1919753
#! The proliferation of GPS-enabledmobile devises and the popularity of social networking have recently led to the rapid growth of Geo-Social Networks (GeoSNs). GeoSNs have created a fertile ground for novel location-based social interactions and advertising. These can be facilitated by GeoSN queries, which extract useful information combining both the social relationships and the current location of the users. This paper constitutes the first systematic work on GeoSN query processing. We propose a general framework that offers flexible data management and algorithmic design. Our architecture segregates the social, geographical and query processing modules. Each GeoSN query is processed via a transparent combination of primitive queries issued to the social and geographical modules. We demonstrate the power of our framework by introducing several "basic" and "advanced" query types, and devising various solutions for each type. Finally, we perform an exhaustive experimental evaluation with real and synthetic datasets, based on realistic implementations with both commercial software (such as MongoDB) and state-of-the-art research methods. Our results confirm the viability of our framework in typical large-scale GeoSNs.

#index 2030473
#* Towards predicting query execution time for concurrent and dynamic database workloads
#@ Wentao Wu;Yun Chi;Hakan Hacígümüş;Jeffrey F. Naughton
#t 2013
#c 4
#% 1252
#% 118647
#% 136740
#% 289242
#% 448194
#% 722887
#% 755976
#% 765467
#% 765468
#% 783936
#% 993447
#% 1174742
#% 1182933
#% 1206909
#% 1206984
#% 1301004
#% 1549874
#% 1581874
#% 1618129
#% 1846730
#% 1880469
#% 2010349
#! Predicting query execution time is crucial for many database management tasks including admission control, query scheduling, and progress monitoring. While a number of recent papers have explored this problem, the bulk of the existing work either considers prediction for a single query, or prediction for a static workload of concurrent queries, where by "static" we mean that the queries to be run are fixed and known. In this paper, we consider the more general problem of dynamic concurrent workloads. Unlike most previous work on query execution time prediction, our proposed framework is based on analytic modeling rather than machine learning. We first use the optimizer's cost model to estimate the I/O and CPU requirements for each pipeline of each query in isolation, and then use a combination queueing model and buffer pool model that merges the I/O and CPU requests from concurrent queries to predict running times. We compare the proposed approach with a machine-learning based approach that is a variant of previous work. Our experiments show that our analytic-model based approach can lead to competitive and often better prediction accuracy than its machine-learning based counterpart.

#index 2030474
#* Sketch-based geometric monitoring of distributed stream queries
#@ Minos Garofalakis;Daniel Keren;Vasilis Samoladas
#t 2013
#c 4
#% 214073
#% 273682
#% 333931
#% 397354
#% 397385
#% 480805
#% 492912
#% 576119
#% 578390
#% 654443
#% 654463
#% 654482
#% 654488
#% 654497
#% 765402
#% 800582
#% 801695
#% 810009
#% 816392
#% 864435
#% 874994
#% 874995
#% 960368
#% 993960
#% 993969
#% 1016155
#% 1016178
#% 1054483
#% 1688247
#% 1770341
#% 1848066
#! Emerging large-scale monitoring applications rely on continuous tracking of complex data-analysis queries over collections of massive, physically-distributed data streams. Thus, in addition to the space- and time-efficiency requirements of conventional stream processing (at each remote monitor site), effective solutions also need to guarantee communication efficiency (over the underlying communication network). The complexity of the monitored query adds to the difficulty of the problem -- this is especially true for nonlinear queries (e.g., joins), where no obvious solutions exist for distributing the monitor condition across sites. The recently proposed geometric method offers a generic methodology for splitting an arbitrary (non-linear) global threshold-monitoring task into a collection of local site constraints; still, the approach relies on maintaining the complete stream(s) at each site, thus raising serious efficiency concerns for massive data streams. In this paper, we propose novel algorithms for efficiently tracking a broad class of complex aggregate queries in such distributed-streams settings. Our tracking schemes rely on a novel combination of the geometric method with compact sketch summaries of local data streams, and maintain approximate answers with provable error guarantees, while optimizing space and processing costs at each remote site and communication cost across the network. One of our key technical insights for the effective use of the geometric method lies in exploiting a much lower-dimensional space for monitoring the sketch-based estimation query. Due to the complex, highly nonlinear nature of these estimates, efficiently monitoring the local geometric constraints poses challenging algorithmic issues for which we propose novel solutions. Experimental results on real-life data streams verify the effectiveness of our approach.

#index 2030475
#* Direction-preserving trajectory simplification
#@ Cheng Long;Raymond Chi-Wing Wong;H. V. Jagadish
#t 2013
#c 4
#% 775840
#% 824722
#% 879210
#% 885377
#% 960283
#% 982595
#% 989604
#% 1127437
#% 1206639
#% 1244907
#% 1278580
#% 1407169
#% 1589342
#% 1605948
#% 1643282
#% 1846707
#% 1846721
#% 1974246
#! Trajectories of moving objects are collected in many applications. Raw trajectory data is typically very large, and has to be simplified before use. In this paper, we introduce the notion of direction-preserving trajectory simplification, and show both analytically and empirically that it can support a broader range of applications than traditional position-preserving trajectory simplification. We present a polynomial-time algorithm for optimal direction-preserving simplification, and another approximate algorithm with a quality guarantee. Extensive experimental evaluation with real trajectory data shows the benefit of the new techniques.

#index 2030476
#* Continuous cloud-scale query optimization and processing
#@ Nicolas Bruno;Sapna Jain;Jingren Zhou
#t 2013
#c 4
#% 58375
#% 172900
#% 248793
#% 300167
#% 480803
#% 765456
#% 963669
#% 983467
#% 1063553
#% 1426486
#% 1468421
#% 1523824
#% 1594630
#% 1770416
#% 1783393
#% 1880439
#% 1910909
#! Massive data analysis in cloud-scale data centers plays a crucial role in making critical business decisions. High-level scripting languages free developers from understanding various system trade-offs, but introduce new challenges for query optimization. One key optimization challenge is missing accurate data statistics, typically due to massive data volumes and their distributed nature, complex computation logic, and frequent usage of user-defined functions. In this paper we propose novel techniques to adapt query processing in the Scope system, the cloud-scale computation environment in Microsoft Online Services. We continuously monitor query execution, collect actual runtime statistics, and adapt parallel execution plans as the query executes. We discuss similarities and differences between our approach and alternatives proposed in the context of traditional centralized systems. Experiments on large-scale Scope production clusters show that the proposed techniques systematically solve the challenge of missing/inaccurate data statistics, detect and resolve partition skew and plan structure, and improve query latency by a few folds for real workloads. Although we focus on optimizing high-level languages, the same ideas are also applicable for MapReduce systems.

#index 2030477
#* Optimization strategies for A/B testing on HADOOP
#@ Andrii Cherniak;Huma Zaidi;Vladimir Zadorozhny
#t 2013
#c 4
#% 115661
#% 480966
#% 1023420
#% 1154062
#% 1386049
#% 1451140
#% 1518201
#% 1542029
#% 1588764
#% 1608685
#% 1635071
#% 1770415
#% 1800672
#% 1846596
#% 1898201
#% 1902718
#! In this work, we present a set of techniques that considerably improve the performance of executing concurrent MapReduce jobs. Our proposed solution relies on proper resource allocation for concurrent Hive jobs based on data dependency, inter-query optimization and modeling of Hadoop cluster load. To the best of our knowledge, this is the first work towards Hive/MapReduce job optimization which takes Hadoop cluster load into consideration. We perform an experimental study that demonstrates 233% reduction in execution time for concurrent vs sequential execution schema. We report up to 40% extra reduction in execution time for concurrent job execution after resource usage optimization. The results reported in this paper were obtained in a pilot project to assess the feasibility of migrating A/B testing from Teradata + SAS analytics infrastructure to Hadoop. This work was performed on eBay production Hadoop cluster.

#index 2030478
#* Piranha: optimizing short jobs in Hadoop
#@ Khaled Elmeleegy
#t 2013
#c 4
#% 723279
#% 954300
#% 963669
#% 1063553
#% 1217232
#% 1278391
#% 1328095
#% 1328186
#% 1386049
#% 1468411
#% 1468421
#% 1468423
#% 1468530
#% 1523837
#% 1523841
#% 1523924
#% 1567910
#% 1567923
#% 1594630
#% 1972762
#! Cluster computing has emerged as a key parallel processing platform for large scale data. All major internet companies use it as their major central processing platform. One of cluster computing's most popular examples is MapReduce and its open source implementation Hadoop. These systems were originally designed for batch and massive-scale computations. Interestingly, over time their production workloads have evolved into a mix of a small fraction of large and long-running jobs and a much bigger fraction of short jobs. This came about because these systems end up being used as data warehouses, which store most of the data sets and attract ad hoc, short, data-mining queries. Moreover, the availability of higher level query languages that operate on top of these cluster systems proliferated these ad hoc queries. Since existing systems were not designed for short, latency-sensistive jobs, short interactive jobs suffer from poor response times. In this paper, we present Piranha--a system for optimizing short jobs on Hadoop without affecting the larger jobs. It runs on existing unmodified Hadoop clusters facilitating its adoption. Piranha exploits characteristics of short jobs learned from production workloads at Yahoo! clusters to reduce the latency of such jobs. To demonstrate Piranha's effectiveness, we evaluated its performance using three realistic short queries. Piranha was able to reduce the queries' response times by up to 71%.

#index 2030479
#* Making updates disk-I/O friendly using SSDs
#@ Mohammad Sadoghi;Kenneth A. Ross;Mustafa Canim;Bishwaranjan Bhattacharjee
#t 2013
#c 4
#% 104926
#% 131555
#% 208047
#% 287070
#% 402709
#% 443390
#% 452782
#% 463749
#% 481599
#% 482100
#% 562809
#% 571296
#% 810114
#% 978174
#% 985755
#% 1019164
#% 1022303
#% 1053457
#% 1124990
#% 1127420
#% 1146355
#% 1207002
#% 1328052
#% 1328064
#% 1328139
#% 1348386
#% 1523922
#% 1557839
#% 1581918
#% 1581941
#% 1586198
#% 1668635
#% 1770337
#% 1869835
#% 1880474
#% 2010397
#! Multiversion databases store both current and historical data. Rows are typically annotated with timestamps representing the period when the row is/was valid. We develop novel techniques for reducing index maintenance in multiversion databases, so that indexes can be used effectively for analytical queries over current data without being a heavy burden on transaction throughput. To achieve this end, we re-design persistent index data structures in the storage hierarchy to employ an extra level of indirection. The indirection level is stored on solid state disks that can support very fast random I/Os, so that traversing the extra level of indirection incurs a relatively small overhead. The extra level of indirection dramatically reduces the number of magnetic disk I/Os that are needed for index updates, and localizes maintenance to indexes on updated attributes. Further, we batch insertions within the indirection layer in order to reduce physical disk I/Os for indexing new records. By reducing the index maintenance overhead on transactions, we enable operational data stores to create more indexes to support queries. We have developed a prototype of our indirection proposal by extending the widely used Generalized Search Tree (GiST) open-source project, which is also employed in PostgreSQL. Our working implementation demonstrates that we can significantly reduce index maintenance and/or query processing cost, by a factor of 3. For insertions of new records, our novel batching technique can save up to 90% of the insertion time.

#index 2030480
#* Hadoop GIS: a high performance spatial data warehousing system over mapreduce
#@ Ablimit Aji;Fusheng Wang;Hoang Vo;Rubao Lee;Qiaoling Liu;Xiaodong Zhang;Joel Saltz
#t 2013
#c 4
#% 86950
#% 210186
#% 227934
#% 421050
#% 464205
#% 480651
#% 481455
#% 1063553
#% 1127559
#% 1158543
#% 1217159
#% 1218736
#% 1278123
#% 1278124
#% 1328060
#% 1328095
#% 1328186
#% 1426583
#% 1426584
#% 1532900
#% 1601116
#% 1770148
#% 1880468
#% 1887019
#% 1941013
#% 1962322
#! Support of high performance queries on large volumes of spatial data becomes increasingly important in many application domains, including geospatial problems in numerous fields, location based services, and emerging scientific applications that are increasingly data- and compute-intensive. The emergence of massive scale spatial data is due to the proliferation of cost effective and ubiquitous positioning technologies, development of high resolution imaging technologies, and contribution from a large number of community users. There are two major challenges for managing and querying massive spatial data to support spatial queries: the explosion of spatial data, and the high computational complexity of spatial queries. In this paper, we present Hadoop-GIS - a scalable and high performance spatial data warehousing system for running large scale spatial queries on Hadoop. Hadoop-GIS supports multiple types of spatial queries on MapReduce through spatial partitioning, customizable spatial query engine RESQUE, implicit parallel spatial query execution on MapReduce, and effective methods for amending query results through handling boundary objects. Hadoop-GIS utilizes global partition indexing and customizable on demand local spatial indexing to achieve efficient query processing. Hadoop-GIS is integrated into Hive to support declarative spatial queries with an integrated architecture. Our experiments have demonstrated the high efficiency of Hadoop-GIS on query response and high scalability to run on commodity clusters. Our comparative experiments have showed that performance of Hadoop-GIS is on par with parallel SDBMS and outperforms SDBMS for compute-intensive queries. Hadoop-GIS is available as a set of library for processing spatial queries, and as an integrated software package in Hive.

#index 2030481
#* Statistics collection in oracle spatial and graph: fast histogram construction for complex geometry objects
#@ Bhuvan Bamba;Siva Ravada;Ying Hu;Richard Anderson
#t 2013
#c 4
#% 43163
#% 82346
#% 86950
#% 104472
#% 153260
#% 210190
#% 273887
#% 273941
#% 300193
#% 333947
#% 397396
#% 427199
#% 464062
#% 464876
#% 479648
#% 480093
#% 480805
#% 481749
#% 527004
#% 617867
#% 632072
#% 745487
#% 864426
#% 1015256
#% 1016225
#% 1019108
#% 1063549
#% 1426554
#! Oracle Spatial and Graph is a geographic information system (GIS) which provides users the ability to store spatial data alongside conventional data in Oracle. As a result of the coexistence of spatial and other data, we observe a trend towards users performing increasingly complex queries which involve spatial as well as non-spatial predicates. Accurate selectivity values, especially for queries with multiple predicates requiring joins among numerous tables, are essential for the database optimizer to determine a good execution plan. For queries involving spatial predicates, this requires that reasonably accurate statistics collection has been performed on the spatial data. For extensible data cartridges such as Oracle Spatial and Graph, the optimizer expects to receive accurate predicate selectivity and cost values from functions implemented within the data cartridge. Although statistics collection for spatial data has been researched in academia for a few years; to the best of our knowledge, this is the first work to present spatial statistics collection implementation details for a commercial GIS database. In this paper, we describe our experiences with implementation of statistics collection methods for complex geometry objects within Oracle Spatial and Graph. Firstly, we exemplify issues with previous partitioning-based algorithms in presence of complex geometry objects and suggest enhancements which resolve the issues. Secondly, we propose a main memory implementation which not only speeds up the disk-based partitioning algorithms but also utilizes existing R-tree indexes to provide surprisingly accurate selectivity estimates. Last but not the least, we provide extensive experimental results and an example study which displays the efficacy of our approach on Oracle query performance.

#index 2030482
#* MillWheel: fault-tolerant stream processing at internet scale
#@ Tyler Akidau;Alex Balikov;Kaya Bekiroğlu;Slava Chernyak;Josh Haberman;Reuven Lax;Sam McVeety;Daniel Mills;Paul Nordstrom;Sam Whittle
#t 2013
#c 4
#% 1842
#% 320187
#% 378388
#% 397414
#% 578391
#% 654510
#% 660004
#% 726621
#% 800583
#% 801694
#% 824742
#% 845352
#% 1023420
#% 1054227
#% 1127373
#% 1127396
#% 1426479
#% 1468391
#% 1526990
#% 1535212
#% 1567910
#% 1783374
#% 1874962
#% 1911326
#! MillWheel is a framework for building low-latency data-processing applications that is widely used at Google. Users specify a directed computation graph and application code for individual nodes, and the system manages persistent state and the continuous flow of records, all within the envelope of the framework's fault-tolerance guarantees. This paper describes MillWheel's programming model as well as its implementation. The case study of a continuous anomaly detector in use at Google serves to motivate how many of MillWheel's features are used. MillWheel's programming model provides a notion of logical time, making it simple to write time-based aggregations. MillWheel was designed from the outset with fault tolerance and scalability in mind. In practice, we find that MillWheel's unique combination of scalability, fault tolerance, and a versatile programming model lends itself to a wide variety of problems at Google.

#index 2030483
#* Online, asynchronous schema change in F1
#@ Ian Rae;Eric Rollins;Jeff Shute;Sukhdeep Sodhi;Radek Vingralek
#t 2013
#c 4
#% 125956
#% 126704
#% 287303
#% 301084
#% 336201
#% 449395
#% 487502
#% 488624
#% 571217
#% 588591
#% 632041
#% 1022349
#% 1023420
#% 1044442
#% 1107610
#% 1206869
#% 1523975
#% 1526990
#% 1531204
#% 1688269
#% 1911326
#! We introduce a protocol for schema evolution in a globally distributed database management system with shared data, stateless servers, and no global membership. Our protocol is asynchronous--it allows different servers in the database system to transition to a new schema at different times--and online--all servers can access and update all data during a schema change. We provide a formal model for determining the correctness of schema changes under these conditions, and we demonstrate that many common schema changes can cause anomalies and database corruption. We avoid these problems by replacing corruption-causing schema changes with a sequence of schema changes that is guaranteed to avoid corrupting the database so long as all servers are no more than one schema version behind at any time. Finally, we discuss a practical implementation of our protocol in F1, the database management system that stores data for Google AdWords.

#index 2030484
#* Scuba: diving into data at facebook
#@ Lior Abraham;John Allen;Oleksandr Barykin;Vinayak Borkar;Bhuwan Chopra;Ciprian Gerea;Daniel Merl;Josh Metzler;David Reiss;Subbu Subramanian;Janet L. Wiener;Okay Zed
#t 2013
#c 4
#% 18615
#% 36227
#% 322412
#% 824697
#% 1089604
#% 1328095
#% 1523824
#% 1594617
#% 1770395
#% 1770406
#% 1864659
#% 1880459
#! Facebook takes performance monitoring seriously. Performance issues can impact over one billion users so we track thousands of servers, hundreds of PB of daily network traffic, hundreds of daily code changes, and many other metrics. We require latencies of under a minute from events occuring (a client request on a phone, a bug report filed, a code change checked in) to graphs showing those events on developers' monitors. Scuba is the data management system Facebook uses for most real-time analysis. Scuba is a fast, scalable, distributed, in-memory database built at Facebook. It currently ingests millions of rows (events) per second and expires data at the same rate. Scuba stores data completely in memory on hundreds of servers each with 144 GB RAM. To process each query, Scuba aggregates data from all servers. Scuba processes almost a million queries per day. Scuba is used extensively for interactive, ad hoc, analysis queries that run in under a second over live data. In addition, Scuba is the workhorse behind Facebook's code regression analysis, bug report monitoring, ads revenue monitoring, and performance debugging.

#index 2030485
#* F1: a distributed SQL database that scales
#@ Jeff Shute;Radek Vingralek;Bart Samwel;Ben Handy;Chad Whipkey;Eric Rollins;Mircea Oancea;Kyle Littlefield;David Menestrina;Stephan Ellner;John Cieslewicz;Ian Rae;Traian Stancescu;Himani Apte
#t 2013
#c 4
#% 42401
#% 125598
#% 201869
#% 251359
#% 300169
#% 307360
#% 320902
#% 336201
#% 442700
#% 963669
#% 978404
#% 998845
#% 1357681
#% 1426474
#% 1526990
#% 1770412
#% 1911326
#% 1957296
#% 1972829
#! F1 is a distributed relational database system built at Google to support the AdWords business. F1 is a hybrid database that combines high availability, the scalability of NoSQL systems like Bigtable, and the consistency and usability of traditional SQL databases. F1 is built on Spanner, which provides synchronous cross-datacenter replication and strong consistency. Synchronous replication implies higher commit latency, but we mitigate that latency by using a hierarchical schema model with structured data types and through smart application design. F1 also includes a fully functional distributed SQL query engine and automatic change tracking and publishing.

#index 2030486
#* DB2 with BLU acceleration: so much more than just a column store
#@ Vijayshankar Raman;Gopi Attaluri;Ronald Barber;Naresh Chainani;David Kalmuk;Vincent KulandaiSamy;Jens Leenstra;Sam Lightstone;Shaorong Liu;Guy M. Lohman;Tim Malkemus;Rene Mueller;Ippokratis Pandis;Berni Schiefer;David Sharpe;Richard Sidle;Adam Storm;Liping Zhang
#t 2013
#c 4
#% 114582
#% 286258
#% 411554
#% 427195
#% 442706
#% 480821
#% 824697
#% 960266
#% 1089604
#% 1127400
#% 1206624
#% 1328141
#% 1869840
#% 1971520
#! DB2 with BLU Acceleration deeply integrates innovative new techniques for defining and processing column-organized tables that speed read-mostly Business Intelligence queries by 10 to 50 times and improve compression by 3 to 10 times, compared to traditional row-organized tables, without the complexity of defining indexes or materialized views on those tables. But DB2 BLU is much more than just a column store. Exploiting frequency-based dictionary compression and main-memory query processing technology from the Blink project at IBM Research - Almaden, DB2 BLU performs most SQL operations - predicate application (even range predicates and IN-lists), joins, and grouping - on the compressed values, which can be packed bit-aligned so densely that multiple values fit in a register and can be processed simultaneously via SIMD (single-instruction, multipledata) instructions. Designed and built from the ground up to exploit modern multi-core processors, DB2 BLU's hardware-conscious algorithms are carefully engineered to maximize parallelism by using novel data structures that need little latching, and to minimize data-cache and instruction-cache misses. Though DB2 BLU is optimized for in-memory processing, database size is not limited by the size of main memory. Fine-grained synopses, late materialization, and a new probabilistic buffer pool protocol for scans minimize disk I/Os, while aggressive prefetching reduces I/O stalls. Full integration with DB2 ensures that DB2 with BLU Acceleration benefits from the full functionality and robust utilities of a mature product, while still enjoying order-of-magnitude performance gains from revolutionary technology without even having to change the SQL, and can mix column-organized and row-organized tables in the same tablespace and even within the same query.

#index 2030487
#* The quantcast file system
#@ Michael Ovsiannikov;Silvius Rus;Damian Reeves;Paul Sutter;Sriram Rao;Jim Kelly
#t 2013
#c 4
#% 723279
#% 963669
#% 1518201
#% 1911309
#% 1913796
#! The Quantcast File System (QFS) is an efficient alternative to the Hadoop Distributed File System (HDFS). QFS is written in C++, is plugin compatible with Hadoop MapReduce, and offers several efficiency improvements relative to HDFS: 50% disk space savings through erasure coding instead of replication, a resulting doubling of write throughput, a faster name node, support for faster sorting and logging through a concurrent append feature, a native command line client much faster than hadoop fs, and global feedback-directed I/O device management. As QFS works out of the box with Hadoop, migrating data from HDFS to QFS involves simply executing hadoop distcp. QFS is being developed fully open source and is available under an Apache license from https://github.com/quantcast/qfs. Multi-petabyte QFS instances have been in heavy production use since 2011.

#index 2030488
#* Adaptive and big data scale parallel execution in oracle
#@ Srikanth Bellamkonda;Hua-Gang Li;Unmesh Jagtap;Yali Zhu;Vince Liang;Thierry Cruanes
#t 2013
#c 4
#% 210182
#% 420053
#% 481951
#% 654498
#% 742057
#% 893173
#% 1063548
#% 1328056
#% 1328057
#% 1586197
#% 1880443
#! This paper showcases some of the newly introduced parallel execution methods in Oracle RDBMS. These methods provide highly scalable and adaptive evaluation for the most commonly used SQL operations - joins, group-by, rollup/cube, grouping sets, and window functions. The novelty of these techniques is their use of multi-stage parallelization models, accommodation of optimizer mistakes, and the runtime parallelization and data distribution decisions. These parallel plans adapt based on the statistics gathered on the real data at query execution time. We realized enormous performance gains from these adaptive parallelization techniques. The paper also discusses our approach to parallelize queries with operations that are inherently serial. We believe all these techniques will make their way into big data analytics and other massively parallel database systems.

#index 2030489
#* WOO: a scalable and multi-tenant platform for continuous knowledge base synthesis
#@ Kedar Bellare;Carlo Curino;Ashwin Machanavajihala;Peter Mika;Mandar Rahurkar;Aamod Sane
#t 2013
#c 4
#% 806215
#% 913783
#% 989591
#% 1091267
#% 1206834
#% 1217114
#% 1217238
#% 1278127
#% 1314445
#% 1426543
#% 1523838
#% 1576155
#% 1601203
#% 1641483
#% 1741034
#% 1746877
#% 1872366
#% 1895068
#% 1920015
#% 1942736
#! Search, exploration and social experience on the Web has recently undergone tremendous changes with search engines, web portals and social networks offering a different perspective on information discovery and consumption. This new perspective is aimed at capturing user intents, and providing richer and highly connected experiences. The new battleground revolves around technologies for the ingestion, disambiguation and enrichment of entities from a variety of structured and unstructured data sources - we refer to this process as knowledge base synthesis. This paper presents the design, implementation and production deployment of the Web Of Objects (WOO) system, a Hadoop-based platform tackling such challenges. WOO has been designed and implemented to enable various products in Yahoo! to synthesize knowledge bases (KBs) of entities relevant to their domains. Currently, the implementation of WOO we describe is used by various Yahoo! properties such as Intonow, Yahoo! Local, Yahoo! Events and Yahoo! Search. This paper highlights: (i) challenges that arise in designing, building and operating a platform that handles multi-domain, multi-version, and multi-tenant disambiguation of web-scale knowledge bases (hundreds of millions of entities), (ii) the architecture and technical solutions we devised, and (iii) an evaluation on real-world production datasets.

#index 2030490
#* Entity extraction, linking, classification, and tagging for social media: a wikipedia-based approach
#@ Abhishek Gattani;Digvijay S. Lamba;Nikesh Garera;Mitul Tiwari;Xiaoyong Chai;Sanjib Das;Sri Subramaniam;Anand Rajaraman;Venky Harinarayan;AnHai Doan
#t 2013
#c 4
#% 278104
#% 278109
#% 283136
#% 333943
#% 464434
#% 466892
#% 577318
#% 769877
#% 769884
#% 814976
#% 815884
#% 817920
#% 854637
#% 855108
#% 939376
#% 956564
#% 1019082
#% 1022235
#% 1063570
#% 1166537
#% 1190118
#% 1409954
#% 1478118
#% 1591965
#% 1591966
#% 1711864
#% 1948158
#% 1972770
#! Many applications that process social data, such as tweets, must extract entities from tweets (e.g., "Obama" and "Hawaii" in "Obama went to Hawaii"), link them to entities in a knowledge base (e.g., Wikipedia), classify tweets into a set of predefined topics, and assign descriptive tags to tweets. Few solutions exist today to solve these problems for social data, and they are limited in important ways. Further, even though several industrial systems such as OpenCalais have been deployed to solve these problems for text data, little if any has been published about them, and it is unclear if any of the systems has been tailored for social media. In this paper we describe in depth an end-to-end industrial system that solves these problems for social data. The system has been developed and used heavily in the past three years, first at Kosmix, a startup, and later at WalmartLabs. We show how our system uses a Wikipedia-based global "real-time" knowledge base that is well suited for social data, how we interleave the tasks in a synergistic fashion, how we generate and use contexts and social signals to improve task accuracy, and how we scale the system to the entire Twitter firehose. We describe experiments that show that our system outperforms current approaches. Finally we describe applications of the system at Kosmix and WalmartLabs, and lessons learned.

#index 2030491
#* Overview of turn data management platform for digital advertising
#@ Hazem Elmeleegy;Yinan Li;Yan Qi;Peter Wilmot;Mingxi Wu;Santanu Kolay;Ali Dasdan;Songting Chen
#t 2013
#c 4
#% 378409
#% 480821
#% 572311
#% 654448
#% 765431
#% 805840
#% 824697
#% 998845
#% 1523824
#% 1523924
#% 1581407
#% 1594639
#% 1605941
#% 1938496
#! This paper gives an overview of Turn Data Management Platform (DMP). We explain the purpose of this type of platforms, and show how it is positioned in the current digital advertising ecosystem. We also provide a detailed description of the key components in Turn DMP. These components cover the functions of (1) data ingestion and integration, (2) data warehousing and analytics, and (3) real-time data activation. For all components, we discuss the main technical and research challenges, as well as the alternative design choices. One of the main goals of this paper is to highlight the central role that data management is playing in shaping this fast growing multi-billion dollars industry.

#index 2030492
#* Unicorn: a system for searching the social graph
#@ Michael Curtiss;Iain Becker;Tudor Bosman;Sergey Doroshenko;Lucian Grijincu;Tom Jackson;Sandhya Kunnatur;Soren Lassen;Philip Pronin;Sriram Sankar;Guanghao Shen;Gintaras Woss;Chao Yang;Ning Zhang
#t 2013
#c 4
#% 287528
#% 393844
#% 578337
#% 654442
#% 824693
#% 1022236
#% 1055738
#% 1077150
#% 1217194
#% 1328095
#% 1366460
#% 1399976
#% 1605019
#% 1641941
#% 1770414
#% 1879005
#% 1948131
#% 1948175
#! Unicorn is an online, in-memory social graph-aware indexing system designed to search trillions of edges between tens of billions of users and entities on thousands of commodity servers. Unicorn is based on standard concepts in information retrieval, but it includes features to promote results with good social proximity. It also supports queries that require multiple round-trips to leaves in order to retrieve objects that are more than one edge away from source nodes. Unicorn is designed to answer billions of queries per day at latencies in the hundreds of milliseconds, and it serves as an infrastructural building block for Facebook's Graph Search product. In this paper, we describe the data model and query language supported by Unicorn. We also describe its evolution as it became the primary backend for Facebook's search offerings.

#index 2030493
#* A new service for customer care based on the trentorise bigdata platform
#@ Sergio Ramazzina;Chiara L. Ballari;Daniela Somenzi
#t 2013
#c 4
#! In this paper, we give an overview of a platform implemented in collaboration with the University of Trento to deliver an innovative family of customer care services.

#index 2030494
#* Exploiting the diversity, mass and speed of territorial data by TELCO Operator for better user services
#@ Fabrizio Antonelli;Antonino Casella;Cristiana Chitic;Roberto Larcher;Giovanni Torrisi
#t 2013
#c 4
#% 1846717
#% 1905960

#index 2030495
#* The Trento big data platform for public administration and large companies: use cases and opportunities
#@ Ivan Bedini;Benedikt Elser;Yannis Velegrakis
#t 2013
#c 4
#% 1846717
#% 1905960
#% 1971500

#index 2030496
#* Designing query optimizers for big data problems of the future
#@ Nga Tran;Sreenath Bodagala;Jaimin Dave
#t 2013
#c 4
#% 393641
#% 1217219
#% 1770411
#% 1895060
#! The Vertica SQL Query Optimizer was written from the ground up for the Vertica Analytic Database. Its design, and the tradeoffs we encountered during implementation, support the case that the full power of novel database systems can be realized only with a custom Query Optimizer, carefully crafted exclusively for the system in which it operates.

#index 2030497
#* How to maximize the value of big data with the open source SpagoBI suite through a comprehensive approach
#@ Monica Franceschini
#t 2013
#c 4
#! This paper describes the approach adopted by SpagoBI suite (www.spagobi.org) to manage large volumes of heterogeneous structured and unstructured data, to perform real-time Business Intelligence on Big Data streaming and to give meaning to data through the semantic analysis. SpagoBI supplies meaningful data insights through the main concept of persistable and schedulable datasets, and using tools such as self-service BI, ad-hoc reporting, interactive dashboards and explorative analysis.

#index 2030498
#* Context-aware computing: opportunities and open issues
#@ Edward Y. Chang
#t 2013
#c 4
#% 1610248
#! A 2011 Gartner report [3] describes context-aware computing as a game-changing opportunity for enterprises to improve both productivity and profits. Context-aware computing is about making applications and content more relevant to a user's context, e.g., when and where the user is, thereby improving user experience. For instance, a coupon delivered to a user at a wrong time or at a wrong location is considered a nuisance. On the contrary, receiving a timely, usable coupon before purchasing a merchandise is a treat. Context-aware computing is not a new concept, but the ongoing mobile revolution makes it both necessary and feasible. • Necessary because the mobile phone display is small and information must be delivered with much higher relevance and precision to meet user needs. • Feasible because small, light-weight mobile devices allow users to almost always carry them around, and much can be learned via a phone about its user's habits and states. Context-aware computing involves first acquiring context and then taking context-dependent actions. For instance, a phone can sense a user's location and turn off its GPS unit to conserve power when the user enters a building, or it can collect EKG signals of a user and trigger an alert if the user's heart beats irregularly. Similarly, a restaurant can send a coupon to a user when that user is queued up in front of a nearby restaurant. The useful context can be divided into three categories: information on the user (knowledge of habits, emotional state, biophysiological conditions), the user's environment (time, location, co-location of others, social interaction), and the user's tasks (transportation mode, engaged tasks, general goals) [4]. Context-aware computing can be applied to benefit applications in many areas including but not limited to information retrieval, facility management, productivity enhancement, in addition to the aforementioned three examples representing power management, health care, and commerce, respectively.

#index 2030499
#* Next generation data analytics at IBM research
#@ Oktie Hassanzadeh;Anastasios Kementsietsidis;Benny Kimelfeld;Rajasekar Krishnamurthy;Fatma Özcan;Ippokratis Pandis
#t 2013
#c 4
#% 1206624
#% 1426585
#% 1471192
#% 1560246
#% 1581844
#% 1581945
#% 1592315
#% 1594623
#% 1789653
#% 1798411
#% 1846783
#% 1879051
#% 1880460
#% 1913211
#% 1962320
#% 1962360
#% 1971478
#% 1971530

#index 2030500
#* Learning and intelligent optimization (LION): one ring to rule them all
#@ Mauro Brunato;Roberto Battiti
#t 2013
#c 4
#% 1185455
#! Almost by definition, optimization is a source of a tremendous power for automatically improving processes, decisions, products and services. But its potential is still largely unexploited in most real-world contexts. One of the main reasons blocking its widespread adoption is that standard optimization assumes the existence of a function f(x) to be minimized, while in most real-world business contexts this function does not exist or is extremely difficult and costly to build by hand. Machine learning (ML) comes to the rescue: the function (the model) can be built by machine learning starting from abundant data. By Learning and Intelligent Optimization (LION) we mean this combination of learning from data and optimization which can be applied to complex, dynamic, stochastic contexts. This combination dramatically increases the automation level and puts more power directly in the hands of decision makers without resorting to intermediate layers of data scientists (LION has a huge potential for a self-service usage). Reaching this goal is a huge challenge and it will require research at the boundary between two areas, machine learning and optimization, which have been traditionally separated.

#index 2030501
#* Microsoft SQL server's integrated database approach for modern applications and hardware
#@ David Lomet
#t 2013
#c 4
#% 1581947
#% 1668635
#% 1971520
#% 1971522
#% 2010397
#! Recently, there has been much renewed interest in re-architecting database systems to exploit new hardware. While some efforts have suggested that one needs specialized engines ("one size does not fit all"), the approach pursued by Microsoft's SQL Server has been to integrate multiple elements into a common architecture. This brings customers what they want by reducing data impedance mismatches between database systems that they are using for multiple purposes. This integration is, of course, more easily said than done. But this is, in fact, precisely what the SQL Server team has done.

#index 2030502
#* Odyssey: a multistore system for evolutionary analytics
#@ Hakan Hacígümüş;Jagan Sankaranarayanan;Junichi Tatemura;Jeff LeFevre;Neoklis Polyzotis
#t 2013
#c 4
#% 1783393
#% 1991178

#index 2030503
#* A global entity name system (ENS) for data ecosystems
#@ Paolo Bouquet;Andrea Molinari
#t 2013
#c 4
#% 1121044
#! After decades of schema-centric research on data management and integration, the evolution of data on the web and the adoption of resource-based models seem to have shifted the focus towards an entity-centric approach. Our thesis is that the missing element to achieve the full potential of this approach is the development of what we call an Entity Name System (ENS), namely a system which provides a collection of general services for managing the lifecycle of globally unique identifiers in an open and decentralized environment. The claim is that this system can indeed play the coordination role that the DNS played for the document-centric development of the current web.

#index 2030504
#* SAP HANA: the evolution from a modern main-memory data platform to an enterprise application platform
#@ Vishal Sikka;Franz Färber;Anil Goel;Wolfgang Lehner
#t 2013
#c 4
#% 451553
#% 1022298
#% 1667313
#% 1770406
#% 1890515
#! SAP HANA is a pioneering, and one of the best performing, data platform designed from the grounds up to heavily exploit modern hardware capabilities, including SIMD, and large memory and CPU footprints. As a comprehensive data management solution, SAP HANA supports the complete data life cycle encompassing modeling, provisioning, and consumption. This extended abstract outlines the vision and planned next step of the SAP HANA evolution growing from a core data platform into an innovative enterprise application platform as the foundation for current as well as novel business applications in both on-premise and on-demand scenarios. We argue that only a holistic system design rigorously applying co-design at different levels may yield a highly optimized and sustainable platform for modern enterprise applications.

#index 2030505
#* Keeping the TPC relevant!
#@ Raghunath Nambiar;Meikel Poess
#t 2013
#c 4
#% 1332763
#% 1545214
#% 1891264
#! The Transaction Processing Performance Council (TPC) is a nonprofit organization founded in 1988 to define transaction processing and database benchmarks. Since then, the TPC has played a crucial role in providing the industry with relevant standards for total system performance, price-performance, and energy-efficiency comparisons. TPC benchmarks are widely used by database researchers and academia. Historically known for database-centric standards, the TPC has developed a benchmark for virtualization and is currently developing a multisource data integration benchmark. The technology landscape is changing at a rapid pace, challenging industry experts and researchers to develop innovative techniques for evaluating, measuring, and characterizing the performance of modern application systems. The Technology Conference series on Performance Evaluation and Benchmarking (TPCTC), introduced in 2009, and the new TPC-Express initiatives are steps taken by the TPC to be relevant in the coming years and beyond.

#index 2030506
#* Big data integration
#@ Xin Luna Dong;Divesh Srivastava
#t 2013
#c 4
#! The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This tutorial explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.

#index 2030507
#* Just-in-time compilation for SQL query processing
#@ Stratis D. Viglas
#t 2013
#c 4
#% 44638
#% 54022
#% 136740
#% 148195
#% 227934
#% 273917
#% 273943
#% 319549
#% 411750
#% 479910
#% 640615
#% 764830
#% 864411
#% 1217170
#% 1584724
#% 1586201
#% 1592312
#% 1769269
#% 2010391
#! Just-in-time compilation of SQL queries into native code has recently emerged as a viable alternative to interpretation-based query processing. We present the salient results of research in this fresh area, addressing all aspects of the query processing stack. Throughout the discussion we draw analogies to the general code generation techniques used in contemporary compiler technology. At the same time we describe the open research problems of the area.

#index 2030508
#* Toward scalable transaction processing: evolution of shore-MT
#@ Anastasia Ailamaki;Ryan Johnson;Ippokratis Pandis;Pínar Tözün
#t 2013
#c 4
#% 1181215
#% 1328149
#% 1523856
#% 1523878
#% 1606343
#% 1763280
#% 1968412
#! Designing scalable transaction processing systems on modern multicore hardware has been a challenge for almost a decade. The typical characteristics of transaction processing workloads lead to a high degree of unbounded communication on multicores for conventional system designs. In this tutorial, we initially present a systematic way of eliminating scalability bottlenecks of a transaction processing system, which is based on minimizing the unbounded communication. Then, we show several techniques that apply the presented methodology to minimize logging, locking, latching etc. related bottlenecks of transaction processing systems. In parallel, we demonstrate the internals of the Shore-MT storage manager and how they have evolved over the years in terms of scalability on multicore hardware through such techniques. We also teach how to use Shore-MT with the various design options it offers through its application layer Shore-Kits and Metadata Frontend.

#index 2030509
#* Towards database virtualization for database as a service
#@ Aaron J. Elmore;Carlo Curino;Divyakant Agrawal;Amr El Abbadi
#t 2013
#c 4
#% 805469
#% 963628
#% 1621143
#% 1962351
#! Advances in operating system and storage-level virtualization technologies have enabled the effective consolidation of heterogeneous applications in a shared cloud infrastructure. Novel research challenges arising from this new shared environment include load balancing, workload estimation, resource isolation, machine replication, live migration, and an emergent need of automation to handle large scale operations with minimal manual intervention. Given that databases are at the core of most applications that are deployed in the cloud, database management systems (DBMSs) represent a very important technology component that needs to be virtualized in order to realize the benefits of virtualization from autonomic management of data-intensive applications in large scale data-centers. The goal of this tutorial is to survey the techniques used in providing elasticity in virtual machine systems, shared storage systems, and survey database research on multitenant architectures and elasticity primitives. This foundation of core Database as a Service advances, together with a primer of important related topics in OS and storage-level virtualization, are central for anyone that wants to operate in this area of research.

#index 2030510
#* Mobility and social networking: a data management perspective
#@ Mohamed F. Mokbel;Mohamed Sarwat
#t 2013
#c 4
#% 1047347
#% 1065030
#% 1245094
#% 1278576
#% 1278580
#% 1369273
#% 1400036
#% 1426571
#% 1480830
#% 1482209
#% 1496686
#% 1524387
#% 1550750
#% 1581876
#% 1581900
#% 1581966
#% 1606049
#% 1693922
#% 1695791
#% 1770335
#% 1770345
#% 1770349
#% 1770385
#% 1846747
#% 1846784
#% 1846818
#% 1848116
#% 1869836
#% 1871521
#% 1880437
#% 1880464
#% 1895095
#% 1895096
#% 1913977
#% 1919728
#% 1919753
#% 1941000
#% 1941001
#% 1941033
#% 1941059
#% 1945683
#% 1959799
#% 1978744
#! This tutorial presents the state-of-the-art research that lies at the intersection of two hot topics in the data management community: (1) social networking and (2) mobility. In this tutorial, we give an overview of existing research work, systems, and applications related to both social networking and mobility. In addition, we introduce several resources (i.e., datasets, software tools) as well as a list of promising research directions.

#index 2030511
#* When speed has a price: fast information extraction using approximate algorithms
#@ Gonçalo Simões;Helena Galhardas;Luis Gravano
#t 2013
#c 4
#% 95753
#% 504443
#% 613977
#% 843716
#% 855119
#% 874992
#% 1022288
#% 1130819
#% 1174746
#% 1183368
#% 1206687
#% 1206799
#% 1406799
#% 1536587
#% 1811408
#! A wealth of information produced by individuals and organizations is expressed in natural language text. This is a problem since text lacks the explicit structure that is necessary to support rich querying and analysis. Information extraction systems are sophisticated software tools to discover structured information in natural language text. Unfortunately, information extraction is a challenging and time-consuming task. In this paper, we address the limitations of state-of-the-art systems for the optimization of information extraction programs, with the objective of producing efficient extraction executions. Our solution relies on exploiting a wide range of optimization opportunities. For efficiency, we consider a wide spectrum of execution plans, including approximate plans whose results differ in their precision and recall. Our optimizer accounts for these characteristics of the competing execution plans, and uses accurate predictors of their extraction time, recall, and precision. We demonstrate the efficiency and effectiveness of our optimizer through a large-scale experimental evaluation over real-world datasets and multiple extraction tasks and approaches.

#index 2030512
#* Design and evaluation of storage organizations for read-optimized main memory databases
#@ Craig Chasseur;Jignesh M. Patel
#t 2013
#c 4
#% 235242
#% 300194
#% 397397
#% 411523
#% 411554
#% 451767
#% 571084
#% 617890
#% 824697
#% 893129
#% 1015289
#% 1063542
#% 1089604
#% 1127390
#% 1206624
#% 1328057
#% 1523974
#% 1581849
#% 1594588
#% 1594617
#% 1667313
#% 1770406
#% 1846777
#% 1869840
#% 1895060
#% 1971522
#% 2010440
#! Existing main memory data processing systems employ a variety of storage organizations and make a number of storage-related design choices. The focus of this paper is on systematically evaluating a number of these key storage design choices for main memory analytical (i.e. read-optimized) database settings. Our evaluation produces a number of key insights: First, it is always beneficial to organize data into self-contained memory blocks rather than large files. Second, both column-stores and row-stores display performance advantages for different types of queries, and for high performance both should be implemented as options for the tuple-storage layout. Third, cache-sensitive B+-tree indices can play a major role in accelerating query performance, especially when used in a block-oriented organization. Finally, compression can also play a role in accelerating query performance depending on data distribution and query selectivity.

#index 2030513
#* Aggregating semantic annotators
#@ Luying Chen;Stefano Ortona;Giorgio Orsi;Michael Benedikt
#t 2013
#c 4
#% 340936
#% 464644
#% 466892
#% 754068
#% 754104
#% 756848
#% 855109
#% 855114
#% 855122
#% 867981
#% 953413
#% 983580
#% 1058256
#% 1131145
#% 1190118
#% 1217154
#% 1249541
#% 1274807
#% 1328155
#% 1355029
#% 1370260
#% 1374367
#% 1382173
#% 1538764
#% 1604587
#% 1826232
#% 1888253
#% 1906078
#! A growing number of resources are available for enriching documents with semantic annotations. While originally focused on a few standard classes of annotations, the ecosystem of annotators is now becoming increasingly diverse. Although annotators often have very different vocabularies, with both high-level and specialist concepts, they also have many semantic interconnections. We will show that both the overlap and the diversity in annotator vocabularies motivate the need for semantic annotation integration: middleware that produces a unified annotation on top of diverse semantic annotators. On the one hand, the diversity of vocabulary allows applications to benefit from the much richer vocabulary available in an integrated vocabulary. On the other hand, we present evidence that the most widely-used annotators on the web suffer from serious accuracy deficiencies: the overlap in vocabularies from individual annotators allows an integrated annotator to boost accuracy by exploiting inter-annotator agreement and disagreement. The integration of semantic annotations leads to new challenges, both compared to usual data integration scenarios and to standard aggregation of machine learning tools. We overview an approach to these challenges that performs ontology-aware aggregation. We introduce an approach that requires no training data, making use of ideas from database repair. We experimentally compare this with a supervised approach, which adapts maximal entropy Markov models to the setting of ontology-based annotations. We further experimentally compare both these approaches with respect to ontology-unaware supervised approaches, and to individual annotators.

#index 2030514
#* Discovering denial constraints
#@ Xu Chu;Ihab F. Ilyas;Paolo Papotti
#t 2013
#c 4
#% 152934
#% 296539
#% 384978
#% 397369
#% 487843
#% 765455
#% 891559
#% 981634
#% 1054480
#% 1127381
#% 1127443
#% 1573139
#% 1701178
#% 1897972
#% 1898008
#! Integrity constraints (ICs) provide a valuable tool for enforcing correct application semantics. However, designing ICs requires experts and time. Proposals for automatic discovery have been made for some formalisms, such as functional dependencies and their extension conditional functional dependencies. Unfortunately, these dependencies cannot express many common business rules. For example, an American citizen cannot have lower salary and higher tax rate than another citizen in the same state. In this paper, we tackle the challenges of discovering dependencies in a more expressive integrity constraint language, namely Denial Constraints (DCs). DCs are expressive enough to overcome the limits of previous languages and, at the same time, have enough structure to allow efficient discovery and application in several scenarios. We lay out theoretical and practical foundations for DCs, including a set of sound inference rules and a linear algorithm for implication testing. We then develop an efficient instance-driven DC discovery algorithm and propose a novel scoring function to rank DCs for user validation. Using real-world and synthetic datasets, we experimentally evaluate scalability and effectiveness of our solution.

#index 2030515
#* Diversified top-k graph pattern matching
#@ Wenfei Fan;Xin Wang;Yinghui Wu
#t 2013
#c 4
#% 278831
#% 341672
#% 542102
#% 593696
#% 643566
#% 654442
#% 729923
#% 800508
#% 835906
#% 955712
#% 960261
#% 989645
#% 1016314
#% 1019117
#% 1063513
#% 1074116
#% 1075132
#% 1190093
#% 1291641
#% 1355019
#% 1384287
#% 1450870
#% 1506210
#% 1523818
#% 1594636
#% 1688521
#% 1770131
#% 1845364
#% 1876857
#% 1880433
#! Graph pattern matching has been widely used in e.g., social data analysis. A number of matching algorithms have been developed that, given a graph pattern Q and a graph G, compute the set M(Q,G) of matches of Q in G. However, these algorithms often return an excessive number of matches, and are expensive on large real-life social graphs. Moreover, in practice many social queries are to find matches of a specific pattern node, rather than the entire M(Q,G). This paper studies top-k graph pattern matching. (1) We revise graph pattern matching defined in terms of simulation, by supporting a designated output node uo. Given G and Q, it is to find those nodes in M(Q,G) that match uo, instead of the large set M(Q,G). (2) We study two classes of functions for ranking the matches: relevance functions δr() based on, e.g., social impact, and distance functions δd() to cover diverse elements. (3) We develop two algorithms for computing top-k matches of uo based on δr(), with the early termination property, i.e., they find top-k matches without computing the entire M(Q,G). (4) We also study diversified top-k matching, a bi-criteria optimization problem based on both δr() and δd(). We show that its decision problem is NP-complete. Nonetheless, we provide an approximation algorithm with performance guarantees and a heuristic one with the early termination property. (5) Using real-life and synthetic data, we experimentally verify that our (diversified) top-k matching algorithms are effective, and outperform traditional matching algorithms in efficiency.

#index 2030516
#* Bitlist: new full-text index for low space cost and efficient keyword search
#@ Weixiong Rao;Lei Chen;Pan Hui;Sasu Tarkoma
#t 2013
#c 4
#% 275929
#% 303072
#% 321640
#% 408396
#% 420491
#% 570319
#% 643566
#% 656274
#% 786632
#% 864446
#% 879326
#% 1016130
#% 1055710
#% 1190095
#% 1227595
#% 1399964
#% 1407167
#% 1538767
#! Nowadays Web search engines are experiencing significant performance challenges caused by a huge amount of Web pages and increasingly larger number of Web users. The key issue for addressing these challenges is to design a compact structure which can index Web documents with low space and meanwhile process keyword search very fast. Unfortunately, the current solutions typically separate the space optimization from the search improvement. As a result, such solutions either save space yet with search inefficiency, or allow fast keyword search but with huge space requirement. In this paper, to address the challenges, we propose a novel structure bitlist with both low space requirement and supporting fast keyword search. Specifically, based on a simple and yet very efficient encoding scheme, bitlist uses a single number to encode a set of integer document IDs for low space, and adopts fast bitwise operations for very efficient boolean-based keyword search. Our extensive experimental results on real and synthetic data sets verify that bitlist outperforms the recent proposed solution, inverted list compression [23, 22] by spending 36.71% less space and 61.91% faster processing time, and achieves comparable running time as [8] but with significantly lower space.

#index 2030517
#* RCSI: scalable similarity search in thousand(s) of genomes
#@ Sebastian Wandelt;Johannes Starlinger;Marc Bux;Ulf Leser
#t 2013
#c 4
#% 143306
#% 216781
#% 235941
#% 310502
#% 333679
#% 401434
#% 547438
#% 765262
#% 1102121
#% 1184912
#% 1217204
#% 1231157
#% 1247841
#% 1328126
#% 1335348
#% 1486261
#% 1529961
#% 1581883
#% 1600229
#% 1622030
#% 1639825
#% 1770326
#% 1770342
#% 1791548
#% 1874155
#% 1880471
#% 1974207
#% 2010362
#! Until recently, genomics has concentrated on comparing sequences between species. However, due to the sharply falling cost of sequencing technology, studies of populations of individuals of the same species are now feasible and promise advances in areas such as personalized medicine and treatment of genetic diseases. A core operation in such studies is read mapping, i.e., finding all parts of a set of genomes which are within edit distance k to a given query sequence (k-approximate search). To achieve sufficient speed, current algorithms solve this problem only for one to-be-searched genome and compute only approximate solutions, i.e., they miss some k- approximate occurrences. We present RCSI, Referentially Compressed Search Index, which scales to a thousand genomes and computes the exact answer. It exploits the fact that genomes of different individuals of the same species are highly similar by first compressing the to-be-searched genomes with respect to a reference genome. Given a query, RCSI then searches the reference and all genome-specific individual differences. We propose efficient data structures for representing compressed genomes and present algorithms for scalable compression and similarity search. We evaluate our algorithms on a set of 1092 human genomes, which amount to approx. 3 TB of raw data. RCSI compresses this set by a ratio of 450:1 (26:1 including the search index) and answers similarity queries on a mid-class server in 15 ms on average even for comparably large error thresholds, thereby significantly outperforming other methods. Furthermore, we present a fast and adaptive heuristic for choosing the best reference sequence for referential compression, a problem that was never studied before at this scale.

#index 2030518
#* Approximate MaxRS in spatial databases
#@ Yufei Tao;Xiaocheng Hu;Dong-Wan Choi;Chin-Wan Chung
#t 2013
#c 4
#% 10131
#% 333977
#% 465060
#% 527189
#% 765426
#% 893141
#% 1228145
#% 1238464
#% 1328203
#% 1581825
#% 1594573
#% 1594580
#% 1720751
#% 1880430
#! In the maximizing range sum (MaxRS) problem, given (i) a set P of 2D points each of which is associated with a positive weight, and (ii) a rectangle r of specific extents, we need to decide where to place r in order to maximize the covered weight of r - that is, the total weight of the data points covered by r. Algorithms solving the problem exactly entail expensive CPU or I/O cost. In practice, exact answers are often not compulsory in a MaxRS application, where slight imprecision can often be comfortably tolerated, provided that approximate answers can be computed considerably faster. Motivated by this, the present paper studies the (1 - ε)-approximate MaxRS problem, which admits the same inputs as MaxRS, but aims instead to return a rectangle whose covered weight is at least (1-ε)m*, where m* is the optimal covered weight, and ε can be an arbitrarily small constant between 0 and 1. We present fast algorithms that settle this problem with strong theoretical guarantees.

#index 2030519
#* Multi-tuple deletion propagation: approximations and complexity
#@ Benny Kimelfeld;Jan Vondrák;David P. Woodruff
#t 2013
#c 4
#% 664
#% 286901
#% 291869
#% 341672
#% 378401
#% 416007
#% 731279
#% 874911
#% 907551
#% 992830
#% 1426461
#% 1475507
#% 1488677
#% 1489342
#% 1562962
#% 1654517
#% 1692263
#% 1770134
#% 1912319
#% 1972413
#! This paper studies the computational complexity of the classic problem of deletion propagation in a relational database, where tuples are deleted from the base relations in order to realize a desired deletion of tuples from the view. Such an operation may result in a (sometimes unavoidable) side effect: deletion of additional tuples from the view, besides the intentionally deleted ones. The goal is to minimize the side effect. The complexity of this problem has been well studied in the case where only a single tuple is deleted from the view. However, only little is known within the more realistic scenario of multi-tuple deletion, which is the topic of this paper. The class of conjunctive queries (CQs) is among the most well studied in the literature, and we focus here on views defined by CQs that are self-join free (sjf-CQs). Our main result is a trichotomy in complexity, classifying all sjf-CQs into three categories: those for which the problem is in polynomial time, those for which the problem is NP-hard but polynomial-time approximable (by a constant-factor), and those for which even an approximation (by any factor) is NP-hard to obtain. A corollary of this trichotomy is a dichotomy in the complexity of deciding whether a side-effect-free solution exists, in the multi-tuple case. We further extend the full classification to accommodate the presence of a constant upper bound on the number of view tuples to delete, and the presence of functional dependencies. Finally, we establish (positive and negative) complexity results on approximability for the dual problem of maximizing the number of view tuples surviving (rather than minimizing the side effect incurred in) the deletion propagation.

#index 2030520
#* Supporting distributed feed-following apps over edge devices
#@ Badrish Chandramouli;Suman Nath;Wenchao Zhou
#t 2013
#c 4
#% 116040
#% 330305
#% 340663
#% 464706
#% 479461
#% 480158
#% 555052
#% 800525
#% 800584
#% 810008
#% 874978
#% 1015280
#% 1015597
#% 1016167
#% 1021194
#% 1022275
#% 1164990
#% 1207015
#% 1426571
#% 1449326
#% 1586956
#% 1594581
#% 1594625
#% 1675421
#% 1688281
#% 1741246
#% 1770379
#! In feed-following applications such as Twitter and Facebook, users (consumers) follow a large number of other users (producers) to get personalized feeds, generated by blending producers- feeds. With the proliferation of Cloud-connected smart edge devices such as smartphones, producers and consumers of many feed-following applications reside on edge devices and the Cloud. An important design goal of such applications is to minimize communication (and energy) overhead of edge devices. In this paper, we abstract distributed feed-following applications as a view maintenance problem, with the goal of optimally placing the views on edge devices and in the Cloud to minimize communication overhead between edge devices and the Cloud. The view placement problem for general network topology is NP Hard; however, we show that for the special case of Cloud-edge topology, locally optimal solutions yield a globally optimal view placement solution. Based on this powerful result, we propose view placement algorithms that are highly efficient, yet provably minimize global network cost. Compared to existing works on feed-following applications, our algorithms are more general--they support views with selection, projection, correlation (join) and arbitrary black-box operators, and can even refer to other views. We have implemented our algorithms within a distributed feed-following architecture over real smartphones and the Cloud. Experiments over real datasets indicate that our algorithms are highly scalable and orders-of-magnitude more efficient than existing strategies for optimal placement. Further, our results show that optimal placements generated by our algorithms are often several factors better than simpler schemes.

#index 2030521
#* Rank discovery from web databases
#@ Saravanan Thirumuruganathan;Nan Zhang;Gautam Das
#t 2013
#c 4
#% 268114
#% 340146
#% 413635
#% 480479
#% 659993
#% 879604
#% 943875
#% 956534
#% 960286
#% 993964
#% 1023487
#% 1075132
#% 1127356
#% 1206906
#% 1372686
#% 1426573
#% 1549872
#% 1880432
#% 2010406
#! Many web databases are only accessible through a proprietary search interface which allows users to form a query by entering the desired values for a few attributes. After receiving a query, the system returns the top-k matching tuples according to a pre-determined ranking function. Since the rank of a tuple largely determines the attention it receives from website users, ranking information for any tuple - not just the top-ranked ones - is often of significant interest to third parties such as sellers, customers, market researchers and investors. In this paper, we define a novel problem of rank discovery over hidden web databases. We introduce a taxonomy of ranking functions, and show that different types of ranking functions require fundamentally different approaches for rank discovery. Our technical contributions include principled and efficient randomized algorithms for estimating the rank of a given tuple, as well as negative results which demonstrate the inefficiency of any deterministic algorithm. We show extensive experimental results over real-world databases, including an online experiment at Amazon.com, which illustrates the effectiveness of our proposed techniques.

#index 2030522
#* SPARSI: partitioning sensitive data amongst multiple adversaries
#@ Theodoros Rekatsinas;Amol Deshpande;Ashwin Machanavajjhala
#t 2013
#c 4
#% 38894
#% 205305
#% 576761
#% 864412
#% 879398
#% 1132171
#% 1164877
#% 1198221
#% 1206745
#% 1270275
#% 1287870
#% 1414540
#% 1531256
#% 1567914
#% 1606049
#% 1627692
#% 1697825
#! We present SPARSI, a novel theoretical framework for partitioning sensitive data across multiple non-colluding adversaries. Most work in privacy-aware data sharing has considered disclosing summaries where the aggregate information about the data is preserved, but sensitive user information is protected. Nonetheless, there are applications, including online advertising, cloud computing and crowdsourcing markets, where detailed and fine-grained user data must be disclosed. We consider a new data sharing paradigm and introduce the problem of privacy-aware data partitioning, where a sensitive dataset must be partitioned among k untrusted parties (adversaries). The goal is to maximize the utility derived by partitioning and distributing the dataset, while minimizing the total amount of sensitive information disclosed. The data should be distributed so that an adversary, without colluding with other adversaries, cannot draw additional inferences about the private information, by linking together multiple pieces of information released to her. The assumption of no collusion is both reasonable and necessary in the above application domains that require release of private user information. SPARSI enables us to formally define privacy-aware data partitioning using the notion of sensitive properties for modeling private information and a hypergraph representation for describing the interdependencies between data entries and private information. We show that solving privacy-aware partitioning is, in general, NP-hard, but for specific information disclosure functions, good approximate solutions can be found using relaxation techniques. Finally, we present a local search algorithm applicable to generic information disclosure functions. We conduct a rigorous performance evaluation with real-world and synthetic datasets that illustrates the effectiveness of SPARSI at partitioning sensitive data while minimizing disclosure.

#index 2030523
#* Scalable column concept determination for web tables using large knowledge bases
#@ Dong Deng;Yu Jiang;Guoliang Li;Jian Li;Cong Yu
#t 2013
#c 4
#% 278500
#% 322884
#% 480654
#% 654467
#% 765463
#% 893164
#% 956564
#% 963669
#% 1055684
#% 1063570
#% 1127393
#% 1127425
#% 1152461
#% 1218677
#% 1328133
#% 1328199
#% 1409954
#% 1426543
#% 1523903
#% 1523913
#% 1540293
#% 1587299
#% 1592311
#% 1594614
#% 1654056
#% 1769264
#% 1770326
#% 1770327
#% 1770359
#% 1800659
#% 1846758
#% 1846805
#% 1869827
#% 1943531
#% 1962373
#! Tabular data on the Web has become a rich source of structured data that is useful for ordinary users to explore. Due to its potential, tables on the Web have recently attracted a number of studies with the goals of understanding the semantics of those Web tables and providing effective search and exploration mechanisms over them. An important part of table understanding and search is column concept determination, i.e., identifying the most appropriate concepts associated with the columns of the tables. The problem becomes especially challenging with the availability of increasingly rich knowledge bases that contain hundreds of millions of entities. In this paper, we focus on an important instantiation of the column concept determination problem, namely, the concepts of a column are determined by fuzzy matching its cell values to the entities within a large knowledge base. We provide an efficient and scalable MapReduce-based solution that is scalable to both the number of tables and the size of the knowledge base and propose two novel techniques: knowledge concept aggregation and knowledge entity partition. We prove that both the problem of finding the optimal aggregation strategy and that of finding the optimal partition strategy are NP-hard, and propose efficient heuristic techniques by leveraging the hierarchy of the knowledge base. Experimental results on real-world datasets show that our method achieves high annotation quality and performance, and scales well.

#index 2030524
#* Top-K structural diversity search in large networks
#@ Xin Huang;Hong Cheng;Rong-Hua Li;Lu Qin;Jeffrey Xu Yu
#t 2013
#c 4
#% 1604
#% 278831
#% 333854
#% 397133
#% 397378
#% 881460
#% 960243
#% 1075132
#% 1124590
#% 1166473
#% 1206821
#% 1265149
#% 1399992
#% 1560358
#% 1560424
#% 1581911
#% 1594586
#% 1605988
#% 1688521
#% 1719564
#% 1848109
#% 1880433
#! Social contagion depicts a process of information (e.g., fads, opinions, news) diffusion in the online social networks. A recent study reports that in a social contagion process the probability of contagion is tightly controlled by the number of connected components in an individual's neighborhood. Such a number is termed structural diversity of an individual and it is shown to be a key predictor in the social contagion process. Based on this, a fundamental issue in a social network is to find top-k users with the highest structural diversities. In this paper, we, for the first time, study the top-k structural diversity search problem in a large network. Specifically, we develop an effective upper bound of structural diversity for pruning the search space. The upper bound can be incrementally refined in the search process. Based on such upper bound, we propose an efficient framework for top-k structural diversity search. To further speed up the structural diversity evaluation in the search process, several carefully devised heuristic search strategies are proposed. Extensive experimental studies are conducted in 13 real-world large networks, and the results demonstrate the efficiency and effectiveness of the proposed methods.

#index 2030525
#* Synthetising changes in XML documents as PULs
#@ Federico Cavalieri;Alessandro Solimando;Giovanna Guerrini
#t 2013
#c 4
#% 66654
#% 210212
#% 227859
#% 480126
#% 509718
#% 545958
#% 659923
#% 824676
#% 1290919
#% 1312537
#% 1549850
#% 1668638
#! The ability of efficiently detecting changes in XML documents is crucial in many application contexts. If such changes are represented as XQuery Update Pending Update Lists (PULs), they can then be applied on documents using XQuery Update engines, and document management can take advantage of existing composition, inversion, reconciliation approaches developed in the update processing context. The paper presents an XML edit-script generator with the unique characteristic of using PULs as edit-script language and improving the state of the art from both the performance and the generated edit-script quality perspectives.

#index 2030526
#* DesTeller: a system for destination prediction based on trajectories with privacy protection
#@ Andy Yuan Xue;Rui Zhang;Yu Zheng;Xing Xie;Jianhui Yu;Yong Tang
#t 2013
#c 4
#% 1089823
#% 1442466
#% 1456650
#% 1728806
#% 2010329
#! Destination prediction is an essential task for a number of emerging location based applications such as recommending sightseeing places and sending targeted advertisements. A common approach to destination prediction is to derive the probability of a location being the destination based on historical trajectories. However, existing techniques suffer from the "data sparsity problem", i.e., the number of available historical trajectories is far from sufficient to cover all possible trajectories. This problem considerably limits the amount of query trajectories whose predicted destinations can be inferred. In this demonstration, we showcase a system named "DesTeller" that is interactive, user-friendly, publicly accessible, and capable of answering real-time queries. The underlying algorithm Sub-Trajectory Synthesis (SubSyn) successfully addressed the data sparsity problem and is able to predict destinations for almost every query submitted by travellers. We also consider the privacy protection issue in case an adversary uses SubSyn algorithm to derive sensitive location information of users.

#index 2030527
#* Senbazuru: a prototype spreadsheet database management system
#@ Zhe Chen;Michael Cafarella;Jun Chen;Daniel Prevo;Junfeng Zhuang
#t 2013
#c 4
#% 643004
#% 942088
#% 956564
#% 1150554
#% 1206925
#% 1426518
#% 1628029
#% 1642122
#! Spreadsheets have become a critical data management tool, but they lack explicit relational metadata, making it difficult to join or integrate data across multiple spreadsheets. Because spreadsheet data are widely available on a huge range of topics, a tool that allows easy spreadsheet integration would be hugely beneficial for a variety of users. We demonstrate that Senbazuru, a prototype spreadsheet database management system (SSDBMS), is able to extract relational information from spreadsheets. By doing so, it opens up opportunities for integration among spreadsheets and with other relational sources. Senbazuru allows users to search for relevant spreadsheets in a large corpus, probabilistically constructs a relational version of the data, and offers several relational operations over the resulting extracted data (including joins to other spreadsheet data). Our demonstration is available on two clients: a JavaScript-rich Web site and a touch interface on the iPad. During the demo, Senbazuru will allow VLDB participants to search spreadsheets, extract relational data from them, and apply relational operators such as select and join.

#index 2030528
#* ReqFlex: fuzzy queries for everyone
#@ Grégory Smits;Olivier Pivert;Thomas Girault
#t 2013
#c 4
#% 257089
#% 465167
#% 1931274
#! In this demonstration we present a complete fuzzy-set-based approach to preference queries that tackles the two main questions raised by the introduction of flexibility and personalization when querying relational databases: i) how to efficiently execute preference queries? and, ii) how to help users define preferences and queries? As an answer to the first question, we propose PostgreSQL_f, a module implemented on top of PostgreSQL to handle fuzzy queries. To answer the second question, we propose ReqFlex an intuitive user interface to the definition of preferences and the construction of fuzzy queries.

#index 2030529
#* Comprehensive and interactive temporal query processing with SAP HANA
#@ Martin Kaufmann;Panagiotis Vagenas;Peter M. Fischer;Donald Kossmann;Franz Färber
#t 2013
#c 4
#% 163442
#% 527786
#% 1054486
#% 1614904
#% 1688261
#% 1905961
#% 1972766
#! In this demo, we present a prototype of a main memory database system which provides a wide range of temporal operators featuring predictable and interactive response times. Much of real-life data is temporal in nature, and there is an increasing application demand for temporal models and operations in databases. Nevertheless, SQL:2011 has only recently overcome a decade-long standstill on standardizing temporal features. As a result, few database systems provide any temporal support, and even those only have limited expressiveness and poor performance. Our prototype combines an in-memory column store and a novel, generic temporal index structure named Timeline Index. As we will show on a workload based on real customer use cases, it achieves predictable and interactive query performance for a wide range of temporal query types and data sizes.

#index 2030530
#* Functions are data too: defunctionalization for PL/SQL
#@ Torsten Grust;Nils Schweinsberg;Alexander Ulrich
#t 2013
#c 4
#% 148247
#% 738684
#! We demonstrate a full-fledged implementation of first-class functions for the widely used PL/SQL database programming language. Functions are treated as regular data items that may be (1) constructed at query runtime, (2) stored in and retrieved from tables, (3) assigned to variables, and (4) passed to and from other (higher-order) functions. The resulting PL/SQL dialect concisely and elegantly expresses a wide range of new query idioms which would be cumbersome to formulate if functions remained second-class citizens. We include a diverse set of application scenarios that make these advantages tangible. First-class PL/SQL functions require featherweight syntactic extensions only and come with a non-invasive implementation-- the defunctionalization transformation--that can entirely be built on top of existing relational DBMS infrastructure. An interactive demonstrator helps users to experiment with the "function as data" paradigm and to earn a solid intuition of its inner workings.

#index 2030531
#* NADEEF: a generalized data cleaning system
#@ Amr Ebaid;Ahmed Elmagarmid;Ihab F. Ilyas;Mourad Ouzzani;Jorge-Arnulfo Quiane-Ruiz;Nan Tang;Si Yin
#t 2013
#c 4
#% 480499
#% 810019
#% 1550749
#% 1581885
#% 1763276
#% 1972799
#% 2010379
#! We present NADEEF, an extensible, generic and easy-to-deploy data cleaning system. NADEEF distinguishes between a programming interface and a core to achieve generality and extensibility. The programming interface allows users to specify data quality rules by writing code that implements predefined classes. These classes uniformly define what is wrong with the data and (possibly) how to fix it. We will demonstrate the following features provided by NADEEF. (1) Heterogeneity: The programming interface can be used to express many types of data quality rules beyond the well known CFDs (FDs), MDs and ETL rules. (2) Interdependency: The core algorithms can interleave multiple types of rules to detect and repair data errors. (3) Deployment and extensibility: Users can easily customize NADEEF by defining new types of rules, or by extending the core. (4) Metadata management and data custodians: We show a live data quality dashboard to effectively involve users in the data cleaning process.

#index 2030532
#* QUEST: a keyword search system for relational data based on semantic and machine learning techniques
#@ Sonia Bergamaschi;Francesco Guerra;Matteo Interlandi;Raquel Trillo-Lado;Yannis Velegrakis
#t 2013
#c 4
#% 1467763
#% 1581893
#% 1642104
#% 1651549
#! We showcase QUEST (QUEry generator for STructured sources), a search engine for relational databases that combines semantic and machine learning techniques for transforming keyword queries into meaningful SQL queries. The search engine relies on two approaches: the forward, providing mappings of keywords into database terms (names of tables and attributes, and domains of attributes), and the backward, computing the paths joining the data structures identified in the forward step. The results provided by the two approaches are combined within a probabilistic framework based on the Dempster-Shafer Theory. We demonstrate QUEST capabilities, and we show how, thanks to the flexibility obtained by the probabilistic combination of different techniques, QUEST is able to compute high quality results even with few training data and/or with hidden data sources such as those found in the Deep Web.

#index 2030533
#* GroupFinder: a new approach to top-k point-of-interest group retrieval
#@ Kenneth S. Bøgh;Anders Skovsgaard;Christian S. Jensen
#t 2013
#c 4
#% 1523973
#% 1581877
#% 1618262
#% 1895077
#! The notion of point-of-interest (PoI) has existed since paper road maps began to include markings of useful places such as gas stations, hotels, and tourist attractions. With the introduction of geopositioned mobile devices such as smartphones and mapping services such as Google Maps, the retrieval of PoIs relevant to a user's intent has became a problem of automated spatio-textual information retrieval. Over the last several years, substantial research has gone into the invention of functionality and efficient implementations for retrieving nearby PoIs. However, with a couple of exceptions existing proposals retrieve results at single-PoI granularity. We assume that a mobile device user issues queries consisting of keywords and an automatically supplied geo-position, and we target the common case where the user wishes to find nearby groups of PoIs that are relevant to the keywords. Such groups are relevant to users who wish to conveniently explore several options before making a decision such as to purchase a specific product. Specifically, we demonstrate a practical proposal for finding top-k PoI groups in response to a query. We show how problem parameter settings can be mapped to options that are meaningful to users. Further, although this kind of functionality is prone to combinatorial explosion, we will demonstrate that the functionality can be supported efficiently in practical settings.

#index 2030534
#* A demonstration of SpatialHadoop: an efficient mapreduce framework for spatial data
#@ Ahmed Eldawy;Mohamed F. Mokbel
#t 2013
#c 4
#% 285932
#% 427199
#% 480093
#% 632105
#% 1063553
#% 1296918
#% 1594623
#% 1798378
#% 1869836
#! This demo presents SpatialHadoop as the first full-fledged MapReduce framework with native support for spatial data. SpatialHadoop is a comprehensive extension to Hadoop that pushes spatial data inside the core functionality of Hadoop. SpatialHadoop runs existing Hadoop programs as is, yet, it achieves order(s) of magnitude better performance than Hadoop when dealing with spatial data. SpatialHadoop employs a simple spatial high level language, a two-level spatial index structure, basic spatial components built inside the MapReduce layer, and three basic spatial operations: range queries, k-NN queries, and spatial join. Other spatial operations can be similarly deployed in SpatialHadoop. We demonstrate a real system prototype of SpatialHadoop running on an Amazon EC2 cluster against two sets of real spatial data obtained from Tiger Files and OpenStreetMap with sizes 60GB and 300GB, respectively.

#index 2030535
#* Aggregate profile clustering for telco analytics
#@ Mehmet Ali Abbasoğlu;Buğra Gedik;Hakan Ferhatosmanoğlu
#t 2013
#c 4
#% 466425
#% 1015261

#index 2030536
#* ROSeAnn: reconciling opinions of semantic annotators
#@ Luying Chen;Stefano Ortona;Giorgio Orsi;Michael Benedikt
#t 2013
#c 4
#% 466892
#% 1063570
#% 1249541
#% 1288161
#% 1826232
#% 1906078
#! Named entity extractors can be used to enrich both text and Web documents with semantic annotations. While originally focused on a few standard entity types, the ecosystem of annotators is becoming increasingly diverse, with recognition capabilities ranging from generic to specialised entity types. Both the overlap and the diversity in annotator vocabularies motivate the need for managing and integrating semantic annotations: allowing users to see the results of multiple annotations and to merge them into a unified solution. We demonstrate ROSEANN, a system for the management of semantic annotations. ROSEANN provides users with a unified view over the opinion of multiple independent annotators both on text and Web documents. It allows users to understand and reconcile conflicts between annotations via ontology-aware aggregation. ROSEANN incorporates both supervised aggregation, appropriate when representative training data is available, and an unsupervised method based on the notion of weighted-repair.

#index 2030537
#* RecDB in action: recommendation made easy in relational databases
#@ Mohamed Sarwat;James Avery;Mohamed F. Mokbel
#t 2013
#c 4
#% 452563
#% 813966
#% 956521
#% 1063588
#% 1207006
#% 1217203
#% 1524387
#% 1770385
#% 1869831
#! In this paper, we demonstrate RecDB; a full-fledged database system that provides personalized recommendation to users. We implemented RecDB using an existing open source database system PostgreSQL, and we demonstrate the effectiveness of RecDB using two existing recommendation applications (1) Restaurant Recommendation, (2) Movie Recommendation. To make the demo even more interactive, we showcase a novel application that recommends research papers presented at VLDB 2013 to the conference attendees based on their publication history in DBLP.

#index 2030538
#* POIKILO: a tool for evaluating the results of diversification models and algorithms
#@ Marina Drosou;Evaggelia Pitoura
#t 2013
#c 4
#% 177422
#% 262112
#% 805841
#% 1074133
#% 1166473
#% 1181244
#% 1472964
#% 1581911
#% 1594636
#% 1770354
#% 1798393
#% 1880433
#% 1944322
#! Search result diversification has attracted considerable attention as a means of improving the quality of results retrieved by user queries. In this demonstration, we present Poikilo, a tool to assist users in locating and evaluating diverse results. We provide implementations of a wide suite of models and algorithms to compute and compare diverse results. Users can tune various diversification parameters, combine diversity with relevance and also see how diverse results change over time in the case of streaming data.

#index 2030539
#* CrowdMiner: mining association rules from the crowd
#@ Yael Amsterdamer;Yael Grossman;Tova Milo;Pierre Senellart
#t 2013
#c 4
#% 481290
#% 481779
#% 614619
#% 735357
#% 1581851
#% 1846744
#% 1972790
#! This demo presents CrowdMiner, a system enabling the mining of interesting data patterns from the crowd. While traditional data mining techniques have been used extensively for finding patterns in classic databases, they are not always suitable for the crowd, mainly because humans tend to remember only simple trends and summaries rather than exact details. To address this, CrowdMiner employs a novel crowd-mining algorithm, designed specifically for this context. The algorithm iteratively chooses appropriate questions to ask the crowd, while aiming to maximize the knowledge gain at each step. We demonstrate CrowdMiner through a Well-Being portal, constructed interactively by mining the crowd, and in particular the conference participants, for common health related practices and trends.

#index 2030540
#* TeRec: a temporal recommender system over tweet stream
#@ Chen Chen;Hongzhi Yin;Junjie Yao;Bin Cui
#t 2013
#c 4
#% 1331
#% 823360
#% 1019070
#% 1176909
#% 1581962
#% 1598363
#% 1893816
#! As social media further integrates into our daily lives, people are increasingly immersed in real-time social streams via services such as Twitter and Weibo. One important observation in these online social platforms is that users' interests and the popularity of topics shift very fast, which poses great challenges on existing recommender systems to provide the right topics at the right time. In this paper, we extend the online ranking technique and propose a temporal recommender system - TeRec. In TeRec, when posting tweets, users can get recommendations of topics (hashtags) according to their real-time interests, they can also generate fast feedbacks according to the recommendations. TeRec provides the browser-based client interface which enables the users to access the real time topic recommendations, and the server side processes and stores the real-time stream data. The experimental study demonstrates the superiority of TeRec in terms of temporal recommendation accuracy.

#index 2030541
#* Graph queries in a next-generation Datalog system
#@ Alexander Shkapsky;Kai Zeng;Carlo Zaniolo
#t 2013
#c 4
#% 93791
#% 234756
#% 480289
#% 752760
#% 752802
#% 1246527
#% 1426442
#% 1549835
#% 1581842
#% 1594576
#% 1933351
#% 1933352
#% 1933369
#% 2005400
#! Recent theoretical advances have enabled the use of special monotonic aggregates in recursion. These special aggregates make possible the concise expression and efficient implementation of a rich new set of advanced applications. Among these applications, graph queries are particularly important because of their pervasiveness in data intensive application areas. In this demonstration, we present our Deductive Application Language (DeAL) System, the first of a new generation of Deductive Database Systems that support applications that could not be expressed using regular stratification, or could be expressed using XY-stratification (also supported in DeAL) but suffer from inefficient execution. Using example queries, we will (i) show how complex graph queries can be concisely expressed using DeAL and (ii) illustrate the formal semantics and efficient implementation of these powerful new monotonic constructs.

#index 2030542
#* iRoad: a framework for scalable predictive query processing on road networks
#@ Abdeltawab M. Hendawi;Jie Bao;Mohamed F. Mokbel
#t 2013
#c 4
#% 421124
#% 1015321
#% 1206625
#% 1456850
#% 1940983
#% 1955144
#! This demo presents the iRoad framework for evaluating predictive queries on moving objects for road networks. The main promise of the iRoad system is to support a variety of common predictive queries including predictive point query, predictive range query, predictive KNN query, and predictive aggregate query. The iRoad framework is equipped with a novel data structure, named reachability tree, employed to determine the reachable nodes for a moving object within a specified future time Τ. In fact, the reachability tree prunes the space around each object in order to significantly reduce the computation time. So, iRoad is able to scale up to handle real road networks with millions of nodes, and it can process heavy workloads on large numbers of moving objects. During the demo, audience will be able to interact with iRoad through a well designed Graphical User Interface to issue different types of predictive queries on a real road network, to obtain the predictive heatmap of the area of interest, to follow the creation and the dynamic update of the reachability tree around a specific moving object, and finally to examine the system efficiency and scalability.

#index 2030543
#* SkySuite: a framework of skyline-join operators for static and stream environments
#@ Mithila Nagendra;K. Selçuk Candan
#t 2013
#c 4
#% 288976
#% 340635
#% 465167
#% 480671
#% 800555
#% 806212
#% 849816
#% 893090
#% 893154
#% 907527
#% 1581852
#% 1594603
#% 1798396
#% 2010430
#! Efficient processing of skyline queries has been an area of growing interest over both static and stream environments. Most existing static and streaming techniques assume that the skyline query is applied to a single data source. Unfortunately, this is not true in many applications in which, due to the complexity of the schema, the skyline query may involve attributes belonging to multiple data sources. Recently, in the context of static environments, various hybrid skyline-join algorithms have been proposed. However, these algorithms suffer from several drawbacks: they often need to scan the data sources exhaustively in order to obtain the set of skyline-join results; moreover, the pruning techniques employed to eliminate the tuples are largely based on expensive pairwise tuple-to-tuple comparisons. On the other hand, most existing streaming methods focus on single stream skyline analysis, thus rendering these techniques unsuitable for applications that require a real-time "join" operation to be carried out before the skyline query can be answered. Based on these observations, we introduce and propose to demonstrate SkySuite: a framework of skyline-join operators that can be leveraged to efficiently process skyline-join queries over both static and stream environments. Among others, SkySuite includes (1) a novel Skyline-Sensitive Join (SSJ) operator that effectively processes skyline-join queries in static environments, and (2) a Layered Skyline-window-Join (LSJ) operator that incrementally maintains skyline-join results over stream environments.

#index 2030544
#* Parallel graph processing on graphics processors made easy
#@ Jianlong Zhong;Bingsheng He
#t 2013
#c 4
#% 1023420
#% 1127550
#% 1194170
#% 1407573
#% 1426513
#% 1446955
#% 1451194
#% 1541456
#% 1631034
#! This paper demonstrates Medusa, a programming framework for parallel graph processing on graphics processors (GPUs). Medusa enables developers to leverage the massive parallelism and other hardware features of GPUs by writing sequential C/C++ code for a small set of APIs. This simplifies the implementation of parallel graph processing on the GPU. The runtime system of Medusa automatically executes the user-defined APIs in parallel on the GPU, with a series of graph-centric optimizations based on the architecture features of GPUs. We will demonstrate the steps of developing GPU-based graph processing algorithms with Medusa, and the superior performance of Medusa with both real-world and synthetic datasets.

#index 2030545
#* Mosquito: another one bites the data upload stream
#@ Stefan Richter;Jens Dittrich;Stefan Schuh;Tobias Frey
#t 2013
#c 4
#% 1022262
#% 1523839
#% 1523841
#% 1592316
#% 1621145
#% 1880472
#% 1895084
#% 1962312
#% 1972735
#! Mosquito is a lightweight and adaptive physical design framework for Hadoop. Mosquito connects to existing data pipelines in Hadoop MapReduce and/or HDFS, observes the data, and creates better physical designs, i.e. indexes, as a byproduct. Our approach is minimally invasive, yet it allows users and developers to easily improve the runtime of Hadoop. We present three important use cases: first, how to create indexes as a byproduct of data uploads into HDFS; second, how to create indexes as a byproduct of map tasks; and third, how to execute map tasks as a byproduct of HDFS data uploads. These use cases may even be combined.

#index 2030546
#* NoFTL: database systems on FTL-less flash storage
#@ Sergej Hardock;Ilia Petrov;Robert Gottstein;Alejandro Buchmann
#t 2013
#c 4
#% 985754
#% 1016185
#% 1063620
#% 1085291
#% 1174229
#% 1210165
#% 1213385
#% 1285917
#% 1463492
#% 1581846
#% 1581953
#% 1601165
#% 1693621
#% 2022583
#! The database architecture and workhorse algorithms have been designed to compensate for hard disk properties. The I/O characteristics of Flash memories have significant impact on database systems and many algorithms and approaches taking advantage of those have been proposed recently. Nonetheless on system level Flash storage devices are still treated as HDD compatible block devices, black boxes and fast HDD replacements. This backwards compatibility (both software and hardware) masks the native behaviour, incurs significant complexity and decreases I/O performance, making it non-robust and unpredictable. Database systems have a long tradition of operating directly on RAW storage natively, utilising the physical characteristics of storage media to improve performance. In this paper we demonstrate an approach called NoFTL that goes a step further. We show that allowing for native Flash access and integrating parts of the FTL functionality into the database system yields significant performance increase and simplification of the I/O stack. We created a real-time data-driven Flash emulator and integrated it accordingly into Shore-MT. We demonstrate a performance improvement of up to 3.7× compared to Shore-MT on RAW block-device Flash storage under various TPC workloads.

#index 2030547
#* SmartMonitor: using smart devices to perform structural health monitoring
#@ Dimitrios Kotsakos;Panos Sakkos;Vana Kalogeraki;Dimitirios Gunopulos
#t 2013
#c 4
#% 951762
#% 1896480
#! In this demonstration, we are presenting SmartMonitor, a distributed Structural Health Monitoring (SHM) system consisting of smart devices. Over the last few years, the vast majority of smart devices is equipped with accelerometers that can be utilized towards building SHM systems with hundreds of nodes. We describe a scalable, fault-tolerant communication protocol, that performs best-effort time synchronization of the nodes and is used to implement a decentralized version of the popular peak-picking SHM method. The implemented interactive system can be easily installed in any accelerometer-equipped Android device and the user has a number of options for configuring the system or analyzing the collected data and computed outcomes.

#index 2030548
#* Lazy ETL in action: ETL technology dates scientific data
#@ Yağíz Kargín;Milena Ivanova;Ying Zhang;Stefan Manegold;Martin Kersten
#t 2013
#c 4
#% 223781
#% 273918
#% 487529
#% 845351
#% 1125875
#% 1181213
#% 1217170
#% 1561900
#% 1652704
#% 1770339
#! Both scientific data and business data have analytical needs. Analysis takes place after a scientific data warehouse is eagerly filled with all data from external data sources (repositories). This is similar to the initial loading stage of Extract, Transform, and Load (ETL) processes that drive business intelligence. ETL can also help scientific data analysis. However, the initial loading is a time and resource consuming operation. It might not be entirely necessary, e.g. if the user is interested in only a subset of the data. We propose to demonstrate Lazy ETL, a technique to lower costs for initial loading. With it, ETL is integrated into the query processing of the scientific data warehouse. For a query, only the required data items are extracted, transformed, and loaded transparently on-the-fly. The demo is built around concrete implementations of Lazy ETL for seismic data analysis. The seismic data warehouse is ready for query processing, without waiting for long initial loading. The audience fires analytical queries to observe the internal mechanisms and modifications that realize each of the steps; lazy extraction, transformation, and loading.

#index 2030549
#* EagleTree: exploring the design space of SSD-based algorithms
#@ Niv Dayan;Martin Kjær Svendsen;Matias Bjørling;Philippe Bonnet;Luc Bouganim
#t 2013
#c 4
#% 1085291
#% 1174229
#% 1285917
#% 1639584
#! Solid State Drives (SSDs) are a moving target for system designers: they are black boxes, their internals are undocumented, and their performance characteristics vary across models. There is no appropriate analytical model and experimenting with commercial SSDs is cumbersome, as it requires a careful experimental methodology to ensure repeatability. Worse, performance results obtained on a given SSD cannot be generalized. Overall, it is impossible to explore how a given algorithm, say a hash join or LSM-tree insertions, leverages the intrinsic parallelism of a modern SSD, or how a slight change in the internals of an SSD would impact its overall performance. In this paper, we propose a new SSD simulation framework, named EagleTree, which addresses these problems, and enables a principled study of SSD-Based algorithms. The demonstration scenario illustrates the design space for algorithms based on an SSD-based IO stack, and shows how researchers and practitioners can use EagleTree to perform tractable explorations of this complex design space.

#index 2030550
#* EnviroMeter: a platform for querying community-sensed data
#@ Saket Sathe;Arthur Oviedo;Dipanjan Chakraborty;Karl Aberer
#t 2013
#c 4
#% 654487
#% 765453
#% 1016275
#% 1060263
#% 1487268
#! Efficiently querying data collected from Large-area Community driven Sensor Networks (LCSNs) is a new and challenging problem. In our previous works, we proposed adaptive techniques for learning models (e.g., statistical, nonparametric, etc.) from such data, considering the fact that LCSN data is typically geo-temporally skewed. In this paper, we present a demonstration of EnviroMeter. EnviroMeter uses our adaptive model creation techniques for processing continuous queries on community-sensed environmental pollution data. Subsequently, it efficiently pushes current pollution updates to GPS-enabled smartphones (through its Android application) or displays it via a web-interface. We experimentally demonstrate that our model-based query processing approach is orders of magnitude efficient than processing the queries over indexed raw data.

#index 2030551
#* Scolopax: exploratory analysis of scientific data
#@ Alper Okcan;Mirek Riedewald;Biswanath Panda;Daniel Fink
#t 2013
#c 4
#% 1581925
#! The formulation of hypotheses based on patterns found in data is an essential component of scientific discovery. As larger and richer data sets become available, new scalable and user-friendly tools for scientific discovery through data analysis are needed. We demonstrate Scolopax, which explores the idea of a search engine for hypotheses. It has an intuitive user interface that supports sophisticated queries. Scolopax can explore a huge space of possible hypotheses, returning a ranked list of those that best match the user preferences. To scale to large and complex data sets, Scolopax relies on parallel data management and mining techniques. These include model training, efficient model summary generation, and novel parallel join techniques that together with traditional approaches such as clustering manipulate massive model-summary collections to find the most interesting hypotheses. This demonstration of Scolopax uses a real observational data set, provided by the Cornell Lab of Ornithology. It contains more than 3.3 million bird sightings reported by citizen scientists and has almost 2500 attributes. Conference attendees have the opportunity to make novel discoveries in this data set, ranging from identifying variables that strongly affect bird populations in specific regions to detecting more sophisticated patterns such as habitat competition and migration.

#index 2030552
#* PROPOLIS: provisioned analysis of data-centric processes
#@ Daniel Deutch;Yuval Moskovitch;Val Tannen
#t 2013
#c 4
#% 503870
#% 875055
#% 976987
#% 1063593
#% 1581829
#% 1622305
#% 1675046
#% 1770400
#! We consider in this demonstration the (static) analysis of data-centric process-based applications, namely applications that depend on an underlying database and whose control is guided by a finite state transition system. We observe that analysts of such applications often want to do more than analyze a specific instance of the application's process control and database. In particular they want to interactively test and explore the effect on analysis results of different hypothetical modifications applied to the application's transition system and to the underlying database. To that end, we propose a demonstration of PROPOLIS, a system for PROvisioned PrOcess anaLysIS, namely analysis of data-centric processes under hypothetical modification scenarios. Our solution is based on the notion of a provisioned expression (which in turn is based on the notion of data provenance), namely an expression that captures, in a compact way, the analysis result with respect to all possible combinations of scenarios, and allows for their exploration at interactive speed. We will demonstrate PROPOLIS in the context of an online shopping application, letting participants play the role of analysts.

#index 2030553
#* Feature selection in enterprise analytics: a demonstration using an R-based data analytics system
#@ Pradap Konda;Arun Kumar;Christopher Ré;Vaishnavi Sashikanth
#t 2013
#c 4
#% 722929
#% 929722
#! Enterprise applications are analyzing ever larger amounts of data using advanced analytics techniques. Recent systems from Oracle, IBM, and SAP integrate R with a data processing system to support richer advanced analytics on large data. A key step in advanced analytics applications is feature selection, which is often an iterative process that involves statistical algorithms and data manipulations. From our conversations with data scientists and analysts at enterprise settings, we observe three key aspects about feature selection. First, feature selection is performed by many types of users, not just data scientists. Second, high performance is critical to perform feature selection processes on large data. Third, the provenance of the results and steps in feature selection processes needs to be tracked for purposes of transparency and auditability. Based on our discussions with data scientists and the literature on feature selection practice, we organize a set of operations for feature selection into the Columbus framework. We prototype Columbus as a library usable in the Oracle R Enterprise environment. In this demonstration, we use Columbus to showcase how we can support various types of users of feature selection in one system. We then show how we optimize performance and manage the provenance of feature selection processes.

#index 2030554
#* Flexible query processor on FPGAs
#@ Mohammadreza Najafi;Mohammad Sadoghi;Hans-Arno Jacobsen
#t 2013
#c 4
#% 654497
#% 1328128
#% 1523931
#% 1581898
#% 1770338
#% 1840077
#% 1846732
#! In this work, we demonstrate Flexible Query Processor (FQP), an online reconfigurable event stream query processor. FQP is an FPGA-based query processor that supports select, project and join queries over event streams at line rate. While processing incoming events, FQP can accept new query expressions, a key distinguishing characteristic from related approaches employing FPGAs for acceleration. Our solution aims to address performance limitations experienced with general purpose processors needing to operate at line rate and lack of on the fly reconfigurability with custom designed hardware solutions on FPGAs.

#index 2030555
#* Mastro studio: managing ontology-based data access applications
#@ Cristina Civili;Marco Console;Giuseppe De Giacomo;Domenico Lembo;Maurizio Lenzerini;Lorenzo Lepore;Riccardo Mancini;Antonella Poggi;Riccardo Rosati;Marco Ruzzi;Valerio Santarelli;Domenico Fabio Savo
#t 2013
#c 4
#% 378409
#% 384978
#% 572307
#% 977139
#% 992962
#% 1416180
#% 1581854
#% 1594576
#% 1703714
#% 1922352
#% 1962361
#! Ontology-based data access (OBDA) is a novel paradigm for accessing large data repositories through an ontology, that is a formal description of a domain of interest. Supporting the management of OBDA applications poses new challenges, as it requires to provide effective tools for (i) allowing both expert and non-expert users to analyze the OBDA specification, (ii) collaboratively documenting the ontology, (iii) exploiting OBDA services, such as query answering and automated reasoning over ontologies, e.g., to support data quality check, and (iv) tuning the OBDA application towards optimized performances. To fulfill these challenges, we have built a novel system, called MASTRO STUDIO, based on a tool for automated reasoning over ontologies, enhanced with a suite of tools and optimization facilities for managing OBDA applications. To show the effectiveness of MASTRO STUDIO, we demonstrate its usage in one OBDA application developed in collaboration with the Italian Ministry of Economy and Finance.

#index 2030556
#* PLASMA-HD: probing the lattice structure and makeup of high-dimensional data
#@ David Fuhry;Yang Zhang;Venu Satuluri;Arnab Nandi;Srinivasan Parthasarathy
#t 2013
#c 4
#% 201876
#% 280454
#% 287242
#% 300163
#% 956506
#% 1023422
#% 1063503
#% 1206754
#% 1214705
#% 1707459
#% 1769264
#% 1846696
#! Rapidly making sense of, analyzing, and extracting useful information from large and complex data is a grand challenge. A user tasked with meeting this challenge is often befuddled with questions on where and how to begin to understand the relevant characteristics of such data. Real-world problem scenarios often involve scalability limitations and time constraints. In this paper we present an incremental interactive data analysis system as a step to address this challenge. This system builds on recent progress in the fields of interactive data exploration, locality sensitive hashing, knowledge caching, and graph visualization. Using visual clues based on rapid incremental estimates, a user is provided a multi-level capability to probe and interrogate the intrinsic structure of data. Throughout the interactive process, the output of previous probes can be used to construct increasingly tight coherence estimates across the parameter space, providing strong hints to the user about promising analysis steps to perform next. We present examples, interactive scenarios, and experimental results on several synthetic and real-world datasets which show the effectiveness and efficiency of our approach. The implications of this work are quite broad and can impact fields ranging from top-k algorithms to data clustering and from manifold learning to similarity search.

#index 2030557
#* A demonstration of iterative parallel array processing in support of telescope image analysis
#@ Matthew Moyers;Emad Soroush;Spencer C. Wallace;Simon Krughoff;Jake Vanderplas;Magdalena Balazinska;Andrew Connolly
#t 2013
#c 4
#% 248863
#% 1426582
#% 1880445
#! In this demonstration, we present AscotDB, a new tool for the analysis of telescope image data. AscotDB results from the integration of ASCOT, a Web-based tool for the collaborative analysis of telescope images and their metadata, and SciDB, a parallel array processing engine. We demonstrate the novel data exploration supported by this integrated tool on a 1 TB dataset comprising scientifically accurate, simulated telescope images. We also demonstrate novel iterative-processing features that we added to SciDB in order to support this use-case.

#index 2030558
#* EvenTweet: online localized event detection from twitter
#@ Hamed Abdelhaq;Christian Sengstock;Michael Gertz
#t 2013
#c 4
#% 420057
#% 987218
#% 1292518
#% 1400018
#% 1642299
#% 1848111
#! Microblogging services such as Twitter, Facebook, and Foursquare have become major sources for information about real-world events. Most approaches that aim at extracting event information from such sources typically use the temporal context of messages. However, exploiting the location information of georeferenced messages, too, is important to detect localized events, such as public events or emergency situations. Users posting messages that are close to the location of an event serve as human sensors to describe an event. In this demonstration, we present a novel framework to detect localized events in real-time from a Twitter stream and to track the evolution of such events over time. For this, spatio-temporal characteristics of keywords are continuously extracted to identify meaningful candidates for event descriptions. Then, localized event information is extracted by clustering keywords according to their spatial similarity. To determine the most important events in a (recent) time frame, we introduce a scoring scheme for events. We demonstrate the functionality of our system, called Even-Tweet, using a stream of tweets from Europe during the 2012 UEFA European Football Championship.

#index 2030559
#* IBminer: a text mining tool for constructing and populating InfoBox databases and knowledge bases
#@ Hamid Mousavi;Shi Gao;Carlo Zaniolo
#t 2013
#c 4
#% 509695
#% 1063570
#% 1288161
#% 1746957
#% 1770359
#% 1925702
#! Knowledge bases and structured summaries are playing a crucial role in many applications, such as text summarization, question answering, essay grading, and semantic search. Although, many systems (e.g., DBpedia and YaGo2) provide massive knowledge bases of such summaries, they all suffer from incompleteness, inconsistencies, and inaccuracies. These problems can be addressed and much improved by combining and integrating different knowledge bases, but their very large sizes and their reliance on different terminologies and ontologies make the task very difficult. In this demo, we will demonstrate a system that is achieving good success on this task by: i) employing available interlinks in the current knowledge bases (e.g. externalLink and redirect links in DBpedia) to combine information on individual entities, and ii) using widely available text corpora (e.g. Wikipedia) and our IBminer text-mining system, to generate and verify structured information, and reconcile terminologies across different knowledge bases. We will also demonstrate two tools designed to support the integration process in close collaboration with IBminer. The first is the InfoBox Knowledge-Base Browser (IBKB) which provides structured summaries and their provenance, and the second is the InfoBox Editor (IBE), which is designed to suggest relevant attributes for a user-specified subject, whereby the user can easily improve the knowledge base without requiring any knowledge about the internal terminology of individual systems.

#index 2030560
#* PAQO: a preference-aware query optimizer for PostgreSQL
#@ Nicholas L. Farnan;Adam J. Lee;Panos K. Chrysanthis;Ting Yu
#t 2013
#c 4
#% 994017
#% 1623140
#% 1770401
#! Although the declarative nature of SQL provides great utility to database users, its use in distributed database management systems can leave users unaware of which servers in the system are evaluating portions of their queries. By allowing users to merely say what data they are interested in accessing without providing guidance regarding how to retrieve it, query optimizers can generate plans with unintended consequences to the user (e.g., violating user privacy by revealing sensitive portions of a user's query to untrusted servers, or impacting result freshness by pulling data from stale data stores). To address these types of issues, we have created a framework that empowers users with the ability to specify constraints on the kinds of plans that can be produced by the optimizer to evaluate their queries. Such constraints are specified through an extended version of SQL that we have developed which we call PASQL. With this proposal, we aim to demonstrate PAQO, a version of PostgreSQL's query optimizer that we have modified to produce plans that respect constraints specified through PASQL while optimizing user-specified SQL queries in terms of performance.

#index 2030561
#* eSkyline: processing skyline queries over encrypted data
#@ Suvarna Bothe;Panagiotis Karras;Akrivi Vlachou
#t 2013
#c 4
#% 397367
#% 654480
#% 765448
#% 824670
#% 1195803
#% 1198205
#% 1217157
#% 1442468
#% 1616432
#% 1726261
#% 1798427
#! The advent of cloud computing redefines the traditional query processing paradigm. Whereas computational overhead and memory constraints become less prohibitive, data privacy, security, and confidentiality concerns become top priorities. In particular, as data owners outsource the management of their data to service providers, query processing over such data has more resources to tap into, yet the data oftentimes has to be encrypted so as to prevent unauthorized access. The challenge that arises in such a setting is to devise an encryption scheme that still allows for query results to be efficiently computed using the encrypted data values. An important type of query that raises unconventional requirements in terms of the operator that has to be evaluated is the skyline query, which returns a set of objects in a dataset whose values are not dominated by any other object therein. In this demonstration, we present eSkyline, a prototype system and query interface that enables the processing of skyline queries over encrypted data, even without preserving the order on each attribute as order-preserving encryption would do. Our system comprises of an encryption scheme that facilitates the evaluation of domination relationships, hence allows for state-of-the-art skyline processing algorithms to be used. The actual data values are reconstructed only at the client side, where the encryption key is known. Our demo visualizes the details of the encryption scheme, allows a user to interact with a server, and showcases the efficiency of computing skyline queries and decrypting the results.

#index 2030562
#* GestureQuery: a multitouch database query interface
#@ Lilong Jiang;Michael Mandel;Arnab Nandi
#t 2013
#c 4
#% 214513
#% 322880
#% 479788
#% 960234
#% 960360
#% 1167472
#% 1905566
#% 1975010
#! Multitouch interfaces allow users to directly and interactively manipulate data. We propose bringing such interactive manipulation to the task of querying SQL databases. This paper describes an initial implementation of such an interface for multitouch tablet devices called GestureQuery that translates multitouch gestures into database queries. It provides database users with immediate constructive feedback on their queries, allowing rapid iteration and refinement of those queries. Based on preliminary user studies, Gesture-Query is easier to use, and lets users construct target queries quicker than console-based SQL and visual query builders while maintaining interactive performance.

#index 2030563
#* Mining and linking patterns across live data streams and stream archives
#@ Di Yang;Kaiyu Zhao;Maryam Hasan;Hanyuan Lu;Elke Rundensteiner;Matthew Ward
#t 2013
#c 4
#% 989584
#% 1015261
#% 1019143
#% 1181258
#% 1328182
#% 1426610
#% 1642007
#% 1654045
#% 1680300
#! We will demonstrate the visual analytics system V istreamT, that supports interactive mining of complex patterns within and across live data streams and stream pattern archives. Our system is equipped with both computational pattern mining and visualization techniques, which allow it to not only efficiently discover and manage patterns but also effectively convey the mining results to human analysts through visual displays. In our demonstration, we will illustrate that with V istreamT, analysts can easily submit, monitor and interact with a broad range of query types for pattern mining. This includes novel strategies for extracting complex patterns from streams in real time, summarizing neighbour-based patterns using multi-resolution compression strategies, selectively pushing patterns into the stream archive, validating the popularity or rarity of stream patterns by stream archive matching, and pattern evolution tracking to link patterns across time.

#index 2030564
#* PhotoStand: a map query interface for a database of news photos
#@ Hanan Samet;Marco D. Adelfio;Brendan C. Fruin;Michael D. Lieberman;Jagan Sankaranarayanan
#t 2013
#c 4
#% 46803
#% 291675
#% 427326
#% 572293
#% 800518
#% 836179
#% 951485
#% 967244
#% 1016198
#% 1034772
#% 1035400
#% 1131843
#% 1135150
#% 1166511
#% 1190131
#% 1227637
#% 1298864
#% 1358026
#% 1480777
#% 1511022
#% 1560254
#% 1598418
#% 1641649
#% 1667270
#% 1879065
#% 1941000
#% 1941070
#! PhotoStand enables the use of a map query interface to retrieve news photos associated with news articles that are in turn associated with the principal locations that they mention collected as a result of monitoring the output of over 10,000 RSS news feeds, made available within minutes of publication, and stored in a PostgreSQL database. The news photos are ranked according to their relevance to the clusters of news articles associated with locations at which they are displayed. This work differs from traditional work in this field as the associated locations and topics (by virtue of the cluster with which the articles containing the news photos are associated) are generated automatically without any human intervention such as tagging, and that photos are retrieved by location instead of just by keyword as is the case for many existing systems. In addition, the clusters provide a filtering step for detecting near-duplicate news photos.

#index 2030565
#* Hone: "Scaling down" Hadoop on shared-memory systems
#@ K. Ashwin Kumar;Jonathan Gluck;Amol Deshpande;Jimmy Lin
#t 2013
#c 4
#% 963669
#% 1017256
#% 1322180
#% 1459261
#% 1467081
#% 1475077
#% 1523820
#% 1586678
#% 1730988
#% 1746886
#% 1895055
#! The underlying assumption behind Hadoop and, more generally, the need for distributed processing is that the data to be analyzed cannot be held in memory on a single machine. Today, this assumption needs to be re-evaluated. Although petabyte-scale data-stores are increasingly common, it is unclear whether "typical" analytics tasks require more than a single high-end server. Additionally, we are seeing increased sophistication in analytics, e.g., machine learning, which generally operates over smaller and more refined datasets. To address these trends, we propose "scaling down" Hadoop to run on shared-memory machines. This paper presents a prototype runtime called Hone, intended to be both API and binary compatible with standard (distributed) Hadoop. That is, Hone can take an existing Hadoop jar and efficiently execute it, without modification, on a multi-core shared memory machine. This allows us to take existing Hadoop algorithms and find the most suitable run-time environment for execution on datasets of varying sizes. Our experiments show that Hone can be an order of magnitude faster than Hadoop pseudo-distributed mode (PDM); on dataset sizes that fit into memory, Hone can outperform a fully-distributed 15-node Hadoop cluster in some cases as well.

#index 2030566
#* Ringtail: a generalized nowcasting system
#@ Dolan Antenucci;Erdong Li;Shaobo Liu;Bochun Zhang;Michael J. Cafarella;Christopher Ré
#t 2013
#c 4
#% 722929
#! Social media nowcasting--using online user activity to describe real-world phenomena--is an active area of research to supplement more traditional and costly data collection methods such as phone surveys. Given the potential impact of such research, we would expect general-purpose nowcasting systems to quickly become a standard tool among noncomputer scientists, yet it has largely remained a research topic. We believe a major obstacle to widespread adoption is the nowcasting feature selection problem. Typical nowcasting systems require the user to choose a handful of social media objects from a pool of billions of potential candidates, which can be a time-consuming and error-prone process. We have built RINGTAIL, a nowcasting system that helps the user by automatically suggesting high-quality signals. We demonstrate that RINGTALL can make nowcasting easier by suggesting relevant features for a range of topics. The user provides just a short topic query (e.g., unemployment) and a small conventional dataset in order for RINGTALL to quickly return a usable predictive nowcasting model.

#index 2030567
#* IPS: an interactive package configuration system for trip planning
#@ Min Xie;Laks V. S. Lakshmanan;Peter T. Wood
#t 2013
#c 4
#% 465167
#% 731407
#% 993957
#% 1055701
#% 1328160
#% 1399973
#% 1426504
#% 1429406
#% 1476463
#% 1594578
#% 1594608
#% 1594690
#% 1770328
#% 1770352
#% 1919884
#! When planning a trip, one essential task is to find a set of Places-of-Interest (POIs) which can be visited during the trip. Using existing travel guides or websites such as Lonely Planet and TripAdvisor, the user has to either manually work out a desirable set of POIs or take pre-configured travel packages; the former can be time consuming while the latter lacks flexibility. In this demonstration, we propose an Interactive Package configuration System (IPS), which visualizes different candidate packages on a map, and enables users to configure a travel package through simple interactions, i.e., comparing packages and fixing/removing POIs from a package. Compared with existing trip planning systems, we believe IPS strikes the right balance between flexibility and manual effort.

#index 2030568
#* R2-D2: a system to support probabilistic path prediction in dynamic environments via "Semi-lazy" learning
#@ Jingbo Zhou;Anthony K. H. Tung;Wei Wu;Wee Siong Ng
#t 2013
#c 4
#% 765452
#% 769899
#% 1099023
#% 1206625
#% 1214685
#% 1760888
#% 1991809
#! Path prediction is presently an important area of research with a wide range of applications. However, most of the existing path prediction solutions are based on eager learning methods which commit to a model or a set of patterns extracted from historical trajectories. Such methods do not perform very well in dynamic environments where the objects' trajectories are affected by many irregular factors which are not captured by pre-defined models or patterns. In this demonstration, we present the "R2-D2" system that supports probabilistic path prediction in dynamic environments. The core of our system is a "semi-lazy" learning approach to probabilistic path prediction which builds a prediction model on the fly using historical trajectories that are selected dynamically based on the trajectories of target objects. Our "R2-D2" system has a visual interface that shows how our path prediction algorithm works on several real-world datasets. It also allows us to experiment with various parameter settings.

#index 2030569
#* REEF: retainable evaluator execution framework
#@ Byung-Gon Chun;Tyson Condie;Carlo Curino;Chris Douglas;Sergiy Matusevych;Brandon Myers;Shravan Narayanamurthy;Raghu Ramakrishnan;Sriram Rao;Josh Rosen;Russell Sears;Markus Weimer
#t 2013
#c 4
#% 316575
#% 342377
#% 983467
#% 1002134
#% 1567923
#% 1594630
#% 1783392
#% 1913796
#% 1913816
#% 2017829
#! In this demo proposal, we describe REEF, a framework that makes it easy to implement scalable, fault-tolerant runtime environments for a range of computational models. We will demonstrate diverse workloads, including extract-transform-load MapReduce jobs, iterative machine learning algorithms, and ad-hoc declarative query processing. At its core, REEF builds atop YARN (Apache Hadoop 2's resource manager) to provide retainable hardware resources with lifetimes that are decoupled from those of computational tasks. This allows us to build persistent (cross-job) caches and cluster-wide services, but, more importantly, supports high-performance iterative graph processing and machine learning algorithms. Unlike existing systems, REEF aims for composability of jobs across computational models, providing significant performance and usability gains, even with legacy code. REEF includes a library of interoperable data management primitives optimized for communication and data movement (which are distinct from storage locality). The library also allows REEF applications to access external services, such as user-facing relational databases. We were careful to decouple lower levels of REEF from the data models and semantics of systems built atop it. The result was two new standalone systems: Tang, a configuration manager and dependency injector, and Wake, a state-of-the-art event-driven programming and data movement framework. Both are language independent, allowing REEF to bridge the JVM and .NET.

#index 2030570
#* OmniDB: towards portable and efficient query processing on parallel CPU/GPU architectures
#@ Shuhao Zhang;Jiong He;Bingsheng He;Mian Lu
#t 2013
#c 4
#% 393844
#% 479821
#% 489819
#% 824697
#% 893219
#% 983261
#% 1082166
#% 1270566
#% 1328057
#% 1426530
#% 1581849
#% 1639247
#% 1789653
#% 1980686
#! Driven by the rapid hardware development of parallel CPU/GPU architectures, we have witnessed emerging relational query processing techniques and implementations on those parallel architectures. However, most of those implementations are not portable across different architectures, because they are usually developed from scratch and target at a specific architecture. This paper proposes a kernel-adapter based design (OmniDB), a portable yet efficient query processor on parallel CPU/GPU architectures. OmniDB attempts to develop an extensible query processing kernel (qKernel) based on an abstract model for parallel architectures, and to leverage an architecture-specific layer (adapter) to make qKernel be aware of the target architecture. The goal of OmniDB is to maximize the common functionality in qKernel so that the development and maintenance efforts for adapters are minimized across different architectures. In this demo, we demonstrate our initial efforts in implementing OmniDB, and present the preliminary results on the portability and efficiency.

#index 2030571
#* Complete approximations of incomplete queries
#@ Ognjen Savković;Paramita Mirza;Alex Tomasi;Werner Nutt
#t 2013
#c 4
#% 67457
#% 481786
#% 841653
#% 880394
#% 1217124
#% 1426458
#% 1920034
#! We present a system that computes for a query that may be incomplete, complete approximations from above and from below. We assume a setting where queries are posed over a partially complete database, that is, a database that is generally incomplete, but is known to contain complete information about specific aspects of its application domain. Which parts are complete, is described by a set of so-called table-completeness statements. Previous work led to a theoretical framework and an implementation that allowed one to determine whether in such a scenario a given conjunctive query is guaranteed to return a complete set of answers or not. With the present demonstrator we show how to reformulate the original query in such a way that answers are guaranteed to be complete. If there exists a more general complete query, there is a unique most specific one, which we find. If there exists a more specific complete query, there may even be infinitely many. In this case, we find the least specific specializations whose size is bounded by a threshold provided by the user. Generalizations are computed by a fixpoint iteration, employing an answer set programming engine. Specializations are found leveraging unification from logic programming.

#index 2030572
#* User analytics with UbeOne: insights into web printing
#@ Georgia Koutrika;Qian Lin;Jerry Liu
#t 2013
#c 4
#% 452563
#% 1355035
#% 1356185
#% 1482279
#% 1844578
#! As web and mobile applications become more sensitive to the user context, there is a shift from purely off-line processing of user actions (log analysis) to real-time user analytics that can generate information about the user context to be instantly leveraged by the application. Ubeone is a system that enables both real-time and aggregate analytics from user data. The system is designed as a set of lightweight, composeable mechanisms that can progressively and collectively analyze a user action, such as pinning, saving or printing a web page. We will demonstrate the system capabilities on analyzing a live feed of URLs printed through a proprietary, web browser plug-in. This is in fact the first analysis of web printing activity. We will also give a taste of how the system can enable instant recommendations based on the user context.

#index 2030573
#* DiAl: distributed streaming analytics anywhere, anytime
#@ Ivo Santos;Marcel Tilly;Badrish Chandramouli;Jonathan Goldstein
#t 2013
#c 4
#! Connected devices are expected to grow to 50 billion in 2020. Through our industrial partners and their use cases, we validated the importance of inflight data processing to produce results with low latency, in particular local and global data analytics capabilities. In order to cope with the scalability challenges posed by distributed streaming analytics scenarios, we propose two new technologies: (1) JStreams, a low footprint and efficient JavaScript complex event processing engine supporting local analytics on heterogeneous devices and (2) DiAlM, a distributed analytics management service that leverages cloud-edge evolving topologies. In the demonstration, based on a real manufacturing use case, we walk through a situation where operators supervise manufacturing equipment through global analytics, and drill down into alarm cases on the factory floor by locally inspecting the data generated by the manufacturing equipment.

#index 2030574
#* Big and useful: what's in the data for me?
#@ Rada Chirkova;Jun Yang
#t 2013
#c 4

#index 2030575
#* Universal indexing of arbitrary similarity models
#@ Tomáš Bartoš
#t 2013
#c 4
#% 124074
#% 169940
#% 342827
#% 570313
#% 818938
#% 857113
#% 997496
#% 1285923
#% 1434103
#% 1573313
#% 1586177
#% 1586190
#% 1688286
#% 1896050
#% 1971365
#% 2004556
#! The increasing amount of available unstructured content together with the growing number of large nonrelational databases put more emphasis on the content-based retrieval and precisely on the area of similarity searching. Although there exist several indexing methods for efficient querying, not all of them are best-suited for arbitrary similarity models. Having a metric space, we can easily apply metric access methods but for nonmetric models which typically better describe similarities between generally unstructured objects the situation is a little bit more complicated. To address this challenge, we introduce SIMDEX, the universal framework that is capable of finding alternative indexing methods that will serve for efficient yet effective similarity searching for any similarity model. Using trivial or more advanced methods for the incremental exploration of possible indexing techniques, we are able to find alternative methods to the widely used metric space model paradigm. Through experimental evaluations, we validate our approach and show how it outperforms the known indexing methods.

#index 2030576
#* Why it is time for a HyPE: a hybrid query processing engine for efficient GPU coprocessing in DBMS
#@ Sebastian Breß;Gunter Saake
#t 2013
#c 4
#% 595202
#% 765419
#% 874997
#% 1063508
#% 1127550
#% 1270566
#% 1370106
#% 1370113
#% 1426530
#% 1482629
#% 1498622
#% 1545530
#% 1549904
#% 1550752
#% 1555047
#% 1601782
#% 1789652
#% 1789654
#% 1886977
#% 1933410
#! GPU acceleration is a promising approach to speed up query processing of database systems by using low cost graphic processors as coprocessors. Two major trends have emerged in this area: (1) The development of frameworks for scheduling tasks in heterogeneous CPU/GPU platforms, which is mainly in the context of coprocessing for applications and does not consider specifics of database-query processing and optimization. (2) The acceleration of database operations using efficient GPU algorithms, which typically cannot be applied easily on other database systems, because of their analytical-algorithm-specific cost models. One major challenge is how to combine traditional database query processing with GPU coprocessing techniques and efficient database operation scheduling in a GPU-aware query optimizer. In this thesis, we develop a hybrid query processing engine, which extends the traditional physical optimization process to generate hybrid query plans and to perform a cost-based optimization in a way that the advantages of CPUs and GPUs are combined. Furthermore, we aim at a portable solution between different GPU-accelerated database management systems to maximize applicability. Preliminary results indicate great potential.

#index 2030577
#* Database support for unstructured meshes
#@ Alireza Rezaei Mahdiraji;Peter Baumann
#t 2013
#c 4
#% 64542
#% 102583
#% 152493
#% 797466
#% 998070
#% 1016206
#% 1019798
#% 1167498
#% 1346473
#! Despite ubiquitous usage of unstructured mesh in many application domains (e.g., computer aided design, scientific simulation, climate modeling, etc.), there is no specialized mesh database which supports storing and querying such data structures. Existing mesh libraries use file-based APIs which do not support declarative querying and are difficult to maintain. A mesh database can benefit these domains in several ways such as: declarative query language, ease of maintenance, query optimization, etc. In this thesis work, the core idea is to have a very general model which can represent objects from different domains and specialize it to smaller object classes using combinatorial constraints. We propose the Incidence multi-Graph Complex (ImG-Complex) data model for storing combinatorial aspect of meshes in a database. We extend incidence graph (IG) representation with multi-incidence information (ImG) to represent a class of objects which we call ImG-Complexes. ImG-Complex can support a wide range of application domains. We introduce optional and application-specific constraints to restrain the general ImG model to specific object classes or specific geometric representations. The constraints check validity of meshes based on the properties of the modeled object class. Finally, we show how graph databases can be utilized and reused to query some combinatorial mesh queries based on the (possibly constrained) ImG model. In particular, we show the strengths and limitations of a graph-only query language in expressing combinatorial mesh queries.

#index 2030578
#* Domain specific multistage query language for medical document repositories
#@ Aastha Madaan;Subhash Bhalla
#t 2013
#c 4
#% 345710
#% 814647
#% 864512
#% 1035135
#% 1394469
#% 1584317
#% 1598340
#% 1879209
#! Vast amount of medical information is increasingly available on the Web. As a result, seeking medical information through queries is gaining importance in the medical domain. The existing keyword-based search engines such as Google, Yahoo fail to suffice the needs of the health-care workers (who are well-versed with the domain knowledge required for querying) using these they often face results which are irrelevant and not useful for their tasks. In this paper, we present the need and the challenges for a user-level, domain-specific query language for the specialized document repositories of the medical domain. This topic has not been sufficiently addressed by the existing approaches including SQL-like query languages or general-purpose keyword-based search engines and document-level indexing based search. We aim to bridge the gap between information needs of the skilled/semi-skilled domain users and the query capability provided by the query language. Overcoming such a challenge can facilitate effective use of large volume of information on the Web (and in the electronic health records (EHRs)repositories).

#index 2030579
#* Realtime analysis of information diffusion in social media
#@ Io Taxidou;Peter Fischer
#t 2013
#c 4
#% 378388
#% 397372
#% 397379
#% 949164
#% 1002007
#% 1015280
#% 1022208
#% 1063501
#% 1063512
#% 1065404
#% 1176970
#% 1399992
#% 1400018
#% 1426513
#% 1426611
#% 1451242
#% 1475157
#% 1482397
#% 1523815
#% 1536508
#% 1536509
#% 1560425
#% 1561564
#% 1573340
#% 1605988
#% 1723621
#% 1747092
#% 1769265
#% 1798404
#% 1915006
#% 2004558
#! The goal of this thesis is to investigate real-time analysis methods on social media with a focus on information diffusion. From a conceptual point of view, we are interested both in the structural, sociological and temporal aspects of information diffusion in social media with a twist on the real time factor of what is happening right now. From a technical side, the sheer size of current social media services (100's of millions of users) and the large amount of data produced by these users renders conventional approaches for these costly analyses impossible. For that, we need to go beyond the state-of-the-art infrastructure for data-intensive computation. Our high level goal is to investigate how information diffuses in real time on the underlying social network and the role of different users in the propagation process. We plan to implement these analyses with full and partially missing datasets and compare the cost and quality of both approaches.

#index 2030580
#* Mining frequent patterns with differential privacy
#@ Luca Bonomi;Li Xiong
#t 2013
#c 4
#% 152934
#% 576761
#% 819551
#% 864412
#% 913783
#% 960288
#% 985041
#% 1029084
#% 1068712
#% 1206749
#% 1206992
#% 1372692
#% 1451190
#% 1670071
#% 1880451
#% 1907065
#% 1919782
#% 1944323
#% 1962328
#! The mining of frequent patterns is a fundamental component in many data mining tasks. A considerable amount of research on this problem has led to a wide series of efficient and scalable algorithms for mining frequent patterns. However, releasing these patterns is posing concerns on the privacy of the users participating in the data. Indeed the information from the patterns can be linked with a large amount of data available from other sources creating opportunities for adversaries to break the individual privacy of the users and disclose sensitive information. In this proposal, we study the mining of frequent patterns in a privacy preserving setting. We first investigate the difference between sequential and itemset patterns, and second we extend the definition of patterns by considering the absence and presence of noise in the data. This leads us in distinguishing the patterns between exact and noisy. For exact patterns, we describe two novel mining techniques that we previously developed. The first approach has been applied in a privacy preserving record linkage setting, where our solution is used to mine frequent patterns which are employed in a secure transformation procedure to link records that are similar. The second approach improves the mining utility results using a two-phase strategy which allows to effectively mine frequent substrings as well as prefixes patterns. For noisy patterns, first we formally define the patterns according to the type of noise and second we provide a set of potential applications that require the mining of these patterns. We conclude the paper by stating the challenges in this new setting and possible future research directions.

#index 2030581
#* Automatic ontology-based user profile learning from heterogeneous web resources in a big data context
#@ Anett Hoppe;C. Nicolle;A. Roxin
#t 2013
#c 4
#% 198058
#% 287214
#% 763853
#% 863392
#% 934385
#% 956564
#% 1019113
#% 1131827
#% 1176183
#% 1310271
#% 1555377
#% 1715224
#% 1747943
#% 1912790
#% 1957530
#! The Web has developed to the biggest source of information and entertainment in the world. By its size, its adaptability and flexibility, it challenged our current paradigms on information sharing in several areas. By offering everybody the opportunity to release own contents in a fast and cheap way, the Web already led to a revolution of the traditional publishing world and just now, it commences to change the perspective on advertisements. With the possibility to adapt the contents displayed on a page dynamically based on the viewer's context, campaigns launched to target rough customer groups will become an element of the past. However, this new ecosystem, that relates advertisements with the user, heavily relies on the quality of the underlying user profile. This profile has to be able to model any combination of user characteristics, the relations between its composing elements and the uncertainty that stems from the automated processing of real-world data. The work at hand describes the beginnings of a PhD project that aims to tackle those issues using a combination of data analysis, ontology engineering and processing of big data resources provided by an industrial partner. The final goal is to automatically construct and populate a profile ontology for each user identified by the system. This allows to associate these users to high-value audience segments in order to drive digital marketing.

#index 2030582
#* Scalable transactions across heterogeneous NoSQL key-value data stores
#@ Akon Dey;Alan Fekete;Uwe Röhm
#t 2013
#c 4
#% 201869
#% 403195
#% 998845
#% 1054227
#% 1063488
#% 1127560
#% 1137670
#% 1400975
#% 1426489
#% 1426492
#% 1526990
#% 1538766
#% 1625041
#% 1625058
#% 1636459
#% 1875031
#% 1911326
#% 1938401
#! Many cloud systems provide data stores with limited features, especially they may not provide transactions, or else restrict transactions to a single item. We propose a approach that gives multi-item transactions across heterogeneous data stores, using only a minimal set of features from each store such as single item consistency, conditional update, and the ability to include extra metadata within a value. We offer a client-coordinated transaction protocol that does not need a central coordinating infrastructure. A prototype implementation has been built as a Java library and measured with an extension of YCSB benchmark to exercise multi-item transactions.

#index 2030583
#* Getting unique solution in data exchange
#@ Nhung Ngo;Enrico Franconi
#t 2013
#c 4
#% 258208
#% 801691
#% 806215
#% 826032
#% 874882
#% 1054485
#% 1063722
#% 1063723
#% 1153332
#% 1232194
#% 1270567
#% 1424598
#% 1426418
#% 1470497
#% 1581822
#% 1771511
#% 1933388
#! A schema mapping is a high-level specification in which the relationship between two database schemas is described. In data exchange, schema mappings are one-way mappings that describe which data can be brought from source data to target data. Therefore, given a source instance and a mapping, there might be more than one valid target instance. This fact causes many problems in query answering over target data for non-conjunctive queries. To make query answering feasible for all queries, we focus on a methodology for extending the original schema mapping to guarantee the uniqueness of target instance corresponding to a source instance. To this end, we introduce a theoretical framework where the problem is transformed to an abduction problem, namely, definability abduction. We apply the framework to relational data exchange setting and solve the problem by pointing out minimal solutions according to a specific semantic minimality criterion.

#index 2030584
#* Storing and processing temporal data in a main memory column store
#@ Martin Kaufmann;Donald Kossmann
#t 2013
#c 4
#% 163442
#% 287070
#% 565462
#% 571296
#% 800003
#% 1688261
#% 1905961
#% 1972766
#% 2010315
#% 2010409
#! Managing and accessing temporal data is of increasing importance in industry. So far, most companies model the time dimension on the application layer rather than pushing down the operators to the database, which leads to a significant performance overhead. The goal of this PhD thesis is to develop a native support of temporal features for SAP HANA, which is a commercial in-memory column store database system. We investigate different alternatives to store temporal data physically and analyze the trade-offs arising from different memory layouts which cluster the data either by time or by space dimension. Taking into account the underlying physical representation, different temporal operators such as temporal aggregation, time travel and temporal join have to be executed efficiently. We present a novel data structure called Timeline Index and algorithms based on this index, which have a very competitive performance for all temporal operators beating existing best-of-breed approaches by factors, sometimes even by orders of magnitude. The results of this thesis are currently being integrated into HANA, with the goal of being shipped to the customers as a productive release within the next few months.

#index 2030585
#* Efficiency and security in similarity cloud services
#@ Stepan Kozak;Pavel Zezula
#t 2013
#c 4
#% 381870
#% 479973
#% 635689
#% 765448
#% 857113
#% 1132151
#% 1227766
#% 1285930
#% 1377683
#% 1386180
#% 1407672
#% 1414468
#% 1474477
#% 1501149
#% 1557476
#% 1692335
#% 1846713
#% 1907092
#% 2010445
#! With growing popularity of cloud services, the trend in the industry is to outsource the data to a 3rd party system that provides searching in the data as a service. This approach naturally brings privacy concerns about the (potentially sensitive) data. Recently, quite extensive research of outsourcing classic exact-match or keyword search has been done. However, not much attention has been paid to the outsourcing of the similarity search, which becomes more and more important in information retrieval applications. In this work, we propose to the research community a model of outsourcing similarity search to the cloud environment (so called similarity cloud). We establish privacy and efficiency requirements to be laid down for the similarity cloud with an emphasis on practical use of the system in real applications; this requirement list can be used as a general guideline for practical system analysis and we use it to analyze current existing approaches. We propose two new similarity indexes that ensure data privacy and thus are suitable for search systems outsourced in a cloud. The balance of the first proposed technique EM-Index is more on the efficiency side while the other (DSH Index) shifts this balance more to the privacy side.

#index 2030586
#* Fast cartography for data explorers
#@ Thibault Sellam;Martin Kersten
#t 2013
#c 4
#% 115608
#% 378388
#% 434617
#% 443086
#% 459025
#% 949485
#% 1018054
#% 1165480
#% 1218714
#% 1328102
#% 1488676
#! Exploration is the act of investigating unknown regions. An analyst exploring a database cannot, by definition, compose the right query or use the appropriate data mining algorithm. However, current data management tools cannot operate without well defined instructions. Therefore, browsing an unknown database can be a very tedious process. Our project, Atlas, is an attempt to circumvent this problem. Atlas is an active DBMS front-end, designed for database exploration. It generates and ranks several data maps from a user query. A data map is a small set of database queries (less than a dozen), in which each query describes an interesting region of the database. The user can pick one and submit it for further exploration. In order to support interaction, the system should operate in quasi-real time, possibly at the cost of precision, and require as little input parameters as possible. We draft a framework to generate such data maps, and introduce several short-to long-terms research problems.

#index 2030587
#* Question selection for crowd entity resolution
#@ Steven Euijong Whang;Peter Lofgren;Hector Garcia-Molina
#t 2013
#c 4
#% 201889
#% 765548
#% 913783
#% 1328216
#% 1426567
#% 1452857
#% 1581851
#% 1581980
#% 1597466
#% 1628171
#% 1630317
#% 1746845
#% 1880463
#% 1895096
#% 1955028
#! We study the problem of enhancing Entity Resolution (ER) with the help of crowdsourcing. ER is the problem of clustering records that refer to the same real-world entity and can be an extremely difficult process for computer algorithms alone. For example, figuring out which images refer to the same person can be a hard task for computers, but an easy one for humans. We study the problem of resolving records with crowdsourcing where we ask questions to humans in order to guide ER into producing accurate results. Since human work is costly, our goal is to ask as few questions as possible. We propose a probabilistic framework for ER that can be used to estimate how much ER accuracy we obtain by asking each question and select the best question with the highest expected accuracy. Computing the expected accuracy is #P-hard, so we propose approximation techniques for efficient computation. We evaluate our best question algorithms on real and synthetic datasets and demonstrate how we can obtain high ER accuracy while significantly reducing the number of questions asked to humans.

#index 2030588
#* A comparison of knives for bread slicing
#@ Alekh Jindal;Endre Palatinus;Vladimir Pavlov;Jens Dittrich
#t 2013
#c 4
#% 872
#% 58381
#% 69094
#% 411562
#% 445775
#% 462007
#% 765176
#% 765431
#% 990389
#% 1015289
#% 1206624
#% 1523974
#% 1621145
#! Vertical partitioning is a crucial step in physical database design in row-oriented databases. A number of vertical partitioning algorithms have been proposed over the last three decades for a variety of niche scenarios. In principle, the underlying problem remains the same: decompose a table into one or more vertical partitions. However, it is not clear how good different vertical partitioning algorithms are in comparison to each other. In fact, it is not even clear how to experimentally compare different vertical partitioning algorithms. In this paper, we present an exhaustive experimental study of several vertical partitioning algorithms. We categorize vertical partitioning algorithms along three dimensions. We survey six vertical partitioning algorithms and discuss their pros and cons. We identify the major differences in the use-case settings for different algorithms and describe how to make an apples-to-apples comparison of different vertical partitioning algorithms under the same setting. We propose four metrics to compare vertical partitioning algorithms. We show experimental results from the TPC-H and SSB benchmark and present four key lessons learned: (1) we can do four orders of magnitude less computation and still find the optimal layouts, (2) the benefits of vertical partitioning depend strongly on the database buffer size, (3) HillClimb is the best vertical partitioning algorithm, and (4) vertical partitioning for TPC-H-like benchmarks can improve over column layout by only up to 5%.

#index 2030589
#* Efficient error-tolerant query autocompletion
#@ Chuan Xiao;Jianbin Qin;Wei Wang;Yoshiharu Ishikawa;Koji Tsuda;Kunihiko Sadakane
#t 2013
#c 4
#% 2324
#% 66210
#% 82523
#% 288885
#% 319518
#% 333981
#% 379448
#% 397388
#% 408638
#% 480654
#% 654500
#% 765262
#% 766461
#% 879610
#% 999292
#% 1022220
#% 1077150
#% 1190092
#% 1206824
#% 1217200
#% 1217204
#% 1355035
#% 1386395
#% 1400017
#% 1488194
#% 1560144
#% 1560365
#% 1560366
#% 1581932
#% 1618134
#% 1654056
#% 1846689
#! Query autocompletion is an important feature saving users many keystrokes from typing the entire query. In this paper we study the problem of query autocompletion that tolerates errors in users' input using edit distance constraints. Previous approaches index data strings in a trie, and continuously maintain all the prefixes of data strings whose edit distance from the query are within the threshold. The major inherent problem is that the number of such prefixes is huge for the first few characters of the query and is exponential in the alphabet size. This results in slow query response even if the entire query approximately matches only few prefixes. In this paper, we propose a novel neighborhood generation-based algorithm, IncNGTrie, which can achieve up to two orders of magnitude speedup over existing methods for the error-tolerant query autocompletion problem. Our proposed algorithm only maintains a small set of active nodes, thus saving both space and time to process the query. We also study efficient duplicate removal which is a core problem in fetching query answers. In addition, we propose optimization techniques to reduce our index size, as well as discussions on several extensions to our method. The efficiency of our method is demonstrated against existing methods through extensive experiments on real datasets.

#index 2030590
#* Top-k publish-subscribe for social annotation of news
#@ Alexander Shraer;Maxim Gurevich;Marcus Fontoura;Vanja Josifovski
#t 2013
#c 4
#% 198335
#% 228097
#% 271199
#% 333854
#% 333938
#% 342372
#% 378388
#% 387427
#% 479649
#% 646220
#% 661478
#% 730065
#% 731408
#% 818229
#% 869508
#% 1070831
#% 1127386
#% 1292554
#% 1450839
#% 1482227
#! Social content, such as Twitter updates, often have the quickest first-hand reports of news events, as well as numerous commentaries that are indicative of public view of such events. As such, social updates provide a good complement to professionally written news articles. In this paper we consider the problem of automatically annotating news stories with social updates (tweets), at a news website serving high volume of pageviews. The high rate of both the pageviews (millions to billions a day) and of the incoming tweets (more than 100 millions a day) make real-time indexing of tweets ineffective, as this requires an index that is both queried and updated extremely frequently. The rate of tweet updates makes caching techniques almost unusable since the cache would become stale very quickly. We propose a novel architecture where each story is treated as a subscription for tweets relevant to the story's content, and new algorithms that efficiently match tweets to stories, proactively maintaining the top-k tweets for each story. Such top-k pub-sub consumes only a small fraction of the resource cost of alternative solutions, and can be applicable to other large scale content-based publish-subscribe problems. We demonstrate the effectiveness of our approach on realworld data: a corpus of news stories from Yahoo! News and a log of Twitter updates.

#index 2030591
#* Efficient querying of inconsistent databases with binary integer programming
#@ Phokion G. Kolaitis;Enela Pema;Wang-Chiew Tan
#t 2013
#c 4
#% 23902
#% 273687
#% 404719
#% 464915
#% 476576
#% 582130
#% 727668
#% 752741
#% 783532
#% 810020
#% 852145
#% 913783
#% 949372
#% 960262
#% 1100700
#% 1224935
#% 1347336
#% 1370260
#% 1401744
#% 1464322
#% 1494951
#% 1528623
#% 1679459
#% 1747244
#% 1898008
#! An inconsistent database is a database that violates one or more integrity constraints. A typical approach for answering a query over an inconsistent database is to first clean the inconsistent database by transforming it to a consistent one and then apply the query to the consistent database. An alternative and more principled approach, known as consistent query answering, derives the answers to a query over an inconsistent database without changing the database, but by taking into account all possible repairs of the database. In this paper, we study the problem of consistent query answering over inconsistent databases for the class for conjunctive queries under primary key constraints. We develop a system, called EQUIP, that represents a fundamental departure from existing approaches for computing the consistent answers to queries in this class. At the heart of EQUIP is a technique, based on Binary Integer Programming (BIP), that repeatedly searches for repairs to eliminate candidate consistent answers until no further such candidates can be eliminated. We establish rigorously the correctness of the algorithms behind EQUIP and carry out an extensive experimental investigation that validates the effectiveness of our approach. Specifically, EQUIP exhibits good and stable performance on conjunctive queries under primary key constraints, it significantly outperforms existing systems for computing the consistent answers of such queries in the case in which the consistent answers are not first-order rewritable, and it scales well.

#index 2030592
#* Piggybacking on social networks
#@ Aristides Gionis;Flavio Junqueira;Vincent Leroy;Marco Serafini;Ingmar Weber
#t 2013
#c 4
#% 256685
#% 303305
#% 511151
#% 881526
#% 989500
#% 1023420
#% 1426571
#! The popularity of social-networking sites has increased rapidly over the last decade. A basic functionalities of social-networking sites is to present users with streams of events shared by their friends. At a systems level, materialized per-user views are a common way to assemble and deliver such event streams on-line and with low latency. Access to the data stores, which keep the user views, is a major bottleneck of social-networking systems. We propose to improve the throughput of these systems by using social piggybacking, which consists of processing the requests of two friends by querying and updating the view of a third common friend. By using one such hub view, the system can serve requests of the first friend without querying or updating the view of the second. We show that, given a social graph, social piggybacking can minimize the overall number of requests, but computing the optimal set of hubs is an NP-hard problem. We propose an O(log n) approximation algorithm and a heuristic to solve the problem, and evaluate them using the full Twitter and Flickr social graphs, which have up to billions of edges. Compared to existing approaches, using social piggybacking results in similar throughput in systems with few servers, but enables substantial throughput improvements as the size of the system grows, reaching up to a 2-factor increase. We also evaluate our algorithms on a real social networking system prototype and we show that the actual increase in throughput corresponds nicely to the gain anticipated by our cost function.

#index 2030593
#* Schema extraction for tabular data on the web
#@ Marco D. Adelfio;Hanan Samet
#t 2013
#c 4
#% 322880
#% 348147
#% 427326
#% 464434
#% 522565
#% 643004
#% 755816
#% 781729
#% 816181
#% 956500
#% 967256
#% 1127393
#% 1288159
#% 1298932
#% 1328200
#% 1523913
#% 1592311
#% 1770327
#% 1770418
#% 1869827
#! Tabular data is an abundant source of information on the Web, but remains mostly isolated from the latter's interconnections since tables lack links and computer-accessible descriptions of their structure. In other words, the schemas of these tables -- attribute names, values, data types, etc. -- are not explicitly stored as table metadata. Consequently, the structure that these tables contain is not accessible to the crawlers that power search engines and thus not accessible to user search queries. We address this lack of structure with a new method for leveraging the principles of table construction in order to extract table schemas. Discovering the schema by which a table is constructed is achieved by harnessing the similarities and differences of nearby table rows through the use of a novel set of features and a feature processing scheme. The schemas of these data tables are determined using a classification technique based on conditional random fields in combination with a novel feature encoding method called logarithmic binning, which is specifically designed for the data table extraction task. Our method provides considerable improvement over the well-known WebTables schema extraction method. In contrast with previous work that focuses on extracting individual relations, our method excels at correctly interpreting full tables, thereby being capable of handling general tables such as those found in spreadsheets, instead of being restricted to HTML tables as is the case with the WebTables method. We also extract additional schema characteristics, such as row groupings, which are important for supporting information retrieval tasks on tabular data.

#index 2030594
#* Streaming algorithms for k-core decomposition
#@ Ahmet Erdem Saríyüce;Buğra Gedik;Gabriela Jacques-Silva;Kun-Lung Wu;Ümit V. Çatalyürek
#t 2013
#c 4
#% 101409
#% 166767
#% 907530
#% 956540
#% 957998
#% 1100989
#% 1173140
#% 1497614
#% 1506227
#% 1558532
#% 1594586
#% 1635210
#% 1688455
#% 1846696
#! A k-core of a graph is a maximal connected subgraph in which every vertex is connected to at least k vertices in the subgraph. k-core decomposition is often used in large-scale network analysis, such as community detection, protein function prediction, visualization, and solving NP-Hard problems on real networks efficiently, like maximal clique finding. In many real-world applications, networks change over time. As a result, it is essential to develop efficient incremental algorithms for streaming graph data. In this paper, we propose the first incremental k-core decomposition algorithms for streaming graph data. These algorithms locate a small subgraph that is guaranteed to contain the list of vertices whose maximum k-core values have to be updated, and efficiently process this subgraph to update the k-core decomposition. Our results show a significant reduction in run-time compared to non-incremental alternatives. We show the efficiency of our algorithms on different types of real and synthetic graphs, at different scales. For a graph of 16 million vertices, we observe speedups reaching a million times, relative to the non-incremental algorithms.

#index 2030595
#* Discovering linkage points over web data
#@ Oktie Hassanzadeh;Ken Q. Pu;Soheil Hassas Yeganeh;Renée J. Miller;Lucian Popa;Mauricio A. Hernández;Howard Ho
#t 2013
#c 4
#% 321635
#% 378409
#% 572314
#% 654458
#% 765433
#% 893116
#% 924747
#% 1129527
#% 1206834
#% 1269854
#% 1328086
#% 1343447
#% 1455643
#% 1523867
#% 1560246
#% 1876044
#% 1880476
#% 1942724
#% 1962360
#! A basic step in integration is the identification of linkage points, i.e., finding attributes that are shared (or related) between data sources, and that can be used to match records or entities across sources. This is usually performed using a match operator, that associates attributes of one database to another. However, the massive growth in the amount and variety of unstructured and semi-structured data on the Web has created new challenges for this task. Such data sources often do not have a fixed pre-defined schema and contain large numbers of diverse attributes. Furthermore, the end goal is not schema alignment as these schemas may be too heterogeneous (and dynamic) to meaningfully align. Rather, the goal is to align any overlapping data shared by these sources. We will show that even attributes with different meanings (that would not qualify as schema matches) can sometimes be useful in aligning data. The solution we propose in this paper replaces the basic schema-matching step with a more complex instance-based schema analysis and linkage discovery. We present a framework consisting of a library of efficient lexical analyzers and similarity functions, and a set of search algorithms for effective and efficient identification of linkage points over Web data. We experimentally evaluate the effectiveness of our proposed algorithms in real-world integration scenarios in several domains.

#index 2030596
#* IS-Label: an independent-set based labeling scheme for point-to-point distance querying
#@ Ada Wai-Chee Fu;Huanhuan Wu;James Cheng;Raymond Chi-Wing Wong
#t 2013
#c 4
#% 41684
#% 282771
#% 303087
#% 548494
#% 617131
#% 722530
#% 793252
#% 813718
#% 847101
#% 919827
#% 985929
#% 1063472
#% 1091917
#% 1181254
#% 1181255
#% 1263166
#% 1292553
#% 1314064
#% 1328210
#% 1355056
#% 1412885
#% 1426510
#% 1482228
#% 1484129
#% 1581881
#% 1597258
#% 1611331
#% 1676469
#% 1770356
#% 1770357
#% 1880447
#% 1924355
#% 1940424
#! We study the problem of computing shortest path or distance between two query vertices in a graph, which has numerous important applications. Quite a number of indexes have been proposed to answer such distance queries. However, all of these indexes can only process graphs of size barely up to 1 million vertices, which is rather small in view of many of the fast-growing real-world graphs today such as social networks and Web graphs. We propose an efficient index, which is a novel labeling scheme based on the independent set of a graph. We show that our method can handle graphs of size orders of magnitude larger than existing indexes.

#index 2030597
#* Supporting user-defined functions on uncertain data
#@ Thanh T. L. Tran;Yanlei Diao;Charles Sutton;Anna Liu
#t 2013
#c 4
#% 300185
#% 481915
#% 891549
#% 893133
#% 992830
#% 1016178
#% 1127415
#% 1148476
#% 1206717
#% 1206735
#% 1206879
#% 1655707
#% 1910910
#! Uncertain data management has become crucial in many sensing and scientific applications. As user-defined functions (UDFs) become widely used in these applications, an important task is to capture result uncertainty for queries that evaluate UDFs on uncertain data. In this work, we provide a general framework for supporting UDFs on uncertain data. Specifically, we propose a learning approach based on Gaussian processes (GPs) to compute approximate output distributions of a UDF when evaluated on uncertain input, with guaranteed error bounds. We also devise an online algorithm to compute such output distributions, which employs a suite of optimizations to improve accuracy and performance. Our evaluation using both real-world and synthetic functions shows that our proposed GP approach can outperform the state-of-the-art sampling approach with up to two orders of magnitude improvement for a variety of UDFs.

#index 2030598
#* Incremental and accuracy-aware personalized pagerank through scheduled approximation
#@ Fanwei Zhu;Yuan Fang;Kevin Chen-Chuan Chang;Jing Ying
#t 2013
#c 4
#% 577329
#% 641979
#% 898311
#% 956551
#% 1016176
#% 1055877
#% 1206709
#% 1451191
#% 1531275
#% 1581927
#% 1588228
#% 1872230
#! As Personalized PageRank has been widely leveraged for ranking on a graph, the efficient computation of Personalized PageRank Vector (PPV) becomes a prominent issue. In this paper, we propose FastPPV, an approximate PPV computation algorithm that is incremental and accuracy-aware. Our approach hinges on a novel paradigm of scheduled approximation: the computation is partitioned and scheduled for processing in an "organized" way, such that we can gradually improve our PPV estimation in an incremental manner, and quantify the accuracy of our approximation at query time. Guided by this principle, we develop an efficient hub based realization, where we adopt the metric of hub-length to partition and schedule random walk tours so that the approximation error reduces exponentially over iterations. Furthermore, as tours are segmented by hubs, the shared substructures between different tours (around the same hub) can be reused to speed up query processing both within and across iterations. Finally, we evaluate FastPPV over two real-world graphs, and show that it not only significantly outperforms two state-of-the-art baselines in both online and offline phrases, but also scale well on larger graphs. In particular, we are able to achieve near-constant time online query processing irrespective of graph size.

#index 2030599
#* Efficient simrank-based similarity join over large graphs
#@ Weiguo Zheng;Lei Zou;Yansong Feng;Lei Chen;Dongyan Zhao
#t 2013
#c 4
#% 258598
#% 420495
#% 577273
#% 805904
#% 864462
#% 955712
#% 960304
#% 976785
#% 1027251
#% 1181229
#% 1192931
#% 1195980
#% 1206699
#% 1250042
#% 1265149
#% 1292553
#% 1328183
#% 1366465
#% 1372721
#% 1523825
#% 1523900
#% 1581921
#% 1594585
#% 1642126
#! Graphs have been widely used to model complex data in many real-world applications. Answering vertex join queries over large graphs is meaningful and interesting, which can benefit friend recommendation in social networks and link prediction, etc. In this paper, we adopt "SimRank" to evaluate the similarity of two vertices in a large graph because of its generality. Note that "SimRank" is purely structure dependent and it does not rely on the domain knowledge. Specifically, we define a SimRank-based join (SRJ) query to find all the vertex pairs satisfying the threshold in a data graph G. In order to reduce the search space, we propose an estimated shortest-path distance based upper bound for SimRank scores to prune unpromising vertex pairs. In the verification, we propose a novel index, called h-go cover, to efficiently compute the SimRank score of a single vertex pair. Given a graph G, we only materialize the SimRank scores of a small proportion of vertex pairs (called h-go covers), based on which, the SimRank score of any vertex pair can be computed easily. In order to handle large graphs, we extend our technique to the partition-based framework. Thorough theoretical analysis and extensive experiments over both real and synthetic datasets confirm the efficiency and effectiveness of our solution.

#index 2030600
#* A performance study of three disk-based structures for indexing and querying frequent itemsets
#@ Guimei Liu;Andre Suchitra;Limsoon Wong
#t 2013
#c 4
#% 115465
#% 115466
#% 152934
#% 210166
#% 249989
#% 280454
#% 300120
#% 322412
#% 338609
#% 462238
#% 464873
#% 481290
#% 577216
#% 726628
#% 729979
#% 824710
#% 881485
#% 881569
#% 941039
#% 1081577
#% 1594568
#! Frequent itemset mining is an important problem in the data mining area. Extensive efforts have been devoted to developing efficient algorithms for mining frequent itemsets. However, not much attention is paid on managing the large collection of frequent itemsets produced by these algorithms for subsequent analysis and for user exploration. In this paper, we study three structures for indexing and querying frequent itemsets: inverted files, signature files and CFP-tree. The first two structures have been widely used for indexing general set-valued data. We make some modifications to make them more suitable for indexing frequent itemsets. The CFP-tree structure is specially designed for storing frequent itemsets. We add a pruning technique based on length-2 frequent itemsets to make it more efficient for processing superset queries. We study the performance of the three structures in supporting five types of containment queries: exact match, subset/superset search and immediate subset/superset search. Our results show that no structure can outperform other structures for all the five types of queries on all the datasets. CFP-tree shows better overall performance than the other two structures.

#index 2030601
#* TripleBit: a fast and compact system for large scale RDF data
#@ Pingpeng Yuan;Pu Liu;Buwen Wu;Hai Jin;Wenya Zhang;Ling Liu
#t 2013
#c 4
#% 289282
#% 519567
#% 565473
#% 728100
#% 823646
#% 866981
#% 1022236
#% 1055731
#% 1055791
#% 1098453
#% 1127431
#% 1127610
#% 1217194
#% 1366460
#% 1399937
#% 1409918
#% 1523817
#% 1592340
#% 1655412
#% 1746912
#! The volume of RDF data continues to grow over the past decade and many known RDF datasets have billions of triples. A grant challenge of managing this huge RDF data is how to access this big RDF data efficiently. A popular approach to addressing the problem is to build a full set of permutations of (S, P, O) indexes. Although this approach has shown to accelerate joins by orders of magnitude, the large space overhead limits the scalability of this approach and makes it heavyweight. In this paper, we present TripleBit, a fast and compact system for storing and accessing RDF data. The design of TripleBit has three salient features. First, the compact design of TripleBit reduces both the size of stored RDF data and the size of its indexes. Second, TripleBit introduces two auxiliary index structures, ID-Chunk bit matrix and ID-Predicate bit matrix, to minimize the cost of index selection during query evaluation. Third, its query processor dynamically generates an optimal execution ordering for join queries, leading to fast query execution and effective reduction on the size of intermediate results. Our experiments show that TripleBit outperforms RDF-3X, MonetDB, BitMat on LUBM, UniProt and BTC 2012 benchmark queries and it offers orders of mangnitude performance improvement for some complex join queries.

#index 2030602
#* CorrectDB: SQL engine with practical query authentication
#@ Sumeet Bajaj;Radu Sion
#t 2013
#c 4
#% 104987
#% 438481
#% 507696
#% 657774
#% 745532
#% 761411
#% 810042
#% 838424
#% 874980
#% 881074
#% 1022267
#% 1044475
#% 1206896
#% 1211647
#% 1217147
#% 1328176
#% 1478288
#% 1480809
#% 1488562
#% 1581863
#% 1621153
#% 1642101
#% 1642364
#% 1664115
#% 1664117
#! Clients of outsourced databases need Query Authentication (QA) guaranteeing the integrity (correctness and completeness), and authenticity of the query results returned by potentially compromised providers. Existing results provide QA assurances for a limited class of queries by deploying several software cryptographic constructs. Here, we show that, to achieve QA, however, it is significantly cheaper and more practical to deploy server-hosted, tamper-proof co-processors, despite their higher acquisition costs. Further, this provides the ability to handle arbitrary queries. To reach this insight, we extensively survey existing QA work and identify interdependencies and efficiency relationships. We then introduce CorrectDB, a new DBMS with full QA assurances, leveraging server-hosted, tamper-proof, trusted hardware in close proximity to the outsourced data.

#index 2030603
#* Hybrid storage management for database systems
#@ Xin Liu;Kenneth Salem
#t 2013
#c 4
#% 72579
#% 459947
#% 521989
#% 978378
#% 1053457
#% 1092670
#% 1092673
#% 1127391
#% 1328052
#% 1426580
#% 1523920
#% 1523922
#% 1581847
#% 1581941
#% 1869841
#% 1880474
#! The use of flash-based solid state drives (SSDs) in storage systems is growing. Adding SSDs to a storage system not only raises the question of how to manage the SSDs, but also raises the question of whether current buffer pool algorithms will still work effectively. We are interested in the use of hybrid storage systems, consisting of SSDs and hard disk drives (HDDs), for database management. We present cost-aware replacement algorithms, which are aware of the difference in performance between SSDs and HDDs, for both the DBMS buffer pool and the SSDs. In hybrid storage systems, the physical access pattern to the SSDs depends on the management of the DBMS buffer pool. We studied the impact of buffer pool caching policies on SSD access patterns. Based on these studies, we designed a cost-adjusted caching policy to effectively manage the SSD. We implemented these algorithms in MySQL's InnoDB storage engine and used the TPC-C workload to demonstrate that these cost-aware algorithms outperform previous algorithms.

#index 2030604
#* Scorpion: explaining away outliers in aggregate queries
#@ Eugene Wu;Samuel Madden
#t 2013
#c 4
#% 318704
#% 459025
#% 464215
#% 479957
#% 480820
#% 1023420
#% 1041316
#% 1127414
#% 1581888
#% 1581916
#% 1741027
#% 1765139
#% 1800591
#% 2010396
#! Database users commonly explore large data sets by running aggregate queries that project the data down to a smaller number of points and dimensions, and visualizing the results. Often, such visualizations will reveal outliers that correspond to errors or surprising features of the input data set. Unfortunately, databases and visualization systems do not provide a way to work backwards from an outlier point to the common properties of the (possibly many) unaggregated input tuples that correspond to that outlier. We propose Scorpion, a system that takes a set of user-specified outlier points in an aggregate query result as input and finds predicates that explain the outliers in terms of properties of the input tuples that are used to compute the selected outlier results. Specifically, this explanation identifies predicates that, when applied to the input data, cause the outliers to disappear from the output. To find such predicates, we develop a notion of influence of a predicate on a given output, and design several algorithms that efficiently search for maximum influence predicates over the input data. We show that these algorithms can quickly find outliers in two real data sets (from a sensor deployment and a campaign finance data set), and run orders of magnitude faster than a naive search algorithm while providing comparable quality on a synthetic data set.

#index 2030605
#* Ratio threshold queries over distributed data sources
#@ Rajeev Gupta;Krithi Ramamritham;Mukesh Mohania
#t 2013
#c 4
#% 654443
#% 654488
#% 805844
#% 874994
#% 956526
#% 997490
#% 1016145
#% 1327640
#! Continuous aggregation queries over dynamic data are used for real time decision making and timely business intelligence. In this paper we consider queries where a client wants to be notified if the ratio of two aggregates over distributed data crosses a specified threshold. Consider these scenarios: a mechanism designed to defend against distributed denial of service attacks may be triggered when the fraction of packets arriving to a subnet is more than 5% of the total packets; or a distributed store chain withdraws its discount on luxury goods when sales of luxury goods constitute more than 20% of the overall sales. The challenge in executing such ratio threshold queries (RTQs) lies in incurring the minimal amount of communication necessary for propagation of updates from data sources to the aggregator node where the client query is executed. We address this challenge by proposing schemes for converting the client ratio threshold condition into conditions on individual distributed data sources. Whenever the condition associated with a source is violated, the source pushes its data values to the aggregator, which in turn pulls data values from other sources to determine whether the client threshold condition is indeed violated. We present algorithms to minimize the number of source condition violations (i.e., the number of pushes) while ensuring that no violation of the client threshold condition is missed. Further, in case of a source condition violation, we propose efficient selective pulling algorithms for intelligently choosing additional sources whose data should be pulled by the aggregator. Using performance evaluation on synthetic and real traces of data updates we show that our algorithms result in up to an order of magnitude less number of messages compared to existing approaches in the literature.

#index 2030606
#* On the complexity of query result diversification
#@ Ting Deng;Wenfei Fan
#t 2013
#c 4
#% 66645
#% 183738
#% 384978
#% 598376
#% 643566
#% 805841
#% 813966
#% 824782
#% 917695
#% 960287
#% 1063713
#% 1075132
#% 1127465
#% 1166473
#% 1181244
#% 1190093
#% 1200806
#% 1206662
#% 1207001
#% 1214668
#% 1217203
#% 1328135
#% 1372731
#% 1450870
#% 1472964
#% 1581410
#% 1581896
#% 1590539
#% 1594636
#% 1620192
#% 1770131
#% 1770140
#% 1770354
#! Query result diversification is a bi-criteria optimization problem for ranking query results. Given a database D, a query Q and a positive integer k, it is to find a set of k tuples from Q(D) such that the tuples are as relevant as possible to the query, and at the same time, as diverse as possible to each other. Subsets of Q(D) are ranked by an objective function defined in terms of relevance and diversity. Query result diversification has found a variety of applications in databases, information retrieval and operations research. This paper studies the complexity of result diversification for relational queries. We identify three problems in connection with query result diversification, to determine whether there exists a set of k tuples that is ranked above a bound with respect to relevance and diversity, to assess the rank of a given k-element set, and to count how many k-element sets are ranked above a given bound. We study these problems for a variety of query languages and for three objective functions. We establish the upper and lower bounds of these problems, all matching, for both combined complexity and data complexity. We also investigate several special settings of these problems, identifying tractable cases.

#index 2030607
#* Streaming quotient filter: a near optimal approximate duplicate detection approach for data streams
#@ Sourav Dutta;Ankur Narang;Suman K. Bera
#t 2013
#c 4
#% 2833
#% 214073
#% 283822
#% 307424
#% 322884
#% 333926
#% 340179
#% 345087
#% 387508
#% 411437
#% 424292
#% 459945
#% 625402
#% 646223
#% 654461
#% 729913
#% 730067
#% 745534
#% 805457
#% 805840
#% 810044
#% 821930
#% 874972
#% 963454
#% 978241
#% 1111952
#% 1171636
#% 1247843
#% 1248210
#% 1482304
#% 1656854
#% 1798394
#% 1880475
#! The unparalleled growth and popularity of the Internet coupled with the advent of diverse modern applications such as search engines, on-line transactions, climate warning systems, etc., has catered to an unprecedented expanse in the volume of data stored world-wide. Efficient storage, management, and processing of such massively exponential amount of data has emerged as a central theme of research in this direction. Detection and removal of redundancies and duplicates in real-time from such multi-trillion record-set to bolster resource and compute efficiency constitutes a challenging area of study. The infeasibility of storing the entire data from potentially unbounded data streams, with the need for precise elimination of duplicates calls for intelligent approximate duplicate detection algorithms. The literature hosts numerous works based on the well-known probabilistic bitmap structure, Bloom Filter and its variants. In this paper we propose a novel data structure, Streaming Quotient Filter, (SQF) for efficient detection and removal of duplicates in data streams. SQF intelligently stores the signatures of elements arriving on a data stream, and along with an eviction policy provides near zero false positive and false negative rates. We show that the near optimal performance of SQF is achieved with a very low memory requirement, making it ideal for real-time memory-efficient de-duplication applications having an extremely low false positive and false negative tolerance rates. We present detailed theoretical analysis of the working of SQF, providing a guarantee on its performance. Empirically, we compare SQF to alternate methods and show that the proposed method is superior in terms of memory and accuracy compared to the existing solutions. We also discuss Dynamic SQF for evolving streams and the parallel implementation of SQF.

#index 2030608
#* On repairing structural problems in semi-structured data
#@ Flip Korn;Barna Saha;Divesh Srivastava;Shanshan Ying
#t 2013
#c 4
#% 66654
#% 183698
#% 288885
#% 333679
#% 344422
#% 378392
#% 397373
#% 659923
#% 810019
#% 810555
#% 893098
#% 1022285
#% 1349285
#% 1426276
#% 1523812
#% 1581885
#% 1602922
#% 1642117
#% 1661446
#% 1668638
#% 1728672
#% 1870311
#! Semi-structured data such as XML are popular for data interchange and storage. However, many XML documents have improper nesting where open - and close-tags are unmatched. Since some semi-structured data (e.g., Latex) have a flexible grammar and since many XML documents lack an accompanying DTD or XSD, we focus on computing a syntactic repair via the edit distance. To solve this problem, we propose a dynamic programming algorithm which takes cubic time. While this algorithm is not scalable, well-formed substrings of the data can be pruned to enable faster computation. Unfortunately, there are still cases where the dynamic program could be very expensive; hence, we give branch-and-bound algorithms based on various combinations of two heuristics, called MinCost and MaxBenefit, that trade off between accuracy and efficiency. Finally, we experimentally demonstrate the performance of these algorithms on real data.

#index 2030609
#* A distributed algorithm for large-scale generalized matching
#@ Faraz Makari Manshadi;Baruch Awerbuch;Rainer Gemulla;Rohit Khandekar;Julián Mestre;Mauro Sozio
#t 2013
#c 4
#% 66387
#% 109653
#% 122671
#% 150164
#% 320150
#% 656764
#% 879398
#% 919831
#% 985928
#% 1211745
#% 1211807
#% 1299401
#% 1426655
#% 1431676
#% 1433951
#% 1581411
#% 1581996
#% 1605920
#% 1665186
#% 1769265
#% 1872376
#! Generalized matching problems arise in a number of applications, including computational advertising, recommender systems, and trade markets. Consider, for example, the problem of recommending multimedia items (e.g., DVDs) to users such that (1) users are recommended items that they are likely to be interested in, (2) every user gets neither too few nor too many recommendations, and (3) only items available in stock are recommended to users. State-of-the-art matching algorithms fail at coping with large real-world instances, which may involve millions of users and items. We propose the first distributed algorithm for computing near-optimal solutions to large-scale generalized matching problems like the one above. Our algorithm is designed to run on a small cluster of commodity nodes (or in a MapReduce environment), has strong approximation guarantees, and requires only a poly-logarithmic number of passes over the input. In particular, we propose a novel distributed algorithm to approximately solve mixed packing-covering linear programs, which include but are not limited to generalized matching problems. Experiments on real-world and synthetic data suggest that a practical variant of our algorithm scales to very large problem sizes and can be orders of magnitude faster than alternative approaches.

#index 2030610
#* The LLUNATIC data-cleaning framework
#@ Floris Geerts;Giansalvatore Mecca;Paolo Papotti;Donatello Santoro
#t 2013
#c 4
#% 583
#% 663
#% 273687
#% 384978
#% 727668
#% 810019
#% 826032
#% 1022228
#% 1054480
#% 1054484
#% 1063725
#% 1130461
#% 1180000
#% 1206717
#% 1370260
#% 1523810
#% 1523812
#% 1538793
#% 1550749
#% 1581824
#% 1581885
#% 1618131
#% 1897972
#% 1898008
#! Data-cleaning (or data-repairing) is considered a crucial problem in many database-related tasks. It consists in making a database consistent with respect to a set of given constraints. In recent years, repairing methods have been proposed for several classes of constraints. However, these methods rely on ad hoc decisions and tend to hard-code the strategy to repair conflicting values. As a consequence, there is currently no general algorithm to solve database repairing problems that involve different kinds of constraints and different strategies to select preferred values. In this paper we develop a uniform framework to solve this problem. We propose a new semantics for repairs, and a chase-based algorithm to compute minimal solutions. We implemented the framework in a DBMS-based prototype, and we report experimental results that confirm its good scalability and superior quality in computing repairs.

#index 2030611
#* Sharing data and work across concurrent analytical queries
#@ Iraklis Psaroudakis;Manos Athanassoulis;Anastasia Ailamaki
#t 2013
#c 4
#% 36117
#% 152943
#% 286991
#% 300166
#% 333848
#% 393641
#% 463578
#% 481450
#% 565469
#% 617869
#% 810039
#% 830700
#% 993385
#% 1022230
#% 1022231
#% 1022262
#% 1023420
#% 1127399
#% 1181215
#% 1328132
#% 1328168
#% 1426545
#% 1565407
#% 1730732
#! Today's data deluge enables organizations to collect massive data, and analyze it with an ever-increasing number of concurrent queries. Traditional data warehouses (DW) face a challenging problem in executing this task, due to their query-centric model: each query is optimized and executed independently. This model results in high contention for resources. Thus, modern DW depart from the query-centric model to execution models involving sharing of common data and work. Our goal is to show when and how a DW should employ sharing. We evaluate experimentally two sharing methodologies, based on their original prototype systems, that exploit work sharing opportunities among concurrent queries at run-time: Simultaneous Pipelining (SP), which shares intermediate results of common sub-plans, and Global Query Plans (GQP), which build and evaluate a single query plan with shared operators. First, after a short review of sharing methodologies, we show that SP and GQP are orthogonal techniques. SP can be applied to shared operators of a GQP, reducing response times by 20%-48% in workloads with numerous common sub-plans. Second, we corroborate previous results on the negative impact of SP on performance for cases of low concurrency. We attribute this behavior to a bottleneck caused by the push-based communication model of SP. We show that pull-based communication for SP eliminates the overhead of sharing altogether for low concurrency, and scales better on multi-core machines than push-based SP, further reducing response times by 82%-86% for high concurrency. Third, we perform an experimental analysis of SP, GQP and their combination, and show when each one is beneficial. We identify a trade-off between low and high concurrency. In the former case, traditional query-centric operators with SP perform better, while in the latter case, GQP with shared operators enhanced by SP give the best results.

#index 2030612
#* Skyline operator on anti-correlated distributions
#@ Haichuan Shang;Masaru Kitsuregawa
#t 2013
#c 4
#% 288976
#% 289148
#% 319601
#% 321455
#% 465167
#% 654480
#% 800555
#% 864451
#% 903013
#% 993954
#% 1092017
#% 1217183
#% 1217185
#% 1328116
#% 1372698
#% 1443437
#% 1581826
#! Finding the skyline in a multi-dimensional space is relevant to a wide range of applications. The skyline operator over a set of d-dimensional points selects the points that are not dominated by any other point on all dimensions. Therefore, it provides a minimal set of candidates for the users to make their personal trade-off among all optimal solutions. The existing algorithms establish both the worst case complexity by discarding distributions and the average case complexity by assuming dimensional independence. However, the data in the real world is more likely to be anti-correlated. The cardinality and complexity analysis on dimensionally independent data is meaningless when dealing with anti-correlated data. Furthermore, the performance of the existing algorithms becomes impractical on anti-correlated data. In this paper, we establish a cardinality model for anti-correlated distributions. We propose an accurate polynomial estimation for the expected value of the skyline cardinality. Because the high skyline cardinality downgrades the performance of most existing algorithms on anti-correlated data, we further develop a determination and elimination framework which extends the well-adopted elimination strategy. It achieves remarkable effectiveness and efficiency. The comprehensive experiments on both real datasets and benchmark synthetic datasets demonstrate that our approach significantly outperforms the state-of-the-art algorithms under a wide range of settings.

#index 2030613
#* Low-latency multi-datacenter databases using replicated commit
#@ Hatem Mahmoud;Faisal Nawab;Alexander Pucher;Divyakant Agrawal;Amr El Abbadi
#t 2013
#c 4
#% 4800
#% 9241
#% 166215
#% 201869
#% 251359
#% 336201
#% 444142
#% 689722
#% 810043
#% 866984
#% 978404
#% 989488
#% 998845
#% 1009832
#% 1013977
#% 1127560
#% 1127596
#% 1146717
#% 1238427
#% 1426489
#% 1426589
#% 1541196
#% 1592341
#% 1625032
#% 1625057
#% 1625058
#% 1770319
#% 1911326
#% 1972829
#! Web service providers have been using NoSQL datastores to provide scalability and availability for globally distributed data at the cost of sacrificing transactional guarantees. Recently, major web service providers like Google have moved towards building storage systems that provide ACID transactional guarantees for globally distributed data. For example, the newly published system, Spanner, uses Two-Phase Commit and Two-Phase Locking to provide atomicity and isolation for globally distributed data, running on top of Paxos to provide fault-tolerant log replication. We show in this paper that it is possible to provide the same ACID transactional guarantees for multi-datacenter databases with fewer cross-datacenter communication trips, compared to replicated logging. Instead of replicating the transactional log, we replicate the commit operation itself, by running Two-Phase Commit multiple times in different datacenters and using Paxos to reach consensus among datacenters as to whether the transaction should commit. Doing so not only replaces several inter-datacenter communication trips with intra-datacenter communication trips, but also allows us to integrate atomic commitment and isolation protocols with consistent replication protocols to further reduce the number of cross-datacenter communication trips needed for consistent replication; for example, by eliminating the need for an election phase in Paxos. We analyze our approach in terms of communication trips to compare it against the log replication approach, then we conduct an extensive experimental study to compare the performance and scalability of both approaches under various multi-datacenter setups.

#index 2030614
#* Distribution-based query scheduling
#@ Yun Chi;Hakan Hacígümüş;Wang-Pin Hsiung;Jeffrey F. Naughton
#t 2013
#c 4
#% 130338
#% 152933
#% 302486
#% 309705
#% 341937
#% 408638
#% 435110
#% 565341
#% 754085
#% 781787
#% 785216
#% 786866
#% 843801
#% 978779
#% 998845
#% 1022748
#% 1127968
#% 1181272
#% 1189857
#% 1206909
#% 1206984
#% 1357693
#% 1373477
#% 1581874
#% 1592314
#% 1621139
#% 1863150
#% 1880469
#% 1913804
#! Query scheduling, a fundamental problem in database management systems, has recently received a renewed attention, perhaps in part due to the rise of the "database as a service" (DaaS) model for database deployment. While there has been a great deal of work investigating different scheduling algorithms, there has been comparatively little work investigating what the scheduling algorithms can or should know about the queries to be scheduled. In this work, we investigate the efficacy of using histograms describing the distribution of likely query execution times as input to the query scheduler. We propose a novel distribution-based scheduling algorithm, Shepherd, and show that Shepherd substantially outperforms state-of-the-art point-based methods through extensive experimentation with both synthetic and TPC workloads.

#index 2030615
#* Making queries tractable on big data with preprocessing: through the eyes of complexity theory
#@ Wenfei Fan;Floris Geerts;Frank Neven
#t 2013
#c 4
#% 69503
#% 101922
#% 183411
#% 194127
#% 205419
#% 216513
#% 219211
#% 245657
#% 378409
#% 384978
#% 390132
#% 404719
#% 571628
#% 572311
#% 643566
#% 857282
#% 1023420
#% 1063501
#% 1064994
#% 1068068
#% 1214643
#% 1349599
#% 1369418
#% 1372690
#% 1407271
#% 1451193
#% 1472960
#% 1484141
#% 1504043
#% 1549835
#% 1560413
#% 1574712
#% 1581836
#% 1581923
#% 1654514
#% 1770332
#% 1798375
#% 1798386
#! A query class is traditionally considered tractable if there exists a polynomial-time (PTIME) algorithm to answer its queries. When it comes to big data, however, PTIME algorithms often become infeasible in practice. A traditional and effective approach to coping with this is to preprocess data off-line, so that queries in the class can be subsequently evaluated on the data efficiently. This paper aims to provide a formal foundation for this approach in terms of computational complexity. (1) We propose a set of Π-tractable queries, denoted by ΠTQ0, to characterize classes of queries that can be answered in parallel poly-logarithmic time (NC) after PTIME preprocessing. (2) We show that several natural query classes are Π-tractable and are feasible on big data. (3) We also study a set ΠTQ of query classes that can be effectively converted to Π-tractable queries by refactorizing its data and queries for preprocessing. We introduce a form of NC reductions to characterize such conversions. (4) We show that a natural query class is complete for ΠTQ. (5) We also show that ΠTQ0 ⊂ P unless P = NC, i.e., the set ΠTQ0 of all Π-tractable queries is properly contained in the set P of all PTIME queries. Nonetheless, ΠTQ = P, i.e., all PTIME query classes can be made Π-tractable via proper refactorizations. This work is a step towards understanding the tractability of queries in the context of big data.

#index 2030616
#* Answering planning queries with the crowd
#@ Haim Kaplan;Ilia Lotosh;Tova Milo;Slava Novgorodov
#t 2013
#c 4
#% 643566
#% 743353
#% 813966
#% 1065099
#% 1292493
#% 1526538
#% 1581851
#% 1581964
#% 1730733
#% 1746876
#% 1746896
#% 1770349
#% 1770351
#% 1846744
#% 1869838
#% 1880463
#% 1895096
#% 1959488
#% 2010446
#! Recent research has shown that crowd sourcing can be used effectively to solve problems that are difficult for computers, e.g., optical character recognition and identification of the structural configuration of natural proteins. In this paper we propose to use the power of the crowd to address yet another difficult problem that frequently occurs in a daily life - answering planning queries whose output is a sequence of objects/actions, when the goal, i.e, the notion of "best output", is hard to formalize. For example, planning the sequence of places/attractions to visit in the course of a vacation, where the goal is to enjoy the resulting vacation the most, or planning the sequence of courses to take in an academic schedule planning, where the goal is to obtain solid knowledge of a given subject domain. Such goals may be easily understandable by humans, but hard or even impossible to formalize for a computer. We present a novel algorithm for efficiently harnessing the crowd to assist in answering such planning queries. The algorithm builds the desired plans incrementally, choosing at each step the 'best' questions so that the overall number of questions that need to be asked is minimized. We prove the algorithm to be optimal within its class and demonstrate experimentally its effectiveness and efficiency.

#index 2030617
#* Hardware-oblivious parallelism for in-memory column-stores
#@ Max Heimel;Michael Saecker;Holger Pirk;Stefan Manegold;Volker Markl
#t 2013
#c 4
#% 599235
#% 765419
#% 850738
#% 874997
#% 961238
#% 988651
#% 1023420
#% 1051712
#% 1052066
#% 1063508
#% 1089604
#% 1241839
#% 1268643
#% 1270566
#% 1328057
#% 1328185
#% 1426486
#% 1426531
#% 1488666
#% 1491875
#% 1541400
#% 1609101
#% 1917212
#% 1933410
#% 1933421
#! The multi-core architectures of today's computer systems make parallelism a necessity for performance critical applications. Writing such applications in a generic, hardware-oblivious manner is a challenging problem: Current database systems thus rely on labor-intensive and error-prone manual tuning to exploit the full potential of modern parallel hardware architectures like multi-core CPUs and graphics cards. We propose an alternative design for a parallel database engine, based on a single set of hardware-oblivious operators, which are compiled down to the actual hardware at runtime. This design reduces the development overhead for parallel database engines, while achieving competitive performance to hand-tuned systems. We provide a proof-of-concept for this design by integrating operators written using the parallel programming framework OpenCL into the open-source database MonetDB. Following this approach, we achieve efficient, yet highly portable parallel code without the need for optimization by hand. We evaluated our implementation against MonetDB using TPC-H derived queries and observed a performance that rivals that of MonetDB's query execution on the CPU and surpasses it on the GPU. In addition, we show that the same set of operators runs nearly unchanged on a GPU, demonstrating the feasibility of our approach.

#index 2030618
#* Permuting data on random-access block storage
#@ Risi Thonangi;Jun Yang
#t 2013
#c 4
#% 146881
#% 152687
#% 227880
#% 341100
#% 442578
#% 479450
#% 481951
#% 706982
#% 1012007
#% 1426531
#% 2010440
#! Permutation is a fundamental operator for array data, with applications in, for example, changing matrix layouts and reorganizing data cubes. We consider the problem of permuting large quantities of data stored on secondary storage that supports fast random block accesses, such as solid state drives and distributed key-value stores. Faster random accesses open up interesting new opportunities for permutation. While external merge sort has often been used for permutation, it is an overkill that fails to exploit the property of permutation fully and carries unnecessary overhead in storing and comparing keys. We propose faster algorithms with lower memory requirements for a large, useful class of permutations. We also tackle practical challenges that traditional permutation algorithms have not dealt with, such as exploiting random block accesses more aggressively, considering the cost asymmetry between reads and writes, and handling arbitrary data dimension sizes (as opposed to perfect powers often assumed by previous work). As a result, our algorithms are faster and more broadly applicable.

#index 2030619
#* Improving flash write performance by using update frequency
#@ Radu Stoica;Anastasia Ailamaki
#t 2013
#c 4
#% 107692
#% 131555
#% 176348
#% 279648
#% 978505
#% 985754
#% 1085291
#% 1124295
#% 1127302
#% 1150117
#% 1174229
#% 1181215
#% 1311636
#% 1480363
#% 1581846
#% 1639584
#% 1639693
#% 1765822
#% 1786461
#% 1995678
#! Solid-state drives (SSDs) are quickly becoming the default storage medium as the cost of NAND flash memory continues to drop. However, flash memory introduces new challenges, as data cannot be eciently updated in-place. To overcome the technology's limitations, SSDs incorporate a software Flash Translation Layer (FTL) that implements out-of-place updates, typically by storing data in a log-structured fashion. Despite a large number of existing FTL algorithms, SSD performance, predictability, and lifetime remain an issue, especially for the write-intensive workloads specific to database applications. In this paper, we show how to design FTLs that are more efficient by using the I/O write skew to guide data placement on flash memory. We model the relationship between data placement and write performance for basic I/O write patterns and detail the most important concepts of writing to flash memory: i) the trade-o between the extra capacity available and write overhead, ii) the benefit of adapting data placement to write skew, iii) the impact of the cleaning policy, and iv) how to estimate the best achievable write performance for a given I/O workload. Based on the findings of the theoretical model, we propose a new principled data placement algorithm that can be incorporated into existing FTLs. We show the benefits of our data placement algorithm when running micro-benchmarks and real database I/O traces: our data placement algorithm reduces write overhead by 20% - 75% when compared to state-of-art techniques.

#index 2030620
#* Efficient indexing for diverse query results
#@ Lu Li;Chee-Yong Chan
#t 2013
#c 4
#% 805841
#% 879686
#% 1206662
#% 1348342
#% 1472964
#% 1581410
#% 1581911
#% 1594636
#! This paper examines the problem of computing diverse query results which is useful for browsing search results in online shopping applications. The search results are diversified wrt a sequence of output attributes (termed d-order) where an attribute that appears earlier in the d-order has higher priority for diversification. We present a new indexing technique, D-Index, to efficiently compute diverse query results for queries with static or dynamic d-orders. Our performance evaluation demonstrates that our D-Index outperforms the state-of-the-art techniques developed for queries with static or dynamic d-orders.

#index 2030621
#* Reducing uncertainty of schema matching via crowdsourcing
#@ Chen Jason Zhang;Lei Chen;H. V. Jagadish;Chen Caleb Cao
#t 2013
#c 4
#% 297675
#% 480134
#% 572314
#% 800005
#% 960246
#% 993981
#% 1063534
#% 1190673
#% 1206636
#% 1206833
#% 1206962
#% 1217251
#% 1526538
#% 1550748
#% 1581851
#% 1730010
#% 1846710
#% 1880463
#% 1880477
#! Schema matching is a central challenge for data integration systems. Automated tools are often uncertain about schema matchings they suggest, and this uncertainty is inherent since it arises from the inability of the schema to fully capture the semantics of the represented data. Human common sense can often help. Inspired by the popularity and the success of easily accessible crowdsourcing platforms, we explore the use of crowdsourcing to reduce the uncertainty of schema matching. Since it is typical to ask simple questions on crowdsourcing platforms, we assume that each question, namely Correspondence Correctness Question (CCQ), is to ask the crowd to decide whether a given correspondence should exist in the correct matching. We propose frameworks and efficient algorithms to dynamically manage the CCQs, in order to maximize the uncertainty reduction within a limited budget of questions. We develop two novel approaches, namely "Single CCQ" and "Multiple CCQ", which adaptively select, publish and manage the questions. We verified the value of our solutions with simulation and real implementation.

#index 2030622
#* Travel cost inference from sparse, spatio temporally correlated time series using Markov models
#@ Bin Yang;Chenjuan Guo;Christian S. Jensen
#t 2013
#c 4
#% 864397
#% 891559
#% 1044452
#% 1077150
#% 1581878
#% 1594572
#% 1605948
#% 1941009
#! The monitoring of a system can yield a set of measurements that can be modeled as a collection of time series. These time series are often sparse, due to missing measurements, and spatiotemporally correlated, meaning that spatially close time series exhibit temporal correlation. The analysis of such time series offers insight into the underlying system and enables prediction of system behavior. While the techniques presented in the paper apply more generally, we consider the case of transportation systems and aim to predict travel cost from GPS tracking data from probe vehicles. Specifically, each road segment has an associated travel-cost time series, which is derived from GPS data. We use spatio-temporal hidden Markov models (STHMM) to model correlations among different traffic time series. We provide algorithms that are able to learn the parameters of an STHMM while contending with the sparsity, spatio-temporal correlation, and heterogeneity of the time series. Using the resulting STHMM, near future travel costs in the transportation network, e.g., travel time or greenhouse gas emissions, can be inferred, enabling a variety of routing services, e.g., eco-routing. Empirical studies with a substantial GPS data set offer insight into the design properties of the proposed framework and algorithms, demonstrating the effectiveness and efficiency of travel cost inferencing.

#index 2046050
#* Probabilistic query rewriting for efficient and effective keyword search on graph data
#@ Lei Zhang;Thanh Tran;Achim Rettinger
#t 2013
#c 4
#% 104443
#% 824693
#% 869501
#% 875017
#% 939629
#% 960243
#% 960259
#% 1015325
#% 1055706
#% 1063537
#% 1127423
#% 1206910
#% 1482251
#% 1594557
#% 1618514
#% 1642090
#% 1846822
#! The problem of rewriting keyword search queries on graph data has been studied recently, where the main goal is to clean user queries by rewriting keywords as valid tokens appearing in the data and grouping them into meaningful segments. The main solution to this problem employs heuristics for ranking query rewrites and a dynamic programming algorithm for computing them. Based on a broader set of queries defined by an existing benchmark, we show that the use of these heuristics does not yield good results. We propose a novel probabilistic framework, which enables the optimality of a query rewrite to be estimated in a more principled way. We show that our approach outperforms existing work in terms of effectiveness and efficiency of query rewriting. More importantly, we provide the first results indicating query rewriting can indeed improve overall keyword search runtime performance and result quality.

#index 2046051
#* QuEval: beyond high-dimensional indexing à la carte
#@ Martin Schäler;Alexander Grebhahn;Reimar Schröter;Sandro Schulze;Veit Köppen;Gunter Saake
#t 2013
#c 4
#% 86950
#% 103743
#% 248796
#% 249321
#% 252304
#% 321455
#% 342828
#% 427199
#% 435141
#% 479462
#% 479649
#% 479973
#% 481599
#% 481956
#% 727659
#% 762054
#% 814646
#% 1001153
#% 1116394
#% 1407672
#% 1619890
#% 1652160
#% 1775143
#% 1846754
#% 1931836
#% 2028667
#! In the recent past, the amount of high-dimensional data, such as feature vectors extracted from multimedia data, increased dramatically. A large variety of indexes have been proposed to store and access such data efficiently. However, due to specific requirements of a certain use case, choosing an adequate index structure is a complex and time-consuming task. This may be due to engineering challenges or open research questions. To overcome this limitation, we present QuEval, an open-source framework that can be flexibly extended w.r.t. index structures, distance metrics, and data sets. QuEval provides a unified environment for a sound evaluation of different indexes, for instance, to support tuning of indexes. In an empirical evaluation, we show how to apply our framework, motivate benefits, and demonstrate analysis possibilities.

#index 2046052
#* Discovering longest-lasting correlation in sequence databases
#@ Yuhong Li;Leong Hou U;Man Lung Yiu;Zhiguo Gong
#t 2013
#c 4
#% 86950
#% 172949
#% 281750
#% 296738
#% 321455
#% 443397
#% 465160
#% 480146
#% 480654
#% 629607
#% 631920
#% 643518
#% 740761
#% 760529
#% 810058
#% 823413
#% 992857
#% 993961
#% 998465
#% 1015330
#% 1044456
#% 1063497
#% 1127609
#% 1176986
#% 1211645
#% 1378458
#% 1426516
#% 1535372
#% 1590537
#% 1606057
#% 1606071
#% 1872261
#% 1880471
#! Most existing work on sequence databases use correlation (e.g., Euclidean distance and Pearson correlation) as a core function for various analytical tasks. Typically, it requires users to set a length for the similarity queries. However, there is no steady way to define the proper length on different application needs. In this work we focus on discovering longest-lasting highly correlated subsequences in sequence databases, which is particularly useful in helping those analyses without prior knowledge about the query length. Surprisingly, there has been limited work on this problem. A baseline solution is to calculate the correlations for every possible subsequence combination. Obviously, the brute force solution is not scalable for large datasets. In this work we study a space-constrained index that gives a tight correlation bound for subsequences of similar length and offset by intra-object grouping and inter-object grouping techniques. To the best of our knowledge, this is the first index to support normalized distance metric of arbitrary length subsequences. Extensive experimental evaluation on both real and synthetic sequence datasets verifies the efficiency and effectiveness of our proposed methods.

#index 2046053
#* PREDIcT: towards predicting the runtime of large scale iterative analytics
#@ Adrian Daniel Popescu;Andrey Balmin;Vuk Ercegovac;Anastasia Ailamaki
#t 2013
#c 4
#% 13014
#% 69503
#% 248821
#% 480803
#% 650944
#% 790143
#% 810056
#% 871356
#% 881526
#% 1026989
#% 1055741
#% 1206984
#% 1318636
#% 1328066
#% 1426513
#% 1426544
#% 1449326
#% 1475077
#% 1523820
#% 1549835
#% 1581874
#% 1588764
#% 1608685
#% 1688297
#% 1769265
#% 1880445
#% 1880469
#% 1901411
#% 1972833
#! Machine learning algorithms are widely used today for analytical tasks such as data cleaning, data categorization, or data filtering. At the same time, the rise of social media motivates recent uptake in large scale graph processing. Both categories of algorithms are dominated by iterative subtasks, i.e., processing steps which are executed repetitively until a convergence condition is met. Optimizing cluster resource allocations among multiple workloads of iterative algorithms motivates the need for estimating their runtime, which in turn requires: i) predicting the number of iterations, and ii) predicting the processing time of each iteration. As both parameters depend on the characteristics of the dataset and on the convergence function, estimating their values before execution is difficult. This paper proposes PREDIcT, an experimental methodology for predicting the runtime of iterative algorithms. PREDIcT uses sample runs for capturing the algorithm's convergence trend and per-iteration key input features that are well correlated with the actual processing requirements of the complete input dataset. Using this combination of characteristics we predict the runtime of iterative algorithms, including algorithms with very different runtime patterns among subsequent iterations. Our experimental evaluation of multiple algorithms on scale-free graphs shows a relative prediction error of 10%-30% for predicting runtime, including algorithms with up to 100× runtime variability among consecutive iterations.

#index 2046054
#* On the embeddability of random walk distances
#@ Xiaohan Zhao;Adelbert Chang;Atish Das Sarma;Haitao Zheng;Ben Y. Zhao
#t 2013
#c 4
#% 274385
#% 577328
#% 577329
#% 730089
#% 745373
#% 770874
#% 805897
#% 823342
#% 850660
#% 869492
#% 881491
#% 889651
#% 898311
#% 902513
#% 937549
#% 949164
#% 956551
#% 983330
#% 1061639
#% 1080347
#% 1114786
#% 1130835
#% 1183359
#% 1291642
#% 1475163
#% 1502090
#% 1581927
#% 1667624
#% 1710594
#% 1915003
#% 1920073
#! Analysis of large graphs is critical to the ongoing growth of search engines and social networks. One class of queries centers around node affinity, often quantified by random-walk distances between node pairs, including hitting time, commute time, and personalized PageRank (PPR). Despite the potential of these "metrics," they are rarely, if ever, used in practice, largely due to extremely high computational costs. In this paper, we investigate methods to scalably and efficiently compute random-walk distances, by "embedding" graphs and distances into points and distances in geometric coordinate spaces. We show that while existing graph coordinate systems (GCS) can accurately estimate shortest path distances, they produce significant errors when embedding random-walk distances. Based on our observations, we propose a new graph embedding system that explicitly accounts for per-node graph properties that affect random walk. Extensive experiments on a range of graphs show that our new approach can accurately estimate both symmetric and asymmetric random-walk distances. Once a graph is embedded, our system can answer queries between any two nodes in 8 microseconds, orders of magnitude faster than existing methods. Finally, we show that our system produces estimates that can replace ground truth in applications with minimal impact on application output.

#index 2046055
#* Instant loading for main memory databases
#@ Tobias Mühlbauer;Wolf Rödiger;Robert Seilbeck;Angelika Reiser;Alfons Kemper;Thomas Neumann
#t 2013
#c 4
#% 397361
#% 479630
#% 480119
#% 845351
#% 857498
#% 1023420
#% 1034491
#% 1127596
#% 1133537
#% 1217159
#% 1328095
#% 1328141
#% 1328186
#% 1574740
#% 1592312
#% 1592316
#% 1594617
#% 1770339
#% 1880472
#% 1882103
#% 1971521
#% 2010343
#! eScience and big data analytics applications are facing the challenge of efficiently evaluating complex queries over vast amounts of structured text data archived in network storage solutions. To analyze such data in traditional disk-based database systems, it needs to be bulk loaded, an operation whose performance largely depends on the wire speed of the data source and the speed of the data sink, i.e., the disk. As the speed of network adapters and disks has stagnated in the past, loading has become a major bottleneck. The delays it is causing are now ubiquitous as text formats are a preferred storage format for reasons of portability. But the game has changed: Ever increasing main memory capacities have fostered the development of in-memory database systems and very fast network infrastructures are on the verge of becoming economical. While hardware limitations for fast loading have disappeared, current approaches for main memory databases fail to saturate the now available wire speeds of tens of Gbit/s. With Instant Loading, we contribute a novel CSV loading approach that allows scalable bulk loading at wire speed. This is achieved by optimizing all phases of loading for modern super-scalar multi-core CPUs. Large main memory capacities and Instant Loading thereby facilitate a very efficient data staging processing model consisting of instantaneous load-work-unload cycles across data archives on a single node. Once data is loaded, updates and queries are efficiently processed with the flexibility, security, and high performance of relational main memory databases.

#index 2046056
#* Adaptive range filters for cold data: avoiding trips to Siberia
#@ Karolina Alexiou;Donald Kossmann;Per-Åke Larson
#t 2013
#c 4
#% 172902
#% 256883
#% 288718
#% 322884
#% 333938
#% 340619
#% 481916
#% 810025
#% 985941
#% 1015256
#% 1082691
#% 1971522
#% 2010312
#! Bloom filters are a great technique to test whether a key is not in a set of keys. This paper presents a novel data structure called ARF. In a nutshell, ARFs are for range queries what Bloom filters are for point queries. That is, an ARF can determine whether a set of keys does not contain any keys that are part of a specific range. This paper describes the principles and methods for efficient implementation of ARFs and presents the results of comprehensive experiments that assess the precision, space, and latency of ARFs. Furthermore, this paper shows how ARFs can be applied to a commercial database system that partitions data into hot and cold regions to optimize queries that involve only hot data.

#index 2046057
#* Scalable progressive analytics on big data in the cloud
#@ Badrish Chandramouli;Jonathan Goldstein;Abdul Quamar
#t 2013
#c 4
#% 170649
#% 227883
#% 273908
#% 273910
#% 420114
#% 480120
#% 765425
#% 864528
#% 960294
#% 963669
#% 1354118
#% 1468411
#% 1523824
#% 1581866
#% 1581928
#% 1730988
#% 1770321
#% 1783374
#% 1846826
#% 1869837
#% 1874962
#% 1880459
#% 1972823
#! Analytics over the increasing quantity of data stored in the Cloud has become very expensive, particularly due to the pay-as-you-go Cloud computation model. Data scientists typically manually extract samples of increasing data size (progressive samples) using domain-specific sampling strategies for exploratory querying. This provides them with user-control, repeatable semantics, and result provenance. However, such solutions result in tedious workflows that preclude the reuse of work across samples. On the other hand, existing approximate query processing systems report early results, but do not offer the above benefits for complex ad-hoc queries. We propose a new progressive analytics system based on a progress model called Prism that (1) allows users to communicate progressive samples to the system; (2) allows efficient and deterministic query processing over samples; and (3) provides repeatable semantics and provenance to data scientists. We show that one can realize this model for atemporal relational queries using an unmodified temporal streaming engine, by re-interpreting temporal event fields to denote progress. Based on Prism, we build Now!, a progressive data-parallel computation framework for Windows Azure, where progress is understood as a first-class citizen in the framework. Now! works with "progress-aware reducers"- in particular, it works with streaming engines to support progressive SQL over big data. Extensive experiments on Windows Azure with real and synthetic workloads validate the scalability and benefits of Now! and its optimizations, over current solutions for progressive analytics.

#index 2046058
#* Scalable XML query processing using parallel pushdown transducers
#@ Peter Ogden;David Thomas;Peter Pietzuch
#t 2013
#c 4
#% 465061
#% 487257
#% 659984
#% 659987
#% 803121
#% 875010
#% 893112
#% 994015
#% 1121342
#% 1179197
#% 1181332
#% 1372695
#% 1523814
#% 1535998
#% 1637334
#% 1684601
#% 1735824
#% 1968414
#! In online social networking, network monitoring and financial applications, there is a need to query high rate streams of XML data, but methods for executing individual XPath queries on streaming XML data have not kept pace with multicore CPUs. For data-parallel processing, a single XML stream is typically split into well-formed fragments, which are then processed independently. Such an approach, however, introduces a sequential bottleneck and suffers from low cache locality, limiting its scalability across CPU cores. We describe a data-parallel approach for the processing of streaming XPath queries based on pushdown transducers. Our approach permits XML data to be split into arbitrarilysized chunks, with each chunk processed by a parallel automaton instance. Since chunks may be malformed, our automata consider all possible starting states for XML elements and build mappings from starting to finishing states. These mappings can be constructed independently for each chunk by different CPU cores. For streaming queries from the XPathMark benchmark, we show a processing throughput of 2.5 GB/s, with near linear scaling up to 64 CPU cores.

#index 2046059
#* Understanding insights into the basic structure and essential issues of table placement methods in clusters
#@ Yin Huai;Siyuan Ma;Rubao Lee;Owen O'Malley;Xiaodong Zhang
#t 2013
#c 4
#% 1082104
#% 1523924
#% 1581407
#% 1581926
#% 1594639
#% 1601116
#% 1621145
#% 1928309
#% 1972762
#! A table placement method is a critical component in big data analytics on distributed systems. It determines the way how data values in a two-dimensional table are organized and stored in the underlying cluster. Based on Hadoop computing environments, several table placement methods have been proposed and implemented. However, a comprehensive and systematic study to understand, to compare, and to evaluate different table placement methods has not been done. Thus, it is highly desirable to gain important insights into the basic structure and essential issues of table placement methods in the context of big data processing infrastructures. In this paper, we present such a study. The basic structure of a data placement method consists of three core operations: row reordering, table partitioning, and data packing. All the existing placement methods are formed by these core operations with variations made by the three key factors: (1) the size of a horizontal logical subset of a table (or the size of a row group), (2) the function of mapping columns to column groups, and (3) the function of packing columns or column groups in a row group into physical blocks. We have designed and implemented a benchmarking tool to provide insights into how variations of each factor affect the I/O performance of reading data of a table stored by a table placement method. Based on our results, we give suggested actions to optimize table reading performance. Results from large-scale experiments have also confirmed that our findings are valid for production workloads. Finally, we present ORC File as a case study to show the effectiveness of our findings and suggested actions.

#index 2046060
#* A probabilistic optimization framework for the empty-answer problem
#@ Davide Mottin;Alice Marascu;Senjuti Basu Roy;Gautam Das;Themis Palpanas;Yannis Velegrakis
#t 2013
#c 4
#% 111308
#% 342961
#% 376266
#% 462772
#% 893105
#% 894444
#% 1016203
#% 1130808
#% 1181286
#% 1190668
#% 1217186
#% 1250145
#% 1355032
#% 1386431
#% 1399998
#% 1418196
#% 1426503
#% 1482250
#% 1681672
#% 1712595
#! We propose a principled optimization-based interactive query relaxation framework for queries that return no answers. Given an initial query that returns an empty answer set, our framework dynamically computes and suggests alternative queries with less conditions than those the user has initially requested, in order to help the user arrive at a query with a non-empty answer, or at a query for which no matter how many additional conditions are ignored, the answer will still be empty. Our proposed approach for suggesting query relaxations is driven by a novel probabilistic framework based on optimizing a wide variety of application-dependent objective functions. We describe optimal and approximate solutions of different optimization problems using the framework. We analyze these solutions, experimentally verify their efficiency and effectiveness, and illustrate their advantage over the existing approaches.

#index 2046061
#* Summarizing answer graphs induced by keyword queries
#@ Yinghui Wu;Shengqi Yang;Mudhakar Srivatsa;Arun Iyengar;Xifeng Yan
#t 2013
#c 4
#% 271130
#% 341672
#% 408396
#% 447958
#% 464883
#% 593696
#% 654442
#% 660000
#% 726173
#% 824693
#% 960259
#% 1063493
#% 1063501
#% 1063512
#% 1063536
#% 1063537
#% 1181246
#% 1195889
#% 1206910
#% 1217235
#% 1223708
#% 1328119
#% 1370256
#% 1573235
#% 1581923
#% 1594642
#% 1632455
#% 1641490
#% 1654055
#! Keyword search has been popularly used to query graph data. Due to the lack of structure support, a keyword query might generate an excessive number of matches, referred to as "answer graphs", that could include different relationships among keywords. An ignored yet important task is to group and summarize answer graphs that share similar structures and contents for better query interpretation and result understanding. This paper studies the summarization problem for the answer graphs induced by a keyword query Q. (1) A notion of summary graph is proposed to characterize the summarization of answer graphs. Given Q and a set of answer graphs G, a summary graph preserves the relation of the keywords in Q by summarizing the paths connecting the keywords nodes in G. (2) A quality metric of summary graphs, called coverage ratio, is developed to measure information loss of summarization. (3) Based on the metric, a set of summarization problems are formulated, which aim to find minimized summary graphs with certain coverage ratio. (a) We show that the complexity of these summarization problems ranges from ptime to NP-complete. (b) We provide exact and heuristic summarization algorithms. (4) Using real-life and synthetic graphs, we experimentally verify the effectiveness and the efficiency of our techniques.

#index 2046062
#* Supporting keyword search in product database: a probabilistic approach
#@ Huizhong Duan;ChengXiang Zhai;Jinxing Cheng;Abhishek Gattani
#t 2013
#c 4
#% 144070
#% 215225
#% 262096
#% 280851
#% 340948
#% 480418
#% 659990
#% 660011
#% 875017
#% 894444
#% 960259
#% 993987
#% 1015325
#% 1022234
#% 1100822
#% 1227648
#% 1581860
#% 1692271
#% 1763376
#% 1876161
#! The ability to let users search for products conveniently in product database is critical to the success of e-commerce. Although structured query languages (e.g. SQL) can be used to effectively access the product database, it is very difficult for end users to learn and use. In this paper, we study how to optimize search over structured product entities (represented by specifications) with keyword queries such as "cheap gaming laptop". One major difficulty in this problem is the vocabulary gap between the specifications of products in the database and the keywords people use in search queries. To solve the problem, we propose a novel probabilistic entity retrieval model based on query generation, where the entities would be ranked for a given keyword query based on the likelihood that a user who likes an entity would pose the query. Different ways to estimate the model parameters would lead to different variants of ranking functions. We start with simple estimates based on the specifications of entities, and then leverage user reviews and product search logs to improve the estimation. Multiple estimation algorithms are developed based on Maximum Likelihood and Maximum a Posteriori estimators. We evaluate the proposed product entity retrieval models on two newly created product search test collections. The results show that the proposed model significantly outperforms the existing retrieval models, benefiting from the modeling of attribute-level relevance. Despite the focus on product retrieval, the proposed modeling method is general and opens up many new opportunities in analyzing structured entity data with unstructured text data. We show the proposed probabilistic model can be easily adapted for many interesting applications including facet generation and review annotation.

#index 2046063
#* A sampling algebra for aggregate estimation
#@ Supriya Nirkhiwale;Alin Dobra;Christopher Jermaine
#t 2013
#c 4
#% 145196
#% 210353
#% 227883
#% 273908
#% 273909
#% 273910
#% 274152
#% 334053
#% 427219
#% 479804
#% 503719
#% 765471
#% 912242
#% 1092009
#% 1231247
#% 1328144
#% 1426545
#! As of 2005, sampling has been incorporated in all major database systems. While efficient sampling techniques are realizable, determining the accuracy of an estimate obtained from the sample is still an unresolved problem. In this paper, we present a theoretical framework that allows an elegant treatment of the problem. We base our work on generalized uniform sampling (GUS), a class of sampling methods that subsumes a wide variety of sampling techniques. We introduce a key notion of equivalence that allows GUS sampling operators to commute with selection and join, and derivation of confidence intervals. We illustrate the theory through extensive examples and give indications on how to use it to provide meaningful estimates in database systems.

#index 2046064
#* A temporal-probabilistic database model for information extraction
#@ Maximilian Dylla;Iris Miliaraki;Martin Theobald
#t 2013
#c 4
#% 176883
#% 190332
#% 273687
#% 319244
#% 340699
#% 384978
#% 798723
#% 810098
#% 850430
#% 977013
#% 992830
#% 1036075
#% 1111133
#% 1127376
#% 1127393
#% 1206717
#% 1206987
#% 1424357
#% 1472106
#% 1486254
#% 1523890
#% 1538784
#% 1573237
#% 1592043
#% 1615075
#% 1693868
#% 1770355
#% 1846727
#% 1913401
#% 1913673
#% 1925702
#% 2010313
#! Temporal annotations of facts are a key component both for building a high-accuracy knowledge base and for answering queries over the resulting temporal knowledge base with high precision and recall. In this paper, we present a temporal-probabilistic database model for cleaning uncertain temporal facts obtained from information extraction methods. Specifically, we consider a combination of temporal deduction rules, temporal consistency constraints and probabilistic inference based on the common possible-worlds semantics with data lineage, and we study the theoretical properties of this data model. We further develop a query engine which is capable of scaling to very large temporal knowledge bases, with nearly interactive query response times over millions of uncertain facts and hundreds of thousands of grounded rules. Our experiments over two real-world datasets demonstrate the increased robustness of our approach compared to related techniques based on constraint solving via Integer Linear Programming (ILP) and probabilistic inference via Markov Logic Networks (MLNs). We are also able to show that our runtime performance is more than competitive to current ILP solvers and the fastest available, probabilistic but non-temporal, database engines.

#index 2046065
#* Counter strike: generic top-down join enumeration for hypergraphs
#@ Pit Fender;Guido Moerkotte
#t 2013
#c 4
#% 201927
#% 210166
#% 220425
#% 408638
#% 465165
#% 480430
#% 495283
#% 565457
#% 571094
#% 745443
#% 893165
#% 960299
#% 1016209
#% 1063510
#% 1594583
#% 1846738
#% 1972786
#% 2010407
#! Finding the optimal execution order of join operations is a crucial task of today's cost-based query optimizers. There are two approaches to identify the best plan: bottom-up and top-down join enumeration. But only the top-down approach allows for branch-and-bound pruning, which can improve compile time by several orders of magnitude while still preserving optimality. For both optimization strategies, efficient enumeration algorithms have been published. However, there are two severe limitations for the top-down approach: The published algorithms can handle only (1) simple (binary) join predicates and (2) inner joins. Since real queries may contain complex join predicates involving more than two relations, and outer joins as well as other non-inner joins, efficient top-down join enumeration cannot be used in practice yet. We develop a novel top-down join enumeration algorithm that overcomes these two limitations. Furthermore, we show that our new algorithm is competitive when compared to the state of the art in bottom-up processing even without playing out its advantage by making use of its branch-and-bound pruning capabilities.

#index 2046066
#* Efficient bulk updates on multiversion B-trees
#@ Daniar Achakeev;Bernhard Seeger
#t 2013
#c 4
#% 41684
#% 56081
#% 58371
#% 287070
#% 443130
#% 462059
#% 479473
#% 480817
#% 481934
#% 571296
#% 656697
#% 878300
#% 1022727
#% 1081215
#% 1127420
#% 1206971
#% 1217211
#% 1523830
#% 1846832
#% 1905961
#% 1962326
#% 1962362
#% 1972782
#! Partial persistent index structures support efficient access to current and past versions of objects, while updates are allowed on the current version. The Multiversion B-Tree (MVBT) represents a partially persistent index-structure with both, asymptotic worst-case performance and excellent performance in real life applications. Updates are performed tuple-by-tuple with the same asymptotic performance as for standard B+trees. To the best of our knowledge, there is no efficient algorithm for bulk loading and bulk update of MVBT and other partially persistent index structures. In this paper, we propose the first loading algorithm for MVBT that meets the lower-bound of external sorting. In addition, our approach is also applicable to bulk updates. This is achieved by combining two basic technologies, weight balancing and buffer tree. Our extensive set of experiments confirm the theoretical findings: Our loading algorithm runs considerably faster than performing updates tuple-by-tuple.

#index 2046067
#* Query-driven approach to entity resolution
#@ Hotham Altwaijry;Dmitri V. Kalashnikov;Sharad Mehrotra
#t 2013
#c 4
#% 201889
#% 310516
#% 810014
#% 871766
#% 913783
#% 967274
#% 993980
#% 1016335
#% 1196287
#% 1201863
#% 1206836
#% 1217162
#% 1272196
#% 1274820
#% 1328143
#% 1380969
#% 1523833
#% 1523834
#% 1523912
#% 1680302
#% 1969908
#! This paper explores "on-the-fly" data cleaning in the context of a user query. A novel Query-Driven Approach (QDA) is developed that performs a minimal number of cleaning steps that are only necessary to answer a given selection query correctly. The comprehensive empirical evaluation of the proposed approach demonstrates its significant advantage in terms of efficiency over traditional techniques for query-driven applications.

#index 2046068
#* Expressiveness and complexity of order dependencies
#@ Jaroslaw Szlichta;Parke Godfrey;Jarek Gryz;Calisto Zuzarte
#t 2013
#c 4
#% 210169
#% 287295
#% 342360
#% 411554
#% 411733
#% 637806
#% 800596
#% 824691
#% 1328157
#% 1538792
#% 1549878
#% 1880441
#! Dependencies play an important role in databases. We study order dependencies (ODs)--and unidirectional order dependencies (UODs), a proper sub-class of ODs--which describe the relationships among lexicographical orderings of sets of tuples. We consider lexicographical ordering, as by the order-by operator in SQL, because this is the notion of order used in SQL and within query optimization. Our main goal is to investigate the inference problem for ODs, both in theory and in practice. We show the usefulness of ODs in query optimization. We establish the following theoretical results: (i) a hierarchy of order dependency classes; (ii) a proof of co-NP-completeness of the inference problem for the subclass of UODs (and ODs); (iii) a proof of co-NP-completeness of the inference problem of functional dependencies (FDs) from ODs in general, but demonstrate linear time complexity for the inference of FDs from UODs; (iv) a sound and complete elimination procedure for inference over ODs; and (v) a sound and complete polynomial inference algorithm for sets of UODs over restricted domains.

#index 2046069
#* Counting and sampling triangles from a graph stream
#@ A. Pavan;Kanat Tangwongsan;Srikanta Tirthapura;Kun-Lung Wu
#t 2013
#c 4
#% 238182
#% 379443
#% 379444
#% 874902
#% 1083625
#% 1214705
#% 1404840
#% 1560415
#% 1622596
#% 1682599
#% 1701869
#% 1719564
#% 1770116
#% 1888912
#! This paper presents a new space-efficient algorithm for counting and sampling triangles--and more generally, constant-sized cliques--in a massive graph whose edges arrive as a stream. Compared to prior work, our algorithm yields significant improvements in the space and time complexity for these fundamental problems. Our algorithm is simple to implement and has very good practical performance on large graphs.

#index 2046070
#* An experimental analysis of iterated spatial joins in main memory
#@ Benjamin Sowell;Marcos Vaz Salles;Tuan Cao;Alan Demers;Johannes Gehrke
#t 2013
#c 4
#% 152937
#% 210187
#% 252304
#% 300174
#% 333940
#% 415957
#% 421124
#% 421127
#% 427199
#% 479797
#% 503887
#% 527166
#% 554918
#% 571047
#% 577311
#% 652272
#% 736290
#% 765453
#% 765454
#% 810061
#% 871761
#% 927035
#% 960236
#% 1015320
#% 1127612
#% 1207009
#% 1298884
#% 1309309
#% 1523880
#% 1618261
#% 1621317
#% 1669479
#! Many modern applications rely on high-performance processing of spatial data. Examples include location-based services, games, virtual worlds, and scientific simulations such as molecular dynamics and behavioral simulations. These applications deal with large numbers of moving objects that continuously sense their environment, and their data access can often be abstracted as a repeated spatial join. Updates to object positions are interspersed with these join operations, and batched for performance. Even for the most demanding scenarios, the data involved in these joins fits comfortably in the main memory of a cluster of machines, and most applications run completely in main memory for performance reasons. Choosing appropriate spatial join algorithms is challenging due to the large number of techniques in the literature. In this paper, we perform an extensive evaluation of repeated spatial join algorithms for distance (range) queries in main memory. Our study is unique in breadth when compared to previous work: We implement, tune, and compare ten distinct algorithms on several workloads drawn from the simulation and spatial indexing literature. We explore the design space of both index nested loops algorithms and specialized join algorithms, as well as the use of moving object indices that can be incrementally maintained. Surprisingly, we find that when queries and updates can be batched, repeatedly re-computing the join result from scratch outperforms using a moving object index in all but the most extreme cases. This suggests that--given the code complexity of index structures for moving objects -- specialized join strategies over simple index structures, such as Synchronous Traversal over R-Trees, should be the methods of choice for the above applications.

#index 2046071
#* Scaling queries over big RDF graphs with semantic hash partitioning
#@ Kisung Lee;Ling Liu
#t 2013
#c 4
#% 202286
#% 274612
#% 805474
#% 1206875
#% 1366460
#% 1374374
#% 1409918
#% 1426513
#% 1586616
#% 1602034
#% 1911311
#! Massive volumes of big RDF data are growing beyond the performance capacity of conventional RDF data management systems operating on a single node. Applications using large RDF data demand efficient data partitioning solutions for supporting RDF data access on a cluster of compute nodes. In this paper we present a novel semantic hash partitioning approach and implement a Semantic HAsh Partitioning-Enabled distributed RDF data management system, called Shape. This paper makes three original contributions. First, the semantic hash partitioning approach we propose extends the simple hash partitioning method through direction-based triple groups and direction-based triple replications. The latter enhances the former by controlled data replication through intelligent utilization of data access locality, such that queries over big RDF graphs can be processed with zero or very small amount of inter-machine communication cost. Second, we generate locality-optimized query execution plans that are more efficient than popular multi-node RDF data management systems by effectively minimizing the inter-machine communication cost for query processing. Third but not the least, we provide a suite of locality-aware optimization techniques to further reduce the partition size and cut down on the inter-machine communication cost during distributed query processing. Experimental results show that our system scales well and can process big RDF datasets more efficiently than existing approaches.

#index 2046072
#* Distributed socialite: a datalog-based language for large-scale graph analysis
#@ Jiwon Seo;Jongsoo Park;Jaeho Shin;Monica S. Lam
#t 2013
#c 4
#% 5965
#% 36310
#% 69503
#% 86933
#% 251359
#% 268079
#% 322884
#% 368248
#% 442938
#% 547958
#% 723279
#% 761220
#% 942873
#% 955712
#% 963669
#% 983467
#% 1063553
#% 1246527
#% 1256394
#% 1386046
#% 1426513
#% 1468421
#% 1523820
#% 1745223
#% 1880446
#% 2010426
#! Large-scale graph analysis is becoming important with the rise of world-wide social network services. Recently in SociaLite, we proposed extensions to Datalog to efficiently and succinctly implement graph analysis programs on sequential machines. This paper describes novel extensions and optimizations of SociaLite for parallel and distributed executions to support large-scale graph analysis. With distributed SociaLite, programmers simply annotate how data are to be distributed, then the necessary communication is automatically inferred to generate parallel code for cluster of multi-core machines. It optimizes the evaluation of recursive monotone aggregate functions using a delta stepping technique. In addition, approximate computation is supported in SociaLite, allowing programmers to trade off accuracy for less time and space. We evaluated SociaLite with six core graph algorithms used in many social network analyses. Our experiment with 64 Amazon EC2 8-core instances shows that SociaLite programs performed within a factor of two with respect to ideal weak scaling. Compared to optimized Giraph, an open-source alternative of Pregel, SociaLite programs are 4 to 12 times faster across benchmark algorithms, and 22 times more succinct on average. As a declarative query language, SociaLite, with the help of a compiler that generates efficient parallel and approximate code, can be used easily to create many social apps that operate on large-scale distributed graphs.

#index 2046073
#* Horton+: a distributed system for processing declarative reachability queries over partitioned graphs
#@ Mohamed Sarwat;Sameh Elnikety;Yuxiong He;Mohamed F. Mokbel
#t 2013
#c 4
#% 32904
#% 69503
#% 148021
#% 268797
#% 283833
#% 343740
#% 397366
#% 464825
#% 481434
#% 765429
#% 823347
#% 824711
#% 963669
#% 1063500
#% 1206685
#% 1206994
#% 1207028
#% 1245882
#% 1394202
#% 1426348
#% 1426510
#% 1426512
#% 1426513
#% 1426603
#% 1504829
#% 1523825
#% 1523882
#% 1581923
#% 1594585
#% 1594600
#% 1594624
#% 1606050
#% 1654051
#% 1668640
#% 1707462
#% 1846755
#% 1875014
#% 1911310
#% 1911311
#% 1918368
#% 1974619
#! Horton+ is a graph query processing system that executes declarative reachability queries on a partitioned attributed multi-graph. It employs a query language, query optimizer, and a distributed execution engine. The query language expresses declarative reachability queries, and supports closures and predicates on node and edge attributes to match graph paths. We introduce three algebraic operators, select, traverse, and join, and a query is compiled into an execution plan containing these operators. As reachability queries access the graph elements in a random access pattern, the graph is therefore maintained in the main memory of a cluster of servers to reduce query execution time. We develop a distributed execution engine that processes a query plan in parallel on the graph servers. Since the query language is declarative, we build a query optimizer that uses graph statistics to estimate predicate selectivity. We experimentally evaluate the system performance on a cluster of 16 graph servers using synthetic graphs as well as a real graph from an application that uses reachability queries. The evaluation shows (1) the efficiency of the optimizer in reducing query execution time, (2) system scalability with the size of the graph and with the number of servers, and (3) the convenience of using declarative queries.

#index 2046074
#* Streaming similarity search over one billion tweets using parallel locality-sensitive hashing
#@ Narayanan Sundaram;Aizana Turmukhametova;Nadathur Satish;Todd Mostak;Piotr Indyk;Samuel Madden;Pradeep Dubey
#t 2013
#c 4
#% 14738
#% 249321
#% 321455
#% 342828
#% 347225
#% 444151
#% 479769
#% 810049
#% 810072
#% 847168
#% 875017
#% 879600
#% 956507
#% 956521
#% 1023422
#% 1328057
#% 1426531
#% 1470583
#% 1606048
#% 1667212
#% 1739419
#% 1912323
#% 1919895
#% 1948188
#! Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing, and many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kd-trees do not perform well) is Locality Sensitive Hashing (LSH), an approximation algorithm for finding similar objects. In this paper, we describe a new variant of LSH, called Parallel LSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports high-throughput streaming of new data. Our approach employs several novel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient algorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration algorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to optimize parameter settings. We show that on a workload where we perform similarity search on a dataset of 1 Billion tweets, with hundreds of millions of new tweets per day, we can achieve query times of 1-2.5 ms. We show that this is an order of magnitude faster than existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH, with table construction times up to 3.7× faster and query times that are 8.3× faster than a basic implementation.

#index 2046075
#* Anti-caching: a new approach to database management system architecture
#@ Justin DeBrabant;Andrew Pavlo;Stephen Tu;Michael Stonebraker;Stan Zdonik
#t 2013
#c 4
#% 13043
#% 54583
#% 273945
#% 427195
#% 442835
#% 466947
#% 479632
#% 481454
#% 769155
#% 813302
#% 979743
#% 1022298
#% 1063543
#% 1127596
#% 1350336
#% 1426489
#% 1526992
#% 1594617
#% 1667313
#% 1770319
#% 1770324
#% 1912834
#% 1971522
#% 1989876
#! The traditional wisdom for building disk-based relational database management systems (DBMS) is to organize data in heavily-encoded blocks stored on disk, with a main memory block cache. In order to improve performance given high disk latency, these systems use a multi-threaded architecture with dynamic record-level locking that allows multiple transactions to access the database at the same time. Previous research has shown that this results in substantial overhead for on-line transaction processing (OLTP) applications [15]. The next generation DBMSs seek to overcome these limitations with architecture based on main memory resident data. To overcome the restriction that all data fit in main memory, we propose a new technique, called anti-caching, where cold data is moved to disk in a transactionally-safe manner as the database grows in size. Because data initially resides in memory, an anti-caching architecture reverses the traditional storage hierarchy of disk-based systems. Main memory is now the primary storage device. We implemented a prototype of our anti-caching proposal in a high-performance, main memory OLTP DBMS and performed a series of experiments across a range of database sizes, workload skews, and read/write mixes. We compared its performance with an open-source, disk-based DBMS optionally fronted by a distributed main memory cache. Our results show that for higher skewed workloads the anti-caching architecture has a performance advantage over either of the other architectures tested of up to 9× for a data size 8× larger than memory.

#index 2046076
#* Understanding hierarchical methods for differentially private histograms
#@ Wahbeh Qardaji;Weining Yang;Ninghui Li
#t 2013
#c 4
#% 576110
#% 809245
#% 977011
#% 1061644
#% 1426322
#% 1426454
#% 1451189
#% 1496267
#% 1523886
#% 1581864
#% 1595893
#% 1605968
#% 1625086
#% 1670071
#% 1730731
#% 1740518
#% 1846816
#% 1846817
#% 1880452
#% 1932886
#! In recent years, many approaches to differentially privately publish histograms have been proposed. Several approaches rely on constructing tree structures in order to decrease the error when answer large range queries. In this paper, we examine the factors affecting the accuracy of hierarchical approaches by studying the mean squared error (MSE) when answering range queries. We start with one-dimensional histograms, and analyze how the MSE changes with different branching factors, after employing constrained inference, and with different methods to allocate the privacy budget among hierarchy levels. Our analysis and experimental results show that combining the choice of a good branching factor with constrained inference outperform the current state of the art. Finally, we extend our analysis to multi-dimensional histograms. We show that the benefits from employing hierarchical methods beyond a single dimension are significantly diminished, and when there are 3 or more dimensions, it is almost always better to use the Flat method instead of a hierarchy.

#index 2046077
#* Towards social data platform: automatic topic-focused monitor for twitter stream
#@ Rui Li;Shengjie Wang;Kevin Chen-Chuan Chang
#t 2013
#c 4
#% 281251
#% 447946
#% 729923
#% 1055680
#% 1091268
#% 1164877
#% 1206920
#% 1369421
#% 1400018
#% 1426611
#% 1450965
#% 1472943
#% 1560414
#% 1642034
#% 1747177
#% 1845813
#% 1846751
#! Many novel applications have been built based on analyzing tweets about specific topics. While these applications provide different kinds of analysis, they share a common task of monitoring "target" tweets from the Twitter stream for a topic. The current solution for this task tracks a set of manually selected keywords with Twitter APIs. Obviously, this manual approach has many limitations. In this paper, we propose a data platform to automatically monitor target tweets from the Twitter stream for any given topic. To monitor target tweets in an optimal and continuous way, we design Automatic Topic-focused Monitor (ATM), which iteratively 1) samples tweets from the stream and 2) selects keywords to track based on the samples. To realize ATM, we develop a tweet sampling algorithm to sample sufficient unbiased tweets with available Twitter APIs, and a keyword selection algorithm to efficiently select keywords that have a near-optimal coverage of target tweets under cost constraints. We conduct extensive experiments to show the effectiveness of ATM. E.g., ATM covers 90% of target tweets for a topic and improves the manual approach by 49%.

#index 2046078
#* Simple, fast, and scalable reachability oracle
#@ Ruoming Jin;Guan Wang
#t 2013
#c 4
#% 47573
#% 58365
#% 88051
#% 527029
#% 722530
#% 789799
#% 824692
#% 864462
#% 960304
#% 1044451
#% 1063514
#% 1102989
#% 1194484
#% 1194592
#% 1206685
#% 1217208
#% 1314064
#% 1412885
#% 1482190
#% 1523819
#% 1531325
#% 1581922
#% 1592313
#% 1594607
#% 1597258
#% 1676469
#% 1688299
#% 1770333
#% 1798415
#% 1848112
#% 1863842
#% 1880447
#% 1880448
#% 1972760
#% 1972787
#% 2030596
#! A reachability oracle (or hop labeling) assigns each vertex v two sets of vertices: Lout(v) and Lin(v), such that u reaches v iff Lout(u) ∩ Lin(v) ≠ 0. Despite their simplicity and elegance, reachability oracles have failed to achieve efficiency in more than ten years since their introduction: The main problem is high construction cost, which stems from a set-cover framework and the need to materialize transitive closure. In this paper, we present two simple and efficient labeling algorithms, Hierarchical-Labeling and Distribution-Labeling, which can work onmassive real-world graphs: Their construction time is an order of magnitude faster than the set-cover based labeling approach, and transitive closure materialization is not needed. On large graphs, their index sizes and their query performance can now beat the state-of-the-art transitive closure compression and online search approaches.

#index 2046079
#* Aggregation and ordering in factorised databases
#@ Nurzhan Bakibayev;Tomáš Kočiský;Dan Olteanu;Jakub Závodný
#t 2013
#c 4
#% 6259
#% 44876
#% 102787
#% 287478
#% 384978
#% 393844
#% 416030
#% 479821
#% 481608
#% 481904
#% 765431
#% 768808
#% 824697
#% 847068
#% 993241
#% 1015332
#% 1083337
#% 1523974
#% 1594649
#% 1615075
#% 1623070
#% 1733564
#% 1818427
#% 1880442
#% 1914832
#% 1992375
#% 2030485
#! A common approach to data analysis involves understanding and manipulating succinct representations of data. In earlier work, we put forward a succinct representation system for relational data called factorised databases and reported on the main-memory query engine FDB for select-project-join queries on such databases. In this paper, we extend FDB to support a larger class of practical queries with aggregates and ordering. This requires novel optimisation and evaluation techniques. We show how factorisation coupled with partial aggregation can effectively reduce the number of operations needed for query evaluation. We also show how factorisations of query results can support enumeration of tuples in desired orders as efficiently as listing them from the unfactorised, sorted results. We experimentally observe that FDB can outperform off-the-shelf relational engines by orders of magnitude.

#index 2046080
#* Parallel computation of skyline and reverse skyline queries using mapreduce
#@ Yoonjae Park;Jun-Ki Min;Kyuseok Shim
#t 2013
#c 4
#% 1331
#% 86950
#% 317933
#% 465167
#% 480671
#% 654480
#% 849816
#% 864453
#% 893150
#% 993954
#% 1022203
#% 1022226
#% 1023420
#% 1177872
#% 1206866
#% 1312536
#% 1426601
#% 1482210
#% 1495595
#% 1581853
#% 1587209
#% 1594566
#% 1688273
#% 1697268
#% 1818426
#% 1847978
#! The skyline operator and its variants such as dynamic skyline and reverse skyline operators have attracted considerable attention recently due to their broad applications. However, computations of such operators are challenging today since there is an increasing trend of applications to deal with big data. For such data-intensive applications, the MapReduce framework has been widely used recently. In this paper, we propose efficient parallel algorithms for processing the skyline and its variants using MapReduce. We first build histograms to effectively prune out nonskyline (non-reverse skyline) points in advance. We next partition data based on the regions divided by the histograms and compute candidate (reverse) skyline points for each region independently using MapReduce. Finally, we check whether each candidate point is actually a (reverse) skyline point in every region independently. Our performance study confirms the effectiveness and scalability of the proposed algorithms.

#index 2046081
#* Fast iterative graph computation with block updates
#@ Wenlei Xie;Guozhang Wang;David Bindel;Alan Demers;Johannes Gehrke
#t 2013
#c 4
#% 69503
#% 69847
#% 201869
#% 202285
#% 214389
#% 224113
#% 238376
#% 239588
#% 268079
#% 290137
#% 458556
#% 469765
#% 577329
#% 798967
#% 800924
#% 822383
#% 963206
#% 991663
#% 1318636
#% 1426513
#% 1506495
#% 1540339
#% 1621137
#% 1693889
#% 1750359
#% 1769265
#% 1872377
#% 1872632
#% 1911310
#% 1911311
#! Scaling iterative graph processing applications to large graphs is an important problem. Performance is critical, as data scientists need to execute graph programs many times with varying parameters. The need for a high-level, high-performance programming model has inspired much research on graph programming frameworks. In this paper, we show that the important class of computationally light graph applications - applications that perform little computation per vertex - has severe scalability problems across multiple cores as these applications hit an early "memory wall" that limits their speedup. We propose a novel block-oriented computation model, in which computation is iterated locally over blocks of highly connected nodes, significantly improving the amount of computation per cache miss. Following this model, we describe the design and implementation of a block-aware graph processing runtime that keeps the familiar vertex-centric programming paradigm while reaping the benefits of block-oriented execution. Our experiments show that block-oriented execution significantly improves the performance of our framework for several graph applications.

#index 2092350
#* Active and accelerated learning of cost models for optimizing scientific applications
#@ Piyush Shivam;Shivnath Babu;Jeff Chase
#t 2006
#c 4
#% 79312
#% 152633
#% 228007
#% 340306
#% 571094
#% 610294
#% 723288
#% 760777
#% 763995
#% 765129
#% 799003
#% 822370
#% 824675
#% 825657
#% 843813
#% 963607
#% 979367
#% 1016179
#% 1142436
#! We present the NIMO system that automatically learns cost models for predicting the execution time of computational-science applications running on large-scale networked utilities such as computational grids. Accurate cost models are important for selecting efficient plans for executing these applications on the utility. Computational-science applications are often scripts (written, e.g., in languages like Perl or Matlab) connected using a workflow-description language, and therefore, pose different challenges compared to modeling the execution of plans for declarative queries with well-understood semantics. NIMO generates appropriate training samples for these applications to learn fairly-accurate cost models quickly using statistical learning techniques. NIMO's approach is active and noninvasive: it actively deploys and monitors the application under varying conditions, and obtains its training data from passive instrumentation streams that require no changes to the operating system or applications. Our experiments with real scientific applications demonstrate that NIMO significantly reduces the number of training samples and the time to learn fairly-accurate cost models.

