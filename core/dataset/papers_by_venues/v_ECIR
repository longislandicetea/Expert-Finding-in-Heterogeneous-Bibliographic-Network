#index 758590
#* Advances In Information Retrieval: 26th European Conference On Ir Research, Ecir 2004, Sunderland, Uk, April 5-7, 2004, Proceedings (LECTURE NOTES IN COMPUTER SCIENCE)
#@ Sharon McDonald;John Tait
#t 2004
#c 16

#index 925961
#* Advances in Information Retrieval: 28th European Conference on IR Research, ECIR 2006, London, UK, April 10-12, 2006, Proceedings (Lecture Notes in Computer Science)
#@ Mounia Lalmas;Andrew MacFarlane;Stefan Rüger;Anastasios Tombros;Theodora Tsikrika;Alexei Yavlinsky
#t 2006
#c 16

#index 936543
#* Advances in Information Retrieval: 27th European Conference on IR Research, ECIR 2005, Santiago de Compostela, Spain, March 21-23, 2005, Proceedings (Lecture Notes in Computer Science)
#@ David E. Losada;Juan M. Fernández-Luna
#t 2005
#c 16

#index 1195825
#* Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval
#@ Mohand Boughanem;Catherine Berrut;Josiane Mothe;Chantal Soule-Dupuy
#t 2009
#c 16

#index 1195826
#* Query Evolution
#@ W. Bruce Croft
#t 2009
#c 16
#! Search engine queries have evolved over the past 30 years from complex Boolean formulations to short lists of "keywords." Despite the apparent simplicity of short queries, choosing the right keywords can be difficult, and understanding user intentions is a major challenge. Techniques such as query expansion and context-based profiles have been developed to address these problems, but with limited success. Rather than trying to infer user intentions from very short queries, another approach is to improve query processing and retrieval models for long queries. In particular, query transformation is a new approach to improving search that appears to have considerable potential. In this approach, queries are transformed into one or more new queries using probabilistic models for generation or search of query archives. I will describe various transformation models and the role of a retrieval model in using these transformations. Examples will be given from applications such as collaborative question answering and forum search.

#index 1195827
#* Searching User Generated Content: What's Next?
#@ Maarten Rijke
#t 2009
#c 16
#! In recent years, blog search has received a lot of attention. Since the launch of dedicated evaluation efforts and the release of blog data sets, our understanding of blog search has deepened considerably. But there's more to user generated content than blogs and there's more to searching user generated content than looking for material that is relevant or opinionated or highly rated by readers or authors. In this talk, a number of user generated content search scenarios from the media analysis and intelligence domains will be detailed. From these, recurring themes such as credibility, people finding, impact prediction, unusual event detection, report and summary generation--all on user generated content--will be identified as highly relevant research directions.

#index 1195828
#* Upcoming Industrial Needs for Search
#@ Gregory Grefenstette
#t 2009
#c 16
#! Enterprise search and web searching have different goals and characteristics. Whereas internet browsing can sometimes be seen as a form of entertainment, enterprise search involves activities in which search is mainly a tool. People have work they want to get done. In this context, the idea of relevance in documents is different. Community can become as important as content in search. Work-related search engines of the future will provide much greater analysis and structuring of documents at index time, and searchers will have more powerful tools at retrieval time. We will discuss these and other trends, and show what new methods and techniques should be targeted to improve enterprise search.

#index 1195829
#* Mean-Variance Analysis: A New Document Ranking Theory in Information Retrieval
#@ Jun Wang
#t 2009
#c 16
#% 248214
#% 262096
#% 262112
#% 280852
#% 330687
#% 340899
#% 411762
#% 734592
#% 766448
#% 805841
#% 871574
#% 879618
#% 891559
#% 1083539
#% 1348342
#! This paper concerns document ranking in information retrieval. In information retrieval systems, the widely accepted probability ranking principle (PRP) suggests that, for optimal retrieval, documents should be ranked in order of decreasing probability of relevance. In this paper, we present a new document ranking paradigm, arguing that a better, more general solution is to optimize top-n ranked documents as a whole, rather than ranking them independently. Inspired by the Modern Portfolio Theory in finance, we quantify a ranked list of documents on the basis of its expected overall relevance (mean) and its variance; the latter serves as a measure of risk, which was rarely studied for document ranking in the past. Through the analysis of the mean and variance, we show that an optimal rank order is the one that maximizes the overall relevance (mean) of the ranked list at a given risk level (variance). Based on this principle, we then derive an efficient document ranking algorithm. It extends the PRP by considering both the uncertainty of relevance predictions and correlations between retrieved documents. Furthermore, we quantify the benefits of diversification, and theoretically show that diversifying documents is an effective way to reduce the risk of document ranking. Experimental results on the collaborative filtering problem confirms the theoretical insights with improved recommendation performance, e.g., achieved over 300% performance gain over the PRP-based ranking on the user-based recommendation.

#index 1195830
#* Risk-Aware Information Retrieval
#@ Jianhan Zhu;Jun Wang;Michael Taylor;Ingemar J. Cox
#t 2009
#c 16
#% 169781
#% 248214
#% 262096
#% 340948
#% 375017
#% 411760
#% 642974
#% 818238
#% 879618
#% 1074057
#% 1130993
#% 1348342
#! Probabilistic retrieval models usually rank documents based on a scalar quantity. However, such models lack any estimate for the uncertainty associated with a document's rank. Further, such models seldom have an explicit utility (or cost) that is optimized when ranking documents. To address these issues, we take a Bayesian perspective that explicitly considers the uncertainty associated with the estimation of the probability of relevance, and propose an asymmetric cost function for document ranking. Our cost function has the advantage of adjusting the risk in document retrieval via a single parameter for any probabilistic retrieval model. We use the logit model to transform the document posterior distribution with probability space [0,1] into a normal distribution with variable space ( *** *** , + *** ). We apply our risk adjustment approach to a language modelling framework for risk adjustable document ranking. Our experimental results show that our risk-aware model can significantly improve the performance of language models, both with and without background smoothing. When our method is applied to a language model without background smoothing, it can perform as well as the Dirichlet smoothing approach.

#index 1195831
#* A Comparative Study of Utilizing Topic Models for Information Retrieval
#@ Xing Yi;James Allan
#t 2009
#c 16
#% 280819
#% 280856
#% 311027
#% 340899
#% 340901
#% 340948
#% 342707
#% 722904
#% 766430
#% 827581
#% 876017
#% 879587
#% 940042
#! We explore the utility of different types of topic models for retrieval purposes. Based on prior work, we describe several ways that topic models can be integrated into the retrieval process. We evaluate the effectiveness of different types of topic models within those retrieval approaches. We show that: (1) topic models are effective for document smoothing; (2) more rigorous topic models such as Latent Dirichlet Allocation provide gains over cluster-based models; (3) more elaborate topic models that capture topic dependencies provide no additional gains; (4) smoothing documents by using their similar documents is as effective as smoothing them by using topic models; (5) doing query expansion should utilize topics discovered in the top feedback documents instead of coarse-grained topics from the whole corpus; (6) generally, incorporating topics in the feedback documents for building relevance models can benefit the performance more for queries that have more relevant documents.

#index 1195832
#* Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval
#@ Mohand Boughanem;Catherine Berrut;Josiane Mothe;Chantal Soule-Dupuy
#t 2009
#c 16

#index 1195833
#* Synchronous Collaborative Information Retrieval: Techniques and Evaluation
#@ Colum Foley;Alan F. Smeaton
#t 2009
#c 16
#% 118728
#% 822126
#% 998795
#% 1035578
#% 1074090
#! Synchronous Collaborative Information Retrieval refers to systems that support multiple users searching together at the same time in order to satisfy a shared information need. To date most SCIR systems have focussed on providing various awareness tools in order to enable collaborating users to coordinate the search task. However, requiring users to both search and coordinate the group activity may prove too demanding. On the other hand without effective coordination policies the group search may not be effective. In this paper we propose and evaluate novel system-mediated techniques for coordinating a group search. These techniques allow for an effective division of labour across the group whereby each group member can explore a subset of the search space. We also propose and evaluate techniques to support automated sharing of knowledge across searchers in SCIR, through novel collaborative and complementary relevance feedback techniques. In order to evaluate these techniques, we propose a framework for SCIR evaluation based on simulations. To populate these simulations we extract data from TREC interactive search logs. This work represent the first simulations of SCIR to date and the first such use of this TREC data.

#index 1195834
#* Movie Recommender: Semantically Enriched Unified Relevance Model for Rating Prediction in Collaborative Filtering
#@ Yashar Moshfeghi;Deepak Agarwal;Benjamin Piwowarski;Joemon M. Jose
#t 2009
#c 16
#% 173879
#% 211044
#% 314933
#% 330687
#% 420515
#% 428272
#% 452563
#% 465928
#% 535905
#% 578684
#% 643007
#% 712016
#% 734593
#% 734594
#% 813966
#% 818216
#% 840924
#% 1051062
#! Collaborative recommender systems aim to recommend items to a user based on the information gathered from other users who have similar interests. The current state-of-the-art systems fail to consider the underlying semantics involved when rating an item. This in turn contributes to many false recommendations. These models hinder the possibility of explaining why a user has a particular interest or why a user likes a particular item . In this paper, we develop an approach incorporating the underlying semantics involved in the rating. Experiments on a movie database show that this improves the accuracy of the model.

#index 1195835
#* Revisiting IR Techniques for Collaborative Search Strategies
#@ Hideo Joho;David Hannah;Joemon M. Jose
#t 2009
#c 16
#% 54435
#% 128268
#% 232719
#% 281186
#% 309088
#% 397161
#% 427921
#% 643001
#% 998795
#% 1065269
#% 1074090
#% 1074132
#% 1093783
#% 1289575
#% 1348064
#! This paper revisits some of the established Information Retrieval (IR) techniques to investigate effective collaborative search strategies. We devised eight search strategies that divided labour and shared knowledge in teams using relevance feedback and clustering. We evaluated the performance of strategies with a user simulation enhanced by a query-pooling method. Our results show that relevance feedback is successful at formulating effective collaborative strategies while further effort is needed for clustering. We also measured the extent to which additional members improved the performance and an effect of search progress on the improvement.

#index 1195836
#* Active Sampling for Rank Learning via Optimizing the Area under the ROC Curve
#@ Pinar Donmez;Jaime G. Carbonell
#t 2009
#c 16
#% 116165
#% 169717
#% 349550
#% 411762
#% 464268
#% 466887
#% 565531
#% 577224
#% 734915
#% 770753
#% 770771
#% 818239
#% 823360
#% 840882
#% 879588
#% 989628
#% 1100053
#% 1100072
#% 1387560
#% 1665126
#! Learning ranking functions is crucial for solving many problems, ranging from document retrieval to building recommendation systems based on an individual user's preferences or on collaborative filtering. Learning-to-rank is particularly necessary for adaptive or personalizable tasks, including email prioritization, individualized recommendation systems, personalized news clipping services and so on. Whereas the learning-to-rank challenge has been addressed in the literature, little work has been done in an active-learning framework, where requisite user feedback is minimized by selecting only the most informative instances to train the rank learner. This paper addresses active rank-learning head on, proposing a new sampling strategy based on minimizing hinge rank loss, and demonstrating the effectiveness of the active sampling method for rankSVM on two standard rank-learning datasets. The proposed method shows convincing results in optimizing three performance metrics, as well as improvement against four baselines including entropy-based, divergence- based, uncertainty-based and random sampling methods.

#index 1195837
#* Regression Rank: Learning to Meet the Opportunity of Descriptive Queries
#@ Matthew Lease;James Allan;W. Bruce Croft
#t 2009
#c 16
#% 218982
#% 262096
#% 324129
#% 340899
#% 340901
#% 342707
#% 742399
#% 750863
#% 818262
#% 879584
#% 940010
#% 976952
#% 1016296
#% 1019124
#% 1024555
#% 1074052
#% 1074112
#% 1106239
#! We present a new learning to rank framework for estimating context-sensitive term weights without use of feedback. Specifically, knowledge of effective term weights on past queries is used to estimate term weights for new queries. This generalization is achieved by introducing secondary features correlated with term weights and applying regression to predict term weights given features. To improve support for more focused retrieval like question answering, we conduct document retrieval experiments with TREC description queries on three document collections. Results show significantly improved retrieval accuracy.

#index 1195838
#* Active Learning Strategies for Multi-Label Text Classification
#@ Andrea Esuli;Fabrizio Sebastiani
#t 2009
#c 16
#% 169717
#% 170649
#% 280817
#% 311034
#% 565531
#% 722797
#% 763708
#% 829975
#% 869526
#% 961194
#% 1289485
#% 1387560
#% 1392498
#% 1478821
#% 1682421
#! Active learning refers to the task of devising a ranking function that, given a classifier trained from relatively few training examples, ranks a set of additional unlabeled examples in terms of how much further information they would carry, once manually labeled, for retraining a (hopefully) better classifier. Research on active learning in text classification has so far concentrated on single-label classification; active learning for multi-label classification, instead, has either been tackled in a simulated (and, we contend, non-realistic) way, or neglected tout court . In this paper we aim to fill this gap by examining a number of realistic strategies for tackling active learning for multi-label classification. Each such strategy consists of a rule for combining the outputs returned by the individual binary classifiers as a result of classifying a given unlabeled document. We present the results of extensive experiments in which we test these strategies on two standard text classification datasets.

#index 1195839
#* Joint Ranking for Multilingual Web Search
#@ Wei Gao;Cheng Niu;Ming Zhou;Kam-Fai Wong
#t 2009
#c 16
#% 132779
#% 309095
#% 705252
#% 722312
#% 734915
#% 840846
#% 987226
#% 987235
#% 1074075
#% 1674907
#% 1717293
#! Ranking for multilingual information retrieval (MLIR) is a task to rank documents of different languages solely based on their relevancy to the query regardless of query's language. Existing approaches are focused on combining relevance scores of different retrieval settings, but do not learn the ranking function directly. We approach Web MLIR ranking within the learning-to-rank (L2R) framework. Besides adopting popular L2R algorithms to MLIR, a joint ranking model is created to exploit the correlations among documents, and induce the joint relevance probability for all the documents. Using this method, the relevant documents of one language can be leveraged to improve the relevance estimation for documents of different languages. A probabilistic graphical model is trained for the joint relevance estimation. Especially, a hidden layer of nodes is introduced to represent the salient topics among the retrieved documents, and the ranks of the relevant documents and topics are determined collaboratively while the model approaching to its thermal equilibrium. Furthermore, the model parameters are trained under two settings: (1) optimize the accuracy of identifying relevant documents; (2) directly optimize information retrieval evaluation measures, such as mean average precision. Benchmarks show that our model significantly outperforms the existing approaches for MLIR tasks.

#index 1195840
#* Diversity, Assortment, Dissimilarity, Variety: A Study of Diversity Measures Using Low Level Features for Video Retrieval
#@ Martin Halvey;P. Punitha;David Hannah;Robert Villa;Frank Hopfgartner;Anuj Goyal;Joemon M. Jose
#t 2009
#c 16
#% 316780
#% 318785
#% 342687
#% 490785
#% 490928
#% 566642
#% 805841
#% 818266
#% 905224
#% 993076
#% 1132476
#% 1410878
#% 1855804
#! In this paper we present a number of methods for re-ranking video search results in order to introduce diversity into the set of search results. The usefulness of these approaches is evaluated in comparison with similarity based measures, for the TRECVID 2007 collection and tasks [11]. For the MAP of the search results we find that some of our approaches perform as well as similarity based methods. We also find that some of these results can improve the P@N values for some of the lower N values. The most successful of these approaches was then implemented in an interactive search system for the TRECVID 2008 interactive search tasks. The responses from the users indicate that they find the more diverse search results extremely useful.

#index 1195841
#* Bayesian Mixture Hierarchies for Automatic Image Annotation
#@ Vassilios Stathopoulos;Joemon M. Jose
#t 2009
#c 16
#% 269188
#% 304947
#% 318785
#% 345829
#% 457912
#% 642989
#% 642990
#% 713059
#% 891559
#% 975105
#% 990337
#% 1502531
#% 1677726
#% 1781724
#! Previous research on automatic image annotation has shown that accurate estimates of the class conditional densities in generative models have a positive effect in annotation performance. We focus on the problem of density estimation in the context of automatic image annotation and propose a novel Bayesian hierarchical method for estimating mixture models of Gaussian components. The proposed methodology is examined in a well-known benchmark image collection and the results demonstrate its competitiveness with the state of the art.

#index 1195842
#* XML Multimedia Retrieval: From Relevant Textual Information to Relevant Multimedia Fragments
#@ Mouna Torjmen;Karen Pinel-Sauvagnat;Mohand Boughanem
#t 2009
#c 16
#% 318785
#% 443259
#% 860956
#% 878916
#% 944352
#% 1100827
#% 1674754
#% 1674755
#% 1674756
#% 1674757
#% 1739432
#% 1914867
#! In this paper, we are interested in XML multimedia document retrieval, whose aim is to find relevant multimedia components (i.e. XML fragments containing another media than text) that focus on the user needs. The work described here is carried out with images, but can be extended to any other media. We propose an XML multimedia fragment retrieval approach based on two steps. In a first step, we search for relevant images and then we retrieve the best multimedia fragments corresponding to these images. Image retrieval is done using textual and structural information from ascendant, sibling and direct descendant nodes in the XML tree, while multimedia fragment retrieval is done by evaluating the score of ancestors of images retrieved in the first step. Experiments were done on the INEX 2006 and 2007 Multimedia Fragments task and show the interest of our method.

#index 1195843
#* Effectively Searching Maps in Web Documents
#@ Qingzhao Tan;Prasenjit Mitra;C. Lee Giles
#t 2009
#c 16
#% 51682
#% 131590
#% 169811
#% 197394
#% 212685
#% 262069
#% 269217
#% 290703
#% 318785
#% 378546
#% 420480
#% 642992
#% 643052
#% 783474
#% 874503
#% 955489
#% 1016365
#% 1065329
#! Maps are an important source of information in archaeology and other sciences. Users want to search for historical maps to determine recorded history of the political geography of regions at different eras, to find out where exactly archaeological artifacts were discovered, etc. Currently, they have to use a generic search engine and add the term map along with other keywords to search for maps. This crude method will generate a significant number of false positives that the user will need to cull through to get the desired results. To reduce their manual effort, we propose an automatic map identification, indexing, and retrieval system that enables users to search and retrieve maps appearing in a large corpus of digital documents using simple keyword queries. We identify features that can help in distinguishing maps from other figures in digital documents and show how a Support-Vector-Machine-based classifier can be used to identify maps. We propose map-level-metadata e.g., captions, references to the maps in text, etc. and document-level metadata, e.g., title, abstract, citations, how recent the publication is, etc. and show how they can be automatically extracted and indexed. Our novel ranking algorithm weights different metadata fields differently and also uses the document-level metadata to help rank retrieved maps. Empirical evaluations show which features should be selected and which metadata fields should be weighted more. We also demonstrate improved retrieval results in comparison to adaptations of existing methods for map retrieval. Our map search engine has been deployed in an online map-search system that is part of the Blind-Review digital library system.

#index 1195844
#* Enhancing Expert Finding Using Organizational Hierarchies
#@ Maryam Karimzadehgan;Ryen W. White;Matthew Richardson
#t 2009
#c 16
#% 144034
#% 146494
#% 220708
#% 241021
#% 262096
#% 280850
#% 309493
#% 402437
#% 662755
#% 730082
#% 750863
#% 766430
#% 766431
#% 818254
#% 838528
#% 879570
#% 913206
#% 956516
#% 1041735
#% 1074127
#% 1392465
#% 1415734
#% 1499467
#! The task in expert finding is to identify members of an organization with relevant expertise on a given topic. In existing expert finding systems, profiles are constructed from sources such as email or documents, and used as the basis for expert identification. In this paper, we leverage the organizational hierarchy (depicting relationships between managers, subordinates, and peers) to find members for whom we have little or no information. We propose an algorithm to improve expert finding performance by considering not only the expertise of the member, but also the expertise of his or her neighbors. We show that providing this additional information to an expert finding system improves its retrieval performance.

#index 1195845
#* A Vector Space Model for Ranking Entities and Its Application to Expert Search
#@ Gianluca Demartini;Julien Gaugaz;Wolfgang Nejdl
#t 2009
#c 16
#% 280840
#% 321635
#% 878916
#% 879570
#% 907515
#% 907525
#% 956551
#% 987276
#% 1022234
#% 1024552
#% 1077150
#% 1107144
#% 1177243
#% 1348341
#% 1392466
#% 1413152
#% 1415730
#% 1415732
#! Entity Ranking has recently become an important search task in Information Retrieval. The goal is not to find documents matching query terms, but, instead, finding entities. In this paper we propose a formal model to search entities as well as a complete Entity Ranking system, providing examples of its application to the enterprise context. We experimentally evaluate our system on the Expert Search task in order to show how it can be adapted to different scenarios. The results show that combining simple IR techniques we improve of 53% in terms of P@10 over our baseline.

#index 1195846
#* Sentiment-Oriented Contextual Advertising
#@ Teng-Kai Fan;Chia-Hui Chang
#t 2009
#c 16
#% 281156
#% 411248
#% 740191
#% 746885
#% 815915
#% 818265
#% 854646
#% 855279
#% 855282
#% 879633
#% 939848
#% 939897
#% 983583
#% 987262
#% 1019092
#% 1700552
#! Web advertising (Online advertising) is a form of promotion that uses the World Wide Web for the expressed purpose of delivering marketing messages to attract customers. This paper addresses the mechanism of Content-Oriented advertising (Contextual advertising), which refers to the assignment of relevant ads within the content of a generic web page, e.g. blogs. As blogs become a platform for expressing personal opinion, they naturally contain various kinds of expressions, including both facts and comments of both a positive and negative nature. In this paper, we propose the utilization of sentiment detection to improve Web-based contextual advertising. The proposed SOCA (Sentiment-Oriented Contextual Advertising) framework aims to combine contextual advertising matching with sentiment analysis to select ads that are related to the positive (and neutral) aspects of a blog and rank them according to their relevance. We experimentally validate our approach using a set of data that includes both real ads and actual blog pages. The results clearly indicate that our proposed method can effectively identify those ads that are positively correlated with the given blog pages.

#index 1195847
#* Lexical Graphs for Improved Contextual Ad Recommendation
#@ Symeon Papadopoulos;Fotis Menemenis;Yiannis Kompatsiaris;Ben Bratu
#t 2009
#c 16
#% 321635
#% 815267
#% 818265
#% 869484
#% 879633
#% 956546
#% 987262
#% 1040857
#% 1664779
#! Contextual advertising is a form of online advertising presenting consistent revenue growth since its inception. In this work, we study the problem of recommending a small set of ads to a user based solely on the currently viewed web page, often referred to as content-targeted advertising. Matching ads with web pages is a challenging task for traditional information retrieval systems due to the brevity and sparsity of advertising text, which leads to the widely recognized vocabulary impedance problem. To this end, we propose the use of lexical graphs created from web corpora as a means of computing improved content similarity metrics between ads and web pages. The results of our experimental study provide evidence of significant improvement in the perceived relevance of the recommended ads.

#index 1195848
#* A Probabilistic Retrieval Model for Semistructured Data
#@ Jinyoung Kim;Xiaobing Xue;W. Bruce Croft
#t 2009
#c 16
#% 109187
#% 132779
#% 144034
#% 262096
#% 397418
#% 413551
#% 654442
#% 750863
#% 783474
#% 945788
#% 993987
#% 1721867
#! Retrieving semistructured (XML) data typically requires either a structured query such as XPath, or a keyword query that does not take structure into account. In this paper, we infer structural information automatically from keyword queries and incorporate this into a retrieval model. More specifically, we propose the concept of a mapping probability, which maps each query word into a related field (or XML element). This mapping probability is used as a weight to combine the language models estimated from each field. Experiments on two test collections show that our retrieval model based on mapping probabilities outperforms baseline techniques significantly.

#index 1195849
#* Model Fusion in Conceptual Language Modeling
#@ Loic Maisonnasse;Eric Gaussier;Jean-Pierre Chevallet
#t 2009
#c 16
#% 262096
#% 340899
#% 448728
#% 730071
#% 786551
#% 987274
#% 1019110
#% 1106214
#% 1106228
#! We study in this paper the combination of different concept detection methods for conceptual indexing. Conceptual indexing shows effective results when large knowledge bases are available. But concept detection is not always accurate and errors limit interest of concept usage. A solution to solve this problem is to combine different concept detection methods. In this paper, we investigate several ways to combine concept detection methods, both on queries and documents, within the framework of the language modeling approach to IR. Our experiments show that our model fusion improves the standard language model by up to 17% on mean average precision.

#index 1195850
#* Graded-Inclusion-Based Information Retrieval Systems
#@ Patrick Bosc;Vincent Claveau;Olivier Pivert;Laurent Ughetto
#t 2009
#c 16
#% 228824
#% 259460
#% 319273
#% 330260
#% 1053429
#% 1065781
#% 1107591
#% 1347520
#% 1739439
#! This paper investigates the use of fuzzy logic mechanisms coming from the database community, namely graded inclusions, to model the information retrieval process. In this framework, documents and queries are represented by fuzzy sets, which are paired with operations like fuzzy implications and T-norms. Through different experiments, it is shown that only some among the wide range of fuzzy operations are relevant for information retrieval. When appropriate settings are chosen, it is possible to mimic classical systems, thus yielding results rivaling those of state-of-the-art systems. These positive results validate the proposed approach, while negative ones give some insights on the properties needed by such a model. Moreover, this paper shows the added-value of this graded inclusion-based model, which gives new and theoretically grounded ways for a user to easily weight his query terms, to include negative information in his queries, or to expand them with related terms.

#index 1195851
#* Multidimensional Relevance: A New Aggregation Criterion
#@ Célia Costa Pereira;Mauro Dragoni;Gabriella Pasi
#t 2009
#c 16
#% 167557
#% 235918
#% 406493
#% 569577
#% 608769
#% 867123
#% 948379
#% 977141
#% 987322
#% 1042074
#! In this paper, a new model for aggregating multiple criteria evaluations for relevance assessment is proposed. An information retrieval context is considered, where relevance is modelled as a multidimensional property of documents. In the paper, the proposed aggregation operator is applied to define a model for personalized Information Retrieval (IR), in which four criteria are considered in order to assess document relevance: aboutness , coverage , appropriateness and reliability . The originality of this approach lies in the aggregation of the considered criteria in a prioritized way, by considering the existence of a prioritization relationship over the criteria. Such a prioritization is modeled by making the weights associated with a criterion dependent upon the satisfaction of the higher-priority criteria. This way, it is possible to take into account the fact that the weight of a less important criterion should be proportional to the satisfaction degree of the more important criterion. In the paper, some preliminary experimental results are also reported.

#index 1195852
#* Using Multiple Query Aspects to Build Test Collections without Human Relevance Judgments
#@ Miles Efron
#t 2009
#c 16
#% 144076
#% 184496
#% 232703
#% 262097
#% 340890
#% 340892
#% 359718
#% 728355
#% 766409
#% 766410
#% 835027
#% 838529
#% 879598
#% 893639
#% 948378
#% 985820
#% 987201
#% 1076651
#! Collecting relevance judgments (qrels) is an especially challenging part of building an information retrieval test collection. This paper presents a novel method for creating test collections by offering a substitute for relevance judgments. Our method is based on an old idea in IR: a single information need can be represented by many query articulations. We call different articulations of a particular need query aspects . By combining the top k documents retrieved by a single system for multiple query aspects, we build judgment-free qrels whose rank ordering of IR systems correlates highly with rankings based on human relevance judgments.

#index 1195853
#* If I Had a Million Queries
#@ Ben Carterette;Virgil Pavlu;Evangelos Kanoulas;Javed A. Aslam;James Allan
#t 2009
#c 16
#% 318407
#% 879598
#% 879632
#% 907496
#% 987201
#% 987238
#% 987239
#% 1074132
#! As document collections grow larger, the information needs and relevance judgments in a test collection must be well-chosen within a limited budget to give the most reliable and robust evaluation results. In this work we analyze a sample of queries categorized by length and corpus-appropriateness to determine the right proportion needed to distinguish between systems. We also analyze the appropriate division of labor between developing topics and making relevance judgments, and show that only a small, biased sample of queries with sparse judgments is needed to produce the same results as a much larger sample of queries.

#index 1195854
#* The Combination and Evaluation of Query Performance Prediction Methods
#@ Claudia Hauff;Leif Azzopardi;Djoerd Hiemstra
#t 2009
#c 16
#% 144034
#% 340948
#% 397161
#% 804915
#% 879614
#% 987260
#% 1073876
#% 1279327
#% 1415713
#% 1415785
#! In this paper, we examine a number of newly applied methods for combining pre-retrieval query performance predictors in order to obtain a better prediction of the query's performance. However, in order to adequately and appropriately compare such techniques, we critically examine the current evaluation methodology and show how using linear correlation coefficients (i) do not provide an intuitive measure indicative of a method's quality, (ii) can provide a misleading indication of performance, and (iii) overstate the performance of combined methods. To address this, we extend the current evaluation methodology to include cross validation, report a more intuitive and descriptive statistic, and apply statistical testing to determine significant differences. During the course of a comprehensive empirical study over several TREC collections, we evaluate nineteen pre-retrieval predictors and three combination methods.

#index 1195855
#* Investigating Learning Approaches for Blog Post Opinion Retrieval
#@ Shima Gerani;Mark J. Carman;Fabio Crestani
#t 2009
#c 16
#% 269217
#% 279755
#% 311027
#% 344447
#% 413613
#% 750863
#% 783474
#% 879598
#% 983599
#% 987226
#% 1019145
#% 1415716
#! Blog post opinion retrieval is the problem of identifying posts which express an opinion about a particular topic. Usually the problem is solved using a 3 step process in which relevant posts are first retrieved, then opinion scores are generated for each document, and finally the opinion and relevance scores are combined to produce a single ranking. In this paper, we study the effectiveness of classification and rank learning techniques for solving the blog post opinion retrieval problem. We have chosen not to rely on external lexicons of opinionated terms, but investigate to what extent the list of opinionated terms can be mined from the same corpus of relevance/opionion assessments that are used to train the retrieval system. We compare popular feature selection methods such as the weighted log likelihood ratio and mutual information for use both in selecting terms for training an opinionated document classifier and also as term weights for generating simpler (not learning based) aggregate opinion scores for documents. We thereby analyze what performance gains result from learning in the opinion detection phase. Furthermore we compare different learning and not learning based methods for combining relevance and opinion information in order to generate a ranked list of opinionated posts, thereby investigating the effect of learning on the ranking phase.

#index 1195856
#* Integrating Proximity to Subjective Sentences for Blog Opinion Retrieval
#@ Rodrygo L. T. Santos;Ben He;Craig Macdonald;Iadh Ounis
#t 2009
#c 16
#% 397205
#% 786840
#% 818262
#% 943811
#% 987356
#% 1074158
#% 1074168
#% 1130915
#% 1348062
#% 1415716
#% 1742093
#! Opinion finding is a challenging retrieval task, where it has been shown that it is especially difficult to improve over a strongly performing topic-relevance baseline. In this paper, we propose a novel approach for opinion finding, which takes into account the proximity of query terms to subjective sentences in a document. We adapt two state-of-the-art opinion detection techniques to identify subjective sentences from the retrieved documents. Our first technique uses the OpinionFinder toolkit to classify the subjectiveness of sentences in a document. Our second technique uses an automatically generated dictionary of subjective terms derived from the document collection itself to identify the most subjective sentences in a document. We extend the Divergence From Randomness (DFR) proximity model to integrate the proximity of query terms to the subjective sentences identified by either of the proposed techniques. We evaluate these techniques on five different strong baselines across two different query datasets from the TREC Blog track. We show that we can significantly improve over the baselines and that, in several settings, our proposed techniques can at least match the top performing systems at the TREC Blog track.

#index 1195857
#* Adapting Naive Bayes to Domain Adaptation for Sentiment Analysis
#@ Songbo Tan;Xueqi Cheng;Yuefen Wang;Hongbo Xu
#t 2009
#c 16
#% 266292
#% 344447
#% 466263
#% 838522
#% 854646
#% 855172
#% 1019099
#% 1019180
#% 1269755
#% 1272110
#! In the community of sentiment analysis, supervised learning techniques have been shown to perform very well. When transferred to another domain, however, a supervised sentiment classifier often performs extremely bad. This is so-called domain-transfer problem. In this work, we attempt to attack this problem by making the maximum use of both the old-domain data and the unlabeled new-domain data. To leverage knowledge from the old-domain data, we proposed an effective measure, i.e., Frequently Co-occurring Entropy (FCE), to pick out generalizable features that occur frequently in both domains and have similar occurring probability. To gain knowledge from the new-domain data, we proposed Adapted Naïve Bayes (ANB), a weighted transfer version of Naive Bayes Classifier. The experimental results indicate that proposed approach could improve the performance of base classifier dramatically, and even provide much better performance than the transfer-learning baseline, i.e. the Naïve Bayes Transfer Classifier (NTBC).

#index 1195858
#* PathRank: Web Page Retrieval with Navigation Path
#@ Jianqiang Li;Yu Zhao
#t 2009
#c 16
#% 265141
#% 287204
#% 290830
#% 309749
#% 325198
#% 330609
#% 330765
#% 348178
#% 387427
#% 577339
#% 590523
#% 642980
#% 642981
#% 643069
#% 754059
#% 766462
#% 805896
#% 818233
#% 824560
#% 869527
#% 956533
#% 963890
#! This paper describes a path-based method to use the multi-step navigation information discovered from website structures for web page ranking. Use of hyperlinks to enhance page ranking has been widely studied. The underlying assumption is that hyperlinks convey recommendations. Although this technique has been used successfully in global web search, it produces poor results for website search, because the majority of the hyperlinks in local websites are used to organize information and convey no recommendations. This paper defines the Hierarchical Navigation Path (HNP) as a new resource to exploit these hyperlinks for improved web search. HNP is composed of multi-step hyperlinks in visitors' website navigation. It provides indications of the content of the destination page. The HierPathExt algorithm is given to extract HNPs in local websites. Then, the PathRank algorithm is created to use HNPs for web page retrieval. The experiments show that our approach results in significant improvements over existing solutions.

#index 1195859
#* Query Expansion Using External Evidence
#@ Zhijun Yin;Milad Shokouhi;Nick Craswell
#t 2009
#c 16
#% 218978
#% 262096
#% 310567
#% 340901
#% 342707
#% 342961
#% 348155
#% 411762
#% 730007
#% 810906
#% 838530
#% 838532
#% 869500
#% 879585
#% 907493
#% 987222
#% 987231
#% 1074078
#% 1074079
#% 1074081
#% 1074099
#! Automatic query expansion may be used in document retrieval to improve search effectiveness. Traditional query expansion methods are based on the document collection itself. For example, pseudo-relevance feedback (PRF) assumes that the top retrieved documents are relevant, and uses the terms extracted from those documents for query expansion. However, there are other sources of evidence that can be used for expansion, some of which may give better search results with greater efficiency at query time. In this paper, we use the external evidence, especially the hints obtained from external web search engines to expand the original query. We explore 6 different methods using search engine query log, snippets and search result documents. We conduct extensive experiments, with state of the art PRF baselines and careful parameter tuning, on three TREC collections: AP, WT10g, GOV2. Log-based methods do not show consistent significant gains, despite being very efficient at query-time. Snippet-based expansion, using the summaries provided by an external search engine, provides significant effectiveness gains with good efficiency at query-time.

#index 1195860
#* Selective Application of Query-Independent Features in Web Information Retrieval
#@ Jie Peng;Iadh Ounis
#t 2009
#c 16
#% 268079
#% 340934
#% 397126
#% 766462
#% 783474
#% 818255
#% 1392433
#% 1674994
#! The application of query-independent features, such as PageRank, can boost the retrieval effectiveness of a Web Information Retrieval (IR) system. In some previous works, a query-independent feature is uniformly applied to all queries. Other works predict the most useful feature based on the query type. However, the accuracy of the current query type prediction methods is not high. In this paper, we investigate a novel approach that applies the most appropriate query-independent feature on a per-query basis, and does not require the knowledge of the query type. The approach is based on an estimate of the divergence between the retrieved document scores' distributions prior to, and after the integration of a query-independent feature. We evaluate our approach on the TREC .GOV Web test collection and the mixed topic sets from TREC 2003 & 2004 Web search tasks. Our experimental results demonstrate that the selective application of a query-independent feature on a per-query basis is very effective and robust. In particular, it outperforms a query type prediction-based method, even when this method is simulated with a 100% query type prediction accuracy.

#index 1195861
#* Measuring the Search Effectiveness of a Breadth-First Crawl
#@ Dennis Fetterly;Nick Craswell;Vishwa Vinay
#t 2009
#c 16
#% 268087
#% 281166
#% 281251
#% 330609
#% 411762
#% 453327
#% 480136
#% 766409
#% 805879
#% 818255
#% 907496
#% 956536
#% 987237
#% 1016177
#% 1022233
#% 1055714
#% 1074172
#% 1599360
#! Previous scalability experiments found that early precision improves as collection size increases. However, that was under the assumption that a collection's documents are all sampled with uniform probability from the same population. We contrast this to a large breadth-first web crawl, an important scenario in real-world Web search, where the early documents have quite different characteristics from the later documents. Having observed that NDCG@100 (measured over a set of reference queries) begins to plateau in the initial stages of the crawl, we investigate a number of possible reasons for this behaviour. These include the web-pages themselves, the metric used to measure retrieval effectiveness as well as the set of relevance judgements used.

#index 1195862
#* Using Contextual Information to Improve Search in Email Archives
#@ Wouter Weerkamp;Krisztian Balog;Maarten Rijke
#t 2009
#c 16
#% 146494
#% 280850
#% 287253
#% 340901
#% 378496
#% 766484
#% 766525
#% 803545
#% 818204
#% 818337
#% 838067
#% 869649
#% 879568
#% 879584
#% 879585
#% 939909
#% 1074094
#% 1074097
#! In this paper we address the task of finding topically relevant email messages in public discussion lists. We make two important observations. First, email messages are not isolated, but are part of a larger online environment. This context, existing on different levels, can be incorporated into the retrieval model. We explore the use of thread, mailing list, and community content levels, by expanding our original query with term from these sources. We find that query models based on contextual information improve retrieval effectiveness. Second, email is a relatively informal genre, and therefore offers scope for incorporating techniques previously shown useful in searching user-generated content. Indeed, our experiments show that using query-independent features (email length, thread size, and text quality), implemented as priors, results in further improvements.

#index 1195863
#* Part of Speech Based Term Weighting for Information Retrieval
#@ Christina Lioma;Roi Blanco
#t 2009
#c 16
#% 118752
#% 158687
#% 169781
#% 218982
#% 714615
#% 719598
#% 751569
#% 815912
#% 816060
#% 818247
#% 818255
#% 857180
#% 983610
#% 987229
#% 1392438
#% 1392447
#% 1392482
#! Automatic language processing tools typically assign to terms so-called `weights' corresponding to the contribution of terms to information content. Traditionally, term weights are computed from lexical statistics, e.g., term frequencies. We propose a new type of term weight that is computed from part of speech (POS) n-gram statistics. The proposed POS-based term weight represents how informative a term is in general, based on the `POS contexts' in which it generally occurs in language. We suggest five different computations of POS-based term weights by extending existing statistical approximations of term information measures. We apply these POS-based term weights to information retrieval, by integrating them into the model that matches documents to queries. Experiments with two TREC collections and 300 queries, using TF-IDF & BM25 as baselines, show that integrating our POS-based term weights to retrieval always leads to gains (up to +33.7% from the baseline). Additional experiments with a different retrieval model as baseline (Language Model with Dirichlet priors smoothing) and our best performing POS-based term weight, show retrieval gains always and consistently across the whole smoothing range of the baseline.

#index 1195864
#* Word Particles Applied to Information Retrieval
#@ Evandro B. Gouvêa;Bhiksha Raj
#t 2009
#c 16
#% 309114
#% 389155
#% 1775144
#! Document retrieval systems conventionally use words as the basic unit of representation, a natural choice since words are primary carriers of semantic information. In this paper we propose the use of a different, phonetically defined unit of representation that we call "particles". Particles are phonetic sequences that do not possess meaning. Both documents and queries are converted from their standard word-based form into sequences of particles. Indexing and retrieval is performed with particles. Experiments show that this scheme is capable of achieving retrieval performance that is comparable to that from words when the text in the documents and queries are clean, and can result in significantly improved retrieval when they are noisy.

#index 1195865
#* "They Are Out There, If You Know Where to Look": Mining Transliterations of OOV Query Terms for Cross-Language Information Retrieval
#@ Raghavendra Udupa;Saravanan K;Anton Bakalov;Abhijit Bhole
#t 2009
#c 16
#% 262096
#% 397143
#% 562054
#% 579944
#% 609093
#% 643018
#% 730025
#% 741114
#% 748556
#% 750863
#% 786574
#% 807747
#% 810634
#% 815913
#% 854571
#% 854584
#% 855302
#% 874256
#% 939511
#% 1074077
#% 1130992
#% 1275632
#% 1717371
#! It is well known that the use of a good Machine Transliteration system improves the retrieval performance of Cross-Language Information Retrieval (CLIR) systems when the query and document languages have different orthography and phonetic alphabets. However, the effectiveness of a Machine Transliteration system in CLIR is limited by its ability to produce relevant transliterations, i.e. those transliterations which are actually present in the relevant documents. In this work, we propose a new approach to the problem of finding transliterations for out-of-vocabulary query terms. Instead of "generating" the transliterations using a Machine Transliteration system, we "mine" them, using a transliteration similarity model, from the top CLIR results for the query. We treat the query and each of the top results as "comparable" documents and search for transliterations in these comparable document pairs. We demonstrate the effectiveness of our approach using queries in two languages from two different linguistic families to retrieve English documents from two standard CLEF collections. We also compare our results with those of a state-of-the-art Machine Transliteration system.

#index 1195866
#* E-Mail Classification for Phishing Defense
#@ Wilfried N. Gansterer;David Pölz
#t 2009
#c 16
#% 136350
#% 831093
#% 860076
#% 863311
#% 869514
#% 872621
#% 875414
#% 879580
#% 926881
#% 955001
#% 956558
#% 956559
#% 975803
#% 989149
#% 1059405
#% 1684659
#! We discuss a classification-based approach for filtering phishing messages in an e-mail stream. Upon arrival, various features of every e-mail are extracted. This forms the basis of a classification process which detects potentially harmful phishing messages. We introduce various new features for identifying phishing messages and rank established as well as newly introduced features according to their significance for this classification problem. Moreover, in contrast to classical binary classification approaches (spam vs. not spam), a more refined ternary classification approach for filtering e-mail data is investigated which automatically distinguishes three message types: ham (solicited e-mail), spam, and phishing. Experiments with representative data sets illustrate that our approach yields better classification results than existing phishing detection methods. Moreover, the direct ternary classification proposed is compared to a sequence of two binary classification processes. Direct one-step ternary classification is not only more efficient, but is also shown to achieve better accuracy than repeated binary classification.

#index 1195867
#* Multi-facet Rating of Product Reviews
#@ Stefano Baccianella;Andrea Esuli;Fabrizio Sebastiani
#t 2009
#c 16
#% 332658
#% 465754
#% 577355
#% 770778
#% 872759
#% 907490
#% 939346
#% 939896
#% 956642
#% 987243
#% 1127964
#% 1299754
#% 1411134
#! Online product reviews are becoming increasingly available, and are being used more and more frequently by consumers in order to choose among competing products. Tools that rank competing products in terms of the satisfaction of consumers that have purchased the product before, are thus also becoming popular. We tackle the problem of rating (i.e., attributing a numerical score of satisfaction to) consumer reviews based on their textual content. We here focus on multi-facet review rating, i.e., on the case in which the review of a product (e.g., a hotel) must be rated several times, according to several aspects of the product (for a hotel: cleanliness, centrality of location, etc.). We explore several aspects of the problem, with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. We present the results of experiments conducted on a dataset of more than 15,000 reviews that we have crawled from a popular hotel review site.

#index 1195868
#* Exploiting Surface Features for the Prediction of Podcast Preference
#@ Manos Tsagkias;Martha Larson;Maarten Rijke
#t 2009
#c 16
#% 926881
#% 1021664
#% 1035587
#% 1074111
#% 1131178
#% 1215355
#% 1261574
#% 1740551
#% 1858640
#! Podcasts display an unevenness characteristic of domains dominated by user generated content, resulting in potentially radical variation of the user preference they enjoy. We report on work that uses easily extractable surface features of podcasts in order to achieve solid performance on two podcast preference prediction tasks: classification of preferred vs. non-preferred podcasts and ranking podcasts by level of preference. We identify features with good discriminative potential by carrying out manual data analysis, resulting in a refinement of the indicators of an existent podcast preference framework. Our preference prediction is useful for topic-independent ranking of podcasts, and can be used to support download suggestion or collection browsing.

#index 1195869
#* A Topic-Based Measure of Resource Description Quality for Distributed Information Retrieval
#@ Mark Baillie;Mark J. Carman;Fabio Crestani
#t 2009
#c 16
#% 280819
#% 301225
#% 340146
#% 447946
#% 643012
#% 722904
#% 800569
#% 818211
#% 869499
#% 874992
#% 879587
#% 987255
#% 987256
#% 1077150
#% 1415751
#% 1682446
#% 1684716
#! The aim of query-based sampling is to obtain a sufficient, representative sample of an underlying (text) collection. Current measures for assessing sample quality are too coarse grain to be informative. This paper outlines a measure of finer granularity based on probabilistic topic models of text. The assumption we make is that a representative sample should capture the broad themes of the underlying text collection. If these themes are not captured, then resource selection will be affected in terms of performance, coverage and reliability. For example, resource selection algorithms that require extrapolation from a small sample of indexed documents to determine which collections are most likely to hold relevant documents may be affected by samples which do not reflect the topical density of a collection. To address this issue we propose to measure the relative entropy between topics obtained in a sample with respect to the complete collection. Topics are both modelled from the collection and inferred in the sample using latent Dirichlet allocation. The paper outlines an analysis and evaluation of this methodology across a number of collections and sampling algorithms.

#index 1195870
#* Simple Adaptations of Data Fusion Algorithms for Source Selection
#@ Georgios Paltoglou;Michail Salampasis;Maria Satratzemi
#t 2009
#c 16
#% 194246
#% 232703
#% 282422
#% 287463
#% 306468
#% 309133
#% 340146
#% 340936
#% 413594
#% 480479
#% 567255
#% 642992
#% 643011
#% 643012
#% 672628
#% 722311
#% 722312
#% 729027
#% 783473
#% 818211
#% 818212
#% 852010
#% 879604
#% 907525
#% 987255
#% 1019091
#% 1392444
#% 1682446
#! Source selection deals with the problem of selecting the most appropriate information sources from the set of, usually non-intersecting, available document collections. On the other hand, data fusion techniques (also known as metasearch techniques) deal with the problem of aggregating the results from multiple, usually completely or partly intersecting, document sources in order to provide a wider coverage and a more effective retrieval result. In this paper we study some simple adaptations to traditional data fusion algorithms for the task of source selection in uncooperative distributed information retrieval environments. The experiments demonstrate that the performance of data fusion techniques at source selection tasks is comparable with that of state-of-the-art source selection algorithms and they are often able to surpass them.

#index 1195871
#* Document Compaction for Efficient Query Biased Snippet Generation
#@ Yohannes Tsegay;Simon J. Puglisi;Andrew Turpin;Justin Zobel
#t 2009
#c 16
#% 253191
#% 262036
#% 280835
#% 292221
#% 326522
#% 340916
#% 387427
#% 413587
#% 717120
#% 750863
#% 805862
#% 867054
#% 907504
#% 944348
#% 987208
#% 1306081
#! Current web search engines return query-biased snippets for each document they list in a result set. For efficiency, search engines operating on large collections need to cache snippets for common queries, and to cache documents to allow fast generation of snippets for uncached queries. To improve the hit rate on a document cache during snippet generation, we propose and evaluate several schemes for reducing document size, hence increasing the number of documents in the cache. In particular, we argue against further improvements to document compression, and argue for schemes that prune documents based on the a priori likelihood that a sentence will be used as part of a snippet for a given document. Our experiments show that if documents are reduced to less than half their original size, 80% of snippets generated are identical to those generated from the original documents. Moreover, as the pruned, compressed surrogates are smaller, 3-4 times as many documents can be cached.

#index 1195872
#* Organizing Suggestions in Autocompletion Interfaces
#@ Alia Amin;Michiel Hildebrand;Jacco Ossenbruggen;Vanessa Evers;Lynda Hardman
#t 2009
#c 16
#% 313719
#% 346553
#% 823348
#% 879567
#% 987211
#% 987212
#! We describe two user studies that investigate organization strategies of autocompletion in a known-item search task: searching for terms taken from a thesaurus. In Study 1, we explored ways of grouping term suggestions from two different thesauri (TGN and WordNet) and found that different thesauri may require different organization strategies. Users found Group organization more appropriate for location names from TGN, while Alphabetical works better for object names from WordNet. In Study 2, we compared three different organization strategies (Alphabetical , Group and Composite ) for location name search tasks. The results indicate that for TGN autocompletion interfaces help improve the quality of keywords, Group and Composite organization help users search faster, and is perceived easier to understand and to use than Alphabetical .

#index 1195873
#* Building a Graph of Names and Contextual Patterns for Named Entity Classification
#@ César Pablo-Sánchez;Paloma Martínez
#t 2009
#c 16
#% 301241
#% 815307
#% 854663
#% 854819
#% 874992
#% 1275038
#% 1288556
#% 1291356
#% 1732738
#! An algorithm that bootstraps the acquisition of large dictionaries of entity types (names) and pattern types from a few seeds and a large unannotated corpora is presented. The algorithm iteratively builds a bigraph of entities and collocated patterns by querying the text. Several classes simultaneously compete to label the entity types. Different experiments have been carried to acquire resources from a 1GB corpus of Spanish news. The usefulness of the acquired list of entity types for the task of Name Classification has also been evaluated with good results for a weakly supervised method.

#index 1195874
#* Combination of Documents Features Based on Simulated Click-through Data
#@ Ali Mohammad Bidoki;James A. Thom
#t 2009
#c 16
#% 387427
#% 411762
#% 577224
#% 728360
#% 734915
#% 783482
#% 840846
#% 879567
#% 879588
#! Many different ranking algorithms based on content and context have been used in web search engines to find pages based on a user query. Furthermore, to achieve better performance some new solutions combine different algorithms. In this paper we use simulated click-through data to learn how to combine many content and context features of web pages. This method is simple and practical to use with actual click-through data in a live search engine. The proposed approach is evaluated using the LETOR benchmark and we found it is competitive to Ranking SVM based on user judgments.

#index 1195875
#* Discovering Association Rules on Experiences from Large-Scale Blog Entries
#@ Takeshi Kurashima;Ko Fujimura;Hidenori Okuda
#t 2009
#c 16
#% 481290
#% 805873
#% 815915
#% 855300
#% 867057
#! This paper proposes a method for discovering association rules on peoples' experiences extracted from a large-scale set of blog entries. In our definition, a person's experience can be expressed by five attributes: time, location, activity, opinion and emotion. The system implementing our proposed method actually generates and ranks association rules between attributes by applying several interestingness measures proposed in the area of data mining to the experiences extracted from 48 million blog entries. An experiment shows that the system successfully mines peoples' activities and emotions which are specific to location and time period.

#index 1195876
#* Extracting Geographic Context from the Web: GeoReferencing in MyMoSe
#@ Álvaro Zubizarreta;Pablo Fuente;José M. Cantera;Mario Arias;Jorge Cabrero;Guido García;César Llamas;Jesús Vegas
#t 2009
#c 16
#% 177551
#% 290148
#% 508117
#% 766441
#% 796109
#% 815280
#% 817846
#% 844398
#% 855108
#% 998521
#! Many Web pages are clearly related to specific locations. Identifying this geographic focus is the cornerstone of the next generation of geographic context aware search services. This paper shows a multistage method for assigning a geographic focus to Web pages (GeoReferencing), using several heuristics for toponym disambiguation and a scoring function for focus determination. We provide an experimental methodology for evaluating the accuracy of the system with Web pages in English and Spanish. Finally, we have obtained promising results, reaching an accuracy of over 70% with a town-level resolution.

#index 1195877
#* What Else Is There? Search Diversity Examined
#@ Mark Sanderson;Jiayu Tang;Thomas Arni;Paul Clough
#t 2009
#c 16
#% 262112
#% 642975
#% 879618
#% 905224
#% 1024548
#% 1074113
#% 1074133
#% 1106212
#% 1432276
#% 1432290
#% 1916106
#! This paper describes a study on diversity in image search results. One of the first test collections explicitly built to study diversity --- the ImageCLEFPhoto 2008 collection --- was used in an evaluation exercise in the summer of 2008. Analyzing 200 of the runs submitted by 24 research groups enabled the relationship between precision and result diversity to be studied. In addition, the level of diversity present in search results produced by retrieval systems built without explicit support for diversity was computed. The remaining potential to improve on diversity was calculated and finally, a significant preference by users for diverse search results was shown.

#index 1195878
#* Using Second Order Statistics to Enhance Automated Image Annotation
#@ Ainhoa Llorente;Stefan Rüger
#t 2009
#c 16
#% 279755
#% 387427
#% 457912
#% 642989
#% 706106
#% 722927
#% 780862
#% 839975
#% 889273
#% 903603
#% 990253
#% 990338
#% 1148273
#% 1432279
#% 1677726
#! We examine whether a traditional automated annotation system can be improved by using background knowledge. Traditional means any machine learning approach together with image analysis techniques. We use as a baseline for our experiments the work done by Yavlinsky et al. [1] who deployed non-parametric density estimation. We observe that probabilistic image analysis by itself is not enough to describe the rich semantics of an image. Our hypothesis is that more accurate annotations can be produced by introducing additional knowledge in the form of statistical co-occurrence of terms. This is provided by the context of images that otherwise independent keyword generation would miss. We test our algorithm with two different datasets: Corel 5k and ImageCLEF 2008. For the Corel 5k dataset, we obtain significantly better results while our algorithm appears in the top quartile of all methods submitted in ImageCLEF 2008.

#index 1195879
#* Classifying and Characterizing Query Intent
#@ Azin Ashkan;Charles L. Clarke;Eugene Agichtein;Qi Guo
#t 2009
#c 16
#% 590523
#% 805878
#% 869550
#% 949162
#% 956546
#% 996725
#! Understanding the intent underlying users' queries may help personalize search results and improve user satisfaction. In this paper, we develop a methodology for using ad clickthrough logs, query specific information, and the content of search engine result pages to study characteristics of query intents, specially commercial intent. The findings of our study suggest that ad clickthrough features, query features, and the content of search engine result pages are together effective in detecting query intent. We also study the effect of query type and the number of displayed ads on the average clickthrough rate. As a practical application of our work, we show that modeling query intent can improve the accuracy of predicting ad clickthrough for previously unseen queries.

#index 1195880
#* Design and Evaluation of a University-Wide Expert Search Engine
#@ Ruud Liebregts;Toine Bogers
#t 2009
#c 16
#% 730082
#% 781141
#% 879570
#% 987261
#% 1047421
#! We present an account of designing and evaluating a university-wide expert search engine. We performed system-based evaluation to determine the optimal retrieval settings and an extensive user-based evaluation with three different user groups: scientific researchers, students, and outside visitors of the website looking for experts. Our search engine significantly outperformed the old search system in terms of effectiveness, efficiency, and user satisfaction.

#index 1195881
#* A Study of the Impact of Index Updates on Distributed Query Processing for Web Search
#@ Charalampos Sarigiannis;Vassilis Plachouras;Ricardo Baeza-Yates
#t 2009
#c 16
#% 578337
#% 728102
#% 754058
#% 756015
#% 764562
#% 978374
#% 987216
#% 1055108
#! Query processing in Web search engines today is mainly performed within a single site or data center, which is required to scale as the Web grows and users require fast answers to their queries. Constraints in the size and cost of data centers, however, may limit the scalability of search engines. Multi-site search engines that perform distributed query processing represent one way to overcome such constraints. Each site processes locally as many queries as possible, keeping latency low without contacting remote sites. Forwarding a query to remote sites depends on the document collection of remote sites. Multi-site search engines pose several new challenges. When a site updates its index, it has to inform other sites. The updates, however, are not instantaneous due to the volume of data exchanged or possible network failures. During the period of time that there are index inconsistencies across sites, queries may not be forwarded optimally. In this work, we investigate the impact of index inconsistencies on a distributed query processing algorithm, when there are index updates, and we observe that delayed index information propagation reduces the effectiveness of query processing, because queries are less likely to be routed optimally.

#index 1195882
#* Generic and Spatial Approaches to Image Search Results Diversification
#@ Monica Lestari Paramita;Jiayu Tang;Mark Sanderson
#t 2009
#c 16
#% 262112
#% 566642
#% 642975
#% 879618
#% 907493
#% 987205
#% 1075340
#% 1271267
#% 1742129
#! We propose a generic diversity and two novel spatial diversity algorithms for (image) search result diversification. The outputs of the algorithms are compared with the standard search results (which contains no diversity implementation) and found to be promising. In particular, the geometric mean spatial diversity algorithm manages to achieve good geographical diversity while not significantly reducing precision. To the best of our knowledge, such a quantitive evaluation of spatial diversity algorithms for context based image retrieval is new to the community.

#index 1195883
#* Studying Query Expansion Effectiveness
#@ Ben He;Iadh Ounis
#t 2009
#c 16
#% 326522
#% 399890
#% 857180
#% 1019093
#% 1074081
#! Query expansion is an effective technique in improving the retrieval performance for ad-hoc retrieval. However, query expansion can also fail, leading to a degradation of the retrieval performance. In this paper, we aim to provide a better understanding of query expansion by an empirical study on what factors can affect query expansion, and how these factors affect query expansion. We examine how the quality of the query, measured by the first-pass retrieval performance, is related to the effectiveness of query expansion. Our experimental results only show a moderate relation between them, indicating that the first-pass retrieval has only a moderate impact on the effectiveness of query expansion. Our results also show that the feedback documents should not only be relevant, but should also have a dedicated interest in the topic.

#index 1195884
#* Correlation of Term Count and Document Frequency for Google N-Grams
#@ Martin Klein;Michael L. Nelson
#t 2009
#c 16
#% 281209
#% 309145
#% 321635
#% 590525
#% 674852
#% 679872
#% 735137
#% 781168
#% 807338
#% 881071
#% 1051901
#% 1107070
#% 1131150
#! For bounded datasets such as the TREC Web Track (WT10g) the computation of term frequency (TF) and inverse document frequency (IDF) is not difficult. However, when the corpus is the entire web, direct IDF calculation is impossible and values must instead be estimated. Most available datasets provide values for term count (TC) meaning the number of times a certain term occurs in the entire corpus. Intuitively this value is different from document frequency (DF) , the number of documents (e.g., web pages) a certain term occurs in. We investigate the relationship between TC and DF values of terms occurring in the Web as Corpus (WaC) and also the similarity between TC values obtained from the WaC and the Google N-gram dataset. A strong correlation between the two would gives us confidence in using the Google N-grams to estimate accurate IDF values which for example is the foundation to generate well performing lexical signatures based on the TF-IDF scheme. Our results show a very strong correlation between TC and DF within the WaC with Spearman's ρ *** 0.8 (p ≤ 2.2×10*** 16) and a high similarity between TC values from the WaC and the Google N-grams.

#index 1195885
#* A Cost-Aware Strategy for Query Result Caching in Web Search Engines
#@ Ismail Sengor Altingovde;Rifat Ozcan;Özgür Ulusoy
#t 2009
#c 16
#% 296646
#% 860861
#% 863348
#% 978378
#% 987214
#% 987215
#% 1019182
#% 1055849
#% 1404875
#% 1683906
#% 1915933
#! Search engines and large scale IR systems need to cache query results for efficiency and scalability purposes. In this study, we propose to explicitly incorporate the query costs in the static caching policy. To this end, a query's cost is represented by its execution time, which involves CPU time to decompress the postings and compute the query-document similarities to obtain the final top-N answers. Simulation results using a large Web crawl data and a real query log reveal that the proposed strategy improves overall system performance in terms of the total query execution time.

#index 1195886
#* Quality-Oriented Search for Depression Portals
#@ Thanh Tang;David Hawking;Ramesh Sankaranarayana;Kathleen M. Griffiths;Nick Craswell
#t 2009
#c 16
#% 309095
#% 838406
#% 862154
#! The problem of low-quality information on the Web is nowhere more important than in the domain of health, where unsound information and misleading advice can have serious consequences. The quality of health web sites can be rated by subject experts against evidence-based guidelines. We previously developed an automated quality rating technique (AQA) for depression websites and showed that it correlated 0.85 with such expert ratings. In this paper, we use AQA to filter or rerank Google results returned in response to queries relating to depression. We compare this to an unrestricted quality-oriented (AQA based) focused crawl starting from an Open Directory category and a conventional crawl with manually constructed seedlist and inclusion rules. The results show that post-processed Google outperforms other forms of search engine restricted to the domain of depressive illness on both relevance and quality.

#index 1195887
#* Evaluation of Text Clustering Algorithms with N-Gram-Based Document Fingerprints
#@ Javier Parapar;Álvaro Barreiro
#t 2009
#c 16
#% 118771
#% 255137
#% 296738
#% 375017
#% 397148
#% 654447
#% 660332
#% 766430
#% 768815
#% 907618
#% 1130957
#! This paper presents a new approach designed to reduce the computational load of the existing clustering algorithms by trimming down the documents size using fingerprinting methods. Thorough evaluation was performed over three different collections and considering four different metrics. The presented approach to document clustering achieved good values of effectiveness with considerable save in memory space and computation time.

#index 1195888
#* Exploiting Flickr Tags and Groups for Finding Landmark Photos
#@ Rabeeh Abbasi;Sergey Chernov;Wolfgang Nejdl;Raluca Paiu;Steffen Staab
#t 2009
#c 16
#% 190581
#% 402289
#% 903606
#% 967244
#% 997189
#% 1055701
#% 1065253
#! Many people take pictures of different city landmarks and post them to photo-sharing systems like Flickr. They also add tags and place photos in Flickr groups, created around particular themes. Using tags, other people can search for representative landmark images of places of interest. Searching for landmarks using tags results into many non-landmark photos and provides poor landmark summary for a city. In this paper we propose a new method to identify landmark photos using tags and social Flickr groups. In contrast to similar modern systems, our approach is also applicable when GPS-coordinates for photos are not available. Presented user study shows that the proposed method outperforms state-of-the-art systems for landmark finding.

#index 1195889
#* Refining Keyword Queries for XML Retrieval by Combining Content and Structure
#@ Desislava Petkova;W. Bruce Croft;Yanlei Diao
#t 2009
#c 16
#% 314740
#% 397364
#% 413551
#% 479465
#% 869501
#% 879696
#% 894441
#% 1674743
#% 1674744
#% 1721851
#! The structural heterogeneity and complexity of XML repositories makes query formulation challenging for users who have little knowledge of XML. To assist its users, an XML retrieval system can have a keyword-based interface, relegating the task of combining textual and structural clues to the retrieval algorithm. In this work, we propose an automatic query refinement method to transform a keyword query into structured XML queries that capture the original information need and conform to the underlying XML data. We formulate query generation as a search problem, and show the effectiveness of the method in generating accurate content-and-structure queries.

#index 1195890
#* Cover Coefficient-Based Multi-document Summarization
#@ Gonenc Ercan;Fazli Can
#t 2009
#c 16
#% 88045
#% 119916
#% 262112
#% 816173
#% 1223706
#% 1272053
#! In this paper we present a generic, language independent multi-document summarization system forming extracts using the cover coefficient concept. Cover Coefficient-based Summarizer (CCS) uses similarity between sentences to determine representative sentences. Experiments indicate that CCS is an efficient algorithm that is able to generate quality summaries online.

#index 1195891
#* A Practitioner's Guide for Static Index Pruning
#@ Ismail Sengor Altingovde;Rifat Ozcan;Özgür Ulusoy
#t 2009
#c 16
#% 340887
#% 907504
#% 987323
#! We compare the term- and document-centric static index pruning approaches as described in the literature and investigate their sensitivity to the scoring functions employed during the pruning and actual retrieval stages.

#index 1195892
#* Revisiting N-Gram Based Models for Retrieval in Degraded Large Collections
#@ Javier Parapar;Ana Freire;Álvaro Barreiro
#t 2009
#c 16
#% 420481
#% 504885
#% 857180
#% 1130848
#! The traditional retrieval models based on term matching are not effective in collections of degraded documents (output of OCR or ASR systems for instance). This paper presents a n-gram based distributed model for retrieval on degraded text large collections. Evaluation was carried out with both the TREC Confusion Track and Legal Track collections showing that the presented approach outperforms in terms of effectiveness the classical term centred approach and the most of the participant systems in the TREC Confusion Track.

#index 1195893
#* A Simple Linear Ranking Algorithm Using Query Dependent Intercept Variables
#@ Nir Ailon
#t 2009
#c 16
#% 734915
#% 983820
#% 987227
#% 987240
#% 987241
#% 1074065
#! The LETOR website contains three information retrieval datasets used as a benchmark for testing machine learning ideas for ranking. Participating algorithms are measured using standard IR ranking measures (NDCG, precision, MAP). Similarly to other participating algorithms, we train a linear classifier. In contrast, we define an additional free benchmark variable for each query. This allows expressing the fact that results for different queries are incomparable for the purpose of determining relevance. The results are slightly better yet significantly simpler than the reported participating algorithms.

#index 1195894
#* Measurement Techniques and Caching Effects
#@ Stefan Pohl;Alistair Moffat
#t 2009
#c 16
#% 213786
#% 290703
#% 893128
#% 987214
#% 987215
#! Overall query execution time consists of the time spent transferring data from disk to memory, and the time spent performing actual computation. In any measurement of overall time on a given hardware configuration, the two separate costs are aggregated. This makes it hard to reproduce results and to infer which of the two costs is actually affected by modifications proposed by researchers. In this paper we show that repeated submissions of the same query provides a means to estimate the computational fraction of overall query execution time. The advantage of separate measurements is exemplified for a particular optimization that is, as it turns out, reducing computational costs only. Finally, by exchange of repeated query terms with surrogates that have similar document-frequency, we are able to measure the natural caching effects that arise as a consequence of term repetitions in query logs.

#index 1195895
#* On Automatic Plagiarism Detection Based on n-Grams Comparison
#@ Alberto Barrón-Cedeño;Paolo Rosso
#t 2009
#c 16
#% 1713597
#! When automatic plagiarism detection is carried out considering a reference corpus, a suspicious text is compared to a set of original documents in order to relate the plagiarised text fragments to their potential source. One of the biggest difficulties in this task is to locate plagiarised fragments that have been modified (by rewording, insertion or deletion, for example) from the source text. The definition of proper text chunks as comparison units of the suspicious and original texts is crucial for the success of this kind of applications. Our experiments with the METER corpus show that the best results are obtained when considering low level word n -grams comparisons (n = {2,3}).

#index 1195896
#* Exploiting Visual Concepts to Improve Text-Based Image Retrieval
#@ Sabrina Tollari;Marcin Detyniecki;Christophe Marsala;Ali Fakeri-Tabrizi;Massih-Reza Amini;Patrick Gallinari
#t 2009
#c 16
#% 722927
#% 990258
#% 1040539
#% 1432276
#% 1432279
#% 1727375
#! In this paper, we study how to automatically exploit visual concepts in a text-based image retrieval task. First, we use Forest of Fuzzy Decision Trees (FFDTs) to automatically annotate images with visual concepts. Second, using optionally WordNet, we match visual concepts and textual query. Finally, we filter the text-based image retrieval result list using the FFDTs. This study is performed in the context of two tasks of the CLEF2008 international campaign: the Visual Concept Detection Task (VCDT) (17 visual concepts) and the photographic retrieval task (ImageCLEFphoto) (39 queries and 20k images). Our best VCDT run is the 4th best of the 53 submitted runs. The ImageCLEFphoto results show that there is a clear improvement, in terms of precision at 20, when using the visual concepts explicitly appearing in the query.

#index 1195897
#* Choosing the Best MT Programs for CLIR Purposes --- Can MT Metrics Be Helpful?
#@ Kimmo Kettunen
#t 2009
#c 16
#% 397143
#% 561307
#% 807745
#% 815902
#% 939575
#% 995522
#% 1275655
#! This paper describes usage of MT metrics in choosing the best candidates for MT-based query translation resources. Our main metrics is METEOR, but we also use NIST and BLEU. Language pair of our evaluation is English *** German, because MT metrics still do not offer very many language pairs for comparison. We evaluated translations of CLEF 2003 topics of four different MT programs with MT metrics and compare the metrics evaluation results to results of CLIR runs. Our results show, that for long topics the correlations between achieved MAPs and MT metrics is high (0.85-0.94), and for short topics lower but still clear (0.63-0.72). Overall it seems that MT metrics can easily distinguish the worst MT programs from the best ones, but smaller differences are not so clearly shown. Some of the intrinsic properties of MT metrics do not also suit for CLIR resource evaluation purposes, because some properties of translation metrics, especially evaluation of word order, are not significant in CLIR.

#index 1195898
#* Entropy-Based Static Index Pruning
#@ Lei Zheng;Ingemar J. Cox
#t 2009
#c 16
#% 144034
#% 324129
#% 336784
#% 340887
#% 907504
#% 1077150
#% 1392436
#! We propose a new entropy-based algorithm for static index pruning. The algorithm computes an importance score for each document in the collection based on the entropy of each term. A threshold is set according to the desired level of pruning and all postings associated with documents that score below this threshold are removed from the index, i.e. documents are removed from the collection. We compare this entropy-based approach with previous work by Carmel et al. [1], for both the Financial Times (FT) and Los Angeles Times (LA) collections. Experimental results reveal that the entropy-based approach has superior performance on the FT collection, for both precision at 10 (P@10) and mean average precision (MAP). However, for the LA collection, Carmel's method is generally superior with MAP. The variation in performance across collections suggests that a hybrid algorithm that incorporates elements of both methods might have more stable performance across collections. A simple hybrid method is tested, in which a first 10% pruning is performed using the entropy-based method, and further pruning is performed by Carmel's method. Experimental results show that the hybird algorithm can slightly improve that of Carmel's, but performs significantly worse than the entropy-based method on the FT collection.

#index 1195899
#* Representing User Navigation in XML Retrieval with Structural Summaries
#@ Mir Sadek Ali;Mariano P. Consens;Birger Larsen
#t 2009
#c 16
#% 918685
#% 1130924
#% 1206738
#! This poster presents a novel way to represent user navigation in XML retrieval using collection statistics from XML summaries. Currently, developing user navigation models in XML retrieval is costly and the models are specific to collected user assessments. We address this problem by proposing summary navigation models which describe user navigation in terms of XML summaries. We develop our proposal using assessments collected in the interactive track at INEX 2006. Our preliminary results suggest that summary navigation models can represent user navigation in a way that is effective for evaluation and allows economic re-use of assessments for new tasks and collections.

#index 1195900
#* ESUM: An Efficient System for Query-Specific Multi-document Summarization
#@ C. Ravindranath Chowdary;P. Sreenivasa Kumar
#t 2009
#c 16
#% 230530
#% 262112
#% 938761
#% 939968
#% 1074089
#% 1223706
#! In this paper, we address the problem of generating a query-specific extractive summary in a an efficient manner for a given set of documents. In many of the current solutions, the entire collection of documents is modeled as a single graph which is used for summary generation. Unlike these approaches, in this paper, we model each individual document as a graph and generate a query-specific summary for it. These individual summaries are then intelligently combined to produce the final summary. This approach greatly reduces the computational complexity.

#index 1195901
#* Using WordNet's Semantic Relations for Opinion Detection in Blogs
#@ Malik Muhammad Missen;Mohand Boughanem
#t 2009
#c 16
#% 608936
#! The Opinion Detection from blogs has always been a challenge for researchers. One of the challenges faced is to find such documents that specifically contain opinion on users' information need. This requires text processing on sentence level rather than on document level. In this paper, we have proposed an opinion detection approach. The proposed approach focuses on above problem by processing documents on sentence level using different semantic similarity relations of WordNet between sentence words and list of weighted query words expanded through encyclopedia Wikipedia. According to initial results, our approach performs well with MAP of 0.28 and P@10 of 0.64 with improvement of 27% over baseline results. TREC Blog 2006 data is used as test data collection.

#index 1195902
#* Improving Opinion Retrieval Based on Query-Specific Sentiment Lexicon
#@ Seung-Hoon Na;Yeha Lee;Sang-Hyob Nam;Jong-Hyeok Lee
#t 2009
#c 16
#% 1410876
#% 1410947
#! Lexicon-based approaches have been widely used for opinion retrieval due to their simplicity. However, no previous work has focused on the domain-dependency problem in opinion lexicon construction. This paper proposes simple feedback-style learning for query-specific opinion lexicon using the set of top-retrieved documents in response to a query. The proposed learning starts from the initial domain-independent general lexicon and creates a query-specific lexicon by re-updating the opinion probability of the initial lexicon based on top-retrieved documents. Experimental results on recent TREC test sets show that the query-specific lexicon provides a significant improvement over previous approaches, especially in BLOG-06 topics.

#index 1195903
#* Automatically Maintained Domain Knowledge: Initial Findings
#@ Deirdre Lungley;Udo Kruschwitz
#t 2009
#c 16
#% 280849
#% 306468
#% 719140
#% 823348
#% 987203
#% 1019086
#% 1055675
#% 1055676
#% 1783055
#! This paper explores the use of implicit user feedback in adapting the underlying domain model of an intranet search system. The domain model, a Formal Concept Analysis (FCA) lattice, is used as an interactive interface to allow user exploration of the context of an intranet query. Implicit user feedback is harnessed here to surmount the difficulty of achieving optimum document descriptors, essential for a browsable lattice. We present the results of a first user study of query refinements proposed by our adapted lattice.

#index 1195904
#* A Framework of Evaluation for Question-Answering Systems
#@ Sarra Ayari;Brigitte Grau
#t 2009
#c 16
#% 1288541
#! Evaluating complex system is a complex task. Evaluation campaigns are organized each year to test different systems on global results, but they do not evaluate the relevance of the criteria used. Our purpose consist in modifying the intermediate results created by the components and inserting the new results into the process, without modifying the components. We will describe our framework of glass-box evaluation.

#index 1195905
#* Combining Content and Context Similarities for Image Retrieval
#@ Xiaojun Wan
#t 2009
#c 16
#% 334771
#% 387427
#% 718437
#% 1040539
#! CBIR has been a challenging problem and its performance relies on the underlying image similarity (distance) metric. Most existing metrics evaluate pairwise image similarity based only on image content, which is denoted as content similarity . In this study we propose a novel similarity metric to make use of the image contexts in an image collection. The context of an image is built by constructing a vector with each dimension representing the content similarity between the image and any image in the image collection. The context similarity between two images is obtained by computing the similarity between the corresponding context vectors using the vector similarity functions. The content similarity and the context similarity are then combined to evaluate the overall image similarity. Experimental results demonstrate that the use of the context similarity can significantly improve the retrieval performance.

#index 1195906
#* Investigating the Global Semantic Impact of Speech Recognition Error on Spoken Content Collections
#@ Martha Larson;Manos Tsagkias;Jiyin He;Maarten Rijke
#t 2009
#c 16
#% 947424
#% 991016
#% 1055317
#! Errors in speech recognition transcripts have a negative impact on effectiveness of content-based speech retrieval and present a particular challenge for collections containing conversational spoken content. We propose a Global Semantic Distortion (GSD) metric that measures the collection-wide impact of speech recognition error on spoken content retrieval in a query-independent manner. We deploy our metric to examine the effects of speech recognition substitution errors. First, we investigate frequent substitutions, cases in which the recognizer habitually mis-transcribes one word as another. Although habitual mistakes have a large global impact, the long tail of rare substitutions has a more damaging effect. Second, we investigate semantically similar substitutions, cases in which the word spoken and the word recognized do not diverge radically in meaning. Similar substitutions are shown to have slightly less global impact than semantically dissimilar substitutions.

#index 1195907
#* Supervised Semantic Indexing
#@ Bing Bai;Jason Weston;Ronan Collobert;David Grangier
#t 2009
#c 16
#% 280819
#% 387427
#% 577224
#% 722904
#% 838486
#% 840846
#% 929722
#% 1069003
#% 1074207
#! We present a class of models that are discriminatively trained to directly map from the word content in a query-document or document- document pair to a ranking score. Like Latent Semantic Indexing (LSI), our models take account of correlations between words (synonymy, pol- ysemy). However, unlike LSI our models are trained with a supervised signal directly on the task of interest, which we argue is the reason for our superior results. We provide an empirical study on Wikipedia documents, using the links to define document-document or query-document pairs, where we obtain state-of-the-art performance using our method.

#index 1195908
#* Split and Merge Based Story Segmentation in News Videos
#@ Anuj Goyal;P. Punitha;Frank Hopfgartner;Joemon M. Jose
#t 2009
#c 16
#% 466410
#% 741060
#% 780819
#% 817489
#% 1677686
#! Segmenting videos into smaller, semantically related segments which ease the access of the video data is a challenging open research. In this paper, we present a scheme for semantic story segmentation based on anchor person detection. The proposed model makes use of a split and merge mechanism to find story boundaries. The approach is based on visual features and text transcripts. The performance of the system was evaluated using TRECVid 2003 CNN and ABC videos. The results show that the system is in par with state-of-the-art classifier based systems.

#index 1195909
#* Encoding Ordinal Features into Binary Features for Text Classification
#@ Andrea Esuli;Fabrizio Sebastiani
#t 2009
#c 16
#% 253191
#% 269217
#% 311034
#% 420146
#% 1682421
#! We propose a method by means of which supervised learning algorithms that only accept binary input can be extended to use ordinal (i.e., integer-valued) input. This is much needed in text classification, since it becomes thus possible to endow these learning devices with term frequency information, rather than just information on the presence/absence of the term in the document. We test two different learners based on "boosting", and show that the use of our method allows them to obtain effectiveness gains. We also show that one of these boosting methods, once endowed with the representations generated by our method, outperforms an SVM learner with tfidf-weighted input.

#index 1195910
#* Topic and Trend Detection in Text Collections Using Latent Dirichlet Allocation
#@ Levent Bolelli;Şeyda Ertekin;C. Lee Giles
#t 2009
#c 16
#% 722904
#% 769906
#% 788094
#% 875959
#% 881498
#% 956608
#! Algorithms that enable the process of automatically mining distinct topics in document collections have become increasingly important due to their applications in many fields and the extensive growth of the number of documents in various domains. In this paper, we propose a generative model based on latent Dirichlet allocation that integrates the temporal ordering of the documents into the generative process in an iterative fashion. The document collection is divided into time segments where the discovered topics in each segment is propagated to influence the topic discovery in the subsequent time segments. Our experimental results on a collection of academic papers from CiteSeer repository show that segmented topic model can effectively detect distinct topics and their evolution over time.

#index 1195911
#* Measuring Similarity of Geographic Regions for Geographic Information Retrieval
#@ Andreas Henrich;Volker Lüdecke
#t 2009
#c 16
#% 664387
#% 1055910
#! Representations of geographic regions play a decisive role in geographic information retrieval, where the query is specified by a conceptual part and a geographic part. One aspect is to use them as query footprint which is then applied for the geographic ranking of documents. Users often specify textual descriptions of geographic regions that are not contained in the underlying gazetteer or geographic database. Approaches that automatically determine a geographic footprint for those locations have a strong need for measuring the quality of this footprint, for evaluation as well as for automatical parameter learning. This quality is determined by the 'similarity' between the footprint and a correct representation of that region. In this paper we introduce three domain-specific points of view for measuring the similarity between representations of geographic regions for geographic information retrieval. For each point of view (strict similarity, visual similarity and similarity in ranking) we introduce a dedicated measure, two of which are novel measures that we propose in this paper.

#index 1195912
#* Towards the Selection of Induced Syntactic Relations
#@ Nicolas Béchet;Mathieu Roche;Jacques Chauché
#t 2009
#c 16
#% 78171
#% 211043
#% 458630
#% 459374
#% 747566
#! We propose in this paper to use NLP approaches to validate induced syntactic relations. We focus on a Web Validation system, a Semantic Vector-based approach, and finally a Combined system. The Semantic Vector approach is a Roget-based approach which computes a syntactic relation as a vector. The Web Validation technique uses a search engine to determine the relevance of a syntactic relation. We experiment our approaches on real-world data set. The ROC curves are used to evaluate the results.

#index 1195913
#* DiffPost: Filtering Non-relevant Content Based on Content Difference between Two Consecutive Blog Posts
#@ Sang-Hyob Nam;Seung-Hoon Na;Yeha Lee;Jong-Hyeok Lee
#t 2009
#c 16
#% 340948
#% 956499
#% 1410876
#% 1410947
#! One of the important issues in blog search engines is to extract the cleaned text from blog post. In practice, this extraction process is confronted with many non-relevant contents in the original blog post, such as menu, banner, site description, etc, causing the ranking be less-effective. The problem is that these non-relevant contents are not encoded in a unified way but encoded in many different ways between blog sites. Thus, the commercial vendor of blog sites should consider tuning works such as making human-driven rules for eliminating these non-relevant contents for all blog sites. However, such tuning is a very inefficient process. Rather than this labor-intensive method, this paper first recognizes that many of these non-relevant contents are not changed between several consequent blog posts, and then proposes a simple and effective DiffPost algorithm to eliminate them based on content difference between two consequent blog posts in the same blog site. Experimental result in TREC blog track is remarkable, showing that the retrieval system using DiffPost makes an important performance improvement of about 10% MAP (Mean Average Precision) increase over that without DiffPost.

#index 1195914
#* An Unsupervised Approach to Product Attribute Extraction
#@ Santosh Raju;Prasad Pingali;Vasudeva Varma
#t 2009
#c 16
#% 855200
#% 939896
#% 963349
#% 1275209
#! Product Attribute Extraction is the task of automatically discovering attributes of products from text descriptions. In this paper, we propose a new approach which is both unsupervised and domain independent to extract the attributes. With our approach, we are able to achieve 92% precision and 62% recall in our experiments. Our experiments with varying dataset sizes show the robustness of our algorithm. We also show that even a minimum of 5 descriptions provide enough information to identify attributes.

#index 1195915
#* Workshop on Contextual Information Access, Seeking and Retrieval Evaluation
#@ Bich-Liên Doan;Joemon M. Jose;Massimo Melucci;Lynda Tamine-Lechani
#t 2009
#c 16
#! The main purpose of this workshop is to bring together IR researchers working on or interested in the evaluation of approaches to contextual information access, seeking and retrieval, and let them to share their latest research results, to express their opinions on the related issues, and to promote discussion on the future directions of evaluation.

#index 1195916
#* Workshop on Information Retrieval over Social Networks
#@ Stephane Marchand-Maillet;Arjen P. Vries;Mor Naaman
#t 2009
#c 16
#! Popular online communities and services such as Flickr, Youtube, Facebook or LinkedIn are spearheading an emerging type of information on the Web. This information is composed of classical textual and multimedia data, in concert with additional data (tags, annotations, comments, ratings). Perhaps most significantly, the information is overlaid on an explicit social network created by the participants of each of these communities. The result is a rich structure of interrelationships between content items, participants and services. Although the size of such networks requires the use of advanced Information Retrieval techniques, classical IR models are not tailored for this type of content as they do not (in general) take advantage of the particular structure and unique aspects of this socially-driven content.This workshop proposes to report about the state-of-the- art in this direction and to gather a relevant panel of researchers working in the field. This workshop will consist of research papers that address Information Retrieval over Social Networks, including: Applications of Information Retrieval over Social Network Adapted IR models for Social Networks Mining Social Network data Privacy issues in Social Network information retrieval Trust and Reliability issues in Social Network information retrieval Knowledge and Content Discovery in Social Networks Information diffusion over Social Networks Performance evaluation for the above (measures, test collections)

#index 1195917
#* Workshop on Geographic Information on the Internet Workshop (GIIW)
#@ Gregory Grefenstette;Pierre-Alain Moëllic;Adrian Popescu;Florence Sèdes
#t 2009
#c 16
#! Finding geographically-based information constitutes a common use of Web search engines, for a variety of user needs. With the rapid growth of the volume of geographically-related information on the Web, efficient and adaptable ways of tagging, browsing and accessing relevant documents still needs to be found. Structuring and mashing-up geographic information from different Web data sources is one appealing alternative to long term efforts of manually creating large scale geographic resources such as The Alexandria Digital Library or Geonames , whose constructions are costly and not necessarily adapted to specific applications.

#index 1195918
#* Current Developments in Information Retrieval Evaluation
#@ Thomas Mandl
#t 2009
#c 16
#! In the last decade, many evaluation results have been created within the evaluation initiatives like TREC, NTCIR and CLEF. The large amount of data available has led to substantial research on the validity of the evaluation procedure. An evaluation based on the Cranfield paradigm requires basically topics as descriptions of information needs, a document collection, systems to compare, human jurors to judge the documents retrieved by the systems against the information needs descriptions and some metric to compare the systems. For all these elements, there has been a scientific discussion. How many topics, systems, jurors and juror decisions are necessary to achieve valid results? How can the validity be measured? Which metrics are the most reliable ones and which metrics are appropriate from a user perspective? Examples from current CLEF experiments are used to illustrate some of the issues. User based evaluations confront test users with the results of search systems and let them solve information tasks given in the experiment. In such a test setting, the performance of the user can be measured by observing the number of relevant documents he finds. This measure can be compared to a gold standard of relevance for the search topic to see if the perceived performance correlates with an objective notion of relevance defined by a juror. In addition, the user can be asked about his satisfaction with the search system and its results. In recent years, there has a growing concern on how well the results of batch and user studies correlate. When systems improve in a batch comparison and bring more relevant documents into the results list, do users get a benefit from this improvement? Are users more satisfied with better result lists and do better systems enable them to find more relevant documents? Some studies could not confirm this relation between system performance and user satisfaction.

#index 1195919
#* Information Extraction and Linking in a Retrieval Context
#@ Marie-Francine Moens;Djoerd Hiemstra
#t 2009
#c 16
#! We witness a growing interest and capabilities of automatic content recognition (often referred to as information extraction) in various media sources that identify entities (e.g. persons, locations and products) and their semantic attributes (e.g., opinions expressed towards persons or products, relations between entities).These extraction techniques are most advanced for text sources, but they are also researched for other media, for instance for recognizing persons and objects in images or video. The extracted information enriches and adds semantic meaning to document and queries (the latter e.g., in a relevance feedback setting). In addition, content recognition techniques trigger automated linking of information across documents and even across media. This situation poses a number of opportunities and challenges for retrieval and ranking models. For instance, instead of returning full documents, information extraction provides the means to return very focused results in the form of entities such as persons and locations. Another challenge is to integrate content recognition and content retrieval as much as possible, for instance by using the probabilistic output from the information extraction tools in the retrieval phase. These approaches are important steps towards semantic search, i.e., retrieval approaches that truly use the semantics of the data.

#index 1195920
#* Mining Query Logs
#@ Salvatore Orlando;Fabrizio Silvestri
#t 2009
#c 16
#! Web Search Engines (WSEs) have stored in their query logs information about users since they started to operate. This information often serves many purposes. The primary focus of this tutorial is to introduce to the discipline of query log mining. We will show its foundations, by giving a unified view on the literature on query log analysis, and also present in detail the basic algorithms and techniques that could be used to extract useful knowledge from this (potentially) infinite source of information. Finally, we will discuss how the extracted knowledge can be exploited to improve different quality features of a WSE system, mainly its effectiveness and efficiency.

#index 1387532
#* Proceedings of the 25th European conference on IR research
#@ Fabrizio Sebastiani
#t 2003
#c 16

#index 1387533
#* Document retrieval: shallow data, deep theories; historical reflections, potential directions
#@ Karen Spärck Jones
#t 2003
#c 16
#% 3621
#% 46803
#% 54436
#% 71749
#% 73046
#% 86371
#% 109190
#% 158687
#% 183255
#% 262096
#% 280850
#% 280851
#% 288166
#% 321635
#% 324129
#% 375017
#% 375379
#% 397127
#% 397160
#% 428369
#% 719598
#% 817581
#% 836019
#% 840583
#% 857180
#% 1305656
#! This paper reviews the development of statistically-based retrieval. Since the 1950s statistical techniques have clearly demonstrated their practical worth and statistical theories their staying power, for document or text retrieval. In the last decade the TREC programme, and the Web, have offered new retrieval challenges to which these methods have successfully risen. They are now one element in the much wider and very productive spread of statistical methods to all areas of information and language processing, in which innovative approaches to modelling their data and tasks are being applied.

#index 1387534
#* Annotation and retrieval of structured video documents
#@ Marco Bertini;Alberto Del Bimbo;Walter Nunziati
#t 2003
#c 16
#% 194202
#% 239706
#% 278431
#% 316187
#% 334751
#% 341285
#% 403487
#% 434969
#% 495225
#% 589730
#% 593658
#% 970032
#! Live Logging and Posterity Logging are the two basic applications for video databases. The former aims at providing effective annotation of video in quasi-real time and supports extraction of meaningful clips from the live stream; the latter provides annotation for later reuse of video material and is the prerequisite for retrieval by content from video digital libraries. Both require that information is adequately structured with interchange format and that annotation is performed, at a great extent, automatically. Video information structure must encompass both low-intermediate level video organization and event relationships that define specific highlights and situations. Analysis of the visual data of the video stream permits to extract hints, identify events and detect highlights. All of this must be supported by a-priori knowledge of the subject and effective reasoning engines capable to capture the inherent semantics of the visual events.

#index 1387535
#* Improving the evaluation of web search systems
#@ Cathal Gurrin;Alan F. Smeaton
#t 2003
#c 16
#% 262061
#% 268079
#% 282905
#% 283833
#% 309151
#% 309749
#% 340679
#% 397204
#% 729027
#! Linkage analysis as an aid to web search has been assumed to be of significant benefit and we know that it is being implemented by many major Search Engines. Why then have few TREC participants been able to scientifically prove the benefits of linkage analysis over the past three years? In this paper we put forward reasons why disappointing results have been found and we identify the linkage density requirements of a dataset to faithfully support experiments into linkage analysis. We also report a series of linkage-based retrieval experiments on a more densely linked dataset culled from the TREC web documents.

#index 1387536
#* When are links useful? experiments in text classification
#@ Michelle Fisher;Richard Everson
#t 2003
#c 16
#% 280819
#% 290830
#% 340928
#% 420495
#% 729027
#! Link analysis methods have become popular for information access tasks, especially information retrieval, where the link information in a document collection is used to complement the traditionally used content information. However, there has been little firm evidence to confirm the utility of link information. We show that link information can be useful when the document collection has a sufficiently high link density and links are of sufficiently high quality. We report experiments on text classification of the Cora and WebKB data sets using Probabilistic Latent Semantic Analysis and Probabilistic Hypertext Induced Topic Selection. Comparison with manually assigned classes shows that link information enhances classification in data with sufficiently high link density, but is detrimental to performance at low link densities or if the quality of the links is degraded. We introduce a new frequency-based method for selecting the most useful citations from a document collection for use in the model.

#index 1387537
#* Hierarchical classification of HTML documents with WebClassII
#@ Michelangelo Ceci;Donato Malerba
#t 2003
#c 16
#% 285
#% 67565
#% 309141
#% 344447
#% 465747
#% 465895
#% 466078
#% 487285
#% 1499570
#! This paper describes a new method for the classification of a HTML document into a hierarchy of categories. The hierarchy of categories is involved in all phases of automated document classification, namely feature extraction, learning, and classification of a new document. The innovative aspects of this work are the feature selection process, the automated threshold determination for classification scores, and an experimental study on real-word Web documents that can be associated to any node in the hierarchy. Moreover, a new measure for the evaluation of system performances has been introduced in order to compare three different techniques (flat, hierarchical with proper training sets, hierarchical with hierarchical training sets). The method has been implemented in the context of a client-server application, named WebClassII. Results show that for hierarchical techniques it is better to use hierarchical training sets.

#index 1387538
#* Hierarchical indexing and flexible element retrieval for structured document
#@ Hang Cui;Ji-Rong Wen;Tat-Seng Chua
#t 2003
#c 16
#% 41669
#% 169809
#% 169811
#% 169813
#% 197837
#% 204662
#% 232677
#% 236416
#% 262069
#% 292684
#% 337435
#% 339373
#% 340914
#% 479803
#% 1387539
#! As more and more structured documents, such as the SGML or XML documents, become available on the Web, there is a growing demand to develop effective structured document retrieval which exploits both content and hierarchical structure of documents and return document elements with appropriate granularity. Previous work on partial retrieval of structured document has limited applications due to the requirement of structured queries and restriction that the document structure cannot be traversed according to queries. In this paper, we put forward a method for flexible element retrieval which can retrieve relevant document elements with arbitrary granularity against natural language queries. The proposed techniques constitute a novel hierarchical index propagation and pruning mechanism and an algorithm of ranking document elements based on the hierarchical index. The experimental results show that our method significantly outperforms other existing methods. Our method also shows robustness to the long-standing problems of text length normalization and threshold setting in structured document retrieval.

#index 1387539
#* Construction of a test collection for the focussed retrieval of structured documents
#@ Gabriella Kazai;Mounia Lalmas;Jane Reid
#t 2003
#c 16
#% 41669
#% 118743
#% 144011
#% 167558
#% 169809
#% 169811
#% 194254
#% 262069
#% 262105
#% 268079
#% 309104
#% 340911
#% 340914
#% 346556
#% 387427
#% 458404
#% 1387540
#% 1742068
#! In this paper, we examine the methodological issues involved in constructing test collections of structured documents and obtaining best entry points for the evaluation of the focussed retrieval of document components. We describe a pilot test of the proposed test collection construction methodology performed on a document collection of Shakespeare plays. In our analysis, we examine the effect of query complexity and type on overall query difficulty, the use of multiple relevance judges for each query, the problem of obtaining exhaustive relevance assessments from participants, and the method of eliciting relevance assessments and best entry points. Our findings indicate that the methodology is indeed feasible in this small-scale context, and merits further investigation.

#index 1387540
#* User behaviour in the context of structured documents
#@ Karen Finesilver;Jane Reid
#t 2003
#c 16
#% 41669
#% 70473
#% 166954
#% 169811
#% 207830
#% 262069
#% 268079
#% 270713
#% 309104
#% 340914
#% 346556
#% 458404
#% 503227
#% 1387539
#% 1742068
#! This paper describes a small-scale experimental study examining user behaviour in the context of structured documents. Two variants of the same interface to support information seeking were implemented, one highlighting relevant objects and one highlighting best entry points (BEPs). BEPs are intended to support users' natural information seeking behaviour by providing optimal starting points for browsing to relevant objects. Analysis of the results from the comparative study of these two interfaces shows that the BEP interface was strongly preferred to the relevant object interface, and that appropriate usage of BEPs can lead to improved task performance. However, the study also highlighted shortcomings related to the inconsistent nature of BEPs and to BEP interface design.

#index 1387541
#* Attaining fast and successful searches in E-commerce environments
#@ Raz Lin;Sarit Kraus;Jeffrey Tew
#t 2003
#c 16
#% 202011
#% 250678
#% 252753
#% 268079
#% 280447
#% 283169
#% 289770
#% 314933
#% 319705
#% 1275346
#! Current online stores suffer from a cardinal problem. There are too many products to offer, and customers find themselves lost due to the vast selection. As opposed to traditional stores, there is little or no guidance that helps the customers as they search. In this paper, we propose a new approach for searching in online stores. This approach is based on algorithms commonly used in recommendation systems, but which are rarely used for searches in online stores. We employ this approach for both keyword and browse searches, and present an implementation of this approach. We compared several search guide algorithms experimentally, and the experiments' results show that the suggested algorithms are applicable to the domain of online stores.

#index 1387542
#* Learning user similarity and rating style for collaborative recommendation
#@ Lily F. Tian;Kwok-Wai Cheung
#t 2003
#c 16
#% 220709
#% 242483
#% 465906
#% 764476
#% 1650569
#% 1784711
#! Information filtering is an area getting more important as we have long been flooded with too much information. Product brokering in e-commerce is a typical example and systems which can recommend products to their users in a personalized manner have been studied rigoriously in recent years. Collaborative filtering is one of the commonly used approaches where careful choices of the user similarity measure and the rating style representation are required, and yet there is no guarantee for their optimality. In this paper, we propose the use of machine learning techniques to learn the user similarity as well as the rating style. A criterion function measuring the prediction errors is used and several problem formulations are proposed together with their learning algorithms. We have evaluated our proposed methods using the EachMovie dataset and succeeded in obtaining significant improvement in recommendation accuracy when compared with the standard correlation method.

#index 1387543
#* Spoken information extraction from Italian broadcast news
#@ Vanessa Sandrini;Marcello Federico
#t 2003
#c 16
#% 394014
#% 744539
#% 747923
#% 815341
#% 854673
#! Current research on information extraction from spoken documents is mainly focused on the recognition of named entities, such as names of organizations, locations and persons, within transcripts automatically generated by a speech recognizer. In this work we present research carried out at ITC-irst on named entity recognition in Italian broadcast news. In particular, an original statistical named entity tagger is described which can be trained with relatively little language resources: a seed list of named entities and a large untagged text corpus. Moreover, the paper discusses and presents named entity recognition experiments with case sensitive automatic transcripts, generated by the ITC-irst speech recognizer, and by training the named entity model with seed lists of different size.

#index 1387544
#* Taming wild phrases
#@ Cornelis H. A. Koster;Mark Seutter
#t 2003
#c 16
#% 86531
#% 129655
#% 275837
#% 332658
#% 355752
#% 425020
#% 458397
#% 465754
#% 524040
#% 756915
#% 1290081
#% 1783148
#! In this paper the suitability of different document representations for automatic document classification is compared, investigating a whole range of representations between bag-of-words and bag-of-phrases. We look at some of their statistical properties, and determine for each representation the optimal choice of classification parameters and the effect of Term Selection. Phrases are represented by an abstraction called Head/Modifier pairs. Rather than just throwing phrases and keywords together, we shall start with pure HM pairs and gradually add more keywords to the document representation. We use the classification on keywords as the baseline, which we compare with the contribution of the pure HM pairs to classification accuracy, and the incremental contributions from heads and modifiers. Finally, we measure the accuracy achieved with all words and all HM pairs combined, which turns out to be only marginally above the baseline. We conclude that even the most careful term selection cannot overcome the differences in Document Frequency between phrases and words, and propose the use of term clustering to make phrases more cooperative.

#index 1387545
#* Stemming and decompounding for German text retrieval
#@ Martin Braschler;Bärbel Ripplinger
#t 2003
#c 16
#% 115470
#% 144074
#% 208934
#% 218982
#% 218985
#% 218989
#% 248075
#% 248078
#% 248218
#% 286307
#% 561157
#% 561168
#% 561328
#% 561331
#% 570318
#% 741043
#! The stemming problem, i.e. finding a common stem for different forms of a term, has been extensively studied for English, but considerably less is known for other languages. Previously, it has been claimed that stemming is essential for highly declensional languages. We report on our experiments on stemming for German, where an additional issue is the handling of compounds, which are formed by concatenating several words. Rarely do studies on stemming for any language cover more than one or two different approaches. This paper makes a major contribution that transcends its focus on German by investigating a complete spectrum of approaches, ranging from language-independent to elaborate linguistic methods. The main findings are that stemming is beneficial even when using a simple approach, and that carefully designed decompounding, the splitting of compound words, remarkably boosts performance. All findings are based on a thorough analysis using a large reliable test collection.

#index 1387546
#* Question answering system for incomplete and noisy data: methods and measures for its evaluation
#@ Lili Aunimo;Oskari Heinonen;Reeta Kuuskoski;Juha Makkonen;Renaud Petit;Otso Virtanen
#t 2003
#c 16
#% 67565
#% 188076
#% 207677
#% 420519
#% 742083
#% 742406
#! We present a question answering system that can handle noisy and incomplete natural language data, and methods and measures for the evaluation of question answering systems. Our question answering system is based on the vector space model and linguistic analysis of the natural language data. In the evaluation procedure, we test eight different preprocessing schemes for the data, and come to the conclusion that lemmatization combined with breaking compound words into their constituents gives significantly better results than the baseline. The evaluation process is based on stratified random sampling and bootstrapping. To measure the correctness of an answer, we use partial credits as well as full credits.

#index 1387547
#* Term proximity scoring for keyword-based retrieval systems
#@ Yves Rasolofo;Jacques Savoy
#t 2003
#c 16
#% 56830
#% 184489
#% 259998
#% 267454
#% 296646
#% 306468
#% 306494
#% 306504
#% 323131
#% 330787
#% 340928
#% 342680
#% 406493
#% 643559
#! This paper suggests the use of proximity measurement in combination with the Okapi probabilistic model. First, using the Okapi system, our investigation was carried out in a distributed retrieval framework to calculate the same relevance score as that achieved by a single centralized index. Second, by applying a term-proximity scoring heuristic to the top documents returned by a keyword-based system, our aim is to enhance retrieval performance. Our experiments were conducted using the TREC8, TREC9 and TREC10 test collections, and show that the suggested approach is stable and generally tends to improve retrieval effectiveness especially at the top documents retrieved.

#index 1387548
#* Propositional logic representations for documents and queries: a large-scale evaluation
#@ David E. Losada;Alvaro Barreiro
#t 2003
#c 16
#% 144011
#% 144012
#% 144076
#% 169809
#% 169811
#% 248218
#% 262067
#% 280824
#% 340994
#% 384634
#% 566401
#% 572497
#% 617219
#! Expressive power is a potential source of benefits for Information Retrieval. Indeed, a number of works have been traditionally devoting their efforts to defining models able to manage structured documents. Similarly, many researchers have looked at query formulation and proposed different methods to generate structured queries. Nevertheless few attempts have addressed the combination of both expressive documents and expressive queries and its effects on retrieval performance. This is mostly due to the lack of a coherent and expressive framework in which both documents and queries can be handled in an homogeneous and efficient way. In this work we aim at filling this gap. We test the impact of logical representations for documents and queries under a large-scale evaluation. The experiments show clearly that, under the same conditions, the use of logical representations for both documents and queries leads to significant improvements in retrieval performance. Moreover, the overall performance results make evident that logic-based approaches can be competitive in the field of Information Retrieval.

#index 1387549
#* From uncertain inference to probability of relevance for advanced IR applications
#@ Henrik Nottelmann;Norbert Fuhr
#t 2003
#c 16
#% 33922
#% 109191
#% 111304
#% 118756
#% 120111
#% 169780
#% 172898
#% 176530
#% 282422
#% 340146
#% 481748
#! Uncertain inference is a probabilistic generalisation of the logical view on databases, ranking documents according to their probabilities that they logically imply the query. For tasks other than ad-hoc retrieval, estimates of the actual probability of relevance are required. In this paper, we investigate mapping functions between these two types of probability. For this purpose, we consider linear and logistic functions. The former have been proposed before, whereas we give a new theoretic justification for the latter. In a series of upper-bound experiments, we compare the goodness of fit of the two models. A second series of experiments investigates the effect on the resulting retrieval quality in the fusion step of distributed retrieval. These experiments show that good estimates of the actual probability of relevance can be achieved, and the logistic model outperforms the linear one. However, retrieval quality for distributed retrieval (only merging, without resource selection) is only slightly improved by using the logistic function.

#index 1387550
#* Topic detection and tracking with spatio-temporal evidence
#@ Juha Makkonen;Helena Ahonen-Myka;Marko Salmenkivi
#t 2003
#c 16
#% 194989
#% 309100
#% 316546
#% 329261
#% 350859
#% 445316
#% 575579
#% 577297
#% 677440
#% 854249
#% 995518
#! Topic Detection and Tracking is an event-based information organization task where online news streams are monitored in order to spot new unreported events and link documents with previously detected events. The detection has proven to perform rather poorly with traditional information retrieval approaches. We present an approach that formalizes temporal expressions and augments spatial terms with ontological information and uses this data in the detection. In addition, instead using a single term vector as a document representation, we split the terms into four semantic classes and process and weigh the classes separately. The approach is motivated by experiments.

#index 1387551
#* Clustering and visualization in a multi-lingual multi-document summarization system
#@ Hsin-Hsi Chen;June-Jei Kuo;Tsei-Chun Su
#t 2003
#c 16
#% 283171
#% 316519
#% 318409
#% 400058
#% 755815
#% 1223706
#! To measure the similarity of words, sentences, and documents is one of the major issues in multi-lingual multi-document summarization. This paper presents five strategies to compute the multilingual sentence similarity. The experimental results show that sentence alignment without considering the word position or order in a sentence obtains the best performance. Besides, two strategies are proposed for multilingual document clustering. The two-phase strategy (translation after clustering) is better than one-phase strategy (translation before clustering). Translation deferred to sentence clustering, which reduces the propagation of translation errors, is most promising. Moreover, three strategies are proposed to tackle the sentence clustering. Complete link within a cluster has the best performance, however, the subsumption-based clustering has the advantage of lower computation complexity and similar performance. Finally, two visualization models (i.e., focusing and browsing), which consider the users' language preference, are proposed.

#index 1387552
#* A hybrid relevance-feedback approach to text retrieval
#@ Zhao Xu;Xiaowei Xu;Kai Yu;Volker Tresp
#t 2003
#c 16
#% 118726
#% 260001
#% 420077
#% 458379
#% 464284
#% 466419
#% 722797
#% 837668
#% 1272282
#! Relevance feedback (RF) has been an effective query modification approach to improving the performance of information retrieval (IR) by interactively asking a user whether a set of documents are relevant or not to a given query concept. The conventional RF algorithms either converge slowly or cost a user's additional efforts in reading irrelevant documents. This paper surveys several RF algorithms and introduces a novel hybrid RF approach using a support vector machine (HRFSVM), which actively selects the uncertain documents as well as the most relevant ones on which to ask users for feedback. It can efficiently rank documents in a natural way for user browsing. We conduct experiments on Reuters-21578 dataset and track the precision as a function of feedback iterations. Experimental results have shown that HRFSVM significantly outperforms two other RF algorithms.

#index 1387553
#* Improving the evaluation of web search systems
#@ Cathal Gurrin;Alan F. Smeaton
#t 2003
#c 16
#% 194246
#% 262061
#% 268079
#% 268114
#% 273926
#% 282422
#% 282905
#% 283833
#% 309151
#% 309749
#% 340679
#% 397204
#% 413635
#% 481748
#% 567255
#% 729027
#! Linkage analysis as an aid to web search has been assumed to be of significant benefit and we know that it is being implemented by many major Search Engines. Why then have few TREC participants been able to scientifically prove the benefits of linkage analysis over the past three years? In this paper we put forward reasons why disappointing results have been found and we identify the linkage density requirements of a dataset to faithfully support experiments into linkage analysis. We also report a series of linkage-based retrieval experiments on a more densely linked dataset culled from the TREC web documents.

#index 1387554
#* Using Kullback-Leibler distance for text categorization
#@ Brigitte Bigi
#t 2003
#c 16
#% 115608
#% 169718
#% 169806
#% 260001
#% 278099
#% 280817
#% 280856
#% 311597
#% 326522
#% 344447
#% 357744
#% 397149
#% 425047
#% 458379
#% 465895
#! A system that performs text categorization aims to assign appropriate categories from a predefined classification scheme to incoming documents. These assignments might be used for varied purposes such as filtering, or retrieval. This paper introduces a new effective model for text categorization with great corpus (more or less 1 million documents). Text categorization is performed using the Kullback-Leibler distance between the probability distribution of the document to classify and the probability distribution of each category. Using the same representation of categories, experiments show a significant improvement when the above mentioned method is used. KLD method achieve substantial improvements over the tfidf performing method.

#index 1387555
#* Discretizing continuous attributes in AdaBoost for text categorization
#@ Pio Nardiello;Fabrizio Sebastiani;Alessandro Sperduti
#t 2003
#c 16
#% 46803
#% 156186
#% 194284
#% 235377
#% 302391
#% 311034
#% 316508
#% 344447
#% 465754
#% 1272280
#% 1478477
#! We focus on two recently proposed algorithms in the family of "boosting"-based learners for automated text classification, ADABOOST. MH and ADABOOST.MHKR. While the former is a realization of the well-known ADABOOST algorithm specifically aimed at multilabel text categorization, the latter is a generalization of the former based on the idea of learning a committee of classifier sub-committees. Both algorithms have been among the best performers in text categorization experiments so far. A problem in the use of both algorithms is that they require documents to be represented by binary vectors, indicating presence or absence of the terms in the document. As a consequence, these algorithms cannot take full advantage of the "weighted" representations (consisting of vectors of continuous attributes) that are customary in information retrieval tasks, and that provide a much more significant rendition of the document's content than binary representations. In this paper we address the problem of exploiting the potential of weighted representations in the context of ADABOOST-like algorithms by discretizing the continuous attributes through the application of entropy-based discretization methods. We present experimental results on the Reuters-21578 text categorization collection, showing that for both algorithms the version with discretized continuous attributes outperforms the version with traditional binary representations.

#index 1387556
#* Combining naive bayes and n-gram language models for text classification
#@ Fuchun Peng;Dale Schuurmans
#t 2003
#c 16
#% 68236
#% 234992
#% 246831
#% 246832
#% 262096
#% 273454
#% 279755
#% 280903
#% 318412
#% 344447
#% 458369
#% 466101
#% 740416
#! We augment the naive Bayes model with an n-gram language model to address two shortcomings of naive Bayes text classifiers. The chain augmented naive Bayes classifiers we propose have two advantages over standard naive Bayes classifiers. First, a chain augmented naive Bayes model relaxes some of the independence assumptions of naive Bayes--allowing a local Markov chain dependence in the observed variables--while still permitting efficient inference and learning. Second, smoothing techniques from statistical language modeling can be used to recover better estimates than the Laplace smoothing techniques usually used in naive Bayes classification. Our experimental results on three real world data sets show that we achieve substantial improvements over standard naive Bayes classification, while also achieving state of the art performance that competes with the best known methods in these cases.

#index 1387557
#* WebDocBall: a graphical visualization tool for web search results
#@ Jesús Vegas;Pablo de la Fuente;Fabio Crestani
#t 2003
#c 16
#% 96285
#% 96296
#% 115181
#% 172811
#% 173425
#% 201992
#% 214669
#% 268079
#% 387427
#% 458406
#% 588673
#! In the Web search process people often think that the hardest work is done by the search engines or by the directories which are entrusted with finding the Web pages. While this is partially true, a not less important part of the work is done by the user, who has to decide which page is relevant from the huge set of retrieved pages. In this paper we present a graphical visualisation tool aimed at helping users to determine the relevance of a Web page with respect to its structure. Such tool can help the user in the often tedious task of deciding which page is relevant enough to deserve a visit.

#index 1387558
#* Relevance feedback for content-based image retrieval: what can three mouse clicks achieve?
#@ Daniel C. Heesch;Stefan Rüger
#t 2003
#c 16
#% 261856
#% 280860
#% 287139
#% 298855
#% 328355
#% 406493
#% 443413
#% 458403
#% 479788
#% 561003
#% 592183
#% 592279
#% 617183
#% 1180245
#% 1180384
#% 1854912
#% 1855134
#% 1857498
#% 1857842
#! We introduce a novel relevance feedback method for content-based image retrieval and demonstrate its effectiveness using a subset of the Corel Gallery photograph collection and five low-level colour descriptors. Relevance information is translated into updated, analytically computed descriptor weights and a new query representation, and thus the system combines movement in both query and weight space. To assess the effectiveness of relevance feedback, we first determine the weight set that is optimal on average for a range of possible queries. The resulting multiple-descriptor retrieval model yields significant performance gains over all the single-descriptor models and provides the benchmark against which we measure the additional improvement through relevance feed-back. We model a number of scenarios of user-system interaction that differ with respect to the precise type and the extent of relevance feedback. In all scenarios, relevance feedback leads to a significant improvement of retrieval performance suggesting that feedback-induced performance gain is a robust phenomenon. Based on a comparison of the different scenarios, we identify optimal interaction models that yield high performance gains at a low operational cost for the user. To support the proposed relevant feedback technique we developed a novel presentation paradigm that allows relevance to be treated as a continuous variable.

#index 1387559
#* Query-based document skimming: a user-centred evaluation of relevance profiling
#@ David J. Harper;Ivan Koychev;Yixing Sun
#t 2003
#c 16
#% 201992
#% 208932
#% 208935
#% 232677
#% 262089
#% 262096
#% 280864
#% 378487
#! We present a user-centred, task-oriented, comparative evaluation of two query-based document skimming tools. ProfileSkim bases within-document retrieval on computing a relevance profile for a document and query; FindSkim provides similar functionality to the web browser Find-command. A novel simulated work task was devised, where experiment participants are asked to identify (index) relevant pages of an electronic book, given subjects from the existing book index. This subject index provides the ground truth, against which the indexing results can be compared. Our major hypothesis was confirmed, namely ProfileSkim proved significantly more efficient than Find-Skim, as measured by time for task. Moreover, indexing task effectiveness, measured by typical IR measures, demonstrated that ProfileSkim was better than FindSkim in identifying relevant pages, although not significantly so. The experiments confirm the potential of relevance profiling to improve query-based document skimming, which should prove highly beneficial for users trying to identify relevant information within long documents.

#index 1387560
#* Representative sampling for text classification using support vector machines
#@ Zhao Xu;Kai Yu;Volker Tresp;Xiaowei Xu;Jizhi Wang
#t 2003
#c 16
#% 116165
#% 252011
#% 420077
#% 458379
#% 464284
#% 466263
#% 466419
#% 565531
#% 722797
#% 837668
#! In order to reduce human efforts, there has been increasing interest in applying active learning for training text classifiers. This paper describes a straightforward active learning heuristic, representative sampling, which explores the clustering structure of 'uncertain' documents and identifies the representative samples to query the user opinions, for the purpose of speeding up the convergence of Support Vector Machine (SVM) classifiers. Compared with other active learning algorithms, the proposed representative sampling explicitly addresses the problem of selecting more than one unlabeled documents. In an empirical study we compared representative sampling both with random sampling and with SVM active learning. The results demonstrated that representative sampling offers excellent learning performance with fewer labeled documents and thus can reduce human efforts in text classification tasks.

#index 1387561
#* Chinese text categorization based on the binary weighting model with non-binary smoothing
#@ Xue Dejun;Sun Maosong
#t 2003
#c 16
#% 165110
#% 169718
#% 190581
#% 232653
#% 311034
#% 344447
#% 406493
#% 458369
#% 458379
#% 458397
#% 465754
#% 465895
#% 532068
#% 815308
#% 1499573
#! In Text Categorization (TC) based on the vector space model, feature weighting is vital for the categorization effectiveness. Various non-binary weighting schemes are widely used for this purpose. By emphasizing the category discrimination capability of features, the paper firstly puts forward a new weighting scheme TF*IDF*IG. Upon the fact that refined statistics may have more chance to meet sparse data problem, we re-evaluate the role of the Binary Weighting Model (BWM) in TC for further consideration. As a consequence, a novel approach named the Binary Weighting Model with Non-Binary Smoothing (BWM-NBS) is then proposed so as to overcome the drawback of BWM. A TC system for Chinese texts using words as features is implemented. Experiments on a large-scale Chinese document collection with 71,674 texts show that the F1 metric of categorization performance of BWM-NBS gets to 94.9% in the best case, which is 26.4% higher than that of TF*IDF, 19.1% higher than that of TF*IDF*IG, and 5.8% higher than that of BWM under the same condition. Moreover, BWM-NBS exhibits the strong stability in categorization performance.

#index 1387562
#* A study on optimal parameter tuning for Rocchio text classifier
#@ Alessandro Moschitti
#t 2003
#c 16
#% 46803
#% 144009
#% 194301
#% 232646
#% 243728
#% 262050
#% 262085
#% 275837
#% 287284
#% 318412
#% 344447
#% 458379
#% 465754
#% 465895
#% 648826
#! Current trend in operational text categorization is the designing of fast classification tools. Several studies on improving accuracy of fast but less accurate classifiers have been recently carried out. In particular, enhanced versions of the Rocchio text classifier, characterized by high performance, have been proposed. However, even in these extended formulations the problem of tuning its parameters is still neglected. In this paper, a study on parameters of the Rocchio text classifier has been carried out to achieve its maximal accuracy. The result is a model for the automatic selection of parameters. Its main feature is to bind the searching space so that optimal parameters can be selected quickly. The space has been bound by giving a feature selection interpretation of the Rocchio parameters. The benefit of the approach has been assessed via extensive cross evaluation over three corpora in two languages. Comparative analysis shows that the performances achieved are relatively close to the best TC models (e.g. Support Vector Machines).

#index 1387563
#* Optimization of restricted searches in web directories using hybrid data structures
#@ Fidel Cacheda;Victor Carneiro;Carmen Guerrero;Angel Viña
#t 2003
#c 16
#% 23321
#% 86532
#% 115465
#% 211566
#% 249989
#% 259985
#% 268079
#% 287214
#% 342876
#% 415986
#% 443316
#% 487134
#% 659288
#! The need of efficient tools in order to manage, retrieve and filter the information in the WWW is clear. Web directories are taxonomies for the classification of Web documents. These kind of information retrieval systems present a specific type of search where the document collection is restricted to one area of the category graph. This paper introduces a specific data architecture for Web directories that improves the performance of restricted searches. That architecture is based on a hybrid data structure composed of an inverted file with multiple embedded signature files. Two variants are presented: hybrid architecture with total information and with partial information. This architecture has been analyzed by means of developing both variants to be compared with a basic model. The performance of the restricted queries was clearly improved, especially the hybrid model with partial information, which yielded a positive response under any load of the search system.

#index 1387564
#* Similarity join in metric spaces
#@ Vlastislav Dohnal;Claudio Gennaro;Pasquale Savino;Pavel Zezula
#t 2003
#c 16
#% 131061
#% 294634
#% 333679
#% 341773
#% 342827
#% 397373
#% 479462
#% 480496
#% 480654
#% 655302
#! Similarity join in distance spaces constrained by the metric postulates is the necessary complement of more famous similarity range and the nearest neighbors search primitives. However, the quadratic computational complexity of similarity joins prevents from applications on large data collections. We first study the underlying principles of such joins and suggest three categories of implementation strategies based on filtering, partitioning, or similarity range searching. Then we study an application of the D-index to implement the most promising alternative of range searching. Though also this approach is not able to eliminate the intrinsic quadratic complexity of similarity joins, significant performance improvements are confirmed by experiments.

#index 1387565
#* An efficient compression code for text databases
#@ Nieves R. Brisaboa;Eva L. Iglesias;Gonzalo Navarro;José R. Paramá
#t 2003
#c 16
#% 57849
#% 68236
#% 311799
#% 387427
#% 401434
#% 420492
#% 430485
#% 438325
#% 587844
#! We present a new compression format for natural language texts, allowing both exact and approximate search without decompression. This new code -called End-Tagged Dense Code- has some advantages with respect to other compression techniques with similar features such as the Tagged Huffman Code of [Moura et al., ACM TOIS 2000]. Our compression method obtains (i) better compression ratios, (ii) a simpler vocabulary representation, and (iii) a simpler and faster encoding. At the same time, it retains the most interesting features of the method based on the Tagged Huffman Code, i.e., exact search for words and phrases directly on the compressed text using any known sequential pattern matching algorithm, efficient word-based approximate and extended searches without any decoding, and efficient decompression of arbitrary portions of the text. As a side effect, our analytical results give new upper and lower bounds for the redundancy of d-ary Huffman codes.

#index 1387566
#* Compressing semistructured text databases
#@ Joaquín Adiego;Gonzalo Navarro;Pablo de la Fuente
#t 2003
#c 16
#% 3244
#% 57849
#% 184486
#% 300153
#% 311799
#% 375076
#% 420492
#% 438325
#% 656375
#! We describe a compression model for semistructured documents, called Structural Contexts Model, which takes advantage of the context information usually implicit in the structure of the text. The idea is to use a separate semiadaptive model to compress the text that lies inside each different structure type (e.g., different XML tag). The intuition behind the idea is that the distribution of all the texts that belong to a given structure type should be similar, and different from that of other structure types. We test our idea using a word-based Huffman coding, which is the standard for compressing large natural language textual databases, and showt hat our compression method obtains significant improvements in compression ratios. We also analyze the possibility that storing separate models may not pay off if the distribution of different structure types is not different enough, and present a heuristic to merge models with the aim of minimizing the total size of the compressed database. This technique gives an additional improvement over the plain technique. The comparison against existing prototypes shows that our method is a competitive choice for compressed text databases.

#index 1387567
#* Vertical searching in juridical digital libraries
#@ Maria de Lourdes da Silveira;Berthier Ribeiro-Neto;Rodrigo de Freitas Vale;Rodrigo Tôrres Assumpção
#t 2003
#c 16
#% 44876
#% 144029
#% 157884
#% 219047
#% 309104
#% 309254
#% 327251
#% 330263
#% 406493
#! In the world of modern digital libraries, the searching for juridical information of interest is a current and relevant problem. We approach this problem from the perspective that a new searching mechanism, specialized in the juridical area, will work better than standard solutions. We propose a specialized (or vertical) searching mechanism that combines information from a juridical thesaurus with information generated by a standard searching mechanism (the classic vector space model), using the framework of a Bayesian belief network. Our vertical searching mechanism is evaluated using a reference collection of 552,573 documents. The results show improvements in retrieval performance, suggesting that the study and development of vertical searching mechanisms is a promising research direction.

#index 1387568
#* Corpus-based thesaurus construction for image retrieval in specialist domains
#@ Khurshid Ahmad;Mariam Tariq;Bogdan Vrusias;Chris Handy
#t 2003
#c 16
#% 248070
#% 328355
#% 363038
#% 756964
#% 1783207
#! This paper explores the use of texts that are related to an image collection, also known as collateral texts, for building thesauri in specialist domains to aid in image retrieval. Corpus linguistic and information extraction methods are used for identifying key terms and semantic relationships in specialist texts that may be used for query expansion purposes. The specialist domain context imposes certain constraints on the language used in the texts, which makes the texts computationally more tractable.

#index 1387569
#* Generating extracts with genetic algorithms
#@ Enrique Alfonseca;Pilar Rodríguez
#t 2003
#c 16
#% 114994
#% 207677
#% 266370
#% 288614
#% 387791
#% 742437
#! This paper describes an application of genetic algorithms for text summarisation. We have built a sentence extraction algorithm that overcomes some of the drawbacks of traditional sentence extractors, and takes into consideration different features of the summaries. The fitness function can be easily modified in order to incorporate features such as user modelling and adaptation. The system has been evaluated with standard procedures, and the obtained results are very good.

#index 1387570
#* The ITC-irst news on demand platform
#@ Nicola Bertoldi;Fabio Brugnara;Mauro Cettolo;Marcello Federico;Diego Giuliani;Erwin Leeuwis;Vanessa Sandrini
#t 2003
#c 16
#% 239594
#% 293977
#% 293978
#% 293979
#% 293981
#% 357744
#% 397144
#% 561165
#% 854673
#! The rapid growth of the Information Society is increasing the demand for technologies enabling access of multimedia data by content. An interesting example is given by the broadcasting companies and the content providers, which require today effective technologies to support the management and access of their huge audiovisual archives, both for internal use and public services. The Multilingual Natural Speech Technology lab at ITC-irst has been engaged for several years in developing core technologies suited for audio indexing and retrieval. In this paper, a news-on-demand experimental platform is presented which integrates several technologies in the area of spoken language processing and information retrieval. In particular, an overview of the whole system architecture will be given, together with a short description of the achieved state-of-the-art in each single technology.

#index 1387571
#* Evaluating peer-to-peer networking for information retrieval within the context of meta-searching
#@ Iraklis A. Klampanos;James J. Barnes;Joemon M. Jose
#t 2003
#c 16
#% 344448
#% 562344
#! Peer-to-peer (P2P) computing has shown an unexpected growth and development during the recent years. P2P networking is being applied from B2B enterprise solutions to more simple, everyday file-sharing applications like Gnutella clients. In this paper we are investigating the use of the Gnutella P2P protocol for Information Retrieval by means of building and evaluating a general-purpose Web meta-search engine. A similar project, Infrasearch, existed in the past only to demonstrate the usefulness of P2P computing beyond just file-sharing. In this work, a Java-based Gnutella enabled meta-search engine has been developed and used in order to evaluate the suitability and usefulness of P2P networking in the aforementioned context. Our conclusions concerning time efficiency in different networking topologies are presented. Finally, limitations of this approach as well as future research areas and issues on the subject are discussed.

#index 1387572
#* Parallel computing for term selection in routing/filtering
#@ Andy MacFarlane;Stephen E. Robertson;Julie A. McCann
#t 2003
#c 16
#% 172486
#% 383492
#! It has been postulated that a method of selecting terms in either routing or filtering using relevance feedback would be to evaluate every possible combination of terms in a training set and determine which combination yields the best retrieval results. Whilst this is not a realistic proposition because of the enormous size of the search space, some heuristics have been developed on the Okapi system to tackle the problem which are computationally intensive. This paper describes parallel computing techniques that have been applied to these heuristics to reduce the time it takes to select to select terms.

#index 1387573
#* A weighting scheme for star-graphs
#@ Jean Martinet;Iadh Ounis;Yves Chiaramella;Philippe Mulhem
#t 2003
#c 16
#% 46803
#% 247116
#% 262095
#% 406493
#% 660404
#% 1271970
#! A star-graph is a conceptual graph that contains a single relation, with some concepts linked to it. They are elementary pieces of information describing combinations of concepts. We use star-graphs as descriptors - or index terms - for image content representation. This allows for relational indexing and expression of complex user needs, in comparison to classical text retrieval, where simple keywords are generally used as document descriptors. In classical text retrieval, the keywords are weighted to give emphasis to good document descriptors and discriminators where the most popular weighting schemes are based on variations of tf.idf. In this paper, we present an extension of tf.idf, introducing a new weighting scheme suited for star-graphs. This weighting scheme is based on a local analysis of star-graphs indexing a document and a global analysis of star-graphs across the whole collection. We show and discuss some preliminary results evaluating the performance of this weighting scheme applied to image retrieval.

#index 1387574
#* Phrase-based hierarchical clustering of web search results
#@ Irmina Masłowska
#t 2003
#c 16
#% 18713
#% 46809
#% 55490
#% 218992
#% 262045
#% 304321
#% 375017
#% 710338
#! The paper addresses the problem of clustering text documents coming from the Web. We apply clustering to support users in interactive browsing through hierarchically organized search results as opposed to the standard ranked-list presentation. We propose a clustering method that is tailored to on-line processing of Web documents and takes into account the time aspect, the particular requirements of clustering texts, and readability of the produced hierarchy. Finally, we present the user interface of an actual system in which the method is applied to the results of a popular search engine.

#index 1387575
#* Aggregated feature retrieval for MPEG-7
#@ Jiamin Ye;Alan F. Smeaton
#t 2003
#c 16
#% 273425
#% 397166
#% 420506
#% 447811
#% 465018
#! In this paper we present an initial study on the use of both high and low level MPEG-7 descriptions for video retrieval. A brief survey of current XML indexing techniques shows that an IR-based retrieval method provides a better foundation for retrieval as it satisfies important retrieval criteria such as content ranking and approximate matching. An aggregation technique for XML document retrieval is adapted to an MPEG-7 indexing structure by assigning semantic meanings to various audio/visual features and this is presented here.

#index 1387576
#* Document retrieval in the context of question answering
#@ Christof Monz
#t 2003
#c 16
#% 169809
#% 208934
#% 218978
#% 218985
#% 328532
#% 854793
#! Current question answering systems rely on document retrieval as a means of providing documents which are likely to contain an answer to a user's question. A question answering system heavily depends on the effectiveness of a retrieval system: If a retrieval system fails to find any relevant documents for a question, further processing steps to extract an answer will inevitably fail, too. In this paper, we compare the effectiveness of some common retrieval techniques with respect to their usefulness for question answering.

#index 1387577
#* A study of the usefulness of institutions' acronyms as web queries
#@ Sándor Dominich;Júlia Góth;Adrienn Skrop
#t 2003
#c 16
#% 280041
#% 286304
#% 301261
#! Many people in Hungary use the Web to obtain information from public institutions and organizations. Because these users typically do not know the URL of the desired institution's home page, they use a Web search engine to get there. Institutions' names are usually difficult to recall exactly, thus they are not being used as queries in search engines. Instead, the acronyms of institutions are being used: they are easy to remember and are extensively used in media and by people in everyday life. The paper is concerned with studying the usefulness of the acronyms of Hungarian institutions and organisations present on the Web. The study shows that the majority of acronyms lack this ability. Causes are presented, and possible remedies are suggested. Because the method used in the paper is language independent, it can be used to carry out a similar study in another country too.

#index 1387578
#* Building a hierarchy of events and topics for newspaper digital libraries
#@ Aurora Pons-Porrata;Rafael Berlanga-Llavori;José Ruiz-Shulcloper
#t 2003
#c 16
#% 262042
#% 280404
#% 309096
#% 483058
#% 710374
#! In this paper we propose an incremental hierarchical clustering algorithm for on-line event detection. This algorithm is applied to a set of newspaper articles in order to discover the structure of topics and events that they describe. In the first level, articles with a high temporal-semantic similarity are clustered together into events. In the next levels of the hierarchy, these events are successively clustered so that composite events and topics can be discovered. The results obtained for the F1-measure and the Detection Cost demonstrate the validity of our algorithm for on-line event detection tasks.

#index 1387579
#* A machine learning approach for the curation of biomedical literature
#@ Min Shi;David S. Edwin;Rakesh Menon;Lixiang Shen;Jonathan Y. K. Lim;Han Tong Loh;S. Sathiya Keerthi;Chong Jin Ong
#t 2003
#c 16
#% 191914
#% 471758
#% 1378224
#! In the field of the biomedical sciences there exists a vast repository of information located within large quantities of research papers. Very often, researchers need to spend considerable amounts of time reading through entire papers before being able to determine whether or not they should be curated (archived). In this paper, we present an automated text classification system for the classification of biomedical papers. This classification is based on whether there is experimental evidence for the expression of molecular gene products for specified genes within a given paper. The system performs pre-processing and data cleaning, followed by feature extraction from the raw text. It subsequently classifies the paper using the extracted features with a Naïve Bayes Classifier. Our approach has made it possible to classify (and curate) biomedical papers automatically, thus potentially saving considerable time and resources. The system proved to be highly accurate, and won honourable mention in the KDD Cup 2002 task 1.

#index 1387580
#* Automatic construction of theme melody index from music database for fast content-based retrievals
#@ Chang-Hwan Shin;Kyong-I Ku;KiChang Kim;Yoo-Sung Kim
#t 2003
#c 16
#% 194192
#% 204646
#% 479462
#% 631989
#% 641568
#! In traditional content-based music information retrieval systems, users may face with longer response time, since the traditional systems mostly do syntactic processing to match query melody and whole melodies of the underlying music database. Hence, there has been a growing need for theme melody index that can support to quick retrieve the relevant music to user's query melody. In this paper, we suggested an automatic mechanism for constructing the theme melody index from large music database and also showed how the theme melody index can be used for content-based music retrievals by implementing a prototype system.

#index 1387581
#* A personalized information search process based on dialoguing agents and user profiling
#@ Giovanni Semeraro;Marco Degemmis;Pasquale Lops;Ulrich Thiel;Marcello L'Abbate
#t 2003
#c 16
#% 241264
#% 290482
#% 406493
#% 423995
#% 438371
#% 465922
#! The amount of information available on the web, as well as the number of e-businesses and web shoppers, is growing exponentially. Customers have to spend a lot of time to browse the net in order to find relevant information. One way to overcome this problem is to use dialoguing agents that exploit user profiles to generate personal recommendations. This paper presents a system, designed according to this approach, that adopts a query refinement mechanism to improve the search process of an Internet commerce web site.

#index 1392427
#* Proceedings of the 29th European conference on IR research
#@ Giambattista Amati;Claudio Carpineto;Giovanni Romano
#t 2007
#c 16

#index 1392428
#* The next generation web search and the demise of the classic IR model
#@ Andrei Broder
#t 2007
#c 16
#! The classic IR model assumes a human engaged in activity that generates an "information need". This need is verbalized and then expressed as a query to search engine over a defined corpus. In the past decade, Web search engines have evolved from a first generation based on classic IR algorithms scaled to web size and thus supporting only informational queries, to a second generation supporting navigational queries using web specific information (primarily link analysis), to a third generation enabling transactional and other "semantic" queries based on a variety of technologies aimed to directly satisfy the unexpressed "user intent", thus moving further and further away from the classic model. What is coming next? In this talk, we identify two trends, both representing "short-circuits" of the model: The first is the trend towards context driven Information Supply (IS), that is, the goal of Web IR will widen to include the supply of relevant information from multiple sources without requiring the user to make an explicit query. The information supply concept greatly precedes information retrieval; what is new in the web framework, is the ability to supply relevant information specific to a given activity and a given user, while the activity is being performed. Thus the entire verbalization and query-formation phase are eliminated. The second trend is "social search" driven by the fact that the Web has evolved to being simultaneously a huge repository of knowledge and a vast social environment. As such, it is often more effective to ask the members of a given web milieu rather than construct elaborate queries. This short-circuits only the query formulation, but allows information finding activities such as opinion elicitation and discovery of social norms, that are not expressible at all as queries against a fixed corpus.

#index 1392429
#* The last half-century: a perspective on experimentation in information retrieval
#@ Stephen Robertson
#t 2007
#c 16
#! The experimental evaluation of information retrieval systems has a venerable history. Long before the current notion of a search engine, in fact before search by computer was even feasible, people in the library and information science community were beginning to tackle the evaluation issue. Sometimes it feels as though evaluation methodology has become fixed (stable or frozen, according to your viewpoint). However, this is far from the case. Interest in methodological questions is as great now as it ever was, and new ideas are continuing to develop. This talk will be a personal take on the field.

#index 1392430
#* Learning in hyperlinked environments
#@ Marco Gori
#t 2007
#c 16
#! A remarkable number of important problems in different domains (e.g. web mining, pattern recognition, biology...) are naturally modeled by functions defined on graphical domains, rather than on traditional vector spaces. Following the recent developments in statistical relational learning, in this talk, I introduce Diffusion Learning Machines (DLM) whose computation is very much related to Web ranking schemes based on link analysis. Using arguments from function approximation theory, I argue that, as a matter of fact, DLM can compute any conceivable ranking function on the Web. The learning is based on a human supervision scheme that takes into account both the content and the links of the pages. I give very promising experimental results on artificial tasks and on the learning of functions used in link analysys, like PageRank, HITS, and TrustRank. Interestingly, the proposed learning mechanism is proven to be effective also when the rank depends jointly on the page content and on the links. Finally, I argue that the propagation of the relationships expressed by the links reduces dramatically the sample complexity with respect to traditional learning machines operating on vector spaces, thus making it reasonable the application to real-world problems on the Web, like spam detection and page classification.

#index 1392431
#* A parameterised search system
#@ Roberto Cornacchia;Arjen P. De Vries
#t 2007
#c 16
#% 163444
#% 214702
#% 318390
#% 345199
#% 451549
#% 768898
#% 845359
#% 875010
#% 1348341
#! This paper introduces the concept of a Parameterised Search System (PSS), which allows flexibility in user queries, and, more importantly, allows system engineers to easily define customised search strategies. Putting this idea into practise requires a carefully designed system architecture that supports a declarative abstraction language for the specification of search strategies. These specifications should stay as close as possible to the problem definition (i.e., the retrieval model to be used in the search application), abstracting away the details of the physical organisation of data and content. We show how extending an existing XML retrieval system with an abstraction mechanism based on array databases meets this requirement.

#index 1392432
#* Similarity measures for short segments of text
#@ Donald Metzler;Susan Dumais;Christopher Meek
#t 2007
#c 16
#% 144034
#% 280851
#% 340901
#% 340948
#% 342707
#% 838508
#% 869500
#% 869501
#% 939939
#! Measuring the similarity between documents and queries has been extensively studied in information retrieval. However, there are a growing number of tasks that require computing the similarity between two very short segments of text. These tasks include query reformulation, sponsored search, and image retrieval. Standard text similarity measures perform poorly on such tasks because of data sparseness and the lack of context. In this work, we study this problem from an information retrieval perspective, focusing on text representations and similarity measures. We examine a range of similarity measures, including purely lexical measures, stemming, and language modeling-based measures. We formally evaluate and analyze the methods on a query-query similarity task using 363,822 queries from a web search log. Our analysis provides insights into the strengths and weaknesses of each method, including important tradeoffs between effectiveness and efficiency.

#index 1392433
#* Multinomial randomness models for retrieval with document fields
#@ Vassilis Plachouras;Iadh Ounis
#t 2007
#c 16
#% 132779
#% 397128
#% 411760
#% 766489
#% 766498
#% 783474
#% 879678
#% 1674994
#! Document fields, such as the title or the headings of a document, offer a way to consider the structure of documents for retrieval. Most of the proposed approaches in the literature employ either a linear combination of scores assigned to different fields, or a linear combination of frequencies in the term frequency normalisation component. In the context of the Divergence From Randomness framework, we have a sound opportunity to integrate document fields in the probabilistic randomness model. This paper introduces novel probabilistic models for incorporating fields in the retrieval process using a multinomial randomness model and its information theoretic approximation. The evaluation results from experiments conducted with a standard TREC Web test collection show that the proposed models perform as well as a state-of-the-art field-based weighting model, while at the same time, they are theoretically founded and more extensible than current field-based models.

#index 1392434
#* On score distributions and relevance
#@ Stephen Robertson
#t 2007
#c 16
#% 280854
#% 340934
#% 340938
#% 453327
#! We discuss the idea of modelling the statistical distributions of scores of documents, classified as relevant or non-relevant. Various specific combinations of standard statistical distributions have been used for this purpose. Some theoretical considerations indicate problems with some of the choices of pairs of distributions. Specifically, we revisit a generalisation of the well-known inverse relationship between recall and precision: some choices of pairs of distributions violate this generalised relationship. We identify the choices and the violations, and explore some of the consequences of this theoretical view.

#index 1392435
#* Modeling term associations for ad-hoc retrieval performance within language modeling framework
#@ Xing Wei;W. Bruce Croft
#t 2007
#c 16
#% 29585
#% 73028
#% 144029
#% 218978
#% 242411
#% 262096
#% 280819
#% 280851
#% 340895
#% 340901
#% 340948
#% 648346
#% 722904
#% 766430
#% 818240
#% 838530
#% 879579
#% 879587
#% 907578
#% 1077150
#! Previous research has shown that using term associations could improve the effectiveness of information retrieval (IR) systems. However, most of the existing approaches focus on query reformulation. Document reformulation has just begun to be studied recently. In this paper, we study how to utilize term association measures to do document modeling, and what types of measures are effective in document language models. We propose a probabilistic term association measure, compare it to some traditional methods, such as the similarity co-efficient and window-based methods, in the language modeling (LM) framework, and show that significant improvements over query likelihood (QL) retrieval can be obtained. We also compare the method with state-of-the-art document modeling techniques based on latent mixture models.

#index 1392436
#* Static pruning of terms in inverted files
#@ Roi Blanco;Álvaro Barreiro
#t 2007
#c 16
#% 198335
#% 290703
#% 336784
#% 340887
#% 393450
#% 397150
#% 805862
#% 818247
#! This paper addresses the problem of identifying collection dependent stop-words in order to reduce the size of inverted files. We present four methods to automatically recognise stop-words, analyse the tradeoff between efficiency and effectiveness, and compare them with a previous pruning approach. The experiments allow us to conclude that in some situations stop-words pruning is competitive with respect to other inverted file reduction techniques.

#index 1392437
#* Efficient indexing of versioned document sequences
#@ Michael Herscovici;Ronny Lempel;Sivan Yogev
#t 2007
#c 16
#% 118741
#% 198335
#% 241155
#% 255137
#% 330706
#% 340141
#% 387427
#% 387508
#% 408396
#% 655485
#% 869542
#% 1688264
#! Many information systems keep multiple versions of documents. Examples include content management systems, version control systems (e.g. ClearCase, CVS), Wikis, and backup and archiving solutions. Often, it is desired to enable free-text search over such repositories, i.e. to enable submitting queries that may match any version of any document. We propose an indexing method that takes advantage of the inherent redundancy present in versioned documents by solving a variant of the multiple sequence alignment problem. The scheme produces an index that is much more compact than a standard index that treats each version independently. In experiments over publicly available versioned data, our method achieved compaction ratios of 81% as compared with standard indexing, while supporting the same retrieval capabilities.

#index 1392438
#* Light syntactically-based index pruning for information retrieval
#@ Christina Lioma;Iadh Ounis
#t 2007
#c 16
#% 169781
#% 198294
#% 290703
#% 340887
#% 340916
#% 375017
#% 983589
#% 1306081
#! Most index pruning techniques eliminate terms from an index on the basis of the contribution of those terms to the content of the documents. We present a novel syntactically-based index pruning technique, which uses exclusively shallow syntactic evidence to decide upon which terms to prune. This type of evidence is document-independent, and is based on the assumption that, in a general collection of documents, there exists an approximately proportional relation between the frequency and content of 'blocks of parts of speech' (POS blocks) [5]. POS blocks are fixed-length sequences of nouns, verbs, and other parts of speech, extracted from a corpus. We remove from the index, terms that correspond to low-frequency POS blocks, using two different strategies: (i) considering that low-frequency POS blocks correspond to sequences of content-poor words, and (ii) considering that low-frequency POS blocks, which also contain 'non content-bearing parts of speech', such as prepositions for example, correspond to sequences of contentpoor words. We experiment with two TREC test collections and two statistically different weighting models. Using full indices as our baseline, we show that syntactically-based index pruning overall enhances retrieval performance, in terms of both average and early precision, for light pruning levels, while also reducing the size of the index. Our novel low-cost technique performs at least similarly to other related work, even though it does not consider document-specific information, and as such it is more general.

#index 1392439
#* Sorting out the document identifier assignment problem
#@ Fabrizio Silvestri
#t 2007
#c 16
#% 230434
#% 394709
#% 397151
#% 453323
#% 570319
#% 648114
#% 656242
#% 656274
#% 754117
#% 766445
#% 786632
#% 818230
#% 818284
#% 1306081
#% 1715618
#! The compression of Inverted File indexes in Web Search Engines has received a lot of attention in these last years. Compressing the index not only reduces space occupancy but also improves the overall retrieval performance since it allows a better exploitation of the memory hierarchy. In this paper we are going to empirically show that in the case of collections of Web Documents we can enhance the performance of compression algorithms by simply assigning identifiers to documents according to the lexicographical ordering of the URLs. We will validate this assumption by comparing several assignment techniques and several compression algorithms on a quite large document collection composed by about six million documents. The results are very encouraging since we can improve the compression ratio up to 40% using an algorithm that takes about ninety seconds to finish using only 100 MB of main memory.

#index 1392440
#* Efficient construction of FM-index using overlapping block processing for large scale texts
#@ Di Zhang;Yunquan Zhang;Jing Chen
#t 2007
#c 16
#% 143306
#% 325324
#% 339936
#% 593970
#% 723896
#% 1702450
#! In previous implementations of FM-index, the construction algorithms usually need several times larger memory than text size. Sometimes the memory requirement prevents the FM-index from being employed in processing large scale texts. In this paper, we design an approach to constructing FM-index based on overlapping block processing. It can build the FM-index in linear time and constant temporary memory space, especially suitable for large scale texts. Instead of loading and indexing text as a whole, the new approach splits the text into blocks of fixed size, and then indexes them respectively. To assure the correctness and effectiveness of query operation, before indexing, we further append certain length of succeeding characters to the end of each block. The experimental results show that, with a slight loss on the compression ratio and query performance, our implementation provides a faster and more flexible solution for the problem of construction efficiency.

#index 1392441
#* Performance comparison of clustered and replicated information retrieval systems
#@ Fidel Cacheda;Victor Carneiro;Vassilis Plachouras;Iadh Ounis
#t 2007
#c 16
#% 109210
#% 148659
#% 183334
#% 219013
#% 249153
#% 267454
#% 309139
#% 339621
#% 397186
#% 438557
#% 504881
#% 578337
#% 659288
#% 766447
#% 816374
#% 879608
#% 976948
#% 1715631
#! The amount of information available over the Internet is increasing daily as well as the importance and magnitude of Web search engines. Systems based on a single centralised index present several problems (such as lack of scalability), which lead to the use of distributed information retrieval systems to effectively search for and locate the required information. A distributed retrieval system can be clustered and/or replicated. In this paper, using simulations, we present a detailed performance analysis, both in terms of throughput and response time, of a clustered system compared to a replicated system. In addition, we consider the effect of changes in the query topics over time. We show that the performance obtained for a clustered system does not improve the performance obtained by the best replicated system. Indeed, the main advantage of a clustered system is the reduction of network traffic. However, the use of a switched network eliminates the bottleneck in the network, markedly improving the performance of the replicated systems. Moreover, we illustrate the negative performance effect of the changes over time in the query topics when a distributed clustered system is used. On the contrary, the performance of a distributed replicated system is query independent.

#index 1392442
#* A study of a weighting scheme for information retrieval in hierarchical peer-to-peer networks
#@ Massimo Melucci;Alberto Poggiani
#t 2007
#c 16
#% 46803
#% 218982
#% 316534
#% 349973
#% 590523
#% 643013
#% 672628
#% 722312
#% 729027
#% 730035
#% 766469
#% 808553
#% 836066
#% 963496
#% 1348354
#% 1715595
#% 1715596
#% 1742090
#! The experimental results show that the proposed simple weighting scheme helps retrieve a significant proportion of relevant data after traversing only a small portion of a peer-to-peer hierarchical peer network in a depth-first manner. A real, large, highly heterogeneous test collection searched by very short, ambiguous queries was used for supporting the results. The efficiency and the effectiveness would suggest the implementation, for instance, in audio-video information retrieval systems, digital libraries or personal archives.

#index 1392443
#* A decision-theoretic model for decentralised query routing in hierarchical peer-to-peer networks
#@ Henrik Nottelmann;Norbert Fuhr
#t 2007
#c 16
#% 194246
#% 280853
#% 282422
#% 340175
#% 413594
#% 481748
#% 643011
#% 722312
#% 730035
#% 1742090
#! Efficient and effective routing of content-based queries is an emerging problem in peer-to-peer networks, and can be seen as an extension of the traditional "resource selection" problem. The decision-theoretic framework for resource selection aims, in contrast to other approaches, at minimising overall costs including e.g. monetary costs, time and retrieval quality. A variant of this framework has been successfully applied to hierarchical peer-to-peer networks (where peers are partitioned into DL peers and hubs), but that approach considers retrieval quality only. This paper proposes a new model which is capable of considering also the time costs of hubs (i.e., the number of hops in subsequent steps). The evaluation on a large test-bed shows that this approach dramatically reduces the overall retrieval costs.

#index 1392444
#* Central-rank-based collection selection in uncooperative distributed information retrieval
#@ Milad Shokouhi
#t 2007
#c 16
#% 194246
#% 227891
#% 280856
#% 287463
#% 301225
#% 340146
#% 340934
#% 413594
#% 480479
#% 567255
#% 643011
#% 643012
#% 722311
#% 722312
#% 764562
#% 783473
#% 818211
#% 818221
#% 852010
#% 879604
#% 1682446
#% 1684716
#! Collection selection is one of the key problems in distributed information retrieval. Due to resource constraints it is not usually feasible to search all collections in response to a query. Therefore, the central component (broker) selects a limited number of collections to be searched for the submitted queries. During the past decade, several collection selection algorithms have been introduced. However, their performance varies on different testbeds. We propose a new collection-selection method based on the ranking of downloaded sample documents. We test our method on six testbeds and show that our technique can significantly outperform other state-of-the-art algorithms in most cases. We also introduce a new testbed based on the TREC GOV2 documents.

#index 1392445
#* Results merging algorithm using multiple regression models
#@ George Paltoglou;Michail Salampasis;Maria Satratzemi
#t 2007
#c 16
#% 194246
#% 227891
#% 263704
#% 309133
#% 309253
#% 340146
#% 340948
#% 643012
#% 722312
#% 852010
#! This paper describes a new algorithm for merging the results of remote collections in a distributed information retrieval environment. The algorithm makes use only of the ranks of the returned documents, thus making it very efficient in environments where the remote collections provide the minimum of cooperation. Assuming that the correlation between the ranks and the relevancy scores can be expressed through a logistic function and using sampled documents from the remote collections the algorithm assigns local scores to the returned ranked documents. Subsequently, using a centralized sample collection and through linear regression, it assigns global scores, thus producing a final merged document list for the user. The algorithm's effectiveness is measured against two state-of-the-art results merging algorithms and its performance is found to be superior to them in environments where the remote collections do not provide relevancy scores.

#index 1392446
#* Segmentation of search engine results for effective data-fusion
#@ Milad Shokouhi
#t 2007
#c 16
#% 194275
#% 230432
#% 232703
#% 309211
#% 309253
#% 330769
#% 340934
#% 340936
#% 344448
#% 420464
#% 643559
#% 709230
#% 722312
#% 766409
#% 818221
#% 869499
#% 879582
#% 881955
#! Metasearch and data-fusion techniques combine the rank lists of multiple document retrieval systems with the aim of improving search coverage and precision. We propose a new fusion method that partitions the rank lists of document retrieval systems into chunks. The size of chunks grows exponentially in the rank list. Using a small number of training queries, the probabilities of relevance of documents in different chunks are approximated for each search system. The estimated probabilities and normalized document scores are used to compute the final document ranks in the merged list. We show that our proposed method produces higher average precision values than previous systems across a range of testbeds.

#index 1392447
#* Query hardness estimation using Jensen-Shannon divergence among multiple scoring functions
#@ Javed A. Aslam;Virgil Pavlu
#t 2007
#c 16
#% 115608
#% 397161
#% 730072
#% 810906
#% 818267
#% 818276
#% 879613
#% 879632
#! We consider the issue of query performance, and we propose a novel method for automatically predicting the difficulty of a query. Unlike a number of existing techniques which are based on examining the ranked lists returned in response to perturbed versions of the query with respect to the given collection or perturbed versions of the collection with respect to the given query, our technique is based on examining the ranked lists returned by multiple scoring functions (retrieval engines) with respect to the given query and collection. In essence, we propose that the results returned by multiple retrieval engines will be relatively similar for "easy" queries but more diverse for "difficult" queries. By appropriately employing Jensen-Shannon divergence to measure the "diversity" of the returned results, we demonstrate a methodology for predicting query difficulty whose performance exceeds existing state-of the-art techniques on TREC collections, often remarkably so.

#index 1392448
#* Query reformulation and refinement using NLP-based sentence clustering
#@ Frédéric Roulland;Aaron Kaplan;Stefania Castellani;Claude Roux;Antonietta Grasso;Karin Pettersson;Jacki O'Neill
#t 2007
#c 16
#% 118771
#% 169819
#% 280840
#% 280849
#% 329090
#% 340951
#% 478240
#% 490619
#% 578306
#% 741881
#% 1677932
#! We have developed an interactive query refinement tool that helps users search a knowledge base for solutions to problems with electronic equipment. The system is targeted towards non-technical users, who are often unable to formulate precise problem descriptions on their own. Two distinct but interrelated functionalities support the refinement of a vague, non-technical initial query into a more precise problem description: a synonymy mechanism that allows the system to match non-technical words in the query with corresponding technical terms in the knowledge base, and a novel refinement mechanism that helps the user build up successively longer and more precise problem descriptions starting from the seed of the initial query. A natural language parser is used both in the application of context-sensitive synonymy rules and the construction of the refinement tree.

#index 1392449
#* Automatic morphological query expansion using analogy-based machine learning
#@ Fabienne Moreau;Vincent Claveau;Pascale Sébillot
#t 2007
#c 16
#% 115470
#% 169729
#% 208934
#% 218985
#% 241238
#% 286307
#% 561154
#% 561168
#! Information retrieval systems (IRSs) usually suffer from a low ability to recognize a same idea that is expressed in different forms. A way of improving these systems is to take into account morphological variants. We propose here a simple yet effective method to recognize these variants that are further used so as to enrich queries. In comparison with already published methods, our system does not need any external resources or a priori knowledge and thus supports many languages. This new approach is evaluated against several collections, 6 different languages and is compared to existing tools such as a stemmer and a lemmatizer. Reported results show a significant and systematic improvement of the whole IRS efficiency both in terms of precision and recall for every language.

#index 1392450
#* Advanced structural representations for question classification and answer re-ranking
#@ Silvia Quarteroni;Alessandro Moschitti;Suresh Manandhar;Roberto Basili
#t 2007
#c 16
#% 269217
#% 330616
#% 722926
#% 742218
#% 743284
#% 766520
#% 807296
#% 815896
#% 818251
#% 905498
#% 939636
#% 1344879
#% 1665151
#! In this paper, we study novel structures to represent information in three vital tasks in question answering: question classification, answer classification and answer reranking. We define a new tree structure called PAS to represent predicate-argument relations, as well as a new kernel function to exploit its representative power. Our experiments with Support Vector Machines and several tree kernel functions suggest that syntactic information helps specific task as question classification, whereas, when data sparseness is higher as in answer classification, studying coarse semantic information like PAS is a promising research area.

#index 1392451
#* Incorporating diversity and density in active learning for relevance feedback
#@ Zuobing Xu;Ram Akella;Yi Zhang
#t 2007
#c 16
#% 118726
#% 236729
#% 262112
#% 340899
#% 342707
#% 466887
#% 565531
#% 818209
#! Relevance feedback, which uses the terms in relevant documents to enrich the user's initial query, is an effective method for improving retrieval performance. An associated key research problem is the following: Which documents to present to the user so that the user's feedback on the documents can significantly impact relevance feedback performance. This paper views this as an active learning problem and proposes a new algorithm which can efficiently maximize the learning benefits of relevance feedback. This algorithm chooses a set of feedback documents based on relevancy, document diversity and document density. Experimental results show a statistically significant and appreciable improvement in the performance of our new approach over the existing active feedback methods.

#index 1392452
#* Relevance feedback using weight propagation compared with information-theoretic query expansion
#@ Fadi Yamout;Michael Oakes;John Tait
#t 2007
#c 16
#% 118726
#% 223810
#% 290830
#% 340948
#% 375017
#% 379178
#% 387427
#% 406493
#% 411760
#% 438136
#% 549441
#% 660001
#% 742666
#% 1742135
#! A new Relevance Feedback (RF) technique called Weight Propagation has been developed which provides greater retrieval effectiveness and computational efficiency than previously described techniques. Documents judged relevant by the user propagate positive weights to documents close by in vector similarity space, while documents judged not relevant propagate negative weights to such neighbouring documents. Retrieval effectiveness is improved since the documents are treated as independent vectors rather than being merged into a single vector as is the case with traditional vector model RF techniques, or by determining the documents relevancy based in part on the lengths of all the documents as with traditional probabilistic RF techniques. Improving the computational efficiency of Relevance Feedback by considering only documents in a given neighbourhood means that the Weight Propagation technique can be used with large collections.

#index 1392453
#* A retrieval evaluation methodology for incomplete relevance assessments
#@ Mark Baillie;Leif Azzopardi;Ian Ruthven
#t 2007
#c 16
#% 133889
#% 262097
#% 262102
#% 340890
#% 450042
#% 570323
#% 643020
#% 766409
#% 766410
#% 818222
#% 857180
#% 879598
#% 879650
#! In this paper we a propose an extended methodology for laboratory based Information Retrieval evaluation under incomplete relevance assessments. This new protocol aims to identify potential uncertainty during system comparison that may result from incompleteness. We demonstrate how this methodology can lead towards a finer grained analysis of systems. This is advantageous, because the detection of uncertainty during the evaluation process can guide and direct researchers when evaluating new systems over existing and future test collections.

#index 1392454
#* Evaluating query-independent object features for relevancy prediction
#@ Andres R. Masegosa;Hideo Joho;Joemon M. Jose
#t 2007
#c 16
#% 44876
#% 129987
#% 243728
#% 309095
#% 729437
#% 766454
#% 790840
#% 799040
#% 805200
#% 818206
#% 818258
#% 835027
#% 840577
#% 850133
#% 893636
#% 998622
#% 1269504
#! This paper presents a series of experiments investigating the effectiveness of query-independent features extracted from retrieved objects to predict relevancy. Features were grouped into a set of conceptual categories, and individually evaluated based on click-through data collected in a laboratory-setting user study. The results showed that while textual and visual features were useful for relevancy prediction in a topic-independent condition, a range of features can be effective when topic knowledge was available. We also re-visited the original study from the perspective of significant features identified by our experiments.

#index 1392455
#* The utility of information extraction in the classification of books
#@ Tom Betts;Maria Milosavljevic;Jon Oberlander
#t 2007
#c 16
#% 280817
#% 301259
#% 332658
#% 344447
#% 465754
#% 466101
#% 751593
#% 855113
#! We describe work on automatically assigning classification labels to books using the Library of Congress Classification scheme. This task is non-trivial due to the volume and variety of books that exist. We explore the utility of Information Extraction (IE) techniques within this text categorisation (TC) task, automatically extracting structured information from the full text of books. Experimental evaluation of performance involves a corpus of books from Project Gutenberg. Results indicate that a classifier which combines methods and tools from IE and TC significantly improves over a state-of-the-art text classifier, achieving a classification performance of Fβ=1 = 0.8099.

#index 1392456
#* Combined syntactic and semantic Kernels for text classification
#@ Stephan Bloehdorn;Alessandro Moschitti
#t 2007
#c 16
#% 344447
#% 430757
#% 458379
#% 633682
#% 722926
#% 743284
#% 815303
#% 896031
#% 915364
#% 938695
#% 938706
#% 1656540
#% 1665151
#% 1673568
#! The exploitation of syntactic structures and semantic background knowledge has always been an appealing subject in the context of text retrieval and information management. The usefulness of this kind of information has been shown most prominently in highly specialized tasks, such as classification in Question Answering (QA) scenarios. So far, however, additional syntactic or semantic information has been used only individually. In this paper, we propose a principled approach for jointly exploiting both types of information. We propose a new type of kernel, the Semantic Syntactic Tree Kernel (SSTK), which incorporates linguistic structures, e.g. syntactic dependencies, and semantic background knowledge, e.g. term similarity based on WordNet, to automatically learn question categories in QA. We show the power of this approach in a series of experiments with a well known Question Classification dataset.

#index 1392457
#* Fast large-scale spectral clustering by sequential shrinkage optimization
#@ Tie-Yan Liu;Huai-Yuan Yang;Xin Zheng;Tao Qin;Wei-Ying Ma
#t 2007
#c 16
#% 313959
#% 415163
#% 647793
#% 724227
#! In many applications, we need to cluster large-scale data objects. However, some recently proposed clustering algorithms such as spectral clustering can hardly handle large-scale applications due to the complexity issue, although their effectiveness has been demonstrated in previous work. In this paper, we propose a fast solver for spectral clustering. In contrast to traditional spectral clustering algorithms that first solve an eigenvalue decomposition problem, and then employ a clustering heuristic to obtain labels for the data points, our new approach sequentially decides the labels of relatively well-separated data points. Because the scale of the problem shrinks quickly during this process, it can be much faster than the traditional methods. Experiments on both synthetic data and a large collection of product records show that our algorithm can achieve significant improvement in speed as compared to traditional spectral clustering algorithms.

#index 1392458
#* A probabilistic model for clustering text documents with multiple fields
#@ Shanfeng Zhu;Ichigaku Takigawa;Shuqin Zhang;Hiroshi Mamitsuka
#t 2007
#c 16
#% 296738
#% 329531
#% 387427
#% 458369
#% 729911
#% 734917
#% 826918
#% 874491
#! We address the problem of clustering documents with multiple fields, such as scientific literature with the distinct fields: title, abstract, keywords, main text and references. By taking into consideration of the distinct word distributions of each field, we propose a new probabilistic model, Field Independent Clustering Model (FICM), for clustering documents with multiple fields. The benefits of FICM come not only from integrating the discrimination abilities of each field but also from the power of selecting the most suitable component probabilistic model for each field. We examined the performance of FICM on the problem of clustering biomedical documents with three fields (title, abstract and MeSH). From the genomics track data of TREC 2004 and TREC 2005, we randomly generated 60 datasets where the number of classes in each dataset ranged from 3 to 12. By applying the appropriate configuration of generative models for each field, FICM outperformed a classical multinomial model in 59 out of the total 60 datasets, of which 47 were statistically significant at the 95% level, and FICM also outperformed a multivariate Bernoulli model in 52 out of the total 60 datasets, of which 36 were statistically significant at the 95% level.

#index 1392459
#* Personalized communities in a distributed recommender system
#@ Sylvain Castagnos;Anne Boyer
#t 2007
#c 16
#% 124010
#% 173879
#% 202011
#% 330687
#% 528156
#% 616944
#% 767656
#% 810583
#% 1223314
#% 1650569
#% 1704228
#! The amount of data exponentially increases in information systems and it becomes more and more difficult to extract the most relevant information within a very short time. Among others, collaborative filtering processes help users to find interesting items by modeling their preferences and by comparing them with users having the same tastes. Nevertheless, there are a lot of aspects to consider when implementing such a recommender system. The number of potential users and the confidential nature of some data are taken into account. This paper introduces a new distributed recommender system based on a user-based filtering algorithm. Our model has been transposed for Peer-to-Peer architectures. It has been especially designed to deal with problems of scalability and privacy. Moreover, it adapts its prediction computations to the density of the user neighborhood.

#index 1392460
#* Information recovery and discovery in collaborative web search
#@ Maurice Coyle;Barry Smyth
#t 2007
#c 16
#% 152251
#% 268079
#% 292151
#% 309792
#% 330705
#% 341964
#% 803556
#% 832099
#% 857477
#% 857478
#% 879692
#% 936941
#% 1289575
#! When we search for information we are usually either trying to recover something that we have found in the past or trying to discover some new information. In this paper we will evaluate how the collaborative Web search technique, which personalizes search results for communities of like-minded users, can help in recovery- and discovery-type search tasks in a corporate search scenario.

#index 1392461
#* Collaborative filtering based on transitive correlations between items
#@ Alexandros Nanopoulos
#t 2007
#c 16
#% 124010
#% 173879
#% 202010
#% 280852
#% 314933
#% 320432
#% 330687
#% 397155
#% 452563
#% 734590
#% 734593
#% 734594
#% 818216
#% 879627
#% 1650569
#% 1719270
#% 1742072
#! With existing collaborative filtering algorithms, a user has to rate a sufficient number of items, before receiving reliable recommendations. To overcome this limitation, we provide the insight that correlations between items can form a network, in which we examine transitive correlations between items. The emergence of power laws in such networks signifies the existence of items with substantially more transitive correlations. The proposed algorithm finds highly correlative items and provides effective recommendations by adapting to user preferences. We also develop pruning criteria that reduce computation time. Detailed experimental results illustrate the superiority of the proposed method.

#index 1392462
#* Entropy-based authorship search in large document collections
#@ Ying Zhao;Justin Zobel
#t 2007
#c 16
#% 184486
#% 197387
#% 253191
#% 324129
#% 340904
#% 387427
#% 393059
#% 397127
#% 421720
#% 578558
#% 719598
#% 722930
#% 748738
#% 750863
#% 763708
#% 766431
#% 770870
#% 867054
#% 984061
#% 1676557
#% 1682064
#! The purpose of authorship search is to identify documents written by a particular author in large document collections. Standard search engines match documents to queries based on topic, and are not applicable to authorship search. In this paper we propose an approach to authorship search based on information theory. We propose relative entropy of style markers for ranking, inspired by the language models used in information retrieval. Our experiments on collections of newswire texts show that, with simple style markers and sufficient training data, documents by a particular author can be accurately found from within large collections. Although effectiveness does degrade as collection size is increased, with even 500,000 documents nearly half of the top-ranked documents are correct matches. We have also found that the authorship search approach can be used for authorship attribution, and is much more scalable than state-of-art approaches in terms of the collection size and the number of candidate authors.

#index 1392463
#* Use of topicality and information measures to improve document representation for story link detection
#@ Chirag Shah;Koji Eguchi
#t 2007
#c 16
#% 67565
#% 194284
#% 262096
#% 340901
#% 342707
#% 443984
#% 575571
#% 766444
#% 838532
#% 855200
#% 907612
#% 995518
#! Several information organization, access, and filtering systems can benefit from different kind of document representations than those used in traditional Information Retrieval (IR). Topic Detection and Tracking (TDT) is an example of such a domain. In this paper we demonstrate that traditional methods for term weighing does not capture topical information and this leads to inadequate representation of documents for TDT applications. We present various hypotheses regarding the factors that can help in improving the document representation for Story Link Detection (SLD) - a core task of TDT. These hypotheses are tested using various TDT corpora. From our experiments and analysis we found that in order to obtain a faithful representation of documents in TDT domain, we not only need to capture a term's importance in traditional IR sense, but also evaluate its topical behavior. Along with defining this behavior, we propose a novel measure that captures a term's importance at the corpus level as well as its discriminating power for topics. This new measure leads to a much better document representation as reflected by the significant improvements in the results.

#index 1392464
#* Ad hoc retrieval of documents with topical opinion
#@ Jason Skomorowski;Olga Vechtomova
#t 2007
#c 16
#% 279755
#% 529193
#% 577355
#% 578875
#% 727877
#% 741940
#% 746885
#% 815915
#% 853874
#% 854646
#! With a growing amount of subjective content distributed across the Web, there is a need for a domain-independent information retrieval system that would support ad hoc retrieval of documents expressing opinions on a specific topic of the user's query. In this paper we present a lightweight method for ad hoc retrieval of documents which contain subjective content on the topic of the query. Documents are ranked by the likelihood each document expresses an opinion on a query term, approximated as the likelihood any occurrence of the query term is modified by a subjective adjective. Domain-independent user-based evaluation of the proposed method was conducted, and shows statistically significant gains over the baseline system.

#index 1392465
#* Probabilistic models for expert finding
#@ Hui Fang;ChengXiang Zhai
#t 2007
#c 16
#% 169781
#% 340948
#% 342707
#% 879570
#! A common task in many applications is to find persons who are knowledgeable about a given topic (i.e., expert finding). In this paper, we propose and develop a general probabilistic framework for studying expert finding problem and derive two families of generative models (candidate generation models and topic generation models) from the framework. These models subsume most existing language models proposed for expert finding. We further propose several techniques to improve the estimation of the proposed models, including incorporating topic expansion, using a mixture model to model candidate mentions in the supporting documents, and defining an email count-based prior in the topic generation model. Our experiments show that the proposed estimation strategies are all effective to improve retrieval accuracy.

#index 1392466
#* Using relevance feedback in expert search
#@ Craig Macdonald;Iadh Ounis
#t 2007
#c 16
#% 312701
#% 879570
#% 907525
#% 1742070
#! In Enterprise settings, expert search is considered an important task. In this search task, the user has a need for expertise - for instance, they require assistance from someone about a topic of interest. An expert search system assists users with their "expertise need" by suggesting people with relevant expertise to the topic of interest. In this work, we apply an expert search approach that does not explicitly rank candidates in response to a query, but instead implicitly ranks candidates by taking into account a ranking of document with respect to the query topic. Pseudo-relevance feedback, aka query expansion, has been shown to improve retrieval performance in adhoc search tasks. In this work, we investigate to which extent query expansion can be applied in an expert search task to improve the accuracy of the generated ranking of candidates. We define two approaches for query expansion, one based on the initial of ranking of documents for the query topic. The second approach is based on the final ranking of candidates. The aims of this paper are two-fold. Firstly, to determine if query expansion can be successfully applied in the expert search task, and secondly, to ascertain if either of the two forms of query expansion can provide robust, improved retrieval performance. We perform a thorough evaluation contrasting the two query expansion approaches in the context of the TREC 2005 and 2006 Enterprise tracks.

#index 1392467
#* Using topic shifts for focussed access to XML repositories
#@ Elham Ashoori;Mounia Lalmas
#t 2007
#c 16
#% 340948
#% 397127
#% 719598
#% 748583
#% 752308
#% 803545
#% 1674716
#% 1721867
#% 1733303
#% 1742098
#! In focussed XML retrieval, a retrieval unit is an XML element that not only contains information relevant to a user query, but also is specific to the query. INEX defines a relevant element to be at the right level of granularity if it is exhaustive and specific to the user's request - i.e., it discusses fully the topic requested in the user's query and no other topics. The exhaustivity and specificity dimensions are both expressed in terms of the "quantity" of topics discussed within each element. We therefore propose to use the number of topic shifts in an XML element, to express the "quantity" of topics discussed in an element as a mean to capture specificity. We experimented with a number of element-specific smoothing methods within the language modelling framework. These methods enable us to adjust the amount of smoothing required for each XML element depending on its number of topic shifts, to capture specificity. Using the number of topic shifts combined with element length improves retrieval effectiveness, thus indicating that the number of topic shifts is a useful evidence in focussed XML retrieval.

#index 1392468
#* Feature- and query-based table of contents generation for XML documents
#@ Zoltán Szlávik;Anastasios Tombros;Mounia Lalmas
#t 2007
#c 16
#% 262036
#% 288614
#% 766416
#% 824703
#% 874260
#% 878916
#% 893634
#% 1674746
#% 1674747
#% 1674748
#% 1674754
#% 1681993
#% 1682011
#! The availability of a document's logical structure in XML retrieval allows retrieval systems to return document portions (elements) instead of whole documents. This helps searchers focusing their attention to the relevant content within a document. However, other, e.g. sibling or parent, elements of retrieved elements may also be important as they provide context to the retrieved elements. The use of table of contents (TOC) offers an overview of a document and shows the most important elements and their relations to each other. In this paper, we investigate what searchers think is important in automatic TOC generation. We ask searchers to indicate their preferences for element features (depth, length, relevance) in order to generate TOCs that help them complete information seeking tasks. We investigate what these preferences are, and what are the characteristics of the TOCs generated by searchers' settings. The results have implications for the design of intelligent TOC generation approaches for XML retrieval.

#index 1392469
#* Setting per-field normalisation hyper-parameters for the named-page finding search task
#@ Ben He;Iadh Ounis
#t 2007
#c 16
#% 218982
#% 397183
#% 643069
#% 783474
#% 818261
#% 857180
#% 1348355
#% 1715606
#! Per-field normalisation has been shown to be effective for Web search tasks, e.g. named-page finding. However, per-field normalisation also suffers from having hyper-parameters to tune on a per-field basis. In this paper, we argue that the purpose of per-field normalisation is to adjust the linear relationship between field length and term frequency. We experiment with standard Web test collections, using three document fields, namely the body of the document, its title, and the anchor text of its incoming links. From our experiments, we find that across different collections, the linear correlation values, given by the optimised hyper-parameter settings, are proportional to the maximum negative linear correlation. Based on this observation, we devise an automatic method for setting the per-field normalisation hyper-parameter values without the use of relevance assessment for tuning. According to the evaluation results, this method is shown to be effective for the body and title fields. In addition, the difficulty in setting the per-field normalisation hyper-parameter for the anchor text field is explained.

#index 1392470
#* Combining evidence for relevance criteria: a framework and experiments in web retrieval
#@ Theodora Tsikrika;Mounia Lalmas
#t 2007
#c 16
#% 167557
#% 255170
#% 268079
#% 290830
#% 397126
#% 458404
#% 487135
#% 679851
#% 766463
#% 766498
#% 818254
#% 818255
#! We present a framework that assesses relevance with respect to several relevance criteria, by combining the query-dependent and query-independent evidence indicating these criteria. This combination of evidence is modelled in a uniform way, irrespective of whether the evidence is associated with a single document or related documents. The framework is formally expressed within Dempster-Shafer theory. It is evaluated for web retrieval in the context of TREC's Topic Distillation task. Our results indicate that aggregating content-based evidence from the linked pages of a page is beneficial, and that the additional incorporation of their homepage evidence further improves the effectiveness.

#index 1392471
#* Classifier fusion for SVM-based multimedia semantic indexing
#@ Stéphane Ayache;Georges Quénot;Jérôme Gensel
#t 2007
#c 16
#% 722925
#% 730161
#% 743284
#% 839912
#% 840066
#% 884155
#% 907496
#% 1374356
#% 1727343
#! Concept indexing in multimedia libraries is very useful for users searching and browsing but it is a very challenging research problem as well. Combining several modalities, features or concepts is one of the key issues for bridging the gap between signal and semantics. In this paper, we present three fusion schemes inspired from the classical early and late fusion schemes. First, we present a kernel-based fusion scheme which takes advantage of the kernel basis of classifiers such as SVMs. Second, we integrate a new normalization process into the early fusion scheme. Third, we present a contextual late fusion scheme to merge classification scores of several concepts. We conducted experiments in the framework of the official TRECVID'06 evaluation campaign and we obtained significant improvements with the proposed fusion schemes relatively to usual fusion schemes.

#index 1392472
#* Search of spoken documents retrieves well recognized transcripts
#@ Mark Sanderson;Xiao Mang Shou
#t 2007
#c 16
#% 501799
#% 501924
#% 508268
#% 742225
#! This paper presents a series of analyses and experiments on spoken document retrieval systems: search engines that retrieve transcripts produced by speech recognizers. Results show that transcripts that match queries well tend to be recognized more accurately than transcripts that match a query less well. This result was described in past literature, however, no study or explanation of the effect has been provided until now. This paper provides such an analysis showing a relationship between word error rate and query length. The paper expands on past research by increasing the number of recognitions systems that are tested as well as showing the effect in an operational speech retrieval system. Potential future lines of enquiry are also described.

#index 1392473
#* Natural language processing for usage based indexing of web resources
#@ Anne Boyer;Armelle Brun
#t 2007
#c 16
#% 202011
#% 387427
#% 734590
#% 1223314
#! The identification of reliable and interesting items on Internet becomes more and more difficult and time consuming. This paper is a position paper describing our intended work in the framework of multimedia information retrieval by browsing techniques within web navigation. It relies on a usage-based indexing of resources: we ignore the nature, the content and the structure of resources. We describe a new approach taking advantage of the similarity between statistical modeling of language and document retrieval systems. A syntax of usage is computed that designs a Statistical Grammar of Usage (SGU). A SGU enables resources classification to perform a personalized navigation assistant tool. It relies both on collaborative filtering to compute virtual communities of users and classical statistical language models. The resulting SGU is a community dependent SGU.

#index 1392474
#* Harnessing trust in social search
#@ Peter Briggs;Barry Smyth
#t 2007
#c 16
#% 127574
#% 202011
#% 220711
#% 240628
#% 268079
#% 567950
#% 803556
#% 1704290
#! The social Web emphasises the increased role of millions of users in the creation of a new type of online content, often expressed in the form of opinions or judgements. This has led to some novel approaches to information access that take advantage of user opinions and activities as a way to guide users as they browse or search for information. We describe a social search technique that harnesses the experiences of a network of searchers to generate result recommendations that can complement the search results that are returned by some standard Web search engine.

#index 1392475
#* How to compare bilingual to monolingual cross-language information retrieval
#@ Franco Crivellari;Giorgio Maria Di Nunzio;Nicola Ferro
#t 2007
#c 16
#% 144074
#% 818292
#% 879590
#% 1674896
#% 1916037
#! The study of cross-lingual Information Retrieval Systems (IRSs) and a deep analysis of system performances should provide guidelines, hints, and directions to drive the design and development of the next generation MultiLingual Information Access (MLIA) systems. In addition, effective tools for interpreting and comparing the experimental results should be made easily available to the research community. To this end, we propose a twofold methodology for the evaluation of Cross Language Information Retrieval (CLIR) systems: statistical analyses to provide MLIA researchers with quantitative and more sophisticated analysis techniques; and graphical tools to allow for a more qualitative comparison and an easier presentation of the results. We provide concrete examples about how the proposed methodology can be applied by studying the monolingual and bilingual tasks of the Cross-Language Evaluation Forum (CLEF) 2005 and 2006 campaigns.

#index 1392476
#* Multilingual text classification using ontologies
#@ Gerard De Melo;Stefan Siersdorfer
#t 2007
#c 16
#% 174161
#% 190581
#% 217207
#% 269217
#% 344447
#% 818313
#% 832331
#% 1299481
#! In this paper, we investigate strategies for automatically classifying documents in different languages thematically, geographically or according to other criteria. A novel linguistically motivated text representation scheme is presented that can be used with machine learning algorithms in order to learn classifications from pre-classified examples and then automatically classify documents that might be provided in entirely different languages. Our approach makes use of ontologies and lexical resources but goes beyond a simple mapping from terms to concepts by fully exploiting the external knowledge manifested in such resources and mapping to entire regions of concepts. For this, a graph traversal algorithm is used to explore related concepts that might be relevant. Extensive testing has shown that our methods lead to significant improvements compared to existing approaches.

#index 1392477
#* Using visual-textual mutual information and entropy for inter-modal document indexing
#@ Jean Martinet;Shin'ichi Satoh
#t 2007
#c 16
#% 152035
#% 448776
#% 642989
#% 642990
#% 782526
#% 1727351
#! This paper presents a contribution in the domain of automatic visual document indexing based on inter-modal analysis, in the form of a statistical indexing model. The approach is based on intermodal document analysis, which consists in modeling and learning some relationships between several modalities from a data set of annotated documents in order to extract semantics. When one of the modalities is textual, the learned associations can be used to predict a textual index for visual data from a new document (image or video). More specifically, the presented approach relies on a learning process in which associations between visual and textual information are characterized by the mutual information of the modalities. Besides, the model uses the information entropy of the distribution of the visual modality against the textual modality as a second source to select relevant indexing terms. We have implemented the proposed information theoretic model, and the results of experiments assessing its performance on two collections (image and video) show that information theory is an interesting framework to automatically annotate documents.

#index 1392478
#* A study of global inference algorithms in multi-document summarization
#@ Ryan McDonald
#t 2007
#c 16
#% 70370
#% 194251
#% 283171
#% 816173
#% 939548
#% 939705
#% 939845
#% 1223708
#% 1261540
#% 1261547
#% 1306081
#! In this work we study the theoretical and empirical properties of various global inference algorithms for multi-document summarization. We start by defining a general framework for inference in summarization. We then present three algorithms: The first is a greedy approximate method, the second a dynamic programming approach based on solutions to the knapsack problem, and the third is an exact algorithm that uses an Integer Linear Programming formulation of the problem. We empirically evaluate all three algorithms and show that, relative to the exact solution, the dynamic programming algorithm provides near optimal results with preferable scaling properties.

#index 1392479
#* Document representation using global association distance model
#@ José E. Medina-Pagola;Ansel Y. Rodríguez;Abdel Hechavarría;José Hernández Palancar
#t 2007
#c 16
#% 228088
#% 318412
#% 474630
#% 840583
#% 1671947
#! Text information processing depends critically on the proper representation of documents. Traditional models, like the vector space model, have significant limitations because they do not consider semantic relations amongst terms. In this paper we analyze a document representation using the association graph scheme and present a new approach called Global Association Distance Model (GADM). At the end, we compare GADM using K-NN classifier with the classical vector space model and the association graph model.

#index 1392480
#* Sentence level sentiment analysis in the presence of conjuncts using linguistic analysis
#@ Arun Meena;T. V. Prabhakar
#t 2007
#c 16
#% 577246
#% 577355
#% 723399
#% 746885
#% 815915
#% 854646
#% 855282
#% 938687
#% 939346
#% 1707816
#! In this paper we present an approach to extract sentiments associated with a phrase or sentence. Sentiment analysis has been attempted mostly for documents typically a review or a news item. Conjunctions have a substantial impact on the overall sentiment of a sentence, so here we present how atomic sentiments of individual phrases combine together in the presence of conjuncts to decide the overall sentiment of a sentence. We used word dependencies and dependency trees to analyze the sentence constructs and were able to get results close to 80%. We have also analyzed the effect of WordNet on the accuracy of the results over General Inquirer.

#index 1392481
#* PageRank: when order changes
#@ Massimo Melucci;Luca Pretto
#t 2007
#c 16
#% 309749
#% 805895
#% 878224
#! As PageRank is a ranking algorithm, it is of prime interest to study the order induced by its values on webpages. In this paper a thorough mathematical analysis of PageRank-induced order changes when the damping factor varies is provided. Conditions that do not allow variations in the order are studied, and the mechanisms that make the order change are mathematically investigated. Moreover the influence on the order of a truncation in the actual computation of PageRank through a power series is analysed. Experiments carried out on a large Web digraph to integrate the mathematical analysis show that PageRank -- while working on a real digraph -- tends to hinder variations in the order of large rankings, presenting a high stability in its induced order both in the face of large variations of the damping factor value and in the face of truncations in its computation.

#index 1392482
#* Model tree learning for query term weighting in question answering
#@ Christof Monz
#t 2007
#c 16
#% 136350
#% 251517
#% 290482
#% 321055
#% 714615
#% 755394
#% 783555
#% 815868
#% 854668
#% 1387576
#! Question answering systems rely on retrieval components to identify documents that contain an answer to a user's question. The formulation of queries that are used for retrieving those documents has a strong impact on the effectiveness of the retrieval component. Here, we focus on predicting the importance of terms from the original question. We use model tree machine learning techniques in order to assign weights to query terms according to their usefulness for identifying documents that contain an answer. Incorporating the learned weights into a state-of-the-art retrieval system results in statistically significant improvements.

#index 1392483
#* Examining repetition in user search behavior
#@ Mark Sanderson;Susan Dumais
#t 2007
#c 16
#% 590523
#% 765412
#% 766447
#% 805878
#% 879692
#% 1348355
#! This paper describes analyses of the repeated use of search engines. It is shown that users commonly re-issue queries, either to examine search results deeply or simply to query again, often days or weeks later. Hourly and weekly periodicities in behavior are observed for both queries and clicks. Navigational queries were found to be repeated differently from others.

#index 1392484
#* Popularity weighted ranking for academic digital libraries
#@ Yang Sun;C. Lee Giles
#t 2007
#c 16
#% 144074
#% 201991
#% 281174
#% 290830
#% 309095
#% 340892
#% 805896
#% 869534
#! We propose a popularity weighted ranking algorithm for academic digital libraries that uses the popularity factor of a publication venue overcoming the limitations of impact factors. We compare our method with the naive PageRank, citation counts and HITS algorithm, three popular measures currently used to rank papers beyond lexical similarity. The ranking results are evaluated by discounted cumulative gain(DCG) method using four human evaluators. We show that our proposed ranking algorithm improves the DCG performance by 8.5% on average compared to naive PageRank, 16.3% compared to citation count and 23.2% compared to HITS. The algorithm is also evaluated by click through data from CiteSeer usage log.

#index 1392485
#* Naming functions for the vector space model
#@ Yannis Tzitzikas;Yannis Theoharis
#t 2007
#c 16
#% 733374
#% 737451
#% 783515
#% 838545
#% 838547
#% 862117
#% 1715599
#! The Vector Space Model (VSM) is probably the most widely used model for retrieving information from text collections (and recently from over other kinds of corpi). Assuming this model, we study the problem of finding the best query that "names" (or describes) a given (unordered or ordered) set of objects. We formulate several variations of this problem and we provide methods and algorithms for solving them.

#index 1392486
#* Effective use of semantic structure in XML retrieval
#@ Roelof Van Zwol;Tim Van Loosbroek
#t 2007
#c 16
#% 766409
#% 879687
#% 879696
#% 932897
#% 1674727
#% 1721851
#! The objective of XML retrieval is to return relevant XML document fragments that answer a given user information need, by exploiting the document structure. The focus in this article is on automatically deriving and using semantic XML structure to enhance the retrieval performance of XML retrieval systems. Based on a naive approach for named entity detection, we discuss how the structure of an XML document can be enriched using the Reuters 21587 news collection. Based on a retrieval performance experiment, we study the effect of the additional semantic structure on the retrieval performance of our XSee search engine for XML documents. The experiment provides some initial evidence that an XML retrieval system significantly benefits from having meaningful XML structure.

#index 1392487
#* Searching documents based on relevance and type
#@ Jun Xu;Yunbo Cao;Hang Li;Nick Craswell;Yalou Huang
#t 2007
#c 16
#% 235918
#% 287204
#% 337225
#% 340928
#% 397126
#% 406493
#% 746867
#% 818258
#! This paper extends previous work on document retrieval and document type classification, addressing the problem of 'typed search'. Specifically, given a query and a designated document type, the search system retrieves and ranks documents not only based on the relevance to the query, but also based on the likelihood of being the designated document type. The paper formalizes the problem in a general framework consisting of 'relevance model' and 'type model'. The relevance model indicates whether or not a document is relevant to a query. The type model indicates whether or not a document belongs to the designated document type. We consider three methods for combing the models: linear combination of scores, thresholding on the type score, and a hybrid of the previous two methods. We take course page search and instruction document search as examples and have conducted a series of experiments. Experimental results show our proposed approaches can significantly outperform the baseline methods.

#index 1392488
#* Investigation of the effectiveness of cross-media indexing
#@ Murat Yakici;Fabio Crestani
#t 2007
#c 16
#% 280815
#% 280856
#% 562344
#% 914223
#% 1682033
#! Cross-media analysis and indexing leverage the individual potential of each indexing information provided by different modalities, such as speech, text and image, to improve the effectiveness of information retrieval and filtering in later stages. The process does not only constitute generating a merged representation of the digital content, such as MPEG-7, but also enriching it in order to help remedy the imprecision and noise introduced during the low-level analysis phases. It has been hypothesized that a system that combines different media descriptions of the same multi-modal audio-visual segment in a semantic space will perform better at retrieval and filtering time. In order to validate this hypothesis, we have developed a cross-media indexing system which utilises the Multiple Evidence approach by establishing links among the modality specific textual descriptions in order to depict topical similarity.

#index 1392489
#* Improve ranking by using image information
#@ Qing Yu;Shuming Shi;Zhiwei Li;Ji-Rong Wen;Wei-Ying Ma
#t 2007
#c 16
#% 219847
#% 268079
#% 309095
#% 330709
#% 433999
#% 577224
#% 676177
#% 754078
#% 780874
#! This paper explores the feasibility of including image information embedded in Web pages in relevance computation to improve search performance. In determining the ranking of Web pages against a given query, most (if not all) modern Web search engines consider two kinds of factors: text information (including title, URL, body text, anchor text, etc) and static ranking (e.g. PageRank [1]). Although images have been widely used to help represent Web pages and carry valuable information, little work has been done to take advantage of them in computing the relevance score of a Web page given a query. We propose, in this paper, a framework to contain image information in ranking functions. Preliminary experimental results show that, when image information is used properly, ranking results can be improved.

#index 1392490
#* N-step PageRank for web search
#@ Li Zhang;Tao Qin;Tie-Yan Liu;Ying Bao;Hang Li
#t 2007
#c 16
#% 290830
#% 406493
#% 1289272
#! PageRank has been widely used to measure the importance of web pages based on their interconnections in the web graph. Mathematically speaking, PageRank can be explained using a Markov random walk model, in which only the direct outlinks of a page contribute to its transition probability. In this paper, we propose improving the PageRank algorithm by looking N-step ahead when constructing the transition probability matrix. The motivation comes from the similar "looking N-step ahead" strategy that is successfully used in computer chess. Specifically, we assume that if the random surfer knows the N-step outlinks of each web page, he/she can make a better decision on choosing which page to navigate for the next time. It is clear that the classical PageRank algorithm is a special case of our proposed N-step PageRank method. Experimental results on the dataset of TREC Web track show that our proposed algorithm can boost the search accuracy of classical PageRank by more than 15% in terms of mean average precision.

#index 1392491
#* Authorship attribution via combination of evidence
#@ Ying Zhao;Phil Vines
#t 2007
#c 16
#% 578558
#% 729957
#% 750863
#% 1676557
#% 1682064
#! Authorship attribution is a process of determining who wrote a particular document. We have found that different systems work well for particular sets of authors but not others. In this paper, we propose three authorship attribution systems, based on different ways of combining existing methodologies. All systems show better effectiveness than the state-of-art methods.

#index 1392492
#* Cross-document entity tracking
#@ Roxana Angheluta;Marie-Francine Moens
#t 2007
#c 16
#% 747890
#% 1250185

#index 1392493
#* Enterprise people and skill discovery using tolerant retrieval and visualization
#@ Jan Brunnert;Omar Alonso;Dirk Riehle
#t 2007
#c 16
#% 526880
#% 730082
#% 732614
#% 838066
#% 847008
#% 879570
#! Understanding an enterprise's workforce and skill-set can be seen as the key to understanding an organization's capabilities. In today's large organizations it has become increasingly difficult to find people that have specific skills or expertise or to explore and understand the overall picture of an organization's portfolio of topic expertise. This article presents a case study of analyzing and visualizing such expertise with the goal of enabling human users to assess and quickly find people with a desired skill set. Our approach is based on techniques like n-grams, clustering, and visualization for improving the user search experience for people and skill.

#index 1392494
#* Experimental results of the signal processing approach to distributional clustering of terms on reuters-21578 collection
#@ Marta Capdevila Dalmau;Oscar W. Márquez Flórez
#t 2007
#c 16
#% 262059
#% 344447
#% 722934
#% 926881
#! Distributional Clustering has showed to be an effective and powerful approach to supervised term extraction aimed at reducing the original indexing space dimensionality for Automatic Text Categorization [2]. In a recent paper [1] we introduced a new Signal Processing approach to Distributional Clustering which reached categorization results on 20 Newsgroups dataset similar to those obtained by other information-theoretic approaches [3][4][5]. Here we re-validate our method by showing that the 90-categories Reuters-21578 benchmark collection can be indexed with a minimum loss of categorization accuracy (around 2% with Naïve Bayes categorizer) with only 50 clusters.

#index 1392495
#* Overall comparison at the standard levels of recall of multiple retrieval methods with the Friedman test
#@ José M. Casanova;Manuel A. Presedo Quindimil;Álvaro Barreiro
#t 2007
#c 16
#% 144074
#% 208934
#% 262067
#% 262102
#% 750863
#% 818222
#% 1192696
#! We propose a new application of the Friedman statistical test of significance to compare multiple retrieval methods. After measuring the average precision at the eleven standard levels of recall, our application of the Friedman test provides a global comparison of the methods. In some experiments this test provides additional and useful information to decide if methods are different.

#index 1392496
#* Building a desktop search test-bed
#@ Sergey Chernov;Pavel Serdyukov;Paul-Alexandru Chirita;Gianluca Demartini;Wolfgang Nejdl
#t 2007
#c 16
#% 818259
#% 869508
#% 869510
#% 869536
#% 879565
#! In the last years several top-quality papers utilized temporary Desktop data and/or browsing activity logs for experimental evaluation. Building a common testbed for the Personal Information Management community is thus becoming an indispensable task. In this paper we present a possible dataset design and discuss the means to create it.

#index 1392497
#* Hierarchical browsing of video key frames
#@ Gianluigi Ciocca;Raimondo Schettini
#t 2007
#c 16
#% 92148
#% 435081
#! We propose an innovative, general purpose, method to the selection and hierarchical representation of key frames of a video sequence for video summarization. It is able to create a hierarchical storyboard that the user may easily browse. The method is composed by three different steps. The first removes meaningless key frames, using supervised classification performed by a neural network on the basis of pictorial features and a visual attention model algorithm. The second step provides for the grouping of the key frames into clusters to allow multilevel summary using both low level and high level features. The third step identifies the default summary level that is shown to the users: starting from this set of key frames, the users can then browse the video content at different levels of detail

#index 1392498
#* Active learning with history-based query selection for text categorisation
#@ Michael Davy;Saturnino Luz
#t 2007
#c 16
#% 169717
#% 344447
#% 376266
#% 466419
#% 565531
#! Automated text categorisation systems learn a generalised hypothesis from large numbers of labelled examples. However, in many domains labelled data is scarce and expensive to obtain. Active learning is a technique that has shown to reduce the amount of training data required to produce an accurate hypothesis. This paper proposes a novel method of incorporating predictions made in previous iterations of active learning into the selection of informative unlabelled examples. We show empirically how this method can lead to increased classification accuracy compared to alternative techniques.

#index 1392499
#* Fighting link spam with a two-stage ranking strategy
#@ Guang-Gang Geng;Chun-Heng Wang;Qiu-Dan Li;Yuan-Ping Zhu
#t 2007
#c 16
#% 818225
#% 869470
#% 869471
#! Most of the existing combating web spam techniques focus on the spam detection itself, which are separated from the ranking process. In this paper, we propose a two-stage ranking strategy, which makes good use of hyperlink information among Websites and Website's intra structure information. The proposed method incorporates web spam detection into the ranking process and penalizes the ranking score of potential spam pages, instead of removing them arbitrarily. Preliminary experimental results show that our method is feasible and effective.

#index 1392500
#* Improving naive Bayes text classifier using smoothing methods
#@ Feng He;Xiaoqing Ding
#t 2007
#c 16
#% 344447
#% 465754
#% 748738
#! The performance of naive Bayes text classifier is greatly influenced by parameter estimation, while the large vocabulary and scarce labeled training set bring difficulty in parameter estimation. In this paper, several smoothing methods are introduced to estimate parameters in naive Bayes text classifier. The proposed approaches can achieve better and more stable performance than Laplace smoothing

#index 1392501
#* Term selection and query operations for video retrieval
#@ Bouke Huurnink;Maarten De Rijke
#t 2007
#c 16
#% 248218
#% 732845
#% 732848
#% 737423
#% 780818
#% 1715627
#! We investigate the influence of term selection and query operations on the text retrieval component of video search. Our main finding is that the greatest gain is to be found in the combination of character n-grams, stemmed text, and proximity terms.

#index 1392502
#* An effective threshold-based neighbor selection in collaborative filtering
#@ Taek-Hun Kim;Sung-Bong Yang
#t 2007
#c 16
#% 301259
#% 1650321
#% 1650569
#! In this paper we present a recommender system using an effective threshold-based neighbor selection in collaborative filtering. The proposed method uses the substitute neighbors of the test customer who may have an unusual preferences or who are the first rater. The experimental results show that the recommender systems using the proposed method find the proper neighbors and give a good prediction quality.

#index 1392503
#* Combining multiple sources of evidence in XML multimedia documents: an inference network incorporating element language models
#@ Zhigang Kong;Mounia Lalmas
#t 2007
#c 16
#% 111303
#% 789959
#% 1674754
#! This work makes use of the semantic structure and logical structure in XML documents, and their combination to represent and retrieve XML multimedia content. We develop a Bayesian network incorporating element language models for the retrieval of a mixture of text and image. In addition, an element-based collection language model is used in the element language model smoothing. The proposed approach was successfully evaluated on the INEX 2005 multimedia data set.

#index 1392504
#* Language model based query classification
#@ Andreas Merkel;Dietrich Klakow
#t 2007
#c 16
#% 262096
#% 340948
#% 815303
#! In this paper we propose a new way of using language models in query classification for question answering systems. We used a Bayes classifier as classification paradigm. Experimental results show that our approach outperforms current classification methods like Naive Bayes and SVM.

#index 1392505
#* Integration of text and audio features for genre classification in music information retrieval
#@ Robert Neumayer;Andreas Rauber
#t 2007
#c 16
#% 267568
#% 741122
#% 741169
#% 839931
#! Multimedia content can be described in versatile ways as its essence is not limited to one view. For music data these multiple views could be a song's audio features as well as its lyrics. Both of these modalities have their advantages as text may be easier to search in and could cover more of the 'content semantics' of a song, while omitting other types of semantic categorisation. (Psycho) acoustic feature sets, on the other hand, provide the means to identify tracks that 'sound similar' while less supporting other kinds of semantic categorisation. Those discerning characteristics of different feature sets meet users' differing information needs. We will explain the nature of text and audio feature sets which describe the same audio tracks. Moreover, we will propose the use of textual data on top of low level audio features for music genre classification. Further, we will show the impact of different combinations of audio features and textual features based on content words.

#index 1392506
#* Retrieval method for video content in different format based on spatiotemporal features
#@ Xuefeng Pan;Jintao Li;Yongdong Zhang;Sheng Tang;Juan Cao
#t 2007
#c 16
#% 1775172
#% 1858257
#! In this paper a robust video content retrieval method based on spatiotemporal features is proposed. To date, most video retrieval methods are using the character of video key frames. This kind of frame based methods is not robust enough for different video format. With our method, the temporal variation of visual information is presented using spatiotemporal slice. Then the DCT is used to extract feature of slice. With this kind of feature, a robust video content retrieval algorithm is developed. The experiment results show that the proposed feature is robust for variant video format.

#index 1392507
#* Combination of document priors in web information retrieval
#@ Jie Peng;Iadh Ounis
#t 2007
#c 16
#% 309150
#% 397126
#! Query independent features (also called document priors), such as the number of incoming links to a document, its PageRank, or the length of its associated URL, have been explored to boost the retrieval effectiveness of Web Information Retrieval (IR) systems. The combination of such query independent features could further enhance the retrieval performance. However, most current combination approaches are based on heuristics, which ignore the possible dependence between the document priors. In this paper, we present a novel and robust method for combining document priors in a principled way. We use a conditional probability rule, which is derived from Kolmogorov's axioms. In particular, we investigate the retrieval performance attainable by our combination of priors method, in comparison to the use of single priors and a heuristic prior combination method. Furthermore, we examine when and how document priors should be combined.

#index 1392508
#* Enhancing expert search through query modeling
#@ Pavel Serdyukov;Sergey Chernov;Wolfgang Nejdl
#t 2007
#c 16
#% 342707
#% 768898
#% 879570
#! An expert finding is a very common task among enterprise search activities, while its usual retrieval performance is far from the quality of the Web search. Query modeling helps to improve traditional document retrieval, so we propose to apply it in a new setting. We adopt a general framework of language modeling for expert finding. We show how expert language models can be used for advanced query modeling. A preliminary experimental evaluation on TREC Enterprise Track 2006 collection shows that our method improves the retrieval precision on the expert finding task.

#index 1392509
#* A hierarchical consensus architecture for robust document clustering
#@ Xavier Sevillano;Germán Cobo;Francesc Alías;Joan Claudi Socoró
#t 2007
#c 16
#% 344447
#% 722902
#! A major problem encountered by text clustering practitioners is the difficulty of determining a priori which is the optimal text representation and clustering technique for a given clustering problem. As a step towards building robust document partitioning systems, we present a strategy based on a hierarchical consensus clustering architecture that operates on a wide diversity of document representations and partitions. The conducted experiments show that the proposed method is capable of yielding a consensus clustering that is comparable to the best individual clustering available even in the presence of a large number of poor individual labelings, outperforming classic nonhierarchical consensus approaches in terms of performance and computational cost.

#index 1392510
#* Summarisation and novelty: an experimental investigation
#@ Simon Sweeney;Fabio Crestani;David E. Losada
#t 2007
#c 16
#% 346551
#% 643014
#% 874712

#index 1392511
#* A layered approach to context-dependent user modelling
#@ Elena Vildjiounaite;Sanna Kallio
#t 2007
#c 16
#% 799294
#% 801785
#! This works presents a method for explicit acquisition of context-dependent user preferences (preferences which change depending on a user situation, e.g., higher interest in outdoor activities if it is sunny than if it is raining) for Smart Home - intelligent environment, which recognises contexts of its inhabitants (such as presence of people, activities, events, weather etc) via home and mobile devices and provides personalized proactive support to the users. Since a set of personally important situations, which affect user preferences, is user-dependent, and since many situations can be described only in fuzzy terms, we provide users with an easy way to develop personal context ontology and to map it fuzzily into common ontology via GUI. Backward mapping, by estimating the probability of occurrence of a user-defined situation, allows retrieval of preferences from all components of the user model.

#index 1392512
#* A Bayesian approach for learning document type relevance
#@ Peter C. K. Yeung;Stefan Büttcher;Charles L. A. Clarke;Maheedhar Kolla
#t 2007
#c 16
#% 783474
#! Retrieval accuracy can be improved by considering which document type should be filtered out and which should be ranked higher in the result list. Hence, document type can be used as a key factor for building a re-ranking retrieval model. We take a simple approach for considering document type in the retrieval process. We adapt the BM25 scoring function to weight term frequency based on the document type and take the Bayesian approach to estimate the appropriate weight for each type. Experimental results show that our approach improves on search precision by as much as 19%.

#index 1415705
#* Proceedings of the IR research, 30th European conference on Advances in information retrieval
#@ Craig Macdonald;Iadh Ounis;Vassilis Plachouras;Ian Ruthven;Ryen W. White
#t 2008
#c 16

#index 1415706
#* Some(what) grand challenges for information retrieval
#@ Nicholas J. Belkin
#t 2008
#c 16

#index 1415707
#* Web search: challenges and directions
#@ Amit Singhal
#t 2008
#c 16

#index 1415708
#* You are a document too: web mining and IR for next-generation information literacy
#@ Bettina Berendt
#t 2008
#c 16

#index 1415709
#* Discounted cumulated gain based evaluation of multiple-query IR sessions
#@ Kalervo Järvelin;Susan L. Price;Lois M. L. Delcambre;Marianne Lykke Nielsen
#t 2008
#c 16
#% 208935
#% 309095
#% 340892
#% 411762
#% 835027
#% 1019103
#% 1348075
#! IR research has a strong tradition of laboratory evaluation of systems. Such research is based on test collections, pre-defined test topics, and standard evaluation metrics. While recent research has emphasized the user viewpoint by proposing user-based metrics and non-binary relevance assessments, the methods are insufficient for truly user-based evaluation. The common assumption of a single query per topic and session poorly represents real life. On the other hand, one well-known metric for multiple queries per session, instance recall, does not capture early (within session) retrieval of (highly) relevant documents. We propose an extension to the Discounted Cumulated Gain (DCG) metric, the Session-based DCG (sDCG) metric for evaluation scenarios involving multiple query sessions, graded relevance assessments, and open-ended user effort including decisions to stop searching. The sDCG metric discounts relevant results from later queries within a session. We exemplify the sDCG metric with data from an interactive experiment, we discuss how the metric might be applied, and we present research questions for which the metric is helpful.

#index 1415710
#* Here or there: preference judgments for relevance
#@ Ben Carterette;Paul N. Bennett;David Maxwell Chickering;Susan T. Dumais
#t 2008
#c 16
#% 100008
#% 262105
#% 309095
#% 577224
#% 766409
#% 818221
#% 840846
#% 879598
#% 879655
#% 1783188
#! Information retrieval systems have traditionally been evaluated over absolute judgments of relevance: each document is judged for relevance on its own, independent of other documents that may be on topic. We hypothesize that preference judgments of the form "document A is more relevant than document B" are easier for assessors to make than absolute judgments, and provide evidence for our hypothesis through a study with assessors. We then investigate methods to evaluate search engines using preference judgments. Furthermore, we show that by using inferences and clever selection of pairs to judge, we need not compare all pairs of documents in order to apply evaluation methods.

#index 1415711
#* Using clicks as implicit judgments: expectations versus observations
#@ Falk Scholer;Milad Shokouhi;Bodo Billerbeck;Andrew Turpin
#t 2008
#c 16
#% 478627
#% 577224
#% 729027
#% 805200
#% 818221
#% 818257
#% 823348
#% 857180
#% 879561
#% 879565
#% 879566
#% 879567
#% 946521
#% 987189
#% 987208
#% 987222
#% 989628
#! Clickthrough data has been the subject of increasing popularity as an implicit indicator of user feedback. Previous analysis has suggested that user click behaviour is subject to a quality bias--that is, users click at different rank positions when viewing effective search results than when viewing less effective search results. Based on this observation, it should be possible to use click data to infer the quality of the underlying search system. In this paper we carry out a user study to systematically investigate how click behaviour changes for different levels of search system effectiveness as measured by information retrieval performance metrics. Our results show that click behaviour does not vary systematically with the quality of search results. However, click behaviour does vary significantly between individual users, and between search topics. This suggests that using direct click behaviour--click rank and click frequency--to infer the quality of the underlying search system is problematic. Further analysis of our user click data indicates that the correspondence between clicks in a search result list and subsequent confirmation that the clicked resource is actually relevant is low. Using clicks as an implicit indication of relevance should therefore be done with caution.

#index 1415712
#* Clustering template based web documents
#@ Thomas Gottron
#t 2008
#c 16
#% 255137
#% 348180
#% 534048
#% 577281
#% 729939
#% 729974
#% 730038
#% 754108
#% 807298
#% 810759
#% 894509
#% 936239
#% 939562
#% 956499
#! More and more documents on theWorldWideWeb are based on templates. On a technical level this causes those documents to have a quite similar source code and DOM tree structure. Grouping together documents which are based on the same template is an important task for applications that analyse the template structure and need clean training data. This paper develops and compares several distance measures for clustering web documents according to their underlying templates. Combining those distance measures with different approaches for clustering, we show which combination of methods leads to the desired result.

#index 1415713
#* Effective pre-retrieval query performance prediction using similarity and variability evidence
#@ Ying Zhao;Falk Scholer;Yohannes Tsegay
#t 2008
#c 16
#% 51237
#% 290703
#% 324129
#% 397161
#% 590523
#% 729027
#% 750863
#% 766497
#% 804915
#% 818267
#% 850131
#% 857180
#% 867054
#% 907544
#% 944349
#% 987260
#% 1192696
#! Query performance prediction aims to estimate the quality of answers that a search system will return in response to a particular query. In this paper we propose a new family of pre-retrieval predictors based on information at both the collection and document level. Pre-retrieval predictors are important because they can be calculated from information that is available at indexing time; they are therefore more efficient than predictors that incorporate information obtained from actual search results. Experimental evaluation of our approach shows that the new predictors give more consistent performance than previously proposed pre-retrieval methods across a variety of data types and search tasks.

#index 1415714
#* iCluster: a self-organizing overlay network for P2P information retrieval
#@ Paraskevi Raftopoulou;Euripides G. M. Petrakis
#t 2008
#c 16
#% 340176
#% 348182
#% 449870
#% 636008
#% 730035
#% 737424
#% 779475
#% 821737
#% 902713
#% 943040
#% 1386265
#! We present iCluster, a self-organizing peer-to-peer overlay network for supporting full-fledged information retrieval in a dynamic environment. iCluster works by organizing peers sharing common interests into clusters and by exploiting clustering information at query time for achieving low network traffic and high recall. We define the criteria for peer similarity and peer selection, and we present the protocols for organizing the peers into clusters and for searching within the clustered organization of peers. iCluster is evaluated on a realistic peer-to-peer environment using real-world data and queries. The results demonstrate significant performance improvements (in terms of clustering efficiency, communication load and retrieval accuracy) over a state-of-the-art peer-to-peer clustering method. Compared to exhaustive search by flooding, iCluster exchanged a small loss in retrieval accuracy for much less message flow.

#index 1415715
#* Labeling categories and relationships in an evolving social network
#@ Ming-Shun Lin;Hsin-Hsi Chen
#t 2008
#c 16
#% 290830
#% 805849
#% 807295
#% 869502
#% 869503
#% 939627
#% 1275206
#! Modeling and naming general entity-entity relationships is challenging in construction of social networks. Given a seed denoting a person name, we utilize Google search engine, NER (Named Entity Recognizer) parser, and CODC (Co-Occurrence Double Check) formula to construct an evolving social network. For each entity pair in the network, we try to label their categories and relationships. Firstly, we utilize an open directory project (ODP) resource, which is the largest human-edited directory of the web, to build a directed graph, and then use three ranking algorithms, PageRank, HITS, and a Markov chain random process to extract potential categories defined in the ODP. These categories capture the major contexts of the designated named entities. Finally, we combine the ranks of these categories and tf*idf scores of noun phrases to extract relationships. In our experiments, total 6 evolving social networks with 618 pairs of named entities demonstrate that the Markov chain random process is better than the other two algorithms.

#index 1415716
#* Automatic construction of an opinion-term vocabulary for ad hoc retrieval
#@ Giambattista Amati;Edgardo Ambrosi;Marco Bianchi;Carlo Gaibisso;Giorgio Gambosi
#t 2008
#c 16
#% 218978
#% 340934
#% 746885
#% 748499
#% 815915
#% 854646
#% 855279
#% 939897
#% 1261565
#% 1392464
#% 1715628
#% 1721869
#% 1742070
#! We present a method to automatically generate a term-opinion lexicon. We also weight these lexicon terms and use them at real time to boost the ranking with opinionated-content documents. We define very simple models both for opinion-term extraction and document ranking. Both the lexicon model and retrieval model are assessed. To evaluate the quality of the lexicon we compare performance with a well-established manually generated opinion-term dictionary. We evaluate the effectiveness of the term-opinion lexicon using the opinion task evaluation data of the TREC 2007 blog track.

#index 1415717
#* A comparison of social bookmarking with traditional search
#@ Beate Krause;Andreas Hotho;Gerd Stumme
#t 2008
#c 16
#% 590523
#% 765412
#% 805839
#% 855601
#% 878624
#% 879738
#% 956509
#% 956544
#% 967260
#% 1667787
#! Social bookmarking systems allow users to store links to internet resources on a web page. As social bookmarking systems are growing in popularity, search algorithms have been developed that transfer the idea of link-based rankings in the Web to a social bookmarking system's data structure. These rankings differ from traditional search engine rankings in that they incorporate the rating of users. In this study, we compare search in social bookmarking systems with traditionalWeb search. In the first part, we compare the user activity and behaviour in both kinds of systems, as well as the overlap of the underlying sets of URLs. In the second part,we compare graph-based and vector space rankings for social bookmarking systems with commercial search engine rankings. Our experiments are performed on data of the social bookmarking system Del.icio.us and on rankings and log data from Google, MSN, and AOL. We will show that part of the difference between the systems is due to different behaviour (e. g., the concatenation of multi-word lexems to single terms in Del.icio.us), and that real-world events may trigger similar behaviour in both kinds of systems. We will also show that a graph-based ranking approach on folksonomies yields results that are closer to the rankings of the commercial search engines than vector space retrieval, and that the correlation is high in particular for the domains that are well covered by the social bookmarking system.

#index 1415718
#* Effects of aligned corpus quality and size in corpus-based CLIR
#@ Tuomas Talvensaari
#t 2008
#c 16
#% 218982
#% 218989
#% 262046
#% 281251
#% 340966
#% 397143
#% 397217
#% 420520
#% 807747
#% 810907
#% 919705
#% 939575
#% 1072567
#% 1100330
#! Aligned corpora are often-used resources in CLIR systems. The three qualities of translation corpora that most dramatically affect the performance of a corpus-based CLIR system are: (1) topical nearness to the translated queries, (2) the quality of the alignments, and (3) the size of the corpus. In this paper, the effects of these factors are studied and evaluated. Topics of two different domains (news and genomics) are translated with corpora of varying alignment quality, ranging from a clean parallel corpus to noisier comparable corpora. Also, the sizes of the corpora are varied. The results show that of the three qualities, topical nearness is the most crucial factor, outweighing both other factors. This indicates that noisy comparable corpora should be used as complimentary resources, when parallel corpora are not available for the domain in question.

#index 1415719
#* Exploring the effects of language skills on multilingual web search
#@ Jennifer Marlow;Paul Clough;Juan Cigarrán Recuero;Javier Artiles
#t 2008
#c 16
#% 811383
#% 859465
#% 1674925
#% 1916062
#! Multilingual access is an important area of research, especially given the growth in multilingual users of online resources. A large body of research exists for Cross-Language Information Retrieval (CLIR); however, little of this work has considered the language skills of the end user, a critical factor in providing effective multilingual search functionality. In this paper we describe an experiment carried out to further understand the effects of language skills on multilingual search. Using the Google Translate service, we show that users have varied language skills that are non-trivial to assess and can impact their multilingual searching experience and search effectiveness.

#index 1415720
#* A novel implementation of the FITE-TRT translation method
#@ Aki Loponen;Ari Pirkola;Kalervo Järvelin;Heikki Keskustalo
#t 2008
#c 16
#% 219033
#% 333679
#% 643018
#% 874256
#! Cross-language Information Retrieval requires good methods for translating cross-lingual spelling variants which are not covered by the available dictionary resources. FITE-TRT is an established method employing frequency-based identification of translation equivalents received from transformation rule based translation. This study further develops and evaluates the FITE-TRT method. The paper contributes on four areas. First, an efficient implementation for the FITE-TRT method is discussed. Secondly, a novel iterative FITE-TRT translation approach is developed in order to further improve the effectiveness of the method. Thirdly, the effectiveness of FITE-TRT is assessed in three classes of source-target word similarity. FITE-TRT was found to be very strong in the class of the most similar source and target words and only becomes unsuccessful when the words were dissimilar. Fourthly, in comparison to n-gram and s-gram matching methods, FITE-TRT is shown consistently stronger. All in all, FITE-TRT clearly outperforms the fuzzy string matching methods under comparable conditions. Therefore it is the method of choice for the identification of translation equivalents of crosslingual spelling variants when the requirements for the result quality are high.

#index 1415721
#* The BNB distribution for text modeling
#@ Stéphane Clinchant;Eric Gaussier
#t 2008
#c 16
#% 262096
#% 411760
#% 742513
#% 840903
#% 875981
#! We first review in this paper the burstiness and aftereffect of future sampling phenomena, and propose a formal, operational criterion to characterize distributions according to these phenomena. We then introduce the Beta negative binomial distribution for text modeling, and show its relations to several models (in particular to the Laplace law of succession and to the tf-itf model used in the Divergence from Randomness framework of [2]). We finally illustrate the behavior of this distribution on text categorization and information retrieval experiments.

#index 1415722
#* Utilizing passage-based language models for document retrieval
#@ Michael Bendersky;Oren Kurland
#t 2008
#c 16
#% 144011
#% 169809
#% 169811
#% 169813
#% 232677
#% 262096
#% 328532
#% 340901
#% 340948
#% 413592
#% 719598
#% 766431
#% 766464
#% 818204
#% 818241
#% 827581
#% 939939
#% 1674724
#% 1721867
#! We show that several previously proposed passage-based document ranking principles, along with some new ones, can be derived from the same probabilistic model. We use language models to instantiate specific algorithms, and propose a passage language model that integrates information from the ambient document to an extent controlled by the estimated document homogeneity. Several document-homogeneity measures that we propose yield passage language models that are more effective than the standard passage model for basic document retrieval and for constructing and utilizing passage-based relevance models; the latter outperform a document-based relevance model. We also show that the homogeneity measures are effective means for integrating document-query and passage-query similarity information for document retrieval.

#index 1415723
#* A statistical view of binned retrieval models
#@ Donald Metzler;Trevor Strohman;W. Bruce Croft
#t 2008
#c 16
#% 169781
#% 227815
#% 262096
#% 280851
#% 287253
#% 340887
#% 340901
#% 340948
#% 342707
#% 411760
#% 642992
#% 766414
#% 766429
#% 766430
#% 766431
#% 818230
#% 818239
#% 818263
#% 879584
#% 879585
#% 879587
#% 879611
#% 879650
#% 907504
#! Many traditional information retrieval models, such as BM25 and language modeling, give good retrieval effectiveness, but can be difficult to implement efficiently. Recently, document-centric impact models were developed in order to overcome some of these efficiency issues. However, such models have a number of problems, including poor effectiveness, and heuristic term weighting schemes. In this work, we present a statistical view of document-centric impact models. We describe how such models can be treated statistically and propose a supervised parameter estimation technique. We analyze various theoretical and practical aspects of the model and show that weights estimated using our new estimation technique are significantly better than the integer-based weights used in previous studies.

#index 1415724
#* Video corpus annotation using active learning
#@ Stéphane Ayache;Georges Quénot
#t 2008
#c 16
#% 169717
#% 450951
#% 451056
#% 780820
#% 789792
#% 840003
#% 874673
#% 884196
#% 888942
#% 903632
#% 992174
#! Concept indexing in multimedia libraries is very useful for users searching and browsing but it is a very challenging research problem as well. Beyond the systems' implementations issues, semantic indexing is strongly dependent upon the size and quality of the training examples. In this paper, we describe the collaborative annotation system used to annotate the High Level Features (HLF) in the development set of TRECVID 2007. This system is web-based and takes advantage of Active Learning approach. We show that Active Learning allows simultaneously getting the most useful information from the partial annotation and significantly reducing the annotation effort per participant relatively to previous collaborative annotations.

#index 1415725
#* Use of implicit graph for recommending relevant videos: a simulated evaluation
#@ David Vallet;Frank Hopfgartner;Joemon Jose
#t 2008
#c 16
#% 731615
#% 766454
#% 840424
#% 987212
#% 987222
#% 996182
#! In this paper, we propose a model for exploiting community based usage information for video retrieval. Implicit usage information from a pool of past users could be a valuable source to address the difficulties caused due to the semantic gap problem. We propose a graph-based implicit feedback model in which all the usage information can be represented. A number of recommendation algorithms were suggested and experimented. A simulated user evaluation is conducted on the TREC VID collection and the results are presented. Analyzing the results we found some common characteristics on the best performing algorithms, which could indicate the best way of exploiting this type of usage information.

#index 1415726
#* Using terms from citations for IR: some first results
#@ Anna Ritchie;Simone Teufel;Stephen Robertson
#t 2008
#c 16
#% 142615
#% 232912
#% 268079
#% 309145
#% 940040
#% 987287
#% 987331
#% 1223761
#% 1261537
#% 1278510
#% 1532585
#% 1532611
#% 1532616
#! We present the results of experiments using terms from citations for scientific literature search. To index a given document, we use terms used by citing documents to describe that document, in combination with terms from the document itself. We find that the combination of terms gives better retrieval performance than standard indexing of the document terms alone and present a brief analysis of our results. This paper marks the first experimental results from a new test collection of scientific papers, created by us in order to study citation-based methods for IR.

#index 1415727
#* Automatic extraction of domain-specific stopwords from labeled documents
#@ Masoud Makrehchi;Mohamed S. Kamel
#t 2008
#c 16
#% 266215
#% 286307
#% 375017
#% 413637
#% 465754
#% 465895
#% 466078
#% 585752
#% 608320
#% 722935
#% 724582
#% 724607
#% 762313
#% 763708
#% 770778
#% 785544
#% 818219
#% 824375
#! Automatic extraction of domain-specific stopword list from a large labeled corpus is discussed. Most researches remove the stopwords using a standard stopword list, and high and low document frequencies. In this paper, a new approach for stopword extraction based on the notion of backward filter level performance and sparsity measure of training data, is proposed. First, we discuss the motivation for updating existing lists or building new ones. Second, based on the proposed backward filter-level performance, we examine the effectiveness of high document frequency filtering for stopword reduction. Finally, a new method for building general and domain-specific stopwords is proposed. The method assumes that a set of candidate stopwords must have minimum information content and prediction capacity, which can be estimated by a classifier performance. The proposed approach is extensively compared with other methods including inverse document frequency and information gain. According to the comparative study, the proposed approach offers more promising results, which guarantee minimum information loss by filtering out most stopwords.

#index 1415728
#* Book search experiments: investigating IR methods for the indexing and retrieval of books
#@ Hengzhi Wu;Gabriella Kazai;Michael Taylor
#t 2008
#c 16
#% 169781
#% 213786
#% 411762
#% 745450
#% 766417
#% 768898
#% 783474
#% 857180
#% 867054
#% 879609
#% 1035577
#! Through mass-digitization projects and with the use of OCR technologies, digitized books are becoming available on the Web and in digital libraries. The unprecedented scale of these efforts, the unique characteristics of the digitized material as well as the unexplored possibilities of user interactions make full-text book search an exciting area of information retrieval (IR) research. Emerging research questions include: How appropriate and effective are traditional IR models when applied to books? What book specific features (e.g., back-of-book index) should receive special attention during the indexing and retrieval processes? How can we tackle scalability? In order to answer such questions, we developed an experimental platform to facilitate rapid prototyping of a book search system as well as to support large-scale tests. Using this system, we performed experiments on a collection of 10 000 books, evaluating the efficiency of a novel multi-field inverted index and the effectiveness of the BM25F retrieval model adapted to books, using book-specific fields.

#index 1415729
#* Using a task-based approach in evaluating the usability of BoBIs in an e-book environment
#@ Noorhidawati Abdullah;Forbes Gibb
#t 2008
#c 16
#% 56449
#% 109328
#% 127576
#% 928134
#% 934297
#! This paper reports on a usability evaluation of BoBIs (Back-of-the-book Indexes) as searching and browsing tools in an e-book environment. This study employed a task-based approach and within-subject design. The retrieval performance of a BoBI was compared with a ToC and Full-Text Search tool in terms of their respective effectiveness and efficiency for finding information in e-books. The results demonstrated that a BoBI was significantly more efficient (faster) and useful compared to a ToC or Full-Text Search tool for finding information in an e-book environment.

#index 1415730
#* Exploiting locality of Wikipedia links in entity ranking
#@ Jovan Pehcevski;Anne-Marie Vercoustre;James A. Thom
#t 2008
#c 16
#% 268079
#% 290830
#% 413612
#% 766462
#% 803543
#% 878916
#% 879576
#% 987276
#! Information retrieval from web and XML document collections ever more focused on returning entities instead of web pages or XML elements. There are many research fields involving named entities; one such field is known as entity ranking, where one goal is to rank entities in response to a query supported with a short list of entity examples. In this paper, we describe our approach to ranking entities from the Wikipedia XML document collection. Our approach utilises the known categories and the link structure of Wikipedia, and more importantly, exploits link co-occurrences to improve the effectiveness of entity ranking. Using the broad context of a full Wikipedia page as a baseline, we evaluate two different algorithms for identifying narrow contexts around the entity examples: one that uses predefined types of elements such as paragraphs, lists and tables; and another that dynamically identifies the contexts by utilising the underlying XML document structure. Our experiments demonstrate that the locality of Wikipedia links can be exploited to significantly improve the effectiveness of entity ranking.

#index 1415731
#* The importance of link evidence in Wikipedia
#@ Jaap Kamps;Marijn Koolen
#t 2008
#c 16
#% 283833
#% 290830
#% 397126
#% 590523
#% 642992
#% 838460
#% 878916
#! Wikipedia is one of the most popular information sources on the Web. The free encyclopedia is densely linked. The link structure in Wikipedia differs from the Web at large: internal links in Wikipedia are typically based on words naturally occurring in a page, and link to another semantically related entry. Our main aim is to find out if Wikipedia's link structure can be exploited to improve ad hoc information retrieval. We first analyse the relation between Wikipedia links and the relevance of pages. We then experiment with use of link evidence in the focused retrieval of Wikipedia content, based on the test collection of INEX 2006. Our main findings are: First, our analysis of the link structure reveals that the Wikipedia link structure is a (possibly weak) indicator of relevance. Second, our experiments on INEX ad hoc retrieval tasks reveal that if the link evidence is made sensitive to the local context we see a significant improvement of retrieval effectiveness. Hence, in contrast with earlier TREC experiments using crawled Web data, we have shown that Wikipedia's link structure can help improve the effectiveness of ad hoc retrieval.

#index 1415732
#* High quality expertise evidence for expert search
#@ Craig Macdonald;David Hannah;Iadh Ounis
#t 2008
#c 16
#% 397126
#% 577339
#% 768904
#% 818255
#% 838464
#% 869649
#% 879570
#% 907525
#% 913206
#% 1019093
#% 1089461
#% 1392465
#% 1392466
#% 1392507
#% 1415760
#% 1715628
#% 1742070
#! In an Enterprise setting, an expert search system can assist users with their "expertise need" by suggesting people with relevant expertise to the topic of interest. These systems typically work by associating documentary evidence of expertise to each candidate expert, and then ranking the candidates by the extent to which the documents in their profile are about the query. There are three important factors that affect the retrieval performance of an expert search system - firstly, the selection of the candidate profiles (the documents associated with each candidate), secondly, how the topicality of the documents is measured, and thirdly how the evidence of expertise from the associated documents is combined. In this work, we investigate a new dimension to expert finding, namely whether some documents are better indicators of expertise than others in each candidate's profile. We apply five techniques to predict the quality documents in candidate profiles, which are likely to be good indicators of expertise. The techniques applied include the identification of possible candidate homepages, and of clustering the documents in each profile to determine the candidate's main areas of expertise. The proposed approaches are evaluated on three expert search task from recent TREC Enterprise tracks and provide conclusions.

#index 1415733
#* Associating people and documents
#@ Krisztian Balog;Maarten De Rijke
#t 2008
#c 16
#% 340948
#% 879570
#% 907525
#% 913206
#% 1392465
#! Since the introduction of the Enterprise Track at TREC in 2005, the task of finding experts has generated a lot of interest within the research community. Numerous models have been proposed that rank candidates by their level of expertise with respect to some topic. Common to all approaches is a component that estimates the strength of the association between a document and a person. Forming such associations, then, is a key ingredient in expertise search models. In this paper we introduce and compare a number of methods for building document-people associations. Moreover, we make underlying assumptions explicit, and examine two in detail: (i) independence of candidates, and (ii) frequency is an indication of strength. We show that our refined ways of estimating the strength of associations between people and documents leads to significant improvements over the state-of-the-art in the end-to-end expert finding task.

#index 1415734
#* Modeling documents as mixtures of persons for expert finding
#@ Pavel Serdyukov;Djoerd Hiemstra
#t 2008
#c 16
#% 260775
#% 269954
#% 280819
#% 290830
#% 340901
#% 342707
#% 730082
#% 838464
#% 869649
#% 879570
#% 907525
#% 913206
#% 956516
#% 987261
#% 1019189
#% 1392465
#% 1392466
#% 1392508
#! In this paper we address the problem of searching for knowledgeable persons within the enterprise, known as the expert finding (or expert search) task. We present a probabilistic algorithm using the assumption that terms in documents are produced by people who are mentioned in them.We represent documents retrieved to a query as mixtures of candidate experts language models. Two methods of personal language models extraction are proposed, as well as the way of combining them with other evidences of expertise. Experiments conducted with the TREC Enterprise collection demonstrate the superiority of our approach in comparison with the best one among existing solutions.

#index 1415735
#* Ranking users for intelligent message addressing
#@ Vitor R. Carvalho;William W. Cohen
#t 2008
#c 16
#% 46803
#% 280817
#% 340936
#% 387427
#% 465895
#% 642992
#% 662755
#% 730082
#% 879570
#% 907525
#% 1392465
#! Finding persons who are knowledgeable on a given topic (i.e. Expert Search) has become an active area of recent research [1, 2, 3]. In this paper we investigate the related task of Intelligent Message Addressing, i.e., finding persons who are potential recipients of a message under composition given its current contents, its previously-specified recipients or a few initial letters of the intended recipient contact (intelligent auto-completion). We begin by providing quantitative evidence, from a very large corpus, of how frequently email users are subject to message addressing problems. We then propose several techniques for this task, including adaptations of well-known formal models of Expert Search. Surprisingly, a simple model based on the K-Nearest-Neighbors algorithm consistently outperformed all other methods. We also investigated combinations of the proposed methods using fusion techniques, which leaded to significant performance improvements over the baselines models. In auto-completion experiments, the proposed models also outperformed all standard baselines. Overall, the proposed techniques showed ranking performance of more than 0.5 in MRR over 5202 queries from 36 different email users, suggesting intelligent message addressing can be a welcome addition to email.

#index 1415736
#* Facilitating query decomposition in query language modeling by association rule mining using multiple sliding windows
#@ Dawei Song;Qiang Huang;Stefan Rüger;Peter Bruza
#t 2008
#c 16
#% 152934
#% 320944
#% 340899
#% 340901
#% 340948
#% 413592
#% 572500
#% 643014
#% 766428
#% 818240
#% 838530
#% 907543
#% 987231
#% 987341
#% 1072684
#! This paper presents a novel framework to further advance the recent trend of using query decomposition and high-order term relationships in query language modeling, which takes into account terms implicitly associated with different subsets of query terms. Existing approaches, most remarkably the language model based on the Information Flow method are however unable to capture multiple levels of associations and also suffer from a high computational overhead. In this paper, we propose to compute association rules from pseudo feedback documents that are segmented into variable length chunks via multiple sliding windows of different sizes. Extensive experiments have been conducted on various TREC collections and our approach significantly outperforms a baseline Query Likelihood language model, the Relevance Model and the Information Flow model.

#index 1415737
#* Viewing term proximity from a different perspective
#@ Ruihua Song;Michael J. Taylor;Ji-Rong Wen;Hsiao-Wuen Hon;Yong Yu
#t 2008
#c 16
#% 35937
#% 109190
#% 157910
#% 268079
#% 287253
#% 306494
#% 306504
#% 323131
#% 336784
#% 397205
#% 413593
#% 649567
#% 766428
#% 818262
#% 879651
#% 1387547
#% 1715627
#! This paper extends the state-of-the-art probabilistic model BM25 to utilize term proximity from a new perspective. Most previous work only consider dependencies between pairs of terms, and regard phrases as additional independent evidence. It is difficult to estimate the importance of a phrase and its extra contribution to a relevance score, as the phrase actually overlaps with the component terms. This paper proposes a new approach. First, query terms are grouped locally into non-overlapping phrases that may contain one or more query terms. Second, these phrases are not scored independently but are instead treated as providing a context for the component query terms. The relevance contribution of a term occurrence is measured by how many query terms occur in the context phrase and how compact they are. Third, we replace term frequency by the accumulated relevance contribution. Consequently, term proximity is easily integrated into the probabilistic model. Experimental results on TREC-10 and TREC-11 collections show stable improvements in terms of average precision and significant improvements in terms of top precisions.

#index 1415738
#* Extending probabilistic data fusion using sliding windows
#@ David Lillis;Fergus Toolan;Rem Collier;John Dunnion
#t 2008
#c 16
#% 169774
#% 194246
#% 194275
#% 232703
#% 268078
#% 309211
#% 340936
#% 397125
#% 413613
#% 420464
#% 672628
#% 766409
#% 784148
#% 879582
#% 1023307
#% 1392446
#! Recent developments in the field of data fusion have seen a focus on techniques that use training queries to estimate the probability that various documents are relevant to a given query and use that information to assign scores to those documents on which they are subsequently ranked. This paper introduces SlideFuse, which builds on these techniques, introducing a sliding window in order to compensate for situations where little relevance information is available to aid in the estimation of probabilities. SlideFuse is shown to perform favourably in comparison with CombMNZ, ProbFuse and SegFuse. CombMNZ is the standard baseline technique against which data fusion algorithms are compared whereas ProbFuse and SegFuse represent the state-of-the-art for probabilistic data fusion methods.

#index 1415739
#* Semi-supervised document classification with a mislabeling error model
#@ Anastasia Krithara;Massih R. Amini;Jean-Michel Renders;Cyril Goutte
#t 2008
#c 16
#% 252011
#% 280819
#% 311027
#% 397136
#% 397139
#% 458379
#% 466263
#% 722312
#% 1279293
#! This paper investigates a new extension of the Probabilistic Latent Semantic Analysis (PLSA) model [6] for text classification where the training set is partially labeled. The proposed approach iteratively labels the unlabeled documents and estimates the probabilities of its labeling errors. These probabilities are then taken into account in the estimation of the new model parameters before the next round. Our approach outperforms an earlier semi-supervised extension of PLSA introduced by [9] which is based on the use of fake labels. However, it maintains its simplicity and ability to solve multiclass problems. In addition, it gives valuable information about the most uncertain and difficult classes to label. We perform experiments over the 20Newsgroups, WebKB and Reuters document collections and show the effectiveness of our approach over two other semi-supervised algorithms applied to these text classification problems.

#index 1415740
#* Improving term frequency normalization for multi-topical documents and application to language modeling approaches
#@ Seung-Hoon Na;In-Su Kang;Jong-Hyeok Lee
#t 2008
#c 16
#% 169781
#% 218982
#% 262096
#% 328532
#% 340948
#% 766412
#% 818263
#% 987232
#! Term frequency normalization is a serious issue since lengths of documents are various. Generally, documents become long due to two different reasons - verbosity and multi-topicality. First, verbosity means that the same topic is repeatedly mentioned by terms related to the topic, so that term frequency is more increased than the well-summarized one. Second, multi-topicality indicates that a document has a broad discussion of multi-topics, rather than single topic. Although these document characteristics should be differently handled, all previous methods of term frequency normalization have ignored these differences and have used a simplified length-driven approach which decreases the term frequency by only the length of a document, causing an unreasonable penalization. To attack this problem, we propose a novel TF normalization method which is a type of partially-axiomatic approach. We first formulate two formal constraints that the retrieval model should satisfy for documents having verbose and multitopicality characteristic, respectively. Then, we modify language modeling approaches to better satisfy these two constraints, and derive novel smoothing methods. Experimental results show that the proposed method increases significantly the precision for keyword queries, and substantially improves MAP (Mean Average Precision) for verbose queries.

#index 1415741
#* Probabilistic document length priors for language models
#@ Roi Blanco;Alvaro Barreiro
#t 2008
#c 16
#% 169781
#% 218982
#% 262096
#% 268079
#% 375017
#% 397126
#% 641770
#% 730008
#% 750863
#% 766409
#% 783474
#% 818255
#% 857180
#! This paper addresses the issue of devising a new document prior for the language modeling (LM) approach for Information Retrieval. The prior is based on term statistics, derived in a probabilistic fashion and portrays a novel way of considering document length. Furthermore, we developed a new way of combining document length priors with the query likelihood estimation based on the risk of accepting the latter as a score. This prior has been combined with a document retrieval language model that uses Jelinek-Mercer (JM), a smoothing technique which does not take into account document length. The combination of the prior boosts the retrieval performance, so that it outperforms a LM with a document length dependent smoothing component (Dirichlet prior) and other state of the art high-performing scoring function (BM25). Improvements are significant, robust across different collections and query sizes.

#index 1415742
#* Applying maximum entropy to known-item email retrieval
#@ Sirvan Yahyaei;Christof Monz
#t 2008
#c 16
#% 211044
#% 227819
#% 253191
#% 274731
#% 314739
#% 642992
#% 766414
#% 783474
#% 789960
#% 1387547
#! It is becoming increasingly common in information retrieval to combine evidence from multiple resources to compute the retrieval status value of documents. Although this has led to considerable improvements in several retrieval tasks, one of the outstanding issues is estimation of the respective weights that should be associated with the different sources of evidence. In this paper we propose to use maximum entropy in combination with the limited memory LBFG algorithm to estimate feature weights. Examining the effectiveness of our approach with respect to the known-item finding task of enterprise track of TREC shows that it significantly outperforms a standard retrieval baseline and leads to competitive performance.

#index 1415743
#* Computing information retrieval performance measures efficiently in the presence of tied scores
#@ Frank McSherry;Marc Najork
#t 2008
#c 16
#% 57485
#% 411762
#% 857180
#! The Information Retrieval community uses a variety of performance measures to evaluate the effectiveness of scoring functions. In this paper, we show how to adapt six popular measures -- precision, recall, F1, average precision, reciprocal rank, and normalized discounted cumulative gain -- to cope with scoring functions that are likely to assign many tied scores to the results of a search. Tied scores impose only a partial ordering on the results, meaning that there are multiple possible orderings of the result set, each one performing differently. One approach to cope with ties would be to average the performance values across all possible result orderings; but unfortunately, generating result permutations requires super-exponential time. The approach presented in this paper computes precisely the same performance value as the approach of averaging over all permutations, but does so as efficiently as the original, tie-oblivious measures.

#index 1415744
#* Towards characterization of actor evolution and interactions in news corpora
#@ Rohan Choudhary;Sameep Mehta;Amitabha Bagchi;Rahul Balakrishnan
#t 2008
#c 16
#% 220643
#% 783535
#% 823344
#% 881529
#% 881538
#! The natural way to model a news corpus is as a directed graph where stories are linked to one another through a variety of relationships. We formalize this notion by viewing each news story as a set of actors, and by viewing links between stories as transformations these actors go through. We propose and model a simple and comprehensive set of transformations: create, merge, split, continue, and cease. These transformations capture evolution of a single actor and interactions among multiple actors. We present algorithms to rank each transformation and show how ranking helps us to infer important relationships between actors and stories in a corpus. We demonstrate the effectiveness of our notions by experimenting on large news corpora.

#index 1415745
#* The impact of semantic class identification and semantic role labeling on natural language answer extraction
#@ Bahadorreza Ofoghi;John Yearwood;Liping Ma
#t 2008
#c 16
#% 747891
#% 817611
#% 882442
#% 939748
#% 1734539
#! In satisfying an information need by a Question Answering (QA) system, there are text understanding approaches which can enhance the performance of final answer extraction. Exploiting the FrameNet lexical resource in this process inspires analysis of the levels of semantic representation in the automated practice where the task of semantic class and role labeling takes place. In this paper, we analyze the impact of different levels of semantic parsing on answer extraction with respect to the individual sub-tasks of frame evocation and frame element assignment.

#index 1415746
#* Improving complex interactive question answering with Wikipedia anchor text
#@ Ian MacKinnon;Olga Vechtomova
#t 2008
#c 16
#% 940038
#% 983662
#% 1250381
#% 1275012
#! When the objective of an information retrieval task is to return a nugget rather than a document, query terms that exist in a document will often not be used in the most relevant information nugget in the document. In this paper, a new method of query expansion is proposed based on the Wikipedia link structure surrounding the most relevant articles selected automatically. Evaluated with the Nuggeteer automatic scoring software, an increase in the F-scores is found from the TREC Complex Interactive Question Answering task when integrating this expansion into an already high-performing baseline system.

#index 1415747
#* A cluster-sensitive graph model for query-oriented multi-document summarization
#@ Furu Wei;Wenjie Li;Qin Lu;Yanxiang He
#t 2008
#c 16
#% 268079
#% 816173
#% 832330
#% 938761
#% 939547
#% 939968
#% 961700
#% 1272053
#! In this paper, we develop a novel cluster-sensitive graph model for query-oriented multi-document summarization. Upon it, an iterative algorithm, namely QoCsR, is built. As there is existence of natural clusters in the graph in the case that a document comprises a collection of sentences, we suggest distinguishing intra- and inter-document sentence relations in order to take into consideration the influence of cluster (i.e. document) global information on local sentence evaluation. In our model, five kinds of relations are involved among the three objects, i.e. document, sentence and query. Three of them are new and normally ignored in previous graph-based models. All these relations are then appropriately formulated in the QoCsR algorithm though in different ways. ROUGE evaluations shows that QoCsR can outperform the best DUC 2005 participating systems.

#index 1415748
#* Evaluating text representations for retrieval of the best group of documents
#@ Xiaoyong Liu;W. Bruce Croft
#t 2008
#c 16
#% 144034
#% 218992
#% 228105
#% 262096
#% 280850
#% 342660
#% 427921
#% 766430
#% 766431
#% 810906
#% 879676
#% 940042
#% 1216772
#! Cluster retrieval assumes that the probability of relevance of a document should depend on the relevance of other similar documents to the same query. The goal is to find the best group of documents. Many studies have examined the effectiveness of this approach, by employing different retrieval methods or clustering algorithms, but few have investigated text representations. This paper revisits the problem of retrieving the best group of documents, from the language-modeling perspective. We analyze the advantages and disadvantages of a range of representation techniques, derive features that characterize the good document groups, and experiment with a new probabilistic representation as a first step toward incorporating these features. Empirical evaluation demonstrates that the relationship between documents can be leveraged in retrieval when a good representation technique is available, and that retrieving the best group of documents can be more effective than retrieving individual documents.

#index 1415749
#* Enhancing relevance models with adaptive passage retrieval
#@ Xiaoyan Li;Zhigang Zhu
#t 2008
#c 16
#% 144034
#% 232677
#% 262096
#% 280850
#% 280864
#% 300542
#% 328532
#% 340899
#% 340901
#% 342707
#% 397126
#% 413592
#% 730070
#% 766476
#% 783506
#% 879585
#% 1043048
#! Passage retrieval and pseudo relevance feedback/query expansion have been reported as two effective means for improving document retrieval in literature. Relevance models, while improving retrieval in most cases, hurts performance on some heterogeneous collections. Previous research has shown that combining passage-level evidence with pseudo relevance feedback brings added benefits. In this paper, we study passage retrieval with relevance models in the language-modeling framework for document retrieval. An adaptive passage retrieval approach is proposed to document ranking based on the best passage of a document given a query. The proposed passage ranking method is applied to two relevance-based language models: the Lavrenko-Croft relevance model and our robust relevance model. Experiments are carried out with three query sets on three different collections from TREC. Our experimental results show that combining adaptive passage retrieval with relevance models (particularly the robust relevance model) consistently outperforms solely applying relevance models on full-length document retrieval.

#index 1415750
#* Ontology matching using vector space
#@ Zahra Eidoon;Nasser Yazdani;Farhad Oroumchian
#t 2008
#c 16
#% 348187
#% 480645
#% 787098
#% 1683880
#% 1705177
#% 1727485
#! Interoperability of heterogeneous systems on the Web will be achieved through an agreement between the underlying ontologies. Ontology matching is an operation that takes two ontologies and determines their semantic mapping. This paper presents a method of ontology matching which is based on modeling ontologies in a vector space and estimating their similarity degree by matching their concept vectors. The proposed method is successfully applied to the test suit of Ontology Alignment Evaluation Initiative 2005 [10] and compared to the results reported by other methods. In terms of precision and recall, the results look promising.

#index 1415751
#* Accessibility in information retrieval
#@ Leif Azzopardi;Vishwa Vinay
#t 2008
#c 16
#% 290703
#% 375017
#% 747116
#% 768898
#% 907504
#% 987215
#! This paper introduces the concept of accessibility from the field of transportation planning and adopts it within the context of Information Retrieval (IR). An analogy is drawn between the fields, which motivates the development of document accessibility measures for IR systems. Considering the accessibility of documents within a collection given an IR System provides a different perspective on the analysis and evaluation of such systems which could be used to inform the design, tuning and management of current and future IR systems.

#index 1415752
#* Semantic relationships in multi-modal graphs for automatic image annotation
#@ Vassilios Stathopoulos;Jana Urban;Joemon Jose
#t 2008
#c 16
#% 318785
#% 387427
#% 457912
#% 642989
#% 722927
#% 784963
#% 903609
#% 903632
#% 990312
#% 1677726
#! It is important to integrate contextual information in order to improve the inaccurate results of current approaches for automatic image annotation. Graph based representations allow incorporation of such information. However, their behaviour has not been studied in this context. We conduct extensive experiments to show the properties of such representations using semantic relationships as a type of contextual information. We also experimented with different similarity measures for semantic features and results are presented.

#index 1415753
#* Conversation detection in email systems
#@ Shai Erera;David Carmel
#t 2008
#c 16
#% 169717
#% 230532
#% 791731
#% 936928
#% 1546444
#! This work explores a novel approach for conversation detection in email mailboxes. This approach clusters messages into coherent conversations by using a similarity function among messages that takes into consideration all relevant email attributes, such as message subject, participants, date of submission, and message content. The detection algorithm is evaluated against a manual partition of two email mailboxes into conversations. Experimental results demonstrate the superiority of our detection algorithm over several other alternative approaches.

#index 1415754
#* Efficient multimedia time series data retrieval under uniform scaling and normalisation
#@ Waiyawuth Euachongprasit;Chotirat Ann Ratanamahatana
#t 2008
#c 16
#% 749216
#% 795273
#% 809264
#% 824705
#% 883467
#% 1016194
#! As the world has shifted towards manipulation of information and its technology, we have been increasingly overwhelmed by the amount of available multimedia data while having higher expectations to fully exploit these data at hands. One of the attempts is to develop content-based multimedia information retrieval systems, which greatly facilitate us to intuitively search by its contents; a classic example is a Query-by-Humming system. Nevertheless, typical content-based search for multimedia data usually requires a large amount of storages and is computationally intensive. Recently, time series representation has been successfully applied to a wide variety of research, including multimedia retrieval due to the great reduction in time and space complexity. Besides, an enhancement, Uniform Scaling, has been proposed and applied prior to distance calculation, as well as it has been demonstrated that Uniform Scaling can outperform Euclidean distance. These previous work on Uniform Scaling, nonetheless, overlook the importance and effects of normalisation, which make their frameworks impractical for real world data. Therefore, in this paper, we justify this importance of normalisation in multimedia data and propose an efficient solution for searching multimedia time series data under Uniform Scaling and normalisation.

#index 1415755
#* Integrating structure and meaning: a new method for encoding structure for text classification
#@ Jonathan M. Fishbein;Chris Eliasmith
#t 2008
#c 16
#% 644336
#% 939718
#! Current representation schemes for automatic text classification treat documents as syntactically unstructured collections of words or 'concepts'. Past attempts to encode syntactic structure have treated part-of-speech information as another word-like feature, but have been shown to be less effective than non-structural approaches. We propose a new representation scheme using Holographic Reduced Representations (HRRs) as a technique to encode both semantic and syntactic structure. This method improves on previous attempts in the literature by encoding the structure across all features of the document vector while preserving text semantics. Our method does not increase the dimensionality of the document vectors, allowing for efficient computation and storage. We present classification results of our HRR text representations versus Bag-of-Concepts representations and show that our method of including structure improves text classification results.

#index 1415756
#* A Wikipedia-based multilingual retrieval model
#@ Martin Potthast;Benno Stein;Maik Anderka
#t 2008
#c 16
#% 397145
#% 714425
#% 807750
#% 987258
#% 987347
#% 1275012
#! This paper introduces CL-ESA, a new multilingual retrieval model for the analysis of cross-language similarity. The retrieval model exploits the multilingual alignment of Wikipedia: given a document d written in language L we construct a concept vector d for d, where each dimension i in d quantifies the similarity of d with respect to a document di* chosen from the "L-subset" of Wikipedia. Likewise, for a second document d′ written in language L′, L ≠ L′, we construct a concept vector d′, using from the L′-subset of the Wikipedia the topic-aligned counterparts d′i* of our previously chosen documents. Since the two concept vectors d and d′ are collection-relative representations of d and d′ they are language-independent. I. e., their similarity can directly be computed with the cosine similarity measure, for instance. We present results of an extensive analysis that demonstrates the power of this new retrieval model: for a query document d the topically most similar documents from a corpus in another language are properly ranked. Salient property of the new retrieval model is its robustness with respect to both the size and the quality of the index document collection.

#index 1415757
#* Filaments of meaning in word space
#@ Jussi Karlgren;Anders Holst;Magnus Sahlgren
#t 2008
#c 16
#% 321635
#% 565900
#! Word space models, in the sense of vector space models built on distributional data taken from texts, are used to model semantic relations between words. We argue that the high dimensionality of typical vector space models lead to unintuitive effects on modeling likeness of meaning and that the local structure of word spaces is where interesting semantic relations reside.We show that the local structure of word spaces has substantially different dimensionality and character than the global space and that this structure shows potential to be exploited for further semantic analysis using methods for local analysis of vector space structure rather than globally scoped methods typically in use today such as singular value decomposition or principal component analysis.

#index 1415758
#* Finding the best picture: cross-media retrieval of content
#@ Koen Deschacht;Marie-Francine Moens
#t 2008
#c 16
#% 719598
#% 722927
#% 740763
#% 741072
#% 990258
#% 1112538
#! We query the pictures of Yahoo! News for persons and objects by using the accompanying news captions as an indexing annotation. Our aim is to find these pictures on top of the answer list in which the sought persons or objects are most prominently present. We demonstrate that an appearance or content model based on syntactic, semantic and discourse analysis of the short news text is only useful for finding the best picture of a person of object if the database contains photos each picturing many entities. In other circumstances a simpler bag-of-nouns representation has a good performance. The appearance models are tested in a probabilistic ranking function.

#index 1415759
#* Robust query-specific pseudo feedback document selection for query expansion
#@ Qiang Huang;Dawei Song;Stefan Rüger
#t 2008
#c 16
#% 309095
#% 340892
#% 411762
#% 458396
#% 879619
#% 916675
#% 987230
#% 987299
#% 1676546
#! In document retrieval using pseudo relevance feedback, after initial ranking, a fixed number of top-ranked documents are selected as feedback to build a new expansion query model. However, very little attention has been paid to an intuitive but critical fact that the retrieval performance for different queries is sensitive to the selection of different numbers of feedback documents. In this paper, we explore two approaches to incorporate the factor of query-specific feedback document selection in an automatic way. The first is to determine the "optimal" number of feedback documents with respect to a query by adopting the clarity score and cumulative gain. The other approach is that, instead of capturing the optimal number, we hope to weaken the effect of the numbers of feedback document, i.e., to improve the robustness of the pseudo relevance feedback process, by a mixture model. Our experimental results show that both approaches improve the overall retrieval performance.

#index 1415760
#* Expert search evaluation by supporting documents
#@ Craig Macdonald;Iadh Ounis
#t 2008
#c 16
#% 766409
#% 879570
#% 907525
#% 913206
#% 1392466
#% 1415732
#% 1715628
#! An expert search system assists users with their "expertise need" by suggesting people with relevant expertise to their query. Most systems work by ranking documents in response to the query, then ranking the candidates using information from this initial document ranking and known associations between documents and candidates. In this paper, we aim to determine whether we can approximate an evaluation of the expert search system using the underlying document ranking. We evaluate the accuracy of our document ranking evaluation by assessing how closely each measure correlates to the ground truth evaluation of the candidate ranking. Interestingly, we find that improving the underlying ranking of documents does not necessarily result in an improved candidate ranking.

#index 1415761
#* Ranking categories for web search
#@ Gianluca Demartini;Paul-Alexandru Chirita;Ingo Brunkhorst;Wolfgang Nejdl
#t 2008
#c 16
#% 118771
#% 297550
#% 766433
#% 1742086
#! In the context of Web Search, clustering based engines are emerging as an alternative for the classical ones. In this paper we analyse different possible ranking algorithms for ordering clusters of documents within a search result. More specifically, we investigate approaches based on document rankings, on the similarities between the user query and the search results, on the quality of the produced clusters, as well as some document independent approaches. Even though we use a topic based hierarchy for categorizing the URLs, our metrics can be applied to other clusters as well. An empirical analysis with a group of 20 subjects showed that the average similarity between the user query and the documents within each category yields the best cluster ranking.

#index 1415762
#* Key design issues with visualising images using Google earth
#@ Paul Clough;Simon Read
#t 2008
#c 16
#% 194863
#% 358412
#! Using map visualisation tools and earth browsers to display images in a spatial context is integral to many photo-sharing sites and commercial image archives, yet little academic research has been conducted into the utility and functionality of such systems. In developing a prototype system to explore the use of Google Earth in the visualisation of news photos, we have elicited key design issues based on user evaluations of Panoramio and two custom-built spatio-temporal image browsing prototypes. We discuss the implications of these design issues, with particular emphasis on visualising news photos.

#index 1415763
#* Methods for augmenting semantic models with structural information for text classification
#@ Jonathan M. Fishbein;Chris Eliasmith
#t 2008
#c 16
#% 644336
#% 719299
#% 939718
#! Current representation schemes for automatic text classification treat documents as syntactically unstructured collections of words or 'concepts'. Past attempts to encode syntactic structure have treated part-of-speech information as another word-like feature, but have been shown to be less effective than non-structural approaches. Here, we investigate three methods to augment semantic modelling with syntactic structure, which encode the structure across all features of the document vector while preserving text semantics. We present classification results for these methods versus the Bag-of-Concepts semantic modelling representation to determine which method best improves classification scores.

#index 1415764
#* Use of temporal expressions in web search
#@ Sérgio Nunes;Cristina Ribeiro;Gabriel David
#t 2008
#c 16
#% 766447
#% 772012
#% 817550
#% 844287
#% 878624
#% 1024551
#! While trying to understand and characterize users' behavior online, the temporal dimension has received little attention by the research community. This exploratory study uses two collections of web search queries to investigate the use of temporal information needs. Using state-of-the-art information extraction techniques we identify temporal expressions in these queries. We find that temporal expressions are rarely used (1.5% of queries) and, when used, they are related to current and past events. Also, there are specific topics where the use of temporal expressions is more visible.

#index 1415765
#* Towards an automatically generated music information system via web content mining
#@ Markus Schedl;Peter Knees;Tim Pohle;Gerhard Widmer
#t 2008
#c 16
#% 452448
#% 867054
#% 905103
#% 987248
#% 1742125
#! This paper presents first steps towards building a music information system like last.fm, but with the major difference that the data is automatically retrieved from the WWW using web content mining techniques. We first review approaches to some major problems of music information retrieval (MIR), which are required to achieve the ultimate aim, and we illustrate how these approaches can be put together to create the automatically generated music information system (AGMIS). The problems addressed in this paper are similar and prototypical artist detection, album cover retrieval, band member and instrumentation detection, automatic tagging of artists, and browsing/exploring web pages related to a music artist. Finally, we elaborate on the currently ongoing work of evaluating the methods on a large dataset of more than 600, 000 music artists and on a first prototypical implementation of AGMIS.

#index 1415766
#* Investigating the effectiveness of clickthrough data for document reordering
#@ Milad Shokouhi;Falk Scholer;Andrew Turpin
#t 2008
#c 16
#% 167556
#% 478627
#% 577224
#% 720198
#% 729027
#% 805200
#% 823348
#% 879565
#% 879566
#% 946521
#% 987222
#% 989628
#% 1004294
#! User clicks--also known as clickthrough data--have been cited as an implicit form of relevance feedback. Previous work suggests that relative preferences between documents can be accurately derived from user clicks. In this paper, we analyze the impact of document reordering--based on clickthrough--on search effectiveness, measured using both TREC and user relevance judgments. We also propose new strategies for document reordering that can outperform current techniques. Preliminary results show that current reordering methods do not lead to consistent improvements of search quality, but may even lead to poorer results if not used with care.

#index 1415767
#* Analysis of link graph compression techniques
#@ David Hannah;Craig Macdonald;Iadh Ounis
#t 2008
#c 16
#% 290830
#% 754117
#! Links between documents have been shown to be useful in various Information Retrieval (IR) tasks - for example, Google has been telling us for many years now that the PageRank authority measure is at the heart of its relevance calculations. To use such link analysis techniques in a search engine, special tools are required to store the link matrix of the collection of documents, due to the high number of links typically involved. This work is concerned with the application of compression to the link graph. We compare several techniques of compressing link graphs, and conclude on speed and space metrics, using various standard IR test collections.

#index 1415768
#* An evaluation and analysis of incorporating term dependency for ad-hoc retrieval
#@ Hao Lang;Bin Wang;Gareth Jones;Jintao Li;Yang Xu
#t 2008
#c 16
#% 818262
#% 987229
#% 987356
#! Although many retrieval models incorporating term dependency have been developed, it is still unclear whether term dependency information can consistently enhance retrieval performance for different queries. We present a novel model that captures the main components of a topic and the relationship between those components and the power of term dependency to improve retrieval performance. Experimental results demonstrate that the power of term dependency strongly depends on the relationship between these components. Without relevance information, the model is still useful by predicting the components based on global statistical information. We show the applicability of the model for adaptively incorporating term dependency for individual queries.

#index 1415769
#* An evaluation measure for distributed information retrieval systems
#@ Hans Friedrich Witschel;Florian Holz;Gregor Heinrich;Sven Teresniak
#t 2008
#c 16
#% 194246
#% 309133
#% 643013
#% 730035
#% 878657
#% 907454
#% 907503
#% 987256
#% 1043051
#! This paper is concerned with the evaluation of distributed and peer-to-peer information retrieval systems. A new measure is introduced that compares results of a distributed retrieval system to those of a centralised system, fully exploiting the ranking of the latter as an indicator of gradual relevance. Problems with existing evaluation approaches are verified experimentally.

#index 1415770
#* Optimizing language models for polarity classification
#@ Michael Wiegand;Dietrich Klakow
#t 2008
#c 16
#% 158687
#% 340948
#% 854646
#! This paper investigates the usage of various types of language models on polarity text classification - a subtask in opinion mining which deals with distinguishing between positive and negative opinions in natural language. We focus on the intrinsic benefit of different types of language models. This means that we try to find the optimal settings of a language model by examining different types of normalization, their interaction with smoothing and the benefit of class-based modeling.

#index 1415771
#* Improving web image retrieval using image annotations and inference network
#@ Peng Huang;Jiajun Bu;Chun Chen;Guang Qiu
#t 2008
#c 16
#% 387427
#% 740763
#% 818271
#% 839975
#! Currently text-based retrieval approaches, which utilize web textual information to index and access images, are still widely used by many modern prevalent search engines due to the nature of simplicity and effectiveness. However, page documents often include texts irrelevant to image contents, becoming an obstacle for high-quality image retrieval. In this paper we propose a novel model to improve traditional text-based image retrieval by integrating weighted image annotation keywords and web texts seamlessly. Different from traditional text-based image retrieval models, the proposed model retrieves and ranks images depending on not only texts of web document but also image annotations. To verify the proposed model, some term-based queries are performed on three models, and results have shown that our model performs best.

#index 1415772
#* Slide-film interface: overcoming small screen limitations in mobile web search
#@ Roman Y. Shtykh;Jian Chen;Qun Jin
#t 2008
#c 16
#% 826508
#% 1715637
#! It is well known that alongside with search engine performance improvements and functionality enhancements one of the determinant factors of user acceptance of any search service is the interface. This factor is particularly important for mobile Web search mostly due to small screen limitations of handheld devices. In this paper we propose scrolless mobile Web search interface to decrease search efforts that are multiplied due to these limitations, and discuss its potential advantages and drawbacks over conventional one.

#index 1415773
#* A document-centered approach to a natural language music search engine
#@ Peter Knees;Tim Pohle;Markus Schedl;Dominik Schnitzer;Klaus Seyerlehner
#t 2008
#c 16
#% 987247
#% 987248
#% 1548057
#! We propose a new approach to a music search engine that can be accessed via natural language queries. As with existing approaches, we try to gather as much contextual information as possible for individual pieces in a (possibly large) music collection by means of Web retrieval. While existing approaches use this textual information to construct representations of music pieces in a vector space model, in this paper, we propose a document-centered technique to retrieve music pieces relevant to arbitrary natural language queries. This technique improves the quality of the resulting document rankings substantially. We report on the current state of the research and discuss current limitations, as well as possible directions to overcome them.

#index 1415774
#* Collaborative topic tracking in an enterprise environment
#@ Conny Franke;Omar Alonso
#t 2008
#c 16
#% 220708
#% 452563
#% 575570
#% 1004299
#! Business users in an enterprise need to keep track of relevant information available on the Web for strategic decisions like mergers and acquisitions. Traditionally this is done by the user performing standing queries or alert mechanisms based on topics. A much richer tracking can be done by providing a way for users to initiate and share topics in a single place. In this paper we present an alternative model and prototype for tracking topics of interest based on a continuous user collaboration.

#index 1415775
#* Graph-based profile similarity calculation method and evaluation
#@ Hassan Naderi;Béatrice Rumpler
#t 2008
#c 16
#% 25998
#% 399447
#% 410276
#% 1289575
#% 1386865
#! Collaborative Information Retrieval (CIR) is a new technique for resolving the current problem of information retrieval systems. A CIR system registers the previous user interactions to response to the subsequent user queries more efficiently. But, the goals and the characteristics of two users may be different; so when they send the same query to a CIR system, they may be interested in two different lists of documents. To resolve this problem, we have developed a personalized CIR system, called PERCIRS, which is based on the similarity between two user profiles. In this paper, we propose a new method for User Profile Similarity Calculation UPSC. Finally, we introduce a mechanism for evaluating UPSC methods.

#index 1415776
#* The good, the bad, the difficult, and the easy: something wrong with information retrieval evaluation?
#@ Stefano Mizzaro
#t 2008
#c 16
#% 810906
#% 857180
#% 907493
#% 987252
#! TREC-like evaluations do not consider topic ease and difficulty. However, it seems reasonable to reward good effectiveness on difficult topics more than good effectiveness on easy topics, and to penalize bad effectiveness on easy topics more than bad effectiveness on difficult topics. This paper shows how this approach leads to evaluation results that could be more reasonable, and that are different to some extent. I provide a general analysis of this issue, propose a novel framework, and experimentally validate a part of it.

#index 1415777
#* Hybrid method for personalized search in digital libraries
#@ Thanh-Trung Van;Michel Beigbeder
#t 2008
#c 16
#% 281366
#% 779859
#% 874463
#% 1290045
#% 1739989
#! In this paper we present our work about personalized search in digital libraries. The search results could be reranked while taking into account specific information needs of different people. We study many methods for this purpose: citation-based method, content-based method and hybrid method. We conducted experiments to compare performances of these methods. Experimental results show that our approaches are promising and applicable in digital libraries.

#index 1415778
#* Exploiting session context for information retrieval: a comparative study
#@ Gaurav Pandey;Julia Luxenburger
#t 2008
#c 16
#% 340899
#% 340948
#% 342707
#% 818207
#% 818259
#% 838547
#% 881540
#! Hard queries are known to benefit from relevance feedback provided by users. It is, however, also known that users are generally reluctant to provide feedback when searching for information. A natural resort not demanding any active user participation is to exploit implicit feedback from the previous user search behavior, i.e., from the context of the current search session. In this work, we present a comparative study on the performance of the three most prominent retrieval models, the vector-space, probabilistic, and language-model based retrieval frameworks, when additional session context is incorporated.

#index 1415779
#* Structural re-ranking with cluster-based retrieval
#@ Seung-Hoon Na;In-Su Kang;Jong-Hyeok Lee
#t 2008
#c 16
#% 340948
#% 766431
#% 818266
#% 879575
#% 907557
#% 940042
#! Re-ranking (RR) and Cluster-based Retrieval (CR) have been polar methods for improving retrieval effectiveness by using inter-document similarities. However, RR and CR improve precision and recall respectively, not simultaneously. Thus, the improvement through RR and CR may be different according to whether a query is recall-deficient or not. However, previous researchers missed out this point, and separately investigated individual approaches, causing a limited improvement. To reflect all of positive effects by RR and CR, this paper proposes RCR, the re-ranking with cluster-based retrieval where RR is applied to initially-retrieved results of CR. Experimental results show that RCR significantly improves the baseline, while CR or RR sometimes does not significantly improve the baseline.

#index 1415780
#* Automatic vandalism detection in Wikipedia
#@ Martin Potthast;Benno Stein;Robert Gerling
#t 2008
#c 16
#% 751850
#% 954955
#% 961564
#% 967495
#% 967642
#% 998622
#% 1016107
#! We present results of a new approach to detect destructive article revisions, so-called vandalism, inWikipedia. Vandalism detection is a one-class classification problem, where vandalism edits are the target to be identified among all revisions. Interestingly, vandalism detection has not been addressed in the Information Retrieval literature by now. In this paper we discuss the characteristics of vandalism as humans recognize it and develop features to render vandalism detection as a machine learning task. We compiled a large number of vandalism edits in a corpus, which allows for the comparison of existing and new detection approaches. Using logistic regression we achieve 83% precision at 77% recall with our model. Compared to the rule-based methods that are currently applied in Wikipedia, our approach increases the F-Measure performance by 49% while being faster at the same time.

#index 1415781
#* Evaluating paragraph retrieval for why-QA
#@ Suzan Verberne;Lou Boves;Nelleke Oostdijk;Peter-Arno Coppen
#t 2008
#c 16
#% 878916
#% 995462
#! We implemented a baseline approach to why-question answering based on paragraph retrieval. Our implementation incorporates the QAP ranking algorithm with addition of a number of surface features (cue words and XML markup). With this baseline system, we obtain an accuracy-at-10 of 57.0% with an MRR of 0.31. Both the baseline and the proposed evaluation method are good starting points for the current research and for other researchers working on the problem of why-QA. We also experimented with the addition of smart question analysis features to our baseline system (answer type and informational value of the subject). This however did not give significant improvement to our baseline. In the near future, we will investigate what other linguistic features can facilitate re-ranking in order to increase accuracy.

#index 1415782
#* Revisit of nearest neighbor test for direct evaluation of inter-document similarities
#@ Seung-Hoon Na;In-Su Kang;Jong-Hyeok Lee
#t 2008
#c 16
#% 187772
#% 228105
#% 342658
#% 375017
#% 766429
#% 766431
#% 846237
#% 940042
#! Recently, cluster-based retrieval has been successfully applied to improve retrieval effectiveness. The core part of cluster-based retrieval is interdocument similarities. Although inter-document similarities can be investigated independently of cluster-based retrieval and be further improved in various ways, their direct evaluation has not been seriously considered. Considering that there are many cluster-based retrieval methods, such a direct evaluation method can separate the work of inter-document similarities from the work of cluster-based retrieval. For this purpose, this paper revisits Voorhee's nearest neighbor test as such a direct evaluation, by mainly focusing on whether or not the test is correlated to the retrieval effectiveness. Experimental results consistently verify the use of the nearest neighbor test. As a result, we conclude that the improvement of retrieval effectiveness can be well-predictable from direct evaluation, even without performing runs of cluster-based retrieval.

#index 1415783
#* A comparison of named entity patterns from a user analysis and a system analysis
#@ Masnizah Mohd;Fabio Crestani;Ian Ruthven
#t 2008
#c 16
#% 445316
#% 735078
#% 766444
#% 987219
#% 1783034
#! This paper investigates the detection of named entity (NE) patterns by comparing the results of NE patterns resulting from a user analysis and a system analysis. Findings revealed that there are difference in NE patterns detected by system and user, something that may affect the performance of a TDT system based on NE detection.

#index 1415784
#* Query-based inter-document similarity using probabilistic co-relevance model
#@ Seung-Hoon Na;In-Su Kang;Jong-Hyeok Lee
#t 2008
#c 16
#% 228105
#% 375017
#% 766430
#% 766431
#% 879578
#! Inter-document similarity is the critical information which determines whether or not the cluster-based retrieval improves the baseline. However, a theoretical work on inter-document similarity has not been investigated, even though such work can provide a principle to define a more improved similarity in a well-motivated direction. To support this theory, this paper starts from pursuing an ideal inter-document similarity that optimally satisfies the cluster-hypothesis. We propose a probabilistic principle of inter-document similarities; the optimal similarity of two documents should be proportional to the probability that they are co-relevant to an arbitrary query. Based on this principle, the study of the inter-document similarity is formulated to attack the estimation problem of the co-relevance model of documents. Furthermore, we obtain that the optimal inter-document similarity should be defined using queries as its basic unit, not terms, namely a query-based similarity. We strictly derive a novel query-based similarity from the co-relevance model, without any heuristics. Experimental results show that the new query-based inter-document similarity significantly improves the previously-used term-based similarity in the context of Voorhee's evaluation measure.

#index 1415785
#* Using coherence-based measures to predict query difficulty
#@ Jiyin He;Martha Larson;Maarten De Rijke
#t 2008
#c 16
#% 397161
#% 766497
#% 810906
#% 818267
#% 944349
#% 995516
#! We investigate the potential of coherence-based scores to predict query difficulty. The coherence of a document set associated with each query word is used to capture the quality of a query topic aspect. A simple query coherence score, QC-1, is proposed that requires the average coherence contribution of individual query terms to be high. Two further query scores, QC-2 and QC-3, are developed by constraining QC- 1 in order to capture the semantic similarity among query topic aspects. All three query coherence scores show the correlation with average precision necessary to make them good predictors of query difficulty. Simple and efficient, the measures require no training data and are competitive with language model-based clarity scores.

#index 1415786
#* Efficient processing of category-restricted queries for web directories
#@ Ismail Sengor Altingovde;Fazli Can;Özgür Ulusoy
#t 2008
#c 16
#% 787544
#% 987380
#% 1051061
#% 1387563
#% 1663085
#! We show that a cluster-skipping inverted index (CS-IIS) is a practical and efficient file structure to support category-restricted queries for searching Web directories. The query processing strategy with CS-IIS improves CPU time efficiency without imposing any limitations on the directory size.

#index 1415787
#* Focused browsing: providing topical feedback for link selection in hypertext browsing
#@ Gareth J. F. Jones;Quixiang Li
#t 2008
#c 16
#% 249091
#% 262036
#% 281251
#% 309507
#% 424010
#% 717133
#% 835189
#% 1404874
#! When making decisions about whether to navigate to a linked page, users of standard browsers of hypertextual documents returned by an information retrieval search engine are entirely reliant on the content of the anchortext associated with links and the surrounding text. This information is often insufficient for them to make reliable decisions about whether to open a linked page, and they can find themselves following many links to pages which are not helpful with subsequent return to the previous page. We describe a prototype focused browsing application which provides feedback on the likely usefulness of each page linked from the current one, and a term cloud preview of the contents of each linked page. Results from an exploratory experiment suggest that users can find this useful in improving their search efficiency.

#index 1415788
#* The impact of named entity normalization on information retrieval for question answering
#@ Mahboob Alam Khalid;Valentin Jijkoun;Maarten De Rijke
#t 2008
#c 16
#% 268079
#% 709765
#% 939376
#% 987274
#% 1250362
#% 1271267
#% 1288629
#% 1299594
#! In the named entity normalization task, a system identifies a canonical unambiguous referent for names like Bush or Alabama. Resolving synonymy and ambiguity of such names can benefit end-to-end information access tasks. We evaluate two entity normalization methods based on Wikipedia in the context of both passage and document retrieval for question anwering. We find that even a simple normalization method leads to improvements of early precision, both for document and passage retrieval. Moreover, better normalization results in better retrieval performance.

#index 1415789
#* Efficiency issues in information retrieval workshop
#@ Roi Blanco;Fabrizio Silvestri
#t 2008
#c 16

#index 1415790
#* Exploiting semantic annotations in information retrieval
#@ Omar Alonso;Hugo Zaragoza
#t 2008
#c 16

#index 1415791
#* Workshop on novel methodologies for evaluation in information retrieval
#@ Mark Sanderson;Martin Braschler;Nicola Ferro;Julio Gonzalo
#t 2008
#c 16

#index 1415792
#* ECIR 2008 tutorials: search and discovery in user-generated text content
#@ Maarten De Rijke;Wouter Weerkamp
#t 2008
#c 16

#index 1415793
#* ECIR 2008 tutorials: advanced language modeling approaches (case study: expert search)
#@ Djoerd Hiemstra
#t 2008
#c 16

#index 1415794
#* ECIR 2008 tutorials: researching and building IR applications using terrier
#@ Craig Macdonald;Ben He
#t 2008
#c 16

#index 1587333
#* Proceedings of the 33rd European conference on Advances in information retrieval
#@ Paul Clough;Colum Foley;Cathal Gurrin;Hyowon Lee;Gareth J. F. Jones
#t 2011
#c 16

#index 1587334
#* IR research: systems, interaction, evaluation and theories
#@ Kalervo Järvelin
#t 2011
#c 16
#% 187999
#% 835027
#% 955710
#% 1263597
#% 1455255
#! The ultimate goal of information retrieval (IR) research is to create ways to support humans to better access information in order to better carry out their (work) tasks. Because of this, IR research has a primarily technological interest in knowledge creation - how to find information (better)? IR research therefore has a constructive aspect (to create novel systems) and an evaluative aspect (are they any good?). Evaluation is sometimes referred to as a hallmark and distinctive feature of IR research. No claim on IR system performance is granted any merit unless proven through evaluation. Technological innovation alone is not sufficient. In fact, much research in IR deals with IR evaluation and its methodology.

#index 1587335
#* Ad retrieval systems in vitro and in vivo: knowledge-based approaches to computational advertising
#@ Evgeniy Gabrilovich
#t 2011
#c 16
#% 1227616
#! Over the past decade, online advertising became the principal economic force behind many an Internet service, from major search engines to globe-spanning social networks to blogs. There is often a tension between online advertising and user experience, but on the other hand, advertising revenue enables a myriad of free Web services to the public and fosters a great deal of innovation. Matching the advertisers' message to a receptive and interested audience benefits both sides; indeed, literally hundreds of millions of users occasionally click on the ads, hence they should be considered relevant to the users' information needs by current IR evaluation principles. The utility of ads can be better explained by considering advertising as a medium of information [2, 3]. Similarly to aggregated search [1], which enhances users' Web search experience with relevant news, local results, user-generated content, or multimedia, online advertising provides another rich source of content. This source, however, is in a complexity class of its own, due to the brevity of bid phrases, ad text being optimized for presentation rather than indexing, and multiple, possibly contradictory utility functions.

#index 1587336
#* The value of user feedback
#@ Thorsten Joachims
#t 2011
#c 16
#! Information retrieval systems and their users engage in a dialogue. While the main flow of information is from the system to the user, feedback from the user to the system provides many opportunities for short-term and long-term learning. In this talk, I will explore two interrelated questions that are central to the effective use of feedback. First, how can user feedback be collected so that it does not lay a burden on the user? I will argue that the mechanisms for collecting feedback have to be integrated into the design of the retrieval process, so that the user's short-term goals are well-aligned with the system's goal of collecting feedback. Second, if this integration succeeds, how valuable is the information that user feedback provides? For the tasks of retrieval evaluation and query disambiguation, the talk will quantify by how much user feedback can save human annotator effort and improve retrieval quality respectively.

#index 1587337
#* Text classification for a large-scale taxonomy using dynamically mixed local and global models for a node
#@ Heung-Seon Oh;Yoonjung Choi;Sung-Hyon Myaeng
#t 2011
#c 16
#% 287214
#% 465747
#% 465754
#% 466078
#% 466501
#% 642986
#% 783478
#% 818266
#% 829975
#% 987221
#% 987262
#% 1074128
#% 1227578
#% 1401398
#% 1457107
#! Hierarchical text classification for a large-scale Web taxonomy is challenging because the number of categories hierarchically organized is large and the training data for deep categories are usually sparse. It's been shown that a narrow-down approach involving a search of the taxonomical tree is an effective method for the problem. A recent study showed that both local and global information for a node is useful for further improvement. This paper introduces two methods for mixing local and global models dynamically for individual nodes and shows they improve classification effectiveness by 5% and 30%, respectively, over and above the state-of-art method.

#index 1587338
#* User-related tag expansion for web document clustering
#@ Peng Li;Bin Wang;Wei Jin;Yachao Cui
#t 2011
#c 16
#% 465754
#% 577355
#% 722904
#% 727861
#% 766430
#% 766433
#% 995468
#% 1035588
#% 1055739
#% 1055743
#% 1074073
#% 1077150
#% 1107631
#% 1127482
#% 1166510
#% 1214660
#% 1250362
#% 1289518
#% 1292643
#! As high quality descriptors of web page semantics, social annotations or tags have been used for web document clustering and achieved promising results. However, most web pages have few tags(less than 10). This sparsity seriously limits the usage of tags on clustering. In this work, we propose a user-related tag expansion method to overcome the problem, which incorporates additional useful tags into the original tag document by utilizing user tagging as background knowledge. Unfortunately, simply adding tags may cause topic drift, i.e., the dominant topic(s) of the original document may be changed. This problem is addressed in this research by designing a novel generative model called Folk-LDA, which jointly models original and expanded tags as independent observations. Experimental results show that (1)Our user-related tag expansion method can be effectively applied to over 90% tagged web documents; (2)Folk-LDA can alleviate the topic drift in expansion, especially for those topic-specific documents; (3) Compared to word-based clustering, our approach using only tags achieves a statistically significant increase of 39% on F1 score while reducing 76% terms involved in computation at best.

#index 1587339
#* A comparative experimental assessment of a threshold selection algorithm in hierarchical text categorization
#@ Andrea Addis;Giuliano Armano;Eloisa Vargiu
#t 2011
#c 16
#% 140588
#% 194284
#% 318412
#% 340904
#% 714701
#% 763708
#% 919207
#% 945300
#! Most of the research on text categorization has focused on mapping text documents to a set of categories among which structural relationships hold, i.e., on hierarchical text categorization. For solutions of a hierarchical problem that make use of an ensemble of classifiers, the behavior of each classifier typically depends on an acceptance threshold, which turns a degree of membership into a dichotomous decision. In principle, the problem of finding the best acceptance thresholds for a set of classifiers related with taxonomic relationships is a hard problem. Hence, devising effective ways for finding suboptimal solutions to this problem may have great importance. In this paper, we assess a greedy threshold selection algorithm aimed at finding a suboptimal combination of thresholds in a hierarchical text categorization setting. Comparative experiments, performed on Reuters, report the performance of the proposed threshold selection algorithm against a relaxed brute-force algorithm and against two state-of-the-art algorithms. Results highlight the effectiveness of the approach.

#index 1587340
#* Improving tag-based recommendation by topic diversification
#@ Christian Wartena;Martin Wibbels
#t 2011
#c 16
#% 769915
#% 805841
#% 1017565
#% 1052902
#% 1107631
#% 1152455
#% 1230989
#% 1280704
#% 1280708
#% 1323323
#% 1396094
#% 1417104
#! Collaborative tagging has emerged as a mechanism to describe items in large on-line collections. Tags are assigned by users to describe and find back items, but it is also tempting to describe the users in terms of the tags they assign or in terms of the tags of the items they are interested in. The tag-based profile thus obtained can be used to recommend new items. If we recommend new items by computing their similarity to the user profile or to all items seen by the user, we run into the risk of recommending only neutral items that are a bit relevant for each topic a user is interested in. In order to increase user satisfaction many recommender systems not only optimize for accuracy but also for diversity. Often it is assumed that there exists a trade-off between accuracy and diversity. In this paper we introduce topic aware recommendation algorithms. Topic aware algorithms first detect different interests in the user profile and then generate recommendations for each of these interests. We study topic aware variants of three tag based recommendation algorithms and show that each of them gives better recommendations than their base variants, both in terms of precision and recall and in terms of diversity.

#index 1587341
#* A joint model of feature mining and sentiment analysis for product review rating
#@ Jorge Carrillo de Albornoz;Laura Plaza;Pablo Gervás;Alberto Díaz
#t 2011
#c 16
#% 286069
#% 465914
#% 786539
#% 815915
#% 938687
#% 939848
#% 983583
#% 1250237
#% 1268503
#% 1481484
#! The information in customer reviews is of great interest to both companies and consumers. This information is usually presented as non-structured free-text so that automatically extracting and rating user opinions about a product is a challenging task. Moreover, this opinion highly depends on the product features on which the user judgments and impressions are expressed. Following this idea, our goal is to predict the overall rating of a product review based on the user opinion about the different product features that are evaluated in the review. To this end, the system first identifies the features that are relevant to consumers when evaluating a certain type of product, as well as the relative importance or salience of such features. The system then extracts from the review the user opinions about the different product features and quantifies such opinions. The salience of the different product features and the values that quantify the user opinions about them are used to construct a Vector of Feature Intensities which represents the review and will be the input to a machine learning model that classifies the review into different rating categories. Our method is evaluated over 1000 hotel reviews from booking.com. The results compare favorably with those achieved by other systems addressing similar evaluations.

#index 1587342
#* Modeling answerer behavior in collaborative question answering systems
#@ Qiaoling Liu;Eugene Agichtein
#t 2011
#c 16
#% 838398
#% 838464
#% 881477
#% 956516
#% 1019165
#% 1035587
#% 1055679
#% 1055718
#% 1055738
#% 1083720
#% 1130900
#% 1166519
#% 1183154
#% 1187376
#% 1190249
#% 1207005
#% 1214658
#% 1292492
#% 1399976
#% 1455279
#% 1491834
#! A key functionality in Collaborative Question Answering (CQA) systems is the assignment of the questions from information seekers to the potential answerers. An attractive solution is to automatically recommend the questions to the potential answerers with expertise or interest in the question topic. However, previous work has largely ignored a key problem in question recommendation - namely, whether the potential answerer is likely to accept and answer the recommended questions in a timely manner. This paper explores the contextual factors that influence the answerer behavior in a large, popular CQA system, with the goal to inform the construction of question routing and recommendation systems. Specifically, we consider when users tend to answer questions in a large-scale CQA system, and how answerers tend to choose the questions to answer. Our results over a dataset of more than 1 million questions draw from a real CQA system could help develop more realistic evaluation methods for question recommendation, and inform the design of future question recommender systems.

#index 1587343
#* Clash of the typings: finding controversies and children's topics within queries
#@ Karl Gyllstrom;Marie-Francine Moens
#t 2011
#c 16
#% 290679
#% 1019130
#% 1035586
#% 1384136
#% 1450937
#% 1455269
#! The TadPolemic system identifies whether web search queries (1) are controversial in nature and/or (2) pertain to children's topics. We are incorporating it into a children's web search engine to assist children's search during difficult topics, as well as to provide filtering or mitigation of bias in results when children search for contentious topics. We show through an evaluation that the system is effective at detecting kids' topics and controversies for a broad range of topics. Though designed to assist children, we believe these methods are generalizable beyond young audiences and can be usefully applied in other contexts.

#index 1587344
#* Are semantically related links more effective for retrieval?
#@ Marijn Koolen;Jaap Kamps
#t 2011
#c 16
#% 25942
#% 290830
#% 309145
#% 340147
#% 572502
#% 818241
#% 818255
#% 878916
#% 879575
#% 896031
#% 907542
#% 1019074
#% 1100801
#% 1166528
#% 1250381
#% 1263600
#% 1275285
#! Why do links work? Link-based ranking algorithms are based on the often implicit assumption that linked documents are semantically related to each other, and that link information is therefore useful for retrieval. Although the benefits of link information are well researched, this underlying assumption on why link evidence works remains untested, and the main aim of this paper is to do exactly that. Specifically, we use Wikipedia because it has a dense link structure in combination with a large category structure, which allows for an independent measurement of the semantic relatedness of linked documents. Our main findings are that: 1) global, query-independent link evidence, is not affected by the semantic nature of the links, and 2) for local, query-dependent link evidence, the effectiveness of links increases as their semantic distance decreases. That is, we directly observe that links between semantically related pages are more effective for ad hoc retrieval than links between unrelated ones. These findings confirm and quantify the underlying assumption of existing link-based methods, which sheds further light on our understanding of the nature of link evidence. Such deeper understanding is instrumental for the development of novel link-based methods.

#index 1587345
#* Caching for realtime search
#@ Edward Bortnikov;Ronny Lempel;Kolman Vornovitsky
#t 2011
#c 16
#% 141540
#% 288718
#% 577302
#% 860861
#% 1190098
#% 1355017
#% 1399951
#% 1418196
#% 1450839
#% 1834787
#! Modern search engines feature real-time indices, which incorporate changes to content within seconds. As search engines also cache search results for reducing user latency and back-end load, without careful real-time management of search results caches, the engine might return stale search results to users despite the efforts invested in keeping the underlying index up to date. A recent paper proposed an architectural component called CIP - the cache invalidation predictor. CIPs invalidate supposedly stale cache entries upon index modifications. Initial evaluation showed the ability to keep the performance benefits of caching without sacrificing much the freshness of search results returned to users. However, it was conducted on a synthetic workload in a simplified setting, using many assumptions. We propose new CIP heuristics, and evaluate them in an authentic environment - on the real evolving corpus and query stream of a large commercial news search engine. Our CIPs operate in conjunction with realistic cache settings, and we use standard metrics for evaluating cache performance. We show that a classical cache replacement policy, LRU, completely fails to guarantee freshness over time, whereas our CIPs serve 97% of the queries with fresh results. Our policies incur a negligible impact on the baseline's cache hit rate, in contrast with traditional age-based invalidation, which must severely reduce the cache performance in order to achieve the same freshness. We demonstrate that the computational overhead of our algorithms is minor, and that they even allow reducing the cache's memory footprint.

#index 1587346
#* Enhancing deniability against query-logs
#@ Avi Arampatzis;Pavlos S. Efraimidis;George Drosatos
#t 2011
#c 16
#% 728195
#% 748600
#% 836145
#% 874387
#% 878624
#% 956557
#% 983653
#% 1014490
#% 1130893
#% 1250381
#% 1280748
#% 1357695
#% 1389881
#% 1523848
#! We propose a method for search privacy on the Internet, focusing on enhancing plausible deniability against search engine query-logs. The method approximates the target search results, without submitting the intended query and avoiding other exposing queries, by employing sets of queries representing more general concepts. We model the problem theoretically, and investigate the practical feasibility and effectiveness of the proposed solution with a set of real queries with privacy issues on a large web collection. The findings may have implications for other IR research areas, such as query expansion and fusion in meta-search.

#index 1587347
#* On the contributions of topics to system evaluation
#@ Stephen Robertson
#t 2011
#c 16
#% 290830
#% 397163
#% 879631
#% 907493
#% 987252
#% 1263583
#% 1263604
#% 1278067
#% 1292721
#! We consider the selection of good subsets of topics for system evaluation. It has previously been suggested that some individual topics and some subsets of topics are better for system evaluation than others: given limited resources, choosing the best subset of topics may give significantly better prediction of overall system effectiveness than (for example) choosing random subsets. Earlier experimental results are extended, with particular reference to generalisation: the ability of a subset of topics selected on the basis on one collection of system runs to perform well in evaluating another collection of system runs. It turns out to be hard to establish generalisability; it is not at all clear that it is possible to identify subsets of topics that are good for general evaluation.

#index 1587348
#* A methodology for evaluating aggregated search results
#@ Jaime Arguello;Fernando Diaz;Jamie Callan;Ben Carterette
#t 2011
#c 16
#% 907495
#% 1074093
#% 1166523
#% 1227616
#% 1227617
#% 1399990
#% 1415710
#% 1450898
#% 1450915
#% 1482230
#! Aggregated search is the task of incorporating results from different specialized search services, or verticals, into Web search results. While most prior work focuses on deciding which verticals to present, the task of deciding where in the Web results to embed the vertical results has received less attention. We propose a methodology for evaluating an aggregated set of results. Our method elicits a relatively small number of human judgements for a given query and then uses these to facilitate a metric-based evaluation of any possible presentation for the query. An extensive user study with 13 verticals confirms that, when users prefer one presentation of results over another, our metric agrees with the stated preference. By using Amazon's Mechanical Turk, we show that reliable assessments can be obtained quickly and inexpensively.

#index 1587349
#* Design and implementation of relevance assessments using crowdsourcing
#@ Omar Alonso;Ricardo Baeza-Yates
#t 2011
#c 16
#% 885438
#% 1047423
#% 1130866
#% 1227633
#% 1252624
#% 1264744
#% 1338557
#% 1478132
#% 1697429
#% 1697468
#! In the last years crowdsourcing has emerged as a viable platform for conducting relevance assessments. The main reason behind this trend is that makes possible to conduct experiments extremely fast, with good results and at low cost. However, like in any experiment, there are several details that would make an experiment work or fail. To gather useful results, user interface guidelines, inter-agreement metrics, and justification analysis are important aspects of a successful crowdsourcing experiment. In this work we explore the design and execution of relevance judgments using Amazon Mechanical Turk as crowdsourcing platform, introducing a methodology for crowdsourcing relevance assessments and the results of a series of experiments using TREC 8 with a fixed budget. Our findings indicate that workers are as good as TREC experts, even providing detailed feedback for certain query-document pairs. We also explore the importance of document design and presentation when performing relevance assessment tasks. Finally, we show our methodology at work with several examples that are interesting in their own.

#index 1587350
#* In search of quality in crowdsourcing for search engine evaluation
#@ Gabriella Kazai
#t 2011
#c 16
#% 751818
#% 1074134
#% 1150163
#% 1151011
#% 1252624
#% 1478132
#% 1489441
#! Crowdsourcing is increasingly looked upon as a feasible alternative to traditional methods of gathering relevance labels for the evaluation of search engines, offering a solution to the scalability problem that hinders traditional approaches. However, crowdsourcing raises a range of questions regarding the quality of the resulting data. What indeed can be said about the quality of the data that is contributed by anonymous workers who are only paid cents for their efforts? Can higher pay guarantee better quality? Do better qualified workers produce higher quality labels? In this paper, we investigate these and similar questions via a series of controlled crowdsourcing experiments where we vary pay, required effort and worker qualifications and observe their effects on the resulting label quality, measured based on agreement with a gold set.

#index 1587351
#* Summarizing a document stream
#@ Hiroya Takamura;Hikaru Yokono;Manabu Okumura
#t 2011
#c 16
#% 425095
#% 1261456
#% 1272209
#% 1292654
#% 1392478
#% 1470582
#% 1470583
#% 1470662
#% 1470696
#! We introduce the task of summarizing a stream of short documents on microblogs such as Twitter. On microblogs, thousands of short documents on a certain topic such as sports matches or TV dramas are posted by users. Noticeable characteristics of microblog data are that documents are often very highly redundant and aligned on timeline. There can be thousands of documents on one event in the topic. Two very similar documents will refer to two distinct events when the documents are temporally distant. We examine the microblog data to gain more understanding of those characteristics, and propose a summarization model for a stream of short documents on timeline, along with an approximate fast algorithm for generating summary.We empirically show that our model generates a good summary on the datasets of microblog documents on sports matches.

#index 1587352
#* A link prediction approach to recommendations in large-scale user-generated content systems
#@ Nitin Chiluka;Nazareno Andrade;Johan Pouwelse
#t 2011
#c 16
#% 452563
#% 730089
#% 935278
#% 1002005
#% 1055761
#% 1190127
#% 1291642
#! Recommending interesting and relevant content from the vast repositories of User-Generated Content systems (UGCs) such as YouTube, Flickr and Digg is a significant challenge. Part of this challenge stems from the fact that classical collaborative filtering techniques - such as k-Nearest Neighbor - cannot be assumed to perform as well in UGCs as in other applications. Such technique has severe limitations regarding data sparsity and scalability that are unfitting for UGCs. In this paper, we employ adaptations of popular Link Prediction algorithms that were shown to be effective in massive online social networks for recommending items in UGCs. We evaluate these algorithms on a large dataset we collect from Flickr. Our results suggest that Link Prediction algorithms are a more scalable and accurate alternative to classical collaborative filtering in the context of UGCs. Moreover, our experiments show that the algorithms considering the immediate neighborhood of users in an user-item graph to recommend items outperform the algorithms that use the entire graph structure for the same. Finally, we find that, contrary to intuition, exploiting explicit social links among users in the recommendation algorithms improves only marginally their performance.

#index 1587353
#* Topic classification in social media using metadata from hyperlinked objects
#@ Sheila Kinsella;Alexandre Passant;John G. Breslin
#t 2011
#c 16
#% 879625
#% 1035587
#% 1074131
#% 1292558
#% 1301004
#% 1406465
#% 1409954
#% 1454267
#! Social media presents unique challenges for topic classification, including the brevity of posts, the informal nature of conversations, and the frequent reliance on external hyperlinks to give context to a conversation. In this paper we investigate the usefulness of these external hyperlinks for determining the topic of an individual post. We focus specifically on hyperlinks to objects which have related metadata available on the Web, including Amazon products and YouTube videos. Our experiments show that the inclusion of metadata from hyperlinked objects in addition to the original post content improved classifier performance measured with the F-score from 84% to 90%. Further, even classification based on object metadata alone outperforms classification based on the original post content.

#index 1587354
#* Peddling or creating? investigating the role of twitter in news reporting
#@ Ilija Subašić;Bettina Berendt
#t 2011
#c 16
#% 78171
#% 131322
#% 1040837
#% 1130976
#% 1194140
#% 1214671
#! The widespread use of social media is regarded by many as the emergence of a new highway for information and news sharing promising a new information-driven "social revolution". In this paper, we analyze how this idea transfers to the news reporting domain. To analyze the role of social media in news reporting, we ask whether citizen journalists tend to create news or peddle (re-report) existing content. We introduce a framework for exploring divergence between news sources by providing multiple views on corpora in comparison. The results of our case study comparing Twitter and other news sources suggest that a major role of Twitter authors consists of neither creating nor peddling, but extending them by commenting on news.

#index 1587355
#* Latent sentiment model for weakly-supervised cross-lingual sentiment classification
#@ Yulan He
#t 2011
#c 16
#% 464465
#% 1074125
#% 1074166
#% 1251720
#% 1264731
#% 1264775
#% 1292503
#% 1292560
#% 1328329
#% 1338553
#% 1481483
#! In this paper, we present a novel weakly-supervised method for crosslingual sentiment analysis. In specific, we propose a latent sentimentmodel (LSM) based on latent Dirichlet allocation where sentiment labels are considered as topics. Prior information extracted from English sentiment lexicons through machine translation are incorporated into LSM model learning, where preferences on expectations of sentiment labels of those lexicon words are expressed using generalized expectation criteria. An efficient parameter estimation procedure using variational Bayes is presented. Experimental results on the Chinese product reviews show that the weakly-supervised LSM model performs comparably to supervised classifiers such as Support vector Machines with an average of 81% accuracy achieved over a total of 5484 review documents. Moreover, starting with a generic sentiment lexicon, the LSM model is able to extract highly domainspecific polarity words from text.

#index 1587356
#* Fractional similarity: cross-lingual feature selection for search
#@ Jagadeesh Jagarlamudi;Paul N. Bennett
#t 2011
#c 16
#% 309095
#% 590523
#% 754059
#% 783474
#% 783482
#% 818239
#% 840846
#% 879567
#% 1074132
#% 1132411
#% 1292644
#% 1338581
#% 1442577
#% 1450911
#% 1471315
#! Training data as well as supplementary data such as usagebased click behavior may abound in one search market (i.e., a particular region, domain, or language) and be much scarcer in another market. Transfer methods attempt to improve performance in these resourcescarce markets by leveraging data across markets. However, differences in feature distributions across markets can change the optimal model. We introduce a method called Fractional Similarity, which uses query-based variance within a market to obtain more reliable estimates of feature deviations across markets. An empirical analysis demonstrates that using this scoring method as a feature selection criterion in cross-lingual transfer improves relevance ranking in the foreign language and compares favorably to a baseline based on KL divergence

#index 1587357
#* Is a query worth translating: ask the users!
#@ Ahmed Hefny;Kareem Darwish;Ali Alkahky
#t 2011
#c 16
#% 262046
#% 340892
#% 789959
#% 807750
#% 817469
#% 879590
#% 881477
#% 987250
#% 1074075
#% 1132418
#% 1195839
#% 1195865
#% 1215368
#% 1275632
#% 1373775
#% 1450910
#% 1481362
#% 1674907
#! Users in many regions of the world are multilingual and they issue similar queries in different languages. Given a source language query, we propose query picking which involves finding equivalent target language queries in a large query log. Query picking treats translation as a search problem, and can serve as a translation method in the context of cross-language and multilingual search. Further, given that users usually issue queries when they think they can find relevant content, the success of query picking can serve as a strong indicator to the projected success of cross-language and multilingual search. In this paper we describe a system that performs query picking and we show that picked queries yield results that are statistically indistinguishable from a monolingual baseline. Further, using query picking to predict the effectiveness of cross-language results can have statistically significant effect on the success of multilingual search with improvements over a monolingual baseline. Multilingual merging methods that do not account for the success of query picking can often hurt retrieval effectiveness.

#index 1587358
#* Balancing exploration and exploitation in learning to rank online
#@ Katja Hofmann;Shimon Whiteson;Maarten de Rijke
#t 2011
#c 16
#% 296646
#% 384911
#% 577224
#% 590523
#% 818221
#% 989628
#% 1035578
#% 1073938
#% 1073970
#% 1074092
#% 1130811
#% 1166517
#% 1173704
#% 1195836
#% 1211840
#% 1268491
#% 1292763
#% 1392451
#% 1450912
#% 1496802
#! As retrieval systems become more complex, learning to rank approaches are being developed to automatically tune their parameters. Using online learning to rank approaches, retrieval systems can learn directly from implicit feedback, while they are running. In such an online setting, algorithms need to both explore new solutions to obtain feedback for effective learning, and exploit what has already been learned to produce results that are acceptable to users. We formulate this challenge as an exploration-exploitation dilemma and present the first online learning to rank algorithm that works with implicit feedback and balances exploration and exploitation. We leverage existing learning to rank data sets and recently developed click models to evaluate the proposed algorithm. Our results show that finding a balance between exploration and exploitation can substantially improve online retrieval performance, bringing us one step closer to making online learning to rank work in practice.

#index 1587359
#* ReFER: effective relevance feedback for entity ranking
#@ Tereza Iofciu;Gianluca Demartini;Nick Craswell;Arjen P. de Vries
#t 2011
#c 16
#% 378485
#% 742666
#% 878916
#% 987276
#% 1019189
#% 1022234
#% 1074126
#% 1074220
#% 1074223
#% 1100822
#% 1100827
#% 1415730
#% 1450969
#% 1483553
#% 1697440
#! Web search increasingly deals with structured data about people, places and things, their attributes and relationships. In such an environment an important sub-problem is matching a user's unstructured free-text query to a set of relevant entities. For example, a user might request 'Olympic host cities'. The most challenging general problem is to find relevant entities, of the correct type and characteristics, based on a free-text query that need not conform to any single ontology or category structure. This paper presents an entity ranking relevance feedback model, based on example entities specified by the user or on pseudo feedback. It employs the Wikipedia category structure, but augments that structure with 'smooth categories' to deal with the sparseness of the raw category information. Our experiments show the effectiveness of the proposed method, whether applied as a pseudo relevance feedback method or interactively with the user in the loop.

#index 1587360
#* The limits of retrieval effectiveness
#@ Ronan Cummins;Mounia Lalmas;Colm O'Riordan
#t 2011
#c 16
#% 1227647
#% 1227719
#% 1292526
#% 1450920
#% 1494813
#! Best match systems in Information Retrieval have long been one of the most predominant models used in both research and practice. It is argued that the effectiveness of these types of systems for the ad hoc task in IR has plateaued. In this short paper, we conduct experiments to find the upper limits of performance of these systems from three different perspectives. Our results on TREC data show that there is much room for improvement in terms of term-weighting and query reformulation in the ad hoc task given an entire information need.

#index 1587361
#* Learning conditional random fields from unaligned data for natural language understanding
#@ Deyu Zhou;Yulan He
#t 2011
#c 16
#% 464434
#% 983905
#% 1147649
#% 1251725
#! In this paper, we propose a learning approach to train conditional random fields from unaligned data for natural language understanding where input to model learning are sentences paired with predicate formulae (or abstract semantic annotations) without word-level annotations. The learning approach resembles the expectation maximization algorithm. It has two advantages, one is that only abstract annotations are needed instead of fully word-level annotations, and the other is that the proposed learning framework can be easily extended for training other discriminative models, such as support vector machines, from abstract annotations. The proposed approach has been tested on the DARPA Communicator Data. Experimental results show that it outperforms the hidden vector state (HVS) model, a modified hidden Markov model also trained on abstract annotations. Furthermore, the proposed method has been compared with two other approaches, one is the hybrid framework (HF) combining the HVS model and the support vector hidden Markov model, and the other is discriminative training of the HVS model (DT). The proposed approach gives a relative error reduction rate of 18.7% and 8.3% in F-measure when compared with HF and DT respectively.

#index 1587362
#* Subspace tracking for latent semantic analysis
#@ Radim Řehůřek
#t 2011
#c 16
#% 36160
#% 67565
#% 303703
#% 305242
#% 485556
#% 1132141
#% 1855047
#! Modern applications of Latent Semantic Analysis (LSA) must deal with enormous (often practically infinite) data collections, calling for a single-pass matrix decomposition algorithm that operates in constant memory w.r.t. the collection size. This paper introduces a streamed distributed algorithm for incremental SVD updates. Apart from the theoretical derivation, we present experiments measuring numerical accuracy and runtime performance of the algorithm over several data collections, one of which is the whole of the English Wikipedia.

#index 1587363
#* Text retrieval methods for item ranking in collaborative filtering
#@ Alejandro Bellogín;Jun Wang;Pablo Castells
#t 2011
#c 16
#% 124004
#% 280852
#% 321635
#% 397154
#% 813966
#% 1077150
#% 1083539
#% 1263574
#% 1650569
#! Collaborative Filtering (CF) aims at predicting unknown ratings of a user from other similar users. The uniqueness of the problem has made its formulation distinctive to other information retrieval problems. While the formulation has proved to be effective in rating prediction tasks, it has limited the potential connections between these algorithms and Information Retrieval (IR) models. In this paper we propose a common notational framework for IR and rating-based CF, as well as a technique to provide CF data with a particular structure, in order to be able to use any IR weighting function with it. We argue that the flexibility of our approach may yield to much better performing algorithms. In fact, in this work we have found that IR models perform well in item ranking tasks, along with different normalization strategies.

#index 1587364
#* Classifying with co-stems: a new representation for information filtering
#@ Nedim Lipka;Benno Stein
#t 2011
#c 16
#% 89358
#% 144034
#% 252011
#% 424028
#% 770870
#% 854646
#% 869471
#% 938687
#% 1016107
#% 1190357
#% 1263886
#% 1400087
#% 1697470
#! Besides the content the writing style is an important discriminator in information filtering tasks. Ideally, the solution of a filtering task employs a text representation that models both kinds of characteristics. In this respect word stems are clearly content capturing, whereas word suffixes qualify as writing style indicators. Though the latter feature type is used for part of speech tagging, it has not yet been employed for information filtering in general. We propose a text representation that combines both the output of a stemming algorithm (stems) and the stem-reduced words (co-stems). A co-stem can be a prefix, an infix, a suffix, or a concatenation of prefixes, infixes, or suffixes. Using accepted standard corpora, we analyze the discriminative power of this representation for a broad range of information filtering tasks to provide new insights into the adequacy and task-specificity of text representation models. Altogether we observe that co-stem-based representations outperform the classical bag of words model for several filtering tasks.

#index 1587365
#* Interactive trademark image retrieval by fusing semantic and visual content
#@ Marçal Rusiñol;David Aldavert;Dimosthenis Karatzas;Ricardo Toledo;Josep Lladós
#t 2011
#c 16
#% 249321
#% 443975
#% 479973
#% 677554
#% 832602
#% 990324
#% 1358531
#% 1855213
#! In this paper we propose an efficient queried-by-example retrieval system which is able to retrieve trademark images by similarity from patent and trademark offices' digital libraries. Logo images are described by both their semantic content, by means of the Vienna codes, and their visual contents, by using shape and color as visual cues. The trademark descriptors are then indexed by a locality-sensitive hashing data structure aiming to perform approximate k-NN search in high dimensional spaces in sub-linear time. The resulting ranked lists are combined by using a weighted Condorcet method and a relevance feedback step helps to iteratively revise the query and refine the obtained results. The experiments demonstrate the effectiveness and efficiency of this system on a realistic and large dataset.

#index 1587366
#* Dynamic two-stage image retrieval from large multimodal databases
#@ Avi Arampatzis;Konstantinos Zagoris;Savvas A. Chatzichristofis
#t 2011
#c 16
#% 194284
#% 766409
#% 905199
#% 1081621
#% 1121956
#% 1190089
#% 1227642
#% 1263582
#% 1279857
#% 1436776
#% 1450984
#% 1474484
#% 1492790
#% 1492803
#% 1858012
#% 1916127
#! Content-based image retrieval (CBIR) with global features is notoriously noisy, especially for image queries with low percentages of relevant images in a collection. Moreover, CBIR typically ranks the whole collection, which is inefficient for large databases. We experiment with a method for image retrieval from multimodal databases, which improves both the effectiveness and efficiency of traditional CBIR by exploring secondary modalities. We perform retrieval in a two-stage fashion: first rank by a secondary modality, and then perform CBIR only on the top-K items. Thus, effectiveness is improved by performing CBIR on a 'better' subset. Using a relatively 'cheap' first stage, efficiency is also improved via the fewer CBIR operations performed. Our main novelty is that K is dynamic, i.e. estimated per query to optimize a predefined effectiveness measure. We show that such dynamic two-stage setups can be significantly more effective and robust than similar setups with static thresholds previously proposed

#index 1587367
#* Comparing twitter and traditional media using topic models
#@ Wayne Xin Zhao;Jing Jiang;Jianshu Weng;Jing He;Ee-Peng Lim;Hongfei Yan;Xiaoming Li
#t 2011
#c 16
#% 722904
#% 769906
#% 769967
#% 1055682
#% 1214671
#% 1338675
#% 1355042
#% 1399992
#% 1400018
#% 1471244
#% 1472943
#% 1517899
#% 1536506
#% 1561559
#! Twitter as a new form of social media can potentially contain much useful information, but content analysis on Twitter has not been well studied. In particular, it is not clear whether as an information source Twitter can be simply regarded as a faster news feed that covers mostly the same information as traditional news media. In This paper we empirically compare the content of Twitter with a traditional news medium, New York Times, using unsupervised topic modeling. We use a Twitter-LDA model to discover topics from a representative sample of the entire Twitter. We then use text mining techniques to compare these Twitter topics with topics from New York Times, taking into consideration topic categories and types. We also study the relation between the proportions of opinionated tweets and retweets and topic categories and types. Our comparisons show interesting and useful findings for downstream IR or DM applications.

#index 1587368
#* Exploiting thread structures to improve smoothing of language models for forum post retrieval
#@ Huizhong Duan;Chengxiang Zhai
#t 2011
#c 16
#% 169781
#% 262096
#% 340899
#% 340948
#% 342707
#% 397129
#% 766430
#% 879602
#% 940042
#% 1074109
#% 1130311
#% 1195862
#% 1227593
#% 1227598
#% 1292733
#% 1721867
#! Due to many unique characteristics of forum data, forum post retrieval is different from traditional document retrieval and web search, raising interesting research questions about how to optimize the accuracy of forum post retrieval. In this paper, we study how to exploit the naturally available raw thread structures of forums to improve retrieval accuracy in the language modeling framework. Specifically, we propose and study two different schemes for smoothing the language model of a forum post based on the thread containing the post. We explore several different variants of the two schemes to exploit thread structures in different ways. We also create a human annotated test data set for forum post retrieval and evaluate the proposed smoothing methods using this data set. The experiment results show that the proposed methods for leveraging forum threads to improve estimation of document language models are effective, and they outperform the existing smoothing methods for the forum post retrieval task.

#index 1587369
#* Incorporating query expansion and quality indicators in searching microblog posts
#@ Kamran Massoudi;Manos Tsagkias;Maarten de Rijke;Wouter Weerkamp
#t 2011
#c 16
#% 340901
#% 750863
#% 1074097
#% 1301020
#% 1384210
#% 1399966
#% 1399992
#% 1400018
#% 1400082
#% 1429423
#! We propose a retrieval model for searching microblog posts for a given topic of interest. We develop a language modeling approach tailored to microblogging characteristics, where redundancy-based IR methods cannot be used in a straightforward manner. We enhance this model with two groups of quality indicators: textual and microblog specific. Additionally, we propose a dynamic query expansion model for microblog post retrieval. Experimental results on Twitter data reveal the usefulness of boolean search, and demonstrate the utility of quality indicators and query expansion in microblog search

#index 1587370
#* Discovering fine-grained sentiment with latent variable structured prediction models
#@ Oscar Täckström;Ryan McDonald
#t 2011
#c 16
#% 464434
#% 746885
#% 769892
#% 815915
#% 854646
#% 939848
#% 939897
#% 1013670
#% 1338590
#% 1470682
#% 1481637
#! In this paper we investigate the use of latent variable structured prediction models for fine-grained sentiment analysis in the common situation where only coarse-grained supervision is available. Specifically, we show how sentencelevel sentiment labels can be effectively learned from document-level supervision using hidden conditional random fields (HCRFs) [10]. Experiments show that this technique reduces sentence classification errors by 22% relative to using a lexicon and 13% relative to machine-learning baselines.

#index 1587371
#* Combining global and local semantic contexts for improving biomedical information retrieval
#@ Duy Dinh;Lynda Tamine
#t 2011
#c 16
#% 169729
#% 218978
#% 940042
#% 987274
#% 1039825
#% 1156209
#% 1156211
#% 1432309
#! In the context of biomedical information retrieval (IR), this paper explores the relationship between the document's global context and the query's local context in an attempt to overcome the term mismatch problem between the user query and documents in the collection. Most solutions to this problem have been focused on expanding the query by discovering its context, either global or local. In a global strategy, all documents in the collection are used to examine word occurrences and relationships in the corpus as a whole, and use this information to expand the original query. In a local strategy, the top-ranked documents retrieved for a given query are examined to determine terms for query expansion. We propose to combine the document's global context and the query's local context in an attempt to increase the term overlap between the user query and documents in the collection via document expansion (DE) and query expansion (QE). The DE technique is based on a statistical method (IR-based) to extract the most appropriate concepts (global context) from each document. The QE technique is based on a blind feedback approach using the top-ranked documents (local context) obtained in the first retrieval stage. A comparative experiment on the TREC 2004 Genomics collection demonstrates that the combination of the document's global context and the query's local context shows a significant improvement over the baseline. The MAP is significantly raised from 0.4097 to 0.4532 with a significant improvement rate of +10.62% over the baseline. The IR performance of the combined method in terms of MAP is also superior to official runs participated in TREC 2004 Genomics and is comparable to the performance of the best run (0.4075).

#index 1587372
#* Smoothing click counts for aggregated vertical search
#@ Jangwon Seo;W. Bruce Croft;Kwang Hyun Kim;Joon Ho Lee
#t 2011
#c 16
#% 340948
#% 375017
#% 577224
#% 719598
#% 794860
#% 818221
#% 838528
#% 879567
#% 879593
#% 989628
#% 1074092
#% 1074093
#% 1150174
#% 1190055
#% 1227621
#! Clickthrough data is a critical feature for improving web search ranking. Recently, many search portals have provided aggregated search, which retrieves relevant information from various heterogeneous collections called verticals. In addition to the well-known problem of rank bias, clickthrough data recorded in the aggregated search environment suffers from severe sparseness problems due to the limited number of results presented for each vertical. This skew in clickthrough data, which we call rank cut, makes optimization of vertical searches more difficult. In this work, we focus on mitigating the negative effect of rank cut for aggregated vertical searches. We introduce a technique for smoothing click counts based on spectral graph analysis. Using real clickthrough data from a vertical recorded in an aggregated search environment, we show empirically that clickthrough data smoothed by this technique is effective for improving the vertical search

#index 1587373
#* Automatic people tagging for expertise profiling in the enterprise
#@ Pavel Serdyukov;Mike Taylor;Vishwa Vinay;Matthew Richardson;Ryen W. White
#t 2011
#c 16
#% 907525
#% 913206
#% 987261
#% 998805
#% 1055704
#% 1074115
#% 1074117
#% 1130922
#% 1190090
#% 1195844
#% 1227750
#% 1275180
#% 1415734
#! In an enterprise search setting, there is a class of queries for which people, rather than documents, are desirable answers. However, presenting users with just a list of names of knowledgeable employees without any description of their expertise may lead to confusion, lack of trust in search results, and abandonment of the search engine. At the same time, building a concise meaningful description for a person is not a trivial summarization task. In this paper, we propose a solution to this problem by automatically tagging people for the purpose of profiling their expertise areas in the scope of the enterprise where they are employed. We address the novel task of automatic people tagging by using a machine learning algorithm that combines evidence that a certain tag is relevant to a certain employee acquired from different sources in the enterprise. We experiment with the data from a large distributed organization, which also allows us to study sources of expertise evidence that have been previously overlooked, such as personal click-through history. The evaluation of the proposed methods shows that our technique clearly outperforms state of the art approaches.

#index 1587374
#* Text classification: a sequential reading approach
#@ Gabriel Dulac-Arnold;Ludovic Denoyer;Patrick Gallinari
#t 2011
#c 16
#% 219052
#% 260001
#% 292684
#% 344447
#% 458379
#% 543918
#% 722803
#% 840583
#% 894252
#% 1415722
#% 1432977
#! We propose to model the text classification process as a sequential decision process. In this process, an agent learns to classify documents into topics while reading the document sentences sequentially and learns to stop as soon as enough information was read for deciding. The proposed algorithm is based on a modelisation of Text Classification as a Markov Decision Process and learns by using Reinforcement Learning. Experiments on four different classical mono-label corpora show that the proposed approach performs comparably to classical SVM approaches for large training sets, and better for small training sets. In addition, the model automatically adapts its reading process to the quantity of training information provided.

#index 1587375
#* Domain adaptation for text categorization by feature labeling
#@ Cristina Kadar;José Iria
#t 2011
#c 16
#% 464465
#% 466263
#% 722904
#% 956522
#% 1074125
#% 1074129
#% 1214639
#% 1214758
#% 1261539
#% 1270196
#% 1270680
#% 1270716
#% 1271481
#% 1305479
#% 1386130
#! We present a novel approach to domain adaptation for text categorization, which merely requires that the source domain data are weakly annotated in the form of labeled features. The main advantage of our approach resides in the fact that labeling words is less expensive than labeling documents. We propose two methods, the first of which seeks to minimize the divergence between the distributions of the source domain, which contains labeled features, and the target domain, which contains only unlabeled data. The second method augments the labeled features set in an unsupervised way, via the discovery of a shared latent concept space between source and target. We empirically show that our approach outperforms standard supervised and semi-supervised methods, and obtains results competitive to those reported by state-of-the-art domain adaptation methods, while requiring considerably less supervision.

#index 1587376
#* TEMPER: a temporal relevance feedback method
#@ Mostafa Keikha;Shima Gerani;Fabio Crestani
#t 2011
#c 16
#% 280846
#% 340901
#% 342707
#% 840583
#% 1074081
#% 1074094
#% 1074171
#% 1130913
#% 1130914
#% 1223498
#% 1227614
#% 1227630
#% 1292749
#% 1450879
#% 1742093
#! The goal of a blog distillation (blog feed search) method is to rank blogs according to their recurrent relevance to the query. An interesting property of blog distillation which differentiates it from traditional retrieval tasks is its dependency on time. In this paper we investigate the effect of time dependency in query expansion. We propose a framework, TEMPER, which selects different terms for different times and ranks blogs according to their relevancy to the query over time. By generating multiple expanded queries based on time, we are able to capture the dynamics of the topic both in aspects and vocabulary usage. We show performance gains over the baseline techniques which generate a single expanded query using the top retrieved posts or blogs irrespective of time.

#index 1587377
#* Terms of a feather: content-based news recommendation and discovery using twitter
#@ Owen Phelan;Kevin McCarthy;Mike Bennett;Barry Smyth
#t 2011
#c 16
#% 220709
#% 344447
#% 578684
#% 956521
#% 961586
#% 1040837
#% 1080077
#% 1106120
#% 1190122
#% 1287290
#% 1396094
#% 1396106
#% 1399992
#% 1476495
#! User-generated content has dominated the web's recent growth and today the so-called real-time web provides us with unprecedented access to the real-time opinions, views, and ratings of millions of users. For example, Twitter's 200m+ users are generating in the region of 1000+ tweets per second. In this work, we propose that this data can be harnessed as a useful source of recommendation knowledge. We describe a social news service called Buzzer that is capable of adapting to the conversations that are taking place on Twitter to ranking personal RSS subscriptions. This is achieved by a content-based approach of mining trending terms from both the public Twitter timeline and from the timeline of tweets published by a user's own Twitter friend subscriptions. We also present results of a live-user evaluation which demonstrates how these ranking strategies can add better item filtering and discovery value to conventional recency-based RSS ranking techniques.

#index 1587378
#* Topical and structural linkage in wikipedia
#@ Kelly Y. Itakura;Charles L. A. Clarke;Shlomo Geva;Andrew Trotman;Wei Chi Huang
#t 2011
#c 16
#% 262105
#% 868096
#% 1019082
#% 1100832
#% 1100835
#% 1130858
#% 1263251
#% 1292682
#% 1480887
#% 1489458
#! We explore statistical properties of links within Wikipedia. We demonstrate that a simple algorithm can predict many of the links that would normally be added to a new article, without considering the topic of the article itself. We then explore a variant of topic-oriented PageRank, which can effectively identify topical links within existing articles, when compared with manual judgments of their topical relevance. Based on these results, we suggest that linkages within Wikipedia arise from a combination of structural requirements and topical relationships

#index 1587379
#* An analysis of time-instability in web search results
#@ Jinyoung Kim;Vitor R. Carvalho
#t 2011
#c 16
#% 577370
#% 754058
#% 987211
#% 1166533
#! Due to the dynamic nature of web and the complex architectures of modern commercial search engines, top results in major search engines can change dramatically over time. Our experimental data shows that, for all three major search engines (Google, Bing and Yahoo!), approximately 90% of queries have their top 10 results altered within a period of ten days. Although this instability is expected in some situations such as in news-related queries, it is problematic in general because it can dramatically affect retrieval performance measurements and negatively affect users' perception of search quality (for instance, when users cannot re-find a previously found document). In this work we present the first large scale study on the degree and nature of these changes. We introduce several types of query instability, and several metrics to quantify it.We then present a quantitative analysis using 12,600 queries collected from a commercial web search engine over several weeks. Our analysis shows that the results from all major search engines have similar levels of instability, and that many of these changes are temporary. We also identified classes of queries with clearly different instability profiles - for instance, navigational queries are considerably more stable than non-navigational, while longer queries are significantly less stable than shorter ones.

#index 1587380
#* Rules of thumb for information acquisition from large and redundant data
#@ Wolfgang Gatterbauer
#t 2011
#c 16
#% 117070
#% 481749
#% 805799
#% 838536
#% 869616
#% 874992
#% 987275
#% 1172640
#% 1289516
#% 1300556
#% 1587380
#! We develop an abstract model of information acquisition from redundant data. We assume a random sampling process from data which contain information with bias and are interested in the fraction of information we expect to learn as function of (i) the sampled fraction (recall) and (ii) varying bias of information (redundancy distributions). We develop two rules of thumb with varying robustness. We first show that, when information bias follows a Zipf distribution, the 80-20 rule or Pareto principle does surprisingly not hold, and we rather expect to learn less than 40% of the information when randomly sampling 20% of the overall data. We then analytically prove that for large data sets, randomized sampling from power-law distributions leads to "truncated distributions" with the same power-law exponent. This second rule is very robust and also holds for distributions that deviate substantially from a strict power law. We further give one particular family of powerlaw functions that remain completely invariant under sampling. Finally, we validate our model with two large Web data sets: link distributions to web domains and tag distributions on delicious.com.

#index 1587381
#* Bringing why-QA to web search
#@ Suzan Verberne;Lou Boves;Wessel Kraaij
#t 2011
#c 16
#% 342398
#% 397160
#% 818221
#% 855235
#% 1055738
#% 1122105
#% 1186527
#% 1548983
#% 1565540
#! We investigated to what extent users could be satisfied by a web search engine for answering causal questions. We used an assessment environment in which a web search interface was simulated. For 1 401 why-queries from a search engine log we pre-retrieved the first 10 results using Bing. 311 queries were assessed by human judges. We found that even without clicking a result, 25.2% of the why-questions is answered on the first result page. If we count an intended click on a result as a vote for relevance, then 74.4% of the why-questions gets at least one relevant answer in the top-10. 10% of why-queries asked to web search engines are not answerable according to human assessors.

#index 1587382
#* The power of peers
#@ Nick Craswell;Dennis Fetterly;Marc Najork
#t 2011
#c 16
#% 282905
#% 309779
#% 1166529
#! We present a study of the contributions of three classes of ranking signals: BM25F, a retrieval function that is based on words in the content of web pages and the anchors that link to them; SALSA, a link-based feature that takes all or part of the result set to a query as input; and matching-anchor count (MAC), a feature that measures precise matches between queries and anchors pointing to result pages. All three features incorporate both link and textual features, but in varying degrees. BM25F is the state-of-the art exponent of Salton's term-vector model, and is based on a solid theoretical foundation; the two other features are somewhat more ad-hoc. We studied the impact of two factors that go into the formation of SALSA's "base" set: whether to use conjunctive or disjunctive query semantics, and how many results to include into the base set. We found that the choice of query semantics has little impact on the effectiveness of SALSA (with conjunctive semantics having a slight edge); more surprisingly, we found that limiting the size of the base set to a few hundred results of high expected quality maximizes performance. Furthermore, we experimented with various linear combinations of BM25F, MAC and SALSA. In doing so, we made a remarkable observation: adding BM25F to a two-way weighted linear combination of MAC and SALSA does not increase performance in any statistically significant way.

#index 1587383
#* Introducing the user-over-ranking hypothesis
#@ Benno Stein;Matthias Hagen
#t 2011
#c 16
#% 534271
#% 878624
#% 1074112
#% 1126945
#% 1195837
#% 1450865
#% 1450900
#% 1450970
#% 1467778
#% 1495123
#% 1517947
#! The User-over-Ranking hypothesis states that rather the user herself than a web search engine's ranking algorithm can help to improve retrieval performance. The means are longer queries that provide additional keywords. Readers who take this hypothesis for granted should recall the fact that virtually no user and none of the search index providers consider its implications. For readers who feel insecure about the claim, our paper gives empirical evidence.

#index 1587384
#* Second chance: a hybrid approach for dynamic result caching in search engines
#@ I. Sengor Altingovde;Rifat Ozcan;B. Barla Cambazoglu;Özgür Ulusoy
#t 2011
#c 16
#% 860861
#% 878624
#% 987215
#% 1190098
#% 1195885
#! Result caches are vital for efficiency of search engines. In this work, we propose a novel caching strategy in which a dynamic result cache is split into two layers: an HTML cache and a docID cache. The HTML cache in the first layer stores the result pages computed for queries. The docID cache in the second layer stores ids of documents in search results. Experiments under various scenarios show that, in terms of average query processing time, this hybrid caching approach outperforms the traditional approach, which relies only on the HTML cache.

#index 1587385
#* Learning models for ranking aggregates
#@ Craig Macdonald;Iadh Ounis
#t 2011
#c 16
#% 879570
#% 907525
#% 987241
#% 987356
#% 1019084
#% 1074094
#% 1074171
#% 1130913
#% 1227712
#% 1263610
#% 1268491
#% 1292752
#% 1450914
#% 1473303
#% 1537464
#! Aggregate ranking tasks are those where documents are not the final ranking outcome, but instead an intermediary component. For instance, in expert search, a ranking of candidate persons with relevant expertise to a query is generated after consideration of a document ranking. Many models exist for aggregate ranking tasks, however obtaining an effective and robust setting for different aggregate ranking tasks is difficult to achieve. In this work, we propose a novel learned approach to aggregate ranking, which combines different document ranking features as well as aggregate ranking approaches. We experiment with our proposed approach using two TREC test collections for expert and blog search. Our experimental results attest the effectiveness and robustness of a learned model for aggregate ranking across different settings

#index 1587386
#* Efficient compressed inverted index skipping for disjunctive text-queries
#@ Simon Jonassen;Svein Erik Bratsberg
#t 2011
#c 16
#% 198335
#% 213786
#% 228097
#% 730065
#% 818229
#% 864446
#% 867054
#% 976948
#% 987214
#% 1019138
#% 1035571
#% 1055710
#% 1055871
#% 1190095
#% 1683906
#% 1739411
#! In this paper we look at a combination of bulk-compression, partial query processing and skipping for document-ordered inverted indexes. We propose a new inverted index organization, and provide an updated version of the MaxScore method by Turtle and Flood and a skipping-adapted version of the space-limited adaptive pruning method by Lester et al. Both our methods significantly reduce the number of processed elements and reduce the average query latency by more than three times. Our experiments with a real implementation and a large document collection are valuable for a further research within inverted index skipping and query processing optimizations.

#index 1587387
#* Within-document term-based index pruning with statistical hypothesis testing
#@ Sree Lekha Thota;Ben Carterette
#t 2011
#c 16
#% 212665
#% 213786
#% 340887
#% 453323
#% 719598
#% 750863
#% 786632
#% 805862
#% 879611
#% 907504
#% 987323
#% 1019182
#% 1742113
#! Document-centric static index pruning methods provide smaller indexes and faster query times by dropping some withindocument term information from inverted lists. We present a method of pruning inverted lists derived from the formulation of unigram language models for retrieval. Our method is based on the statistical significance of term frequency ratios: using the two-sample two-proportion (2P2N) test, we statistically compare the frequency of occurrence of a word within a given document to the frequency of its occurrence in the collection to decide whether to prune it. Experimental results show that this technique can be used to significantly decrease the size of the index and querying speed with less compromise to retrieval effectiveness than similar heuristic methods. Furthermore, we give a formal statistical justification for such methods.

#index 1587388
#* SkipBlock: self-indexing for block-based inverted list
#@ Stéphane Campinas;Renaud Delbru;Giovanni Tummarello
#t 2011
#c 16
#% 69316
#% 213786
#% 464843
#% 864446
#% 949659
#% 1019138
#% 1035571
#% 1349832
#% 1739411
#! In large web search engines the performance of Information Retrieval systems is a key issue. Block-based compression methods are often used to improve the search performance, but current self-indexing techniques are not adapted to such data structure and provide suboptimal performance. In this paper, we present SkipBlock, a self-indexing model for block-based inverted lists. Based on a cost model, we show that it is possible to achieve significant improvements on both search performance and structure's space storage.

#index 1587389
#* Weight-based boosting model for cross-domain relevance ranking adaptation
#@ Peng Cai;Wei Gao;Kam-Fai Wong;Aoying Zhou
#t 2011
#c 16
#% 734915
#% 987241
#% 1074063
#% 1450849
#% 1456843
#! Adaptation techniques based on importance weighting were shown effective for RankSVM and RankNet, viz., each training instance is assigned a target weight denoting its importance to the target domain and incorporated into loss functions. In this work, we extend RankBoost using importance weighting framework for ranking adaptation. We find it non-trivial to incorporate the target weight into the boosting-based ranking algorithms because it plays a contradictory role against the innate weight of boosting, namely source weight that focuses on adjusting source-domain ranking accuracy. Our experiments show that among three variants, the additive weight-based RankBoost, which dynamically balances the two types of weights, significantly and consistently outperforms the baseline trained directly on the source domain.

#index 1587390
#* What makes re-finding information difficult? a study of email re-finding
#@ David Elsweiler;Mark Baillie;Ian Ruthven
#t 2011
#c 16
#% 65965
#% 126628
#% 199528
#% 214751
#% 247268
#% 297545
#% 309767
#% 318453
#% 340394
#% 415117
#% 642983
#% 734988
#% 751830
#% 751851
#% 805898
#% 832099
#% 860036
#% 987195
#% 987249
#% 1292597
#% 1384094
#% 1392496
#! Re-finding information that has been seen or accessed before is a task which can be relatively straight-forward, but often it can be extremely challenging, time-consuming and frustrating. Little is known, however, about what makes one re-finding task harder or easier than another. We performed a user study to learn about the contextual factors that influence users' perception of task difficulty in the context of re-finding email messages. 21 participants were issued re-finding tasks to perform on their own personal collections. The participants' responses to questions about the tasks combined with demographic data and collection statistics for the experimental population provide a rich basis to investigate the variables that can influence the perception of difficulty. A logistic regression model was developed to examine the relationships between variables and determine whether any factors were associated with perceived task difficulty. The model reveals strong relationships between difficulty and the time lapsed since a message was read, remembering when the sought-after email was sent, remembering other recipients of the email, the experience of the user and the user's filing strategy. We discuss what these findings mean for the design of re-finding interfaces and future re-finding research.

#index 1587391
#* A user-oriented model for expert finding
#@ Elena Smirnova;Krisztian Balog
#t 2011
#c 16
#% 260775
#% 402437
#% 662755
#% 730082
#% 739372
#% 750863
#% 879570
#% 907525
#% 913206
#% 987261
#% 987348
#% 1019135
#% 1043042
#% 1047421
#% 1133171
#% 1150169
#% 1195844
#% 1195880
#% 1392465
#% 1396090
#% 1400942
#% 1415732
#% 1432785
#! Expert finding addresses the problem of retrieving a ranked list of people who are knowledgeable on a given topic. Several models have been proposed to solve this task, but so far these have focused solely on returning the most knowledgeable people as experts on a particular topic. In this paper we argue that in a real-world organizational setting the notion of the "best expert" also depends on the individual user and her needs.We propose a user-oriented approach that balances two factors that influence the user's choice: time to contact an expert, and the knowledge value gained after. We use the distance between the user and an expert in a social network to estimate contact time, and consider various social graphs, based on organizational hierarchy, geographical location, and collaboration, as well as the combination of these. Using a realistic test set, created from interactions of employees with a university-wide expert search engine, we demonstrate substantial improvements over a state-of-the-art baseline on all retrieval measures.

#index 1587392
#* Simulating simple and fallible relevance feedback
#@ Feza Baskaya;Heikki Keskustalo;Kalervo Järvelin
#t 2011
#c 16
#% 262036
#% 306468
#% 340882
#% 397164
#% 580079
#% 742666
#% 793013
#% 1051040
#% 1135992
#% 1195833
#% 1227640
#% 1292769
#% 1526568
#% 1742085
#! Much of the research in relevance feedback (RF) has been performed under laboratory conditions using test collections and either test persons or simple simulation. These studies have given mixed results. The design of the present study is unique. First, the initial queries are realistically short queries generated by real end-users. Second, we perform a user simulation with several RF scenarios. Third, we simulate human fallibility in providing RF, i.e., incorrectness in feedback. Fourth, we employ graded relevance assessments in the evaluation of the retrieval results. The research question is: how does RF affect IR performance when initial queries are short and feedback is fallible? Our findings indicate that very fallible feedback is no different from pseudorelevance feedback (PRF) and not effective on short initial queries. However, RF with empirically observed fallibility is as effective as correct RF and able to improve the performance of short initial queries.

#index 1587393
#* AutoEval: an evaluation methodology for evaluating query suggestions using query logs
#@ M-Dyaa Albakour;Udo Kruschwitz;Nikolaos Nanas;Yunhyong Kim;Dawei Song;Maria Fasli;Anne De Roeck
#t 2011
#c 16
#% 280849
#% 340890
#% 728105
#% 818221
#% 857130
#% 987222
#% 1173699
#% 1201880
#% 1495111
#% 1517886
#! User evaluations of search engines are expensive and not easy to replicate. The problem is even more pronounced when assessing adaptive search systems, for example system-generated query modification suggestions that can be derived from past user interactions with a search engine. Automatically predicting the performance of different modification suggestion models before getting the users involved is therefore highly desirable. AutoEval is an evaluation methodology that assesses the quality of query modifications generated by a model using the query logs of past user interactions with the system. We present experimental results of applying this methodology to different adaptive algorithms which suggest that the predicted quality of different algorithms is in line with user assessments. This makes AutoEval a suitable evaluation framework for adaptive interactive search engines

#index 1587394
#* To seek, perchance to fail: expressions of user needs in internet video search
#@ Christoph Kofler;Martha Larson;Alan Hanjalic
#t 2011
#c 16
#% 424261
#% 1056077
#% 1130878
#% 1432775
#% 1455271
#! This work investigates user expressions of content needs in Internet video search, focusing on cases in which users have failed to meet their search goals, although relevant content is reasonably certain to exist. We study expressions of user needs in the form of requests (i.e., questions) formulated in natural language and published to Yahoo! Answers. Experiments show that classifiers can distinguish requests associated with search-goal failure. We identify a group of 'easy-to-predict' requests (cases for which the classifier predicts search-goal failure well) and compile an inventory of strategies used by users to express search goals in these cases. In a final set of experiments, we demonstrate the feasibility of predicting search-goal failure based on query-like representations of the original natural-language requests. The results of our study are intended to inform the future development of indexing and retrieval techniques for Internet video that target difficult queries.

#index 1587395
#* Passage reranking for question answering using syntactic structures and answer types
#@ Elif Aktolga;James Allan;David A. Smith
#t 2011
#c 16
#% 309124
#% 642979
#% 740915
#% 818253
#% 879612
#% 939343
#% 1299651
#% 1713600
#! Passage Retrieval is a crucial step in question answering systems, one that has been well researched in the past. Due to the vocabulary mismatch problem and independence assumption of bag-of-words retrieval models, correct passages are often ranked lower than other incorrect passages in the retrieved list. Whereas in previous work, passages are reranked only on the basis of syntactic structures of questions and answers, our method achieves a better ranking by aligning the syntactic structures based on the question's answer type and detected named entities in the candidate passage. We compare our technique with strong retrieval and reranking baselines. Experimental results using the TREC QA 1999-2003 datasets show that our method significantly outperforms the baselines over all ranks in terms of the MRR measure.

#index 1587396
#* An iterative approach to text segmentation
#@ Fei Song;William M. Darling;Adnan Duric;Fred W. Kroon
#t 2011
#c 16
#% 278106
#% 448786
#% 706148
#% 742204
#% 748583
#% 815855
#% 1264752
#% 1270688
#% 1410887
#! We present divSeg, a novel method for text segmentation that iteratively splits a portion of text at its weakest point in terms of the connectivity strength between two adjacent parts. To search for the weakest point, we apply two different measures: one is based on language modeling of text segmentation and the other, on the interconnectivity between two segments. Our solution produces a deep and narrow binary tree - a dynamic object that describes the structure of a text and that is fully adaptable to a user's segmentation needs. We treat it as a separate task to flatten the tree into a broad and shallow hierarchy either through supervised learning of a document set or explicit input of how a text should be segmented. The rich structure of our created tree further allows us to segment documents at varying levels such as topic, sub-topic, etc. We evaluated our new solution on a set of 265 articles from Discover magazine where the topic structures are unknown and need to be discovered. Our experimental results show that the iterative approach has the potential to generate better segmentation results than several leading baselines, and the separate flattening step allows us to adapt the results to different levels of details and user preferences.

#index 1587397
#* Improving query focused summarization using look-ahead strategy
#@ Rama Badrinath;Suresh Venkatasubramaniyan;C. E. Veni Madhavan
#t 2011
#c 16
#% 268079
#% 939968
#% 961700
#% 1130898
#% 1133172
#% 1264797
#% 1272053
#% 1275220
#% 1310413
#% 1544449
#! Query focused summarization is the task of producing a compressed text of original set of documents based on a query. Documents can be viewed as graph with sentences as nodes and edges can be added based on sentence similarity. Graph based ranking algorithms which use 'Biased random surfer model' like topic-sensitive LexRank have been successfully applied to query focused summarization. In these algorithms, random walk will be biased towards the sentences which contain query relevant words. Specifically, it is assumed that random surfer knows the query relevance score of the sentence to where he jumps. However, neighbourhood information of the sentence to where he jumps is completely ignored. In this paper, we propose look-ahead version of topic-sensitive LexRank. We assume that random surfer not only knows the query relevance of the sentence to where he jumps but he can also look N-step ahead from that sentence to find query relevance scores of future set of sentences. Using this look ahead information, we figure out the sentences which are indirectly related to the query by looking at number of hops to reach a sentence which has query relevant words. Then we make the random walk biased towards even to the indirect query relevant sentences along with the sentences which have query relevant words. Experimental results show 20.2% increase in ROUGE-2 score compared to topic-sensitive LexRank on DUC 2007 data set. Further, our system outperforms best systems in DUC 2006 and results are comparable to state of the art systems.

#index 1587398
#* A generalized method for word sense disambiguation based on wikipedia
#@ Chenliang Li;Aixin Sun;Anwitaman Datta
#t 2011
#c 16
#% 286069
#% 747890
#% 816067
#% 854641
#% 855094
#% 939381
#% 1019082
#% 1083703
#% 1130858
#% 1190121
#% 1214660
#% 1250362
#% 1250381
#% 1260448
#% 1292487
#% 1450830
#! In this paper we propose a general framework for word sense disambiguation using knowledge latent in Wikipedia. Specifically, we exploit the rich and growing Wikipedia corpus in order to achieve a large and robust knowledge repository consisting of keyphrases and their associated candidate topics. Keyphrases are mainly derived from Wikipedia article titles and anchor texts associated with wikilinks. The disambiguation of a given keyphrase is based on both the commonness of a candidate topic and the context-dependent relatedness where unnecessary (and potentially noisy) context information is pruned. With extensive experimental evaluations using different relatedness measures, we show that the proposed technique achieved comparable disambiguation accuracies with respect to state-of-the-art techniques, while incurring orders of magnitude less computation cost.

#index 1587399
#* Representing document lengths with identifiers
#@ Raffaele Perego;Fabrizio Silvestri;Nicola Tonellotto
#t 2011
#c 16
#% 1077150
#% 1343447
#% 1480887
#! The length of each indexed document is needed by most common text retrieval scoring functions to rank it with respect to the current query. For efficiency purposes information retrieval systems maintain this information in the main memory. This paper proposes a novel strategy to encode the length of each document directly in the document identifier, thus reducing main memory demand. The technique is based on a simple document identifier assignment method and a function allowing the approximate length of each indexed document to be computed analytically.

#index 1587400
#* Free-text search versus complex web forms
#@ Kien Tjin-Kam-Jet;Dolf Trieschnigg;Djoerd Hiemstra
#t 2011
#c 16
#% 459241
#% 855020
#% 1127557
#% 1402199
#% 1497264
#! We investigated the use of free-text queries as an alternative means for searching 'behind' web forms. We conducted a user study where we evaluated our prototype free-text interface in a travel planner scenario. Our results show that users prefer this free-text interface over the original web form and that they are about 9% faster on average at completing their search tasks.

#index 1587401
#* Multilingual log analysis: LogCLEF
#@ Giorgio Maria Di Nunzio;Johannes Leveling;Thomas Mandl
#t 2011
#c 16
#% 1190072
#% 1312816
#% 1494817
#% 1494818
#% 1494821
#! The current lack of recent and long-term query logs makes the verifiability and repeatability of log analysis experiments very limited. A first attempt in this direction has been made within the Cross-Language Evaluation Forum in 2009 in a track named LogCLEF which aims to stimulate research on user behaviour in multilingual environments and promote standard evaluation collections of log data.We report on similarities and differences of the most recent activities for LogCLEF.

#index 1587402
#* A large-scale system evaluation on component-level
#@ Jens Kürsten;Maximilian Eibl
#t 2011
#c 16
#% 340899
#% 907544
#% 1077150
#% 1195854
#% 1227586
#% 1292550
#% 1494823
#% 1496282
#% 1742070
#! This article describes a large-scale empirical evaluation across different types of English text collections. We ran about 140,000 experiments and analyzed the results on system component-level to find out if we can select configurations that perform reliable on specific types of corpora. To our own surprise we observed that a specific set of configuration parameters achieved 95% of the optimal average MAP across all collections. We conclude that this configuration could be used as baseline reference for evaluation of new IR approaches on English text corpora.

#index 1587403
#* Should MT systems be used as black boxes in CLIR?
#@ Walid Magdy;Gareth J. F. Jones
#t 2011
#c 16
#% 1215368
#% 1450905
#! The translation stage in cross language information retrieval (CLIR) acts as the main enabling stage to cross the language barrier between documents and queries. In recent years machine translation (MT) systems have become the dominant approach to translation in CLIR. However, unlike information retrieval (IR), MT focuses on the morphological and syntactical quality of the sentence. This requires large training resources and high computational power for training and translation. We present a novel technique for MT designed specifically for CLIR. In this method IR text pre-processing in the form of stop word removal and stemming are applied to the MT training corpus prior to the training phase. Applying this pre-processing step is found to significantly speed up the translation process without affecting the retrieval quality.

#index 1587404
#* Video retrieval based on words-of-interest selection
#@ Lei Wang;Dawei Song;Eyad Elyan
#t 2011
#c 16
#% 1058303
#% 1169843
#% 1279775
#! Query-by-example video retrieval is receiving an increasing attention in recent years. One of the state-of-art approaches is the Bag-of-visual Words (BoW) based technique, where images are described by a set of local features mapped to a discrete set of visual words. Such techniques, however, ignores spatial relations between visual words. In this paper, we present a content based video retrieval technique based on selected Words-of-Interest (WoI) that utilizes visual words spatial proximity constraint identified from the query. Experiments carried out on a public video database demonstrate promising results of our approach that outperform the classical BoW approach.

#index 1587405
#* Classic children's literature - difficult to read ?
#@ Dolf Trieschnigg;Claudia Hauff
#t 2011
#c 16
#% 861988
#% 1076733
#! Classic children's literature such as Alice in Wonderland is nowadays freely available thanks to initiatives such as Project Gutenberg. Due to diverging vocabularies and style, these texts are often not readily understandable to children in the present day. Our goal is to make such texts more accessible by aiding children in the reading process, in particular by automatically identifying the terms that result in low readability. As a first step, in this poster we report on a preliminary user study that investigates the extent of the vocabulary problem. We also propose and evaluate a basic approach to detect such difficult terminology.

#index 1587406
#* Applying machine learning diversity metrics to data fusion in information retrieval
#@ David Leonard;David Lillis;Lusheng Zhang;Fergus Toolan;Rem W. Collier;John Dunnion
#t 2011
#c 16
#% 232703
#% 551723
#% 1415738
#! The Supervised Machine Learning task of classification has parallels with Information Retrieval (IR): in each case, items (documents in the case of IR) are required to be categorised into discrete classes (relevant or non-relevant). Thus a parallel can also be drawn between classifier ensembles, where evidence from multiple classifiers are combined to achieve a superior result, and the IR data fusion task. This paper presents preliminary experimental results on the applicability of classifier ensemble diversity metrics in data fusion. Initial results indicate a relationship between the quality of the fused result set (as measured by MAP) and the diversity of its inputs

#index 1587407
#* Reranking collaborative filtering with multiple self-contained modalities
#@ Yue Shi;Martha Larson;Alan Hanjalic
#t 2011
#c 16
#% 734590
#% 734594
#% 813966
#% 1131845
#% 1227601
#% 1486771
#! A reranking algorithm, Multi-Rerank, is proposed to refine the recommendation list generated by collaborative filtering approaches. Multi-Rerank is capable of capturing multiple self-contained modalities, i.e., item modalities extractable from user-item matrix, to improve recommendation lists. Experimental results indicate that Multi-Rerank is effective for improving various CF approaches and additional benefits can be achieved when reranking with multiple modalities rather than a single modality.

#index 1587408
#* How far are we in trust-aware recommendation?
#@ Yue Shi;Martha Larson;Alan Hanjalic
#t 2011
#c 16
#% 790459
#% 1001279
#% 1130901
#% 1227602
#% 1287243
#% 1480670
#! Social trust holds great potential for improving recommendation and much recent work focuses on the use of social trust for rating prediction, in particular, in the context of the Epinions dataset. An experimental comparison with trust-free, naïve approaches suggests that state-of-the-art social-trust-aware recommendation approaches, in particular Social Trust Ensemble (STE), can fail to isolate the true added value of trust. We demonstrate experimentally that not only trust-set users, but also random users can be exploited to yield recommendation improvement via STE. Specific users, however, do benefit from use of social trust, and we conclude with an investigation of their characteristics.

#index 1587409
#* Re-ranking for multimedia indexing and retrieval
#@ Bahjat Safadi;Georges Quénot
#t 2011
#c 16
#% 990300
#% 997095
#! We proposed a re-ranking method for improving the performance of semantic video indexing and retrieval. Experimental results show that the proposed re-ranking method is effective and it improves the system performance on average by about 16-22% on TRECVID 2010 semantic indexing task.

#index 1587410
#* Combining query translation techniques to improve cross-language information retrieval
#@ Benjamin Herbert;György Szarvas;Iryna Gurevych
#t 2011
#c 16
#% 340895
#% 732849
#% 732851
#% 1432220
#% 1432241
#% 1450958
#! In this paper we address the combination of query translation approaches for cross-language information retrieval (CLIR). We translate queries with Google Translate and extend them with new translations obtained by mapping noun phrases in the query to concepts in the target language using Wikipedia. For two CLIR collections, we show that the proposed model provides meaningful translations that improve the strong baseline CLIR model based on a top performing SMT system.

#index 1587411
#* Back to the roots: mean-variance analysis of relevance estimations
#@ Guido Zuccon;Leif Azzopardi;Keith van Rijsbergen
#t 2011
#c 16
#% 784148
#% 1195829
#% 1227591
#% 1697434
#! Recently, mean-variance analysis has been proposed as a novel paradigm to model document ranking in Information Retrieval. The main merit of this approach is that it diversifies the ranking of retrieved documents. In its original formulation, the strategy considers both the mean of relevance estimates of retrieved documents and their variance. However, when this strategy has been empirically instantiated, the concepts of mean and variance are discarded in favour of a point-wise estimation of relevance (to replace the mean) and of a parameter to be tuned or, alternatively, a quantity dependent upon the document length (to replace the variance). In this paper we revisit this ranking strategy by going back to its roots: mean and variance. For each retrieved document, we infer a relevance distribution from a series of point-wise relevance estimations provided by a number of different systems. This is used to compute the mean and the variance of document relevance estimates. On the TREC Clueweb collection, we show that this approach improves the retrieval performances. This development could lead to new strategies to address the fusion of relevance estimates provided by different systems.

#index 1587412
#* A novel re-ranking approach inspired by quantum measurement
#@ Xiaozhao Zhao;Peng Zhang;Dawei Song;Yuexian Hou
#t 2011
#c 16
#% 327117
#% 340948
#% 758200
#% 1482184
#% 1697443
#! Quantum theory (QT) has recently been employed to advance the theory of information retrieval (IR). A typical method, namely the Quantum Probability Ranking Principle (QPRP), was proposed to re-rank top retrieved documents by considering the interdependencies between documents through the "quantum interference". In this paper, we attempt to explore another important QT concept, namely the "quantum measurement". Inspired by the photon polarization experiment underpinning the "quantum measurement", we propose a novel reranking approach. Evaluation on several TREC data sets shows that in ad-hoc retrieval, our method can significantly improve the first-round ranking from a baseline retrieval model, and also outperform the QPRP.

#index 1587413
#* Simple vs. sophisticated approaches for patent prior-art search
#@ Walid Magdy;Patrice Lopez;Gareth J. F. Jones
#t 2011
#c 16
#% 1450905
#% 1494802
#! Patent prior-art search is concerned with finding all filed patents relevant to a given patent application. We report a comparison between two search approaches representing the state-of-the-art in patent prior-art search. The first approach uses simple and straightforward information retrieval (IR) techniques, while the second uses much more sophisticated techniques which try to model the steps taken by a patent examiner in patent search. Experiments show that the retrieval effectiveness using both techniques is statistically indistinguishable when patent applications contain some initial citations. However, the advanced search technique is statistically better when no initial citations are provided. Our findings suggest that less time and effort can be exerted by applying simple IR approaches when initial citations are provided.

#index 1587414
#* Towards quantum-based DB+IR processing based on the principle of polyrepresentation
#@ David Zellhöfer;Ingo Frommholz;Ingo Schmitt;Mounia Lalmas;Keith van Rijsbergen
#t 2011
#c 16
#% 758200
#% 835027
#% 893639
#% 1021953
#% 1051060
#% 1312979
#% 1455257
#% 1462677
#! The cognitively motivated principle of polyrepresentation still lacks a theoretical foundation in IR. In this work, we discuss two competing polyrepresentation frameworks that are based on quantum theory. Both approaches support different aspects of polyrepresentation, where one is focused on the geometric properties of quantum theory while the other has a strong logical basis. We compare both approaches and outline how they can be combined to express further aspects of polyrepresentation.

#index 1587415
#* ATTention: understanding authors and topics in context of temporal evolution
#@ Nasir Naveed;Sergej Sizov;Steffen Staab
#t 2011
#c 16
#% 722904
#% 788094
#% 868088
#% 881498
#% 1155704
#! Understanding thematic trends and user roles is an important challenge in the field of information retrieval. In this contribution, we present a novel model for analyzing evolution of user's interests with respect to produced content over time. Our approach ATTention (a name derived from analysis of Authors and Topics in the Temporal context) addresses this problem by means of Bayesian modeling of relations between authors, latent topics and temporal information. We also present results of preliminary evaluations with scientific publication datasets and discuss opportunities of model use in novel mining and recommendation scenarios.

#index 1587416
#* Role of emotional features in collaborative recommendation
#@ Yashar Moshfeghi;Joemon M. Jose
#t 2011
#c 16
#% 1051062
#% 1074100
#% 1195834
#! The aim of this poster is to investigate the role of emotion in the collaborative filtering task. For this purpose, a kernel-based collaborative recommendation technique is used. The experiment is conducted on two MovieLens data sets. The emotional features are extracted from the movie reviews and plot summaries. The results show that emotional features are capable of enhancing recommendation effectiveness.

#index 1587417
#* The importance of the depth for text-image selection strategy in learning-to-rank
#@ David Buffoni;Sabrina Tollari;Patrick Gallinari
#t 2011
#c 16
#% 40313
#% 387427
#% 577224
#% 750863
#% 990258
#% 1211822
#% 1227635
#% 1916106
#! We examine the effect of the number documents being pooled, for constructing training sets, has on the performance of the learning-torank (LTR) approaches that use it to build our ranking functions. Our investigation takes place in a multimedia setting and uses the ImageCLEF photo 2006 dataset based on text and visual features. Experiments show that our LTR algorithm, OWPC,outperforms other baselines.

#index 1587418
#* Personal blog retrieval using opinion features
#@ Shima Gerani;Mostafa Keikha;Mark Carman;Fabio Crestani
#t 2011
#c 16
#% 279755
#! Faceted blog distillation aims at finding blogs with recurring interest to a topic while satisfying a specific facet of interest. In this paper we focus on the personal facet and propose a method that uses opinion features as indicators of personal content. Experimental results on TREC BLOG08 data-set confirm our intuition that personal blogs are more opinionated.

#index 1587419
#* Processing queries in session in a quantum-inspired IR framework
#@ Ingo Frommholz;Benjamin Piwowarski;Mounia Lalmas;Keith van Rijsbergen
#t 2011
#c 16
#% 378462
#% 758200
#% 1130868
#% 1348342
#% 1482184
#! In a search session, users tend to reformulate their queries, for instance because they want to generalise or specify them, or because they are undergoing a drift in their information need. This motivates to regard queries not in isolation, but within the session they are embedded in. In this poster, we propose an approach inspired by quantum mechanics to represent queries and their reformulations as density operators. Differently constructed densities can potentially be applied for different types of query reformulation. To do so, we propose and discuss indicators that can hint us to the type of query reformulation we are dealing with.

#index 1587420
#* Towards predicting relevance using a quantum-like framework
#@ Emanuele Di Buccio;Massimo Melucci;Dawei Song
#t 2011
#c 16
#% 758200
#% 907516
#% 1450876
#% 1697443
#! In this paper, the user's relevance state is modeled using quantum-like probability and the interference term is proposed so as to model the evolution of the state and the user's uncertainty about the assessment. The theoretical framework has been formulated and the results of an experimental user study based on a TREC test collection have been reported.

#index 1587421
#* Fusion vs. two-stage for multimodal retrieval
#@ Avi Arampatzis;Konstantinos Zagoris;Savvas A. Chatzichristofis
#t 2011
#c 16
#% 194284
#% 1190089
#% 1227642
#% 1450984
#% 1587366
#% 1916127
#! We compare two methods for retrieval from multimodal collections. The first is a score-based fusion of results, retrieved visually and textually. The second is a two-stage method that visually re-ranks the top-K results textually retrieved. We discuss their underlying hypotheses and practical limitations, and contact a comparative evaluation on a standardized snapshot of Wikipedia. Both methods are found to be significantly more effective than single-modality baselines, with no clear winner but with different robustness features. Nevertheless, two-stage retrieval provides efficiency benefits over fusion.

#index 1587422
#* Combination of feature selection methods for text categorisation
#@ Robert Neumayer;Rudolf Mayer;Kjetil Nørvåg
#t 2011
#c 16
#% 344447
#% 413637
#% 722935
#% 907577
#! Feature selection plays a vital role in text categorisation. A range of different methods have been developed, each having unique properties and selecting different features. We show some results of an extensive study of feature selection approaches using a wide range of combination methods. We performed experiments on 18 test collections and report a subset of the results.

#index 1587423
#* Time-Surfer: time-based graphical access to document content
#@ Hector Llorens;Estela Saquete;Borja Navarro;Robert Gaizauskas
#t 2011
#c 16
#% 987394
#% 1024551
#% 1472159
#% 1697416
#% 1697483
#! This demonstration presents a novel interactive graphical interface to document content focusing on the time dimension. The objective of Time-Surfer is to let users search and explore information related to a specific period, event, or event participant within a document. The system is based on the automatic detection not only of time expressions, but also of events and temporal relations. Through a zoomable timeline interface, it brings users an dynamic picture of the temporal distribution of events within a document. Time-Surfer has been successfully applied to history and biographical articles from Wikipedia.

#index 1587424
#* ARES: a retrieval engine based on sentiments sentiment-based search result annotation and diversification
#@ Gianluca Demartini
#t 2011
#c 16
#% 1355060
#% 1400021
#% 1457110
#% 1475758
#! This paper introduces a system enriching the standard web search engine interface with sentiment information. Additionally, it exploits such annotations to diversify the result list based on the different sentiments expressed by retrieved web pages. Thanks to the annotations, the end user is aware of which opinions the search engine is showing her and, thanks to the diversification, she can see an overview of the different opinions expressed about the requested topic. We describe the methods used for computing sentiment scores of web search results and for re-ranking them in order to cover different sentiment classes. The proposed system, built on top of commercial search engine APIs, is available on-line.

#index 1587425
#* Web search query assistance functionality for young audiences
#@ Carsten Eickhoff;Tamara Polajnar;Karl Gyllstrom;Sergio Duarte Torres;Richard Glassey
#t 2011
#c 16
#% 197968
#% 344923
#% 1384136
#% 1450937
#% 1455283
#% 1482344
#! The Internet plays an important role in people's daily lives. This is not only true for adults, but also holds for children; however, current web search engines are designed with adult users and their cognitive abilities in mind. Consequently, children face considerable barriers when using these information systems. In this work, we demonstrate the use of query assistance and search moderation techniques as well as appropriate interface design to overcome or mitigate these challenges.

#index 1587426
#* Conversation retrieval from twitter
#@ Matteo Magnani;Danilo Montesi;Gabriele Nunziante;Luca Rossi
#t 2011
#c 16
#% 215225
#% 232662
#% 268079
#% 340914
#% 360717
#% 754116
#% 765408
#% 1015269
#! The process of retrieving conversations from social network sites differs from traditional Web information retrieval because it involves human communication aspects, like the degree of interest in the conversation explicitly or implicitly expressed by the interacting people and their influence/popularity. Our demo allows users to include these aspects into the search process. The system allows the retrieval of millions of conversations generated on the popular Twitter social network site, and in particular conversations about trending topics.

#index 1587427
#* Finding useful users on twitter: twittomender the followee recommender
#@ John Hannon;Kevin McCarthy;Barry Smyth
#t 2011
#c 16
#% 220711
#% 1169572
#% 1183090
#% 1396094
#% 1476470
#! This paper examines an application for finding pertinent friends (followees) on Twitter. Whilst Twitter provides a great basis for receiving information, we believe a potential downfall lies in the lack of an effective way in which users of Twitter can find other Twitter users to follow. We apply several recommendation techniques to build a followee recommender for Twitter. We evaluate a variety of different recommendation strategies, using real-user data, to demonstrate the potential for this recommender system to correctly identify and promote interesting users who are worth following.

#index 1587428
#* Visual exploration of health information for children
#@ Frans van der Sluis;Sergio Duarte Torres;Djoerd Hiemstra;Betsy van Dijk;Frea Kruisinga
#t 2011
#c 16
#% 27049
#% 328524
#% 1489457
#! Children experience several difficulties retrieving information using current Information Retrieval (IR) systems. Particularly, children struggle to find the right keywords to construct queries given their lack of domain knowledge. This problem is even more critical in the case of the specialized health domain. In this work we present a novel method to address this problem using a cross-media search interface in which the textual data is searched through visual images. This solution aims to solve the recall and recognition problem which is salient for health information, by replacing the need for a vocabulary with the easy task of recognising the different body parts.

#index 1697411
#* Proceedings of the 32nd European conference on Advances in Information Retrieval
#@ Cathal Gurrin;Yulan He;Gabriella Kazai;Udo Kruschwitz;Suzanne Little
#t 2010
#c 16

#index 1697412
#* Recent developments in information retrieval
#@ Cathal Gurrin;Yulan He;Gabriella Kazai;Udo Kruschwitz;Suzanne Little;Thomas Roelleke;Stefan Rüger;Keith van Rijsbergen
#t 2010
#c 16
#% 1697416
#% 1697417
#% 1697418
#% 1697419
#% 1697420
#% 1697421
#% 1697422
#% 1697423
#% 1697424
#% 1697425
#% 1697426
#% 1697427
#% 1697428
#% 1697429
#% 1697430
#% 1697431
#% 1697432
#% 1697433
#% 1697434
#% 1697435
#% 1697436
#% 1697437
#% 1697438
#% 1697439
#% 1697440
#% 1697441
#% 1697442
#% 1697443
#% 1697444
#% 1697445
#% 1697446
#% 1697447
#% 1697448
#% 1697449
#% 1697450
#% 1697451
#% 1697452
#% 1697453
#% 1697454
#% 1697455
#% 1697456
#% 1697457
#% 1697458
#% 1697459
#! This paper summarizes the scientific work presented at the 32nd European Conference on Information Retrieval. It demonstrates that information retrieval (IR) as a research area continues to thrive with progress being made in three complementary sub-fields, namely IR theory and formal methods together with indexing and query representation issues, furthermore Web IR as a primary application area and finally research into evaluation methods and metrics. It is the combination of these areas that gives IR its solid scientific foundations. The paper also illustrates that significant progress has been made in other areas of IR. The keynote speakers addressed three such subject fields, social search engines using personalization and recommendation technologies, the renewed interest in applying natural language processing to IR, and multimedia IR as another fast-growing area.

#index 1697413
#* Web search futures: personal, collaborative, social
#@ Barry Smyth
#t 2010
#c 16
#! In this talk we will discuss where Web search may be heading, focusing on a number of large-scale research projects that are trying to develop the “next big thing” in Web search. We will consider some important recent initiatives on how to improve the quality of the Web search experience by helping search engines to respond to our individual needs and preferences. In turn, we will focus on some innovative work on how to take advantage of the inherently collaborative nature of Web search as we discuss recent attempts to develop so-called “social search engines”.

#index 1697414
#* IR, NLP, and visualization
#@ Hinrich Schütze
#t 2010
#c 16
#! In the last ten years natural language processing (NLP) has become an essential part of many information retrieval systems, mainly in the guise of question answering, summarization, machine translation and preprocessing such as decompounding. However, most of these methods are shallow. More complex natural language processing is not yet sufficiently reliable to be used in IR. I will discuss how new visualization technology and rich interactive environments offer new opportunities for complex NLP in IR.

#index 1697415
#* Image and natural language processing for multimedia information retrieval
#@ Mirella Lapata
#t 2010
#c 16
#! Image annotation, the task of automatically generating description words for a picture, is a key component in various image search and retrieval applications. Creating image databases for model development is, however, costly and time consuming, since the keywords must be hand-coded and the process repeated for new collections. In this work we exploit the vast resource of images and documents available on the web for developing image annotation models without any human involvement. We describe a probabilistic framework based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics. Applications of this framework to image annotation and retrieval show performance gains over previously proposed approaches, despite the noisy nature of our dataset. We also discuss how the proposed model can be used for story picturing, i.e., to find images that appropriately illustrate a text and demonstrate its utility when interfaced with an image caption generator.

#index 1697416
#* A language modeling approach for temporal information needs
#@ Klaus Berberich;Srikanta Bedathur;Omar Alonso;Gerhard Weikum
#t 2010
#c 16
#% 262096
#% 730070
#% 768285
#% 805848
#% 807756
#% 943834
#% 960414
#% 1024551
#% 1077150
#% 1107069
#% 1130999
#% 1150163
#% 1227692
#% 1415764
#! This work addresses information needs that have a temporal dimension conveyed by a temporal expression in the user’s query. Temporal expressions such as “in the 1990s” are frequent, easily extractable, but not leveraged by existing retrieval models. One challenge when dealing with them is their inherent uncertainty. It is often unclear which exact time interval a temporal expression refers to. We integrate temporal expressions into a language modeling approach, thus making them first-class citizens of the retrieval model and considering their inherent uncertainty. Experiments on the New York Times Annotated Corpus using Amazon Mechanical Turk to collect queries and obtain relevance assessments demonstrate that our approach yields substantial improvements in retrieval effectiveness.

#index 1697417
#* Analyzing information retrieval methods to recover broken web links
#@ Juan Martinez-Romo;Lourdes Araujo
#t 2010
#c 16
#% 115608
#% 209685
#% 340928
#% 344929
#% 595994
#% 881071
#% 884589
#% 1077150
#% 1107070
#% 1111091
#% 1206708
#! In this work we compare different techniques to automatically find candidate web pages to substitute broken links. We extract information from the anchor text, the content of the page containing the link, and the cache page in some digital library. The selected information is processed and submitted to a search engine. We have compared different information retrieval methods for both, the selection of terms used to construct the queries submitted to the search engine, and the ranking of the candidate pages that it provides, in order to help the user to find the best replacement. In particular, we have used term frequencies, and a language model approach for the selection of terms; and cooccurrence measures and a language model approach for ranking the final results. To test the different methods, we have also defined a methodology which does not require the user judgments, what increases the objectivity of the results.

#index 1697418
#* Between bags and trees – constructional patterns in text used for attitude identification
#@ Jussi Karlgren;Gunnar Eriksson;Magnus Sahlgren;Oscar Täckström
#t 2010
#c 16
#% 732844
#% 742406
#% 791663
#% 939897
#% 1117691
#% 1271268
#% 1271319
#! This paper describes experiments to use non-terminological information to find attitudinal expressions in written English text. The experiments are based on an analysis of text with respect to not only the vocabulary of content terms present in it (which most other approaches use as a basis for analysis) but also with respect to presence of structural features of the text represented by constructional features (typically disregarded by most other analyses). In our analysis, following a construction grammar framework, structural features are treated as occurrences, similarly to the treatment of vocabulary features. The constructional features in play are chosen to potentially signify opinion but are not specific to negative or positive expressions. The framework is used to classify clauses, headlines, and sentences from three different shared collections of attitudinal data. We find that constructional features transfer well across different text collections and that the information couched in them integrates easily with a vocabulary based approach, yielding improvements in classification without complicating the application end of the processing framework.

#index 1697419
#* Improving medical information retrieval with PICO element detection
#@ Florian Boudin;Lixin Shi;Jian-Yun Nie
#t 2010
#c 16
#% 766115
#% 958454
#! Without a well formulated and structured question, it can be very difficult and time consuming for physicians to identify appropriate resources and search for the best available evidence for medical treatment in evidence-based medicine (EBM). In EBM, clinical studies and questions involve four aspects: Population/Problem (P), Intervention (I), Comparison (C) and Outcome (O), which are known as PICO elements. It is intuitively more advantageous to use these elements in Information Retrieval (IR). In this paper, we first propose an approach to automatically identify the PICO elements in documents and queries. We test several possible approaches to use the identified elements in IR. Experiments show that it is a challenging task to determine accurately PICO elements. However, even with noisy tagging results, we can still take advantage of some PICO elements, namely I and P elements, to enhance the retrieval process, and this allows us to obtain significantly better retrieval effectiveness than the state-of-the-art methods.

#index 1697420
#* The role of query sessions in extracting instance attributes from web search queries
#@ Marius Paşca;Enrique Alfonseca;Enrique Robledo-Arnuncio;Ricardo Martin-Brualla;Keith Hall
#t 2010
#c 16
#% 78171
#% 129144
#% 284796
#% 296646
#% 330617
#% 438557
#% 757350
#% 783482
#% 869501
#% 869651
#% 956503
#% 956564
#% 983614
#% 989578
#% 1019109
#% 1055735
#% 1083705
#% 1089602
#% 1127393
#% 1130868
#% 1130878
#% 1130879
#% 1130927
#% 1136348
#% 1195914
#% 1214708
#% 1250402
#% 1264824
#% 1269447
#% 1269723
#% 1270281
#% 1275182
#% 1275208
#% 1275209
#% 1310432
#% 1338552
#% 1712125
#! Per-instance attributes are acquired using a weakly supervised extraction method which exploits anonymized Web-search query sessions, as an alternative to isolated, individual queries. Examples of these attributes are top speed for chevrolet corvette, or population density for brazil). Inherent challenges associated with using sessions for attribute extraction, such as a large majority of within-session queries not being related to attributes, are overcome by using attributes globally extracted from isolated queries as an unsupervised filtering mechanism. In a head-to-head qualitative comparison, the ranked lists of attributes generated by merging attributes extracted from query sessions, on one hand, and from isolated queries, on another hand, are about 12% more accurate on average, than the attributes extracted from isolated queries by a previous method.

#index 1697421
#* Transliteration equivalence using canonical correlation analysis
#@ Raghavendra Udupa;Mitesh M. Khapra
#t 2010
#c 16
#% 248218
#% 562054
#% 579944
#% 748556
#% 750863
#% 786574
#% 810634
#% 815913
#% 816070
#% 854584
#% 855302
#% 855563
#% 874256
#% 938719
#% 940001
#% 1195865
#% 1227610
#% 1260754
#% 1264824
#% 1338711
#% 1717371
#! We address the problem of Transliteration Equivalence, i.e. determining whether a pair of words in two different languages (e.g.Auden, ऑडेन) are name transliterations or not. This problem is at the heart of Mining Name Transliterations (MINT) from various sources of multilingual text data including parallel, comparable, and non-comparable corpora and multilingual news streams. MINT is useful in several cross-language tasks including Cross-Language Information Retrieval (CLIR), Machine Translation (MT), and Cross-Language Named Entity Retrieval. We propose a novel approach to Transliteration Equivalence using language-neutral representations of names. The key idea is to consider name transliterations in two languages as two views of the same semantic object and compute a low-dimensional common feature space using Canonical Correlation Analysis (CCA). Similarity of the names in the common feature space forms the basis for classifying a pair of names as transliterations. We show that our approach outperforms state-of-the-art baselines in the CLIR task for Hindi-English (3 collections) and Tamil-English (2 collections).

#index 1697422
#* Explicit search result diversification through sub-queries
#@ Rodrygo L. T. Santos;Jie Peng;Craig Macdonald;Iadh Ounis
#t 2010
#c 16
#% 217812
#% 262112
#% 411762
#% 642975
#% 643012
#% 766433
#% 879686
#% 1024548
#% 1074133
#% 1166473
#% 1227591
#% 1392444
#! Queries submitted to a retrieval system are often ambiguous. In such a situation, a sensible strategy is to diversify the ranking of results to be retrieved, in the hope that users will find at least one of these results to be relevant to their information need. In this paper, we introduce xQuAD, a novel framework for search result diversification that builds such a diversified ranking by explicitly accounting for the relationship between documents retrieved for the original query and the possible aspects underlying this query, in the form of sub-queries. We evaluate the effectiveness of xQuAD using a standard TREC collection. The results show that our framework markedly outperforms state-of-the-art diversification approaches under a simulated best-case scenario. Moreover, we show that its effectiveness can be further improved by estimating the relative importance of each identified sub-query. Finally, we show that our framework can still outperform the simulated best-case scenario of the state-of-the-art diversification approaches using sub-queries automatically derived from the baseline document ranking itself.

#index 1697423
#* Interpreting user inactivity on search results
#@ Sofia Stamou;Efthimis N. Efthimiadis
#t 2010
#c 16
#% 320432
#% 450040
#% 590523
#% 731615
#% 754059
#% 766472
#% 805200
#% 818315
#% 879565
#% 943049
#% 946521
#% 954948
#% 987211
#% 1004294
#% 1024550
#% 1043040
#% 1077044
#% 1083643
#% 1130811
#% 1227582
#% 1292473
#% 1348355
#! The lack of user activity on search results was until recently perceived as a sign of user dissatisfaction from retrieval performance, often, referring to such inactivity as a failed search (negative search abandonment). However, recent studies suggest that some search tasks can be achieved in the contents of the results displayed without the need to click through them (positive search abandonment); thus they emphasize the need to discriminate between successful and failed searches without follow-up clicks. In this paper, we study users’ inactivity on search results in relation to their pursued search goals and investigate the impact of displayed results on user clicking decisions. Our study examines two types of post-query user inactivity: pre-determined and post-determined depending on whether the user started searching with a preset intention to look for answers only within the result snippets and did not intend to click through the results, or the user inactivity was decided after the user had reviewed the list of retrieved documents. Our findings indicate that 27% of web searches in our sample are conducted with a pre-determined intention to look for answers in the results’ list and 75% of them can be satisfied in the contents of the displayed results. Moreover, in nearly half the queries that did not yield result visits, the desired information is found in the result snippets.

#index 1697424
#* Learning to select a ranking function
#@ Jie Peng;Craig Macdonald;Iadh Ounis
#t 2010
#c 16
#% 232703
#% 340934
#% 577224
#% 766463
#% 818225
#% 987241
#% 1019084
#% 1074065
#% 1195860
#% 1263610
#! Learning To Rank (LTR) techniques aim to learn an effective document ranking function by combining several document features. While the function learned may be uniformly applied to all queries, many studies have shown that different ranking functions favour different queries, and the retrieval performance can be significantly enhanced if an appropriate ranking function is selected for each individual query. In this paper, we propose a novel Learning To Select framework that selectively applies an appropriate ranking function on a per-query basis. The approach employs a query feature to identify similar training queries for an unseen query. The ranking function which performs the best on this identified training query set is then chosen for the unseen query. In particular, we propose the use of divergence, which measures the extent that a document ranking function alters the scores of an initial ranking of documents for a given query, as a query feature. We evaluate our method using tasks from the TREC Web and Million Query tracks, in combination with the LETOR 3.0 and LETOR 4.0 feature sets. Our experimental results show that our proposed method is effective and robust for selecting an appropriate ranking function on a per-query basis. In particular, it always outperforms three state-of-the-art LTR techniques, namely Ranking SVM, AdaRank, and the automatic feature selection method.

#index 1697425
#* Mining anchor text trends for retrieval
#@ Na Dai;Brian D. Davison
#t 2010
#c 16
#% 268079
#% 280834
#% 309095
#% 643069
#% 730070
#% 750865
#% 754125
#% 783474
#% 805878
#% 880399
#% 960414
#% 1019135
#% 1055705
#% 1130999
#% 1227604
#% 1227605
#% 1227614
#! Anchor text has been considered as a useful resource to complement the representation of target pages and is broadly used in web search. However, previous research only uses anchor text of a single snapshot to improve web search. Historical trends of anchor text importance have not been well modeled in anchor text weighting strategies. In this paper, we propose a novel temporal anchor text weighting method to incorporate the trends of anchor text creation over time, which combines historical weights of anchor text by propagating the anchor text weights among snapshots over the time axis. We evaluate our method on a real-world web crawl from the Stanford WebBase. Our results demonstrate that the proposed method can produce a significant improvement in ranking quality.

#index 1697426
#* Predicting query performance via classification
#@ Kevyn Collins-Thompson;Paul N. Bennett
#t 2010
#c 16
#% 722754
#% 818267
#% 827581
#% 879614
#% 907544
#% 944349
#% 956632
#% 987230
#% 987231
#% 987265
#% 987283
#% 987299
#% 1130851
#% 1392447
#% 1650705
#! We investigate using topic prediction data, as a summary of document content, to compute measures of search result quality. Unlike existing quality measures such as query clarity that require the entire content of the top-ranked results, class-based statistics can be computed efficiently online, because class information is compact enough to precompute and store in the index. In an empirical study we compare the performance of class-based statistics to their language-model counterparts for two performance-related tasks: predicting query difficulty and expansion risk. Our findings suggest that using class predictions can offer comparable performance to full language models while reducing computation overhead.

#index 1697427
#* A case for automatic system evaluation
#@ Claudia Hauff;Djoerd Hiemstra;Leif Azzopardi;Franciska de Jong
#t 2010
#c 16
#% 144034
#% 312689
#% 340890
#% 643020
#% 728355
#% 766407
#% 838529
#% 866256
#% 879632
#% 948378
#% 987252
#% 987265
#% 1195852
#% 1278067
#% 1392447
#! Ranking a set retrieval systems according to their retrieval effectiveness without relying on relevance judgments was first explored by Soboroff et al. [13]. Over the years, a number of alternative approaches have been proposed, all of which have been evaluated on early TREC test collections. In this work, we perform a wider analysis of system ranking estimation methods on sixteen TREC data sets which cover more tasks and corpora than previously. Our analysis reveals that the performance of system ranking estimation approaches varies across topics. This observation motivates the hypothesis that the performance of such methods can be improved by selecting the “right” subset of topics from a topic set. We show that using topic subsets improves the performance of automatic system ranking methods by 26% on average, with a maximum of 60%. We also observe that the commonly experienced problem of underestimating the performance of the best systems is data set dependent and not inherent to system ranking estimation. These findings support the case for automatic system evaluation and motivate further research.

#index 1697428
#* Aggregation of multiple judgments for evaluating ordered lists
#@ Hyun Duk Kim;ChengXiang Zhai;Jiawei Han
#t 2010
#c 16
#% 329537
#% 459006
#% 660658
#% 786539
#% 815142
#% 817487
#% 879582
#% 939549
#% 939756
#% 950043
#% 958441
#% 1213427
#% 1262954
#% 1262955
#% 1272344
#% 1274973
#% 1712170
#! Many tasks (e.g., search and summarization) result in an ordered list of items. In order to evaluate such an ordered list of items, we need to compare it with an ideal ordered list created by a human expert for the same set of items. To reduce any bias, multiple human experts are often used to create multiple ideal ordered lists. An interesting challenge in such an evaluation method is thus how to aggregate these different ideal lists to compute a single score for an ordered list to be evaluated. In this paper, we propose three new methods for aggregating multiple order judgments to evaluate ordered lists: weighted correlation aggregation, rank-based aggregation, and frequent sequential pattern-based aggregation. Experiment results on ordering sentences for text summarization show that all the three new methods outperform the state of the art average correlation methods in terms of discriminativeness and robustness against noise. Among the three proposed methods, the frequent sequential pattern-based method performs the best due to the flexible modeling of agreements and disagreements among human experts at various levels of granularity.

#index 1697429
#* Evaluation and user preference study on spatial diversity
#@ Jiayu Tang;Mark Sanderson
#t 2010
#c 16
#% 262112
#% 642975
#% 818266
#% 907493
#% 1074133
#% 1150163
#% 1195882
#% 1206746
#% 1292871
#% 1742129
#! Spatial diversity is a relatively new branch of research in the context of spatial information retrieval. Although the assumption that spatially diversified results may meet users’ needs better seems reasonable, there has been little hard evidence in the literature indicating so. In this paper, we will show the potentials of spatial diversity by not only the traditional evaluation metrics (precision and cluster recall), but also through a user preference study using Amazon Mechanical Turk. The encouraging results from the latter prove that users do have strong preference on spatially diversified results.

#index 1697430
#* News comments: exploring, modeling, and online prediction
#@ Manos Tsagkias;Wouter Weerkamp;Maarten de Rijke
#t 2010
#c 16
#% 1016332
#% 1017567
#% 1190088
#% 1192696
#% 1204039
#% 1292698
#% 1742093
#! Online news agents provide commenting facilities for their readers to express their opinions or sentiments with regards to news stories. The number of user supplied comments on a news article may be indicative of its importance, interestingness, or impact. We explore the news comments space, and compare the log-normal and the negative binomial distributions for modeling comments from various news agents. These estimated models can be used to normalize raw comment counts and enable comparison across different news sites. We also examine the feasibility of online prediction of the number of comments, based on the volume observed shortly after publication. We report on solid performance for predicting news comment volume in the long run, after short observation. This prediction can be useful for identifying news stories with the potential to “take off,” and can be used to support front page optimization for news sites.

#index 1697431
#* Query performance prediction: evaluation contrasted with effectiveness
#@ Claudia Hauff;Leif Azzopardi;Djoerd Hiemstra;Franciska de Jong
#t 2010
#c 16
#% 218978
#% 262084
#% 298183
#% 340901
#% 397159
#% 397161
#% 413634
#% 643033
#% 783506
#% 818267
#% 879614
#% 907544
#% 955496
#% 987260
#% 987265
#% 987299
#% 1227757
#% 1415713
#! Query performance predictors are commonly evaluated by reporting correlation coefficients to denote how well the methods perform at predicting the retrieval performance of a set of queries. Despite the amount of research dedicated to this area, one aspect remains neglected: how strong does the correlation need to be in order to realize an improvement in retrieval effectiveness in an operational setting? We address this issue in the context of two settings: Selective Query Expansion and Meta-Search. In an empirical study, we control the quality of a predictor in order to examine how the strength of the correlation achieved, affects the effectiveness of an adaptive retrieval system. The results of this study show that many existing predictors fail to achieve a correlation strong enough to reliably improve the retrieval effectiveness in the Selective Query Expansion as well as the Meta-Search setting.

#index 1697432
#* A framework for evaluating automatic image annotation algorithms
#@ Konstantinos Athanasakos;Vassilios Stathopoulos;Joemon M. Jose
#t 2010
#c 16
#% 269217
#% 304947
#% 318785
#% 457912
#% 642989
#% 642990
#% 645687
#% 721163
#% 722904
#% 760805
#% 852098
#% 901260
#% 975105
#% 1502531
#! Several Automatic Image Annotation (AIA) algorithms have been introduced recently, which have been found to outperform previous models. However, each one of them has been evaluated using either different descriptors, collections or parts of collections, or "easy" settings. This fact renders their results non-comparable, while we show that collection-specific properties are responsible for the high reported performance measures, and not the actual models. In this paper we introduce a framework for the evaluation of image annotation models, which we use to evaluate two state-of-the-art AIA algorithms. Our findings reveal that a simple Support Vector Machine (SVM) approach using Global MPEG-7 Features outperforms state-of-the-art AIA models across several collection settings. It seems that these models heavily depend on the set of features and the data used, while it is easy to exploit collection-specific properties, such as tag popularity especially in the commonly used Corel 5K dataset and still achieve good performance.

#index 1697433
#* BASIL: effective near-duplicate image detection using gene sequence alignment
#@ Hung-sik Kim;Hau-Wen Chang;Jeongkyu Lee;Dongwon Lee
#t 2010
#c 16
#% 780859
#% 780860
#% 990310
#% 990329
#% 996149
#% 997096
#% 997140
#% 1055721
#% 1130882
#% 1131850
#% 1181279
#% 1436633
#% 1775643
#! Finding near-duplicate images is a task often found in Multimedia Information Retrieval (MIR). Toward this effort, we propose a novel idea by bridging two seemingly unrelated fields – MIR and Biology. That is, we propose to use the popular gene sequence alignment algorithm in Biology, i.e., BLAST, in detecting near-duplicate images. Under the new idea, we study how various image features and gene sequence generation methods (using gene alphabets such as A, C, G, and T in DNA sequences) affect the accuracy and performance of detecting near-duplicate images. Our proposal, termed as BLASTed Image Linkage (BASIL), is empirically validated using various real data sets. This work can be viewed as the “first” step toward bridging MIR and Biology fields in the well-studied near-duplicate image detection problem.

#index 1697434
#* Beyond shot retrieval: searching for broadcast news items using language models of concepts
#@ Robin Aly;Aiden Doherty;Djoerd Hiemstra;Alan Smeaton
#t 2010
#c 16
#% 296645
#% 300542
#% 340936
#% 750863
#% 879571
#% 903632
#% 905105
#% 905168
#% 939386
#% 997242
#% 1074096
#% 1195829
#% 1231244
#% 1292876
#% 1381456
#% 1442294
#! Current video search systems commonly return video shots as results. We believe that users may better relate to longer, semantic video units and propose a retrieval framework for news story items, which consist of multiple shots. The framework is divided into two parts: (1) A concept based language model which ranks news items with known occurrences of semantic concepts by the probability that an important concept is produced from the concept distribution of the news item and (2) a probabilistic model of the uncertain presence, or risk, of these concepts. In this paper we use a method to evaluate the performance of story retrieval, based on the TRECVID shot-based retrieval groundtruth. Our experiments on the TRECVID 2005 collection show a significant performance improvement against four standard methods.

#index 1697435
#* Ranking fusion methods applied to on-line handwriting information retrieval
#@ Sebastián Peña Saldarriaga;Emmanuel Morin;Christian Viard-Gaudin
#t 2010
#c 16
#% 144076
#% 169768
#% 219050
#% 232703
#% 296534
#% 309093
#% 340934
#% 340936
#% 413613
#% 420464
#% 640429
#% 718884
#% 728360
#% 766453
#% 784148
#% 875230
#% 908460
#% 987266
#% 1182812
#% 1283447
#% 1283587
#% 1715611
#! This paper presents an empirical study on the application of ranking fusion methods in the context of handwriting information retrieval. Several works in the electronic text-domain suggest that significant improvements in retrieval performance can be achieved by combining different approaches to IR. In the handwritten-domain, two quite different families of retrieval approaches are encountered. The first family is based on standard approaches carried out on texts obtained through handwriting recognition, therefore regarded as noisy texts, while the second one is recognition-free using word spotting algorithms. Given the large differences that exist between these two families of approaches (document and query representations, matching methods, etc.), we hypothesize that fusion methods applied to the handwritten-domain can also bring significant effectiveness improvements. Results show that for texts having a word error rate (wer) lower than 23%, the performances achieved with the combined system are close to the performances obtained with clean digital texts, i.e. without transcription errors. In addition, for poorly recognized texts (wer 52%), improvements can also be obtained with standard fusion methods. Furthermore, we present a detailed analysis of the fusion performances, and show that existing indicators of expected improvements are not accurate in our context.

#index 1697436
#* Improving query correctness using centralized probably approximately correct (PAC) search
#@ Ingemar Cox;Jianhan Zhu;Ruoxun Fu;Lars Kai Hansen
#t 2010
#c 16
#% 296646
#% 340175
#% 411762
#% 496291
#% 578337
#% 766447
#% 860861
#% 961584
#% 987277
#% 990354
#% 1180871
#% 1246374
#% 1263571
#! A non-deterministic architecture for information retrieval, known as probably approximately correct (PAC) search, has recently been proposed. However, for equivalent storage and computational resources, the performance of PAC is only 63% of a deterministic system. We propose a modification to the PAC architecture, introducing a centralized query coordination node. To respond to a query, random sampling of computers is replaced with pseudo-random sampling using the query as a seed. Then, for queries that occur frequently, this pseudo-random sample is iteratively refined so that performance improves with each iteration. A theoretical analysis is presented that provides an upper bound on the performance of any iterative algorithm. Two heuristic algorithms are then proposed to iteratively improve the performance of PAC search. Experiments on the TREC-8 dataset demonstrate that performance can improve from 67% to 96% in just 10 iterations, and continues to improve with each iteration. Thus, for queries that occur 10 or more times, the performance of a non-deterministic PAC architecture can closely match that of a deterministic system.

#index 1697437
#* Learning to distribute queries into web search nodes
#@ Marcelo Mendoza;Mauricio Marín;Flavio Ferrarotti;Barbara Poblete
#t 2010
#c 16
#% 281184
#% 562981
#% 571036
#% 577302
#% 729918
#% 805864
#% 860861
#% 1055108
#% 1074360
#% 1083669
#% 1089473
#% 1117691
#% 1131117
#% 1190095
#% 1190098
#% 1267045
#% 1292755
#% 1834787
#! Web search engines are composed of a large set of search nodes and a broker machine that feeds them with queries. A location cache keeps minimal information in the broker to register the search nodes capable of producing the top-N results for frequent queries. In this paper we show that it is possible to use the location cache as a training dataset for a standard machine learning algorithm and build a predictive model of the search nodes expected to produce the best approximated results for queries. This can be used to prevent the broker from sending queries to all search nodes under situations of sudden peaks in query traffic and, as a result, avoid search node saturation. This paper proposes a logistic regression model to quickly predict the most pertinent search nodes for a given query.

#index 1697438
#* Text clustering for peer-to-peer networks with probabilistic guarantees
#@ Odysseas Papapetrou;Wolf Siberski;Norbert Fuhr
#t 2010
#c 16
#% 190611
#% 248027
#% 280856
#% 338588
#% 340175
#% 552172
#% 730035
#% 763708
#% 939576
#% 1006337
#% 1260389
#! Text clustering is an established technique for improving quality in information retrieval, for both centralized and distributed environments. However, for highly distributed environments, such as peer-to-peer networks, current clustering algorithms fail to scale. Our algorithm for peer-to-peer clustering achieves high scalability by using a probabilistic approach for assigning documents to clusters. It enables a peer to compare each of its documents only with very few selected clusters, without significant loss of clustering quality. The algorithm offers probabilistic guarantees for the correctness of each document assignment to a cluster. Extensive experimental evaluation with up to 100000 peers and 1 million documents demonstrates the scalability and effectiveness of the algorithm.

#index 1697439
#* XML retrieval using pruned element-index files
#@ Ismail Sengor Altingovde;Duygu Atilgan;Özgür Ulusoy
#t 2010
#c 16
#% 340887
#% 654442
#% 867054
#% 878916
#% 907504
#% 987216
#% 987323
#% 1037634
#% 1074067
#% 1195891
#% 1263220
#% 1263233
#% 1292744
#% 1674724
#% 1674734
#% 1721866
#! An element-index is a crucial mechanism for supporting content-only (CO) queries over XML collections. A full element-index that indexes each element along with the content of its descendants involves a high redundancy and reduces query processing efficiency. A direct index, on the other hand, only indexes the content that is directly under each element and disregards the descendants. This results in a smaller index, but possibly in return to some reduction in system effectiveness. In this paper, we propose using static index pruning techniques for obtaining more compact index files that can still result in comparable retrieval performance to that of a full index. We also compare the retrieval performance of these pruning based approaches to some other strategies that make use of a direct element-index. Our experiments conducted along with the lines of INEX evaluation framework reveal that pruned index files yield comparable to or even better retrieval performance than the full index and direct index, for several tasks in the ad hoc track.

#index 1697440
#* Category-based query modeling for entity search
#@ Krisztian Balog;Marc Bron;Maarten de Rijke
#t 2010
#c 16
#% 169784
#% 287253
#% 340899
#% 750863
#% 754059
#% 879570
#% 1019189
#% 1041733
#% 1052710
#% 1074097
#% 1074126
#% 1100824
#% 1100827
#% 1100828
#% 1100829
#% 1263246
#% 1263247
#% 1263248
#% 1263249
#% 1742093
#! Users often search for entities instead of documents and in this setting are willing to provide extra input, in addition to a query, such as category information and example entities. We propose a general probabilistic framework for entity search to evaluate and provide insight in the many ways of using these types of input for query modeling. We focus on the use of category information and show the advantage of a category-based representation over a term-based representation, and also demonstrate the effectiveness of category-based expansion using example entities. Our best performing model shows very competitive performance on the INEX-XER entity ranking and list completion tasks.

#index 1697441
#* Maximum margin ranking algorithms for information retrieval
#@ Shivani Agarwal;Michael Collins
#t 2010
#c 16
#% 411762
#% 577224
#% 734915
#% 829043
#% 840846
#% 840882
#% 879588
#% 983820
#% 983905
#% 987226
#% 987240
#% 987241
#% 1035577
#% 1039843
#% 1083633
#% 1117688
#% 1211797
#% 1232022
#% 1442575
#% 1442579
#% 1674801
#% 1817412
#! Machine learning ranking methods are increasingly applied to ranking tasks in information retrieval (IR). However ranking tasks in IR often differ from standard ranking tasks in machine learning, both in terms of problem structure and in terms of the evaluation criteria used to measure performance. Consequently, there has been much interest in recent years in developing ranking algorithms that directly optimize IR ranking measures. Here we propose a family of ranking algorithms that preserve the simplicity of standard pair-wise ranking methods in machine learning, yet show performance comparable to state-of-the-art IR ranking algorithms. Our algorithms optimize variations of the hinge loss used in support vector machines (SVMs); we discuss three variations, and in each case, give simple and efficient stochastic gradient algorithms to solve the resulting optimization problems. Two of these are stochastic gradient projection algorithms, one of which relies on a recent method for l1,∞-norm projections; the third is a stochastic exponentiated gradient algorithm. The algorithms are simple and efficient, have provable convergence properties, and in our preliminary experiments, show performance close to state-of-the-art algorithms that directly optimize IR ranking measures.

#index 1697442
#* Query aspect based term weighting regularization in information retrieval
#@ Wei Zheng;Hui Fang
#t 2010
#c 16
#% 67565
#% 109190
#% 120104
#% 218982
#% 229348
#% 232645
#% 262096
#% 340948
#% 375017
#% 766412
#% 766440
#% 766525
#% 790839
#% 818262
#% 818263
#% 869501
#% 879579
#% 987229
#% 1074112
#% 1227636
#! Traditional retrieval models assume that query terms are independent and rank documents primarily based on various term weighting strategies including TF-IDF and document length normalization. However, query terms are related, and groups of semantically related query terms may form query aspects. Intuitively, the relations among query terms could be utilized to identify hidden query aspects and promote the ranking of documents covering more query aspects. Despite its importance, the use of semantic relations among query terms for term weighting regularization has been under-explored in information retrieval. In this paper, we study the incorporation of query term relations into existing retrieval models and focus on addressing the challenge, i.e., how to regularize the weights of terms in different query aspects to improve retrieval performance. Specifically, we first develop a general strategy that can systematically integrate a term weighting regularization function into existing retrieval functions, and then propose two specific regularization functions based on the guidance provided by constraint analysis. Experiments on eight standard TREC data sets show that the proposed methods are effective to improve retrieval accuracy.

#index 1697443
#* Using the quantum probability ranking principle to rank interdependent documents
#@ Guido Zuccon;Leif Azzopardi
#t 2010
#c 16
#% 262112
#% 375017
#% 642975
#% 758200
#% 879618
#% 1051038
#% 1051060
#% 1074133
#% 1166473
#% 1194537
#% 1194539
#% 1194540
#% 1227591
#% 1263586
#% 1263589
#% 1263590
#! A known limitation of the Probability Ranking Principle (PRP) is that it does not cater for dependence between documents. Recently, the Quantum Probability Ranking Principle (QPRP) has been proposed, which implicitly captures dependencies between documents through “quantum interference”. This paper explores whether this new ranking principle leads to improved performance for subtopic retrieval, where novelty and diversity is required. In a thorough empirical investigation, models based on the PRP, as well as other recently proposed ranking strategies for subtopic retrieval (i.e. Maximal Marginal Relevance (MMR) and Portfolio Theory(PT)), are compared against the QPRP. On the given task, it is shown that the QPRP outperforms these other ranking strategies. And unlike MMR and PT, one of the main advantages of the QPRP is that no parameter estimation/tuning is required; making the QPRP both simple and effective. This research demonstrates that the application of quantum theory to problems within information retrieval can lead to significant improvements.

#index 1697444
#* Wikipedia-based semantic smoothing for the language modeling approach to information retrieval
#@ Xinhui Tu;Tingting He;Long Chen;Jing Luo;Maoyuan Zhang
#t 2010
#c 16
#% 262096
#% 280851
#% 340899
#% 340948
#% 342707
#% 375017
#% 397128
#% 397129
#% 766430
#% 818240
#% 1083703
#% 1112744
#% 1117027
#% 1250362
#% 1275012
#% 1289518
#! Semantic smoothing for the language modeling approach to information retrieval is significant and effective to improve retrieval performance. In previous methods such as the translation model, individual terms or phrases are used to do semantic mapping. These models are not very efficient when faced with ambiguous words and phrases because they are unable to incorporate contextual information. To overcome this limitation, we propose a novel Wikipedia-based semantic smoothing method that decomposes a document into a set of weighted Wikipedia concepts and then maps those unambiguous Wikipedia concepts into query terms. The mapping probabilities from each Wikipedia concept to individual terms are estimated through the EM algorithm. Document models based on Wikipedia concept mapping are then derived. The new smoothing method is evaluated on the TREC Ad Hoc Track (Disks 1, 2, and 3) collections. Experiments show significant improvements over the two-stage language model, as well as the language model with translation-based semantic smoothing.

#index 1697445
#* A performance prediction approach to enhance collaborative filtering performance
#@ Alejandro Bellogín;Pablo Castells
#t 2010
#c 16
#% 397161
#% 420515
#% 420539
#% 766408
#% 790459
#% 813966
#% 818216
#% 818267
#% 879613
#% 879627
#% 893735
#% 907544
#% 987260
#% 995518
#% 1106091
#% 1166053
#% 1195854
#% 1392447
#! Performance prediction has gained increasing attention in the IR field since the half of the past decade and has become an established research topic in the field. The present work restates the problem in the area of Collaborative Filtering (CF), where it has barely been researched so far. We investigate the adaptation of clarity-based query performance predictors to predict neighbor performance in CF. A predictor is proposed and introduced in a kNN CF algorithm to produce a dynamic variant where neighbor ratings are weighted based on their predicted performance. The properties of the predictor are empirically studied by, first, checking the correlation of the predictor output with a proposed measure of neighbor performance. Then, the performance of the dynamic kNN variant is examined on different sparsity and neighborhood size conditions, where the variant consistently outperforms the baseline algorithm, with increasing difference on small neighborhoods.

#index 1697446
#* Collaborative filtering: the aim of recommender systems and the significance of user ratings
#@ Jennifer Redpath;David H. Glass;Sally McClean;Luke Chen
#t 2010
#c 16
#% 124010
#% 173879
#% 202011
#% 220706
#% 314933
#% 415107
#% 734590
#% 766448
#% 805841
#% 860672
#% 915844
#% 936908
#% 1000869
#% 1006328
#% 1650569
#! This paper investigates the significance of numeric user ratings in recommender systems by considering their inclusion / exclusion in both the generation and evaluation of recommendations. When standard evaluation metrics are used, experimental results show that inclusion of numeric rating values in the recommendation process does not enhance the results. However, evaluating the accuracy of a recommender algorithm requires identifying the aim of the system. Evaluation metrics such as precision and recall evaluate how well a system performs at recommending items that have been previously rated by the user. By contrast, a new metric, known as Approval Rate, is intended to evaluate how well a system performs at recommending items that would be rated highly by the user. Experimental results demonstrate that these two aims are not synonymous and that for an algorithm to attempt both obscures the investigation. The results also show that appropriate use of numeric rating valuesin the process of calculating user similarity can enhance the performance when Approval Rate is used.

#index 1697447
#* Goal-driven collaborative filtering – a directional error based approach
#@ Tamas Jambor;Jun Wang
#t 2010
#c 16
#% 124010
#% 280852
#% 309095
#% 357282
#% 375017
#% 397153
#% 411762
#% 729437
#% 734590
#% 734594
#% 879583
#% 879627
#% 1074061
#% 1077150
#% 1083539
#% 1127484
#% 1260273
#% 1650470
#% 1650569
#! Collaborative filtering is one of the most effective techniques for making personalized content recommendation. In the literature, a common experimental setup in the modeling phase is to minimize, either explicitly or implicitly, the (expected) error between the predicted ratings and the true user ratings, while in the evaluation phase, the resulting model is again assessed by that error. In this paper, we argue that defining an error function that is fixed across rating scales is however limited, and different applications may have different recommendation goals thus error functions. For example, in some cases, we might be more concerned about the highly predicted items than the ones with low ratings (precision minded), while in other cases, we want to make sure not to miss any highly rated items (recall minded). Additionally, some applications might require to produce a top-N recommendation list, where the rank-based performance measure becomes valid. To address this issue, we propose a flexible optimization framework that can adapt to individual recommendation goals. We introduce a Directional Error Function to capture the cost (risk) of each individual predictions, and it can be learned from the specified performance measures at hand. Our preliminary experiments on a real data set demonstrate that significant performance gains have been achieved.

#index 1697448
#* Personalizing web search with folksonomy-based user and document profiles
#@ David Vallet;Iván Cantador;Joemon M. Jose
#t 2010
#c 16
#% 169781
#% 253188
#% 956544
#% 956579
#% 1074070
#% 1127482
#% 1152472
#% 1396090
#% 1409929
#% 1667787
#! Web search personalization aims to adapt search results to a user based on his tastes, interests and needs. The way in which such personal preferences are captured, modeled and exploited distinguishes the different personalization strategies. In this paper, we propose to represent a user profile in terms of social tags, manually provided by users in folksonomy systems to describe, categorize and organize items of interest, and investigate a number of novel techniques that exploit the users’ social tags to re-rank results obtained with a Web search engine. An evaluation conducted with a dataset from Delicious social bookmarking system shows that our personalization techniques clearly outperform state of the art approaches.

#index 1697449
#* Tripartite hidden topic models for personalised tag suggestion
#@ Morgan Harvey;Mark Baillie;Ian Ruthven;Mark J. Carman
#t 2010
#c 16
#% 329569
#% 722904
#% 869504
#% 1055704
#% 1055796
#% 1667787
#! Social tagging systems provide methods for users to categorise resources using their own choice of keywords (or “tags”) without being bound to a restrictive set of predefined terms. Such systems typically provide simple tag recommendations to increase the number of tags assigned to resources. In this paper we extend the latent Dirichlet allocation topic model to include user data and use the estimated probability distributions in order to provide personalised tag suggestions to users. We describe the resulting tripartite topic model in detail and show how it can be utilised to make personalised tag suggestions. Then, using data from a large-scale, real life tagging system, test our system against several baseline methods. Our experiments show a statistically significant increase in performance of our model over all key metrics, indicating that the model could be successfully used to provide further social tagging tools such as resource suggestion and collaborative filtering.

#index 1697450
#* Extracting multilingual topics from unaligned comparable corpora
#@ Jagadeesh Jagarlamudi;Hal Daumé
#t 2010
#c 16
#% 340897
#% 340948
#% 579944
#% 642990
#% 722904
#% 854571
#% 989650
#% 1190212
#% 1417061
#! Topic models have been studied extensively in the context of monolingual corpora. Though there are some attempts to mine topical structure from cross-lingual corpora, they require clues about document alignments. In this paper we present a generative model called JointLDA which uses a bilingual dictionary to mine multilingual topics from an unaligned corpus. Experiments conducted on different data sets confirm our conjecture that jointly modeling the cross-lingual corpora offers several advantages compared to individual monolingual models. Since the JointLDA model merges related topics in different languages into a single multilingual topic: a) it can fit the data with relatively fewer topics. b) it has the ability to predict related words from a language different than that of the given document. In fact it has better predictive power compared to the bag-of-word based translation model leaving the possibility for JointLDA to be preferred over bag-of-word model for Cross-Lingual IR applications. We also found that the monolingual models learnt while optimizing the cross-lingual copora are more effective than the corresponding LDA models.

#index 1697451
#* Improving retrievability of patents in prior-art search
#@ Shariq Bashir;Andreas Rauber
#t 2010
#c 16
#% 240201
#% 281396
#% 340901
#% 723326
#% 750863
#% 843731
#% 855221
#% 987229
#% 987264
#% 987331
#% 1074080
#% 1074081
#% 1130863
#% 1227608
#% 1227613
#% 1227746
#% 1267424
#% 1292722
#! Prior-art search is an important task in patent retrieval. The success of this task relies upon the selection of relevant search queries. Typically terms for prior-art queries are extracted from the claim fields of query patents. However, due to the complex technical structure of patents, and presence of terms mismatch and vague terms, selecting relevant terms for queries is a difficult task. During evaluating the patents retrievability coverage of prior-art queries generated from query patents, a large bias toward a subset of the collection is experienced. A large number of patents either have a very low retrievability score or can not be discovered via any query. To increase the retrievability of patents, in this paper we expand prior-art queries generated from query patents using query expansion with pseudo relevance feedback. Missing terms from query patents are discovered from feedback patents, and better patents for relevance feedback are identified using a novel approach for checking their similarity with query patents. We specifically focus on how to automatically select better terms from query patents based on their proximity distribution with prior-art queries that are used as features for computing similarity. Our results show, that the coverage of prior-art queries can be increased significantly by incorporating relevant queries terms using query expansion.

#index 1697452
#* Mining OOV translations from mixed-language web pages for cross language information retrieval
#@ Lei Shi
#t 2010
#c 16
#% 211044
#% 458405
#% 464434
#% 740915
#% 747823
#% 747947
#% 750865
#% 766425
#% 766427
#% 766495
#% 854073
#% 939562
#% 939914
#% 943823
#% 1013670
#! Translating Out-Of-Vocabulary (OOV) terms is crucial for Cross Language Information Retrieval (CLIR). In this paper, we propose a method that automatically acquires a large quantity of OOV translations from the web. Different from previous approaches that rely on a finite set of hand-crafted extraction rules, our method adaptively learns translation extraction patterns based on the observation that translation pairs on the same page tend to appear following similar layout patterns. The learned patterns are leveraged in a discriminative translation extraction model that treats translation extraction from a mixed language bilingual web page as a sequence labeling task in order to exploit useful relations among translation pairs on the page. Experiments demonstrate that our proposed method out-performs earlier work with marked improvement on OOV translation mining quality.

#index 1697453
#* On foreign name search
#@ Jason Soo;Ophir Frieder
#t 2010
#c 16
#% 219033
#% 318940
#% 324015
#% 413588
#% 859497
#% 1077150
#% 1130949
#! We address foreign name search in a highly diverse user community. User sophistication ranges from highly experienced archivists to apprehensive users who shy away from technology; apprehensive users dominate system use. Thus, all system interfaces must assume minimal dependency on the user. Our foreign names search approach, called Segments, is language independent; thus, there is no need to determine the language of origin from the diverse candidate set of thirteen languages. We compare Segments against traditional n-gram and Soundex based solutions. Actual and synthetic queries are used to search a names data set resident in the United States Holocaust Memorial Museum. We also search a subset of the 1990 United States Census Bureau Surnames data set to evaluate the performance of Segments on a predominately language specific (English) collection. Our results demonstrate statistically significant performance gains over both traditional approaches. The described approach supports search efforts at the United States Holocaust Memorial Museum.

#index 1697454
#* Promoting ranking diversity for biomedical information retrieval using wikipedia
#@ Xiaoshi Yin;Xiangji Huang;Zhoujun Li
#t 2010
#c 16
#% 169777
#% 217251
#% 262112
#% 397133
#% 642975
#% 819781
#% 1019105
#% 1176863
#% 1227584
#% 1227615
#% 1227687
#% 1275012
#% 1400942
#! In this paper, we propose a cost-based re-ranking method to promote ranking diversity for biomedical information retrieval. The proposed method concerns with finding passages that cover many different aspects of a query topic. First, aspects covered by retrieved passages are detected and explicitly presented by Wikipedia concepts. Then, an aspect filter based on a two-stage model is introduced. It ranks the detected aspects in decreasing order of the probability that an aspect is generated by the query. Finally, retrieved passages are re-ranked using the proposed cost-based re-ranking method which ranks a passage according to the number of new aspects covered by the passage and the query-relevance of aspects covered by the passage. A series of experiments conducted on the TREC 2006 and 2007 Genomics collections demonstrate the effectiveness of the proposed method in promoting ranking diversity for biomedical information retrieval.

#index 1697455
#* Temporal shingling for version identification in web archives
#@ Ralf Schenkel
#t 2010
#c 16
#% 201935
#% 255137
#% 345087
#% 347225
#% 544011
#% 571725
#% 640706
#% 730067
#% 731406
#% 769944
#% 879600
#% 956507
#% 978157
#% 1055715
#% 1074121
#% 1213450
#! Building and preserving archives of the evolving Web has been an important problem in research. Given the huge volume of content that is added or updated daily, identifying the right versions of pages to store in the archive is an important building block of any large-scale archival system. This paper presents temporal shingling, an extension of the well-established shingling technique for measuring how similar two snapshots of a page are. This novel method considers the lifespan of shingles to differentiate between important updates that should be archived and transient changes that may be ignored. Extensive experiments demonstrate the tradeoff between archive size and version coverage, and show that the novel method yields better archive coverage at smaller sizes than existing techniques.

#index 1697456
#* Biometric response as a source of query independent scoring in lifelog retrieval
#@ Liadh Kelly;Gareth J. F. Jones
#t 2010
#c 16
#% 818255
#% 869534
#% 952490
#% 1093776
#% 1132933
#% 1279814
#! Personal lifelog archives contain digital records captured from an individual’s daily life, e.g. emails, web pages downloaded and SMSs sent or received. While capturing this information is becoming increasingly easy, subsequently locating relevant items in response to user queries from within these archives is a significant challenge. This paper presents a novel query independent static biometric scoring approach for re-ranking result lists retrieved from a lifelog using a BM25 model for content and content + context data. For this study we explored the utility of galvanic skin response (GSR) and skin temperature (ST) associated with past experience of items as a measure of potential future significance of items. Results obtained indicate that our static scoring techniques are useful in re-ranking retrieved result lists.

#index 1697457
#* Enabling interactive query expansion through eliciting the potential effect of expansion terms
#@ Nuzhah Gooda Sahib;Anastasios Tombros;Ian Ruthven
#t 2010
#c 16
#% 54435
#% 87766
#% 92696
#% 214709
#% 232719
#% 262036
#% 378486
#% 397130
#% 508411
#% 642985
#% 643001
#% 943042
#% 1227623
#! Despite its potential to improve search effectiveness, previous research has shown that the uptake of interactive query expansion (IQE) is limited. In this paper, we investigate one method of increasing the uptake of IQE by displaying summary overviews that allow searchers to view the impact of their expansion decisions in real time, engage more with suggested terms, and support them in making good expansion decisions. Results from our user studies show that searchers use system-generated suggested terms more frequently if they know the impact of doing so on their results. We also present evidence that the usefulness of our proposed IQE approach is highest when searchers attempt unfamiliar or difficult information seeking tasks. Overall, our work presents strong evidence that searchers are more likely to engage with suggested terms if they are supported by the search interface.

#index 1697458
#* Evaluation of an adaptive search suggestion system
#@ Sascha Kriewel;Norbert Fuhr
#t 2010
#c 16
#% 4676
#% 85443
#% 142618
#% 305867
#% 345736
#% 452640
#% 508274
#% 655498
#% 751596
#% 819783
#% 841893
#% 893761
#% 966981
#% 1263597
#% 1406456
#% 1682009
#% 1709427
#! This paper describes an adaptive search suggestion system based on case–based reasoning techniques, and details an evaluation of its usefulness in helping users employ better search strategies. A user experiment with 24 participants was conducted using a between–subjects design. One group received search suggestions for the first two out of three tasks, while the other didn’t. Results indicate a correlation between search success, expressed as number of relevant documents saved, and use of suggestions. In addition, users who received suggestions used significantly more of the advanced tools and options of the search system — even after suggestions were switched off during a later task.

#index 1697459
#* How different are language models andword clouds?
#@ Rianne Kaptein;Djoerd Hiemstra;Jaap Kamps
#t 2010
#c 16
#% 262096
#% 288166
#% 342707
#% 643001
#% 766429
#% 789959
#% 869525
#% 955013
#% 956649
#% 956704
#% 1065169
#% 1065421
#% 1077150
#% 1678713
#! Word clouds are a summarised representation of a document’s text, similar to tag clouds which summarise the tags assigned to documents. Word clouds are similar to language models in the sense that they represent a document by its word distribution. In this paper we investigate the differences between word cloud and language modelling approaches, and specifically whether effective language modelling techniques also improve word clouds. We evaluate the quality of the language model using a system evaluation test bed, and evaluate the quality of the resulting word cloud with a user study. Our experiments show that different language modelling techniques can be applied to improve a standard word cloud that uses a TF weighting scheme in combination with stopword removal. Including bigrams in the word clouds and a parsimonious term weighting scheme are the most effective in both the system evaluation and the user study.

#index 1697460
#* Colouring the dimensions of relevance
#@ Ulises Cerviño Beresi;Yunhyong Kim;Mark Baillie;Ian Ruthven;Dawei Song
#t 2010
#c 16
#% 167557
#% 260244
#% 435730
#! In this article we introduce a visualisation technique for analysing relevance and interaction data. It allows the researcher to quickly detect emerging patterns in both interactions and relevance criteria usage. The concept of “relevance criteria profile”, which provides a global view of user behaviour in judging the relevance of the retrieved information, is developed. We discuss by example, using data from a live search user study, how these tools support the data analysis.

#index 1697461
#* On improving pseudo-relevance feedback using pseudo-irrelevant documents
#@ Karthik Raman;Raghavendra Udupa;Pushpak Bhattacharya;Abhijit Bhole
#t 2010
#c 16
#% 340901
#% 342707
#% 1117691
#% 1166534
#! Pseudo-Relevance Feedback (PRF) assumes that the top-ranking n documents of the initial retrieval are relevant and extracts expansion terms from them. In this work, we introduce the notion of pseudo-irrelevant documents, i.e. high-scoring documents outside of top n that are highly unlikely to be relevant. We show how pseudo-irrelevant documents can be used to extract better expansion terms from the top-ranking n documents: good expansion terms are those which discriminate the top-ranking n documents from the pseudo-irrelevant documents. Our approach gives substantial improvements in retrieval performance over Model-based Feedback on several test collections.

#index 1697462
#* Laplacian co-hashing of terms and documents
#@ Dell Zhang;Jun Wang;Deng Cai;Jinsong Lu
#t 2010
#c 16
#% 342621
#% 593047
#% 1077150
#% 1215859
#! A promising way to accelerate similarity search is semantic hashing which designs compact binary codes for a large number of documents so that semantically similar documents are mapped to similar codes within a short Hamming distance. In this paper, we introduce the novel problem of co-hashing where both documents and terms are hashed simultaneously according to their semantic similarities. Furthermore, we propose a novel algorithm Laplacian Co-Hashing (LCH) to solve this problem which directly optimises the Hamming distance.

#index 1697463
#* Query difficulty prediction for contextual image retrieval
#@ Xing Xing;Yi Zhang;Mei Han
#t 2010
#c 16
#% 198058
#% 818059
#% 854641
#! This paper explores how to predict query difficulty for contextual image retrieval. We reformulate the problem as the task of predicting how difficult to represent a query as images. We propose to use machine learning algorithms to learn the query difficulty prediction models based on the characteristics of the query words as well as the query context. More specifically, we focus on noun word/phrase queries and propose four features based on several assumptions. We created an evaluation data set by hand and compare several machine learning algorithms on the prediction task. Our preliminary experimental results show the effectiveness of our proposed features and the stable performance using different classification models.

#index 1697464
#* Estimating translation probabilities from the web for structured queries on CLIR
#@ Xabier Saralegi;Maddalen Lopez de Lacalle
#t 2010
#c 16
#% 262046
#% 262047
#% 643017
#% 747947
#! We present two methods for estimating replacement probabilities without using parallel corpora. The first method proposed exploits the possible translation probabilities latent in Machine Readable Dictionaries (MRD). The second method is more robust, and exploits context similarity-based techniques in order to estimate word translation probabilities using the Internet as a bilingual comparable corpus. The experiments show a statistically significant improvement over non weighted structured queries in terms of MAP by using the replacement probabilities obtained with the proposed methods. The context similarity-based method is the one that yields the most significant improvement.

#index 1697465
#* Using weighted tagging to facilitate enterprise search
#@ Shengwen Yang;Jianming Jin;Yuhong Xiong
#t 2010
#c 16
#% 860021
#% 869548
#% 956544
#% 1019186
#% 1035588
#! Motivated by the success of social tagging in web communities, this paper proposes a novel document tagging method more suitable for the enterprise environment, named weighted tagging. The method allows users to tag a document with weighted tags which are then used as an additional source for the query matching and relevance scoring to improve the search results. The method enables a user-driven search result ranking by adapting the relevance score of a search result through weighted tags based on user feedbacks. A prototype intranet search system has been built to demonstrate the viability of the method.

#index 1697466
#* An empirical study of query specificity
#@ Avi Arampatzis;Jaap Kamps
#t 2010
#c 16
#% 397161
#% 448736
#% 944349
#% 1074200
#% 1292546
#! We analyse the statistical behavior of query-associated quantities in query-logs, namely, the sum and mean of IDF of query terms, otherwise known as query specificity and query mean specificity. We narrow down the possibilities for modeling their distributions to gamma, log-normal, or log-logistic, depending on query length and on whether the sum or the mean is considered. The results have applications in query performance prediction and artificial query generation.

#index 1697467
#* Semantically enhanced term frequency
#@ Christof Müller;Iryna Gurevych
#t 2010
#c 16
#% 324129
#% 940042
#% 1195831
#% 1227677
#% 1270225
#% 1270267
#% 1275012
#% 1432242
#! In this paper, we complement the term frequency, which is used in many bag-of-words based information retrieval models, with information about the semantic relatedness of query and document terms. Our experiments show that when employed in the standard probabilistic retrieval model BM25, the additional semantic information significantly outperforms the standard term frequency, and also improves the effectiveness when additional query expansion is applied. We further analyze the impact of different lexical semantic resources on the IR effectiveness.

#index 1697468
#* Crowdsourcing assessments for XML ranked retrieval
#@ Omar Alonso;Ralf Schenkel;Martin Theobald
#t 2010
#c 16
#% 878916
#% 1095875
#% 1100801
#% 1264744
#! Crowdsourcing has gained a lot of attention as a viable approach for conducting IR evaluations. This paper shows through a series of experiments on INEX data that crowdsourcing can be a good alternative for relevance assessment in the context of XML retrieval.

#index 1697469
#* Evaluating server selection for federated search
#@ Paul Thomas;Milad Shokouhi
#t 2010
#c 16
#% 280856
#% 309133
#% 481748
#% 643012
#% 660301
#% 1227629
#% 1264113
#% 1392444
#! Previous evaluations of server selection methods for federated search have either used metrics which are unconnected with user satisfaction, or have not been able to account for confounding factors due to other search components. We propose a new framework for evaluating federated search server selection techniques. In our model, we isolate the effect of other confounding factors such as server summaries and result merging. Our results suggest that state-of-the-art server selection techniques are generally effective but result merging methods can be significantly improved. Furthermore, we show that the performance differences among server selection techniques can be obscured by ineffective merging.

#index 1697470
#* A comparison of language identification approaches on short, query-style texts
#@ Thomas Gottron;Nedim Lipka
#t 2010
#c 16
#% 763708
#% 1192961
#% 1494819
#% 1633195
#! In a multi-language Information Retrieval setting, the knowledge about the language of a user query is important for further processing. Hence, we compare the performance of some typical approaches for language detection on very short, query-style texts. The results show that already for single words an accuracy of more than 80% can be achieved, for slightly longer texts we even observed accuracy values close to 100%.

#index 1697471
#* Filtering documents with subspaces
#@ Benjamin Piwowarski;Ingo Frommholz;Yashar Moshfeghi;Mounia Lalmas;Keith van Rijsbergen
#t 2010
#c 16
#% 758200
#% 1051060
#% 1194536
#! We propose an approach to build a subspace representation for documents. This more powerful representation is a first step towards the development of a quantum-based model for Information Retrieval (IR). To validate our methodology, we apply it to the adaptive document filtering task.

#index 1697472
#* User's latent interest-based collaborative filtering
#@ Biyun Hu;Zhoujun Li;Jun Wang
#t 2010
#c 16
#% 173879
#% 330687
#! Memory-based collaborative filtering is one of the most popular methods used in recommendation systems. It predicts a user’s preference based on his or her similarity to other users. Traditionally, the Pearson correlation coefficient is often used to compute the similarity between users. In this paper we develop novel memory-based approach that incorporates user’s latent interest. The interest level of a user is first estimated from his/her ratings for items through a latent trait model, and then used for computing the similarity between users. Experimental results show that the proposed method outperforms the traditional memory-based one.

#index 1697473
#* Evaluating the potential of explicit phrases for retrieval quality
#@ Andreas Broschart;Klaus Berberich;Ralf Schenkel
#t 2010
#c 16
#% 35937
#% 109190
#% 766440
#% 783474
#% 879651
#% 1019133
#% 1404894
#% 1715627
#! This paper evaluates the potential impact of explicit phrases on retrieval quality through a case study with the TREC Terabyte benchmark. It compares the performance of user- and system-identified phrases with a standard score and a proximity-aware score, and shows that an optimal choice of phrases, including term permutations, can significantly improve query performance.

#index 1697474
#* Developing a test collection for the evaluation of integrated search
#@ Marianne Lykke;Birger Larsen;Haakon Lund;Peter Ingwersen
#t 2010
#c 16
#% 397164
#% 955710
#% 985820
#! The poster discusses the characteristics needed in an information retrieval (IR) test collection to facilitate the evaluation of integrated search, i.e. search across a range of different sources but with one search box and one ranked result list, and describes and analyses a new test collection constructed for this purpose. The test collection consists of approx. 18,000 monographic records, 160,000 papers and journal articles in PDF and 275,000 abstracts with a varied set of metadata and vocabularies from the physics domain, 65 topics based on real work tasks and corresponding graded relevance assessments. The test collection may be used for systems- as well as user-oriented evaluation.

#index 1697475
#* Retrieving customary web language to assist writers
#@ Benno Stein;Martin Potthast;Martin Trenkmann
#t 2010
#c 16
#% 279755
#% 805883
#% 893128
#% 939532
#% 943822
#% 1075132
#! This paper introduces Netspeak, a Web service which assists writers in finding adequate expressions. To provide statistically relevant suggestions, the service indexes more than 1.8 billion n-grams, n≤5, along with their occurrence frequencies on the Web. If in doubt about a wording, a user can specify a query that has wildcards inserted at those positions where she feels uncertain. Queries define patterns for which a ranked list of matching n-grams along with usage examples are retrieved. The ranking reflects the occurrence frequencies of the n-grams and informs about both absolute and relative usage. Given this choice of customary wordings, one can easily select the most appropriate. Especially second-language speakers can learn about style conventions and language usage. To guarantee response times within milliseconds we have developed an index that considers occurrence probabilities, allowing for a biased sampling during retrieval. Our analysis shows that the extreme speedup obtained with this strategy (factor 68) comes without significant loss in retrieval quality.

#index 1697476
#* Enriching peer-to-peer file descriptors using association rules on query logs
#@ Nazli Goharian;Ophir Frieder;Wai Gen Yee;Jay Mundrawala
#t 2010
#c 16
#% 818916
#! We describe a P2P association rule mining descriptor enrichment approach that statistically significantly increases accuracy by greater than 15% over the non-enriched baseline. Unlike the state-of-the-art enrichment approach however, the proposed solution does not introduce additional network load.

#index 1697477
#* Cross-language high similarity search: why no sub-linear time bound can be expected
#@ Maik Anderka;Benno Stein;Martin Potthast
#t 2010
#c 16
#% 479649
#% 762054
#% 956506
#% 1227596
#% 1227677
#% 1415756
#! This paper contributes to an important variant of cross-language information retrieval, called cross-language high similarity search. Given a collection D of documents and a query q in a language different from the language of D, the task is to retrieve highly similar documents with respect to q. Use cases for this task include cross-language plagiarism detection and translation search. The current line of research in cross-language high similarity search resorts to the comparison of q and the documents in D in a multilingual concept space—which, however, requires a linear scan of D. Monolingual high similarity search can be tackled in sub-linear time, either by fingerprinting or by “brute force n-gram indexing”, as it is done by Web search engines. We argue that neither fingerprinting nor brute force n-gram indexing can be applied to tackle cross-language high similarity search, and that a linear scan is inevitable. Our findings are based on theoretical and empirical insights.

#index 1697478
#* Exploiting result consistency to select query expansions for spoken content retrieval
#@ Stevan Rudinac;Martha Larson;Alan Hanjalic
#t 2010
#c 16
#% 397161
#% 818267
#% 1130851
#% 1311687
#% 1406326
#! We propose a technique that predicts both if and how expansion should be applied to individual queries. The prediction is made on the basis of the topical consistency of the top results of the initial results lists returned by the unexpanded query and several query expansion alternatives. We use the coherence score, known to capture the tightness of topical clustering structure, and also propose two simplified coherence indicators. We test our technique in a spoken content retrieval task, with the intention of helping to control the effects of speech recognition errors. Experiments use 46 semantic-theme-based queries defined by VideoCLEF 2009 over the TRECVid 2007 and 2008 video data sets. Our indicators make the best choice roughly 50% of the time. However, since they predict the right query expansion in critical cases, overall MAP improves. The approach is computationally lightweight and requires no training data.

#index 1697479
#* Statistics of online user-generated short documents
#@ Giacomo Inches;Mark J. Carman;Fabio Crestani
#t 2010
#c 16
#% 279755
#! User-generated short documents assume an important role in online communication due to the established utilization of social networks and real-time text messaging on the Internet. In this paper we compare the statistics of different online user-generated datasets and traditional TREC collections, investigating their similarities and differences. Our results support the applicability of traditional techniques also to user-generated short documents albeit with proper preprocessing.

#index 1697480
#* Mining neighbors' topicality to better control authority flow
#@ Na Dai;Brian D. Davison;Yaoshuang Wang
#t 2010
#c 16
#% 348173
#% 766462
#% 818254
#% 879576
#% 907542
#% 1041735
#% 1074106
#! Web pages are often recognized by others through contexts. These contexts determine how linked pages influence and interact with each other. When differentiating such interactions, the authority of web pages can be better estimated by controlling the authority flows among pages. In this work, we determine the authority distribution by examining the topicality relationship between associated pages. In addition, we find it is not enough to quantify the influence of authority propagation from only one type of neighbor, such as parent pages in PageRank algorithm, since web pages, like people, are influenced by diverse types of neighbors within the same network. We propose a probabilistic method to model authority flows from different sources of neighbor pages. In this way, we distinguish page authority interaction by incorporating the topical context and the relationship between associated pages. Experiments on the 2003 and 2004 TREC Web Tracks demonstrate that this approach outperforms other competitive topical ranking models and produces a more than 10% improvement over PageRank on the quality of top 10 search results. When increasing the types of incorporated neighbor sources, the performance shows stable improvements.

#index 1697481
#* Finding wormholes with flickr geotags
#@ Maarten Clements;Pavel Serdyukov;Arjen P. de Vries;Marcel J. T. Reinders
#t 2010
#c 16
#% 967244
#% 987205
#% 997189
#% 1190131
#% 1227637
#! We propose a kernel convolution method to predict similar locations (wormholes) based on human travel behaviour. A scaling parameter can be used to define a set of relevant users to the target location and we show how the geotags of these users can effectively be aggregated to predict a ranking of similar locations. We evaluate results on world and city level using several independent test collections.

#index 1697482
#* Enhancing n-gram-based summary evaluation using information content and a taxonomy
#@ Mijail Kabadjov;Josef Steinberger;Ralf Steinberger;Massimo Poesio;Bruno Pouliquen
#t 2010
#c 16
#% 816173
#% 1275285
#! In this paper we propose a novel information-theoretic metric for automatic summary evaluation when model summaries are available as in the setting of the AESOP task of the Update Summarization track of the Text Analysis Conference (TAC). The metric is based on the concept of information content operationalized by using a taxonomy. Hereby, we present and discuss the results obtained at TAC 2009.

#index 1697483
#* NEAT: news exploration along time
#@ Omar Alonso;Klaus Berberich;Srikanta Bedathur;Gerhard Weikum
#t 2010
#c 16
#! There are a number of efforts towards building applications that leverage temporal information in documents. The demonstration of our NEAT (News Exploration Along Time) prototype system that we propose here, is an attempt towards building an intuitive and exploratory interface for search results over large news archives using timelines. The demonstration uses the New York Times Annotated Corpus as an illustrative example of such a news archive. The NEAT system consists of two parts: the back-end server extracts and stores in an index all the temporal information from documents, and performs important phrase discovery from sentences that have time-sensitive information. The front-end user interface, anchors the results of a keyword search along the timeline where the user can explore and browse results at different points in time. To aid in this exploration, the interesting phrases discovered from the result documents are displayed on the timeline to provide an overview. Another key feature of NEAT, which distinguishes it from other timeline-based approaches, is the adoption of semantic temporal annotations to anchor results on the timeline. An appropriate choice of personally-identifiable temporal annotations can enable users to more effectively contextualize results. For example, Barack Obama was elected in 2008 and Germany hosted the FIFA World Cup in 2006. We gathered temporal annotations at large-scale by crowdsourcing it over Amazon Mechanical Turk (AMT). Each HIT (Human Intelligence Task) on AMT consists of a request to expand a temporal expression (such as a year, a time-interval, or decade, etc.) with an entity (e.g., a person, country, organization etc.). Based on the agreement level among workers, we derive key entities for constructing a semantic temporal annotation layer on top the timeline. The outcome is a manually annotated timeline that can be very useful to anchor search results. Examples of annotations produced by crowdsourcing are (1969: Woodstock, Moon landing), (1970: Nixon), and (2003-2009: Iraq war) to name a few with different time granularities. The demonstration consists of an exploratory search interface where we show how queries can produce different timelines and how one can use temporal information to discover interesting facts.

#index 1697484
#* Opinion summarization of web comments
#@ Martin Potthast;Steffen Becker
#t 2010
#c 16
#% 722308
#% 805873
#% 907489
#% 1190068
#% 1227704
#% 1260722
#! All kinds of Web sites invite visitors to provide feedback on comment boards. Typically, submitted comments are published immediately on the same page, so that new visitors can get an idea of the opinions of previous visitors. Popular multimedia items, such as videos and images, frequently get up to thousands of comments, which is too much to be read in reasonable time. I.e., visitors read, if at all, only the newest comments and hence get an incomplete and possibly misleading picture of the overall opinion. To address this issue we introduce OPINIONCLOUD, a technology to summarize and visualize opinions that are expressed in the form of Web comments.

#index 1697485
#* EUROGENE: multilingual retrieval and machine translation applied to human genetics
#@ Petr Knoth;Trevor Collins;Elsa Sklavounou;Zdenek Zdrahal
#t 2010
#c 16
#! The objective of Eurogene is to collect a critical mass of educational content in the field of human genetics in nine European languages and to build a platform that will support the retrieval, sharing and navigation over the learning content. The Eurogene platform is already operational and is being used by the genetics community. In this paper, a part of the Eurogene platform related to the retrieval and machine translation of domain specific content is described. Our contribution lies in an approach for domain-specific adaption of cross-language information retrieval (CLIR) and machine translation (MT). The CLIR system is based on a multilingual domain ontology which is also used as a synchronization component between CLIR and MT. The MT system is adapted to the target domain using the terminology represented in the ontology and using statistical training performed on a collection of parallel texts. In the statistical training phase, new translations of a term can be discovered and used for ontology updating. The paper is organized as follows. First, we describe the motivation for our approach and the multilingual domain ontology. Later, the CLIR and MT components and their domain adaption and synchronization are discussed.

#index 1697486
#* NETSPEAK: assisting writers in choosing words
#@ Martin Potthast;Martin Trenkmann;Benno Stein
#t 2010
#c 16
#! Netspeak is a Web service which helps writers in finding alternative expressions for what they want to say. It provides a large index of writing samples in the form of n-grams, n≤5, along with an efficient means to retrieve them by the use of wildcard queries. When in doubt about a phrasing, a user can get additional evidence by retrieving samples that match a given context. The figure below shows the results for a query where a user is interested in the two most frequently written words between “looks” and “me”. The first two columns give an idea about the customariness of each result, and the user can select the one most appropriate for her sentence.

#index 1697487
#* A data analysis and modelling framework for the evaluation of interactive information retrieval
#@ Ralf Bierig;Michael Cole;Jacek Gwizdka;Nicholas J. Belkin
#t 2010
#c 16
#% 1415706
#! Over the last two decades, Interactive Information Retrieval (IIR) has established a new direction within the long tradition of IR that introduces the user at its center and poses new challenges for system evaluation. IR systems can improve performance by utilizing information about the entire interactive process of search. This approach has so far only been initially explored [1,2] with much potential for the future. This demonstration describes an extensible data analysis and modelling framework that enables researchers to integrate, explore and analyze interactive experiment data obtained from task-based IIR experiments and build and test models of interactive user behavior.

#index 1715591
#* Proceedings of the 27th European conference on Advances in Information Retrieval Research
#@ David E. Losada;Juan M. Fernández-Luna
#t 2005
#c 16

#index 1715592
#* A probabilistic logic for information retrieval
#@ C. J. ‘Keith' van Rijsbergen
#t 2005
#c 16
#% 261352
#% 384634
#% 758200
#% 836017
#% 1561813
#! One of the most important models for IR derives from the representation of documents and queries as vectors in a vector space. I will show how logic emerges from the geometry of such a vector space. As a consequence of looking at such a space in terms of states and observables I will show how an appropriate probability measure can be constructed on this space which may be the basis for a suitable probabilistic logic for information retrieval.

#index 1715593
#* Applications of web query mining
#@ Ricardo Baeza-Yates
#t 2005
#c 16
#% 232674
#% 296646
#% 298183
#% 309209
#% 309767
#% 310567
#% 323131
#% 330617
#% 340888
#% 387427
#% 438557
#% 466653
#% 476462
#% 501656
#% 552181
#% 554898
#% 590526
#% 622705
#% 728105
#% 1712595
#! Server logs of search engines store traces of queries submitted by users, which include queries themselves along with Web pages selected in their answers. The same is true in Web site logs where queries and later actions are recorded from search engine referrers or from an internal search box. In this paper we present two applications based in analyzing and clustering queries. The first one suggest changes to improve the text and structure of a Web site and the second does relevance ranking boosting and query recommendation in search engines.

#index 1715594
#* BuddyNet: history-based P2P search
#@ Yilei Shao;Randolph Wang
#t 2005
#c 16
#% 340175
#% 340176
#% 344403
#% 349973
#% 433981
#% 505869
#% 622634
#% 643013
#% 646221
#% 646239
#% 674136
#% 723298
#% 805478
#! Peer-to-peer file sharing has become a very popular Internet application. P2P systems such as Gnutella and Kazaa work well when the number of peers is small. Their performances degraded significantly when the number of peers scales. In order to overcome the scalability problem, numerous research groups have experimented with different approaches. We conduct a novel evaluation study on Kazaa traffic focusing on the interest-based locality. Our analysis shows that strong interest-based locality exist in P2P systems and can be exploited to improve performance. Based on our findings, we propose a history-based P2P search algorithm and topology adaptation mechanism. The resulting system naturally clusters peers with similar interests to each other and greatly improves the efficiency for searching. We test our design through simulations; the results show significant reduction in total system load and large speedup in search efficiency compared to random walk and interest shortcut schemes. In addition, we show that our system is more robust under dynamic situations.

#index 1715595
#* A suite of testbeds for the realistic evaluation of peer-to-peer information retrieval systems
#@ Iraklis A. Klampanos;Victor Poznański;Joemon M. Jose;Peter Dickman
#t 2005
#c 16
#% 340176
#% 349973
#% 391644
#% 397204
#% 397441
#% 505869
#% 511663
#% 610851
#% 643013
#% 730035
#% 737424
#! Peer-to-peer (P2P) networking continuously gains popularity among computing science researchers. The problem of information retrieval (IR) over P2P networks is being addressed by researchers attempting to provide valuable insight as well as solutions for its successful deployment. All published studies have, so far, been evaluated by simulation means, using well-known document collections (usually acquired from TREC). Researchers test their systems using divided collections whose documents have been previously distributed to a number of simulated peers. This practice leads to two problems: First, there is little justification in favour of the document distributions used by relevant studies and second, since different studies use different experimental testbeds, there is no common ground for comparing the solutions proposed. In this work, we contribute a number of different document testbeds for evaluating P2P IR systems. Each of these has been deduced from TREC's WT10g collection and corresponds to different potential P2P IR application scenarios. We analyse each methodology and testbed with respect to the document distributions achieved as well as to the location of relevant items within each setting. This work marks the beginning of an effort to provide more realistic evaluation environments for P2P IR systems as well as to create a common ground for comparisons of existing and future architectures.

#index 1715596
#* Federated search of text-based digital libraries in hierarchical peer-to-peer networks
#@ Jie Lu;Jamie Callan
#t 2005
#c 16
#% 227891
#% 280856
#% 340175
#% 340176
#% 340941
#% 481748
#% 505869
#% 636008
#% 643011
#% 643012
#% 646221
#% 722312
#% 730035
#% 783550
#! Peer-to-peer architectures are a potentially powerful model for developing large-scale networks of text-based digital libraries, but peer-to-peer networks have so far provided very limited support for text-based federated search of digital libraries using relevance-based ranking. This paper addresses the problems of resource representation, resource ranking and selection, and result merging for federated search of text-based digital libraries in hierarchical peer-to-peer networks. Existing approaches to text-based federated search are adapted and new methods are developed for resource representation and resource selection according to the unique characteristics of hierarchical peer-to-peer networks. Experimental results demonstrate that the proposed approaches offer a better combination of accuracy and efficiency than more common alternatives for federated search in peer-to-peer networks.

#index 1715597
#* ‘Beauty' of the world wide web—cause, goal, or principle
#@ Sándor Dominich;Júlia Góth;Mária Horváth;Tamás Kiezer
#t 2005
#c 16
#% 281214
#% 283833
#% 309749
#% 510723
#! It is known that the degree distribution in the World Wide Web (WWW) obeys a power law whose degree exponent exhibits a fairly robust behaviour. The usual method, linear regression, used to construct the power law is not based on any, probably existing, intrinsic property of the WWW which it is assumed to reflect. In the present paper, statistical evidence is given to conjecture that at the heart of this robustness property lies the Golden Section. Applications of this conjecture are also presented and discussed.

#index 1715598
#* sPLMap: a probabilistic approach to schema matching
#@ Henrik Nottelmann;Umberto Straccia
#t 2005
#c 16
#% 280038
#% 283052
#% 292510
#% 333990
#% 340146
#% 342705
#% 344447
#% 378409
#% 572314
#% 654458
#% 654459
#% 765433
#% 801676
#! This paper introduces the first formal framework for learning mappings between heterogeneous schemas which is based on logics and probability theory. This task, also called “schema matching”, is a crucial step in integrating heterogeneous collections. As schemas may have different granularities, and as schema attributes do not always match precisely, a general-purpose schema mapping approach requires support for uncertain mappings, and mappings have to be learned automatically. The framework combines different classifiers for finding suitable mapping candidates (together with their weights), and selects that set of mapping rules which is the most likely one. Finally, the framework with different variants has been evaluated on two different data sets.

#index 1715599
#* Encoding XML in vector spaces
#@ Vinay Kakade;Prabhakar Raghavan
#t 2005
#c 16
#% 340914
#% 345708
#% 345712
#% 479956
#% 552188
#% 577218
#% 577353
#% 642993
#% 654442
#% 729941
#% 754116
#% 757550
#% 765408
#% 765423
#% 840583
#! We develop a framework for representing XML documents and queries in vector spaces and build indexes for processing text-centric semi-structured queries that support a proximity measure between XML documents. The idea of using vector spaces for XML retrieval is not new. In this paper we (i) unify prior approaches into a single framework; (ii) develop techniques to eliminate special purpose auxiliary computations (outside the vector space) used previously; (iii) give experimental evidence on benchmark queries that our approach is competitive in its retrieval quality and (iv) as an immediate consequence of the framework, are able to classify and cluster XML documents.

#index 1715600
#* Features combination for extracting gene functions from MEDLINE
#@ Patrick Ruch;Laura Perret;Jacques Savoy
#t 2005
#c 16
#% 71752
#% 194251
#% 194284
#% 219051
#% 280835
#% 309115
#% 309116
#% 376266
#% 387791
#% 397161
#% 465754
#% 469402
#% 478604
#% 742399
#% 748733
#% 853693
#% 1223716
#% 1223722
#% 1478493
#! This paper describes and evaluates a summarization system that extracts the gene function textual descriptions (called GeneRIF) based on a MedLine record. Inputs for this task include both a locus (a gene in the LocusLink database), and a pointer to a MedLine record supporting the GeneRIF. In the suggested approach we merge two independent phrase extraction strategies. The first proposed strategy (LASt) uses argumentative, positional and structural features in order to suggest a GeneRIF. The second extraction scheme (LogReg) incorporates statistical properties to select the most appropriate sentence as the GeneRIF. Based on the TREC-2003 genomic collection, the basic extraction strategies are already competitive (52.78% for LASt and 52.28% for LogReg, respectively). When used in a combined approach, the extraction task clearly shows improvement, achieving a Dice score of over 55%.

#index 1715601
#* Filtering for profile-biased multi-document summarization
#@ Sana Leila Châar;Olivier Ferret;Christian Fluhr
#t 2005
#c 16
#% 259990
#% 742204
#% 816173
#% 817578
#% 939711
#% 1223706
#! In this article, we present an information filtering method that selects from a set of documents their most significant excerpts in relation to a user profile. This method relies on both structured profiles and a topical analysis of documents. The topical analysis is also used for expanding a profile in relation to a particular document by selecting the terms of the document that are closely linked to those of the profile. This expansion is a way for selecting in a more reliable way excerpts that are linked to profiles but also for selecting excerpts that may bring new and interesting information about their topics. This method was implemented by the REDUIT system, which was successfully evaluated for document filtering and passage extraction.

#index 1715602
#* Automatic text summarization based on word-clusters and ranking algorithms
#@ Massih R. Amini;Nicolas Usunier;Patrick Gallinari
#t 2005
#c 16
#% 144013
#% 194251
#% 218978
#% 266370
#% 280835
#% 280838
#% 309116
#% 340936
#% 397136
#% 734915
#% 757855
#% 815924
#% 823317
#% 1306081
#! This paper investigates a new approach for Single Document Summarization based on a Machine Learning ranking algorithm. The use of machine learning techniques for this task allows one to adapt summaries to the user needs and to the corpus characteristics. These desirable properties have motivated an increasing amount of work in this field over the last few years. Most approaches attempt to generate summaries by extracting text-spans (sentences in our case) and adopt the classification framework which consists to train a classifier in order to discriminate between relevant and irrelevant spans of a document. A set of features is first used to produce a vector of scores for each sentence in a given document and a classifier is trained in order to make a global combination of these scores. We believe that the classification criterion for training a classifier is not adapted for SDS and propose an original framework based on ranking for this task. A ranking algorithm also combines the scores of different features but its criterion tends to reduce the relative misordering of sentences within a document. Features we use here are either based on the state-of-the-art or built upon word-clusters. These clusters are groups of words which often co-occur with each other, and can serve to expand a query or to enrich the representation of the sentences of the documents. We analyze the performance of our ranking algorithm on two data sets – the Computation and Language (cmp_lg) collection of TIPSTER SUMMAC and the WIPO collection. We perform comparisons with different baseline – non learning – systems, and a reference trainable summarizer system based on the classification framework. The experiments show that the learning algorithms perform better than the non-learning systems while the ranking algorithm outperforms the classifier. The difference of performance between the two learning algorithms depends on the nature of datasets. We give an explanation of this fact by the different separability hypothesis of the data made by the two learning algorithms.

#index 1715603
#* Comparing topiary-style approaches to headline generation
#@ Ruichao Wang;Nicola Stokes;William P. Doran;Eamonn Newman;Joe Carthy;John Dunnion
#t 2005
#c 16
#% 280909
#% 309115
#% 677173
#% 740329
#% 746865
#% 815290
#% 816173
#% 855368
#! In this paper we compare a number of Topiary-style headline generation systems. The Topiary system, developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels for headlines, while our approach, the LexTrim system, identifies key summary words by analysing the lexical cohesive structure of a text. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection. The results of these experiments show that a baseline system that identifies topic descriptors for headlines using term frequency counts outperforms the LexTrim and Topiary systems. A manual evaluation of the headlines also confirms this result.

#index 1715604
#* Improving retrieval effectiveness by using key terms in top retrieved documents
#@ Yang Lingpeng;Ji Donghong;Zhou Guodong;Nie Yu
#t 2005
#c 16
#% 120104
#% 232647
#% 262084
#% 316898
#% 399890
#% 406493
#% 578875
#% 939717
#! In this paper, we propose a method to improve the precision of top retrieved documents in Chinese information retrieval where the query is a short description by re-ordering retrieved documents in the initial retrieval. To re-order the documents, we firstly find out terms in query and their importance scales by making use of the information derived from top N(NK(NK) documents by what kinds of terms of query they contain. That is, we first automatically extract key terms from top N retrieved documents, then we collect key terms that occur in query and their document frequencies in the N retrieved documents, finally we use these collected terms to re-order the initially retrieved documents. Each collected term is assigned a weight by its length and its document frequency in top N retrieved documents. Each document is re-ranked by the sum of weights of collected terms it contains. In our experiments on 42 query topics in NTCIR3 Cross Lingual Information Retrieval (CLIR) dataset, an average 17.8%-27.5% improvement can be made for top 10 documents and an average 6.6%-26.9% improvement can be made for top 100 documents at relax/rigid relevance judgment and different parameter setting.

#index 1715605
#* Evaluating relevance feedback algorithms for searching on small displays
#@ Vishwa Vinay;Ingemar J. Cox;Natasa Milic-Frayling;Ken Wood
#t 2005
#c 16
#% 115473
#% 118726
#% 184493
#% 224702
#% 232719
#% 281147
#% 297600
#% 324192
#% 330780
#% 343130
#% 428278
#% 577323
#% 1854912
#! Searching online information resources using mobile devices is affected by displays on which only a small fraction of the set of ranked documents can be displayed. In this paper, we ask whether the search effort can be reduced, on average, by user feedback indicating a single most relevant document in each display. For small display sizes and limited user actions, we are able to construct a tree representing all possible outcomes. Examination of the tree permits us to compute an upper limit on relevance feedback performance. Three standard feedback algorithms are considered – Rocchio, Robertson/Sparck-Jones and a Bayesian algorithm. Two display strategies are considered, one based on maximizing the immediate information gain and the other on most likely documents. Our results bring out the strengths and weaknesses of the algorithms, and the need for exploratory display strategies with conservative feedback algorithms.

#index 1715606
#* Term frequency normalisation tuning for BM25 and DFR models
#@ Ben He;Iadh Ounis
#t 2005
#c 16
#% 218982
#% 296646
#% 324129
#% 340146
#% 375017
#% 397183
#% 411760
#% 730008
#! The term frequency normalisation parameter tuning is a crucial issue in information retrieval (IR), which has an important impact on the retrieval performance. The classical pivoted normalisation approach suffers from the collection-dependence problem. As a consequence, it requires relevance assessment for each given collection to obtain the optimal parameter setting. In this paper, we tackle the collection-dependence problem by proposing a new tuning method by measuring the normalisation effect. The proposed method refines and extends our methodology described in [7]. In our experiments, we evaluate our proposed tuning method on various TREC collections, for both the normalisation 2 of the Divergence From Randomness (DFR) models and the BM25's normalisation method. Results show that for both normalisation methods, our tuning method significantly outperforms the robust empirically-obtained baselines over diverse TREC collections, while having a marginal computational cost.

#index 1715607
#* Improving the context-based influence diagram model for structured document retrieval: removing topological restrictions and adding new evaluation methods
#@ Luis M. de Campos;Juan M. Fernández-Luna;Juan F. Huete
#t 2005
#c 16
#% 22388
#% 34262
#% 44876
#% 46437
#% 262069
#% 269954
#% 351595
#% 387427
#% 397166
#% 487135
#% 789964
#! In this paper we present the theoretical developments necessary to extend the existing Context-based Influence Diagram Model for Structured Documents (CID model), in order to improve its retrieval performance and expressiveness. Firstly, we make it more flexible and general by removing the original restrictions on the type of structured documents that CID represents. This extension requires the design of a new algorithm to compute the posterior probabilities of relevance. Another contribution is related to the evaluation of the influence diagram. The computation of the expected utilities in the original CID model was approximated by applying an independence criterion. We present another approximation that does not assume independence, as well as an exact evaluation method.

#index 1715608
#* Knowing-aboutness: question-answering using a logic-based framework
#@ Terence Clifton;William Teahan
#t 2005
#c 16
#% 220131
#% 280862
#% 319874
#% 330616
#% 340953
#% 392811
#% 742082
#% 742102
#% 815868
#% 815916
#! We describe the background and motivation for a logic-based framework, based on the theory of “Knowing-Aboutness”, and its specific application to Question-Answering. We present the salient features of our system, and outline the benefits of our framework in terms of a more integrated architecture that is more easily evaluated. Favourable results are presented in the TREC 2004 Question-Answering evaluation.

#index 1715609
#* Modified LSI model for efficient search by metric access methods
#@ Tomáš Skopal;Pavel Moravec
#t 2005
#c 16
#% 169817
#% 224113
#% 248027
#% 257145
#% 262096
#% 276754
#% 281750
#% 306497
#% 340886
#% 342827
#% 342828
#% 387427
#% 463737
#% 479462
#% 510499
#% 571079
#% 593842
#% 655302
#! Text collections represented in LSI model are hard to search efficiently (i.e. quickly), since there exists no indexing method for the LSI matrices. The inverted file, often used in both boolean and classic vector model, cannot be effectively utilized, because query vectors in LSI model are dense. A possible way for efficient search in LSI matrices could be the usage of metric access methods (MAMs). Instead of cosine measure, the MAMs can utilize the deviation metric for query processing as an equivalent dissimilarity measure. However, the intrinsic dimensionality of collections represented by LSI matrices is often large, which decreases MAMs' performance in searching. In this paper we introduce σ-LSI, a modification of LSI in which we artificially decrease the intrinsic dimensionality of LSI matrices. This is achieved by an adjustment of singular values produced by SVD. We show that suitable adjustments could dramatically improve the efficiency when searching by MAMs, while the precision/recall values remain preserved or get only slightly worse.

#index 1715610
#* PIRE: an extensible IR engine based on probabilistic datalog
#@ Henrik Nottelmann
#t 2005
#c 16
#% 33922
#% 36683
#% 103705
#% 109191
#% 118756
#% 120111
#% 169780
#% 176471
#% 176530
#% 262096
#% 280038
#% 292510
#% 458744
#% 480418
#% 643011
#% 723305
#! This paper introduces PIRE, a probabilistic IR engine. For both document indexing and retrieval, PIRE makes heavy use of probabilistic Datalog, a probabilistic extension of predicate Horn logics. Using such a logical framework together with probability theory allows for defining and using data types (e.g. text, names, numbers), different weighting schemes (e.g. normalised tf, tf.idf or BM25) and retrieval functions (e.g. uncertain inference, language models). Extending the system thus is reduced to adding new rules. Furthermore, this logical framework provide a powerful tool for including additional background knowledge into the retrieval process.

#index 1715611
#* Data fusion with correlation weights
#@ Shengli Wu;Sally McClean
#t 2005
#c 16
#% 111303
#% 124009
#% 144076
#% 232703
#% 340934
#% 340936
#% 413613
#% 413634
#% 420464
#% 784148
#! This paper is focused on the effect of correlation on data fusion for multiple retrieval results. If some of the retrieval results involved in data fusion correlate more strongly than the others, their common opinion will dominate the voting process in data fusion. This may degrade the effectiveness of data fusion in many cases, especially when very good results appear to be a minority. For solving this problem, we assign each result a weight, which is derived from the correlation coefficient of that result to the other results, then the linear combination method can be used for data fusion. The evaluation of the effectiveness of the proposed method with TREC 5 ( ad hoc track) results is reported. Furthermore, we explore the relationship between results correlation and data fusion by some experiments, and demonstrate that a relationship between them does exists.

#index 1715612
#* Using restrictive classification and meta classification for junk elimination
#@ Stefan Siersdorfer;Gerhard Weikum
#t 2005
#c 16
#% 99690
#% 132938
#% 209021
#% 260001
#% 276506
#% 279755
#% 309141
#% 387427
#% 420077
#% 458379
#% 465754
#% 629610
#% 729932
#% 766435
#% 783476
#% 1080960
#! This paper addresses the problem of performing supervised classification on document collections containing also junk documents. With ”junk documents” we mean documents that do not belong to the topic categories (classes) we are interested in. This type of documents can typically not be covered by the training set; nevertheless in many real world applications (e.g. classification of web or intranet content, focused crawling etc.) such documents occur quite often and a classifier has to make a decision about them. We tackle this problem by using restrictive methods and ensemble-based meta methods that may decide to leave out some documents rather than assigning them to inappropriate classes with low confidence. Our experiments with four different data sets show that the proposed techniques can eliminate a relatively large fraction of junk documents while dismissing only a significantly smaller fraction of potentially interesting documents.

#index 1715613
#* On compression-based text classification
#@ Yuval Marton;Ning Wu;Lisa Hellerstein
#t 2005
#c 16
#% 318412
#% 344447
#% 420507
#% 423359
#% 423365
#% 465754
#% 588082
#% 642987
#% 650285
#% 735077
#% 748738
#% 816178
#% 1387556
#! Compression-based text classification methods are easy to apply, requiring virtually no preprocessing of the data. Most such methods are character-based, and thus have the potential to automatically capture non-word features of a document, such as punctuation, word-stems, and features spanning more than one word. However, compression-based classification methods have drawbacks (such as slow running time), and not all such methods are equally effective. We present the results of a number of experiments designed to evaluate the effectiveness and behavior of different compression-based text classification methods on English text. Among our experiments are some specifically designed to test whether the ability to capture non-word (including super-word) features causes character-based text compression methods to achieve more accurate classification.

#index 1715614
#* Ontology as a search-tool: a study of real users' query formulation with and without conceptual support
#@ Sari Suomela;Jaana Kekäläinen
#t 2005
#c 16
#% 111456
#% 187756
#% 197986
#% 223368
#% 318787
#% 329088
#% 329090
#% 420521
#% 575729
#! This study examines 16 real users' use of an ontology as a search tool. The users' queries constructed with the help of a Concept-based Information Retrieval Interface (CIRI) were compared to queries created independently based on the same search task description. Also the effectiveness of the CIRI queries was compared to the users' unaided queries. The simulated search task method was used to make the searching situations as close to real as possible. Due to CIRI's query expansion feature the number of search terms was remarkably higher in ontology queries than in Direct interface queries. The search results were evaluated with generalised precision and generalised relative recall as well as precision based on personal assessments. The Direct interface queries performed better in all methods of comparison.

#index 1715615
#* An analysis of query similarity in collaborative web search
#@ Evelyn Balfe;Barry Smyth
#t 2005
#c 16
#% 194299
#% 232713
#% 253188
#% 342961
#% 348155
#% 643560
#% 769569
#! Web search logs provide an invaluable source of information regarding the search behaviour of users. This information can be reused to aid future searches, especially when these logs contain the searching histories of specific communities of users. To date this information is rarely exploited as most Web search techniques continue to rely on the more traditional term-based IR approaches. In contrast, the I-SPY system attempts to reuse past search behaviours as a means to re-rank result-lists according to the implied preferences of like-minded communities of users. It relies on the ability to recognise previous search sessions that are related to the current target search by looking for similarities between past and current queries. We have previously shown how a simple model of query similarity can significantly improve search performance by implementing this reuse approach. In this paper we build on previous work by evaluating alternative query similarity models.

#index 1715616
#* A probabilistic interpretation of precision, recall and F-score, with implication for evaluation
#@ Cyril Goutte;Eric Gaussier
#t 2005
#c 16
#% 144074
#% 236052
#% 269217
#% 375017
#% 411762
#% 757423
#% 939784
#! We address the problems of 1/ assessing the confidence of the standard point estimates, precision, recall and F-score, and 2/ comparing the results, in terms of precision, recall and F-score, obtained using two different methods. To do so, we use a probabilistic setting which allows us to obtain posterior distributions on these performance indicators, rather than point estimates. This framework is applied to the case where different methods are run on different datasets from the same source, as well as the standard situation where competing results are obtained on the same data.

#index 1715617
#* Exploring cost-effective approaches to human evaluation of search engine relevance
#@ Kamal Ali;Chi-Chao Chang;Yunfang Juan
#t 2005
#c 16
#% 109187
#% 208931
#% 262102
#% 268079
#% 309093
#% 309151
#% 411762
#% 561315
#% 766407
#% 766409
#% 1783186
#! In this paper, we examine novel and less expensive methods for search engine evaluation that do not rely on document relevance judgments. These methods, described within a proposed framework, are motivated by the increasing focus on search results presentation, by the growing diversity of documents and content sources, and by the need to measure effectiveness relative to other search engines. Correlation analysis of the data obtained from actual tests using a subset of the methods in the framework suggest that these methods measure different aspects of the search engine. In practice, we argue that the selection of the test method is a tradeoff between measurement intent and cost.

#index 1715618
#* Document identifier reassignment through dimensionality reduction
#@ Roi Blanco;Álvaro Barreiro
#t 2005
#c 16
#% 118749
#% 290703
#% 393450
#% 570319
#% 656274
#% 766445
#! Most modern retrieval systems use compressed Inverted Files (IF) for indexing. Recent works demonstrated that it is possible to reduce IF sizes by reassigning the document identifiers of the original collection, as it lowers the average distance between documents related to a single term. Variable-bit encoding schemes can exploit the average gap reduction and decrease the total amount of bits per document pointer. However, approximations developed so far requires great amounts of time or use an uncontrolled memory size. This paper presents an efficient solution to the reassignment problem consisting in reducing the input data dimensionality using a SVD transformation. We tested this approximation with the Greedy-NN TSP algorithm and one more efficient variant based on dividing the original problem in sub-problems. We present experimental tests and performance results in two TREC collections, obtaining good compression ratios with low running times. We also show experimental results about the tradeoff between dimensionality reduction and compression, and time performance.

#index 1715619
#* Scalability influence on retrieval models: an experimental methodology
#@ Amélie Imafouo;Michel Beigbeder
#t 2005
#c 16
#% 262097
#% 262102
#% 290703
#% 306494
#% 312689
#% 318416
#% 340890
#% 453327
#% 612492
#% 735073
#% 766409
#% 1387547
#! Few works in Information Retrieval (IR) tackled the questions of Information Retrieval Systems (IRS) effectiveness and efficiency in the context of scalability in corpus size. We propose a general experimental methodology to study the scalability influence on IR models. This methodology is based on the construction of a collection on which a given characteristic C is the same whatever be the portion of collection selected. This new collection called uniform can be split into sub-collection of growing size on which some given properties will be studied. We apply our methodology to WT10G (TREC9 collection) and consider the characteristic C to be the distribution of relevant documents on a collection. We build a uniform WT10G, sample it into sub-collections of increasing size and use these sub-collections to study the impact of corpus volume increase on standards IRS evaluation measures (recall/precision, high precision).

#index 1715620
#* The role of multi-word units in interactive information retrieval
#@ Olga Vechtomova
#t 2005
#c 16
#% 35937
#% 56830
#% 196896
#% 218978
#% 279755
#% 288541
#% 321635
#% 324192
#% 643001
#% 740900
#% 757280
#% 1414372
#% 1783134
#! The paper presents several techniques for selecting noun phrases for interactive query expansion following pseudo-relevance feedback and a new phrase search method. A combined syntactico-statistical method was used for the selection of phrases. First, noun phrases were selected using a part-of-speech tagger and a noun-phrase chunker, and secondly, different statistical measures were applied to select phrases for query expansion. Experiments were also conducted studying the effectiveness of noun phrases in document ranking. We analyse the problems of phrase weighting and suggest new ways of addressing them. A new method of phrase matching and weighting was developed, which specifically addresses the problem of weighting overlapping and non-contiguous word sequences in documents.

#index 1715621
#* Dictionary-based CLIR loses highly relevant documents
#@ Raija Lehtokangas;Heikki Keskustalo;Kalervo Järvelin
#t 2005
#c 16
#% 262047
#% 309095
#% 340892
#% 397164
#% 420520
#% 561144
#% 575729
#! Research on cross-language information retrieval (CLIR) has typically been restricted to settings using binary relevance assessments. In this paper, we present evaluation results for dictionary-based CLIR using graded relevance assessments in a best match retrieval environment. A text database containing newspaper articles and a related set of 35 search topics were used in the tests. First, monolingual baseline queries were automatically formed from the topics. Secondly, source language topics (in English, German, and Swedish) were automatically translated into the target language (Finnish), using both structured and unstructured queries. Effectiveness of the translated queries was compared to that of the monolingual queries. CLIR performance was evaluated using three relevance criteria: stringent, regular, and liberal. When regular or liberal criteria were used, a reasonable performance was achieved. Adopting stringent criteria caused a considerable loss of performance, when compared to monolingual Finnish performance.

#index 1715622
#* Football video segmentation based on video production strategy
#@ Reede Ren;Joemon M. Jose
#t 2005
#c 16
#% 335489
#% 435068
#% 451991
#% 545956
#% 661692
#% 971222
#% 1389564
#% 1775237
#% 1855495
#% 1858183
#% 1858223
#! We present a statistical approach for parsing football video structures. Based on video production conventions, a new generic structure called ‘attack' is identified, which is an equivalent of scene in other video domains. We define four video segments to construct it, namely play, focus, replay and break. Two middle level visual features, play field ratio and zoom size, are also computed. The detection process includes a two-pass classifier, a combination of Gaussian Mixture Model and Hidden Markov Models. A general suffix tree is introduced to identify and organize ‘attack'. In experiments, video structure classification accuracy of about 86% is achieved on broadcasting World Cup 2002 video data.

#index 1715623
#* Fractional distance measures for content-based image retrieval
#@ Peter Howarth;Stefan Rüger
#t 2005
#c 16
#% 376266
#% 464888
#% 465031
#% 733133
#% 1717349
#! We have applied the concept of fractional distance measures, proposed by Aggarwal et al. [1], to content-based image retrieval. Our experiments show that retrieval performances of these measures consistently outperform the more usual Manhattan and Euclidean distance metrics when used with a wide range of high-dimensional visual features. We used the parameters learnt from a Corel dataset on a variety of different collections, including the TRECVID 2003 and ImageCLEF 2004 datasets. We found that the specific optimum parameters varied but the general performance increase was consistent across all 3 collections. To squeeze the last bit of performance out of a system it would be necessary to train a distance measure for a specific collection. However, a fractional distance measure with parameter p = 0.5 will consistently outperform both L1 and L2 norms.

#index 1715624
#* Combining visual semantics and texture characterizations for precision-oriented automatic image retrieval
#@ Mohammed Belkhatir
#t 2005
#c 16
#% 2298
#% 54459
#% 262095
#% 316148
#% 316199
#% 318785
#% 334502
#% 413559
#% 435065
#% 522284
#% 563749
#% 589911
#% 626558
#% 642989
#% 726267
#% 1112584
#! The growing need for ‘intelligent' image retrieval systems leads to new architectures combining visual semantics and signal features that rely on highly expressive frameworks while providing fully-automated indexing and retrieval processes. Indeed, addressing the issue of integrating the two main approaches in the image indexing and retrieval literature (i.e. signal and semantic) is a viable solution for achieving significant retrieval quality. This paper presents a multi-facetted framework featuring visual semantics and signal texture descriptions for automatic image retrieval. It relies on an expressive representation formalism handling high-level image descriptions and a full-text query framework in an attempt to operate image indexing and retrieval operations beyond trivial low-level processes and loosely-coupled state-of-the-art systems. At the experimental level, we evaluate the retrieval performance of our system through recall and precision indicators on a test collection of 2500 photographs used in several world-class publications.

#index 1715625
#* Applying associative relationship on the clickthrough data to improve web search
#@ Xue-Mei Jiang;Wen-Guan Song;Hua-Jun Zeng
#t 2005
#c 16
#% 27049
#% 54413
#% 194299
#% 214673
#% 253188
#% 268073
#% 268079
#% 308745
#% 310567
#% 311871
#% 330617
#% 340928
#% 387427
#% 577224
#% 577273
#% 591792
#% 641976
#! The performance of web search engines may often deteriorate due to the diversity and noise contained within web pages. Some methods proposed to use clickthrough data to achieve more accurate information for web pages as well as improve the search performance. However, sparseness became the great challenge in exploiting clickthrough data. In this paper, we propose a novel algorithm to exploit the user clickthrough data. It first explores the relationship between queries and web pages to mine out co-visiting as the associative relationship among the Web pages, and then Spreading Activation mechanism is used to re-rank the results of Web search. Our approach could alleviate such sparseness and the experimental results on a large set of MSN clickthrough log data show a significant improvement on search performance over the DirectHit algorithm as well as the baseline search engine.

#index 1715626
#* Factors affecting web page similarity
#@ Anastasios Tombros;Zeeshan Ali
#t 2005
#c 16
#% 11646
#% 211526
#% 214673
#% 218992
#% 255137
#% 268073
#% 281209
#% 290830
#% 300967
#% 300971
#% 343768
#% 348165
#% 375017
#% 427921
#% 447948
#% 534048
#% 637577
#% 643069
#% 728756
#% 729974
#% 751569
#% 826267
#! Tools that allow effective information organisation, access and navigation are becoming increasingly important on the Web. Similarity between web pages is a concept that is central to such tools. In this paper, we examine the effect that content and layout-related aspects of web pages have on web page similarity. We consider the textual content contained within common HTML tags, the structural layout of pages, and the query terms contained within pages. Our study shows that combinations of factors can yield more promising results than individual factors, and that different aspects of web pages affect similarities between pages in a different manner. We found a number of factors that, when taken into account, can result in effective measures of similarity between web pages. Query information in particular, proved to be important for the effective organisation of web pages.

#index 1715627
#* Boosting web retrieval through query operations
#@ Gilad Mishne;Maarten de Rijke
#t 2005
#c 16
#% 109190
#% 118737
#% 169774
#% 298182
#% 306468
#% 323131
#% 387427
#% 406493
#% 438557
#% 504904
#% 642992
#% 648308
#% 854668
#% 1387547
#! We explore the use of phrase and proximity terms in the context of web retrieval, which is different from traditional ad-hoc retrieval both in document structure and in query characteristics. We show that for this type of task, the usage of both phrase and proximity terms is highly beneficial for early precision as well as for overall retrieval effectiveness. We also analyze why phrase and proximity terms are far more effective for web retrieval than for ad-hoc retrieval.

#index 1715628
#* Terrier information retrieval platform
#@ Iadh Ounis;Gianni Amati;Vassilis Plachouras;Ben He;Craig Macdonald;Douglas Johnson
#t 2005
#c 16
#! Terrier is a modular platform for the rapid development of large-scale Information Retrieval (IR) applications. It can index various document collections, including TREC and Web collections. Terrier also offers a range of document weighting and query expansion models, based on the Divergence From Randomness framework. It has been successfully used for ad-hoc retrieval, cross-language retrieval, Web IR and intranet search, in a centralised or distributed setting.

#index 1715629
#* Físréal: a low cost terabyte search engine
#@ Paul Ferguson;Cathal Gurrin;Peter Wilkins;Alan F. Smeaton
#t 2005
#c 16
#! In this poster we describe the development of a distributed search engine, referred to as Físréal, which utilises inexpensive workstations, yet attains fast retrieval performance for Terabyte-sized collections. We also discuss the process of leveraging additional meaning from the structure of HTML, as well as the use of anchor text documents to increase retrieval performance.

#index 1715630
#* Query formulation for answer projection
#@ Gilad Mishne;Maarten de Rijke
#t 2005
#c 16
#% 571493
#% 854668
#% 938759
#! We examine the effects of various query modifications on the problem of answer projection — the task of retrieving documents that support a given answer to a question. We compare different techniques such as phrase searches and term weighting, and show that some models achieve significant improvements over unmodified queries.

#index 1715631
#* Network analysis for distributed information retrieval architectures
#@ Fidel Cacheda;Victor Carneiro;Vassilis Plachouras;Iadh Ounis
#t 2005
#c 16
#% 339621
#% 397186
#% 1348062
#! In this study, we present the analysis of the interconnection network of a distributed Information Retrieval (IR) system, by simulating a switched network versus a shared access network. The results show that the use of a switched network improves the performance, especially in a replicated system because the switched network prevents the saturation of the network, particularly when using a large number of query servers.

#index 1715632
#* SnapToTell: a Singapore image test bed for ubiquitous information access from camera
#@ Jean-Pierre Chevallet;Joo-Hwee Lim;Ramnath Vasudha
#t 2005
#c 16
#% 120270
#! With the proliferation of camera phones, many novel applications and services are emerging. In this paper, we present the SnapToTell system, which provides information directory service to tourists, based on pictures taken by the camera phones and location information. We present also experimental results on scene recognition based on a realistic data set of scenes and locations in Singapore which form a new original application oriented image test bed freely available.

#index 1715633
#* Acquisition of translation knowledge of syntactically ambiguous named entity
#@ Takeshi Kutsumi;Takehiko Yoshimi;Katsunori Kotani;Ichiko Sata;Hitoshi Isahara
#t 2005
#c 16
#% 811367
#% 815913
#% 855296
#! Bilingual dictionaries are essential components of cross-lingual information retrieval applications. The automatic acquisition of proper names and their translations from bilingual corpora is especially important, because a significant portion of the entries not listed in the dictionaries would be proper names.

#index 1715634
#* IR and OLAP in XML document warehouses
#@ Juan M. Pérez;Torben Bach Pedersen;Rafael Berlanga;María J. Aramburu
#t 2005
#c 16
#% 309202
#% 340901
#% 378064
#! In this paper we propose to combine IR and OLAP (On-Line Analytical Processing) technologies to exploit a warehouse of text-rich XML documents. In the system we plan to develop, a multidimensional implementation of a relevance modeling document model will be used for interactively querying the warehouse by allowing navigation in the structure of documents and in a concept hierarchy of query terms. The facts described in the relevant documents will be ranked and analyzed in a novel OLAP cube model able to represent and manage facts with relevance indexes.

#index 1715635
#* Manipulating the relevance models of existing search engines
#@ Oisín Boydell;Cathal Gurrin;Alan F. Smeaton;Barry Smyth
#t 2005
#c 16
#% 769569
#! Collaborative search refers to how the search behavior of communities of users can be used to influence the ranking of search results. In this poster we describe how this technique, as instantiated in the I-SPY meta-search engine can be used as a general mechanism for implementing a different relevance feedback strategy. We evaluate a relevance feedback strategy based on anchor-text and query similarity using the TREC2004 Terabyte track document collection.

#index 1715636
#* Enhancing web search result lists using interaction histories
#@ Maurice Coyle;Barry Smyth
#t 2005
#c 16
#% 127574
#% 262084
#% 268079
#% 769569
#! As a method for information retrieval (IR) on the Web, search engines have become the tool of choice for most online users. However, despite the variety of next generation approaches to Web search we have seen recently (e.g. [1,2]), the problems of information overload, vague user queries and spam still have the effect that many search sessions end in user frustration. Generally search engines are criticised for returning result lists that have low precision, where the user's information need is not satisfied by any of the returned result pages.

#index 1715637
#* An evaluation of gisting in mobile search
#@ Karen Church;Mark T. Keane;Barry Smyth
#t 2005
#c 16
#% 475237
#% 803556
#! Mobile devices suffer from limited screen real-estate and restricted text input capabilities. In the recent past these limitations have greatly effected the usability of many mobile Internet applications [1], largely because little effort has been typically made to take account of the special features of the mobile Internet. These limitations are especially problematic for mobile search-engines: they restrict the number of results that can be displayed per screen and impact the type of queries that are likely to be provided. Nevertheless, most attempts to provide mobile search engines have involved making only simplistic adaptations to standard search interfaces. For example, fewer results per page are returned and the ‘snippet' text associated with each result may be truncated [2]. We believe that more fundamental adaptations are necessary if search technology is to succeed in the mobile space. In this paper we focus on the snippet text issue and we argue that providing paragraphs of descriptive text alongside each result is a luxury that does not make sense in the context of mobile device limitations. We describe how the I-SPY system [3] can track and record past queries that have resulted in the selection of a given result page and we argue that these related queries can be used to help users understand the context of a search result in place of more verbose snippet text.

#index 1715638
#* Video shot classification using lexical context
#@ Stéphane Ayache;Georges Quénot;Mbarek Charhad
#t 2005
#c 16
#% 379602
#! Associating concepts to video segments is essential for content-based video retrieval. We present here a semantic classifier working from text transcriptions coming from automatic speech recognition (ASR). The system is based on a Bayesian classifier, it is fully linked with a knowledge base which contains an ontology and named entities from several domains. The system is trained from a set of positive and negative examples for each indexed concept. It has been evaluated using the TREC VIDEO protocol and conditions for the detection of visual concepts. Three versions are compared: a baseline one, using only word as units, a second, using additionally named entities, and a last one enriched with semantic classes information.

#index 1715639
#* Age dependent document priors in link structure analysis
#@ Claudia Hauff;Leif Azzopardi
#t 2005
#c 16
#% 280850
#! Much research has been performed investigating how links between web pages can be exploited in an Information Retrieval setting [1,4]. In this poster, we investigate the application of the Barabási-Albert model to link structure analysis on a collection of web documents within the language modeling framework. Our model utilizes the web structure as described by a Scale Free Network and derives a document prior based on a web document's age and linkage. Preliminary experiments indicate the utility of our approach over other current link structure algorithms and warrants further research.

#index 1715640
#* Improving image representation with relevance judgements from the searchers
#@ Liudmila V. Boldareva
#t 2005
#c 16
#% 1273828
#! In visual information retrieval, a semantic gap exists due to the poor match between machine-understood content of an information object and the userpercepted one. The mismatch of perception results in di.culties for a user in formulating the query, and consequently in inability for the retrieval system to produce satisfactory answers. Adding searcher's relevance judgements for (intermediary) search results is known to improve the retrieval. With relevance feedback the system learns the user's information need through interaction.

#index 1715641
#* Temporal shot clustering analysis for video concept detection
#@ Dayong Ding;Le Chen;Bo Zhang
#t 2005
#c 16
#% 780820
#% 1715644
#! The phenomenon that conceptually related shots appear together in videos is called temporal shot clustering. This phenomenon is a useful cue for video concept detection, which is one of basic steps in content-based video indexing and retrieval. We propose a method, called temporal shot clustering analysis, to improve results of video concept detection by exploiting the temporal shot clustering phenomenon. Two other methods are compared with temporal shot clustering analysis on the TRECVID 2003 dataset. Experiments showed that temporal shot clustering is of much benefit for video concept detection, and that temporal shot clustering method outperforms the other methods.

#index 1715642
#* IRMAN: software framework for IR in mobile social cyberspaces
#@ Zia Syed;Fiona Walsh
#t 2005
#c 16
#% 409279
#% 663683
#! With the increasing popularity of blogs (online journals) as a medium for expressing personal thoughts and advice, and users becoming more mobile, we foresee an opportunity for such opinionated content to be utilised as information sources in the mobile arena. In this short paper, we present IRMAN (Information Retrieval in Mobile Adhoc Networks), a software framework for Peer-to-Peer (P2P) IR over Mobile AdHoc Networks (MANET). A Java based prototype system has been developed based on the aforementioned framework for creating, retrieving, and sharing user blogs on handhelds in mobile social cyberspaces.

#index 1715643
#* Assigning geographical scopes to web pages
#@ Bruno Martins;Marcirio Chaves;Mário J. Silva
#t 2005
#c 16
#% 397186
#% 480467
#% 744539
#% 766441
#! Finding automatic ways of attaching geographical scopes to on-line resources, also called “geo-referencing” documents, is a challenging problem, getting increasing attention [1,5,3]. Here we present a system architecture and a process for identifying the geographical scope of Web pages, defining a scope as the region where more people than average would find that page relevant. We rely on typical Web IR heuristics (i.e. feature weighting, hypertext topic locality, anchor description) and assumptions on how people use geographical references in documents. The method involves three major steps. First, geographical named entities are identified in the text. Next, we propagate the found named entities through the Web linkage graph. Finally, a geographical ontology is used to disambiguate among the named entities associated to a document, this way selecting the most likely scope. In the future, we plan on using scopes in new location-aware search tools.

#index 1715644
#* AP-based borda voting method for feature extraction in TRECVID-2004
#@ Le Chen;Dayong Ding;Dong Wang;Fuzong Lin;Bo Zhang
#t 2005
#c 16
#% 340936
#% 1828408
#! We present a novel fusion method — AP-based Borda voting method (APBB)— for rankings. Due to its adaptive weighting scheme, APBB outperforms many traditional methods. Comparative experiments on TRECVID 2004 data were carried out and showed the robustness and effectiveness of this method.

#index 1742067
#* Proceedings of the 28th European conference on Advances in Information Retrieval
#@ Mounia Lalmas;Andy MacFarlane;Stefan Rüger;Anastasios Tombros;Theodora Tsikrika
#t 2006
#c 16

#index 1742068
#* Progress in information retrieval
#@ Mounia Lalmas;Stefan Rüger;Theodora Tsikrika;Alexei Yavlinsky
#t 2006
#c 16
#% 1742070
#% 1742071
#% 1742072
#% 1742073
#% 1742074
#% 1742075
#% 1742076
#% 1742077
#% 1742078
#% 1742079
#% 1742080
#% 1742081
#% 1742082
#% 1742083
#% 1742084
#% 1742085
#% 1742086
#% 1742087
#% 1742088
#% 1742089
#% 1742090
#% 1742091
#% 1742092
#% 1742093
#% 1742094
#% 1742095
#% 1742096
#% 1742097
#% 1742098
#% 1742099
#% 1742100
#% 1742101
#% 1742102
#% 1742103
#% 1742104
#% 1742105
#% 1742106
#% 1742107
#% 1742108
#% 1742109
#% 1742110
#% 1742111
#% 1742112
#% 1742113
#% 1742114
#% 1742115
#% 1742116
#% 1742117
#% 1742118
#% 1742119
#% 1742120
#% 1742121
#% 1742122
#% 1742123
#% 1742124
#% 1742125
#% 1742126
#% 1742127
#% 1742128
#% 1742129
#% 1742130
#% 1742131
#% 1742132
#% 1742133
#% 1742134
#% 1742135
#% 1742136
#! This paper summarises the scientific work presented at the 28th European Conference on Information Retrieval and demonstrates that the field has not only significantly progressed over the last year but has also continued to make inroads into areas such as Genomics, Multimedia, Peer-to-Peer and XML retrieval.

#index 1742069
#* Enterprise search — the new frontier?
#@ David Hawking
#t 2006
#c 16
#! The advent of the current generation of Web search engines around 1998 challenged the relevance of academic information retrieval research – established evaluation methodologies didn't scale and nor did they reflect the diverse purposes to which search engines are now put. Academic ranking algorithms of the time almost completely ignored the features which underpin modern web search: query-independent evidence and evidence external to the document. Unlike their commercial counterparts, academic researchers have for years been unable to access Web scale collections and their corresponding link graphs and search logs.

#index 1742070
#* Frequentist and bayesian approach to information retrieval
#@ Giambattista Amati
#t 2006
#c 16
#% 81669
#% 169781
#% 262096
#% 280851
#% 321635
#% 326522
#% 340899
#% 342707
#% 406493
#% 411760
#% 730008
#% 750863
#% 766463
#% 818261
#% 1633095
#% 1715628
#! We introduce the hypergeometric models KL, DLH and DLLH using the DFR approach, and we compare these models to other relevant models of IR. The hypergeometric models are based on the probability of observing two probabilities: the relative within-document term frequency and the entire collection term frequency. Hypergeometric models are parameter-free models of IR. Experiments show that these models have an excellent performance with small and very large collections. We provide their foundations from the same IR probability space of language modelling (LM). We finally discuss the difference between DFR and LM. Briefly, DFR is a frequentist (Type I), or combinatorial approach, whilst language models use a Bayesian (Type II) approach for mixing the two probabilities, being thus inherently parametric in its nature.

#index 1742071
#* Using proportional transportation distances for measuring document similarity
#@ Xiaojun Wan;Jianwu Yang
#t 2006
#c 16
#% 169781
#% 218982
#% 325683
#% 340948
#% 375017
#% 387427
#% 397137
#% 457991
#% 465914
#% 604673
#% 643064
#% 719598
#% 748583
#% 786504
#! A novel document similarity measure based on the Proportional Transportation Distance (PTD) is proposed in this paper. The proposed measure improves on the previously proposed similarity measure based on optimal matching by allowing many-to-many matching between subtopics of documents. After documents are decomposed into sets of subtopics, the Proportional Transportation Distance is employed to evaluate the similarity between sets of subtopics for two documents by solving a transportation problem. Experiments on TDT-3 data demonstrate its good ability for measuring document similarity and also its high robustness, i.e. it does not rely on the underlying document decomposition algorithm largely as the optimal matching based measure.

#index 1742072
#* A user-item relevance model for log-based collaborative filtering
#@ Jun Wang;Arjen P. de Vries;Marcel J. T. Reinders
#t 2006
#c 16
#% 144074
#% 262096
#% 280852
#% 320432
#% 330687
#% 340948
#% 342687
#% 375017
#% 397127
#% 397153
#% 406493
#% 452563
#% 734594
#% 818216
#% 874253
#% 1273828
#% 1650470
#% 1650569
#! Implicit acquisition of user preferences makes log-based collaborative filtering favorable in practice to accomplish recommendations. In this paper, we follow a formal approach in text retrieval to re-formulate the problem. Based on the classic probability ranking principle, we propose a probabilistic user-item relevance model. Under this formal model, we show that user-based and item-based approaches are only two different factorizations with different independence assumptions. Moreover, we show that smoothing is an important aspect to estimate the parameters of the models due to data sparsity. By adding linear interpolation smoothing, the proposed model gives a probabilistic justification of using TF×IDF-like item ranking in collaborative filtering. Besides giving the insight understanding of the problem of collaborative filtering, we also show experiments in which the proposed method provides a better recommendation performance on a music play-list data set.

#index 1742073
#* Generating search term variants for text collections with historic spellings
#@ Andrea Ernst-Gerlach;Norbert Fuhr
#t 2006
#c 16
#% 115462
#% 219033
#% 221974
#% 275837
#% 290482
#% 461072
#% 1715610
#! In this paper, we describe a new approach for retrieval in texts with non-standard spelling, which is important for historic texts in English or German. For this purpose, we present a new algorithm for generating search term variants in ancient orthography. By applying a spell checker on a corpus of historic texts, we generate a list of candidate terms for which the contemporary spellings have to be assigned manually. Then our algorithm produces a set of probabilistic rules. These probabilities can be considered for ranking in the retrieval stage. An experimental comparison shows that our approach outperforms competing methods.

#index 1742074
#* Efficient phrase querying with common phrase index
#@ Matthew Chang;Chung Keung Poon
#t 2006
#c 16
#% 109190
#% 210172
#% 212665
#% 213786
#% 249989
#% 253191
#% 262099
#% 280839
#% 301263
#% 323131
#% 340886
#% 359132
#% 397150
#! In this paper, we propose a common phrase index as an efficient index structure to support phrase queries in a very large text database. Our structure is an extension of previous index structures for phrases and achieves better query efficiency with negligible extra storage cost. In our experimental evaluation, a common phrase index has 5% and 20% improvement in query time for the overall and large queries (queries of long phrases) respectively over an auxiliary nextword index. Moreover, it uses only 1% extra storage cost. Compared with an inverted index, our improvement is 40% and 72% for the overall and large queries respectively.

#index 1742075
#* Document length normalization using effective level of term frequency in large collections
#@ Soheila Karbasi;Mohand Boughanem
#t 2006
#c 16
#% 46803
#% 217268
#% 218982
#% 280040
#% 375017
#% 406493
#% 730008
#% 766412
#% 1306081
#% 1715606
#! The effectiveness of the information retrieval systems is largely dependent on term-weighting. Most current term-weighting approaches involve the use of term frequency normalization. We develop here a method to assess the potential role of the term frequency-inverse document frequency measures that are commonly used in text retrieval systems. Since automatic information retrieval systems have to deal with documents of varying sizes and terms of varying frequencies, we carried out preliminary tests to evaluate the effect of term-weighing items on the retrieval performance. With regard to the preliminary tests, we identify a novel factor (effective level of term frequency) that represents the document content based on its length and maximum term-frequency. This factor is used to find the maximum main terms within the documents and an appropriate subset of documents containing the query terms. We show that, all document terms need not be considered for ranking a document with respect to a query. Regarding the result of the experiments, the effective level of term frequency (EL) is a significant factor in retrieving relevant documents, especially in large collections. Experiments were under-taken on TREC collections to evaluate the effectiveness of our proposal.

#index 1742076
#* Beyond the web: retrieval in social information spaces
#@ Sebastian Marius Kirsch;Melanie Gnasa;Armin B. Cremers
#t 2006
#c 16
#% 220708
#% 290830
#% 296646
#% 389364
#% 1599303
#! We research whether the inclusion of information about an information user's social environment and his position in the social network of his peers leads to an improval in search effectiveness. Traditional information retrieval methods fail to address the fact that information production and consumption are social activities. We ameliorate this problem by extending the domain model of information retrieval to include social networks. We describe a technique for information retrieval in such an enviroment and evaluate it in comparison to vector space retrieval.

#index 1742077
#* Evaluating web search result summaries
#@ Shao Fen Liang;Siobhan Devlin;John Tait
#t 2006
#c 16
#% 312689
#% 381263
#% 740925
#% 744549
#% 814903
#% 817578
#% 818305
#% 1291575
#! The aim of our research is to produce and assess short summaries to aid users' relevance judgements, for example for a search engine result page. In this paper we present our new metric for measuring summary quality based on representativeness and judgeability, and compare the summary quality of our system to that of Google. We discuss the basis for constructing our evaluation methodology in contrast to previous relevant open evaluations, arguing that the elements which make up an evaluation methodology: the tasks, data and metrics, are interdependent and the way in which they are combined is critical to the effectiveness of the methodology. The paper discusses the relationship between these three factors as implemented in our own work, as well as in SUMMAC/MUC/DUC.

#index 1742078
#* Measuring the complexity of a collection of documents
#@ Vishwa Vinay;Ingemar J. Cox;Natasa Milic-Frayling;Ken Wood
#t 2006
#c 16
#% 32813
#% 36672
#% 248027
#% 375017
#% 465031
#% 818267
#% 826267
#% 1633021
#% 1650387
#! Some text collections are more difficult to search or more complex to organize into topics than others. What properties of the data characterize this complexity? We use a variation of the Cox-Lewis statistic to measure the natural tendency of a set of points to fall into clusters. We compute this quantity for document collections that are represented as a set of term vectors. We consider applications of the Cox-Lewis statistic in three scenarios: comparing clusterability of different text collections using the same representation, comparing different representations of the same text collection, and predicting the query performance based on the clusterability of the query results set. Our experimental results show a correlation between the observed effectiveness and this statistic, thereby demonstrating the utility of such data analysis in text retrieval.

#index 1742079
#* Sentence retrieval with LSI and topic identification
#@ David Parapar;Álvaro Barreiro
#t 2006
#c 16
#% 257145
#% 340884
#% 397161
#% 766525
#% 814952
#! This paper presents two sentence retrieval methods. We adopt the task definition done in the TREC Novelty Track: sentence retrieval consists in the extraction of the relevant sentences for a query from a set of relevant documents for that query. We have compared the performance of the Latent Semantic Indexing (LSI) retrieval model against the performance of a topic identification method, also based on Singular Value Decomposition (SVD) but with a different sentence selection method. We used the TREC Novelty Track collections from years 2002 and 2003 for the evaluation. The results of our experiments show that these techniques, particularly sentence retrieval based on topic identification, are valid alternative approaches to other more ad-hoc methods devised for this task.

#index 1742080
#* Ranking web news via homepage visual layout and cross-site voting
#@ Jinyi Yao;Jue Wang;Zhiwei Li;Mingjing Li;Wei-Ying Ma
#t 2006
#c 16
#% 290830
#% 309749
#% 316546
#% 1394202
#! Reading news is one of the most popular activities when people surf the internet. As too many news sources provide independent news information and each has its own preference, detecting unbiased important news might be very useful for users to keep up to date with what are happening in the world. In this paper we present a novel method to identify important news in web environment which consists of diversified online news sites. We observe that a piece of important news generally occupies visually significant place in some homepage of a news site and import news event will be reported by many news sites. To explore these two properties, we model the relationship between homepages, news and latent events by a tripartite graph, and present an algorithm to identify important news in this model. Based on this algorithm, we implement a system TOPSTORY to dynamically generate homepages for users to browse important news reports. Our experimental study indicates the effectiveness of proposed approach.

#index 1742081
#* Clustering-Based searching and navigation in an online news source
#@ Simón C. Smith;M. Andrea Rodríguez
#t 2006
#c 16
#% 309098
#% 387427
#% 445316
#% 607944
#% 657358
#% 730043
#% 794512
#% 807295
#% 1387550
#! The growing amount of online news posted on the WWW demands new algorithms that support topic detection, search, and navigation of news documents. This work presents an algorithm for topic detection that considers the temporal evolution of news and the structure of web documents. Then, it uses the results of the topic detection algorithm for searching and navigating in an online news source. An experimental evaluation with a collection of online news in Spanish indicates the advantages of incorporating the temporal aspect and structure of documents in the topic detection of news. In addition, topic-based clusters are well suited for guiding the search and navigation of news.

#index 1742082
#* Mobile clustering engine
#@ Claudio Carpineto;Andrea Della Pietra;Stefano Mizzaro;Giovanni Romano
#t 2006
#c 16
#% 342962
#% 508265
#% 643068
#% 739771
#% 742991
#% 807295
#% 807348
#! Although mobile information retrieval is seen as the next frontier of the search market, the rendering of results on mobile devices is still unsatisfactory. We present Credino, a clustering engine for PDAs based on the theory of concept lattices that can help overcome some specific challenges posed by small-screen, narrow-band devices. Credino is probably the first clustering engine for mobile devices freely available for testing on the Web. An experimental evaluation, besides confirming that finding information is more difficult on a PDA than on a desktop computer, suggests that mobile clustering engine is more effective than mobile search engine.

#index 1742083
#* Improving quality of search results clustering with approximate matrix factorisations
#@ Stanislaw Osinski
#t 2006
#c 16
#% 67565
#% 218992
#% 262045
#% 281186
#% 329562
#% 342617
#% 643008
#% 710338
#% 754124
#% 766432
#% 813043
#% 1387282
#! In this paper we show how approximate matrix factorisations can be used to organise document summaries returned by a search engine into meaningful thematic categories. We compare four different factorisations (SVD, NMF, LNMF and K-Means/Concept Decomposition) with respect to topic separation capability, outlier detection and label quality. We also compare our approach with two other clustering algorithms: Suffix Tree Clustering (STC) and Tolerance Rough Set Clustering (TRC). For our experiments we use the standard merge-then-cluster approach based on the Open Directory Project web catalogue as a source of human-clustered document summaries.

#index 1742084
#* Adapting the naive bayes classifier to rank procedural texts
#@ Ling Yin;Richard Power
#t 2006
#c 16
#% 280817
#% 290482
#% 318412
#% 344447
#% 397177
#% 413633
#% 466240
#% 854882
#% 1260645
#% 1650665
#! This paper presents a machine-learning approach for ranking web documents according to the proportion of procedural text they contain. By ‘procedural text' we refer to ordered lists of steps, which are very common in some instructional genres such as online manuals. Our initial training corpus is built up by applying some simple heuristics to select documents from a large collection and contains only a few documents with a large proportion of procedural texts. We adapt the Naive Bayes classifier to better fit this less than ideal training corpus. This adapted model is compared with several other classifiers in ranking procedural texts using different sets of features and is shown to perform well when only highly distinctive features are used.

#index 1742085
#* The effects of relevance feedback quality and quantity in interactive relevance feedback: a simulation based on user modeling
#@ Heikki Keskustalo;Kalervo Järvelin;Ari Pirkola
#t 2006
#c 16
#% 67565
#% 318941
#% 340892
#% 397164
#% 575729
#% 580079
#% 742666
#% 793013
#% 1348075
#! Experiments on the effectiveness of relevance feedback with real users are time-consuming and expensive. This makes simulation for rapid testing desirable. We define a user model, which helps to quantify some interaction decisions involved in simulated relevance feedback. First, the relevance criterion defines the relevance threshold of the user to accept documents as relevant to his/her needs. Second, the browsing effort refers to the patience of the user to browse through the initial list of retrieved documents in order to give feedback. Third, the feedback effort refers to the effort and ability of the user to collect feedback documents. We use the model to construct several simulated relevance feedback scenarios in a laboratory setting. Using TREC data providing graded relevance assessments, we study the effect of the quality and quantity of the feedback documents on the effectiveness of the relevance feedback and compare this to the pseudo-relevance feedback. Our results indicate that one can compensate large amounts of relevant but low quality feedback by small amounts of highly relevant feedback.

#index 1742086
#* Using query profiles for clarification
#@ Henning Rode;Djoerd Hiemstra
#t 2006
#c 16
#% 65948
#% 118726
#% 337521
#% 389801
#% 643520
#% 766408
#! The following paper proposes a new kind of relevance feedback. It shows how so-called query profiles can be employed for disambiguation and clarification. Query profiles provide useful summarized previews on the retrieved answers to a given query. They outline ambiguity in the query and when combined with appropriate means of interactivity allow the user to easily adapt the final ranking. Statistical analysis of the profiles even enables the retrieval system to automatically suggest search restrictions or preferences. The paper shows a preliminary experimental study of the proposed feedback methods within the setting of TREC's interactive HARD track.

#index 1742087
#* Lexical entailment for information retrieval
#@ Stéphane Clinchant;Cyril Goutte;Eric Gaussier
#t 2006
#c 16
#% 228088
#% 262096
#% 280851
#% 363038
#% 406493
#% 465754
#% 588693
#% 719598
#% 722935
#% 740900
#% 933785
#% 938719
#% 1269526
#! Textual Entailment has recently been proposed as an application independent task of recognising whether the meaning of one text may be inferred from another. This is potentially a key task in many NLP applications. In this contribution, we investigate the use of various lexical entailment models in Information Retrieval, using the language modelling framework. We show that lexical entailment potentially provides a significant boost in performance, similar to pseudo-relevance feedback, but at a lower computational cost. In addition, we show that the performance is relatively stable with respect to the corpus the lexical entailment measure is estimated on.

#index 1742088
#* A hybrid approach to index maintenance in dynamic text retrieval systems
#@ Stefan Büttcher;Charles L. A. Clarke
#t 2006
#c 16
#% 86532
#% 169814
#% 172922
#% 188587
#% 249989
#% 378239
#% 655485
#% 747117
#% 801833
#% 810908
#% 838465
#% 838541
#! In-place and merge-based index maintenance are the two main competing strategies for on-line index construction in dynamic information retrieval systems based on inverted lists. Motivated by recent results for both strategies, we investigate possible combinations of in-place and merge-based index maintenance. We present a hybrid approach in which long posting lists are updated in-place, while short lists are updated using a merge strategy. Our experimental results show that this hybrid approach achieves better indexing performance than either method (in-place, merge-based) alone.

#index 1742089
#* Efficient parallel computation of pagerank
#@ Christian Kohlschütter;Paul-Alexandru Chirita;Wolfgang Nejdl
#t 2006
#c 16
#% 427394
#% 457936
#% 480136
#% 565488
#% 577328
#% 610830
#% 744920
#% 754088
#% 769506
#% 805897
#% 838516
#% 1016164
#! PageRank inherently is massively parallelizable and distributable, as a result of web's strict host-based link locality. We show that the Gauß-Seidel iterative method can actually be applied in such a parallel ranking scenario in order to improve convergence. By introducing a two-dimensional web model and by adapting the PageRank to this environment, we present efficient methods to compute the exact rank vector even for large-scale web graphs in only a few minutes and iteration steps, with intrinsic support for incremental web crawling, and without the need for page sorting/reordering or for sharing global rank information.

#index 1742090
#* Comparing different architectures for query routing in peer-to-peer networks
#@ Henrik Nottelmann;Norbert Fuhr
#t 2006
#c 16
#% 194246
#% 280853
#% 282422
#% 340175
#% 413594
#% 496291
#% 643011
#% 722312
#% 730035
#! Efficient and effective routing of content-based queries is an emerging problem in peer-to-peer networks, and can be seen as an extension of the traditional “resource selection” problem. Although some approaches have been proposed, finding the best architecture (defined by the network topology, the underlying selection method, and its integration into peer-to-peer networks) is still an open problem. This paper investigates different building blocks of such architectures, among them the decision-theoretic framework, CORI, hierarchical networks, distributed hash tables and HyperCubes. The evaluation on a large test-bed shows that the decision-theoretic framework can be applied effectively and cost-efficiently onto peer-to-peer networks.

#index 1742091
#* Automatic document organization in a p2p environment
#@ Stefan Siersdorfer;Sergej Sizov
#t 2006
#c 16
#% 35764
#% 99690
#% 132938
#% 209021
#% 266215
#% 276506
#% 379340
#% 387427
#% 420077
#% 552172
#% 629610
#% 703747
#% 722902
#% 727929
#% 729932
#% 766435
#% 783476
#% 998588
#% 1080960
#% 1683910
#! This paper describes an efficient method to construct reliable machine learning applications in peer-to-peer (P2P) networks by building ensemble based meta methods. We consider this problem in the context of distributed Web exploration applications like focused crawling. Typical applications are user-specific classification of retrieved Web contents into personalized topic hierarchies as well as automatic refinements of such taxonomies using unsupervised machine learning methods (e.g. clustering). Our approach is to combine models from multiple peers and to construct the advanced decision model that takes the generalization performance of multiple ‘local' peer models into account. In addition, meta algorithms can be applied in a restrictive manner, i.e. by leaving out some ‘uncertain' documents. The results of our systematic evaluation show the viability of the proposed approach.

#index 1742092
#* Exploring URL hit priors for web search
#@ Ruihua Song;Guomao Xin;Shuming Shi;Ji-Rong Wen;Wei-Ying Ma
#t 2006
#c 16
#% 262061
#% 287282
#% 387427
#% 397126
#% 590523
#% 754059
#% 805878
#% 818233
#% 818254
#% 818255
#! URL usually contains meaningful information for measuring the relevance of a Web page to a query in Web search. Some existing works utilize URL depth priors (i.e. the probability of being a good page given the length and depth of a URL) for improving some types of Web search tasks. This paper suggests the use of the location of query terms occur in a URL for measuring how well a web page is matched with a user's information need in web search. First, we define and estimate URL hit types, i.e. the priori probability of being a good answer given the type of query term hits in the URL. The main advantage of URL hit priors (over depth priors) is that it can achieve stable improvement for both informational and navigational queries. Second, an obstacle of exploiting such priors is that shortening and concatenation are frequently used in a URL. Our investigation shows that only 30% URL hits are recognized by an ordinary word breaking approach. Thus we combine three methods to improve matching. Finally, the priors are integrated into the probabilistic model for enhancing web document retrieval. Our experiments were conducted using 7 query sets of TREC2002, TREC2003 and TREC2004, and show that the proposed approach is stable and improve retrieval effectiveness by 4%~11% for navigational queries and 10% for informational queries.

#index 1742093
#* A study of blog search
#@ Gilad Mishne;Maarten de Rijke
#t 2006
#c 16
#% 296646
#% 306468
#% 323135
#% 438557
#% 577360
#% 590523
#% 754059
#% 766447
#% 801838
#% 807420
#% 810175
#% 853543
#! We present an analysis of a large blog search engine query log, exploring a number of angles such as query intent, query topics, and user sessions. Our results show that blog searches have different intents than general web searches, suggesting that the primary targets of blog searchers are tracking references to named entities, and locating blogs by theme. In terms of interest areas, blog searchers are, on average, more engaged in technology, entertainment, and politics than web searchers, with a particular interest in current events. The user behavior observed is similar to that in general web search: short sessions with an interest in the first few results only.

#index 1742094
#* A comparative study of the effectiveness of search result presentation on the web
#@ Hideo Joho;Joemon M. Jose
#t 2006
#c 16
#% 218992
#% 253188
#% 262036
#% 297550
#% 344930
#% 397175
#% 717120
#% 807644
#% 814952
#% 835027
#! Presentation of search results in Web-based information retrieval (IR) systems has been dominated by a textual form of information such as the title, snippet, URL, and/or file type of retrieved documents. On the other hand, document's visual aspects such as the layout, colour scheme, or presence of images have been studied in a limited context with regard to their effectiveness of search result presentation. This paper presents a comparative evaluation of textual and visual forms of document summaries as the additional document surrogate in the search result presentation. In our study, a sentence-based summarisation technique was used to create a textual document summary, and the thumbnail image of web pages was used to represent a visual summary. The experimental results suggest that both have the cases where the additional elements contributed to a positive effect not only in users' relevance assessment but also in query re/formulation. The results also suggest that the two forms of document summary are likely to have different contexts to facilitate user's search experience. Therefore, our study calls for further research on adaptive models of IR systems to make use of their advantages in appropriate contexts.

#index 1742095
#* Bricks: the building blocks to tackle query formulation in structured document retrieval
#@ Roelof van Zwol;Jeroen Baas;Herre van Oostendorp;Frans Wiering
#t 2006
#c 16
#% 214577
#% 340920
#% 394790
#% 580082
#% 766415
#% 835027
#% 920505
#% 1721852
#% 1721861
#% 1721882
#! Structured document retrieval focusses on the retrieval of relevant document fragments for a given information need that contains both structural and textual aspects. We focus here on the theory behind Bricks, a visual query formulation technique for structured document retrieval that aims at reducing the complexity of the query formulation process and required knowledge of the underlying document structure for the user, while maintaining full expression power, as offered by the NEXI query language for XML retrieval. In addition, we present the outcomes of a large scale usability experiment, which compared Bricks to a keyword-based and a NEXI-based interface. The results show that participants were more successful at completing a search assignments using Bricks. Furthermore, we observed that the participants were also able to successfully complete complex search assignments significantly faster, when using the Bricks interface.

#index 1742096
#* Structural feedback for keyword-based XML retrieval
#@ Ralf Schenkel;Martin Theobald
#t 2006
#c 16
#% 45311
#% 92696
#% 397358
#% 654442
#% 742666
#% 766417
#% 785542
#% 810052
#% 810913
#% 824703
#% 838452
#% 1015352
#! Keyword-based queries are an important means to retrieve information from XML collections with unknown or complex schemas. Relevance Feedback integrates relevance information provided by a user to enhance retrieval quality. For keyword-based XML queries, feedback engines usually generate an expanded keyword query from the content of elements marked as relevant or nonrelevant. This approach that is inspired by text-based IR completely ignores the semistructured nature of XML. This paper makes the important step from pure content-based to structural feedback. It presents a framework that expands a keyword query into a full-fledged content-and-structure query. Extensive experiments with the established INEX benchmark and our TopX search engine show the feasibility of our approach.

#index 1742097
#* Machine learning ranking for structured information retrieval
#@ Jean-Noël Vittaut;Patrick Gallinari
#t 2006
#c 16
#% 118756
#% 169774
#% 272510
#% 411762
#% 564279
#% 790835
#% 803546
#% 818255
#% 920505
#% 931132
#% 1715602
#! We consider the Structured Information Retrieval task which consists in ranking nested textual units according to their relevance for a given query, in a collection of structured documents. We propose to improve the performance of a baseline Information Retrieval system by using a learning ranking algorithm which operates on scores computed from document elements and from their local structural context. This model is trained to optimize a Ranking Loss criterion using a training set of annotated examples composed of queries and relevance judgments on a subset of the document elements. The model can produce a ranked list of documents elements which fulfills a given information need expressed in the query. We analyze the performance of our algorithm on the INEX collection and compare it to a baseline model which is an adaptation of Okapi to Structured Information Retrieval.

#index 1742098
#* Generating and retrieving text segments for focused access to scientific documents
#@ Caterina Caracciolo;Maarten de Rijke
#t 2006
#c 16
#% 24624
#% 46803
#% 64900
#% 123782
#% 144011
#% 144012
#% 169809
#% 197837
#% 199760
#% 201992
#% 211514
#% 221576
#% 230521
#% 232677
#% 378487
#% 382563
#% 504890
#% 691255
#% 706148
#% 742204
#% 748583
#% 927251
#! When presented with a retrieved document, users of a search engine are usually left with the task of pinning down the relevant information inside the document. Often this is done by a time-consuming combination of skimming, scrolling and Ctrl+F. In the setting of a digital library for scientific literature the issue is especially urgent when dealing with reference works, such as surveys and handbooks, as these typically contain long documents. Our aim is to develop methods for providing a “go-read-here” type of retrieval functionality, which points the user to a segment where she can best start reading to find out about her topic of interest. We examine multiple query-independent ways of segmenting texts into coherent chunks that can be returned in response to a query. Most (experienced) authors use paragraph breaks to indicate topic shifts, thus providing us with one way of segmenting documents. We compare this structural method with semantic text segmentation methods, both with respect to topical focus and relevancy. Our experimental evidence is based on manually segmented scientific documents and a set of queries against this corpus. Structural segmentation based on contiguous blocks of relevant paragraphs is shown to be a viable solution for our intended application of providing “go-read-here” functionality.

#index 1742099
#* Browsing personal images using episodic memory (time + location)
#@ Chufeng Chen;Michael Oakes;John Tait
#t 2006
#c 16
#% 247268
#% 248127
#% 262089
#% 378541
#% 452642
#% 476215
#% 555288
#% 642984
#% 760826
#% 789897
#% 824531
#! In this paper we consider episodic memory for system design in image retrieval. Time and location are the main factors in episodic memory, and these types of data were combined for image event clustering. We conducted a user studies to compare five image browsing systems using searching time and user satisfaction as criteria for success. Our results showed that the browser which clusters images based on time and location data combined was significantly better than four other more standard browsers. This suggests that episodic memory is potentially useful for improving personal image management.

#index 1742100
#* An information retrieval system for motion capture data
#@ Bastian Demuth;Tido Röder;Meinard Müller;Bernhard Eberhardt
#t 2006
#c 16
#% 771057
#% 810109
#% 815985
#% 816095
#% 1016194
#! Motion capturing has become an important tool in fields such as sports sciences, biometrics, and particularly in computer animation, where large collections of motion material are accumulated in the production process. In order to fully exploit motion databases for reuse and for the synthesis of new motions, one needs efficient retrieval and browsing methods to identify similar motions. So far, only ad-hoc methods for content-based motion retrieval have been proposed, which lack efficiency and rely on quantitative, numerical similarity measures, making it difficult to identify logically related motions. We propose an efficient motion retrieval system based on the query-by-example paradigm, which employs qualitative, geometric similarity measures. This allows for intuitive and interactive browsing in a purely content-based fashion without relying on textual annotations. We have incorporated this technology in a novel user interface facilitating query formulation as well as visualization and ranking of search results.

#index 1742101
#* Can a workspace help to overcome the query formulation problem in image retrieval?
#@ Jana Urban;Joemon M. Jose
#t 2006
#c 16
#% 116374
#% 238283
#% 262089
#% 406493
#% 789358
#% 871566
#% 1740553
#! We have proposed a novel image retrieval system that incorporates a workspace where users can organise their search results. A task-oriented and user-centred experiment has been devised involving design professionals and several types of realistic search tasks. We study the workspace's effect on two aspects: task conceptualisation and query formulation. A traditional relevance feedback system serves as baseline. The results of this study show that the workspace is more useful with respect to both of the above aspects. The proposed approach leads to a more effective and enjoyable search experience.

#index 1742102
#* A fingerprinting technique for evaluating semantics based indexing
#@ Eduard Hoenkamp;Sander van Dijk
#t 2006
#c 16
#% 218988
#% 262870
#% 340899
#% 572499
#% 766481
#! The quality of search engines depends usually on the content of the returned documents rather than on the text used to express this content. So ideally, search techniques should be directed more toward the semantic dependencies underlying documents than toward the texts themselves. The most visible examples in this direction are Latent Semantic Analysis (LSA), and the Hyperspace Analog to Language (HAL). If these techniques are really based on semantic dependencies, as they contend, then they should be applicable across languages. To investigate this contention we used electronic versions of two kinds of material with their translations: a novel, and a popular treatise about cosmology. We used the analogy of fingerprinting as employed in forensics to establish whether individuals are related. Genetic fingerprinting uses enzymes to split the DNA and then compare the resulting band patterns. Likewise, in our research we used queries to split a document into fragments. If a search technique really isolates fragments semantically related to the query, then a document and its translation should have similar band patterns. In this paper we (1) present the fingerprinting technique, (2) introduce the material used, and (3) report results of an evaluation for two semantic indexing techniques.

#index 1742103
#* A cross-language approach to historic document retrieval
#@ Marijn Koolen;Frans Adriaans;Jaap Kamps;Maarten de Rijke
#t 2006
#c 16
#% 118765
#% 131061
#% 236052
#% 288885
#% 732844
#% 732845
#% 732847
#% 927251
#! Our cultural heritage, as preserved in libraries, archives and museums, is made up of documents written many centuries ago. Large-scale digitization initiatives make these documents available to non-expert users through digital libraries and vertical search engines. For a user, querying a historic document collection may be a disappointing experience: queries involving modern words may not be very effective for retrieving documents that contain many historic terms. We propose a cross-language approach to historic document retrieval, and investigate (1) the automatic construction of translation resources for historic languages, and (2) the retrieval of historic documents using cross-language information retrieval techniques. Our experimental evidence is based on a collection of 17th century Dutch documents and a set of 25 known-item topics in modern Dutch. Our main findings are as follows: First, we are able to automatically construct rules for modernizing historic language based on comparing (a) phonetic sequence similarity, (b) the relative frequency of consonant and vowel sequences, and (c) the relative frequency of character n-gram sequences, of historic and modern corpora. Second, modern queries are not very effective for retrieving historic documents, but the historic language tools lead to a substantial improvement in retrieval effectiveness. The improvements are above and beyond the improvement due to using a modern stemming algorithm (whose effectiveness actually goes up when the historic language is modernized).

#index 1742104
#* Automatic acquisition of chinese–english parallel corpus from the web
#@ Ying Zhang;Ke Wu;Jianfeng Gao;Phil Vines
#t 2006
#c 16
#% 81669
#% 262047
#% 280826
#% 288948
#% 340966
#% 458405
#% 465760
#% 482240
#% 735134
#% 735135
#% 744035
#! Parallel corpora are a valuable resource for tasks such as cross-language information retrieval and data-driven natural language processing systems. Previously only small scale corpora have been available, thus restricting their practical use. This paper describes a system that overcomes this limitation by automatically collecting high quality parallel bilingual corpora from the web. Previous systems used a single principle feature for parallel web page verification, whereas we use multiple features to identify parallel texts via a k-nearest-neighbor classifier. Our system was evaluated using a data set containing 6500 Chinese–English candidate parallel pairs that have been manually annotated. Experiments show that the use of a k-nearest-neighbors classifier with multiple features achieves substantial improvements over the systems that use any one of these features. The system achieved a precision rate of 95% and a recall rate of 97%, and thus is a significant improvement over earlier work.

#index 1742105
#* Fast discovery of similar sequences in large genomic collections
#@ Yaniv Bernstein;Michael Cameron
#t 2006
#c 16
#% 143306
#% 201935
#% 235941
#% 255137
#% 290703
#% 300103
#% 309093
#% 504572
#% 728115
#% 790903
#% 838536
#% 978157
#! Detection of highly similar sequences within genomic collections has a number of applications, including the assembly of expressed sequence tag data, genome comparison, and clustering sequence collections for improved search speed and accuracy. While several approaches exist for this task, they are becoming infeasible — either in space or in time — as genomic collections continue to grow at a rapid pace. In this paper we present an approach based on document fingerprinting for identifying highly similar sequences. Our approach uses a modest amount of memory and executes in a time roughly proportional to the size of the collection. We demonstrate substantial speed improvements compared to the CD-HIT algorithm, the most successful existing approach for clustering large protein sequence collections.

#index 1742106
#* Using concept-based indexing to improve language modeling approach to genomic IR
#@ Xiaohua Zhou;Xiaodan Zhang;Xiaohua Hu
#t 2006
#c 16
#% 169768
#% 262096
#% 278109
#% 280851
#% 286069
#% 324129
#% 340899
#% 591629
#% 730063
#% 750863
#% 829971
#% 884518
#% 1290067
#! Genomic IR, characterized by its highly specific information need, severe synonym and polysemy problem, long term name and rapid growing literature size, is challenging IR community. In this paper, we are focused on addressing the synonym and polysemy issue within the language model framework. Unlike the ways translation model and traditional query expansion techniques approach this issue, we incorporate concept-based indexing into a basic language model for genomic IR. In particular, we adopt UMLS concepts as indexing and searching terms. A UMLS concept stands for a unique meaning in the biomedicine domain; a set of synonymous terms will share same concept ID. Therefore, the new approach makes the document ranking effective while maintaining the simplicity of language models. A comparative experiment on the TREC 2004 Genomics Track data shows significant improvements are obtained by incorporating concept-based indexing into a basic language model. The MAP (mean average precision) is significantly raised from 29.17% (the baseline system) to 36.94%. The performance of the new approach is also significantly superior to the mean (21.72%) of official runs participated in TREC 2004 Genomics Track and is comparable to the performance of the best run (40.75%). Most official runs including the best run extensively use various query expansion and pseudo-relevance feedback techniques while our approach does nothing except for the incorporation of concept-based indexing, which evidences the view that semantic smoothing, i.e. the incorporation of synonym and sense information into the language models, is a more standard approach to achieving the effects traditional query expansion and pseudo-relevance feedback techniques target.

#index 1742107
#* The effects on topic familiarity on online search behaviour and use of relevance criteria
#@ Lei Wen;Ian Ruthven;Pia Borlund
#t 2006
#c 16
#% 167617
#% 187999
#% 260244
#% 397172
#% 807648
#! This paper presents an experimental study on the effect of topic familiarity on the assessment behaviour of online searchers. In particular we investigate the effect of topic familiarity on the resources and relevance criteria used by searchers. Our results indicate that searching on an unfamiliar topic leads to use of more generic and fewer specialised resources and that searchers employ different relevance criteria when searching on less familiar topics.

#index 1742108
#* PERC: a personal email classifier
#@ Shih-Wen Ke;Chris Bowerman;Michael Oakes
#t 2006
#c 16
#% 262050
#% 340904
#% 478128
#% 581658
#% 642998
#! Improving the accuracy of assigning new email messages to small folders can reduce the likelihood of users creating duplicate folders for some topics. In this paper we presented a hybrid classification model, PERC, and use the Enron Email Corpus to investigate the performance of kNN, SVM and PERC in a simulation of a real-time situation. Our results show that PERC is significantly better at assigning messages to small folders. The effects of different parameter settings for the classifiers are discussed.

#index 1742109
#* Influence diagrams for contextual information retrieval
#@ Lynda Tamine-Lechani;Mohand Boughanem
#t 2006
#c 16
#% 292151
#% 351595
#% 729625
#% 766413
#% 1656345
#! The purpose of contextual information retrieval is to make some exploration towards designing user specific search engines that are able to adapt the retrieval model to the variety of differences on user's contexts. In this paper we propose an influence diagram based retrieval model which is able to incorporate contexts, viewed as user's long-term interests into the retrieval process.

#index 1742110
#* Morphological variation of arabic queries
#@ Asaad Alberair;Mark Sanderson
#t 2006
#c 16
#% 144034
#% 208934
#! Although it has been shown that in test collection based studies, stemming improves retrieval effectiveness in an information retrieval system, morphological variations of queries searching on the same topic are less well understood. This work examines the broad morphological variation that searchers of an Arabic retrieval system put into their queries. In this study, 15 native Arabic speakers were asked to generate queries, morphological variants of query words were collated across users. Queries composed of either the commonest or rarest variants of each word were submitted to a retrieval system and the effectiveness of the searches was measured. It was found that queries composed of the more popular morphological variants were more likely to retrieve relevant documents that those composed of less popular.

#index 1742111
#* Combining short and long term audio features for TV sports highlight detection
#@ Bin Zhang;Weibei Dou;Liming Chen
#t 2006
#c 16
#% 137711
#% 316187
#% 729437
#! As bearer of high-level semantics, audio signal is being more and more used in content-based multimedia retrieval. In this paper, we investigate TV tennis game highlight detection based on the use of both short and long term audio features and propose two approaches, decision fusion and hierarchical classifier, in order to combine these two kinds of audio features. As more information is included in decision making, the overall performance of the system is enhanced.

#index 1742112
#* Object-Based access to TV rushes video
#@ Alan F. Smeaton;Gareth J. F. Jones;Hyowon Lee;Noel E. O'Connor;Sorin Sav
#t 2006
#c 16
#% 318785
#% 724320
#% 737425
#% 741404
#% 881952
#! Recent years have seen the development of different modalities for video retrieval. The most common of these are (1) to use text from speech recognition or closed captions, (2) to match keyframes using image retrieval techniques like colour and texture [6] and (3) to use semantic features like “indoor”, “outdoor” or “persons”. Of these, text-based retrieval is the most mature and useful, while image-based retrieval using low-level image features usually depends on matching keyframes rather than whole-shots. Automatic detection of video concepts is receiving much attention and as progress is made in this area we will see consequent impact on the quality of video retrieval. In practice it is the combination of these techniques which realises the most useful, and effective, video retrieval as shown by us repeatedly in TRECVid [5].

#index 1742113
#* An efficient computation of the multiple-bernoulli language model
#@ Leif Azzopardi;David E. Losada
#t 2006
#c 16
#% 262096
#% 750863
#% 766503
#! The Multiple Bernoulli (MB) Language Model has been generally considered too computationally expensive for practical purposes and superseded by the more efficient multinomial approach. While, the model has many attractive properties, little is actually known about the retrieval effectiveness of the MB model due to its high cost of execution. In this paper, we show how an efficient implementation of this model can be achieved. The resulting method is comparable in terms of efficiency to other standard term matching algorithms (such as the vector space model, BM25 and the multinomial Language Model).

#index 1742114
#* Title and snippet based result re-ranking in collaborative web search
#@ Oisín Boydell;Barry Smyth
#t 2006
#c 16
#% 375017
#% 803556
#% 807295
#% 818221
#% 818259
#% 838547
#% 1289575
#! Collaborative Web search is a form of meta-search that manipulates the results of underlying Web search engines in response to the learned preferences of a given community of users. Results that have previously been selected in response to similar queries by community members are promoted in the returned results. However, promotion is limited to these previously-selected results and in this paper we describe and evaluate how relevant results without a selection history can also be promoted by exploiting snippet-text and title similarities.

#index 1742115
#* A classification of IR effectiveness metrics
#@ Gianluca Demartini;Stefano Mizzaro
#t 2006
#c 16
#% 100008
#% 157903
#% 187763
#% 234793
#% 235918
#% 262107
#% 375017
#% 406493
#% 411762
#% 757548
#% 766409
#! Effectiveness is a primary concern in the information retrieval (IR) field. Various metrics for IR effectiveness have been proposed in the past; we take into account all the 44 metrics we are aware of, classifying them into a two-dimensional grid. The classification is based on the notions of relevance, i.e., if (or how much) a document is relevant, and retrieval, i.e., if (how much) a document is retrieved. To our knowledge, no similar classification has been proposed so far.

#index 1742116
#* Experiments on average distance measure
#@ Vincenzo Della Mea;Gianluca Demartini;Luca Di Gaspero;Stefano Mizzaro
#t 2006
#c 16
#% 397164
#% 757548
#! ADM (Average Distance Measure) is an IR effectiveness metric based on the assumptions of continuous relevance and retrieval. This paper presents some novel experimental results on two different test collections: TREC 8, re-assessed on 4-levels relevance judgments, and TREC 13 TeraByte collection. The results confirm that ADM correlation with standard measures is high, even when using less data, i.e., few documents.

#index 1742117
#* Phrase clustering without document context
#@ Eric SanJuan;Fidelia Ibekwe-SanJuan
#t 2006
#c 16
#% 397148
#% 1294847
#! We applied different clustering algorithms to the task of clustering multi-word terms in order to reflect a humanly built ontology. Clustering was done without the usual document co-occurrence information. Our clustering algorithm, CPCL (Classification by Preferential Clustered Link) is based on general lexico-syntactic relations which do not require prior domain knowledge or the existence of a training set. Results show that CPCL performs well in terms of cluster homogeneity and shows good adaptability for handling large and sparse matrices.

#index 1742118
#* Rapid development of web-based monolingual question answering systems
#@ Edward W. D. Whittaker;Julien Hamonic;Dong Yang;Tor Klingberg;Sadaoki Furui
#t 2006
#c 16
#% 849047
#! In this paper we describe the application of our statistical pattern classification approach to question answering (QA) to the rapid development of monolingual QA systems. We show how the approach has been applied successfully to QA in English, Japanese, Chinese, Russian and Swedish to form the basis of our publicly accessible web-based multilingual QA system at http://asked.jp.

#index 1742119
#* Filtering obfuscated email spam by means of phonetic string matching
#@ Valerio Freschi;Andrea Seraghiti;Alessandro Bogliolo
#t 2006
#c 16
#% 104482
#% 235941
#% 324015
#! Rule-based email filters mainly rely on the occurrence of critical words to classify spam messages. However, perceptive obfuscation techniques can be used to elude exact pattern matching. In this paper we propose a new technique for filtering obfuscated email spam that performs approximate pattern matching both on the original message and on its phonetic transcription.

#index 1742120
#* Sprinkling: supervised latent semantic indexing
#@ Sutanu Chakraborti;Robert Lothian;Nirmalie Wiratunga;Stuart Watt
#t 2006
#c 16
#% 269217
#% 342670
#% 376266
#% 453325
#% 728295
#! Latent Semantic Indexing (LSI) is an established dimensionality reduction technique for Information Retrieval applications. However, LSI generated dimensions are not optimal in a classification setting, since LSI fails to exploit class labels of training documents. We propose an approach that uses class information to influence LSI dimensions whereby class labels of training documents are endoded as new terms, which are appended to the documents. When LSI is carried out on the augmented term-document matrix, terms pertaining to the same class are pulled closer to each other. Evaluation over experimental data reveals significant improvement in classification accuracy over LSI. The results also compare favourably with naive Support Vector Machines.

#index 1742121
#* Web-Based multiple choice question answering for english and arabic questions
#@ Rawia Awadallah;Andreas Rauber
#t 2006
#c 16
#% 340953
#% 342674
#% 397158
#% 853908
#% 1271242
#% 1673018
#! Answering multiple-choice questions, where a set of possible answers is provided together with the question, constitutes a simplified but nevertheless challenging area in question answering research. This paper introduces and evaluates two novel techniques for answer selection. It furthermore analyses in how far performance figures obtained using the English language Web as data source can be transferred to less dominant languages on the Web, such as Arabic. Result evaluation is based on questions from both the English and the Arabic versions of the TV show “Who wants to be a Millionaire?” as well as on the TREC-2002 QA data.

#index 1742122
#* Authoritative re-ranking of search results
#@ Toine Bogers;Antal van den Bosch
#t 2006
#c 16
#% 218978
#% 249143
#% 329698
#% 730082
#! We examine the use of authorship information in information retrieval for closed communities by extracting expert rankings for queries. We demonstrate that these rankings can be used to re-rank baseline search results and improve performance significantly. We also perform experiments in which we base expertise ratings only on first authors or on all except the final authors, and find that these limitations do not further improve our re-ranking method.

#index 1742123
#* Readability applied to information retrieval
#@ Lorna Kane;Joe Carthy;John Dunnion
#t 2006
#c 16
#% 7553
#% 136350
#% 260244
#% 720198
#% 780030
#% 861988
#! Readability refers to all characteristics of a document that contribute to its ‘ease of understanding or comprehension due to the style of writing' [1]. The readability of a text is dependent on a number of factors, including but not constrained to; its legibility, syntactic difficulty, semantic difficulty and the organization of the text [2]. As many as 228 variables were found to influence the readability of a text in Gray and Leary's seminal study [2]. These variables were classified as relating to document content, style, format or, features of organization.

#index 1742124
#* Automatic determination of feature weights for multi-feature CBIR
#@ Peter Wilkins;Paul Ferguson;Cathal Gurrin;Alan F. Smeaton
#t 2006
#c 16
#% 645687
#! Image and video retrieval are both currently dominated by approaches which combine the outputs of several different representations or features. The ways in which the combination can be done is an established research problem in content-based image retrieval (CBIR). These approaches vary from image clustering through to semantic frameworks and mid-level visual features to ultimately determine sets of relative weights for the non-linear combination of features. Simple approaches to determining these weights revolve around executing a standard set of queries with known relevance judgements on some form of training data and is iterative in nature. Whilst successful, this requires both training data and human intervention to derive the optimal weights.

#index 1742125
#* Towards automatic retrieval of album covers
#@ Markus Schedl;Peter Knees;Tim Pohle;Gerhard Widmer
#t 2006
#c 16
#! We present first steps towards intelligent retrieval of music album covers from the web. The continuous growth of electronic music distribution constantly increases the interest in methods to automatically provide added value like lyrics or album covers. While existing approaches rely on large proprietary databases, we focus on methods that make use of the whole web by using Google's or A9.com's image search. We evaluate the current state of the approach and point out directions for further improvements.

#index 1742126
#* Clustering sentences for discovering events in news articles
#@ Martina Naughton;Nicholas Kushmerick;Joe Carthy
#t 2006
#c 16
#% 279755
#% 818215
#! We investigate the use of clustering methods for the task of grouping the text spans in a news article that refer to the same event. We provide evidence that the order in which events are described is structured in a way that can be exploited during clustering. We evaluate our approach on a corpus of news articles describing events that have occurred in the Iraqi War.

#index 1742127
#* Specificity helps text classification
#@ Lucas Bouma;Maarten de Rijke
#t 2006
#c 16
#% 290482
#% 344447
#! We examine the impact on classification effectiveness of semantic differences in categories. Specifically, we measure broadness and narrowness of categories in terms of their distance to the root of a hierarchically organized thesaurus. Using categories of four different levels degrees of broadness, we show that classifying documents into narrow categories gives better scores than classifying them into broad terms, which we attribute to the fact that more specific categories are associated with terms with a higher discriminatory power.

#index 1742128
#* A declarative DB-Powered approach to IR
#@ Roberto Cornacchia;Arjen P. de Vries
#t 2006
#c 16
#% 137862
#% 163444
#% 507656
#% 864446
#% 1348341
#! We present a prototype system using array comprehensions to bridge the gap between databases and information retrieval. It allows researchers to express their retrieval models in the General Matrix Framework for Information Retrieval [1], and have these executed on relational database systems with negligible effort.

#index 1742129
#* Judging the spatial relevance of documents for GIR
#@ Paul D. Clough;Hideo Joho;Ross Purves
#t 2006
#c 16
#% 330677
#% 836102
#! Geographic Information Retrieval (GIR) is concerned with the retrieval of documents based on both thematic and geographic content. An important issue in GIR, as for all IR, is relevance. In this paper we argue that spatial relevance should be considered independently from thematic relevance, and propose an initial scheme. A pilot study to assess this relevance scheme is presented, with initial results suggesting that users can distinguish between these two relevance dimensions, and that furthermore they have different properties. We suggest that spatial relevance requires greater assessor effort and more localised geographic knowledge than judging thematic relevance.

#index 1742130
#* Probabilistic score normalization for rank aggregation
#@ Miriam Fernández;David Vallet;Pablo Castells
#t 2006
#c 16
#% 232703
#% 340934
#% 342710
#% 728360
#% 733578
#% 1671771
#! Rank aggregation is a pervading operation in IR technology. We hypothesize that the performance of score-based aggregation may be affected by artificial, usually meaningless deviations consistently occurring in the input score distributions, which distort the combined result when the individual biases differ from each other. We propose a score-based rank aggregation model where the source scores are normalized to a common distribution before being combined. Early experiments on available data from several TREC collections are shown to support our proposal.

#index 1742131
#* Learning links between a user's calendar and information needs
#@ Elena Vildjiounaite;Vesa Kyllönen
#t 2006
#c 16
#% 733578
#% 801785
#! Personal information needs depend on long-term interests and on current and future situations (contexts): people are mainly interested in weather forecasts for future destinations, and in toy advertisements when a child's birthday approaches. As computer capabilities for being aware of users' contexts grow, the users' willingness to set manually rules for context-based information retrieval will decrease. Thus computers must learn to associate user contexts with information needs in order to collect and present information proactively. This work presents experiments with training a SVM (Support Vector Machines) classifier to learn user information needs from calendar information.

#index 1742132
#* Supporting relevance feedback in video search
#@ Cathal Gurrin;Dag Johansen;Alan F. Smeaton
#t 2006
#c 16
#% 780870
#! WWW Video Search Engines have become increasingly commonplace within the last few years and at the same time video retrieval research has been receiving more attention with the annual TRECVid series of workshops. In this paper we evaluate methods of relevance feedback for video search engines operating over TV news data. We show for both video shots and TV news stories, that an optimal number of terms can be identified to compose a new query for feedback and that in most cases; the number of documents employed for feedback does not have a great effect on these optimal numbers of terms.

#index 1742133
#* Intrinsic plagiarism detection
#@ Sven Meyer zu Eissen;Benno Stein
#t 2006
#c 16
#% 201935
#% 571725
#% 770870
#! Current research in the field of automatic plagiarism detection for text documents focuses on algorithms that compare plagiarized documents against potential original documents. Though these approaches perform well in identifying copied or even modified passages, they assume a closed world: a reference collection must be given against which a plagiarized document can be compared. This raises the question whether plagiarized passages within a document can be detected automatically if no reference is given, e. g. if the plagiarized passages stem from a book that is not available in digital form. We call this problem class intrinsic plagiarism detection. The paper is devoted to this problem class; it shows that it is possible to identify potentially plagiarized passages by analyzing a single document with respect to variations in writing style. Our contributions are fourfold: (i) a taxonomy of plagiarism delicts along with detection methods, (ii) new features for the quantification of style aspects, (iii) a publicly available plagiarism corpus for benchmark comparisons, and (iv) promising results in non-trivial plagiarism detection settings: in our experiments we achieved recall values of 85% with a precision of 75% and better.

#index 1742134
#* Investigating biometric response for information retrieval applications
#@ Colum Mooney;Micheál Scully;Gareth J. F. Jones;Alan F. Smeaton
#t 2006
#c 16
#% 839919
#! Current information retrieval systems make no measurement of the user's response to the searching process or the information itself. Existing psychological studies show that subjects exhibit measurable physiological responses when carrying out certain tasks, e.g. when viewing images, which generally result in heightened emotional states. We find that users exhibit measurable biometric behaviour in the form of galvanic skin response when watching movies, and engaging in interactive tasks. We examine how this data might be exploited in the indexing of data for search and within the search process itself.

#index 1742135
#* Relevance feedback using weight propagation
#@ Fadi Yamout;Michael Oakes;John Tait
#t 2006
#c 16
#% 223810
#% 387427
#% 411406
#% 549441
#% 783482
#! A new Relevance Feedback (RF) technique is developed to improve upon the efficiency and performance of existing techniques. This is based on propagating positive and negative weights from documents judged relevant and not relevant respectively, to other documents, which are deemed similar according to one of a number of criteria. The performance and efficiency improve since the documents are treated as independent vectors rather than being merged into a single vector as is the case with traditional approaches, and only the documents considered in a given neighbourhood are inspected. This is especially important when using large test collections.

#index 1742136
#* Context-Specific frequencies and discriminativeness for the retrieval of structured documents
#@ Jun Wang;Thomas Roelleke
#t 2006
#c 16
#% 169809
#% 194246
#% 340914
#% 345712
#% 458404
#! Structured document retrieval requires the ranking of document elements. Previous approaches either aggregate term weights or retrieval status values, or propose alternatives to idf, for example, ief (inverse element frequency). We propose and investigate in this paper a new approach: Context-specific idf, which is, in contrast to aggregation-based ranking functions, parameter-free.

#index 1805990
#* Proceedings of the 34th European conference on Advances in Information Retrieval
#@ Ricardo Baeza-Yates;Arjen P. Vries;Hugo Zaragoza;B. Barla Cambazoglu;Vanessa Murdock
#t 2012
#c 16

#index 1805991
#* Explaining query modifications: an alternative interpretation of term addition and removal
#@ Vera Hollink;Jiyin He;Arjen de Vries
#t 2012
#c 16
#% 397127
#% 449294
#% 643057
#% 741411
#% 825824
#% 987304
#% 995516
#% 1021477
#% 1111094
#% 1224723
#% 1292473
#% 1314978
#% 1348333
#% 1415785
#% 1432244
#% 1450885
#% 1492785
#% 1563613
#% 1587854
#% 1697478
#! In the course of a search session, searchers often modify their queries several times. In most previous work analyzing search logs, the addition of terms to a query is identified with query specification and the removal of terms with query generalization. By analyzing the result sets that motivated searchers to make modifications, we show that this interpretation is not always correct. In fact, our experiments indicate that in the majority of cases the modifications have the opposite functions. Terms are often removed to get rid of irrelevant results matching only part of the query and thus to make the result set more specific. Similarly, terms are often added to retrieve more diverse results. We propose an alternative interpretation of term additions and removals and show that it explains the deviant modification behavior that was observed.

#index 1805992
#* Modeling transactional queries via templates
#@ Edward Bortnikov;Pinar Donmez;Amit Kagian;Ronny Lempel
#t 2012
#c 16
#% 198058
#% 408396
#% 590523
#% 609433
#% 805878
#% 879634
#% 963669
#% 1043040
#% 1355017
#% 1376118
#% 1399933
#% 1560359
#% 1676560
#! Search queries have been roughly classified into three categories --- navigational, informational and transactional. The latter group includes queries that aim to perform some Web-mediated task, often by interacting with parameterized Web services. In order to assist users in completing tasks online, one of the first building blocks is identifying whether and which transactional use-case is associated with each query. This paper describes a framework and an algorithm for automatically generating compact representations of queries associated with transactional use cases. We mine search click logs for queries that lead to clicks on pages associated with a use-case, generalize the set of mined queries into templates by replacing query terms with taxonomy categories, and eliminate redundancies. This approach allows associating the use-case with queries unseen in the log sample, while keeping a concise model. Our methodology allows a business owner to select an appropriate operating point that balances the tradeoff between precision and recall. We report the results of an offline evaluation of our framework on three transactional domains, and demonstrate the viability of the approach.

#index 1805993
#* Exploring query patterns in email search
#@ Morgan Harvey;David Elsweiler
#t 2012
#c 16
#% 214751
#% 339375
#% 642983
#% 987195
#% 987211
#% 1047435
#% 1348355
#% 1355035
#% 1392483
#% 1587390
#% 1598336
#% 1598337
#! Despite Email being the most popular communication medium currently in use and that people have been shown to regularly re-use messages, very little is known about how people actually search within email clients. In this paper we present a detailed analysis of email search behaviour obtained from a study of 47 users. We uncover a number of behavioral patterns that contrast with those previously observed in web search. From our findings, we describe ways in which email search could be improved and conclude with a short discussion of possible future work.

#index 1805994
#* Interactive search support for difficult web queries
#@ Abdigani Diriye;Giridhar Kumaran;Jeff Huang
#t 2012
#c 16
#% 214709
#% 329090
#% 642985
#% 643001
#% 924476
#% 987212
#% 1074052
#% 1074112
#% 1173692
#% 1227623
#% 1227647
#% 1292473
#% 1450900
#% 1450964
#% 1456294
#! Short and common web queries are aptly supported by state-of-the-art search engines but performance and user experience are degraded when web queries are longer and less common. Extending previous solutions that automatically shorten queries, we introduce searchAssist: a novel search interface that provides interactive support for difficult web queries. The query logs and questionnaires from a naturalistic study of 90 web users' search behaviors show that the usage rate of searchAssist for difficult queries was almost 40%. The results also highlight the importance of term dropping for long queries, and the improvements obtained in topical relevance when our searchers used searchAssist.

#index 1805995
#* Predicting the future impact of news events
#@ Julien Gaugaz;Patrick Siehndel;Gianluca Demartini;Tereza Iofciu;Mihai Georgescu;Nicola Henze
#t 2012
#c 16
#% 235918
#% 818215
#% 939376
#% 987219
#% 1115311
#% 1131144
#% 1155643
#% 1166523
#% 1190132
#% 1292698
#% 1451233
#% 1475758
#% 1482459
#% 1860710
#! The amount of news content on the Web is increasing, enabling users to access news articles coming from a variety of sources: from newswires, news agencies, blogs, and at various places, e.g. even within Web search engines result pages. Anyhow, it still is a challenge for current search engines to decide which news events are worth being shown to the user (either for a newsworthy query or in a news portal). In this paper we define the task of predicting the future impact of news events. Being able to predict event impact will, for example, enable a newspaper to decide whether to follow a specific event or not, or a news search engine which stories to display. We define a flexible framework that, given some definition of impact, can predict its future development at the beginning of the event. We evaluate several possible definitions of event impact and experimentally identify the best features for each of them.

#index 1805996
#* Detection of news feeds items appropriate for children
#@ Tamara Polajnar;Richard Glassey;Leif Azzopardi
#t 2012
#c 16
#% 269217
#% 458369
#% 458379
#% 939396
#% 1093792
#% 1251414
#% 1384136
#% 1450937
#% 1455283
#% 1536555
#! Identifying child-appropriate web content is an important yet difficult classification task. This novel task is characterised by attempting to determine age/child appropriateness (which is not necessarily topic-based), despite the presence of unbalanced class sizes and the lack of quality training data with human judgements of appropriateness. Classification of feeds, a subset of web content, presents further challenges due to their temporal nature and short document format. In this paper, we discuss these challenges and present baseline results for this task through an empirical study that classifies incoming news stories as appropriate (or not) for children. We show that while the naïve Bayes approach produces a higher AUC it is vulnerable to the imbalanced data problem, and that support vector machine provides a more robust overall solution. Our research shows that classifying children's content is a non-trivial task that has greater complexities than standard text based classification. While the F-score values are consistent with other research examining age-appropriate text classification, we introduce a new problem with a new dataset.

#index 1805997
#* Comparing tweets and tags for URLs
#@ Morgan Harvey;Mark Carman;David Elsweiler
#t 2012
#c 16
#% 161722
#% 279755
#% 855601
#% 956515
#% 956544
#% 1035588
#% 1040837
#% 1227592
#% 1384287
#% 1429423
#% 1536553
#% 1794765
#! The free-form tags available from social bookmarking sites such as Delicious have been shown to be useful for a number of purposes and could serve as a cheap source of metadata about URLs on the web. Unfortunately recent years have seen a reduction in the popularity of such sites, however at the same time microblogging sites such as Twitter have exploded in popularity. On these sites users submit short messages (or "tweets") about what they are currently reading, thinking and doing and often post URLs. In this work we look into the similarity between top tags drawn from Delicious and high-frequency terms from tweets to ascertain whether Twitter data could serve as a useful replacement for Delicious. We investigate how these terms compare with web page content, whether or not top Twitter terms converge and determine if the terms are mostly descriptive (and therefore useful) or if they are mostly expressing sentiment or emotion. We discover that provided a large number of tweets are available referring to a chosen URL then the top terms drawn from these tweets are similar to Delicious tags and could therefore be used for similar purposes.

#index 1805998
#* Geo-Location estimation of flickr images: social web based enrichment
#@ Claudia Hauff;Geert-Jan Houben
#t 2012
#c 16
#% 340948
#% 1040837
#% 1194140
#% 1227637
#% 1278585
#% 1355297
#% 1450997
#% 1482254
#% 1528636
#% 1583918
#% 1583922
#% 1611804
#% 1650298
#% 1697481
#% 1715211
#! Estimating the geographic location of images is a task which has received a lot of attention in recent years. Large numbers of items uploaded to Flickr do not contain GPS-based latitude/longitude coordinates, although it would be beneficial to obtain such geographic information for a wide variety of potential applications such as travelogues and visual place descriptions. While most works in this area consider an image's textual meta-data to estimate its geo-location, we consider an additional textual dimension: the image owner's traces on the social Web, in particular on the micro-blogging platform Twitter. We investigate the following question: does enriching an image's available textual meta-data with a user's tweets improve the accuracy of the geographic location estimation process? The results show that this is indeed the case; in an oracle setting, the median error in kilometres decreases by 87%, in the best automatic approach the median error decreases by 56%.

#index 1805999
#* A field relevance model for structured document retrieval
#@ Jin Young Kim;W. Bruce Croft
#t 2012
#c 16
#% 262096
#% 340901
#% 642992
#% 783474
#% 827581
#% 976952
#% 987339
#% 1195848
#% 1195889
#% 1227648
#% 1292597
#% 1355019
#! Many search applications involve documents with structure or fields. Since query terms often are related to specific structural components, mapping queries to fields and assigning weights to those fields is critical for retrieval effectiveness. Although several field-based retrieval models have been developed, there has not been a formal justification of field weighting. In this work, we aim to improve the field weighting for structured document retrieval. We first introduce the notion of field relevance as the generalization of field weights, and discuss how it can be estimated using relevant documents, which effectively implements relevance feedback for field weighting. We then propose a framework for estimating field relevance based on the combination of several sources. Evaluation on several structured document collections show that field weighting based on the suggested framework improves retrieval effectiveness significantly.

#index 1806000
#* Relation based term weighting regularization
#@ Hao Wu;Hui Fang
#t 2012
#c 16
#% 67565
#% 109190
#% 120104
#% 218982
#% 262037
#% 262096
#% 340948
#% 375017
#% 766412
#% 766440
#% 818262
#% 818263
#% 879579
#% 987229
#% 1697442
#! Traditional retrieval models compute term weights based on only the information related to individual terms such as TF and IDF. However, query terms are related. Intuitively, these relations could provide useful information about the importance of a term in the context of other query terms. For example, query "perl tutorial" specifies that a user look for information relevant to both perl and tutorial. Thus, a document containing both terms should have higher relevance score than the ones with only one of them. However, if the IDF value of "tutorial" is much smaller than "perl", existing retrieval models may assign the document lower score than those containing multiple occurrences of "perl". It is clear that the importance of a term should be dependent on not only collection statistics but also the relations with other query terms. In this work, we study how to utilize semantic relations among query terms to regularize term weighting. Experiment results over TREC collections show that the proposed strategy is effective to improve the retrieval performance.

#index 1806001
#* A new approach to answerer recommendation in community question answering services
#@ Zhenlei Yan;Jie Zhou
#t 2012
#c 16
#% 722904
#% 734590
#% 750863
#% 838464
#% 1055738
#% 1130900
#% 1190249
#% 1207005
#% 1214694
#% 1227721
#% 1399976
#% 1482384
#% 1491834
#! Community Question Answering (CQA) service which enables users to ask and answer questions have emerged popular on the web. However, lots of questions usually can't be resolved by appropriate answerers effectively. To address this problem, we present a novel approach to recommend users who are most likely to be able to answer the new question. Differently with previous methods, this approach utilizes the inherent semantic relations among asker-question-answerer simultaneously and perform the Answerer Recommendation task based on tensor factorization. Experimental results on two real-world CQA dataset show that the proposed method is able to recommend appropriate answerers for new questions and outperforms other state-of-the-art approaches.

#index 1806002
#* On the modeling of entities for ad-hoc entity search in the web of data
#@ Robert Neumayer;Krisztian Balog;Kjetil Nørvåg
#t 2012
#c 16
#% 642992
#% 783474
#% 1100822
#% 1133171
#% 1166534
#% 1195848
#% 1263220
#% 1263244
#% 1292565
#% 1400010
#% 1475756
#% 1483553
#% 1641483
#% 1719970
#% 1721867
#! The Web of Data describes objects, entities, or "things" in terms of their attributes and their relationships, using RDF statements. There is a need to make this wealth of knowledge easily accessible by means of keyword search. Despite recent research efforts in this direction, there is a lack of understanding of how structured semantic data is best represented for text-based entity retrieval. The task we are addressing in this paper is ad-hoc entity search: the retrieval of RDF resources that represent an entity described in the keyword query. We build upon and formalise existing entity modeling approaches within a generative language modeling framework, and compare them experimentally using a standard test collection, provided by the Semantic Search Challenge evaluation series. We show that these models outperform the current state-of-the-art in terms of retrieval effectiveness, however, this is done at the cost of abandoning a large part of the semantics behind the data. We propose a novel entity model capable of preserving the semantics associated with entities, without sacrificing retrieval effectiveness.

#index 1806003
#* Result disambiguation in web people search
#@ Richard Berendsen;Bogomil Kovachev;Evangelia-Paraskevi Nastou;Maarten de Rijke;Wouter Weerkamp
#t 2012
#c 16
#% 832266
#% 874258
#% 914328
#% 939515
#% 958200
#% 1019124
#% 1034802
#% 1105828
#% 1202162
#% 1213625
#% 1227667
#% 1271267
#% 1450830
#% 1598338
#% 1642012
#% 1765813
#% 1771970
#! We study the problem of disambiguating the results of a web people search engine: given a query consisting of a person name plus the result pages for this query, find correct referents for all mentions by clustering the pages according to the different people sharing the name. While the problem has been studied extensively, we discover that the increasing availability of results retrieved from social media platforms causes state-of-the-art methods to break down. We analyze the problem and propose a dual strategy where we distinguish between results obtained from social media platforms and those obtained from other sources. In our dual strategy, the two types of documents are disambiguated separately, using different strategies, and their results are then merged. We study several instantiations for the different stages in our proposed strategy and manage to achieve state-of-the-art performance.

#index 1806004
#* On smoothing average precision
#@ Stephen Robertson
#t 2012
#c 16
#% 309093
#% 318407
#% 879631
#% 907493
#% 907496
#% 1074139
#% 1442574
#% 1622344
#! On the basis of a theoretical analysis of issues around populations and sampling, for both topics and documents, and parameters with which we hope to characterise the effectiveness of different systems, we propose a modification to the traditional average precision metric. This modification involves both transformation and (in the estimation of the parameter) smoothing. The modified version is shown to have certain distributional advantages, on a substantial dataset. In particular, the distribution of values of the modified metric, over topics for a given system/run, is approximately normal.

#index 1806005
#* New metrics for meaningful evaluation of informally structured speech retrieval
#@ Maria Eskevich;Walid Magdy;Gareth J. F. Jones
#t 2012
#c 16
#% 575729
#% 741058
#% 742204
#% 879677
#% 1100802
#% 1106238
#% 1480887
#! Search effectiveness for tasks where the retrieval units are clearly defined documents is generally evaluated using standard measures such as mean average precision (MAP). However, many practical speech search tasks focus on content within large spoken files lacking defined structure. These data must be segmented into smaller units for search which may only partially overlap with relevant material. We introduce two new metrics for the evaluation of search effectiveness for informally structured speech data: mean average segment precision (MASP) which measures retrieval performance in terms of both content segmentation and ranking with respect to relevance; and mean average segment distance-weighted precision (MASDWP) which takes into account the distance between the start of the relevant segment and the retrieved segment. We demonstrate the effectiveness of these new metrics on a retrieval test collection based on the AMI meeting corpus.

#index 1806006
#* On aggregating labels from multiple crowd workers to infer relevance of documents
#@ Mehdi Hosseini;Ingemar J. Cox;Nataša Milić-Frayling;Gabriella Kazai;Vishwa Vinay
#t 2012
#c 16
#% 309093
#% 312689
#% 838536
#% 879632
#% 1074132
#% 1074134
#% 1252624
#% 1264744
#% 1450896
#% 1452857
#% 1536551
#% 1598354
#% 1598440
#% 1598516
#! We consider the problem of acquiring relevance judgements for information retrieval (IR) test collections through crowdsourcing when no true relevance labels are available. We collect multiple, possibly noisy relevance labels per document from workers of unknown labelling accuracy. We use these labels to infer the document relevance based on two methods. The first method is the commonly used majority voting (MV) which determines the document relevance based on the label that received the most votes, treating all the workers equally. The second is a probabilistic model that concurrently estimates the document relevance and the workers accuracy using expectation maximization (EM). We run simulations and conduct experiments with crowdsourced relevance labels from the INEX 2010 Book Search track to investigate the accuracy and robustness of the relevance assessments to the noisy labels. We observe the effect of the derived relevance judgments on the ranking of the search systems. Our experimental results show that the EM method outperforms the MV method in the accuracy of relevance assessments and IR systems ranking. The performance improvements are especially noticeable when the number of labels per document is small and the labels are of varied quality.

#index 1806007
#* How random walks can help tourism
#@ Claudio Lucchese;Raffaele Perego;Fabrizio Silvestri;Hossein Vahabi;Rossano Venturini
#t 2012
#c 16
#% 349208
#% 881496
#% 987205
#% 1127466
#% 1190131
#% 1214661
#% 1227601
#% 1429406
#% 1450997
#% 1482236
#% 1484412
#% 1536568
#! On-line photo sharing services allow users to share their touristic experiences. Tourists can publish photos of interesting locations or monuments visited, and they can also share comments, annotations, and even the GPS traces of their visits. By analyzing such data, it is possible to turn colorful photos into metadata-rich trajectories through the points of interest present in a city. In this paper we propose a novel algorithm for the interactive generation of personalized recommendations of touristic places of interest based on the knowledge mined from photo albums and Wikipedia. The distinguishing features of our approach are multiple. First, the underlying recommendation model is built fully automatically in an unsupervised way and it can be easily extended with heterogeneous sources of information. Moreover, recommendations are personalized according to the places previously visited by the user. Finally, such personalized recommendations can be generated very efficiently even on-line from a mobile device.

#index 1806008
#* Retrieving candidate plagiarised documents using query expansion
#@ Rao Muhammad Adeel Nawab;Mark Stevenson;Paul Clough
#t 2012
#c 16
#% 866659
#% 1109275
#% 1192974
#% 1261349
#% 1264738
#% 1544119
#% 1558416
#% 1715628
#! External plagiarism detection systems compare suspicious texts against a reference collection to identify the original one(s). The suspicious text may not contain a verbatim copy of the reference collection since plagiarists often try to disguise their behaviour by altering the text. For large reference collections, such as those accessible via the internet, it is not practical to compare the suspicious text with every document in the reference collection. Consequently many approaches to plagiarism detection begin by identifying a set of candidate documents from the reference collection. We report an IR-based approach to the candidate document selection problem that uses query expansion to identify candidates which have been altered. The reported system outperforms a previously reported approach and is also robust to changes in the reference collection text.

#index 1806009
#* Reliability prediction of webpages in the medical domain
#@ Parikshit Sondhi;V. G. Vinod Vydiswaran;Cheng Xiang Zhai
#t 2012
#c 16
#% 190581
#% 268079
#% 269217
#% 458379
#% 799636
#% 818916
#% 862154
#% 877122
#% 897598
#% 1022742
#% 1099345
#% 1125910
#% 1606037
#! In this paper, we study how to automatically predict reliability of web pages in the medical domain. Assessing reliability of online medical information is especially critical as it may potentially influence vulnerable patients seeking help online. Unfortunately, there are no automated systems currently available that can classify a medical webpage as being reliable, while manual assessment cannot scale up to process the large number of medical pages on the Web. We propose a supervised learning approach to automatically predict reliability of medical webpages. We developed a gold standard dataset using the standard reliability criteria defined by the Health on Net Foundation and systematically experimented with different link and content based feature sets. Our experiments show promising results with prediction accuracies of over 80%. We also show that our proposed prediction method is useful in applications such as reliability-based re-ranking and automatic website accreditation.

#index 1806010
#* Automatic foldering of email messages: a combination approach
#@ Tony Tam;Artur Ferreira;André Lourenço
#t 2012
#c 16
#% 115608
#% 214751
#% 321635
#% 402289
#% 466434
#% 466912
#% 722929
#% 796212
#% 891559
#% 929722
#% 1077150
#% 1451162
#% 1552869
#% 1558464
#! Automatic organization of email messages into folders is both an open problem and challenge for machine learning techniques. Besides the effect of email overload, which affects many email users worldwide, there are some increasing difficulties caused by the semantics applied by each user. The varying number of folders and their meaning are personal and in many cases pose difficulties to learning methods. This paper addresses automatic organization of email messages into folders, based on supervised learning algorithms. The textual fields of the email message (subject and body) are considered for learning, with different representations, feature selection methods, and classifiers. The participant fields are embedded into a vector-space model representation. The classification decisions from the different email fields are combined by majority voting. Experiments on a subset of the Enron Corpus and on a private email data set show the significant improvement over both single classifiers on these fields as well as over previous works.

#index 1806011
#* A log-logistic model-based interpretation of TF normalization of BM25
#@ Yuanhua Lv;ChengXiang Zhai
#t 2012
#c 16
#% 169781
#% 218982
#% 262096
#% 324129
#% 340948
#% 411760
#% 766412
#% 783474
#% 907546
#% 960413
#% 987229
#% 1074104
#% 1195837
#% 1263575
#% 1292709
#% 1305656
#% 1355019
#% 1450848
#% 1450858
#% 1598452
#% 1641914
#% 1642160
#! The effectiveness of BM25 retrieval function is mainly due to its sub-linear term frequency (TF) normalization component, which is controlled by a parameter k1. Although BM25 was derived based on the classic probabilistic retrieval model, it has been so far unclear how to interpret its parameter k1 probabilistically, making it hard to optimize the setting of this parameter. In this paper, we provide a novel probabilistic interpretation of the BM25 TF normalization and its parameter k1 based on a log-logistic model for the probability of seeing a document in the collection with a given level of TF. The proposed interpretation allows us to derive different approaches to estimation of parameter k1 based solely on the current collection without requiring any training data, thus effectively eliminating one free parameter from BM25. Our experiment results show that the proposed approaches can accurately predict the optimal k1 without requiring training data and achieve better or comparable retrieval performance to a well-tuned BM25 where k1 is optimized based on training data.

#index 1806012
#* Score transformation in linear combination for multi-criteria relevance ranking
#@ Shima Gerani;ChengXiang Zhai;Fabio Crestani
#t 2012
#c 16
#% 232703
#% 340934
#% 340948
#% 342710
#% 485325
#% 818255
#% 879662
#% 987226
#% 987241
#% 995515
#% 1019145
#% 1195856
#% 1292546
#% 1450879
#% 1536512
#% 1558080
#! In many Information Retrieval (IR) tasks, documents should be ranked based on a combination of multiple criteria. Therefore, we would need to score a document in each criterion aspect of relevance and then combine the criteria scores to generate a final score for each document. Linear combination of these aspect scores has so far been the dominant approach due to its simplicity and effectiveness. However, such a strategy of combination requires that the scores to be combined are "comparable" to each other, an assumption that generally does not hold due to the different ways of scoring each criterion. Thus it is necessary to transform the raw scores for different criteria appropriately to make them more comparable before combination. In this paper we propose a new principled approach to score transformation in linear combination, in which we would learn a separate non-linear transformation function for each relevance criterion based on the Alternating Conditional Expectation (ACE) algorithm and BoxCox Transformation. Experimental results show that the proposed method is effective and is also robust against non-linear perturbations of the original scores.

#index 1806013
#* Axiomatic analysis of translation language model for information retrieval
#@ Maryam Karimzadehgan;ChengXiang Zhai
#t 2012
#c 16
#% 262096
#% 280851
#% 340948
#% 375017
#% 397128
#% 766412
#% 1450869
#% 1558471
#! Statistical translation models have been shown to outperform simple document language models which rely on exact matching of words in the query and documents. A main challenge in applying translation models to ad hoc information retrieval is to estimate a translation model without training data. In this paper, we perform axiomatic analysis of translation language model for retrieval in order to gain insights about how to optimize the estimation of translation probabilities. We propose a set of constraints that a reasonable translation language model should satisfy. We check these constraints on the state-of-the-art translation estimation method based on Mutual Information and find that it does not satisfy most of the constraints. We then propose a new estimation method that better satisfies the defined constraints. Experimental results on representative TREC data sets show that the proposed new estimation method outperforms the existing Mutual Information-based estimation, suggesting that the proposed constraints are indeed helpful for designing better estimation methods for translation language model.

#index 1806014
#* An information-based cross-language information retrieval model
#@ Bo Li;Eric Gaussier
#t 2012
#c 16
#% 94369
#% 262046
#% 309112
#% 397144
#% 411760
#% 420520
#% 730026
#% 732851
#% 735135
#% 755472
#% 766412
#% 879579
#% 1154026
#% 1450858
#% 1471855
#% 1622334
#% 1622343
#! We present in this paper well-founded cross-language extensions of the recently introduced models in the information-based family for information retrieval, namely the LL (log-logistic) and SPL (smoothed power law) models of [4]. These extensions are based on (a) a generalization of the notion of information used in the information-based family, (b) a generalization of the random variables also used in this family, and (c) the direct expansion of query terms with their translations. We then review these extensions from a theoretical point-of-view, prior to assessing them experimentally. The results of the experimental comparisons between these extensions and existing CLIR systems, on three collections and three language pairs, reveal that the cross-language extension of the LL model provides a state-of-the-art CLIR system, yielding the best performance overall.

#index 1806015
#* Extended expectation maximization for inferring score distributions
#@ Keshi Dai;Virgil Pavlu;Evangelos Kanoulas;Javed A. Aslam
#t 2012
#c 16
#% 280854
#% 340934
#% 340938
#% 340941
#% 818205
#% 891559
#% 1019125
#% 1227642
#% 1263583
#% 1292546
#% 1392434
#% 1450859
#% 1558080
#% 1558083
#! Inferring the distributions of relevant and nonrelevant documents over a ranked list of scored documents returned by a retrieval system has a broad range of applications including information filtering, recall-oriented retrieval, metasearch, and distributed IR. Typically, the distribution of documents over scores is modeled by a mixture of two distributions, one for the relevant and one for the nonrelevant documents, and expectation maximization (EM) is run to estimate the mixture parameters. A large volume of work has focused on selecting the appropriate form of the two distributions in the mixture. In this work we consider the form of the distributions as a given and we focus on the inference algorithm. We extend the EM algorithm (a) by simultaneously considering the ranked lists of documents returned by multiple retrieval systems, and (b) by encoding in the algorithm the constraint that the same document retrieved by multiple systems should have the same, global, probability of relevance. We test the new inference algorithm using TREC data and we demonstrate that it outperforms the regular EM algorithm. It is better calibrated in inferring the probability of document's relevance, and it is more effective when applied on the task of metasearch.

#index 1806016
#* Top-k retrieval using facility location analysis
#@ Guido Zuccon;Leif Azzopardi;Dell Zhang;Jun Wang
#t 2012
#c 16
#% 262112
#% 411762
#% 642975
#% 818241
#% 818916
#% 879618
#% 920792
#% 1074133
#% 1077150
#% 1130311
#% 1190093
#% 1227591
#% 1292528
#% 1313373
#% 1451006
#% 1558079
#% 1598393
#% 1697443
#! The top-k retrieval problem aims to find the optimal set of k documents from a number of relevant documents given the user's query. The key issue is to balance the relevance and diversity of the top-k search results. In this paper, we address this problem using Facility Location Analysis taken from Operations Research, where the locations of facilities are optimally chosen according to some criteria. We show how this analysis technique is a generalization of state-of-the-art retrieval models for diversification (such as the Modern Portfolio Theory for Information Retrieval), which treat the top-k search results like "obnoxious facilities" that should be dispersed as far as possible from each other. However, Facility Location Analysis suggests that the top-k search results could be treated like "desirable facilities" to be placed as close as possible to their customers. This leads to a new top-k retrieval model where the best representatives of the relevant documents are selected. In a series of experiments conducted on two TREC diversity collections, we show that significant improvements can be made over the current state-of-the-art through this alternative treatment of the top-k retrieval problem.

#index 1806017
#* An interactive paper and digital pen interface for query-by-sketch image retrieval
#@ Roman Kreuzer;Michael Springmann;Ihab Al Kabary;Heiko Schuldt
#t 2012
#c 16
#% 227481
#% 437405
#% 458521
#% 982791
#% 1132472
#% 1250888
#% 1268613
#% 1393474
#% 1407675
#% 1482354
#% 1857841
#! A major challenge when dealing with large collections of digital images is to find relevant objects, especially when no metadata on the objects is available. Content-based image retrieval (CBIR) addresses this problem but usually lacks query images that are good enough to express the user's information need. Therefore, in Query-by-Sketch, CBIR has been considered with user provided sketches as query objects --- but so far, this has suffered from the limitations of existing user interfaces. In this paper, we present a novel user interface for query by sketch that exploits emergent interactive paper and digital pen technology. Users can draw sketches on paper in a user-friendly way. Search can be started interactively from the paper front-end, due to a streaming interface from the digital pen to the underlying CBIR system. We present the implementation of the interactive paper/digital pen interface on top of QbS, our system for CBIR using sketches, and we present in detail the evaluation of the system on the basis of the MIRFLICKR-25000 image collection.

#index 1806018
#* Image abstraction in crossmedia retrieval for text illustration
#@ Filipe Coelho;Cristina Ribeiro
#t 2012
#c 16
#% 318785
#% 780136
#% 860956
#% 860959
#% 940440
#% 991230
#% 1040539
#% 1041734
#% 1132151
#% 1132472
#% 1375844
#% 1484718
#% 1495093
#% 1507047
#% 1667643
#! Text illustration is a multimedia retrieval task that consists in finding suitable images to illustrate text fragments such as blog entries, news reports or children stories. In this paper we describe a crossmedia retrieval system which, given a textual input, selects a short list of candidate images from a large media collection. This approach makes use of a recently proposed method to map metadata and visual features into a common textual representation that can be handled by traditional information retrieval engines. Content-based analysis is enhanced by visual abstraction, namely the Anisotropic Kuwahara Filter, which impacts feature information captured by the Joint Composite and Speeded Up Robust Features visual descriptors. For evaluation purposes, we used the well-established MIRFlickr photo collection, with 25,000 photos and user tags collected from Flickr as well as manual annotations provided as image retrieval groundtruth. Results show that image abstraction can improve visual retrieval as well as significantly reduce processing and storage requirements, even more when paired with Google's WebP image format. We conclude that applying a visual rerank after an initial text retrieval step improves the quality of results, and that the adopted text mapping method for visual descriptors provides an effective crossmedia approach for text illustration.

#index 1806019
#* A latent variable ranking model for content-based retrieval
#@ Ariadna Quattoni;Xavier Carreras;Antonio Torralba
#t 2012
#c 16
#% 143194
#% 464291
#% 464631
#% 577224
#% 763697
#% 763708
#% 770782
#% 812418
#% 829025
#% 879605
#% 883981
#% 983905
#% 1211797
#% 1386134
#% 1502472
#! Since their introduction, ranking SVM models [11] have become a powerful tool for training content-based retrieval systems. All we need for training a model are retrieval examples in the form of triplet constraints, i.e. examples specifying that relative to some query, a database item a should be ranked higher than database item b. These types of constraints could be obtained from feedback of users of the retrieval system. Most previous ranking models learn either a global combination of elementary similarity functions or a combination defined with respect to a single database item. Instead, we propose a "coarse to fine" ranking model where given a query we first compute a distribution over "coarse" classes and then use the linear combination that has been optimized for queries of that class. These coarse classes are hidden and need to be induced by the training algorithm. We propose a latent variable ranking model that induces both the latent classes and the weights of the linear combination for each class from ranking triplets. Our experiments over two large image datasets and a text retrieval dataset show the advantages of our model over learning a global combination as well as a combination for each test point (i.e. transductive setting). Furthermore, compared to the transductive approach our model has a clear computational advantages since it does not need to be retrained for each test query.

#index 1806020
#* Language modelling of constraints for text clustering
#@ Javier Parapar;Álvaro Barreiro
#t 2012
#c 16
#% 313959
#% 340901
#% 464291
#% 464608
#% 466890
#% 750863
#% 766433
#% 769881
#% 879615
#% 915231
#% 916785
#% 987341
#% 1074080
#% 1085668
#% 1263601
#% 1292730
#% 1415749
#% 1603758
#! Constrained clustering is a recently presented family of semi-supervised learning algorithms. These methods use domain information to impose constraints over the clustering output. The way in which those constraints (typically pair-wise constraints between documents) are introduced is by designing new clustering algorithms that enforce the accomplishment of the constraints. In this paper we present an alternative approach for constrained clustering where, instead of defining new algorithms or objective functions, the constraints are introduced modifying the document representation by means of their language modelling. More precisely the constraints are modelled using the well-known Relevance Models successfully used in other retrieval tasks such as pseudo-relevance feedback. To the best of our knowledge this is the first attempt to try such approach. The results show that the presented approach is an effective method for constrained clustering even improving the results of existing constrained clustering algorithms.

#index 1806021
#* A framework for unsupervised spam detection in social networking sites
#@ Maarten Bosma;Edgar Meij;Wouter Weerkamp
#t 2012
#c 16
#% 290830
#% 662755
#% 730082
#% 753425
#% 754098
#% 889273
#% 936910
#% 1019165
#% 1125907
#% 1190060
#% 1400002
#% 1450883
#% 1736221
#! Social networking sites offer users the option to submit user spam reports for a given message, indicating this message is inappropriate. In this paper we present a framework that uses these user spam reports for spam detection. The framework is based on the HITS web link analysis framework and is instantiated in three models. The models subsequently introduce propagation between messages reported by the same user, messages authored by the same user, and messages with similar content. Each of the models can also be converted to a simple semi-supervised scheme. We test our models on data from a popular social network and compare the models to two baselines, based on message content and raw report counts. We find that our models outperform both baselines and that each of the additions (reporters, authors, and similar messages) further improves the performance of the framework.

#index 1806022
#* Classification of short texts by deploying topical annotations
#@ Daniele Vitale;Paolo Ferragina;Ugo Scaiella
#t 2012
#c 16
#% 344447
#% 466580
#% 869500
#% 956570
#% 975019
#% 987328
#% 1055680
#% 1214667
#% 1250381
#% 1269107
#% 1272267
#% 1289518
#% 1450992
#% 1482395
#% 1598472
#% 1607030
#% 1711796
#! We propose a novel approach to the classification of short texts based on two factors: the use of Wikipedia-based annotators that have been recently introduced to detect the main topics present in an input text, represented via Wikipedia pages, and the design of a novel classification algorithm that measures the similarity between the input text and each output category by deploying only their annotated topics and the Wikipedia link-structure. Our approach waives the common practice of expanding the feature-space with new dimensions derived either from explicit or from latent semantic analysis. As a consequence it is simple and maintains a compact intelligible representation of the output categories. Our experiments show that it is efficient in construction and query time, accurate as state-of-the-art classifiers (see e.g. Phan et al. WWW '08), and robust with respect to concept drifts and input sources.

#index 1806023
#* Cluster labeling for multilingual scatter/gather using comparable corpora
#@ Goutham Tholpadi;Mrinal Kanti Das;Chiranjib Bhattacharyya;Shirish Shevade
#t 2012
#c 16
#% 118771
#% 413609
#% 722904
#% 787502
#% 807363
#% 813043
#% 878454
#% 961706
#% 1025655
#% 1077150
#% 1202162
#% 1227579
#% 1227594
#% 1338620
#% 1387551
#% 1450829
#! Scatter/Gather systems are increasingly becoming useful in browsing document corpora. Usability of the present-day systems are restricted to monolingual corpora, and their methods for clustering and labeling do not easily extend to the multilingual setting, especially in the absence of dictionaries/machine translation. In this paper, we study the cluster labeling problem for multilingual corpora in the absence of machine translation, but using comparable corpora. Using a variational approach, we show that multilingual topic models can effectively handle the cluster labeling problem, which in turn allows us to design a novel Scatter/Gather system ShoBha. Experimental results on three datasets, namely the Canadian Hansards corpus, the entire overlapping Wikipedia of English, Hindi and Bengali articles, and a trilingual news corpus containing 41,000 articles, confirm the utility of the proposed system.

#index 1806024
#* Adaptive time-to-live strategies for query result caching in web search engines
#@ Sadiye Alici;Ismail Sengor Altingovde;Rifat Ozcan;B. Barla Cambazoglu;Özgür Ulusoy
#t 2012
#c 16
#% 577302
#% 577370
#% 590523
#% 615595
#% 878624
#% 987215
#% 1074067
#% 1190098
#% 1301004
#% 1399951
#% 1404875
#% 1450839
#% 1563624
#% 1587345
#% 1587379
#% 1598431
#% 1682107
#! An important research problem that has recently started to receive attention is the freshness issue in search engine result caches. In the current techniques in literature, the cached search result pages are associated with a fixed time-to-live (TTL) value in order to bound the staleness of search results presented to the users, potentially as part of a more complex cache refresh or invalidation mechanism. In this paper, we propose techniques where the TTL values are set in an adaptive manner, on a per-query basis. Our results show that the proposed techniques reduce the fraction of stale results served by the cache and also decrease the fraction of redundant query evaluations on the search engine backend compared to a strategy using a fixed TTL value for all queries.

#index 1806025
#* Intra-query concurrent pipelined processing for distributed full-text retrieval
#@ Simon Jonassen;Svein Erik Bratsberg
#t 2012
#c 16
#% 198335
#% 249153
#% 435115
#% 508264
#% 879608
#% 976948
#% 987214
#% 1019169
#% 1055149
#% 1190095
#% 1267034
#% 1383607
#% 1480887
#% 1583623
#% 1587386
#% 1598433
#% 1683906
#! Pipelined query processing over a term-wise distributed inverted index has superior throughput at high query multiprogramming levels. However, due to long query latencies this approach is inefficient at lower levels. In this paper we explore two types of intra-query parallelism within the pipelined approach, parallel execution of a query on different nodes and concurrent execution on the same node. According to the experimental results, our approach reaches the throughput of the state-of-the-art method at about half of the latency. On the single query case the observed latency improvement is up to 2.6 times.

#index 1806026
#* Usefulness of sentiment analysis
#@ Jussi Karlgren;Magnus Sahlgren;Fredrik Olsson;Fredrik Espinoza;Ola Hamfors
#t 2012
#c 16
#% 854646
#% 1026431
#% 1127964
#% 1129994
#% 1132480
#% 1292506
#% 1301020
#% 1360586
#% 1587370
#% 1642910
#! What can text sentiment analysis technology be used for, and does a more usage-informed view on sentiment analysis pose new requirements on technology development?

#index 1806027
#* Modeling static caching in web search engines
#@ Ricardo Baeza-Yates;Simon Jonassen
#t 2012
#c 16
#% 194299
#% 340888
#% 577302
#% 729343
#% 805864
#% 860861
#% 987215
#% 1074067
#% 1089473
#% 1190095
#% 1190098
#% 1404875
#% 1558842
#% 1587384
#% 1715628
#% 1834787
#% 1870668
#! In this paper we model a two-level cache of a Web search engine, such that given memory resources, we find the optimal split fraction to allocate for each cache, results and index. The final result is very simple and implies to compute just five parameters that depend on the input data and the performance of the search engine. The model is validated through extensive experimental results and is motivated on capacity planning and the overall optimization of the search architecture.

#index 1806028
#* Integrating interactive visualizations in the search process of digital libraries and IR systems
#@ Daniel Hienert;Frank Sawitzki;Philipp Schaer;Philipp Mayr
#t 2012
#c 16
#% 802859
#% 882649
#% 1147592
#% 1573948
#% 1624250
#! Interactive visualizations for exploring and retrieval have not yet become an integral part of digital libraries and information retrieval systems. We have integrated a set of interactive graphics in a real world social science digital library. These visualizations support the exploration of search queries, results and authors, can filter search results, show trends in the database and can support the creation of new search queries. The use of weighted brushing supports the identification of related metadata for search facets. In a user study we verify that users can gain insights from statistical graphics intuitively and can adopt interaction techniques.

#index 1806029
#* On theoretically valid score distributions in information retrieval
#@ Ronan Cummins;Colm O'Riordan
#t 2012
#c 16
#% 1392434
#% 1450904
#% 1558080
#% 1748071
#! In this paper, we aim to investigate the practical usefulness of the Recall-Fallout Convexity Hypothesis (RFCH) for a number of document score distribution (SD) models. We compare SD models that do not automatically adhere to the RFCH to modified versions of the same SD models that do adhere to the RFCH. We compare these models using the inference of average precision as a measure of utility. For the three models studied in this paper, we conclude that adhering to the RFCH is practically useful for the two-normal model, makes no difference for the two-gamma model, and degrades the performance of the two-lognormal model.

#index 1806030
#* Adaptive temporal query modeling
#@ Maria-Hendrike Peetz;Edgar Meij;Maarten de Rijke;Wouter Weerkamp
#t 2012
#c 16
#% 340901
#% 730070
#% 960414
#% 1077150
#% 1130999
#% 1451009
#% 1587369
#% 1587376
#% 1598383
#% 1697440
#! We present an approach to query modeling that uses the temporal distribution of documents in an initially retrieved set of documents. Such distributions tend to exhibit bursts, especially in news-related document collections. We hypothesize that documents in those bursts are more likely to be relevant and update the query model with the most distinguishing terms in high-quality documents sampled from bursts. We evaluate the effectiveness of our models on a test collection of blog posts.

#index 1806031
#* The design of a visual history tool to help users refind information within a website
#@ Trien V. Do;Roy A. Ruddle
#t 2012
#c 16
#% 588724
#% 805200
#% 869476
#% 1183230
#% 1195858
#% 1313201
#% 1697448
#! On the WWW users frequently revisit information they have previously seen, but "keeping found things found" is difficult when the information has not been visited frequently or recently, even if a user knows which website contained the information. This paper describes the design of a tool to help users refind information within a given website. The tool encodes data about a user's interest in webpages (measured by dwell time), the frequency and recency of visits, and navigational associations between pages, and presents navigation histories in list- and graph-based forms.

#index 1806032
#* Analyzing the polarity of opinionated queries
#@ Sergiu Chelaru;Ismail Sengor Altingovde;Stefan Siersdorfer
#t 2012
#c 16
#% 878624
#% 1475758
#% 1587343
#! In this paper, we present an in-depth analysis of Web search queries for controversial topics, focusing on query sentiment. To this end, we conduct extensive user assessments as well as an automatic sentiment analysis using the SentiWordNet thesaurus.

#index 1806033
#* Semi-automatic document classification: exploiting document difficulty
#@ Miguel Martinez-Alvarez;Sirvan Yahyaei;Thomas Roelleke
#t 2012
#c 16
#% 340904
#% 344447
#% 879613
#! There are circumstances where classification is required only if a certain condition, such a specific level of quality, is met. This paper investigates a semi-automatic solution where only the predictions for the documents which are more likely to be correctly classified would be considered. This method provides high-quality automatic classification for large subsets of the collection and employs human expertise for the "most complicated" decisions. This research presents different approaches to measure document difficulty and it discusses the benefits of applying it for semi-automatic classification. In addition, experiments are carried out to show the results achieved for different subsets of the collection. Experiments prove that it is possible to improve quality significantly with large subsets (i.e. 13% micro-f1 increase with 70% of documents) of two different collections. Furthermore, it shows how it provides a flexible mechanism to apply automatic classification to specific subsets while specific constrains are met.

#index 1806034
#* Investigating summarization techniques for geo-tagged image indexing
#@ Ahmet Aker;Xin Fan;Mark Sanderson;Robert Gaizauskas
#t 2012
#c 16
#% 1375835
#% 1471305
#% 1642146
#! Images with geo-tagging information are increasingly available on the Web. However, such images need to be annotated with additional textual information if they are to be retrievable, since users do not search by geo-coordinates. We propose to automatically generate such textual information by (1) generating toponyms from the geo-tagging information (2) retrieving Web documents using toponyms as queries (3) summarizing the retrieved documents. The summaries are then used to index the images. In this paper we investigate how various summarization techniques affect image retrieval performance and show significant improvements can be obtained when using the summaries for indexing.

#index 1806035
#* Handling OOV words in indian-language --- english CLIR
#@ Parin Chheda;Manaal Faruqui;Pabitra Mitra
#t 2012
#c 16
#% 1077150
#% 1099154
#! Because of the lack of resources Cross-lingual information retrieval is a difficult task for many Indian languages. Google Translate provides an easy way of translation from Indian languages to English but due to lexicon limitations most of the out-of-vocabulory words get transliterated letter by letter along with their suffix resulting in an unusually long string. The resulting string often does not match its intended translation which hurts retrieval. We propose an approach to extract the correct word from such strings using word segmentation along with approximate string matching using Soundex algorithm & Levenshtein distance. We evaluate our approach across three Indian languages and find an average improvement of 5.8% MAP on the FIRE-2010 dataset.

#index 1806036
#* Using a medical thesaurus to predict query difficulty
#@ Florian Boudin;Jian-Yun Nie;Martin Dawes
#t 2012
#c 16
#% 397161
#% 804915
#% 879613
#% 944349
#% 1415713
#% 1481546
#! Estimating query performance is the task of predicting the quality of results returned by a search engine in response to a query. In this paper, we focus on pre-retrieval prediction methods for the medical domain. We propose a novel predictor that exploits a thesaurus to ascertain how difficult queries are. In our experiments, we show that our predictor outperforms the state-of-the-art methods that do not use a thesaurus.

#index 1806037
#* Studying a personality coreference network in a news stories photo collection
#@ José Devezas;Filipe Coelho;Sérgio Nunes;Cristina Ribeiro
#t 2012
#c 16
#! We build and analyze a coreference network based on entities from photo descriptions, where nodes represent personalities and edges connect people mentioned in the same photo description. We identify and characterize the communities in this network and propose taking advantage of the context provided by community detection methodologies to improve text illustration and general search.

#index 1806038
#* Phrase pair classification for identifying subtopics
#@ Sujatha Das;Prasenjit Mitra;C. Lee Giles
#t 2012
#c 16
#% 280849
#% 340901
#% 754068
#% 756964
#% 1019093
#% 1026937
#% 1176887
#% 1218678
#% 1392466
#! Automatic identification of subtopics for a given topic is desirable because it eliminates the need for manual construction of domain-specific topic hierarchies. In this paper, we design features based on corpus statistics to design a classifier for identifying the (subtopic, topic) links between phrase pairs. We combine these features along with the commonly-used syntactic patterns to classify phrase pairs from datasets in Computer Science and WordNet. In addition, we show a novel application of our is-a-subtopic-of classifier for query expansion in Expert Search and compare it with pseudo-relevance feedback.

#index 1806039
#* Full and mini-batch clustering of news articles with Star-EM
#@ Matthias Gallé;Jean-Michel Renders
#t 2012
#c 16
#% 262042
#% 329562
#% 375017
#% 741881
#% 791715
#% 1260711
#% 1536572
#! We present a new threshold-based clustering algorithm for news articles. The algorithm consists of two phases: in the first, a local optimum of a score function that captures the quality of a clustering is found with an Expectation-Maximization approach. In the second phase, the algorithm reduces the number of clusters and, in particular, is able to build non-spherical---shaped clusters. We also give a mini-batch version which allows an efficient dynamic processing of data points as they arrive in groups. Our experiments on the TDT5 benchmark collection show the superiority of both versions of this algorithm compared to other state-of-the-art alternatives.

#index 1806040
#* Assessing and predicting vertical intent for web queries
#@ Ke Zhou;Ronan Cummins;Martin Halvey;Mounia Lalmas;Joemon M. Jose
#t 2012
#c 16
#% 424308
#% 1227616
#% 1642923
#! Aggregating search results from a variety of heterogeneous sources, i.e. so-called verticals [1], such as news, image, video and blog, into a single interface has become a popular paradigm in web search. In this paper, we present the results of a user study that collected more than 1,500 assessments of vertical intent over 320 web topics. Firstly, we show that users prefer diverse vertical content for many queries and that the level of inter-assessor agreement for the task is fair [2]. Secondly, we propose a methodology to predict the vertical intent of a query using a search engine log by exploiting click-through data, and show that it outperforms traditional approaches.

#index 1806041
#* Predicting IMDB movie ratings using social media
#@ Andrei Oghina;Mathias Breuss;Manos Tsagkias;Maarten de Rijke
#t 2012
#c 16
#% 1292698
#% 1301004
#% 1470600
#% 1697430
#! We predict IMDb movie ratings and consider two sets of features: surface and textual features. For the latter, we assume that no social media signal is isolated and use data from multiple channels that are linked to a particular movie, such as tweets from Twitter and comments from YouTube. We extract textual features from each channel to use in our prediction model and we explore whether data from either of these channels can help to extract a better set of textual feature for prediction. Our best performing model is able to rate movies very close to the observed values.

#index 1806042
#* Squeezing the ensemble pruning: faster and more accurate categorization for news portals
#@ Cagri Toraman;Fazli Can
#t 2012
#c 16
#% 551723
#% 1748100
#! Recent studies show that ensemble pruning works as effective as traditional ensemble of classifiers (EoC). In this study, we analyze how ensemble pruning can improve text categorization efficiency in time-critical real-life applications such as news portals. The most crucial two phases of text categorization are training classifiers and assigning labels to new documents; but the latter is more important for efficiency of such applications. We conduct experiments on ensemble pruning-based news article categorization to measure its accuracy and time cost. The results show that our heuristics reduce the time cost of the second phase. Also we can make a trade-off between accuracy and time cost to improve both of them with appropriate pruning degrees.

#index 1806043
#* A general framework for people retrieval in social media with multiple roles
#@ Amin Mantrach;Jean-Michel Renders
#t 2012
#c 16
#% 879570
#% 1587391
#% 1598457
#! Internet users are more and more playing multiple roles when connected on the Web, such as "posting", "commenting", "tagging" and "sharing" different kinds of information on various social media. Despite the research interest in the field of social networks, few has been done up to now w.r.t. information access in multi-relational social networks where queries can be multifaceted queries (e.g. a mix of textual key-words and key-persons in some social context). We propose a unified and efficient framework to address such complex queries on multi-modal "social" collections, working in 3 distinct phases, namely: (I) aggregation of documents into modal profiles, (II) expansion of mono-modal subqueries to mono-modal and multi-modal subqueries, (III) relevance score computation through late fusion of the different similarities deduced from profiles and subqueries obtained during the first two phases. Experiments on the ENRON email collection for a recipient proposal task show that competitive results can be obtained using the proposed framework.

#index 1806044
#* Analysis of query reformulations in a search engine of a local web site
#@ M-Dyaa Albakour;Udo Kruschwitz;Nikolaos Nanas;Ibrahim Adeyanju;Dawei Song;Maria Fasli;Anne De Roeck
#t 2012
#c 16
#% 642985
#% 950658
#% 1130811
#% 1292473
#% 1523401
#! This study examines reformulations of queries submitted to a search engine of a university Web site with a focus on (implicitly derived) user satisfaction and the performance of the underlying search engine. Using a search log of a university Web site we examined all reformulations submitted in a 10-week period and studied the relation between the popularity of the reformulation and the performance of the search engine estimated using a number of clickthrough-based measures. Our findings are a step towards building better query recommendation systems and suggest a number of metrics to evaluate query recommendation systems.

#index 1806045
#* Temporal pseudo-relevance feedback in microblog retrieval
#@ Stewart Whiting;Iraklis A. Klampanos;Joemon M. Jose
#t 2012
#c 16
#% 729936
#% 1467831
#% 1536506
#% 1581252
#% 1587369
#% 1598523
#! Twitter has become a major outlet for news, discussion and commentary of on-going events and trends. Effective searching of Twitter collections poses a number of issues for traditional document-based information retrieval (IR) approaches, such as limited document term statistics and spam. In this paper we propose a novel approach to pseudo-relevance feedback, based upon the temporal profiles of n-grams extracted from the top N relevance feedback tweets. A weighted graph is used to model temporal correlation between n-grams, with a PageRank variant employed to combine both pseudo-relevant document term distribution and temporal collection evidence. Preliminary experiments with the TREC Microblogging 2011 Twitter corpus indicate that through parameter optimisation, retrieval effectiveness can be improved.

#index 1806046
#* Learning adaptive domain models from click data to bootstrap interactive web search
#@ Deirdre Lungley;Udo Kruschwitz;Dawei Song
#t 2012
#c 16
#% 280849
#% 384416
#% 823348
#% 1603005
#! Today, searchers exploring the World Wide Web have come to expect enhanced search interfaces --- query completion and related searches have become standard. Here we propose a Formal Concept Analysis lattice as an underlying domain model to provide a source of query refinements. The initial lattice is constructed using NLP. User clicks on documents, seen as implicit user feedback, are harnessed to adapt it. In this paper, we explore the viability of this adaptation process and the results we present demonstrate its promise and limitations for proposing initial effective refinements when searching the diverse WWW domain.

#index 1806047
#* A little interaction can go a long way: enriching the query formulation process
#@ Abdigani Diriye;Anastasios Tombros;Ann Blandford
#t 2012
#c 16
#% 29585
#% 295522
#% 346553
#% 643001
#% 832356
#% 857478
#% 1130854
#% 1213434
#! This poster argues for a need for more dialogue and richer information and interaction during query formulation between the user and the system. We present two novel methods --- query previews and categorised Interactive Query Expansions --- that seek to do just this. Our method enriches a searcher's query formulation by leveraging semantic information to help identify the topicality of the term, and the outcomes of its selection. The initial findings are largely positive and suggest user preference.

#index 1806048
#* Learning to rank from relevance feedback for e-discovery
#@ Peter Lubell-Doughtie;Katja Hofmann
#t 2012
#c 16
#% 1074078
#% 1077150
#% 1292491
#% 1415778
#! In recall-oriented search tasks retrieval systems are privy to a greater amount of user feedback. In this paper we present a novel method of combining relevance feedback with learning to rank. Our experiments use data from the 2010 TREC Legal track to demonstrate that learning to rank can tune relevance feedback to improve result rankings for specific queries, even with limited amounts of user feedback.

#index 1806049
#* When simple is (more than) good enough: effective semantic search with (almost) no semantics
#@ Robert Neumayer;Krisztian Balog;Kjetil Nørvåg
#t 2012
#c 16
#% 642992
#% 783474
#% 1400010
#% 1475756
#% 1641483
#! Using keyword queries to find entities has emerged as one of the major search types on the Web. In this paper, we study the task of ad-hoc entity retrieval: keyword search in a collection of structured data. We start with a baseline retrieval system that constructs pseudo documents from RDF triples and introduce three extensions: preprocessing of URIs, using two-fielded retrieval models, and boosting popular domains. Using the query sets of the 2010 and 2011 Semantic Search Challenge, we show that our straightforward approach outperforms all previously reported results, some generated by far more complex systems.

#index 1806050
#* Evaluating personal information retrieval
#@ Liadh Kelly;Paul Bunbury;Gareth J. F. Jones
#t 2012
#c 16
#% 835189
#% 1077150
#% 1189155
#% 1292597
#! Evaluation of personal search over an individual's personal information space on the desktop or elsewhere is problematic for reasons relating both to the personal and private nature of the data and the associated personal information needs of collection owners. Indeed challenges associated with evaluation in this space are recognised as one of the key factors hindering the development of research in personal information retrieval. We present the "personal information retrieval evaluation (PIRE)" tool, which provides a solution to this evaluation problem using a ‘living laboratory' approach. This tool allows for the evaluation of retrieval techniques using ‘real' individuals' personal collections, queries and result sets, in a cross-comparable repeatable way, while importantly maintaining an individual's informational privacy.

#index 1806051
#* Applying power graph analysis to weighted graphs
#@ Niels Bloom
#t 2012
#c 16
#% 731279
#% 1021428
#! We expanded Power Graph Analysis for use with weighted graphs, applying the technique to document categorisation with promising results. With the additional weight information we were able to create more accurate representations of the underlying data while maintaining a high level of edge reduction and improving visualisation of the graph.

#index 1806052
#* An investigation of term weighting approaches for microblog retrieval
#@ Paul Ferguson;Neil O'Hare;James Lanagan;Owen Phelan;Kevin McCarthy
#t 2012
#c 16
#% 324129
#% 1484274
#% 1587369
#! The use of effective term frequency weighting and document length normalisation strategies have been shown over a number of decades to have a significant positive effect for document retrieval. When dealing with much shorter documents, such as those obtained from microblogs, it would seem intuitive that these would have less benefit. In this paper we investigate their effect on microblog retrieval performance using the Tweets2011 collection from the TREC 2011 Microblog Track.

#index 1806053
#* On the size of full element-indexes for XML keyword search
#@ Duygu Atilgan;Ismail Sengor Altingovde;Özgür Ulusoy
#t 2012
#c 16
#% 290703
#% 654442
#% 810052
#! We show that a full element-index can be as space-efficient as a direct index with Dewey ids, after compression using typical techniques.

#index 1806054
#* Combining probabilistic language models for aspect-based sentiment retrieval
#@ Lisette García-Moya;Henry Anaya-Sánchez;Rafael Berlanga-Llavori
#t 2012
#c 16
#% 1544171
#% 1592078
#! In this paper, we present a new methodology aimed at retrieving relevant product aspects from a collection of customer reviews, as well as the most salient sentiments expressed about them. Our proposal is both unsupervised and domain independent, and does not relies on NLP techniques such as parsing or dependence analysis. In our experiments, the proposed method achieves good values of precision. It is also shown that our approach is capable of properly retrieving the relevant aspects and their sentiments even from individual reviews.

#index 1806055
#* In praise of laziness: a lazy strategy for web information extraction
#@ Rifat Ozcan;Ismail Sengor Altingovde;Özgür Ulusoy
#t 2012
#c 16
#% 246243
#% 266216
#% 889107
#% 915347
#% 1642223
#! A large number of Web information extraction algorithms are based on machine learning techniques. For such extraction algorithms, we propose employing a lazy learning strategy to build a specialized model for each test instance to improve the extraction accuracy and avoid the disadvantages of constructing a single general model.

#index 1806056
#* LiveTweet: monitoring and predicting interesting microblog posts
#@ Arifah Che Alhadi;Thomas Gottron;Jérôme Kunegis;Nasir Naveed
#t 2012
#c 16
#% 1641934
#% 1697470
#! This paper describes the LiveTweet application, a system for automatically analysing and predicting the interestingness of microblog posts. Based on a stream of recent microblog posts the system tracks user interactions on Twitter that indicate interesting content. An incremental Naive Bayes model is trained to learn the characteristics of tweets which are considered interesting by the users. Finally, the probability of a microblog post to be retweeted is used as metric for its interestingness.

#index 1806057
#* A user interface for query-by-sketch based image retrieval with color sketches
#@ Ivan Giangreco;Michael Springmann;Ihab Al Kabary;Heiko Schuldt
#t 2012
#c 16
#% 1132472
#% 1482354
#! This demo will interactively show a system that exploits a novel user interface, running on Tablet PCs or graphic tablets, that provides query-by-sketch based image retrieval using color sketches. The system uses Angular Radial Partitioning (ARP) for the edge information in the sketches and color moments in the CIELAB space, combined with a distance metric that is robust to deviations in color as they usually need to be taken into account with user-generated color sketches.

#index 1806058
#* Crisees: real-time monitoring of social media streams to support crisis management
#@ David Maxwell;Stefan Raue;Leif Azzopardi;Chris Johnson;Sarah Oates
#t 2012
#c 16
#! The Crisees demonstrator is a service that aggregates and collects social media streams to support Crisis Managment.

#index 1806059
#* A mailbox search engine using query multi-modal expansion and community-based smoothing
#@ Amin Mantrach;Jean-Michel Renders
#t 2012
#c 16
#! This demo introduces a new tool (or plug-in) for any email client that automatically decomposes the (personal or shared) mailbox into new virtual folders, corresponding to topics and communities, in an unsupervised way to lighten end-user load. The proposed software implements a retrieval system where the user can search for emails but also for people by submitting a double-faceted query: "key words" and "key persons". The software is able to retrieve three kind of documents that a matching search-based system would not retrieve. Firstly, by using person profiles, the software will rank documents related to the key persons without requiring them to be participant (i.e. being author or recipient). Secondly, the system will retrieve documents sharing the same topics as the key words but not necessarily containing them. Thirdly, the proposed solution will also retrieve other participants who are members of the communities associated to the key persons.

#index 1806060
#* EmSe: supporting children's information needs within a hospital environment
#@ Leif Azzopardi;Doug Dowie;Sergio Duarte;Carsten Eickhoff;Richard Glassey;Karl Gyllstrom;Djoerd Hiemstra;Franciska de Jong;Frea Kruisinga;Kelly Marshall;Sien Moens;Tamara Polajnar;Frans van der Sluis;Arjen de Vries
#t 2012
#c 16
#% 1536555
#% 1587343
#% 1587428
#! The Emma Search (EmSe) demonstrator developed for the Emma Children's Hospital showcases the PuppyIR project and PuppyIR framework for building information services for children.

#index 1806061
#* Retro: time-based exploration of product reviews
#@ Jannik Strötgen;Omar Alonso;Michael Gertz
#t 2012
#c 16
#% 577355
#% 769892
#% 815915
#% 907490
#! Most e-commerce websites organize and present product reviews around ratings with hardly any feature to view them in a time-oriented way. Often, there is a way to sort reviews by time but no further temporal analysis is possible. Thus, usually, only few reviews are part of a user's review analysis process, and there is no way to analyze all reviews of a product collectively. In this paper, we describe Retro, a search engine for exploring product reviews using temporal information.

#index 1806062
#* Querium: a session-based collaborative search system
#@ Abdigani Diriye;Gene Golovchinsky
#t 2012
#c 16
#% 805898
#% 998795
#% 1047437
#% 1047490
#% 1074090
#% 1384196
#% 1455260
#! People's information-seeking can span multiple sessions, and can be collaborative in nature. Existing commercial offerings do not effectively support searchers to share, save, collaborate or revisit their information. In this demo paper we present Querium: a novel session-based collaborative search system that lets users search, share, resume and collaborate with other users. Querium provides a number of novel search features in a collaborative setting, including relevance feedback, query fusion, faceted search, and search histories.

#index 1967734
#* Proceedings of the 35th European conference on Advances in Information Retrieval
#@ Pavel Serdyukov;Pavel Braslavski;Sergei O. Kuznetsov;Jaap Kamps;Stefan Rüger
#t 2013
#c 16

#index 1967735
#* Using intent information to model user behavior in diversified search
#@ Aleksandr Chuklin;Pavel Serdyukov;Maarten de Rijke
#t 2013
#c 16
#% 325001
#% 818221
#% 879686
#% 1035578
#% 1074092
#% 1074133
#% 1095876
#% 1166473
#% 1166517
#% 1190055
#% 1190056
#% 1227616
#% 1292528
#% 1417383
#% 1450915
#% 1536510
#% 1536576
#% 1560356
#% 1587348
#% 1598438
#% 1641937
#% 1642151
#% 1693908
#! A result page of a modern commercial search engine often contains documents of different types targeted to satisfy different user intents (news, blogs, multimedia). When evaluating system performance and making design decisions we need to better understand user behavior on such result pages. To address this problem various click models have previously been proposed. In this paper we focus on result pages containing fresh results and propose a way to model user intent distribution and bias due to different document presentation types. To the best of our knowledge this is the first work that successfully uses intent and layout information to improve existing click models.

#index 1967736
#* Understanding relevance: an fMRI study
#@ Yashar Moshfeghi;Luisa R. Pinto;Frank E. Pollick;Joemon M. Jose
#t 2013
#c 16
#% 214709
#% 235918
#% 766454
#% 818221
#% 835027
#% 907516
#% 1015626
#% 1053505
#% 1279814
#% 1292861
#% 1450875
#! Relevance is one of the key concepts in Information Retrieval (IR). A huge body of research exists that attempts to understand this concept so as to operationalize it for IR systems. Despite advances in the past few decades, answering the question "How does relevance happen?" is still a big challenge. In this paper, we investigate the connection between relevance and brain activity. Using functional Magnetic Resonance Imaging (fMRI), we measured the brain activity of eighteen participants while they performed four topical relevance assessment tasks on relevant and non-relevant images. The results of this experiment revealed three brain regions in the frontal, parietal and temporal cortex where brain activity differed between processing relevant and non-relevant documents. This is an important step in unravelling the nature of relevance and therefore better utilising it for effective retrieval.

#index 1967737
#* An exploratory study of sensemaking in collaborative information seeking
#@ Yihan Tao;Anastasios Tombros
#t 2013
#c 16
#% 802864
#% 857478
#% 902313
#% 998795
#% 1015010
#% 1047490
#% 1183271
#% 1303203
#% 1355315
#% 1806062
#! With the ubiquity of current information retrieval systems, users move beyond individual searching to performing complex information seeking tasks together with collaborators for social, leisure or professional purposes. In this paper, we investigate the sensemaking behaviour of online searchers in terms of sensemaking strategies, sharing of information, construction of a shared representation and sharing of task progress and status. We also looked into the support provided to them by search systems in the collaborative information seeking process. We report the results of an observational user study where 24 participants, in groups of 3, completed a travel planning task. Our results show that current tools do not sufficiently support searchers in most aspects of the collaborative sensemaking process. Our findings have implications for the design of collaborative information seeking systems.

#index 1967738
#* Exploiting user comments for audio-visual content indexing and retrieval
#@ Carsten Eickhoff;Wen Li;Arjen P. de Vries
#t 2013
#c 16
#% 642992
#% 643520
#% 783474
#% 855200
#% 955010
#% 1019161
#% 1074117
#% 1218675
#% 1227626
#% 1279781
#% 1507035
#% 1583921
#% 1587350
#% 1598417
#% 1642157
#% 1806041
#% 1879079
#! State-of-the-art content sharing platforms often require users to assign tags to pieces of media in order to make them easily retrievable. Since this task is sometimes perceived as tedious or boring, annotations can be sparse. Commenting on the other hand is a frequently used means of expressing user opinion towards shared media items. This work makes use of time series analyses in order to infer potential tags and indexing terms for audio-visual content from user comments. In this way, we mitigate the vocabulary gap between queries and document descriptors. Additionally, we show how large-scale encyclopaedias such as Wikipedia can aid the task of tag prediction by serving as surrogates for high-coverage natural language vocabulary lists. Our evaluation is conducted on a corpus of several million real-world user comments from the popular video sharing platform YouTube, and demonstrates significant improvements in retrieval performance.

#index 1967739
#* An evaluation of labelling-game data for video retrieval
#@ Riste Gligorov;Michiel Hildebrand;Jacco van Ossenbruggen;Lora Aroyo;Guus Schreiber
#t 2013
#c 16
#% 561315
#% 751818
#% 967325
#% 1019124
#% 1064154
#% 1343447
#% 1495421
#% 1587349
#% 1587350
#% 1589649
#% 1747878
#% 1879079
#! Games with a purpose (GWAPs) are increasingly used in audio-visual collections as a mechanism for annotating videos through tagging. This trend is driven by the assumption that user tags will improve video search. In this paper we study whether this is indeed the case. To this end, we create an evaluation dataset that consists of: (i) a set of videos tagged by users via video labelling game, (ii) a set of queries derived from real-life query logs, and (iii) relevance judgements. Besides user tags from the labelling game, we exploit the existing metadata associated with the videos (textual descriptions and curated in-house tags) and closed captions. Our findings show that search based on user tags alone outperforms search based on all other metadata types. Combining user tags with the other types of metadata yields an increase in search performance of 33%. We also find that the search performance of user tags steadily increases as more tags are collected.

#index 1967740
#* Multimodal re-ranking of product image search results
#@ Joyce M. dos Santos;João M. B. Cavalcanti;Patricia C. Saraiva;Edleno S. de Moura
#t 2013
#c 16
#% 406493
#% 413560
#% 592155
#% 1115943
#% 1121948
#% 1227639
#% 1231669
#% 1279857
#% 1412624
#% 1434052
#% 1484421
#% 1543160
#% 1560382
#% 1583883
#% 1587366
#% 1649123
#% 1857840
#! In this article we address the problem of searching for products using an image as query, instead of the more popular approach of searching by textual keywords. With the fast development of the Internet, the popularization of mobile devices and e-commerce systems, searching specific products by image has become an interesting research topic. In this context, Content-Based Image Retrieval (CBIR) techniques have been used to support and enhance the customer shopping experience. We propose an image re-ranking strategy based on multimedia information available on product databases. Our re-ranking strategy relies on category and textual information associated to the top-k images of an initial ranking computed purely with CBIR techniques. Experiments were carried out with users' relevance judgment on two image datasets collected from e-commerce Web sites. Our results show that our re-ranking strategy outperforms the baselines when using only CBIR techniques.

#index 1967741
#* Predicting information diffusion in social networks using content and user's profiles
#@ Cédric Lagnier;Ludovic Denoyer;Eric Gaussier;Patrick Gallinari
#t 2013
#c 16
#% 577217
#% 729923
#% 1077150
#% 1107420
#% 1214671
#% 1269888
#% 1333069
#% 1535333
#% 1613641
#! Predicting the diffusion of information on social networks is a key problem for applications like Opinion Leader Detection, Buzz Detection or Viral Marketing. Many recent diffusion models are direct extensions of the Cascade and Threshold models, initially proposed for epidemiology and social studies. In such models, the diffusion process is based on the dynamics of interactions between neighbor nodes in the network (the social pressure), and largely ignores important dimensions as the content of the piece of information diffused. We propose here a new family of probabilistic models that aims at predicting how a content diffuses in a network by making use of additional dimensions: the content of the piece of information diffused, user's profile and willingness to diffuse. These models are illustrated and compared with other approaches on two blog datasets. The experimental results obtained on these datasets show that taking into account the content of the piece of information diffused is important to accurately model the diffusion process.

#index 1967742
#* How tagging pragmatics influence tag sense discovery in social annotation systems
#@ Thomas Niebler;Philipp Singer;Dominik Benz;Christian Körner;Markus Strohmaier;Andreas Hotho
#t 2013
#c 16
#% 279755
#% 577285
#% 811281
#% 855601
#% 1152471
#% 1152476
#% 1215464
#% 1267489
#% 1281980
#% 1399985
#% 1409951
#% 1429421
#% 1586582
#% 1603812
#% 1655418
#% 1667787
#! The presence of emergent semantics in social annotation systems has been reported in numerous studies. Two important problems in this context are the induction of semantic relations among tags and the discovery of different senses of a given tag. While a number of approaches for discovering tag senses exist, little is known about which factors influence the discovery process. In this paper, we analyze the influence of user pragmatic factors. We divide taggers into different pragmatic distinctions. Based on these distinctions, we identify subsets of users whose annotations allow for a more precise and complete discovery of tag senses. Our results provide evidence for a link between tagging pragmatics and semantics and provide another argument for including pragmatic factors in semantic extraction methods. Our work is relevant for improving search, retrieval and browsing in social annotation systems, as well as for optimizing ontology learning algorithms based on tagging data.

#index 1967743
#* A unified framework for monolingual and cross-lingual relevance modeling based on probabilistic topic models
#@ Ivan Vulić;Marie-Francine Moens
#t 2013
#c 16
#% 232656
#% 262096
#% 280819
#% 280851
#% 340897
#% 340901
#% 397145
#% 507684
#% 579944
#% 722904
#% 740915
#% 750863
#% 879587
#% 879590
#% 1195831
#% 1297066
#% 1338620
#% 1417061
#% 1471293
#% 1536542
#% 1592228
#% 1697450
#! We explore the potential of probabilistic topic modeling within the relevance modeling framework for both monolingual and cross-lingual ad-hoc retrieval. Multilingual topic models provide a way to represent documents in a structured and coherent way, regardless of their actual language, by means of language-independent concepts, that is, cross-lingual topics. We show how to integrate the topical knowledge into a unified relevance modeling framework in order to build quality retrieval models in monolingual and cross-lingual contexts. The proposed modeling framework processes all documents uniformly and does not make any conceptual distinction between monolingual and cross-lingual modeling. Our results obtained from the experiments conducted on the standard CLEF test collections reveal that fusing the topical knowledge and relevance modeling leads to building monolingual and cross-lingual retrieval models that outperform several strong baselines. We show that that the topical knowledge coming from a general Web-generated corpus boosts retrieval scores. Additionally, we show that within this framework the estimation of cross-lingual relevance models may be performed by exploiting only a general non-parallel corpus.

#index 1967744
#* Semantic search log k-anonymization with generalized k-cores of query concept graph
#@ Claudio Carpineto;Giovanni Romano
#t 2013
#c 16
#% 78171
#% 576761
#% 956557
#% 1074112
#% 1190072
#% 1292623
#% 1328187
#% 1598371
#% 1648449
#% 1692264
#% 1879023
#! Search log k-anonymization is based on the elimination of infrequent queries under exact (or nearly exact) matching conditions, which usually results in a big data loss and impaired utility. We present a more flexible, semantic approach to k-anonymity that consists of three steps: query concept mining, automatic query expansion, and affinity assessment of expanded queries. Based on the observation that many infrequent queries can be seen as refinements of a more general frequent query, we first model query concepts as probabilistically weighted n-grams and extract them from the search log data. Then, after expanding the original log queries with their weighted concepts, we find all the k-affine expanded queries under a given affinity threshold Θ, modeled as a generalized k-core of the graph of Θ-affine queries. Experimenting with the AOL data set, we show that this approach achieves levels of privacy comparable to those of plain k-anonymity while at the same time reducing the data losses to a great extent.

#index 1967745
#* A joint classification method to integrate scientific and social networks
#@ Mahmood Neshati;Ehsaneddin Asgari;Djoerd Hiemstra;Hamid Beigy
#t 2013
#c 16
#% 464434
#% 937552
#% 1133171
#% 1498354
#% 1565539
#% 1587391
#% 1650403
#% 1673026
#% 1868015
#! In this paper, we address the problem of scientific-social network integration to find a matching relationship between members of these networks. Utilizing several name similarity patterns and contextual properties of these networks, we design a focused crawler to find high probable matching pairs, then the problem of name disambiguation is reduced to predict the label of each candidate pair as either true or false matching. By defining matching dependency graph, we propose a joint label prediction model to determine the label of all candidate pairs simultaneously. An extensive set of experiments have been conducted on six test collections obtained from the DBLP and the Twitter networks to show the effectiveness of the proposed joint label prediction model.

#index 1967746
#* Using document-quality measures to predict web-search effectiveness
#@ Fiana Raiber;Oren Kurland
#t 2013
#c 16
#% 268079
#% 280864
#% 309150
#% 340901
#% 340948
#% 397161
#% 818241
#% 838472
#% 879613
#% 879614
#% 907544
#% 987260
#% 987265
#% 1130851
#% 1263599
#% 1415713
#% 1450861
#% 1450964
#% 1467729
#% 1482276
#% 1536512
#% 1621236
#% 1879131
#! The query-performance prediction task is estimating retrieval effectiveness in the absence of relevance judgments. The task becomes highly challenging over theWeb due to, among other reasons, the effect of low quality (e.g., spam) documents on retrieval performance. To address this challenge, we present a novel prediction approach that utilizes queryindependent document-quality measures. While using these measures was shown to improve Web-retrieval effectiveness, this is the first study demonstrating the clear merits of using them for query-performance prediction. Evaluation performed with large scale Web collections shows that our methods post prediction quality that often surpasses that of state-of-the-art predictors, including those devised specifically for Web retrieval.

#index 1967747
#* Training efficient tree-based models for document ranking
#@ Nima Asadi;Jimmy Lin
#t 2013
#c 16
#% 209021
#% 400847
#% 411762
#% 565528
#% 1164190
#% 1292771
#% 1328061
#% 1355057
#% 1560393
#% 1598342
#% 1598344
#% 1604467
#% 1870957
#! Gradient-boosted regression trees (GBRTs) have proven to be an effective solution to the learning-to-rank problem. This work proposes and evaluates techniques for training GBRTs that have efficient runtime characteristics. Our approach is based on the simple idea that compact, shallow, and balanced trees yield faster predictions: thus, it makes sense to incorporate some notion of execution cost during training to "encourage" trees with these topological characteristics. We propose two strategies for accomplishing this: the first, by directly modifying the node splitting criterion during tree induction, and the second, by stagewise tree pruning. Experiments on a standard learning-to-rank dataset show that the pruning approach is superior; one balanced setting yields an approximately 40% decrease in prediction latency with minimal reduction in output quality as measured by NDCG.

#index 1967748
#* DTD based costs for tree-edit distance in structured information retrieval
#@ Cyril Laitang;Karen Pinel-Sauvagnat;Mohand Boughanem
#t 2013
#c 16
#% 289193
#% 327432
#% 411760
#% 492005
#% 498538
#% 547947
#% 826007
#% 879696
#% 940343
#% 1100802
#% 1310457
#% 1386500
#% 1401393
#% 1622389
#% 1622392
#% 1622397
#% 1674718
#% 1674740
#% 1733307
#! In this paper we present a Structured Information Retrieval (SIR) model based on graph matching. Our approach combines content propagation, which handles sibling relationships, with a document-query structure matching process. The latter is based on Tree-Edit Distance (TED) which is the minimum set of insert, delete, and replace operations to turn one tree to another. To our knowledge this algorithm has never been used in ad-hoc SIR. As the effectiveness of TED relies both on the input tree and the edit costs, we first present a focused subtree extraction technique which selects the most representative elements of the document w.r.t the query. We then describe our TED costs setting based on the Document Type Definition (DTD). Finally we discuss our results according to the type of the collection (data-oriented or text-oriented). Experiments are conducted on two INEX test sets: the 2010 Datacentric collection and the 2005 Ad-hoc one.

#index 1967749
#* Ranked accuracy and unstructured distributed search
#@ Sami Richardson;Ingemar J. Cox
#t 2013
#c 16
#% 411762
#% 446428
#% 730035
#% 879606
#% 987215
#% 1035578
#% 1095876
#% 1263571
#% 1292528
#% 1415769
#! Non-uniformly distributing documents in an unstructured peer-to-peer (P2P) network has been shown to improve both the expected search length and search accuracy, where accuracy is defined as the size of the intersection of the documents retrieved by a constrained, probabilistic search and the documents that would have been retrieved by an exhaustive search, normalized by the size of the latter. However neither metric considers the relative ranking of the documents in the retrieved sets. We therefore introduce a new performance metric, rank-accuracy, that is a rank weighted score of the top-k documents retrieved. By replicating documents across nodes based on their retrieval rate (a function of query frequency), and rank, we show that average rank-accuracy can be improved. The practical performance of rank-aware search is demonstrated using a simulated network of 10,000 nodes and queries drawn from a Yahoo! web search log.

#index 1967750
#* Learning to rank from structures in hierarchical text classification
#@ Qi Ju;Alessandro Moschitti;Richard Johansson
#t 2013
#c 16
#% 269217
#% 309141
#% 464434
#% 465747
#% 466078
#% 757423
#% 763699
#% 763708
#% 770763
#% 770796
#% 783478
#% 815896
#% 829975
#% 939353
#% 961135
#% 961192
#% 1055686
#% 1074128
#% 1227578
#% 1299522
#% 1450868
#% 1538188
#% 1583287
#% 1665151
#% 1913323
#! In this paper, we model learning to rank algorithms based on structural dependencies in hierarchical multi-label text categorization (TC). Our method uses the classification probability of the binary classifiers of a standard top-down approach to generate k-best hypotheses. The latter are generated according to their global probability while at the same time satisfy the structural constraints between father and children nodes. The rank is then refined using Support Vector Machines and tree kernels applied to a structural representation of hypotheses, i.e., a hierarchy tree in which the outcome of binary one-vs-all classifiers is directly marked in its nodes. Our extensive experiments on the whole Reuters Corpus Volume 1 show that our models significantly improve over the state of the art in TC, thanks to the use of structural dependecies.

#index 1967751
#* Folktale classification using learning to rank
#@ Dong Nguyen;Dolf Trieschnigg;Mariët Theune
#t 2013
#c 16
#% 194246
#% 413594
#% 575570
#% 815882
#% 838508
#% 995518
#% 1055317
#% 1130896
#% 1166531
#% 1498840
#% 1766487
#% 1798468
#% 1806008
#% 1966798
#! We present a learning to rank approach to classify folktales, such as fairy tales and urban legends, according to their story type, a concept that is widely used by folktale researchers to organize and classify folktales. A story type represents a collection of similar stories often with recurring plot and themes. Our work is guided by two frequently used story type classification schemes. Contrary to most information retrieval problems, the text similarity in this problem goes beyond topical similarity. We experiment with approaches inspired by distributed information retrieval and features that compare subject-verb-object triplets. Our system was found to be highly effective compared with a baseline system.

#index 1967752
#* Open-Set classification for automated genre identification
#@ Dimitrios A. Pritsos;Efstathios Stamatatos
#t 2013
#c 16
#% 722811
#% 891559
#% 1158264
#% 1226275
#% 1270768
#% 1348065
#% 1539248
#% 1558413
#% 1642242
#! Automated Genre Identification (AGI) of web pages is a problem of increasing importance since web genre (e.g. blog, news, e-shops, etc.) information can enhance modern Information Retrieval (IR) systems. The state-of-the-art in this field considers AGI as a closed-set classification problem where a variety of web page representation and machine learning models have intensively studied. In this paper, we study AGI as an open-set classification problem which better formulates the real world conditions of exploiting AGI in practice. Focusing on the use of content information, different text representation methods (words and character n-grams) are tested. Moreover, two classification methods are examined, one-class SVM learners, used as a baseline, and an ensemble of classifiers based on random feature subspacing, originally proposed for author identification. It is demonstrated that very high precision can be achieved in open-set AGI while recall remains relatively high.

#index 1967753
#* Semantic tagging of places based on user interest profiles from online social networks
#@ Vinod Hegde;Josiane Xavier Parreira;Manfred Hauswirth
#t 2013
#c 16
#% 57990
#% 860737
#% 954952
#% 1041604
#% 1055677
#% 1055704
#% 1070653
#% 1107141
#% 1132470
#% 1245937
#% 1263274
#% 1272267
#% 1454311
#% 1598366
#% 1605971
#% 1667202
#% 1810980
#! In recent years, location based services (LBS) have become very popular. The performance of LBS depends on number of factors including how well the places are described. Though LBS enable users to tag places, users rarely do so. On the other hand, users express their interests via online social networks. The common interests of a group of people that has visited a particular place can potentially provide further description for that place. In this work we present an approach that automatically assigns tags to places, based on interest profiles and visits or check-ins of users at places. We have evaluated our approach with real world datasets from popular social network services against a set of manually assigned tags. Experimental results show that we are able to derive meaningful tags for different places and that sets of tags assigned to places are expected to stabilise as more unique users visit places.

#index 1967754
#* Sponsored search ad selection by keyword structure analysis
#@ Kai Hui;Bin Gao;Ben He;Tie-jian Luo
#t 2013
#c 16
#% 864415
#% 915344
#% 950864
#% 1055677
#% 1127383
#% 1130910
#% 1190106
#% 1355052
#% 1399958
#% 1400033
#! In sponsored search, the ad selection algorithm is used to pick out the best candidate ads for ranking, the bid keywords of which are best matched to the user queries. Existing ad selection methods mainly focus on the relevance between user query and selected ads, and consequently the monetization ability of the results is not necessarily maximized. To this end, instead of making selection based on keywords as a whole, our work takes advantages of the different impacts, as revealed in our data study, of different components inside the keywords on both relevance and monetization ability. In particular, we select keyword components and then maximize the relevance and revenue on the component level. Finally, we combine the selected components to generate the bid keywords. The experiments reveal that our method can significantly outperform two baseline algorithms on the metrics including recall, precision and the monetization ability.

#index 1967755
#* Intent-Based browse activity segmentation
#@ Yury Ustinovskiy;Anna Mazur;Pavel Serdyukov
#t 2013
#c 16
#% 186340
#% 642985
#% 1055676
#% 1130868
#% 1130878
#% 1130921
#% 1355038
#% 1450884
#% 1450902
#% 1450946
#% 1598367
#% 1598368
#! Users search and browse activity mined with special toolbars is known to provide diverse valuable information for the search engine. In particular, it helps to understand information need of a searcher, her personal preferences, context of the topic she is currently interested in. Most of the previous studies on the topic either considered the whole user activity for a fixed period of time or divided it relying on some predefined inactivity time-out. It helps to identify groups of web sites visited with the same information need. This paper addresses the problem of automatic segmentation of users browsing logs into logical segments. We propose a method for automatic division of their daily activity into intent-related parts. This segmentation advances the commonly used approaches. We propose several methods for browsing log partitioning and provide detailed study of their performance. We evaluate all algorithms and analyse contributions of various types of features.

#index 1967756
#* Extracting event-related information from article updates in wikipedia
#@ Mihai Georgescu;Nattiya Kanhabua;Daniel Krause;Wolfgang Nejdl;Stefan Siersdorfer
#t 2013
#c 16
#% 197394
#% 262043
#% 279755
#% 729943
#% 818215
#% 889273
#% 956520
#% 987218
#% 1269107
#% 1400018
#% 1415780
#% 1434129
#% 1439717
#% 1482459
#% 1482473
#% 1558464
#% 1711868
#% 1971019
#% 1971020
#! Wikipedia is widely considered the largest and most up-to-date online encyclopedia, with its content being continuously maintained by a supporting community. In many cases, real-life events like new scientific findings, resignations, deaths, or catastrophes serve as triggers for collaborative editing of articles about affected entities such as persons or countries. In this paper, we conduct an in-depth analysis of event-related updates in Wikipedia by examining different indicators for events including language, meta annotations, and update bursts. We then study how these indicators can be employed for automatically detecting event-related updates. Our experiments on event extraction, clustering, and summarization show promising results towards generating entity-specific news tickers and timelines.

#index 1967757
#* Using wordnet hypernyms and dependency features for phrasal-level event recognition and type classification
#@ Yoonjae Jeong;Sung-Hyon Myaeng
#t 2013
#c 16
#% 350859
#% 722822
#% 855369
#% 939941
#% 1261542
#% 1408518
#% 1484322
#% 1567948
#! The goal of this research is to devise a method for recognizing and classifying TimeML events in a more effective way. TimeML is the most recent annotation scheme for processing the event and temporal expressions in natural language processing fields. In this paper, we argue and demonstrate that unit feature dependency information and deep-level WordNet hypernyms are useful for event recognition and type classification. The proposed method utilizes various features including lexical semantic and dependency-based combined features. The experimental results show that our proposed method outperforms a state-of-the-art approach, mainly due to the new strategies. Especially, the performance of noun and adjective events, which have been largely ignored and yet significant, is significantly improved.

#index 1967758
#* Aggregating evidence from hospital departments to improve medical records search
#@ Nut Limsopatham;Craig Macdonald;Iadh Ounis
#t 2013
#c 16
#% 280853
#% 342679
#% 397125
#% 397163
#% 411760
#% 722904
#% 766409
#% 783474
#% 801831
#% 879570
#% 907525
#% 1227604
#% 1392433
#% 1392444
#% 1392466
#% 1482336
#% 1852745
#% 1879144
#! Searching medical records is challenging due to their inherent implicit knowledge --- such knowledge may be known by medical practitioners, but it is hidden from an information retrieval (IR) system. For example, it is intuitive for a medical practitioner to assert that patients with heart disease are likely to have records from the hospital's cardiology department. Hence, we hypothesise that this implicit knowledge can be used to enhance a medical records search system that ranks patients based on the relevance of their medical records to a query. In this paper, we propose to group aggregates of medical records from individual hospital departments, which we refer to as department-level evidence, to capture some of the implicit knowledge. In particular, each department-level aggregate consists of all of the medical records created by a particular hospital department, which is then exploited to enhance retrieval effectiveness. Specifically, we propose two approaches to build the department-level evidence based on a federated search and a voting paradigm, respectively. In addition, we introduce an extended voting technique that could leverage this department-level evidence while ranking. We evaluate the retrieval effectiveness of our approaches in the context of the TREC 2011 Medical Records track. Our results show that modelling department-level evidence of records in medical records search improves retrieval effectiveness. In particular, our proposed approach to leverage department-level evidence built using a voting technique obtains results comparable to the best submitted TREC 2011 Medical Records track systems without requiring any external resources that are exploited in those systems.

#index 1967759
#* An n-gram topic model for time-stamped documents
#@ Shoaib Jameel;Wai Lam
#t 2013
#c 16
#% 287196
#% 577220
#% 722904
#% 788094
#% 868088
#% 875959
#% 876067
#% 881498
#% 1117083
#% 1195910
#% 1424114
#% 1536536
#% 1560380
#% 1605967
#% 1650390
#% 1688451
#% 1715168
#% 1913589
#! This paper presents a topic model that captures the temporal dynamics in the text data along with topical phrases. Previous approaches have relied upon bag-of-words assumption to model such property in a corpus. This has resulted in an inferior performance with less interpretable topics. Our topic model can not only capture changes in the way a topic structure changes over time but also maintains important contextual information in the text data. Finding topical n-grams, when possible based on context, instead of always presenting unigrams in topics does away with many ambiguities that individual words may carry. We derive a collapsed Gibbs sampler for posterior inference. Our experimental results show an improvement over the current state-of-the-art topics over time model.

#index 1967760
#* Influence of timeline and named-entity components on user engagement
#@ Yashar Moshfeghi;Michael Matthews;Roi Blanco;Joemon M. Jose
#t 2013
#c 16
#% 340883
#% 768285
#% 818221
#% 872204
#% 1019189
#% 1047347
#% 1252624
#% 1263597
#% 1268490
#% 1292475
#% 1314928
#% 1384641
#% 1482367
#% 1587350
#% 1598396
#% 1598406
#% 1598426
#! Nowadays, successful applications are those which contain features that captivate and engage users. Using an interactive news retrieval system as a use case, in this paper we study the effect of timeline and named-entity components on user engagement. This is in contrast with previous studies where the importance of these components were studied from a retrieval effectiveness point of view. Our experimental results show significant improvements in user engagement when named-entity and timeline components were installed. Further, we investigate if we can predict user-centred metrics through user's interaction with the system. Results show that we can successfully learn a model that predicts all dimensions of user engagement and whether users will like the system or not. These findings might steer systems that apply a more personalised user experience, tailored to the user's preferences.

#index 1967761
#* Cognitive temporal document priors
#@ Maria-Hendrike Peetz;Maarten de Rijke
#t 2013
#c 16
#% 262096
#% 730070
#% 1077150
#% 1130999
#% 1251781
#% 1587369
#% 1598383
#% 1598444
#% 1642157
#% 1806030
#% 1879083
#% 1924143
#! Temporal information retrieval exploits temporal features of document collections and queries. Temporal document priors are used to adjust the score of a document based on its publication time. We consider a class of temporal document priors that is inspired by retention functions considered in cognitive psychology that are used to model the decay of memory. Many such functions used as a temporal document prior have a positive effect on overall retrieval performance. We examine the stability of this effect across news and microblog collections and discover interesting differences between retention functions. We also study the problem of optimizing parameters of the retention functions as temporal document priors; some retention functions display consistent good performance across large regions of the parameter space. A retention function based on a Weibull distribution is the preferred choice for a temporal document prior.

#index 1967762
#* Combining recency and topic-dependent temporal variation for microblog search
#@ Taiki Miyanishi;Kazuhiro Seki;Kuniaki Uehara
#t 2013
#c 16
#% 340901
#% 730070
#% 750863
#% 960414
#% 1587369
#% 1598383
#% 1692327
#% 1787072
#% 1806030
#! The appearance of microblogging services has led to many short documents being issued by crowds of people. To retrieve useful information from among such a huge quantity of messages, query expansion (QE) is usually used to enrich a user query. Some QE methods for microblog search utilize temporal properties (e.g., recency and temporal variation) derived from the real-time characteristic that many messages are posted by users when an interesting event has recently occurred. Our approach leverages temporal properties for QE and combines them according to the temporal variation of a given topic. Experimental results show that this QE method using automatically combined temporal properties is effective at improving retrieval performance.

#index 1967763
#* Subjectivity annotation of the microblog 2011 realtime adhoc relevance judgments
#@ Georgios Paltoglou;Kevan Buckley
#t 2013
#c 16
#% 207677
#% 854646
#% 1497569
#% 1879168
#! In this work, we extend the Microblog dataset with subjectivity annotations. Our aim is twofold; first, we want to provide a high-quality, multiply-annotated gold standard of subjectivity annotations for the relevance assessments of the real-time adhoc task. Second, we randomly sample the rest of the dataset and annotate it for subjectivity once, in order to create a complementary annotated dataset that is at least an order of magnitude larger than the gold standard. As a result we have 2,389 tweets that have been annotated by multiple humans and 75,761 tweets that have been annotated by one annotator. We discuss issues like inter-annotator agreement, the time that it took annotators to classify tweets in correlation to their subjective content and lastly, the distribution of subjective tweets in relation to topic categorization. The annotated datasets and all relevant anonymised information are freely available for research purposes.

#index 1967764
#* Geo-spatial event detection in the twitter stream
#@ Maximilian Walther;Michael Kaisser
#t 2013
#c 16
#% 939897
#% 1301004
#% 1400018
#% 1426611
#% 1432574
#% 1470583
#% 1846488
#% 1846751
#% 1906935
#! The rise of Social Media services in the last years has created huge streams of information that can be very valuable in a variety of scenarios. What precisely these scenarios are and how the data streams can efficiently be analyzed for each scenario is still largely unclear at this point in time and has therefore created significant interest in industry and academia. In this paper, we describe a novel algorithm for geo-spatial event detection on Social Media streams. We monitor all posts on Twitter issued in a given geographic region and identify places that show a high amount of activity. In a second processing step, we analyze the resulting spatio-temporal clusters of posts with a Machine Learning component in order to detect whether they constitute real-world events or not. We show that this can be done with high precision and recall. The detected events are finally displayed to a user on a map, at the location where they happen and while they happen.

#index 1967765
#* A versatile tool for privacy-enhanced web search
#@ Avi Arampatzis;George Drosatos;Pavlos S. Efraimidis
#t 2013
#c 16
#% 158687
#% 340146
#% 465754
#% 576761
#% 816185
#% 878624
#% 1077150
#% 1587346
#! We consider the problem of privacy leaks suffered by Internet users when they perform web searches, and propose a framework to mitigate them. Our approach, which builds upon and improves recent work on search privacy, approximates the target search results by replacing the private user query with a set of blurred or scrambled queries. The results of the scrambled queries are then used to cover the original user interest. We model the problem theoretically, define a set of privacy objectives with respect to web search and investigate the effectiveness of the proposed solution with a set of real queries on a large web collection. Experiments show great improvements in retrieval effectiveness over a previously reported baseline in the literature. Furthermore, the methods are more versatile, predictably-behaved, applicable to a wider range of information needs, and the privacy they provide is more comprehensible to the end-user.

#index 1967766
#* Exploiting novelty and diversity in tag recommendation
#@ Fabiano Belém;Eder Martins;Jussara Almeida;Marcos Gonçalves
#t 2013
#c 16
#% 387427
#% 974033
#% 987242
#% 1055704
#% 1127458
#% 1190091
#% 1450853
#% 1450855
#% 1598437
#% 1625357
#% 1921853
#! The design and evaluation of tag recommendation methods have focused only on relevance. However, other aspects such as novelty and diversity may be as important to evaluate the usefulness of the recommendations. In this work, we define these two aspects in the context of tag recommendation and propose a novel recommendation strategy that considers them jointly with relevance. This strategy extends a state-of-the-art method based on Genetic Programming to include novelty and diversity metrics both as attributes and as part of the objective function. We evaluate the proposed strategy using data collected from 3 popular Web 2.0 applications: LastFM, YouTube and YahooVideo. Our experiments show that our strategy outperforms the state-of-the-art alternative in terms of novelty and diversity, without harming relevance.

#index 1967767
#* Example based entity search in the web of data
#@ Marc Bron;Krisztian Balog;Maarten de Rijke
#t 2013
#c 16
#% 733833
#% 754095
#% 1166534
#% 1206910
#% 1263248
#% 1292565
#% 1338581
#% 1400010
#% 1400146
#% 1409952
#% 1475756
#% 1475763
#% 1482286
#% 1489451
#% 1489452
#% 1536584
#% 1641483
#% 1642135
#% 1693918
#% 1746811
#% 1806002
#% 1879005
#! The scale of today's Web of Data motivates the use of keyword search-based approaches to entity-oriented search tasks in addition to traditional structure-based approaches, which require users to have knowledge of the underlying schema. We propose an alternative structure-based approach that makes use of example entities and compare its effectiveness with a text-based approach in the context of an entity list completion task. We find that both the text and structure-based approaches are effective in retrieving relevant entities, but that they find different sets of entities. Additionally, we find that the performance of the structure-based approach is dependent on the quality and number of example entities given. We experiment with a number of hybrid techniques that balance between the two approaches and find that a method that uses the example entities to determine the weights of approaches in the combination on a per query basis is most effective.

#index 1967768
#* A fast generative spell corrector based on edit distance
#@ Ishan Chattopadhyaya;Kannappan Sirchabesan;Krishanu Seal
#t 2013
#c 16
#% 312072
#% 322309
#% 322884
#% 324015
#% 1206985
#% 1481657
#% 1484336
#! One of the main challenges in the implementation of web-scale online search systems is the disambiguation of the user input when portions of the input queries are possibly misspelt. Spell correctors that must be integrated with such systems have very stringent restrictions imposed on them; primarily they must possess the ability to handle large volume of concurrent queries and generate relevant spelling suggestions at a very high speed. Often, these systems consist of highend server machines with lots of memory and processing power and the requirement from such spell correctors is to minimize the latency of generating suggestions to a bare minimum. In this paper, we present a spell corrector that we developed to cater to high volume incoming queries for an online search service. It consists of a fast, per-token candidate generator which generates spell suggestions within a distance of two edit operations of an input token. We compare its performance against an n-gram based spell corrector and show that the presented spell candidate generation approach has lower response times.

#index 1967769
#* Being confident about the quality of the predictions in recommender systems
#@ Sergio Cleger-Tamayo;Juan M. Fernández-Luna;Juan F. Huete;Nava Tintarev
#t 2013
#c 16
#% 136350
#% 319705
#% 411762
#% 420539
#% 452664
#% 733577
#% 734590
#% 860672
#% 988200
#% 1001279
#% 1001313
#% 1128905
#% 1386007
#% 1450855
#% 1541728
#% 1625363
#% 1651923
#% 1756046
#% 1872523
#% 1872525
#% 1879143
#! Recommender systems suggest new items to users to try or buy based on their previous preferences or behavior. Many times the information used to recommend these items is limited. An explanation such as"I believe you will like this item, but I do not have enough information to be fully confident about it." may mitigate the issue, but can also damage user trust because it alerts users to the fact that the system might be wrong. The findings in this paper suggest that there is a way of modelling recommendation confidence that is related to accuracy (MAE, RMSE and NDCG) and user rating behaviour (rated vs unrated items). In particular, it was found that unrated items have lower confidence compared to the entire item set - highlighting the importance of explanations for novel but risky recommendations.

#index 1967770
#* Two-Stage learning to rank for information retrieval
#@ Van Dang;Michael Bendersky;W. Bruce Croft
#t 2013
#c 16
#% 734915
#% 818262
#% 840846
#% 976952
#% 987231
#% 987356
#% 1019139
#% 1195836
#% 1227634
#% 1227635
#% 1268491
#% 1338596
#% 1355019
#% 1442574
#% 1442577
#% 1536512
#% 1598394
#% 1693906
#! Current learning to rank approaches commonly focus on learning the best possible ranking function given a small fixed set of documents. This document set is often retrieved from the collection using a simple unsupervised bag-of-words method, e.g. BM25. This can potentially lead to learning a sub-optimal ranking, since many relevant documents may be excluded from the initially retrieved set. In this paper we propose a novel two-stage learning framework to address this problem. We first learn a ranking function over the entire retrieval collection using a limited set of textual features including weighted phrases, proximities and expansion terms. This function is then used to retrieve the best possible subset of documents over which the final model is trained using a larger set of query- and document-dependent features. Empirical evaluation using two web collections unequivocally demonstrates that our proposed two-stage framework, being able to learn its model from more relevant documents, outperforms current learning to rank approaches.

#index 1967771
#* Hybrid query scheduling for a replicated search engine
#@ Ana Freire;Craig Macdonald;Nicola Tonellotto;Iadh Ounis;Fidel Cacheda
#t 2013
#c 16
#% 213786
#% 730035
#% 730065
#% 976948
#% 981572
#% 985830
#% 1166469
#% 1173690
#% 1417245
#% 1598342
#% 1642922
#% 1879054
#% 1879181
#! Search engines use replication and distribution of large indices across many query servers to achieve efficient retrieval. Under high query load, queries can be scheduled to replicas that are expected to be idle soonest, facilitated by the use of predicted query response times. However, the overhead of making response time predictions can hinder the usefulness of query scheduling under low query load. In this paper, we propose a hybrid scheduling approach that combines the scheduling methods appropriate for both low and high load conditions, and can adapt in response to changing conditions. We deploy a simulation framework, which is prepared with actual and predicted response times for real Web search queries for one full day. Our experiments using different numbers of shards and replicas of the 50 million document ClueWeb09 corpus show that hybrid scheduling can reduce the average waiting times of one day of queries by 68% under high load conditions and by 7% under low load conditions w.r.t. traditional scheduling methods.

#index 1967772
#* Latent factor blockmodel for modelling relational data
#@ Sheng Gao;Ludovic Denoyer;Patrick Gallinari;Jun Guo
#t 2013
#c 16
#% 989573
#% 989618
#% 1117695
#% 1312988
#% 1642050
#% 1826335
#! In this paper we address the problem of modelling relational data, which has appeared in many applications such as social network analysis, recommender systems and bioinformatics. Previous studies either consider latent feature based models to do link prediction in the relational data but disregarding local structure in the network, or focus exclusively on capturing network structure of objects based on latent blockmodels without coupling with latent characteristics of objects to avoid redundant information. To combine the benefits of the previous work, we model the relational data as a function of both latent feature factors and latent cluster memberships of objects via our proposed Latent Factor BlockModel (LFBM) to collectively discover globally predictive intrinsic properties of objects and capture the latent block structure. We also develop an optimization transfer algorithm to learn the latent factors. Extensive experiments on the synthetic data and several real world datasets suggest that our proposed LFBM model outperforms the state-of-the-art approaches for modelling the relational data.

#index 1967773
#* Estimation of the collection parameter of information models for IR
#@ Parantapa Goswami;Eric Gaussier
#t 2013
#c 16
#% 262096
#% 340948
#% 766412
#% 1343447
#% 1450858
#% 1558081
#% 1806011
#! In this paper we explore various methods to estimate the collection parameter of the information based models for ad hoc information retrieval. In previous studies, this parameter was set to the average number of documents where the word under consideration appears. We introduce here a fully formalized estimation method for both the log-logistic and the smoothed power law models that leads to improved versions of these models in IR. Furthermore, we show that the previous setting of the collection parameter of the log-logistic model is a special case of the estimated value proposed here.

#index 1967774
#* Increasing stability of result organization for session search
#@ Dongyi Guan;Hui Yang
#t 2013
#c 16
#% 118771
#% 280849
#% 340951
#% 754124
#% 1227594
#% 1450850
#% 1641980
#% 1693884
#% 1913686
#! Search result clustering (SRC) organizes search results into labeled hierarchical structures as an "information lay-of-land", providing users an overview and helping them quickly locate relevant information from piles of search results. Hierarchies built by this process are usually sensitive to query changes. For search sessions with multiple queries, this could be undesirable since it may leave users a seemly random overview and partly diminish the benefits that SRC intents to offer. We propose to integrate external knowledge from Wikipedia when building concept hierarchies to boost their stability for session queries. Our evaluations on both TREC 2010 and 2011 Session tracks demonstrate that the proposed approaches outperform the state-of-the-art hierarchy construction algorithms in stability of search results organization.

#index 1967775
#* Updating users about time critical events
#@ Qi Guo;Fernando Diaz;Elad Yom-Tov
#t 2013
#c 16
#% 340883
#% 879636
#% 1272053
#% 1328326
#% 1482206
#% 1516569
#% 1544449
#% 1561558
#% 1598372
#! During unexpected events such as natural disasters, individuals rely on the information generated by news outlets to form their understanding of these events. This information, while often voluminous, is frequently degraded by the inclusion of unimportant, duplicate, or wrong information. It is important to be able to present users with only the novel, important information about these events as they develop. We present the problem of updating users about time critical news events, and focus on the task of deciding which information to select for updating users as an event develops. We propose a solution to this problem which incorporates techniques from information retrieval and multi-document summarization and evaluate this approach on a set of historic events using a large stream of news documents. We also introduce an evaluation method which is significantly less expensive than traditional approaches to temporal summarization.

#index 1967776
#* Comparing crowd-based, game-based, and machine-based approaches in initial query and query refinement tasks
#@ Christopher G. Harris;Padmini Srinivasan
#t 2013
#c 16
#% 84396
#% 169779
#% 262043
#% 313719
#% 329090
#% 438557
#% 465895
#% 642985
#% 643001
#% 766409
#% 1065266
#% 1252610
#% 1252613
#% 1432722
#% 1526565
#% 1598368
#% 1598550
#% 1666684
#% 1746898
#% 1893300
#! Human computation techniques have demonstrated their ability to accomplish portions of tasks that machine-based techniques find difficult. Query refinement is a task that may benefit from human involvement. We conduct an experiment that evaluates the contributions of two user types: student participants and crowdworkers hired from an online labor market. Human participants are assigned to use one of two query interfaces: a traditional web-based interface or a game-based interface. We ask each group to manually construct queries to respond to TREC information needs and calculate their resulting recall and precision. Traditional web interface users are provided feedback on their initial queries and asked to use this information to reformulate their original queries. Game interface users are provided with instant scoring and ask to refine their queries based on their scores. We measure the resulting feedback-based improvement on each group and compare the results from human computation techniques to machine-based algorithms.

#index 1967777
#* Reducing the uncertainty in resource selection
#@ Ilya Markov;Leif Azzopardi;Fabio Crestani
#t 2013
#c 16
#% 194246
#% 280856
#% 340146
#% 397125
#% 398894
#% 643012
#% 643021
#% 879607
#% 879643
#% 985828
#% 987230
#% 987260
#% 1174737
#% 1195830
#% 1227591
#% 1227616
#% 1227629
#% 1292595
#% 1348342
#% 1392444
#% 1457114
#% 1565813
#% 1697469
#! The distributed retrieval process is plagued by uncertainty. Sampling, selection, merging and ranking are all based on very limited information compared to centralized retrieval. In this paper, we focus our attention on reducing the uncertainty within the resource selection phase by obtaining a number of estimates, rather than relying upon only one point estimate. We propose three methods for reducing uncertainty which are compared against state-of-the-art baselines across three distributed retrieval testbeds. Our results show that the proposed methods significantly improve baselines, reduce the uncertainty and improve robustness of resource selection.

#index 1967778
#* Exploiting time in automatic image tagging
#@ Philip J. McParlane;Joemon M. Jose
#t 2013
#c 16
#% 198058
#% 318785
#% 397145
#% 443413
#% 457912
#% 575570
#% 577220
#% 642989
#% 975105
#% 1055704
#% 1127458
#% 1152471
#% 1214671
#% 1406345
#% 1434079
#% 1447729
#% 1598523
#% 1688104
#% 1697432
#! Existing automatic image annotation (AIA) models that depend solely on low-level image features often produce poor results, particularly when annotating real-life collections. Tag co-occurrence has been shown to improve image annotation by identifying additional keywords associated with user-provided keywords. However, existing approaches have treated tag co-occurrence as a static measure over time, thereby ignoring the temporal trends of many tags. The temporal distribution of tags, however, caused by events, seasons, memes, etc. provide a strong source of evidence beyond keywords for AIA. In this paper we propose a temporal tag co-occurrence approach to improve upon the current state-of-the-art automatic image annotation model. By replacing the annotated tags with more temporally significant tags, we achieve statistically significant increases to annotation accuracy on a real-life timestamped image collection from Flickr.

#index 1967779
#* Using text-based web image search results clustering to minimize mobile devices wasted space-interface
#@ Jose G. Moreno;Gaël Dias
#t 2013
#c 16
#% 262045
#% 780778
#% 780874
#% 860086
#% 954969
#% 1034802
#% 1131924
#% 1190135
#% 1203767
#% 1213625
#% 1266762
#% 1450850
#% 1632441
#% 1649241
#% 1693884
#% 1715673
#% 1806022
#! The recent shift in human-computer interaction from desktop to mobile computing fosters the needs of new interfaces for web image search results exploration. In order to leverage users' efforts, we present a set of state-of-the-art ephemeral clustering algorithms, which allow to summarize web image search results into meaningful clusters. This way of presenting visual information on mobile devices is exhaustively evaluated based on two main criteria: clustering accuracy, which must be maximized, and wasted space-interface, which must be minimized. For the first case, we use a broad set of metrics to evaluate ephemeral clustering over a public golden standard data set of web images. For the second case, we propose a new metric to evaluate the mismatch of the used space-interface between the ground truth and the cluster distribution obtained by ephemeral clustering. The results evidence that there exist high divergences between clustering accuracy and used space maximization. As a consequence, the trade-off of cluster-based exploration of web image search results on mobile devices is difficult to define, although our study evidences some clear positive results.

#index 1967780
#* Discovery and analysis of evolving topical social discussions on unstructured microblogs
#@ Kanika Narang;Seema Nagar;Sameep Mehta;L. V. Subramaniam;Kuntal Dey
#t 2013
#c 16
#% 319244
#% 465914
#% 748499
#% 748600
#% 783633
#% 869480
#% 1227763
#% 1275285
#% 1292506
#% 1358030
#% 1399992
#% 1746831
#% 1747096
#! Social networks have emerged as hubs of user generated content. Online social conversations can be used to retrieve users interests towards given topics and trends. Microblogging platforms like Twitter are primary examples of social networks with significant volumes of topical message exchanges between users. However, unlike traditional online discussion forums, blogs and social networking sites, explicit discussion threads are absent from microblogging networks like Twitter. This inherent absence of any conversation framework makes it challenging to distinguish conversations from mere topical interests. In this work, we explore semantic, social and temporal relationships of topical clusters formed in Twitter to identify conversations. We devise an algorithm comprising of a sequence of steps such as text clustering, topical similarity detection using TF-IDF and Wordnet, and intersecting social, semantic and temporal graphs to discover social conversations around topics. We further qualitatively show the presence of social localization of discussion threads. Our results suggest that discussion threads evolve significantly over social networks on Twitter. Our algorithm to find social discussion threads can be used for settings such as social information spreading applications and information diffusion analyses on microblog networks.

#index 1967781
#* Web credibility: features exploration and credibility prediction
#@ Alexandra Olteanu;Stanislav Peshterliev;Xin Liu;Karl Aberer
#t 2013
#c 16
#% 272764
#% 571351
#% 761334
#% 813966
#% 989505
#% 1016177
#% 1035587
#% 1281981
#% 1560422
#% 1573488
#% 1573489
#% 1711595
#% 1806009
#! The open nature of the World Wide Web makes evaluating webpage credibility challenging for users. In this paper, we aim to automatically assess web credibility by investigating various characteristics of webpages. Specifically, we first identify features from textual content, link structure, webpages design, as well as their social popularity learned from popular social media sites (e.g., Facebook, Twitter). A set of statistical analyses methods are applied to select the most informative features, which are then used to infer webpages credibility by employing supervised learning algorithms. Real dataset-based experiments under two application settings show that we attain an accuracy of 75% for classification, and an improvement of 53% for the mean absolute error (MAE), with respect to the random baseline approach, for regression.

#index 1967782
#* Query suggestions for textual problem solution repositories
#@ P. Deepak;Sutanu Chakraborti;Deepak Khemani
#t 2013
#c 16
#% 81669
#% 348155
#% 838398
#% 838531
#% 869501
#% 879610
#% 987372
#% 1019124
#% 1019146
#% 1074110
#% 1083721
#% 1099644
#% 1109955
#% 1130879
#% 1173699
#% 1400023
#% 1591994
#% 1598413
#% 1642244
#! Textual problem-solution repositories are available today in various forms, most commonly as problem-solution pairs from community question answering systems. Modern search engines that operate on the web can suggest possible completions in real-time for users as they type in queries. We study the problem of generating intelligent query suggestions for users of customized search systems that enable querying over problem-solution repositories. Due to the small scale and specialized nature of such systems, we often do not have the luxury of depending on query logs for finding query suggestions. We propose a retrieval model for generating query suggestions for search on a set of problem solution pairs. We harness the problem solution partition inherent in such repositories to improve upon traditional query suggestion mechanisms designed for systems that search over general textual corpora. We evaluate our technique over real problem-solution datasets and illustrate that our technique provides large and statistically significant improvements over the state-of-the-art technique in query suggestion.

#index 1967783
#* Improving ESA with document similarity
#@ Tamara Polajnar;Nitish Aggarwal;Kartik Asooja;Paul Buitelaar
#t 2013
#c 16
#% 228088
#% 342617
#% 430757
#% 722904
#% 743284
#% 783474
#% 876017
#% 1033867
#% 1275012
#% 1496353
#% 1642154
#% 1911149
#! Explicit semantic analysis (ESA) is a technique for computing semantic relatedness between natural language texts. It is a document-based distributional model similar to latent semantic analysis (LSA), which is often built on the Wikipedia database when it is required for general English usage. Unlike LSA, however, ESA does not use dimensionality reduction, and therefore it is sometimes unable to account for similarity between words that do not co-occur with same concepts, even if their concepts themselves cover similar subjects. In the Wikipedia implementation ESA concepts are Wikipedia articles, and the Wikilinks between the articles are used to overcome the concept-similarity problem. In this paper, we provide two general solutions for integration of concept-concept similarities into the ESA model, ones that do not rely on a particular corpus structure and do not alter the explicit concept-mapping properties that distinguish ESA from models like LSA and latent Dirichlet allocation (LDA).

#index 1967784
#* Ontology-Based word sense disambiguation for scientific literature
#@ Roman Prokofyev;Gianluca Demartini;Alexey Boyarsky;Oleg Ruchayskiy;Philippe Cudré-Mauroux
#t 2013
#c 16
#% 278098
#% 741423
#% 854641
#% 1131827
#% 1598410
#% 1642243
#% 1681357
#% 1730789
#! Scientific documents often adopt a well-defined vocabulary and avoid the use of ambiguous terms. However, as soon as documents from different research sub-communities are considered in combination, many scientific terms become ambiguous as the same term can refer to different concepts from different sub-communities. The ability to correctly identify the right sense of a given term can considerably improve the effectiveness of retrieval models, and can also support additional features such as search diversification. This is even more critical when applied to explorative search systems within the scientific domain. In this paper, we propose novel semi-supervised methods to term disambiguation leveraging the structure of a community-based ontology of scientific concepts. Our approach exploits the graph structure that connects different terms and their definitions to automatically identify the correct sense that was originally picked by the authors of a scientific publication. Experimental evidence over two different test collections from the physics and biomedical domains shows that the proposed method is effective and outperforms state-of-the-art approaches based on feature vectors constructed out of term co-occurrences as well as standard supervised approaches.

#index 1967785
#* A language modeling approach for extracting translation knowledge from comparable corpora
#@ Razieh Rahimi;Azadeh Shakery
#t 2013
#c 16
#% 218989
#% 643038
#% 746870
#% 748574
#% 814859
#% 823405
#% 848151
#% 919705
#% 938719
#% 1166534
#% 1249539
#% 1260666
#% 1471855
#% 1484313
#% 1592227
#% 1748109
#% 1906022
#% 1949157
#! A main challenge in Cross-Language information retrieval is to estimate a translation language model, as its quality directly affects the retrieval performance. The translation language model is built using translation resources such as bilingual dictionaries, parallel corpora, or comparable corpora. In general, high quality resources may not be available for scarce-resource languages. For these languages, efficient exploitation of commonly available resources such as comparable corpora is considered more crucial. In this paper, we focus on using only comparable corpora to extract translation information more efficiently. We propose a language modeling approach for estimating the translation language model. The proposed method is based on probability distribution estimation, and can be tuned easier in comparison with heuristically adjusted previous work. Experiment results show a significant improvement in the translation quality and CLIR performance compared to the previous approaches.

#index 1967786
#* Content-Based re-ranking of text-based image search results
#@ Franck Thollard;Georges Quénot
#t 2013
#c 16
#% 724559
#% 740763
#% 836904
#% 1119135
#% 1131921
#% 1432286
#% 1464094
#% 1494452
#% 1555363
#% 1560382
#% 1587366
#% 1750558
#% 1885636
#! This article presents a method for re-ranking images retrieved by classical search engine using key words for entering queries. This method uses the visual content of the images and it is based on the idea that the relevant images should be similar to each other while the non-relevant images should be different from each other and from relevant images. This idea has been implemented by ranking the images according to their average distances to their nearest neighbors. This query-dependent re-ranking is completed by a query-independent re-ranking taking into account the fact that some types of images are non-relevant for almost all queries. This idea is implemented by training a classifier on results from all queries in the training set. The re-ranking is successfully evaluated on classical datasets built with ExaleadTM and Google ImagesTM search engines.

#index 1967787
#* Encoding local binary descriptors by bag-of-features with hamming distance for visual object categorization
#@ Yu Zhang;Chao Zhu;Stephane Bres;Liming Chen
#t 2013
#c 16
#% 443991
#% 544022
#% 760805
#% 812418
#% 836877
#% 987226
#% 1058303
#% 1148301
#% 1464094
#% 1495388
#% 1495433
#% 1558464
#% 1752196
#! This paper presents a novel method for encoding local binary descriptors for Visual Object Categorization (VOC). Nowadays, local binary descriptors, e.g. LBP and BRIEF, have become very popular in image matching tasks because of their fast computation and matching using binary bitstrings. However, the bottleneck of applying them in the domain of VOC lies in the high dimensional histograms produced by encoding these binary bitstrings into decimal codes. To solve this problem, we propose to encode local binary bitstrings directly by the Bag-of-Features (BoF) model with Hamming distance. The advantages of this approach are two-fold: (1) It solves the high dimensionality issue of the traditional binary bitstring encoding methods, making local binary descriptors more feasible for the task of VOC, especially when more bits are considered; (2) It is computationally efficient because the Hamming distance, which is very suitable for comparing bitstrings, is based on bitwise XOR operations that can be fast computed on modern CPUs. The proposed method is validated by applying on LBP feature for the purpose of VOC. The experimental results on the PASCAL VOC 2007 benchmark show that our approach effectively improves the recognition accuracy compared to the traditional LBP feature.

#index 1967788
#* Recommending high utility query via session-flow graph
#@ Xiaofei Zhu;Jiafeng Guo;Xueqi Cheng;Yanyan Lan;Wolfgang Nejdl
#t 2013
#c 16
#% 310567
#% 330617
#% 387427
#% 411762
#% 591792
#% 869501
#% 869651
#% 987222
#% 1035578
#% 1074193
#% 1130854
#% 1130868
#% 1130879
#% 1270276
#% 1355032
#% 1400017
#% 1482240
#% 1560358
#% 1598368
#% 1598414
#% 1919823
#! Query recommendation is an integral part of modern search engines that helps users find their information needs. Traditional query recommendation methods usually focus on recommending users relevant queries, which attempt to find alternative queries with close search intent to the original query. Whereas the ultimate goal of query recommendation is to assist users to accomplish their search task successfully, while not just find relevant queries in spite of they can sometimes return useful search results. To better achieve the ultimate goal of query recommendation, a more reasonable way is to recommend users high utility queries, i.e., queries that can return more useful information. In this paper, we propose a novel utility query recommendation approach based on absorbing random walk on the session-flow graph, which can learn queries' utility by simultaneously modeling both users' reformulation behaviors and click behaviors. Extensively experiments were conducted on real query logs, and the results show that our method significantly outperforms the state-of-the-art methods under the evaluation metric QRR and MRD.

#index 1967789
#* URL redirection accounting for improving link-based ranking methods
#@ Maksim Zhukovskii;Gleb Gusev;Pavel Serdyukov
#t 2013
#c 16
#% 268079
#% 758290
#% 1450843
#! Traditional link-based web ranking algorithms are applied to web snapshots in the form of webgraphs consisting of pages as vertices and links as edges. Constructing webgraph, researchers do not pay attention to a particular method of how links are taken into account, while certain details may significantly affects the contribution of link-based factors to ranking. Furthermore, researchers use small subgraphs of the webgraph for more efficient evaluation of new algorithms. They usually consider a graph induced by pages, for example, of a certain first level domain. In this paper we reveal a significant dependence of PageRank on the method of accounting redirects while constructing the webgraph. We evaluate several natural ways of redirect accounting on a large-scale domain and find an optimal case, which turns out non-trivial. Moreover, we experimentally compare different ways of extracting a small subgraph for multiple evaluations and reveal some essential shortcomings of traditional approaches.

#index 1967790
#* Lo mejor de dos idiomas: cross-lingual linkage of geotagged wikipedia articles
#@ Dirk Ahlers
#t 2013
#c 16
#% 907386
#% 1925702
#! Different language versions of Wikipedia contain articles referencing the same place. However, an article in one language does not necessarily mean it is available in another language as well and linked to. This paper examines geotagged articles describing places in Honduras in both the Spanish and the English language versions. It demonstrates that a method based on simple features can reliably identify article pairs describing the same semantic place concept and evaluates it against the existing interlinks as well as a manual assessment.

#index 1967791
#* A pilot study on using profile-based summarisation for interactive search assistance
#@ Azhar Alhindi;Udo Kruschwitz;Chris Fox
#t 2013
#c 16
#% 787502
#% 816173
#% 992317
#% 1019116
#% 1164367
#% 1268490
#% 1396086
#% 1622353
#% 1711847
#! Text summarisation is the process of distilling the most important information from a source to produce an abridged version for a particular user or task. This poster investigates the use of profile-based summarisation to provide contextualisation and interactive support for enterprise searches. We employ log analysis to acquire continuously updated profiles to provide profile-based summarisations of search results. These profiles could be capturing an individual's interests or (as discussed here) those of a group of users. Here we report on a first pilot study.

#index 1967792
#* Exploring patent passage retrieval using nouns phrases
#@ Linda Andersson;Parvaz Mahdabi;Allan Hanbury;Andreas Rauber
#t 2013
#c 16
#% 816186
#% 1131208
#% 1879043
#! This paper presents experiments which initially were carried out for the Patent Passage Retrieval track of CLEF-IP 2012. The Passage Retrieval module was implemented independently of the Document Retrieval system. In the Passage Retrieval module we make use of Natural Language Processing applications (WordNet and Stanford Part-of-Speech tagger) for lemmatization and phrase (multi word units) retrieval. We show by applying simple rule-based modifications and only targeting specific language instances (noun phrases) the usage of general NLP tools for phrase retrieval will increase performance of a Patent Passage Information Extraction system.

#index 1967793
#* Characterizing health-related community question answering
#@ Alexander Beloborodov;Artem Kuznetsov;Pavel Braslavski
#t 2013
#c 16
#% 1491092
#% 1598340
#% 1598375
#! Our ongoing project is aimed at improving information access to narrow-domain collections of questions and answers. This poster demonstrates how out-of-the-box tools and domain dictionaries can be applied to community question answering (CQA) content in health domain. This approach can be used to improve user interfaces and search over CQA data, as well as to evaluate content quality. The study is a first-time use of a sizable dataset from the Russian CQA site Otvety@Mail.Ru.

#index 1967794
#* Topic models can improve domain term extraction
#@ Elena Bolshakova;Natalia Loukachevitch;Michael Nokel
#t 2013
#c 16
#% 722904
#% 1464105
#% 1913656
#! The paper describes the results of an experimental study of topic models applied to the task of single-word term extraction. The experiments encompass several probabilistic and non-probabilistic topic models and demonstrate that topic information improves the quality of term extraction, as well as NMF with KL-divergence minimization is the best among the models under study.

#index 1967795
#* A topic person multi-polarization method using friendship network analysis
#@ Zhong-Yong Chen;Chien Chin Chen
#t 2013
#c 16
#% 466675
#% 1077150
#% 1261566
#! In this paper, we leverage competing viewpoints of the persons mentioned in a set of topic documents. We propose a method to construct a friendship network of the persons and present a graph-partition based multi-polarization algorithm to group the persons into clusters with competing viewpoints.

#index 1967796
#* Improving cyberbullying detection with user context
#@ Maral Dadvar;Dolf Trieschnigg;Roeland Ordelman;Franciska de Jong
#t 2013
#c 16
#% 1930663
#! The negative consequences of cyberbullying are becoming more alarming every day and technical solutions that allow for taking appropriate action by means of automated detection are still very limited. Up until now, studies on cyberbullying detection have focused on individual comments only, disregarding context such as users' characteristics and profile information. In this paper we show that taking user context into account improves the detection of cyberbullying.

#index 1967797
#* Snippet-Based relevance predictions for federated web search
#@ Thomas Demeester;Dong Nguyen;Dolf Trieschnigg;Chris Develder;Djoerd Hiemstra
#t 2013
#c 16
#% 1292595
#% 1565813
#% 1919835
#! How well can the relevance of a page be predicted, purely based on snippets? This would be highly useful in a Federated Web Search setting where caching large amounts of result snippets is more feasible than caching entire pages. The experiments reported in this paper make use of result snippets and pages from a diverse set of actual Web search engines. A linear classifier is trained to predict the snippet-based user estimate of page relevance, but also, to predict the actual page relevance, again based on snippets alone. The presented results confirm the validity of the proposed approach and provide promising insights into future result merging strategies for a Federated Web Search setting.

#index 1967798
#* Designing human-readable user profiles for search evaluation
#@ Carsten Eickhoff;Kevyn Collins-Thompson;Paul Bennett;Susan Dumais
#t 2013
#c 16
#% 262107
#% 507691
#% 642999
#% 818259
#% 1399944
#% 1641961
#! Forming an accurate mental model of a user is crucial for the qualitative design and evaluation steps of many information-centric applications such as web search, content recommendation, or advertising. This process can often be time-consuming as search and interaction histories become verbose. In this work, we present and analyze the usefulness of concise human-readable user profiles in order to enhance system tuning and evaluation by means of user studies.

#index 1967799
#* Sentiment classification based on phonetic characteristics
#@ Sergei Ermakov;Liana Ermakova
#t 2013
#c 16
#% 528649
#% 1127964
#% 1130988
#% 1261565
#% 1567948
#! The majority of sentiment classifiers is based on dictionaries or requires large amount of training data. Unfortunately, dictionaries contain only limited data and machine-learning classifiers using word-based features do not consider part of words, which makes them domain-specific, less effective and not robust to orthographic mistakes. We attempt to overcome these drawbacks by developing a context-independent approach. Our main idea is to determine some phonetic features of words that could affect their sentiment polarity. These features are applicable to all words; it eliminates the need to continuous manual dictionary renewal. Our experiments are based on a sentiment dictionary for the Russian language. We apply phonetic features to predict word sentiment based on machine learning.

#index 1967800
#* Cross-Language plagiarism detection using a multilingual semantic network
#@ Marc Franco-Salvador;Parth Gupta;Paolo Rosso
#t 2013
#c 16
#% 563622
#% 732848
#% 1471201
#% 1558415
#! Cross-language plagiarism refers to the type of plagiarism where the source and suspicious documents are in different languages. Plagiarism detection across languages is still in its infancy state. In this article, we propose a new graph-based approach that uses a multilingual semantic network to compare document paragraphs in different languages. In order to investigate the proposed approach, we used the German-English and Spanish-English cross-language plagiarism cases of the PAN-PC'11 corpus. We compare the obtained results with two state-of-the-art models. Experimental results indicate that our graph-based approach is a good alternative for cross-language plagiarism detection.

#index 1967801
#* Classification of opinion questions
#@ Hongping Fu;Zhendong Niu;Chunxia Zhang;Lu Wang;Peng Jiang;Ji Zhang
#t 2013
#c 16
#% 939969
#% 1086226
#! With the increasing growth of opinions on news, services and so on, automatic opinion question answering aims at answering questions involving views of persons, and plays an important role in fields of sentiment analysis and information recommendation. One challenge is that opinion questions may contain different types of question focuses that affect answer extraction, such as holders, comparison and location. In this paper, we build a taxonomy of opinion questions, and propose a hierarchical classification technique to classify opinion questions according to our constructed taxonomy. This technique first uses Bayesian classifier and then employs an approach leveraging semantic similarities between questions. Experimental results show that our approach significantly improves performances over baseline and other related works.

#index 1967802
#* Tempo of search actions to modeling successful sessions
#@ Kazuya Fujikawa;Hideo Joho;Shin-ichi Nakayama
#t 2013
#c 16
#% 1384094
#% 1480202
#% 1598439
#% 1879002
#! Considering search process in the evaluation of interactive information retrieval (IIR) is a challenging issue. This paper explores tempo of search actions (query, click, and judgement) to measure people's search process and performance. When we analyse how people consume their search resource (i.e., a total number of search actions taken to complete a task) over the time, it was observed that there was a different pattern in successful sessions and unsuccessful sessions. Successful sessions tend to have a regular tempo in search actions while poor sessions tend to have uneven distribution of resource usage. The resource consumption graph also allows us to observe where in the search process was affected by experimental conditions. Therefore, this paper suggests that tempo of search actions can be exploited to model successful search sessions.

#index 1967803
#* Near-Duplicate detection for online-shops owners: an FCA-Based approach
#@ Dmitry I. Ignatov;Andrey V. Konstantiov;Yana Chubis
#t 2013
#c 16
#% 279120
#% 384416
#% 544011
#% 769944
#% 1265917
#! We proposed a prototype of near-duplicate detection system for web-shop owners. It's a typical situation for this online businesses to buy description of their goods from so-called copyrighters. Copyrighter can cheat from time to time and provide the owner with some almost identical descriptions for different items. In this paper we demonstrated how we can use FCA for fast clustering and revealing such duplicates in real online perfume shop's datasets.

#index 1967804
#* Incremental reranking for hierarchical text classification
#@ Qi Ju;Alessandro Moschitti
#t 2013
#c 16
#% 763708
#% 770763
#% 815896
#% 829975
#% 961192
#% 1913323
#! The top-down method is efficient and commonly used in hierarchical text classification. Its main drawback is the error propagation from the higher to the lower nodes. To address this issue we propose an efficient incremental reranking model of the top-down classifier decisions. We build a multiclassifier for each hierarchy node, constituted by the latter and its children. Then we generate several classification hypotheses with such classifiers and rerank them to select the best one. Our rerankers exploit category dependencies, which allow them to recover from the multiclassifier errors whereas their application in top-down fashion results in high efficiency. The experimentation on Reuters Corpus Volume 1 (RCV1) shows that our incremental reranking is as accurate as global rerankers but at least one magnitude order faster.

#index 1967805
#* Topic model for user reviews with adaptive windows
#@ Takuya Konishi;Fuminori Kimura;Akira Maeda
#t 2013
#c 16
#% 722904
#% 1055682
#% 1653959
#! We discuss the problem in applying topic models to user reviews. Different from ordinary documents, reviews in a same category are similar to each other. This makes it difficult to estimate meaningful topics from these reviews. In this paper, we develop a new model for this problem using the distance dependent Chinese restaurant process. It need not decide the size of windows and can consider neighboring sentences adaptively. We compare this model to the Multi-grain latent Dirichlet allocation which has been proposed previously, and show that our model achieves better results in terms of perplexity.

#index 1967806
#* Time based feedback and query expansion for twitter search
#@ Naveen Kumar;Benjamin Carterette
#t 2013
#c 16
#% 262096
#% 730070
#! Twitter is an accepted platform among users for expressing views in a short text called a "Tweet" Application of search models to platforms like Twitter is still an open-ended question, though the creation of the TREC Microblog track in 2011 aims to help resolve it. In this paper, we propose a modified language search model by extending a traditional query-likelihood language model with time based feedback and query expansion. The proposed method makes use of two types of feedback, time feedback by evaluating the time distribution of top retrieved tweets, and query expansion by using highly frequent terms in top tweets as expanded terms. Our results suggest that using both types of feedback, we get better results than using a standard language model, and the time-based feedback uniformly improves results whether query expansion is used or not.

#index 1967807
#* Is intent-aware expected reciprocal rank sufficient to evaluate diversity?
#@ Teerapong Leelanupab;Guido Zuccon;Joemon M. Jose
#t 2013
#c 16
#% 642975
#% 1292528
#% 1312812
#! In this paper we define two models of users that require diversity in search results; these models are theoretically grounded in the notion of intrinsic and extrinsic diversity. We then examine Intent-Aware Expected Reciprocal Rank (ERR-IA), one of the official measures used to assess diversity in TREC 2011-12, with respect to the proposed user models. By analyzing ranking preferences as expressed by the user models and those estimated by ERR-IA, we investigate whether ERR-IA assesses document rankings according to the requirements of the diversity retrieval task expressed by the two models. Empirical results demonstrate that ERR-IA neglects query-intents coverage by attributing excessive importance to redundant relevant documents. ERR-IA behavior is contrary to the user models that require measures to first assess diversity through the coverage of intents, and then assess the redundancy of relevant intents. Furthermore, diversity should be considered separately from document relevance and the documents positions in the ranking.

#index 1967808
#* Late data fusion for microblog search
#@ Shangsong Liang;Maarten de Rijke;Manos Tsagkias
#t 2013
#c 16
#% 818212
#% 839912
#% 840066
#% 1194140
#% 1536522
#% 1536561
#% 1536584
#! The character of microblog environments raises challenges for microblog search because relevancy becomes one of the many aspects for ranking documents. We concentrate on merging multiple ranking strategies at post-retrieval time for the TREC Microblog task. We compare several state-of-the-art late data fusion methods, and present a new semi-supervised variant that accounts for microblog characteristics. Our experiments show the utility of late data fusion in microblog search, and that our method helps boost retrieval effectiveness.

#index 1967809
#* A task-specific query and document representation for medical records search
#@ Nut Limsopatham;Craig Macdonald;Iadh Ounis
#t 2013
#c 16
#% 217251
#% 907525
#% 1482195
#% 1598507
#% 1879144
#! One of the challenges of searching in the medical domain is to deal with the complexity and ambiguity of medical terminology. Concept-based representation approaches using terminology from domain-specific resources have been developed to handle such a challenge. However, it has been shown that these techniques are effective only when combined with a traditional term-based representation approach. In this paper, we propose a novel technique to represent medical records and queries by focusing only on medical concepts essential for the information need of a medical search task. Such a representation could enhance retrieval effectiveness since only the medical concepts crucial to the information need are taken into account. We evaluate the retrieval effectiveness of our proposed approach in the context of the TREC 2011 Medical Records track. The results demonstrate the effectiveness of our approach, as it significantly outperforms a baseline where all concepts are represented, and markedly outperforms a traditional term-based representation baseline. Moreover, when combining the relevance scores obtained from our technique and a term-based representation approach, the achieved performance is comparable to the best TREC 2011 systems.

#index 1967810
#* On CORI results merging
#@ Ilya Markov;Avi Arampatzis;Fabio Crestani
#t 2013
#c 16
#% 232703
#% 643012
#% 879662
#% 1174737
#% 1292595
#% 1879192
#! Score normalization and results merging are important components of many IR applications. Recently MinMax--an unsupervised linear score normalization method--was shown to perform quite well across various distributed retrieval testbeds, although based on strong assumptions. The CORI results merging method relaxes these assumptions to some extent and significantly improves the performance of MinMax. We parameterize CORI and evaluate its performance across a range of parameter settings. Experimental results on three distributed retrieval testbeds show that CORI significantly outperforms state-of-the-art results merging and score normalization methods when its parameter goes to infinity.

#index 1967811
#* Detecting friday night party photos: semantics for tag recommendation
#@ Philip J. McParlane;Yelena Mejova;Ingmar Weber
#t 2013
#c 16
#% 198058
#% 589914
#% 1055704
#% 1127458
#% 1649112
#% 1783918
#% 1855132
#! Multimedia annotation is central to its organization and retrieval --- a task which tag recommendation systems attempt to simplify. We propose a photo tag recommendation system which automatically extracts semantics from visual and meta-data features to complement existing tags. Compared to standard content/tag-based models, these automatic tags provide a richer description of the image and especially improve performance in the case of the "cold start problem".

#index 1967812
#* Optimizing nDCG gains by minimizing effect of label inconsistency
#@ Pavel Metrikov;Virgil Pavlu;Javed A. Aslam
#t 2013
#c 16
#% 879630
#% 1074134
#! We focus on nDCG choice of gains, and in particular on the fracture between large differences in exponential gains of high relevance labels and the not-so-small confusion, or inconsistency, between these labels in data. We show that better gains can be derived from data by measuring the label inconsistency, to the point that virtually indistinguishable labels correspond to equal gains. Our derived optimal gains make a better nDCG objective for training Learning to Rank algorithms.

#index 1967813
#* Least square consensus clustering: criteria, methods, experiments
#@ Boris G. Mirkin;Andrey Shestakov
#t 2013
#c 16
#% 722902
#% 1373094
#% 1568426
#% 1619241
#% 1911392
#! We develop a consensus clustering framework developed three decades ago in Russia and experimentally demonstrate that our least squares consensus clustering algorithm consistently outperforms several recent consensus clustering methods.

#index 1967814
#* Domain adaptation of statistical machine translation models with monolingual data for cross lingual information retrieval
#@ Vassilina Nikoulina;Stéphane Clinchant
#t 2013
#c 16
#% 815902
#% 816170
#% 1106179
#% 1587403
#% 1905988
#% 1905990
#% 1913291
#! Statistical Machine Translation (SMT) is often used as a black-box in CLIR tasks. We propose an adaptation method for an SMT model relying on the monolingual statistics that can be extracted from the document collection (both source and target if available). We evaluate our approach on CLEF Domain Specific task (German-English and English-German) and show that very simple document collection statistics integrated in SMT translation model allow to obtain good gains both in terms of IR metrics (MAP, P10) and MT evaluation metrics (BLEU, TER).

#index 1967815
#* Text summarization while maximizing multiple objectives with lagrangian relaxation
#@ Masaaki Nishino;Norihito Yasuda;Tsutomu Hirao;Jun Suzuki;Masaaki Nagata
#t 2013
#c 16
#% 755863
#% 1292654
#% 1392478
#% 1591980
#! We show an extractive text summarization method that solves an optimization problem involving the maximization of multiple objectives. Though we can obtain high quality summaries if we solve the problem exactly with our formulation, it is NP-hard and cannot scale to support large problem size. Our solution is an efficient and high quality approximation method based on Lagrangian relaxation (LR) techniques. In experiments on the DUC'04 dataset, our LR based method matches the performance of state-of-the-art methods.

#index 1967816
#* Towards detection of child sexual abuse media: categorization of the associated filenames
#@ Alexander Panchenko;Richard Beaufort;Hubert Naets;Cédrick Fairon
#t 2013
#c 16
#% 344447
#% 1008080
#% 1450992
#% 1643158
#% 1646906
#% 1751918
#% 1915269
#! This paper approaches the problem of automatic pedophile content identification. We present a system for filename categorization, which is trained to identify suspicious files on P2P networks. In our initial experiments, we used regular pornography data as a substitution of child pornography. Our system separates filenames of pornographic media from the others with an accuracy that reaches 91---97%.

#index 1967817
#* Leveraging latent concepts for retrieving relevant ads for short text
#@ Ankit Patil;Kushal Dave;Vasudeva Varma
#t 2013
#c 16
#% 987262
#! The microblogging platforms are increasingly becoming a lucrative prospect for advertisers to attract the customers. The challenge with advertising on such platforms is that there is very little content to retrieve relevant ads. As the microblogging content is short and noisy and the ads are short too, there is a high amount of lexical/vocabulary mismatch between the micropost and the ads. To bridge this vocabulary mismatch, we propose a conceptual approach that transforms the content into a conceptual space that represent the latent concepts of the content. We empirically show that the conceptual model performs better than various state-of-the-art techniques the performance gain obtained are substantial and significant.

#index 1967818
#* Robust PLSA performs better than LDA
#@ Anna Potapenko;Konstantin Vorontsov
#t 2013
#c 16
#% 280819
#% 722904
#% 1417055
#! In this paper we introduce a generalized learning algorithm for probabilistic topic models (PTM). Many known and new algorithms for PLSA, LDA, and SWB models can be obtained as its special cases by choosing a subset of the following "options": regularization, sampling, update frequency, sparsing and robustness. We show that a robust topic model, which distinguishes specific, background and topic terms, doesn't need Dirichlet regularization and provides controllably sparse solution.

#index 1967819
#* WANTED: focused queries for focused retrieval
#@ Georgina Ramírez
#t 2013
#c 16
#% 1450962
#% 1622392
#! Focused retrieval tasks such as XML or passage retrieval strive to provide direct access to the relevant content of a document. In these scenarios users can pose focused queries, i.e., queries that restrict the type of output the user wants to see. We first analyze several characteristics of this type of requests and show that they differ substantially from the unfocused ones. We also show that typical XML retrieval systems tend to perform poorly on focused queries and that systems ranking differs considerably when processing each of the types. Finally, we argue that the unbalanced number of focused queries in the INEX benchmark topic set might lead to misleading interpretations of the evaluation results. To get a better insight of the systems ability to perform focused search, more focused queries are needed.

#index 1967820
#* Exploiting click logs for adaptive intranet navigation
#@ Sharhida Zawani Saad;Udo Kruschwitz
#t 2013
#c 16
#% 818225
#% 904197
#% 956533
#% 987222
#% 1130868
#% 1227622
#% 1268490
#% 1603973
#% 1622353
#! Web sites and intranets can be difficult to navigate as they tend to be rather static and a new user might have no idea what documents are most relevant to his or her need. Our aim is to capture the navigational behaviour of existing users (as recorded in the click logs) so that we can assist future users by proposing the most relevant pages as they navigate the site without changing the actual Web site and do this adaptively so that a continuous learning cycle is being employed. In this paper we explore three different algorithms that can be employed to learn such suggestions from navigation logs. We find that users managed to conduct the tasks significantly quicker than the (purely frequency-based) baseline by employing ant colony optimisation or random walk approaches to the log data for building a suggestion model.

#index 1967821
#* Leveraging microblogs for spatiotemporal music information retrieval
#@ Markus Schedl
#t 2013
#c 16
#% 1693862
#% 1747131
#% 1747133
#% 1806056
#% 1945841
#! We present results of text data mining experiments for music retrieval, analyzing microblogs gathered from November 2011 to September 2012 to infer music listening patterns all around the world. We assess relationships between particular music preferences and spatial properties, such as month, weekday, and country, and the temporal stability of listening activities. The findings of our study will help improve music retrieval and recommendation systems in that it will allow to incorporate geospatial and cultural information into models for music retrieval, which has not been looked into before.

#index 1967822
#* Topic-Focused summarization of chat conversations
#@ Arpit Sood;Thanvir P Mohamed;Vasudeva Varma
#t 2013
#c 16
#% 722904
#% 939368
#% 1237624
#! In this paper, we propose a novel approach to address the problem of chat summarization. We summarize real-time chat conversations which contain multiple users with frequent shifts in topic. Our approach consists of two phases. In the first phase, we leverage topic modeling using web documents to find the primary topic of discussion in the chat. Then, in the summary generation phase, we build a semantic word space to score sentences based on their association with the primary topic. Experimental results show that our method significantly outperforms the baseline systems on ROUGE F-scores.

#index 1967823
#* Risk ranking from financial reports
#@ Ming-Feng Tsai;Chuan-Ju Wang
#t 2013
#c 16
#% 881477
#% 1270679
#! This paper attempts to use soft information in finance to rank the risk levels of a set of companies. Specifically, we deal with a ranking problem with a collection of financial reports, in which each report is associated with a company. By using text information in the reports, which is so-called the soft information, we apply learning-to-rank techniques to rank a set of companies to keep them in line with their relative risk levels. In our experiments, a collection of financial reports, which are annually published by publicly-traded companies, is employed to evaluate our ranking approach; moreover, a regression-based approach is also carried out for comparison. The experimental results show that our ranking approach not only significantly outperforms the regression-based one, but identifies some interesting relations between financial terms.

#index 1967824
#* An initial investigation on the relationship between usage and findability
#@ Colin Wilkie;Leif Azzopardi
#t 2013
#c 16
#% 290830
#% 776364
#% 1014259
#% 1130863
#! Ensuring that information within a website is findable is particularly important. This is because visitors that cannot find what they are looking for are likely to leave the site or become very frustrated and switch to a competing site. While findability has been touted as important in web design, we wonder to what degree measures of findability are correlated to usage. To this end, we have conducted a preliminary study on three sub-domains across a number of measures of findability.

#index 1967825
#* Sub-sentence extraction based on combinatorial optimization
#@ Norihito Yasuda;Masaaki Nishino;Tsutomu Hirao;Masaaki Nagata
#t 2013
#c 16
#% 1260752
#% 1262964
#% 1270785
#% 1392478
#% 1591980
#! This paper describes the prospect of word extraction for text summarization based on combinatorial optimization. Instead of the commonly used sentence-based approach, word-based approaches are preferable if highly-compressed summarizations are required. However, naively applying conventional methods for word extraction yields excessively fragmented summaries. We avoid this by restricting the number of selected fragments from each sentence to at most one when formulating the maximum coverage problem. Consequently, the method only choose sub-sentences as fragments. Experiments show that our method matches the ROUGE scores of state-of-the-art systems without requiring any training or special parameters.

#index 1967826
#* ADRTrace: detecting expected and unexpected adverse drug reactions from user reviews on social media sites
#@ Andrew Yates;Nazli Goharian
#t 2013
#c 16
#% 939515
#% 1480944
#! We automatically extract adverse drug reactions (ADRs) from consumer reviews provided on various drug social media sites to identify adverse reactions not reported by the United States Food and Drug Administration (FDA) but touted by consumers. We utilize various lexicons, identify patterns, and generate a synonym set that includes variations of medical terms. We identify "expected" and "unexpected" ADRs. Background (drug) language is utilized to evaluate the strength of the detected unexpected ADRs. Evaluation results for our synonym set and ADR extraction are promising.

#index 1967827
#* The impact of temporal intent variability on diversity evaluation
#@ Ke Zhou;Stewart Whiting;Joemon M. Jose;Mounia Lalmas
#t 2013
#c 16
#% 1166473
#% 1190102
#% 1920025
#! To cope with the uncertainty involved with ambiguous or underspecified queries, search engines often diversify results to return documents that cover multiple interpretations, e.g. the car brand, animal or operating system for the query 'jaguar'. Current diversity evaluation measures take the popularity of the subtopics into account and aim to favour systems that promote most popular subtopics earliest in the result ranking. However, this subtopic popularity is assumed to be static over time. In this paper, we hypothesise that temporal subtopic popularity change is common for many topics and argue this characteristic should be considered when evaluating diversity. Firstly, to support our hypothesis we analyse temporal subtopic popularity changes for ambiguous queries through historic Wikipedia article viewing statistics. Further, by simulation, we demonstrate the impact of this temporal intent variability on diversity evaluation.

#index 1967828
#* Re-leashed! the PuppyIR framework for developing information services for children, adults and dogs
#@ Doug Dowie;Leif Azzopardi
#t 2013
#c 16
#% 449291
#% 1455283
#% 1482194
#% 1482344
#% 1783060
#! Children are active information seekers, but research has suggested that services, designed with adults in mind, are a poor fit to their needs [1-3]. The goal of the PuppyIR project is to design, develop and deliver an open source framework for building information services specifically for children, which incorporates the current understanding of children's information seeking needs. This paper describes the framework's architecture, highlights two of its novel information processing components, and marks the release of the framework to the wider Interactive Information Retrieval community. PuppyIR provides an open and common framework for the rapid prototyping, development and evaluation of information services specifically for children.

#index 1967829
#* A web mining tool for assistance with creative writing
#@ Boris A. Galitsky;Sergei O. Kuznetsov
#t 2013
#c 16
#% 1549702
#% 1627916
#% 1911508
#% 1954879
#! We develop a web mining tool for assistance with creative writing. The relevance of web mining is achieved via computing similarities of parse trees for queries and found snippets. To assure the plausible flow of mental states of involved agents, a multi-agent behavior simulator is included in content generation algorithm.

#index 1967830
#* DS4: a distributed social and semantic search system
#@ Dionisis Kontominas;Paraskevi Raftopoulou;Christos Tryfonopoulos;Euripides G. M. Petrakis
#t 2013
#c 16
#% 902713
#% 1002060
#% 1291214
#% 1415714
#! We present $\mathcal{DS}^4$, a Distributed Social and Semantic Search System that allows users to share content among friends and clusters of users. In $\mathcal{DS}^4$ nodes that are semantically, thematically, or socially similar are automatically discovered and logically organised. Content retrieval is then performed by routing the query towards social friends and clusters of nodes that are likely to answer it. In this way, search receives two facets: the social facet, addressing friends, and the semantic facet, addressing nodes that are semantically close to the query. $\mathcal{DS}^4$ is scalable (requires no centralised component), privacy-aware (users maintain ownership and control over their content), automatic (requires no intervention by the user), general (works for any type of content), and adaptive (adjusts to changes of user content or interests). In this work, we aim to design the next generation of social networks that will offer open and adaptive design, and privacy-aware content management.

#index 1967831
#* Serelex: search and visualization of semantically related words
#@ Alexander Panchenko;Pavel Romanov;Olga Morozova;Hubert Naets;Andrey Philippovich;Alexey Romanov;Cédrick Fairon
#t 2013
#c 16
#% 748600
#% 1275285
#% 1279327
#! We present Serelex, a system that provides, given a query in English, a list of semantically related words. The terms are ranked according to an original semantic similarity measure learnt from a huge corpus. The system performs comparably to dictionary-based baselines, but does not require any semantic resource such as WordNet. Our study shows that users are completely satisfied with 70% of the query results.

#index 1967832
#* SIAM: social interaction analysis for multimedia
#@ Jérôme Picault;Myriam Ribière
#t 2013
#c 16
#% 722904
#% 816186
#% 1576173
#! This paper describes the SIAM demonstrator, a system that illustrates the usefulness of indexing multimedia segments thanks to associated microblog posts. From a socialized multimedia content (i.e. video and associated microblog posts on Twitter), the system applies text mining techniques and derives a topic model to index socialized multimedia segments. That result may then be used inside many multimedia applications, such as in-media social navigation, multimedia summarization or composition, or exploration of multimedia collections according to various socially-based viewpoints.

#index 1967833
#* Exploratory search on social media
#@ Aaron Russ;Michael Kaisser
#t 2013
#c 16
#% 1387282
#% 1912795
#! The rise of Social Media creates a wealth of information that can be very valuable for private and professional users alike. But many challenges surrounding this relatively new kind of information are yet unsolved. This is true for algorithms that efficiently and intelligently process such data, but also for methods of how users can conveniently access it and how results are displayed. In this paper we present a tool that lets users perform exploratory search on several Social Media sites in parallel. It gives users the opportunity to explore a topic space, and to better understand facets of current discussions.

#index 1967834
#* VisNavi: citation context visualization and navigation
#@ Farag Saad;Brigitte Mathiak
#t 2013
#c 16
#% 1224718
#! The process of retrieving information for literature review purposes differs from traditional web information retrieval. Literature reviews differentiate between the weightiness of the retrieved data segments. For example, citations and their accompanying information, such as cited author, citation context etc., are a very important consideration when searching for relevant information in literature. However, this information is integrated into a scientific paper, in rich interrelationships, making it very complicated for standard search systems to present and track them efficiently. In this paper, we demonstrate a system, VisNavi, in the form of a visualized star-centered approach that introduces the rich citation interrelationships to the searchers in an effective and navigational appearance.

#index 1967835
#* Face-Based people searching in videos
#@ Jan Sedmidubsky;Michal Batko;Pavel Zezula
#t 2013
#c 16
#% 660309
#% 1279842
#% 1885628
#! We propose a system for retrieving people according to their faces in unannotated video streams. The system processes input videos to extract key-frames on which faces are detected. The detected faces are automatically grouped together to create clusters containing snapshots of the same person. The system also facilitates annotation and manual manipulations with created clusters. On the processed videos the system offers to search for persons in three distinct operations applicable to various scenarios. The system is presented online by indexing five high-quality video streams with the total length of nearly five hours.

#index 1967836
#* Political hashtag trends
#@ Ingmar Weber;Venkata Rama Kiran Garimella;Asmelash Teka
#t 2013
#c 16
#% 1518009
#% 1879112
#% 1905953
#! Political Hashtag Trends (PHT) is an analysis tool for political left-vs.-right polarization of Twitter hashtags. PHT computes a leaning for trending, political hashtags in a given week, giving insights into the polarizing U.S. American issues on Twitter. The leaning of a hashtag is derived in two steps. First, users retweeting a set of "seed users" with a known political leaning, such as Barack Obama or Mitt Romney, are identified and the corresponding leaning is assigned to retweeters. Second, a hashtag is assigned a fractional leaning corresponding to which retweeting users used it. Non-political hashtags are removed by requiring certain hashtag co-occurrence patterns. PHT also offers functionality to put the results into context. For example, it shows example tweets from different leanings, it shows historic information and it links to the New York Times archives to explore a topic in depth. In this paper, we describe the underlying methodology and the functionality of the demo.

#index 1967837
#* OPARS: objective photo aesthetics ranking system
#@ Huang Xiao;Han Xiao;Claudia Eckert
#t 2013
#c 16
#% 1750325
#! As the perception of beauty is subjective across individuals, evaluating the objective aesthetic value of an image is a challenging task in image retrieval system. Unlike current online photo sharing services that take the average rating as the aesthetic score, our system integrates various ratings from different users by jointly modeling images and users' expertise in a regression framework. In the front-end, users are asked to rate images selected by an active learning process. A multi-observer regression model is employed in the back-end to integrate these ratings for predicting the aesthetic value of images. Moreover, the system can be incorporated into current photo sharing services as complement by providing more accurate ratings.

#index 1967838
#* Distributed information retrieval and applications
#@ Fabio Crestani;Ilya Markov
#t 2013
#c 16
#% 194246
#% 280856
#% 340146
#% 397125
#% 643012
#% 643021
#% 1026651
#% 1074094
#% 1130914
#% 1131119
#% 1174737
#% 1227597
#% 1227616
#% 1227628
#% 1227629
#% 1264113
#% 1292595
#% 1392444
#% 1450835
#% 1450840
#% 1450841
#% 1455252
#% 1482223
#% 1482230
#% 1565813
#% 1598559
#% 1641937
#% 1835666
#% 1879192
#% 1919835
#% 1967777
#% 1967810
#! Distributed Information Retrieval (DIR) is a generic area of research that brings together techniques, such as resource selection and results aggregation, dealing with data that, for organizational or technical reasons, cannot be managed centrally. Existing and potential applications of DIR methods vary from blog retrieval to aggregated search and from multimedia and multilingual retrieval to distributed Web search. In this tutorial we briefly discuss main DIR phases, that are resource description, resource selection, results merging and results presentation. The main focus is made on applications of DIR techniques: blog, expert and desktop search, aggregated search and personal meta-search, multimedia and multilingual retrieval. We also discuss a number of potential applications of DIR techniques, such as distributed Web search, enterprise search and aggregated mobile search.

#index 1967839
#* Searching the web of data
#@ Gerard de Melo;Katja Hose
#t 2013
#c 16
#% 783474
#% 956564
#% 1127610
#% 1190676
#% 1227610
#% 1241185
#% 1400010
#% 1409954
#% 1489451
#% 1497261
#% 1523817
#% 1603826
#% 1616664
#% 1641941
#% 1708910
#% 1746862
#% 1746991
#% 1805999
#% 1879005
#% 1920053
#! Search is currently undergoing a major paradigm shift away from the traditional document-centric "10 blue links" towards more explicit and actionable information. Recent advances in this area are Google's Knowledge Graph, Virtual Personal Assistants such as Siri and Google Now, as well as the now ubiquitous entity-oriented vertical search results for places, products, etc. Apart from novel query understanding methods, these developments are largely driven by structured data that is blended into the Web Search experience. We discuss efficient indexing and query processing techniques to work with large amounts of structured data. Finally, we present query interpretation and understanding methods to map user queries to these structured data sources.

#index 1967840
#* Monolingual and cross-lingual probabilistic topic models and their applications in information retrieval
#@ Marie-Francine Moens;Ivan Vulić
#t 2013
#c 16
#% 262096
#% 280819
#% 397145
#% 722904
#% 879587
#% 1195831
#% 1211828
#% 1297066
#% 1305532
#% 1338620
#% 1417061
#% 1471293
#% 1536542
#% 1565542
#% 1592228
#% 1603766
#% 1697450
#% 1726700
#% 1748072
#% 1828477
#% 1906022
#! Probabilistic topic models are a group of unsupervised generative machine learning models that can be effectively trained on large text collections. They model document content as a two-step generation process, i.e., documents are observed as mixtures of latent topics, while topics are probability distributions over vocabulary words. Recently, a significant research effort has been invested into transferring the probabilistic topic modeling concept from monolingual to multilingual settings. Novel topic models have been designed to work with parallel and comparable multilingual data (e.g., Wikipedia or news data discussing the same events). Probabilistic topics models offer an elegant way to represent content across different languages. Their probabilistic framework allows for their easy integration into a language modeling framework for monolingual and cross-lingual information retrieval. Moreover, we present how to use the knowledge from the topic models in the tasks of cross-lingual event clustering, cross-lingual document classification and the detection of cross-lingual semantic similarity of words. The tutorial also demonstrates how semantically similar words across languages are integrated as useful additional evidences in cross-lingual information retrieval models.

#index 1967841
#* Practical online retrieval evaluation
#@ Filip Radlinski;Katja Hofmann
#t 2013
#c 16
#% 577224
#% 857180
#% 879565
#% 946521
#% 987209
#% 1035578
#% 1130811
#% 1214757
#% 1227582
#% 1250379
#% 1400034
#% 1415710
#% 1450892
#% 1450912
#% 1536505
#% 1536519
#% 1641943
#% 1667281
#% 1918346
#% 1919816
#! Online evaluation allows the assessment of information retrieval (IR) techniques based on how real users respond to them. Because this technique is directly based on observed user behavior, it is a promising alternative to traditional offline evaluation, which is based on manual relevance assessments. In particular, online evaluation can enable comparisons in settings where reliable assessments are difficult to obtain (e.g., personalized search) or expensive (e.g., for search by trained experts in specialized collections). Despite its advantages, and its successful use in commercial settings, online evaluation is rarely employed outside of large commercial search engines due to a perception that it is impractical at small scales. The goal of this tutorial is to show how online evaluations can be conducted in such settings, demonstrate software to facilitate its use, and promote further research in the area. We will also contrast online evaluation with standard offline evaluation, and provide an overview of online approaches.

#index 1967842
#* Integrating IR technologies for professional search
#@ Michail Salampasis;Norbert Fuhr;Allan Hanbury;Mihai Lupu;Birger Larsen;Henrik Strindberg
#t 2013
#c 16
#! Professional search in specific domains (e.g. patent, medical, scientific literature, media) usually needs an exploratory type of search which is characterized more often, in comparison to fact finding and question answering web search, by recall-oriented information needs and by uncertainty and evolution or change of the information need. Additionally, the complexity of the tasks that need to be performed by professional searchers, which usually include not only retrieval but also information analysis and monitoring tasks, require association, pipelining and possibly integration of information as well as synchronization and coordination of multiple and potentially concurrent search views produced from different datasets, search tools and UIs. Many facets of IR technology (e.g. exploratory search, aggregated search, federated search, task-based search, IR over query sessions, cognitive IR approaches, Human Computer and Information Retrieval) aim to at least partially address these demands. This workshop aims to stimulate exploratory research, bring together various facets of IR research and promote discussion between researchers towards the development of a generalised framework facilitating the integration of IR technologies and search tools into next generation professional search systems. This envisioned framework should be supported from new or the extension of existing protocols and may influence the design of next generation professional search systems.

#index 1967843
#* From republicans to teenagers --- group membership and search (GRUMPS)
#@ Ingmar Weber;Djoerd Hiemstra;Pavel Serdyukov
#t 2013
#c 16
#% 167557
#% 345735
#% 755685
#% 807644
#% 1019163
#% 1043044
#% 1043960
#% 1166492
#% 1210687
#% 1355051
#% 1392496
#% 1450894
#% 1536504
#% 1536505
#% 1598458
#% 1641960
#% 1921856
#! In the early years of information retrieval, the focus of research was on systems aspects such as crawling, indexing, and relevancy ranking. Over the years, more and more user-related information such as click information or search history has entered the equation creating more and more personalized search experiences, though still within the scope of the same overall system. Though fully personalized search is probably desirable, this individualistic perspective does not exploit the fact that a lot of a users behavior can be explained through their group membership. Children, despite individual differences, share many challenges and needs; as do men, Republicans, Chinese or any user group. This workshop takes a group-centric approach to IR and invites contributions that either (i) propose and evaluate IR systems for a particular user group or that (ii) describe how the search behavior of specific groups differ, potentially requiring a different way of addressing their needs.

#index 1967844
#* Doctoral consortium at ECIR 2013
#@ Hideo Joho;Dmitry I. Ignatov
#t 2013
#c 16
#! This is a short description of Doctoral Consortium at ECIR 2013.

