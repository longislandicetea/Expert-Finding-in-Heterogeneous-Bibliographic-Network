#index 454043
#* Proceedings of the 5th International Conference on Extending Database Technology: Advances in Database Technology
#@ Peter M. G. Apers;Mokrane Bouzeghoub;Georges Gardarin
#t 1996
#c 8

#index 454044
#* Proceedings of the 6th International Conference on Extending Database Technology: Advances in Database Technology
#@ Hans-Jörg Schek;Fèlix Saltor;Isidro Ramos;Gustavo Alonso
#t 1998
#c 8

#index 454045
#* Proceedings of the 7th International Conference on Extending Database Technology: Advances in Database Technology
#@ Carlo Zaniolo;Peter C. Lockemann;Marc H. Scholl;Torsten Grust
#t 2000
#c 8

#index 454046
#* Proceedings of the 8th International Conference on Extending Database Technology: Advances in Database Technology
#@ Christian S. Jensen;Keith G. Jeffery;Jaroslav Pokorný;Simonas Saltenis;Elisa Bertino;Klemens Böhm;Matthias Jarke
#t 2002
#c 8

#index 458539
#* Translating OSQL-Queries into Efficient Set Expressions
#@ Hennie J. Steenhagen;Rolf A. de By;Henk M. Blanken
#t 1996
#c 8

#index 458540
#* Title, Foreword, Sponsorship, Organization, Program Commitee, Additional Referees
#@ 
#t 1996
#c 8

#index 458541
#* Knowledge Discovery from Epidemiological Databases
#@ Gérard Pavillon
#t 1996
#c 8

#index 458542
#* A Formal Temporal Object-Oriented Data Model
#@ Elisa Bertino;Elena Ferrari;Giovanna Guerrini
#t 1996
#c 8

#index 458543
#* Doing Business with the Web: The Informix/Illustra Approach
#@ Brian Baker
#t 1996
#c 8

#index 458544
#* Database Support for Efficiently Maintaining Derived Data
#@ Brad Adelberg;Ben Kao;Hector Garcia-Molina
#t 1996
#c 8

#index 458545
#* LoT: Dynamic Declustering of TSB-Tree Nodes for Parallel Access to Temporal Data
#@ Peter Muth;Achim Kraiss;Gerhard Weikum
#t 1996
#c 8

#index 458546
#* Scalable Update Propagation in Epidemic Replicated Databases
#@ Michael Rabinovich;Narain H. Gehani;Alex Kononov
#t 1996
#c 8

#index 458547
#* Object Framework for Business Applications
#@ Marco Emrich
#t 1996
#c 8

#index 458548
#* Adaptive Parallel Query Execution in DBS3
#@ Luc Bouganim;Benoît Dageville;Patrick Valduriez
#t 1996
#c 8

#index 458549
#* A Hash Partition Strategy for Distributed Query Processing
#@ Chengwen Liu;Hao Chen
#t 1996
#c 8

#index 458550
#* Optimizing Queries with Aggregate Views
#@ Surajit Chaudhuri;Kyuseok Shim
#t 1996
#c 8

#index 458551
#* LabFlow-1: A Database Benchmark for High-Throughput Workflow Management
#@ Anthony J. Bonner;Adel Shrufi;Steve Rozen
#t 1996
#c 8

#index 458552
#* Declustering Spatial Databases on a Multi-Computer Architecture
#@ Nick Koudas;Christos Faloutsos;Ibrahim Kamel
#t 1996
#c 8

#index 458553
#* Correct Schema Transformations
#@ Xiaolei Qian
#t 1996
#c 8

#index 458554
#* Querying TSQL2 Databases with Temporal Logic
#@ Michael H. Böhlen;Jan Chomicki;Richard Thomas Snodgrass;David Toman
#t 1996
#c 8

#index 458555
#* Dynamic Development and Refinement of HyperMedia Documents
#@ Lois M. L. Delcambre;Catherine Hamon;Michel Biezunski;Radhika Reddy;Steven R. Newcomb
#t 1996
#c 8

#index 458556
#* Data Integration using Self-Maintainable Views
#@ Ashish Gupta;H. V. Jagadish;Inderpal Singh Mumick
#t 1996
#c 8

#index 458557
#* Commit Scope Control in Nested Transactions
#@ Qiming Chen;Umeshwar Dayal
#t 1996
#c 8

#index 458558
#* An Assessment of Non-Standard DBMSs for CASE Environments
#@ Udo Kelter;Dirk Däberitz
#t 1996
#c 8

#index 458559
#* Fundamental Techniques for Order Optimization
#@ David E. Simmen;Eugene J. Shekita;Timothy Malkemus
#t 1996
#c 8

#index 458737
#* Minimizing Detail Data in Data Warehouses
#@ Michael O. Akinde;Ole Guttorm Jensen;Michael H. Böhlen
#t 1998
#c 8

#index 458738
#* An Evaluation of Alternative Disk Scheduling Techniques in Support of Variable Bit Rate Continuous Media
#@ Jaber Al-Marri;Shahram Ghandeharizadeh
#t 1998
#c 8

#index 458739
#* A Scheme to Specify and Implement Ad-Hoc Recovery in Workflow Systems
#@ Jian Tang;San-Yih Hwang
#t 1998
#c 8

#index 458740
#* On the Ubiquity of Information Services and the Absence of Guaranteed Service Quality
#@ Gerhard Weikum
#t 1998
#c 8

#index 458741
#* Improving the Query Performance of High-Dimensional Index Structures by Bulk-Load Operations
#@ Stefan Berchtold;Christian Böhm;Hans-Peter Kriegel
#t 1998
#c 8

#index 458742
#* Processing Complex Similarity Queries with Distance-Based Access Methods
#@ Paolo Ciaccia;Marco Patella;Pavel Zezula
#t 1998
#c 8

#index 458743
#* Equal Time for Data on the Internet with WebSemantics
#@ George A. Mihaila;Louiqa Raschid;Anthony Tomasic
#t 1998
#c 8

#index 458744
#* HySpirit - A Probabilistic Inference Engine for Hypermedia Retrieval in Large Databases
#@ Norbert Fuhr;Thomas Rölleke
#t 1998
#c 8

#index 458745
#* Efficient Queries over Web Views
#@ Giansalvatore Mecca;Alberto O. Mendelzon;Paolo Merialdo
#t 1998
#c 8

#index 458746
#* Design and Maintenance of Data-Intensive Web Sites
#@ Paolo Atzeni;Giansalvatore Mecca;Paolo Merialdo
#t 1998
#c 8

#index 458747
#* Parallel Processing of Multiple Aggregate Queries on Shared-Nothing Multiprocessors
#@ Takeshi Fukuda;Hirofumi Matsuzawa
#t 1998
#c 8

#index 458748
#* Using Checksums to Detect Data Corruption
#@ Daniel Barbará;Rajni Goel;Sushil Jajodia
#t 2000
#c 8
#% 92739
#% 172256
#% 210212
#% 238755
#% 275734
#% 444141
#% 463258
#% 592667
#% 631961
#! In this paper, we consider the problem of malicious and intended corruption of data in a database, acting outside of the scope of the database management system. Although detecting an attacker who changes a set of database values at the disk level is a simple task (achievable by attaching signatures to each block of data), a more sophisticated attacker may corrupt the data by replacing the current data with copies of old block images, compromising the integrity of the data. To prevent successful completion of this attack, we provide a defense mechanism that enormously increases the intruders workload, yet maintains a low system cost during an authorized update. Our algorithm calculates and maintains two levels of signatures (checksum values) on blocks of data. The signatures are grouped in a manner that forces an extended series of block copying for any unauthorized update. Using the available information on block sizes, block reference patterns and amount of concurrently active transactions in the database, we calculate the length of this chain of copying, proving that the intruder has to perform a lot of work in order to go undetected. Therefore, our technique makes this type of attack very unlikely. Previous work has not addressed protection methods against this knowledgeable and equipped intruder who is operating outside the database management system.

#index 458749
#* Securing XML Documents
#@ Ernesto Damiani;Sabrina De Capitani di Vimercati;Stefano Paraboschi;Pierangela Samarati
#t 2000
#c 8
#% 91075
#% 227956
#% 246334
#% 264261
#% 281150
#% 309525
#% 442862
#% 443057
#% 479981
#% 664665
#! Web-based applications greatly increase information availability and ease of access, which is optimal for public information. The distribution and sharing by theWeb of information that must be accessed in a selective way requires the definition and enforcement of security controls, ensuring that information will be accessible only to authorized entities. Approaches proposed to this end level, independently from the semantics of the data to be protected and for this reason result limited. The eXtensible Markup Language (XML), a markup language promoted by the World Wide Web Consortium (W3C), represents an important opportunity to solve this problem. We present an access control model to protect information distributed on the Web that, by exploiting XML's own capabilities, allows the definition and enforcement of access restrictions directly on the structure and content of XML documents. We also present a language for the specification of access restrictions that uses standard notations and concepts and briefly describe a system architecture for access control enforcement based on existing technology.

#index 458750
#* Object View Hierarchies in DB2 UDB
#@ Michael J. Carey;Serge Rielau;Bennet Vance
#t 2000
#c 8
#% 102780
#% 116091
#% 169798
#% 196474
#% 238413
#% 278619
#% 322880
#% 341169
#% 385828
#% 462624
#% 479977
#% 479979
#% 481098
#% 481919
#% 614579
#! In this paper, we describe the design and implementation of object views in IBM's DB2 Universal Database system. We first describe the object-oriented aspects of DB2's type system, their implications for views, and our design requirements. We then describe DB2's object view facility, showing its use to create object views of object data and of legacy relational data. We discuss key aspects of its implementation, explaining how the correctness of object view definitions is checked and how we ensure that queries against object views are translated into efficient queries against the underlying stored database. We close by discussing the current status of object views in DB2.

#index 458751
#* On Bounding-Schemas for LDAP Directories
#@ Sihem Amer-Yahia;H. V. Jagadish;Laks V. S. Lakshmanan;Divesh Srivastava
#t 2000
#c 8
#% 140389
#% 156979
#% 237191
#% 237192
#% 248024
#% 248809
#% 273897
#% 273935
#% 464720
#% 464724
#% 479465
#! As our world gets more networked, ever increasing amounts of information are being stored in LDAP directories. While LDAP directories have considerable flexibility in the modeling and retrieval of information for network applications, the notion of schema they provide for enabling consistent and coherent representation of directory information is rather weak. In this paper, we propose an expressive notion of bounding-schemas for LDAP directories, and illustrate their practical utility. Bounding-schemas are based on lower bound and upper bound specifications for the content and structure of an LDAP directory. Given a bounding-schema specification, we present algorithms to efficiently determine: (i) if an LDAP directory is legal w.r.t. the bounding-schema, and (ii) if directory insertions and deletions preserve legality. Finally, we show that the notion of bounding-schemas has wider applicability, beyond the specific context of LDAP directories.

#index 458752
#* Order Based Analysis Functions in NCR Teradata Parallel RDBMS
#@ Ambuj Shatdal
#t 2000
#c 8
#% 340670
#% 503878
#! The decision-support (OLAP) applications commonly use orderbased analysis functions like Rank, Cumulative Total, Moving Average. In past, the applications were forced to compute these important analysis functions outside the database. This resulted in loss of performance and inconvenience. In the NCR Teradata RDBMS V2R3.0.0, we have taken the lead by providing these functions as part of the extended SQL. In this paper we describe the feature and the algorithms. The feature allows computations on very large data sets and makes it significantly faster than what was previously possible.

#index 458753
#* A Data Model for Semistructured Data with Partial and Inconsistent Information
#@ Mengchi Liu;Tok Wang Ling
#t 2000
#c 8
#% 663
#% 98660
#% 101649
#% 158203
#% 162825
#% 198483
#% 210211
#% 210214
#% 212199
#% 248801
#% 248858
#% 261741
#% 289283
#% 340295
#% 442692
#% 459289
#% 463919
#% 464720
#% 464724
#% 464825
#% 481602
#% 481923
#% 504570
#% 565353
#% 614598
#! With the recent popularity of the World Wide Web, an enormous amount of heterogeneous information is now available online. As a result, information about real world objects may spread over different data sources and may be partial and inconsistent. How to manipulate such semistructured data is thus a challenge. Previous work on semistructured data mainly focuses on developing query languages and systems to retrieve semistructured data. Relatively less attention has been paid to the manipulation of such data. In order to manipulate such semistructured data, we need a data model that is more expressive than the existing graph-based and tree-based ones to account for the existence of partial and inconsistent information from different data sources. In this paper, we propose such a data model for semistructured data that allows partial and inconsistent information and discuss how to manipulate such semistructured data.

#index 458754
#* OLAP Query Routing and Physical Design in a Database Cluster
#@ Uwe Röhm;Klemens Böhm;Hans-Jörg Schek
#t 2000
#c 8
#% 28623
#% 83933
#% 116042
#% 129891
#% 188719
#% 273889
#% 339764
#% 340308
#% 481955
#% 565452
#% 571084
#% 631958
#! This article quantifies the benefit from simple data organization schemes and elementary query routing techniques for the PowerDB engine, a system that coordinates a cluster of databases. We report on evaluations for a specific scenario: the workload contains OLAP queries, OLTP queries, and simple updates, borrowed from the TPC-R benchmark.We investigate affinity of OLAP queries and different routing strategies for such queries. We then compare two simple data placement schemes, namely full replication and a hybrid one combining partial replication with partitioning. We run different experiments with queries only, with updates only, and with queries concurrently to simple updates. It turns out that hybrid is superior to full replication, even without updates. Our overall conclusion is that coordinator-based routing has good scaleup properties for scenarios with complex analysis queries.

#index 458755
#* A Unified Approach for Indexed and Non-Indexed Spatial Joins
#@ Lars Arge;Octavian Procopiuc;Sridhar Ramaswamy;Torsten Suel;Jan Vahrenhold;Jeffrey Scott Vitter
#t 2000
#c 8
#% 2115
#% 18614
#% 25924
#% 68091
#% 86950
#% 86952
#% 116065
#% 152937
#% 153260
#% 172909
#% 203445
#% 210186
#% 210187
#% 227932
#% 260036
#% 273685
#% 273886
#% 273887
#% 282908
#% 285932
#% 427199
#% 445701
#% 462331
#% 462957
#% 463436
#% 463595
#% 479453
#% 479797
#% 480093
#% 480756
#% 481304
#% 481428
#% 527012
#% 1081215
#% 1440814
#! Most spatial join algorithms either assume the existence of a spatial index structure that is traversed during the join process, or solve the problem by sorting, partitioning, or on-the-fly index construction. In this paper, we develop a simple plane-sweeping algorithm that unifies the index-based and non-index based approaches. This algorithm processes indexed as well as non-indexed inputs, extends naturally to multiway joins, and can be built easily from a few standard operations. We present the results of a comparative study of the new algorithm with several index-based and non-index based spatial join algorithms. We consider a number of factors, including the relative performance of CPU and disk, the quality of the spatial indexes, and the sizes of the input relations. An important conclusion from our work is that using an index-based approach whenever indexes are available does not always lead to the best execution time, and hence we propose the use of a simple cost model to decide when to follow an index-based approach.

#index 458756
#* Mining Classification Rules from Datasets with Large Number of Many-Valued Attributes
#@ Giovanni Guiffrida;Wesley W. Chu;Dominique M. Hanssens
#t 2000
#c 8
#% 44876
#% 136350
#% 232136
#% 420062
#% 449566
#% 459008
#% 461909
#% 481945
#% 503866
#% 631970
#% 641015
#% 739189
#% 1273395
#% 1499571
#! Decision tree induction algorithms scale well to large datasets for their univariate and divide-and-conquer approach. However, they may fail in discovering effective knowledge when the input dataset consists of a large number of uncorrelated many-valued attributes. In this paper we present an algorithm, Noah, that tackles this problem by applying a multivariate search. Performing a multivariate search leads to a much larger consumption of computation time and memory, this may be prohibitive for large datasets. We remedy this problem by exploiting effective pruning strategies and efficient data structures.We applied our algorithm to a real marketing application of cross-selling. Experimental results revealed that the application database was too complex for C4.5 as it failed to discover any useful knowledge. The application database was also too large for various well known rule discovery algorithms which were not able to complete their task. The pruning techniques used in Noah are general in nature and can be used in other mining systems.

#index 458757
#* Approximate Graph Schema Extraction for Semi-Structured Data
#@ Qiu Yue Wang;Jeffrey X. Yu;Kam-Fai Wong
#t 2000
#c 8
#% 136740
#% 197751
#% 210205
#% 210214
#% 237191
#% 244109
#% 248809
#% 340295
#% 462062
#% 462235
#% 463919
#% 464720
#% 464724
#% 479465
#% 481602
#% 481938
#% 693778
#! Semi-structured data are typically represented in the form of labeled directed graphs. They are self-describing and schemaless. The lack of a schema renders query processing over semi-structured data expensive. To overcome this predicament, some researchers proposed to use the structure of the data for schema representation. Such schemas are commonly referred to as graph schemas. Nevertheless, since semi-structured data are irregular and frequently subjected to modifications, it is costly to construct an accurate graph schema and worse still, it is difficult to maintain it thereafter. Furthermore, an accurate graph schema is generally very large, hence impractical. In this paper, an approximation approach is proposed for graph schema extraction. Approximation is achieved by summarizing the semi-structured data graph using an incremental clustering method. The preliminary experimental results have shown that approximate graph schemas were more compact than the conventional accurate graph schemas and promising in query evaluation that involved regular path expressions.

#index 458758
#* Athena: Mining-Based Interactive Management of Text Database
#@ Rakesh Agrawal;Roberto J. Bayardo, Jr.;Ramakrishnan Srikant
#t 2000
#c 8
#% 115478
#% 118771
#% 159108
#% 165110
#% 234992
#% 249155
#% 271083
#% 376266
#% 458758
#% 466078
#% 481945
#% 482113
#! We describe Athena: a system for creating, exploiting, and maintaining a hierarchy of textual documents through interactive mining-based operations. Requirements of any such system include speed and minimal end-user effort. Athena satisfies these requirements through linear-time classification and clustering engines which are applied interactively to speed the development of accurate models. Naive Bayes classifiers are recognized to be among the best for classifying text. We show that our specialization of the Naive Bayes classifier is considerably more accurate (7 to 29% absolute increase in accuracy) than a standard implementation. Our enhancements include using Lid-stone's law of succession instead of Laplace's law, under-weighting long documents, and over-weighting author and subject. We also present a new interactive clustering algorithm, C-Evolve, for topic discovery. C-Evolve first finds highly accurate cluster digests (partial clusters), gets user feedback to merge and correct these digests, and then uses the classification algorithm to complete the partitioning of the data. By allowing this interactivity in the clustering process, C-Evolve achieves considerably higher clustering accuracy (10 to 20% absolute increase in our experiments) than the popular K-Means and agglomerative clustering methods.

#index 458759
#* Dynamically Optimizing High-Dimensional Index Structures
#@ Christian Böhm;Hans-Peter Kriegel
#t 2000
#c 8
#% 32913
#% 102772
#% 137887
#% 158905
#% 164360
#% 169940
#% 172949
#% 198573
#% 213975
#% 227939
#% 227999
#% 237187
#% 248796
#% 317313
#% 321455
#% 421052
#% 435141
#% 460862
#% 464195
#% 464841
#% 464859
#% 479649
#% 480610
#% 481609
#% 481620
#% 481947
#% 481956
#% 482109
#% 527026
#% 571100
#% 632035
#! In high-dimensional query processing, the optimization of the logical page-size of index structures is an important research issue. Even very simple query processing techniques such as the sequential scan are able to outperform indexes which are not suitably optimized. Page-size optimization based on a cost model faces the problem, that the optimum not only depends on static schema information such as the dimension of the data space but also on dynamically changing parameters such as the number of objects stored in the database and the degree of clustering and correlation in the current data set. Therefore, we propose a method for adapting the page size of an index dynamically during insert processing. Our solution, called DABS-tree, uses a flat directory whose entries consist of an MBR, a pointer to the data page and the size of the data page. Before splitting pages in insert operations, a cost model is consulted to estimate whether the split operation is beneficial. Otherwise, the split is avoided and the logical page-size is adapted instead. A similar rule applies for merging when performing delete operations. We present an algorithm for the management of data pages with varying page-sizes in an index and show that all restructuring operations are locally restricted. We show in our experimental evaluation that the DABS tree outperforms the X-tree by a factor up to 4.6 and the sequential scan by a factor up to 6.6.

#index 458760
#* An Architecture for Management of Large, Distributed, Scientific Data Using SQL/MED and XML
#@ Mark Papiani;Jasmin Wason;Denis A. Nicole
#t 2000
#c 8
#% 213980
#% 278619
#% 346740
#% 482087
#% 1167472
#! We have developed a Web-based architecture and user interface for archiving and manipulating results of numerical simulations being generated by the UK Turbulence Consortium on the United Kingdom's new national scientific supercomputing resource. These simulations produce large datasets, requiring Web-based mechanisms for storage, searching and retrieval of simulation results in the hundreds of gigabytes range. We demonstrate that the new DATALINK type, defined in the draft SQL Management of External Data Standard, which facilitates database management of distributed external data, can help to overcome problems associated with limited bandwidth. We show that a database can meet the apparently divergent requirements of storing both the relatively small simulation result metadata, and the large result files, in a unified way, whilst maintaining database security, recovery and integrity. By managing data in this distributed way, the system allows post-processing of archived simulation results to be performed directly without the cost of having to rematerialise to files. This distribution also reduces access bottlenecks and processor loading. We also show that separating the user interface specification from the user interface processing can provide a number of advantages. We provide a tool to generate automatically a default user interface specification, in the form of an XML document, for a given database. The XML document can be customised to change the appearance of the interface. Our architecture can archive not only data in a distributed fashion, but also applications. These applications are loosely coupled to the datasets (in a many-to-many relationship) via XML defined interfaces. They provide reusable server-side post-processing operations such as data reduction and visualisation.

#index 458761
#* Evolution and Revolutions in LDAP Directory Caches
#@ Olga Kapitskaia;Raymond T. Ng;Divesh Srivastava
#t 2000
#c 8
#% 152939
#% 201696
#% 248806
#% 273705
#% 273707
#% 360716
#% 408396
#% 479958
#% 480780
#% 481916
#% 566126
#% 571072
#% 571216
#! LDAP directories have recently proliferated with the growth of the Internet, and are being used in a wide variety of network-based applications. In this paper, we propose the use of generalized queries, referred to as query templates, obtained by generalizing individual user queries, as the semantic basis for low overhead, high benefit LDAP directory caches for handling declarative queries. We present efficient incremental algorithms that, given a sequence of user queries, maintain a set of potentially beneficial candidate query templates, and select a subset of these candidates for admission into the directory cache. A novel feature of our algorithms is their ability to deal with overlapping query templates. Finally, we demonstrate the advantages of template caches over query caches, with an experimental study based on real data and a prototype implementation of the LDAP directory cache.

#index 458762
#* Efficient Discovery of Functional Dependencies and Armstrong Relations
#@ Stéphane Lopes;Jean-Marc Petit;Lotfi Lakhal
#t 2000
#c 8
#% 6710
#% 6714
#% 18398
#% 89751
#% 125557
#% 129564
#% 169370
#% 172386
#% 193291
#% 248815
#% 275367
#% 280436
#% 289384
#% 289446
#% 384978
#% 387089
#% 420062
#% 464837
#% 477949
#% 481290
#% 632077
#! In this paper, we propose a new efficient algorithm called Dep-Miner for discovering minimal non-trivial functional dependencies from large databases. Based on theoretical foundations, our approach combines the discovery of functional dependencies along with the construction of real-world Armstrong relations (without additional execution time). These relations are small Armstrong relations taking their values in the initial relation. Discovering both minimal functional dependencies and real-world Armstrong relations facilitate the tasks of database administrators when maintaining and analyzing existing databases. We evaluate Dep-Miner performances by using a new benchmark database. Experimental results show both the efficiency of our approach compared to the best current algorithm (i.e. Tane), and the usefulness of real-world Armstrong relations.

#index 458763
#* An Approach to the Semi-Automatic Generation of Mediator Specifications
#@ Birgitta König-Ries
#t 2000
#c 8
#% 22948
#% 55294
#% 58376
#% 116303
#% 169050
#% 194968
#% 273922
#% 274160
#% 285421
#% 400359
#% 464724
#% 479465
#% 479783
#% 481280
#% 511734
#% 535824
#% 631928
#! Mediator architectures have become popular for systems that aim at providing transparent access to heterogeneous information sources. Similar to a view on a database, a mediator answers queries posed against the common domain schema by executing one or more queries against its underlying information sources and mapping the results back into the common schema. The mapping is usually specified declaratively by a set of rules, where the rule heads define queries against a common domain schema and the rule bodies define their implementations in terms of queries against one or more source schemas. In this paper a mechanism is presented that supports finding these mappings, a task that requires a fair amount of knowledge about the underlying schemas and that involves a lot of effort, thus becoming a major obstacle to scalability.

#index 458764
#* Aggregate Aware Caching for Multi-Dimensional Queries
#@ Prasad Deshpande;Jeffrey F. Naughton
#t 2000
#c 8
#% 210182
#% 248040
#% 248806
#% 273917
#% 459024
#% 463760
#% 479646
#% 481916
#% 481951
#% 482081
#% 566126
#% 710694
#! To date, work on caching for OLAP workloads has focussed on using cached results from a previous query as the answer to another query. This strategy is effective when the query stream exhibits a high degree of locality. It unfortunately misses the dramatic performance improvements obtainable when the answer to a query, while not immediately available in the cache, can be computed from data in the cache. In this paper, we consider the common subcase of answering queries by aggregating data in the cache. In order to use aggregation in the cache, one must solve two subproblems: (1) determining when it is possible to answer a query by aggregating data in the cache, and (2) determining the fastest path for this aggregation, since there can be many.We present two strategies - a naive one and a Virtual Count based strategy. The virtual count based method finds if a query is computable from the cache almost instantaneously, with a small overhead of maintaining the summary state of the cache. The algorithm also maintains cost-based information that can be used to figure out the best possible option for computing a query result from the cache. Experiments with our implementation show that aggregation in the cache leads to substantial performance improvement. The virtual count based methods further improve the performance compared to the naive approaches, in terms of cache lookup and aggregation times.

#index 458765
#* Parametric Rectangles: A Model for Querying and Animation of Spatiotemporal Databases
#@ Mengchu Cai;Dinesh Keshwani;Peter Z. Revesz
#t 2000
#c 8
#% 135384
#% 181034
#% 190329
#% 190332
#% 246560
#% 260062
#% 263982
#% 287364
#% 287365
#% 346656
#% 403487
#% 419914
#% 527038
#% 618583
#! We propose parametric rectangles -- cross products of intervals whose end points are functions of time -- as a new data model for representing, querying, and animating spatiotemporal objects with continuous and periodic change. We prove that the model is closed under relational algebra and new spatiotemporal operators and that relational algebra queries can be evaluated in PTIME in the size of any input quadratic non-periodic parametric rectangle database. Finally, we also describe the implementation in our PReSTO database system.

#index 458766
#* Automatic Deployment of Application-Specific Metadata and Code in MOCHA
#@ Manuel Rodriguez-Martinez;Nick Roussopoulos
#t 2000
#c 8
#% 152902
#% 479449
#% 479651
#% 631868
#! Database middleware systems require the deployment of application-specific data types and query operators to the servers and clients in the system. Existing middleware solutions rely on developers and system administrators to port and manually install all this application-specific functionality to all sites in the system. This approach cannot scale to an environment in which there are hundreds of data sources, such as those accessed by the Web and even more custom-tailored applications, since the complexity and the cost involved in maintaining a code base system-wide are enormous. This paper describes a novel metadata-driven framework designed to automate the deployment of all application-specific functionality used by a middleware system. We used Java and XML to implement this framework in MOCHA, a middleware system developed at the University of Maryland. We first present the kind of services, metadata elements and software tools used in MOCHA to automate code deployment. Then, we describe how the features of MOCHA simplify the administration and reduce the management cost of a middleware system in a large scale environment.

#index 458767
#* Temporal View Self-Maintenance
#@ Jun Yang;Jennifer Widom
#t 2000
#c 8
#% 18615
#% 59350
#% 178813
#% 189636
#% 198467
#% 199537
#% 340301
#% 442971
#% 458556
#% 459026
#% 479621
#% 527790
#! View self-maintenance refers to maintaining materialized views without accessing base data. Self-maintenance is particularly useful in data warehousing settings, where base data comes from sources that may be inaccessible. Self-maintenance has been studied for nontemporal views, but is even more important when a warehouse stores temporal views over the history of source data, since the source history needed to perform view maintenance may no longer exist. This paper tackles the self-maintenance problem for temporal views. We show how to derive auxiliary data to be stored at the warehouse so that the warehouse views and auxiliary data can be maintained without accessing the sources. The temporal view self-maintenance problem is considerably harder than the nontemporal case because a temporal view may need to be maintained not only when source data is modified but also as time advances, and these two dimensions of change interact in subtle ways. We also seek to minimize the amount of auxiliary data required, taking into account different source capabilities and update constraints that are common in temporal warehousing scenarios. While our framework and algorithms are presented using a true temporal data model, our results apply directly to the ad-hoc temporal support (i.e., timestamp attributes in the standard relational model) commonly found in data warehouses today.

#index 458768
#* Materialized View Selection for Multi-Cube Data Models
#@ Amit Shukla;Prasad Deshpande;Jeffrey F. Naughton
#t 2000
#c 8
#% 53706
#% 210182
#% 239243
#% 368248
#% 462204
#% 464215
#% 464706
#% 479476
#% 479646
#% 481948
#% 710234
#! OLAP applications use precomputation of aggregate data to improve query response time. While this problem has been well-studied in the recent database literature, to our knowledge all previous work has focussed on the special case in which all aggregates are computed from a single cube (in a star schema, this corresponds to there being a single fact table). This is unfortunate, because many real world applications require aggregates over multiple fact tables. In this paper, we attempt to fill this lack of discussion about the issues arising in multi-cube data models by analyzing these issues. Then we examine performance issues by studying the precomputation problem for multi-cube systems. We show that this problem is significantly more complex than the single cube precomputation problem, and that algorithms and cost models developed for single cube precomputation must be extended to deal well with the multi-cube case. Our results from a prototype implementation show that for multicube workloads substantial performance improvements can be realized by using the multi-cube algorithms.

#index 458769
#* Performance of DB2 Enterprise-Extended Edition on NT with Virtual Interface Architecture
#@ Sivakumar Harinath;Robert L. Grossman;K. Bernhard Schiefer;Xun Xue;Sadique Syed
#t 2000
#c 8
#% 346684
#! DB2 Universal Database Enterprise-Extended Edition (DB2 UDB EEE) is a parallel relational database management system using a shared-nothing architecture. DB2 UDB EEE uses multiple nodes connected by an interconnect and partitions data across these nodes. The communication protocol used between nodes of DB2 UDB EEE has historically been Transmission Control Protocol (TCP) / Internet Protocol (IP) but has now been extended to include the Virtual Interface (VI) Architecture. This paper discusses a new protocol termed Virtual Interface Protocol (VIP), built on top of the primitives provided by the VI Architecture. DB2 UDB EEE with VIP on a fast interconnect has shown significant improvement in reducing the elapsed time of queries when compared with TCP/IP over fast ethernet. This paper discusses the implementation and performance results on a Transaction Processing Council's Decision (TPC-D) support database.

#index 458770
#* Slim-Trees: High Performance Metric Trees Minimizing Overlap Between Nodes
#@ Caetano Traina, Jr.;Agma J. M. Traina;Bernhard Seeger;Christos Faloutsos
#t 2000
#c 8
#% 86950
#% 164360
#% 227937
#% 237187
#% 252304
#% 281750
#% 322309
#% 427199
#% 437509
#% 479462
#% 480093
#% 481460
#% 546130
#! In this paper we present the Slim-tree, a dynamic tree for organizing metric datasets in pages of fixed size. The Slim-tree uses the "fat-factor" which provides a simple way to quantify the degree of overlap between the nodes in a metric tree. It is well-known that the degree of overlap directly affects the query performance of index structures. There are many suggestions to reduce overlap in multidimensional index structures, but the Slim-tree is the first metric structure explicitly designed to reduce the degree of overlap. Moreover, we present new algorithms for inserting objects and splitting nodes. The new insertion algorithm leads to a tree with high storage utilization and improved query performance, whereas the new split algorithm runs considerably faster than previous ones, generally without sacrificing search performance. Results obtained from experiments with real-world data sets show that the new algorithms of the Slim-tree consistently lead to performance improvements. After performing the Slim-down algorithm, we observed improvements up to a factor of 35% for range queries.

#index 458771
#* Persistent Client-Server Database Sessions
#@ Roger S. Barga;David B. Lomet;Thomas Baby;Sanjay Agrawal
#t 2000
#c 8
#% 245790
#% 248823
#% 464823
#% 481624
#! Database systems support recovery, providing high database availability. However, database applications may lose work because of a server failure. In particular, if a database server crashes, volatile server state associated with a client application's session is lost and applications may require operator-assisted restart. This prevents masking server failures and degrades application availability. In this paper, we show how to provide persistent database sessions to client applications across server failures, without the application itself needing to take measures for its recoverability. This offers improved application availability and reduces the application programming task of coping with system errors. Our approach is based on (i) capturing client application's interactions with the database server and (ii) materializing database session state as persistent database tables that are logged on the database server. We exploit a virtual database session. Our procedures detect database server failure and re-map the virtual session to a new session into which we install the saved old session state once the server has recovered. This integrates database server recovery and transparent session recovery. The result is persistent client-server database sessions that survive a server crash without the client application being aware of the outage, except for possible timing considerations. We demonstrate the viability of this approach by describing the design and implementation of Phoenix/ODBC, a prototype system that provides persistent ODBC database sessions; and present early results from a performance evaluation on the costs to persist and recover ODBC database sessions.

#index 458772
#* Trading Quality for Time with Nearest Neighbor Search
#@ Roger Weber;Klemens Böhm
#t 2000
#c 8
#% 86950
#% 213981
#% 217528
#% 227856
#% 227939
#% 248017
#% 248798
#% 427199
#% 479462
#% 479649
#% 479973
#% 481956
#! In many situations, users would readily accept an approximate query result if evaluation of the query becomes faster. In this article, we investigate approximate evaluation techniques based on the VA-File for Nearest-Neighbor Search (NN-Search). The VA-File contains approximations of feature points. These approximations frequently suffice to eliminate the vast majority of points in a first phase. Then, a second phase identifies the NN by computing exact distances of all remaining points. To develop approximate query-evaluation techniques, we proceed in two steps: first, we derive an analytic model for VA-File based NN-search. This is to investigate the relationship between approximation granularity, effectiveness of the filtering step and search performance. In more detail, we develop formulae for the distribution of the error of the bounds and the duration of the different phases of query evaluation. Based on these results, we develop different approximate query evaluation techniques. The first one adapts the bounds to have a more rigid filtering, the second one skips computation of the exact distances. Experiments show that these techniques have the desired effect: for instance, when allowing for a small but specific reduction of result quality, we observed a speedup of 7 in 50-NN search.

#index 458773
#* Querying Graph Databases
#@ Sergio Flesca;Sergio Greco
#t 2000
#c 8
#% 115436
#% 197751
#% 210205
#% 210214
#% 227894
#% 227995
#% 237192
#% 237193
#% 268781
#% 268782
#% 268797
#% 268799
#% 384978
#% 404772
#% 464693
#% 464719
#% 464720
#% 481602
#! Graph data is an emerging model for representing a variety of database contexts ranging from object-oriented databases to hypertext data. Also many of the recursive queries that arise in relational databases are, in practice, graph traversals. In this paper we present a language for searching graph-like databases. The language permits us to express paths in a graph by means of extended regular expressions. The proposed extension is based on the introduction of constructs which permit us i) to define a partial order on the paths used to search the graph and, consequently, on the answers of queries, and ii) to cut off, nondeterministically, tuples with low priority. We present an algebra for partially ordered relations and an algorithm for the computation of path queries. Finally, we present applications to hypertext databases such as the Web.

#index 458774
#* Performance and Availability Assessment for the Configuration of Distributed Workflow Management Systems
#@ Michael Gillmann;Jeanine Weißenfels;Gerhard Weikum;Achim Kraiss
#t 2000
#c 8
#% 29439
#% 185412
#% 187079
#% 207505
#% 266179
#% 384639
#% 391633
#% 403195
#% 437739
#% 459005
#% 511743
#% 565496
#% 614638
#! Workflow management systems (WFMSs) that are geared for the orchestration of enterprise-wide or even "virtual-enterprise"-style business processes across multiple organizations are complex distributed systems. They consist of multiple workflow engines, application servers, and ORB-style communication servers. Thus, deriving a suitable configuration of an entire distributed WFMS for a given application workload is a difficult task. This paper presents a mathematically based method for configuring a distributed WFMS such that the application's demands regarding performance and availability can be met while aiming to minimize the total system costs. The major degree of freedom that the configuration method considers is the replication of the underlying software components, workflow engines and application servers of different types as well as the communication server, on multiple computers for load partitioning and enhanced availability. The mathematical core of the method consists of Markov-chain models, derived from the application's workflow specifications, that allow assessing the overall system's performance, availability, and also its performability in the degraded mode when some server replicas are offline, for given degrees of replication. By iterating over the space of feasible system configurations and assessing the quality of candidate configurations, the developed method determines a configuration with near-minimum costs.

#index 458775
#* A Database Perspective on Building Large Applications - Experience Report
#@ Joachim Thomas;Prithwish Kangsabanik
#t 2000
#c 8
#% 94770
#% 116185
#% 168251
#% 208227
#% 248318
#% 259798
#% 385828
#% 403195
#% 1306114
#! Processing of large applications is inherently associated with database technology, since large applications usually require to process huge amounts of data. The banking sector was among the first commercial settings where large applications and database technology played an important role. This paper summarizes our experiences acquired through conducting a number of application projects at UBS AG. From this perspective, it points out chances and risks with modern software technology for building large database applications.

#index 458776
#* Tamino - An Internet Database System
#@ Harald Schöning;Jürgen Wäsch
#t 2000
#c 8
#! Software AG's Tamino is a novel database server designed to fit the needs of electronic business and worldwide information exchange via the Internet. It is not just an on-top solution based on a database system originally designed for use in other application areas. Rather, it is entirely designed for the specific scenario of HTTP-based access to data represented in XML. These data can stem from various sources, and can be combined on the fly when a corresponding request is encountered. This paper sketches the architecture and the functional features of Tamino, and justifies its various design decisions.

#index 458777
#* XML: Current Developments and Future Challenges for the Database Community
#@ Stefano Ceri;Piero Fraternali;Stefano Paraboschi
#t 2000
#c 8
#% 275367
#% 281149
#% 281150
#% 308463
#% 479465
#% 479981
#! While we can take as a fact "the Web changes everything", we argue that "XML is the means" for such a change to make a significant step forward. We therefore regard XML-related research as the most promising and challenging direction for the community of database researchers. In this paper, we approach XML-related research by taking three progressive perspectives.We first consider XML as a data representation standard (in the small), then as a data interchange standard (in the large), and finally as a basis for building a new repository technology. After a broad and necessarily coarse-grain analysis, we turn our focus to three specific research projects which are currently ongoing at the Politecnico di Milano, concerned with XML query languages, with active document management, and with XML-based specifications of Web sites.

#index 458778
#* Navigation-Driven Evaluation of Virtual Mediated Views
#@ Bertram Ludäscher;Yannis Papakonstantinou;Pavel Velikhov
#t 2000
#c 8
#% 210205
#% 238757
#% 248799
#% 286973
#% 292230
#% 387508
#% 463594
#% 481935
#! The MIX mediator systems incorporates a novel framework for navigation-driven evaluation of virtual mediated views. Its architecture allows the on-demand computation of views and query results as the user navigates them. The evaluation scheme minimizes superfluous source access through the use of lazy mediators that translate incoming client navigations on virtual XML views into navigations on lower level mediators or wrapped sources. The proposed demand-driven approach is inevitable for handling up-to-date mediated views of largeWeb sources or query results. The non-materialization of the query answer is transparent to the client application since clients can navigate the query answer using a subset of the standard DOM API for XML documents. We elaborate on query evaluation in such a framework and show how algebraic plans can be implemented as trees of lazy mediators. Finally, we present a new buffering technique that can mediate between the fine granularity of DOM navigations and the coarse granularity of real world sources. This drastically reduces communication overhead and also simplifies wrapper development. An implementation of the system is available on the Web.

#index 458820
#* A Systematic Approach for Informal Communication During Workflow Execution
#@ Christoph Bussler
#t 2000
#c 8
#% 173861
#! Informal communication is often necessary during workflow execution despite all attempts to define workflow types completely. Traditionally, workflow participants use an e-mail system to communicate informally during (formal) workflow execution. However, the use of two independent software systems causes a set of significant problems due to the systems' autonomy: a disjoint data set makes common management and history analysis impossible. This paper analyzes the problems and suggests a systematic approach for informal communication. An extended workflow architecture is reported on that implements informal coordination with a WFMS's workflow concepts.

#index 458821
#* The Dynamic Data Cube
#@ Steven Geffner;Divyakant Agrawal;Amr El Abbadi
#t 2000
#c 8
#% 210182
#% 227866
#% 462204
#% 464215
#% 481948
#% 481951
#% 631947
#% 670380
#% 682251
#! Range sum queries on data cubes are a powerful tool for analysis. A range sum query applies an aggregation operation (e.g., SUM, AVERAGE) over all selected cells in a data cube, where the selection is specified by providing ranges of values for numeric dimensions. We present the Dynamic Data Cube, a new approach to range sum queries which provides efficient performance for both queries and updates, which handles clustered and sparse data gracefully, and which allows for the dynamic expansion of the data cube in any direction.

#index 458822
#* A Graph-Oriented Model for Articulation of Ontology Interdependencies
#@ Prasenjit Mitra;Gio Wiederhold;Martin L. Kersten
#t 2000
#c 8
#% 85086
#% 108504
#% 266102
#% 268799
#% 458607
#% 463919
#% 480614
#! Ontologies explicate the contents, essential properties, and relationships between terms in a knowledge base. Many sources are now accessible with associated ontologies. Most prior work on the use of ontologies relies on the construction of a single global ontology covering all sources. Such an approach is not scalable and maintainable especially when the sources change frequently. We propose a scalable and easily maintainable approach based on the interoperation of ontologies. To handle user queries crossing the boundaries of the underlying information systems, the interoperation between the ontologies should be precisely defined. Our approach is to use rules that cross the semantic gap by creating an articulation or linkage between the systems. The rules are generated using a semi-automatic articulation tool with the help of a domain expert. To make the ontologies amenable for automatic composition, based on the accumulated knowledge rules, we represent them using a graph-oriented model extended with a small algebraic operator set. ONION, a user-friendly toolkit, aids the experts in bridging the semantic gap in real-life settings. Our framework provides a sound foundation to simplify the work of domain experts, enables integration with public semantic dictionaries, like Wordnet, and will derive ODMG-compliant mediators automatically.

#index 458823
#* Universal Quantification in Relational Databases: A Classification of Data and Algorithms
#@ Ralf Rantzau;Leonard D. Shapiro;Bernhard Mitschang;Quan Wang
#t 2002
#c 8
#% 58359
#% 136740
#% 189638
#% 210183
#% 411662
#% 415987
#% 463894
#% 480091
#% 482103
#% 495266
#! Queries containing universal quantification are used in many applications, including business intelligence applications. Several algorithms have been proposed to implement universal quantification efficiently. These algorithms are presented in an isolated manner in the research literature - typically, no relationships are shown between them. Furthermore, each of these algorithms claims to be superior to others, but in fact each algorithm has optimal performance only for certain types of input data. In this paper, we present a comprehensive survey of the structure and performance of algorithms for universal quantification. We introduce a framework for classifying all possible kinds of input data for universal quantification. Then we go on to identify the most efficient algorithm for each such class. One of the input data classes has not been covered so far. For this class, we propose several new algorithms. For the first time, we are able to identify the optimal algorithm to use for any given input dataset. These two classifications of input data and optimal algorithms are important for query optimization. They allow a query optimizer to make the best selection when optimizing at intermediate steps for the quantification problem.

#index 458824
#* Gene Expression Data Management: A Case Study
#@ Victor M. Markowitz;I-Min A. Chen;Anthony Kosky
#t 2002
#c 8
#% 223781
#% 589371
#! One of the major challenges facing scientists dealing with gene expression data is how to integrate, explore and analyze vast quantities of related data, often residing in multiple heterogeneous data repositories. In this paper we describe the problems involved in managing gene expression data and discuss how these problems have been addressed in the context of Gene Logic's GeneExpress system. The GeneExpress system provides support for the integration of gene expression, gene annotation and sample (clinical) data with various degrees of heterogeneity, and for effective exploration of these data.

#index 458825
#* TDB: A Database System for Digital Rights Management
#@ Radek Vingralek;Umesh Maheshwari;William Shapiro
#t 2002
#c 8
#% 102782
#% 107692
#% 242188
#% 365700
#% 403195
#% 539418
#% 963644
#% 978227
#! In this paper we describe the architecture and implementation of an embedded database system for Digital Rights Management (DRM), TDB. We concentrate on the aspects of TDB's design, which were affected by the requirements of DRM applications. We show that, although it provides additional functionality for DRM applications, TDB performs better then Berkeley DB and has a code footprint comparable to other embedded database systems.

#index 458826
#* With HEART Towards Response Time Guarantees for Message-Based e-Services
#@ Achim Kraiss;Frank Schön;Gerhard Weikum;Uwe Deppisch
#t 2002
#c 8
#% 309705
#% 543875
#! The HEART tool (Help for Ensuring Acceptable Response Times) has been developed by the IT Research and Innovations department of Dresdner Bank for the computation of viable message prioritization in message-based e-services, such as stock brokerage services where service requests of different customer classes with class-specific performance goals have to be served by a server. HEART determines viable message prioritizations in the sense that they satisfy the specified performance goals of customer classes. In this paper, we describe the practical problem setting we address with HEART and outline the functionality of HEART. The demo will show HEART's underlying concepts, its architecture and an example scenario.

#index 458827
#* Composition of Mining Contexts for Efficient Extraction of Association Rules
#@ Cheikh Talibouya Diop;Arnaud Giacometti;Dominique Laurent;Nicolas Spyratos
#t 2002
#c 8
#% 201894
#% 227917
#% 232136
#% 248784
#% 280454
#% 420087
#% 420101
#% 464204
#% 478284
#% 481754
#% 487532
#% 599549
#! Association rule mining often requires the repeated execution of some extraction algorithm for different values of the support and confidence thresholds, as well as for different source datasets. This is an expensive process, even if we use the best existing algorithms. Hence the need for incremental mining, whereby mining results already obtained can be used to accelerate subsequent steps in the mining process.In this paper, we present an approach for the incremental mining of multi-dimensional association rules. In our approach, association rule mining takes place in a mining context which specifies the form of rules to be mined. Incremental mining is obtained by combining mining contexts using relational algebra operations.

#index 458828
#* Schema-Driven Evaluation of Approximate Tree-Pattern Queries
#@ Torsten Schlieder
#t 2002
#c 8
#% 289193
#% 308463
#% 340911
#% 340914
#% 387427
#% 479465
#% 504581
#% 546105
#% 546123
#! We present a simple query language for XML, which supports hierarchical, Boolean-connected query patterns. The interpretation of a query is founded on cost-based query transformations: The total cost of a sequence of transformations measures the similarity between the query and the data and is used to rank the results. We introduce two polynomial-time algorithms that efficiently find the best n answers to the query: The first algorithm finds all approximate results, sorts them by increasing cost, and prunes the result list after the nthen try. The second algorithm uses a structural summary -the schema- of the database to estimate the best k transformed queries, which in turn are executed against the database. We compare both approaches and show that the schema-based evaluation outperforms the pruning approach for small values of n. The pruning strategy is the better choice if n is close to the total number of approximate results for the query.

#index 458829
#* The Index-Based XXL Search Engine for Querying XML Data with Relevance Ranking
#@ Anja Theobald;Gerhard Weikum
#t 2002
#c 8
#% 236416
#% 248801
#% 262069
#% 268079
#% 282102
#% 283050
#% 290830
#% 291299
#% 299941
#% 309726
#% 387427
#% 458744
#% 458822
#% 479465
#% 504580
#% 504581
#% 541480
#% 571098
#% 978382
#! Query languages for XML such as XPath or XQuery support Boolean retrieval: a query result is a (possibly restructured) subset of XML elements or entire documents that satisfy the search conditions of the query. This search paradigm works for highly schematic XML data collections such as electronic catalogs. However, for searching information in open environments such as the Web or intranets of large corporations, ranked retrieval is more appropriate: a query result is a rank list of XML elements in descending order of (estimated) relevance. Web search engines, which are based on the ranked retrieval paradigm, do, however, not consider the additional information and rich annotations provided by the structure of XML documents and their element names. This paper presents the XXL search engine that supports relevance ranking on XML data. XXL is particularly geared for path queries with wildcards that can span multiple XML collections and contain both exact-match as well as semantic-similarity search conditions. In addition, ontological information and suitable index structures are used to improve the search efficiency and effectiveness. XXL is fully implemented as a suite of Java servlets. Experiments with a variety of structurally diverse XML data demonstrate the efficiency of the XXL search engine and underline its effectiveness for ranked retrieval.

#index 458830
#* Semantic Analysis of Business Process Executions
#@ Fabio Casati;Ming-Chien Shan
#t 2002
#c 8
#% 275367
#% 284600
#% 480644
#! Business Process Management Systems log a large amount of operational data about processes and about the (human and automated) resources involved in their executions. This information can be analyzed for assessing the quality of business operations, identify problems, and suggest solutions. However, current process analysis systems lack the functionalities required to provide information that can be immediately digested and used by business analysts to take decisions. In this paper we discuss the limitations of existing approaches and we present a system and a set of techniques, developed at Hewlett-Packard, that overcome this limitations, enabling the use of log data for efficient business-level analysis of business processes.

#index 458831
#* Supporting Efficient Parametric Search of E-Commerce Data: A Loosely-Coupled Solution
#@ Min Wang;Yuan-Chi Chang;Sriram Padmanabhan
#t 2002
#c 8
#% 43163
#% 152934
#% 210160
#% 210190
#% 248822
#% 273901
#% 286258
#% 300120
#% 333948
#% 387508
#% 411554
#% 427219
#% 464177
#% 464204
#% 480154
#% 480629
#% 481290
#% 482092
#! Electronic commerce is emerging as a major application area for database systems. A large number of e-commerce sites provide electronic product catalogs that allow users to search products of interest.Due to the constant evolution and the high sparsity of e-commerce data, most commercial e-commerce systems use the so-called vertical schema for data storage. However, query processing for data stored using vertical schema is extremely slow because current RDBMS, especially its cost-based query optimizer, is designed to deal with traditional horizontal schema efficiently.Most e-commerce systems would like to offer advanced parametric search capabilities to their users. However, most searches are expected to be on-line which means that the query execution should be very fast. RDBMSs require new capabilities and enhancements before they can satisfy the search performance criteria against vertical schema. The tightly-coupled enhancements and additions to a DBMS require considerable amount of work and may take a long time to be accomplished. In this paper, we describe an alternative approach called SAL, a Search Assistant Layer that can be implemented outside a database engine to accommodate the urgent need for efficient parametric search on e-commerce data. Our experimental results show that dramatic performance improvement is provided by SAL for search queries.

#index 458832
#* Rewriting Unions of General Conjunctive Queries Using Views
#@ Junhu Wang;Michael J. Maher;Rodney W. Topor
#t 2002
#c 8
#% 146277
#% 237190
#% 248038
#% 300385
#% 303884
#% 379503
#% 396745
#% 464203
#% 480149
#% 481923
#% 563152
#% 564419
#% 993397
#! The problem of finding contained rewritings of queries using views is of great importance in mediated data integration systems. In this paper, we first present a general approach for finding contained rewritings of unions of conjunctive queries with arbitrary built-in predicates. Our approach is based on an improved method for testing conjunctive query containment in this context. Although conceptually simple, our approach generalizes previous methods for finding contained rewritings of conjunctive queries and is more powerful in the sense that many rewritings that can not be found using existing methods can be found by our approach. Furthermore, nullity-generating dependencies over the base relations can be easily handled. We then present a simplified approach which is less complete, but is much faster than the general approach, and it still finds maximum rewritings in several special cases. Our approaches compare favorably with existing methods.

#index 458833
#* Profit Mining: From Patterns to Actions
#@ Ke Wang;Senqiang Zhou;Jiawei Han
#t 2002
#c 8
#% 136350
#% 152934
#% 220706
#% 232136
#% 280437
#% 280456
#% 420082
#% 443092
#% 465754
#% 481290
#% 481588
#% 481758
#! A major obstacle in data mining applications is the gap between the statistic-based pattern extraction and the value-based decision making. We present a profit mining approach to reduce this gap. In profit mining, we are given a set of past transactions and pre-selected target items, and we like to build a model for recommending target items and promotion strategies to new customers, with the goal of maximizing the net profit. We identify several issues in profit mining and propose solutions. We evaluate the effectiveness of this approach using data sets of a wide range of characteristics.

#index 458834
#* A Systematic Approach to Selecting Maintenance Policies in a Data Warehouse Environment
#@ Henrik Engström;Sharma Chakravarthy;Brian Lings
#t 2002
#c 8
#% 6798
#% 13015
#% 13016
#% 32914
#% 107685
#% 199537
#% 201928
#% 210182
#% 210210
#% 210211
#% 223781
#% 227944
#% 227945
#% 227947
#% 261689
#% 277332
#% 287324
#% 340300
#% 442663
#% 462645
#% 464878
#% 489525
#% 708678
#% 993493
#! Most work on data warehousing addresses aspects related to the internal operation of a data warehouse server, such as selection of views to materialise, maintenance of aggregate views and performance of OLAP queries. Issues related to data warehouse maintenance, i.e. how changes to autonomous sources should be detected and propagated to a warehouse, have been addressed in a fragmented manner. Although data propagation policies, source database capabilities, and user requirements have been addressed individually, their co-dependencies and relationships have not been explored. In this paper, we present a comprehensive framework for evaluating data propagation policies against data warehouse requirements and source capabilities. We formalize data warehouse specification along the dimensions of staleness, response time, storage, and computation cost, and classify source databases according to their data propagation capabilities. A detailed cost-model is presented for a representative set of policies. A prototype tool has been developed to allow an exploration of the various trade-offs.

#index 458835
#* Efficient and Adaptive Processing of Multiple Continuous Queries
#@ Wee Hyong Tok;Stéphane Bressan
#t 2002
#c 8
#% 36117
#% 116082
#% 248795
#% 264691
#% 300166
#% 300167
#% 300179
#% 335726
#% 340305
#% 443298
#% 464056
#% 465151
#% 480296
#% 554726
#% 631962
#! Continuous queries are queries executed on data streams within a potentially open-ended time interval specified by the user and are usually long running. The data streams are likely to exhibit fluctuating characteristics such as varying inter-arrival times, as well as varying data characteristics during the query execution. In the presence of such unpredictable factors, continuous query systems must still be able to efficiently handle large number of queries, as well as to offer acceptable individual query performance.In this paper, we propose and discuss a novel framework, called AdaptiveCQ, for the efficient processing of multiple continuous queries. In our framework, multiple queries share intermediate results at a fine level of granularity. Unlike previous approaches to sharing or reusing that relied on materialization to disk, AdaptiveCQ allows on-the-fly sharing of results. We show that this feature improves both the initial query response time, and the overall response time. Finally, AdaptiveCQ, which extrapolates the idea proposed by the eddy query-processing model, adapts well to fluctuations of the data streams characteristics by this combination of fine grain and on-the-fly sharing. We implemented AdaptiveCQ from scratch in Java and made use of it to conduct the experiments. We present experimental results that substantiate our claim that AdaptiveCQ can provide substantial performance improvements over existing methods of reusing intermediate results that relied on materialization to disk. In addition, we also show that AdaptiveCQ can adapt well to fluctuations in the query environment.

#index 458836
#* Estimating Answer Sizes for XML Queries
#@ Yuqing Wu;Jignesh M. Patel;H. V. Jagadish
#t 2002
#c 8
#% 43163
#% 201921
#% 268747
#% 273897
#% 299984
#% 333981
#% 461918
#% 465018
#% 479648
#% 479806
#% 480488
#% 481266
#% 571046
#% 616528
#% 650962
#! Estimating the sizes of query results, and intermediate results, is crucial to many aspects of query processing. In particular, it is necessary for effective query optimization. Even at the user level, predictions of the total result size can be valuable in "next-step" decisions, such as query refinement. This paper proposes a technique to obtain query result size estimates effectively in an XML database.Queries in XML frequently specify structural patterns, requiring specific relationships between selected elements. Whereas traditional techniques can estimate the number of nodes (XML elements) that will satisfy a node-specific predicate in the query pattern, such estimates cannot easily be combined to provide estimates for the entire query pattern, since element occurrences are expected to have high correlation.We propose a solution based on a novel histogram encoding of element occurrence position. With such position histograms, we are able to obtain estimates of sizes for complex pattern queries, as well as for simpler intermediate patterns that may be evaluated in alternative query plans, by means of a position histogram join (pH-join) algorithm that we introduce. We extend our technique to exploit schema information regarding allowable structure (the no-overlap property) through the use of a coverage histogram.We present an extensive experimental evaluation using several XML data sets, both real and synthetic, with a variety of queries. Our results demonstrate that accurate and robust estimates can be achieved, with limited space, and at a miniscule computational cost. These techniques have been implemented in the context of the TIMBER native XML database [22] at the University of Michigan.

#index 458837
#* Optimizing Scientific Databases for Client Side Data Processing
#@ Etzard Stolte;Gustavo Alonso
#t 2002
#c 8
#% 168862
#% 198465
#% 201980
#% 223781
#% 227872
#% 227883
#% 237179
#% 237190
#% 248812
#% 250246
#% 259995
#% 273902
#% 280448
#% 300171
#% 300185
#% 300193
#% 355270
#% 461458
#% 464056
#% 479984
#% 480306
#% 480465
#% 481452
#% 487519
#% 565500
#% 570889
#% 617861
#! Databases are nowadays one more building block in complex multi-tier architectures. In general, however, they are still designed and optimized with little regard for the applications that will run on top of them. This problem is particularly acute in scientific applications where the data is usually processed at the client and, hence, conventional server side optimizations are of limited help. In this paper we present a variety of techniques and a novel client/server architecture designed to optimize the client side processing of scientific data. The main building block in our approach is to store frequently accessed data as relatively small, wavelet encoded segments. These segments can be processed at different qualities and resolutions, thereby enabling efficient processing of very large data volumes. Experimental results demonstrate that our approach significantly reduces overhead (I/O, transfer across network, decoding and analysis), does not require changes to the analysis routines and provides all possible resolution ranges.

#index 458838
#* Divide-and-Conquer Algorithm for Computing Set Containment Joins
#@ Sergey Melnik;Hector Garcia-Molina
#t 2002
#c 8
#% 152938
#% 172913
#% 210187
#% 237204
#% 318437
#% 333866
#% 458759
#% 480463
#% 482121
#! A set containment join is a join between set-valued attributes of two relations, whose join condition is specified using the subset (驴) operator. Set containment joins are used in a variety of database applications. In this paper, we propose a novel partitioning algorithm called Divide-and-Conquer Set Join (DCJ) for computing set containment joins efficiently. We show that the divide-and-conquer approach outperforms previously suggested algorithms over a wide range of data sets. We present a detailed analysis of DCJ and previously known algorithms and describe their behavior in an implemented testbed.

#index 458839
#* Broadcast-Based Data Access in Wireless Environments
#@ Xu Yang;Athman Bouguettaya
#t 2002
#c 8
#% 169835
#% 172876
#% 287258
#% 380973
#% 443365
#% 632027
#% 635819
#% 640202
#! Broadcast is one of the most suitable forms of information dissemination over wireless networks. It is particularly attractive for resource limited mobile clients in asymmetric communications. To support faster access to information and conserve battery power of mobile clients, a number of indexing schemes have been proposed in recent years. In this paper, we report on our extensive study of some of the most representative indexing schemes. We present a novel adaptive testbed for evaluating wireless data access methods. A comprehensive analytical study of the sample indexing schemes is also presented. Exhaustive simulations of these indexing schemes have been conducted. As a result, selection criteria for the suitability of the indexing schemes for different applications are proposed.

#index 458840
#* The ORDB-Based SFB-501-Reuse-Repository
#@ Wolfgang Mahnke;Norbert Ritter
#t 2002
#c 8
#% 122890
#% 385828
#% 619264
#! Comprehensive reuse and systematic evolution of reuse artifacts as proposed by the Quality Improvement Paradigm (QIP) require an integrated management of (potentially reusable) experience data as well as project-related data. This demonstration presents an approach exploiting object-relational database technology to implement a QIP-driven reuse repository. Our SFB-501- Reuse-Repository is designed to support all phases of a reuse process and the accompanying improvement cycle by providing adequate functionality. Its implementation is based on object-relational database technology along with an infrastructure well suited for these purposes.

#index 458841
#* STYX: Connecting the XML Web to the World of Semantics
#@ Irini Fundulaki;Bernd Amann;Catriel Beeri;Michel Scholl;Anne-Marie Vercoustre
#t 2002
#c 8

#index 458842
#* Managing Web Sites with OntoWebber
#@ Yuhui Jin;Sichun Xu;Stefan Decker;Gio Wiederhold
#t 2002
#c 8
#% 479981
#% 489545
#% 565262
#% 571038
#! Onto Webber is a system for creating and managing data-intensive Web sites. It aims at reducing the efforts for publishing data as static and dynamic Web pages, personalizing user experience for browsing and navigating the data, and maintaining the Web site as well as the underlying data. Based on a domain ontology and a site modeling ontology, site views on the underlying data are constructed as site models. Instantiation of these models will create the browsable Web site. Rule-based manipulation of site models provides a declarative way to personalize and maintain the Web site. In this paper we present the architecture and demonstrate the major components of the system.

#index 458843
#* Temporal Aggregation over Data Streams Using Multiple Granularities
#@ Donghui Zhang;Dimitrios Gunopulos;Vassilis J. Tsotras;Bernhard Seeger
#t 2002
#c 8
#% 2115
#% 178068
#% 198467
#% 227883
#% 235114
#% 310500
#% 319601
#% 333874
#% 333926
#% 333977
#% 338425
#% 390187
#% 443198
#% 465010
#% 479621
#% 479847
#% 565462
#% 571296
#% 594012
#% 618582
#% 631922
#% 632071
#% 660007
#! Temporal aggregation is an important but costly operation for applications that maintain time-evolving data (data warehouses, temporal databases, etc.). In this paper we examine the problem of computing temporal aggregates over data streams. Such aggregates are maintained using multiple levels of temporal granularities: older data is aggregated using coarser granularities while more recent data is aggregated with finer detail. We present specialized indexing schemes for dynamically and progressively maintaining temporal aggregates. Moreover, these schemes can be parameterized. The levels of granularity as well as their corresponding index sizes (or validity lengths) can be dynamically adjusted. This provides a useful trade-off between aggregation detail and storage space. Analytical and experimental results show the efficiency of the proposed structures. Moreover, we discuss how the indexing schemes can be extended to solve the more general range temporal and spatio-temporal aggregation problems.

#index 458844
#* A Database-Supported Workbench for Information Fusion: INFUSE
#@ Oliver Dunemann;Ingolf Geist;Roland Jesse;Kai-Uwe Sattler;Andreas Stephanik
#t 2002
#c 8

#index 458845
#* UMiner: A Data Mining System Handling Uncertainty and Quality
#@ Christos Amanatidis;Maria Halkidi;Michalis Vazirgiannis
#t 2002
#c 8
#% 216499
#% 232102
#% 393792
#% 478277
#! In this paper we present UMiner, a new data mining system, which improves the quality of the data analysis results, handles uncertainty in the clustering & classification process and improves reasoning and decision-making.

#index 458846
#* Navigating Virtual Information Sources with Know-ME
#@ Xufei Qian;Bertram Ludäscher;Maryann E. Martone;Amarnath Gupta
#t 2002
#c 8
#! In many application domains such as biological sciences, information integration faces a challenge usually not observed in simpler applications. Here, the tobe-integrated information sources come from very different sub-specialties (e.g., anatomy and behavioral neuroscience) and have widely diverse schema, often with little or no overlap in attributes. Yet, they can be conceptually integrated because they refer to different aspects of the same physical objects or phenomena. We have proposed model-based mediation (MBM) as an information integration paradigm where information sources with hard-to-correlate schemas may be integrated using auxiliary expert knowledge to hold together widely different data schemas. The expert knowledge is captured in a graph structure called the Knowledge Map. In MBM, we extend the global-as-view architecture by lifting exported source data to conceptual models (CMs) that represent more source specific knowledge than a logical schema. The mediator's IVDs are defined in terms of source CMs and make use of a semantically richer model involving class hierarchies, complex object structure, and rule-defined semantic integrity constraints. Additionally, sources specify object contexts, i.e., formulas that relate a source's conceptual schema with the global domain knowledge maintained at the mediator. In this paper, we introduce a tool called Knowledge Map Explorer (Know-ME) for a user to explore both the domain knowledge, and all data sources that have been integrated using it.

#index 458847
#* On Efficient Matching of Streaming XML Documents and Queries
#@ Laks V. S. Lakshmanan;Sailaja Parthasarathy
#t 2002
#c 8
#% 116082
#% 300179
#% 333938
#% 333982
#% 443298
#% 464879
#% 480296
#% 511917
#% 562456
#% 631962
#% 632046
#% 659995
#! Applications such as online shopping, e-commerce, and supply-chain management require the ability to manage large sets of specifications of products and/or services as well as of consumer requirements, and call for efficient matching of requirements to specifications.Requirements are best viewed as "queries" and specifications as data, often represented in XML. We present a framework where requirements and specifications are both registered with and are maintained by a registry. On a periodical basis, the registry matches new incoming specifications, e.g., of products and services, against requirements, and notifies the owners of the requirements of matches found. This problem is dual to the conventional problem of database query processing in that the size of data (e.g., a document that is streaming by) is quite small compared to the number of registered queries (which can be very large). For performing matches efficiently, we propose the notion of a "requirements index", a notion that is dual to a traditional index. We provide efficient matching algorithms that use the proposed indexes. Our prototype MatchMaker system implementation uses our requirements index-based matching algorithms as a core and provides timely notification service to registered users. We illustrate the effectiveness and scalability of the techniques developed with a detailed set of experiments.

#index 458848
#* An Introduction to the e-XML Data Integration Suite
#@ Georges Gardarin;Antoine Mensch;Anthony Tomasic
#t 2002
#c 8
#% 194963
#% 300143
#% 333935
#% 480822
#% 565470
#% 614579
#! This paper describes the e-XML component suite, a modular product for integrating heterogeneous data sources under an XML schema and querying in real-time the integrated information using XQuery, the emerging W3C standard for XML query. We describe the two main components of the suite, i.e., the repository for warehousing XML and the mediator for distributed query processing. We also discuss some typical applications.

#index 458849
#* The Geometry of Uncertainty in Moving Objects Databases
#@ Goce Trajcevski;Ouri Wolfson;Fengli Zhang;Sam Chamberlain
#t 2002
#c 8
#% 2115
#% 135384
#% 252304
#% 257481
#% 257892
#% 260653
#% 273706
#% 295512
#% 299979
#% 300173
#% 315005
#% 461923
#% 464847
#% 479815
#% 479953
#% 503882
#% 527166
#% 527176
#% 527198
#% 564630
#! This work addresses the problem of querying moving objects databases. which capture the inherent uncertainty associated with the location of moving point objects. We address the issue of modeling, constructing, and querying a trajectories database. We propose to model a trajectory as a 3D cylindrical body. The model incorporates uncertainty in a manner that enables efficient querying. Thus our model strikes a balance between modeling power, and computational efficiency. To demonstrate efficiency, we report on experimental results that relate the length of a trajectory to its size in bytes. The experiments were conducted using a real map of the Chicago Metropolitan area.We introduce a set of novel but natural spatio-temporal operators which capture uncertainty, and are used to express spatio-temporal range queries. We also devise and analyze algorithms to process the operators. The operators have been implemented as a part of our DOMINO project.

#index 458850
#* Designing Functional Dependencies for XML
#@ Mong-Li Lee;Tok Wang Ling;Wai Lup Low
#t 2002
#c 8
#% 114579
#% 236416
#% 287677
#% 299943
#% 322415
#% 322880
#% 330627
#% 389322
#% 665626
#% 755572
#% 1393700
#! Functional dependencies are an integral part of database theory and they form the basis for normalizing relational tables up to BCNF. With the increasing relevance of the data-centric aspects of XML, it is pertinent to study functional dependencies in the context of XML, which will form the basis for further studies into XML keys and normalization. In this work, we investigate the design of functional dependencies in XML databases. We propose FDXML, a notation and DTD for representing functional dependencies in XML. We observe that many databases are hierarchical in nature and the corresponding nested XML data may inevitably contain redundancy. We develop a model based on FDXML to estimate the amount of data replication in XML data. We show how functional dependencies in XML can be verified with a single pass through the XML data, and present supporting experimental results. A platform-independent framework is also drawn up to demonstrate how the techniques proposed in this work can enrich the semantics of XML.

#index 458851
#* Hyperdatabases: Infrastructure for the Information Space
#@ Hans-Jörg Schek
#t 2002
#c 8
#! The amount of stored information is exploding as a consequence of the immense progress in computer and communication technology during the last decades. However tools for accessing relevant information and processing globally distributed information in a convenient manner are under-developed. In short, the infrastructure for the "Information Space" needs much higher attention. In order to improve this situation, we envision the concept of a hyperdatabase that provides database functionality at a much higher level of abstraction, i.e., at the level of complete information components in an n-tier architecture. In analogy to traditional database systems that manage shared data and transactions, a hyperdatabase manages shared information components and transactional processes. It provides "higher-order data independence" by guaranteeing the immunity of applications not only against changes in the data storage and access structures but also against changes in the application components and services, e.g., with respect to the location, implementation, workload, and number of replica of components. In our vision, a hyperdatabase will be the key infrastructure for developing and managing the information space.

#index 458852
#* Cut-and-Pick Transactions for Proxy Log Mining
#@ Wenwu Lou;Guimei Liu;Hongjun Lu;Qiang Yang
#t 2002
#c 8
#% 214673
#% 232136
#% 268184
#% 281209
#% 312874
#% 316139
#% 338308
#% 342655
#% 420120
#% 463903
#% 484856
#% 488583
#% 552181
#% 552186
#% 631914
#% 661023
#% 963898
#! Web logs collected by proxy servers, referred to as proxy logs or proxy traces, contain information about Web document accesses by many users against many Web sites. This "many-to-many" characteristic poses a challenge to Web log mining techniques due to the difficulty in identifying individual access transactions. This is because in a proxy log, user transactions are not clearly bounded and are sometimes interleaved with each other as well as with noise. Most previous work has used simplistic measures such as a fixed time interval as a determination method for the transaction boundaries, and has not addressed the problem of interleaving and noisy transactions. In this paper, we show that this simplistic view can lead to poor performance in building models to predict future access patterns. We present a more advanced cut-and-pick method for determining the access transactions from proxy logs, by deciding on more reasonable transaction boundaries and by removing noisy accesses. Our method takes advantage of the user behavior that in most transactions, the same user typically visits multiple, related Web sites that form clusters. These clusters can be discovered by our algorithm based on the connectivity among Web sites. By using real-world proxy logs, we experimentally show that this cut-and-pick method can produce more accurate transactions that result in Web-access prediction models with higher accuracy.

#index 458853
#* Dynamic Queries over Mobile Objects
#@ Iosif Lazaridis;Kriengkrai Porkaew;Sharad Mehrotra
#t 2002
#c 8
#% 86950
#% 86953
#% 88056
#% 116082
#% 201876
#% 248804
#% 273706
#% 287070
#% 300174
#% 300179
#% 318051
#% 411694
#% 427199
#% 461923
#% 464847
#% 480093
#% 480473
#% 503882
#% 527026
#% 527176
#% 527187
#% 527195
#% 571296
#% 653223
#! Increasingly applications require the storage and retrieval of spatio-temporal information in a database management system. A type of such information is mobile objects, i.e., objects whose location changes continuously with time. Various techniques have been proposed to address problems of incorporating such objects in databases. In this paper, we introduce new query processing techniques for dynamic queries over mobile objects, i.e., queries that are themselves continuously changing with time. Dynamic queries are natural in situational awareness systems when an observer is navigating through space. All objects visible by the observer must be retrieved and presented to her at very high rates, to ensure a high-quality visualization. We show how our proposed techniques offer a great performance improvement over a traditional approach of multiple instantaneous queries.

#index 458854
#* The APPROXML Tool Demonstration
#@ Ernesto Damiani;Nico Lavarini;Stefania Marrara;Barbara Oliboni;Daniele Pasini;Letizia Tanca;Giuseppe Viviani
#t 2002
#c 8
#% 492005
#% 563309

#index 458855
#* A Robust and Self-tuning Page-Replacement Strategy for Spatial Database Systems
#@ Thomas Brinkhoff
#t 2002
#c 8
#% 68089
#% 86950
#% 102805
#% 152902
#% 152943
#% 427199
#% 445701
#% 462218
#% 481092
#% 527004
#% 993385
#! For a spatial database management system, it is an important goal to minimize the I/O-cost of queries and other operations. Several page-replacement strategies have been proposed and compared for standard database systems. In the context of spatial database systems, however, the impact of buffing techniques has not been considered in detail, yet. In this paper, different page-replacement algorithms are compared for performing spatial queries. This study includes well-known techniques like LRU and LRU-K as well as new algorithms observing spatial optimization criteria. Experiments show that spatial page-replacement algorithms outperform LRU buffers for many distributions, but not for all investigated query sets. Therefore, a combination of spatial page-replacement strategies with LRU strategies is proposed and experimentally investigated. An algorithm is presented, which is self-tuning and adapts itself to different or changing query distributions. This adaptable spatial buffer outperforms LRU in respect to the I/O-cost by performance gains of up to 25%.

#index 458856
#* Coupling of FDBS and WfMS for Integrating Database and Application Systems: Architecture, Complexity, Performance
#@ Klaudia Hergula;Theo Härder
#t 2002
#c 8
#% 85086
#% 273912
#% 284600
#% 342683
#% 463919
#% 479449
#% 481101
#% 481923
#% 571060
#% 631918
#% 632002
#% 665529
#! With the emergence of so-called application systems which encapsulate databases and related application components, pure data integration using, for example, a federated database system is not possible anymore. Instead, access via predefined functions is the only way to get data from an application system. As a result, retrieval of such heterogeneous and encapsulated data sources needs the combination of generic query as well as predefined function access. In this paper, we present a middleware approach supporting such novel and extended kind of integration. In particular, so-called federated functions combining functionality of one or more application system calls (local functions) have to be integrated. Starting with the overall architecture, we explain the functionality and cooperation of its core components: a federated database system and, connected via a wrapper, a workflow management system composing and executing the federated functions. Due to missing wrapper support in commercial products, we also explore the use of user-defined table functions. In addition to our workflow solution, we present several alternative architectures where the federated database system directly controls the execution of the requested local functions. These two different approaches are primarily compared w.r.t. their mapping complexity and their performance.

#index 458857
#* Efficient Indexing of Spatiotemporal Objects
#@ Marios Hadjieleftheriou;George Kollios;Vassilis J. Tsotras;Dimitrios Gunopulos
#t 2002
#c 8
#% 56081
#% 58369
#% 58371
#% 76907
#% 86950
#% 102759
#% 137887
#% 213975
#% 273706
#% 287070
#% 296090
#% 299979
#% 300174
#% 315005
#% 328481
#% 427199
#% 443130
#% 443181
#% 443444
#% 462059
#% 480473
#% 480817
#% 481455
#% 503882
#% 527195
#% 571296
#% 659935
#% 659961
#! Spatiotemporal objects i.e., objects which change their position and/or extent over time, appear in many applications. This paper addresses the problem of indexing large volumes of such data. We consider general object movements and extent changes. We further concentrate on "snapshot" as well as small "interval" historical queries on the gathered data. The obvious approach that approximates spatiotemporal objects with MBRs and uses a traditional multidimensional access method to index them is inefficient. Objects that "live" for long time intervals have large MBRs which introduce a lot of empty space. Clustering long intervals has been dealt in temporal databases by the use of partially persistent indices. What differentiates this problem from traditional temporal indexing is that objects are allowed to move/change during their lifetime. Better methods are thus needed to approximate general spatiotemporal objects. One obvious solution is to introduce artificial splits: the lifetime of a long-lived object is split into smaller consecutive pieces. This decreases the empty space but increases the number of indexed MBRs. We first introduce two algorithms for splitting a given spatiotemporal object. Then, given an upper bound on the total number of possible splits, we present three algorithms that decide how the splits should be distributed among the objects so that the total empty space is minimized.

#index 458858
#* Aggregate Processing of Planar Points
#@ Yufei Tao;Dimitris Papadias;Jun Zhang
#t 2002
#c 8
#% 86950
#% 153260
#% 213975
#% 287070
#% 333874
#% 333977
#% 443130
#% 443181
#% 443444
#% 465010
#% 480817
#% 481934
#% 527166
#% 527189
#% 527326
#% 562982
#% 565462
#% 571296
#% 589264
#! Aggregate window queries return summarized information about objects that fall inside a query rectangle (e.g., the number of objects instead of their concrete ids). Traditional approaches for processing such queries usually retrieve considerable extra information, thus compromising the processing cost. The paper addresses this problem for planar points from both theoretical and practical points of view. We show that, an aggregate window query can be answered in logarithmic worst-case time by an indexing structure called the aP-tree. Next we study the practical behavior of the aP-tree and propose efficient cost models that predict the structure size and actual query cost. Extensive experiments show that the aP-tree, while involving more space consumption, accelerates query processing by up to an order of magnitude compared to a specialized method based on R-trees. Furthermore, our cost models are accurate and can be employed for the selection of the most appropriate method, balancing the space and query time tradeoff.

#index 458859
#* Content Schema Evolution in the CoreMedia®; Content Application Platform CAP
#@ Axel Wienberg;Matthias Ernst;Andreas Gawecki;Olaf Kummer;Frank Wienberg;Joachim W. Schmidt
#t 2002
#c 8
#% 41665
#% 98322
#% 211530
#% 248852
#% 318990
#% 480251
#% 480433
#% 555013
#% 585659
#! Based on experience gathered with several releases of the CoreMedia Content Application Platform (CAP), we argue that a modern, generalized Content Management System should, as database systems do, support explicit content schemata. To control the inevitable evolution of the content schema, the schema should be subject to configuration management together with the actual content. We propose a two-layered approach to content schema evolution consisting of - a system level responsible for bookkeeping and integrity issue detection, and - a semi-automatic application level responsible for resolving schema-related issues.A prototype using the proposed approach has been successfully implemented at CoreMedia.

#index 458860
#* An Approach to Integrating Query Refinement in SQL
#@ Michael Ortega-Binderberger;Kaushik Chakrabarti;Sharad Mehrotra
#t 2002
#c 8
#% 41230
#% 213443
#% 248818
#% 270841
#% 300170
#% 309798
#% 387427
#% 437405
#% 443243
#% 464726
#% 479788
#% 480302
#% 480480
#% 522429
#% 1857498
#! With the emergence of applications that require content-based similarity retrieval, techniques to support such a retrieval paradigm over database systems have emerged as a critical area of research. User subjectivity is an important aspect of such queries, i.e., which objects are relevant to the user and which are not depends on the perception of the user. Query refinement is used to handle user subjectivity in similarity search systems. This paper explores how to enhance database systems with query refinement for content-based (similarity) searches in object-relational databases. Query refinement is achieved through relevance feedback where the user judges individual result tuples and the system adapts and restructures the query to better reflect the users information need. We present a query refinement framework and an array of strategies for refinement that address different aspects of the problem. Our experiments demonstrate the effectiveness of the query refinement techniques proposed in this paper.

#index 458861
#* Tree Pattern Relaxation
#@ Sihem Amer-Yahia;SungRan Cho;Divesh Srivastava
#t 2002
#c 8
#% 1921
#% 169817
#% 198335
#% 236416
#% 244371
#% 262069
#% 281149
#% 333845
#% 333981
#% 340914
#% 406493
#% 504581
#! Tree patterns are fundamental to querying tree-structured data like XML. Because of the heterogeneity of XML data, it is often more appropriate to permit approximate query matching and return ranked answers, in the spirit of Information Retrieval, than to return only exact answers. In this paper, we study the problem of approximate XML query matching, based on tree pattern relaxations, and devise efficient algorithms to evaluate relaxed tree patterns. We consider weighted tree patterns, where exact and relaxed weights, associated with nodes and edges of the tree pattern, are used to compute the scores of query answers. We are interested in the problem of finding answers whose scores are at least as large as a given threshold. We design data pruning algorithms where intermediate query results are filtered dynamically during the evaluation process. We develop anoptimization that exploits scores of intermediate results to improve query evaluation efficiency. Finally, we show experimentally that our techniques outperform rewriting-based and post-pruning strategies.

#index 458862
#* Spatio-temporal Information Systems in a Statistical Context
#@ Leonardo Tininini;Mario Paolucci;Giuseppe Sindoni;Stefano De Francisci
#t 2002
#c 8
#% 85086
#% 116303
#% 163438
#% 210352
#% 300384
#% 480460
#% 480493
#% 511887
#! The Italian National Statistics Institute is currently integrating its various spatio-temporal data collections. It has been developed an integrated system, whose implementation relied on Web and relational technologies to cope with data heterogeneity. The system provides users with many different classes of functions with which to analyse and visualise territorial data. It can be viewed as a spatio-temporal data warehouse, where space and time are the main access dimensions to statistical data, but also where space can be analysed according to its temporal mutations as a preliminary step in the design activity of a statistical survey. The system overcomes a drawback of current commercial data warehouse systems, which are not able to cope with the dynamic behaviour of a dimension - that is, its temporal evolution.

#index 458863
#* Selectivity Estimation for Spatial Joins with Geometric Selections
#@ Chengyu Sun;Divyakant Agrawal;Amr El Abbadi
#t 2002
#c 8
#% 152902
#% 287379
#% 300160
#% 464850
#% 498511
#% 527190
#% 527193
#% 659976
#! Spatial join is an expensive operation that is commonly used in spatial database systems. In order to generate efficient query plans for the queries involving spatial join operations, it is crucial to obtain accurate selectivity estimates for these operations. In this paper we introduce a framework for estimating the selectivity of spatial joins constrained by geometric selections. The center piece of the framework is Euler Histogram, which decomposes the estimation process into estimations on vertices, edges and faces. Based on the characteristics of different datasets, different probabilistic models can be plugged into the framework to provide better estimation results. To demonstrate the effectiveness of this framework, we implement it by incorporating two existing probabilistic models, and compare the performance with the Geometric Histogram [1] and the algorithm recently proposed by Mamoulis and Papadias.

#index 458864
#* Bridging the Gap between Response Time and Energy-Efficiency in Broadcast Schedule Design
#@ Wai Gen Yee;Shamkant B. Navathe;Edward Omiecinski;Chris Jermaine
#t 2002
#c 8
#% 872
#% 32884
#% 124011
#% 172876
#% 201897
#% 259634
#% 269631
#% 273893
#% 274199
#% 274209
#% 279165
#% 314369
#% 316486
#% 480125
#% 481777
#% 482107
#% 571066
#% 632025
#% 632027
#! In this paper, we propose techniques for scheduling data broadcasts that are favorable in terms of both response and tuning time. In other words, these techniques ensure that a typical data request will be quickly satisfied and its reception will require a low client-side energy expenditure. By generating broadcast schedules based on Acharya et al.'s broadcast disk paradigm, we bridge the gap between these two mutually exclusive bodies of work-response time and energy expenditure. We prove the utility of our approach analytically and via experiments. Our analysis of optimal scheduling is presented under a variety of assumptions about size and popularity of data items, making our results generalizable to a range of applications.

#index 458865
#* Management of Dynamic Location Information in DOMINO
#@ Ouri Wolfson;Hu Cao;Hai Lin;Goce Trajcevski;Fengli Zhang;Naphtali Rishe
#t 2002
#c 8
#% 295512
#% 458849
#% 503869

#index 458866
#* Approximate Processing of Multiway Spatial Joins in Very Large Databases
#@ Dimitris Papadias;Dinos Arkoumanis
#t 2002
#c 8
#% 10658
#% 23998
#% 86950
#% 124680
#% 126390
#% 152937
#% 160239
#% 172500
#% 273685
#% 273886
#% 280843
#% 307265
#% 369236
#% 383492
#% 427199
#% 443133
#% 464831
#% 527330
#! Existing work on multiway spatial joins focuses on the retrieval of all exact solutions with no time limit for query processing. Depending on the query and data properties, however, exhaustive processing of multiway spatial joins can be prohibitively expensive due to the exponential nature of the problem. Furthermore, if there do not exist any exact solutions, the result will be empty even though there may exist solutions that match the query very closely. These shortcomings motivate the current work, which aims at the retrieval of the best possible (exact or approximate) solutions within a time threshold, since fast retrieval of approximate matches is the only way to deal with the ever increasing amounts of multimedia information in several real time systems. We propose various techniques that combine local and evolutionary search with underlying indexes to prune the search space. In addition to their usefulness as standalone methods for approximate query processing, the techniques can be combined with systematic search to enhance performance when the goal is retrieval of the best solutions.

#index 458867
#* Cobra: A Content-Based Video Retrieval System
#@ Milan Petkovic;Willem Jonker
#t 2002
#c 8
#% 464816

#index 458868
#* DAML+OIL: A Reason-able Web Ontology Language
#@ Ian Horrocks
#t 2002
#c 8
#% 227528
#% 433879
#% 445445
#% 445446
#% 459454
#% 472862
#% 511906
#% 517280
#% 529498
#% 531444
#% 539635
#% 539937
#% 561419
#% 561740
#% 587427
#% 1268741
#% 1279732
#% 1289169
#% 1289174
#! Ontologies are set to play a key role in the "Semantic Web", extending syntactic interoperability to semantic interoperability by providing a source of shared and precisely defined terms. DAML+OIL is an ontology language specifically designed for use on the Web; it exploits existing Web standards (XML and RDF), adding the familiar ontological primitives of object oriented and frame based systems, and the formal rigor of a very expressive description logic. The logical basis of the language means that reasoning services can be provided, both to support ontology design and to make DAML+OIL described Web resources more accessible to automated processes.

#index 458869
#* Efficient Algorithms for Mining Inclusion Dependencies
#@ Fabien De Marchi;Stéphane Lopes;Jean-Marc Petit
#t 2002
#c 8
#% 89751
#% 125557
#% 186583
#% 316709
#% 332166
#% 387089
#% 420062
#% 427873
#% 443343
#% 458275
#% 458762
#% 462214
#% 465035
#% 479814
#% 481290
#% 487843
#% 993429
#! Foreign keys form one of the most fundamental constraints for relational databases. Since they are not always defined in existing databases, algorithms need to be devised to discover foreign keys. One of the underlying problems is known to be the inclusion dependency (IND) inference problem. In this paper a new data mining algorithm for computing unary INDs is given. From unary INDs, we also propose a levelwise algorithmto discover all remaining INDs, where candidate INDs of size i + 1 are generated fromsatisfied INDs of size i, (i 0).An implementation of these algorithms has been achieved and tested against synthetic databases. Up to our knowledge, this paper is the first one to address in a comprehensive manner this data mining problem, from algorithms to experimental results.

#index 458870
#* Building Dynamic Market Places Using HyperQueries
#@ Christian Wiesner;Peter Winklhofer;Alfons Kemper
#t 2002
#c 8
#% 328427
#% 480658
#% 572300

#index 458871
#* Situation Aware Mobile Access to Digital Libraries
#@ Peter Haase
#t 2002
#c 8
#% 584934
#! A conference and exhibition guide has been implemented to demonstrate how context information can be exploited to proactively provide situation relevant information in mobile environments. Different mobile devices, a wireless LAN and sensor technology provide the infrastructure for the situation aware mobile access to the Digital Library. Ontology based knowledge representation and reasoning are used to model the content of the Digital Library and user context and to determine situation relevant content.

#index 458872
#* Efficient OLAP Query Processing in Distributed Data Warehouses
#@ Michael O. Akinde;Michael H. Böhlen;Theodore Johnson;Laks V. S. Lakshmanan;Divesh Srivastava
#t 2002
#c 8
#% 83933
#% 140389
#% 201883
#% 223781
#% 273943
#% 287225
#% 310899
#% 330305
#% 420053
#% 442698
#% 459024
#% 465170
#% 479450
#% 481951
#% 488788
#% 632007
#% 1830683
#! The success of Internet applications has led to an explosive growth in the demand for bandwidth from ISPs. Managing an IP network requires collecting and analyzing network data, such as flow-level traffic statistics. Such analyses can typically be expressed as OLAP queries, e.g., correlated aggregate queries and data cubes. Current day OLAP tools for this task assume the availability of the data in a centralized data warehouse. However, the inherently distributed nature of data collection and the huge amount of data extracted at each collection point make it impractical to gather all data at a centralized site. One solution is to maintain a distributed data warehouse, consisting of local data warehouses at each collection point and a coordinator site, with most of the processing being performed at the local sites. In this paper, we consider the problem of efficient evaluation of OLAP queries over a distributed data warehouse. We have developed the Skalla system for this task. Skalla translates OLAP queries, specified as certain algebraic expressions, into distributed evaluation plans which are shipped to individual sites. Salient properties of our approach are that only partial results are shipped - never parts of the detail data. We propose a variety of optimizations to minimize both the synchronization traffic and the local processing done at each site. We finally present an experimental study based on TPC(R) data. Our results demonstrate the scalability of our techniques and quantify the performance benefits of the optimization techniques that have gone into the Skalla system.

#index 458873
#* Querying with Intrinsic Preferences
#@ Jan Chomicki
#t 2002
#c 8
#% 145194
#% 151859
#% 190332
#% 209634
#% 275032
#% 278287
#% 300170
#% 318601
#% 331835
#% 333951
#% 384978
#% 417542
#% 465167
#% 566111
#% 665415
#% 1650274
#! The handling of user preferences is becoming an increasingly important issue in present-day information systems. Among others, preferences are used for information filtering and extraction to reduce the volume of data presented to the user. They are also used to keep track of user profiles and formulate policies to improve and automate decision making. We propose a logical framework for formulating preferences and its embedding into relational query languages. The framework is simple, and entirely neutral with respect to the properties of preferences. It makes it possible to formulate different kinds of preferences and to use preferences in querying databases. We demonstrate the usefulness of the framework through numerous examples.

#index 458874
#* A Framework for the Physical Design Problem for Data Synopses
#@ Arnd Christian König;Gerhard Weikum
#t 2002
#c 8
#% 152917
#% 172902
#% 210353
#% 242366
#% 248014
#% 248793
#% 248812
#% 248822
#% 273903
#% 273906
#% 273908
#% 273909
#% 282942
#% 333948
#% 479648
#% 479984
#% 480125
#% 480306
#% 480471
#% 482123
#% 632048
#! Maintaining statistics on multidimensional data distributions is crucial for predicting the run-time and result size of queries and data analysis tasks with acceptable accuracy. To this end a plethora of techniques have been proposed for maintaining a compact data "synopsis" on a single table, ranging from variants of histograms to methods based on wavelets and other transforms. However, the fundamental question of how to reconcile the synopses for large information sources with many tables has been largely unexplored. This paper develops a general framework for reconciling the synopses on many tables, which may come from different information sources. It shows how to compute the optimal combination of synopses for a given workload and a limited amount of available memory. The practicality of the approach and the accuracy of the proposed heuristics are demonstrated by experiments.

#index 458875
#* ProPolyne: A Fast Wavelet-Based Algorithm for Progressive Evaluation of Polynomial Range-Sum Queries
#@ Rolfe R. Schmidt;Cyrus Shahabi
#t 2002
#c 8
#% 168260
#% 227866
#% 227883
#% 273902
#% 280448
#% 300193
#% 316551
#% 333872
#% 333977
#% 458821
#% 459022
#% 479822
#% 480306
#% 480628
#% 487692
#% 504019
#% 565500
#% 617881
#% 631947
#! Many range aggregate queries can be efficiently derived from a class of fundamental queries: the polynomial range-sums. After demonstrating how any range-sum can be evaluated exactly in the wavelet domain, we introduce a novel pre-aggregation method called ProPolyne to evaluate arbitrary polynomial range-sums progressively. At each step of the computation, ProPolyne makes the best possible wavelet approximation of the submitted query. The result is a data-independent approximate query answering technique which uses data structures that can be maintained efficiently. ProPolyne's performance as an exact algorithm is comparable to the best known MOLAP techniques. Our experimental results show that this approach of approximating queries rather than compressing data produces consistent and superior approximate results when compared to typical wavelet-based data compression techniques.

#index 458876
#* Ambient Intelligence: Plenty of Challenges by 2010
#@ Jari Ahola
#t 2002
#c 8
#! Ambient Intelligence refers to an environment that is sensitive, adaptive, and responsive to the presence of people. It builds on three recent key technologies: Ubiquitous Computing, Ubiquitous Communication, and Intelligent User Interfaces. To add intelligence to our environment places many challenges also for the databases essential for the implementation of intelligent environments, for instance in homes and workplaces. Such an environment relies heavily on constant information flows from the numerous sensors monitoring not only the environment, but also the occupants. Solving the problem of how to intelligently translate this data to correct behaviors of the environment is the key to be found.

#index 458988
#* Optimal Multi-Block Read Schedule for Partitioned Signature Files
#@ Paolo Ciaccia
#t 1996
#c 8

#index 458989
#* The Need for an Object Relational Model and its Use
#@ Daisy Bonjour
#t 1996
#c 8

#index 458990
#* VALIDITY: Applications of a DOOD System
#@ Oris Friesen;Alexandre Lefebvre;Laurent Vieille
#t 1996
#c 8

#index 458991
#* Semantic Workflow Interoperability
#@ Fabio Casati;Stefano Ceri;Barbara Pernici;Giuseppe Pozzi
#t 1996
#c 8

#index 458992
#* First-Order Queries over Temporal Databases Inexpressible in Temporal Logic
#@ David Toman;Damian Niwinski
#t 1996
#c 8

#index 458993
#* Fine-granularity Locking and Client-Based Logging for Distributed Architectures
#@ Euthimios Panagos;Alexandros Biliris;H. V. Jagadish;Rajeev Rastogi
#t 1996
#c 8

#index 458994
#* Object Query Services for Telecommunication Networks
#@ Jerome Fessy;Yann Lepetit;Philippe Pucheral
#t 1996
#c 8

#index 458995
#* Management of Multiple Models in an Extensible Database Design Tool
#@ Paolo Atzeni;Riccardo Torlone
#t 1996
#c 8

#index 458996
#* Accomodating Integrity Constraints During Database Design
#@ Dimitris Plexousakis;John Mylopoulos
#t 1996
#c 8

#index 458997
#* LH*LH: A scalable High Performance Data Structure for Switched Multicomputers
#@ Jonas S. Karlsson;Witold Litwin;Tore Risch
#t 1996
#c 8

#index 458998
#* Exploiting Persistent Intermediate Code Representations in Open Database Environments
#@ Andreas Gawecki;Florian Matthes
#t 1996
#c 8

#index 458999
#* Amalgamating SGML Documents and Databases
#@ Masatoshi Yoshikawa;Osamu Ichikawa;Shunsuke Uemura
#t 1996
#c 8

#index 459000
#* MATISSE: A Multimedia Web DBMS
#@ Sheldon J. Finkelstein;Eric Lemoine;René Lenaers
#t 1996
#c 8

#index 459001
#* Composite Events in Chimera
#@ Rosa Meo;Giuseppe Psaila;Stefano Ceri
#t 1996
#c 8

#index 459002
#* Version Management for Scientific Databases
#@ I-Min A. Chen;Victor M. Markowitz;Stanley Letovsky;Peter Li;Kenneth H. Fasman
#t 1996
#c 8

#index 459003
#* IRO-DB: Making Relational and Object-Oriented Database Systems Interoperable
#@ Peter Fankhauser;Béatrice Finance;Wolfgang Klas
#t 1996
#c 8

#index 459004
#* Dealing with Asynchrony in Technology Transfer
#@ Gio Wiederhold
#t 1996
#c 8

#index 459005
#* Providing High Availability in Very Large Worklflow Management Systems
#@ Mohan Kamath;Gustavo Alonso;Roger Günthör;C. Mohan
#t 1996
#c 8

#index 459006
#* Mining Sequential Patterns: Generalizations and Performance Improvements
#@ Ramakrishnan Srikant;Rakesh Agrawal
#t 1996
#c 8

#index 459007
#* Indexing Nucleotide Databases for Fast Query Evaluation
#@ Hugh E. Williams;Justin Zobel
#t 1996
#c 8

#index 459008
#* SLIQ: A Fast Scalable Classifier for Data Mining
#@ Manish Mehta;Rakesh Agrawal;Jorma Rissanen
#t 1996
#c 8

#index 459009
#* How to Tackle Schema Validation by View Updating
#@ Hendrik Decker;Ernest Teniente;Toni Urpí
#t 1996
#c 8

#index 459010
#* Modelling Large Scale OLAP Scenarios
#@ Wolfgang Lehner
#t 1998
#c 8

#index 459011
#* Towards Optimal Indexing for Segment Databases
#@ Elisa Bertino;Barbara Catania;Boris Shidlovsky
#t 1998
#c 8

#index 459012
#* Incremental Generalization for Mining in a Data Warehousing Environment
#@ Martin Ester;Rüdiger Wittmann
#t 1998
#c 8

#index 459013
#* Static Management of Integrity in Object-Oriented Databases: Design and Implementation
#@ Véronique Benzaken;Xavier Schaefer
#t 1998
#c 8

#index 459014
#* Efficient Dynamic Programming Algorithms for Ordering Expensive Joins and Selections
#@ Wolfgang Scheufele;Guido Moerkotte
#t 1998
#c 8

#index 459015
#* Referential Actions: From Logical Semantics to Implementation
#@ Bertram Ludäscher;Wolfgang May
#t 1998
#c 8

#index 459016
#* Integration of Incremental View Maintenance into Query Optimizers
#@ Dimitra Vista
#t 1998
#c 8

#index 459017
#* OCB: A Generic Benchmark to Evaluate the Performances of Object-Oriented Database Systems
#@ Jérôme Darmont;Bertrand Petit;Michel Schneider
#t 1998
#c 8

#index 459018
#* A Path Removing Technique for Detecting Trigger Termination
#@ Sin Yeung Lee;Tok Wang Ling
#t 1998
#c 8

#index 459019
#* Dynamic and Structured Presentation of Database Contents on the Web
#@ Motomichi Toyama;Takuhiro Nagafuji
#t 1998
#c 8

#index 459020
#* Pincer Search: A New Algorithm for Discovering the Maximum Frequent Set
#@ Dao-I Lin;Zvi M. Kedem
#t 1998
#c 8

#index 459021
#* Mining Process Models from Workflow Logs
#@ Rakesh Agrawal;Dimitrios Gunopulos;Frank Leymann
#t 1998
#c 8

#index 459022
#* Multivariate and Multidimensional OLAP
#@ Shin-Chung Shao
#t 1998
#c 8

#index 459023
#* Exploring Heterogeneous Biological Databases: Tools and Applications
#@ Anthony Kosky;I-Min A. Chen;Victor M. Markowitz;Ernest Szeto
#t 1998
#c 8

#index 459024
#* Complex Aggregation at Multiple Granularities
#@ Kenneth A. Ross;Divesh Srivastava;Damianos Chatziantoniou
#t 1998
#c 8

#index 459025
#* Discovery-Driven Exploration of OLAP Data Cubes
#@ Sunita Sarawagi;Rakesh Agrawal;Nimrod Megiddo
#t 1998
#c 8

#index 459026
#* Maintaining Temporal Views over Non-Temporal Information Sources for Data Warehousing
#@ Jun Yang;Jennifer Widom
#t 1998
#c 8

#index 459027
#* Fusion Queries over Internet Databases
#@ Ramana Yerneni;Yannis Papakonstantinou;Serge Abiteboul;Hector Garcia-Molina
#t 1998
#c 8

#index 459028
#* A Logical Approach to Multidimensional Databases
#@ Luca Cabibbo;Riccardo Torlone
#t 1998
#c 8

#index 459029
#* Buffer Management in Distributed Database Systems: A Data Mining Based Approach
#@ L. Feng;Hongjun Lu;Y. C. Tay;Anthony K. H. Tung
#t 1998
#c 8

#index 564202
#* Incremental Maintenance of Schema-Restructuring Views
#@ Andreas Koeller;Elke A. Rundensteiner
#t 2002
#c 8
#% 32914
#% 66204
#% 102748
#% 189769
#% 201929
#% 213969
#% 227947
#% 227989
#% 252374
#% 442767
#% 443232
#% 479968
#% 480969
#% 481944
#% 565261
#! An important issue in data integration is the integration of semantically equivalent but schematically heterogeneous data sources. Declarative mechanisms supporting powerful source restructuring for such databases have been proposed in the literature, such as the SQL extension SchemaSQL. However, the issue of incremental maintenance of views defined in such languages remains an open problem.We present an incremental view maintenance algorithm for schema-restructuring views. Our algorithm transforms a source update into an incremental view update, by propagating updates through the operators of a SchemaSQL algebra tree. We observe that schema-restructuring view maintenance requires transformation of data into schema changes and vice versa. Our maintenance algorithm handles any combination of data updates or schema changes and produces a correct sequence of data updates, schema changes, or both as output. In experiments performed on our prototype implementation, we find that incremental view maintenance in SchemaSQL is significantly faster than recomputation in many cases.

#index 565258
#* Monet And Its Geographic Extensions: A Novel Approach to High Performance GIS Processing
#@ Peter A. Boncz;Wilko Quak;Martin L. Kersten
#t 1996
#c 8

#index 565259
#* Reasoning with Aggregation Constraints
#@ Alon Y. Levy;Inderpal Singh Mumick
#t 1996
#c 8

#index 565260
#* The PARK Semantics for Active Rules
#@ Georg Gottlob;Guido Moerkotte;V. S. Subrahmanian
#t 1996
#c 8

#index 565261
#* The CVS Algorithm for View Synchronization in Evolvable Large-Scale Information Systems
#@ Anisoara Nica;Amy J. Lee;Elke A. Rundensteiner
#t 1998
#c 8

#index 565262
#* A Conceptual Model and a Tool Environment for Developing More Scalable, Dynamic, and Customizable Web Applications
#@ Piero Fraternali;Paolo Paolini
#t 1998
#c 8

#index 565263
#* Hierarchical Declustering Schemes for Range Queries
#@ Randeep Bhatia;Rakesh K. Sinha;Chung-Min Chen
#t 2000
#c 8
#% 43179
#% 153400
#% 153633
#% 261738
#% 286962
#% 299983
#% 339622
#% 461922
#% 462233
#% 464718
#% 469603
#% 479936
#% 481109
#% 565263
#% 565461
#% 609704
#% 631955
#% 631956
#% 632069
#% 637794
#! Declustering schemes have been widely used to speed up access time of multi-device storage systems (e.g. disk arrays) in modern geospatial applications. A declustering scheme distributes data items among multiple devices, thus enabling parallel I/O access and speeding up query response time. To date, efficient declustering schemes are only known for small values of M, where M is the number of devices. For large values of M, the search overhead to find an efficient scheme is prohibitive. In this paper, we present an efficient hierarchical technique for building declustering schemes for large values of M based on declustering schemes for small values of M. Using this technique, one may easily construct efficient declustering schemes for large values of M using any known good declustering schemes for small values of M. We analyze the performance of the declustering schemes generated by our technique in 2-dimension, giving tight asymptotic bounds on their response time. For example we show, in 2 dimension, that using optimal declustering schemes for M1 and M2 devices we can construct a scheme for M1M2 devices whose response time is at most seven more than the optimal response time. Our technique generalizes to any d dimension. We also present simulation results to evaluate the performance of our scheme in practice.

#index 565264
#* Plug&Join: An easy-to-use Generic Algorithm for Efficiently Processing Equi and Non-Equi Joins
#@ Jochen Van den Bercken;Martin Schneider;Bernhard Seeger
#t 2000
#c 8
#% 3771
#% 13041
#% 77928
#% 86950
#% 114577
#% 136740
#% 152937
#% 172909
#% 201880
#% 210186
#% 210187
#% 227783
#% 227932
#% 273886
#% 380441
#% 427199
#% 462207
#% 463436
#% 463595
#% 463751
#% 479462
#% 479797
#% 480774
#% 481589
#% 481599
#% 482121
#% 489381
#% 567865
#% 571296
#! This paper presents Plug&Join, a new generic algorithm for efficiently processing a broad class of different join types in extensible database systems. Depending on the join predicate Plug&Join is called with a suitable type of index structure as a parameter. If the inner relation fits in memory, the algorithm builds a memory resident index of the desired type on the inner relation and probes all tuples of the outer relation against the index. Otherwise, a memory resident index is created by sampling the inner relation. The index is then used as a partitioning function for both relations. In order to demonstrate the flexibility of Plug&Join, we present how to implement equi joins, spatial joins and subset joins by using memory resident B+-trees, R-trees and S-trees, respectively. Moreover, results obtained from different experiments for the spatial join show that Plug&Join is competitive to special-purpose methods like the Partition Based Spatial-Merge Join algorithm.

#index 565265
#* Efficient Complex Query Support for Multiversion XML Documents
#@ Shu-Yao Chien;Vassilis J. Tsotras;Carlo Zaniolo;Donghui Zhang
#t 2002
#c 8
#% 2011
#% 58371
#% 182672
#% 210212
#% 281149
#% 281150
#% 287070
#% 442967
#% 443130
#% 443181
#% 462235
#% 479806
#% 479956
#% 480092
#% 480489
#% 480659
#% 480817
#% 480827
#% 504576
#% 504578
#% 571296
#% 659999
#% 665633
#! Managing multiple versions of XML documents represents a critical requirement for many applications. Also, there has been much recent interest in supporting complex queries on XML data (e.g., regular path expressions, structural projections, DIFF queries). In this paper, we examine the problem of supporting efficiently complex queries on multiversioned XML documents. Our approach relies on a scheme based on durable node numbers (DNNs) that preserve the order among the XML tree nodes and are invariant with respect to updates. Using the document's DNNs various complex queries are reduced to combinations of partial version retrieval queries. We examine three indexing schemes to efficiently evaluate partial version retrieval queries in this environment. A thorough performance analysis is then presented to reveal the advantages of each scheme.

#index 565266
#* XQuery by the Book: The IPSI XQuery Demonstrator
#@ Peter Fankhauser;Tobias Groh;Sven Overhage
#t 2002
#c 8
#! The IPSI XQuery Demonstrator (IPSI-XQ) implements the XQuery surface syntax, its mapping to the XQuery Core Language, and the static and dynamic semantics of XQuery Core "by the book", following the formal specification as faithfully as possible. Its main purpose is to provide a framework for testing various language design options, and for experimenting with techniques to use type information for efficiently storing and querying XML.

#index 565267
#* Indexing Values in Continuous Field Databases
#@ Myoung-Ah Kang;Christos Faloutsos;Robert Laurini;Sylvie Servigne
#t 2002
#c 8
#% 3453
#% 45766
#% 64431
#% 68091
#% 86950
#% 86951
#% 115564
#% 153260
#% 212651
#% 214722
#% 240193
#% 260057
#% 287256
#% 359751
#% 427199
#% 480093
#! With the extension of spatial database applications, during the last years continuous field databases emerge as an important research issue in order to deal with continuous natural phenomena during the last years. A field can be represented by a set of cells containing some explicit measured sample points and by arbitrary interpolation methods used to derive implicit values on nonsampled positions. The form of cells depends on the specific data model in an application. In this paper, we present an efficient indexing method on the value domain in a large field database for field value queries (e.g. finding regions where the temperature is between 20 degrees and 30 degrees). The main idea is to divide a field into subfields [15] in order that all of explicit and implicit values inside a subfield are similar each other on the value domain. Then the intervals of the value domain of subfields can be indexed using traditional spatial access methods, like R*-tree [1]. We propose an efficient and effective algorithm for constructing subfields. This is done by using the field property that values close spatially in a field are likely to be closer together. In more details, we linearize cells in order of the Hilbert value of the center position of cells. Then we form subfields by grouping sequentially cells by means of the cost function proposed in this paper, which tries to minimize the probability that subfields will be accessed by a value query. We implemented our method and carried out experiments on real and synthetic data. The results of experiments show that our method dramatically improves query processing time of field value queries compared to linear scanning.

#index 928502
#* Advances in Database Technology -- EDBT 2006: 10 International Conference on Extending Database Technology, Munich, Germany, 26-31 March 2006, Proceedings (Lecture Notes in Computer Science)
#@ Yannis Ioannidis;Marc H. Scholl;Joachim W. Schmidt;Florian Matthes;Mike Hatzopoulos;Klemens Boehm;Alfons Kemper;Torsten Grust;Christian Boehm
#t 2006
#c 8

#index 1044431
#* Proceedings of the 11th international conference on Extending database technology: Advances in database technology
#@ Noureddine Mouaddib;Patrick Valduriez;Alfons Kemper;Mokrane Bouzeghoub;Volker Markl;Laurent Amsaleg;Ioana Manolescu;Jens Teubner
#t 2008
#c 8
#! The EDBT series of conferences is an established and prestigious forum for the exchange of the latest research results in data management. Held every two years in an attractive European location, the conference provides unique opportunities for database researchers, practitioners, developers, and users to explore new ideas, techniques, and tools, and to exchange experiences. The previous EDBT events were held in Venice, Vienna, Cambridge, Avignon, Valencia, Konstanz, Prague, Heraklion and Munich. The 11th International Conference on Extending Database Technology (EDBT 2008) was held from March 25 to 29, 2008 in Nantes, France. Data management constitutes the essential enabling technology for scientific, engineering, business, and social communities. Technological trends, new computation paradigms, novel applications, sophisticated user interactions, they all require robust and flexible database technology to be deployed in a variety of environments and for several diverse purposes. Peer-to-peer architectures, the Grid, personal information systems, pervasive and ubiquitous computing, networked sensors, biomedical informatics, virtual digital libraries, virtual communities, and trust management are just a small sample of the great challenges ahead of us that drive research and development of the next generation of database technology. The new information paradigms and requirements will move our research community away from any narrow interpretation of databases and expand its focus to the hard problems faced by broad visions of data, information, and knowledge management. Researchers submitted contributions that picked up on brand new challenges and explored new and exciting technical directions wherever data management issues may be found. EDBT 2008 invited submissions of original research contributions, as well as proposals for panels, tutorials and software demonstrations. The EDBT conference series covers a broad range of topics, including traditional database management as well new issues arising in any possible domain. To substantially advance the state of the art authors were encouraged to consider novel topics and approaches rather than incremental improvements of existing results.

#index 1044432
#* Reality check: a case study of an EII research prototype encountering customer needs
#@ Eric Simon
#t 2008
#c 8
#! Le Select is a research prototype of an EII (Enterprise Information Integration) system (also known as a mediation system), which was developed from 1998 to 2001 at INRIA (France). Le Select was then transferred to a start-up company named Medience, which was purchased in 2005 by Business Objects. Under its new brand, the EII system now called "Data Federator" is a master piece of Business Objects' Enterprise Information Management (EIM) offering. During this period of nearly ten years, the EII system has been rewritten almost three times, not only for industrializing its code, but mainly because the system was confronted to customer needs that required modifications in order to address functional needs that were not anticipated. The aim of this presentation is to review these main transformations of the system and relate them to Business Intelligence application requirements. We emphasize a few problems such as designing complex views, data cleaning, and query optimization strategies for "narrow" and "mega" queries. The presentation makes extensive use of customer cases to illustrate our purpose. We conclude with a list of open issues.

#index 1044433
#* Biocomputational puzzles: data, algorithms, and visualization
#@ Dennis Shasha
#t 2008
#c 8
#! I solve puzzles for a living. Over the last few years, I've tried to make this activity useful to biologists and scientists in general. This talk will give an overview of some of those attempts, involving the use of combinatorial design to reduce the size of experimental search spaces, visualization of experimental data, and biochemical calculations using DNA. I will attempt to convey the ideas and show the tools rather than focus on mathematical details.

#index 1044434
#* Building web applications without a database system
#@ Donald Kossmann
#t 2008
#c 8
#! Publishing data on the Web has become a commodity. Building, deploying, and operating a Web application, however, are still difficult tasks. This talk argues that one of the reasons is the use of database systems. While such database systems provide a great deal of features, they are hard to deploy and manage, do not scale, and dictate the architecture of the whole application. This talk proposes a new data management architecture and shows how XQuery can be used as a programming language in order to build applications on such a data management infrastructure.

#index 1044435
#* P2P systems with transactional semantics
#@ Shyam Antony;Divyakant Agrawal;Amr El Abbadi
#t 2008
#c 8
#% 8194
#% 9241
#% 201869
#% 273893
#% 337046
#% 340175
#% 340176
#% 340297
#% 566746
#% 578337
#% 636108
#% 709864
#% 765444
#% 789074
#% 793899
#% 810034
#% 810080
#% 814649
#% 893148
#% 960186
#% 960252
#% 1013748
#! Structured P2P systems have been developed for constructing applications at internet scale in cooperative environments and exhibit a number of desirable features such as scalability and self-maintenance. We argue that such systems when augmented with well defined consistency semantics provide an attractive building block for many large scale data processing applications in cluster environments. Towards this end, we study the problem of providing transactional semantics to P-Ring a P2P system which supports efficient range queries. We first extend a commonly used replication protocol in P2P systems to provide well defined guarantees in the presence of concurrent updates and under well defined failure assumptions. A multi-version concurrency control protocol called LSTP which leverages the guarantees of the replication protocol to provide transactional semantics is proposed. LSTP is designed to provide useful consistency semantics over P-Ring for read intensive workloads without sacrificing the scalability and other desirable properties inherent to the system. Under LSTP, read-only transactions are abort-free and non-blocking and the index stores no state for such transactions. We show that LSTP ensures no missed dependencies between transactions and guarantees basic consistency for read-only transactions when update transactions are serializable. The design of LSTP and its provable properties is a proof of concept that P2P systems can be augmented with transactional semantics. Results from a preliminary simulation study are also presented.

#index 1044436
#* Summary management in P2P systems
#@ Rabab Hayek;Guillaume Raschia;Patrick Valduriez;Noureddine Mouaddib
#t 2008
#c 8
#% 107031
#% 248027
#% 279249
#% 340175
#% 340176
#% 342374
#% 448296
#% 480647
#% 496290
#% 610851
#% 636008
#% 723448
#% 745355
#% 762651
#% 824712
#% 889072
#% 1019101
#! Sharing huge, massively distributed databases in P2P systems is inherently difficult. As the amount of stored data increases, data localization techniques become no longer sufficient. A practical approach is to rely on compact database summaries rather than raw database records, whose access is costly in large P2P systems. In this paper, we consider summaries that are synthetic, multidimensional views with two main virtues. First, they can be directly queried and used to approximately answer a query without exploring the original data. Second, as semantic indexes, they support locating relevant nodes based on data content. Our main contribution is to define a summary model for P2P systems, and the appropriate algorithms for summary management. Our performance evaluation shows that the cost of query routing is minimized, while incurring a low cost of summary maintenance.

#index 1044437
#* Semantic peer, here are the neighbors you want!
#@ Wilma Penzo;Stefano Lodi;Federica Mandreoli;Riccardo Martoglia;Simona Sassatelli
#t 2008
#c 8
#% 32926
#% 296738
#% 447948
#% 479462
#% 577357
#% 631984
#% 643013
#% 762652
#% 779475
#% 893115
#% 907436
#% 907455
#% 943040
#% 1406987
#% 1408709
#% 1711388
#% 1851624
#! Peer Data Management Systems (PDMSs) have been introduced as a solution to the problem of large-scale sharing of semantically rich data. A PDMS consists of semantic peers connected through semantic mappings. Querying a PDMS may lead to very poor results, because of the semantic degradation due to the approximations given by the traversal of the semantic mappings, thus leading to the problem of how to boost a network of mappings in a PDMS. In this paper we propose a strategy for the incremental maintenance of a flexible network organization that clusters together peers which are semantically related in Semantic Overlay Networks (SONs), while maintaining a high degree of node autonomy. Semantic features, a summarized representation of clusters, are stored in a "light" structure which effectively assists a newly entering peer when choosing its semantically closest overlay networks. Then, each peer is supported in the selection of its own neighbors within each overlay network according to two policies: Range-based selection and k-NN selection. For both policies, we introduce specific algorithms which exploit a distributed indexing mechanism for efficient network navigation. The proposed approach has been implemented in a prototype where its effectiveness and efficiency have been extensively tested.

#index 1044438
#* Scalable XQuery type matching
#@ Jens Teubner
#t 2008
#c 8
#% 102313
#% 148890
#% 397358
#% 799999
#% 800577
#% 864401
#% 960318
#% 978931
#% 1016150
#% 1018490
#! XML Schema awareness has been an integral part of the XQuery language since its early design stages. Matching XML data against XML types is the main operation that backs up XQuery type expressions, such as typeswitch, instance of, or certain XPath operators. This interaction is particularly vital in data-centric XQuery applications, where data come with detailed type information from an XML Schema document. So far there has been little work on the optimization of those operations. This work presents an efficient implementation of the runtime aspects of XML Schema support. We propose type ranks as a novel and uniform way to implement all facets of type matching in the W3C XQuery Recommendation. As a concise encoding of the type hierarchy defined by an XML Schema document, type ranks minimize the cost of checking the runtime type of XQuery singleton items. By aggregating type ranks, we leverage the grouping capabilities of modern DBMS implementations to efficiently execute type matching on XQuery sequences. In addition, we improve the complexity bounds incurring with typeswitch expressions over existing approaches. Experiments on an off-the-shelf database system demonstrate the potential of our approach.

#index 1044439
#* Engineering succinct DOM
#@ O'Neil Delpratt;Rajeev Raman;Naila Rahman
#t 2008
#c 8
#% 282026
#% 300153
#% 325324
#% 414919
#% 551856
#% 654451
#% 659997
#% 745477
#% 869542
#% 875010
#% 903395
#% 940558
#% 954493
#% 1096070
#% 1388754
#% 1667820
#% 1673669
#% 1719538
#! We describe the engineering of Succinct DOM (SDOM), a DOM implementation, written in C++, which is suitable for in-memory representation of large static XML documents. SDOM avoids the use of pointers, and is based upon succinct data structures, which use an information-theoretically minimum amount of space to represent an object. SDOM gives a space-efficient in-memory representation, with stable and predictable memory usage. The space used by SDOM is an order of magnitude less than that used by a standard C++ DOM representation such as Xerces, but SDOM is extremely fast: navigation is in some cases faster than for a pointer-based representation such as Xerces (even for moderate-sized documents which can comfortably be loaded into main memory by Xerces). A variant, SDOM-CT, applies bzip-based compression to textual and attribute data, and its space usage is comparable with "queryable" XML compressors. Some of these compressors support navigation and/or querying (e.g. subpath queries) of the compressed file. SDOM-CT does not support querying directly, but remains extremely fast: it is several orders of magnitude faster for navigation than queryable XML compressors that support navigation (and only a few times slower than say Xerces).

#index 1044440
#* Revisiting redundancy and minimization in an XPath fragment
#@ Benny Kimelfeld;Yehoshua Sagiv
#t 2008
#c 8
#% 123085
#% 397374
#% 465051
#% 570877
#% 599549
#% 733593
#% 824661
#% 1015267
#! Redundancy and minimization of queries are investigated in a well known fragment of XPath that includes child and descendant edges, branches, wildcards, and multiple output nodes. Contrary to a published result, a proposed technique does not guarantee minimality or even non-redundancy, and it is unknown whether a non-redundant query is also minimal. It is shown that for two sub-fragments, non-redundancy and minimality are the same, and can be realized by means of simple (local) tests. The latter property is used to prove that testing non-redundancy is NP-complete.

#index 1044441
#* Schema merging and mapping creation for relational sources
#@ Rachel Pottinger;Philip A. Bernstein
#t 2008
#c 8
#% 11284
#% 13048
#% 22948
#% 198465
#% 378409
#% 415979
#% 442861
#% 458607
#% 480149
#% 480969
#% 481923
#% 568191
#% 572307
#% 572311
#% 654457
#% 795746
#% 810021
#% 1015326
#% 1206580
#! We address the problem of generating a mediated schema from a set of relational data source schemas and conjunctive queries that specify where those schemas overlap. Unlike past approaches that generate only the mediated schema, our algorithm also generates view definitions, i.e., source-to-mediated schema mappings. Our main goal is to understand the requirements that a mediated schema and views should satisfy, such as completeness, preservation of overlapping information, normalization, and minimality. We show how these requirements influence the detailed structure of schemas and view definitions that are produced. We introduce a normal form for mediated schemas and view definitions, show how to generate them, and prove that schemas and views in this normal form satisfy our requirements. The view definitions in our normal form use stylized GLAV mappings, for which query rewriting is easier than general GLAV mappings. We demonstrate the efficiency of query rewriting in a prototype implementation.

#index 1044442
#* Schema mapping verification: the spicy way
#@ A. Bonifati;G. Mecca;A. Pappalardo;S. Raunich;G. Summa
#t 2008
#c 8
#% 201959
#% 307632
#% 333988
#% 333990
#% 348580
#% 384978
#% 479783
#% 480134
#% 551850
#% 572314
#% 654458
#% 659941
#% 660001
#% 765433
#% 800498
#% 810078
#% 810103
#% 824763
#% 893094
#% 893095
#% 893114
#% 945786
#% 960233
#% 993981
#% 993982
#% 1390190
#% 1688251
#% 1705177
#% 1730010
#% 1915878
#! Schema mapping algorithms rely on value correspondences - i.e., correspondences among semantically related attributes - to produce complex transformations among data sources. These correspondences are either manually specified or suggested by separate modules called schema matchers. The quality of mappings produced by a mapping generation tool strongly depends on the quality of the input correspondences. In this paper, we introduce the Spicy system, a novel approach to the problem of verifying the quality of mappings. Spicy is based on a three-layer architecture, in which a schema matching module is used to provide input to a mapping generation module. Then, a third module, the mapping verification module, is used to check candidate mappings and choose the ones that represent better transformations of the source into the target. At the core of the system stands a new technique for comparing the structure and actual content of trees, called structural analysis. Experimental results show that, by carefully designing the comparison algorithm, it is possible to achieve both good scalability and high precision in mapping selection.

#index 1044443
#* SeMap: a generic mapping construction system
#@ Ting Wang;Rachel Pottinger
#t 2008
#c 8
#% 333988
#% 333990
#% 351595
#% 375017
#% 459496
#% 480645
#% 529190
#% 572314
#% 654458
#% 660001
#% 742769
#% 756671
#% 765409
#% 765433
#% 790844
#% 824658
#% 824735
#% 830529
#% 993982
#% 1289178
#! Most previous schema mapping works focus on creating mappings in specific data models for data transformation, failing to capture a richer set of possible relationships between schema elements. For example, most schema matching approaches might discover that 'TA' in one schema equals 'grad TA' in another one, even though the relationship can be modeled more accurately by saying that 'grad TA' is a specialization of 'TA'. Deepening the mapping semantics in turn allow richer application semantics. This paper presents and proves the effectiveness of SeMap, a system that constructs a complex, semantically richer mapping (including 'Has-a', 'Is-a', 'Associates' and 'Equivalent' relationship types) that can be used across data models. We achieve this goal by: (1) exploiting semantic evidence for possible matches; (2) finding a globally optimal match assignment; (3) identifying the relationship embedded in the selected matches. We implemented our semantic matching approach within a prototype system, SeMap, and showed its accuracy and effectiveness.

#index 1044444
#* Deleting index entries from compliance storage
#@ Soumyadeb Mitra;Marianne Winslett;Nikita Borisov
#t 2008
#c 8
#% 290703
#% 810041
#% 893171
#% 904270
#! In response to regulatory focus on secure retention of electronic records, businesses are using magnetic disks configured as write-once read-many (WORM) compliance storage devices to store business documents such as electronic mail for their mandated retention periods. A document committed to a compliance storage device cannot be altered or deleted even by a superuser until its retention period is over, and hence is secure from attacks originating from company insiders. Secure retention, however, is only a part of a document's lifecycle: it is often crucial to properly delete documents once their retention period ends. It is relatively simple to delete a document, but much harder to remove its index entries from WORM. Yet if these entries are not obliterated, the contents of the deleted document can often be reconstructed. In this paper, we formally define secure deletion of document entries from an inverted index on compliance storage. We show that previously proposed deletion schemes for compliance storage index entries do not meet the objectives of secure deletion. On the other hand, the naive approach to secure deletion results in very poor query performance. To provide secure deletion of index entries without compromising lookup efficiency, we propose a novel indexing technique that employs noise terms, merged posting lists, and deletion epochs. Experiments with real-life data show that lookups in our scheme are 5 times faster than the naive approach.

#index 1044445
#* Online recovery in cluster databases
#@ WeiBin Liang;Bettina Kemme
#t 2008
#c 8
#% 201869
#% 248825
#% 273894
#% 287303
#% 335454
#% 342964
#% 403195
#% 508197
#% 545902
#% 546049
#% 570890
#% 594328
#% 617478
#% 745516
#% 793894
#% 800516
#% 800544
#% 810043
#% 810268
#% 938074
#% 960202
#% 981519
#% 993994
#% 1142433
#! Cluster based replication solutions are an attractive mechanism to provide both high-availability and scalability for the database backend within the multi-tier information systems of service-oriented businesses. An important issue that has not yet received sufficient attention is how database replicas that have failed can be reintegrated into the system or how completely new replicas can be added in order to increase the capacity of the system. Ideally, recovery takes place online, i.e, while transaction processing continues at the replicas that are already running. In this paper we present a complete online recovery solution for database clusters. One important issue is to find an efficient way to transfer the data the joining replica needs. In this paper, we present two data transfer strategies. The first transfers the latest copy of each data item, the second transfers the updates a rejoining replica has missed during its downtime. A second challenge is to coordinate this transfer with ongoing transaction processing such that the joining node does not miss any updates. We present a coordination protocol that can be used with Postgres-R, a replication tool which uses a group communication system for replica control. We have implemented and compared our transfer solutions against a set of parameters, and present heuristics which allow an automatic selection of the optimal strategy for a given configuration.

#index 1044446
#* A concurrency control protocol for parallel B-tree structures without latch-coupling for explosively growing digital content
#@ Tomohiro Yoshihara;Dai Kobayashi;Haruo Yokota
#t 2008
#c 8
#% 9241
#% 64430
#% 102808
#% 102810
#% 115661
#% 116087
#% 152946
#% 286929
#% 287797
#% 291189
#% 300164
#% 300207
#% 403195
#% 437936
#% 460871
#% 571093
#% 587699
#% 631965
#% 723279
#% 746071
#% 770366
#% 803126
#% 916329
#% 963667
#% 1002142
#% 1016185
#% 1186485
#! While shared-nothing parallel infrastructures provide fast processing of explosively growing digital content, managing data efficiently across multiple nodes is important. The value-range partitioning method with parallel B-tree structures in a shared-nothing environment is an efficient approach for handling large amounts of data. To handle large amounts of data, it is also important to provide an efficient concurrency control protocol for the parallel B-tree. Many studies have proposed concurrency control protocols for B-trees, which use latch-coupling. None of these studies has considered that latch-coupling contains a performance bottleneck of sending of messages between processing elements (PEs) in distributed environments because latch-coupling is efficient for a B-tree on a single machine. The only protocol without latch-coupling is the B-link algorithm, but it is difficult to use the B-link algorithm directly on an entire parallel B-tree structure because it is necessary to guarantee the consistency of the side pointers. We propose a new concurrency control protocol named LCFB that requires no latch-coupling in optimistic processes. LCFB reduces the amount of communication between PEs during a B-tree traversal. To detect access path errors in the LCFB protocol caused by removal of latch-coupling, we assign boundary values to each index page. Because a page split may cause page deletion in a Fat-Btree, we also propose an effective method for handling page deletions without latch-coupling. We then combine LCFB with the B-link algorithm within each PE to reduce the cost of Structure Modification Operations (SMOs) in a PE, as a solution to the difficulty of consistency management for the side pointers in a parallel B-tree structure. To compare the performance of the proposed protocol with conventional protocols MARK-OPT, INC-OPT, and ARIES/IM, we implemented them on an autonomous disk system with a Fat-Btree structure. Experimental results in various environments indicate that the system throughput of the proposed protocols is always superior to those of the other protocols, especially in large-scale configurations, and LCFB with the B-link algorithm is effective at higher update ratios.

#index 1044447
#* Robustness in automatic physical database design
#@ Kareem El Gebaly;Ashraf Aboulnaga
#t 2008
#c 8
#% 248815
#% 273901
#% 333947
#% 397371
#% 397390
#% 480158
#% 480803
#% 482100
#% 632100
#% 765427
#% 810016
#% 810017
#% 810026
#% 864424
#% 875027
#% 893130
#% 1016220
#% 1016221
#% 1016225
#% 1688268
#! Automatic physical database design tools rely on "what-if" interfaces to the query optimizer to estimate the execution time of the training query workload under different candidate physical designs. The tools use these what-if interfaces to recommend physical designs that minimize the estimated execution time of the input training workload. In this paper, we argue that minimizing estimated execution time alone can lead to designs with inherent problems. In particular, if the optimizer makes an error in estimating the execution time of some workload queries, then the recommended physical design may actually harm the workload instead of benefiting it. In this sense, the physical design is risky. Moreover, if the production queries are slightly different from the training queries, the recommended physical design may not benefit them at all. In this sense, the physical design is not general. We define Risk and Generality as two new metrics to evaluate the quality of a proposed physical database design, and we show one way of extending the objective function being optimized by a generic physical design advisor to take these measures into account. We have implemented a physical design advisor in PostgreSQL, and we use it to experimentally demonstrate the usefulness of our approach. We show that our two new metrics result in physical designs that are more robust, which means that the user can implement them with a higher degree of confidence. This is particularly important as we move towards truly zero-administration database systems in which there is not the possibility for a DBA to vet the recommendations of the physical design tool before applying them.

#index 1044448
#* Self-organizing strategies for a column-store database
#@ Milena Ivanova;Martin L. Kersten;Niels Nes
#t 2008
#c 8
#% 442850
#% 632100
#% 765176
#% 824697
#% 864446
#% 875026
#% 875062
#% 982557
#% 1015367
#% 1016220
#% 1727150
#! Column-store database systems open new vistas for improved maintenance through self-organization. Individual columns are the focal point, which simplify balancing conflicting requirements. This work presents two workload-driven self-organizing techniques in a column-store, i.e. adaptive segmentation and adaptive replication. Adaptive segmentation splits a column into non-overlapping segments based on the actual query load. Likewise, adaptive replication creates segment replicas. The strategies can support different application requirements by trading off the reorganization overhead for storage cost. Both techniques can significantly improve system performance as demonstrated in an evaluation of different scenarios.

#index 1044449
#* Load distribution of analytical query workloads for database cluster architectures
#@ Thomas Phan;Wen-Syan Li
#t 2008
#c 8
#% 300141
#% 301084
#% 318016
#% 348459
#% 366182
#% 369236
#% 397397
#% 464056
#% 465701
#% 480158
#% 595211
#% 729456
#% 743779
#% 793341
#% 800595
#% 820356
#% 848844
#% 867051
#% 875027
#% 897426
#% 993397
#% 1016220
#% 1697885
#% 1712638
#! Enterprises may have multiple database systems spread across the organization for redundancy or for serving different applications. In such systems, query workloads can be distributed across different servers for better performance. A materialized view, or Materialized Query Table (MQT), is an auxiliary table with pre-computed data that can be used to significantly improve the performance of a database query. In this paper, we propose a framework for coordinating execution of OLAP query workloads across a database cluster with shared nothing architecture. Such coordination is complex since we need to consider (1) the time to build the MQTs, (2) the query execution impact of the MQTs, (3) whether the MQTs can fit in the disk space limitation, (4) server computation power, and (5) the effectiveness of the scheduling and placement algorithms in deriving a combination of configurations so that the workload can be completed in the shortest time period. We frame the problem as a combinatorial problem with a solution space that is exponential in the number of queries, MQTs, and servers. We provide a stochastic search heuristic that finds a near-optimal mapping of queries-to-servers and MQTs-to-servers within an arbitrarily bounded time and compare our solution with an exhaustive search and three standard greedy algorithms. Our search implementation produced schedules within 9% of the optimal found through an exhaustive search and produced better solutions than typical greedy algorithms for both TPC-H and synthetic benchmarks under a variety of experiments. For a key trial where disk space is limited, it produced 15% better results than the next best competitor, corresponding to an absolute wall clock advantage of over 10 hours.

#index 1044450
#* A novel spectral coding in a large graph database
#@ Lei Zou;Lei Chen;Jeffrey Xu Yu;Yansheng Lu
#t 2008
#c 8
#% 288098
#% 288990
#% 378391
#% 427870
#% 443133
#% 466644
#% 629708
#% 765429
#% 864425
#% 893110
#% 960305
#% 989610
#% 1022280
#% 1673591
#! Retrieving related graphs containing a query graph from a large graph database is a key issue in many graph-based applications, such as drug discovery and structural pattern recognition. Because sub-graph isomorphism is a NP-complete problem [4], we have to employ a filter-and-verification framework to speed up the search efficiency, that is, using an effective and efficient pruning strategy to filter out the false positives (graphs that are not possible in the results) as many as possible first, then validating the remaining candidates by subgraph isomorphism checking. In this paper, we propose a novel filtering method, a spectral encoding method, i.e. GCoding. Specifically, we assign a signature to each vertex based on its local structures. Then, we generate a spectral graph code by combining all vertex signatures in a graph. Based on spectral graph codes, we derive a necessary condition for sub-graph isomorphism. Then we propose two pruning rules for sub-graph search problem, and prove that they satisfy the no-false-negative requirement (no dismissal in answers). Since graph codes are in numerical space, we take this advantage and conduct efficient filtering over graph codes. Extensive experiments show that GCoding outperforms existing counterpart methods.

#index 1044451
#* Fast computing reachability labelings for large graphs with high compression rate
#@ Jiefeng Cheng;Jeffrey Xu Yu;Xuemin Lin;Haixun Wang;Philip S. Yu
#t 2008
#c 8
#% 10634
#% 58365
#% 94589
#% 148021
#% 236409
#% 379482
#% 410276
#% 571039
#% 577372
#% 600184
#% 765272
#% 800534
#% 824692
#% 838518
#% 864462
#% 960304
#% 994015
#% 1688299
#! There are numerous applications that need to deal with a large graph and need to query reachability between nodes in the graph. A 2-hop cover can compactly represent the whole edge transitive closure of a graph in O(|V| . |E|1/2) space, and be used to answer reachability query efficiently. However, it is challenging to compute a 2-hop cover. The existing approaches suffer from either large resource consumption or low compression rate. In this paper, we propose a hierarchical partitioning approach to partition a large graph G into two subgraphs repeatedly in a top-down fashion. The unique feature of our approach is that we compute 2-hop cover while partitioning. In brief, in every iteration of top-down partitioning, we provide techniques to compute the 2-hop cover for connections between the two subgraphs first. A cover is computed to cut the graph into two subgraphs, which results in an overall cover with high compression for the entire graph G. Two approaches are proposed, namely a node-oriented approach and an edge-oriented approach. Our approach can efficiently compute 2-hop cover for a large graph with high compression rate. Our extensive experiment studies show that the 2-hop cover for a graph with 1,700,000 nodes and 169 billion connections can be obtained in less than 30 minutes with a compression rate about 40,000 using a PC.

#index 1044452
#* Finding time-dependent shortest paths over large graphs
#@ Bolin Ding;Jeffrey Xu Yu;Lu Qin
#t 2008
#c 8
#% 70370
#% 172380
#% 193110
#% 214719
#% 214769
#% 267476
#% 281884
#% 284109
#% 316943
#% 319648
#% 344035
#% 443105
#% 443208
#% 463251
#% 463583
#% 464223
#% 527029
#% 554910
#% 593917
#% 749474
#% 789798
#% 864397
#! The spatial and temporal databases have been studied widely and intensively over years. In this paper, we study how to answer queries of finding the best departure time that minimizes the total travel time from a place to another, over a road network, where the traffic conditions dynamically change from time to time. We study a generalized form of this problem, called the time-dependent shortest-path problem. A time-dependent graph GT is a graph that has an edge-delay function, wi, j(t), associated with each edge (vi, vj), to be stored in a database. The edge-delay function wi, j(t) specifies how much time it takes to travel from node vi to node vj, if it departs from vi at time t. A user-specified query is to ask the minimum-travel-time path, from a source node, vs, to a destination node, ve, over the time-dependent graph, GT, with the best departure time to be selected from a time interval T. We denote this user query as LTT(vs, ve, T) over GT. The challenge of this problem is the added complexity due to the time dependency in the time-dependent graph. That is, edge delays are not constants, and can vary from time to time. In this paper, we propose a novel algorithm to find the minimum-travel-time path with the best departure time for a LTT(vs, ve, T) query over a large graph GT. Our approach outperforms existing algorithms in terms of both time complexity in theory and efficiency in practice. We will discuss the design of our algorithm, together with its correctness and complexity. We conducted extensive experimental studies over large graphs and will report our findings.

#index 1044453
#* Taxonomy-superimposed graph mining
#@ Ali Cakmak;Gultekin Ozsoyoglu
#t 2008
#c 8
#% 115608
#% 459006
#% 466644
#% 481758
#% 629708
#% 727845
#% 729938
#% 769940
#% 785396
#% 982577
#% 1041337
#% 1275285
#% 1673586
#! New graph structures where node labels are members of hierarchically organized ontologies or taxonomies have become commonplace in different domains, e.g., life sciences. It is a challenging task to mine for frequent patterns in this new graph model which we call taxonomy-superimposed graphs, as there may be many patterns that are implied by the generalization/specialization hierarchy of the associated node label taxonomy. Hence, 'standard' graph mining techniques are not directly applicable. In this paper, we present Taxogram, a taxonomy-superimposed graph mining algorithm that can efficiently discover frequent graph structures in a database of taxonomy-superimposed graphs. Taxogram has two advantages: (i) It performs a subgraph isomorphism test once per class of patterns which are structurally isomorphic, but have different labels, and (ii) it reconciles standard graph mining methods with taxonomy-based graph mining and takes advantage of well-studied methods in the literature. Taxogram has three stages: (a) relabeling nodes in the input database, (b) mining pattern classes/families and constructing associated occurrence indices, and (c) computing patterns and eliminating useless (i.e., over-generalized) patterns by post-processing occurrence indices. Experimental results show that Taxogram is significantly more efficient and more scalable compared to other alternative approaches.

#index 1044454
#* Compacting music signatures for efficient music retrieval
#@ Bin Cui;H. V. Jagadish;Beng Chin Ooi;Kian-Lee Tan
#t 2008
#c 8
#% 121278
#% 137711
#% 172949
#% 261882
#% 286744
#% 289010
#% 341300
#% 341365
#% 342720
#% 396670
#% 413598
#% 443055
#% 451658
#% 480654
#% 631989
#% 654456
#% 730141
#% 730187
#% 741408
#% 780704
#% 780785
#% 905167
#% 997254
#! Music information retrieval is becoming very important with the ever-increasing growth of music content in digital libraries, peer-to-peer systems and the internet. While it is easy to quantize music into a discrete string representation, retrieval by content requires (approximate) sub-string matching, which is hard. In this paper, we present a novel system, called MUSIG, that uses compact MUsic SIGnatures for efficient contentbased music retrieval. The signature is computed as follows: (a) each music file is split into a set of (overlapping) segments; (b) similar segments are clustered together; the number of clusters corresponds to the number of dimensions; (c) for each music file, the number of its segments that fall into a cluster determines the key value in that dimension. Most index structures for multimedia are only able to provide an initial filtering and return a set of candidate answers that must be further examined. For MUSIG, we have also designed a scoring function that permits a ranked answer set to be generated directly based only on the signatures. Our experimental results show that this scheme retains a high degree of accuracy while being very efficient.

#index 1044455
#* Indexing high-dimensional data in dual distance spaces: a symmetrical encoding approach
#@ Yi Zhuang;Yueting Zhuang;Qing Li;Lei Chen;Yi Yu
#t 2008
#c 8
#% 86950
#% 227937
#% 227939
#% 248796
#% 321455
#% 342827
#% 342828
#% 427199
#% 435141
#% 464195
#% 465160
#% 479462
#% 479649
#% 480133
#% 481956
#% 587749
#% 632035
#% 814646
#! Due to the well-known dimensionality curse problem, search in a high-dimensional space is considered as a "hard" problem. In this paper, a novel symmetrical encoding-based index structure, which is called EHD-Tree (for symmetrical Encoding-based Hybrid Distance Tree), is proposed to support fast k-Nearest-Neighbor (k-NN) search in high-dimensional spaces. In an EHD-Tree, all data points are first grouped into clusters by a k-Means clustering algorithm. Then the uniform ID number of each data point is obtained by a dual-distance-driven encoding scheme in which each cluster sphere is partitioned twice according to the dual distances of start- and centroid-distance. Finally, the uniform ID number and the centroid-distance of each data point are combined to get a uniform index key, the latter is then indexed through a partition-based B+-tree. Thus, given a query point, its k-NN search in high-dimensional spaces can be transformed into search in a single dimensional space with the aid of the EHD-Tree index. Extensive performance studies are conducted to evaluate the effectiveness and efficiency of our proposed scheme, and the results demonstrate that this method outperforms the state-of-the-art high dimensional search techniques such as the X-Tree, VA-file, iDistance and NB-Tree, especially when the query radius is not very large.

#index 1044456
#* The TS-tree: efficient time series search and retrieval
#@ Ira Assent;Ralph Krieger;Farzad Afschari;Thomas Seidl
#t 2008
#c 8
#% 86950
#% 248797
#% 271801
#% 287715
#% 342828
#% 359751
#% 427199
#% 435141
#% 460862
#% 479649
#% 480133
#% 480146
#% 481956
#% 577221
#% 632035
#% 654456
#% 662750
#% 765412
#% 765537
#% 805839
#% 907596
#% 993965
#! Continuous growth in sensor data and other temporal data increases the importance of retrieval and similarity search in time series data. Efficient time series query processing is crucial for interactive applications. Existing multidimensional indexes like the R-tree provide efficient querying for only relatively few dimensions. Time series are typically long which corresponds to extremely high dimensional data in multidimensional indexes. Due to massive overlap of index descriptors, multidimensional indexes degenerate for high dimensions and access the entire data by random I/O. Consequently, the efficiency benefits of indexing are lost. In this paper, we propose the TS-tree (time series tree), an index structure for efficient time series retrieval and similarity search. Exploiting inherent properties of time series quantization and dimensionality reduction, the TS-tree indexes high-dimensional data in an overlap-free manner. During query processing, powerful pruning via quantized separator and meta data information greatly reduces the number of pages which have to be accessed, resulting in substantial speed-up. In thorough experiments on synthetic and real world time series data we demonstrate that our TS-tree outperforms existing approaches like the R*-tree or the quantized A-tree.

#index 1044457
#* Anonymity for continuous data publishing
#@ Benjamin C. M. Fung;Ke Wang;Ada Wai-Chee Fu;Jian Pei
#t 2008
#c 8
#% 576761
#% 800514
#% 801690
#% 810011
#% 844340
#% 864406
#% 864412
#% 881497
#% 881546
#% 960291
#% 975045
#% 982549
#% 1725659
#! k-anonymization is an important privacy protection mechanism in data publishing. While there has been a great deal of work in recent years, almost all considered a single static release. Such mechanisms only protect the data up to the first release or first recipient. In practical applications, data is published continuously as new data arrive; the same data may be anonymized differently for a different purpose or a different recipient. In such scenarios, even when all releases are properly k-anonymized, the anonymity of an individual may be unintentionally compromised if recipient cross-examines all the releases received or colludes with other recipients. Preventing such attacks, called correspondence attacks, faces major challenges. In this paper, we systematically characterize the correspondence attacks and propose an efficient anonymization algorithm to thwart the attacks in the model of continuous data publishing.

#index 1044458
#* Ownership protection of shape datasets with geodesic distance preservation
#@ Michail Vlachos;Claudio Lucchese;Deepak Rajan;Philip S. Yu
#t 2008
#c 8
#% 283261
#% 338418
#% 443133
#% 443242
#% 727904
#% 729930
#% 784512
#% 844360
#% 863402
#% 915325
#% 972310
#% 993944
#% 1010961
#% 1669943
#% 1854543
#! Protection of one's intellectual property is a topic with important technological and legal facets. The significance of this issue is amplified nowadays due to the ease of data dissemination through the internet. Here, we provide technological mechanisms for establishing the ownership of a dataset consisting of multiple objects. The objects that we consider in this work are shapes (i.e., two dimensional contours), which abound in disciplines such as medicine, biology, anthropology and natural sciences. The protection of the dataset is achieved through means of embedding of an imperceptible ownership 'seal', that imparts only minute visual distortions. This seal needs to be embedded in the proper data space so that its removal or destruction is particularly difficult. Our technique is robust to many common transformations, such as data rotation, translation, scaling, noise addition and resampling. In addition to that, the proposed scheme also guarantees that important distances between the dataset shapes/objects are not distorted. We achieve this by preserving the geodesic distances between the dataset objects. Geodesic distances capture a significant part of the dataset structure, and their usefulness is recognized in many machine learning, visualization and clustering algorithms. Therefore, if a practitioner uses the protected dataset as input to a variety of mining, machine learning, or database operations, the output will be the same as on the original dataset. We illustrate and validate the applicability of our methods on image shapes extracted from anthropological and natural science data.

#index 1044459
#* Zerber: r-confidential indexing for distributed documents
#@ Sergej Zerr;Elena Demidova;Daniel Olmedilla;Wolfgang Nejdl;Marianne Winslett;Soumyadeb Mitra
#t 2008
#c 8
#% 151495
#% 195456
#% 278831
#% 319849
#% 397367
#% 433922
#% 514193
#% 577239
#% 664705
#% 768898
#% 800509
#% 800514
#% 800515
#% 803779
#% 810254
#% 824762
#% 830694
#% 841404
#% 864406
#% 864412
#% 893171
#% 963446
#% 1015329
#% 1015331
#! To carry out work assignments, small groups distributed within a larger enterprise often need to share documents among themselves while shielding those documents from others' eyes. In this situation, users need an indexing facility that can quickly locate relevant documents that they are allowed to access, without (1) leaking information about the remaining documents, (2) imposing a large management burden as users, groups, and documents evolve, or (3) requiring users to agree on a central completely trusted authority. To address this problem, we propose the concept of r-confidentiality, which captures the degree of information leakage from an index about the terms contained in inaccessible documents. Then we propose the r-confidential Zerber indexing facility for sensitive documents, which uses secret splitting and term merging to provide tunable limits on information leakage, even under statistical attacks; requires only limited trust in a central indexing authority; and is extremely easy to use and administer. Experiments with real-world data show that Zerber offers excellent performance for index insertions and lookups while requiring only a modest amount of storage space and network bandwidth.

#index 1044460
#* XCraft: boosting the performance of active XML materialization
#@ Gabriela Ruberg;Marta Mattoso
#t 2008
#c 8
#% 291459
#% 308351
#% 348459
#% 451429
#% 590497
#% 632068
#% 654465
#% 654485
#% 765420
#% 785188
#% 806637
#% 825665
#% 840149
#% 897426
#% 1390814
#% 1688304
#! An active XML (AXML) document contains tags representing calls to Web services. Therefore, retrieving its contents consists in materializing its data elements by invoking the embedded service calls in a P2P network. In this process, the result of some service calls can be used as input of other calls. Also, usually several peers provide each requested Web service, and peers can collaborate to invoke these services. This often implies a huge search space of many equivalent materialization alternatives, each with different performance. In this paper, we model AXML documents from a workflow perspective and propose a dynamic cost-based optimization strategy to efficiently materialize them, considering the volatility of a typical P2P scenario. Our strategy enables the optimizer, called XCraft, to get more up-to-date information on the status of the peers, and to deliver partial results earlier. Based on a service-oriented algebra of plan operators, we exploit P2P collaboration to delegate both execution and optimization control. Our tests with an XCraft prototype show important performance gains w.r.t. a centralized approach, whilst the optimizer also achieved to drastically reduce the size of the search space.

#index 1044461
#* Exact and inexact methods for selecting views and indexes for OLAP performance improvement
#@ Zohreh Asgharzadeh Talebi;Rada Chirkova;Yahya Fathi;Matthias Stallmann
#t 2008
#c 8
#% 210182
#% 223781
#% 273697
#% 424925
#% 443001
#% 462204
#% 479476
#% 479646
#% 480158
#% 482100
#% 482111
#% 637765
#% 778724
#% 788999
#% 1700143
#% 1720722
#! In on-line analytical processing (OLAP), precomputing (materializing as views) and indexing auxiliary data aggregations is a common way of reducing query-evaluation time costs for important data-analysis queries. We consider an OLAP view- and index-selection problem stated as an optimization problem, where (i) the inputs include the data-warehouse schema, a set of data-analysis queries of interest, and a storage-limit constraint, and (ii) the output is a set of views and indexes that minimizes the costs of the input queries, subject to the storage limit. While greedy and other heuristic strategies for choosing views or indexes might help to some extent in improving the costs, it is highly nontrivial to arrive at a globally optimum solution, one that reduces the processing costs of typical OLAP queries as much as is theoretically possible. In fact, as observed in [17] and to the best of our knowledge, there is no known approximation algorithm for OLAP view or index selection with nontrivial performance guarantees. In this paper we propose a systematic study of the OLAP view- and index-selection problem. Our specific contributions are as follows: (1) We develop an algorithm that effectively and efficiently prunes the space of potentially beneficial views and indexes when given realistic-size instances of the problem. (2) We provide formal proofs that our pruning algorithm keeps at least one globally optimum solution in the search space, thus the resulting integer-programming model is guaranteed to find an optimal solution. (3) We develop a family of algorithms to further reduce the size of the search space, so that we are able to solve larger problem instances, although we no longer guarantee the global optimality of the resulting solution. (4) Finally, we present an experimental comparison of our proposed approaches with the state-of-the-art approaches of [2, 12]. Our experiments show that our approaches to view and index selection result in high-quality solutions --- in fact, in globally optimum solutions for many realistic-size problem instances. Thus, they compare favorably with the well-known OLAP-centered approach of [12] and provide for a winning combination with the end-to-end framework of [2] for generic view and index selection.

#index 1044462
#* Providing freshness guarantees for outsourced databases
#@ Min Xie;Haixun Wang;Jian Yin;Xiaofeng Meng
#t 2008
#c 8
#% 397367
#% 513367
#% 551826
#% 566391
#% 593797
#% 765448
#% 810042
#% 824701
#% 874980
#% 1022267
#! Database outsourcing becomes increasingly attractive as advances in network technologies eliminate the perceived performance difference between in-house databases and out-sourced databases, and price advantages of third-party data-base service providers continue to increase due to economy of scale. However, the potentially explosive growth of database outsourcing is hampered by security concerns, namely data privacy and query integrity of outsourced databases. While privacy issues of outsourced databases have been extensively studied, query integrity for outsourced databases has just started to draw attention from the database community. Currently, there still does not exist a solution that can provide complete integrity. In particular, previous studies have not examined the mechanisms for providing freshness guarantees, that is, the assurance that queries are executed against the most up-to-date data, instead of just some version of the data in the past. Providing a practical solution for freshness guarantees is challenging because continuously monitoring data's up-to-dateness is expensive. In this paper, we perform a thorough study on how to add freshness guarantees over proposed schemes (including authenticated data structure-based and probabilistic-based approaches) to provide integrity assurance. We implement our solutions and perform extensive experiments to quantify the cost. Our experiment results show that we can provide reasonable tight freshness guarantees without sacrificing much performance.

#index 1044463
#* Dynamic skyline queries in metric spaces
#@ Lei Chen;Xiang Lian
#t 2008
#c 8
#% 411758
#% 427199
#% 464195
#% 465167
#% 479462
#% 480146
#% 480671
#% 481279
#% 617886
#% 654480
#% 800555
#% 806212
#% 810024
#% 824671
#% 824672
#% 864452
#% 881921
#% 889094
#% 893150
#% 993954
#% 1022203
#% 1022224
#% 1022225
#% 1022226
#% 1206780
#% 1688273
#! Skyline query is of great importance in many applications, such as multi-criteria decision making and business planning. In particular, a skyline point is a data object in the database whose attribute vector is not dominated by that of any other objects. Previous methods to retrieve skyline points usually assume static data objects in the database (i.e. their attribute vectors are fixed), whereas several recent work focus on skyline queries with dynamic attributes. In this paper, we propose a novel variant of skyline queries, namely metric skyline, whose dynamic attributes are defined in the metric space (i.e. not limited to the Euclidean space). We illustrate an efficient and effective pruning mechanism to answer metric skyline queries through a metric index. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed pruning techniques over the metric index in answering metric skyline queries.

#index 1044464
#* Fast contextual preference scoring of database tuples
#@ Kostas Stefanidis;Evaggelia Pitoura
#t 2008
#c 8
#% 198058
#% 213981
#% 300170
#% 333854
#% 333951
#% 420175
#% 480330
#% 488602
#% 631988
#% 641963
#% 731407
#% 800588
#% 810013
#% 810025
#% 875002
#% 993957
#% 1408811
#% 1727459
#% 1728220
#! To provide users with only relevant data from the huge amount of available information, personalization systems utilize preferences to allow users to express their interest on specific pieces of data. Most often, user preferences vary depending on the circumstances. For instance, when with friends, users may like to watch thrillers, whereas, when with their kids, they may prefer to watch cartoons. Contextual preference systems address this challenge by supporting preferences that depend on the values of contextual attributes such as the surrounding environment, time or location. In this paper, we address the problem of finding interesting data items based on contextual preferences that assign interest scores to pieces of data based on context. To this end, we propose a number of pre-processing steps. Instead of pre-computing scores for all data items under all potential context states, we exploit the hierarchical nature of context attributes to identify representative context states. Furthermore, we introduce a method for grouping preferences based on the similarity of the scores that they produce. This method uses a bitmap representation of preferences and scores with various levels of precision that lead to approximate rankings with different degrees of accuracy. We evaluate our approach using both real and synthetic data sets and present experimental results showing the quality of the scores attained using our methods.

#index 1044465
#* Efficient online top-K retrieval with arbitrary similarity measures
#@ Prasad M Deshpande;Deepak P;Krishna Kummamuru
#t 2008
#c 8
#% 158958
#% 213981
#% 397378
#% 397608
#% 643566
#% 655302
#% 763882
#% 844387
#% 857113
#% 864459
#% 893128
#% 960242
#% 1013994
#% 1016183
#% 1688286
#! The top-k retrieval problem requires finding k objects most similar to a given query object. Similarities between objects are most often computed as aggregated similarities of their attribute values. We consider the case where the similarities between attribute values are arbitrary (non-metric), due to which standard space partitioning indexes cannot be used. Among the most popular techniques that can handle arbitrary similarity measures is the family of threshold algorithms. These were designed as middleware algorithms that assume that similarity lists for each attribute are available and focus on efficiently merging these lists to arrive at the results. In this paper, we explore multi-dimensional indexing of non-metric spaces that can lead to efficient pruning of the search space utilizing inter-attribute relationships, during top-k computation. We propose an indexing structure, the AL-Tree and an algorithm to do top-k retrieval using it in an online fashion. The ALTree exploits the fact that many real world attributes come from a small value space. We show that our algorithm performs much better than the threshold based algorithms in terms of computational cost due to efficient pruning of the search space. Further, it out-performs them in terms of IOs by upto an order of magnitude in case of dense datasets.

#index 1044466
#* Mining all frequent projection-selection queries from a relational table
#@ Tao-Yuan Jen;Dominique Laurent;Nicolas Spyratos
#t 2008
#c 8
#% 36683
#% 232136
#% 420076
#% 458827
#% 546700
#% 550412
#% 729418
#% 823330
#% 1742006
#! In this paper we study the problem of mining all frequent queries in a given database table, a problem known to be intractable even for conjunctive queries. We restrict our attention to projection-selection queries, and we assume that the table to be mined satisfies a set of functional dependencies. Under these assumptions we define a pre-ordering &cupre; over queries and we show the following: (a) the support measure is anti-monotonic (with respect to &cupre;), and (b) if we define q &cupre; q' iff q &cupre; q' and q' &cupre; q then all queries of an equivalence class have the same support. With these results at hand, we further restrict our attention to star schemas of data warehouses. In those schemas, the set of functional dependencies satisfies an important property, namely, the union of keys of all dimension tables is a key for the fact table. The main contribution of this paper is the proposal of a level-wise algorithm for mining all frequent projection-selection queries in a data warehouse over a star schema. Moreover, we show that, in the case of a star schema, the complexity in the number of scans of our algorithm is similar to that of the well known Apriori algorithm, i.e., linear with respect to the number of attributes.

#index 1044467
#* Cost-based query optimization for complex pattern mining on multiple databases
#@ Ruoming Jin;Dave Fuhry;Abdulkareem Alali
#t 2008
#c 8
#% 70370
#% 214664
#% 248014
#% 248784
#% 248785
#% 248813
#% 273899
#% 280409
#% 300120
#% 420101
#% 420126
#% 481290
#% 481954
#% 577215
#% 577216
#% 631966
#% 729935
#% 729998
#% 731604
#% 737325
#% 745491
#% 823390
#% 842610
#% 864404
#% 1016244
#! For complex data mining queries, query optimization issues arise, similar to those for the traditional database queries. However, few works have applied the cost-based query optimization, which is the key technique in optimizing traditional database queries, on complex mining queries. In this work, we develop a cost-based query optimization framework to an important collection of data mining queries, i.e. frequent pattern mining across multiple databases. Specifically, we make the following contributions: 1) We present a rich class of queries on mining frequent itemsets across multiple datasets supported by a SQL-based mechanism. 2) We present an approach to enumerate all possible query plans for the mining queries, and develop a dynamic programming approach and a branch-and-bound approach based on the enumeration algorithm to find optimal query plans with the least mining cost. 3) We introduce models to estimate the cost of individual mining operators. 4) We evaluate our query optimization techniques on both real and synthetic datasets and show significant performance improvements.

#index 1044468
#* On-line discovery of hot motion paths
#@ Dimitris Sacharidis;Kostas Patroumpas;Manolis Terrovitis;Verena Kantere;Michalis Potamias;Kyriakos Mouratidis;Timos Sellis
#t 2008
#c 8
#% 280416
#% 442615
#% 462231
#% 654443
#% 654488
#% 659971
#% 769899
#% 769946
#% 810009
#% 810048
#% 810049
#% 824728
#% 878300
#% 879210
#% 881459
#% 885377
#% 889142
#% 893167
#% 960283
#% 1112737
#% 1409338
#% 1409360
#% 1720762
#! We consider an environment of numerous moving objects, equipped with location-sensing devices and capable of communicating with a central coordinator. In this setting, we investigate the problem of maintaining hot motion paths, i.e., routes frequently followed by multiple objects over the recent past. Motion paths approximate portions of objects' movement within a tolerance margin that depends on the uncertainty inherent in positional measurements. Discovery of hot motion paths is important to applications requiring classification/profiling based on monitored movement patterns, such as targeted advertising, resource allocation, etc. To achieve this goal, we delegate part of the path extraction process to objects, by assigning to them adaptive lightweight filters that dynamically suppress unnecessary location updates and, thus, help reducing the communication overhead. We demonstrate the benefits of our methods and their efficiency through extensive experiments on synthetic data sets.

#index 1044469
#* Schema polynomials and applications
#@ Kenneth A. Ross;Julia Stoyanovich
#t 2008
#c 8
#% 445
#% 262249
#% 267604
#% 278252
#% 393844
#% 400370
#% 544266
#% 564919
#% 571169
#% 893115
#% 1705034
#% 1839894
#! Conceptual complexity is emerging as a new bottleneck as data-base developers, application developers, and database administrators struggle to design and comprehend large, complex schemas. The simplicity and conciseness of a schema depends critically on the idioms available to express the schema. We propose a formal conceptual schema representation language that combines different design formalisms, and allows schema manipulation that exposes the strengths of each of these formalisms. We demonstrate how the schema factorization framework can be used to generate relational, object-oriented, and faceted physical schemas, allowing a wider exploration of physical schema alternatives than traditional methodologies. We illustrate the potential practical benefits of schema factorization by showing that simple heuristics can significantly reduce the size of a real-world schema description. We also propose the use of schema polynomials to model and derive alternative representations for complex relationships with constraints.

#index 1044470
#* Expressive query specification through form customization
#@ Magesh Jayapandian;H. V. Jagadish
#t 2008
#c 8
#% 46186
#% 55914
#% 115412
#% 172997
#% 232476
#% 296371
#% 345710
#% 397365
#% 570875
#% 620053
#% 643863
#% 650962
#% 728413
#% 808601
#% 814647
#% 824659
#% 864512
#% 884422
#% 990386
#! A form-based query interface is usually the preferred means to provide an unsophisticated user access to a database. Not only is such an interface easy to use, requiring no technical training, but it also requires little or no knowledge of how the data is structured in the database. However, a typical form is static and can express only a very limited set of queries, Without room for change, query specification is limited by the expertise and vision of the interface developer at the time the form was created. If an available form cannot express a desired query, the user is stuck. In this paper, we propose a mechanism to let a user modify an existing form to express the desired query. These modifications are themselves specified through filling forms to create an expression in an underlying form manipulation expression language we define. The technical sophistication required to modify forms is not much greater than form filling. We have developed a form editor that implements this form manipulation language. We have also developed a query generator that modifies the form's original query based on a user's changes. We show, by means of a controlled user study, that this tool provides an effective means for specifying complex queries.

#index 1044471
#* Synthesizing structured text from logical database subsets
#@ Alkis Simitsis;Georgia Koutrika;Yannis Alexandrakis;Yannis Ioannidis
#t 2008
#c 8
#% 250867
#% 309726
#% 443465
#% 449855
#% 474643
#% 535044
#% 654442
#% 659990
#% 824693
#% 824695
#% 864456
#% 927224
#% 1015325
#% 1021948
#% 1677912
#% 1698590
#% 1698603
#% 1703055
#! In the classical database world, information access has been based on a paradigm that involves structured, schema-aware, queries and tabular answers. In the current environment, however, where information prevails in most activities of society, serving people, applications, and devices in dramatically increasing numbers, this paradigm has proved to be very limited. On the query side, much work has been done on moving towards keyword queries over structured data. In our previous work, we have touched the other side as well, and have proposed a paradigm that generates entire databases in response to keyword queries. In this paper, we continue in the same direction and propose synthesizing textual answers in response to queries of any kind over structured data. In particular, we study the transformation of a dynamically-generated logical database subset into a narrative through a customizable, extensible, and templatebased process. In doing so, we exploit the structured nature of database schemas and describe three generic translation modules for different formations in the schema, called unary, split, and join modules. We have implemented the proposed translation procedure into our own database front end and have performed several experiments evaluating the textual answers generated as several features and parameters of the system are varied. We have also conducted a set of experiments measuring the effectiveness of such answers on users. The overall results are very encouraging and indicate the promise that our approach has for several applications.

#index 1044472
#* HISSCLU: a hierarchical density-based method for semi-supervised clustering
#@ Christian Böhm;Claudia Plant
#t 2008
#c 8
#% 36672
#% 273890
#% 464291
#% 464608
#% 722807
#% 732893
#% 769881
#% 770782
#% 770796
#% 785580
#% 832809
#% 875967
#% 905895
#% 1390149
#% 1673560
#! In situations where class labels are known for a part of the objects, a cluster analysis respecting this information, i.e. semi-supervised clustering, can give insight into the class and cluster structure of a data set. Several semi-supervised clustering algorithms such as HMRF-K-Means [4], COP-K-Means [26] and the CCL-algorithm [18] have recently been proposed. Most of them extend well-known clustering methods (K-Means [22], Complete Link [17] by enforcing two types of constraints: must-links between objects of the same class and cannot-links between objects of different classes. In this paper, we propose HISSCLU, a hierarchical, density-based method for semi-supervised clustering. Instead of deriving explicit constraints from the labeled objects, HISSCLU expands the clusters starting at all labeled objects simultaneously. During the expansion, class labels are assigned to the unlabeled objects most consistently with the cluster structure. Using this information the hierarchical cluster structure is determined. The result is visualized in a semi-supervised cluster diagram showing both cluster structure as well as class assignment. Compared to methods based on must-links and cannot-links, our method allows a better preservation of the actual cluster structure, particularly if the data set contains several distinct clusters of the same class (i.e. the intra-class data distribution is multimodal). HISSCLU has a determinate result, is efficient and robust against noise. The performance of our algorithm is shown in an extensive experimental evaluation on synthetic and real-world data sets.

#index 1044473
#* Processing transitive nearest-neighbor queries in multi-channel access environments
#@ Xiao Zhang;Wang-Chien Lee;Prasenjit Mitra;Baihua Zheng
#t 2008
#c 8
#% 153260
#% 169835
#% 201876
#% 261733
#% 286237
#% 287466
#% 427199
#% 443127
#% 462059
#% 632027
#% 774726
#% 839150
#% 870648
#% 1066744
#% 1720757
#! Wireless broadcast is an efficient way for information dissemination due to its good scalability [10]. Existing works typically assume mobile devices, such as cell phones and PDAs, can access only one channel at a time. In this paper, we consider a scenario of near future where a mobile device has the ability to process queries using information simultaneously received from multiple channels. We focus on the query processing of the transitive nearest neighbor (TNN) search [19]. Two TNN algorithms developed for a single broadcast channel environment are adapted to our new broadcast enviroment. Based on the obtained insights, we propose two new algorithms, namely Double-NN-Search and Hybrid-NN-Search algorithms. Further, we develop an optimization technique, called approximate-NN (ANN), to reduce the energy consumption in mobile devices. Finally, we conduct a comprehensive set of experiments to validate our proposals. The result shows that our new algorithms provide a better performance than the existing ones and the optimization technique efficiently reduces energy consumption.

#index 1044474
#* Multi-dimensional search for personal information management systems
#@ Christopher Peery;Wei Wang;Amélie Marian;Thu D. Nguyen
#t 2008
#c 8
#% 85445
#% 290703
#% 458861
#% 642993
#% 643566
#% 728195
#% 750867
#% 765408
#% 810108
#% 824681
#% 835189
#% 845350
#% 845359
#% 893119
#% 963490
#% 1016242
#% 1711105
#! With the explosion in the amount of semi-structured data users access and store in personal information management systems, there is a need for complex search tools to retrieve often very heterogeneous data in a simple and efficient way. Existing tools usually index text content, allowing for some IR-style ranking on the textual part of the query, but only consider structure (e.g., file directory) and metadata (e.g., date, file type) as filtering conditions. We propose a novel multi-dimensional approach to semi-structured data searches in personal information management systems by allowing users to provide fuzzy structure and metadata conditions in addition to keyword conditions. Our techniques provide a complex query interface that is more comprehensive than content-only searches as it considers three query dimensions (content, structure, metadata) in the search. We propose techniques to individually score each dimension, as well as a framework to integrate the three dimension scores into a meaningful unified score. Our work is integrated in Wayfinder, an existing fully-functioning file system. We perform a thorough experimental evaluation of our techniques to show the effect of approximating individual dimensions on the overall scores and ranks of files, as well as on query performance. Our experiments show that our scoring strategy adequately takes into account the approximation in each dimension to efficiently evaluate fuzzy multi-dimensional queries. In addition, fuzzy query conditions in non-content dimensions can significantly improve scoring (and thus ranking) accuracy.

#index 1044475
#* Ensuring correctness over untrusted private database
#@ Sarvjeet Singh;Sunil Prabhakar
#t 2008
#c 8
#% 342345
#% 513367
#% 657774
#% 659992
#% 745532
#% 751578
#% 772829
#% 1016171
#% 1016188
#% 1016189
#! In this paper we address the problem of ensuring the correctness of query results returned by an untrusted private database. The database owns the data and may modify it at any time. The querier is allowed to execute queries over this database; however it may not learn anything more than the result of these legal queries. The querier does not necessarily trust the database and would like the owner to furnish proof that the data has not been modified in response to recent events such as the submission of the query. We develop two metrics that capture the correctness of query answers and propose a range of solutions that provide a trade-off between the degree of exposure of private data, and the overhead of generation and verification of the proof. Our proposed solutions are tested on real data through implementation using PostgreSQL.

#index 1044476
#* Data exchange in the presence of arithmetic comparisons
#@ Foto Afrati;Chen Li;Vassia Pavlaki
#t 2008
#c 8
#% 583
#% 663
#% 36181
#% 123118
#% 198924
#% 213972
#% 213982
#% 224743
#% 248038
#% 264858
#% 287339
#% 287733
#% 289384
#% 296539
#% 366807
#% 378409
#% 465053
#% 480134
#% 563759
#% 599549
#% 801691
#% 806215
#% 809235
#% 809239
#% 809247
#% 823106
#% 826032
#% 874879
#% 874880
#% 874882
#% 940867
#% 993981
#% 1661429
#! Data exchange is the problem of transforming data structured under a schema (called source) into data structured under a different schema (called target). The emphasis of data exchange is to materialize a target instance (called solution) that satisfies the relationship between the schemas. Universal solutions were shown to be the most suitable solutions, mainly because they can be used to answer conjunctive queries posed over the target schema. Trying to extend this result to more expressive query languages fails, even if we only add inequalities (≠) to conjunctive queries. In this work we study data exchange in the presence of general arithmetic comparisons (, ≥, =, ≠): (a) We consider queries posed over the target schema that belong to the class of unions of conjunctive queries with arithmetic comparisons (in short CQACs). (b) We exploit arithmetic comparisons to define more expressive data exchange settings, called DEAC settings. In particular, DEAC settings consist of constraints that involve arithmetic comparisons. For that, two new classes of dependencies (tgd-ACs and acgds) are introduced, to capture the need of arithmetic comparisons in source-to-target and target constraints. We show that in DEAC settings the existence of solution problem is in NP. We define a novel chase procedure called AC-chase which is a tree and we prove that it produces a universal solution (appropriately defined to deal with arithmetic comparisons). We show that the new concept of universal solution is the right tool for query answering in the case of unions of CQACs. The complexity of computing certain answers for unions of CQACs is shown to be coNP-complete. Moreover, we identify polynomial cases for a) computing a universal solution and b) computing certain answers. For that, we introduce the succinct AC-chase which is a sequence instead of a tree, but its result is not necessarily a solution. We identify cases where succinct AC-chase returns indeed a universal solution and we investigate the syntactic conditions of the query under which query answering takes polynomial time. We show that the latter is feasible even in cases where the result of chase is not a universal solution.

#index 1044477
#* SPARQLing constraints for RDF
#@ Georg Lausen;Michael Meier;Michael Schmidt
#t 2008
#c 8
#% 20656
#% 32891
#% 69272
#% 398752
#% 459291
#% 514897
#% 956575
#% 1098418
#% 1279263
#% 1289168
#% 1289447
#! The goal of the Semantic Web is to support semantic interoperability between applications exchanging data on the web. The idea heavily relies on data being made available in machine readable format, using semantic markup languages. In this regard, the W3C has standardized RDF as the basic markup language for the Semantic Web. In contrast to relational databases, where data relationships are implicitly given by schema information as well as primary and foreign key constraints, relationships in semantic markup languages are made explicit. When mapping relational data into RDF, it is desirable to maintain the information implied by the origin constraints. As an improvement over existing approaches, our scheme allows for translating conventional databases into RDF without losing general constraints and vital key information. As much as in the relational model, those information are indispensable for data consistency and, as shown by example, can serve as a basis for semantic query optimization. We underline the practicability of our approach by showing that SPARQL, the most popular query language for RDF, can be used as a constraint language, akin to SQL in the relational context. As a theoretical contribution, we also discuss satisfiability for interesting classes of constraints and combinations thereof.

#index 1044478
#* Probabilistic ranked queries in uncertain databases
#@ Xiang Lian;Lei Chen
#t 2008
#c 8
#% 213975
#% 300180
#% 333854
#% 333951
#% 397378
#% 399762
#% 427199
#% 654487
#% 659937
#% 733373
#% 763882
#% 772835
#% 777931
#% 793254
#% 810018
#% 810049
#% 824728
#% 824764
#% 864394
#% 864396
#% 893108
#% 893126
#% 941785
#% 960242
#% 960243
#% 960244
#% 976788
#% 1016183
#% 1016191
#% 1016202
#% 1022203
#% 1022242
#% 1022243
#% 1022276
#% 1022278
#% 1022341
#% 1408794
#% 1669490
#! Recently, many new applications, such as sensor data monitoring and mobile device tracking, raise up the issue of uncertain data management. Compared to "certain" data, the data in the uncertain database are not exact points, which, instead, often locate within a region. In this paper, we study the ranked queries over uncertain data. In fact, ranked queries have been studied extensively in traditional database literature due to their popularity in many applications, such as decision making, recommendation raising, and data mining tasks. Many proposals have been made in order to improve the efficiency in answering ranked queries. However, the existing approaches are all based on the assumption that the underlying data are exact (or certain). Due to the intrinsic differences between uncertain and certain data, these methods are designed only for ranked queries in certain databases and cannot be applied to uncertain case directly. Motivated by this, we propose novel solutions to speed up the probabilistic ranked query (PRank) over the uncertain database. Specifically, we introduce two effective pruning methods, spatial and probabilistic, to help reduce the PRank search space. Then, we seamlessly integrate these pruning heuristics into the PRank query procedure. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed approach in answering PRank queries, in terms of both wall clock time and the number of candidates to be refined.

#index 1044479
#* The SBC-tree: an index for run-length compressed sequences
#@ Mohamed Y. Eltabakh;Wing-Kai Hon;Rahul Shah;Walid G. Aref;Jeffrey S. Vitter
#t 2008
#c 8
#% 23651
#% 50079
#% 115462
#% 124732
#% 128424
#% 143306
#% 235941
#% 271801
#% 273714
#% 279656
#% 282578
#% 287715
#% 288578
#% 289010
#% 303082
#% 317933
#% 326878
#% 341100
#% 431040
#% 443130
#% 453572
#% 464058
#% 480484
#% 493546
#% 544043
#% 571296
#% 593970
#% 656241
#% 736970
#% 736972
#% 745510
#% 749434
#% 749508
#% 762374
#% 824697
#% 1016132
#% 1672968
#! Run-Length-Encoding (RLE) is a data compression technique that is used in various applications, e.g., time series, biological sequences, and multimedia databases. One of the main challenges is how to operate on (e.g., index, search, and retrieve) compressed data without decompressing it. In this paper, we introduce the String B-tree for Compressed sequences, termed the SBC-tree, for indexing and searching RLE-compressed sequences of arbitrary length. The SBC-tree is a two-level index structure based on the well-known String B-tree and a 3-sided range query structure [7]. The SBC-tree supports pattern matching queries such as substring matching, prefix matching, and range search operations over RLE-compressed sequences. The SBC-tree has an optimal external-memory space complexity of O(N/B) pages, where N is the total length of the compressed sequences, and B is the disk page size. Substring matching, prefix matching, and range search execute in an optimal O(logB N + |p|+T/B) I/O operations, where |p| is the length of the compressed query pattern and T is the query output size. The SBC-tree is also dynamic and supports insert and delete operations efficiently. The insertion and deletion of all suffixes of a compressed sequence of length m take O(m logB(N + m)) amortized I/O operations. The SBC-tree index is realized inside PostgreSQL. Performance results illustrate that using the SBC-tree to index RLE-compressed sequences achieves up to an order of magnitude reduction in storage, while retains the optimal search performance achieved by the String B-tree over the uncompressed sequences.

#index 1044480
#* Efficient LCA based keyword search in XML data
#@ Yu Xu;Yannis Papakonstantinou
#t 2008
#c 8
#% 340914
#% 387508
#% 458829
#% 459260
#% 465155
#% 479803
#% 504581
#% 654442
#% 660011
#% 754116
#% 810052
#% 824693
#% 956599
#% 960261
#% 993987
#% 1015258
#% 1016135
#! Keyword search in XML documents based on the notion of lowest common ancestors (LCAs) and modifications of it has recently gained research interest [10, 14, 22]. In this paper we propose an efficient algorithm called Indexed Stack to find answers to keyword queries based on XRank's semantics to LCA [10]. The complexity of the Indexed Stack algorithm is O(kd|S1| log |S|) where k is the number of keywords in the query, d is the depth of the tree and |S1| (|S|) is the occurrence of the least (most) frequent keyword in the query. In comparison, the best worst case complexity of the core algorithms in [10] is O(kd|S|). We analytically and experimentally evaluate the Indexed Stack algorithm and the two core algorithms in [10]. The results show that the Indexed Stack algorithm outperforms in terms of both CPU and I/O costs other algorithms by orders of magnitude when the query contains at least one low frequency keyword along with high frequency keywords. This is important in practice since the frequencies of keywords typically vary significantly.

#index 1044481
#* Querying time-series streams
#@ Vivekanand Gopalkrishnan
#t 2008
#c 8
#% 281750
#% 294634
#% 295512
#% 300174
#% 342690
#% 378388
#% 397380
#% 479462
#% 481460
#% 571043
#% 576113
#% 587721
#% 654481
#% 679321
#% 729931
#% 731408
#% 731409
#% 765403
#% 765412
#% 772835
#% 814369
#% 814646
#% 824728
#% 844343
#% 871045
#% 1016195
#% 1016196
#% 1673562
#! Index trees created using distance based indexing are difficult to maintain online since the distance function involved is often costly to compute. This problem is intensified when the database we are dealing with, is frequently updated, as only limited time is available to perform the maintenance. In this paper, we propose a novel tree maintenance mechanism for the problem of answering approximate k-Nearest Neighbor queries with a probabilistic guarantee on timeseries streams. When the underlying data change, we may choose to defer updating the tree as long as the probabilistic guarantee of answering queries is high. To prolong such deferment, we present innovative techniques that maintain the utility of the tree by migrating its pivots and by partially reconstructing it. As the probabilistic guarantee decays with time and crosses the minimum guarantee threshold, all of the deferred updates are performed. In essence, our work offers an elegant compromise between the accuracy guarantee of query results and the cost of providing them. With extensive empirical studies, we also show the flexibility and efficiency of our approach.

#index 1044482
#* Optimizing on-demand data broadcast scheduling in pervasive environments
#@ Rinku Dewri;Indrakshi Ray;Indrajit Ray;Darrell Whitley
#t 2008
#c 8
#% 201897
#% 203665
#% 259634
#% 274200
#% 281441
#% 290747
#% 310650
#% 369236
#% 413449
#% 615031
#% 615514
#% 628099
#% 780533
#% 790597
#% 804339
#% 830348
#% 839472
#% 844271
#% 845246
#% 863357
#% 879305
#% 1026276
#% 1337346
#% 1670325
#! Data dissemination in pervasive environments is often accomplished by on-demand broadcasting. The time critical nature of the data requests plays an important role in scheduling these broadcasts. Most research in on-demand broadcast scheduling has focused on the timely servicing of requests so as to minimize the number of missed deadlines. However, there exists many pervasive environments where the utility of the data is an equally important criterion as its timeliness. Missing the deadline reduces the utility of the data but does not make it zero. In this work, we address the problem of scheduling on-demand data broadcasts with soft deadlines. We investigate search based optimization techniques to develop broadcast schedulers that make explicit attempts to maximize the utility of data requests as well as service as many requests as possible within the acceptable time limit. Our analysis shows that heuristic driven methods for such problems can be improved by hybridizing them with local search algorithms. We further investigate the option of employing a dynamic optimization technique to facilitate utility gain, thereby surpassing the requirement of a heuristic in the process. An evolution strategy based stochastic hill climber is investigated in this context.

#index 1044483
#* On the brink: searching for drops in sensor data
#@ Gong Chen;Junghoo Cho;Mark H. Hansen
#t 2008
#c 8
#% 172949
#% 260014
#% 333941
#% 461885
#% 466506
#% 617886
#% 729943
#% 729980
#% 757720
#% 765403
#% 893220
#% 1113090
#% 1720749
#! Sensor networks have been widely used to collect data about the environment. When analyzing data from these systems, people tend to ask exploratory questions---they want to find subsets of data, namely signal, reflecting some characteristics of the environment. In this paper, we study the problem of searching for drops in sensor data. Specifically, the search is to find periods in history when a certain amount of drop over a threshold occurs in data within a time span. We propose a framework, SegDiff, for extracting features, compressing them, and transforming the search into standard database queries. Approximate results are returned from the framework with the guarantee that no true events are missed and false positives are within a user specified tolerance. The framework efficiently utilizes space and provides fast response to users' search. Experimental results with real world data demonstrate the efficiency of our framework with respect to feature size and search time.

#index 1044484
#* A stratified approach to progressive approximate joins
#@ Wee Hyong Tok;Stéphane Bressan;Mong-Li Lee
#t 2008
#c 8
#% 1331
#% 273908
#% 333931
#% 378388
#% 428155
#% 479931
#% 480805
#% 654444
#% 745488
#% 789000
#% 810038
#% 982567
#% 993949
#% 993960
#% 1408770
#! Users often do not require a complete answer to their query but rather only a sample. They expect the sample to be either the largest possible or the most representative (or both) given the resources available. We call the query processing techniques that deliver such results 'approximate'. Processing of queries to streams of data is said to be 'progressive' when it can continuously produce results as data arrives. In this paper, we are interested in the progressive and approximate processing of queries to data streams when processing is limited to main memory. In particular, we study one of the main building blocks of such processing: the progressive approximate join. We devise and present several novel progressive approximate join algorithms. We empirically evaluate the performance of our algorithms and compare them with algorithms based on existing techniques. In particular we study the trade-off between maximization of throughput and maximization of representativeness of the sample.

#index 1044485
#* Continuous multi-way joins over distributed hash tables
#@ Stratos Idreos;Erietta Liarou;Manolis Koubarakis
#t 2008
#c 8
#% 116082
#% 300179
#% 330305
#% 340175
#% 397353
#% 443298
#% 636108
#% 726622
#% 734646
#% 765444
#% 765673
#% 839329
#% 864430
#% 878299
#% 884508
#% 884509
#% 993949
#% 1015281
#% 1015296
#% 1016167
#% 1850764
#! This paper studies the problem of evaluating continuous multi-way joins on top of Distributed Hash Tables (DHTs). We present a novel algorithm, called recursive join (RJoin), that takes into account various parameters crucial in a distributed setting i.e., network traffic, query processing load distribution, storage load distribution etc. The key idea of RJoin is incremental evaluation: as relevant tuples arrive continuously, a given multi-way join is rewritten continuously into a join with fewer join operators, and is assigned continuously to different nodes of the network. In this way, RJoin distributes the responsibility of evaluating a continuous multi-way join to many network nodes by assigning parts of the evaluation of each binary join to a different node depending on the values of the join attributes. The actual nodes to be involved are decided by RJoin dynamically after taking into account the rate of incoming tuples with values equal to the values of the joined attributes. RJoin also supports sliding window joins which is a crucial feature, especially for long join paths, since it provides a mechanism to reduce the query processing state and thus keep the cost of handling incoming tuples stable. In addition, RJoin is able to handle message delays due to heavy network traffic. We present a detailed mathematical and experimental analysis of RJoin and study the performance tradeoffs that occur.

#index 1044486
#* Ring-constrained join: deriving fair middleman locations from pointsets via a geometric constraint
#@ Man Lung Yiu;Panagiotis Karras;Nikos Mamoulis
#t 2008
#c 8
#% 86950
#% 152937
#% 201876
#% 248804
#% 287466
#% 300162
#% 300175
#% 427199
#% 462236
#% 730019
#% 757988
#% 806212
#% 814650
#% 824730
#% 976788
#% 1016192
#% 1206628
#% 1720751
#! We introduce a novel spatial join operator, the ring-constrained join (RCJ). Given two sets P and Q of spatial points, the result of RCJ consists of pairs (p, q) (where p ε P, q ε Q) satisfying an intuitive geometric constraint: the smallest circle enclosing p and q contains no other points in P, Q. This new operation has important applications in decision support, e.g., placing recycling stations at fair locations between restaurants and residential complexes. Clearly, RCJ is defined based on a geometric constraint but not on distances between points. Thus, our operation is fundamentally different from the conventional distance joins and closest pairs problems. We are not aware of efficient processing algorithms for RCJ in the literature. A brute-force solution requires computational cost quadratic to input size and it does not scale well for large datasets. In view of this, we develop efficient R-tree based algorithms for computing RCJ, by exploiting the characteristics of the geometric constraint. We evaluate experimentally the efficiency of our methods on synthetic and real spatial datasets. The results show that our proposed algorithms scale well with the data size and have robust performance across different data distributions.

#index 1044487
#* Why go logarithmic if we can go linear?: Towards effective distinct counting of search traffic
#@ Ahmed Metwally;Divyakant Agrawal;Amr El Abbadi
#t 2008
#c 8
#% 1331
#% 2833
#% 25208
#% 58348
#% 69273
#% 102786
#% 210182
#% 214073
#% 243166
#% 249238
#% 273916
#% 277347
#% 299989
#% 336610
#% 379443
#% 379445
#% 397388
#% 397443
#% 420053
#% 464385
#% 480805
#% 481749
#% 519953
#% 654446
#% 654463
#% 725364
#% 745506
#% 749451
#% 765414
#% 846209
#% 893120
#% 981649
#% 993959
#% 993996
#% 1016173
#% 1698257
#! Estimating the number of distinct elements in a large multiset has several applications, and hence has attracted active research in the past two decades. Several sampling and sketching algorithms have been proposed to accurately solve this problem. The goal of the literature has always been to estimate the number of distinct elements while using minimal resources. However, in some modern applications, the accuracy of the estimate is of primal importance, and businesses are willing to trade more resources for better accuracy. Throughout our experience with building a distinct count system at a major search engine, Ask.com, we reviewed the literature of approximating distinct counts, and compared most algorithms in the literature. We deduced that Linear Counting, one of the least used algorithms, has unique and impressive advantages when the accuracy of the distinct count is critical to the business. For other estimators to attain comparable accuracy, they need more space than Linear Counting. We have supported our analytical results through comprehensive experiments. The experimental results highly favor Linear Counting when the number of distinct elements is large and the error tolerance is low.

#index 1044488
#* Automatic content targeting on mobile phones
#@ Giovanni Giuffrida;Catarina Sismeiro;Giuseppe Tribulato
#t 2008
#c 8
#% 246831
#% 329562
#% 740191
#! The mobile phone industry has reached a saturation point. With low growth rates and fewer new customers available to acquire, competition among mobile operators is now focused on attracting competitors' customers. This leads to a significant downward price pressure, the inability by mobile phone providers in deriving reasonable returns from basic telephony services, and an increasing reliance on value added services (VAS) for revenue growth. There are today thousands of such services available for companies to sell to their customers daily. These services include, for example, the provision of sports information, ring-tones, personalized news, weather forecast, and financial trends. Because of the many possible offers, and of the limited contact opportunities (operators tend to cap the number of commercial messages sent to their users and phones have limited-size screens), data mining can play an important role in optimizing message targeting. In this paper we describe our experience in developing a successful automatic system to target users with the most relevant offers. We describe the proposed data mining methods and report on their performance. In addition, we discuss several experiments we implemented on live data. These experiments have been useful to tailor our approach to the specific characteristics of the market under study. We believe this is a very interesting domain for data miners though it is still fairly unexplored. This is despite the availability of very large and detailed logs of customer activity.

#index 1044489
#* BI batch manager: a system for managing batch workloads on enterprise data-warehouses
#@ Abhay Mehta;Chetan Gupta;Umeshwar Dayal
#t 2008
#c 8
#% 4683
#% 77990
#% 172910
#% 268750
#% 322315
#% 323271
#% 330760
#% 391013
#% 442968
#% 480775
#% 480949
#% 481127
#% 481459
#% 504988
#% 578044
#% 602688
#% 786929
#% 864447
#% 926082
#% 993933
#% 1013724
#% 1408968
#! Modern enterprise data warehouses have complex workloads that are notoriously difficult to manage. An important problem in workload management is to run these complex workloads 'optimally'. Traditionally this problem has been studied in the OLTP (Online Transaction Processing) context where MPL (Multi Programming Level) is used as a knob to achieve optimality. However, MPL is a tricky knob in a BI (Business Intelligence) scenario, since a low MPL can easily result in underload and a high MPL can easily result in overload and 'thrashing'. In this work we present BI Batch Manager, a workload management system to run batches of queries 'optimally' on an Enterprise Data Warehouse (EDW). It is comprised of three components: an admission control component, a scheduler and an execution control component. In order to automatically avoid underload and overload, we introduce a novel execution control mechanism, PGM (Priority Gradient Multiprogramming). In PGM, a priority gradient is created for the workload, with each query running at a distinctly different priority level. We demonstrate that this stabilizes the execution of a workload across a wide operating range. We use memory as the controlling factor for our admission control policy -- admitting batches of queries such that their memory requirement equals the available memory on the system. Our scheduling policy of largest memory query as the highest priority query further stabilizes the execution. We validate our BI Batch Manager using varying workloads on a commercial, enterprise class DBMS. We show that it effectively avoids underload and overload (thrashing) and can automatically run BI workloads with 'optimal' performance.

#index 1044490
#* Data challenges at Yahoo!
#@ Ricardo Baeza-Yates;Raghu Ramakrishnan
#t 2008
#c 8
#% 723279
#% 835045
#% 851305
#% 963669
#% 989578
#% 998845
#% 1021199
#! In this short paper we describe the data that Yahoo! handles, the current trends in Web applications, and the many challenges that this poses for Yahoo! Research. These challenges have led to the development of new data systems and novel data mining techniques.

#index 1044491
#* OrthoCluster: a new tool for mining synteny blocks and applications in comparative genomics
#@ Xinghuo Zeng;Matthew J. Nesbitt;Jian Pei;Ke Wang;Ismael A. Vergara;Nansheng Chen
#t 2008
#c 8
#% 832949
#% 1796826
#! By comparing genomes among both closely and distally related species, comparative genomics analysis characterizes structures and functions of different genomes in both conserved and divergent regions. Synteny blocks, which are conserved blocks of genes on chromosomes of related species, play important roles in comparative genomics analysis. Although a few tools have been designed to identify synteny blocks, most of them cannot handle some challenging application requirements, particularly the strandedness of genes, gene inversions, gene duplications, and comparison of more than two genomes. We developed a data mining tool, Ortho-Cluster, which can handle all those challenges. It is publicly available at http://genome.sfu.ca/projects/orthocluster. OrthoCluster takes the annotated gene sets of candidate genomes and pairwise orthologous relationships as input and efficiently identifies the complete set of synteny blocks. In addition, OrthoCluster identifies four types of genome rearrangement events namely inversion, transposition, insertion/deletion, and reciprocal translocation. To be fleexible in various application scenarios, OrthoCluster comes with a systematic set of parameters such as the synteny block size, number of mismatches allowed, whether the strandedness is enforced, whether gene ordering is preserved. Furthermore, OrthoCluster can be used to identify segmental duplication in a genome. In this paper, we introduce the major technical ideas, and present some interesting findings using OrthoCluster.

#index 1044492
#* Social ties and their relevance to churn in mobile telecom networks
#@ Koustuv Dasgupta;Rahul Singh;Balaji Viswanathan;Dipanjan Chakraborty;Sougata Mukherjea;Amit A. Nanavati;Anupam Joshi
#t 2008
#c 8
#% 220708
#% 268079
#% 282905
#% 342596
#% 577217
#% 577367
#% 729923
#% 753425
#% 754098
#% 853535
#% 881460
#% 881523
#% 907530
#% 961278
#% 978648
#% 1777282
#% 1860661
#! Social Network Analysis has emerged as a key paradigm in modern sociology, technology, and information sciences. The paradigm stems from the view that the attributes of an individual in a network are less important than their ties (relationships) with other individuals in the network. Exploring the nature and strength of these ties can help understand the structure and dynamics of social networks and explain real-world phenomena, ranging from organizational efficiency to the spread of information and disease. In this paper, we examine the communication patterns of millions of mobile phone users, allowing us to study the underlying social network in a large-scale communication network. Our primary goal is to address the role of social ties in the formation and growth of groups, or communities, in a mobile network. In particular, we study the 'evolution of churners in an operator's network spanning over a period of four months. Our analysis explores the propensity of a subscriber to churn out of a service provider's network depending on the number of ties (friends) that have already churned. Based on our findings, we propose a spreading activation-based technique that predicts potential churners by examining the current set of churners and their underlying social network. The efficiency of the prediction is expressed as a lift curve, which indicates the fraction of all churners that can be caught when a certain fraction of subscribers were contacted.

#index 1044493
#* Highly scalable trip grouping for large-scale collective transportation systems
#@ Gyozo Gidofalvi;Torben Bach Pedersen;Tore Risch;Erik Zeitler
#t 2008
#c 8
#% 321455
#% 427199
#% 480473
#% 599803
#% 800584
#% 818938
#% 824664
#% 824761
#% 824781
#% 864714
#% 907395
#% 981722
#% 1698975
#% 1698997
#! Transportation-related problems, like road congestion, parking, and pollution, are increasing in most cities. In order to reduce traffic, recent work has proposed methods for vehicle sharing, for example for sharing cabs by grouping "closeby" cab requests and thus minimizing transportation cost and utilizing cab space. However, the methods published so far do not scale to large data volumes, which is necessary to facilitate large-scale collective transportation systems, e.g., ride-sharing systems for large cities. This paper presents highly scalable trip grouping algorithms, which generalize previous techniques and support input rates that can be orders of magnitude larger. The following three contributions make the grouping algorithms scalable. First, the basic grouping algorithm is expressed as a continuous stream query in a data stream management system to allow for a very large flow of requests. Second, following the divide-and-conquer paradigm, four space-partitioning policies for dividing the input data stream into sub-streams are developed and implemented using continuous stream queries. Third, using the partitioning policies, parallel implementations of the grouping algorithm in a parallel computing environment are described. Extensive experimental results show that the parallel implementation using simple adaptive partitioning methods can achieve speed-ups of several orders of magnitude without significantly degrading the quality of the grouping.

#index 1044494
#* Data services in your spreadsheet!
#@ Régis Saint-Paul;Boualem Benatallah;Julien Vayssière
#t 2008
#c 8
#% 810073
#% 832195
#% 875028
#% 1561977
#! End-user programmers---the 45 million of them, as estimated for 2001 in US alone [7]---routinely use spreadsheet to visualize, manipulate, and analyze data. Thanks to this environment, they can build applications that solve their daily problems. Even building a report can be seen as programming an application that takes corporate data as input and outputs a presentation. To build this application, spreadsheet users have to import data and place them in spreadsheet cells, highlight the important pieces, compute maybe some aggregates, add a chart or two. If well done, this application will be used each time data are updated to effortlessly produce a fresh report.

#index 1044495
#* BeMatch: a platform for matchmaking service behavior models
#@ Juan Carlos Corrales;Daniela Grigori;Mokrane Bouzeghoub;Javier Ernesto Burbano
#t 2008
#c 8
#% 519428
#% 779737
#% 837400
#% 900784
#% 900886
#% 961558
#% 977147
#% 993982
#% 1016160
#% 1661809
#% 1715405
#! The capability to easily find useful services (software applications, software components, scientific computations) becomes increasingly critical in several fields. Current approaches for services retrieval are mostly limited to the matching of their inputs/outputs possibly enhanced with some ontological knowledge. Recent works have demonstrated that this approach is not sufficient to discover relevant components. Motivated by these concerns, we have developed BeMatch platform for ranking web services based on behavior matchmaking. We developed matching techniques that operate on behavior models and allow delivery of partial matches and evaluation of semantic distance between these matches and user requirements. Consequently, even if a service satisfying exactly the user requirements does not exist, the most similar ones will be retrieved and proposed for reuse by extension or modification. We exemplify our approach for behavioral services matchmaking by describing two demonstration scenarios for matchmaking BPEL and WSCL protocols, respectively. A demo scenario is also described concerning the tool for evaluating the effectiveness of the behavioral matchmaking method.

#index 1044496
#* The TELAR mobile mashup platform for Nokia internet tablets
#@ Andreas Brodt;Daniela Nicklas
#t 2008
#c 8
#% 893087
#% 894532
#% 955063
#% 967499
#! With the Web 2.0 trend and its participation of end-users more and more data and information services are online accessible, such as web sites, Wikis, or web services. The integration of this plethora of information is taken over by the community: so-called Mashups---web applications that combine data from more than one source into an integrated service---spring up like mushrooms, because they can be easily realized using script languages and web development platforms. Another trend is that mobile devices that get more and more powerful have ubiquitous access to the Web. Local sensors (such as GPS) can easily be connected to these devices. Thus, mobile applications can adapt to the current situation of the user, which can change frequently because of his or her mobility. In this demonstration, we present the Telar Mashup platform, a client-server solution that facilitates the creation of adaptive Mashups for mobile devices such as the Nokia Internet Tablets. On the server side, wrappers allow the integration of data from web-based services. On the client side, a simple implementation of the DCCI specification is used to integrate context information of local sensors into the mobile Web browser, which adapts the Mashup to the user's current location. We show an adaptive, mobile Mashup on the Nokia N810 Internet Tablet.

#index 1044497
#* MCSE: a multimedia context-based security engine
#@ Bechara Al Bouna;Richard Chbeir
#t 2008
#c 8
#% 270775
#% 317991
#% 331761
#% 342328
#% 443485
#% 465014
#% 583831
#% 784407
#% 814213
#% 863465
#% 1696317
#% 1706185
#% 1858126
#! In this paper, we describe our Multimedia Context based Security Engine (MCSE) which is a Java Based Prototype able to integrate multimedia context in order to enforce access control policies. The prototype provides supervised access to a database containing sensitive viral images.

#index 1044498
#* iDataGuard: middleware providing a secure network drive interface to untrusted internet data storage
#@ Ravi Chandra Jammalamadaka;Roberto Gamboni;Sharad Mehrotra;Kent E. Seamons;Nalini Venkatasubramanian
#t 2008
#c 8
#% 151495
#% 397367
#% 664705
#% 713866
#% 725292
#% 772846
#% 830692
#% 830694
#% 913895
#% 1015329
#% 1398143
#! In this demonstration, we present the design and features of iDataGuard. iDataGuard is an interoperable security middleware that allows users to outsource their file systems to heterogeneous data storage providers available on the Internet. Examples of data storage providers include Amazon S3 service, Rapidshare. de and Nivarnix. In the iDataGuard architecture, data storage providers are untrusted. Therefore, iDataGuard preserves data confidentiality and integrity of outsourced information by using cryptographic techniques. iDataGuard effectively builds a secure network drive on top of any data storage provider on the Internet. We propose techniques that realize a secure file system over the heterogeneous data models offered by the diverse storage providers. iDataGuard significantly reduces the development effort required to build applications on top of the storage offered by the IDPs. Applications written to be compatible with iDataGuard, do not have to worry where the data is stored and how the security is enforced. iDataGuard automatically provides such functionality to application developers. To evaluate the practicality of iDataGuard, we implemented a version of the middleware layer to test its performance.

#index 1044499
#* ACCOn: checking consistency of XML write-access control policies
#@ Loreto Bravo;James Cheney;Irini Fundulaki
#t 2008
#c 8
#% 273687
#% 333979
#% 344639
#% 378411
#% 379248
#% 755175
#% 765450
#% 808346
#% 903569
#% 978022
#% 1408531
#! XML access control policies involving updates may contain security flaws, here called inconsistencies, in which a forbidden operation may be simulated by performing a sequence of allowed operations. ACCOn implements i) consistency checking algorithms that examine whether a write-access control policy defined over a DTD is inconsistent and ii) repair algorithms that propose repairs to an inconsistent policy to obtain a consistent one.

#index 1044500
#* Flint: Google-basing the Web
#@ Lorenzo Blanco;Valter Crescenzi;Paolo Merialdo;Paolo Papotti
#t 2008
#c 8
#% 281251
#% 301241
#% 387427
#% 480824
#% 482516
#% 504443
#% 654469
#% 765411
#% 772300
#% 830520
#% 875066
#% 889107
#% 1077150
#% 1271981
#% 1275182
#! Several Web sites deliver a large number of pages, each publishing data about one instance of some real world entity, such as an athlete, a stock quote, a book. Even though it is easy for a human reader to recognize these instances, current search engines are unaware of them. Technologies for the Semantic Web aim at achieving this goal; however, so far they have been of little help in this respect, as semantic publishing is very limited. We have developed a system, called Flint, for automatically searching, collecting and indexing Web pages that publish data representing an instance of a certain conceptual entity. Flint takes as input a small set of labeled sample pages: it automatically infers a description of the underlying conceptual entity and then searches the Web for other pages containing data representing the same entity. Flint automatically extracts data from the collected pages and stores them into a semi-structured self-describing database, such as Google Base. Also, the collected pages can be used to populate a custom search engine; to this end we rely on the facilities provided by Google Co-op.

#index 1044501
#* Mine your own business, mine others' news!
#@ Quang-Khai Pham;Regis Saint-Paul;Boualem Benatallah;Noureddine Mouaddib;Guillaume Raschia
#t 2008
#c 8
#% 232147
#% 333954
#% 463903
#% 464996
#% 480124
#% 745494
#% 824712
#! Major media companies such as The Financial Times, the Wall Street Journal or Reuters generate huge amounts of textual news data on a daily basis. Mining frequent patterns in this mass of information is critical for knowledge workers such as financial analysts, stock traders or economists. Using existing frequent pattern mining (FPM) algorithms for the analysis of news data is difficult because of the size and lack of structuring of the free text news content. In this article, we demonstrate a comprehensive Streaming TEmporAl Data (STEAD) analysis framework for mining frequent patterns in financial news. In this demonstration, we show how the mining task is supported by the use of a Time-Aware Content Summarization algorithm (TACS). This summary generates a concise representation of large volume of data by taking into account the expert's peculiar interest while preserving the news arrival temporal information which is essential for FPM algorithms. We experimented the whole framework on a set of news data from Reuters.

#index 1044502
#* BioScout: a life-science query monitoring system
#@ Anastasios Kementsietsidis;Frank Neven;Dieter Van de Craen
#t 2008
#c 8
#% 992
#% 36117
#% 44054
#% 286916
#% 286991
#% 289282
#% 300166
#% 300179
#% 330305
#% 443065
#% 565473
#% 654468
#% 731408
#% 765126
#% 772131
#% 1675421
#% 1688291
#% 1688292
#% 1692837
#% 1692850
#! Scientific data are available through an increasing number of heterogeneous, independently evolving, sources. Although the sources themselves are independently evolving, the data stored in them are not. There exist inherent and intricate relationships between the distributed data-sets and scientists are routinely required to write distributed queries in this setting. Being non-experts in computer science, the scientists are faced with two major challenges: (i) How to express such distributed queries. This is a non-trivial task, even if we assume that scientists are familiar with query languages like SQL. Such queries can get arbitrarily complex as more sources are considered; (ii) How to efficiently evaluate such distributed queries. An efficient evaluation must account for batches of hundreds (or even thousands) of submitted queries and must optimize all of them as a whole. In this demo, we focus on the biological domain for illustration purposes (our solutions are applicable to other scientific domains) and we present a system, called BioScout, that offers solutions in both of the above challenges. In more detail, we demonstrate the following functionality: (i) in BioScout, scientists draw their queries graphically, resulting in a query graph. The scientist is unaware of the query language used or of any optimization issues. Given the query graph, the system is able to generate, as a first step, an optimal query plan for the submitted query; (ii) BioScout uses four different strategies to combine the optimal query plans of individual queries to generate a global query plan for all the submitted queries. In the demo, we illustrate graphically how each of the four strategies works.

#index 1044503
#* BIBEX: a bibliographic exploration tool based on the DEX graph query engine
#@ Sergio Gómez-Villamor;Gerard Soldevila-Miranda;Aleix Giménez-Vañó;Norbert Martínez-Bazan;Victor Muntés-Mulero;Josep-L. Larriba-Pey
#t 2008
#c 8
#% 151432
#% 154330
#% 184258
#% 442858
#% 481434
#% 660011
#% 864456
#% 1019118
#% 1702418
#! In this demonstration we show the Bibliographic Exploration tool BIBEX. BIBEX is based on the graph database query engine DEX and integrates both the Citeseer and DBLP databases. BIBEX can be found in our web site at www.dama.upc.edu/bibex. BIBEX allows for complex bibliographic search and shows the results of its queries as a combination of graphs and text for two types of scenarios: scientists who want to do complex bibliographic search, and PC members and journal editors who want to have an aid to search best suited reviewers that do not have conflicts of interest with the authors of papers submitted for review. The tool allows to explore the relation between authors, keywords and papers in such a way that a user may perform a complex bibliographic search to investigate the who is who in specific areas of research.

#index 1044504
#* An inductive database and query language in the relational model
#@ Lothar Richter;Jörg Wicker;Kristina Kessler;Stefan Kramer
#t 2008
#c 8
#% 2008
#% 342604
#% 347711
#% 420076
#% 420101
#% 449508
#% 787823
#% 1742007
#! In the demonstration, we will present the concepts and an implementation of an inductive database -- as proposed by Imielinski and Mannila -- in the relational model. The goal is to support all steps of the knowledge discovery process, from pre-processing via data mining to post-processing, on the basis of queries to a database system. The query language SIQL (structured inductive query language), an SQL extension, offers query primitives for feature selection, discretization, pattern mining, clustering, instance-based learning and rule induction. A prototype system processing such queries was implemented as part of the SINDBAD (structured inductive database development) project. Key concepts of this system, among others, are the closure of operators and distances between objects. To support the analysis of multi-relational data, we incorporated multi-relational distance measures based on set distances and recursive descent. The inclusion of rule-based classification models made it necessary to extend the data model and the software architecture significantly. The prototype is applied to three different applications: gene expression analysis, gene regulation prediction and structure-activity relationships (SARs) of small molecules.

#index 1044505
#* Streaming in a connected world: querying and tracking distributed data streams
#@ Graham Cormode;Minos Garofalakis
#t 2008
#c 8
#% 1740384
#! Today, a majority of data is fundamentally distributed in nature. Data for almost any task is collected over a broad area, and streams in at a much greater rate than ever before. In particular, advances in sensor technology and miniaturization have led to the concept of the sensor network: a (typically wireless) collection of sensing devices collecting detailed data about their surroundings. A fundamental question arises: how to query and monitor this rich new source of data? Similar scenarios emerge within the context of monitoring more traditional, wired networks, and in other emerging models such as P2P networks and grid-based computing.

#index 1044506
#* Virtualization and databases: state of the art and research challenges
#@ Ashraf Aboulnaga;Cristiana Amza;Kenneth Salem
#t 2008
#c 8
#! There is currently a lot of interest in resource virtualization as an important technique for addressing the problems of manageability, reliability, and security in computer systems. Resource virtualization decouples the user's perception of hardware and software resources from the actual implementation of these resources. It adds a flexible and programmable layer of software between user applications (such as database systems) and the resources that they use. This layer of software maps the virtual resources perceived by the applications to real physical resources. An example of this layer of software is a virtual machine monitor, which partitions the resources of a machine (CPU, disk, memory, network, etc.) into multiple virtual machines, and independent operating systems and applications can be installed on each virtual machine. The power of resource virtualization comes from the ability to manage the mapping from virtual resources to physical resources in the virtualization layer, and to change it as needed. The trend towards virtualization is of interest to us in the database research community because database systems are increasingly being run in virtualized environments. This presents a major opportunity since virtualization can help in solving many important problems in the areas of database system usability, manageability, deployment, scalability, and availability. Leveraging the capabilities of virtualization to solve these problems will require some effort on the part of our community. At the same time, virtualization poses some unique research challenges that must be addressed to enable database systems to run efficiently in these virtualized environments that are becoming increasingly common. In this tutorial, we will introduce resource virtualization and how it affects database systems. We will present the opportunities that resource virtualization provides for database systems and the unique research challenges that it poses, and we will review ongoing research in this area.

#index 1044507
#* Quality of service and predictability in DBMS
#@ Kai-Uwe Sattler;Wolfgang Lehner
#t 2008
#c 8
#! DBMS are a ubiquitous building block of the software stack in many complex applications. Middleware technologies, application servers and mapping approaches hide the core database technologies just like power, networking infrastructure and operating system services. Furthermore, many enterprise-critical applications demand a certain degree of quality of service (QoS) or guarantees, e.g. wrt. response time, transaction throughput, latency but also completeness or more generally quality of results. Examples of such applications are billing systems in telecommunication, where each telephone call has to be monitored and registered in a database, Ecommerce applications where orders have to be accepted even in times of heavy load and the waiting time of customers should not exceed a few seconds, ERP systems processing a large number of transactions in parallel, or systems for processing streaming or sensor data in realtime, e.g. in process automation of traffic control. As part of complex multilevel software stack, database systems have to share or contribute to these QoS requirements, which means that guarantees have to be given by the DBMS, too, and that the processing of database requests is predictable. Todays mainstream DBMS typically follow a best effort approach: requests are processed as fast as possible without any guarantees: the optimization goal of query optimizers and tuning approaches is rather to minimize resource consumption instead of just fulfilling given service level agreements. However, motivated by the situation described above there is an emerging need for database services providing guarantees or simply behave in a predictable manner and at the same time interact with other components of the software stack in order to fulfill the requirements. This is also driven by the paradigm of service-oriented architectures widely discussed in industry. Currently, this is addressed only by very specialized solutions. Nevertheless, database researchers have developed several techniques contributing to the goal of QoS-aware database systems. The purpose of the tutorial is to introduce database researchers and practitioners to the scope, the challenges and the available techniques to the problem of predictability and QoS agreements in DBMS.

#index 1065540
#* Proceedings of the 2nd international workshop on Scalable stream processing system
#@ Byung S. Lee
#t 2008
#c 8
#! This Second Workshop on Scalable Stream Processing System (SSPS) continued the success of the First Workshop on SSPS. The focus of the workshop is on the scalability issues of a stream processing system challenged by ever increasing load and stringent requirement on the system. Being co-located with EDBT '08 was an ideal setting for the workshop, considering the reputation of EDBT as a top conference on database technology.

#index 1065551
#* Proceedings of the 2008 international workshop on Privacy and anonymity in information society
#@ Farshad Fotouhi;Li Xiong;Traian Marius Truta
#t 2008
#c 8
#! It is our great pleasure to welcome you to the First International Workshop on Privacy and Anonymity in the Information Society (PAIS 2008), collocated with 11th International Conference on Extending Database Technology (EDBT 2008). While the ever increasing computational power together with the huge amount of individual data collected daily by various agencies is of great value for our society, they also pose a significant threat to individuals' privacy. As a result legislators for many countries try to regulate the use and the disclosure of confidential information. Data privacy and anonymity have become a mainstream avenue for research. While privacy is a topic discussed everywhere, data anonymity recently established itself as an emerging area of computer science. Its goal is to produce useful computational solutions for releasing data, while providing scientific guarantees that the identities and other sensitive information of the individuals who are the subjects of the data are protected. The PAIS 2008 workshop is the first in its series and its mission is to provide an open yet focused platform for researchers and practitioners from computer science and other fields that are interacting with computer science in the privacy area such as statistics, healthcare informatics, and law to discuss and present current research challenges and advances in data privacy and anonymity research. The workshop program features 8 papers that cover a variety of topics, including distributed privacy protection, query auditing, k-anonymization and its applications, and micro-aggregation based data anonymization. In addition, the program includes an invited speech by Dr. Josep Domingo-Ferrer, the UNESCO Chair in Data Privacy at Rovira i Virgili University of Tarragona, Catalonia. The title of his talk is: "Location Privacy via Unlinkability: An Alternative to Cloaking and Perturbation". We hope that you find this program interesting and thought-provoking.

#index 1065606
#* Proceedings of the 2008 international workshop on Data management in peer-to-peer systems
#@ Anne Doucet;Stéphane Gançarski;Esther Pacitti
#t 2008
#c 8
#! This volume contains papers selected for presentation at the International Workshop on Data Management in Peer-to-Peer Systems (DAMAP 2008), held on March 25th 2008, in Nantes (France), in conjunction with the 11th International Conference on Extending Database Technology (EDBT 08). The purpose of this first edition of the Damap workshop is to bring together researchers who are involved in designing, managing and implementing peer-to-peer systems for large scale data management applications. The goal is to provide a forum for the exchange of ideas, knowledge and experiences in this domain, and to debate new issues and directions for research and development work in the future. After thorough review process for each of the 14 submissions, we have selected 9 papers. The program also included a keynote speech by Wolfgang Neijdl, researcher at the University of Hannover, Germany.

#index 1070435
#* Proceedings of the 2008 EDBT workshop on Software engineering for tailor-made data management
#@ Sven Apel;Marko Rosenmüller;Gunter Saake;Olaf Spinczyk
#t 2008
#c 8
#! This first edition of the Software Engineering for Tailor-made Data Management (SETMDM) workshop was held in Nantes, France, March 29, 2008 in conjunction with 11th International Conference on Extending Database Technology (EDBT 2008). The goal of this workshop was to bring software engineers and database experts together to discuss about technologies for developing tailor-made data management solutions. The workshop was a full day meeting with four paper sessions. It was introduced by a presentation of the FAME-DBMS project that aims at developing tailor-made data management for embedded systems. Another motivating presentation about a tailor-made database system needed to implement advanced file management functionality concluded the first session. In the second session two papers about tailoring transaction processing and lock protocols were presented. The presented solutions allow a specialization of database technologies for a certain application scenario like XML databases. The first presentation of the next session highlighted the issue of the continuously increasing power consumption in current large computing centers. It concluded that also software systems like DBMS should help to decrease power consumption. Using specialized algorithms to tailor data-management can help to improve the energy efficiency. The next paper focused on the complexity of SQL and presented an approach of decomposing SQL-2003 to obtain any required tailor-made subset of the grammar as a basis for a tailor-made SQL parser. The last two papers concentrated on configurability of parts of database management systems by introducing service-oriented technologies and runtime adaptation. Long discussions about services in databases raised in this session. The workshop concluded with an interesting discussion about the future of database system and the well known statement 'one size fits all'. Pros and cons of this topic were lively discussed.

#index 1071775
#* Proceedings of the 2008 EDBT Ph.D. workshop
#@ Sascha Müller;Wolfgang Lindner;Guillaume Raschia
#t 2008
#c 8
#! This volume contains the post workshop proceedings of the EDBT 2008 Ph.D. Workshop. The workshop was held in conjunction with the 11th International Conference on Extending Database Technology (EDBT) on March 25, 2008, in Nantes, France. Continuing in its tradition, the Ph.D. Workshop brings together Ph.D. students in the field of database technology outside of the EDBT conference series. It offers Ph.D. students the opportunity to present, discuss, and receive feedback on their research in a constructive and international environment. One of this year's EDBT 2008 Ph.D. Workshop's highlights was the joint keynote address of Sophie Cluet and Serge Abiteboul. Their brilliant and revealing talk titled "On (beautiful) lives in academia and after" was quite fascinating for all participants of the workshop. We are very glad that these two extraordinary researchers allowed us to benefit from their comprehensive knowledge and opened our minds for both, conventional and unconventional careers paths.

#index 1095659
#* Proceedings of the 2008 EDBT workshop on Database technologies for handling XML information on the web
#@ 
#t 2008
#c 8
#! This volume contains papers selected for presentation at the Third International Workshop on Database Technologies for Handling XML Information on the Web (DataX'08), held in conjunction with the International Conference on Extending Database Technology (EDBT'08) in Nantes (France) on March 25, 2008. In response to the call for papers, 21 high quality submissions were received. Each paper was carefully reviewed by at least three members of the program committee and external reviewers. As result of this process, 6 papers have been selected as long papers and 4 as short papers for presentation at the workshop. The accepted papers cover a large variety of topics, ranging from query optimization, semantic interoperability and data sharing to stream processing and retrieval on P2P systems. In addition, Prof. Yannis Papakonstantinou accepted our invitation to discuss new and interesting topics in XML and database research. Among the accepted papers for the workshop, we have further selected the best papers that are included in this post-workshop proceedings (5 long papers and 3 short papers).

#index 1181213
#* Data integration flows for business intelligence
#@ Umeshwar Dayal;Malu Castellanos;Alkis Simitsis;Kevin Wilkinson
#t 2009
#c 8
#% 36117
#% 269079
#% 278109
#% 300166
#% 333848
#% 366617
#% 413119
#% 438505
#% 531459
#% 800563
#% 810078
#% 893089
#% 907417
#% 963669
#% 1016606
#% 1041782
#% 1046187
#% 1088931
#% 1149562
#% 1181265
#% 1206731
#% 1406019
#% 1415518
#% 1688313
#% 1704074
#% 1720926
#! Business Intelligence (BI) refers to technologies, tools, and practices for collecting, integrating, analyzing, and presenting large volumes of information to enable better decision making. Today's BI architecture typically consists of a data warehouse (or one or more data marts), which consolidates data from several operational databases, and serves a variety of front-end querying, reporting, and analytic tools. The back-end of the architecture is a data integration pipeline for populating the data warehouse by extracting data from distributed and usually heterogeneous operational sources; cleansing, integrating and transforming the data; and loading it into the data warehouse. Since BI systems have been used primarily for off-line, strategic decision making, the traditional data integration pipeline is a oneway, batch process, usually implemented by extract-transform-load (ETL) tools. The design and implementation of the ETL pipeline is largely a labor-intensive activity, and typically consumes a large fraction of the effort in data warehousing projects. Increasingly, as enterprises become more automated, data-driven, and real-time, the BI architecture is evolving to support operational decision making. This imposes additional requirements and tradeoffs, resulting in even more complexity in the design of data integration flows. These include reducing the latency so that near real-time data can be delivered to the data warehouse, extracting information from a wider variety of data sources, extending the rigidly serial ETL pipeline to more general data flows, and considering alternative physical implementations. We describe the requirements for data integration flows in this next generation of operational BI system, the limitations of current technologies, the research challenges in meeting these requirements, and a framework for addressing these challenges. The goal is to facilitate the design and implementation of optimal flows to meet business requirements.

#index 1181214
#* Optimized union of non-disjoint distributed data sets
#@ Itay Dar;Tova Milo;Elad Verbin
#t 2009
#c 8
#% 122671
#% 249238
#% 286916
#% 299984
#% 322884
#% 330305
#% 340175
#% 446419
#% 479937
#% 481935
#% 496147
#% 600560
#% 762652
#% 800582
#% 812773
#% 824704
#% 838232
#% 874972
#% 960274
#% 960296
#% 989512
#% 1015308
#% 1022215
#% 1206796
#% 1861495
#! In a variety of applications, ranging from data integration to distributed query evaluation, there is a need to obtain sets of data items from several sources (peers) and compute their union. As these sets often contain common data items, avoiding the transmission of redundant information is essential for effective union computation. In this paper we define the notion of optimal union plans for non-disjoint data sets residing on distinct peers, and present efficient algorithms for computing and executing such optimal plans. Our algorithms avoid redundant data transmission and optimally exploit the network bandwidth capabilities. A challenge in the design of optimal plans is the lack of a complete map of the distribution of the data items among peers. We analyze the information required for optimal planning and propose novel techniques to obtain compact, cheap to communicate, description of the data sources. We then exploit it for efficient union computation with reasonable accuracy. We demonstrate experimentally the superiority of our approach over the common naive union computation, showing it improves the performance by an order of magnitude.

#index 1181215
#* Shore-MT: a scalable storage manager for the multicore era
#@ Ryan Johnson;Ippokratis Pandis;Nikos Hardavellas;Anastasia Ailamaki;Babak Falsafi
#t 2009
#c 8
#% 13043
#% 91631
#% 115661
#% 116087
#% 172939
#% 239979
#% 262154
#% 286929
#% 287230
#% 303036
#% 403195
#% 442700
#% 471289
#% 479480
#% 480589
#% 504541
#% 808534
#% 813071
#% 832041
#% 993385
#% 998845
#% 1022298
#% 1129955
#% 1671820
#! Database storage managers have long been able to efficiently handle multiple concurrent requests. Until recently, however, a computer contained only a few single-core CPUs, and therefore only a few transactions could simultaneously access the storage manager's internal structures. This allowed storage managers to use non-scalable approaches without any penalty. With the arrival of multicore chips, however, this situation is rapidly changing. More and more threads can run in parallel, stressing the internal scalability of the storage manager. Systems optimized for high performance at a limited number of cores are not assured similarly high performance at a higher core count, because unanticipated scalability obstacles arise. We benchmark four popular open-source storage managers (Shore, BerkeleyDB, MySQL, and PostgreSQL) on a modern multicore machine, and find that they all suffer in terms of scalability. We briefly examine the bottlenecks in the various storage engines. We then present Shore-MT, a multithreaded and highly scalable version of Shore which we developed by identifying and successively removing internal bottlenecks. When compared to other DBMS, Shore-MT exhibits superior scalability and 2--4 times higher absolute throughput than its peers. We also show that designers should favor scalability to single-thread performance, and highlight important principles for writing scalable storage engines, illustrated with real examples from the development of Shore-MT.

#index 1181216
#* Workload-aware data partitioning in community-driven data grids
#@ Tobias Scholl;Bernhard Bauer;Jessica Müller;Benjamin Gufler;Angelika Reiser;Alfons Kemper
#t 2009
#c 8
#% 43005
#% 68091
#% 115661
#% 340175
#% 385620
#% 397398
#% 415957
#% 505869
#% 822526
#% 831859
#% 867051
#% 902316
#% 907518
#% 914605
#% 960252
#% 982557
#% 1016166
#% 1022334
#% 1028502
#% 1133491
#% 1206783
#% 1688254
#! Collaborative research in various scientific disciplines requires support for scalable data management enabling the efficient correlation of globally distributed data sources. Motivated by the expected data rates of upcoming projects and a growing number of users, communities explore new data management techniques for achieving high throughput. Community-driven data grids deliver such high-throughput data distribution for scientific federations by partitioning data according to application-specific data and query characteristics. Query hot spots are an important and challenging problem in this environment. Existing approaches to load-balancing from Peer-to-Peer (P2P) data management and sensor networks do not directly meet the requirements of a data-intensive e-science environment. In this paper, our contributions are partitioning schemes based on multi-dimensional index structures enabling communities to trade off data load balancing and handling query hot spots via splitting and replication. We evaluate the partitioning schemes with two typical kinds of data sets from the astrophysics domain and workloads extracted from Sloan Digital Sky Survey (SDSS) query traces and perform throughput measurements in real and simulated networks. The experiments demonstrate the improved workload distribution capabilities and give promising directions for the development of future community grids.

#index 1181217
#* Sequenced spatio-temporal aggregation in road networks
#@ Igor Timko;Michael H. Böhlen;Johann Gamper
#t 2009
#c 8
#% 361445
#% 421124
#% 427199
#% 458858
#% 458865
#% 464847
#% 465010
#% 481928
#% 495249
#% 527189
#% 565462
#% 571296
#% 578404
#% 660007
#% 729850
#% 745458
#% 745486
#% 789017
#% 878301
#% 1015340
#% 1046421
#% 1054486
#% 1688261
#! Many applications of spatio-temporal databases require support for sequenced spatio-temporal (SST) aggregation, e. g., when analyzing traffic density in a city. Conceptually, an SST aggregation produces one aggregate value for each point in time and space. This paper is the first to propose a method to efficiently evaluate SST aggregation queries for the COUNT, SUM, and AVG aggregation functions. Based on a discrete time model and a discrete, 1.5 dimensional space model that represents a road network, we generalize the concept of (temporal) constant intervals towards constant rectangles that represent maximal rectangles in the space-time domain over which the aggregation result is constant. We propose a new data structure, termed SST-tree, which extends the Balanced Tree for one-dimensional temporal aggregation towards the support for two-dimensional, spatio-temporal aggregation. The main feature of the Balanced Tree to store constant intervals in a compact way by using two counters is extended towards a compact representation of constant rectangles in the space-time domain. We propose and evaluate two variants of the SST-tree. The SSTT-tree and SSTH-tree use trees and hashmaps to manage spacestamps, respectively. Our experiments show that both solutions outperform a brute force approach in terms of memory and time. The SSTH-tree is more efficient in terms of memory, whereas the SSTT-tree is more efficient in terms of time.

#index 1181218
#* Processing probabilistic spatio-temporal range queries over moving objects with uncertainty
#@ Bruce S. E. Chung;Wang-Chien Lee;Arbee L. P. Chen
#t 2009
#c 8
#% 211503
#% 237205
#% 252304
#% 299936
#% 300174
#% 427199
#% 461923
#% 480423
#% 654487
#% 765452
#% 772835
#% 803125
#% 824728
#% 1015320
#% 1016202
#! Range queries for querying the current and future positions of the moving objects have received growing interests in the research community. Existing methods, however, assume that an object only moves along an anticipated path. In this paper, we study the problem of answering probabilistic range queries on moving objects based on an uncertainty model, which captures the possible movements of objects with probabilities. Evaluation of probabilistic queries is challenging due to large objects volume and costly computation. We map the uncertain movements of all objects to a dual space for indexing. By querying the index, we quickly eliminate unqualified objects and employ an approximate approach to examine the remaining candidates for final answer. We conduct a comprehensive performance study, which shows our proposal significantly reduces the number of object examinations and the overall cost of the query evaluation.

#index 1181219
#* Anonymizing moving objects: how to hide a MOB in a crowd?
#@ Roman Yarovoy;Francesco Bonchi;Laks V. S. Lakshmanan;Wendy Hui Wang
#t 2009
#c 8
#% 103741
#% 201876
#% 248030
#% 300174
#% 333854
#% 643566
#% 800515
#% 800571
#% 801690
#% 864406
#% 864412
#% 864662
#% 893151
#% 911803
#% 1022243
#% 1080161
#% 1206713
#% 1676510
#% 1700134
#! Moving object databases (MOD) have gained much interest in recent years due to the advances in mobile communications and positioning technologies. Study of MOD can reveal useful information (e.g., traffic patterns and congestion trends) that can be used in applications for the common benefit. In order to mine and/or analyze the data, MOD must be published, which can pose a threat to the location privacy of a user. Indeed, based on prior knowledge of a user's location at several time points, an attacker can potentially associate that user to a specific moving object (MOB) in the published database and learn her position information at other time points. In this paper, we study the problem of privacy-preserving publishing of moving object database. Unlike in microdata, we argue that in MOD, there does not exist a fixed set of quasi-identifier (QID) attributes for all the MOBs. Consequently the anonymization groups of MOBs (i.e., the sets of other MOBs within which to hide) may not be disjoint. Thus, there may exist MOBs that can be identified explicitly by combining different anonymization groups. We illustrate the pitfalls of simple adaptations of classical k-anonymity and develop a notion which we prove is robust against privacy attacks. We propose two approaches, namely extreme-union and symmetric anonymization, to build anonymization groups that provably satisfy our proposed k-anonymity requirement, as well as yield low information loss. We ran an extensive set of experiments on large real-world and synthetic datasets of vehicular traffic. Our results demonstrate the effectiveness of our approach.

#index 1181220
#* Type-based categorization of relational attributes
#@ Babak Ahmadi;Marios Hadjieleftheriou;Thomas Seidl;Divesh Srivastava;Suresh Venkatasubramanian
#t 2009
#c 8
#% 243166
#% 262059
#% 309128
#% 374537
#% 387427
#% 397369
#% 420072
#% 443393
#% 464837
#% 479973
#% 571297
#% 616528
#% 830529
#% 889294
#% 915256
#% 1206637
#! In this work we concentrate on categorization of relational attributes based on their data type. Assuming that attribute type/characteristics are unknown or unidentifiable, we analyze and compare a variety of type-based signatures for classifying the attributes based on the semantic type of the data contained therein (e.g., router identifiers, social security numbers, email addresses). The signatures can subsequently be used for other applications as well, like clustering and index optimization/compression. This application is useful in cases where very large data collections that are generated in a distributed, ungoverned fashion end up having unknown, incomplete, inconsistent or very complex schemata and schema level meta-data. We concentrate on heuristically generating type-based attribute signatures based on both local and global computation approaches. We show experimentally that by decomposing data into q-grams and then considering signatures based on q-gram distributions, we achieve very good classification accuracy under the assumption that a large sample of the data is available for building the signatures. Then, we turn our attention to cases where a very small sample of the data is available, and hence accurately capturing the q-gram distribution of a given data type is almost impossible. We propose techniques based on dimensionality reduction and soft-clustering that exploit correlations between attributes to improve classification accuracy.

#index 1181221
#* AlphaSum: size-constrained table summarization using value lattices
#@ K. Selçuk Candan;Huiping Cao;Yan Qi;Maria Luisa Sapino
#t 2009
#c 8
#% 213981
#% 223781
#% 302725
#% 410276
#% 443463
#% 463895
#% 503400
#% 577239
#% 617864
#% 622489
#% 635924
#% 638521
#% 643566
#% 726681
#% 800514
#% 801690
#% 810011
#% 824712
#% 833135
#% 853016
#% 960239
#% 960246
#% 960359
#% 996348
#% 1063535
#% 1692959
#% 1693388
#! Consider a scientist who wants to explore multiple data sets to select the relevant ones for further analysis. Since the visualization real estate may put a stringent constraint on how much detail can be presented to this user in a single page, effective table summarization techniques are needed to create summaries that are both sufficiently small and effective in communicating the available content. In this paper, we first argue that table summarization can benefit from knowledge about acceptable value clustering alternatives for clustering the values in the database. We formulate the problem of table summarization with the help of value lattices. We then provide a framework to express alternative clustering strategies and to account for various utility measures (such as information loss) in assessing different summarization alternatives. Based on this interpretation, we introduce three preference criteria, max-min-util (cautious), max-sum-util (cumulative), and pareto-util, for the problem of table summarization. To tackle with the inherent complexity, we rely on the properties of the fuzzy interpretation to further develop a novel ranked set cover based evaluation mechanism (RSC). These are brought together in an AlphaSum, table summarization system. Experimental evaluations showed that RSC improves both execution times and the summary qualities in AlphaSum, by pruning the search space more effectively than the existing solutions.

#index 1181222
#* Answering aggregate keyword queries on relational databases using minimal group-bys
#@ Bin Zhou;Jian Pei
#t 2009
#c 8
#% 115465
#% 273916
#% 333925
#% 333927
#% 408396
#% 410276
#% 464215
#% 479795
#% 745506
#% 824693
#% 845359
#% 874894
#% 875017
#% 960235
#% 960243
#% 960245
#% 960259
#% 960285
#% 993987
#% 1015294
#% 1015325
#% 1063538
#! Keyword search has been recently extended to relational databases to retrieve information from text-rich attributes. However, all the existing methods focus on finding individual tuples matching a set of query keywords from one table or the join of multiple tables. In this paper, we motivate a novel problem of aggregate keyword search: finding minimal group-bys covering a set of query keywords well, which is useful in many applications. We develop two interesting approaches to tackle the problem, and further extend our methods to allow partial matches. An extensive empirical evaluation using both real data sets and synthetic data sets is reported to verify the effectiveness of aggregate keyword search and the efficiency of our methods.

#index 1181223
#* Rule-based multi-query optimization
#@ Mingsheng Hong;Mirek Riedewald;Christoph Koch;Johannes Gehrke;Alan Demers
#t 2009
#c 8
#% 36117
#% 116043
#% 116629
#% 300179
#% 333938
#% 386232
#% 397353
#% 411554
#% 480938
#% 481448
#% 810032
#% 875004
#% 875022
#% 993949
#% 1015279
#% 1016210
#% 1063480
#% 1688281
#! Data stream management systems usually have to process many long-running queries that are active at the same time. Multiple queries can be evaluated more efficiently together than independently, because it is often possible to share state and computation. Motivated by this observation, various Multi-Query Optimization (MQO) techniques have been proposed. However, these approaches suffer from two limitations. First, they focus on very specialized workloads. Second, integrating MQO techniques for CQL-style stream engines and those for event pattern detection engines is even harder, as the processing models of these two types of stream engines are radically different. In this paper, we propose a rule-based MQO framework. This framework incorporates a set of new abstractions, extending their counterparts, physical operators, transformation rules, and streams, in a traditional RDBMS or stream processing system. Within this framework, we can integrate new and existing MQO techniques through the use of transformation rules. This allows us to build an expressive and scalable stream system. Just as relational optimizers are crucial for the success of RDBMSes, a powerful multi-query optimizer is needed for data stream processing. This work lays the foundation for such a multi-query optimizer, creating opportunities for future research. We experimentally demonstrate the efficacy of our approach.

#index 1181224
#* Managing long-running queries
#@ Stefan Krompass;Harumi Kuno;Janet L. Wiener;Kevin Wilkinson;Umeshwar Dayal;Alfons Kemper
#t 2009
#c 8
#% 170893
#% 713832
#% 765467
#% 765468
#% 800589
#% 810056
#% 893175
#% 960280
#% 1022263
#% 1022294
#% 1688297
#! Business Intelligence query workloads that run against very large data warehouses contain queries whose execution times range, sometimes unpredictably, from seconds to hours. The presence of even a handful of long-running queries can significantly slow down a workload consisting of thousands of queries, creating havoc for queries that require a quick response. Long-running queries are a known problem in all commercial database products. However, we have not seen a thorough classification of long-running queries nor a systematic study of the most effective corrective actions. We present here a systematic study of workload management policies, including many implemented by commercial database vendors. Our goal is to enable a system to: (1) recognize long-running queries and categorize them in terms of their impact on performance and (2) determine and take (automatically!) the most effective control actions to remedy the situation. To this end, we identify common workload management scenarios involving long-running queries, and create a taxonomy of long-running queries. We carry out an extensive set of experiments to evaluate different management policies and the relative and absolute thresholds that they may use. We find that in some scenarios, the right combination of policies can reduce the runtime of a workload by a factor of two, but that in other scenarios, any action taken increases runtime. One surprising result was that relative thresholds for execution control can compensate for inaccurate cost estimates, so that Kill&Requeue actions perform as well as Suspend&Resume.

#index 1181225
#* Continuous visible nearest neighbor queries
#@ Yunjun Gao;Baihua Zheng;Wang-Chien Lee;Gencai Chen
#t 2009
#c 8
#% 86950
#% 201876
#% 261733
#% 287466
#% 397377
#% 427199
#% 461923
#% 464859
#% 465004
#% 480093
#% 480830
#% 527187
#% 527328
#% 566699
#% 745464
#% 765163
#% 800571
#% 800572
#% 810061
#% 814650
#% 832568
#% 843877
#% 864466
#% 893092
#% 993955
#% 1016191
#% 1408837
#% 1720757
#! In this paper, we identify and solve a new type of spatial queries, called continuous visible nearest neighbor (CVNN) search. Given a data set P, an obstacle set O, and a query line segment q, a CVNN query returns a set of (p, R) tuples such that p ε P is the nearest neighbor (NN) to every point r along the interval R ε q as well as p is visible to r. Note that p may be NULL, meaning that all points in P are invisible to all points in R, due to the obstruction of some obstacles in O. In this paper, we formulate the problem and propose efficient algorithms for CVNN query processing, assuming that both P and O are indexed by R-trees. In addition, we extend our techniques to several variations of the CVNN query. Extensive experiments verify the efficiency and effectiveness of our proposed algorithms using both real and synthetic datasets.

#index 1181226
#* Query ranking in probabilistic XML data
#@ Lijun Chang;Jeffrey Xu Yu;Lu Qin
#t 2009
#c 8
#% 340914
#% 345712
#% 397375
#% 465044
#% 654442
#% 660011
#% 800547
#% 824681
#% 977012
#% 993985
#% 1015258
#% 1063520
#% 1063522
#% 1063720
#% 1127375
#% 1206645
#% 1206646
#% 1408834
#% 1688305
#! Twig queries have been extensively studied as a major fragment of XPATH queries to query XML data. In this paper, we study PXML-RANK query, (Q, k), which is to rank top-k probabilities of the answers of a twig query Q in probabilistic XML (PXML) data. A new research issue is how to compute top-k probabilities of answers of a twig query Q in PXML in the presence of containment (ancestor/descendant) relationships. In the presence of the ancestor/descendant relationships, the existing dynamic programming approaches to rank top-k probabilities over a set of tuples cannot be directly applied, because any node/edge in PXML may have impacts on the top-k probabilities of answers. We propose new algorithms to compute PXML-RANK queries efficiently and give conditions under which a PXML-RANK query can be processed efficiently without enumeration of all the possible worlds. We conduct extensive performance studies using both real and large benchmark datasets, and confirm the efficiency of our algorithms.

#index 1181227
#* On rewriting XPath queries using views
#@ Foto Afrati;Rada Chirkova;Manolis Gergatsoulis;Benny Kimelfeld;Vassia Pavlaki;Yehoshua Sagiv
#t 2009
#c 8
#% 198465
#% 273696
#% 273924
#% 303884
#% 465053
#% 576102
#% 632039
#% 733593
#% 824661
#% 824690
#% 893135
#% 940867
#% 955661
#% 1015260
#% 1015267
#% 1016134
#% 1044440
#! The problem of rewriting a query using a materialized view is studied for a well known fragment of XPath that includes the following three constructs: wildcards, descendant edges and branches. In earlier work, determining the existence of a rewriting was shown to be coNP-hard, but no tight complexity bound was given. While it was argued that Σ3p is an upper bound, the proof was based on results that have recently been refuted. Consequently, the exact complexity (and even decidability) of this basic problem has been unknown, and there have been no practical rewriting algorithms if the query and the view use all the three constructs mentioned above. It is shown that under fairly general conditions, there are only two candidates for rewriting and hence, the problem can be practically solved by two containment tests. In particular, under these conditions, determining the existence of a rewriting is coNP-complete. The proofs utilize various novel techniques for reasoning about XPath patterns. For the general case, the exact complexity remains unknown, but it is shown that the problem is decidable.

#index 1181228
#* Parallelization of XPath queries using multi-core processors: challenges and experiences
#@ Rajesh Bordawekar;Lipyeow Lim;Oded Shmueli
#t 2009
#c 8
#% 299275
#% 330305
#% 345693
#% 394617
#% 428147
#% 814648
#% 893106
#% 960276
#% 964486
#% 983052
#% 1179197
#% 1368900
#! In this study, we present experiences of parallelizing XPath queries using the Xalan XPath engine on shared-address space multi-core systems. For our evaluation, we consider a scenario where an XPath processor uses multiple threads to concurrently navigate and execute individual XPath queries on a shared XML document. Given the constraints of the XML execution and data models, we propose three strategies for parallelizing individual XPath queries: Data partitioning, Query partitioning, and Hybrid (query and data) partitioning. We experimentally evaluated these strategies on an x86 Linux multi-core system using a set of XPath queries, invoked on a variety of XML documents using the Xalan XPath APIs. Experimental results demonstrate that the proposed parallelization strategies work very effectively in practice; for a majority of XPath queries under evaluation, the execution performance scaled linearly as the number of threads was increased. Results also revealed the pros and cons of the different parallelization strategies for different XPath query patterns.

#index 1181229
#* GADDI: distance index based subgraph matching in biological networks
#@ Shijie Zhang;Shirong Li;Jiong Yang
#t 2009
#c 8
#% 184048
#% 196798
#% 288990
#% 340031
#% 342604
#% 378391
#% 387427
#% 519555
#% 543962
#% 577219
#% 629646
#% 629708
#% 664691
#% 727845
#% 727896
#% 765429
#% 769940
#% 769951
#% 772884
#% 810072
#% 824711
#% 833025
#% 833960
#% 841960
#% 864425
#% 864475
#% 960305
#% 989610
#% 1022280
#% 1030877
#% 1044450
#% 1063500
#% 1206703
#% 1387890
#% 1387891
#% 1411112
#% 1717545
#! Currently, a huge amount of biological data can be naturally represented by graphs, e.g., protein interaction networks, gene regulatory networks, etc. The need for indexing large graphs is an urgent research problem of great practical importance. The main challenge is size. Each graph may contain thousands (or more) vertices. Most of the previous work focuses on indexing a set of small or medium sized database graphs (with only tens of vertices) and finding whether a query graph occurs in any of these. In this paper, we are interested in finding all the matches of a query graph in a given large graph of thousands of vertices, which is a very important task in many biological applications. This increases the complexity significantly. We propose a novel distance measurement which reintroduces the idea of frequent substructures in a single large graph. We devise the novel structure distance based approach (GADDI) to efficiently find matches of the query graph. GADDI is further optimized by the use of a dynamic matching scheme to minimize redundant calculations. Last but not least, a number of real and synthetic data sets are used to evaluate the efficiency and scalability of our proposed method.

#index 1181230
#* A novel approach for efficient supergraph query processing on graph databases
#@ Shuo Zhang;Jianzhong Li;Hong Gao;Zhaonian Zou
#t 2009
#c 8
#% 288990
#% 378391
#% 408396
#% 443344
#% 466644
#% 629603
#% 629708
#% 654476
#% 729938
#% 731608
#% 765429
#% 772884
#% 824658
#% 864425
#% 960305
#% 1022279
#% 1022280
#% 1044450
#% 1127380
#% 1206686
#! In recent years, large amount of data modeled by graphs, namely graph data, have been collected in various domains. Efficiently processing queries on graph databases has attracted a lot of research attentions. Supergraph query is a kind of new and important queries in practice. A supergraph query, q, on a graph database D is to retrieve all graphs in D such that q is a supergraph of them. Because the number of graphs in databases is large and subgraph isomorphism testing is NP-complete, efficiently processing such queries is a big challenge. This paper first proposes an optimal compact method for organizing graph databases. Common subgraphs of the graphs in a database are stored only once in the compact organization of the database, in order to reduce the overall cost of subgraph isomorphism testings from stored graphs to queries during query processing. Then, an exact algorithm and an approximate algorithm for generating significant feature set with optimal order are proposed to construct indices on graph databases. The optimal order on the feature set is to reduce the number of subgraph isomorphism testings during query processing. Based on the compact organization of graph databases, a novel algorithm of testing subgraph isomorphisms from multiple graphs to one graph is presented. Finally, based on all these techniques, a query processing method is proposed. Analytical and experimental results show that the proposed algorithms outper-form the existing similar algorithms by one to two orders of magnitude.

#index 1181231
#* Flexible query answering on graph-modeled data
#@ Federica Mandreoli;Riccardo Martoglia;Giorgio Villani;Wilma Penzo
#t 2009
#c 8
#% 27049
#% 333854
#% 660011
#% 765408
#% 810052
#% 810072
#% 824693
#% 838414
#% 863389
#% 956564
#% 960237
#% 960259
#% 960279
#% 1015258
#% 1021948
#% 1022286
#% 1063537
#% 1206703
#! The largeness and the heterogeneity of most graph-modeled datasets in several database application areas make the query process a real challenge because of the lack of a complete knowledge of the vocabulary used, as well as of the information about the structural relationships between the data. To overcome these problems, flexible query answering capabilities are an essential need. In this paper we present a general model for supporting approximate queries on graph-modeled data. Approximation is both on the vocabularies and the structure. The model is general in that it is not bound to a specific graph data model, rather it gracefully accommodates labeled directed/undirected data graphs with labeled/unlabeled edges. The query answering principles underlying the model are not compelled to a specific data graph, instead they are founded on properties inferable from the data model the data graph conforms to. We complement the work with a ranking model to deal with data approximations and with an efficient top-k retrieval algorithm which smartly accesses ad-hoc data structures and generates the most promising answers in an order correlated with the ranking measures. Experimental results prove the good effectiveness and efficiency of our proposal on different real world datasets.

#index 1181232
#* Privacy-preserving data mashup
#@ Noman Mohammed;Benjamin C. M. Fung;Ke Wang;Patrick C. K. Hung
#t 2009
#c 8
#% 136350
#% 152980
#% 248030
#% 300184
#% 443463
#% 575966
#% 575969
#% 576762
#% 577239
#% 577289
#% 635215
#% 654448
#% 729930
#% 765449
#% 800514
#% 800515
#% 823358
#% 874989
#% 881483
#% 881497
#% 881546
#% 883236
#% 893100
#% 937550
#% 951837
#% 960291
#% 975045
#% 1044457
#% 1068533
#% 1706196
#! Mashup is a web technology that combines information from more than one source into a single web application. This technique provides a new platform for different data providers to flexibly integrate their expertise and deliver highly customizable services to their customers. Nonetheless, combining data from different sources could potentially reveal person-specific sensitive information. In this paper, we study and resolve a real-life privacy problem in a data mashup application for the financial industry in Sweden, and propose a privacy-preserving data mashup (PPMashup) algorithm to securely integrate private data from different data providers, whereas the integrated data still retains the essential information for supporting general data exploration or a specific data mining task, such as classification analysis. Experiments on real-life data suggest that our proposed method is effective for simultaneously preserving both privacy and information usefulness, and is scalable for handling large volume of data.

#index 1181233
#* On the comparison of microdata disclosure control algorithms
#@ Rinku Dewri;Indrajit Ray;Indrakshi Ray;Darrell Whitley
#t 2009
#c 8
#% 443463
#% 576761
#% 576762
#% 577239
#% 785363
#% 800514
#% 800515
#% 864406
#% 864412
#% 864663
#% 874989
#% 876338
#% 881551
#% 957727
#% 996348
#% 1206574
#% 1206623
#% 1777256
#! Privacy models such as k-anonymity and l-diversity typically offer an aggregate or scalar notion of the privacy property that holds collectively on the entire anonymized data set. However, they fail to give an accurate measure of privacy with respect to the individual tuples. For example, two anonymizations achieving the same value of k in the k-anonymity model will be considered equally good with respect to privacy protection. However, it is quite possible that for one of the anonymizations a majority of the individual tuples have lesser probabilities of privacy breaches than their counterparts in the other anonymization. We therefore reject the notion that all anonymizations satisfying a particular privacy property, such as k-anonymity, are equally good. The scalar or aggregate value used in privacy models is often biased towards a fraction of the data set, resulting in higher privacy for some individuals and minimalistic for others. Consequently, to better compare anonymization algorithms, there is a need to formalize and measure this bias. Towards this end, we advocate the use of vector-based methods for representing privacy and other measurable properties of an anonymization. We represent the measure of a given property for an anonymized data set using a property vector. Anonymizations are then compared using quality index functions that quantify the effectiveness of the property vectors. A formal analysis with respect to their scope and limitations is provided. Finally, we present preference based techniques when comparisons are to be made across multiple properties induced by anonymizations.

#index 1181234
#* Detecting privacy violations in database publishing using disjoint queries
#@ Millist W. Vincent;Mukesh Mohania;Mizuho Iwaihara
#t 2009
#c 8
#% 64420
#% 67453
#% 164560
#% 268764
#% 322115
#% 384978
#% 481128
#% 576110
#% 576761
#% 765449
#% 864412
#% 874893
#% 874988
#% 942359
#% 976984
#% 1015329
#% 1022246
#% 1408525
#% 1661427
#% 1700133
#% 1700137
#% 1707132
#! We present a new method of detecting privacy violations in the context of database publishing. Our method defines a published view V to preserve the privacy of a secret query Q if V and Q return no tuples in common, over all possible database instances. We then establish necessary and sufficient conditions that characterize when V preserves the privacy of Q in terms of the projected inequalities in the queries, both for conjunctive queries and queries with negation. We also show that integrity constraints have an effect on privacy, and derive a test for ensuring privacy preservation in the presence of FD constraints. The issue of privacy preservation in the presence of multiple views is investigated, and we show that it can reduced to the single view case for a suitably chosen view.

#index 1181235
#* On keys, foreign keys and nullable attributes in relational mapping systems
#@ Luca Cabibbo
#t 2009
#c 8
#% 36181
#% 384978
#% 480134
#% 480429
#% 765432
#% 809239
#% 826032
#% 881740
#% 893094
#% 956601
#% 960233
#% 960272
#% 993981
#% 1063594
#% 1206614
#% 1661428
#! We consider the following scenario for a mapping system: given a source schema, a target schema, and a set of value correspondences between these two schemas, generate an executable transformation (i.e., a set of queries) to compute target instances from source instances. We base this computation on two main components: (i) a schema mapping generation algorithm, to compute a declarative schema mapping from the correspondences, and (ii) a query generation algorithm, to compute a transformation from the schema mapping. In this paper, we introduce novel schema mapping and query generation algorithms for mappings between relational schemas with keys, foreign keys and nullable attributes. We extend current relational mapping algorithms (e.g., those proposed in the Clio framework), which are able to deal only in a more limited way with such integrity constraints. As a further contribution, we propose referenced-attribute correspondences, which permit to specify more precise mappings than traditional attribute correspondences, while retaining a simple and intuitive semantics.

#index 1181236
#* A runtime approach to model-independent schema and data translation
#@ Paolo Atzeni;Luigi Bellomarini;Francesca Bugiotti;Giorgio Gianforme
#t 2009
#c 8
#% 458995
#% 465057
#% 480134
#% 806215
#% 810078
#% 960233
#% 960353
#% 1015303
#% 1126564
#% 1409371
#% 1410676
#% 1599316
#% 1688267
#% 1728141
#! A runtime approach to model-generic translation of schema and data is proposed. It is based on our previous work on MIDST, a platform conceived to perform translations in an off-line fashion. In the original approach, the source database is imported into a dictionary, where it is stored according to a universal model. Then, the translation is applied within the tool as a composition of elementary transformation steps, specified as Datalog programs. Finally, the result is exported into the operational system. Here we illustrate a new, lightweight approach where the database is not imported. The tool needs only to know the model and the schema of the source database and generates views on the operational system that transform the underlying data (stored in the source schema) according to the corresponding schema in the target model. Views are generated in an almost automatic way, on the basis of the Datalog rules for schema translation.

#index 1181237
#* A methodology for preference-based personalization of contextual data
#@ Antonio Miele;Elisa Quintarelli;Letizia Tanca
#t 2009
#c 8
#% 227894
#% 300170
#% 465167
#% 731407
#% 745519
#% 875002
#% 875003
#% 940695
#% 993957
#% 1206753
#% 1207079
#% 1409374
#% 1727459
#% 1728920
#! The widespread use of mobile appliances, with limitations in terms of storage, power, and connectivity capability, requires to minimize the amount of data to be loaded on user's devices, in order to quickly select only the information that is really relevant for the users in their current contexts: in such a scenario, specific methodologies and techniques focused on data reduction must be applied. We propose an extension to the data tailoring approach of Context-ADDICT, whose aim is to dynamically hook and integrate heterogeneous data to be stored on small, possibly mobile devices. The main goal of our extension is to personalize the context-dependent data obtained by means of the Context-ADDICT methodology, by allowing the user to express preferences that specify which data s/he is more interested in (and which not) in each specific context. This step allows us to impose a partial order among the data, and to load only the top (most preferred) portion of the data chunks. A running example is used to better illustrate the approach.

#index 1181238
#* Scalable stream join processing with expensive predicates: workload distribution and adaptation by time-slicing
#@ Song Wang;Elke Rundensteiner
#t 2009
#c 8
#% 115661
#% 116040
#% 158047
#% 330305
#% 378388
#% 765435
#% 800502
#% 810095
#% 824770
#% 824781
#% 875006
#% 875022
#% 889657
#% 893139
#% 1015278
#% 1015280
#% 1015282
#% 1016156
#% 1016167
#! Multi-way stream joins with expensive join predicates lead to great challenge for real-time (or close to real-time) stream processing. Given the memory- and CPU-intensive nature of such stream join queries, scalable processing on a cluster must be employed. This paper proposes a novel scheme for distributed processing of generic multi-way joins with window constraints, called Pipelined State Partitioning (PSP). We target generic joins with arbitrarily join conditions, which are used in non-trivial stream applications such as image matching and biometric recognizing. The PSP scheme partitions the states into disjoint slices in the time domain, and then distributes the fine-grained states in the cluster, forming a virtual computation ring. Compared to replication-based distribution of non-equi-joins, PSP scheme is superior since: (1) zero state duplication and thus no repeated computations, (2) pipelined processing of every input tuple on multiple nodes to achieve low response time, and (3) cost-based adaptive workload distribution. We have implemented the proposed PSP schemes within the CAPE DSMS. Our experimental study demonstrates the significant performance improvements compared to the state-of-the-art generic distributed stream join algorithms.

#index 1181239
#* Indexing density models for incremental learning and anytime classification on data streams
#@ Thomas Seidl;Ira Assent;Philipp Kranen;Ralph Krieger;Jennifer Herrmann
#t 2009
#c 8
#% 86950
#% 136350
#% 204531
#% 218443
#% 248797
#% 280480
#% 310500
#% 342639
#% 420077
#% 427199
#% 430784
#% 464605
#% 466405
#% 527850
#% 577271
#% 729437
#% 729932
#% 829991
#% 915345
#% 1070235
#% 1080140
#! Classification of streaming data faces three basic challenges: it has to deal with huge amounts of data, the varying time between two stream data items must be used best possible (anytime classification) and additional training data must be incrementally learned (anytime learning) for applying the classifier consistently to fast data streams. In this work, we propose a novel index-based technique that can handle all three of the above challenges using the established Bayes classifier on effective kernel density estimators. Our novel Bayes tree automatically generates (adapted efficiently to the individual object to be classified) a hierarchy of mixture densities that represent kernel density estimators at successively coarser levels. Our probability density queries together with novel classification improvement strategies provide the necessary information for very effective classification at any point of interruption. Moreover, we propose a novel evaluation method for anytime classification using Poisson streams and demonstrate the anytime learning performance of the Bayes tree.

#index 1181240
#* Exploiting the power of relational databases for efficient stream processing
#@ Erietta Liarou;Romulo Goncalves;Stratos Idreos
#t 2009
#c 8
#% 300179
#% 317871
#% 397353
#% 428155
#% 480768
#% 654497
#% 788215
#% 788216
#% 810039
#% 824664
#% 874996
#% 875006
#% 1016169
#! Stream applications gained significant popularity over the last years that lead to the development of specialized stream engines. These systems are designed from scratch with a different philosophy than nowadays database engines in order to cope with the stream applications requirements. However, this means that they lack the power and sophisticated techniques of a full fledged database system that exploits techniques and algorithms accumulated over many years of database research. In this paper, we take the opposite route and design a stream engine directly on top of a database kernel. Incoming tuples are directly stored upon arrival in a new kind of system tables, called baskets. A continuous query can then be evaluated over its relevant baskets as a typical one-time query exploiting the power of the relational engine. Once a tuple has been seen by all relevant queries/operators, it is dropped from its basket. A basket can be the input to a single or multiple similar query plans. Furthermore, a query plan can be split into multiple parts each one with its own input/output baskets allowing for flexible load sharing query scheduling. Contrary to traditional stream engines, that process one tuple at a time, this model allows batch processing of tuples, e.g., query a basket only after x tuples arrive or after a time threshold has passed. Furthermore, we are not restricted to process tuples in the order they arrive. Instead, we can selectively pick tuples from a basket based on the query requirements exploiting a novel query component, the basket expressions. We investigate the opportunities and challenges that arise with such a direction and we show that it carries significant advantages. We propose a complete architecture, the DataCell, which we implemented on top of an open-source column-oriented DBMS. A detailed analysis and experimental evaluation of the core algorithms using both micro benchmarks and the standard Linear Road benchmark demonstrate the potential of this new approach.

#index 1181241
#* A sampling approach for XML query selectivity estimation
#@ Cheng Luo;Zhewei Jiang;Wen-Chi Hou;Feng Yu;Qiang Zhu
#t 2009
#c 8
#% 58348
#% 82346
#% 137885
#% 210188
#% 273908
#% 277347
#% 397364
#% 397375
#% 397379
#% 465018
#% 481749
#% 650962
#% 654453
#% 745463
#% 765406
#% 765423
#% 810046
#% 824667
#% 824668
#% 864448
#% 864449
#% 918685
#% 993970
#! As the Extensible Markup Language (XML) rapidly establishes itself as the de facto standard for presenting, storing, and exchanging data on the Internet, large volume of XML data and their supporting facilities start to surface. A fast and accurate selectivity estimation mechanism is of practical importance because selectivity estimation plays a fundamental role in XML query optimization. Recently proposed techniques are all based on some forms of structure synopses that could be time-consuming to build and not effective for summarizing complex structure relationships. In this research, we propose an innovative sampling method that can capture the tree structures and intricate relationships among nodes in a simple and effective way. The derived sample tree is stored as a synopsis for selectivity estimation. Extensive experimental results show that, in comparison with the state-of-the-art structure synopses, specifically the TreeSketch and Xseed synopses, our sample tree synopsis applies to a broader range of query types, requires several orders of magnitude less construction time, and generates estimates with considerably better precision for complex datasets.

#index 1181242
#* Recursion in XQuery: put your distributivity safety belt on
#@ Loredana Afanasiev;Torsten Grust;Maarten Marx;Jan Rittinger;Jens Teubner
#t 2009
#c 8
#% 13014
#% 334006
#% 349958
#% 384978
#% 397407
#% 458599
#% 462000
#% 473117
#% 659999
#% 850728
#% 874910
#% 875010
#% 893208
#% 993938
#% 994015
#% 1015298
#% 1016150
#! We introduce a controlled form of recursion in XQuery, an inflationary fixed point operator, familiar from the context of relational databases. This operator imposes restrictions on the expressible types of recursion, but it is sufficiently versatile to capture a wide range of interesting use cases, including Regular XPath and its core transitive closure operator. While the optimization of general user-defined recursive functions in XQuery appears elusive, we describe how inflationary fixed points can be efficiently evaluated, provided that the recursive XQuery expressions are distributive. We test distributivity syntactically and algebraically, and provide experimental evidence that XQuery processors can benefit substantially from this mode of evaluation.

#index 1181243
#* Expressive, yet tractable XML keys
#@ Sven Hartmann;Sebastian Link
#t 2009
#c 8
#% 321051
#% 332151
#% 384978
#% 398752
#% 428149
#% 465051
#% 563958
#% 578570
#% 630971
#% 733268
#% 733593
#% 742566
#% 771227
#% 804840
#% 826029
#% 826031
#% 942355
#% 983074
#% 1027247
#% 1143832
#% 1397814
#% 1661433
#% 1712426
#! Constraints are important for a variety of XML recommendations and applications. Consequently, there are numerous opportunities for advancing the treatment of XML semantics. In particular, suitable notions of keys will enhance XML's capabilities of modeling, managing and processing native XML data. However, the different ways of accessing and comparing XML elements make it challenging to balance expressiveness and tractability. We investigate XML keys which uniquely identify XML elements based on a very general notion of value-equality: isomorphic subtrees with the identity on data values. Previously, an XML key fragment has been recognised that is robust in the sense that its implication problem can be expressed as the reachability problem in a suitable digraph. We analyse the impact of extending this fragment by structural keys that uniquely identify XML elements independently of any data. We establish a sound and complete set of inference rules for this expressive fragment of XML keys, and encode these rules in an algorithm that decides the associated implication problem in time quadratic in the size of the input keys. Consequently, we gain significant expressiveness without any loss of efficiency in comparison to less expressive XML key fragments.

#index 1181244
#* It takes variety to make a world: diversification in recommender systems
#@ Cong Yu;Laks Lakshmanan;Sihem Amer-Yahia
#t 2009
#c 8
#% 262112
#% 319705
#% 333854
#% 754124
#% 805841
#% 805863
#% 813966
#% 848640
#% 860672
#% 864456
#% 879686
#% 881500
#% 960287
#% 1001299
#% 1207001
#! Recommendations in collaborative tagging sites such as del.icio.us and Yahoo! Movies, are becoming increasingly important, due to the proliferation of general queries on those sites and the ineffectiveness of the traditional search paradigm to address those queries. Regardless of the underlying recommendation strategy, item-based or user-based, one of the key concerns in producing recommendations, is over-specialization, which results in returning items that are too homogeneous. Traditional solutions rely on post-processing returned items to identify those which differ in their attribute values (e.g., genre and actors for movies). Such approaches are not always applicable when intrinsic attributes are not available (e.g., URLs in del.icio.us). In a recent paper [20], we introduced the notion of explanation-based diversity and formalized the diversification problem as a compromise between accuracy and diversity. In this paper, we develop efficient diversification algorithms built upon this notion. The algorithms explore compromises between accuracy and diversity. We demonstrate their efficiency and effectiveness in diversification on two real life data sets: del.icio.us and Yahoo! Movies.

#index 1181245
#* Supporting annotations on relations
#@ Mohamed Y. Eltabakh;Walid G. Aref;Ahmed K. Elmagarmid;Mourad Ouzzani;Yasin N. Silva
#t 2009
#c 8
#% 378401
#% 772131
#% 778466
#% 810115
#% 825661
#% 845351
#% 864469
#% 1016204
#% 1063709
#% 1206792
#% 1408533
#! Annotations play a key role in understanding and curating databases. Annotations may represent comments, descriptions, lineage information, among several others. Annotation management is a vital mechanism for sharing knowledge and building an interactive and collaborative environment among database users and scientists. What makes it challenging is that annotations can be attached to database entities at various granularities, e.g., at the table, tuple, column, cell levels, or more generally, to any subset of cells that results from a select statement. Therefore, simple comment fields in tuples would not work because of the combinatorial nature of the annotations. In this paper, we present extensions to current database management systems to support annotations. We propose storage schemes to efficiently store annotations at multiple granularities, i.e., at the table, tuple, column, and cell levels. Compared to storing the annotations with the individual cells, the proposed schemes achieve more than an order-of-magnitude reduction in storage and up to 70% saving in the query execution time. We define types of annotations that inherit different behaviors. Through these types, users can specify, for example, whether or not an annotation is continuously applied over newly inserted data and whether or not an annotation is archived when the base data is modified. These annotation types raise several storage and processing challenges that are addressed in the paper. We propose declarative ways to add, archive, query, and propagate annotations. The proposed mechanisms are realized through extensions to the standard SQL. We implemented the proposed functionalities inside PostgreSQL with an easy to use Excel-based front-end graphical interface.

#index 1181246
#* Data clouds: summarizing keyword search results over structured data
#@ Georgia Koutrika;Zahra Mohammadi Zadeh;Hector Garcia-Molina
#t 2009
#c 8
#% 268079
#% 644109
#% 659990
#% 660011
#% 864456
#% 875003
#% 956649
#% 993987
#% 1015325
#% 1016176
#% 1035573
#% 1065169
#% 1206698
#! Keyword searches are attractive because they facilitate users searching structured databases. On the other hand, tag clouds are popular for navigation and visualization purposes over unstructured data because they can highlight the most significant concepts and hidden relationships in the underlying content dynamically. In this paper, we propose coupling the flexibility of keyword searches over structured data with the summarization and navigation capabilities of tag clouds to help users access a database. We propose using clouds over structured data (data clouds) to summarize the results of keyword searches over structured data and to guide users to refine their searches. The cloud presents the most significant words associated with the search results. Our keyword search model allows searching for entities than can span multiple tables in the database rather than just tuples, as existing keyword searches over databases do. We present several methods to compute the scores both for the entities and for the terms in the search results. We describe algorithms for keyword searches with data clouds and we present our system, CourseCloud, that offers a unified search and browse interface to a course database. We present experimental results showing (a) the appropriateness of the methods used for scoring terms, (b) the performance of the proposed algorithms, and (c) the effectiveness of CourseCloud compared to typical search and browse interfaces to a course database.

#index 1181247
#* Sample synopses for approximate answering of group-by queries
#@ Philipp Rösch;Wolfgang Lehner
#t 2009
#c 8
#% 1331
#% 210190
#% 248822
#% 273909
#% 274152
#% 300195
#% 333955
#% 465152
#% 465162
#% 479984
#% 480306
#% 480471
#% 480966
#% 481779
#% 654486
#% 765425
#% 765455
#% 810007
#% 875050
#% 1015310
#% 1015328
#% 1206644
#! With the amount of data in current data warehouse databases growing steadily, random sampling is continuously gaining in importance. In particular, interactive analyses of large datasets can greatly benefit from the significantly shorter response times of approximate query processing. Typically, those analytical queries partition the data into groups and aggregate the values within the groups. Further, with the commonly used roll-up and drill-down operations a broad range of group-by queries is posed to the system, which makes the construction of highly-specialized synopses difficult. In this paper, we propose a general-purpose sampling scheme that is biased in order to answer group-by queries with high accuracy. While existing techniques focus on the size of the group when computing its sample size, our technique is based on its standard deviation. The basic idea is that the more homogeneous a group is, the less representatives are required in order to give a good estimate. With an extensive set of experiments, we show that our approach reduces both the estimation error and the construction cost compared to existing techniques.

#index 1181248
#* A query processor for prediction-based monitoring of data streams
#@ Sergio Ilarri;Ouri Wolfson;Eduardo Mena;Arantza Illarramendi;Prasad Sistla
#t 2009
#c 8
#% 295512
#% 341471
#% 378388
#% 387427
#% 397377
#% 427199
#% 464847
#% 555045
#% 570888
#% 721276
#% 765402
#% 788219
#% 788231
#% 799138
#% 799139
#% 806214
#% 847525
#% 853011
#% 857492
#% 864435
#% 864706
#% 874983
#% 878299
#% 940697
#% 958197
#% 1206570
#% 1408812
#! Networks of sensors are used in many different fields, from industrial applications to surveillance applications. A common feature of these applications is the necessity of a monitoring infrastructure that analyzes a large number of data streams and outputs values that satisfy certain constraints. In this paper, we present a query processor for monitoring queries in a network of sensors with prediction functions. Sensors communicate their values according to a threshold policy, and the proposed query processor leverages prediction functions to compare tuples efficiently and to generate answers even in the absence of new incoming tuples. Two types of constraints are managed by the query processor: window-join constraints and value constraints. Uncertainty issues are considered to assign probabilistic values to the results returned to the user. Moreover, we have developed an appropriate buffer management strategy, that takes into account the contributions of the prediction functions contained in the tuples. We also present some experimental results that show the benefits of the proposal.

#index 1181249
#* Flower-CDN: a hybrid P2P overlay for efficient query processing in CDN
#@ Manal El Dick;Esther Pacitti;Bettina Kemme
#t 2009
#c 8
#% 35764
#% 256883
#% 340175
#% 401983
#% 464787
#% 496287
#% 566745
#% 793890
#% 1019128
#% 1112006
#% 1711102
#% 1914106
#! Many websites with a large user base, e.g., websites of nonprofit organizations, do not have the financial means to install large web-servers or use specialized content distribution networks such as Akamai. For those websites, we have developed Flower-CDN, a locality-aware P2P based content-distribution network (CDN) in which the users that are interested in a website support the distribution of its content. The idea is that peers keep the content they retrieve and later serve it to other peers that are close to them in locality. Our architecture is a hybrid between structured and unstructured networks. When a new client requests some content from a website, a locality-aware DHT quickly finds a peer in its neighborhood that has the content available. Additionally, all peers in a given locality that maintain content of a particular website build an unstructured content overlay. Within this overlay, peers gossip information about their content allowing the system to maintain accurate information despite churn. In our performance evaluation, we compare Flower-CDN with an existing P2P-CDN strictly based on DHT and not locality aware. Flower-CDN reduces lookup latency by a factor of 9 and transfer distance by a factor of 2. We also show that Flower-CDN's gossip has low overhead and can be adjusted according to hit ratio requirements and bandwidth availability.

#index 1181250
#* Zerber+R: top-k retrieval from a confidential index
#@ Sergej Zerr;Daniel Olmedilla;Wolfgang Nejdl;Wolf Siberski
#t 2009
#c 8
#% 397367
#% 433922
#% 577239
#% 664705
#% 800514
#% 800515
#% 830694
#% 893171
#% 963446
#% 1014463
#% 1015329
#% 1015331
#% 1044459
#! Privacy-preserving document exchange among collaboration groups in an enterprise as well as across enterprises requires techniques for sharing and search of access-controlled information through largely untrusted servers. In these settings search systems need to provide confidentiality guarantees for shared information while offering IR properties comparable to the ordinary search engines. Top-k is a standard IR technique which enables fast query execution on very large indexes and makes systems highly scalable. However, indexing access-controlled information for top-k retrieval is a challenging task due to the sensitivity of the term statistics used for ranking. In this paper we present Zerber+R -- a ranking model which allows for privacy-preserving top-k retrieval from an outsourced inverted index. We propose a relevance score transformation function which makes relevance scores of different terms indistinguishable, such that even if stored on an untrusted server they do not reveal information about the indexed data. Experiments on two real-world data sets show that Zerber+R makes economical usage of bandwidth and offers retrieval properties comparable with an ordinary inverted index.

#index 1181251
#* Efficient top-k count queries over imprecise duplicates
#@ Sunita Sarawagi;Vinay S Deshpande;Sourabh Kasliwal
#t 2009
#c 8
#% 36672
#% 310516
#% 314740
#% 460812
#% 480654
#% 568377
#% 577238
#% 654467
#% 765463
#% 800590
#% 844289
#% 850014
#% 864417
#% 870896
#% 874975
#% 893164
#% 912246
#% 915242
#% 937552
#% 993980
#% 1022229
#% 1022259
#% 1063496
#% 1063534
#% 1070886
#% 1075132
#% 1083704
#% 1250257
#! We propose efficient techniques for processing various Top-K count queries on data with noisy duplicates. Our method differs from existing work on duplicate elimination in two significant ways: First, we dedup on the fly only the part of the data needed for the answer --- a requirement in massive and evolving sources where batch deduplication is expensive. The non-local nature of the problem of partitioning data into duplicate groups, makes it challenging to filter only those tuples forming the K largest groups. We propose a novel method of successively collapsing and pruning records which yield an order of magnitude reduction in running time compared to deduplicating the entire data first. Second, we return multiple high scoring answers to handle situations where it is impossible to resolve if two records are indeed duplicates of each other. Since finding even the highest scoring deduplication is NP-hard, the existing approach is to deploy one of many variants of score-based clustering algorithms which do not easily generalize to finding multiple groupings. We model deduplication as a segmentation of a linear embedding of records and present a polynomial time algorithm for finding the R highest scoring answers. This method closely matches the accuracy of an exact exponential time algorithm on several datasets.

#index 1181252
#* The C-ND tree: a multidimensional index for hybrid continuous and non-ordered discrete data spaces
#@ Changqing Chen;Sakti Pramanik;Qiang Zhu;Watve Alok;Gang Qian
#t 2009
#c 8
#% 86950
#% 227939
#% 271801
#% 287715
#% 411694
#% 427199
#% 450454
#% 458174
#% 464195
#% 464841
#% 480093
#% 481956
#% 631963
#% 683563
#% 860862
#% 871760
#% 893495
#% 1015306
#! Contemporary database applications often perform queries in hybrid data spaces (HDS) where vectors can have a mix of continuous valued and non-ordered discrete valued dimensions. To support efficient query processing for an HDS, a robust indexing method is required. Existing indexing techniques to process queries efficiently either apply to continuous data spaces (e.g., the R-tree) or non-ordered discrete data spaces (e.g., the ND-tree). No techniques directly indexing vectors in HDSs have been reported in the literature. In this paper, we propose a new multidimensional indexing technique, called the C-ND tree, to directly index vectors in an HDS. To build such an index, we first introduce some essential geometric concepts (e.g., hybrid bounding rectangle) in HDSs. The C-ND tree structure and the relevant tree building and query processing algorithms based on these geometric concepts in HDSs are then presented. Strategies have been suggested to make the values in continuous dimensions and non-ordered discrete dimensions comparable and controllable. Novel node splitting heuristics which exploit characteristics of both continuous and discrete dimensions are proposed. Performance of the C-ND tree is compared with that of linear scan, R*-tree and ND-tree using range queries on hybrid data. Experimental results demonstrate that the C-ND tree is quite promising in supporting range queries in HDSs.

#index 1181253
#* G-hash: towards fast kernel-based similarity search in large graph databases
#@ Xiaohong Wang;Aaron Smalter;Jun Huan;Gerald H. Lushington
#t 2009
#c 8
#% 269226
#% 378391
#% 601159
#% 722757
#% 765429
#% 813990
#% 840863
#% 840908
#% 844291
#% 864425
#% 937108
#% 1022280
#% 1072360
#% 1117041
#% 1206703
#! Structured data including sets, sequences, trees and graphs, pose significant challenges to fundamental aspects of data management such as efficient storage, indexing, and similarity search. With the fast accumulation of graph databases, similarity search in graph databases has emerged as an important research topic. Graph similarity search has applications in a wide range of domains including cheminformatics, bioinformatics, sensor network management, social network management, and XML documents, among others. Most of the current graph indexing methods focus on subgraph query processing, i.e. determining the set of database graphs that contains the query graph and hence do not directly support similarity search. In data mining and machine learning, various graph kernel functions have been designed to capture the intrinsic similarity of graphs. Though successful in constructing accurate predictive and classification models for supervised learning, graph kernel functions have (i) high computational complexity and (ii) non-trivial difficulty to be indexed in a graph database. Our objective is to bridge graph kernel function and similarity search in graph databases by proposing (i) a novel kernel-based similarity measurement and (ii) an efficient indexing structure for graph data management. Our method of similarity measurement builds upon local features extracted from each node and their neighboring nodes in graphs. A hash table is utilized to support efficient storage and fast search of the extracted local features. Using the hash table, a graph kernel function is defined to capture the intrinsic similarity of graphs and for fast similarity query processing. We have implemented our method, which we have named G-hash, and have demonstrated its utility on large chemical graph databases. Our results show that the G-hash method achieves state-of-the-art performance for k-nearest neighbor (k-NN) classification. Most importantly, the new similarity measurement and the index structure is scalable to large database with smaller indexing size, faster indexing construction time, and faster query processing time as compared to state-of-the-art indexing methods such as C-tree, gIndex, and GraphGrep.

#index 1181254
#* On-line exact shortest distance query processing
#@ Jiefeng Cheng;Jeffrey Xu Yu
#t 2009
#c 8
#% 58365
#% 88051
#% 94589
#% 148021
#% 327432
#% 338382
#% 379482
#% 398749
#% 410276
#% 442858
#% 443208
#% 443533
#% 600184
#% 729923
#% 761950
#% 787540
#% 800534
#% 824692
#% 833631
#% 836529
#% 864462
#% 881460
#% 960259
#% 960304
#% 994015
#% 1044451
#% 1055756
#% 1063513
#% 1063514
#% 1065980
#% 1688299
#! Shortest-path query processing not only serves as a long established routine for numerous applications in the past but also is of increasing popularity to support novel graph applications in very large databases nowadays. For a large graph, there is the new scenario to query intensively against arbitrary nodes, asking to quickly return node distance or even shortest paths. And traditional main memory algorithms and shortest paths materialization become inadequate. We are interested in graph labelings to encode the underlying graphs and assign labels to nodes to support efficient query processing. Surprisingly, the existing work of this category mainly emphasizes on reachability query processing, while no sufficient effort has been given to distance labelings to support querying exact shortest distances between nodes. Distance labelings must be developed on the graph in whole to correctly retain node distance information. It makes many existing methods to be inapplicable. We focus on fast computing distance-aware 2-hop covers, which can encode the all-pairs shortest paths of a graph in O(|V|·|E|1/2) space. Our approach exploits strongly connected components collapsing and graph partitioning to gain speed, while it can overcome the challenges in correctly retaining node distance information and appropriately encoding all-pairs shortest paths with small overhead. Furthermore, our approach avoids pre-computing all-pairs shortest paths, which can be prohibitive over large graphs. We conducted extensive performance studies, and confirm the efficiency of our proposed new approaches.

#index 1181255
#* Efficiently indexing shortest paths by exploiting symmetry in graphs
#@ Yanghua Xiao;Wentao Wu;Jian Pei;Wei Wang;Zhenying He
#t 2009
#c 8
#% 207879
#% 348615
#% 410276
#% 442858
#% 480081
#% 566109
#% 766381
#% 800534
#% 818476
#% 824692
#% 838518
#% 864462
#% 906306
#% 960304
#% 1063472
#% 1131498
#! Shortest path queries (SPQ) are essential in many graph analysis and mining tasks. However, answering shortest path queries on-the-fly on large graphs is costly. To online answer shortest path queries, we may materialize and index shortest paths. However, a straightforward index of all shortest paths in a graph of N vertices takes O(N2) space. In this paper, we tackle the problem of indexing shortest paths and online answering shortest path queries. As many large real graphs are shown richly symmetric, the central idea of our approach is to use graph symmetry to reduce the index size while retaining the correctness and the efficiency of shortest path query answering. Technically, we develop a framework to index a large graph at the orbit level instead of the vertex level so that the number of breadth-first search trees materialized is reduced from O(N) to O(|Δ|), where |Δ| ≤ N is the number of orbits in the graph. We explore orbit adjacency and local symmetry to obtain compact breadth-first-search trees (compact BFS-trees). An extensive empirical study using both synthetic data and real data shows that compact BFS-trees can be built efficiently and the space cost can be reduced substantially. Moreover, online shortest path query answering can be achieved using compact BFS-trees.

#index 1181256
#* Estimating the number of frequent itemsets in a large database
#@ Ruoming Jin;Scott McCallen;Yuri Breitbart;Dave Fuhry;Dong Wang
#t 2009
#c 8
#% 225588
#% 227919
#% 232136
#% 248014
#% 280456
#% 300120
#% 481290
#% 481779
#% 579314
#% 769883
#% 769910
#% 778215
#% 814645
#% 844391
#% 867881
#% 1016244
#! Estimating the number of frequent itemsets for minimal support α in a large dataset is of great interest from both theoretical and practical perspectives. However, finding not only the number of frequent itemsets, but even the number of maximal frequent itemsets, is #P-complete. In this study, we provide a theoretical investigation on the sampling estimator. We discover and prove several fundamental but also rather surprising properties of the sampling estimator. We also propose a novel algorithm to estimate the number of frequent itemsets without using sampling. Our detailed experimental results have shown the accuracy and efficiency of our proposed approach.

#index 1181257
#* FOGGER: an algorithm for graph generator discovery
#@ Zhiping Zeng;Jianyong Wang;Jun Zhang;Lizhu Zhou
#t 2009
#c 8
#% 234979
#% 314836
#% 384416
#% 466644
#% 577218
#% 629708
#% 727845
#% 729938
#% 765429
#% 769907
#% 769940
#% 798763
#% 823357
#% 833120
#% 841960
#% 864460
#% 864461
#% 881553
#% 915288
#% 915301
#% 960305
#% 989640
#% 989646
#% 1022280
#% 1250571
#! To our best knowledge, all existing graph pattern mining algorithms can only mine either closed, maximal or the complete set of frequent subgraphs instead of graph generators which are preferable to the closed subgraphs according to the Minimum Description Length principle in some applications. In this paper, we study a new problem of frequent subgraph mining, called frequent connected graph generator mining, which poses significant challenges due to the underlying complexity associated with frequent subgraph mining as well as the absence of Apriori property for graph generators. Whereas, we still present an efficient solution FOGGER for this new problem. By exploring some properties of graph generators, two effective pruning techniques, backward edge pruning and forward edge pruning, are proposed to prune the branches of the well-known DFS code enumeration tree that do not contain graph generators. To further improve the efficiency, an effective index structure, ADI++, is also devised to facilitate the subgraph isomorphism checking. We experimentally evaluate various aspects of FOGGER using both real and synthetic datasets. Our results demonstrate that the two pruning techniques are effective in pruning the unpromising parts of search space, and FOGGER is efficient and scalable in terms of the base size of input databases. Meanwhile, the performance study for graph generator-based classification model shows that generator-based model is much simpler and can achieve almost the same accuracy for classifying chemical compounds in comparison with closed subgraph-based model.

#index 1181258
#* Neighbor-based pattern detection for windows over streaming data
#@ Di Yang;Elke A. Rundensteiner;Matthew O. Ward
#t 2009
#c 8
#% 210173
#% 230138
#% 273890
#% 300136
#% 479658
#% 479791
#% 576113
#% 578388
#% 594012
#% 878299
#% 881938
#% 889089
#% 893104
#% 989584
#% 1015261
#% 1016157
#% 1019143
#! The discovery of complex patterns such as clusters, outliers, and associations from huge volumes of streaming data has been recognized as critical for many domains. However, pattern detection with sliding window semantics, as required by applications ranging from stock market analysis to moving object tracking remains largely unexplored. Applying static pattern detection algorithms from scratch to every window is prohibitively expensive due to their high algorithmic complexity. This work tackles this problem by developing the first solution for incremental detection of neighbor-based patterns specific to sliding window scenarios. The specific pattern types covered in this work include density-based clusters and distance-based outliers. Incremental pattern computation in highly dynamic streaming environments is challenging, because purging a large amount of to-be-expired data from previously formed patterns may cause complex pattern changes including migration, splitting, merging and termination of these patterns. Previous incremental neighbor-based pattern detection algorithms, which were typically not designed to handle sliding windows, such as incremental DBSCAN, are not able to solve this problem efficiently in terms of both CPU and memory consumption. To overcome this, we exploit the "predictability" property of sliding windows to elegantly discount the effect of expiring objects on the remaining pattern structures. Our solution achieves minimal CPU utilization, while still keeping the memory utilization linear in the number of objects in the window. Our comprehensive experimental study, using both synthetic as well as real data from domains of stock trades and moving object monitoring, demonstrates superiority of our proposed strategies over alternate methods in both CPU and memory utilization.

#index 1181259
#* Efficient constraint evaluation in categorical sequential pattern mining for trajectory databases
#@ Leticia I. Gomez;Alejandro A. Vaisman
#t 2009
#c 8
#% 152934
#% 310559
#% 315005
#% 443502
#% 459006
#% 463903
#% 464996
#% 479971
#% 481290
#% 562299
#% 949146
#! The classic Generalized Sequential Patterns (GSP) algorithm returns all frequent sequences present in a database. However, usually a few ones are interesting from a user's point of view. Thus, post-processing tasks are required in order to discard uninteresting sequences. To avoid this drawback, languages based on regular expressions (RE) were proposed to restrict frequent sequences to the ones that satisfy user-specified constraints. In all of these languages, REs are applied over items, which limits their applicability in complex real-world situations. We propose a much powerful language, based on regular expressions, denoted RE-SPaM, where the basic elements are constraints defined over the (temporal and non-temporal) attributes of the items to be mined. Expressions in this language may include attributes, functions over attributes, and variables. We specify the syntax and semantics of RE-SPaM, and present a comprehensive set of examples to illustrate its expressive power. We study in detail how the expressions can be used to prune the resulting sequences in the mining process. In addition, we introduce techniques that allow pruning sequences in the early stages of the process, reducing the need to access the database, making use of the categorization of the attributes that compose the items, and of the automaton that accepts the language generated by the RE. Finally, we present experimental results. Although in this paper we focus on trajectory databases, our approach is general enough for being applied to other settings.

#index 1181260
#* Flexible and efficient querying and ranking on hyperlinked data sources
#@ Ramakrishna Varadarajan;Vagelis Hristidis;Louiqa Raschid;Maria-Esther Vidal;Luis Ibáñez;Héctor Rodríguez-Drumond
#t 2009
#c 8
#% 227883
#% 236409
#% 333854
#% 348173
#% 453464
#% 464825
#% 479803
#% 481602
#% 576214
#% 801673
#% 879574
#% 879576
#% 907437
#% 952823
#% 993987
#% 1015257
#% 1016176
#% 1206684
#% 1206702
#! There has been an explosion of hyperlinked data in many domains, e.g., the biological Web. Expressive query languages and effective ranking techniques are required to convert this data into browsable knowledge. We propose the Graph Information Discovery (GID) framework to support sophisticated user queries on a rich web of annotated and hyperlinked data entries, where query answers need to be ranked in terms of some customized ranking criteria, e.g., PageRank or ObjectRank. GID has a data model that includes a schema graph and a data graph, and an intuitive query interface. The GID framework allows users to easily formulate queries consisting of sequences of hard filters (selection predicates) and soft filters (ranking criteria); it can also be combined with other specialized graph query languages to enhance their ranking capabilities. GID queries have a well-defined semantics and are implemented by a set of physical operators, each of which produces a ranked result graph. We discuss rewriting opportunities to provide an efficient evaluation of GID queries. Soft filters are a key feature of GID and they are implemented using authority flow ranking techniques; these are query dependent rankings and are expensive to compute at runtime. We present approximate optimization techniques for GID soft filter queries based on the properties of random walks, and using novel path-length-bound and graph-sampling approximation techniques. We experimentally validate our optimization techniques on large biological and bibliographic datasets. Our techniques can produce high quality (Top K) answers with a savings of up to an order of magnitude, in comparison to the evaluation time for the exact solution.

#index 1181261
#* RankClus: integrating clustering with ranking for heterogeneous information network analysis
#@ Yizhou Sun;Jiawei Han;Peixiang Zhao;Zhijun Yin;Hong Cheng;Tianyi Wu
#t 2009
#c 8
#% 268079
#% 290830
#% 313959
#% 577273
#% 800607
#% 805896
#% 893124
#% 995140
#% 1016177
#% 1269973
#% 1663614
#! As information networks become ubiquitous, extracting knowledge from information networks has become an important task. Both ranking and clustering can provide overall views on information network data, and each has been a hot topic by itself. However, ranking objects globally without considering which clusters they belong to often leads to dumb results, e.g., ranking database and computer architecture conferences together may not make much sense. Similarly, clustering a huge number of objects (e.g., thousands of authors) in one huge cluster without distinction is dull as well. In this paper, we address the problem of generating clusters for a specified type of objects, as well as ranking information for all types of objects based on these clusters in a multi-typed (i.e., heterogeneous) information network. A novel clustering framework called RankClus is proposed that directly generates clusters integrated with ranking. Based on initial K clusters, ranking is applied separately, which serves as a good measure for each cluster. Then, we use a mixture model to decompose each object into a K-dimensional vector, where each dimension is a component coefficient with respect to a cluster, which is measured by rank distribution. Objects then are reassigned to the nearest cluster under the new measure space to improve clustering. As a result, quality of clustering and ranking are mutually enhanced, which means that the clusters are getting more accurate and the ranking is getting more meaningful. Such a progressive refinement process iterates until little change can be made. Our experiment results show that RankClus can generate more accurate clusters and in a more efficient way than the state-of-the-art link-based clustering methods. Moreover, the clustering results with ranks can provide more informative views of data compared with traditional clustering.

#index 1181262
#* Evaluating very large datalog queries on social networks
#@ Royi Ronen;Oded Shmueli
#t 2009
#c 8
#% 11797
#% 36683
#% 55408
#% 348183
#% 384978
#% 562464
#% 599549
#% 752731
#% 752760
#% 809267
#% 907435
#% 1016321
#% 1061896
#% 1095661
#% 1733563
#! We consider a near future scenario in which users of a Web 2.0 application, such as a social network, contribute to the application not only data, but also rules which automatically query, utilize and create the data. For example, a user of a social network can define rules that automatically manage the user's friends list, the sending of various announcements, filtering of messages and more. We examine the probable case of automated addition of connections by a participant. The connections to be added are defined using a query, associated to each participant. For this, we introduce and study the Query Network model, a graph-based model in which every node models a network participant and is associated with a Datalog rule. The union of all these individual user rules constitutes a very large, recursive, Datalog program whose size is of the order of magnitude of the size of the data being queried (data whose size in a social network can easily exceed 1TB). This greatly differs from the traditional assumption that queries are small and data are large. In particular, traditional optimizers will be hard pressed to handle such queries. This is the case even if queries are 'translated' to SQL (using views) and their union is transformed to a very large SQL query. We have designed, built and experimented with evaluation algorithms for such query networks. Experiments with both synthetic and real datasets demonstrate the usefulness and high effectiveness of our methods. Extensions to the model are proposed, their implementation and testing are the subject of on-going work.

#index 1181263
#* A sequential indexing scheme for flash-based embedded systems
#@ Shaoyi Yin;Philippe Pucheral;Xiaofeng Meng
#t 2009
#c 8
#% 91245
#% 131555
#% 320113
#% 322884
#% 340933
#% 893215
#% 907005
#% 951778
#% 957221
#% 960238
#% 963436
#% 985922
#% 1114598
#% 1117702
#! NAND Flash has become the most popular stable storage medium for embedded systems. As on-board storage capacity increases, the need for efficient indexing techniques arises. Such techniques are very challenging to design due to a combination of NAND Flash constraints (for example the block-erase-before-page-rewrite constraint and limited number of erase cycles) and embedded system constraints (for example tiny RAM and resource consumption predictability). Previous work adapted traditional indexing methods to cope with Flash constraints by deferring index updates using a log and batching them to decrease the number of rewrite operations in Flash memory. However, these methods were not designed with embedded system constraints in mind and do not address them. In this paper, we propose a new alternative for indexing Flash-resident data that specifically addresses the embedded context. This approach, called PBFilter, organizes the index structure in a purely sequential way. Key lookups are sped up thanks to two principles called Summarization and Partitioning. We instantiate these principles with data structures and algorithms based on Bloom Filters and show the effectiveness of this approach through a comprehensive performance study.

#index 1181264
#* Secondary bitmap indexes with vertical and horizontal partitioning
#@ Guadalupe Canahuate;Tan Apaydin;Ahmet Sacan;Hakan Ferhatosmanoglu
#t 2009
#c 8
#% 248814
#% 273904
#% 316523
#% 342735
#% 479808
#% 480329
#% 504155
#% 571294
#% 655987
#% 739871
#% 800526
#% 824697
#% 1008297
#% 1016130
#! Traditional bitmap indexes are utilized as a special type of primary or clustered indexes where the queries are answered by performing fast logical operations supported by hardware. Answers are mapped to the physical data by using the row id of each tuple. Bitmaps represent the i-th tuple in the original table with the i-th bit position of the index. Run-length compression is used to reduce the size of the bitmaps and it has been shown that ordered data is significantly better compressed. However, for large-scale and dynamic datasets it is infeasible to keep the data always sorted. Partitioning can be used to keep the data in smaller and manageable chunks, where a different bitmap index is built for each chunk. We propose a novel bitmap index design with partitioning which serves as basis for non-clustered bitmap indexes. Individual bitmaps are not stored, only an Existence Bitmap (EB) for the existing ranks of the full table is maintained. This approach improves update performance of sorted bitmaps and does not require maintaining a heap as the underlying table, nor the same ordering for all the partitions. A one dimensional index is used over the ranks to map the bits in the EB to the physical order of the data, which allows queries to run even faster. The proposed approach, called ranked Non-Clustered Bitmaps (rNCB), is compared against traditional bitmaps using FastBit and shows significant performance gains.

#index 1181265
#* Automating the loading of business process data warehouses
#@ Malu Castellanos;Alkis Simitsis;Kevin Wilkinson;Umeshwar Dayal
#t 2009
#c 8
#% 159337
#% 385321
#% 572314
#% 800609
#% 810038
#% 810078
#% 824935
#% 993956
#% 1016606
#% 1022296
#% 1068971
#% 1127403
#% 1131096
#% 1206731
#% 1688307
#! Business processes drive the operations of an enterprise. In the past, the focus was primarily on business process design, modeling, and automation. Recently, enterprises have realized that they can benefit tremendously from analyzing the behavior of their business processes with the objective of optimizing or improving them. In our research, we address the problem of warehousing business process execution data so that we can analyze their behavior using the analytic and reporting tools that are available in data warehouse environments. We build upon our previous work that described the design and implementation of a generic process data warehouse for use with any business processes. In this paper, we show how to automate the population of the generic process warehouse by tracking business events from an application environment. Typically, the source data consists of event streams that indicate changes in the business process state (i.e., progression of the process). The target schema is designed to allow querying of task and process execution data. The core of our approach for processing progression data relies on the construction of generic templates that specify the semantics of the event streams extraction and the subsequent transformations that translate the underlying IT events into business data changes. Using this extensible template mechanism, we show how to automate the construction of mappings to populate the generic process warehouse using two-levels of mappings that are applied in two-phases. Interestingly, our approach of using ETL technology for warehousing process data can be seen the other way around. An arbitrary ETL process can be modeled as a business process. Hence, we describe the benefit of modeling ETL as a business process and illustrate how to use our approach to warehouse ETL execution data, and to monitor and analyze the progress of ETL processes. Finally, we discuss implementation issues.

#index 1181266
#* Hiding distinguished ones into crowd: privacy-preserving publishing data with outliers
#@ Hui (Wendy) Wang;Ruilin Liu
#t 2009
#c 8
#% 248030
#% 300136
#% 300183
#% 300184
#% 321455
#% 427199
#% 443463
#% 478624
#% 481956
#% 570886
#% 576761
#% 577239
#% 729912
#% 800515
#% 801690
#% 810011
#% 864406
#% 864412
#% 881546
#% 881551
#% 893100
#% 960289
#% 1022247
#% 1063505
#! Publishing microdata raises concerns of individual privacy. When there exist outlier records in the microdata, the distinguishability of the outliers enables their privacy to be easier to be compromised than that of regular ones. However, none of the existing anonymization techniques can provide sufficient protection to the privacy of the outliers. In this paper, we study the problem of anonymizing the micro-data that contains outliers. We define the distinguishability-based attack by which the adversary can infer the existence of outliers as well as their private information from the anonymized microdata. To defend against the distinguishability-based attack, we define the plain k-anonymity as the privacy principle. Based on the definition, we categorize the outliers into two types, the ones that cannot be hidden by any plain k-anonymous group (called global outliers) and the ones that can (called local outliers). We propose the algorithm to efficiently anonymize local outliers with low information loss. Our experiments demonstrate the efficiency and effectiveness of our approach.

#index 1181267
#* An efficient online auditing approach to limit private data disclosure
#@ Haibing Lu;Yingjiu Li;Vijayalakshmi Atluri;Jaideep Vaidya
#t 2009
#c 8
#% 3421
#% 67453
#% 286825
#% 286828
#% 287297
#% 287795
#% 299344
#% 300184
#% 333876
#% 340475
#% 344105
#% 431425
#% 443478
#% 488743
#% 575966
#% 576762
#% 577239
#% 630970
#% 727904
#% 739061
#% 785363
#% 809244
#% 810010
#% 843878
#% 844360
#% 862156
#% 864412
#% 864880
#% 874893
#% 893101
#% 942359
#% 1014207
#% 1016172
#% 1018342
#% 1206679
#% 1393831
#! In a database system, disclosure of confidential private data may occur if users can put together the answers of past queries. Traditional access control mechanisms cannot guard against such breaches to private data. Online auditing techniques have been advanced to limit such disclosure of private data. Essentially, before answering any query, these techniques inspect the answers of the past queries to determine whether answering this query would compromise the stated data disclosure policies. While the primary requirement for online auditing is high efficiency, existing auditing approaches are expensive with respect to both computational time and space. Specifically, this cost is excessive in the general case of auditing arbitrary aggregate queries over real-valued confidential attributes with respect to interval-based privacy disclosure. In this paper, we model this problem as the well-studied linear programming (LP) problem and propose an efficient online auditing solution for constantly monitoring the bounds of protected attributes. The previously proposed approaches in this direction repetitively employ the LP. Consequently, for each new query, they require evaluation of the entire set of answers to past queries. In this paper, we propose a novel approach to employ LP that keeps the prior evaluation state in a concise form and conducts an incremental evaluation. Basically, our approach treats the online auditing problem as a series of updation problems. Each time when a new query is issued by a user, instead of solving a new LP problem with up-to-date log of all queries, we modify the existing bounds obtained in auditing previous queries based on certain rules so as to get the updated bounds with the new query added. Since it significantly reduces the computation time and storage space, our approach offers the first practical solution for the interval-based online auditing problem. Our experimental results demonstrate that our solution is about 30 times faster than the existing solutions.

#index 1181268
#* Continuous privacy preserving publishing of data streams
#@ Bin Zhou;Yi Han;Jian Pei;Bin Jiang;Yufei Tao;Yan Jia
#t 2009
#c 8
#% 248030
#% 321455
#% 338425
#% 378388
#% 379444
#% 427199
#% 443463
#% 576761
#% 576762
#% 577239
#% 800515
#% 801690
#% 810011
#% 824726
#% 864406
#% 864412
#% 874892
#% 874989
#% 881483
#% 881497
#% 881551
#% 893151
#% 960291
#% 982549
#% 1022264
#% 1022265
#% 1044457
#% 1206629
#% 1206632
#% 1700134
#! Recently, privacy preserving data publishing has received a lot of attention in both research and applications. Most of the previous studies, however, focus on static data sets. In this paper, we study an emerging problem of continuous privacy preserving publishing of data streams which cannot be solved by any straightforward extensions of the existing privacy preserving publishing methods on static data. To tackle the problem, we develop a novel approach which considers both the distribution of the data entries to be published and the statistical distribution of the data stream. An extensive performance study using both real data sets and synthetic data sets verifies the effectiveness and the efficiency of our methods.

#index 1181269
#* Top-k dominating queries in uncertain databases
#@ Xiang Lian;Lei Chen
#t 2009
#c 8
#% 213975
#% 333977
#% 654480
#% 654487
#% 659937
#% 772835
#% 810049
#% 824728
#% 824764
#% 864396
#% 874982
#% 893151
#% 893167
#% 960257
#% 976788
#% 1016191
#% 1022203
#% 1022226
#% 1022242
#% 1044478
#% 1063485
#% 1063520
#% 1166600
#% 1206645
#% 1206646
#% 1206732
#% 1206735
#% 1206781
#% 1408794
#% 1669490
#! Due to the existence of uncertain data in a wide spectrum of real applications, uncertain query processing has become increasingly important, which dramatically differs from handling certain data in a traditional database. In this paper, we formulate and tackle an important query, namely probabilistic top-k dominating (PTD) query, in the uncertain database. In particular, a PTD query retrieves k uncertain objects that are expected to dynamically dominate the largest number of uncertain objects. We propose an effective pruning approach to reduce the PTD search space, and present an efficient query procedure to answer PTD queries. Furthermore, approximate PTD query processing and the case where the PTD query is issued from an uncertain query object are also discussed. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed PTD query processing approaches.

#index 1181270
#* Evaluating probability threshold k-nearest-neighbor queries over uncertain data
#@ Reynold Cheng;Lei Chen;Jinchuan Chen;Xike Xie
#t 2009
#c 8
#% 232102
#% 427199
#% 442830
#% 527176
#% 654487
#% 772835
#% 824728
#% 864396
#% 893151
#% 893167
#% 893189
#% 981671
#% 1015297
#% 1016178
#% 1016196
#% 1016201
#% 1022203
#% 1022341
#% 1063485
#% 1063520
#% 1063568
#% 1081581
#% 1127376
#% 1127377
#% 1127408
#% 1127416
#% 1206716
#% 1408794
#! In emerging applications such as location-based services, sensor monitoring and biological management systems, the values of the database items are naturally imprecise. For these uncertain databases, an important query is the Probabilistic k-Nearest-Neighbor Query (k-PNN), which computes the probabilities of sets of k objects for being the closest to a given query point. The evaluation of this query can be both computationally- and I/O-expensive, since there is an exponentially large number of k object-sets, and numerical integration is required. Often a user may not be concerned about the exact probability values. For example, he may only need answers that have sufficiently high confidence. We thus propose the Probabilistic Threshold k-Nearest-Neighbor Query (T-k-PNN), which returns sets of k objects that satisfy the query with probabilities higher than some threshold T. Three steps are proposed to handle this query efficiently. In the first stage, objects that cannot constitute an answer are filtered with the aid of a spatial index. The second step, called probabilistic candidate selection, significantly prunes a number of candidate sets to be examined. The remaining sets are sent for verification, which derives the lower and upper bounds of answer probabilities, so that a candidate set can be quickly decided on whether it should be included in the answer. We also examine spatially-efficient data structures that support these methods. Our solution can be applied to uncertain data with arbitrary probability density functions. We have also performed extensive experiments to examine the effectiveness of our methods.

#index 1181271
#* PROUD: a probabilistic approach to processing similarity queries over uncertain data streams
#@ Mi-Yen Yeh;Kun-Lung Wu;Philip S. Yu;Ming-Syan Chen
#t 2009
#c 8
#% 248822
#% 578390
#% 654487
#% 659936
#% 823333
#% 824686
#% 824728
#% 864394
#% 864396
#% 893167
#% 907519
#% 960257
#% 977008
#% 993961
#% 1016201
#% 1016202
#% 1022203
#% 1127397
#% 1206645
#% 1206646
#% 1206689
#% 1206690
#% 1206714
#% 1206716
#% 1206717
#% 1206732
#% 1688247
#! We present PROUD -- A PRObabilistic approach to processing similarity queries over Uncertain Data streams, where the data streams here are mainly time series streams. In contrast to data with certainty, an uncertain series is an ordered sequence of random variables. The distance between two uncertain series is also a random variable. We use a general uncertain data model, where only the mean and the deviation of each random variable at each timestamp are available. We derive mathematical conditions for progressively pruning candidates to reduce the computation cost. We then apply PROUD to a streaming environment where only sketches of streams, like wavelet synopses, are available. Extensive experiments are conducted to evaluate the effectiveness of PROUD and compare it with Det, a deterministic approach that directly processes data without considering uncertainty. The results show that, compared with Det, PROUD offers a flexible trade-off between false positives and false negatives by controlling a threshold, while maintaining a similar computation cost. In contrast, Det does not provide such flexibility. This trade-off is important as in some applications false negatives are more costly, while in others, it is more critical to keep the false positives low.

#index 1181272
#* Fair, effective, efficient and differentiated scheduling in an enterprise data warehouse
#@ Chetan Gupta;Abhay Mehta;Song Wang;Umesh Dayal
#t 2009
#c 8
#% 31408
#% 280450
#% 282614
#% 338364
#% 347216
#% 361248
#% 379461
#% 580692
#% 580973
#% 593936
#% 750148
#% 864447
#% 879524
#% 893131
#% 960280
#% 963907
#% 1206825
#! A typical online Business Intelligence (BI) workload consists of a combination of short, less intensive queries, along with long, resource intensive queries. As such, the longest queries in a typical BI workload may take several orders of magnitude more time to execute, compared with the shortest queries in the workload. This makes it challenging to design a good Mixed Workload Scheduler (MWS). In this paper we first define the design criteria that make a 'good' MWS. We then use these criteria to design rFEED, a MWS that is fair, effective, efficient, and differentiated. We simulate real workloads and compare our rFEED MWS with models of the current best of breed commercial systems. We show that the rFEED MWS works extremely well.

#index 1181273
#* Efficient identification of starters and followers in social media
#@ Michael Mathioudakis;Nick Koudas
#t 2009
#c 8
#% 61018
#% 256609
#% 309748
#% 342596
#% 480328
#% 729923
#% 754107
#% 832271
#% 847218
#% 1019178
#% 1022269
#% 1035587
#% 1055736
#% 1083675
#! Activity and user engagement in social media such as web logs, wikis, online forums or social networks has been increasing at unprecedented rates. In relation to social behavior in various human activities, user activity in social media indicates the existence of individuals that consistently drive or stimulate 'discussions' in the online world. Such individuals are considered as 'starters' of online discussions in contrast with 'followers' that primarily engage in discussions and follow them. In this paper, we formalize notions of 'starters' and 'followers' in social media. Motivated by the challenging size of the available information related to online social behavior, we focus on the development of random sampling approaches allowing us to achieve significant efficiency while identifying starters and followers. In our experimental section we utilize BlogScope, our social media warehousing platform under development at the University of Toronto. We demonstrate the scalability and accuracy of our sampling approaches using real data establishing the practical utility of our techniques in a real social media warehousing environment.

#index 1181274
#* A data damage tracking quarantine and recovery (DTQR) scheme for mission-critical database systems
#@ Kun Bai;Peng Liu
#t 2009
#c 8
#% 9241
#% 114582
#% 235643
#% 300458
#% 443542
#% 488472
#% 507694
#% 583844
#% 592667
#% 646035
#% 664674
#% 800604
#% 844725
#% 874998
#% 904964
#% 913931
#% 915319
#% 1711256
#! Database security research aims to protect a database from unintended activities, such as authenticated misuse, malicious attacks. In recent years, surviving DBMS from an attack is becoming even more crucial because networks have become more open and the increasingly critical role that database servers are playing nowadays. Unlike the traditional database failure/attack recovery mechanisms, in this paper, we propose a light-weight dynamic Data Damage Tracking, Quarantine, and Recovery (DTQR) solution. We built the DTQR scheme into the kernel of PostgreSQL. We comprehensively study this approach from a few aspects (e.g., system overhead, impact of the intrusion detection system), and the experimental results demonstrated that our DTQR can sustain an excellent data service while healing the database server when it is under a malicious attack.

#index 1181275
#* Unrestricted wavelet synopses under maximum error bound
#@ Chaoyi Pang;Qing Zhang;David Hansen;Anthony Maeder
#t 2009
#c 8
#% 248822
#% 257637
#% 273902
#% 397389
#% 801684
#% 823333
#% 824685
#% 824686
#% 847111
#% 989609
#% 1016154
#% 1397447
#% 1702985
#% 1817010
#! Constructing Haar wavelet synopses under a given approximation error has many real world applications. In this paper, we take a novel approach towards constructing unrestricted Haar wavelet synopses under an error bound on uniform norm (L∞). We provide two approximation algorithms which both have linear time complexity and a (log N)-approximation ratio. The space complexities of these two algorithms are O (log N) and O (N) respectively. These two algorithms have the advantage of being both simple in structure and naturally adaptable for stream data processing. Unlike traditional approaches for synopses construction that rely heavily on examining wavelet coefficients and their summations, the proposed construction methods solely depend on examining the original data and are extendable to other findings. Extensive experiments indicate that these techniques are highly practical and surpass related ones in both efficiency and effectiveness.

#index 1181276
#* Distributed similarity search in high dimensions using locality sensitive hashing
#@ Parisa Haghani;Sebastian Michel;Karl Aberer
#t 2009
#c 8
#% 86786
#% 248796
#% 328797
#% 340175
#% 340176
#% 427199
#% 443243
#% 479973
#% 480632
#% 646221
#% 762054
#% 770901
#% 783519
#% 805905
#% 814646
#% 823842
#% 847166
#% 864421
#% 866709
#% 898309
#% 960252
#% 996715
#% 1022281
#% 1022284
#% 1206695
#% 1408703
#% 1688254
#% 1711096
#! In this paper we consider distributed K-Nearest Neighbor (KNN) search and range query processing in high dimensional data. Our approach is based on Locality Sensitive Hashing (LSH) which has proven very efficient in answering KNN queries in centralized settings. We consider mappings from the multi-dimensional LSH bucket space to the linearly ordered set of peers that jointly maintain the indexed data and derive requirements to achieve high quality search results and limit the number of network accesses. We put forward two such mappings that come with these salient properties: being locality preserving so that buckets likely to hold similar data are stored on the same or neighboring peers and having a predictable output distribution to ensure fair load balancing. We show how to leverage the linearly aligned data for efficient KNN search and how to efficiently process range queries which is, to the best of our knowledge, not possible in existing LSH schemes. We show by comprehensive performance evaluations using real world data that our approach brings major performance and accuracy gains compared to state-of-the-art.

#index 1181277
#* Multiplicative synopses for relative-error metrics
#@ Panagiotis Karras
#t 2009
#c 8
#% 210190
#% 248822
#% 273901
#% 273902
#% 273903
#% 299982
#% 333872
#% 333946
#% 333947
#% 333948
#% 347226
#% 378404
#% 397385
#% 399763
#% 411355
#% 413604
#% 427219
#% 453491
#% 479648
#% 481266
#% 482092
#% 492932
#% 494333
#% 572308
#% 742562
#% 802243
#% 803119
#% 824686
#% 850727
#% 864426
#% 866990
#% 893160
#% 943617
#% 956456
#% 989571
#% 989609
#% 1016153
#% 1016154
#% 1072646
#% 1126563
#% 1676483
#% 1688247
#% 1702985
#% 1817010
#! Existing hierarchical summarization techniques fail to provide synopses good in terms of relative-error metrics. This paper introduces multiplicative synopses: a summarization paradigm tailored for effective relative-error summarization. This paradigm is inspired from previous hierarchical index-based summarization schemes, but goes beyond them by altering their underlying data representation mechanism. Existing schemes have decomposed the summarized data based on sums and differences of values, resulting in what we call additive synopses. We argue that the incapacity of these models to handle relative-error metrics stems exactly from this additive nature of their representation mechanism. We substitute this additive nature by a multiplicative one. We argue that this is more appropriate for achieving low-relative-error data approximations. We develop an efficient linear-time dynamic programming scheme for one-dimensional multiplicative synopsis construction under general relative-error-based metrics, and a special scheme for the case of maximum relative error. We generalize our schemes to higher data dimensionality and we show a surprising additional benefit gained by our special scheme for maximum relative error in this case. In our experimental study, we verify the higher efficacy of our model on relative-error-oriented summarization problems.

#index 1181278
#* LCS-Hist: taming massive high-dimensional data cube compression
#@ Alfredo Cuzzocrea;Paolo Serafino
#t 2009
#c 8
#% 43163
#% 211575
#% 259995
#% 273887
#% 300193
#% 333947
#% 397388
#% 482092
#% 593113
#% 660006
#% 801684
#% 839161
#% 852296
#% 949148
#% 956455
#% 960294
#% 964808
#! The problem of efficiently compressing massive high-dimensional data cubes still waits for efficient solutions capable of overcoming well-recognized scalability limitations of state-of-the-art histogram-based techniques, which perform well on small-in-size low-dimensional data cubes, whereas their performance in both representing the input data domain and efficiently supporting approximate query answering against the generated compressed data structure decreases dramatically when data cubes grow in dimension number and size. To overcome this relevant research challenge, in this paper we propose LCS-Hist, an innovative multidimensional histogram devising a complex methodology that combines intelligent data modeling and processing techniques in order to tame the annoying problem of compressing massive high-dimensional data cubes. With respect to similar histogram-based proposals, our technique introduces (i) a surprising consumption of the storage space available to house the compressed representation of the input data cube, and (ii) a superior scalability on high-dimensional data cubes. Finally, several experimental results performed against various classes of data cubes confirm the advantages of LCS-Hist, even in comparison with those given by state-of-the-art similar techniques.

#index 1181279
#* Caching content-based queries for robust and efficient image retrieval
#@ Fabrizio Falchi;Claudio Lucchese;Salvatore Orlando;Raffaele Perego;Fausto Rabitti
#t 2009
#c 8
#% 294634
#% 296646
#% 342827
#% 342828
#% 465014
#% 479462
#% 479649
#% 577302
#% 645687
#% 679321
#% 731409
#% 818938
#% 857113
#% 860861
#% 990329
#% 1026892
#% 1040539
#% 1131117
#% 1834787
#! In order to become an effective complement to traditional Web-scale text-based image retrieval solutions, content-based image retrieval must address scalability and efficiency issues. In this paper we investigate the possibility of caching the answers to content-based image retrieval queries in metric space, with the aim of reducing the average cost of query processing, and boosting the overall system throughput. Our proposal exploits the similarity between the query object and the cache content, and allows the cache to return approximate answers with acceptable quality guarantee even if the query processed has never been encountered in the past. Moreover, since popular images that are likely to be used as query have several near-duplicate versions, we show that our caching algorithm is robust, and does not suffer of cache pollution problems due to near-duplicate query objects. We report on very promising results obtained with a collection of one million high-quality digital photos. We show that it is worth pursuing caching strategies also in similarity search systems, since the proposed caching techniques can have a significant impact on performance, like caching on text queries has been proven effective for traditional Web search engines.

#index 1181280
#* An approach to detecting relevant updates to cached data using XML and active databases
#@ Essam Mansour;Hagen Höpfner
#t 2009
#c 8
#% 23898
#% 268764
#% 279206
#% 343184
#% 411644
#% 599549
#% 743286
#% 923676
#% 926886
#% 1712571
#! Client/server information systems use caching techniques to reduce the volume of transmitted data as well as response time and, especially in the case of systems with mobile clients, to reduce energy consumptions. Updating the server database might cause inconsistencies between server data and cached data. Guaranteeing consistency at least demands to invalidate outdated caches. To avoid invalidation of caches that are not affected by a particular update one must check the relevancy of each update for each cache. It has been proven, that this can only be done on a stateful server. This paper presents the purely database system (DBS) based DRUpE method for checking the relevance of server side updates to cached data by analyzing the intersection between modified data and cached data. A non-empty intersection means that the update operations are relevant to the cached data. The necessary cache descriptions are stored in form of XML-documents inside the DBS. The paper introduces the used XML-model XReal as well as the relevancy proof-of-concept system Uptime. The main contribution of our work is that the system utilizes the DBS utilities to detect update relevance, notify clients and manage the required repository of the queries issued by the clients. Hence, no additional middleware is required in order to realize consistency aware client/server information systems, even if clients are small footprinted mobile devices.

#index 1181281
#* Self-tuning query mesh for adaptive multi-route query processing
#@ Rimma V. Nehme;Elke A. Rundensteiner;Elisa Bertino
#t 2009
#c 8
#% 115608
#% 145196
#% 191910
#% 248793
#% 285924
#% 300167
#% 338425
#% 341700
#% 342600
#% 345857
#% 376266
#% 378388
#% 378408
#% 383470
#% 397352
#% 397353
#% 420109
#% 435117
#% 458245
#% 458300
#% 480803
#% 480940
#% 715955
#% 729932
#% 742047
#% 745441
#% 765435
#% 785131
#% 824714
#% 824795
#% 864485
#% 1015282
#% 1016208
#% 1016269
#% 1026989
#% 1422718
#% 1673620
#! In real-life applications, different subsets of data may have distinct statistical properties, e.g., various websites may have diverse visitation rates, different categories of stocks may have dissimilar price fluctuation patterns. For such applications, it can be fruitful to eliminate the commonly made single execution plan assumption and instead execute a query using several plans, each optimally serving a subset of data with particular statistical properties. Furthermore, in dynamic environments, data properties may change continuously, thus calling for adaptivity. The intriguing question is: can we have an execution strategy that (1) is plan-based to leverage on all the benefits of traditional plan-based systems, (2) supports multiple plans each customized for different subset of data, and yet (3) is as adaptive as "plan-less" systems like Eddies? While the recently proposed Query Mesh (QM) approach provides a foundation for such an execution paradigm, it does not address the question of adaptivity required for highly dynamic environments. In this work, we fill this gap by proposing a Self-Tuning Query Mesh (ST-QM) --- an adaptive solution for content-based multi-plan execution engines. ST-QM addresses adaptive query processing by abstracting it as a concept drift problem --- a well-known subject in machine learning. Such abstraction allows to discard adaptivity candidates (i.e., the cases indicating a change in the environment) early in the process if they are insignificant or not "worthwhile" to adapt to, and thus minimize the adaptivity overhead. A unique feature of our aproach is that all logical transformations to the execution strategy get translated into a single inexpensive physical operation --- the classifier change. Our experimental evaluation using a continuous query engine shows the performance benefits of ST-QM approach over the alternatives, namely the non-adaptive and the Eddies-based solutions.

#index 1181282
#* Retrieving meaningful relaxed tightest fragments for XML keyword search
#@ Lingbo Kong;Rémi Gilleron;Aurélien Lemay Mostrare
#t 2009
#c 8
#% 186
#% 47710
#% 282469
#% 309726
#% 330627
#% 340911
#% 340914
#% 387427
#% 397366
#% 654442
#% 754116
#% 810052
#% 824703
#% 855859
#% 863389
#% 956599
#% 960261
#% 1015258
#% 1016135
#% 1019060
#% 1019187
#% 1044480
#% 1063493
#% 1063537
#% 1127424
#% 1206722
#% 1206957
#% 1408825
#% 1669517
#! Adapting keyword search to XML data has been attractive recently, generalized as XML keyword search (XKS). One of its key tasks is to return the meaningful fragments as the result. [1] is the latest work following this trend, and it focuses on returning the fragments rooted at SLCA (Smallest LCA -- Lowest Common Ancestor) nodes. To guarantee that the fragments only contain interesting nodes, [1] proposes a contributor-based filtering mechanism in its MaxMatch algorithm. However, the filtering mechanism is not sufficient. It will commit the false positive problem (discarding interesting nodes) and the redundancy problem (keeping uninteresting nodes). In this paper, our interest is to propose a framework of retrieving meaningful fragments rooted at not only the SLCA nodes, but all LCA nodes. We begin by introducing the concept of Relaxed Tightest Fragment (RTF) as the basic result type. Then we propose a new filtering mechanism to overcome those two problems in Max-Match. Its kernel is the concept of valid contributor, which helps to distinguish the interesting children of a node. The new filtering mechanism is then to prune the nodes in a RTF which are not valid contributors to their parents. Based on the valid contributor concept, our ValidRTF algorithm not only overcomes those two problems in MaxMatch, but also satisfies the axiomatic properties deduced in [1] that an XKS technique should satisfy. We compare ValidRTF with MaxMatch on real and synthetic XML data. The result verifies our claims, and shows the effectiveness of our valid-contributor-based filtering mechanism.

#index 1181283
#* Approximate substring selectivity estimation
#@ Hongrae Lee;Raymond T. Ng;Kyuseok Shim
#t 2009
#c 8
#% 210189
#% 235941
#% 243166
#% 273705
#% 299984
#% 311808
#% 341144
#% 387427
#% 577238
#% 616528
#% 654454
#% 654467
#% 745489
#% 824684
#% 824717
#% 893164
#% 956458
#% 1016219
#% 1022218
#% 1022227
#! We study the problem of estimating selectivity of approximate substring queries. Its importance in databases is ever increasing as more and more data are input by users and are integrated with many typographical errors and different spelling conventions. To begin with, we consider edit distance for the similarity between a pair of strings. Based on information stored in an extended N-gram table, we propose two estimation algorithms, MOF and LBS for the task. The latter extends the former with ideas from set hashing signatures. The experimental results show that MOF is a light-weight algorithm that gives fairly accurate estimations. However, if more space is available, LBS can give better accuracy than MOF and other baseline methods. Next, we extend the proposed solution to other similarity predicates, SQL LIKE operator and Jaccard similarity.

#index 1181284
#* Finding frequent co-occurring terms in relational keyword search
#@ Yufei Tao;Jeffrey Xu Yu
#t 2009
#c 8
#% 273908
#% 273909
#% 333854
#% 654442
#% 660011
#% 810052
#% 824693
#% 874894
#% 875017
#% 875018
#% 960243
#% 960245
#% 960259
#% 960284
#% 960285
#% 993987
#% 1015325
#% 1016176
#% 1206801
#! Given a set Q of keywords, conventional keyword search (KS) returns a set of tuples, each of which (i) is obtained from a single relation, or by joining multiple relations, and (ii) contains all the keywords in Q. This paper proposes a relevant problem called frequent co-occurring term (FCT) retrieval. Specifically, given a keyword set Q and an integer k, a FCT query reports the k terms that are not in Q, but appear most frequently in the result of a KS query with the same Q. FCT search is able to discover the concepts that are closely related to Q. Furthermore, it is also an effective tool for refining the keyword set Q of traditional keyword search. While a FCT query can be trivially supported by solving the corresponding KS query, we provide a faster algorithm that extracts the correct results without evaluating any KS query at all. The effectiveness and efficiency of our techniques are verified with extensive experiments on real data.

#index 1181285
#* Time-completeness trade-offs in record linkage using adaptive query processing
#@ Roald Lengu;Paolo Missier;Alvaro A. A. Fernandes;Giovanna Guerrini;Marco Mesiti
#t 2009
#c 8
#% 136740
#% 172902
#% 340635
#% 452359
#% 454618
#% 480496
#% 480499
#% 480654
#% 659991
#% 765434
#% 765456
#% 800590
#% 824787
#% 864392
#% 864551
#% 903332
#% 913783
#% 960301
#% 1026989
#% 1684636
#% 1688293
#% 1728700
#! Applications that involve data integration among multiple sources often require a preliminary step of data reconciliation in order to ensure that tuples match correctly across the sources. In dynamic settings such as data mashups, however, traditional offline data reconciliation techniques that require prior availability of the data may not be applicable. The alternative, performing similarity joins at query time, is computationally expensive, while ignoring the mismatch problem altogether leads to an incomplete integration. In this paper we make the assumption that, in some dynamic integration scenarios, users may agree to trade the completeness of a join result in return for a faster computation. We explore the consequences of this assumption by proposing a novel, hybrid join algorithm that involves a combination of exact and approximate join operators, managed using adaptive query processing techniques. The algorithm is optimistic: it can switch between physical join operators multiple times throughout query processing, but it only resorts to approximate join operators when there is statistical evidence that result completeness is compromised. Our experiments show that sensible savings in join execution time can be achieved in practice, at the expense of a modest reduction in result completeness.

#index 1181286
#* Interactive query refinement
#@ Chaitanya Mishra;Nick Koudas
#t 2009
#c 8
#% 7492
#% 137885
#% 152585
#% 210182
#% 227894
#% 273908
#% 288975
#% 442902
#% 462772
#% 643566
#% 765456
#% 864451
#% 893105
#% 893172
#% 893748
#% 902467
#% 1015317
#% 1016203
#% 1063507
#% 1127404
#! We investigate the problem of refining SQL queries to satisfy cardinality constraints on the query result. This has applications to the many/few answers problems often faced by database users. We formalize the problem of query refinement and propose a framework to support it in a database system. We introduce an interactive model of refinement that incorporates user feedback to best capture user preferences. Our techniques are designed to handle queries having range and equality predicates on numerical and categorical attributes. We present an experimental evaluation of our framework implemented in an open source data manager and demonstrate the feasibility and practical utility of our approach.

#index 1181287
#* Continuous probabilistic nearest-neighbor queries for uncertain trajectories
#@ Goce Trajcevski;Roberto Tamassia;Hui Ding;Peter Scheuermann;Isabel F. Cruz
#t 2009
#c 8
#% 103743
#% 115996
#% 201876
#% 211801
#% 287466
#% 295512
#% 417825
#% 458857
#% 527176
#% 574283
#% 579313
#% 657739
#% 749907
#% 765453
#% 771228
#% 772835
#% 800571
#% 800572
#% 810120
#% 835018
#% 871761
#% 879210
#% 879211
#% 893092
#% 919460
#% 983259
#% 993955
#% 1058620
#% 1063595
#% 1156044
#% 1206716
#! This work addresses the problem of processing continuous nearest neighbor (NN) queries for moving objects trajectories when the exact position of a given object at a particular time instant is not known, but is bounded by an uncertainty region. As has already been observed in the literature, the answers to continuous NN-queries in spatio-temporal settings are time parameterized in the sense that the objects in the answer vary over time. Incorporating uncertainty in the model yields additional attributes that affect the semantics of the answer to this type of queries. In this work, we formalize the impact of uncertainty on the answers to the continuous probabilistic NN-queries, provide a compact structure for their representation and efficient algorithms for constructing that structure. We also identify syntactic constructs for several qualitative variants of continuous probabilistic NN-queries for uncertain trajectories and present efficient algorithms for their processing.

#index 1181288
#* Reverse k-nearest neighbor search in dynamic and general metric databases
#@ Elke Achtert;Hans-Peter Kriegel;Peer Kröger;Matthias Renz;Andreas Züfle
#t 2009
#c 8
#% 86950
#% 201876
#% 300163
#% 333977
#% 427199
#% 465009
#% 479462
#% 527189
#% 730019
#% 875013
#% 889094
#% 907572
#% 1016191
#! In this paper, we propose an original solution for the general reverse k-nearest neighbor (RkNN) search problem. Compared to the limitations of existing methods for the RkNN search, our approach works on top of any hierarchically organized tree-like index structure and, thus, is applicable to any type of data as long as a metric distance function is defined on the data objects. We will exemplarily show how our approach works on top of the most prevalent index structures for Euclidean and metric data, the R-Tree and the M-Tree, respectively. Our solution is applicable for arbitrary values of k and can also be applied in dynamic environments where updates of the database frequently occur. Although being the most general solution for the RkNN problem, our solution outperforms existing methods in terms of query execution times because it exploits different strategies for pruning false drops and identifying true hits as soon as possible.

#index 1181289
#* Top-k dominant web services under multi-criteria matching
#@ Dimitrios Skoutas;Dimitris Sacharidis;Alkis Simitsis;Verena Kantere;Timos Sellis
#t 2009
#c 8
#% 2115
#% 232703
#% 288976
#% 340936
#% 387427
#% 413613
#% 420464
#% 465167
#% 480671
#% 519428
#% 577335
#% 767403
#% 806212
#% 875012
#% 879582
#% 890344
#% 901931
#% 912241
#% 914650
#% 987266
#% 987288
#% 993954
#% 1004294
#% 1016160
#% 1022203
#% 1022225
#% 1022242
#% 1063559
#% 1092017
#% 1408811
#% 1561977
#% 1674907
#! As we move from a Web of data to a Web of services, enhancing the capabilities of the current Web search engines with effective and efficient techniques for Web services retrieval and selection becomes an important issue. Traditionally, the relevance of a Web service advertisement to a service request is determined by computing an overall score that aggregates individual matching scores among the various parameters in their descriptions. Two drawbacks characterize such approaches. First, there is no single matching criterion that is optimal for determining the similarity between parameters. Instead, there are numerous approaches ranging from using Information Retrieval similarity metrics up to semantic logic-based inference rules. Second, the reduction of individual scores to an overall similarity leads to significant information loss. Since there is no consensus on how to weight these scores, existing methods are typically pessimistic, adopting a worst-case scenario. As a consequence, several services, e.g., those having a single unrelated parameter, can be excluded from the result set, even though they are potentially good alternatives. In this work, we present a methodology that overcomes both deficiencies. Given a request, we introduce an objective measure that assigns a dominance score to each advertised Web service. This score takes into consideration all the available criteria for each parameter in the request. We investigate three distinct definitions of dominance score, and we devise efficient algorithms that retrieve the top-k most dominant Web services in each case. Extensive experimental evaluation on real requests and relevance sets, as well as on synthetically generated scenarios, demonstrates both the effectiveness of the proposed technique and the efficiency of the algorithms.

#index 1181290
#* Ranking objects based on relationships and fixed associations
#@ Albert Angel;Surajit Chaudhuri;Gautam Das;Nick Koudas
#t 2009
#c 8
#% 273909
#% 333854
#% 810018
#% 874975
#% 875001
#% 893128
#% 912239
#% 1015317
#% 1022277
#% 1022338
#% 1063474
#% 1063570
#% 1063713
#! Text corpora are often enhanced by additional metadata which relate real-world entities, with each document in which such entities are discussed. Such relationships are typically obtained through widely available Information Extraction tools. At the same time, interesting known associations typically hold among these entities. For instance, a corpus might contain discussions on hotels, cities and airlines; fixed associations among these entities may include: airline A operates a flight to city C, hotel H is located in city C. A plethora of applications necessitate the identification of associated entities, each best matching a given set of keywords. Consider the sample query: Find a holiday package in a "pet-friendly" hotel, located in a "historical" yet "lively" city, with travel operated by an "economical" and "safe" airline. These keywords are unlikely to occur in the textual description of entities themselves, (e.g., the actual hotel name or the city name or the airline name). Consequently to answer such queries, one needs to exploit both relationships between entities and documents (e.g., keyword "pet-friendly" occurs in a document that contains an entity specifying a hotel name H), and the known associations between entities (e.g., hotel H is located in city C). In this work, we focus on the class of "entity package finder" queries outlined above. We demonstrate that existing techniques cannot be efficiently adapted to solve this problem, as the resulting algorithm relies on estimations with excessive runtime and/or storage overheads. We propose an efficient algorithm to process such queries, over large corpora. We devise early pruning and termination strategies, in the presence of joins and aggregations (executed on entities extracted from text), that do not depend on any estimates. Our analysis and experimental evaluation on real and synthetic data demonstrates the efficiency and scalability of our approach.

#index 1181291
#* Towards integrated and efficient scientific sensor data processing: a database approach
#@ Ji Wu;Yongluan Zhou;Karl Aberer;Kian-Lee Tan
#t 2009
#c 8
#% 32893
#% 210184
#% 435138
#% 461921
#% 482106
#% 572268
#% 619859
#% 810029
#% 818938
#% 874976
#% 875016
#% 1016206
#% 1060252
#% 1063468
#% 1063518
#% 1389159
#! In this work, we focus on managing scientific environmental data, which are measurement readings collected from wireless sensors. In environmental science applications, raw sensor data often need to be validated, interpolated, aligned and aggregated before being used to construct meaningful result sets. Due to the lack of a system that integrates all the necessary processing steps, scientists often resort to multiple tools to manage and process the data, which can severely affect the efficiency of their work. In this paper, we propose a new data processing framework, HyperGrid, to address the problem. HyperGrid adopts a generic data model and a generic query processing and optimization framework. It offers an integrated environment to store, query, analyze and visualize scientific datasets. The experiments on real query set and data set show that the framework not only introduces little processing overhead, but also provides abundant opportunities to optimize the processing cost and thus significantly enhances the processing efficiency.

#index 1181292
#* Flexible and scalable storage management for data-intensive stream processing
#@ Irina Botan;Gustavo Alonso;Peter M. Fischer;Donald Kossmann;Nesime Tatbul
#t 2009
#c 8
#% 654497
#% 660004
#% 726621
#% 726622
#% 788216
#% 810063
#% 914605
#% 993385
#% 1016169
#% 1022208
#! Data Stream Management Systems (DSMS) operate under strict performance requirements. Key to meeting such requirements is to efficiently handle time-critical tasks such as managing internal states of continuous query operators, traffic on the queues between operators, as well as providing storage support for shared computation and archived data. In this paper, we introduce a general purpose storage management framework for DSMSs that performs these tasks based on a clean, loosely-coupled, and flexible system design that also facilitates performance optimization. An important contribution of the framework is that, in analogy to buffer management techniques in relational database systems, it uses information about the access patterns of streaming applications to tune and customize the performance of the storage manager. In the paper, we first analyze typical application requirements at different granularities in order to identify important tunable parameters and their corresponding values. Based on these parameters, we define a general-purpose storage management interface. Using the interface, a developer can use our SMS (Storage Manager for Streams) to generate a customized storage manager for streaming applications. We explore the performance and potential of SMS through a set of experiments using the Linear Road benchmark.

#index 1181293
#* A view selection algorithm with performance guarantee
#@ Nicolas Hanusse;Sofian Maabout;Radu Tofan
#t 2009
#c 8
#% 210182
#% 248791
#% 273697
#% 420053
#% 420062
#% 462204
#% 465003
#% 479646
#% 481290
#% 579314
#% 841959
#% 1016173
#% 1016602
#% 1044461
#! A view selection algorithm takes as input a fact table and computes a set of views to store in order to speed up queries. The performance of view selection algorithm is usually measured by three criteria: (1) the amount of memory to store the selected views, (2) the query response time and (3) the time complexity of this algorithm. The two first measurements deal with the output of the algorithm. No existing solutions give good trade-off between amount of memory and queries cost with a small time complexity. We propose in this paper an algorithm guaranteeing a constant approximation factor of queries response time with respect to the optimal solution. Moreover, the time complexity for a D-dimensional fact table is O (D * 2D) corresponding to the fastest known algorithm. We provide an experimental comparison with two other well known algorithms showing that our approach also gives good performance in terms of memory.

#index 1181294
#* Efficient provenance storage over nested data collections
#@ Manish Kumar Anand;Shawn Bowers;Timothy McPhillips;Bertram Ludäscher
#t 2009
#c 8
#% 384978
#% 462212
#% 480659
#% 570878
#% 577523
#% 769242
#% 806211
#% 825661
#% 874898
#% 875015
#% 879809
#% 960258
#% 976789
#% 1004604
#% 1034470
#% 1042654
#% 1042656
#% 1042658
#% 1051708
#% 1059972
#% 1063544
#% 1063545
#% 1063593
#% 1126775
#% 1174010
#% 1179173
#% 1206750
#% 1270147
#% 1396743
#% 1671611
#% 1692849
#% 1728173
#% 1728174
#% 1728175
#! Scientific workflow systems are increasingly used to automate complex data analyses, largely due to their benefits over traditional approaches for workflow design, optimization, and provenance recording. Many workflow systems employ a simple dependency model to represent the provenance of data produced by workflow runs. Although commonly adopted, this model does not capture explicit data dependencies introduced by "provenance-aware" processes, and it can lead to inefficient storage when workflow data is complex or structured. We present a provenance model, extending the conventional approach, that supports (i) explicit data dependencies and (ii) nested data collections. Our model adopts techniques from reference-based XML versioning, adding annotations for process and data dependencies. We present strategies and reduction techniques to store immediate and transitive provenance information within our model, and examine trade-offs among update time, storage size, and query response time. We evaluate our approach on real-world and synthetic workflow execution traces, demonstrating significant reductions in storage size, while also reducing the time required to store and query provenance information.

#index 1181295
#* Schema-conscious filtering of XML documents
#@ Panu Silvasti;Seppo Sippu;Eljas Soisalon-Soininen
#t 2009
#c 8
#% 321327
#% 462235
#% 464724
#% 480296
#% 654476
#% 730053
#% 731408
#% 791182
#% 974626
#% 1127624
#! In a publish-subscribe system based on filtering of XML documents, subscribers specify their interests with profiles expressed in the XPath language. The system processes a stream of XML documents and delivers to subscribers a notification or content of documents that match the profiles. For filtering with profiles expressed as linear XPath queries, automaton-based approaches exist where the intractable size growth of a preconstructed deterministic finite automaton is avoided by using a nondeterministic automaton. In this article we examine how these general approaches, which do not assume the existence of any specific schema or document type definition (DTD), might benefit from the knowledge that all the XML documents to be filtered obey a given DTD. We present an algorithm that utilizes the DTD in the preprocessing phase of the filtering automaton to prune out descendant operators (//) and wildcards (*) from the linear XPath filters. Experiments with data obtained from the XML Data Repository of the Univ. of Washington indicate that filter pruning can increase the throughput of the nondeterministic YFilter automaton by Diao et al. by a factor of 2 to 20. We also present a new filtering algorithm that is based on a backtracking deterministic finite automaton derived from the classic Aho--Corasick pattern-matching automaton. This automaton has a size linear in the sum of the sizes of the filters. For our algorithm, we obtained a throughput of 15 MB/sec for filters pruned from one million original filters (with all wildcards and non-leading descendant operators eliminated), representing an improvement by a factor of 2 to 3 upon the throughput of YFilter.

#index 1181296
#* Provenance for nested subqueries
#@ Boris Glavic;Gustavo Alonso
#t 2009
#c 8
#% 248014
#% 287005
#% 318704
#% 320062
#% 415987
#% 564426
#% 810115
#% 825661
#% 960323
#% 976987
#% 983262
#% 1092014
#% 1206861
#! Data provenance is essential in applications such as scientific computing, curated databases, and data warehouses. Several systems have been developed that provide provenance functionality for the relational data model. These systems support only a subset of SQL, a severe limitation in practice since most of the application domains that benefit from provenance information use complex queries. Such queries typically involve nested subqueries, aggregation and/or user defined functions. Without support for these constructs, a provenance management system is of limited use. In this paper we address this limitation by exploring the problem of provenance derivation when complex queries are involved. More precisely, we demonstrate that the widely used definition of Why-provenance fails in the presence of nested subqueries, and show how the definition can be modified to produce meaningful results for nested subqueries. We further present query rewrite rules to transform an SQL query into a query propagating provenance. The solution introduced in this paper allows us to track provenance information for a far wider subset of SQL than any of the existing approaches. We have incorporated these ideas into the Perm provenance management system engine and used it to evaluate the feasibility and performance of our approach.

#index 1181297
#* A data model for trip planning in multimodal transportation systems
#@ Joel Booth;Prasad Sistla;Ouri Wolfson;Isabel F. Cruz
#t 2009
#c 8
#% 32904
#% 109496
#% 116590
#% 315005
#% 408396
#% 443521
#% 452816
#% 461923
#% 462469
#% 462945
#% 811957
#% 878301
#! This paper introduces the problem of modeling urban transportation systems in a database where certain aspects of the data are probabilistic in nature. The transportation network is composed of multiple modes (e.g., automobile, bus, train, pedestrian) that the user can alternate between. A trip -- a path between an origin and destination subject to some constraints -- is the central concept. How these trips and the network can be represented as both a graph and relational model, as well as the requirements for querying are the main contributions of this paper. A set of operators are defined to work over these transportation concepts and they are integrated within a SQL-like syntax to express queries over the uncertain transportation network. Additionally, the paper shows how this model can be integrated within other moving objects and spatio-temporal data models, and how these graph-based queries can be processed.

#index 1181298
#* Parsimonious temporal aggregation
#@ Juozas Gordevičius;Johann Gamper;Michael Böhlen
#t 2009
#c 8
#% 64150
#% 70370
#% 260014
#% 452818
#% 466506
#% 479648
#% 565462
#% 578404
#% 617886
#% 726629
#% 789017
#% 987257
#% 1116036
#% 1688261
#! Temporal aggregation is a crucial operator in temporal databases and has been studied in various flavors, including instant temporal aggregation (ITA) and span temporal aggregation (STA), each having its strengths and weaknesses. In this paper we define a new temporal aggregation operator, called parsimonious temporal aggregation (PTA), which comprises two main steps: (i) it computes the ITA result over the input relation and (ii) it compresses this intermediate result to a user-specified size c by merging adjacent tuples and keeping the induced total error minimal; the compressed ITA result is returned as the final result. By considering the distribution of the input data and allowing to control the result size, PTA combines the best features of ITA and STA. We provide two evaluation algorithms for PTA queries. First, the oPTA algorithm computes an exact solution, by applying dynamic programming to explore all possibilities to compress the ITA result and selecting the compression with the minimal total error. It runs in O(n2pc) time and O(n2) space, where n is the size of the input relation and p is the number of aggregation functions in the query. Second, the more efficient gPTA algorithm computes an approximate solution by greedily merging the most similar ITA result tuples, which, however, does not guarantee a compression with a minimal total error. gPTA intermingles the two steps of PTA and avoids large intermediate results. The compression step of gPTA runs in O(np log(c + δ)) time and O(c + δ) space, where δ is a small buffer for "look ahead". An empirical evaluation shows good results: considerable reductions of the result size introduce only small errors, and gPTA scales to large data sets and is only slightly worse than the exact solution of PTA.

#index 1181299
#* Fast object search on road networks
#@ Ken C. K. Lee;Wang-Chien Lee;Baihua Zheng
#t 2009
#c 8
#% 1722
#% 214719
#% 318437
#% 322884
#% 408396
#% 443105
#% 443208
#% 443533
#% 729850
#% 803127
#% 813973
#% 824723
#% 893162
#% 1015321
#% 1016199
#% 1688257
#! In this paper, we present ROAD, a general framework to evaluate Location-Dependent Spatial Queries (LDSQ)s that searches for spatial objects on road networks. By exploiting search space pruning technique and providing a dynamic object mapping mechanism, ROAD is very efficient and flexible for various types of queries, namely, range search and nearest neighbor search, on objects over large-scale networks. ROAD is named after its two components, namely, Route Overlay and Association Directory, designed to address the network traversal and object access aspects of the framework. In ROAD, a large road network is organized as a hierarchy of interconnected regional sub-networks (called Rnets) augmented with 1) shortcuts for accelerating network traversals; and 2) object abstracts for guiding traversals. In this paper, we present (i) the Rnet hierarchy and several properties useful to construct Rnet hierarchy, (ii) the design and implementation of the ROAD framework, (iii) efficient object search algorithms for various queries, and (iv) incremental update techniques for framework maintenance in presence of object and network changes. We conducted extensive experiments with real road networks to evaluate ROAD. The experiment result shows the superiority of ROAD over the state-of-the-art approaches.

#index 1181300
#* Finding the influence set through skylines
#@ Xiaobing Wu;Yufei Tao;Raymong Chi-Wing Wong;Ling Ding;Jeffrey Xu Yu
#t 2009
#c 8
#% 86950
#% 201876
#% 465167
#% 480671
#% 800555
#% 806212
#% 810024
#% 824670
#% 824671
#% 824672
#% 864451
#% 864452
#% 864453
#% 864495
#% 875011
#% 875012
#% 875025
#% 893150
#% 993954
#% 1022203
#% 1022224
#% 1022225
#% 1022226
#% 1022266
#% 1063485
#% 1063486
#% 1063487
#% 1206767
#% 1688253
#% 1688273
#! Given a set P of products, a set O of customers, and a product p ε P, a bichromatic reverse skyline query retrieves all the customers in O that do not find any other product in P to be absolutely better than p. More specifically, a customer o ε O is in the reverse skyline of p ε P if and only no other product in P better matches the preference of o on all dimensions. The only existing bichromatic reverse skyline algorithm, which we refer to as basic, is designed for uncertain data. This paper focuses on traditional datasets, where each object is a precise point. Since a precise point can be regarded as a special uncertain object, basic can still be applied. However, as precise data are inherently easier to handle than uncertain data, one should expect that basic can be further improved by taking advantage of the reduced problem complexity. Indeed, we observe several non-trivial heuristics that can optimize the access order to achieve stronger pruning power. Motivated by this, we propose a new algorithm called BRS, and prove that BRS never entails more I/Os than basic. Besides our theoretical analysis, we also perform extensive experiments to show that in practice BRS usually outperforms basic by a large factor. For example, when both P and O follow the anti-correlated distribution, BRS is faster than basic by an order of magnitude. Finally, we address a new variation of bichromatic reverse skyline search where the conventional definition of dynamic skylines no longer makes sense.

#index 1181301
#* Efficient skyline computation in metric space
#@ David Fuhry;Ruoming Jin;Donghui Zhang
#t 2009
#c 8
#% 2115
#% 227937
#% 281750
#% 288976
#% 294634
#% 443698
#% 465167
#% 479462
#% 480671
#% 481460
#% 617175
#% 806212
#% 814650
#% 824671
#% 824672
#% 875012
#% 875013
#% 893150
#% 993954
#% 1044463
#% 1688273
#! Given a set of n query points in a general metric space, a metric-space skyline (MSS) query asks what are the closest points to all these query points in the database. Here, consider for any point p, if there are no other points in the database which have less or equal distance to all the query points, then p is denoted as one of the closest points to the query points. This problem is a direct generalization of the recently proposed spatial-skyline query problem, where all the points are located in two or three dimensional Euclidean space. It is also closely related with the nearest neighbor (NN) query, the range query and the common skyline query problem. In this paper, we have developed new algorithms to aggressively prune non-skyline points from the search space. We also contribute two new optimization techniques to reduce the number of distance computations and dominance tests. Our experimental evaluation has shown the effectiveness and efficiency of our approach.

#index 1181302
#* Efficient skyline retrieval with arbitrary similarity measures
#@ Deepak P;Prasad M Deshpande;Debapriyo Majumdar;Raghu Krishnapuram
#t 2009
#c 8
#% 321455
#% 333854
#% 451645
#% 480671
#% 643566
#% 654480
#% 806212
#% 824670
#% 857113
#% 864452
#% 893128
#% 993954
#% 1044465
#! A skyline query returns a set of objects that are not dominated by other objects. An object is said to dominate another if it is closer to the query than the latter on all factors under consideration. In this paper, we consider the case where the similarity measures may be arbitrary and do not necessarily come from a metric space. We first explore middleware algorithms, analyze how skyline retrieval for non-metric spaces can be done on the middleware backend, and lay down a necessary and sufficient stopping condition for middleware-based skyline algorithms. We develop the Balanced Access Algorithm, which is provably more IO-friendly than the state-of-the-art algorithm for skyline query processing on middleware and show that BAA outperforms the latter by orders of magnitude. We also show that without prior knowledge about data distributions, it is unlikely to have a middleware algorithm that is more IO-friendly than BAA. In fact, we empirically show that BAA is very close to the absolute lower bound of IO costs for middleware algorithms. Further, we explore the non-middleware setting and devise an online algorithm for skyline retrieval which uses a recently proposed value space index over non-metric spaces (AL-Tree [10]). The AL-Tree based algorithm is able to prune subspaces and efficiently maintain candidate sets leading to better performance. We compare our algorithms to existing ones which can work with arbitrary similarity measures and show that our approaches are better in terms of computational and disk access costs leading to significantly better response times.

#index 1181303
#* Transactions on the multiversion B+-tree
#@ Tuukka Haapasalo;Ibrahim Jaluta;Bernhard Seeger;Seppo Sippu;Eljas Soisalon-Soininen
#t 2009
#c 8
#% 10392
#% 58371
#% 114582
#% 116086
#% 201869
#% 287070
#% 403195
#% 427199
#% 442967
#% 443524
#% 495256
#% 571296
#% 803126
#% 810114
#% 814649
#% 864422
#% 866983
#% 1019164
#! The multiversion B+-tree (MVBT) by Becker et al. assumes a single-data-item update model in which each new version created for a data item is given a timestamp that is unique across the entire MVBT. In this paper, we extend the MVBT model with multi-action transactions such that all (final) data-item versions created by a transaction are given the same timestamp. We show that the MVBT algorithms can be modified to work in a setting in which multiple readonly transactions and a single updating transaction operate concurrently in snapshot isolation on the MVBT, without compromising the asymptotically optimal time complexity of key inserts, key deletes, and key-range scans on any version. The structural consistency and balance of the MVBT is guaranteed by short-duration latching of pages, redo-only logging of structure modifications (version splits, key splits and page merges), and redo-undo logging of key insertions and deletions. The redo pass of our ARIES-based restart-recovery algorithm always produces a structurally consistent and balanced MVBT on which any undo action by a backward-rolling updating transaction can be performed logically if a physical undo is not possible. The standard steal-and-no-force buffering policy is assumed.

#index 1181304
#* Efficient maintenance techniques for views over active documents
#@ Serge Abiteboul;Pierre Bourhis;Bogdan Marinoiu
#t 2009
#c 8
#% 23902
#% 29253
#% 66097
#% 152928
#% 170898
#% 268788
#% 325384
#% 384978
#% 452753
#% 465061
#% 473117
#% 479629
#% 726621
#% 733593
#% 765420
#% 765488
#% 770168
#% 799147
#% 805907
#% 809267
#% 810045
#% 824761
#% 875015
#% 997023
#% 1015338
#% 1016258
#% 1016321
#% 1016325
#% 1072645
#% 1206773
#% 1206775
#% 1688304
#! Many Web applications are based on dynamic interactions between Web components exchanging flows of information. Such a situation arises for instance in mashup systems or when monitoring distributed autonomous systems. Our work is in this challenging context that has generated recently a lot of attention; see Web 2.0. We introduce the axlog formal model for capturing such interactions and show how this model can be supported efficiently. The central component is the axlog widget defined by one tree-pattern query or more, over an active document (in the Active XML style) that includes some input streams of updates. A widget generates a stream of updates for each query, the updates that are needed to maintain the view corresponding to the query. We exploit an array of known technologies: datalog optimization techniques such as Differential or MagicSet, constraint query languages, and efficient XML filtering (YFilter). The novel optimization technique we propose is based on fundamental new notions: a relevance (different than that of MagicSet), satisfiability and provenance for active documents. We briefly discuss an implementation of an axlog engine, an application that we used to test the approach, and results of experiments.

#index 1181305
#* Towards materialized view selection for distributed databases
#@ Leonardo Weiss F. Chaves;Erik Buchmann;Fabian Hueske;Klemens Böhm
#t 2009
#c 8
#% 13018
#% 248014
#% 287261
#% 300164
#% 315024
#% 316549
#% 333962
#% 464706
#% 479476
#% 479646
#% 479792
#% 480149
#% 480158
#% 482111
#% 820356
#% 853008
#% 958201
#% 1022221
#% 1083735
#% 1676093
#% 1777088
#! Materialized views (MV) can significantly improve the query performance of relational databases. In this paper, we consider MVs to optimize complex scenarios where many heterogeneous nodes with different resource constraints (e.g., CPU, IO and network bandwidth) query and update numerous tables on different nodes. Such problems are typical for large enterprises, e.g., global retailers storing thousands of relations on hundreds of nodes at different subsidiaries. Choosing which views to materialize in a distributed, complex scenario is NP-hard. Furthermore, the solution space is huge, and the large number of input factors results in non-monotonic cost models. This prohibits the straightforward use of brute-force algorithms, greedy approaches or proposals from organic computing. For the same reason, all solutions for choosing MVs we are aware of do not consider either distributed settings or update costs. In this paper we describe an algorithmic framework which restricts the sets of considered MVs so that a genetic algorithm can be applied. In order to let the genetic algorithm converge quickly, we generate initial populations based on knowledge on database tuning, and devise a selection function which restricts the solution space by taking the similarity of MV configurations into account. We evaluate our approach both with artificial settings and a real-world RFID scenario from retail. For a small setting consisting of 24 tables distributed over 9 nodes, an exhaustive search needs 10 hours processing time. Our approach derives a comparable set of MVs within 30 seconds. Our approach scales well: Within 15 minutes it chooses a set of MVs for a real-world scenario consisting of 1,000 relations, 400 hosts, and a workload of 3,000 queries and updates.

#index 1181306
#* Personalizing entity detection and recommendation with a fusion of web log mining techniques
#@ Kathleen Tsoukalas;Bin Zhou;Jian Pei;Davor Cubranic
#t 2009
#c 8
#% 532186
#% 748600
#% 1019131
#% 1121284
#% 1279327
#! Given the proliferation of technology sites and the growing diversity of their readership, readers are more and more likely to encounter specialized language and terminology that they may lack the sufficient background to understand. Such sites may lose readership and the experience of readers may be impacted negatively if readers cannot quickly and easily find information about terms they wish to learn more about. We developed a system using a fusion of web log mining techniques that extracts, identifies, and recommends personalized terms to readers by utilizing information found in individual and global web query logs. In addition, the system presents relevant information related to these terms inline with the text. Our system outperforms some other related systems developed in the literature with special regard to usability.

#index 1181307
#* Estimating aggregates in time-constrained approximate queries in Oracle
#@ Ying Hu;Seema Sundara;Jagannathan Srinivasan
#t 2009
#c 8
#% 58348
#% 210353
#% 248820
#% 1022304
#! The concept of time-constrained SQL queries was introduced to address the problem of long-running SQL queries. A key approach adopted for supporting time-constrained SQL queries is to use sampling to reduce the amount of data that needs to be processed, thereby allowing completion of the query in the specified time constraint. However, sampling does make the query results approximate and hence requires the system to estimate the values of the expressions (especially aggregates) occurring in the select list. Thus, coming up with estimates for aggregates is crucial for time-constrained approximate SQL queries to be useful, which is the focus of this paper. Specifically, we address the problem of estimating commonly occurring aggregates (namely, SUM, COUNT, AVG, MEDIAN, MIN, and MAX) in time-constrained approximate queries. We give both point and interval estimates for SUM, COUNT, AVG, and MEDIAN using Bernoulli sampling for various type of queries, including join processing with cross product sampling. For MIN (MAX), we give the confidence level that the proportion 100γ% of the population will exceed the MIN (or be less than the MAX) obtained from the sampled data.

#index 1181308
#* BaseX & DeepFS joint storage for filesystem and database
#@ Alexander Holupirek;Christian Grün;Marc H. Scholl
#t 2009
#c 8
#% 397358
#% 824777
#% 960362
#! Mere storage of personal data in state-of-the-art filesystems is a markedly well done job in current operating systems. Convenient access to and information retrieval from such data, however, is crucial to leverage the stored information. Thereby database style query languages can be of great use. We demonstrate a user level filesystem implementation that is built on recent semi-structured database storage techniques. As such, it serves as a storage layer for the BaseX XQuery processor and, while it appears to the operating system as a conventional filesystem, a large part of its content can be queried using XPath/XQuery.

#index 1181309
#* Xoom: a tool for zooming in and out of XML documents
#@ Maya Ramanath;Kondreddi Sarath Kumar
#t 2009
#c 8
#% 397364
#% 438324
#% 893115
#% 1063493
#% 1206738
#% 1207232
#! Suppose there is a large corpus of XML documents, each of which describes a movie released in the last 30 years (for example, extracted from IMDB). A movie enthusiast wants to make a list of interesting movies based on various criteria, such as, the genre, lead actors, directors, etc. She first decides to narrow the focus to just thrillers. However, she then has to look into each document individually, since only then is it possible for her to tell whether the combination of actors, directors, etc. interests her. This would be time-consuming if the documents in question contain hundreds of tags each. Instead, she could use our tool Xoom (XML-Zoom) which can extract and present the key information in every document. This would drastically cut down the time to go through the documents. She could then use Xoom to zoom into specific portions of each of the remaining documents, instead of opening and scanning them from top to bottom. In this proposal, we describe the construction of Xoom and outline a demonstration.

#index 1181310
#* HIDE: heterogeneous information DE-identification
#@ James Gardner;Li Xiong;Kanwei Li;James J. Lu
#t 2009
#c 8
#% 464434
#% 576761
#% 785363
#% 800514
#% 800515
#% 800557
#% 864406
#% 864412
#% 939999
#% 960289
#% 960291
#% 1114635
#! While there is an increasing need to share data that may contain personal information, such data sharing must preserve individual privacy without disclosing any identifiable information. A considerable amount of research in the data privacy community has been devoted to formalizing the notion of identifiability with many techniques for anonymization, but is focused exclusively on structured data. On the other hand, efforts on de-identifying medical text documents in the medical informatics community are highly specialized for specific document types or a subset of identifiers. In addition, they rely on simple identifier removal or grouping techniques and do not take advantage of the research developments in the data privacy community. We developed an integrated system, HIDE, for Heterogeneous Information DE-identification including structured and unstructured data utilizing existing anonymization techniques. We demonstrate a prototype of our system and show the effectiveness of our approach through a set of real data augmented with synthesized data.

#index 1181311
#* MVT: a schema mapping validation tool
#@ Guillem Rull;Carles Farré;Ernest Teniente;Toni Urpí
#t 2009
#c 8
#% 572314
#% 578668
#% 824148
#% 893095
#% 893196
#% 993981
#% 1016268
#% 1019174
#% 1063578
#% 1063580
#% 1065125
#% 1081947
#! Schema mappings define relationships between schemas in a declarative way. We demonstrate MVT, a mapping validation tool that allows the designer to ask whether the mapping has certain desirable properties. The answers to these questions will provide information on whether the mapping adequately matches the intended needs and requirements. MVT is able to deal with a highly expressive class of mappings and database schemas, which allows the use of negations, order comparisons and null values. The tool does not only provide a Boolean answer as test result, but also a feedback for that result. Depending on the tested property and on the test result, the provided feedback can be in the form of example schema instances, or in the form of an explanation, that is, highlighting the mapping assertions and schema constraints responsible for getting such a result.

#index 1181312
#* A tool for mapping discovery over revealing schemas
#@ Verena Kantere;Dimos Bousounis;Timos Sellis
#t 2009
#c 8
#% 334025
#% 378409
#% 723449
#% 745528
#% 762652
#% 1063578
#% 1131308
#! In a world of wide-scale information sharing, the decentralized coordination has to consolidate a variety of heterogeneity. Shared data are described in different formats, i.e. data structures, values and schemas. Querying manifold such sources entails techniques that can bridge the data formats. Some of these techniques deal with producing mappings for the schemas of data. The existing techniques view complementary aspects of the schema mapping problem. Important ones, consider producing all the possible mappings for a pair of schemas, insinuating any accompanying semantics in the mappings and adapting correct mappings as schemas evolve. Towards this end we have developed a solution that is fine-tuned for the discovery of mappings as schemas of autonomous sources are gradually revealed. In this demonstration we exhibit a new prototype tool that implements this solution. The tool provides a mechanism that realizes discovery of correct mappings as schemas are revealed. Mapping discovery is schema-centric and incorporates new semantics as they are unveiled. Mapping experience is reused and possible mappings are ranked so that the best choice is presented. The core mechanism collaborates with an automatic schema matching tool and the user that lightly guides the mapping process. The demonstration presents two application scenarios that prove the suitability of this prototype tool and the effectiveness of the implemented mapping solution in realistic situations of of data integration and exchange between heterogeneous autonomous sources.

#index 1181313
#* GCIP: exploiting the generation and optimization of integration processes
#@ Matthias Boehm;Uwe Wloka;Dirk Habich;Wolfgang Lehner
#t 2009
#c 8
#% 765434
#% 800563
#% 893118
#% 1022253
#% 1206578
#! As a result of the changing scope of data management towards the management of highly distributed systems and applications, integration processes have gained in importance. Such integration processes represent an abstraction of workflow-based integration tasks. In practice, integration processes are pervasive and the performance of complete IT infrastructures strongly depends on the performance of the central integration platform that executes the specified integration processes. In this area, the three major problems are: (1) significant development efforts, (2) low portability, and (3) inefficient execution. To overcome those problems, we follow a model-driven generation approach for integration processes. In this demo proposal, we want to introduce the so-called GCIP Framework (Generation of Complex Integration Processes) which allows the modeling of integration process and the generation of different concrete integration tasks. The model-driven approach opens opportunities for rule-based and workload-based optimization techniques.

#index 1181314
#* CourseCloud: summarizing and refining keyword searches over structured data
#@ Georgia Koutrika;Zahra Mohammadi Zadeh;Hector Garcia-Molina
#t 2009
#c 8
#% 659990
#% 660011
#% 864456
#% 1016176
#% 1181246
#! In this demo, we show data clouds that summarize the results of keyword searches over structured data. Data clouds provide insight into the database contents, hints for query modification and refinement and can lead to serendipitous discoveries of diverse results. In this demo paper: • we summarize the main issues for generating data clouds • we give an overview of our framework for keyword searching with summaries (clouds) • we describe our system CourseCloud that allows searching for courses and their evaluation.

#index 1181315
#* EventSummarizer: a tool for summarizing large event sequences
#@ Jerry Kiernan;Evimaria Terzi
#t 2009
#c 8
#% 397383
#% 463903
#% 577226
#% 949146
#% 1083670
#! We present EventSummarizer - a tool for extracting comprehensive summaries from large event sequences. EventSummarizer takes as input a sequence with events of different types that occur during an observation period, and creates a partitioning of this time period into contiguous non-overlapping intervals such that each interval can be described by a simple model. Within each interval local associations between events of different types are reported. EventSummarizer runs on top of any Relational DataBase Management System (RDBMS), on tables with a timestamp attribute. Our system is parameter free and has a visual interface that provides the user with a global view of the input sequence via the segmentation of the timeline. The easy-to-use interface provides the user with the option to further examine the activity and associations of event types within each segment.

#index 1181316
#* High-performance information extraction with AliBaba
#@ Peter Palaga;Long Nguyen;Ulf Leser;Jörg Hakenberg
#t 2009
#c 8
#% 480654
#% 770307
#% 833912
#% 875064
#% 905978
#% 906505
#% 1022234
#% 1206701
#% 1696325
#! A wealth of information is available only in web pages, patents, publications etc. Extracting information from such sources is challenging, both due to the typically complex language processing steps required and to the potentially large number of texts that need to be analyzed. Furthermore, integrating extracted data with other sources of knowledge often is mandatory for subsequent analysis. In this demo, we present the AliBaba system for scalable information extraction from biomedical documents. Unlike many other systems, AliBaba performs both entity extraction and relationship extraction and graphically visualizes the resulting network of inter-connected objects. It leverages the PubMed search engine for selection of relevant documents. The technical novelty of AliBaba is twofold: (a) its ability to automatically learn language patterns for relationship extraction without an annotated corpus, and (b) its high performance pattern matching algorithm. We show that a simple yet effective pattern filtering technique improves the runtime of the system drastically without harming its extraction effectiveness. Although AliBaba has been implemented for biomedical texts, its underlying principles should also be applicable in any other domain.

#index 1181317
#* Exploiting similarity-aware grouping in decision support systems
#@ Yasin N. Silva;Muhammad U. Arshad;Walid G. Aref
#t 2009
#c 8
#% 751496
#% 960244
#% 1206820
#! Decision Support Systems (DSS) are information systems that support decision making processes. In many scenarios these systems are built on top of data managed by DBMSs and make extensive use of its underlying grouping and aggregation capabilities, i.e., Group-by operation. Unfortunately, the standard grouping operator has the inherent limitation of being based only on equality, i.e., all the tuples in a group share the same values of the grouping attributes. Similarity-based Group-by (SGB) has been recently proposed as an extension aimed to overcome this limitation. SGB allows fast formation of groups with similar objects under different grouping strategies and the pipelining of results for further processing. This demonstration presents how SGB can be effectively used to build useful DSSs. The presented DSS has been built around the data model and queries of the TPC-H benchmark intending to be representative of complex business analysis applications. The system provides intuitive dashboards that exploit similarity aggregation queries to analyze: (1) customer clustering, (2) profit and revenue, (3) marketing campaigns, and (4) discounts. The presented DSS runs on top of PostgreSQL whose query engine is extended with similarity grouping operators.

#index 1181318
#* MarcoPolo: a community system for sharing and integrating travel information on maps
#@ Yueguo Chen;Su Chen;Yu Gu;Mei Hui;Feng Li;Chen Liu;Liangxu Liu;Beng Chin Ooi;Xiaoyan Yang;Dongxiang Zhang;Yuan Zhou
#t 2009
#c 8
#% 960243
#% 967244
#% 1063537
#! The tagging technique has been widely applied in existing Web 2.0 systems, where users label resources with tags for effective classification and efficient retrieval of resources. Location-aware geographical tags (geo-tags) are required if users want to mark location-sensitive resources to digital maps. Large volumes of different kinds of user-created tags pose challenges to the effective organization of community resources using tags. Issues such as guaranteeing the quality of tags and supporting various tag-based queries emerge. In this demo, we present MarcoPolo, a Web 2.0 community system that allows users to define the hierarchical textual geo-tags and mark resources to a map using geo-tags. Statistical and feedback mechanisms are applied to guarantee the quality of tags (including geo-tags). The MarcoPolo system provides two effective interfaces for users to browse and search resources: one is the keyword-based interface and the other is the map-based interface.

#index 1181319
#* NNexus: an automatic linker for collaborative web-based corpora
#@ James Gardner;Aaron Krowne;Li Xiong
#t 2009
#c 8
#% 387427
#% 832138
#% 869521
#% 1130858
#% 1209659
#! Collaborative online encyclopedias or knowledge bases such as Wikipedia and PlanetMath are becoming increasingly popular. We demonstrate NNexus, a generalization of the automatic linking engine of PlanetMath.org and the first system that automates the process of linking disparate "encyclopedia" entries into a fully-connected conceptual network. The main challenges of this problem space include: 1) linking quality (correctly identifying which terms to link and which entry to link to with minimal effort on the part of users), 2) efficiency and scalability, and 3) generalization to multiple knowledge bases and web-based information environment. We present NNexus that utilizes subject classification and other metadata to address these challenges and demonstrate its effectiveness and efficiency through multiple real world corpora.

#index 1181320
#* Performance evaluation in database research: principles and experience
#@ Stefan Manegold;Ioana Manolescu;Stefan Manegold;Ioana Manolescu
#t 2009
#c 8
#! A significant part of today's database research focuses on improving performance of a specific system. Quantitative experiments are the best way to validate such results. However, performing experiments is not always easy. Besides the complexity of the system under test, designing an experiment, choosing the right environment and parameter values, analyzing the data which is gathered, and reporting it to a third party in an expressive and intelligible way is hard. In this tutorial, we present a general road-map to the above steps, including tips and tricks on how to organize and present code that performs experiments, so that an outsider can repeat them. The tutorial is primarily aimed at MS and PhD students seeking to improve their experiment practices, but more senior attendants may also find it interesting.

#index 1181321
#* Geographic privacy-aware knowledge discovery and delivery
#@ Fosca Giannotti;Dino Pedreschi;Yannis Theodoridis
#t 2009
#c 8
#! A flood of data pertinent to moving objects is available today, and will be more in the near future, particularly due to the automated collection of privacy-sensitive telecom data from mobile phones and other location-aware devices. Such wealth of data, referenced both in space and time, may enable novel classes of applications of high societal and economic impact, provided that the discovery of consumable and concise knowledge out of these raw data is made possible. Recent research activities have developed theory, techniques and systems for geographic knowledge discovery and delivery, some of them based on privacy-preserving methods for extracting knowledge from large amounts of raw data referenced in space and time. All these efforts aim at devising knowledge discovery and analysis methods for trajectories of moving objects. The fundamental hypothesis is that it is possible, in principle, to aid citizens in their mobile activities by analysing the traces of their past activities by means of data mining techniques. For instance, behavioural patterns derived from mobile trajectories may allow inducing traffic flow information, capable to help people travel efficiently, to help public administrations in traffic-related decision making for sustainable mobility and security management, as well as to help mobile operators in optimising bandwidth and power allocation on the network. On the other hand, it is clear that the use of personal sensitive data arouses concerns about citizen's privacy rights. In this tutorial, we establish a framework for the challenges and the mining solutions for the geographic information collected by Moving Object Database (MOD) engines. We first discuss the challenges of collecting mobility data, and elaborate on the impact of trajectory data analysis in several modern applications. We then discuss methodologies and techniques to collect raw data, reconstruct trajectory information, and efficiently store it in MODs. We continue with an overview of knowledge discovery approaches for movement data. Finally, we propose a research agenda and identify areas where interdisciplinary studies are needed.

#index 1181322
#* Scalable OLAP and mining of information networks
#@ Jiawei Han;Xifeng Yan;Philip S. Yu
#t 2009
#c 8
#! With the ubiquity of information networks and their broad applications, there have been numerous studies on the construction, online analytical processing, and mining of information networks in multiple disciplines, including social network analysis, World-Wide Web, database systems, data mining, machine learning, and networked communication and information systems. In this tutorial, we present an organized picture on scalable OLAP (online analytical processing) and mining of information networks, with the inclusion of the following topics: (1) an introduction to information networks and information network analysis, (2) general statistical behavior of information networks, (3) mining frequent subgraphs in large graphs and networks, (4) data integration, data cleaning and data validation in information networks, (5) clustering graphs and information networks, (6) classification of graphs and information networks; (7) summarization and simplification of graphs and information networks, (8) OLAP and multidimensional analysis of information networks, (9) evolution of dynamic information networks, and (10) research challenges on OLAP and mining of information networks.

#index 1372680
#* Provenance for database transformations
#@ Val Tannen
#t 2010
#c 8
#! Database transformations (queries, views, mappings) take apart, filter, and recombine source data in order to populate warehouses, materialize views, and provide inputs to analysis tools. As they do so, applications often need to track the relationship between parts and pieces of the sources and parts and pieces of the transformations' output. This relationship is what we call database provenance. This talk presents an approach to database provenance that is based on two observations. First, provenance is a kind of annotation, and we can develop a general approach to annotation propagation that also covers other applications, for example to uncertainty and access control. In fact, provenance turns out to be the most general kind of such annotation, in a precise and practically useful sense. Second, the propagation of annotation through a broad class of transformations relies on just two operations: one when annotations are jointly used and one when they are used alternatively. This leads to annotations forming a specific algebraic structure, a commutative semiring. The semiring approach works for annotating tuples, field values and attributes in standard relations, in nested relations (complex values), and for annotating nodes in (unordered) XML. It works for transformations expressed in the positive fragment of relational algebra, nested relational calculus, unordered XQuery, as well as for Datalog, GLAV schema mappings, and tgd constraints. Specific semirings correspond to earlier approaches to provenance, while others correspond to forms of uncertainty, trust, cost, and access control. This is joint work with J. N. Foster, T. J. Green, Z. Ives, and G. Karvounarakis, done in part within the frameworks of the Orchestra and pPOD projects.

#index 1372681
#* Scalable ontology-based information systems
#@ Ian Horrocks
#t 2010
#c 8
#! Ontologies and ontology based systems are becoming increasingly important in meeting the demand for more powerful and flexible information systems. Requirements for such systems include the need to deal with incomplete and semi-structured information, to integrate information from heterogeneous sources, to employ richer and more flexible schemas, and for query answers to reflect both knowledge and data. Provision of such enhanced capabilities must, however, be in addition to, and not instead of, the well-established features of existing database systems, in particular their robust scalability. Achieving this is, of course, extremely challenging. In this talk I will present some recent research efforts that tackle this problem, including investigations of tractable fragments, new algorithmic techniques, new optimisations and the exploitation of relational database technology.

#index 1372682
#* Feedback-driven result ranking and query refinement for exploring semi-structured data collections
#@ Huiping Cao;Yan Qi;K. Selçuk Candan;Maria Luisa Sapino
#t 2010
#c 8
#% 201876
#% 289011
#% 321635
#% 465167
#% 479465
#% 571052
#% 654476
#% 742666
#% 768910
#% 791182
#% 941785
#% 960246
#% 960359
#% 1013630
#% 1019159
#% 1022244
#% 1022274
#% 1074078
#% 1074242
#% 1126560
#% 1134508
#% 1206780
#% 1217155
#% 1292746
#% 1674723
#% 1688265
#% 1712555
#% 1742096
#! Feedback process has been used extensively in document-centric applications, such as text retrieval and multimedia retrieval. Recently, there have been efforts to apply feedback to semi-structured XML document collections as well. In this paper, we note that feedback can also be an effective tool for exploring (through result ranking and query refinement) large semi-structured data collections. In particular, in large scale data sharing and curation environments, where the user may not know the structure of the data, queries may initially be overly vague. Given a path query and a set of results identified by the system to this query over the data, we consider two types of feedback: Soft feedback captures the user's preference for some features over the others. Hard feedback, on the other hand, expresses users' assertions regarding whether certain features should be further enforced or, in contrast, are to be avoided. Both soft and hard feedback can be "positive" or "negative". For soft feedback, we develop a probabilistic feature significance measure and describe how to use this for ranking results in the presence of dependencies between the path features. To deal with the hard feedback efficiently (i.e., fast enough for interactive exploration), we present finite automata based query refinement solutions. In particular, we present a novel LazyDFA+ algorithm for managing hard feedback. We also describe optimizations that leverage the inherently iterative nature of the feedback process. We bring together these techniques in AXP, a system for adaptive and exploratory path retrieval. The experimental results show the effectiveness of the proposed techniques.

#index 1372683
#* Beyond pages: supporting efficient, scalable entity search with dual-inversion index
#@ Tao Cheng;Kevin Chen-Chuan Chang
#t 2010
#c 8
#% 212665
#% 330616
#% 463735
#% 610078
#% 730022
#% 742102
#% 754068
#% 781169
#% 805864
#% 805883
#% 807729
#% 854668
#% 869535
#% 875001
#% 875061
#% 956501
#% 956631
#% 960235
#% 1015265
#% 1022234
#% 1055710
#% 1355028
#% 1834787
#! Entity search, a significant departure from page-based retrieval, finds data, i.e., entities, embedded in documents directly and holistically across the whole collection. This paper aims at distilling and abstracting the essential computation requirements of entity search. From the dual views of reasoning--entity as input and entity as output, we propose a dual-inversion framework, with two indexing and partition schemes, towards efficient and scalable query processing. We systematically evaluate our framework using a prototype over a 3TB real Web corpus with 150M pages and over 20 entity types extracted. Our experiments in two concrete application settings show our techniques of on average, 2 to 4 orders of magnitude speed-up, over the keyword-based baseline, with reasonable space overhead.

#index 1372684
#* Processing XPath queries with forward and downward axes over XML streams
#@ Makoto Onizuka
#t 2010
#c 8
#% 390964
#% 397375
#% 462235
#% 480296
#% 487257
#% 654476
#% 659995
#% 731408
#% 781453
#% 791182
#% 800620
#% 809253
#% 814648
#% 814651
#% 824798
#% 835819
#% 864465
#% 893134
#% 960256
#% 982759
#% 993950
#% 1015276
#% 1016258
#% 1022330
#! We propose an XPath processing algorithm that efficiently evaluates XPath queries in XP{↓, →, *, []} over XML streams. An XPath query is expressed with axes, which are binary relations between nodes in XML streams: '↓' identifies the child/descendant axes and '→' indicates the following/following-sibling axes. The proposed algorithm evaluates XPath queries within one XML parsing pass and outputs the fragments found in XML streams as the query results. The difficulty of XP{↓, →, *, []} evaluation lies in establishing dynamic scope control for the following/following-sibling axes. The algorithm uses double-layered non-deterministic finite automata (NFA) to resolve this issue. First layer NFA is compiled from XPath queries and is able to evaluate sub-queries in XP{↓, →, *}. Second layer NFA handles predicate parts. It is dynamically maintained during XML parsing: a state is constructed from a pair of the corresponding state in the first layer automaton and the currently parsed node in the XML stream. Layered NFA achieves O(|D||Q|) time complexity by introducing a state sharing technique, which avoids the exponential growth in the state size of Layered NFA by eliminating redundant transitions. We validate the efficiency of the algorithm through empirical experiments and show that Layered NFA is up to four times faster, and twice as fast on average, than existing algorithms.

#index 1372685
#* Correlation aware synchronization for near real time decision support systems
#@ Ying Yan;Wen-Syan Li
#t 2010
#c 8
#% 114994
#% 227885
#% 330682
#% 333969
#% 369236
#% 397355
#% 465007
#% 479813
#% 745536
#% 765469
#% 790617
#% 864489
#% 867051
#% 875020
#% 1206944
#% 1239007
#% 1313373
#% 1414317
#% 1712364
#! Many large companies, especially those in financial and insurance service sectors, approach the market with a decentralized management structure, such as by line of business or geographical market segments. However, these companies require access to distributed and possibly heterogeneous data sources for corporate level decision making. In this paper, we focus on challenges of supporting a decision support system (DSS) based on a hybrid approach (i.e. a federation system with replication of frequently accessed remote data sources) for time-sensitive agile business intelligence applications. The response time requirement (and a realistic goal) for such a DSS is near real time (i.e. 2 ~ 3 minutes to 20 ~ 30 minutes). The users of a DSS care about not only the response time but also the time stamp of the business operation reports since out-dated reports introduce uncertainty and risks to decision-making. Thus, the information value of a report decreases as time passes. We present a framework of correlation aware synchronization of replicas used in DSS to optimize information values of business reports as a whole. The framework exploits correlation of usage and synchronization latency of replicas in a single query and a workload of queries for an optimal synchronization schedule. We have conducted extensive evaluations based on both TPC-H and synthetic workload. The proposed correlation aware synchronization effectively improves up to 50% of information value comparing with fixed synchronization plans on average.

#index 1372686
#* Turbo-charging hidden database samplers with overflowing queries and skew reduction
#@ Arjun Dasgupta;Nan Zhang;Gautam Das
#t 2010
#c 8
#% 268114
#% 340146
#% 397378
#% 480479
#% 480810
#% 783692
#% 869499
#% 943875
#% 956455
#% 956534
#% 960286
#% 993964
#% 1206906
#% 1217158
#% 1423051
#! Recently, there has been growing interest in random sampling from online hidden databases. These databases reside behind form-like web interfaces which allow users to execute search queries by specifying the desired values for certain attributes, and the system responds by returning a few (e.g., top-k) tuples that satisfy the selection conditions, sorted by a suitable scoring function. In this paper, we consider the problem of uniform random sampling over such hidden databases. A key challenge is to eliminate the skew of samples incurred by the selective return of highly ranked tuples. To address this challenge, all state-of-the-art samplers share a common approach: they do not use overflowing queries. This is done in order to avoid favoring highly ranked tuples and thus incurring high skew in the retrieved samples. However, not considering overflowing queries substantially impacts sampling efficiency. In this paper, we propose novel sampling techniques which do leverage overflowing queries. As a result, we are able to significantly improve sampling efficiency over the state-of-the-art samplers, while at the same time substantially reduce the skew of generated samples. We conduct extensive experiments over synthetic and real-world databases to illustrate the superiority of our techniques over the existing ones.

#index 1372687
#* Region-based online promotion analysis
#@ Tianyi Wu;Yizhou Sun;Cuiping Li;Jiawei Han
#t 2010
#c 8
#% 223781
#% 227866
#% 420053
#% 818916
#% 893127
#% 903016
#% 985040
#% 993958
#% 1063475
#% 1063518
#% 1176884
#% 1181261
#% 1292567
#% 1328118
#% 1328184
#! This paper addresses a fundamental and challenging problem with broad applications: efficient processing of region-based promotion queries, i.e., to discover the top-k most interesting regions for effective promotion of an object (e.g., a product or a person) given by user, where a region is defined over continuous ranged dimensions. In our problem context, the object can be promoted in a region when it is top-ranked in it. Such type of promotion queries involves an exponentially large search space and expensive aggregation operations. For efficient query processing, we study a fresh, principled framework called region-based promotion cube (RepCube). Grounded on a solid cost analysis, we first develop a partial materialization strategy to yield the provably maximum online pruning power given a storage budget. Then, cell relaxation is performed to further reduce the storage space while ensuring the effectiveness of pruning using a given bound. Extensive experiments conducted on large data sets show that our proposed method is highly practical, and its efficiency is one to two orders of magnitude higher than baseline solutions.

#index 1372688
#* The Data Cyclotron query processing scheme
#@ R. Goncalves;M. Kersten
#t 2010
#c 8
#% 1758
#% 32884
#% 115661
#% 201897
#% 227885
#% 300164
#% 330305
#% 340175
#% 340176
#% 480500
#% 864446
#% 874997
#% 884962
#% 911171
#% 931369
#% 1005865
#% 1016220
#% 1021950
#% 1022202
#% 1089604
#% 1206595
#% 1222049
#% 1239006
#! Distributed database systems exploit static workload characteristics to steer data fragmentation and data allocation schemes. However, the grand challenge of distributed query processing is to come up with a self-organizing architecture, which exploits all resources to manage the hot data set, minimize query response time, and maximize throughput without global co-ordination. In this paper, we introduce the Data Cyclotron architecture which addresses the challenges using turbulent data movement through a storage ring built from distributed main memory capitalizing modern remote-DMA facilities. Queries assigned to individual nodes interact with the Data Cyclotron by picking up data fragments continuously flowing around, i.e., the hot set. Each data fragment carries a level of interest (LOI) metric, which represents the cumulative query interest as the fragment passes around the ring multiple times. A fragment with a LOI below a given threshold, inversely proportional to the ring load, is pulled out to free up resources. This threshold is dynamically adjusted in a distributed manor based on ring characteristics and query needs. It optimizes the resource utilization keeping the average data access delay low. The proposed architecture has a modest impact on existing query execution engines. This is illustrated using an extensive validated simulation study for the Data Cyclotron protocols. The results underpin their robustness in turbulent workload scenarios as well as in the TPC-H scenario. Furthermore, we think that using state-of-the-art network technology, e.g., RDMA, could lead to even more promising results. The Data Cyclotron architecture opens a new vista for modern distributed database architectures with a plethora of research challenges barely scratched upon.

#index 1372689
#* Gossiping personalized queries
#@ Xiao Bai;Marin Bertier;Rachid Guerraoui;Anne-Marie Kermarrec;Vincent Leroy
#t 2010
#c 8
#% 290703
#% 322884
#% 397608
#% 399057
#% 610851
#% 754126
#% 824704
#% 824762
#% 832349
#% 985402
#% 987193
#% 987313
#% 1002007
#% 1063526
#% 1074116
#% 1112006
#% 1131214
#% 1232759
#% 1409929
#% 1706166
#! This paper presents P3Q, a fully decentralized gossip-based protocol to personalize query processing in social tagging systems. P3Q dynamically associates each user with social acquaintances sharing similar tagging behaviours. Queries are gossiped among such acquaintances, computed on the fly in a collaborative, yet partitioned manner, and results are iteratively refined and returned to the querier. Analytical and experimental evaluations convey the scalability of P3Q for top-k query processing. More specifically, we show that on a 10,000-user delicious trace, with little storage at each user, the queries are accurately computed within reasonable time and bandwidth consumption. We also report on the inherent ability of P3Q to cope with users updating profiles and departing.

#index 1372690
#* Optimizing joins in a map-reduce environment
#@ Foto N. Afrati;Jeffrey D. Ullman
#t 2010
#c 8
#% 126338
#% 175284
#% 268079
#% 273911
#% 290830
#% 300167
#% 397353
#% 480966
#% 723279
#% 765500
#% 800502
#% 893118
#% 960326
#% 1015278
#% 1023420
#% 1054227
#% 1063553
#% 1127354
#% 1127560
#% 1180004
#% 1206585
#! Implementations of map-reduce are being used to perform many operations on very large data. We examine strategies for joining several relations in the map-reduce environment. Our new approach begins by identifying the "map-key," the set of attributes that identify the Reduce process to which a Map process must send a particular tuple. Each attribute of the map-key gets a "share," which is the number of buckets into which its values are hashed, to form a component of the identifier of a Reduce process. Relations have their tuples replicated in limited fashion, the degree of replication depending on the shares for those map-key attributes that are missing from their schema. We study the problem of optimizing the shares, given a fixed number of Reduce processes. An algorithm for detecting and fixing problems where an attribute is "mistakenly" included in the map-key is given. Then, we consider two important special cases: chain joins and star joins. In each case we are able to determine the map-key and determine the shares that yield the least replication. While the method we propose is not always superior to the conventional way of using map-reduce to implement joins, there are some important cases involving large-scale data where our method wins, including: (1) analytic queries in which a very large fact table is joined with smaller dimension tables, and (2) queries involving paths through graphs with high out-degree, such as the Web or a social network.

#index 1372691
#* k-symmetry model for identity anonymization in social networks
#@ Wentao Wu;Yanghua Xiao;Wei Wang;Zhenying He;Zhihui Wang
#t 2010
#c 8
#% 443463
#% 576761
#% 956511
#% 1029292
#% 1063476
#% 1127360
#% 1131498
#% 1181255
#% 1206763
#% 1328188
#! With more and more social network data being released, protecting the sensitive information within social networks from leakage has become an important concern of publishers. Adversaries with some background structural knowledge about a target individual can easily re-identify him from the network, even if the identifiers have been replaced by randomized integers(i.e., the network is naively-anonymized). Since there exists numerous topological information that can be used to attack a victim's privacy, to resist such structural re-identification becomes a great challenge. Previous works only investigated a minority of such structural attacks, without considering protecting against re-identification under any potential structural knowledge about a target. To achieve this objective, in this paper we propose k-symmetry model, which modifies a naively-anonymized network so that for any vertex in the network, there exist at least k -- 1 structurally equivalent counterparts. We also propose sampling methods to extract approximate versions of the original network from the anonymized network so that statistical properties of the original network could be evaluated. Extensive experiments show that we can successfully recover a variety of such properties of the original network through aggregations on quite a small number of sample graphs.

#index 1372692
#* Private record matching using differential privacy
#@ Ali Inan;Murat Kantarcioglu;Gabriel Ghinita;Elisa Bertino
#t 2010
#c 8
#% 86950
#% 300184
#% 301569
#% 575969
#% 576761
#% 654448
#% 659991
#% 743280
#% 819551
#% 864406
#% 864412
#% 864413
#% 864414
#% 913783
#% 954159
#% 960288
#% 1127419
#% 1206749
#% 1670071
#% 1718378
#% 1740518
#% 1779022
#! Private matching between datasets owned by distinct parties is a challenging problem with several applications. Private matching allows two parties to identify the records that are close to each other according to some distance functions, such that no additional information other than the join result is disclosed to any party. Private matching can be solved securely and accurately using secure multi-party computation (SMC) techniques, but such an approach is prohibitively expensive in practice. Previous work proposed the release of sanitized versions of the sensitive datasets which allows blocking, i.e., filtering out sub-sets of records that cannot be part of the join result. This way, SMC is applied only to a small fraction of record pairs, reducing the matching cost to acceptable levels. The blocking step is essential for the privacy, accuracy and efficiency of matching. However, the state-of-the-art focuses on sanitization based on k-anonymity, which does not provide sufficient privacy. We propose an alternative design centered on differential privacy, a novel paradigm that provides strong privacy guarantees. The realization of the new model presents difficult challenges, such as the evaluation of distance-based matching conditions with the help of only a statistical queries interface. Specialized versions of data indexing structures (e.g., kd-trees) also need to be devised, in order to comply with differential privacy. Experiments conducted on the real-world Census-income dataset show that, although our methods provide strong privacy, their effectiveness in reducing matching cost is not far from that of k-anonymity based counterparts.

#index 1372693
#* The hardness and approximation algorithms for l-diversity
#@ Xiaokui Xiao;Ke Yi;Yufei Tao
#t 2010
#c 8
#% 67453
#% 263982
#% 300184
#% 410276
#% 443463
#% 488324
#% 576111
#% 576761
#% 577239
#% 785363
#% 800514
#% 800515
#% 801690
#% 809245
#% 810011
#% 824726
#% 864406
#% 874892
#% 874988
#% 874989
#% 881483
#% 881546
#% 881551
#% 883236
#% 893100
#% 937550
#% 960239
#% 960289
#% 960291
#% 1022246
#% 1022247
#% 1022264
#% 1022265
#% 1022266
#% 1221178
#% 1335820
#% 1700134
#! The existing solutions to privacy preserving publication can be classified into the theoretical and heuristic categories. The former guarantees provably low information loss, whereas the latter incurs gigantic loss in the worst case, but is shown empirically to perform well on many real inputs. While numerous heuristic algorithms have been developed to satisfy advanced privacy principles such as l-diversity, t-closeness, etc., the theoretical category is currently limited to k-anonymity which is the earliest principle known to have severe vulnerability to privacy attacks. Motivated by this, we present the first theoretical study on l-diversity, a popular principle that is widely adopted in the literature. First, we show that optimal l-diverse generalization is NP-hard even when there are only 3 distinct sensitive values in the microdata. Then, an (l · d)-approximation algorithm is developed, where d is the dimensionality of the underlying dataset. This is the first known algorithm with a non-trivial bound on information loss. Extensive experiments with real datasets validate the effectiveness and efficiency of proposed solution.

#index 1372694
#* Let SQL drive the XQuery workhorse (XQuery join graph isolation)
#@ Torsten Grust;Manuel Mayr;Jan Rittinger
#t 2010
#c 8
#% 116043
#% 210169
#% 318049
#% 397375
#% 487257
#% 742563
#% 765488
#% 803121
#% 824680
#% 875010
#% 893112
#% 942739
#% 960317
#% 994015
#% 1015274
#% 1016150
#% 1022209
#% 1217193
#% 1698909
#! A purely relational account of the true XQuery semantics can turn any relational database system into an XQuery processor. Compiling nested expressions of the fully compositional XQuery language, however, yields odd algebraic plan shapes featuring scattered distributions of join operators that currently overwhelm commercial SQL query optimizers. This work rewrites such plans before submission to the relational database back-end. Once cast into the shape of join graphs, we have found off-the-shelf relational query optimizers---the B-tree indexing subsystem and join tree planner, in particular---to cope and even be autonomously capable of "reinventing" advanced processing strategies that have originally been devised specifically for the XQuery domain, e.g., XPath step reordering, axis reversal, and path stitching. Performance assessments provide evidence that relational query engines are among the most versatile and efficient XQuery processors readily available today.

#index 1372695
#* Statistics-based parallelization of XPath queries in shared memory systems
#@ Rajesh Bordawekar;Lipyeow Lim;Anastasios Kementsietsidis;Bryant Wei-Lun Kok
#t 2010
#c 8
#% 299275
#% 330305
#% 345693
#% 394617
#% 397364
#% 397379
#% 458836
#% 465018
#% 480488
#% 650962
#% 824675
#% 824752
#% 881734
#% 893106
#% 957907
#% 960276
#% 964486
#% 983052
#% 993968
#% 993970
#% 1016149
#% 1063550
#% 1127387
#% 1129950
#% 1179197
#% 1181228
#% 1368900
#! The wide availability of commodity multi-core systems presents an opportunity to address the latency issues that have plaqued XML query processing. However, simply executing multiple XML queries over multiple cores merely addresses the throughput issue: intra-query parallelization is needed to exploit multiple processing cores for better latency. Toward this effort, this paper investigates the parallelization of individual XPath queries over shared-address space multi-core processors. Much previous work on parallelizing XPath in a distributed setting failed to exploit the shared memory parallelism of multi-core systems. We propose a novel, end-to-end parallelization framework that determines the optimal way of parallelizing an XML query. This decision is based on a statistics-based approach that relies both on the query specifics and the data statistics. At each stage of the parallelization process, we evaluate three alternative approaches, namely, data-, query-, and hybrid-partitioning. For a given XPath query, our parallelization algorithm uses XML statistics to estimate the relative efficiencies of these different alternatives and find an optimal parallel XPath processing plan. Our experiments using well-known XML documents validate our parallel cost model and optimization framework, and demonstrate that it is possible to accelerate XPath processing using commodity multi-core systems.

#index 1372696
#* Efficient physical operators for cost-based XPath execution
#@ Haris Georgiadis;Minas Charalambides;Vasilis Vassalos
#t 2010
#c 8
#% 340144
#% 397366
#% 397375
#% 482653
#% 745463
#% 800577
#% 824667
#% 824675
#% 881734
#% 893112
#% 993953
#% 1015298
#% 1217192
#% 1304602
#% 1688278
#% 1698909
#! The creation of a generic and modular query optimization and processing infrastructure can provide significant benefits to XML data management. Key pieces of such an infrastructure are the physical operators that are available to the execution engine, to turn queries into execution plans. Such operators, to be efficient, need to implement sophisticated algorithms for logical XPath or XQuery operations. Moreover, to enable a cost-based optimizer to choose among them correctly, it is also necessary to provide cost models for such operator implementations. In this paper we present two novel families of algorithms for XPath physical operators, called LookUp (LU) and Sort-Merge-based (SM), along with detailed cost models. Our algorithms have significantly better performance compared to existing techniques over any one of a variety of different XML storage systems that provide a set of common primitive access methods. To substantiate the robustness and efficiency of our physical operators, we evaluate their individual performance over four different XML storage engines against operators that implement existing XPath processing techniques. We also demonstrate the performance gains for twig processing of using plans consisting of our operators compared to a state of the art holistic technique, specifically Twig2Stack. Additionally, we evaluate the precision of our cost models, and we conduct an analysis of the sensitivity of our algorithms and cost models to a variety of parameters.

#index 1372697
#* Adaptive join processing in pipelined plans
#@ Kwanchai Eurviriyanukul;Norman W. Paton;Alvaro A. A. Fernandes;Steven J. Lynden
#t 2010
#c 8
#% 136740
#% 172900
#% 248793
#% 300167
#% 387508
#% 480825
#% 765434
#% 765437
#% 765456
#% 810016
#% 810056
#% 1016208
#% 1026989
#% 1728694
#% 1728700
#! In adaptive query processing, the way in which a query is evaluated is changed in the light of feedback obtained from the environment during query evaluation. Such feedback may, for example, establish that misleading selectivity estimates were used when the query was compiled, leading to the optimizer choosing an inappropriate join order or unsuitable join algorithms. This paper describes how joins can be reordered, and the join algorithms used replaced, while they are being evaluated in pipelined plans. Where joins are reordered and/or replaced during their evaluation, the approach avoids duplicating work that has already been carried out, by resuming from where the previous plan left off. The approach has been evaluated empirically, and shown to be effective for improving query performance in the light of misleading selectivity estimates.

#index 1372698
#* BSkyTree: scalable skyline computation using a balanced pivot selection
#@ Jongwuk Lee;Seung-won Hwang
#t 2010
#c 8
#% 287414
#% 288976
#% 289148
#% 654480
#% 824670
#% 864451
#% 993954
#% 1022224
#% 1022225
#% 1092017
#% 1206998
#% 1217183
#% 1254218
#! Skyline queries have gained a lot of attention for multi-criteria analysis in large-scale datasets. While existing skyline algorithms have focused mostly on exploiting data dominance to achieve efficiency, we propose that data incomparability should be treated as another key factor in optimizing skyline computation. Specifically, to optimize both factors, we first identify common modules shared by existing non-index skyline algorithms, and then analyze them to develop a cost model to guide a balanced pivot point selection. Based on the cost model, we lastly implement our balanced pivot selection in two algorithms, BSkyTree-S and BSkyTree-P, treating both dominance and incomparability as key factors. Our experimental results demonstrate that proposed algorithms outperform state-of-the-art skyline algorithms up to two orders of magnitude.

#index 1372699
#* Stream schema: providing and exploiting static metadata for data stream processing
#@ Peter M. Fischer;Kyumars Sheykh Esmaili;Renée J. Miller
#t 2010
#c 8
#% 101943
#% 114677
#% 378388
#% 397352
#% 462060
#% 479814
#% 578391
#% 654444
#% 654497
#% 726621
#% 736391
#% 771230
#% 781453
#% 824674
#% 824795
#% 1015299
#% 1016169
#% 1022208
#% 1063480
#% 1065547
#% 1127373
#% 1127569
#% 1181292
#% 1206741
#% 1207016
#% 1217239
#% 1688281
#! Schemas, and more generally metadata specifying structural and semantic constraints, are invaluable in data management. They facilitate conceptual design and enable checking of data consistency. They also play an important role in permitting semantic query optimization, that is, optimization and processing strategies that are often highly effective, but only correct for data conforming to a given schema. While the use of metadata is well-established in relational and XML databases, the same is not true for data streams. The existing work mostly focuses on the specification of dynamic information. In this paper, we consider the specification of static metadata for streams in a model called Stream Schema. We show how Stream Schema can be used to validate the consistency of streams. By explicitly modeling stream constraints, we show that stream queries can be simplified by removing predicates or subqueries that check for consistency. This can greatly enhance pro-grammability of stream processing systems. We also present a set of semantic query optimization strategies that both permit compile-time checking of queries (for example, to detect empty queries) and new runtime processing options, options that would not have been possible without a Stream Schema specification. Case studies on two stream processing platforms (covering different applications and underlying stream models), along with an experimental evaluation, show the benefits of Stream Schema.

#index 1372700
#* Warm cache costing: a feedback optimization technique for buffer pool aware costing
#@ H. S. Ramanujam;Edwin Seputis
#t 2010
#c 8
#% 102784
#% 136740
#% 169841
#% 244119
#% 248793
#% 273901
#% 321250
#% 411554
#% 427219
#% 480803
#% 565457
#% 1052068
#% 1206720
#! Most modern RDBMS depend on the query processing optimizer's cost model to choose the best execution plan for a given query. Since the physical IO (PIO) is a costly operation to execute, it naturally has an important weight in RDBMS classical cost models, which assume that the data is disk-resident and does not fit in the available main memory. However, this assumption is no longer true with the advent of cheap large main memories. In this paper, we discuss the importance of considering the buffer-cache occupancy during optimization and propose the Warm Cache Costing (WCC) model as a new technique for buffer-pool aware query optimization. The WCC-model is a novel feedback optimization technique, based on the execution statistics by learning PIO-compensation (PIOC) factors. The PIOC factor defines the average percentage of a table which is cached in the buffer pool. These PIOC factors are used in subsequent query optimizations to better estimate the PIO, thus leading to better plans. These techniques have been implemented in Sybase Adaptive Server Enterprise (ASE) database system. We have observed that they provide considerable improvements in query timings, in Decision Support environments and with almost negligible regression(if any) in other environments. This model enjoys the advantage of requiring no change to the buffer manager or other modules underlying the query processing layer and therefore is easy to implement. Also, since this model is part of an extensive feedback optimization architecture, other techniques using feedback optimization framework can be plugged in easily.

#index 1372701
#* Position list word aligned hybrid: optimizing space and performance for compressed bitmaps
#@ François Deliège;Torben Bach Pedersen
#t 2010
#c 8
#% 227861
#% 248814
#% 316523
#% 333930
#% 462217
#% 466953
#% 480329
#% 482713
#% 571294
#% 838525
#% 864446
#% 866981
#% 893158
#% 1197046
#! Compressed bitmap indexes are increasingly used for efficiently querying very large and complex databases. The Word Aligned Hybrid (WAH) bitmap compression scheme is commonly recognized as the most efficient compression scheme in terms of CPU efficiency. However, WAH compressed bitmaps use a lot of storage space. This paper presents the Position List Word Aligned Hybrid (PLWAH) compression scheme that improves significantly over WAH compression by better utilizing the available bits and new CPU instructions. For typical bit distributions, PLWAH compressed bitmaps are often half the size of WAH bitmaps and, at the same time, offer an even better CPU efficiency. The results are verified by theoretical estimates and extensive experiments on large amounts of both synthetic and real-world data.

#index 1372702
#* Reducing metadata complexity for faster table summarization
#@ K. Selçuk Candan;Mario Cataldi;Maria Luisa Sapino
#t 2010
#c 8
#% 223781
#% 248790
#% 274612
#% 438137
#% 443463
#% 443531
#% 463895
#% 479465
#% 577239
#% 617864
#% 631985
#% 635924
#% 726681
#% 800515
#% 801690
#% 810011
#% 824712
#% 853016
#% 864412
#% 881551
#% 885860
#% 907535
#% 956565
#% 1022265
#% 1022287
#% 1063535
#% 1181221
#% 1408782
#% 1692959
#% 1693388
#% 1696308
#! Since the visualization real estate puts stringent constraints on how much data can be presented to the users at once, table summarization is an essential tool in helping users quickly explore large data sets. An effective summary needs to minimize the information loss due to the reduction in details. Summarization algorithms leverage the redundancy in the data to identify value and tuple clustering strategies that represent the (almost) same amount of information with a smaller number of data representatives. It has been shown that, when available, metadata, such as value hierarchies associated to the attributes of the tables, can help greatly reduce the resulting information loss. However, table summarization, whether carried out through data analysis performed on the table from scratch or supported through already available metadata, is an expensive operation. We note that the table summarization process can be significantly sped up when the metadata used for supporting the summarization itself is pre-processed to reduce the unnecessary details. The pre-processing of the metadata, however, needs to be performed carefully to ensure that it does not add significant amounts of additional loss to the table summarization process. In this paper, we propose a tRedux algorithm for value hierarchy pre-processing and reduction. Experimental evaluations show that, depending on the table and taxonomy complexity, metadata summarization can provide gains in table summarization time that can range (in absolute values) from seconds to 10s-of-1000s of seconds. Consequently, while resulting in only an extra ~ 20% reduction in table quality, tRedux can provide ~ 2x speedups in table summarization time. Experiments also show that tRedux has a better performance than alternative metadata reduction strategies in supporting table summarization; and, as the taxonomy complexity increases, the absolute gains of tRedux also increase.

#index 1372703
#* Anchoring millions of distinct reads on the human genome within seconds
#@ Tien Huynh;Michail Vlachos;Isidore Rigoutsos
#t 2010
#c 8
#% 210186
#% 833451
#% 960303
#% 1041597
#% 1231157
#! With the advent of next-generation DNA sequencing machines, there is an increasing need for the development of computational tools that can anchor accurately and expediently the millions of generated short DNA sequences (or reads) onto the genomes of target organisms. In this work, we describe 'Q-Pick', a new and efficient method for solving this problem. Q-Pick allows the rapid identification and anchoring of such reads with possible wildcards in large genomic databases, while guaranteeing completeness of results and efficiency of operation. Q-Pick requires very spartan memory and computational resources, and is trivially amenable to SIMD implementation; it can also be easily extended to handle longer reads, e.g. 75-mers or longer. Our experiments indicate that Q-Pick can anchor millions of distinct short reads against both strands of a mammalian genome in seconds, using a single-core computer processor.

#index 1372704
#* Suffix tree construction algorithms on modern hardware
#@ Dimitris Tsirogiannis;Nick Koudas
#t 2010
#c 8
#% 24756
#% 143823
#% 176031
#% 198770
#% 235941
#% 289010
#% 300194
#% 300312
#% 317163
#% 480484
#% 509775
#% 566122
#% 737254
#% 745510
#% 789004
#% 824719
#% 829998
#% 940924
#% 960303
#% 1022230
#% 1022311
#% 1116726
#% 1206844
#% 1217209
#! Suffix trees are indexing structures that enhance the performance of numerous string processing algorithms. In this paper, we propose cache-conscious suffix tree construction algorithms that are tailored to CMP architectures. The proposed algorithms utilize a novel sample-based cache partitioning algorithm to improve cache performance and exploit on-chip parallelism on CMPs. Furthermore, several compression techniques are applied to effectively trade space for cache performance. Through an extensive experimental evaluation using real text data from different domains, we demonstrate that the algorithms proposed herein exhibit better cache performance than their cache-unaware counterparts and effectively utilize all processing elements, achieving satisfactory speedup.

#index 1372705
#* Splash: ad-hoc querying of data and statistical models
#@ Lujun Fang;Kristen LeFevre
#t 2010
#c 8
#% 115661
#% 201921
#% 210182
#% 248813
#% 280441
#% 342704
#% 420053
#% 420064
#% 464998
#% 479787
#% 481290
#% 482092
#% 664547
#% 725307
#% 789091
#% 824734
#% 844324
#% 865371
#% 874976
#% 881575
#% 960324
#% 978636
#% 981000
#% 993943
#% 1022276
#% 1072639
#% 1328066
#% 1328120
#% 1706199
#% 1711256
#! Data mining is increasingly performed by people who are not computer scientists or professional programmers. It is often done as an iterative process involving multiple ad-hoc tasks, as well as data pre- and post-processing, all of which must be executed over large databases. In order to make data mining more accessible, it is critical to provide a simple, easy-to-use language that allows the user to specify ad-hoc data processing, model construction, and model manipulation. Simultaneously, it is necessary for the underlying system to scale up to large datasets. Unfortunately, while each of these requirements can be satisfied, individually, by existing systems, no system fully satisfies all criteria. In this paper, we present a system called Splash to fill this void. Splash supports an extended relational data model and SQL query language, which allows for the natural integration of statistical modeling and ad-hoc data processing. It also supports a novel representatives operator to help explain models using a limited number of examples. We have developed a prototype implementation of Splash. Our experimental study indicates that it scales well to large input datasets. Further, to demonstrate the simplicity of the language, we conducted a case study using Splash to perform a series of exploratory analyses using network log data. Our study indicates that the query-based interface is simpler than a common data mining software package, and it often requires less programming effort to use.

#index 1372706
#* Techniques for efficiently querying scientific workflow provenance graphs
#@ Manish Kumar Anand;Shawn Bowers;Bertram Ludäscher
#t 2010
#c 8
#% 275922
#% 864462
#% 875046
#% 879803
#% 879809
#% 1028475
#% 1042656
#% 1042658
#% 1042660
#% 1059972
#% 1063500
#% 1063514
#% 1063544
#% 1063545
#% 1063593
#% 1181294
#% 1206750
#% 1207025
#% 1728174
#% 1728182
#! A key advantage of scientific workflow systems over traditional scripting approaches is their ability to automatically record data and process dependencies introduced during workflow runs. This information is often represented through provenance graphs, which can be used by scientists to better understand, reproduce, and verify scientific results. However, while most systems record and store data and process dependencies, few provide easy-to-use and efficient approaches for accessing and querying provenance information. Instead, users formulate provenance graph queries directly against physical data representations (e.g., relational, XML, or RDF), leading to queries that are difficult to express and expensive to evaluate. We address these problems through a high-level query language tailored for expressing provenance graph queries. The language is based on a general model of provenance supporting scientific workflows that process XML data and employ update semantics. Query constructs are provided for querying both structure and lineage information. Unlike other languages that return sets of nodes as answers, our query language is closed, i.e., answers to lineage queries are sets of lineage dependencies (edges) allowing answers to be further queried. We provide a formal semantics for the language and present novel techniques for efficiently evaluating lineage queries. Experimental results on real and synthetic provenance traces demonstrate that our lineage based optimizations outperform an in-memory and standard database implementation by orders of magnitude. We also show that our strategies are feasible and can significantly reduce both provenance storage size and query execution time when compared with standard approaches.

#index 1372707
#* Fine-grained and efficient lineage querying of collection-based workflow provenance
#@ Paolo Missier;Norman W. Paton;Khalid Belhajjame
#t 2010
#c 8
#% 462072
#% 769242
#% 825661
#% 875046
#% 879803
#% 879809
#% 978444
#% 1016179
#% 1022327
#% 1028454
#% 1034470
#% 1036075
#% 1042649
#% 1042658
#% 1042660
#% 1063500
#% 1063544
#% 1063545
#% 1063571
#% 1174010
#% 1181294
#% 1206750
#% 1207025
#% 1410802
#% 1692848
#% 1720925
#% 1728177
#! The management and querying of workflow provenance data underpins a collection of activities, including the analysis of workflow results, and the debugging of workflows or services. Such activities require efficient evaluation of lineage queries over potentially complex and voluminous provenance logs. Näive implementations of lineage queries navigate provenance logs by joining tables that represent the flow of data between connected processors invoked from workflows. In this paper we provide an approach to provenance querying that: (i) avoids joins over provenance logs by using information about the workflow definition to inform the construction of queries that directly target relevant lineage results; (ii) provides fine grained provenance querying, even for workflows that create and consume collections; and (iii) scales effectively to address complex workflows, workflows with large intermediate data sets, and queries over multiple workflows.

#index 1372708
#* Lost source provenance
#@ Jing Zhang;H. V. Jagadish
#t 2010
#c 8
#% 289370
#% 480481
#% 632040
#% 715288
#% 864422
#% 875015
#% 976987
#% 1063544
#% 1063709
#% 1063736
#% 1126566
#% 1127421
#% 1181296
#% 1217186
#% 1661440
#! As the use of derived information has grown in recent years, the importance of provenance has been recognized, and there has been a great deal of effort devoted to developing techniques to identify individual source tuples used in the derivation of any result tuple. Often, however, the source database may have been updated since the result was derived, and the source tuples of interest are not in the database any more. In such situations, the provenance management system has to reconstruct relevant historical fragments of the source database as they were at derivation time. In this paper, we develop techniques to address this problem. Our experimental assessment shows that these techniques do so efficiently, and with low storage overhead.

#index 1372709
#* Bridging the gap between intensional and extensional query evaluation in probabilistic databases
#@ Abhay Jha;Dan Olteanu;Dan Suciu
#t 2010
#c 8
#% 44876
#% 150115
#% 231738
#% 303620
#% 311758
#% 388024
#% 708752
#% 810098
#% 893167
#% 976984
#% 977013
#% 1016201
#% 1027276
#% 1063521
#% 1127376
#% 1206717
#% 1206732
#% 1206987
#% 1217251
#! There are two broad approaches to query evaluation over probabilistic databases: (1) Intensional Methods proceed by manipulating expressions over symbolic events associated with uncertain tuples. This approach is very general and can be applied to any query, but requires an expensive postprocessing phase, which involves some general-purpose probabilistic inference. (2) Extensional Methods, on the other hand, evaluate the query by translating operations over symbolic events to a query plan; extensional methods scale well, but they are restricted to safe queries. In this paper, we bridge this gap by proposing an approach that can translate the evaluation of any query into extensional operators, followed by some post-processing that requires probabilistic inference. Our approach uses characteristics of the data to adapt smoothly between the two evaluation strategies. If the query is safe or becomes safe because of the data instance, then the evaluation is completely extensional and inside the database. If the query/data combination departs from the ideal setting of a safe query, then some intensional processing is performed, whose complexity depends only on the distance from the ideal setting.

#index 1372710
#* Probabilistic threshold k nearest neighbor queries over moving objects in symbolic indoor space
#@ Bin Yang;Hua Lu;Christian S. Jensen
#t 2010
#c 8
#% 201876
#% 287466
#% 300174
#% 397377
#% 427199
#% 438456
#% 527176
#% 527187
#% 574283
#% 729850
#% 772835
#% 793537
#% 824723
#% 993955
#% 1015321
#% 1016199
#% 1080155
#% 1127377
#% 1181270
#% 1245052
#% 1292533
#! The availability of indoor positioning renders it possible to deploy location-based services in indoor spaces. Many such services will benefit from the efficient support for k nearest neighbor (kNN) queries over large populations of indoor moving objects. However, existing kNN techniques fall short in indoor spaces because these differ from Euclidean and spatial network spaces and because of the limited capabilities of indoor positioning technologies. To contend with indoor settings, we propose the new concept of minimal indoor walking distance (MIWD) along with algorithms and data structures for distance computing and storage; and we differentiate the states of indoor moving objects based on a positioning device deployment graph, utilize these states in effective object indexing structures, and capture the uncertainty of object locations. On these foundations, we study the probabilistic threshold kNN (PTkNN) query. Given a query location q and a probability threshold T, this query returns all subsets of k objects that have probability larger than T of containing the kNN query result of q. We propose a combination of three techniques for processing this query. The first uses the MIWD metric to prune objects that are too far away. The second uses fast probability estimates to prune unqualified objects and candidate result subsets. The third uses efficient probability evaluation for computing the final result on the remaining candidate subsets. An empirical study using both synthetic and real data shows that the techniques are efficient.

#index 1372711
#* Probabilistic path queries in road networks: traffic uncertainty aware path selection
#@ Ming Hua;Jian Pei
#t 2010
#c 8
#% 196407
#% 319222
#% 415580
#% 443208
#% 599545
#% 849816
#% 875476
#% 985930
#% 985951
#% 1019096
#% 1022268
#% 1063472
#% 1308787
#% 1676469
#! Path queries such as "finding the shortest path in travel time from my hotel to the airport" are heavily used in many applications of road networks. Currently, simple statistic aggregates such as the average travel time between two vertices are often used to answer path queries. However, such simple aggregates often cannot capture the uncertainty inherent in traffic. In this paper, we study how to take traffic uncertainty into account in answering path queries in road networks. To capture the uncertainty in traffic such as the travel time between two vertices, the weight of an edge is modeled as a random variable and is approximated by a set of samples. We propose three novel types of probabilistic path queries using basic probability principles: (1) a probabilistic path query like "what are the paths from my hotel to the airport whose travel time is at most 30 minutes with a probability of at least 90%?"; (2) a weight-threshold top-k path query like "what are the top-3 paths from my hotel to the airport with the highest probabilities to take at most 30 minutes?"; and (3) a probability-threshold top-k path query like "what are the top-3 shortest paths from my hotel to the airport whose travel time is guaranteed by a probability of at least 90%?" To evaluate probabilistic path queries efficiently, we develop three efficient probability calculation methods: an exact algorithm, a constant factor approximation method and a sampling based approach. Moreover, we devise the P* algorithm, a best-first search method based on a novel hierarchical partition tree index and three effective heuristic evaluation functions. An extensive empirical study using real road networks and synthetic data sets shows the effectiveness of the proposed path queries and the efficiency of the query evaluation methods.

#index 1372712
#* A simple (yet powerful) algebra for pervasive environments
#@ Yann Gripay;Frédérique Laforest;Jean-Marc Petit
#t 2010
#c 8
#% 273912
#% 300169
#% 300179
#% 387089
#% 433335
#% 433343
#% 787114
#% 801671
#% 839661
#% 845350
#% 893118
#% 993932
#% 1208230
#% 1668029
#% 1688304
#! Querying non-conventional data is recognized as a major issue in new environments and applications such as those occurring in pervasive computing. A key issue is the ability to query data, streams and services in a declarative way. Our overall objective is to make the development of pervasive applications easier through database principles. In this paper, through the notion of virtual attributes and binding patterns, we define a data-centric view of pervasive environments: the classical notion of database is extended to come up with a broader notion, defined as relational pervasive environment, integrating data, streams and active/passive services. Then, the so-called Serena algebra is proposed with operators to homogeneously handle data and services. Moreover, the notion of stream can also be smoothly integrated into this algebra. A prototype of Pervasive Environment Management System has been implemented on which first experiments have been conducted to validate our approach.

#index 1372713
#* Self-selecting, self-tuning, incrementally optimized indexes
#@ Goetz Graefe;Harumi Kuno
#t 2010
#c 8
#% 36119
#% 64791
#% 155393
#% 287715
#% 326335
#% 435159
#% 463917
#% 481771
#% 482831
#% 544469
#% 867058
#% 960268
#% 997495
#% 1022202
#% 1044448
#% 1052068
#% 1206648
#! In a relational data warehouse with many tables, the number of possible and promising indexes exceeds human comprehension and requires automatic index tuning. While monitoring and reactive index tuning have been proposed, adaptive indexing focuses on adapting the physical database layout for and by actual queries. "Database cracking" is one such technique. Only if and when a column is used in query predicates, an index for the column is created; and only if and when a key range is queried, the index is optimized for this key range. The effect is akin to a sort that is adaptive and incremental. This sort is, however, very inefficient, particularly when applied on block-access devices. In contrast, traditional index creation sorts data with an efficient merge sort optimized for block-access devices, but it is neither adaptive nor incremental. We propose adaptive merging, an adaptive, incremental, and efficient technique for index creation. Index optimization focuses on key ranges used in actual queries. The resulting index adapts more quickly to new data and to new query patterns than database cracking. Sort efficiency is comparable to that of traditional B-tree creation. Nonetheless, the new technique promises better query performance than database cracking, both in memory and on block-access storage.

#index 1372714
#* Minimizing database repros using language grammars
#@ Nicolas Bruno
#t 2010
#c 8
#% 286280
#% 345031
#% 397375
#% 465167
#% 479656
#% 868113
#% 1063540
#% 1247787
#! Database engines and database-centric applications have become complex software systems. Ensuring bug-free database services is therefore a very difficult task. Whenever possible, bugs that are uncovered during testing are associated with a repro, or sequence of steps that deterministically reproduce the problem. Unfortunately, due to factors such as automated test generation, repros are generally too long and complex. This issue prevents developers reacting quickly to new bugs, since usually a long manual "repro-minimization" phase occurs before the actual debugging takes place. In this paper we present a fully automated technique to minimize database repros that leverages underlying language grammars and thus is significantly more focused than previous approaches. Our approach has been successfully used in two commercial database products to isolate and simplify bugs during early development stages. We show that our technique consistently results in repros that are as concise or simpler and obtained much faster than alternative ones carefully constructed manually.

#index 1372715
#* Efficient and scalable multi-geography route planning
#@ Vidhya Balasubramanian;Dmitri V. Kalashnikov;Sharad Mehrotra;Nalini Venkatasubramanian
#t 2010
#c 8
#% 115559
#% 207879
#% 214769
#% 289053
#% 443208
#% 443533
#% 527029
#% 576214
#% 660985
#% 677994
#% 778699
#% 871766
#% 960236
#% 967274
#% 976785
#% 1016199
#% 1063472
#% 1214770
#% 1230352
#% 1276426
#% 1655672
#! This paper considers the problem of Multi-Geography Route Planning (MGRP) where the geographical information may be spread over multiple heterogeneous interconnected maps. We first design a flexible and scalable representation to model individual geographies and their interconnections. Given such a representation, we develop an algorithm that exploits precomputation and caching of geographical data for path planning. A utility-based approach is adopted to decide which paths to precompute and store. To validate the proposed approach we test the algorithm over the workload of a campus level evacuation simulation that plans evacuation routes over multiple geographies: indoor CAD maps, outdoor maps, pedestrian and transportation networks, etc. The empirical results indicate that the MGRP algorithm with the proposed utility based caching strategy significantly outperforms the state of the art solutions when applied to a large university campus data under varying conditions.

#index 1372716
#* Querying trajectories using flexible patterns
#@ Marcos R. Vieira;Petko Bakalov;Vassilis J. Tsotras
#t 2010
#c 8
#% 333854
#% 378405
#% 443521
#% 464058
#% 480473
#% 480817
#% 763881
#% 765451
#% 769899
#% 803125
#% 824724
#% 838535
#% 866987
#% 878300
#% 881459
#% 960283
#% 993955
#% 1063480
#% 1072634
#! The wide adaptation of GPS and cellular technologies has created many applications that collect and maintain large repositories of data in the form of trajectories. Previous work on querying/analyzing trajectorial data typically falls into methods that either address spatial range and NN queries, or, similarity based queries. Nevertheless, trajectories are complex objects whose behavior over time and space can be better captured as a sequence of interesting events. We thus facilitate the use of motion "pattern" queries which allow the user to select trajectories based on specific motion patterns. Such patterns are described as regular expressions over a spatial alphabet that can be implicitly or explicitly anchored to the time domain. Moreover, we are interested in "flexible" patterns that allow the user to include "variables" in the query pattern and thus greatly increase its expressive power. In this paper we introduce a framework for efficient processing of flexible pattern queries. The framework includes an underlying indexing structure and algorithms for query processing using different evaluation strategies. An extensive performance evaluation of this framework shows significant performance improvement when compared to existing solutions.

#index 1372717
#* Querying spatial patterns
#@ Vishwakarma Singh;Arnab Bhattacharya;Ambuj K. Singh
#t 2010
#c 8
#% 215214
#% 287466
#% 333854
#% 345848
#% 381202
#% 427199
#% 428926
#% 443133
#% 487221
#% 588726
#% 641573
#% 645687
#% 652272
#% 730016
#% 737972
#% 755467
#% 760805
#% 780859
#% 784274
#% 824724
#% 824956
#% 840455
#% 844327
#% 883972
#% 1030801
#% 1728708
#% 1775720
#! Spatial data are common in many scientific and commercial domains such as geographical information systems and gene/protein expression profiles. Querying for distribution patterns on such data can discover underlying spatial relationships and suggest avenues for further scientific exploration. Supporting such pattern retrieval requires not only the formulation of an appropriate scoring function for defining relevant connected subregions, but also the design of new access methods that can scale to large databases. In this paper, we propose a solution to this problem of querying significant sub-regions on spatial data provided as raster images. We design a scoring scheme to measure the similarity of subregions. All the raster images are tiled and each alignment of the query and a database image produces a tile score matrix. We show that the problem of finding the best connected subregion from this matrix is NP-hard and develop a dynamic programming heuristic. With this heuristic, we develop two index-based scalable search strategies, TARS and SPARS, to query patterns in large data repositories. Experimental results on real image datasets show that TARS offers an 87% improvement for small queries, and SPARS a 52% improvement in runtime for large queries, as compared to linear search. Qualitative tests on real datasets achieve precision of more than 80%.

#index 1372718
#* Indexing relations on the web
#@ Sergio Luis Sardi Mergen;Juliana Freire;Carlos Alberto Heuser
#t 2010
#c 8
#% 273927
#% 300288
#% 303884
#% 328424
#% 572311
#% 572314
#% 654459
#% 659990
#% 660011
#% 845350
#% 960237
#% 993987
#% 1022257
#% 1127393
#% 1134501
#! There has been a substantial increase in the volume of (semi) structured data on the Web. This opens new opportunities for exploring and querying these data that goes beyond the keyword-based queries traditionally used on the Web. But supporting queries over a very large number of apparently disconnected Web sources is challenging. In this paper we propose index methods that capture both the structure of the sources and connections between them. The indexes are designed for data that is represented as relations, such as HTML tables, and support queries with predicates. We show how associations between overlapping sources are discovered, captured in the indexes, and used to derive query rewritings that join multiple sources. We demonstrate, through an experimental evaluation, that our approach scales to a large number of sources.

#index 1372719
#* An execution environment for C-SPARQL queries
#@ Davide Francesco Barbieri;Daniele Braga;Stefano Ceri;Michael Grossniklaus
#t 2010
#c 8
#% 116043
#% 198467
#% 287241
#% 300179
#% 321468
#% 428155
#% 443298
#% 654507
#% 725366
#% 788216
#% 801677
#% 878299
#% 893153
#% 907520
#% 930938
#% 977005
#% 1008060
#% 1015296
#% 1016170
#% 1065547
#% 1121040
#% 1413157
#% 1673562
#% 1684016
#% 1696286
#! Continuous SPARQL (C-SPARQL) is proposed as new language for continuous queries over streams of RDF data. It covers a gap in the Semantic Web abstractions which is needed for many emerging applications, including our focus on Urban Computing. In this domain, sensor-based information on roads must be processed to deduce localized traffic conditions and then produce traffic management strategies. Executing C-SPARQL queries requires the effective integration of SPARQL and streaming technologies, which capitalize over a decade of research and development; such integration poses several nontrivial challenges. In this paper we (a) show the syntax and semantics of the C-SPARQL language together with some examples; (b) introduce a query graph model which is an intermediate representation of queries devoted to optimization; (c) discuss the features of an execution environment that leverages existing technologies; (d) introduce optimizations in terms of rewriting rules applied to the query graph model, so as to efficiently exploit the execution environment; and (e) show evidence of the effectiveness of our optimizations on a prototype of execution environment.

#index 1372720
#* Rewrite techniques for performance optimization of schema matching processes
#@ Eric Peukert;Henrike Berthold;Erhard Rahm
#t 2010
#c 8
#% 317975
#% 480645
#% 480654
#% 572314
#% 660001
#% 723085
#% 735938
#% 790848
#% 807443
#% 864614
#% 903009
#% 924747
#% 957170
#% 960233
#% 993982
#% 1019068
#% 1022236
#% 1090726
#% 1090777
#% 1092531
#% 1207106
#% 1246170
#% 1288166
#% 1409938
#% 1696305
#% 1705177
#! A recurring manual task in data integration, ontology alignment or model management is finding mappings between complex meta data structures. In order to reduce the manual effort, many matching algorithms for semi-automatically computing mappings were introduced. Unfortunately, current matching systems severely lack performance when matching large schemas. Recently, some systems tried to tackle the performance problem within individual matching approaches. However, none of them developed solutions on the level of matching processes. In this paper we introduce a novel rewrite-based optimization technique that is generally applicable to different types of matching processes. We introduce filter-based rewrite rules similar to predicate push-down in query optimization. In addition we introduce a modeling tool and recommendation system for rewriting matching processes. Our evaluation on matching large web service message types shows significant performance improvements without losing the quality of automatically computed results.

#index 1372721
#* Fast computation of SimRank for static and dynamic information networks
#@ Cuiping Li;Jiawei Han;Guoming He;Xin Jin;Yizhou Sun;Yintao Yu;Tianyi Wu
#t 2010
#c 8
#% 80854
#% 200694
#% 290830
#% 411762
#% 447948
#% 577273
#% 729938
#% 769887
#% 769952
#% 805904
#% 806956
#% 810072
#% 818218
#% 840846
#% 881460
#% 881480
#% 881493
#% 915344
#% 961564
#% 989586
#% 989643
#% 1127384
#! Information networks are ubiquitous in many applications and analysis on such networks has attracted significant attention in the academic communities. One of the most important aspects of information network analysis is to measure similarity between nodes in a network. SimRank is a simple and influential measure of this kind, based on a solid theoretical "random surfer" model. Existing work computes SimRank similarity scores in an iterative mode. We argue that the iterative method can be infeasible and inefficient when, as in many real-world scenarios, the networks change dynamically and frequently. We envision non-iterative method to bridge the gap. It allows users not only to update the similarity scores incrementally, but also to derive similarity scores for an arbitrary subset of nodes. To enable the non-iterative computation, we propose to rewrite the SimRank equation into a non-iterative form by using the Kronecker product and vectorization operators. Based on this, we develop a family of novel approximate SimRank computation algorithms for static and dynamic information networks, and give their corresponding theoretical justification and analysis. The non-iterative method supports efficient processing of various node analysis including similarity tracking and centrality tracking on evolving information networks. The effectiveness and efficiency of our proposed methods are evaluated on synthetic and real data sets.

#index 1372722
#* Probabilistic ranking over relations
#@ Lijun Chang;Jeffrey Xu Yu;Lu Qin;Xuemin Lin
#t 2010
#c 8
#% 278831
#% 480819
#% 643566
#% 659255
#% 763882
#% 777931
#% 824697
#% 864394
#% 893167
#% 893189
#% 903016
#% 907562
#% 960293
#% 983263
#% 992830
#% 1044448
#% 1063520
#% 1063542
#% 1063713
#% 1070886
#% 1075132
#% 1127377
#% 1147662
#% 1206717
#% 1206832
#% 1206893
#% 1206972
#% 1217145
#% 1217168
#% 1217169
#% 1217170
#% 1217174
#% 1328151
#! Probabilistic top-k ranking queries have been extensively studied due to the fact that data obtained can be uncertain in many real applications. A probabilistic top-k ranking query ranks objects by the interplay of score and probability, with an implicit assumption that both scores based on which objects are ranked and probabilities of the existence of the objects are stored in the same relation. We observe that in general scores and probabilities are highly possible to be stored in different relations, for example, in column-oriented DBMSs and in data warehouses. In this paper we study probabilistic top-k ranking queries when scores and probabilities are stored in different relations. We focus on reducing the join cost in probabilistic top-k ranking. We investigate two probabilistic score functions, discuss the upper/lower bounds in random access and sequential access, and provide insights on the advantages and disadvantages of random/sequential access in terms of upper/lower bounds. We also propose random, sequential, and hybrid algorithms to conduct probabilistic top-k ranking. We conducted extensive performance studies using real and synthetic datasets, and report our findings in this paper.

#index 1372723
#* Privacy preserving group nearest neighbor queries
#@ Tanzima Hashem;Lars Kulik;Rui Zhang
#t 2010
#c 8
#% 86950
#% 201876
#% 427199
#% 527026
#% 745464
#% 814650
#% 836178
#% 893151
#% 907397
#% 911803
#% 956531
#% 1034732
#% 1063478
#% 1118289
#% 1206712
#% 1206881
#% 1208213
#% 1399015
#% 1409348
#% 1409349
#% 1415670
#! User privacy in location-based services has attracted great interest in the research community. We introduce a novel framework based on a decentralized architecture for privacy preserving group nearest neighbor queries. A group nearest neighbor (GNN) query returns the location of a meeting place that minimizes the aggregate distance from a spread out group of users; for example, a group of users can ask for a restaurant that minimizes the total travel distance from them. We identify the challenges in preserving user privacy for GNN queries and provide a comprehensive solution to this problem. In our approach, users provide their locations as regions instead of exact points to a location service provider (LSP) to preserve their privacy. The LSP returns a set of candidate answers that includes the actual group nearest neighbor. We develop a private filter that determines the actual group nearest neighbor from the retrieved candidate answers without revealing user locations to any involved party, including the LSP. We also propose an efficient algorithm to evaluate GNN queries with respect to the provided set of regions (the users' imprecise locations). An extensive experimental study shows the effectiveness of our proposed technique.

#index 1372724
#* Finding misplaced items in retail by clustering RFID data
#@ Leonardo Weiss Ferreira Chaves;Erik Buchmann;Klemens Böhm
#t 2010
#c 8
#% 625677
#% 644230
#% 824747
#% 873104
#% 893102
#% 893103
#% 926881
#% 1009981
#% 1016227
#% 1036083
#% 1043218
#% 1063523
#% 1065043
#% 1083735
#% 1093763
#% 1150445
#% 1191525
#% 1206706
#% 1206747
#% 1206879
#% 1354049
#% 1719095
#! In retail, products are organized according to layout plans, so-called planograms. Compliance to planograms is important, since good product placement can significantly increase sales. Currently, retailers are about to implement RFID installations consisting of smart shelves and RFID-tagged items to support in-store logistics and processes. In principle, they can also use these installations to implement planogram compliance verification: Each antenna is supposed to detect all tagged items in one location of the planogram. But due to physical constraints, RFID tags can be identified by more than one RFID antenna. Thus, one cannot decide if an item carrying such a tag complies with the planogram. We propose a new method called RPCV which checks planogram compliance on large databases of items. It is based on the observation that the number of times an antenna identifies each item of a certain product type roughly follows a normal distribution. RPCV represents each item as a two-dimensional vector containing the number of readings both by the right antenna and by wrong ones according to the planogram. It clusters this data, separately for each product type. A cluster then is a set of correctly placed items or of misplaced ones. RPCV produces one order of magnitude less wrong predictions than current state of the art, and it requires less data to yield good predictions. A study with RFID-equipped goods and smart shelves shows that our approach is effective in realistic scenarios.

#index 1372725
#* Subsumption and complementation as data fusion operators
#@ Jens Bleiholder;Sascha Szott;Melanie Herschel;Frank Kaufer;Felix Naumann
#t 2010
#c 8
#% 172933
#% 213983
#% 220425
#% 322619
#% 393907
#% 465057
#% 481935
#% 514144
#% 569755
#% 572314
#% 591566
#% 659928
#% 732477
#% 751496
#% 765457
#% 768941
#% 809242
#% 810020
#% 824688
#% 893149
#% 913783
#% 993981
#% 1129527
#% 1130969
#% 1720717
#! The goal of data fusion is to combine several representations of one real world object into a single, consistent representation, e.g., in data integration. A very popular operator to perform data fusion is the minimum union operator. It is defined as the outer union and the subsequent removal of subsumed tuples. Minimum union is used in other applications as well, for instance in database query optimization to rewrite outer join queries, in the semantic web community in implementing Sparql's optional operator, etc. Despite its wide applicability, there are only few efficient implementations, and until now, minimum union is not a relational database primitive. This paper fills this gap as we present implementations of subsumption that serve as a building block for minimum union. Furthermore, we consider this operator as database primitive and show how to perform optimization of query plans in presence of subsumption and minimum union through rule-based plan transformations. Experiments on both artificial and real world data show that our algorithms outperform existing algorithms used for subsumption in terms of runtime and they scale to large volumes of data. In the context of data integration, we observe that performing data fusion calls for more than subsumption and minimum union. Therefore, another contribution of this paper is the definition of the complementation and complement union operators. Intuitively, these allow to merge tuples that have complementing values and thus eliminate unnecessary null-values.

#index 1372726
#* HARRA: fast iterative hashed record linkage for large-scale data collections
#@ Hung-sik Kim;Dongwon Lee
#t 2010
#c 8
#% 201889
#% 310516
#% 443393
#% 479973
#% 577238
#% 577309
#% 654467
#% 765463
#% 777329
#% 809460
#% 810014
#% 847168
#% 864392
#% 893164
#% 898309
#% 931291
#% 937552
#% 956506
#% 975019
#% 1019087
#% 1022281
#% 1055684
#% 1083744
#% 1201863
#% 1206695
#% 1250576
#% 1269495
#! We study the performance issue of the "iterative" record linkage (RL) problem, where match and merge operations may occur together in iterations until convergence emerges. We first propose the Iterative Locality-Sensitive Hashing (ILSH) that dynamically merges LSH-based has tables for quick and accurate blocking. Then, by exploiting inherent characteristics within/across data sets, we develop a suite of I-LSH-based RL algorithms, named as HARRA (HAshed RecoRd linkAge). The superiority of HARRA in speed over competing RL solutions is thoroughly validated using various real data sets. While maintaining equivalent or comparable accuracy levels, for instance, HARRA runs: (1) 4.5 and 10.5 times faster than StringMap and R-Swoosh in iteratively linking 4,000 x 4,000 short records (i.e., one of the small test cases), and (2) 5.6 and 3.4 times faster than basic LSH and Multi-Probe LSH algorithms in iteratively linking 400,000 x 400,000 long records (i.e., the largest test case).

#index 1372727
#* Keyword search for data-centric XML collections with long text fields
#@ Arash Termehchy;Marianne Winslett
#t 2010
#c 8
#% 115608
#% 227919
#% 330617
#% 397366
#% 465155
#% 466476
#% 654442
#% 765455
#% 810052
#% 813989
#% 863389
#% 881478
#% 956599
#% 994015
#% 1015258
#% 1016135
#% 1019060
#% 1077038
#% 1077150
#% 1206957
#% 1292476
#% 1294435
#% 1305154
#! Users who are unfamiliar with database query languages can search XML data sets using keyword queries. Current approaches for supporting such queries are either for text-centric XML, where the structure is very simple and long text fields predominate; or data-centric, where the structure is very rich. However, long text fields are becoming more common in data-centric XML, and existing approaches deliver relatively poor precision, recall, and ranking for such data sets. In this paper, we introduce an XML keyword search method that provides high precision, recall, and ranking quality for data-centric XML, even when long text fields are present. Our approach is based on a new group of structural relationships called normalized term presence correlation (NTPC). In a one-time setup phase, we compute the NTPCs for a representative DB instance, then use this information to rank candidate answers for all subsequent queries, based on each answer's structure. Our experiments with 65 user-supplied queries over two real-world XML data sets show that NTPC-based ranking is always as effective as the best previously available XML keyword search method for data-centric data sets, and provides better precision, recall, and ranking than previous approaches when long text fields are present. As the straightforward approach for computing NTPCs is too slow, we also present algorithms to compute NTPCs efficiently.

#index 1372728
#* Fast ELCA computation for keyword queries on XML data
#@ Rui Zhou;Chengfei Liu;Jianxin Li
#t 2010
#c 8
#% 465155
#% 479803
#% 654442
#% 659990
#% 660011
#% 810052
#% 824693
#% 875017
#% 956599
#% 960243
#% 960259
#% 960261
#% 993987
#% 1015258
#% 1016135
#% 1019060
#% 1044480
#% 1063539
#% 1127424
#% 1181282
#% 1217198
#! Keyword search is integrated in many applications on account of the convenience to convey users' query intention. Recently, answering keyword queries on XML data has drawn the attention of web and database communities, because the success of this research will relieve users from learning complex XML query languages, such as XPath/XQuery, and/or knowing the underlying schema of the queried XML data. As a result, information in XML data can be discovered much easier. To model the result of answering keyword queries on XML data, many LCA (lowest common ancestor) based notions have been proposed. In this paper, we focus on ELCA (Exclusive LCA) semantics, which is first proposed by Guo et al. and afterwards named by Xu and Papakonstantinou. We propose an algorithm named Hash Count to find ELCAs efficiently. Our analysis shows the complexity of Hash Count algorithm is O(kd|S1|), where k is the number of keywords, d is the depth of the queried XML document and |S1| is the frequency of the rarest keyword. This complexity is the best result known so far. We also evaluate the algorithm on a real DBLP dataset, and compare it with the state-of-the-art algorithms. The experimental results demonstrate the advantage of Hash Count algorithm in practice.

#index 1372729
#* Suggestion of promising result types for XML keyword search
#@ Jianxin Li;Chengfei Liu;Rui Zhou;Wei Wang
#t 2010
#c 8
#% 309726
#% 654442
#% 745463
#% 810052
#% 864456
#% 956599
#% 960261
#% 993970
#% 1015258
#% 1016135
#% 1019060
#% 1044480
#% 1074200
#% 1127424
#% 1181282
#% 1206677
#% 1206957
#% 1292476
#% 1328135
#! Although keyword query enables inexperienced users to easily search XML database with no specific knowledge of complex structured query languages or XML data schemas, the ambiguity of keyword query may result in generating a great number of results that may be classified into different types. For users, each result type implies a possible search intention. To improve the performance of keyword query, it is desirable to efficiently work out the most relevant result type from the data to be retrieved. Several recent research works have focused on this interesting problem by using data schema information or pure IR-style statical information. However, this problem is still open due to some requirements. (1) The data to be retrieved may not contain schema information; (2) Relevant result types should be efficiently computed before keyword query evaluation; (3) The correlation between a result type and a keyword query should be measured by analyzing the distribution of relevant values and structures within the data. As we know, none of existing work satisfies the above three requirements together. To address the problem, we propose an estimation-based approach to compute the promising result types for a keyword query, which can help a user quickly narrow down to her specific information need. To speed up the computation, we designed new algorithms based on the indexes to be built. Finally, we present a set of experimental results that evaluate the proposed algorithms and show the potential of this work.

#index 1372730
#* Feedback-based annotation, selection and refinement of schema mappings for dataspaces
#@ Khalid Belhajjame;Norman W. Paton;Suzanne M. Embury;Alvaro A. A. Fernandes;Cornelia Hedeler
#t 2010
#c 8
#% 156337
#% 300288
#% 333988
#% 375017
#% 378409
#% 480134
#% 572314
#% 717100
#% 742666
#% 793341
#% 800551
#% 824737
#% 826032
#% 845350
#% 874876
#% 893095
#% 945786
#% 945908
#% 955762
#% 1044442
#% 1063533
#% 1127413
#% 1190673
#% 1206612
#% 1692828
#! The specification of schema mappings has proved to be time and resource consuming, and has been recognized as a critical bottleneck to the large scale deployment of data integration systems. In an attempt to address this issue, dataspaces have been proposed as a data management abstraction that aims to reduce the up-front cost required to setup a data integration system by gradually specifying schema mappings through interaction with end users in a pay-as-you-go fashion. As a step in this direction, we explore an approach for incrementally annotating schema mappings using feedback obtained from end users. In doing so, we do not expect users to examine mapping specifications; rather, they comment on results to queries evaluated using the mappings. Using annotations computed on the basis of user feedback, we present a method for selecting from the set of candidate mappings, those to be used for query evaluation considering user requirements in terms of precision and recall. In doing so, we cast mapping selection as an optimization problem. Mapping annotations may reveal that the quality of schema mappings is poor. We also show how feedback can be used to support the derivation of better quality mappings from existing mappings through refinement. An evolutionary algorithm is used to efficiently and effectively explore the large space of mappings that can be obtained through refinement. The results of evaluation exercises show the effectiveness of our solution for annotating, selecting and refining schema mappings.

#index 1372731
#* PerK: personalized keyword search in relational databases through preferences
#@ Kostas Stefanidis;Marina Drosou;Evaggelia Pitoura
#t 2010
#c 8
#% 177422
#% 300170
#% 333854
#% 577224
#% 660011
#% 731407
#% 800588
#% 805841
#% 810018
#% 824693
#% 875002
#% 875017
#% 894444
#% 960243
#% 960259
#% 993957
#% 993987
#% 1015325
#% 1016176
#% 1016207
#% 1021948
#% 1044464
#% 1127465
#% 1181244
#% 1206662
#% 1206753
#% 1272396
#% 1328135
#! Keyword-based search in relational databases allows users to discover relevant information without knowing the database schema or using complicated queries. However, such searches may return an overwhelming number of results, often loosely related to the user intent. In this paper, we propose personalizing keyword database search by utilizing user preferences. Query results are ranked based on both their relevance to the query and their preference degree for the user. To further increase the quality of results, we consider two new metrics that evaluate the goodness of the result as a set, namely coverage of many user interests and content diversity. We present an algorithm for processing preference queries that uses the preferential order between keywords to direct the joining of relevant tuples from multiple relations. We then show how to reduce the complexity of this algorithm by sharing computational steps. Finally, we report evaluation results of the efficiency and effectiveness of our approach.

#index 1372732
#* Efficient computation of trade-off skylines
#@ Christoph Lofi;Ulrich Güntzer;Wolf-Tilo Balke
#t 2010
#c 8
#% 465167
#% 566111
#% 654480
#% 824670
#% 824671
#% 824672
#% 875012
#% 993954
#% 1075132
#% 1408810
#% 1688273
#% 1712421
#% 1716958
#! When selecting alternatives from large amounts of data, trade-offs play a vital role in everyday decision making. In databases this is primarily reflected by the top-k retrieval paradigm. But recently it has been convincingly argued that it is almost impossible for users to provide meaningful scoring functions for top-k retrieval, subsequently leading to the adoption of the skyline paradigm. Here users just specify the relevant attributes in a query and all suboptimal alternatives are filtered following the Pareto semantics. Up to now the intuitive concept of compensation, however, cannot be used in skyline queries, which also contributes to the often unmanageably large result set sizes. In this paper we discuss an innovative and efficient method for computing skylines allowing the use of qualitative trade-offs. Such trade-offs compare examples from the database on a focused subset of attributes. Thus, users can provide information on how much they are willing to sacrifice to gain an improvement in some other attribute(s). Our contribution is the design of the first skyline algorithm allowing for qualitative compensation across attributes. Moreover, we also provide an novel trade-off representation structure to speed up retrieval. Indeed our experiments show efficient performance allowing for focused skyline sets in practical applications. Moreover, we show that the necessary amount of object comparisons can be sped up by an order of magnitude using our indexing techniques.

#index 1372733
#* How to authenticate graphs without leaking
#@ Ashish Kundu;Elisa Bertino
#t 2010
#c 8
#% 70370
#% 495870
#% 513367
#% 657774
#% 745532
#% 761411
#% 810042
#% 853532
#% 874980
#% 893099
#% 927452
#% 948805
#% 989646
#% 1063476
#% 1063514
#% 1127362
#% 1127363
#% 1201868
#% 1206573
#% 1389866
#% 1394513
#% 1415851
#% 1563740
#% 1784889
#! Secure data sharing in multi-party environments requires that both authenticity and confidentiality of the data be assured. Digital signature schemes are commonly employed for authentication of data. However, no such technique exists for directed graphs, even though such graphs are one of the most widely used data organization structures. Existing schemes for DAGs are authenticity-preserving but not confidentiality-preserving, and lead to leakage of sensitive information during authentication. In this paper, we propose two schemes on how to authenticate DAGs and directed cyclic graphs without leaking, which are the first such schemes in the literature. It is based on the structure of the graph as defined by depth-first graph traversals and aggregate signatures. Graphs are structurally different from trees in that they have four types of edges: tree, forward, cross, and back-edges in a depth-first traversal. The fact that an edge is a forward, cross or a back-edge conveys information that is sensitive in several contexts. Moreover, back-edges pose a more difficult problem than the one posed by forward, and cross-edges primarily because back-edges add bidirectional properties to graphs. We prove that the proposed technique is both authenticity-preserving and non-leaking. While providing such strong security properties, our scheme is also efficient, as supported by the performance results.

#index 1372734
#* Trustworthy vacuuming and litigation holds in long-term high-integrity records retention
#@ Ragib Hasan;Marianne Winslett
#t 2010
#c 8
#% 318704
#% 322884
#% 397367
#% 411437
#% 449857
#% 659992
#% 824701
#% 960241
#% 1016171
#% 1022267
#% 1127593
#% 1130895
#% 1206828
#% 1206860
#% 1412853
#% 1651365
#! Relational databases are periodically vacuumed to remove tuples that have expired. During the discovery phase of litigation, plaintiffs ask defendants for access to information related to their case. The requested information is then subject to a litigation hold, which means that the information cannot be deleted. Vacuuming exposes a database to a new threat -- adversaries can try to thwart database auditing mechanism by masquerading an illegal tuple deletion as a vacuuming operation, and delete an unexpired tuple, or a tuple under a litigation hold. In this paper, we provide a generic framework for auditing vacuuming, augment existing database integrity audit mechanisms to support vacuuming, formalize the notion of a litigation hold, and identify key issues in the context of database systems for long-term, high-integrity records retention. Then, we propose several schemes for efficiently implementing trustworthy litigation holds. Finally, we evaluate the efficiency and tradeoffs of the different schemes using a series of experiments.

#index 1372735
#* Algorithm-safe privacy-preserving data publishing
#@ Xin Jin;Nan Zhang;Gautam Das
#t 2010
#c 8
#% 115608
#% 577239
#% 800514
#% 800515
#% 801690
#% 810011
#% 864406
#% 874892
#% 874988
#% 874989
#% 881483
#% 881546
#% 881551
#% 893100
#% 893151
#% 960239
#% 960289
#% 960291
#% 1015140
#% 1022246
#% 1022247
#% 1022264
#% 1022265
#% 1022266
#% 1063476
#% 1063504
#% 1063505
#% 1206581
#% 1206582
#% 1206745
#% 1206814
#% 1328175
#% 1328189
#% 1670071
#% 1700134
#! This paper develops toolsets for eliminating algorithm-based disclosure from existing privacy-preserving data publishing algorithms. We first show that the space of algorithm-based disclosure is larger than previously believed and thus more prevalent and dangerous. Then, we formally define Algorithm-Safe Publishing (ASP) to model the threats from algorithm-based disclosure. To eliminate algorithm-based disclosure from existing data publishing algorithms, we propose two generic tools for revising their design: worst-case eligibility test and stratified pick-up. We demonstrate the effectiveness of our tools by using them to transform two popular existing l-diversity algorithms, Mondrian and Hilb, to SP-Mondrian and SP-Hilb which are algorithm-safe. We conduct extensive experiments to demonstrate the effectiveness of SP-Mondrian and SP-Hilb in terms of data utility and efficiency.

#index 1372736
#* BronzeGate: real-time transactional data obfuscation for GoldenGate
#@ Shenoda Guirguis;Alok Pareek
#t 2010
#c 8
#% 951981
#% 1065553
#% 1206992
#! Data privacy laws have appeared recently, such as the HIPAA laws for protecting medical records, and the PCI guidelines for protecting Credit Card information. Data privacy can be defined as maintaining the privacy of Personal Identifiable Information (PII) from unauthorized accessing. PII includes any piece of data that can be used alone, or in conjunction with additional information, to uniquely identify an individual. Examples of such information include national identification numbers, credit card numbers, as well as financial and medical records. Access control methods and data encryption provide a level of data protection from unauthorized access, however, it is not enough; it does not prohibit identity thefts. It was reported that 70% of the data privacy breaches are internal breaches that involve an employee from the enterprise who has access to some training or testing database replica, which contains all the PII. In addition to access control, we need techniques to obfuscate (i.e., mask or dim) the datasets used for training, testing and analysis purposes. A good data obfuscation technique would, among other features, preserve the data usability while protecting its privacy. This challenge is further complicated when real time requirements are added. In this paper we present BronzeGate: Obfuscated GoldenGate, the GoldenGate's real-time solution for transactional data privacy while maintaining data usability. BronzeGate utilizes different obfuscation functions for different data types to securely obfuscate the data, on real-time, while maintaining its statistical characteristics.

#index 1372737
#* Logging last resource optimization for distributed transactions in Oracle WebLogic server
#@ Tom Barnes;Adam Messinger;Paul Parkinson;Amit Ganesh;German Shegalov;Saraswathy Narayan;Srinivas Kareenhalli
#t 2010
#c 8
#% 4619
#% 336201
#% 481129
#% 531907
#% 602803
#! State-of-the-art OLTP systems execute distributed transactions using XA-2PC protocol, a presumed-abort variant of the Two-Phase Commit (2PC) protocol. While the XA specification provides for the Read-Only and 1PC optimizations of 2PC, it does not deal with another important optimization, coined Nested 2PC. In this paper, we describe the Logging Last Resource (LLR) optimization in Oracle WebLogic Server (WLS). It adapts and improves the Nested 2PC optimization to/for the Java Enterprise Edition (JEE) environment. It allows reducing the number of forced (synchronous) writes and the number of exchanged messages when executing distributed transactions that span multiple transactional resources including a SQL database integrated as a JDBC datasource. This optimization has been validated in SPECjAppServer2004 (a standard industry benchmark for JEE) and a variety of internal benchmarks. LLR has been successfully deployed by high-profile customers in mission-critical high-performance applications.

#index 1372738
#* DEDUCE: at the intersection of MapReduce and stream processing
#@ Vibhore Kumar;Henrique Andrade;Buğra Gedik;Kun-Lung Wu
#t 2010
#c 8
#% 223781
#% 654507
#% 654510
#% 723279
#% 800584
#% 844161
#% 954300
#% 960326
#% 963669
#% 995806
#% 1026962
#% 1063553
#% 1063555
#% 1142434
#! MapReduce and stream processing are two emerging, but different, paradigms for analyzing, processing and making sense of large volumes of modern day data. While MapReduce offers the capability to analyze several terabytes of stored data, stream processing solutions offer the ability to process, possibly, a few million updates every second. However, there is an increasing number of data processing applications which need a solution that effectively and efficiently combines the benefits of MapReduce and stream processing to address their data processing needs. For example, in the automated stock trading domain, applications usually require periodic analysis of large amounts of stored data to generate a model using MapReduce, which is then used to process a stream of incident updates using a stream processing system. This paper presents Deduce, which extends IBM's System S stream processing middleware with support for MapReduce by providing (1) language and runtime support for easily specifying and embedding MapReduce jobs as elements of a larger data-flow, (2) capability to describe reusable modules that can be used as map and reduce tasks, and (3) configuration parameters that can be tweaked to control and manage the usage of shared resources by the MapReduce and stream processing components. We describe the motivation for Deduce and the design and implementation of the MapReduce extensions for System S, and then present experimental results.

#index 1372739
#* Aggregation of asynchronous electric power consumption time series knowing the integral
#@ Raja Chiky;Laurent Decreusefond;Georges Hébrail
#t 2010
#c 8
#% 333926
#! More and more data mining algorithms are applied to a large number of long time series issued by many distributed sensors. The consequence of the huge volume of data is that data warehouses often contain asynchronous time series, i.e. the values have been sampled and are not anymore observed at the same instants. This is a problem when applying data mining algorithms to such asynchronous time series. The standard way to solve this problem is to interpolate intermediate points. We present here two new interpolation approaches which take into account the knowledge of the integral of the time series between two points. The first approach is naive and uses the history of slope values. The second approach is stochastic and provides a confidence interval of interpolated values. The two methods have been assessed experimentally on a real dataset of electric power consumption time series issued from smart meters.

#index 1372740
#* An experimental study of time-constrained aggregate queries
#@ Ying Hu;Wen-Chi Hou;Seema Sundara;Jagannathan Srinivasan
#t 2010
#c 8
#% 58348
#% 102786
#% 116084
#% 210353
#% 227883
#% 248820
#% 273908
#% 273909
#% 273910
#% 503719
#% 818916
#% 960294
#% 963669
#% 1022304
#% 1181307
#! Although the notion of time-constrained query was first introduced two decades ago to address the problem of long running SQL queries, none of the commercial database systems support such a feature. This is rather surprising given the fact that database systems are beginning to accommodate large datasets in the order of terabytes to petabytes. Thus, the long running SQL query problem needs to be addressed. Recently, at Oracle we investigated and proposed a mechanism of supporting time-constrained quenes to provide quick approximate answers by use of sampling for such long running SQL quenes. This we followed up by coming up with error estimates as a measure of goodness for the approximation. To further validate our time-constrained query work, in this paper we present an experimental study conducted on our time-constrained query prototype built on the Oracle Database. It is our hope that this work will revive interest in time-constrained queries.

#index 1372741
#* Xbase: cloud-enabled information appliance for healthcare
#@ Wen-Syan Li;Jianfeng Yan;Ying Yan;Jin Zhang
#t 2010
#c 8
#% 322884
#% 397360
#% 479465
#% 660000
#% 824663
#% 963669
#% 1013630
#% 1063553
#% 1328060
#% 1328095
#! XML is a more desirable format for modeling and storing clinical data in EMR (Electronic medical record) applications for its extendibility; however, existing EMR systems either are built on top of RDBMS or file systems or lack of support for complex and large scale healthcare applications, such as treatment effectiveness analysis and procedure optimization. SAP Technology Lab, China is developing a clouds-enabled information appliance, Xbase, built on top of Hadoop, which is the first XML-based information appliance designed specifically for large scale and complex healthcare applications. XML presents a different set of challenges for query processing, indexing, parallelism, and distributed computing using existing Hadoop's APIs as well as its HDFS storage infrastructure and MapReduce framework. In this paper, we describe system architecture and internal designs of Xbase as well as how the indexing is mapped to RDBMS and Hadoop. We also discuss why we select Hadoop over other candidates, such as Hbase, Google's Bigtable, and Hive.

#index 1372742
#* A plan for OLAP
#@ Bernhard Jaecksch;Wolfgang Lehner;Franz Faerber
#t 2010
#c 8
#% 308509
#% 464215
#% 1063542
#! So far, data warehousing has often been discussed in the light of complex OLAP queries and as reporting facility for operative data. We argue that business planning as a means to generate plan data is an equally important cornerstone of a data warehouse system, and we propose it to be a first-class citizen within an OLAP engine. We introduce an abstract model describing relevant aspects of the planning process in general and the requirements it poses to a planning engine. Furthermore, we show that business planning lends itself well to parallelization and benefits from a column-store much like traditional OLAP does. We then develop a physical model specifically targeted at a highly parallel column-store, and with our implementation, we show nearly linear scaling behavior.

#index 1372743
#* Augmenting OLAP exploration with dynamic advanced analytics
#@ Benjamin Leonhardi;Bernhard Mitschang;Rubén Pulido;Christoph Sieb;Michael Wurst
#t 2010
#c 8
#% 246016
#% 488751
#% 757579
#% 783944
#! Online Analytical Processing (OLAP) is a popular technique for explorative data analysis. Usually, a fixed set of dimensions (such as time, place, etc.) is used to explore and analyze various subsets of a given, multi-dimensional data set. These subsets are selected by constraining one or several of the dimensions, for instance, showing sales only in a given year and geographical location. Still, such aggregates are often not enough. Important information can only be discovered by combining several dimensions in a multidimensional analysis. Most existing approaches allow to add new dimensions either statically or dynamically. These approaches support, however, only the creation of global dimensions that are not interactive for the user running the report. Furthermore, they are mostly restricted to data clustering and the resulting dimensions cannot be interactively refined. In this paper we propose a technique and an architectural solution that is based on an interaction concept for creating OLAP dimensions on subsets of the data dynamically, triggered interactively by the user, based on arbitrary multi-dimensional grouping mechanisms. This approach allows combining the advantages of both, OLAP exploration and interactive multidimensional analysis. We demonstrate the industry-strength of our solution architecture using a setup of IBM® InfoSphere™ Warehouse data mining and Cognos® BI as reporting engine. Use cases and industrial experiences are presented showing how insight derived from data mining can be transparently presented in the reporting front end, and how data mining algorithms can be invoked from the front end, achieving closed-loop integration.

#index 1372744
#* Advanced knowledge discovery on movement data with the GeoPKDD system
#@ M. Nanni;R. Trasarti;C. Renso;F. Giannotti;D. Pedreschi
#t 2010
#c 8
#% 873099
#% 989604
#% 1046207
#% 1135184
#% 1206713
#% 1214685
#% 1248220
#! The growing availability of mobile devices produces an enormous quantity of personal tracks which calls for advanced analysis methods capable of extracting knowledge out of massive trajectories datasets. In this paper we present an experiment on a real world scenario that demonstrates the strong analytical power of massive, raw trajectory data made available as a by-product of telecom services, in unveiling the complexity of urban mobility. The experiment has been made possible by the GeoPKDD system, an integrated platform for complex analysis of mobility data. The system combines spatio-temporal querying capabilities with data mining and semantic technologies, thus providing a full support for the Mobility Knowledge Discovery process.

#index 1372745
#* Timely YAGO: harvesting, querying, and visualizing temporal knowledge from Wikipedia
#@ Yafang Wang;Mingjie Zhu;Lizhen Qu;Marc Spaniol;Gerhard Weikum
#t 2010
#c 8
#% 754068
#% 956564
#% 1019061
#% 1024551
#% 1271301
#% 1271605
#% 1275182
#% 1409954
#! Recent progress in information extraction has shown how to automatically build large ontologies from high-quality sources like Wikipedia. But knowledge evolves over time; facts have associated validity intervals. Therefore, ontologies should include time as a first-class dimension. In this paper, we introduce Timely YAGO, which extends our previously built knowledge base YAGO with temporal aspects. This prototype system extracts temporal facts from Wikipedia infoboxes, categories, and lists in articles, and integrates these into the Timely YAGO knowledge base. We also support querying temporal facts, by temporal predicates in a SPARQL-style language. Visualization of query results is provided in order to better understand of the dynamic nature of knowledge.

#index 1372746
#* PARINDA: an interactive physical designer for PostgreSQL
#@ Cristina Maier;Debabrata Dash;Ioannis Alagiannis;Anastasia Ailamaki;Thomas Heinis
#t 2010
#c 8
#% 36119
#% 765176
#% 778724
#% 810026
#% 875062
#% 1016220
#% 1022293
#% 1193594
#% 1206949
#% 1207101
#! One of the most challenging tasks for the database administrator is to physically design the database to attain optimal performance for a given workload. Physical design is hard because it requires the selection of an optimal set of design features from a vast search space. There have been many commercial tools available to automatically suggest the physical design, for a given a set of queries. These tools are, however, based on greedy heuristic pruning, which reduces their usefulness. Furthermore, they are not interactive, as the APIs to simulate the indexes and tables are product specific and hidden from the database administrators. Finally, all these tools are built specifically for commercial systems and there is lack of automated physical designers for open source DBMSs. In this demonstration we introduce -PARINDA - an interactive physical designer for an open source DBMS. Given a workload containing a set of queries, this tool allows the DBA to efficiently simulate various physical design features and get immediate feedback on their effectiveness. It also incorporates recent advances in non-greedy physical design techniques to provide close to optimal suggestions. Although it has been prototyped for several different DBMSs, we demonstrate the usefulness and efficiency of the tool while running on the open source DBMS---PostgreSQL--using large real-world scientific datasets and query workloads.

#index 1372747
#* BIAEditor: matching process and operational data for a business impact analysis
#@ Sylvia Radeschütz;Florian Niedermann;Wolfgang Bischoff
#t 2010
#c 8
#% 408921
#% 572314
#% 798936
#% 874140
#% 1022296
#% 1044495
#% 1065125
#! A profound analysis of all relevant business data in the company is necessary for optimizing business processes effectively. Current analyses typically exclusively run on business process execution data or on operational business data stored in a data warehouse. However, to achieve a more informative analysis and to fully optimize a company's business, a consolidation of all major business data sources is indispensable. Recent matching algorithms are insufficient for this task, since they are restricted either to schema or to process matching. Our demonstration presents BIAEditor that allows to annotate and match process variables and operational data models in order to perform such a global business impact analysis.

#index 1372748
#* Pattern detector: fast detection of suspicious stream patterns for immediate reaction
#@ Ira Assent;Hardy Kremer;Stephan Günnemann;Thomas Seidl
#t 2010
#c 8
#% 248797
#% 359751
#% 779472
#% 809264
#% 893161
#% 993965
#% 1127609
#% 1328178
#! Detecting emerging problems in information and manufacturing systems is the goal of monitoring tools. Good and timely detection of problematic conditions from measured indicators requires efficient and effective detection of critical patterns in a stream of incoming observations. We present Pattern Detector, an interactive system which is capable of immediate detection and signaling of such patterns. Using user-defined query patterns which indicate e.g. low rate denial-of-service attacks in network traffic, this system signals problems fast and transparently. The underlying detection algorithm is based on matching patterns using the Dynamic Time Warping (DTW). Fast query processing is achieved by reliably filtering out candidates via a highly efficient multistep filter-and-refine framework, anticipatory DTW (ADTW). This framework is capable of processing continuous streams such that appropriate action can be taken as soon as suspicious patterns occur. While our pattern detector system is developed specifically for network traffic by incorporating recent patterns from computer networking, it easily generalizes to many online stream monitoring tasks.

#index 1372749
#* BP-Ex: a uniform query engine for business process execution traces
#@ Eran Balan;Tova Milo;Tal Sterenzy
#t 2010
#c 8
#% 351041
#% 487125
#% 754120
#% 809234
#% 880165
#% 893117
#% 946931
#% 957971
#% 1022252
#% 1022296
#% 1180017
#% 1181213
#! Many enterprises nowadays use business processes, based on the BPEL standard, to achieve their goals. Analyzing the execution of such processes is critical for enforcing business policies and meeting efficiency and reliability goals. The BP-Ex system presented in this demo is an important component of BP-Suite, a novel tools suite based on the BPEL standard, which offers a uniform, query-based, user-friendly interface for BP analysis. BP-suite allows to gracefully combine the analysis of process specifications, monitoring of run time behavior, and posteriorly querying of execution traces (logs), for a comprehensive process management. BP-Ex is the BP-Suite query engine for process execution traces. The goal of this demo is to highlight the particular challenges that had to be addressed to support the suite's uniform, intuitive query interface, over (possibly very large) execution traces, and to demonstrate the novel optimization techniques that had to be developed for that.

#index 1372750
#* B-Fabric: the Swiss Army Knife for life sciences
#@ Can Türker;Fuat Akal;Dieter Joho;Christian Panse;Simon Barkow-Oesterreicher;Hubert Rehrauer;Ralph Schlapbach
#t 2010
#c 8
#% 1396737
#! This paper demonstrates B-Fabric, an all-in-one solution for two major purposes in life sciences. On the one hand, it is a system for the integrated management of experimental data and scientific annotations. On the other hand, it is a system infrastructure supporting on-the fly coupling of user applications, and thus serving as extensible platform for fast-paced, cutting-edge, collaborative research.

#index 1372751
#* FPGAs: a new point in the database design space
#@ Rene Mueller;Jens Teubner
#t 2010
#c 8
#% 765419
#% 800491
#% 831186
#% 850738
#% 874997
#% 960254
#% 963669
#% 1022232
#% 1022311
#% 1063508
#% 1127563
#% 1207014
#% 1328128
#% 1328141
#% 1328185
#! In line with the insight that "one size" of databases will not fit all application needs [19] the database community is currently exploring various alternatives to commodity, CPU-based system designs. One particular candidate in this trend are field-programmable gate arrays (FPGAs), programmable chips that allow tailor-made hardware designs optimized for specific systems, applications, or even user queries. With a focus on database use, this tutorial introduces into FPGA technology, demonstrates its potential, but also pinpoints some challenges that need to be addressed before FPGA-accelerated database systems can go mainstream. The goal of this tutorial is to develop an intuition of an FPGA development cycle, receive guidelines for a "good" FPGA design, but also learn the limitations that hardware-implemented database processing faces. Our more high-level ambition is to spur a broader interest in database processing on novel hardware technology.

#index 1372752
#* Querying the deep web
#@ Andrea Calì;Davide Martinenghi
#t 2010
#c 8
#% 198466
#% 273912
#% 273923
#% 342359
#% 480479
#% 572311
#% 630963
#% 726626
#% 801668
#% 801698
#% 810110
#% 824775
#% 874895
#% 943616
#% 1148409
#% 1206616
#% 1328200
#! Data stored outside Web pages and accessible from the Web, typically through HTML forms, consitute the so-called Deep Web. Such data are of great value, but difficult to query and search. We survey techniques to optimize query processing on the Deep Web, in a setting where data are represented in the relational model. We illustrate optimizations both at query plan generation time and at runtime, highlighting the role of integrity constraints. We discuss several prototype systems that address the query processing problem.

#index 1549835
#* Map-reduce extensions and recursive queries
#@ Foto N. Afrati;Vinayak Borkar;Michael Carey;Neoklis Polyzotis;Jeffrey D. Ullman
#t 2011
#c 8
#% 23895
#% 86933
#% 153640
#% 172952
#% 234797
#% 261358
#% 309749
#% 368248
#% 444212
#% 479933
#% 723279
#% 963669
#% 983467
#% 1063553
#% 1068741
#% 1127354
#% 1127559
#% 1372690
#% 1426442
#% 1426486
#% 1426513
#% 1468421
#% 1523820
#% 1594630
#% 1745215
#! We survey the recent wave of extensions to the popular map-reduce systems, including those that have begun to address the implementation of recursive queries using the same computing environment as map-reduce. A central problem is that recursive tasks cannot deliver their output only at the end, which makes recovery from failures much more complicated than in map-reduce and its nonrecursive extensions. We propose several algorithmic ideas for efficient implementation of recursions in the map-reduce environment and discuss several alternatives for supporting recovery from failures without restarting the entire job.

#index 1549836
#* Database researchers: plumbers or thinkers?
#@ Gerhard Weikum
#t 2011
#c 8
#% 956564
#% 1089602
#% 1166490
#% 1183369
#% 1190065
#% 1190118
#% 1214667
#% 1250397
#% 1355059
#% 1409954
#% 1426449
#% 1495583
#% 1523866
#% 1536527
#% 1733683
#% 1735364
#! DB researchers have traditionally focused on engine-centered issues such as indexing, query processing, and transactions. Data mining has broadened the community's viewpoint towards algorithmic and statistical issues. However, DB research has always had a tendency to shy away from seemingly elusive long-term challenges with AI flavor. On the other hand, the current explosion of digital content in enterprises and the Internet, is mostly caused by user-created information like text, tags, photos, videos, and not by seeing more well-designed databases of the traditional kind. In this situation, I question the traditional skepticism of DB researchers towards "AI-complete" problems and the DB community's reluctance to embark on seemingly non-DB-ish grand challenges. Big questions that I see as great opportunities also for DB research include: 1) automatic extraction of relational facts from natural-language text and multimodal contexts [4, 6, 21], 2) automatic disambiguation of named-entity mentions and general phrases in text and speech [10, 11], 3) large-scale gathering of factual-knowledge candidates and their reconciliation into comprehensive knowledge bases [1, 2, 8, 13, 19], 4) reasoning on uncertain hypotheses, for knowledge discovery and semantic search [9, 14, 16, 17, 20], 5) deep and real-time question answering, e.g., to enable computers to win quiz game shows [7], 6) machine-reading of scientific publications and fictional literature, to enable corpus-wide analyses and enable researchers in science and humanities to develop hypotheses and quickly focus on the most relevant issues [3, 5]. I believe that successfully tackling these topics requires efficient data-centric algorithms, scalable methods and architectures, and system-level thinking - virtues that are richly available in the DB research community. Moreover, I would encourage our community to look across the fence and get more engaged on the exciting challenges outside the traditionally narrow boundaries of the DB realm. I will illustrate these points by examples from my own research on knowledge management [12, 15, 18, 19]. Breakthroughs will require long-term stamina. In the meantime, steady incremental progress is better than not embarking on these important problems at all.

#index 1549837
#* Novel techniques to reduce search space in multiple minimum supports-based frequent pattern mining algorithms
#@ R. Uday Kiran;P. Krishna Reddy
#t 2011
#c 8
#% 152934
#% 280487
#% 342643
#% 481290
#% 717219
#% 729418
#% 765520
#% 893373
#% 937941
#% 985041
#% 1035815
#% 1494943
#% 1697220
#! Frequent patterns are an important class of regularities that exist in a transaction database. Certain frequent patterns with low minimum support (minsup) value can provide useful information in many real-world applications. However, extraction of these frequent patterns with single minsup-based frequent pattern mining algorithms such as Apriori and FP-growth leads to "rare item problem." That is, at high minsup value, the frequent patterns with low minsup are missed, and at low minsup value, the number of frequent patterns explodes. In the literature, "multiple minsups framework" was proposed to discover frequent patterns. Furthermore, frequent pattern mining techniques such as Multiple Support Apriori and Conditional Frequent Pattern-growth (CFP-growth) algorithms have been proposed. As the frequent patterns mined with this framework do not satisfy downward closure property, the algorithms follow different types of pruning techniques to reduce the search space. In this paper, we propose an efficient CFP-growth algorithm by proposing new pruning techniques. Experimental results show that the proposed pruning techniques are effective.

#index 1549838
#* Mining closed discriminative dyadic sequential patterns
#@ David Lo;Hong Cheng; Lucia
#t 2011
#c 8
#% 280409
#% 412479
#% 463903
#% 464996
#% 481290
#% 729933
#% 729938
#% 745515
#% 818916
#% 823384
#% 989617
#% 1055790
#% 1063502
#% 1197048
#% 1206864
#% 1214677
#% 1338670
#% 1426340
#! A lot of data are in sequential formats. In this study, we are interested in sequential data that goes in pairs. There are many interesting datasets in this format coming from various domains including parallel textual corpora, duplicate bug reports, and other pairs of related sequences of events. Our goal is to mine a set of closed discriminative dyadic sequential patterns from a database of sequence pairs each belonging to one of the two classes +ve and -ve. These dyadic sequential patterns characterize the discriminating facets contrasting the two classes. They are potentially good features to be used for the classification of dyadic sequential data. They can be used to characterize and flag correct and incorrect translations from parallel textual corpora, automate the manual and time consuming duplicate bug report detection process, etc. We provide a solution of this new problem by proposing new search space traversal strategy, projected database structure, pruning properties, and novel mining algorithms. To demonstrate the scalability and utility of our solution, we have experimented with both synthetic and real datasets. Experiment results show that our solution is scalable. Mined patterns are also able to improve the accuracy of one possible downstream application, namely the detection of duplicate bug reports using pattern-based classification.

#index 1549839
#* Sequenced event set pattern matching
#@ Bruno Cadonna;Johann Gamper;Michael H. Böhlen
#t 2011
#c 8
#% 271199
#% 333938
#% 459001
#% 480938
#% 481448
#% 482088
#% 631974
#% 726621
#% 763881
#% 838512
#% 875004
#% 993949
#% 1063480
#% 1206571
#% 1206641
#% 1217161
#% 1217239
#% 1688281
#! Event pattern matching is a query technique where a sequence of input events is matched against a complex pattern that specifies constraints on extent, order, values, and quantification of matching events. The increasing importance of such query techniques is underpinned by a significant amount of research work, the availability of commercial products, and by a recent proposal to extend SQL for event pattern matching. The proposed SQL extension includes an operator PERMUTE, which allows to express patterns that match any permutation of a set of events. No implementation of this operator is known to the authors. In this paper, we study the sequenced event set pattern matching problem, which is the problem of matching a sequence of input events against a complex pattern that specifies a sequence of sets of events rather than a sequence of single events. Similar to the PERMUTE operator, events that match with a set specified in the pattern can occur in any permutation, whereas events that match with different sets have to be strictly consecutive, following the order of the sets in the pattern specification. We formally define the problem of sequenced event set pattern matching, propose an automaton-based evaluation algorithm, and provide a detailed analysis of its runtime complexity. An empirical evaluation with real-world data shows that our algorithm outperforms a brute force approach that uses existing techniques to solve the sequenced event set pattern matching problem, and it validates the results from our complexity analysis.

#index 1549840
#* GPX-matcher: a generic boolean predicate-based XPath expression matcher
#@ Mohammad Sadoghi;Ioana Burcea;Hans-Arno Jacobsen
#t 2011
#c 8
#% 158911
#% 271199
#% 333938
#% 465061
#% 480649
#% 567463
#% 570879
#% 654476
#% 731408
#% 800593
#% 812783
#% 864440
#% 893134
#% 1022274
#% 1090772
#% 1114714
#% 1246265
#% 1286060
#% 1302863
#% 1328112
#% 1523931
#! Content-based architectures for XML data dissemination are gaining increasing attention both in academia and industry. These dissemination networks are the building blocks of selective information dissemination applications which have wide applicability such as sharing and integrating information in both scientific and corporate domains. At the heart of these dissemination services is a fast engine for matching of an incoming XML message against stored XPath expressions to determine interested consumers for the message. To achieve the ultra-low response time, predominant in financial message processing, the XPath expression matching must be done efficiently. In this paper, we develop and evaluate a novel algorithm based on a unique encoding of XPath expressions and XML messages, unlike dominating automaton-based algorithms, for efficiently solving this matching problem. We demonstrate a matching time in the millisecond range for millions of XPath expressions which significantly outperforms state-of-the-art algorithms.

#index 1549841
#* An optimal strategy for monitoring top-k queries in streaming windows
#@ Di Yang;Avani Shastri;Elke A. Rundensteiner;Matthew O. Ward
#t 2011
#c 8
#% 300180
#% 397378
#% 410276
#% 654443
#% 733373
#% 765418
#% 766671
#% 875022
#% 875023
#% 878299
#% 994013
#% 1016183
#% 1127375
#% 1181258
#% 1292554
#% 1328182
#% 1442465
#! Continuous top-k queries, which report a certain number (k) of top preferred objects from data streams, are important for a broad class of real-time applications, ranging from financial analysis to network traffic monitoring. Existing solutions for tackling this problem aim to reduce the computational costs by incrementally updating the top-k results upon each window slide. However, they all suffer from the performance bottleneck of periodically requiring a complete recomputation of the top-k results from scratch. Such an operation is not only computationally expensive but also causes significant memory consumption, as it requires keeping all objects alive in the query window. To solve this problem, we identify the "Minimal Top-K candidate set" (MTK), namely the subset of stream objects that is both necessary and sufficient for continuous top-k monitoring. Based on this theoretical foundation, we design the MinTopk algorithm that elegantly maintains MTK and thus eliminates the need for recomputation. We prove the optimality of the MinTopk algorithm in both CPU and memory utilization for continuous top-k monitoring. Our experimental study shows that both the efficiency and scalability of our proposed algorithm is clearly superior to the state-of-the-art solutions.

#index 1549842
#* Fast and accurate computation of equi-depth histograms over data streams
#@ Hamid Mousavi;Carlo Zaniolo
#t 2011
#c 8
#% 210190
#% 220551
#% 248820
#% 333931
#% 338425
#% 378388
#% 414993
#% 479648
#% 482123
#% 745533
#% 801696
#% 803602
#% 874903
#% 982573
#% 1015256
#% 1292584
#! Equi-depth histograms represent a fundamental synopsis widely used in both database and data stream applications, as they provide the cornerstone of many techniques such as query optimization, approximate query answering, distribution fitting, and parallel database partitioning. Equi-depth histograms try to partition a sequence of data in a way that every part has the same number of data items. In this paper, we present a new algorithm to estimate equi-depth histograms for high speed data streams over sliding windows. While many previous methods were based on quantile computations, we propose a new method called BAr Splitting Histogram (BASH) that provides an expected ε-approximate solution to compute the equi-depth histogram. Extensive experiments show that BASH is at least four times faster than one of the best existing approaches, while achieving similar or better accuracy and in some cases using less memory. The experimental results also indicate that BASH is more stable on data affected by frequent concept shifts.

#index 1549843
#* Energy proportionality for disk storage using replication
#@ Jinoh Kim;Doron Rotem
#t 2011
#c 8
#% 105972
#% 212829
#% 310775
#% 342368
#% 442633
#% 642363
#% 723279
#% 730787
#% 835193
#% 888089
#% 949834
#% 963873
#% 995880
#% 1034471
#% 1050824
#% 1060676
#% 1095874
#% 1426497
#% 1433985
#% 1468210
#% 1559177
#! Saving energy for storage is of major importance as storage devices (and cooling them off) may contribute over 25 percent of the total energy consumed in a datacenter. Recent work introduced the concept of energy proportionality and argued that it is a more relevant metric than just energy saving as it takes into account the tradeoff between energy consumption and performance. In this paper, we present a novel approach, called FREP (Fractional Replication for Energy Proportionality), for energy management in large datacenters. FREP includes a replication strategy and basic functions to enable flexible energy management. Specifically, our method provides performance guarantees by adaptively controlling the power states of a group of disks based on observed and predicted workloads. Our experiments, using a set of real and synthetic traces, show that FREP dramatically reduces energy requirements with a minimal response time penalty.

#index 1549844
#* Caching query-biased snippets for efficient retrieval
#@ Diego Ceccarelli;Claudio Lucchese;Salvatore Orlando;Raffaele Perego;Fabrizio Silvestri
#t 2011
#c 8
#% 288614
#% 413587
#% 449746
#% 577302
#% 578337
#% 860861
#% 987208
#% 1089473
#% 1190098
#% 1195871
#% 1306081
#% 1417245
#% 1834787
#! Web Search Engines' result pages contain references to the top-k documents relevant for the query submitted by a user. Each document is represented by a title, a snippet and a URL. Snippets, i.e. short sentences showing the portions of the document being relevant to the query, help users to select the most interesting results. The snippet generation process is very expensive, since it may require to access a number of documents for each issued query. We assert that caching, a popular technique used to enhance performance at various levels of any computing systems, can be very effective in this context. We design and experiment several cache organizations, and we introduce the concept of supersnippet, that is the set of sentences in a document that are more likely to answer future queries. We show that supersnippets can be built by exploiting query logs, and that in our experiments a supersnippet cache answers up to 62% of the requests, remarkably outperforming other caching approaches.

#index 1549845
#* Efficient and scalable data evolution with column oriented databases
#@ Ziyang Liu;Bin He;Hui-I Hsiao;Yi Chen
#t 2011
#c 8
#% 227861
#% 332166
#% 393844
#% 442967
#% 449395
#% 576093
#% 655987
#% 816391
#% 824697
#% 824736
#% 866981
#% 875026
#% 893129
#% 945866
#% 960234
#% 1015303
#% 1016131
#% 1022236
#% 1063542
#% 1127411
#% 1127421
#% 1301127
#% 1318729
#% 1372701
#% 1523930
#% 1699937
#% 1727901
#% 1728154
#! Database evolution is the process of updating the schema of a database or data warehouse (schema evolution) and evolving the data to the updated schema (data evolution). It is often desired or necessitated when changes occur to the data or the query workload, the initial schema was not carefully designed, or more knowledge of the database is known and a better schema is concluded. The Wikipedia database, for example, has had more than 170 versions in the past 5 years [8]. Unfortunately, although much research has been done on the schema evolution part, data evolution has long been a prohibitively expensive process, which essentially evolves the data by executing SQL queries and re-constructing indexes. This prevents databases from being flexibly and frequently changed based on the need and forces schema designers, who cannot afford mistakes, to be highly cautious. Techniques that enable efficient data evolution will undoubtedly make life much easier. In this paper, we study the efficiency of data evolution, and discuss the techniques for data evolution on column oriented databases, which store each attribute, rather than each tuple, contiguously. We show that column oriented databases have a better potential than traditional row oriented databases for supporting data evolution, and propose a novel data-level data evolution framework on column oriented databases. Our approach, as suggested by experimental evaluations on real and synthetic data, is much more efficient than the query-level data evolution on both row and column oriented databases, which involves unnecessary access of irrelevant data, materializing intermediate results and re-constructing indexes.

#index 1549846
#* Native support of multi-tenancy in RDBMS for software as a service
#@ Oliver Schiller;Benjamin Schiller;Andreas Brodt;Bernhard Mitschang
#t 2011
#c 8
#% 531907
#% 864445
#% 918842
#% 1063561
#% 1142967
#% 1207027
#% 1217216
#% 1217217
#! Software as a Service (SaaS) facilitates acquiring a huge number of small tenants by providing low service fees. To achieve low service fees, it is essential to reduce costs per tenant. For this, consolidating multiple tenants onto a single relational schema instance turned out beneficial because of low overheads per tenant and scalable manageability. This approach implements data isolation between tenants, per-tenant schema extension and further tenant-centric data management features in application logic. This is complex, disables some optimization opportunities in the RDBMS and represents a conceptual misstep with Separation of Concerns in mind. Therefore, we contribute first features of a RDBMS to support tenant-aware data management natively. We introduce tenants as first-class database objects and propose the concept of a tenant context to isolate a tenant from other tenants. We present a schema inheritance concept that allows sharing a core application schema among tenants while enabling schema extensions per tenant. Finally, we evaluate a preliminary implementation of our approach.

#index 1549847
#* SLA-tree: a framework for efficiently supporting SLA-based decisions in cloud computing
#@ Yun Chi;Hyun Jin Moon;Hakan Hacigümüş;Junichi Tatemura
#t 2011
#c 8
#% 408396
#% 413626
#% 435110
#% 654849
#% 741340
#% 750148
#% 785216
#% 786866
#% 799363
#% 843801
#% 893131
#% 1055813
#% 1063542
#% 1070396
#% 1181272
#% 1206984
#% 1213279
#% 1215805
#% 1426550
#% 1426589
#! As cloud computing becomes increasingly important in database systems, many new challenges and opportunities have arisen. One challenge is that in cloud computing, business profit plays a central role. Hence, it is very important for a cloud service provider to quickly make profit-oriented decisions. In this paper, we propose a novel data structure, called SLA-tree, to efficiently support profit-oriented decision making. SLA-tree is built on two pieces of information: (1) a set of buffered queries waiting to be executed, which represents the scheduled events that will happen in the near future, and (2) a service level agreement (SLA) for each query, which indicates the different profits for the query for varying query response times. By constructing the SLA-tree, we efficiently support the answering of certain profit-oriented "what if" questions. Answers to these questions in turn can be applied to different profit-oriented decisions in cloud computing such as profit-aware scheduling, dispatching, and capacity planning. Extensive experimental results based on both synthetic and real-world data demonstrate the effectiveness and efficiency of our SLA-tree framework.

#index 1549848
#* On enhancing scalability for distributed RDF/S stores
#@ George Tsatsanifos;Dimitris Sacharidis;Timos Sellis
#t 2011
#c 8
#% 280
#% 23651
#% 58365
#% 86786
#% 286258
#% 348182
#% 379483
#% 379484
#% 427199
#% 577320
#% 577358
#% 577359
#% 598374
#% 754123
#% 801677
#% 858178
#% 917698
#% 1022236
#% 1127431
#% 1152464
#% 1153279
#% 1155760
#% 1304544
#% 1408727
#% 1702419
#% 1711388
#! This work presents MIDAS-RDF, a distributed P2P RDF/S repository that is built on top of a distributed multi-dimensional index structure. MIDAS-RDF features fast retrieval of RDF triples satisfying various pattern queries by translating them into multi-dimensional range queries, which can be processed by the underlying index in hops logarithmic to the number of peers. More importantly, MIDAS-RDF utilizes a labeling scheme to handle expensive transitive closure computations efficiently. This allows for distributed RDFS reasoning in a more scalable way compared to existing methods, as also demonstrated by our extensive experimental study. Furthermore, MIDAS-RDF supports a publish-subscribe model that enables remote peers to selectively subscribe to RDF content.

#index 1549849
#* Answering tree pattern queries using views: a revisit
#@ Junhu Wang;Jiang Li;Jeffrey Xu Yu
#t 2011
#c 8
#% 733593
#% 824661
#% 824690
#% 893135
#% 1107579
#% 1169490
#% 1181227
#% 1194689
#% 1328134
#% 1337605
#% 1411297
#% 1587728
#! We revisit the problem of answering tree pattern queries using views. We first show that, for queries and views that do not have nodes labeled with the wildcard *, there is an alternative to the approach of query rewriting which does not require us to find any rewritings explicitly yet which produces the same answers as the maximal contained rewriting. Then, using the new approach, we give a simple criterion and a corresponding algorithm for identifying redundant view answers, which are view answers that can be ignored when evaluating the maximal contained rewriting. Finally, for queries and views that do have nodes labeled *, we provide a method to find the maximal contained rewriting and show how to answer the query using views without explicitly finding the rewritings.

#index 1549850
#* Dynamic reasoning on XML updates
#@ Federico Cavalieri;Giovanna Guerrini;Marco Mesiti
#t 2011
#c 8
#% 428150
#% 463580
#% 480659
#% 659923
#% 864400
#% 905328
#% 1022210
#% 1046512
#% 1092015
#% 1266686
#% 1328114
#% 1384854
#% 1464042
#% 1688277
#% 1718234
#! In many emerging XML application contexts and distributed execution environments (like disconnected and cloud computing, collaborative editing and document versioning) the server that determines the updates to be performed on a document, by evaluating an XQuery Update expression, is not always the same that actually makes such updates -represented as Pending Update Lists (PULs)-effective. The process of generating the PUL is thus decoupled from that of making its effect persistent on the document. The PUL executor needs to manage several PULs, that, depending on the application context, are to be executed as sequential or parallel update requests, possibly relying on application-specific policies. This requires some capabilities of dynamic reasoning on updates. In the paper, we state the most relevant properties to reason on, develop the corresponding algorithms and present a PUL handling system, providing an experimental evaluation of this system.

#index 1549851
#* Algebraic incremental maintenance of XML views
#@ Angela Bonifati;Martin Goodfellow;Ioana Manolescu;Domenica Sileo
#t 2011
#c 8
#% 43031
#% 152928
#% 210182
#% 286901
#% 570877
#% 791181
#% 805907
#% 810045
#% 824661
#% 824690
#% 864465
#% 864516
#% 874911
#% 875007
#% 893136
#% 994015
#% 1015275
#% 1016134
#% 1181304
#% 1196553
#% 1206683
#% 1217201
#% 1328114
#% 1523876
#% 1594601
#% 1718234
#% 1721253
#% 1733292
#! Materialized views can bring important performance benefits when querying XML documents. In the presence of XML document changes, materialized views need to be updated to faithfully reflect the changed document. In this work, we present an algebraic approach for propagating source updates to XML materialized views expressed in a powerful XML tree pattern formalism. Our approach differs from the state of the art in the area in two important ways. First, it relies on set-oriented, algebraic operations, to be contrasted with node-based previous approaches. Second, it exploits state-of-the-art features of XML stores and XML query evaluation engines, notably XML structural identifiers and associated structural join algorithms. We present algorithms for determining how updates should be propagated to views, and highlight the benefits of our approach over existing algorithms through a series of experiments.

#index 1549852
#* Keyword-based, context-aware selection of natural language query patterns
#@ Giorgio Orsi;Letizia Tanca;Eugenio Zimeo
#t 2011
#c 8
#% 156337
#% 169779
#% 384978
#% 387427
#% 420175
#% 659990
#% 660011
#% 665856
#% 744789
#% 745968
#% 790013
#% 810101
#% 820875
#% 840583
#% 875063
#% 901496
#% 993987
#% 1050777
#% 1052968
#% 1063536
#% 1200310
#% 1206910
#% 1217200
#% 1223424
#% 1246534
#% 1289408
#% 1328105
#% 1409940
#% 1426537
#% 1693286
#% 1728984
#! Pervasive access to distributed data sources by means of mobile devices is becoming a frequent realistic operational context in many application domains. In these scenarios data access may be thwarted by the scarce knowledge that users have of the application and of the underlying data schemas and complicated by limited query interfaces, due to the small size of the devices. A viable solution to this problem could be expressing the queries in natural language; however, in applications like medical emergencies, data management systems must obey requirements such as very fast and precise data access which make this solution infeasible. To reduce the time needed to get answers to user queries, the paper proposes a lightweight, context-aware approach based on the combination of keywords with natural language queries. The method employs ontologies and query patterns to support the users in formulating the most appropriate query for retrieving the desired data. Precision and query efficiency are further improved by focusing searches only to the data which are meaningful w.r.t. the current context, thus supporting the users' situation awareness. The approach has been integrated in the SAFE system, developed for mobile and Web, and has been applied in cardiology to support medical personnel in emergency interventions on patients affected by chronic cerebro-vascular diseases. Experimental results have shown that the proposed solution significantly reduces the time to get useful data w.r.t. traditional form-based approaches.

#index 1549853
#* Unified structure and content search for personal information management systems
#@ Wei Wang;Amélie Marian;Thu D. Nguyen
#t 2011
#c 8
#% 85445
#% 397358
#% 397375
#% 458861
#% 642993
#% 643566
#% 750867
#% 765408
#% 810108
#% 824681
#% 845350
#% 893119
#% 936919
#% 963490
#% 1021954
#% 1037636
#% 1044474
#% 1055755
#% 1077150
#! User data stored in personal information systems is growing massively. Simultaneously, this data is increasingly distributed across multiple organizational domains such as email, music databases, and photo albums, some of which are structured automatically by applications. Powerful search tools are needed to help users locate data in these expanding yet fragmented data sets. In this paper, we present a novel fuzzy search approach that considers approximate matches to structure and content query conditions. Our framework uses unified data and query processing models so that structure conditions can be approximately matched by content and vice versa. Our models also unify external structure (e.g., directories) with internal structure (e.g., XML structure), supporting integrated queries matched to a single data domain. We propose indexes and algorithms for efficient query processing. We evaluate our approach using a real data set, showing that it can leverage structure information to significantly improve search accuracy, yet is robust to mistakes in query conditions.

#index 1549854
#* TopRecs: Top-k algorithms for item-based collaborative filtering
#@ Mohammad Khabbaz;Laks V. S. Lakshmanan
#t 2011
#c 8
#% 173879
#% 330687
#% 397378
#% 643566
#% 729626
#% 734594
#% 787903
#% 813966
#% 891559
#% 956521
#% 960242
#% 960243
#% 1016183
#% 1036161
#% 1063475
#% 1063597
#% 1127498
#% 1217203
#% 1328172
#% 1650569
#! Recommender systems help users find their items of interest from large data collections with little effort. Collaborative filtering (CF) is one of the most popular approaches for making recommendations. While significant work has been done on improving accuracy of CF methods, some of the most popular CF approaches are limited in terms of scalability and efficiency. The size of data in modern recommender systems is growing rapidly in terms of both new users and items and new ratings. Item-based recommendation is one of the CF approaches used widely in practice. It computes and uses an item-item similarity matrix in order to predict unknown ratings. Previous works on item-based CF method confirm its usefulness in providing high quality top-k results. In this paper, we design a scalable algorithm for top-k recommendations using this method. We achieve this by probabilistic modeling of the similarity matrix. A unique challenge here is that the ratings that are aggregated to produce the aggregate predicted score for a user should be obtained from different lists for different candidate items and the aggregate function is non-monotone. We propose a layered architecture for CF systems that facilitates computation of the most relevant items for a given user. We design efficient top-k algorithms and data structures in order to achieve high scalability. Our algorithm is based on abstracting the key computation of a CF algorithm in terms of two operations -- probe and explore. The algorithm uses a cost-based optimization whereby we express the overall cost as a function of a similarity threshold and determine its optimal value for minimizing the cost. We empirically evaluate our theoretical results on a large real world dataset. Our experiments show our exact top-k algorithm achieves better scalability compared to solid baseline algorithms.

#index 1549855
#* Efficient answering of set containment queries for skewed item distributions
#@ Manolis Terrovitis;Panagiotis Bouros;Panos Vassiliadis;Timos Sellis;Nikos Mamoulis
#t 2011
#c 8
#% 69316
#% 115462
#% 115466
#% 212665
#% 213786
#% 227783
#% 249989
#% 274490
#% 290703
#% 300169
#% 303072
#% 318437
#% 333866
#% 333950
#% 333981
#% 340886
#% 342397
#% 397123
#% 397151
#% 480463
#% 480926
#% 510483
#% 569755
#% 572294
#% 654454
#% 726628
#% 765463
#% 800580
#% 867054
#% 874704
#% 874993
#% 907561
#% 987275
#% 1206677
#% 1221039
#% 1227711
#% 1328179
#% 1426543
#% 1426579
#! In this paper we address the problem of efficiently evaluating containment (i.e., subset, equality, and superset) queries over set-valued data. We propose a novel indexing scheme, the Ordered Inverted File (OIF) which, differently from the state-of-the-art, indexes set-valued attributes in an ordered fashion. We introduce query processing algorithms that practically treat containment queries as range queries over the ordered postings lists of OIF and exploit this ordering to quickly prune unnecessary page accesses. OIF is simple to implement and our experiments on both real and synthetic data show that it greatly outperforms the current state-of-the-art methods for all three classes of containment queries.

#index 1549856
#* Subspace clustering for indexing high dimensional data: a main memory index based on local reductions and individual multi-representations
#@ Stephan Günnemann;Hardy Kremer;Dominik Lenhard;Thomas Seidl
#t 2011
#c 8
#% 86950
#% 164360
#% 172949
#% 248792
#% 248797
#% 273891
#% 300131
#% 376266
#% 397384
#% 427199
#% 430746
#% 435141
#% 464888
#% 479649
#% 479973
#% 480133
#% 480307
#% 480632
#% 481956
#% 727868
#% 765518
#% 814646
#% 946436
#% 1165480
#% 1217189
#% 1292599
#% 1310270
#% 1318668
#% 1663653
#! Fast similarity search in high dimensional feature spaces is crucial in today's applications. Since the performance of traditional index structures degrades with increasing dimensionality, concepts were developed to cope with this curse of dimensionality. Most of the existing concepts exploit global correlations between dimensions to reduce the dimensionality of the feature space. In high dimensional data, however, correlations are often locally constrained to a subset of the data and every object can participate in several of these correlations. Accordingly, discarding the same set of dimensions for each object based on global correlations and ignoring the different correlations of single objects leads to significant loss of information. These aspects are relevant due to the direct correspondence between the degree of information preserved and the achievable query performance. We introduce a novel main memory index structure with increased information content for each single object compared to a global approach. This is achieved by using individual dimensions for each data object by applying the method of subspace clustering. The structure of our index is based on a multi-representation of objects reflecting their multiple correlations; that is, besides the general increase of information per object, we provide several individual representations for each single data object. These multiple views correspond to different local reductions per object and enable more effective pruning. In thorough experiments on real and synthetic data, we demonstrate that our novel solution achieves low query times and outperforms existing approaches designed for high dimensional data.

#index 1549857
#* On (not) indexing quadratic form distance by metric access methods
#@ Tomáš Skopal;Tomáš Bartoš;Jakub Lokoč
#t 2011
#c 8
#% 158959
#% 169940
#% 224113
#% 307247
#% 342827
#% 342828
#% 393247
#% 397685
#% 443889
#% 464888
#% 465031
#% 469413
#% 479462
#% 479655
#% 479788
#% 482109
#% 527186
#% 679321
#% 997496
#% 1279867
#% 1550142
#! The quadratic form distance (QFD) has been utilized as an effective similarity function in multimedia retrieval, in particular, when a histogram representation of objects is used. Unlike the widely used Euclidean distance, the QFD allows to arbitrarily correlate the histogram bins (dimensions), allowing thus to better model the similarity between histograms. However, unlike Euclidean distance, which is of linear time complexity, the QFD requires quadratic time to evaluate the similarity of two objects. In consequence, indexing and querying a database under QFD are expensive operations. In this paper we show that, given static correlations between dimensions, the QFD space can be transformed into an equivalent Euclidean space. Thus, the overall complexity of indexing and searching in the QFD similarity model can be reduced qualitatively. Besides the theoretical time complexity analysis of our approach applied to several metric access methods, in experimental evaluation we show the real-time speedup on a real-world image database.

#index 1549858
#* SeMiTri: a framework for semantic annotation of heterogeneous trajectories
#@ Zhixian Yan;Dipanjan Chakraborty;Christine Parent;Stefano Spaccapietra;Karl Aberer
#t 2011
#c 8
#% 86950
#% 95730
#% 152937
#% 435148
#% 824722
#% 960412
#% 1035397
#% 1040086
#% 1052661
#% 1063572
#% 1127436
#% 1210517
#% 1214143
#% 1247777
#% 1278579
#% 1298894
#% 1298896
#% 1298920
#% 1302862
#% 1426623
#% 1445726
#% 1451250
#% 1480847
#% 1480943
#% 1523885
#% 1728806
#% 1737586
#! GPS devices allow recording the movement track of the moving object they are attached to. This data typically consists of a stream of spatio-temporal (x,y,t) points. For application purposes the stream is transformed into finite subsequences called trajectories. Existing knowledge extraction algorithms defined for trajectories mainly assume a specific context (e.g. vehicle movements) or analyze specific parts of a trajectory (e.g. stops), in association with data from chosen geographic sources (e.g. points-of-interest, road networks). We investigate a more comprehensive semantic annotation framework that allows enriching trajectories with any kind of semantic data provided by multiple 3rd party sources. This paper presents SeMiTri - the framework that enables annotating trajectories for any kind of moving objects. Doing so, the application can benefit from a "semantic trajectory" representation of the physical movement. The framework and its algorithms have been designed to work on trajectories with varying data quality and different structures, with the objective of covering abstraction requirements of a wide range of applications. Performance of SeMiTri has been evaluated using many GPS datasets from multiple sources -- including both fast moving objects (e.g. cars, trucks) and people's trajectories (e.g. with smartphones). These two kinds of experiments are reported in this paper.

#index 1549859
#* Efficient execution plans for distributed skyline query processing
#@ João B. Rocha-Junior;Akrivi Vlachou;Christos Doulkeridis;Kjetil Nørvåg
#t 2011
#c 8
#% 14513
#% 190407
#% 465167
#% 864453
#% 907529
#% 1063486
#% 1065612
#% 1114757
#% 1166462
#% 1177872
#% 1201865
#% 1206767
#% 1247776
#% 1267293
#% 1512991
#% 1688253
#! In this paper, we study the generation of efficient execution plans for skyline query processing in large-scale distributed environments. In such a setting, each server stores autonomously a fraction of the data, thus all servers need to process the skyline query. An execution plan defines the order in which the individual skyline queries are processed on different servers, and influences the performance of query processing. Querying servers consecutively reduces the amount of transferred data and the number of queried servers, since skyline points obtained by one server prune points in the subsequent servers, but also increases the latency of the system. To address this trade-off, we introduce a novel framework, called SkyPlan, for processing distributed skyline queries that generates execution plans aiming at optimizing the performance of query processing. Thus, we quantify the gain of querying consecutively different servers. Then, execution plans are generated that maximize the overall gain, while also taking into account additional objectives, such as bounding the maximum number of hops required for the query or balancing the load on different servers fairly. Finally, we present an algorithm for distributed processing based on the generated plan that continuously refines the execution plan during in-network processing. Our framework consistently outperforms the state-of-the-art algorithm.

#index 1549860
#* Probabilistic range queries for uncertain trajectories on road networks
#@ Kai Zheng;Goce Trajcevski;Xiaofang Zhou;Peter Scheuermann
#t 2011
#c 8
#% 295512
#% 453538
#% 458849
#% 464847
#% 527176
#% 654487
#% 657739
#% 729850
#% 729851
#% 771228
#% 772835
#% 824722
#% 824723
#% 836161
#% 878301
#% 885388
#% 889142
#% 893092
#% 1015321
#% 1016199
#% 1072575
#% 1072634
#% 1181287
#% 1328209
#% 1405080
#% 1428630
#! Trajectories representing the motion of moving objects are typically obtained via location sampling, e.g. using GPS or road-side sensors, at discrete time-instants. In-between consecutive samples, nothing is known about the whereabouts of a given moving object. Various models have been proposed (e.g., sheared cylinders; spacetime prisms) to represent the uncertainty of the moving objects both in unconstrained Euclidian space, as well as road networks. In this paper, we focus on representing the uncertainty of the objects moving along road networks as time-dependent probability distribution functions, assuming availability of a maximal speed on each road segment. For these settings, we introduce a novel indexing mechanism -- UTH (Uncertain Trajectories Hierarchy), based upon which efficient algorithms for processing spatio-temporal range queries are proposed. We also present experimental results that demonstrate the benefits of our proposed methodologies.

#index 1549861
#* TAGs: scalable threshold-based algorithms for proximity computation in graphs
#@ A. Lyritsis;A. N. Papadopoulos;Y. Manolopoulos
#t 2011
#c 8
#% 186
#% 8919
#% 68589
#% 124524
#% 237380
#% 283833
#% 310514
#% 329789
#% 333854
#% 410276
#% 769887
#% 798044
#% 805906
#% 823342
#% 842290
#% 844334
#% 881480
#% 881496
#% 915344
#% 1038931
#% 1318663
#% 1372657
#% 1682572
#! A fundamental and very useful operation in graphs is the computation of the proximity between nodes, i.e., the degree of dissimilarity (or similarity) between two nodes v and u. This is an important tool both in graph databases and graph mining applications, because it provides the base to support more complex tasks such as graph partitioning, clustering, classification, to name a few. All methods proposed in the literature assume that proximity is computed on a single graph by using a single distance measure. In addition, most of them focus on the proximity between node pairs. In this work, we present for the first time, scalable algorithms that: (i) they support proximity computation in multiple graph instances, (ii) they enable the utilization of several distance measures, (iii) they support proximity queries around a source node without limiting to node pairs and (iv) they support extensions for metric-based and skyline query processing. The main result of our work is the design of Threshold Algorithms for Graphs (denoted as TAGs), which are studied and evaluated experimentally by using real-life as well as synthetic graphs, based on both the G(n, p) Erdõs-Rényi model and power law degree distributions.

#index 1549862
#* Projection for XML update optimization
#@ Mohamed-Amine Baazizi;Nicole Bidoit;Dario Colazzo;Noor Malla;Marina Sahakyan
#t 2011
#c 8
#% 827135
#% 893111
#% 960258
#% 1015272
#% 1092015
#% 1266686
#% 1328114
#% 1495312
#% 1661444
#% 1661451
#% 1718234
#! While projection techniques have been extensively investigated for XML querying, we are not aware of applications to XML updating. This paper investigates a projection based optimization mechanism for XQuery Update Facility expressions in the presence of a schema. This paper includes a formal development and study of the method as well as experiments testifying its effectiveness.

#index 1549863
#* Efficient reverse skyline retrieval with arbitrary non-metric similarity measures
#@ Prasad M. Deshpande;P. Deepak
#t 2011
#c 8
#% 248806
#% 300163
#% 321455
#% 427199
#% 451645
#% 465009
#% 465167
#% 479462
#% 487887
#% 643566
#% 654480
#% 730019
#% 875013
#% 902727
#% 1022226
#% 1044465
#% 1063485
#% 1107566
#% 1123924
#% 1235295
#% 1312536
#% 1523905
#% 1727538
#! A Reverse Skyline query returns all objects whose skyline contains the query object. In this paper, we consider Reverse Skyline query processing where the distance between attribute values are not necessarily metric. We outline real world cases that motivate Reverse Skyline processing in such scenarios. We consider various optimizations to develop efficient algorithms for Reverse Skyline processing. Firstly, we consider block-based processing of objects to optimize on IO costs. We then explore pre-processing to re-arrange objects on disk to speed-up computational and IO costs. We then present our main contribution, which is a method of using group-level reasoning and early pruning to micro-optimize processing by reducing attribute level comparisons. An extensive empirical evaluation with real-world datasets and synthetic data of varying characteristics shows that our optimization techniques are indeed very effective in dramatically speeding Reverse Skyline processing, both in terms of computational costs and IO costs.

#index 1549864
#* Fast random graph generation
#@ Sadegh Nobari;Xuesong Lu;Panagiotis Karras;Stéphane Bressan
#t 2011
#c 8
#% 1331
#% 18646
#% 54968
#% 190611
#% 243299
#% 309749
#% 348060
#% 431105
#% 798509
#% 837654
#% 963669
#% 988651
#% 991569
#% 1001365
#% 1024515
#% 1127360
#% 1214705
#% 1226124
#% 1394202
#% 1736577
#! Today, several database applications call for the generation of random graphs. A fundamental, versatile random graph model adopted for that purpose is the Erd&odblac;s-Rényi Γv,p model. This model can be used for directed, undirected, and multipartite graphs, with and without self-loops; it induces algorithms for both graph generation and sampling, hence is useful not only in applications necessitating the generation of random structures but also for simulation, sampling and in randomized algorithms. However, the commonly advocated algorithm for random graph generation under this model performs poorly when generating large graphs, and fails to make use of the parallel processing capabilities of modern hardware. In this paper, we propose PPreZER, an alternative, data parallel algorithm for random graph generation under the Erd&odblac;s-Rényi model, designed and implemented in a graphics processing unit (GPU). We are led to this chief contribution of ours via a succession of seven intermediary algorithms, both sequential and parallel. Our extensive experimental study shows an average speedup of 19 for PPreZER with respect to the baseline algorithm.

#index 1549865
#* Symmetrizations for clustering directed graphs
#@ Venu Satuluri;Srinivasan Parthasarathy
#t 2011
#c 8
#% 274612
#% 283833
#% 313959
#% 594009
#% 823403
#% 840965
#% 867050
#% 956506
#% 1002007
#% 1013696
#% 1077150
#% 1214695
#% 1386131
#% 1404189
#% 1663632
#! Graph clustering has generally concerned itself with clustering undirected graphs; however the graphs from a number of important domains are essentially directed, e.g. networks of web pages, research papers and Twitter users. This paper investigates various ways of symmetrizing a directed graph into an undirected graph so that previous work on clustering undirected graphs may subsequently be leveraged. Recent work on clustering directed graphs has looked at generalizing objective functions such as conductance to directed graphs and minimizing such objective functions using spectral methods. We show that more meaningful clusters (as measured by an external ground truth criterion) can be obtained by symmetrizing the graph using measures that capture in- and out-link similarity, such as bibliographic coupling and co-citation strength. However, direct application of these similarity measures to modern large-scale power-law networks is problematic because of the presence of hub nodes, which become connected to the vast majority of the network in the transformed undirected graph. We carefully analyze this problem and propose a Degree-discounted similarity measure which is much more suitable for large-scale networks. We show extensive empirical validation.

#index 1549866
#* Efficient discovery of frequent subgraph patterns in uncertain graph databases
#@ Odysseas Papapetrou;Ekaterini Ioannou;Dimitrios Skoutas
#t 2011
#c 8
#% 288990
#% 322884
#% 478274
#% 481290
#% 629708
#% 727845
#% 729923
#% 729938
#% 730089
#% 731608
#% 765429
#% 769907
#% 769940
#% 769951
#% 772830
#% 864425
#% 902448
#% 960305
#% 1063502
#% 1083509
#% 1206686
#% 1292524
#% 1318582
#% 1372711
#% 1523884
#! Mining frequent subgraph patterns in graph databases is a challenging and important problem with applications in several domains. Recently, there is a growing interest in generalizing the problem to uncertain graphs, which can model the inherent uncertainty in the data of many applications. The main difficulty in solving this problem results from the large number of candidate subgraph patterns to be examined and the large number of subgraph isomorphism tests required to find the graphs that contain a given pattern. The latter becomes even more challenging, when dealing with uncertain graphs. In this paper, we propose a method that uses an index of the uncertain graph database to reduce the number of comparisons needed to find frequent subgraph patterns. The proposed algorithm relies on the apriori property for enumerating candidate subgraph patterns efficiently. Then, the index is used to reduce the number of comparisons required for computing the expected support of each candidate pattern. It also enables additional optimizations with respect to scheduling and early termination, that further increase the efficiency of the method. The evaluation of our approach on three real-world datasets as well as on synthetic uncertain graph databases demonstrates the significant cost savings with respect to the state-of-the-art approach.

#index 1549867
#* Finding closed frequent item sets by intersecting transactions
#@ Christian Borgelt;Xiaoyuan Yang;Ruben Nogales-Cadenas;Pedro Carmona-Saez;Alberto Pascual-Montano
#t 2011
#c 8
#% 232136
#% 300120
#% 338609
#% 481290
#% 662759
#% 729984
#% 765132
#% 785383
#! Most known frequent item set mining algorithms work by enumerating candidate item sets and pruning infrequent candidates. An alternative method, which works by intersecting transactions, is much less researched. To the best of our knowledge, there are only two basic algorithms: a cumulative scheme, which is based on a repository with which new transactions are intersected, and the Carpenter algorithm, which enumerates and intersects candidate transaction sets. These approaches yield the set of so-called closed frequent item sets, since any such item set can be represented as the intersection of some subset of the given transactions. In this paper we describe a considerably improved implementation scheme of the cumulative approach, which relies on a prefix tree representation of the already found intersections. In addition, we present an improved way of implementing the Carpenter algorithm. We demonstrate that on specific data sets, which occur particularly often in the area of gene expression analysis, our implementations significantly outperform enumeration approaches to frequent item set mining.

#index 1549868
#* Aspect-oriented relational algebra
#@ Curtis E. Dyreson
#t 2011
#c 8
#% 168773
#% 237303
#% 287268
#% 327230
#% 361445
#% 378553
#% 392374
#% 447946
#% 463424
#% 480129
#% 552019
#% 654457
#% 742561
#% 745525
#% 803468
#% 810761
#% 875015
#% 998907
#% 1015307
#% 1016204
#% 1054486
#% 1107578
#% 1482331
#! In this paper we apply the aspect-oriented programming (AOP) paradigm to the relational algebra. AOP is a way to add support for cross-cutting concerns to existing code without directly modifying that code. Data, like code, also has cross-cutting concerns such as versioning, privacy, and reliability. AOP techniques can be used to weave metadata around an application's data. The metadata imbues the data with additional semantics that must be observed in constraint and query processing. In this paper we show how to modify the relational algebra to process data woven together with metadata. We also analyze the overhead on evaluating an aspect-enhanced query.

#index 1549869
#* Synopses for probabilistic data over large domains
#@ Nicholas D. Larusso;Ambuj Singh
#t 2011
#c 8
#% 333948
#% 617839
#% 745494
#% 765451
#% 801684
#% 823333
#% 893167
#% 907562
#% 976984
#% 989609
#% 1016201
#% 1063521
#% 1127609
#% 1206735
#% 1206892
#% 1328153
#! Many real world applications produce data with uncertainties drawn from measurements over a continuous domain space. Recent research in the area of probabilistic databases has mainly focused on managing and querying discrete data in which the domain is limited to a small number of values (i.e. on the order of 10). When the size of the domain increases, current methods fail due to their nature of explicitly storing each value/probability pair. Such methods are not capable of extending their use to continuous-valued attributes. In this paper, we provide a scalable, accurate, space efficient probabilistic data synopsis for uncertain attributes defined over a continuous domain. Our synopsis construction methods are all error-aware to ensure that our synopsis provides an accurate representation of the underlying data given a limited space budget. Additionally, we are able to provide approximate query results over the synopsis with error bounds. We provide an extensive experimental evaluation to show that our proposed methods improve upon the current state of the art in terms of construction time and query accuracy. In particular, our synopsis can be constructed in O(N2) time (where N is the number of tuples in the database). We also demonstrate the ability of our synopsis to answer a variety of interesting queries on a real data set and show that our query error is reduced by up to an order of magnitude over the previous state-of-the-art method.

#index 1549870
#* Data integration with dependent sources
#@ Anish Das Sarma;Xin Luna Dong;Alon Halevy
#t 2011
#c 8
#% 237194
#% 248821
#% 297675
#% 341672
#% 458601
#% 480332
#% 481786
#% 482108
#% 572314
#% 800180
#% 810073
#% 893089
#% 960365
#% 1478768
#% 1523915
#! Data integration systems offer users a uniform interface to a set of data sources. Previous work has typically assumed that the data sources are independent of each other; however, in scenarios involving large numbers of sources, such as the Web or large enterprises, there is an eco-system of dependent sources, where some sources copy parts of their data from others. This paper considers the new optimization problems that arise while answering queries over large number of dependent sources. These are the (1) cost-minimization problem: what is the minimum cost we must incur to get all answer tuples, (2) maximum-coverage problem: given a bound on the cost, how can we get the maximum possible coverage, and (3) the source-ordering problem: for a set of data sources, what is the best order to query them so as to retrieve answer tuples as fast as possible. We consider these optimization problems under several cost models and we show that, in general, they are intractable. We describe effective approximation algorithms that enable us to solve these problems in practice. We then identify the causes of the high complexity and show that for restricted classes, the optimization problems can be solved in polynomial time.

#index 1549871
#* Constructing concept relation network and its application to personalized web search
#@ Kenneth Wai-Ting Leung;Hing Yuet Fung;Dik Lun Lee
#t 2011
#c 8
#% 577224
#% 637576
#% 807295
#% 956553
#% 956564
#% 956649
#% 1035573
#% 1065169
#% 1119129
#% 1166752
#% 1206702
#% 1410894
#! Search engines are very effective in finding relevant pages for a query. When a query is ambiguous, the search engine returns a mix of results for different semantic interpretations of the query. This paper proposes a method to extract concepts from the search results of a query, and, treating each retrieved concept as a query, it recursively constructs a network of concepts related to different semantic interpretations of the query. By connecting networks of concepts obtained from different queries, a large integrated network, called Concept Relation Network (CRN), is formed. CRN is a semantic network that can be automatically constructed and maintained using existing search engines (e.g., Google) on the web. Taking advantage of large scale commercial search engines, CRN is able to derive a large number of highly coherent, highly related concepts. We study several ways to weight the connections between the concepts in CRN. By distinguishing between location concepts and content concepts, we analyze the ambiguity of each type of concepts individually. We also propose to extract concept clusters from CRN based on different graph topology. We observe that complete subgraphs in CRN can be used to effectively determine semantically related concepts. Finally, we apply CRN to search engine personalization. Experimental results show that the application of CRN to a concept-based personalization algorithm significantly improves precision comparing to the baseline.

#index 1549872
#* Effective and efficient sampling methods for deep web aggregation queries
#@ Fan Wang;Gagan Agrawal
#t 2011
#c 8
#% 227883
#% 300132
#% 333955
#% 465162
#% 572308
#% 631935
#% 654486
#% 864405
#% 955762
#% 956455
#% 960286
#% 960294
#% 1015328
#% 1022241
#% 1023487
#% 1201871
#% 1206653
#% 1206906
#% 1372686
#% 1426573
#% 1535156
#% 1605830
#! A large part of the data on the World Wide Web resides in the deep web. Executing structured, high-level queries on deep web data sources involves a number of challenges, several of which arise because query execution engines have a very limited access to data. In this paper, we consider the problem of executing aggregation queries involving data enumeration on these data sources, which requires sampling. The existing work in this area (HDSampler and its variants) is based on simple random sampling. We observe that this approach cannot obtain good estimates when the data is skewed. While there has been a lot of work on sampling skewed data, the existing methods are based on prior knowledge of data, and are therefore not applicable to hidden databases. In this paper, we present two prior-knowledge-free sampling algorithms, Adaptive Neighborhood Sampling (ANS) and Two Phase adaptive Sampling (TPS), which allow an aggregation query to be answered with a high accuracy (even when there is a skew), and a low sampling cost. For this purpose, we have developed robust estimators for aggregation functions including AVG, MAX, and MIN. Our experiments show that for data with a moderate or a large skew, ANS and TPS yield more accurate estimates, outperforming HDSampler by a factor of 4 on the average. Even for the cases where data has a small skew, our TPS method has an important advantage, which is that it has only one-third of the sampling costs of HDSampler.

#index 1549873
#* Making interval-based clustering rank-aware
#@ Julia Stoyanovich;Sihem Amer-Yahia;Tova Milo
#t 2011
#c 8
#% 248792
#% 262112
#% 280417
#% 308739
#% 316481
#% 342660
#% 346526
#% 411762
#% 754124
#% 765518
#% 769910
#% 805863
#% 860672
#% 875003
#% 879567
#% 881500
#% 960244
#% 960287
#% 1016203
#% 1063597
#% 1083629
#% 1165480
#% 1181244
#% 1181261
#% 1206662
#% 1292614
#% 1594669
#! In online applications, such as online dating, users often query and rank large collections of structured items. Top results tend to be homogeneous, which hinders data exploration. For example, a dating website user who is looking for a partner between 20 and 40 years old, and who sorts the matches by income from higher to lower, will see a large number of matches in their late 30s who hold an MBA degree and work in the financial industry, before seeing any matches in different age groups and walks of life. An alternative to presenting results in a ranked list is to find clusters in the result space, identified by a combination of attributes that correlate with rank. Such clusters may describe matches between 35 and 40 with an MBA, matches between 25 and 30 who work in the software industry, etc., allowing for data exploration of ranked results. We refer to the problem of finding such clusters as rank-aware interval-based clustering and argue that it is not addressed by standard clustering algorithms. We formally define the problem and, to solve it, propose a novel measure of locality, together with a family of clustering quality measures appropriate for this application scenario. These ingredients may be used by a variety of clustering algorithms, and we present BARAC, a particular subspace-clustering algorithm that enables rank-aware interval-based clustering in domains with heterogeneous attributes. We validate the effectiveness of our approach with a large-scale user study, and perform an extensive experimental evaluation of efficiency, demonstrating that our methods are practical on the large scale. Our evaluation is performed on large datasets from Yahoo! Personals, a leading online dating site, and on restaurant data from Yahoo! Local.

#index 1549874
#* Predicting completion times of batch query workloads using interaction-aware models and simulation
#@ Mumtaz Ahmad;Songyun Duan;Ashraf Aboulnaga;Shivnath Babu
#t 2011
#c 8
#% 300166
#% 857367
#% 926881
#% 960265
#% 963935
#% 981519
#% 981520
#% 983465
#% 1044489
#% 1063541
#% 1119400
#% 1130825
#% 1142433
#% 1164968
#% 1166495
#% 1181024
#% 1181224
#% 1206952
#% 1206984
#% 1247793
#% 1328132
#% 1328213
#% 1468266
#% 1468504
#% 1618129
#! A question that database administrators (DBAs) routinely need to answer is how long a batch query workload will take to complete. This question arises, for example, while planning the execution of different report-generation workloads to fit within available time windows. To answer this question accurately, we need to take into account that the typical workload in a database system consists of mixes of concurrent queries. Interactions among different queries in these mixes need to be modeled, rather than the conventional approach of considering each query separately. This paper presents a new approach for estimating workload completion times that takes the significant impact of query interactions into account. This approach builds performance models using an experiment-driven technique, by sampling the space of possible query mixes and fitting statistical models to the observed performance at these samples. No prior assumptions are made about the internal workings of the database system or the cause of query interactions, making the models robust and portable. We show that a careful choice of sampling and statistical modeling strategies can result in accurate models, and we present a novel interaction-aware workload simulator that uses these models to estimate workload completion times. An experimental evaluation with complex TPC-H queries on IBM DB2 shows that this approach consistently predicts workload completion times with less than 20% error.

#index 1549875
#* Memory-efficient frequent-itemset mining
#@ Benjamin Schlegel;Rainer Gemulla;Wolfgang Lehner
#t 2011
#c 8
#% 1727
#% 152934
#% 280456
#% 300120
#% 322412
#% 481290
#% 481779
#% 607803
#% 678196
#% 765529
#% 818916
#% 824699
#% 881464
#% 946709
#% 1018118
#% 1022310
#% 1127463
#% 1127532
#% 1562157
#! Efficient discovery of frequent itemsets in large datasets is a key component of many data mining tasks. In-core algorithms---which operate entirely in main memory and avoid expensive disk accesses---and in particular the prefix tree-based algorithm FP-growth are generally among the most efficient of the available algorithms. Unfortunately, their excessive memory requirements render them inapplicable for large datasets with many distinct items and/or itemsets of high cardinality. To overcome this limitation, we propose two novel data structures---the CFP-tree and the CFP-array---, which reduce memory consumption by about an order of magnitude. This allows us to process significantly larger datasets in main memory than previously possible. Our data structures are based on structural modifications of the prefix tree that increase compressability, an optimized physical representation, lightweight compression techniques, and intelligent node ordering and indexing. Experiments with both real-world and synthetic datasets show the effectiveness of our approach.

#index 1549876
#* Link-based hidden attribute discovery for objects on Web
#@ Jiuming Huang;Haixun Wang;Yan Jia;Ariel Fuxman
#t 2011
#c 8
#% 240955
#% 312860
#% 345271
#% 378032
#% 431536
#% 433726
#% 464720
#% 577321
#% 660272
#% 765411
#% 805846
#% 840966
#% 855087
#% 857478
#% 864416
#% 878935
#% 881505
#% 938984
#% 956500
#% 1132868
#% 1696304
#! Information extraction from the Web is of growing importance. Objects on the Web are often associated with many attributes that describe the objects. It is essential to extract these attributes and map them to their corresponding objects. However, much attribute information about an object is hidden in the dynamic user interaction and is not on the Web page that describes the object. Existing information extraction approaches focus on getting information from the object Web page only, which means a lot of attribute information is lost. In this paper, we study the dynamic user interaction on exploratory search Websites and propose a novel link-based approach to discover attributes and map them to objects. We build an exploratory search model for exploratory Web sites, and we propose algorithms for identifying, clustering, and relationship mining of related Web pages based on the model. Using the unsupervised method in our approach, we are able to discover hidden attributes not explicitly shown on object Web pages. We test our approach on two online shopping Websites. We achieve high precision and recall: For entirely crawled Web sites the precision and recall are 98% and 97% respectively. For randomly crawled (sampled) Web sites the precision and recall are 98% and 80% respectively.

#index 1549877
#* Real-time approximate Range Motif discovery & data redundancy removal algorithm
#@ Ankur Narang;Souvik Bhattcherjee
#t 2011
#c 8
#% 201935
#% 248798
#% 248820
#% 248821
#% 248822
#% 307424
#% 322884
#% 340179
#% 411437
#% 459945
#% 479649
#% 479973
#% 646223
#% 654461
#% 805457
#% 848832
#% 874972
#% 960181
#% 963454
#% 1053490
#% 1111952
#% 1164862
#% 1171636
#% 1189345
#% 1247843
#% 1544270
#% 1656854
#! Removing redundancy in the data is an important problem as it helps in resource and compute efficiency for downstream processing of massive (10 million to 100 million records) datasets. In application domains such as IR, stock markets, telecom and others there is a strong need for real-time data redundancy removal of enormous amounts of data flowing at the rate of 1Gb/s or higher. We consider the problem of finding Range Motifs (clusters) over records in a large dataset such that records within the same cluster are approximately close to each other. This problem is closely related to the approximate nearest neighbour search but is more computationally expensive. Real-time scalable approximate Range Motif discovery on massive datasets is a challenging problem. We present the design of novel sequential and parallel approximate Range Motif discovery and data de-duplication algorithms using Bloom filters. We establish asymptotic upper bounds on the false positive and false negative rates for our algorithm. Further, time complexity analysis of our parallel algorithm on multi-core architectures has been presented. For 10 million records, our parallel algorithm can perform approximate Range Motif discovery and data de-duplication, on 4 sets (clusters), in 59s, on 16 core Intel Xeon 5570 architecture. This gives a throughput of around 170K records/s and around 700Mb/s (using records of size 4K bits). To the best of our knowledge, this is the highest real-time throughput for approximate Range Motif discovery and data redundancy removal on such massive datasets.

#index 1549878
#* Queries on dates: fast yet not blind
#@ Jaroslaw Szlichta;Parke Godfrey;Jarek Gryz;Wenbin Ma;Przemyslaw Pawluk;Calisto Zuzarte
#t 2011
#c 8
#% 237198
#% 378064
#% 392740
#% 393530
#% 393641
#% 397350
#% 461921
#% 482093
#% 882592
#! Data warehouses are repositories of electronically stored data which are designed to support reporting and analysis. The analysis of historical data often involves aggregation over time. Thus, time is critical in the design of a data warehouse. We describe novel techniques for storing date information and optimization of queries that reference the date dimension. We show how to embed intelligence into the date key and how to exploit monotonic dependencies. We present the value of these techniques for the improvement of performance when combined with partitioning and indexes. We evaluate these techniques on our prototype implemented in IBM® DB2® V9.7 over the current draft version of the TPC-DS benchmark.

#index 1549879
#* Designing integration flows using hypercubes
#@ Kevin Wilkinson;Alkis Simitsis
#t 2011
#c 8
#% 385321
#% 461921
#% 503884
#% 1181213
#% 1217226
#% 1296945
#% 1415590
#% 1531195
#! The design and implementation of an ETL (extract-transform-load) process for a data warehouse proceeds from a conceptual model to a logical model, and then a physical model and implementation. The conceptual model conveys at a high level the data sources and targets, and the transformation steps from sources to targets. The current state of the art is to express the conceptual model informally using text descriptions and diagrams. This makes the process of deriving a logical model time-consuming and error-prone. Our work is towards a system that covers the whole ETL lifecycle by injecting several layers of optimization and validation throughout the whole process starting with the business level objectives and ending with flow execution. In this paper, we focus on the ETL conceptual layer and present a solution that assists consultants in their task of defining the needs and requirements at the early stages of an integration project. We present a conceptual model for ETL based on hypercubes and hypercube operations. This is a formal model that captures the semantics of ETL at a high-level but that can also be machine-translated into a logical model for ETL. The use of hypercubes at the conceptual level renders a design that can be easily understood by business users and so reduces design and development time and produces a result that accurately captures service level agreements and business requirements.

#index 1549880
#* Experience in Continuous analytics as a Service (CaaaS)
#@ Qiming Chen;Meichun Hsu;Hans Zeller
#t 2011
#c 8
#% 654510
#% 726621
#% 875006
#% 878299
#% 882833
#% 1063555
#% 1127354
#% 1181240
#% 1267854
#% 1270449
#% 1328186
#% 1547397
#! Mobile applications, such as those on WebOS, increasingly depend on continuous analytics results of real-time events, for monitoring oil & gas production, watching traffic status and detecting accident, etc, which has given rise to the need of providing Continuous analytics as a Service (CaaaS). While representing a paradigm shift in cloud computing, CaaaS poses several challenges in scalability, latency, time-window semantics, transaction control and result-set staging. A data stream is infinite thus can only be analyzed in granules. We propose a continuous query model over both static relations and dynamic streaming data, which allows a long-standing SQL query instance to run cycle by cycle, each cycle for a chunk of data from the data stream, using a cut-and-rewind mechanism. We further support the cycle-based transaction model with cycle-based isolation and visibility, for delivering analytics results to the clients continuously while the query is running. To have the continuously generated analytics results staged efficiently, we developed the table-ring and label switching mechanism characterized by staging data through metadata manipulation without physical data moving and copying. To scale-out analytics computation, we support both parallel database based and network distributed Map-Reduce based infrastructure with multiple cooperating engines. We have built the proposed infrastructure by extending the PostgreSQL engine. We tested the throughput and latency of this service based on a well-known stream processing benchmark; the results show that the proposed approach is highly competitive. Our experiments indicate that the database technology can be extended and applied to real-time continuous analytics service provisioning.

#index 1549881
#* SocialSearch: enhancing entity search with social network matching
#@ Gae-won You;Seung-won Hwang;Zaiqing Nie;Ji-Rong Wen
#t 2011
#c 8
#% 269217
#% 577224
#% 741892
#% 748026
#% 805885
#% 809460
#% 881477
#% 1040837
#% 1183090
#% 1214742
#% 1259854
#% 1287226
#% 1355059
#! This paper introduces the problem of matching people names to their corresponding social network identities such as their Twitter accounts. Existing tools for this purpose build upon naive textual matching and inevitably suffer low precision, due to false positives (e.g., fake impersonator accounts) and false negatives (e.g., accounts using nicknames). To overcome these limitations, we leverage "relational" evidences extracted from the Web corpus. In particular, as such an example, weadopt Web document co-occurrences, which can be interpreted as an "implicit" counterpart of Twitter follower relationships. Using both textual and relational features, we learn a ranking function aggregating these features for the accurate ordering of candidate matches. Another key contribution of this paper is to formulate confidence scoring as a separate problem from relevance ranking. A baseline approach is to use the relevance of the top match itself as the confidence score. In contrast, we train a separate classifier, using not only the top relevance score but also various statistical features extracted from the relevance scores of all candidates, and empirically validate to outperform the baseline approach. We evaluate our proposed system using real-life internetscale entity-relationship and social network graphs.

#index 1549882
#* Road crash proneness prediction using data mining
#@ Richi Nayak;Daniel Emerson;Justin Weligamage;Noppadol Piyatrapoomi
#t 2011
#c 8
#% 448204
#% 1026180
#! Developing safe and sustainable road systems is a common goal in all countries. Applications to assist with road asset management and crash minimization are sought universally. This paper presents a data mining methodology using decision trees for modeling the crash proneness of road segments using available road and crash attributes. The models quantify the concept of crash proneness and demonstrate that road segments with only a few crashes have more in common with non-crash roads than roads with higher crash counts. This paper also examines ways of dealing with highly unbalanced data sets encountered in the study.

#index 1549883
#* Schema matching and mapping: from usage to evaluation
#@ Angela Bonifati;Yannis Velegrakis
#t 2011
#c 8
#% 315025
#% 378409
#% 480134
#% 572314
#% 576100
#% 826032
#% 924747
#% 993981
#% 1044442
#% 1063580
#% 1127370
#% 1127589
#% 1372699
#% 1400778
#% 1688252
#! This tutorial provides an overview of current evaluation techniques for schema matching and mapping tasks and tools, alongside existing and broadly used evaluation scenarios. The objective is to introduce the audience into the area of matching and mapping system evaluation, and to highlight the need for leveraging robust benchmarks and yardsticks for the comparison of the different matching and mapping tasks. Open research problems will be identified and presented. The tutorial is for both experienced researchers and unfamiliar investigators looking for a quick and complete introduction to the topic.

#index 1549884
#* Big data and cloud computing: current state and future opportunities
#@ Divyakant Agrawal;Sudipto Das;Amr El Abbadi
#t 2011
#c 8
#% 287351
#% 442700
#% 963669
#% 998845
#% 1002142
#% 1022200
#% 1063488
#% 1127560
#% 1217159
#% 1217160
#% 1217216
#% 1217217
#% 1328066
#% 1328095
#% 1328130
#% 1328186
#% 1426492
#% 1426585
#% 1468219
#% 1523840
#% 1523962
#% 1726439
#! Scalable database management systems (DBMS)---both for update intensive application workloads as well as decision support systems for descriptive and deep analytics---are a critical part of the cloud infrastructure and play an important role in ensuring the smooth transition of applications from the traditional enterprise infrastructures to next generation cloud infrastructures. Though scalable data management has been a vision for more than three decades and much research has focussed on large scale data management in traditional enterprise setting, cloud computing brings its own set of novel challenges that must be addressed to ensure the success of data management solutions in the cloud environment. This tutorial presents an organized picture of the challenges faced by application developers and DBMS designers in developing and deploying internet scale applications. Our background study encompasses both classes of systems: (i) for supporting update heavy applications, and (ii) for ad-hoc analytics and decision support. We then focus on providing an in-depth analysis of systems for supporting update intensive web-applications and provide a survey of the state-of-the-art in this domain. We crystallize the design choices made by some successful systems large scale database management systems, analyze the application demands and access patterns, and enumerate the desiderata for a cloud-bound DBMS.

#index 1549885
#* The hidden web, XML and the Semantic Web: scientific data management perspectives
#@ Fabian M. Suchanek;Aparna S. Varde;Richi Nayak;Pierre Senellart
#t 2011
#c 8
#% 576366
#% 869611
#% 956564
#% 1035793
#% 1131145
#% 1194653
#% 1300591
#% 1409954
#! The World Wide Web no longer consists just of HTML pages. Our work sheds light on a number of trends on the Internet that go beyond simple Web pages. The hidden Web provides a wealth of data in semi-structured form, accessible through Web forms and Web services. These services, as well as numerous other applications on the Web, commonly use XML, the eXtensible Markup Language. XML has become the lingua franca of the Internet that allows customized markups to be defined for specific domains. On top of XML, the Semantic Web grows as a common structured data source. In this work, we first explain each of these developments in detail. Using real-world examples from scientific domains of great interest today, we then demonstrate how these new developments can assist the managing, harvesting, and organization of data on the Web. On the way, we also illustrate the current research avenues in these domains. We believe that this effort would help bridge multiple database tracks, thereby attracting researchers with a view to extend database technology.

#index 1549886
#* A probabilistic XML merging tool
#@ Talel Abdessalem;M. Lamine Ba;Pierre Senellart
#t 2011
#c 8
#% 782819
#% 800547
#% 956520
#% 1291113
#% 1291120
#% 1384859
#% 1523864
#! This demonstration paper presents a probabilistic XML data merging tool, that represents the outcome of semi-structured document integration as a probabilistic tree. The system is fully automated and integrates methods to evaluate the uncertainty (modeled as probability values) of the result of the merge. It is based on the two-way tree-merge technique and an uncertain data model defined using probabilistic event variables. The resulting probabilistic repository can be queried using a subset of the XPath query language. The demonstration application is based on revisions of the Wikipedia encyclopedia: a Wikipedia article is no longer considered as the latest valid revision but as the merge of all possible revisions, some of which are uncertain.

#index 1549887
#* Taking the OXPath down the deep web
#@ Andrew Sellers;Tim Furche;Georg Gottlob;Giovanni Grasso;Christian Schallhart
#t 2011
#c 8
#% 340295
#% 480648
#% 511665
#% 850728
#% 1022288
#% 1127582
#! Although deep web analysis has been studied extensively, there is no succinct formalism to describe user interactions with AJAX-enabled web applications. Toward this end, we introduce OXPath as a superset of XPath 1.0. Beyond XPath, OXPath is able (1) to fill web forms and trigger DOM events, (2) to access dynamically computed CSS attributes, (3) to navigate between visible form fields, and (4) to mark relevant information for extraction. This way, OXPath expressions can closely simulate the human interaction relevant for navigation rather than rely exclusively on the HTML structure. Thus, they are quite resilient against technical changes. We demonstrate the expressiveness and practical efficacy of OXPath to tackle a group flight planning problem. We use the OXPath implementation and visual interface to access the popular, highly-scripted travel site Kayak. We show, how to formulate OXPath expressions to extract all booking information with just a few lines of code.

#index 1549888
#* SPRINT: ranking search results by paths
#@ Christoph Böhm;Eyk Kny;Benjamin Emde;Ziawasch Abedjan;Felix Naumann
#t 2011
#c 8
#% 548463
#% 956564
#% 1019117
#% 1130956
#% 1292553
#% 1292748
#! Graph-structured data abounds and has become the subject of much attention in the past years, for instance when searching and analyzing social network structures. Measures such as the shortest path or the number of paths between two nodes are used as proxies for similarity or relevance[1]. These approaches benefit from the fact that the measures are determined from some context node, e.g., "me" in a social network. With Sprint, we apply these notions to a new domain, namely ranking web search results using the link-path-structure among pages. Sprint demonstrates the feasibility and effectiveness of Searching by Path Ranks on the INTernet with two use cases: First, we re-rank intranet search results based on the position of the user's homepage on the graph. Second, as a live proof-of-concept we dynamically re-rank Wikipedia search results based on the currently viewed page: When viewing the Java software page, a search for "Sun" ranks Sun Microsystems higher than the star at the center of our solar system. We evaluate the first use case with a user study. The second use case is the focus of the demonstration and allows users to actively test our system with any combination of context page and search term.

#index 1549889
#* A query optimization assistant for XPath
#@ Haris Georgiadis;Minas Charalambidis;Vasilis Vassalos
#t 2011
#c 8
#% 397375
#% 654514
#% 810036
#% 824667
#% 960260
#% 994015
#% 1015298
#% 1217192
#% 1372696
#! We demonstrate a generic and extensible cost-based optimization and execution system for XPath queries, named GeCOEX, using a comprehensive suite of query analyzing and administrative tools, named QuOAX. GeCOEX supports many different physical operator implementations and XML storage engines and is agnostic to the underlying physical data model. Its optimizer is the first generic cost-based optimizer for XPath queries that always picks the cheapest estimated plan, among a very large number of possible plans, for a wide range of XPath queries and different datasets in a very small fraction of the time required for efficient execution. The QuOAX suite provides administration tools that allow the user to add new -- or deactivate already deployed -- physical operator implementations, physical operator cost models and rewriting rules and also to make use of different XML storage and XML statistics estimators. QuOAX also provides query plan analysis and visualization tools that allow users to visualize the physical plan chosen by the optimizer or all possible generated physical plans for a given query and to execute any of those plans. QuOAX helps users to i) easily test new XPath processing techniques, comparing them directly with existing ones and identifying the situations to which they show promise, ii) improve the effectiveness of the optimizer and iii) find out the appropriate access methods or indices that are beneficial for a specific workload.

#index 1549890
#* TPM: supporting pattern matching queries for road-network trajectory data
#@ Gook-Pil Roh;Seung-won Hwang
#t 2011
#c 8
#% 86950
#% 342828
#% 479462
#% 481460
#% 800572
#% 810048
#% 824729
#% 1063471
#% 1633139
#! With the advent of ubiquitous computing, we can easily collect large scale trajectory data from moving vehicles. This paper presents TPM (Trajectory Pattern Miner), a software aimed at pattern matching queries for road-network trajectory data, which complements existing efforts focusing on (a) a spatio-temporal window query for location-based service or (b) Euclidean space with no restriction. To overcome limitations of prior research, TPM supports three types of pattern matching queries-- whole, subpattern, and reverse sub-pattern matching for road-network trajectories. We demonstrate application scenarios for each type of pattern matching queries using large-scale real-life trajectory data.

#index 1549891
#* QueryViz: helping users understand SQL queries and their patterns
#@ Jonathan Danaparamita;Wolfgang Gatterbauer
#t 2011
#c 8
#% 58377
#% 152949
#% 183418
#% 317981
#% 481447
#% 654440
#% 960234
#% 1102246
#% 1180960
#% 1218714
#% 1305906
#% 1488676
#% 1523949
#! We present QueryViz, a novel visualization tool for SQL queries that reduces the time needed to read and understand existing queries. It targets two principal audiences: (i) users who often issue the same or similar queries and who need to quickly browse through a repository of existing queries; and (ii) novices that try to familiarize themselves with the logic behind alternative patterns of SQL queries. QueryViz uses as input only two strings: the database schema and the SQL query. It can thus serve as light-weight add-on to existing database systems and is also available via an online interface at http://queryviz.com. In this demonstration, we explain our visual alphabet, walk through the visualization algorithm, and let users experience the difference in understanding SQL queries from text or our graphical representation while browsing through repositories of well-known textbook SQL queries.

#index 1549892
#* True language-level SQL debugging
#@ Torsten Grust;Fabian Kliebhan;Jan Rittinger;Tom Schreiber
#t 2011
#c 8
#% 32878
#% 287005
#% 1022307
#% 1217244
#! We demonstrate Habitat, a declarative observational debugger for SQL. Habitat facilitates true language-level (not: plan-level) debugging of, probably flawed, SQL queries that yield unexpected results. Users may mark arbitrary SQL subexpressions---ranging from literals, over fragments of predicates, to entire subquery blocks---to observe whether these evaluate as expected. From the marked SQL text, Habitat's algebraic compiler derives a new query whose result represents the values of the desired observations. These observations are generated by the target SQL database host itself. Prior data extraction or extra debugging middleware is not required. Habitat merges multiple observations into a single (nested) tabular display, letting a user explore the relationship of various observations. Filter predicates furthermore ease the interpretation of large results.

#index 1549893
#* SITAC: discovering semantically identical temporally altering concepts in text archives
#@ Amal Kaluarachchi;Debjani Roychoudhury;Aparna S. Varde;Gerhard Weikum
#t 2011
#c 8
#% 280488
#% 287242
#% 577273
#% 938705
#% 1482436
#! This paper demonstrates a system called SITAC based on our proposed approach to automate the discovery of concepts (called SITACs) in text sources that are identical semantically but alter their names over time. This system is developed to perform time-aware translation of queries over text corpora by incorporating terminology evolution, thus providing more accurate responses to users, e.g., query processing on Mumbai should automatically take into account its former name Bombay. The SITAC system constitutes a novel collaborative framework of natural language processing, association rule mining and contextual similarity.

#index 1549894
#* Unraveling multi-dimensional data using pDView
#@ Luigi Di Caro;Maria Luisa Sapino;K. Selçuk Candan
#t 2011
#c 8
#% 20855
#% 319273
#% 775965
#% 789224
#% 881050
#% 907535
#% 1717030
#! We present the pattern development view (pDView) system for multidimensional scientific data visualization. The pDView system relies on a novel pattern development tree (pDTree) structure to unravel patterns in multidimensional data without having to rely on visualizations that require either significant degrees of projections that eliminate certain dimensions at the expense of the others or introduce significant visual overhead due to overly-rich multi-dimensional graphic interfaces. Instead, pDView maps data along all its relevant dimensions onto a pDTree structure, capturing and visualizing the underlying fundamental relationships. The user is able to vary contextual parameters to observe the strength and robustness of these relationships under different situations.

#index 1549895
#* RanKloud: a scalable ranked query processing framework on hadoop
#@ K. Selçuk Candan;Parth Nagarkar;Mithila Nagendra;Renwei Yu
#t 2011
#c 8
#% 300162
#% 643566
#% 963669
#% 1023422
#% 1063486
#% 1063553
#% 1127559
#% 1215321
#% 1217159
#% 1227596
#! The popularity of batch-oriented cluster architectures like Hadoop is on the rise. These batch-based systems successfully achieve high degrees of scalability by carefully allocating resources and leveraging opportunities to parallelize basic processing tasks. However, they are known to fall short in certain application domains such as large scale media analysis. In these applications, the utility of a given data element plays a vital role in a particular analysis task, and this utility most often depends on the way the data is collected or interpreted. However, existing batch data processing frameworks do not consider data utility in allocating resources, and hence fail to optimize for ranked/top-k query processing in which the user is interested in obtaining a relatively small subset of the best result instances. A naïve implementation of these operations on an existing system would need to enumerate more candidates than needed, before it can filter out the k best results. We note that such waste can be avoided by utilizing utility-aware task partitioning and resource allocation strategies that can prune unpromising objects from consideration. In this demonstration, we introduce RanKloud, an efficient and scalable utility-aware parallel processing system built for the analysis of large media datasets. RanKloud extends Hadoop's MapReduce paradigm to provide support for ranked query operations, such as k-nearest neighbor and k-closest pair search, skylines, skyline-joins, and top-k join processing.

#index 1798373
#* Proceedings of the 15th International Conference on Extending Database Technology
#@ Elke Rundensteiner;Volker Markl;Ioana Manolescu;Sihem Amer-Yahia;Felix Naumann;Ismail Ari
#t 2012
#c 8
#! Welcome to the 2012 Edition of the International Conference on Extending Database Technology (EDBT). This year, EDBT is taking place in Berlin, Germany on March 26-30, 2012. The International Conference on Extending Database Technology is known as a leading international forum for database researchers, practitioners, developers, and users to discuss cutting-edge ideas, and to exchange techniques, tools, and experiences related to data management. Data management constitutes the essential enabling technology for scientific, engineering, business, and social communities; and its technology is driven by the needs in applications ranging from the social web, scientific discoveries, virtual libraries, to embedded systems. We are very pleased to offer you an exciting program on these important and timely topics. This year, as in prior years, EDBT is co-located with the International Conference on Database Theory (ICDT).

#index 1798374
#* Towards an ecosystem of structured data on the web
#@ Alon Y. Halevy
#t 2012
#c 8
#% 756964
#% 1127393
#% 1426594
#% 1592311
#! We are in the midst of very exciting times in which structured data is having a profound impact on many aspects of our lives. In many countries, citizens take for granted the fact that governments, local authorities, and non-government organizations should make a variety of data sets available to the public. These data sets span a variety of topics such as economic indicators, crime statistics, educational data, government spending and campaign contributions. Journalists and other data aficionados are fueling this trend by turning this data into visualizations and stories that are spread by social networks and seen by millions of people [8]. These visualizations, stories and public attention, in turn, lead to new questions and hence a demand for additional data.

#index 1798375
#* Inside "Big Data management": ogres, onions, or parfaits?
#@ Vinayak Borkar;Michael J. Carey;Chen Li
#t 2012
#c 8
#% 2853
#% 115661
#% 188719
#% 208047
#% 235914
#% 319473
#% 393844
#% 442850
#% 466944
#% 479905
#% 479920
#% 723279
#% 800491
#% 963669
#% 983467
#% 998845
#% 1018819
#% 1054227
#% 1063553
#% 1127559
#% 1217159
#% 1278123
#% 1278124
#% 1328060
#% 1426486
#% 1426513
#% 1426543
#% 1426584
#% 1468421
#% 1573340
#% 1594630
#% 1871198
#! In this paper we review the history of systems for managing "Big Data" as well as today's activities and architectures from the (perhaps biased) perspective of three "database guys" who have been watching this space for a number of years and are currently working together on "Big Data" problems. Our focus is on architectural issues, and particularly on the components and layers that have been developed recently (in open source and elsewhere) and on how they are being used (or abused) to tackle challenges posed by today's notion of "Big Data". Also covered is the approach we are taking in the ASTERIX project at UC Irvine, where we are developing our own set of answers to the questions of the "right" components and the "right" set of layers for taming the "Big Data" beast. We close by sharing our opinions on what some of the important open questions are in this area as well as our thoughts on how the dataintensive computing community might best seek out answers.

#index 1798376
#* Clydesdale: structured data processing on MapReduce
#@ Tim Kaldewey;Eugene J. Shekita;Sandeep Tata
#t 2012
#c 8
#% 442850
#% 479821
#% 480821
#% 824697
#% 960326
#% 1022298
#% 1023420
#% 1044448
#% 1063542
#% 1127559
#% 1206647
#% 1217159
#% 1217169
#% 1217232
#% 1328060
#% 1328186
#% 1372690
#% 1426486
#% 1426584
#% 1523837
#% 1523841
#% 1523924
#% 1566972
#% 1567923
#% 1573238
#% 1581407
#% 1581926
#% 1594639
#! MapReduce has emerged as a promising architecture for large scale data analytics on commodity clusters. The rapid adoption of Hive, a SQL-like data processing language on Hadoop (an open source implementation of MapReduce), shows the increasing importance of processing structured data on MapReduce platforms. MapReduce offers several attractive properties such as the use of low-cost hardware, fault-tolerance, scalability, and elasticity. However, these advantages have required a substantial performance sacrifice. In this paper we introduce Clydesdale, a novel system for structured data processing on Hadoop -- a popular implementation of MapReduce. We show that Clydesdale provides more than an order of magnitude in performance improvements compared to existing approaches without requiring any changes to the underlying platform. Clydesdale is aimed at workloads where the data fits a star schema. It draws on column oriented storage, tailored join-plans, and multi-core execution strategies and carefully fits them into the constraints of a typical MapReduce platform. Using the star schema benchmark, we show that Clydesdale is on average 38x faster than Hive. This demonstrates that MapReduce in general, and Hadoop in particular, is a far more compelling platform for structured data processing than previous results suggest.

#index 1798377
#* An optimization framework for map-reduce queries
#@ Leonidas Fegaras;Chengkai Li;Upa Gupta
#t 2012
#c 8
#% 562668
#% 954300
#% 960326
#% 963669
#% 983467
#% 997039
#% 1063553
#% 1127559
#% 1217232
#% 1278123
#% 1328060
#% 1328095
#% 1354118
#% 1426486
#% 1426488
#% 1467704
#% 1471595
#% 1523820
#% 1523841
#% 1566972
#% 1573238
#% 1594630
#! We present an effective optimization framework for general SQL-like map-reduce queries, which is based on a novel query algebra and uses a small number of higher-order physical operators that are directly implementable on existing map-reduce systems, such as Hadoop. Although our framework is applicable to any SQL-like map-reduce query language, we focus on a powerful query language, called MRQL. Current map-reduce query languages, such as HiveQL and PigLatin, enable users to plug-in custom map-reduce scripts into queries for those jobs that cannot be declaratively coded in the query language, which may result to suboptimal, error-prone, and hard-to-maintain code. In contrast to these languages, MRQL is expressive enough to capture most of these computations in declarative form and at the same time is amenable to optimization. We describe an optimization framework that maps the algebraic forms derived from the MRQL queries to efficient workflows of map-reduce operations that consist of our physical plan operators. We also describe many algebraic optimizations, such as fusing cascading map-reduce jobs into one job and synthesizing a combine function from the reduce function of a map-reduce job. Finally, we report on a prototype system implementation and we show some performance results of evaluating MRQL queries on a small cluster of computers.

#index 1798378
#* Efficient parallel kNN joins for large data in MapReduce
#@ Chi Zhang;Feifei Li;Jeffrey Jestes
#t 2012
#c 8
#% 58352
#% 316942
#% 421050
#% 464205
#% 480596
#% 587697
#% 783643
#% 942985
#% 943214
#% 963669
#% 973024
#% 1016192
#% 1282271
#% 1350296
#% 1426543
#% 1426551
#% 1523902
#% 1532900
#% 1549895
#% 1581925
#% 1581926
#% 1910177
#! In data mining applications and spatial and multimedia databases, a useful tool is the kNN join, which is to produce the k nearest neighbors (NN), from a dataset S, of every point in a dataset R. Since it involves both the join and the NN search, performing kNN joins efficiently is a challenging task. Meanwhile, applications continue to witness a quick (exponential in some cases) increase in the amount of data to be processed. A popular model nowadays for large-scale data processing is the shared-nothing cluster on a number of commodity machines using MapReduce [6]. Hence, how to execute kNN joins efficiently on large data that are stored in a MapReduce cluster is an intriguing problem that meets many practical needs. This work proposes novel (exact and approximate) algorithms in MapReduce to perform efficient parallel kNN joins on large data. We demonstrate our ideas using Hadoop. Extensive experiments in large real and synthetic datasets, with tens or hundreds of millions of records in both R and S and up to 30 dimensions, have demonstrated the efficiency, effectiveness, and scalability of our methods.

#index 1798379
#* Differentially private search log sanitization with optimal output utility
#@ Yuan Hong;Jaideep Vaidya;Haibing Lu;Mingrui Wu
#t 2012
#c 8
#% 25998
#% 800515
#% 864406
#% 956557
#% 985041
#% 1019163
#% 1089472
#% 1190072
#% 1198224
#% 1206678
#% 1214684
#% 1292623
#% 1318624
#% 1328187
#% 1535414
#% 1595893
#% 1598371
#% 1732708
#% 1740518
#! Web search logs contain extremely sensitive data, as evidenced by the recent AOL incident. However, storing and analyzing search logs can be very useful for many purposes (i.e. investigating human behavior). Thus, an important research question is how to privately sanitize search logs. Several search log anonymization techniques have been proposed with concrete privacy models. However, in all of these solutions, the output utility of the techniques is only evaluated rather than being maximized in any fashion. Indeed, for effective search log anonymization, it is desirable to derive the outputs with optimal utility while meeting the privacy standard. In this paper, we propose utility-maximizing sanitization based on the rigorous privacy standard of differential privacy, in the context of search logs. Specifically, we utilize optimization models to maximize the output utility of the sanitization for different applications, while ensuring that the production process satisfies differential privacy. An added benefit is that our novel randomization strategy maintains the schema integrity in the output search logs. A comprehensive evaluation on real search logs validates the approach and demonstrates its robustness and scalability.

#index 1798380
#* Integrating historical noisy answers for improving data utility under differential privacy
#@ Shixi Chen;Shuigeng Zhou;Sourav S. Bhowmick
#t 2012
#c 8
#% 115608
#% 211044
#% 464434
#% 739899
#% 810028
#% 874891
#% 963241
#% 977011
#% 1029084
#% 1061644
#% 1198227
#% 1426322
#% 1426328
#% 1426454
#% 1426455
#% 1521654
#% 1523886
#% 1581864
#% 1581865
#% 1732708
#% 1740518
#! Differential privacy is a robust principle for privacy preserving data analysis tasks, and has been successfully applied to a variety of applications. However, the number of queries that can be answered is limited for preventing privacy disclosure. Once the privacy budget is exhausted, all succeeding queries must be rejected. Therefore, each of the historical query answers is valuable and it is important to exploit them together to learn more about the data. We propose to integrate all available linear query answers into a consistent form that embodies our knowledge learned from the noisy answers, obtaining more accurate answers to past queries and even new queries, improving the data utility. Two distinct approaches are developed for this purpose, one via principle component analysis, and another via maximum entropy method. The second approach also generates a synthetic database, which is useful for differentially private data publishing. One important goal of our work is to ensure that the running time of our approaches does not grow with the cardinality of the universe of a data tuple, so that high-dimensional data with very large domain can still be tackled efficiently.

#index 1798381
#* Mining probabilistically frequent sequential patterns in uncertain databases
#@ Zhou Zhao;Da Yan;Wilfred Ng
#t 2012
#c 8
#% 310559
#% 329537
#% 341704
#% 397383
#% 459006
#% 463903
#% 464996
#% 893189
#% 989604
#% 1016178
#% 1063531
#% 1189215
#% 1206706
#% 1214624
#% 1214633
#% 1393138
#% 1426506
#% 1426529
#% 1451166
#% 1523853
#% 1592778
#% 1607952
#! Data uncertainty is inherent in many real-world applications such as environmental surveillance and mobile tracking. As a result, mining sequential patterns from inaccurate data, such as sensor readings and GPS trajectories, is important for discovering hidden knowledge in such applications. Previous work uses expected support as the measurement of pattern frequentness, which has inherent weaknesses with respect to the underlying probability model, and is therefore ineffective for mining high-quality sequential patterns from uncertain sequence databases. In this paper, we propose to measure pattern frequentness based on the possible world semantics. We establish two uncertain sequence data models abstracted from many real-life applications involving uncertain sequence data, and formulate the problem of mining probabilistically frequent sequential patterns (or p-FSPs) from data that conform to our models. Based on the prefix-projection strategy of the famous PrefixSpan algorithm, we develop two new algorithms, collectively called U-PrefixSpan, for p-FSP mining. U-PrefixSpan effectively avoids the problem of "possible world explosion", and when combined with our three pruning techniques and one validating technique, achieves good performance. The efficiency and effectiveness of U-PrefixSpan are verified through extensive experiments on both real and synthetic datasets.

#index 1798382
#* RecStore: an extensible and adaptive framework for online recommender queries inside the database engine
#@ Justin J. Levandoski;Mohamed Sarwat;Mohamed F. Mokbel;Michael D. Ekstrand
#t 2012
#c 8
#% 173879
#% 220706
#% 220711
#% 279164
#% 280852
#% 330687
#% 342687
#% 414514
#% 428272
#% 452563
#% 734590
#% 767656
#% 801785
#% 813966
#% 824789
#% 860673
#% 956521
#% 987671
#% 1016161
#% 1055785
#% 1217203
#% 1650399
#% 1650569
#! Most recommendation methods (e.g., collaborative filtering) consist of (1) a computationally intense offline phase that computes a recommender model based on users' opinions of items, and (2) an online phase consisting of SQL-based queries that use the model (generated offline) to derive user preferences and provide recommendations for interesting items. Current application usage trends require a completely online recommender process, meaning the recommender model must update in real time as new opinions enter the system. To tackle this problem, we propose RecStore, a DBMS storage engine module capable of efficient online model maintenance. Externally, models managed by RecStore behave as relational tables, thus existing SQL-based recommendation queries remain unchanged while gaining online model support. RecStore maintains internal statistics and data structures aimed at providing efficient incremental updates to the recommender model, while employing an adaptive strategy for internal maintenance and load shedding to realize a balance between efficiency in updates or query processing based on system workloads. RecStore is also extensible, supporting a declarative syntax for defining recommender models. The efficacy of RecStore is demonstrated by providing the implementation details of three state-of-the-art collaborative filtering models. We provide an extensive experimental evaluation of a prototype of RecStore, built inside the storage engine of PostgreSQL, using a real-life recommender system workload.

#index 1798383
#* Supporting top-K item exchange recommendations in large online communities
#@ Zhan Su;Anthony K. H. Tung;Zhenjie Zhang
#t 2012
#c 8
#% 341704
#% 347190
#% 460797
#% 942495
#% 963362
#% 1181318
#% 1207006
#! Item exchange is becoming a popular behavior and widely supported in more and more online community systems, e. g. online games and social network web sites. Traditional manual search for possible exchange pairs is neither efficient nor effective. Automatic exchange pairing is increasingly demanding in such community systems, and potentially leading to new business opportunities. To meet the needs on item exchange in the market, each user in the system is entitled to list some items he/she no longer needs, as well as some required items he/she is seeking for. Given the values of all items, an exchange between two users is eligible if 1) they both have some unneeded items the other one wants, and 2) the exchange items from both sides are approximately of the same total value. To efficiently support exchange recommendation services, especially with frequent updates on the listed items, new data structures are proposed in this paper to maintain promising exchange pairs for each user. Extensive experiments on both synthetic and real data sets are conducted to evaluate our proposed solutions.

#index 1798384
#* Limiting link disclosure in social network analysis through subgraph-wise perturbation
#@ Amin Milani Fard;Ke Wang;Philip S. Yu
#t 2012
#c 8
#% 274612
#% 576111
#% 810028
#% 956511
#% 1063476
#% 1074831
#% 1190108
#% 1195950
#% 1206763
#% 1281958
#% 1328188
#% 1415851
#% 1426540
#% 1467722
#% 1523849
#% 1594593
#! Link disclosure between two individuals in a social network could be a privacy breach. To limit link disclosure, previous works modeled a social network as an undirected graph and randomized a link over the entire domain of links, which leads to considerable structural distortion to the graph. In this work, we address this issue in two steps. First, we model a social network as a directed graph and randomize the destination of a link while keeping the source of a link intact. The randomization ensures that, if the prior belief about the destination of a link is bounded by some threshold, the posterior belief, given the published graph, is no more than another threshold. Then, we further reduce structural distortion by a subgraph-wise perturbation in which the given graph is partitioned into several subgraphs and randomization of destination nodes is performed within each subgraph. The benefit of subgraph-wise perturbation is that it retains a destination node with a higher retention probability and replaces a destination node with a node from a local neighborhood. We study the trade-off of utility and privacy of subgraph-wise perturbation.

#index 1798385
#* On optimizing relational self-joins
#@ Yu Cao;Yongluan Zhou;Chee-Yong Chan;Kian-Lee Tan
#t 2012
#c 8
#% 18614
#% 136740
#% 273682
#% 279585
#% 427195
#% 442965
#% 479460
#% 479654
#% 479905
#% 480272
#% 480774
#% 1206586
#! Self-join, which joins a relation with itself, is a prevalent operation in relational database systems. Despite its wide applicability, there has been little attention devoted to improving its performance. In this paper, we present SCALE (Sort for Clustered Access with Lazy Evaluation), an efficient self-join algorithm, which takes advantage of the fact that both inputs of a self-join operation are instances of the same relation. SCALE first sorts the relation on one join attribute, say R. A. In this way, for every value of the other join attribute, say R. B, its matching R. A tuples are essentially clustered. As SCALE scans the sorted relation, each tuple is joined with its matching tuples co-existing in memory. For tuples where full-range clustered accesses to their matching tuples are not possible, they are buffered and the unfinished part of join processing deferred. Such lazy evaluation minimizes the need for "random" access to the matching tuples. SCALE further optimizes the memory allocation for clustered access and lazy evaluation to keep the processing cost minimal. Our analytical study shows that SCALE degenerates gracefully to a Sort-Merge Join in the worst case. We have also implemented SCALE in PostgreSQL, and results of our extensive experimental study show that it outperforms both Sort-Merge Join and Hybrid Hash Join by a wide margin in (almost) all cases.

#index 1798386
#* Transitive closure and recursive Datalog implemented on clusters
#@ Foto N. Afrati;Jeffrey D. Ullman
#t 2012
#c 8
#% 23895
#% 32905
#% 86080
#% 122396
#% 153640
#% 164370
#% 194120
#% 273695
#% 309749
#% 368248
#% 479933
#% 722530
#% 723279
#% 864462
#% 983467
#% 1023420
#% 1068741
#% 1127354
#% 1218741
#% 1386046
#% 1426486
#% 1426513
#% 1468421
#% 1523819
#% 1523820
#% 1531325
#% 1549835
#% 1594630
#% 1688299
#% 1693954
#! Implementing recursive algorithms on computing clusters presents a number of new challenges. In particular, we consider the endgame problem: later rounds of a recursion often transfer only small amounts of data, causing high overhead for interprocessor communication. One way to deal with the endgame problem is to use an algorithm that reduces the number of rounds of the recursion. Especially, in an application like transitive closure ("TC") there are several recursive-doubling algorithms that use a logarithmic, rather than linear, number of rounds. Unfortunately, recursive-doubling algorithms can deduce many more facts than the linear TC algorithms, which could negate the cost savings from the elimination of the overhead due to the proliferation of small files. We are thus led to consider TC algorithms that, like the linear algorithms, have the unique decomposition property that assures paths are discovered only once. We find that many such algorithms exist, and we show that they are incomparable, in that any of them could prove best on some data --- even lower in cost than the linear algorithms in some cases. The recursive-doubling approach to TC extends to other recursions as well. However, it is not acceptable to reduce the number of rounds at the expense of a major increase in the number of facts that are deduced. In this paper, we prove it is possible to implement any Datalog program of right-linear chain rules with a logarithmic number of rounds and no order-of-magnitude increase in the number of facts deduced. On the other hand, there are linear recursions for which the two goals of reducing the number of rounds and maintaining the total number of deduced facts cannot be met simultaneously. We show that the reachability problem cannot be solved in logarithmic rounds without using a binary predicate, thus squaring the number of potential facts to be deduced. We also show that the samegeneration recursion cannot be solved in logarithmic rounds without using a predicate of arity three.

#index 1798387
#* Shortest-path queries for complex networks: exploiting low tree-width outside the core
#@ Takuya Akiba;Christian Sommer;Ken-ichi Kawarabayashi
#t 2012
#c 8
#% 56706
#% 98188
#% 214769
#% 300079
#% 379482
#% 725363
#% 729923
#% 754117
#% 785120
#% 787540
#% 793252
#% 813718
#% 813733
#% 818476
#% 823342
#% 833631
#% 875476
#% 881460
#% 881491
#% 906306
#% 937549
#% 960259
#% 983902
#% 1002007
#% 1019117
#% 1086063
#% 1096057
#% 1127407
#% 1130956
#% 1181254
#% 1181255
#% 1206910
#% 1230568
#% 1292553
#% 1355056
#% 1399997
#% 1412885
#% 1426510
#% 1431694
#% 1475163
#% 1482228
#% 1560413
#% 1574683
#% 1611296
#! We present new and improved methods for efficient shortest-path query processing. Our methods are tailored to work for two specific classes of graphs: graphs with small tree-width and complex networks. Seemingly unrelated at first glance, these two classes of graphs have some commonalities: complex networks are known to have a core--fringe structure with a dense core and a tree-like fringe. Our main contributions are efficient algorithms and data structures on three different levels. First, we provide two new methods for graphs with small but not necessarily constant tree-width. Our methods achieve new tradeoffs between space and query time. Second, we present an improved tree-decomposition-based method for complex networks, utilizing the methods for graphs with small tree-width. Third, we extend our method to handle the highly inter-connected core with existing exact and approximate methods. We evaluate our algorithms both analytically and experimentally. We prove that our algorithms for low-tree-width graphs achieve improved tradeoffs between space and query time. Our experiments on several real-world complex networks further confirm the efficiency of our methods: Both the exact and the hybrid method have faster preprocessing and query times than existing methods. The hybrid method in particular provides an improved tradeoff between space and accuracy.

#index 1798388
#* User oriented trajectory search for trip recommendation
#@ Shuo Shang;Ruogu Ding;Bo Yuan;Kexin Xie;Kai Zheng;Panos Kalnis
#t 2012
#c 8
#% 46803
#% 172949
#% 376266
#% 427199
#% 453538
#% 460862
#% 462231
#% 631923
#% 659971
#% 765451
#% 810049
#% 814646
#% 824722
#% 838407
#% 885388
#% 960281
#% 982560
#% 1016195
#% 1022268
#% 1127422
#% 1206801
#% 1206997
#% 1328073
#% 1328137
#% 1426523
#% 1523828
#% 1581875
#% 1581877
#% 1594674
#! Trajectory sharing and searching have received significant attentions in recent years. In this paper, we propose and investigate a novel problem called User Oriented Trajectory Search (UOTS) for trip recommendation. In contrast to conventional trajectory search by locations (spatial domain only), we consider both spatial and textual domains in the new UOTS query. Given a trajectory data set, the query input contains a set of intended places given by the traveler and a set of textual attributes describing the traveler's preference. If a trajectory is connecting/close to the specified query locations, and the textual attributes of the trajectory are similar to the traveler'e preference, it will be recommended to the traveler for reference. This type of queries can bring significant benefits to travelers in many popular applications such as trip planning and recommendation. There are two challenges in the UOTS problem, (i) how to constrain the searching range in two domains and (ii) how to schedule multiple query sources effectively. To overcome the challenges and answer the UOTS query efficiently, a novel collaborative searching approach is developed. Conceptually, the UOTS query processing is conducted in the spatial and textual domains alternately. A pair of upper and lower bounds are devised to constrain the searching range in two domains. In the meantime, a heuristic searching strategy based on priority ranking is adopted for scheduling the multiple query sources, which can further reduce the searching range and enhance the query efficiency notably. Furthermore, the devised collaborative searching approach can be extended to situations where the query locations are ordered. The performance of the proposed UOTS query is verified by extensive experiments based on real and synthetic trajectory data in road networks.

#index 1798389
#* Top-k spatial keyword queries on road networks
#@ João B. Rocha-Junior;Kjetil Nørvåg
#t 2012
#c 8
#% 46803
#% 340886
#% 443208
#% 527189
#% 867054
#% 1015321
#% 1077150
#% 1127445
#% 1181299
#% 1206801
#% 1206997
#% 1328137
#% 1555383
#% 1581877
#% 1594674
#% 1618262
#% 1692266
#! With the popularization of GPS-enabled devices there is an increasing interest for location-based queries. In this context, one interesting problem is processing top-k spatial keyword queries. Given a set of objects with a textual description (e.g., menu of a restaurant), a query location (latitude and longitude), and a set of query keywords, a top-k spatial keyword query returns the k best objects ranked in terms of both distance to the query location and textual relevance to the query keywords. So far, the research on this problem has assumed Euclidean space. In order to process such queries efficiently, spatio-textual indexes combining R-trees and inverted files are employed. However, for most real applications, the distance between the objects and query location is constrained by a road network (shortest path) and cannot be computed efficiently using R-trees. In this paper, we address, for the first time, the challenging problem of processing top-k spatial keyword queries on road networks where the distance between the query location and the spatial object is the shortest path. We formalize the new query type, and present novel indexing structures and algorithms that are able to process such queries efficiently. Finally, we perform an experimental evaluation that shows the efficiency of our approach.

#index 1798390
#* Relevance search in heterogeneous networks
#@ Chuan Shi;Xiangnan Kong;Philip S. Yu;Sihong Xie;Bin Wu
#t 2012
#c 8
#% 220711
#% 313959
#% 462239
#% 577273
#% 577329
#% 805896
#% 915344
#% 989654
#% 1016176
#% 1016199
#% 1055877
#% 1127384
#% 1181261
#% 1206821
#% 1372721
#% 1451228
#% 1457044
#% 1482198
#% 1495579
#% 1606031
#! Conventional research on similarity search focuses on measuring the similarity between objects with the same type. However, in many real-world applications, we need to measure the relatedness between objects with different types. For example, in automatic expert profiling, people are interested in finding the most relevant objects to an expert, where the objects can be of various types, such as research areas, conferences and papers, etc. With the surge of study on heterogeneous networks, the relatedness measure on objects with different types becomes increasingly important. In this paper, we study the relevance search problem in heterogeneous networks, where the task is to measure the relatedness of heterogeneous objects (including objects with the same type or different types). We propose a novel measure, called HeteSim, with the following attributes: (1) a path-constrained measure: the relatedness of object pairs are defined based on the search path that connect two objects through following a sequence of node types; (2) a uniform measure: it can measure the relatedness of objects with the same or different types in a uniform framework; (3) a semi-metric measure: HeteSim has some good properties (e.g., self-maximum and symmetric), that are crucial to many tasks. Empirical studies show that HeteSim can effectively evaluate the relatedness of heterogeneous objects. Moreover, in the query and clustering tasks, it can achieve better performances than conventional measures.

#index 1798391
#* An adaptive algorithm for online time series segmentation with error bound guarantee
#@ Zhenghua Xu;Rui Zhang;Ramamohanarao Kotagiri;Udaya Parampalli
#t 2012
#c 8
#% 38697
#% 248798
#% 310488
#% 319524
#% 326303
#% 330932
#% 378388
#% 397389
#% 461885
#% 466506
#% 549273
#% 578400
#% 617886
#% 662750
#% 713860
#% 765445
#% 800574
#% 801684
#% 960255
#% 979303
#% 993961
#% 1022239
#% 1068972
#% 1147658
#% 1378172
#% 1513025
#! The volume of time series data grows rapidly in various applications such as network traffic management, telecommunications, finance and sensor network. To reduce the cost of storage, transmission and processing of time series data, the need for more compact representations of time series data is compelling. Segmentation is one of the most commonly used methods to meet this requirement. Both PLA and PPA are common segmentation methods which divide a time series into segments and use a linear function or a polynomial function to approximate each segment, respectively. However, while most of the current PLA and PPA methods aim to minimize the holistic error between the approximation and the original time series, few works try to represent time series as compact as possible with an error bound guarantee on each data point. Furthermore, in many real world situations, the patterns of the time series do not follow a constant rule such that using only one type of functions may not yield the best compaction. Motivated by these observations, we propose an online segmentation algorithm which approximates time series by a set of different types of candidate functions (polynomials of different orders, exponential functions, etc.) and adaptively chooses the most compact one as the pattern of the time series changes. A challenge in this approach is to determine the approximation function on the fly ("online"). Thereby, we further propose a novel method to efficiently generate the compact approximation of a time series in an online fashion for several types of candidate functions. This method incrementally narrows the feasible coefficient spaces of candidate functions in coefficient coordinate systems such that it can make each segment as long as possible given an error bound on each data point. Extensive experimental results show that our algorithm generates more compact approximations of the time series with lower average errors than the state-of-the-art algorithm.

#index 1798392
#* Transactional stream processing
#@ Irina Botan;Peter M. Fischer;Donald Kossmann;Nesime Tatbul
#t 2012
#c 8
#% 116082
#% 135384
#% 198467
#% 201869
#% 279187
#% 336201
#% 533937
#% 800583
#% 810063
#% 864528
#% 878299
#% 1016169
#% 1022208
#% 1181292
#% 1372699
#% 1424590
#% 1426597
#% 1523815
#% 1606345
#% 1688280
#% 1700120
#! Many stream processing applications require access to a multitude of streaming as well as stored data sources. Yet there is no clear semantics for correct continuous query execution over these data sources in the face of concurrent access and failures. Instead, today's Stream Processing Systems (SPSs) hard-code transactional concepts in their execution models, making them both hard to understand and inflexible to use. In this paper, we show that we can successfully reuse the traditional transactional theory (with some minimal extensions) in order to cleanly define the correct interaction of a set of continuous and one-time queries concurrently accessing both streaming and stored data sources. The result is a unified transactional model (UTM) for query processing over streams as well as traditional databases. We present a transaction manager that implements this model on top of an existing storage manager for streams (MXQuery/SMS). Experiments on the Linear Road Benchmark show that our transaction manager flexibly ensures correctness in case of concurrency and failures, without sacrificing from performance. Moreover, this model is powerful enough to express the implicit transactional behaviors of a representative set of state-of-the-art SPSs.

#index 1798393
#* Dynamic diversification of continuous data
#@ Marina Drosou;Evaggelia Pitoura
#t 2012
#c 8
#% 105138
#% 177422
#% 805841
#% 875957
#% 1166473
#% 1181244
#% 1190093
#% 1206662
#% 1269314
#% 1328120
#% 1328135
#% 1472964
#% 1594636
#% 1598392
#% 1641999
#! Result diversification has recently attracted considerable attention as a means of increasing user satisfaction in recommender systems, as well as in web and database search. In this paper, we focus on the problem of selecting the k-most diverse items from a result set. Whereas previous research has mainly considered the static version of the problem, in this paper, we exploit the dynamic case in which the result set changes over time, as for example, in the case of notification services. We define the Continuous k-Diversity Problem along with appropriate constraints that enforce continuity requirements on the diversified results. Our proposed approach is based on cover trees and supports dynamic item insertion and deletion. The diversification problem is in general NP-complete; we provide theoretical bounds that characterize the quality of our solution based on cover trees with respect to the optimal solution. Finally, we report experimental results concerning the efficiency and effectiveness of our approach on a variety of real and synthetic datasets.

#index 1798394
#* Towards "intelligent compression" in streams: a biased reservoir sampling based Bloom filter approach
#@ Sourav Dutta;Souvik Bhattacherjee;Ankur Narang
#t 2012
#c 8
#% 1331
#% 2833
#% 214073
#% 248812
#% 283822
#% 307424
#% 322884
#% 333926
#% 340179
#% 345087
#% 379444
#% 387508
#% 411437
#% 424292
#% 459945
#% 480805
#% 625402
#% 646223
#% 654461
#% 729913
#% 730067
#% 805457
#% 805840
#% 810044
#% 874972
#% 893138
#% 918001
#% 963454
#% 978241
#% 1171636
#% 1247843
#% 1248210
#% 1482304
#% 1656854
#! With the explosion of information stored world-wide, data intensive computing has emerged as a central area of research. Efficient management and processing of this massively exponential amount of data from diverse sources, such as telecommunication call data records, telescope imagery, online transaction records, web pages, stock markets, medical records (monitoring critical health conditions of patients), climate warning systems, etc., has become a necessity. Removing redundancy from such huge (multi-billion records) datasets results in resource and compute efficiency for downstream processing and constitutes an important area of study. "Intelligent compression" or deduplication in streaming scenarios, for precise identification and elimination of duplicates from the unbounded data stream is a greater challenge given the real-time nature of data arrival. Stable Bloom Filters (SBF) [13] address this problem to a certain extent. However, SBF suffers from a high false negative rate and slow convergence rate, thereby rendering it inefficient for applications with low false negative rate tolerance. In this paper, we present a novel reservoir sampling based Bloom filter (RSBF) technique, based on the combined concepts of reservoir sampling and Bloom filters for approximate detection of duplicates in data streams. Using detailed theoretical analysis we prove analytical bounds on its false positive rate, false negative rate and convergence rates with low memory requirements. We show that RSBF outperforms SBF in terms of false negative rates and convergence rates while consuming the same amount of memory. Using empirical analysis on real-world datasets (3 million records) and synthetic datasets with around 1 billion records, we demonstrate upto 2× improvement in false negative rate with better convergence rates as compared to SBF, while maintaining comparable false positive rates. To the best of our knowledge, this is the first attempt to integrate reservoir sampling method with Bloom filters for deduplication in streaming scenarios.

#index 1798395
#* Efficient approximation of the maximal preference scores by lightweight cubic views
#@ Yueguo Chen;Bin Cui;Xiaoyong Du;Anthony K. H. Tung
#t 2012
#c 8
#% 227894
#% 300180
#% 333951
#% 465167
#% 643566
#% 659993
#% 733373
#% 803119
#% 875023
#% 893108
#% 893126
#% 903013
#% 1016183
#% 1022217
#% 1022278
#% 1075132
#% 1206766
#% 1217185
#! Given a multi-features data set, a best preference query (BPQ) computes the maximal preference score (MPS) that the tuples in the data set can achieve with respect to a preference function. BPQs are very useful in applications where users want to efficiently check whether many individual data sets contain tuples that are of interest to them. Although a BPQ can be naïvely answered by issuing a top-1 query and computing the score from the returned tuple, doing so might require to load a larger number of tuples externally. In this paper, we address the problem of efficient processing BPQs by using lightweight cubic (3-dimensional) views. With these in-memory views, the MPSs of BPQs can be efficiently estimated with an error bound guaranteed, by paying only a small number of I/Os. Extensive experimental results over real-life data sets show that our approximate solution can achieve the efficiency of up to three orders of magnitude compared to exact solutions, with certain accuracy guaranteed.

#index 1798396
#* Skyline-sensitive joins with LR-pruning
#@ Mithila Nagendra;K. Selçuk Candan
#t 2012
#c 8
#% 100803
#% 288976
#% 465167
#% 480671
#% 907527
#% 993954
#% 1022225
#% 1063486
#% 1578299
#% 1581852
#% 1594603
#! Efficient processing of skyline queries has been an area of growing interest. Most existing techniques assume that the skyline query is applied to a single data table. Unfortunately, this is not true in many applications where, due to the complexity of the schema, the skyline query may involve attributes belonging to multiple tables. Recently, various hybrid skyline-join algorithms have been proposed. However, the current proposals suffer from several drawbacks: they often need to scan the input tables exhaustively in order to obtain the set of skyline-join results; moreover, the pruning techniques employed to eliminate the tuples are largely based on expensive pairwise tuple-to-tuple comparisons. In this paper, we aim to address these shortcomings by proposing two novel skyline-join algorithms, namely skyline-sensitive join (S 2J) and symmetric skyline-sensitive join (S 3J), to process skyline queries over multiple tables. Our approaches compute the results using a novel layer/region pruning technique (LR-pruning) that prunes the join space in blocks as opposed to individual data points, thereby avoiding excessive pairwise point-to-point dominance checks. Furthermore, the S 3J algorithm utilizes an early stopping condition in order to successfully compute the skyline results by accessing only a subset of the input tables. We report extensive experimental results that confirm the advantages of the proposed algorithms over the state-of-the-art skyline-join techniques.

#index 1798397
#* Top-k interesting phrase mining in ad-hoc collections using sequence pattern indexing
#@ Chuancong Gao;Sebastian Michel
#t 2012
#c 8
#% 152934
#% 397388
#% 463903
#% 464996
#% 577220
#% 810064
#% 857482
#% 902452
#% 1083649
#% 1127403
#% 1166508
#% 1206650
#% 1451226
#% 1495585
#% 1523914
#! In this paper we consider the problem of mining frequently occurring interesting phrases in large document collections in an ad-hoc fashion. Ad-hoc refers to the ability to perform such analyses over text corpora that can be an arbitrary subset of a global set of documents. Most of the times the identification of these ad-hoc document collections is driven by a user or application defined query with the aim of gathering statistics describing the sub-collection, as a starting point for further data analysis tasks. Our approach to mine the top-k most interesting phrases consists of a novel indexing technique, called Sequence Pattern Indexing (SeqPattIndex), that benefits from the observation that phrases often overlap sequentially. We devise a forest based index for phrases and an further improved version with additional redundancy elimination power. The actual top-k phrase mining algorithm operating on these indices is a combination of a simple merge join and inspired by the pattern-growth framework from the data mining community, making use of early termination and search space pruning technologies that enhance the runtime performance. Overall, our approach has on average a lower index space consumption as well as a lower runtime for the top-k phrase mining task, as we demonstrate in the experimental evaluation using real-world data.

#index 1798398
#* Optimizing index deployment order for evolving OLAP
#@ Hideaki Kimura;Carleton Coffrin;Alexander Rasin;Stanley B. Zdonik
#t 2012
#c 8
#% 36119
#% 116086
#% 132560
#% 248815
#% 383492
#% 391417
#% 464878
#% 480158
#% 482100
#% 487502
#% 875027
#% 997495
#% 1016220
#% 1207101
#% 1207103
#% 1328211
#% 1328212
#% 1424561
#% 1523893
#% 1573236
#! Many database applications deploy hundreds or thousands of indexes to speed up query execution. Despite a plethora of prior work on index selection, designing and deploying indexes remains a difficult task for database administrators. First, real-world businesses often require online index deployment, and the traditional off-line approach to index selection ignores intermediate workload performance during index deployment. Second, recent work on on-line index selection does not address effects of complex interactions that manifest during index deployment. In this paper, we propose a new approach that incorporates transitional design performance into the overall problem of physical database design. We call our approach Incremental Database Design. As the first step in this direction, we study the problem of ordering index deployment. The benefits of a good index deployment order are twofold: (1) a prompt query runtime improvement and (2) a reduced total time to deploy the design. Finding an effective deployment order is difficult due to complex index interaction and a factorial number of possible solutions. We formulate a mathematical model to represent the index ordering problem and demonstrate that Constraint Programming (CP) is a more efficient solution compared to other methods such as mixed integer programming and A * search. In addition to exact search techniques, we also study local search algorithms that make significant improvements over a greedy solution with minimal computational overhead. Our empirical analysis using the TPC-H dataset shows that our pruning techniques can reduce the size of the search space by many orders of magnitude. Using the TPC-DS dataset, we verify that our local search algorithm is a highly scalable and stable method for quickly finding the best known solutions.

#index 1798399
#* Distance histogram computation based on spatiotemporal uniformity in scientific data
#@ Anand Kumar;Vladimir Grupcev;Yongke Yuan;Yi-Cheng Tu;Gang Shen
#t 2012
#c 8
#% 113681
#% 116393
#% 234694
#% 392735
#% 731404
#% 798519
#% 845351
#% 1207017
#% 1346416
#% 1433974
#% 1618130
#! Large data generated by scientific applications imposes challenges in storage and efficient query processing. Many queries against scientific data are analytical in nature and require super-linear computation time using straightforward methods. Spatial distance histogram (SDH) is one of the basic queries to analyze the molecular simulation (MS) data, and it takes quadratic time to compute using brute-force approach. Often, an SDH query is executed continuously to analyze the simulation system over a period of time. This adds to the total time required to compute SDH. In this paper, we propose an approximate algorithm to compute SDH efficiently over consecutive time periods. In our approach, data is organized into a Quad-tree based data structure. The spatial locality of the particles (at given time) in each node of the tree is acquired to determine the particle distribution. Similarly, the temporal locality of particles (between consecutive time periods) in each node is also acquired. The spatial distribution and temporal locality are utilized to compute the approximate SDH at every time instant. The performance is boosted by storing and updating the spatial distribution information over time. The efficiency and accuracy of the proposed algorithm is supported by mathematical analysis and results of extensive experiments using biological data generated from real MS studies.

#index 1798400
#* A generic data model and query language for spatiotemporal OLAP cube analysis
#@ Leticia I. Gómez;Silvia A. Gómez;Alejandro A. Vaisman
#t 2012
#c 8
#% 280448
#% 393641
#% 413118
#% 461921
#% 482093
#% 503884
#% 631946
#% 758494
#% 1298877
#% 1490156
#% 1493450
#% 1642889
#% 1711266
#! Nowadays, organizations need to use OLAP (On Line Analytical Processing) tools together with geographical information. To support this, the notion of SOLAP (Spatial OLAP) arouse, aimed at exploring spatial data in the same way as OLAP operates over tables. SOLAP however, only accounts for discrete spatial data. More sophisticated GIS-based decision support systems are increasingly being needed, to handle more complex types of data, like continuous fields. Fields describe physical phenomena that change continuously in time and/or space (e.g., temperature). Although many models have been proposed for adding spatial information to OLAP tools, no one allows the user to perceive data as a cube, and analyze any type of spatial data, continuous or discrete, together with typical alphanumerical discrete OLAP data, using only the classic OLAP operators (e.g., Roll-up, Drill-down). In this paper we propose an algebra that operates over data cubes, independently of the underlying data types and physical data representation. That means, in our approach, the final user only sees the typical OLAP operators at the query level. At lower abstraction levels we provide discrete and continuous spatial data support as well as different ways of partitioning the space. We also describe a proof-of-concept implementation to illustrate the ideas presented in the paper. As far as we are aware of, this is the first proposal that allows analyzing discrete and continuous spatiotemporal data and OLAP cubes together, using just the traditional OLAP operations, thus providing a very general framework for spatiotemporal data analysis.

#index 1798401
#* Proceedings of the 15th International Conference on Extending Database Technology
#@ Elke Rundensteiner;Volker Markl;Ioana Manolescu;Sihem Amer-Yahia;Felix Naumann;Ismail Ari
#t 2012
#c 8
#! Welcome to the 2012 Edition of the International Conference on Extending Database Technology (EDBT). This year, EDBT is taking place in Berlin, Germany on March 26-30, 2012. The International Conference on Extending Database Technology is known as a leading international forum for database researchers, practitioners, developers, and users to discuss cutting-edge ideas, and to exchange techniques, tools, and experiences related to data management. Data management constitutes the essential enabling technology for scientific, engineering, business, and social communities; and its technology is driven by the needs in applications ranging from the social web, scientific discoveries, virtual libraries, to embedded systems. We are very pleased to offer you an exciting program on these important and timely topics. This year, as in prior years, EDBT is co-located with the International Conference on Database Theory (ICDT).

#index 1798402
#* Subscription indexes for web syndication systems
#@ Zeinab Hmedeh;Harris Kourdounakis;Vassilis Christophides;Cedric du Mouza;Michel Scholl;Nicolas Travers
#t 2012
#c 8
#% 158911
#% 271199
#% 297191
#% 317317
#% 333938
#% 480147
#% 646220
#% 766447
#% 813187
#% 867054
#% 867878
#% 978475
#% 1117066
#% 1127386
#% 1206844
#% 1328112
#% 1631205
#! The explosion of published information on the Web leads to the emergence of a Web syndication paradigm, which transforms the passive reader into an active information collector. Information consumers subscribe to RSS/Atom feeds and are notified whenever a piece of news (item) is published. The success of this Web syndication now offered on Web sites, blogs, and social media, however raises scalability issues. There is a vital need for efficient real-time filtering methods across feeds, to allow users to follow effectively personally interesting information. We investigate in this paper three indexing techniques for users' subscriptions based on inverted lists or on an ordered trie. We present analytical models for memory requirements and matching time and we conduct a thorough experimental evaluation to exhibit the impact of critical workload parameters on these structures.

#index 1798403
#* Heuristics-based query optimisation for SPARQL
#@ Petros Tsialiamanis;Lefteris Sidirourgos;Irini Fundulaki;Vassilis Christophides;Peter Boncz
#t 2012
#c 8
#% 36683
#% 408396
#% 571763
#% 824697
#% 824755
#% 1022236
#% 1055731
#% 1098453
#% 1127402
#% 1127431
#% 1127610
#% 1190676
#% 1206875
#% 1217194
#% 1218670
#% 1269903
#% 1366460
#% 1399937
#% 1409918
#% 1413091
#% 1424588
#% 1581858
#% 1594602
#% 1602034
#% 1655429
#% 1737597
#! Query optimization in RDF Stores is a challenging problem as SPARQL queries typically contain many more joins than equivalent relational plans, and hence lead to a large join order search space. In such cases, cost-based query optimization often is not possible. One practical reason for this is that statistics typically are missing in web scale setting such as the Linked Open Datasets (LOD). The more profound reason is that due to the absence of schematic structure in RDF, join-hit ratio estimation requires complicated forms of correlated join statistics; and currently there are no methods to identify the relevant correlations beforehand. For this reason, the use of good heuristics is essential in SPARQL query optimization, even in the case that are partially used with cost-based statistics (i.e., hybrid query optimization). In this paper we describe a set of useful heuristics for SPARQL query optimizers. We present these in the context of a new Heuristic SPARQL Planner (HSP) that is capable of exploiting the syntactic and the structural variations of the triple patterns in a SPARQL query in order to choose an execution plan without the need of any cost model. For this, we define the variable graph and we show a reduction of the SPARQL query optimization problem to the maximum weight independent set problem. We implemented our planner on top of the MonetDB open source column-store and evaluated its effectiveness against the state-of-the-art RDF-3X engine as well as comparing the plan quality with a relational (SQL) equivalent of the benchmarks.

#index 1798404
#* See what's enBlogue: real-time emergent topic identification in social media
#@ Foteini Alvanaki;Sebastian Michel;Krithi Ramamritham;Gerhard Weikum
#t 2012
#c 8
#% 262042
#% 262043
#% 411762
#% 577214
#% 729980
#% 766444
#% 818215
#% 864609
#% 967452
#% 993961
#% 1019124
#% 1022338
#% 1077150
#% 1214671
#% 1219788
#% 1227763
#% 1355045
#% 1426611
#% 1432574
#% 1536525
#% 1536536
#% 1581969
#% 1606346
#! With the increasing popularity of Web 2.0 streams, people become overwhelmed by the available information. This is partly countered by tagging blog posts and tweets, so that users can filter messages according to their tags. However, this is insufficient for detecting newly emerging topics that are not reflected by a single tag but are rather expressed by unusual tag combinations. This paper presents enBlogue, an approach for automatically detecting such emergent topics. EnBlogue uses a time-sliding window to compute statistics about tags and tag-pairs. These statistics are then used to identify unusual shifts in correlations, most of the time caused by real-world events. We analyze the strength of these shifts and measure the degree of unpredictability they include, used to rank tag-pairs expressing emergent topics. Additionally, this "indicator of surprise" is carried over to subsequent time points, as user interests do not abruptly vanish from one moment to the other. To avoid monitoring all tag-pairs we can also select a subset of tags, e. g., the most popular or volatile of them, to be used as seed-tags for subsequent pair-wise correlation computations. The system is fully implemented and publicly available on the Web, processing live Twitter data. We present experimental studies based on real world datasets demonstrating both the prediction quality by means of a user study and the efficiency of enBlogue.

#index 1798405
#* Searching by corpus with fingerprints
#@ Charu C. Aggarwal;Wangqun Lin;Philip S. Yu
#t 2012
#c 8
#% 1921
#% 24076
#% 35937
#% 46803
#% 54468
#% 213786
#% 227783
#% 235941
#% 249989
#% 281480
#% 300103
#% 318437
#% 394709
#% 406493
#% 420487
#% 1166508
#% 1451021
#! The growing sizes of text repositories on the world wide web has created a need for efficient indexing and retrieval methods for text collections. Almost all of the text retrieval and indexing methods have been designed for the case of simple keyword search, in which a few keywords are specified, and the text is retrieved on the basis of matches to these keywords. However, in many applications there is a need for a greater specificity during the search, such as the use of phrases, sentences, text fragments, or even documents for the retrieval process. An even more general case is one in which a collection of documents is available as a query to the search process. In such cases, it is desirable to return sets of all pairwise similar documents. Such queries are referred to as corpus to corpus queries, and are computationally intensive because of the very large number of document pairs which need to be compared. Such cases cannot be efficiently processed by the available indexing and searching methods. Most of the currently available techniques can index the text based on only a small number of keywords or representative phrases. In this paper, we design a compressed finger print index which can support the following more general queries: (a) The method can process very efficient document-to-corpus search because of their efficient bit-wise operations for the search process. (b) We further extend the method to work for corpus-to-corpus queries, in which it is desirable to determine the most similar pairs of documents in two collections. We design an efficient search technique which is able to reduce the search time for large collections. The key technique used to enable this is an efficient fingerprint representation, which can be used effectively for the search process. To the best of our knowledge, this is the first work on corpus-based search in massive document collections.

#index 1798406
#* Aggregate queries on probabilistic record linkages
#@ Ming Hua;Jian Pei
#t 2012
#c 8
#% 380725
#% 388024
#% 443030
#% 480654
#% 577263
#% 577522
#% 654487
#% 729913
#% 824733
#% 844289
#% 864394
#% 875066
#% 893121
#% 907562
#% 977008
#% 977014
#% 1022205
#% 1022259
#% 1127415
#% 1568421
#! Record linkage analysis, which matches records referring to the same real world entities from different data sets, is an important task in data integration. Uncertainty often exists in record linkages due to incompleteness or ambiguity in data. Fortunately, the state-of-the-art probabilistic record linkage methods are capable of computing the probability that two records referring to the same entity. In this paper, we study the novel aggregate queries on probabilistic record linkages, such as counting the number of matched records. We address several fundamental issues. First, we advocate that the answer to an aggregate query on probabilistic record linkages is a probability distribution of possible answers derived from possible worlds. Second, we identify the category of compatible linkages only on which the answers to aggregate queries can be determined properly when the probabilities of individual linkages are available but the joint distributions of multiple linkages are unavailable. Third, we give a quadratic exact algorithm and two approximation algorithms to answer aggregate queries.

#index 1798407
#* Efficient distributed query processing for autonomous RDF databases
#@ Fabian Prasser;Alfons Kemper;Klaus A. Kuhn
#t 2012
#c 8
#% 330305
#% 462059
#% 565473
#% 754121
#% 1022236
#% 1111655
#% 1127610
#% 1333447
#% 1366460
#% 1399974
#% 1413160
#% 1413162
#% 1540320
#% 1540323
#% 1592340
#% 1594583
#% 1594602
#! The inherent flexibility of the RDF data model has led to its notable adoption in many domains, especially in the area of life-sciences. Some of these domains have an emerging need to access data integrated from various distributed sources of information. It is not always possible to implement this by simply loading all data into one central RDF store. For example, in the context of inter-institutional collaboration for drug development and clinical research participants often want to maintain control over their local databases. Alternatively, distributed query processing techniques can be utilized to evaluate queries by accessing the remote data sources only on demand and in conformance with local authorization models. In this paper we present an efficient approach to distributed query processing for large autonomous RDF databases. The groundwork is laid by a comprehensive RDF-specific schema- and instance-level synopsis. We present an optimizer that is able to utilize this synopsis to generate compact execution plans by precisely determining, at compile-time, those sources that are relevant to a query. Furthermore we present a tightly integrated query engine that is able to further reduce the volume of intermediate results at run-time. An extensive evaluation shows that our approach improves query execution times by up to two and transferred data volumes by up to three orders of magnitude compared to a naïve implementation.

#index 1798408
#* CRSI: a compact randomized similarity index for set-valued features
#@ Petros Venetis;Yannis Sismanis;Berthold Reinwald
#t 2012
#c 8
#% 211059
#% 249238
#% 249321
#% 255137
#% 347225
#% 387427
#% 479973
#% 519953
#% 654466
#% 730065
#% 765463
#% 805839
#% 823403
#% 864392
#% 869500
#% 879600
#% 893164
#% 956506
#% 960250
#% 967028
#% 989512
#% 1055684
#% 1074121
#% 1127368
#% 1206821
#! We propose a similarity index for set-valued features and study algorithms for executing various set similarity queries on it. Such queries are fundamental for many application areas, including data integration and cleaning, data profiling as well as near duplicate document detection. In this paper, we focus on Jaccard similarity and present estimators that work for arbitrary similarity thresholds based on a single similarity index. We show how to build this similarity index a-priori, without knowledge about query similarity thresholds, based on recently proposed synopses for multiset operations. The index is deployed using existing disk-based inverted indexing implementations and our algorithms exploit available techniques, like skip-lists, to further optimize the query performance. The index has provably small space footprints, is orders of magnitude smaller and faster to create/incrementally maintain than exact solutions, and the algorithms provide approximate answers, with an error that is controlled by a user-specified parameter. We prove the error bounds of our algorithms analytically, and, finally, we demonstrate the performance of the algorithms and verify their accuracy experimentally.

#index 1798409
#* VAST-Tree: a vector-advanced and compressed structure for massive data tree traversal
#@ Takeshi Yamamuro;Makoto Onizuka;Toshio Hitaka;Masashi Yamamuro
#t 2012
#c 8
#% 287715
#% 300194
#% 333949
#% 464987
#% 479769
#% 479819
#% 479821
#% 480119
#% 580978
#% 765419
#% 864446
#% 874997
#% 1015288
#% 1127563
#% 1222052
#% 1227595
#% 1241839
#% 1328102
#% 1426530
#% 1426531
#% 1468279
#% 1482300
#% 1523855
#% 1550752
#% 1625107
#% 1745137
#! We propose a compact and efficient index structure for massive data sets. Several indexing techniques are widely-used and well-known such as binary trees and B+trees. Unfortunately, we find that these techniques suffer major two shortcomings when applied to massive sets; first, their indices are so large they could overflow regular main memory, and, second, they suffer from a variety of penalties (e.g., conditional branches, low cache hits, and TLB misses), which restricts the number of instructions executed per processor cycle. Our state-of-the-art index structure, called VAST-Tree, classifies branch nodes into multiple layers. It applies existing techniques such as cache-conscious, aligned, and branch-free structures to the top layers of branch nodes in trees. Next, it applies the adaptive compression technique to save space and harness data parallelism with SIMD instructions to the middle and bottom layers of branch nodes. Moreover, a processor-friendly compression technique is applied to leaf nodes. The end result is that trees are much more compact and traversal efficiency is high. We implement a prototype and show its resulting index size and performance as compared to binary trees, and the hardware-conscious technique called FAST which currently offers the highest performance. Compared to current alternatives, VAST-Tree compacts the branch nodes by more than 95%, and the overall index size by 47-84% given that there are 230 keys. With 228 keys, it has roughly 6.0-times and 1.24-times throughput and saves the memory consumption by more than 94.7% and 40.5% as compared to binary trees and FAST, respectively.

#index 1798410
#* Repair-oriented relational schemas for multidimensional databases
#@ Mahkameh Yaghmaie;Leopoldo Bertossi;Sina Ariyan
#t 2012
#c 8
#% 273687
#% 308509
#% 340144
#% 384978
#% 388009
#% 443378
#% 465028
#% 473009
#% 479948
#% 487676
#% 503400
#% 572459
#% 582130
#% 631925
#% 810020
#% 833135
#% 879041
#% 960233
#% 1370260
#% 1401744
#% 1433975
#% 1523975
#% 1661438
#% 1712553
#% 1728683
#% 1798410
#% 1898008
#! Summarizability in a multidimensional (MD) database refers to the correct reusability of pre-computed aggregate queries (or views) when computing higher-level aggregations or roll-ups. A dimension instance has this property if and only if it is strict and homogeneous. A dimension instance may fail to satisfy either of these two semantics conditions, and has to be repaired, restoring strictness and homogeneity. In this work, we take a relational approach to the problem of repairing dimension instances. A dimension repair is obtained by translating the dimension instance into a relational instance, repairing the latter using established techniques in the relational framework, and properly inverting the process. We show that the common relational star and snowflake schemas for MD databases are not the best choice for this process. Actually, for this purpose, we propose and formalize the path relational schema, which becomes the basis for obtaining dimensional repairs. The path schema turns out to have useful properties in general, as a basis for a relational representation and implementation of MD databases and data warehouses. It is also particularly suitable for restoring MD summarizability through relational repairs. We compare the dimension repairs so obtained with existing repair approaches for MD databases.

#index 1798411
#* Adaptive MapReduce using situation-aware mappers
#@ Rares Vernica;Andrey Balmin;Kevin S. Beyer;Vuk Ercegovac
#t 2012
#c 8
#% 58352
#% 201883
#% 340670
#% 442923
#% 480966
#% 983467
#% 1022230
#% 1085307
#% 1217159
#% 1278124
#% 1328060
#% 1328186
#% 1386049
#% 1426486
#% 1426488
#% 1426543
#% 1426584
#% 1468411
#% 1468530
#% 1475077
#% 1523820
#% 1523837
#% 1523839
#% 1523841
#% 1567910
#% 1581928
#% 1594630
#% 1608685
#! We propose new adaptive runtime techniques for MapReduce that improve performance and simplify job tuning. We implement these techniques by breaking a key assumption of MapReduce that mappers run in isolation. Instead, our mappers communicate through a distributed meta-data store and are aware of the global state of the job. However, we still preserve the fault-tolerance, scalability, and programming API of MapReduce. We utilize these "situation-aware mappers" to develop a set of techniques that make MapReduce more dynamic: (a) Adaptive Mappers dynamically take multiple data partitions (splits) to amortize mapper start-up costs; (b) Adaptive Combiners improve local aggregation by maintaining a cache of partial aggregates for the frequent keys; (c) Adaptive Sampling and Partitioning sample the mapper outputs and use the obtained statistics to produce balanced partitions for the reducers. Our experimental evaluation shows that adaptive techniques provide up to 3x performance improvement, in some cases, and dramatically improve performance stability across the board.

#index 1798412
#* "Cut me some slack": latency-aware live migration for databases
#@ Sean Barker;Yun Chi;Hyun Jin Moon;Hakan Hacigümüş;Prashant Shenoy
#t 2012
#c 8
#% 327132
#% 602711
#% 723288
#% 893153
#% 963628
#% 966956
#% 978779
#% 983483
#% 1312540
#% 1426489
#% 1536332
#% 1550914
#% 1581871
#% 1581872
#% 1592341
#% 1594596
#! Cloud-based data management platforms often employ multitenant databases, where service providers achieve economies of scale by consolidating multiple tenants on shared servers. In such database systems, a key functionality for service providers is database migration, which is useful for dynamic provisioning, load balancing, and system maintenance. Practical migration solutions have several requirements, including high availability, low performance overhead, and self-management. We present Slacker, an end-to-end database migration system at the middleware level satisfying these requirements. Slacker leverages off-the-shelf hot backup tools to achieve live migration with effectively zero down-time. Additionally, Slacker minimizes the performance impact of migrations on both the migrating tenant and collocated tenants by leveraging 'migration slack', or resources that can be used for migration without excessively impacting query latency. We apply a PID controller to this problem, allowing Slacker to automatically detect and exploit migration slack in real time. Using our prototype, we demonstrate that Slacker effectively controls interference during migrations, maintaining latency within 10% of a given latency target, while still performing migrations rapidly and efficiently.

#index 1798413
#* Peak power plays in database engines
#@ Mayuresh Kunjir;Puneet K. Birwa;Jayant R. Haritsa
#t 2012
#c 8
#% 300196
#% 765467
#% 765468
#% 777874
#% 824756
#% 828604
#% 960264
#% 993945
#% 1022297
#% 1070440
#% 1127556
#% 1134501
#% 1395145
#% 1426521
#% 1562244
#! Database engines often consume significant power during query processing activities, motivating researchers to investigate the redesign of their internals to minimize these overheads. While the prior literature has dealt exclusively with average power considerations, our focus here is on peak power consumption. We begin by profiling the peak power behavior of a representative suite of popular commercial database engines in benchmark query processing environments, and demonstrate that their consumption can often be substantial. Then, we develop a pipeline-based model of query execution plans that lends itself to accurately estimating peak power consumption, suggesting its gainful employment in server design and capacity planning. More potently, given a space of competing plan choices, it could help identify plans with attractive tradeoffs between peak-power and time-efficiency considerations, and we present sample instances of such tradeoffs. Finally, we discuss extensions of our modeling approach to inductive pipelines and multi-query workloads.

#index 1798414
#* Finding top-k similar graphs in graph databases
#@ Yuanyuan Zhu;Lu Qin;Jeffrey Xu Yu;Hong Cheng
#t 2012
#c 8
#% 17293
#% 260974
#% 378391
#% 447719
#% 765429
#% 769891
#% 772183
#% 810072
#% 864425
#% 960305
#% 1022279
#% 1022280
#% 1044450
#% 1127380
#% 1181230
#% 1181253
#% 1426577
#% 1523835
#% 1618133
#% 1682600
#! Querying similar graphs in graph databases has been widely studied in graph query processing in recent years. Existing works mainly focus on subgraph similarity search and supergraph similarity search. In this paper, we study the problem of finding top-k graphs in a graph database that are most similar to a query graph. This problem has many applications, such as image retrieval and chemical compound structure search. Regarding the similarity measure, feature based and kernel based similarity measures have been used in the literature. But such measures are rough and may lose the connectivity information among substructures. In this paper, we introduce a new similarity measure based on the maximum common subgraph (MCS) of two graphs. We show that this measure can better capture the common and different structures of two graphs. Since computing the MCS of two graphs is NP-hard, we propose an algorithm to answer the top-k graph similarity query using two distance lower bounds with different computational costs, in order to reduce the number of MCS computations. We further introduce an indexing technique, which can better make use of the triangle property of similarities among graphs in the database to get tighter lower bounds. Three different indexing methods are proposed with different tradeoffs between pruning power and construction cost. We conducted extensive performance studies on large real datasets to evaluate the performance of our approaches.

#index 1798415
#* I/O cost minimization: reachability queries processing over massive graphs
#@ Zhiwei Zhang;Jeffrey Xu Yu;Lu Qin;Qing Zhu;Xiaofang Zhou
#t 2012
#c 8
#% 47573
#% 58365
#% 88051
#% 341704
#% 379482
#% 577372
#% 765272
#% 800534
#% 824692
#% 838518
#% 864462
#% 960304
#% 1044451
#% 1055756
#% 1063514
#% 1074714
#% 1206685
#% 1217208
#% 1523819
#% 1581922
#% 1688299
#! Given a directed graph G, a reachability query (u, v) asks whether there exists a path from a node u to a node v in G. The existing studies support reachability queries using indexing techniques, where both the graph and the index are required to reside in main memory. However, they cannot handle reachability queries on massive graphs, when the graph and the index cannot be entirely held in memory because of the high I/O cost. In this paper, we focus on how to minimize the I/O cost when answering reachability queries on massive graphs that cannot reside entirely in memory. First, we propose a new Yes-Label scheme, as a complement of the No-Label used in GRAIL [23], to reduce the number of intermediate results generated. Second, we show how to minimize the number of I/Os using a heap-on-disk data structure when traversing a graph. We also propose new methods to partition the heap-on-disk, in order to ensure that only sequential I/Os are performed. Third, we analyze our approaches and show how to extend our approaches to answer multiple reachability queries effectively. Finally, we conducted extensive performance studies on both large synthetic and large real graphs, and confirm the efficiency of our approaches.

#index 1798416
#* Finding maximal k-edge-connected subgraphs from a large graph
#@ Rui Zhou;Chengfei Liu;Jeffrey Xu Yu;Weifa Liang;Baichen Chen;Jianxin Li
#t 2012
#c 8
#% 58664
#% 117497
#% 124524
#% 237380
#% 274612
#% 282226
#% 313959
#% 370988
#% 498852
#% 577356
#% 823357
#% 937549
#% 956459
#% 989654
#% 991134
#% 1083508
#% 1214695
#% 1214721
#% 1426539
#! In this paper, we study how to find maximal k-edge-connected subgraphs from a large graph. k-edge-connected subgraphs can be used to capture closely related vertices, and finding such vertex clusters is interesting in many applications, e. g., social network analysis, bioinformatics, web link research. Compared with other explicit structures for modeling vertex clusters, such as quasi-clique, k-core, which only set the requirement on vertex degrees, k-edge-connected subgraph further requires high connectivity within a subgraph (a stronger requirement), and hence defines a more closely related vertex cluster. To find maximal k-edge-connected subgraphs from a graph, a basic approach is to repeatedly apply minimum cut algorithm to the connected components of the input graph until all connected components are k-connected. However, the basic approach is very expensive if the input graph is large. To tackle the problem, we propose three major techniques: vertex reduction, edge reduction and cut pruning. These speed-up techniques are applied on top of the basic approach. We conduct extensive experiments and show that the speed-up techniques are very effective.

#index 1798417
#* SIMP: accurate and efficient near neighbor search in high dimensional spaces
#@ Vishwakarma Singh;Ambuj K. Singh
#t 2012
#c 8
#% 237187
#% 248017
#% 249321
#% 252304
#% 318703
#% 321455
#% 333975
#% 347225
#% 427199
#% 443329
#% 479462
#% 479649
#% 479973
#% 489532
#% 527188
#% 645687
#% 745496
#% 760805
#% 762054
#% 805905
#% 814646
#% 818938
#% 847166
#% 871357
#% 1022281
#% 1023422
#% 1130875
#% 1131853
#% 1217189
#% 1372717
#% 1426578
#! Near neighbor search in high dimensional spaces is useful in many applications. Existing techniques solve this problem efficiently only for the approximate cases. These solutions are designed to solve r-near neighbor queries for a fixed query range or for a set of query ranges with probabilistic guarantees, and then extended for nearest neighbor queries. Solutions supporting a set of query ranges suffer from prohibitive space cost. There are many applications which are quality sensitive and need to efficiently and accurately support near neighbor queries for all query ranges. In this paper, we propose a novel indexing and querying scheme called Spatial Intersection and Metric Pruning (SIMP). It efficiently supports r-near neighbor queries in very high dimensional spaces for all query ranges with 100% quality guarantee and with practical storage costs. Our empirical studies on three real datasets having dimensions between 32 and 256 and sizes up to 10 million show a superior performance of SIMP over LSH, Multi-Probe LSH, LSB tree, and iDistance. Our scalability tests on real datasets having as many as 100 million points of dimensions up to 256 establish that SIMP scales linearly with query range, dataset dimension, and dataset size.

#index 1798418
#* Effectively indexing the multi-dimensional uncertain objects for range searching
#@ Ying Zhang;Wenjie Zhang;Qianlu Lin;Xuemin Lin
#t 2012
#c 8
#% 45766
#% 213975
#% 317933
#% 572293
#% 749907
#% 983259
#% 1016202
#% 1117705
#% 1127377
#% 1177871
#% 1206689
#% 1206879
#% 1217128
#% 1328209
#% 1464054
#% 1488675
#% 1523851
#% 1669490
#% 1890013
#! The range searching problem is fundamental in a wide spectrum of applications such as radio frequency identification (RFID) networks, location based services (LBS), and global position system (GPS). As the uncertainty is inherent in those applications, it is highly demanded to address the uncertainty in the range search since the traditional techniques cannot be applied due to the inherence difference between the uncertain data and traditional data. In the paper, we propose a novel indexing structure, named U-Quadtree, to organize the uncertain objects in a multi-dimensional space such that the range searching can be answered efficiently by applying filtering techniques. Particularly, based on some insights of the range search on uncertain data, we propose a cost model which carefully considers various factors that may impact the performance of the range searching. Then an effective and efficient index construction algorithm is proposed to build the optimal U-Quadtree regarding the cost model. Comprehensive experiments demonstrate that our technique outperforms the existing works for range searching on multi-dimensional uncertain objects.

#index 1798419
#* SFA: a symbolic fourier approximation and index for similarity search in high dimensional datasets
#@ Patrick Schäfer;Mikael Högqvist
#t 2012
#c 8
#% 86950
#% 172949
#% 227924
#% 252304
#% 333941
#% 427199
#% 460862
#% 501658
#% 570438
#% 578400
#% 631923
#% 659936
#% 662750
#% 745513
#% 765451
#% 992857
#% 1022238
#% 1044456
#% 1083693
#% 1375186
#% 1535372
#! Time series analysis, as an application for high dimensional data mining, is a common task in biochemistry, meteorology, climate research, bio-medicine or marketing. Similarity search in data with increasing dimensionality results in an exponential growth of the search space, referred to as Curse of Dimensionality. A common approach to postpone this effect is to apply approximation to reduce the dimensionality of the original data prior to indexing. However, approximation involves loss of information, which also leads to an exponential growth of the search space. Therefore, indexing an approximation with a high dimensionality, i. e. high quality, is desirable. We introduce Symbolic Fourier Approximation (SFA) and the SFA trie which allows for indexing of not only large datasets but also high dimensional approximations. This is done by exploiting the trade-off between the quality of the approximation and the degeneration of the index by using a variable number of dimensions to represent each approximation. Our experiments show that SFA combined with the SFA trie can scale up to a factor of 5--10 more indexed dimensions than previous approaches. Thus, it provides lower page accesses and CPU costs by a factor of 2--25 respectively 2--11 for exact similarity search using real world and synthetic data.

#index 1798420
#* Introducing MapLan to map banking survey data into a time series database
#@ Manuel Günter
#t 2012
#c 8
#% 168251
#% 314266
#% 480483
#% 632040
#% 893089
#! In order to fulfill its monetary policy function, the Swiss National Bank (SNB) collects statistical data on the economy. The SNB stores results of the regularly held surveys in a specialized database (primary), ordered by surveys and survey forms. After validation the data has to be transferred in another specialized database (secondary) where it can be accessed by economists. The secondary database keeps the data in time series that are hierarchically arranged by statistical taxonomies. The data transfer from the primary to the secondary database feeds 1.5 million time series. Mapping and transformation logic was hard-coded in legacy programs. They were cumbersome to manage and intransparent to the economists in charge. In this paper we describe a novel approach called MapLan, a Java-based data mapping system featuring a domain specific language. The MapLan system not only performs the data transformation and mapping, it also produces complete data lineage information. This paper shows in practice that domain specific languages are an efficient tool to solve two pressing data mapping and transformation problems of statistical databases. One problem is that of mapping the large and heterogeneous schemas of statistical databases in an efficient and manageable way. The other problem is the business need for complete data lineage of the target time series.

#index 1798421
#* Extending a general-purpose streaming system for XML
#@ Mark Mendell;Howard Nasgaard;Eric Bouillet;Martin Hirzel;Buǧra Gedik
#t 2012
#c 8
#% 300179
#% 346651
#% 397375
#% 504575
#% 654476
#% 654477
#% 805866
#% 875010
#% 875029
#% 878299
#% 994015
#% 995806
#% 1523955
#% 1675682
#% 1736399
#! General-purpose streaming systems support diverse application domains with powerful and user-defined stream operators. Most general-purpose streaming systems have their own, non-XML, internal data representation. However, streaming input is often either a sequence of small XML documents, or a scan of a huge document. Prior work on XML streaming focuses on filtering, not transforming, XML, and does not describe how to integrate with a general-purpose streaming system. This paper describes how to integrate an XML transformer with a streaming system by designing a specification syntax that is both consistent with the existing system and familiar to XML users. After type-checking the specification, we compile it to an efficient automaton driven by SAX events. Our approach extends the underlying streaming system with XML support without changing its core architecture, and the same technique could be used for other extensions beyond XML.

#index 1798422
#* Mining search behavior and user-generated content: presentation at the industrial session - EDBT/ICDT 2012
#@ Carlos Castillo
#t 2012
#c 8
#% 729923
#% 1130868
#% 1173699
#% 1355017
#% 1450894
#% 1560422
#% 1605972
#! In the first part of this presentation, we will overview two systems that gather and display intelligence from search behavior: "Yahoo! Search Clues" and "Yahoo! Political Insights". In the second part, we will discuss two real-world problems encountered when mining user-generated content: determining which pieces of content are credible, and modeling how users influence each other.

#index 1798423
#* Data management with SAPs in-memory computing engine
#@ Joos-Hendrik Boese;Cafer Tosun;Christian Mathis;Franz Faerber
#t 2012
#c 8
#% 1217145
#% 1614904
#% 1615897
#% 1667313
#! We present some architectural and technological insights on SAP's HANA database and derive research challenges for future enterprise application development. The HANA database management system [1] was developed to meet changed requirements of modern business applications. Nowadays, these require fast and complex analytical data processing coupled with traditional transactional data management. Additionally, fast and agile decision processes that take operational data as well as structured and unstructured information into account are the current key driver in enterprises for business success. In conventional system landscapes currently found in enterprises, dedicated systems are used for analytical and transactional data processing. In contrast, HANA follows a more holistic data management approach by integrating OLTP and OLAP functionality in a single system and by adding features beyond traditional database management systems, such as graph or text processing for semi- and unstructured data. While in common three-tier architectures, compute-intensive applications run at the application server layer and data is loaded into the main memory of application servers, enterprise applications developed for or moved to HANA are more tightly integrated with the database. The main principle of application development for HANA is to execute data-intensive computations in the database close to the raw data in order to prevent expensive data movement. This shift in application design poses new challenges to the application developer: in order to utilize HANA efficiently, he has to think differently about how to design his application. We'll address these challenges and present some open questions in this area in the second part of the talk.

#index 1798424
#* Tailoring entity resolution for matching product offers
#@ Hanna Köpcke;Andreas Thor;Stefan Thomas;Erhard Rahm
#t 2012
#c 8
#% 519556
#% 878935
#% 913783
#% 1314445
#% 1459821
#% 1523838
#% 1581406
#% 1605958
#% 1733004
#! Product matching is a challenging variation of entity resolution to identify representations and offers referring to the same product. Product matching is highly difficult due to the broad spectrum of products, many similar but different products, frequently missing or wrong values, and the textual nature of product titles and descriptions. We propose the use of tailored approaches for product matching based on a preprocessing of product offers to extract and clean new attributes usable for matching. In particular, we propose a new approach to extract and use so-called product codes to identify products and distinguish them from similar product variations. We evaluate the effectiveness of the proposed approaches with challenging real-life datasets with product offers from online shops. We also show that the UPC information in product offers is often error-prone and can lead to insufficient match decisions.

#index 1798425
#* Towards principled design support for scalable OLTP workloads
#@ Bin Liu;Junichi Tatemura;Hakan Hacigümüş
#t 2012
#c 8
#% 998845
#% 1002142
#% 1654050
#! Supporting online transaction processing (OLTP) workload in a scalable and elastic fashion is a challenging task. With the advent of cloud-based systems, supporting entity group based consistency is a viable, scalable, and cost-effective option. This approach remains attractive in the presence of systems supporting the highest level of consistency, due to the relative high cost and performance degradation of the latter. In this paper, we briefly introduce our on-going work for assisting application developers to design OLTP workload for entity group based systems. The goal is providing a suite of user-friendly design tools for new-breed databases to achieve scalability and elasticity.

#index 1798426
#* Similarity in (spatial, temporal and) spatio-temporal datasets
#@ Dimitrios Gunopulos;Goce Trajcevski
#t 2012
#c 8
#% 127521
#% 159792
#% 172949
#% 230174
#% 270995
#% 325683
#% 333941
#% 334035
#% 487887
#% 504066
#% 659971
#% 761237
#% 765164
#% 769899
#% 769961
#% 771228
#% 795273
#% 799398
#% 810049
#% 818916
#% 861116
#% 878302
#% 881459
#% 893220
#% 899659
#% 907184
#% 907380
#% 951768
#% 960283
#% 985888
#% 992857
#% 1017096
#% 1035386
#% 1035419
#% 1038325
#% 1039748
#% 1046187
#% 1107568
#% 1127436
#% 1127437
#% 1127609
#% 1135946
#% 1166608
#% 1176919
#% 1176952
#% 1181287
#% 1183826
#% 1207112
#% 1207116
#% 1211645
#% 1267835
#% 1291978
#% 1292933
#% 1298860
#% 1298875
#% 1298896
#% 1318898
#% 1409361
#% 1428694
#% 1592778
#% 1598237
#% 1663642
#% 1678593
#! Similarity among mobile entities is an important type of query for many application domains. This tutorial provides a comprehensive overview of the different challenges related to assessing the similarity of spatio-temporal objects, along with the corresponding results/techniques.

#index 1798427
#* Distributed skyline processing: a trend in database research still going strong
#@ Katja Hose;Akrivi Vlachou
#t 2012
#c 8
#% 2115
#% 288976
#% 465167
#% 654480
#% 824671
#% 824672
#% 864453
#% 878649
#% 907529
#% 1065612
#% 1114757
#% 1177872
#% 1201865
#% 1206767
#% 1246151
#% 1267293
#% 1409478
#% 1512991
#% 1537153
#% 1549859
#% 1688253
#% 1853527
#! During the last decade, data management and storage have become increasingly distributed. In consideration of the huge amount of data available in such systems, advanced query operators, such as skyline queries, are necessary to help users process the data. For example, a user who is interested in buying a car wants to find a good trade-off between minimum age and minimum price. It is not obvious how much cheaper a car should be, if it is one year older than another car. Thus, the skyline query will retrieve a set of data items that are the best trade-offs for the user's preferences. The skyline operator has been proposed about a decade ago, but research on skyline queries, especially in distributed scenarios, is still an ongoing process. Query processing in distributed environments poses inherent challenges and requires non-traditional techniques due to the distribution of content and the lack of global knowledge. In this tutorial, we will outline the objectives and the main principles that any distributed skyline approach has to fulfill, leading to useful guidelines for the design of efficient distributed skyline algorithms. More importantly, distributed processing of other query types share the same objectives and principles, therefore several of the guidelines are applicable also for other query types. Furthermore, this tutorial will provide a broad survey of the state-of-the-art in distributed skyline processing, present a categorization of the existing approaches based on their characteristics, and point out open research challenges in distributed skyline processing.

#index 1798428
#* Indexing and mining topological patterns for drug discovery
#@ Sayan Ranu;Ambuj K. Singh
#t 2012
#c 8
#% 378391
#% 466644
#% 478274
#% 629603
#% 629708
#% 727845
#% 745514
#% 765429
#% 810072
#% 864425
#% 864475
#% 915228
#% 960305
#% 1022280
#% 1044450
#% 1063502
#% 1127380
#% 1207028
#% 1328111
#% 1328170
#% 1426575
#% 1426577
#! Increased availability of large repositories of chemical compounds has created new challenges and opportunities for the application of data-mining and indexing techniques to problems in chemical informatics. The primary goal in analysis of molecular databases is to identify structural patterns that can predict biological activity. Two of the most popular approaches to representing molecular topologies are graphs and 3D geometries. As a result, the problem of indexing and mining structural patterns map to indexing and mining patterns from graph and 3D geometric databases. In this tutorial, we will first introduce the problem of drug discovery and how computer science plays a critical role in that process. We will then proceed by introducing the problem of performing subgraph and similarity searches on large graph databases. Due to the NP-hardness of the problems, a number of heuristics have been designed in recent years and the tutorial will present an overview of those techniques. Next, we will introduce the problem of mining frequent subgraph patterns along with some of their limitations that ignited the interest in the problem of mining statistically significant subgraph patterns. After presenting an in-depth survey of the techniques on mining significant subgraph patterns, the tutorial will proceed towards the problem of analyzing 3D geometric structures of molecules. Finally, we will conclude by presenting two open computer science problems that can have a significant impact in the field of drug discovery.

#index 1798429
#* Adaptive indexing in modern database kernels
#@ Stratos Idreos;Stefan Manegold;Goetz Graefe
#t 2012
#c 8
#% 36119
#% 64791
#% 463917
#% 482100
#% 544469
#% 875062
#% 893130
#% 960268
#% 997495
#% 1016220
#% 1022202
#% 1207102
#% 1217169
#% 1372713
#% 1545227
#% 1592316
#! Physical design represents one of the hardest problems for database management systems. Without proper tuning, systems cannot achieve good performance. Offline indexing creates indexes a priori assuming good workload knowledge and idle time. More recently, online indexing monitors the workload trends and creates or drops indexes online. Adaptive indexing takes another step towards completely automating the tuning process of a database system, by enabling incremental and partial online indexing. The main idea is that physical design changes continuously, adaptively, partially, incrementally and on demand while processing queries as part of the execution operators. As such it brings a plethora of opportunities for rethinking and improving every single corner of database system design. We will analyze the indexing space between offline, online and adaptive indexing through several state of the art indexing techniques, e. g., what-if analysis and soft indexes. We will discuss in detail adaptive indexing techniques such as database cracking, adaptive merging, sideways cracking and various hybrids that try to balance the online tuning overhead with the convergence speed to optimal performance. In addition, we will discuss how various aspects of modern techniques for database architectures, such as vectorization, bulk processing, column-store execution and storage affect adaptive indexing. Finally, we will discuss several open research topics towards fully automomous database kernels.

#index 1798430
#* A probabilistic convex hull query tool
#@ Zhou Zhao;Da Yan;Wilfred Ng
#t 2012
#c 8
#% 2115
#% 300180
#% 420078
#% 462059
#% 992830
#% 1022203
#! Uncertain data is inherently important in a lot of real-world applications, such as environmental surveillance and mobile tracking. Probabilistic convex hull is very useful for discovering the territory of imprecise data in such applications with a high confidence. In order to deal with this, we propose and study probabilistic convex hull queries based on the possible world semantics, which are able to retrieve the objects whose probability of being on the convex hull is at least α. The demonstration is based on animal tracking whose GPS coordinate is no longer considered to be precise due to device limitation or privacy issues. We demonstrate two interesting results from studying the migration habit of one specific species and the correlation between species through probabilistic convex hull queries.

#index 1798431
#* ColisTrack: testbed for a pervasive environment management system
#@ Yann Gripay;Frédérique Laforest;Francois Lesueur;Nicolas Lumineau;Jean-Marc Petit;Vasile-Marian Scuturici;Samir Sebahi;Sabina Surdu
#t 2012
#c 8
#% 1016169
#% 1372712
#% 1546748
#% 1551286
#! One of the leading challenges for pervasive computing is to ease the application development to smoothly handle the surrounding environment. We consider the case where the environment produces heterogeneous and continuous data, e. g. temperature readings, car positions... We have defined a scenario for containers transportation tracking in a medical context involving the transportation of fragile biological matter in sensor-enhanced containers. This scenario has been simulated as a testbed and offers a very nice setting to measure the agility of data-centric application development. On top of this scenario, we have built a pervasive application using a Pervasive Environment Management System called SoCQ (Service oriented Continuous Queries). SoCQ provides a data-oriented perspective of the pervasive environment, mixing classical data, streams and functionalities. For the demo, our objective is twofold: first, from the application developer point of view, she has access to the underlying SoCQ-schema and she may pose her own SQL-like queries to the simulated environment. Second, from the end-user point of view, she may quite easily interact with the environment either through a general dynamic visualization with Google Maps of hospitals, cars moving along roads and medical containers waiting or being transported, or by getting SMS notifications on her own phone of results of predefined queries.

#index 1798432
#* The mainframe strikes back: elastic multi-tenancy using main memory database systems on a many-core server
#@ Henrik Mühe;Alfons Kemper;Thomas Neumann
#t 2012
#c 8
#% 1127596
#% 1581459
#% 1592312
#% 1594597
#% 1594617
#! Contrary to recent trends in database systems research focussing on scaling out workloads on a cluster of commodity computers, this demo will break grounds for scale-up. We show that an elastic multi-tenancy solution can be achieved by combining a many-core server with a low footprint main memory database system. Total transactional throughput for TPC-C like order-entry transactions reaches up to 2 million transactions per second on a 32 core server while the number of tenants sharing a single server can be varied from a few to hundreds of separate tenants without diminishing total throughput. Contrary to common belief, a scale-up solution provides high flexibility for tenants with growing throughput needs and allows for simple sharing of common resources between different tenants while minimizing hardware and computing overhead. We show that our approach can handle changes in tenant requirements with minimal impact on other tenants on the server. Additionally, we prove that our architecture provides sufficient per-tenant throughput to handle big tenants and scales well with database size.

#index 1798433
#* SOS (save our systems): a uniform programming interface for non-relational systems
#@ Paolo Atzeni;Francesca Bugiotti;Luca Rossi
#t 2012
#c 8
#% 291299
#% 319216
#% 960233
#% 1022298
#% 1054227
#% 1126564
#% 1127571
#% 1181236
#% 1551289
#% 1573168
#% 1573340
#% 1680216
#! The recent growth of non-relational databases (often termed as NoSQL) is an interesting phenomenon that has generated both interest and criticism. One of the major drawbacks that is often referred to is the heterogeneity of the languages and interfaces they offer to developers and users. SOS is proposed as a common interface to them, in order to support application development by hiding the specific details of the various systems. It is based on a metamodeling approach, in the sense that the specific interfaces of the various systems are mapped to a common one. The tool provides interoperability as well, since a single application can interact with several systems at the same time. The demonstration will focus on a simple yet powerful application scenario which accesses three different NoSQL systems.

#index 1798434
#* PeerTrack: a platform for tracking and tracing objects in large-scale traceability networks
#@ Yanbo Wu;Quan Z. Sheng;Damith Ranasinghe;Lina Yao
#t 2012
#c 8
#% 340175
#% 745401
#% 915814
#% 1120909
#% 1164052
#% 1328146
#% 1618944
#% 1644788
#% 1668029
#! The ability to track and trace individual items, especially through large-scale and distributed networks, is the key to realizing many important business applications such as supply chain management, asset tracking, and counterfeit detection. Unfortunately, enabling traceability across independent organizations still poses significant challenges in dealing with large volume of data and sovereignty of the participants. This paper describes PeerTrack, a scalable platform for efficiently and effectively tracking and tracing objects in large-scale traceability networks. With a novel data model, a DHT-based indexer, and a distributed query processor, PeerTrack provides an environment where traceability applications can share data across independent organizations in a peer-to-peer fashion. This paper presents the motivation, system design, implementation, and a proof-of-concept system of the PeerTrack platform.

#index 1798435
#* Fault-tolerant complex event processing using customizable state machine-based operators
#@ Thomas Heinze;Zbigniew Jerzak;André Martin;Lenar Yazdanov;Christof Fetzer
#t 2012
#c 8
#% 1120943
#% 1213386
#% 1281501
#% 1591805
#% 1601104
#% 1660161
#! Modern Complex Event Processing (CEP) systems often need an high degree of customization in order to implement required application logic. The use of declarative languages, such as CQL, often leads to complicated and hard to maintain application code. In this demo, we show how state machine-based CEP operators help to cope with these problems. State machine-based CEP operators allow for a high flexibility as well as a re-usability of application logic components. A major benefit of the presented solution is its easy integration with existing streaming engines, which we demonstrate using StreamMine, a highly parallel and faulttolerant streaming engine prototype. In this demo we show: (1) how state machine-based operators allow for an easy definition of custom, reusable CEP operators, (2) how resulting state machines can be easily combined with existing faulttolerance techniques within StreamMine and (3) how the resulting CEP applications can be tested in a cost efficient way.

#index 1798436
#* Knowledge-based processing of complex stock market events
#@ Kia Teymourian;Malte Rohde;Adrian Paschke
#t 2012
#c 8
#% 177755
#% 654510
#% 1063480
#% 1127357
#% 1136037
#% 1209764
#% 1269337
#% 1333295
#% 1396542
#% 1591800
#% 1688281
#! Usage of background knowledge about events and their relations to other concepts in the application domain, can improve the quality of event processing. In this paper, we describe a system for knowledge-based event detection of complex stock market events based on available background knowledge about stock market companies. Our system profits from data fusion of live event stream and background knowledge about companies which is stored in a knowledge base. Users of our system can express their queries in a rule language which provides functionalities to specify semantic queries about companies in the SPARQL query language for querying the external knowledge base and combine it with event data stream. Background makes it possible to detect stock market events based on companies attributes and not only based on syntactic processing of stock price and volume.

#index 1798437
#* Private-HERMES: a benchmark framework for privacy-preserving mobility data querying and mining methods
#@ Nikos Pelekis;Aris Gkoulalas-Divanis;Marios Vodas;Anargyros Plemenos;Despina Kopanaki;Yannis Theodoridis
#t 2012
#c 8
#% 480473
#% 907184
#% 960283
#% 989604
#% 1046207
#% 1063572
#% 1092320
#% 1135184
#% 1206713
#% 1456653
#% 1496772
#% 1592778
#% 1642017
#% 1763515
#! Mobility data sources feed larger and larger trajectory databases nowadays. Due to the need of extracting useful knowledge patterns that improve services based on users' and customers' behavior, querying and mining such databases has gained significant attention in recent years. However, publishing mobility data may lead to severe privacy violations. In this paper, we present Private-HERMES, an integrated platform for applying data mining and privacy-preserving querying over mobility data. The presented platform provides a two-dimension benchmark framework that includes: (i) a query engine that provides privacy-aware data management functionality of the in-house data via a set of auditing mechanisms that protect the sensitive information against several types of attacks, and (ii) a progressive analysis framework, which, apart from anonymization methods for data publishing, includes various well-known mobility data mining techniques to evaluate the effect of anonymization in the querying and mining results. The demonstration of Private-HERMES via a real-world case study, illustrates the flexibility and usefulness of the platform for supporting privacy-aware data analysis, as well as for providing an extensible blueprint benchmark architecture for privacy-preservation related methods in mobility data.

#index 1798438
#* Evaluating hybrid queries through service coordination in HYPATIA
#@ Víctor Cuevas-Vicenttín;Genoveva Vargas-Solar;Christine Collet
#t 2012
#c 8
#% 210761
#% 654485
#% 1372712
#% 1546748
#! The emergence of mobile and ambient computing technologies brings democratization in the access to information and data; services play a crucial role, thereby opening new research challenges for data querying. A promising method to access data within these novel dynamic environments in a convenient and efficient way is declarative querying. Such queries, which we characterize as hybrid queries, involve streaming and on-demand data originated from services, possibly with temporal and mobile properties. To evaluate these queries we propose an approach implemented in the HYPATIA system, which addresses two main aspects (i) using service coordination for query evaluation and (ii) an efficient and flexible mechanism that facilitates incorporating new capabilities.

#index 1798439
#* A desktop interface over distributed document repositories
#@ Camelia Constantin;Cédric du Mouza;Philippe Rigaux;Virginie Thion-Goasdoué;Nicolas Travers
#t 2012
#c 8
#% 384978
#% 1350173
#% 1581842
#% 1730551
#! The demonstration is devoted to the desktop-level interactions offered by Cador, a content-based document management system currently under development. Cador provides a rule-based language to query and manipulate large collections of documents distributed in repositories. The language is able to define the content of Virtual File Systems (VFS) as views over the document collections. This feature allows users to combine their familiar interface and desktop-based softwares with the powerful search and transformation tools provided by the underlying system. The demonstration shows how VFS views can be created on-demand to present a desktop-based virtual document organization and how standard desktop interactions can be captured and interpreted in terms of document management operations: creation, updates, annotation, derivation of new content thanks to transformation rules, sharing between users, etc. The example application is the management of a large bibliographic database: users can, with a few clicks, organize their bibliographic references, import new references, share them with a group of co-authors and automatically maintain a ready-to-use Bibtex file.

#index 1798440
#* SPARQL-RW: transparent query access over mapped RDF data sources
#@ Konstantinos Makris;Nikos Bikakis;Nektarios Gioldasis;Stavros Christodoulakis
#t 2012
#c 8
#% 529190
#% 1223424
#% 1267393
#% 1330784
#% 1333447
#% 1384841
#% 1409508
#% 1409510
#% 1413162
#% 1528093
#% 1560420
#% 1641515
#% 1651528
#% 1654043
#! The Web of Data is an open environment consisting of very large, inter-linked RDF datasets from various domains (e.g., DBpedia, GeoNames, ACM, PubMed, etc.) accessed through SPARQL queries. Establishing interoperability in this environment has become a major research challenge. This paper presents Sparql--Rw (SPARQL--ReWriting), a framework which provides transparent query access over mapped RDF datasets. The Sparql--Rw provides a generic method for SPARQL query rewriting, with respect to a set of predefined mappings between ontology schemas. To this end, it supports a set of rich and flexible mapping types and it is proved to provide semantics preserving queries.

#index 1798441
#* Intention insider: discovering people's intentions in the social channel
#@ Malu Castellanos;Meichun Hsu;Umeshwar Dayal;Riddhiman Ghosh;Mohamed Dekhil;Carlos Ceja;Marcial Puchi;Perla Ruiz
#t 2012
#c 8
#% 1127964
#% 1581935
#! The rapid proliferation of online forums has made it possible for people to share their intentions, wishes and experiences by posting comments with the aim of getting advice from other members of the forum. Extracting intentions from these comments provides valuable insight for companies who can exploit it to get a competitive edge. However, given the very large amount of this kind of online comments, manually extracting intentions is impractical, time consuming and expensive. Companies need tools that analyze the text to extract intentions and details about them. In this paper we propose to demo one such tool called Intention Insider which has been developed at HP Labs in close collaboration with business units and a few selected customers. The tool can ingest content from online forums or from uploaded files and quickly sift through very large amounts of comments to extract intention information. This information is loaded into a data warehouse to be correlated with other structured data and queried to produce interactive reports and dynamic visualizations that facilitate its exploration at detailed and aggregate levels.

#index 1798442
#* QUASAR: querying annotation, structure, and reasoning
#@ Luying Chen;Michael Benedikt;Evgeny Kharlamov
#t 2012
#c 8
#% 979743
#% 1374367
#% 1426625
#% 1523943
#! An increasing number of systems provide the ability to semantically annotate documents. OpenCalais [4], Evri API [2], Zemanta [6], and Alchemy API [1] are web-hosted systems that return annotated documents, i. e. documents with annotations that are overlayed on the document structure. Many of the annotations can be linked to standard ontologies, such as DBpedia and YAGO. These annotations give insight as to the meaning of documents in a variety of ways, identifying entities and relationships inside them, classifying them according to topic or theme, and giving the attitude or sentiment of a document or document fragment. In order for users (or applications) to make use of these annotations with a means to access and manipulate documents that contain them, we provide a query language for doing this and demonstrate its utility on a demo system built on top of diverse semantic annotators and external ontologies. We explain how integrating semantic annotations and utilizing external knowledge helps in increasing the quality of query answers over annotated documents by both filtering out irrelevant answers and obtaining extra answers that are not explicitly available in the annotated documents.

#index 1798443
#* Realtime healthcare services via nested complex event processing technology
#@ Mo Liu;Medhabi Ray;Dazhi Zhang;Elke A. Rundensteiner;Daniel J. Dougherty;Chetan Gupta;Song Wang;Ismail Ari
#t 2012
#c 8
#% 461897
#% 875004
#% 1217161
#% 1470725
#% 1523936
#% 1581920
#% 1594604
#! Complex Event Processing (CEP) over event streams has become increasingly important for real-time applications ranging from healthcare to supply chain management. In such applications, arbitrarily complex sequence patterns as well as non existence of such complex situations must be detected in real time. To assure real-time responsiveness for detection of such complex pattern over high volume high-speed streams, efficient processing techniques must be designed. Unfortunately the efficient processing of complex sequence queries with negations remains a largely open problem to date. To tackle this shortcoming, we designed optimized strategies for handling nested CEP query. In this demonstration, we propose to showcase these techniques for processing and optimizing nested pattern queries on streams. In particular our demonstration showcases a platform for specifying complex nested queries, and selecting one of the alternative optimized techniques including sub-expression sharing and intermediate result caching to process them. We demonstrate the efficiency of our optimized strategies by graphically comparing the execution time of the optimized solution against that of the default processing strategy of nested CEP queries. We also demonstrate the usage of the proposed technology in several healthcare services.

#index 1798444
#* Distributed data management for large-scale wireless sensor networks simulations
#@ Stephen Wylie;James Heide;Besim Avci;Dennis D. Vaccaro;Oliviu Ghica;Goce Trajcevski
#t 2012
#c 8
#% 250311
#% 410574
#% 731096
#% 806214
#% 862588
#% 879282
#% 960200
#% 1083758
#% 1132755
#% 1480796
#! We tackle two important problems that arise in simulation-based studies of various data-related properties in the context of Wireless Sensor Networks (WSNs): (1) reducing the turnaround time for completing the simulations in a large-scale parameter space; (2) providing database functionalities for a more detailed insight into the simulation's evolution. Towards these goals, we have developed the DiSSIDnet (Distributed System for Simulation and Integrated Development for Wireless Sensor Networks). Leveraging upon our earlier works on the SIDnet-SWANS tool [3], DiSSIDnet not only provides the ability of a synchronized execution of the simulations in a distributed environment, but also maintains the simulation data over the parameter-space in a relational database. In addition to post-simulation queries that can be posed to the database, we also provide the feature of specifying triggers that can generate notifications upon detecting certain events of interest during the simulation process.

#index 1798445
#* Knowing: a generic data analysis application
#@ T. Bernecker;F. Graf;H.-P. Kriegel;N. Seiler;C. Türmer;D. Dill
#t 2012
#c 8
#% 881575
#% 1230848
#% 1301004
#% 1472282
#! Extracting knowledge from data is, in most cases, not restricted to the analysis itself but accompanied by preparation and post-processing steps. Handling data coming directly from the source, e. g. a sensor, often requires preconditioning like parsing and removing irrelevant information before data mining algorithms can be applied to analyze the data. Stand-alone data mining frameworks in general do not provide such components since they require a specified input data format. Furthermore, they are often restricted to the available algorithms or a rapid integration of new algorithms for the purpose of quick testing is not possible. To address this shortcoming, we present the data analysis framework Knowing, which is easily extendible with additional algorithms by using an OSGi compliant architecture. In this demonstration, we apply the Knowing framework to a medical monitoring system recording physical activity. We use the data of 3D accelerometers to detect activities and perform data mining techniques and motion detection to classify and evaluate the quality and amount of physical activities. In the presented use case, patients and physicians can analyze the daily activity processes and perform long term data analysis by using an aggregated view of the results of the data mining process. Developers can integrate and evaluate newly developed algorithms and methods for data mining on the recorded database.

#index 1962312
#* Invisible loading: access-driven data transfer from raw files into database systems
#@ Azza Abouzied;Daniel J. Abadi;Avi Silberschatz
#t 2013
#c 8
#% 954300
#% 1022202
#% 1063553
#% 1127411
#% 1127559
#% 1217159
#% 1217169
#% 1328186
#% 1333820
#% 1372713
#% 1386048
#% 1471595
#% 1770339
#! Commercial analytical database systems suffer from a high "time-to-first-analysis": before data can be processed, it must be modeled and schematized (a human effort), transferred into the database's storage layer, and optionally clustered and indexed (a computational effort). For many types of structured data, this upfront effort is unjustifiable, so the data are processed directly over the file system using the Hadoop framework, despite the cumulative performance benefits of processing this data in an analytical database system. In this paper we describe a system that achieves the immediate gratification of running MapReduce jobs directly over a file system, while still making progress towards the long-term performance benefits of database systems. The basic idea is to piggyback on MapReduce jobs, leverage their parsing and tuple extraction operations to incrementally load and organize tuples into a database system, while simultaneously processing the file system data. We call this scheme Invisible Loading, as we load fractions of data at a time at almost no marginal cost in query latency, but still allow future queries to run much faster.

#index 1962313
#* History repeats itself: sensible and NonsenSQL aspects of the NoSQL hoopla
#@ C. Mohan
#t 2013
#c 8
#% 479983
#% 480313
#! In this paper, I describe some of the recent developments in the database management area, in particular the NoSQL phenomenon and the hoopla associated with it. The goal of the paper is not to do an exhaustive survey of NoSQL systems. The aim is to do a broad brush analysis of what these developments mean - the good and the bad aspects! Based on my more than three decades of database systems work in the research and product arenas, I will outline what are many of the pitfalls to avoid since there is currently a mad rush to develop and adopt a plethora of NoSQL systems in a segment of the IT population, including the research community. In rushing to develop these systems to overcome some of the shortcomings of the relational systems, many good principles of the latter, which go beyond the relational model and the SQL language, have been left by the wayside. Now many of the features that were initially discarded as unnecessary in the NoSQL systems are being brought in, but unfortunately in ad hoc ways. Hopefully, the lessons learnt over three decades with relational and other systems would not go to waste and we wouldn't let history repeat itself with respect to simple minded approaches leading to enormous pain later on for developers as well as users of the NoSQL systems! Caveat: What I express in this paper are my personal opinions and they do not necessarily reflect the opinions of my employer.

#index 1962314
#* From A to E: analyzing TPC's OLTP benchmarks: the obsolete, the ubiquitous, the unexplored
#@ Pınar Tözün;Ippokratis Pandis;Cansu Kaynak;Djordje Jevdjic;Anastasia Ailamaki
#t 2013
#c 8
#% 251473
#% 251474
#% 262154
#% 451767
#% 480119
#% 824657
#% 896770
#% 896780
#% 1022298
#% 1181215
#% 1328149
#% 1480466
#% 1523856
#% 1523878
#% 1542524
#% 1606343
#% 1668635
#% 1716248
#% 1871522
#% 1880460
#% 1967110
#! Introduced in 2007, TPC-E is the most recently standardized OLTP benchmark by TPC. Even though TPC-E has already been around for six years, it has not gained the popularity of its predecessor TPC-C: all the published results for TPC-E use a single database vendor's product. TPC-E is significantly different than its predecessors. Some of its distinguishing characteristics are the non-uniform input creation, longer-running and more complicated transactions, more difficult partitioning etc. These factors slow down the adoption of TPC-E. In turn, there is little knowledge in the community about how TPC-E behaves micro-architecturally and within the database engine. To shed light on TPC-E, we implement it on top of a scalable open-source database engine, Shore-MT, and perform a workload characterization study, comparing it with the previous, much better known OLTP benchmarks of TPC: TPC-B and TPC-C. In parallel, we study the evolution of the OLTP benchmarks throughout the decades. Our results demonstrate that TPC-E exhibits similar micro-architectural behavior to TPC-B and TPC-C, even though it incurs less stall time and higher instructions per cycle. On the other hand, within the database engine it suffers more from logical lock contention. Therefore, we argue that, on the hardware side, TPC-E needs less aggressive processors. Whereas on the software side it can benefit from designs based on intra-transaction parallelism, logical partitioning, and optimistic concurrency control to minimize the effects of lock contention without introducing distributed transactions.

#index 1962315
#* Query-aware compression of join results
#@ Christopher M. Mullins;Lipyeow Lim;Christian A. Lang
#t 2013
#c 8
#% 330891
#% 380584
#% 383594
#% 463895
#% 464843
#% 632026
#% 875026
#% 893159
#% 960266
#% 1328064
#% 1328108
#% 1643518
#% 1646689
#! Client-server database query processing has become an important paradigm in many data processing applications today. In cloud-based data services, for example, queries over structured data are sent to cloud-based servers for processing and the results relayed back to the client devices. Network bandwidth between client devices and cloud-based servers is often a limited resource and the use of data compression to reduce the amount of query result data transmitted would not only conserve bandwidth but also help with battery lifetime in the case of mobile client devices. For query result compression, current data compression methods do not exploit redundancy information that can be inferred from the query structure itself for greater compression. In this paper we propose a novel query-aware compression method for compressing query results sent from database servers to client applications. Our method is based on two key ideas. We exploit redundancy information obtained from the query plan and possibly from the database schema to achieve better compression than standard non-query aware compressors. We use a collection of memory-limited dictionaries to encode attribute values in a lightweight and efficient manner. Each dictionary in the collection of dictionaries are also dynamically resized to adapt to changing temporal access characteristics. We evaluated our method empirically using the TPC-H benchmark show that this technique is effective especially when used in conjunction with standard compressors. Our results show that compression ratios of up to twice that of gzip are possible.

#index 1962316
#* Web data indexing in the cloud: efficiency and cost reductions
#@ Jesús Camacho-Rodríguez;Dario Colazzo;Ioana Manolescu
#t 2013
#c 8
#% 397360
#% 397375
#% 464883
#% 479465
#% 824794
#% 864465
#% 994015
#% 1015327
#% 1063488
#% 1206796
#% 1426550
#% 1523836
#% 1581873
#% 1594601
#% 1594649
#% 1602036
#% 1901452
#% 1920038
#! An increasing part of the world's data is either shared through the Web or directly produced through and for Web platforms, in particular using structured formats like XML or JSON. Cloud platforms are interesting candidates to handle large data repositories, due to their elastic scaling properties. Popular commercial clouds provide a variety of sub-systems and primitives for storing data in specific formats (files, key-value pairs etc.) as well as dedicated sub-systems for running and coordinating execution within the cloud. We propose an architecture for warehousing large-scale Web data, in particular XML, in a commercial cloud platform, specifically, Amazon Web Services. Since cloud users support monetary costs directly connected to their consumption of cloud resources, we focus on indexing content in the cloud. We study the applicability of several indexing strategies, and show that they lead not only to reducing query evaluation time, but also, importantly, to reducing the monetary costs associated with the exploitation of the cloud-based warehouse. Our architecture can be easily adapted to similar cloud-based complex data warehousing settings, carrying over the benefits of access path selection in the cloud.

#index 1962317
#* ProRea: live database migration for multi-tenant RDBMS with snapshot isolation
#@ Oliver Schiller;Nazario Cipriani;Bernhard Mitschang
#t 2013
#c 8
#% 201869
#% 893146
#% 1063524
#% 1063541
#% 1063561
#% 1202159
#% 1217217
#% 1424590
#% 1549846
#% 1581871
#% 1587695
#% 1592341
#% 1798412
#% 1895059
#% 1895065
#% 1901412
#! The consolidation of multiple tenants onto a single RDBMS instance turned out to be benefical with respect to resource utilization and scalability. The consolidation implies that multiple tenants share the physical resources available for the RDBMS instance. If the available resources tend to get insufficient to meet the SLAs agreed with the tenants, migration of a tenant's database from one RDBMS instance to another is compelling. Highly available services demand for live migration techniques that come with minimal service interruption and low performance impact. This paper meets the demand for live migration techniques by contributing ProRea. ProRea is a live database migration approach designed for multi-tenant RDBMS that run OLTP workloads under snapshot isolation. ProRea extends concepts of existing live database migration approaches to accomplish minimal service interruption, high efficiency and very low migration overhead. Measurements of a prototypical ProRea implementation underpin its good performance.

#index 1962318
#* SWAT: a lightweight load balancing method for multitenant databases
#@ Hyun Jin Moon;Hakan Hacıgümüş;Yun Chi;Wang-Pin Hsiung
#t 2013
#c 8
#% 978779
#% 1063561
#% 1142445
#% 1207027
#% 1213303
#% 1217216
#% 1381470
#% 1426499
#% 1426589
#% 1463413
#% 1581871
#% 1581872
#% 1594596
#% 1594649
#% 1594650
#! Multitenant databases achieve cost efficiency through the consolidation of multiple small tenants. However, performance isolation is an inherent problem in multitenant databases due to resource sharing among the tenants. That is, a bursty workload from a co-located tenant, i.e., a noisy neighbor, may affect the performance of the other tenants sharing the same system resources. We address this issue by using a load balancing method that is based on database replica swap. Unlike the traditional data migration-based load balancing, replica swap-based load balancing does not incur data movement, which makes it highly resource- and time-efficient. We propose a novel method of choosing which tenants should be subject to swaps. Our experimental results show that swap-based load balancing effectively reduces the number of SLA violations, which is the main performance metric we choose.

#index 1962319
#* CloudOptimizer: multi-tenancy for I/O-bound OLAP workloads
#@ Hatem A. Mahmoud;Hyun Jin Moon;Yun Chi;Hakan Hacıgümüş;Divyakant Agrawal;Amr El-Abbadi
#t 2013
#c 8
#% 244119
#% 282460
#% 824701
#% 874980
#% 1052068
#% 1063541
#% 1063561
#% 1207027
#% 1487285
#% 1581872
#% 1594596
#% 1594650
#% 1621129
#! Consolidation of multiple databases on the same server allows service providers to save significant resources because many production database servers are often under-utilized. Recent research investigates the problem of minimizing the number of servers required to host a set of tenants when the working sets of tenants are kept in main memory (e.g., in-memory OLAP workloads, or OLTP workloads), thus the memory assigned to each tenant, as well as the I/O bandwidth and CPU time, are all dictated by the working set size of the tenant. Other research investigates the reverse problem when the number of servers is fixed, but the amount of resources allocated to different tenants on the same server needs to be configured to optimize a cost function. In this paper we investigate the problem when neither the number of servers nor the amount of resources allocated to each tenant are fixed. This problem arises when consolidating OLAP workloads of tenants whose service-level agreements (SLAs) allow for queries to be answered from disk. We study the trade-off between the amount of memory and the I/O bandwidth assigned to OLAP workloads, and develop a principled approach for allocating resources to tenants in a manner that minimizes the total number of servers required to host all tenants while satisfying the SLA of each tenant. We then explain how we modified InnoDB, the storage engine of MySQL, to be able to change the amount of resources allocated to each tenant at runtime, so as to account for fluctuations in workloads. Finally, we evaluate our approach experimentally using the TPC-H benchmark to demonstrate its effectiveness and accuracy.

#index 1962320
#* Eagle-eyed elephant: split-oriented indexing in Hadoop
#@ Mohamed Y. Eltabakh;Fatma Özcan;Yannis Sismanis;Peter J. Haas;Hamid Pirahesh;Jan Vondrak
#t 2013
#c 8
#% 271614
#% 479630
#% 729343
#% 960317
#% 963669
#% 1063553
#% 1278123
#% 1328095
#% 1328186
#% 1372690
#% 1426543
#% 1426584
#% 1523837
#% 1523841
#% 1567949
#% 1581925
#% 1581926
#% 1586685
#% 1592315
#% 1872963
#% 1876163
#% 1880472
#! An increasingly important analytics scenario for Hadoop involves multiple (often ad hoc) grouping and aggregation queries with selection predicates over a slowly changing dataset. These queries are typically expressed via high-level query languages such as Jaql, Pig, and Hive, and are used either directly for business-intelligence applications or to prepare the data for statistical model building and machine learning. In such scenarios it has been increasingly recognized that, as in classical databases, techniques for avoiding access to irrelevant data can dramatically improve query performance. Prior work on Hadoop, however, has simply ported classical techniques to the MapReduce setting, focusing on record-level indexing and key-based partition elimination. Unfortunately, record-level indexing only slightly improves overall query performance, because it does not minimize the number of mapper "waves", which is determined by the number of processed splits. Moreover, key-based partitioning requires data reorganization, which is usually impractical in Hadoop settings. We therefore need to re-envision how data access mechanisms are defined and implemented. To this end, we introduce the Eagle-Eyed Elephant (E3) framework for boosting the efficiency of query processing in Hadoop by avoiding accesses of data splits that are irrelevant to the query at hand. Using novel techniques involving inverted indexes over splits, domain segmentation, materialized views, and adaptive caching, E3 avoids accessing irrelevant splits even in the face of evolving workloads and data. Our experiments show that E3 can achieve up to 20x cost savings with small to moderate storage overheads.

#index 1962321
#* Computing n-gram statistics in MapReduce
#@ Klaus Berberich;Srikanta Bedathur
#t 2013
#c 8
#% 290703
#% 329537
#% 329600
#% 379325
#% 459006
#% 463903
#% 729418
#% 741122
#% 769620
#% 778732
#% 867053
#% 944350
#% 963669
#% 985041
#% 1127463
#% 1166534
#% 1227596
#% 1354118
#% 1355060
#% 1399956
#% 1414372
#% 1442076
#% 1467704
#% 1468142
#% 1536517
#% 1581411
#% 1581925
#% 1591914
#% 1710556
#! Statistics about n-grams (i.e., sequences of contiguous words or other tokens in text documents or other string data) are an important building block in information retrieval and natural language processing. In this work, we study how n-gram statistics, optionally restricted by a maximum n-gram length and minimum collection frequency, can be computed efficiently harnessing MapReduce for distributed data processing. We describe different algorithms, ranging from an extension of word counting, via methods based on the Apriori principle, to a novel method Suffix-σ that relies on sorting and aggregating suffixes. We examine possible extensions of our method to support the notions of maximality/closedness and to perform aggregations beyond occurrence counting. Assuming Hadoop as a concrete Map-Reduce implementation, we provide insights on an efficient implementation of the methods. Extensive experiments on The New York Times Annotated Corpus and ClueWeb09 expose the relative benefits and trade-offs of the methods.

#index 1962322
#* Processing multi-way spatial joins on map-reduce
#@ Himanshu Gupta;Bhupesh Chawda;Sumit Negi;Tanveer A. Faruquie;L. V. Subramaniam;Mukesh Mohania
#t 2013
#c 8
#% 210186
#% 210187
#% 273886
#% 316942
#% 342956
#% 458866
#% 464205
#% 1023420
#% 1282271
#% 1372690
#% 1426543
#% 1426584
#% 1535976
#% 1581925
#% 1869836
#! In this paper we investigate the problem of processing multi-way spatial joins on map-reduce platform. We look at two common spatial predicates - overlap and range. We address these two classes of join queries, discuss the challenges and outline novel approaches for executing these queries on a map-reduce framework. We then discuss how we can process join queries involving both overlap and range predicates. Specifically we present a Controlled-Replicate framework using which we design the approaches presented in this paper. The Controlled-Replicate framework is carefully engineered to minimize the communication among cluster nodes. Through experimental evaluations we discuss the complexity of the problem under investigation, details of Controlled-Replicate framework and demonstrate that the proposed approaches comfortably outperform naive approaches.

#index 1962323
#* Rapid experimentation for testing and tuning a production database deployment
#@ Nedyalko Borisov;Shivnath Babu
#t 2013
#c 8
#% 824700
#% 1063558
#% 1213308
#% 1328192
#% 1328213
#% 1453007
#% 1468504
#% 1567923
#% 1581869
#% 1581871
#! The need to perform testing and tuning of database instances with production-like workloads (W), configurations (C), data (D), and resources (R) arises routinely. The further W, C, D, and R used in testing and tuning deviate from what is observed on the production database instance, the lower is the trustworthiness of the testing and tuning tasks done. For example, it is common to hear about performance degradation observed after the production database is upgraded from one software version to another. A typical cause of this problem is that the W, C, D, or R used during upgrade testing differed in some way from that on the production database. Performing testing and tuning tasks in principled and automated ways is very important, especially since---spurred by innovations in cloud computing---the number of database instances that a database administrator (DBA) has to manage is growing rapidly. We present Flex, a platform for trustworthy testing and tuning of production database instances. Flex gives DBAs a high-level language, called Slang, to specify definitions and objectives regarding running experiments for testing and tuning. Flex's orchestrator schedules and runs these experiments in an automated manner that meets the DBA-specified objectives. Flex has been fully prototyped. We present results from a comprehensive empirical evaluation that reveals the effectiveness of Flex on diverse problems such as upgrade testing, near-real-time testing to detect corruption of data, and server configuration tuning. We also report on our experiences taking some of the testing and tuning software described in the literature and porting them to run on the Flex platform.

#index 1962324
#* Towards context-aware search and analysis on social media data
#@ Leon R. A. Derczynski;Bin Yang;Christian S. Jensen
#t 2013
#c 8
#% 1523892
#% 1573369
#% 1594674
#% 1641649
#% 1770322
#% 1872363
#% 1876566
#% 1925639
#% 1940426
#% 1943522
#! Social media has changed the way we communicate. Social media data capture our social interactions and utterances in machine readable format. Searching and analysing massive and frequently updated social media data brings significant and diverse rewards across many different application domains, from politics and business to social science and epidemiology. A notable proportion of social media data comes with explicit or implicit spatial annotations, and almost all social media data has temporal metadata. We view social media data as a constant stream of data points, each containing text with spatial and temporal contexts. We identify challenges relevant to each context, which we intend to subject to context aware querying and analysis, specifically including longitudinal analyses on social media archives, spatial keyword search, local intent search, and spatio-temporal intent search. Finally, for each context, emerging applications and further avenues for investigation are discussed.

#index 1962325
#* Proactive natural language search engine: tapping into structured data on the web
#@ Wensheng Wu
#t 2013
#c 8
#% 452991
#% 765409
#% 960363
#% 1063547
#% 1127393
#% 1217235
#% 1275186
#% 1279761
#% 1399933
#% 1400017
#% 1426566
#% 1587400
#% 1922352
#! In this era of "big data", a key challenge facing the database community is to help average users tap into the huge amounts of structured data on the Web. To address this challenge, we propose a novel proactive template-based engine for searching structured data on the Web using natural language. Departing from conventional search engines, the proposed engine organizes questions it can answer using templates and figures out ahead of time which sources can answer which templates and how. Then, at query time, the engine can simply match queries with the templates and retrieve answers using the pre-compiled evaluation plans. While attractive, building such an engine requires innovations in template creation, query evaluation, and system evolution. In this paper, we propose novel techniques to address these challenges.

#index 1962326
#* Anomaly management using complex event processing: extending data base technology paper
#@ Bastian Hoßbach;Bernhard Seeger
#t 2013
#c 8
#% 344898
#% 883582
#% 917957
#% 960342
#% 1127569
#% 1202160
#% 1217161
#% 1269312
#% 1451081
#% 1460207
#% 1467748
#% 1523815
#% 1523854
#% 1523936
#% 1523975
#% 1591821
#% 1798169
#% 1798436
#% 1868767
#! During the last decade, complex event processing (CEP) has emerged as a technological foundation for many time-critical monitoring applications. CEP is powerful, effective, easy to use and low in costs at the same time. Common CEP applications are for example stock-market analysis, detection of fraudulent credit card use, traffic monitoring and consumption forecasting in power grids. Many application domains are still hard to target by CEP, because state of the art CEP technology is characterized by a static behavior and by a signature-based detection paradigm. In this paper, we motivate substantial improvements of CEP technology by making the behavior of the infrastructure dynamic and by switching the detection paradigm from signatures to anomalies. This leads to multiple changes in the infrastructure that raise interesting and challenging research questions. The resulting dynamic CEP infrastructure not only makes existing applications more powerful and easier to maintain but also enables novel application domains.

#index 1962327
#* Compromising privacy in precise query protocols
#@ Jonathan L. Dautrich, Jr.;Chinya V. Ravishankar
#t 2013
#c 8
#% 397367
#% 725292
#% 765448
#% 799891
#% 937550
#% 975827
#% 1193147
#% 1195803
#% 1299974
#% 1385864
#% 1390038
#% 1412517
#% 1414503
#% 1431619
#% 1543144
#% 1601075
#% 1601106
#% 1664117
#% 1706189
#% 1706207
#% 1726261
#% 1743678
#% 1745624
#% 1853528
#% 1882459
#! Privacy and security for outsourced databases are often provided by Precise Query Protocols (PQPs). In a PQP, records are individually encrypted by a client and stored on a server. The client issues encrypted queries, which are run under encryption at the server, and the server returns the exact set of encrypted tuples needed to satisfy the query. We propose a general attack against the privacy of all PQPs that support range queries, using query results to partially order encrypted records. Existing attacks that seek to order etuples are less powerful and depend on weaknesses specific to particular PQPs. Our novel algorithm identifies permissible positions (loci) for encrypted records by organizing range query results using PQ-trees. These results can then be used to infer attribute values of encrypted records. We propose equivocation and permutation entropy as privacy metrics, and give experimental results that show PQP privacy to be easily compromised by our attack.

#index 1962328
#* Efficient privacy-aware record integration
#@ Mehmet Kuzu;Murat Kantarcioglu;Ali Inan;Elisa Bertino;Elizabeth Durham;Bradley Malin
#t 2013
#c 8
#% 420072
#% 577289
#% 763581
#% 766200
#% 819551
#% 889625
#% 913783
#% 960288
#% 1196287
#% 1198205
#% 1206583
#% 1206749
#% 1206992
#% 1372692
#% 1372726
#% 1386180
#% 1615646
#% 1670071
#% 1740518
#% 1783676
#% 1846713
#% 1861495
#! The integration of information dispersed among multiple repositories is a crucial step for accurate data analysis in various domains. In support of this goal, it is critical to devise procedures for identifying similar records across distinct data sources. At the same time, to adhere to privacy regulations and policies, such procedures should protect the confidentiality of the individuals to whom the information corresponds. Various private record linkage (PRL) protocols have been proposed to achieve this goal, involving secure multi-party computation (SMC) and similarity preserving data transformation techniques. SMC methods provide secure and accurate solutions to the PRL problem, but are prohibitively expensive in practice, mainly due to excessive computational requirements. Data transformation techniques offer more practical solutions, but incur the cost of information leakage and false matches. In this paper, we introduce a novel model for practical PRL, which 1) affords controlled and limited information leakage, 2) avoids false matches resulting from data transformation. Initially, we partition the data sources into blocks to eliminate comparisons for records that are unlikely to match. Then, to identify matches, we apply an efficient SMC technique between the candidate record pairs. To enable efficiency and privacy, our model leaks a controlled amount of obfuscated data prior to the secure computations. Applied obfuscation relies on differential privacy which provides strong privacy guarantees against adversaries with arbitrary background knowledge. In addition, we illustrate the practical nature of our approach through an empirical analysis with data derived from public voter records.

#index 1962329
#* Updating outsourced anatomized private databases
#@ Ahmet Erhan Nergiz;Chris Clifton;Qutaibah M. Malluhi
#t 2013
#c 8
#% 397367
#% 443463
#% 576761
#% 725292
#% 765448
#% 881497
#% 893100
#% 937550
#% 960291
#% 975827
#% 982549
#% 1016189
#% 1022247
#% 1044457
#% 1127418
#% 1246162
#% 1404123
#% 1425698
#% 1594673
#% 1613750
#% 1706207
#% 1725659
#! We introduce operations to safely update an anatomized database. The result is a database where the view of the server satisfies standards such as k-anonymity or l-diversity, but the client is able to query and modify the original data. By exposing data where possible, the server can perform value-added services such as data analysis not possible with fully encrypted data, while still being unable to violate privacy constraints. Update is a key challenge with this model; naïve application of insertion and deletion operations reveals the actual data to the server. This paper shows how data can be safely inserted, deleted, and updated. The key ideas are that data is inserted or updated into an encrypted temporary table until enough data is available to safely decrypt, and that sensitive information of deleted tuples is left behind to ensure privacy of both deleted and undeleted individuals. This approach is proven effective in maintaining the privacy constraint against an adversarial server. The paper also gives empirical results on how much data remains encrypted, and the resulting quality of the server's (anatomized) view of the data, for various update and delete rates.

#index 1962330
#* Efficient and accurate strategies for differentially-private sliding window queries
#@ Jianneng Cao;Qian Xiao;Gabriel Ghinita;Ninghui Li;Elisa Bertino;Kian-Lee Tan
#t 2013
#c 8
#% 248030
#% 325683
#% 576110
#% 654448
#% 857252
#% 864412
#% 977011
#% 1061644
#% 1141473
#% 1190072
#% 1198225
#% 1214684
#% 1372692
#% 1426323
#% 1426454
#% 1426563
#% 1451190
#% 1484081
#% 1489408
#% 1523886
#% 1581865
#% 1730731
#% 1740518
#% 1846816
#! Regularly releasing the aggregate statistics about data streams in a privacy-preserving way not only serves valuable commercial and social purposes, but also protects the privacy of individuals. This problem has already been studied under differential privacy, but only for the case of a single continuous query that covers the entire time span, e.g., counting the number of tuples seen so far in the stream. However, most real-world applications are window-based, that is, they are interested in the statistical information about streaming data within a window, instead of the whole unbound stream. Furthermore, a Data Stream Management System (DSMS) may need to answer numerous correlated aggregated queries simultaneously, rather than a single one. To cope with these requirements, we study how to release differentially private answers for a set of sliding window aggregate queries. We propose two solutions, each consisting of query sampling and composition. We first selectively sample a subset of representative sliding window queries from the set of all the submitted ones. The representative queries are answered by adding Laplace noises in a way satisfying differential privacy. For each non-representative query, we compose its answer from the query results of those representatives. The experimental evaluation shows that our solutions are efficient and effective.

#index 1962331
#* An automatic physical design tool for clustered column-stores
#@ Alexander Rasin;Stan Zdonik
#t 2013
#c 8
#% 210182
#% 223781
#% 248815
#% 397390
#% 464706
#% 480158
#% 482100
#% 631950
#% 765176
#% 810026
#% 818916
#% 824697
#% 875026
#% 1016220
#% 1207101
#% 1426547
#% 1523893
#! Good database design is typically a very difficult and costly process. As database systems get more complex and as the amount of data under management grows, the stakes increase accordingly. Past research produced a number of design tools capable of automatically selecting secondary indexes and materialized views for a known workload. However, a significant bulk of research on automated database design has been done in the context of row-store DBMSes. While this work has produced effective design tools, new specialized database architectures demand a rethinking of automated design algorithms. In this paper, we present results for an automatic design tool that is aimed at column-oriented DBMSes on OLAP workloads. In particular, we have chosen a commercial column store DBMS that supports data sorting. In this setting, the key problem is selecting proper sort orders and compression schemes for the columns as well as appropriate pre-join views. This paper describes our automatic design algorithms as well as the results of some experiments using it on realistic data sets.

#index 1962332
#* Mining frequent serial episodes over uncertain sequence data
#@ Li Wan;Ling Chen;Chengqi Zhang
#t 2013
#c 8
#% 420063
#% 463903
#% 823402
#% 832572
#% 915307
#% 1083674
#% 1189215
#% 1206933
#% 1214624
#% 1214633
#% 1318641
#% 1393138
#% 1411089
#% 1451166
#% 1482221
#% 1535367
#% 1607952
#% 1772032
#% 1798381
#% 1880477
#! Data uncertainty has posed many unique challenges to nearly all types of data mining tasks, creating a need for uncertain data mining. In this paper, we focus on the particular task of mining probabilistic frequent serial episodes (P-FSEs) from uncertain sequence data, which applies to many real applications including sensor readings as well as customer purchase sequences. We first define the notion of P-FSEs, based on the frequentness probabilities of serial episodes under possible world semantics. To discover P-FSEs over an uncertain sequence, we propose: 1) an exact approach that computes the accurate frequentness probabilities of episodes; 2) an approximate approach that approximates the frequency of episodes using probability models; 3) an optimized approach that efficiently prunes a candidate episode by estimating an upper bound of its frequentness probability using approximation techniques. We conduct extensive experiments to evaluate the performance of the developed data mining algorithms. Our experimental results show that: 1) while existing research demonstrates that approximate approaches are orders of magnitudes faster than exact approaches, for P-FSE mining, the efficiency improvement of the approximate approach over the exact approach is marginal; 2) although it has been recognized that the normal distribution based approximation approach is fairly accurate when the data set is large enough, for P-FSE mining, the binomial distribution based approximation achieves higher accuracy when the the number of episode occurrences is limited; 3) the optimized approach clearly outperforms the other two approaches in terms of the runtime, and achieves very high accuracy.

#index 1962333
#* Efficient processing of containment queries on nested sets
#@ Ahmed Ibrahim;George H. L. Fletcher
#t 2013
#c 8
#% 290703
#% 300033
#% 313159
#% 333866
#% 333981
#% 411375
#% 480463
#% 569755
#% 654454
#% 662752
#% 726628
#% 765463
#% 765466
#% 838542
#% 907561
#% 949369
#% 983262
#% 1013630
#% 1051088
#% 1063553
#% 1206601
#% 1224936
#% 1474893
#% 1486652
#% 1523824
#% 1523853
#% 1549855
#% 1552657
#% 1731761
#% 1771493
#% 1910906
#% 1938487
#! We study the problem of computing containment queries on sets which can have both atomic and set-valued objects as elements, i.e., nested sets. Containment is a fundamental query pattern with many basic applications. Our study of nested set containment is motivated by the ubiquity of nested data in practice, e.g., in XML and JSON data management, in business and scientific workflow management, and in web analytics. Furthermore, there are to our knowledge no known efficient solutions to computing containment queries on massive collections of nested sets. Our specific contributions in this paper are: (1) we introduce two novel algorithms for efficient evaluation of containment queries on massive collections of nested sets; (2) we study caching and filtering mechanisms to accelerate query processing in the algorithms; (3) we develop extensions to the algorithms to a) compute several related query types and b) accommodate natural variations of the semantics of containment; and, (4) we present analytic and empirical analyses which demonstrate that both algorithms are efficient and scalable.

#index 1962334
#* Inferential time-decaying Bloom filters
#@ Jonathan L. Dautrich, Jr.;Chinya V. Ravishankar
#t 2013
#c 8
#% 307424
#% 629109
#% 657753
#% 825480
#% 874972
#% 887999
#% 1056330
#% 1114743
#% 1151252
#% 1248210
#% 1460166
#% 1464321
#% 1498912
#% 1555042
#% 1576671
#% 1639423
#% 1641964
#! Time-Decaying Bloom Filters are efficient, probabilistic data structures used to answer queries on recently inserted items. As new items are inserted, memory of older items decays. Incorrect query responses incur penalties borne by the application using the filter. Most existing filters may only be tuned to static penalties, and they ignore Bayesian priors and information latent in the filter. We address these issues in an integrated way by converting existing filters into inferential filters. Inferential filters combine latent filter information with Bayesian priors to make query-specific optimal decisions. Our methods are applicable to any Bloom Filter, but we focus on developing inferential time-decaying filters, which support new query types and sliding window queries with varying error penalties. We develop the inferential version of the existing Timing Bloom Filter. Through experiments on real and synthetic datasets, we show that when penalties are query-specific and prior probabilities are known, the inferential Timing Bloom Filter reduces penalties for incorrect responses to sliding-window queries by up to 70%.

#index 1962335
#* Utility-driven data acquisition in participatory sensing
#@ Mehdi Riahi;Thanasis G. Papaioannou;Immanuel Trummer;Karl Aberer
#t 2013
#c 8
#% 458835
#% 654482
#% 862541
#% 887946
#% 1016178
#% 1029072
#% 1060263
#% 1259710
#% 1298742
#% 1414128
#% 1594581
#% 1675421
#% 1930392
#! Participatory sensing (PS) is becoming a popular data acquisition means for interesting emerging applications. However, as data queries from these applications increase, the sustainability of this platform for multiple concurrent applications is at stake. In this paper, we consider the problem of efficient data acquisition in PS when queries of different types come from different applications. We effectively deal with the issues related to resource constraints, user privacy, data reliability, and uncontrolled mobility. We formulate the problem as multi-query optimization and propose efficient heuristics for its effective solution for the various query types and mixes that enable sustainable sensing. Based on simulations with real and artificial data traces, we found that our heuristic algorithms outperform baseline approaches in a multitude of settings considered.

#index 1962336
#* An RFID and particle filter-based indoor spatial query evaluation system
#@ Jiao Yu;Wei-Shinn Ku;Min-Te Sun;Hua Lu
#t 2013
#c 8
#% 201876
#% 287466
#% 787130
#% 893102
#% 1015321
#% 1036083
#% 1063472
#% 1063523
#% 1065043
#% 1080155
#% 1082178
#% 1181270
#% 1206877
#% 1206879
#% 1209560
#% 1213715
#% 1245052
#% 1292533
#% 1372710
#% 1426506
#% 1692266
#% 1760888
#! People spend a significant amount of time in indoor spaces (e.g., office buildings, subway systems, etc.) in their daily lives. Therefore, it is important to develop efficient indoor spatial query algorithms for supporting various location-based applications. However, indoor spaces differ from outdoor spaces because users have to follow the indoor floor plan for their movements. In addition, positioning in indoor environments is mainly based on sensing devices (e.g., RFID readers) rather than GPS devices. Consequently, we cannot apply existing spatial query evaluation techniques devised for outdoor environments for this new challenge. Because particle filters can be employed to estimate the state of a system that changes over time using a sequence of noisy measurements made on the system, in this research, we propose the particle filter-based location inference method as the basis for evaluating indoor spatial queries with noisy RFID raw data. Furthermore, two novel models, indoor walking graph model and anchor point indexing model, are created for tracking object locations in indoor environments. Based on the inference method and tracking models, we develop innovative indoor range and k nearest neighbor (kNN) query algorithms. We validate our solution through extensive simulations with real-world parameters. Our experimental results show that the proposed algorithms can evaluate indoor spatial queries effectively and efficiently.

#index 1962337
#* A safe zone based approach for monitoring moving skyline queries
#@ Muhammad Aamir Cheema;Xuemin Lin;Wenjie Zhang;Ying Zhang
#t 2013
#c 8
#% 121114
#% 421124
#% 465167
#% 480661
#% 480671
#% 654478
#% 654480
#% 800555
#% 864464
#% 893150
#% 902462
#% 993954
#% 1103006
#% 1107574
#% 1127438
#% 1206900
#% 1230837
#% 1426505
#% 1486264
#% 1594683
#% 1595892
#% 1910904
#! Given a set of criterions, an object o dominates another object ó if o is more preferable than ó according to every criterion. A skyline query returns every object that is not dominated by any other object. In this paper, we study the problem of continuously monitoring a moving skyline query where one of the criterions is the distance between the objects and the moving query. We propose a safe zone based approach to address the challenge of efficiently updating the results as the query moves. A safe zone is the area such that the results of a query remain unchanged as long as the query lies inside this area. Hence, the results are required to be updated only when the query leaves its safe zone. Although the main focus of this paper is to present the techniques for Euclidean distance metric, the proposed techniques are applicable to any metric distance (e.g., Manhattan distance, road network distance). We present several non-trivial optimizations and propose an efficient algorithm for safe zone construction. Our experiments demonstrate that the cost of our safe zone based approach is reasonably close to a lower bound cost and is three orders of magnitude lower than the cost of a naïve algorithm.

#index 1962338
#* Compressed feature-based filtering and verification approach for subgraph search
#@ Karam Gouda;Mosab Hassaan
#t 2013
#c 8
#% 765429
#% 810072
#% 864425
#% 960305
#% 1022280
#% 1044450
#% 1127380
#% 1174743
#% 1206703
#% 1495182
#% 1523835
#% 1523900
#% 1594628
#! Subgraph search in graph datasets is an important problem with numerous applications. Many feature-based indexing methods have been proposed for solving this problem. These methods have to index too many features or select some of them in order to get an index with good pruning capabilities. None of these directions can give an effective solution to all graph indexing issues. In this paper, we propose an efficient indexing approach which improves over current feature-based methods, neither by the costly feature selection nor by explicitly indexing a multitude of features. We achieve this by compressing multiple features into one feature with some neighborhood information encoded. Neighborhood is further used to prune unmatched feature occurrences between the query and data graphs, thus cutting down the search space of subgraph matching, which significantly reduce the verification cost. We implement the approach by exhaustively enumerating small paths as features. A novel path-at-a-time verification method that benefits from the occurrences pruning method is introduced. Via an extensive evaluation on both real and synthetic datasets, we show that our approach is effective and scalable, and outperforms state-of-the-art indexing methods.

#index 1962339
#* Efficient query answering against dynamic RDF databases
#@ François Goasdoué;Ioana Manolescu;Alexandra Roatiş
#t 2013
#c 8
#% 279164
#% 384978
#% 665856
#% 992962
#% 1022236
#% 1127402
#% 1127431
#% 1127610
#% 1152464
#% 1217194
#% 1313373
#% 1366460
#% 1396154
#% 1413125
#% 1523817
#% 1594576
#% 1603794
#% 1605087
#% 1641523
#% 1654043
#! A promising method for efficiently querying RDF data consists of translating SPARQL queries into efficient RDBMS-style operations. However, answering SPARQL queries requires handling RDF reasoning, which must be implemented outside the relational engines that do not support it. We introduce the database (DB) fragment of RDF, going beyond the expressive power of previously studied RDF fragments. We devise novel sound and complete techniques for answering Basic Graph Pattern (BGP) queries within the DB fragment of RDF, exploring the two established approaches for handling RDF semantics, namely reformulation and saturation. In particular, we focus on handling database updates within each approach and propose a method for incrementally maintaining the saturation; updates raise specific difficulties due to the rich RDF semantics. Our techniques are designed to be deployed on top of any RDBMS(-style) engine, and we experimentally study their performance trade-offs.

#index 1962340
#* Efficient breadth-first search on large graphs with skewed degree distributions
#@ Haichuan Shang;Masaru Kitsuregawa
#t 2013
#c 8
#% 240260
#% 340182
#% 443256
#% 476168
#% 722530
#% 810072
#% 813718
#% 823342
#% 843790
#% 1063502
#% 1127380
#% 1181254
#% 1265149
#% 1292553
#% 1426513
#% 1426539
#% 1426575
#% 1426577
#% 1581880
#% 1581921
#% 1592313
#% 1594618
#% 1594621
#% 1618133
#! Many recent large-scale data intensive applications are increasingly demanding efficient graph databases. Distributed graph algorithms, as a core part of practical graph databases, have a wide range of important applications, but have been rarely studied in sufficient detail. These problems are challenging as real graphs are usually extremely large and the intrinsic character of graph data, lacking locality, causes unbalanced computation and communication workloads. In this paper, we explore distributed breadth-first search algorithms with regards to large-scale applications. We propose DPC (Degree-based Partitioning and Communication), a scalable and efficient distributed BFS algorithm which achieves high scalability and performance through novel balancing techniques between computation and communication. In experimental study, we compare our algorithm with two state-of-the-art algorithms under the Graph500 benchmark with a variety of settings. The result shows our algorithm significantly outperforms the existing algorithms under all the settings.

#index 1962341
#* CINEMA: conformity-aware greedy algorithm for influence maximization in online social networks
#@ Hui Li;Sourav S. Bhowmick;Aixin Sun
#t 2013
#c 8
#% 467185
#% 577217
#% 729923
#% 989613
#% 1214641
#% 1451243
#% 1451244
#% 1535380
#% 1628176
#% 1642030
#% 1688456
#! Influence maximization (IM) is the problem of finding a small subset of nodes (seed nodes) in a social network that could maximize the spread of influence. Despite the progress achieved by state-of-the-art greedy IM techniques, they suffer from two key limitations. Firstly, they are inefficient as they can take days to find seeds in very large real-world networks. Secondly, although extensive research in social psychology suggests that humans will readily conform to the wishes or beliefs of others, surprisingly, existing IM techniques are conformity-unaware. That is, they only utilize an individual's ability to influence another but ignores conformity (a person's inclination to be influenced) of the individuals. In this paper, we propose a novel conformity-aware cascade (c2) model which leverages on the interplay between influence and conformity in obtaining the influence probabilities of nodes from underlying data for estimating influence spreads. We propose a novel greedy algorithm called CINEMA that generates high quality seed set by exploiting this model. It first partitions the network into a set of non-overlapping subnetworks and for each of these subnetworks it computes the influence and conformity indices of nodes. Each subnetwork is then associated with a COG-sublist which stores the marginal gains of the nodes in the subnetwork in descending order. The node with maximum marginal gain in each COG-sublist is stored in a data structure called MAG-list. These structures are manipulated by CINEMA to efficiently find the seed set. A key feature of such partitioning-based strategy is that each node's influence computation and updates can be limited to the subnetwork it resides instead of the entire network. Our empirical study with real-world social networks demonstrates that CINEMA generates superior quality seed set compared to state-of-the-art IM approaches.

#index 1962342
#* Pollux: towards scalable distributed real-time search on microblogs
#@ Liwei Lin;Xiaohui Yu;Nick Koudas
#t 2013
#c 8
#% 232771
#% 351041
#% 726621
#% 765470
#% 800583
#% 995806
#% 998845
#% 1127396
#% 1127560
#% 1206600
#% 1384701
#% 1399966
#% 1399992
#% 1400975
#% 1484274
#% 1518214
#% 1535212
#% 1538766
#% 1581900
#% 1581937
#% 1625033
#% 1846784
#! The last few years have witnessed a meteoric rise of microblogging platforms, such as Twitter and Tumblr. The sheer volume of the microblog data and its highly dynamic nature present unique technical challenges for the platforms that provide search services. In particular, the search service must provide real-time response to queries, and continuously update the results as new microblogs are posted. Conventional approaches either cannot keep up with the high update rate, or cannot scale well to handle the large volume of data. We propose Pollux, a system that provides distributed real-time indexing and search service on microblogs. It adopts the distributed stream processing paradigm advocated by the recently developed platforms that are designed for real-time processing of large volume of data, such as Apache S4 and Twitter Storm. Although those open-source platforms have found successful applications in production environments, they lack some critical features required for real-time search. In particular: (1) they only implement partial fault tolerance, and do not provide lossless recovery in the event of a node failure, and (2) they do not have a facility for storing global data, which is necessary in efficiently ranking search results. Addressing those problems, Pollux extends current platforms in two important ways. First, we propose a failover strategy that can ensure high system availability and no data/state loss in the event of a node failure. Second, Pollux adds a global storage facility that supports convenient, efficient, and reliable data storage for shared data. We describe how to apply Pollux to the task of real-time search. We implement Pollux based on Apache S4, and show through extensive experiments on a Twitter dataset that the proposed solutions are effective, and Pollux can achieve excellent scalability.

#index 1962343
#* Semantic queries by example
#@ Lipyeow Lim;Haixun Wang;Min Wang
#t 2013
#c 8
#% 190581
#% 341704
#% 466419
#% 466887
#% 783540
#% 890480
#% 1016217
#% 1022223
#% 1087240
#% 1292633
#% 1677728
#% 1727318
#! With the ever increasing quantities of electronic data, there is a growing need to make sense out of the data. Many advanced database applications are beginning to support this need by integrating domain knowledge encoded as ontologies into queries over relational data. However, it is extremely difficult to express queries against graph structured ontology in the relational SQL query language or its extensions. Moreover, semantic queries are usually not precise, especially when data and its related ontology are complicated. Users often only have a vague notion of their information needs and are not able to specify queries precisely. In this paper, we address these challenges by introducing a novel method to support semantic queries in relational databases with ease. Instead of casting ontology into relational form and creating new language constructs to express such queries, we ask the user to provide a small number of examples that satisfy the query she has in mind. Using those examples as seeds, the system infers the exact query automatically, and the user is therefore shielded from the complexity of interfacing with the ontology. Our approach consists of three steps. In the first step, the user provides several examples that satisfy the query. In the second step, we use machine learning techniques to mine the semantics of the query from the given examples and related ontologies. Finally, we apply the query semantics on the data to generate the full query result. We also implement an optional active learning mechanism to find the query semantics accurately and quickly. Our experiments validate the effectiveness of our approach.

#index 1962344
#* Scalable top-k spatial keyword search
#@ Dongxiang Zhang;Kian-Lee Tan;Anthony K. H. Tung
#t 2013
#c 8
#% 387427
#% 397396
#% 527189
#% 838407
#% 867054
#% 874993
#% 875017
#% 982560
#% 1206801
#% 1206997
#% 1328137
#% 1490145
#% 1555383
#% 1581877
#% 1618262
#% 1641963
#% 1846749
#! In this big data era, huge amounts of spatial documents have been generated everyday through various location based services. Top-k spatial keyword search is an important approach to exploring useful information from a spatial database. It retrieves k documents based on a ranking function that takes into account both textual relevance (similarity between the query and document keywords) and spatial relevance (distance between the query and document locations). Various hybrid indexes have been proposed in recent years which mainly combine the R-tree and the inverted index so that spatial pruning and textual pruning can be executed simultaneously. However, the rapid growth in data volume poses significant challenges to existing methods in terms of the index maintenance cost and query processing time. In this paper, we propose a scalable integrated inverted index, named I3, which adopts the Quadtree structure to hierarchically partition the data space into cells. The basic unit of I3 is the keyword cell, which captures the spatial locality of a keyword. Moreover, we design a new storage mechanism for efficient retrieval of keyword cell and preserve additional summary information to facilitate pruning. Experiments conducted on real spatial datasets (Twitter and Wikipedia) demonstrate the superiority of I3 over existing schemes such as IR-tree and S2I in various aspects: it incurs shorter construction time to build the index, it has lower index storage cost, it is order of magnitude faster in updates, and it is highly scalable and answers top-k spatial keyword queries efficiently.

#index 1962345
#* Panorama: a semantic-aware application search framework
#@ Di Jiang;Jan Vosecky;Kenneth Wai-Ting Leung;Wilfred Ng
#t 2013
#c 8
#% 477937
#% 577224
#% 722904
#% 728360
#% 730065
#% 788094
#% 907550
#% 987214
#% 1166535
#% 1392478
#% 1475756
#% 1480887
#% 1484351
#% 1560379
#% 1591980
#% 1598433
#% 1682445
#% 1693904
#% 1806237
#% 1919722
#! Third-party applications (or commonly referred to the apps) proliferate on the web and mobile platforms in recent years. The tremendous amount of available apps in app market-places suggests the necessity of designing effective app search engines. However, existing app search engines typically ignore the latent semantics in the app corpus and thus usually fail to provide high-quality app snippets and effective app rankings. In this paper, we present a novel framework named Panorama to provide independent search results for Android apps with semantic awareness. We first propose the App Topic Model (ATM) to discover the latent semantics from the app corpus. Based on the discovered semantics, we tackle two central challenges that are faced by current app search engines: (1) how to generate concise and informative snippets for apps and (2) how to rank apps effectively with respect to search queries. To handle the first challenge, we propose several new metrics for measuring the quality of the sentences in app description and develop a greedy algorithm with fixed probability guarantee of near-optimal performance for app snippet generation. To handle the second challenge, we propose a variety of new features for app ranking and also design a new type of inverted index to support efficient Top-k app retrieval. We conduct extensive experiments on a large-scale data collection of Android apps and build an app search engine prototype for human-based performance evaluation. The proposed framework demonstrates superior performance against several strong baselines with respect to different metrics.

#index 1962346
#* Selectivity estimation for hybrid queries over text-rich data graphs
#@ Andreas Wagner;Veli Bicer;Thanh D. Tran
#t 2013
#c 8
#% 210190
#% 248822
#% 273909
#% 333946
#% 333986
#% 571046
#% 722753
#% 745489
#% 824684
#% 874987
#% 1000502
#% 1022218
#% 1022229
#% 1022236
#% 1022288
#% 1273915
#% 1289267
#% 1417383
#% 1482251
#% 1594559
#% 1642083
#% 1646320
#! Many databases today are text-rich, comprising not only structured, but also textual data. Querying such databases involves predicates matching structured data combined with string predicates featuring textual constraints. Based on selectivity estimates for these predicates, query processing as well as other tasks that can be solved through such queries can be optimized. Existing work on selectivity estimation focuses either on string or on structured query predicates alone. Further, probabilistic models proposed to incorporate dependencies between predicates are focused on the relational setting. In this work, we propose a template-based probabilistic model, which enables selectivity estimation for general graph-structured data. Our probabilistic model allows dependencies between structured data and its text-rich parts to be captured. With this general probabilistic solution, BN+, selectivity estimations can be obtained for queries over text-rich graph-structured data, which may contain structured and string predicates (hybrid queries). In our experiments on real-world data, we show that capturing dependencies between structured and textual data in this way greatly improves the accuracy of selectivity estimates without compromising the efficiency.

#index 1962347
#* Skyline probability over uncertain preferences
#@ Qing Zhang;Pengjie Ye;Xuemin Lin;Ying Zhang
#t 2013
#c 8
#% 190611
#% 465167
#% 480671
#% 800555
#% 806212
#% 824671
#% 849816
#% 903013
#% 943612
#% 1022203
#% 1022225
#% 1063485
#% 1092017
#% 1312536
#% 1567778
#% 1613685
#% 1806265
#! Skyline analysis is a key in a wide spectrum of real applications involving multi-criteria optimal decision making. In recent years, a considerable amount of research has been contributed on efficient computation of skyline probabilities over uncertain environment. Most studies if not all, assume uncertainty lies only in attribute values. To the extent of our knowledge, only one study addresses the skyline probability computation problem in scenarios where uncertainty resides in attribute preferences, instead of values. However this study takes a problematic approach by assuming independent object dominance, which we find is not always true in uncertain preference scenarios. In fact this assumption has already been shown to be not necessarily true in uncertain value scenarios. Motivated by this, we revisit the skyline probability computation over uncertain preferences in this paper. We first show that the problem of skyline probability computation over uncertain preferences is #P-complete. Then we propose efficient exact and approximate algorithms to tackle this problem. While the exact algorithm remains exponential in the worst case, our experiments demonstrate its efficiency in practice. The approximate algorithm achieves ε-approximation by the confidence (1 − δ) with time complexity O(dn1/ε2 ln 1/δ), where n is the number of objects and d is the dimensionality. The efficiency and effectiveness of our methods are verified by extensive experimental results on real and synthetic data sets.

#index 1962348
#* SkyDiver: a framework for skyline diversification
#@ George Valkanas;Apostolos N. Papadopoulos;Dimitrios Gunopulos
#t 2013
#c 8
#% 249321
#% 311808
#% 443393
#% 465167
#% 548654
#% 806212
#% 1022242
#% 1024548
#% 1074133
#% 1083508
#% 1166473
#% 1206819
#% 1311045
#% 1328116
#% 1328135
#% 1523906
#% 1581826
#% 1581911
#% 1594651
#% 1693954
#% 1697272
#% 1798393
#% 1845458
#% 1872279
#! Skyline queries have attracted considerable attention by the database community during the last decade, due to their applicability in a series of domains. However, most existing works tackle the problem from an efficiency standpoint, i.e., returning the skyline as quickly as possible. The user is then presented with the entire skyline set, which may be in several cases overwhelming, therefore requiring manual inspection to come up with the most informative data points. To overcome this shortcoming, we propose a novel approach in selecting the k most diverse skyline points, i.e., the ones that best capture the different aspects of both the skyline and the dataset they belong to. We present a novel formulation of diversification which, in contrast to previous proposals, is intuitive, because it is based solely on the domination relationships among points. Consequently, additional artificial distance measures (e.g., Lp norms) among skyline points are not required. We present efficient approaches in solving this problem and demonstrate the efficiency and effectiveness of our approach through an extensive experimental evaluation with both real-life and synthetic data sets.

#index 1962349
#* Subspace global skyline query processing
#@ Mei Bai;Junchang Xin;Guoren Wang
#t 2013
#c 8
#% 465167
#% 480671
#% 654480
#% 806212
#% 824671
#% 824672
#% 864452
#% 875012
#% 993954
#% 1016207
#% 1022203
#% 1022225
#% 1022226
#% 1044463
#% 1063485
#% 1206819
#% 1482311
#% 1531277
#% 1594566
#% 1942047
#! Global skyline, as an important variant of skyline, has been widely applied in multiple criteria decision making, business planning and data mining, while there are no previous studies on the global skyline query in the subspace. Hence in this paper we propose subspace global skyline (SGS) query, which is concerned about global skyline in ad hoc subspace. Firstly, we propose an appropriate index structure RB-tree to rapidly find the initial scan positions of query. Secondly, by making analysis of basic properties of SGS, we propose a single SGS algorithm based on RB-tree (SSRB) to compute SGS points. Then an optimized single SGS algorithm based on RB-tree (OSSRB) is proposed, which can reduce the scan space and improve the computation efficiency in contrast to SSRB. Next, by sharing the scan space of different queries, a multiple SGS algorithm based on RB-tree (MSRB) is proposed to compute multiple SGS (MSGS). Finally, the performances of our proposed algorithms are verified through a large number of simulation experiments.

#index 1962350
#* SWORD: scalable workload-aware data placement for transactional workloads
#@ Abdul Quamar;K. Ashwin Kumar;Amol Deshpande
#t 2013
#c 8
#% 210179
#% 717164
#% 866984
#% 978404
#% 1035579
#% 1050316
#% 1063501
#% 1127596
#% 1166532
#% 1181253
#% 1237169
#% 1426552
#% 1523795
#% 1523799
#% 1541196
#% 1581943
#% 1770324
#% 1846827
#! In this paper, we address the problem of transparently scaling out transactional (OLTP) workloads on relational databases, to support database-as-a-service in cloud computing environment. The primary challenges in supporting such workloads include choosing how to partition the data across a large number of machines, minimizing the number of distributed transactions, providing high data availability, and tolerating failures gracefully. Capturing and modeling the transactional workload over a period of time, and then exploiting that information for data placement and replication has been shown to provide significant benefits in performance, both in terms of transaction latencies and overall throughput. However, such workload-aware data placement approaches can incur very high overheads, and further, may perform worse than naive approaches if the workload changes. In this work, we propose SWORD, a scalable workload-aware data partitioning and placement approach for OLTP workloads, that incorporates a suite of novel techniques to significantly reduce the overheads incurred both during the initial placement, and during query execution at runtime. We model the workload as a hypergraph over the data items, and propose using a hypergraph compression technique to reduce the overheads of partitioning. To deal with workload changes, we propose an incremental data repartitioning technique that modifies data placement in small steps without resorting to complete workload repartitioning. We have built a workload-aware active replication mechanism in SWORD to increase availability and enable load balancing. We propose the use of fine-grained quorums defined at the level of groups of tuples to control the cost of distributed updates, improve throughput, and provide adaptability to different workloads. To our knowledge, SWORD is the first system that uses fine-grained quorums in this context. The results of our experimental evaluation on SWORD deployed on an Amazon EC2 cluster show that our techniques result in orders-of-magnitude reductions in the partitioning and book-keeping overheads, and improve tolerance to failures and workload changes; we also show that choosing quorums based on the query access patterns enables us to better handle query workloads with different read and write access patterns.

#index 1962351
#* PMAX: tenant placement in multitenant databases for profit maximization
#@ Ziyang Liu;Hakan Hacıgümüş;Hyun Jin Moon;Yun Chi;Wang-Pin Hsiung
#t 2013
#c 8
#% 298181
#% 341937
#% 578872
#% 781787
#% 786866
#% 987232
#% 1022748
#% 1063561
#% 1127968
#% 1130292
#% 1153805
#% 1207027
#% 1217216
#% 1325890
#% 1459363
#% 1461972
#% 1511676
#% 1549847
#% 1581871
#% 1581872
#% 1592314
#% 1592341
#% 1594596
#% 1594650
#% 1621139
#% 1798412
#% 1846790
#! There has been a great interest in exploiting the cloud as a platform for database as a service. As with other cloud-based services, database services may enjoy cost efficiency through consolidation: hosting multiple databases within a single physical server. Aggressive consolidation, however, may hurt the service quality, leading to SLA violation penalty, which in turn reduces the total business profit, called SLA profit. In this paper, we consider the problem of tenant placement in the cloud for SLA profit maximization, which, as will be shown in the paper, is strongly NP-hard. We propose SLA profit-aware solutions for database tenant placement based on our model for expected penalty computation for multitenant servers. Specifically, we present two approximation algorithms, which have constant approximation ratios, and we further discuss improving the quality of tenant placement using a dynamic programming algorithm. Extensive experiments based on TPC-W workload verified the performance of the proposed approaches.

#index 1962352
#* Elastic online analytical processing on RAMCloud
#@ Christian Tinnefeld;Donald Kossmann;Martin Grund;Joos-Hendrik Boese;Frank Renkes;Vishal Sikka;Hasso Plattner
#t 2013
#c 8
#% 330305
#% 963451
#% 1008605
#% 1063488
#% 1089604
#% 1196713
#% 1278123
#% 1278124
#% 1332780
#% 1523824
#% 1562418
#% 1581872
#% 1583708
#% 1625033
#% 1642306
#% 1652705
#! A shared-nothing architecture is state-of-the-art for deploying a distributed analytical in-memory database management system: it preserves the in-memory performance advantage by processing data locally on each node but is difficult to scale out. Modern switched fabric communication links such as InfiniBand narrow the performance gap between local and remote DRAM data access to a single order of magnitude. Based on these premises, we introduce a distributed in-memory database architecture that separates the query execution engine and data access: this enables a) the usage of a large-scale DRAM-based storage system such as Stanford's RAMCloud and b) the push-down of bandwidth-intensive database operators into the storage system. We address the resulting challenges such as finding the optimal operator execution strategy and partitioning scheme. We demonstrate that such an architecture delivers both: the elasticity of a shared-storage approach and the performance characteristics of operating on local DRAM.

#index 1962353
#* Skyline queries in crowd-enabled databases
#@ Christoph Lofi;Kinda El Maarry;Wolf-Tilo Balke
#t 2013
#c 8
#% 333854
#% 465167
#% 806212
#% 810098
#% 893167
#% 903013
#% 1014660
#% 1022203
#% 1092017
#% 1206717
#% 1206735
#% 1206780
#% 1217143
#% 1292531
#% 1567778
#% 1581851
#% 1628171
#% 1697282
#% 1730733
#! Skyline queries are a well-established technique for database query personalization and are widely acclaimed for their intuitive query formulation mechanisms. However, when operating on incomplete datasets, skylines queries are severely hampered and often have to resort to highly error-prone heuristics. Unfortunately, incomplete datasets are a frequent phenomenon, especially when datasets are generated automatically using various information extraction or information integration approaches. Here, the recent trend of crowd-enabled databases promises a powerful solution: during query execution, some database operators can be dynamically outsourced to human workers in exchange for monetary compensation, therefore enabling the elicitation of missing values during runtime. Unfortunately, this powerful feature heavily impacts query response times and (monetary) execution costs. In this paper, we present an innovative hybrid approach combining dynamic crowd-sourcing with heuristic techniques in order to overcome current limitations. We will show that by assessing the individual risk a tuple poses with respect to the overall result quality, crowd-sourcing efforts for eliciting missing values can be narrowly focused on only those tuples that may degenerate the expected quality most strongly. This leads to an algorithm for computing skyline sets on incomplete data with maximum result quality, while optimizing crowd-sourcing costs.

#index 1962354
#* From stars to galaxies: skyline queries on aggregate data
#@ Matteo Magnani;Ira Assent
#t 2013
#c 8
#% 465167
#% 799759
#% 806212
#% 893150
#% 1022203
#% 1022226
#% 1206642
#% 1259554
#% 1312536
#% 1523894
#% 1567778
#% 1623240
#% 1692262
#% 1853527
#% 1917657
#% 1919884
#! The skyline operator extracts relevant records from multidimensional databases according to multiple criteria. This operator has received a lot of attention because of its ability to identify the best records in a database without requiring to specify complex parameters like the relative importance of each criterion. However, it has only been defined with respect to single records, while one fundamental functionality of database query languages is aggregation, enabling operations over sets of records. In this paper we introduce aggregate skylines, where the skyline works as a filtering predicate on sets of records. This operator can be used to express queries in the form: return the best groups depending on the features of their elements, and thus provides a powerful combination of grouping and skyline functionality. We define a semantics for aggregate skylines based on a sound theoretical framework and study its computational complexity. We propose efficient algorithms to implement this operator and test them on real and synthetic data, showing that they outperform a direct SQL implementation of up to two orders of magnitude.

#index 1962355
#* Efficient top-k query answering using cached views
#@ Min Xie;Laks V. S. Lakshmanan;Peter T. Wood
#t 2013
#c 8
#% 198465
#% 223563
#% 248038
#% 300180
#% 333951
#% 342828
#% 344105
#% 465167
#% 482110
#% 572311
#% 643566
#% 810018
#% 893108
#% 893126
#% 1015256
#% 1015317
#% 1022217
#% 1022243
#% 1075132
#% 1296952
#% 1441635
#% 1499470
#% 1587670
#% 1770352
#% 1846704
#! Top-k query processing has recently received a significant amount of attention due to its wide application in information retrieval, multimedia search and recommendation generation. In this work, we consider the problem of how to efficiently answer a top-k query by using previously cached query results. While there has been some previous work on this problem, existing algorithms suffer from either limited scope or lack of scalability. In this paper, we propose two novel algorithms for handling this problem. The first algorithm LPTA+ provides significantly improved efficiency compared to the state-of-the-art LPTA algorithm [26] by reducing the number of expensive linear programming problems that need to be solved. The second algorithm we propose leverages a standard space partition-based index structure in order to avoid many of the drawbacks of LPTA-based algorithms, thereby further improving the efficiency of query processing. Through extensive experiments on various datasets, we demonstrate that our algorithms significantly outperform the state of the art.

#index 1962356
#* Enhanced stream processing in a DBMS kernel
#@ Erietta Liarou;Stratos Idreos;Stefan Manegold;Martin Kersten
#t 2013
#c 8
#% 201929
#% 300179
#% 397354
#% 654497
#% 654507
#% 788215
#% 788216
#% 800491
#% 803602
#% 874996
#% 913788
#% 993961
#% 997534
#% 1016208
#% 1026964
#% 1181240
#% 1181292
#% 1206786
#% 1217169
#% 1217170
#% 1328078
#% 1490168
#% 1895076
#! Continuous query processing has emerged as a promising query processing paradigm with numerous applications. A recent development is the need to handle both streaming queries and typical one-time queries in the same application. For example, data warehousing can greatly benefit from the integration of stream semantics, i.e., online analysis of incoming data and combination with existing data. This is especially useful to provide low latency in data-intensive analysis in big data warehouses that are augmented with new data on a daily basis. However, state-of-the-art database technology cannot handle streams efficiently due to their "continuous" nature. At the same time, state-of-the-art stream technology is purely focused on stream applications. The research efforts are mostly geared towards the creation of specialized stream management systems built with a different philosophy than a DBMS. The drawback of this approach is the limited opportunities to exploit successful past data processing technology, e.g., query optimization techniques. For this new problem we need to combine the best of both worlds. Here we take a completely different route by designing a stream engine on top of an existing relational database kernel. This includes reuse of both its storage/execution engine and its optimizer infrastructure. The major challenge then becomes the efficient support for specialized stream features. This paper focuses on incremental window-based processing, arguably the most crucial streamspecific requirement. In order to maintain and reuse the generic storage and execution model of the DBMS, we elevate the problem at the query plan level. Proper optimizer rules, scheduling and intermediate result caching and reuse, allow us to modify the DBMS query plans for efficient incremental processing. We describe in detail the new approach and we demonstrate efficient performance even against specialized stream engines, especially when scalability becomes a crucial factor.

#index 1962357
#* Probabilistic inference of object identifications for event stream analytics
#@ Di Wang;Elke Rundensteiner;Richard Ellison, III;Han Wang
#t 2013
#c 8
#% 207004
#% 716892
#% 875004
#% 893167
#% 893202
#% 960257
#% 960292
#% 1015357
#% 1036083
#% 1063480
#% 1063518
#% 1063523
#% 1065542
#% 1176391
#% 1206772
#% 1206879
#% 1217161
#% 1313373
#% 1328078
#% 1426506
#% 1523936
#% 1550753
#% 1573237
#% 1581920
#% 1606345
#! Recent years have witnessed the emergence of real-time object monitoring applications driven by the explosion of small inexpensive sensors. In many real-world applications, not all sensed events carry the identification of the object whose action they report on, so called "non-ID-ed" events. Reasons range from heterogeneous sensing devices to human's choosing to conceal their identifications. Such non-ID-ed events prevent us from performing object-based analytics, such as tracking, alerting and pattern matching. We propose a probabilistic inference framework, called FISS, to tackle this problem by inferring the missing object identification associated with an event. Specifically, as a foundation we design a time-varying graphic model to capture correspondences between sensed events and objects. Upon this formal model, we elaborate how to adapt the Forward-backward (FB) inference algorithm to continuously infer probabilistic identifications for non-ID-ed events. However, we demonstrate that FB is neither scalable nor efficient over event streams. To overcome this deficiency, we propose a suite of strategies for optimizing its performance, including the selective smoothing technique that significantly reduces the number of random variables that need to be smoothed, and the finish-flag mechanism that enables early termination of backward computations. Our experimental results, using large-volume streams of a real-world healthcare application, demonstrate the accuracy, efficiency, and scalability of FISS. Especially FISS achieves on average 15x higher throughput than our basic FB inference.

#index 1962358
#* High-performance complex event processing using continuous sliding views
#@ Medhabi Ray;Elke A. Rundensteiner;Mo Liu;Chetan Gupta;Song Wang;Ismail Ari
#t 2013
#c 8
#% 443320
#% 461897
#% 464056
#% 481916
#% 562293
#% 571216
#% 810023
#% 838409
#% 874999
#% 875004
#% 1217161
#% 1470725
#% 1499471
#% 1581920
#% 1594604
#! Complex Event Processing (CEP) has become increasingly important for tracking and monitoring anomalies and trends in event streams emitted from business processes such as supply chain management to online stores in e-commerce. These monitoring applications submit complex event queries to track sequences of events that match a given pattern. While the state-of-the-art CEP systems mostly focus on the execution of flat sequence queries, we instead support the execution of nested CEP queries specified by the (NEsted Event Language) NEEL. However the iterative execution often results in the repeated recomputation of similar or even identical results for nested subexpressions as the window slides over the event stream. In this work we thus propose to optimize NEEL execution performance by caching intermediate results. In particular we design two methods of applying selective caching of intermediate results. The first is the Continuous Sliding Caching technique. The second is a further optimization of the previous technique which we call the Interval-Driven Semantic Caching. Techniques for incrementally loading, purging and exploiting the cache content are described. Our experimental study using real-world stock trades evaluates the performance of our proposed caching strategies for different query types.

#index 1962359
#* Data exchange with arithmetic operations
#@ Balder ten Cate;Phokion G. Kolaitis;Walied Othman
#t 2013
#c 8
#% 183411
#% 213972
#% 224743
#% 248038
#% 296539
#% 563608
#% 809239
#% 826032
#% 885630
#% 912245
#% 1044476
#% 1102364
#% 1328193
#% 1348452
#% 1541335
#% 1581857
#% 1735442
#% 1945113
#! Data exchange is the problem of transforming data structured under a source schema into data structured under a target schema, taking into account structural relationships between the two schemas, which are described by a schema mapping. Existing schema-mapping languages lack the ability to express arithmetic operations, such as addition and multiplication, which naturally arise in data warehousing, ETL applications, and applications involving scientific data. We initiate the study of data exchange for arithmetic schema mappings, that is, schema mappings specified by source-to-target dependencies and target dependencies that may include arithmetic formulas interpreted over the algebraic real numbers (we restrict attention to algebraic real numbers to maintain finite presentability). We show that, for arithmetic schema mappings without target dependencies, the existence-of-solutions problem can be solved in polynomial time, and, if a solution exists, then a universal solution (suitably defined) exists and can be computed in polynomial time. In the case of arithmetic schema mappings with a weakly acyclic set of target dependencies, a universal solution may not exist, but a finite universal basis exists (if a solution exists) and can be computed in polynomial space. The existence-of-solutions problem turns out to be NP-hard, and solvable in PSpace. In fact, we show it is ∃R-complete, which means that it has the same complexity as the decision problem for the existential theory of the real numbers, or, equivalently, the problem of deciding whether or not a quantifier-free arithmetic formula has a solution over the real numbers. If we allow only linear arithmetic formulas in the schema mapping and in the query, interpreted over the rational numbers, then the existence-of-solutions problem is NP-complete. We obtain analogous complexity results for the data complexity of computing the certain answers of arithmetic conjunctive queries and linear arithmetic conjunctive queries.

#index 1962360
#* HIL: a high-level scripting language for entity integration
#@ Mauricio Hernández;Georgia Koutrika;Rajasekar Krishnamurthy;Lucian Popa;Ryan Wisnesky
#t 2013
#c 8
#% 346654
#% 378409
#% 480496
#% 564416
#% 572314
#% 654457
#% 826032
#% 850730
#% 913783
#% 1063553
#% 1129527
#% 1183369
#% 1206834
#% 1217114
#% 1232194
#% 1471192
#! We introduce HIL, a high-level scripting language for entity resolution and integration. HIL aims at providing the core logic for complex data processing flows that aggregate facts from large collections of structured or unstructured data into clean, unified entities. Such flows typically include many stages of processing that start from the outcome of information extraction and continue with entity resolution, mapping and fusion. A HIL program captures the overall integration flow through a combination of SQL-like rules that link, map, fuse and aggregate entities. A salient feature of HIL is the use of logical indexes in its data model to facilitate the modular construction and aggregation of complex entities. Another feature is the presence of a flexible, open type system that allows HIL to handle input data that is irregular, sparse or partially known. As a result, HIL can accurately express complex integration tasks, while still being high-level and focused on the logical entities (rather than the physical operations). Compilation algorithms translate the HIL specification into efficient run-time queries that can execute in parallel on Hadoop. We show how our framework is applied to real-world integration of entities in the financial domain, based on public filings archived by the U.S. Securities and Exchange Commission (SEC). Furthermore, we apply HIL on a larger-scale scenario that performs fusion of data from hundreds of millions of Twitter messages into tens of millions of structured entities.

#index 1962361
#* Optimizing query rewriting in ontology-based data access
#@ Floriana Di Pinto;Domenico Lembo;Maurizio Lenzerini;Riccardo Mancini;Antonella Poggi;Riccardo Rosati;Marco Ruzzi;Domenico Fabio Savo
#t 2013
#c 8
#% 198465
#% 303884
#% 378409
#% 572307
#% 572311
#% 893089
#% 938538
#% 992962
#% 1065125
#% 1232193
#% 1279214
#% 1416180
#% 1560420
#% 1581854
#% 1594576
#% 1605090
#% 1641913
#% 1950073
#! In ontology-based data access (OBDA), an ontology is connected to autonomous, and generally pre-existing, data repositories through mappings, so as to provide a high-level, conceptual view over such data. User queries are posed over the ontology, and answers are computed by reasoning both on the ontology and the mappings. Query answering in OBDA systems is typically performed through a query rewriting approach which is divided into two steps: (i) the query is rewritten with respect to the ontology (ontology rewriting of the query); (ii) the query thus obtained is then reformulated over the database schema using the mapping assertions (mapping rewriting of the query). In this paper we present a new approach to the optimization of query rewriting in OBDA. The key ideas of our approach are the usage of inclusion between mapping views and the usage of perfect mappings, which allow us to drastically lower the combinatorial explosion due to mapping rewriting. These ideas are formalized in PerfectMap, an algorithm for OBDA query rewriting. We have experimented PerfectMap in a real-world OBDA scenario: our experimental results clearly show that, in such a scenario, the optimizations of PerfectMap are crucial to effectively perform query answering.

#index 1962362
#* Temporal query processing in Teradata
#@ Mohammed Al-Kateb;Ahmad Ghazal;Alain Crolotte;Ramesh Bhashyam;Jaiprakash Chimanchode;Sai Pavan Pakala
#t 2013
#c 8
#% 287268
#% 361445
#% 481928
#% 999505
#% 1107586
#% 1206971
#% 1905961
#! The importance of temporal data management is evident by the temporal features recently released in major commercial database systems. In Teradata, the temporal feature is based on the TSQL2 specification. In this paper, we present Teradata's implementation approach for temporal query processing. There are two common approaches to support temporal query processing in a database engine. One is through functional query rewrites to convert a temporal query to a semantically-equivalent non-temporal counterpart, mostly by adding time-based constraints. The other is a native support that implements temporal database operations such as scans and joins directly in the DBMS internals. These approaches have competing pros and cons. The rewrite approach is generally simpler to implement. But it adds a structural complexity to original query, which can pose a potential challenge to query optimizer and cause it to generate sub-optimal plans. A native support is expected to perform better. But it usually involves a higher cost of implementation, maintenance, and extension. We discuss why and describe how Teradata adopted the rewrite approach. In addition, we present an evaluation of our approach through a performance study conducted on a variation of the TPC-H benchmark with temporal tables and queries.

#index 1962363
#* Near real-time analytics with IBM DB2 analytics accelerator
#@ Daniel Martin;Oliver Koeth;Johannes Kern;Iliyana Ivanova
#t 2013
#c 8
#% 581671
#% 1206624
#% 1217145
#% 1328186
#% 1523974
#% 1581946
#! The IBM DB2 Analytics Accelerator (IDAA) implements the vision of a universal relational DBMS that processes OLTP and analytical-type queries in a single system, but on two fundamentally different query engines. Based on heuristics in DB2 for z/OS, the DB2 optimizer decides if a query should be executed by "mainline" DB2 or if it is beneficial to offload it to the attached IBM DB2 Analytics Accelerator that operates on copies of the DB2 tables. In this paper, we introduce the "incremental update" functionality of IDAA that keeps these copy tables in sync by employing replication technology that monitors the DB2 transaction log and asynchronously applies the changes to IDAA. This enables near real-time analytics over online data, effectively marrying traditionally separated OLTP and data warehouse environments. With IDAA, analytic queries can access data that is constantly refreshed in contrast to traditional warehouses that are updated on a daily or even weekly basis. Without any changes to the applications and without the need to introduce cross-system ETL flows, an existing operational data store can be used for data warehousing as well. The analytic query performance provided by IDAA makes it possible to execute reports directly against the transactional schema, thus avoiding the need for costly design and maintenance of a separate reporting schema. Additionally, the Accelerator shields DB2 for z/OS as the transactional system from performance degradation caused by the analytical workload and the replication component synchronizes all data changes in near real-time. We present the architecture of the integrated replication component of IDAA and discuss design decisions that we made when combining the different technologies as well as performance characteristics of the resulting system.

#index 1962364
#* AppSleuth: a tool for database tuning at the application level
#@ Wei Cao;Dennis Shasha
#t 2013
#c 8
#% 248824
#% 401436
#% 480158
#% 480803
#% 810016
#% 810026
#% 893178
#% 994014
#% 1016220
#% 1016221
#% 1016265
#% 1022293
#% 1022308
#% 1044447
#% 1207033
#% 1217243
#% 1776656
#% 1880462
#! Excellent work ([1]-[6]) has shown that memory management and transaction concurrency levels can often be tuned automatically by the database management systems. Other excellent work ([7]]-[14]) has shown how to use the optimizer to do automatic physical design or to make the optimizer itself more self-adaptive ([15]-[17]). Our performance tuning experience across various industries (finance, gaming, data warehouses, and travel) has shown that enormous additional tuning benefits (sometimes amounting to orders of magnitude) can come from reengineering application code and table design. The question is: can a tool help in this effort? We believe so. We present a tool called AppSleuth that parses application code and the tracing log for two popular database management systems in order to lead a competent tuner to the hot spots in an application. This paper discusses (i) representative application "delinquent design patterns", (ii) an application code parser to find them, (iii) a log parser to identify the patterns that are critical, and (iv) a display to give a global view of the issue. We present an extended sanitized case study from a real travel application to show the results of the tool at different stages of a tuning engagement, yielding a 300 fold improvement. This is the first tool of its kind that we know of.

#index 1962365
#* Cost exploration of data sharings in the cloud
#@ Samer Al-Kiswany;Hakan Hacıgümüş;Ziyang Liu;Jagan Sankaranarayanan
#t 2013
#c 8
#% 201928
#% 210208
#% 227947
#% 248014
#% 273908
#% 300141
#% 333962
#% 335726
#% 340300
#% 571217
#% 737720
#% 960352
#% 1475073
#% 1581873
#% 1594597
#% 1643315
#% 1730735
#% 1855851
#! Enabling data sharing among mobile apps hosted in the same cloud infrastructure can provide a competitive advantage to the mobile apps by giving them access to rich information as well as increasing the revenue for the cloud provider. We introduce a costing tool that allows application owners (i.e., consumers) and the cloud service provider to assess the cost of a desired data sharing. The costing tool enables the consumers to effectively explore the cost space by choosing between alternative configurations of varying data qualities, specified by the staleness and the accuracy of the data sharing. In other words, staleness and accuracy requirements on the data sharing are used as levers for controlling costs. These capabilities are implemented in a What-if analysis tool, which has been integrated with a large data-sharing platform. We conducted extensive experiments on the integrated platform with a sharing ecosystem created around Twitter data and show the effectiveness of the results produced by the What-if tool.

#index 1962366
#* A performance comparison of parallel DBMSs and MapReduce on large-scale text analytics
#@ Fei Chen;Meichun Hsu
#t 2013
#c 8
#% 480654
#% 643004
#% 782759
#% 810014
#% 824677
#% 855119
#% 939376
#% 1022288
#% 1083705
#% 1166537
#% 1183373
#% 1206687
#% 1217159
#% 1217171
#% 1426543
#% 1467704
#% 1523889
#% 1523956
#% 1573237
#% 1581889
#% 1770346
#% 1770419
#% 1895052
#% 1895053
#! Text analytics has become increasingly important with the rapid growth of text data. Particularly, information extraction (IE), which extracts structured data from text, has received significant attention. Unfortunately, IE is often computationally intensive. To address this issue, MapReduce has been used for large scale IE. Recently, there are emerging efforts from both academia and industry on pushing IE inside DBMSs. This leads to an interesting and important question: Given that both MapReduce and parallel DBMSs are for large scale analytics, which platform is a better choice for large scale IE? In this paper, we propose a benchmark to systematically study the performance of both platforms for large scale IE tasks. The benchmark includes both statistical learning based and rule based IE programs, which have been extensively used in real-world IE tasks. We show how to express these programs on both platforms and conduct experiments on real-world datasets. Our results show that parallel DBMSs is a viable alternative for large scale IE.

#index 1962367
#* Sparkler: supporting large-scale matrix factorization
#@ Boduo Li;Sandeep Tata;Yannis Sismanis
#t 2013
#c 8
#% 280819
#% 340175
#% 435536
#% 435542
#% 534889
#% 956521
#% 960250
#% 1023420
#% 1108903
#% 1217203
#% 1260273
#% 1291642
#% 1358747
#% 1400001
#% 1426486
#% 1426513
#% 1464950
#% 1472299
#% 1479550
#% 1482205
#% 1523820
#% 1526993
#% 1567923
#% 1594623
#% 1594630
#% 1604232
#% 1605920
#% 1730850
#% 1770346
#% 1783374
#% 1874963
#! Low-rank matrix factorization has recently been applied with great success on matrix completion problems for applications like recommendation systems, link predictions for social networks, and click prediction for web search. However, as this approach is applied to increasingly larger datasets, such as those encountered in web-scale recommender systems like Netflix and Pandora, the data management aspects quickly become challenging and form a road-block. In this paper, we introduce a system called Sparkler to solve such large instances of low rank matrix factorizations. Sparkler extends Spark, an existing platform for running parallel iterative algorithms on datasets that fit in the aggregate main memory of a cluster. Sparkler supports distributed stochastic gradient descent as an approach to solving the factorization problem -- an iterative technique that has been shown to perform very well in practice. We identify the shortfalls of Spark in solving large matrix factorization problems, especially when running on the cloud, and solve this by introducing a novel abstraction called "Carousel Maps" (CMs). CMs are well suited to storing large matrices in the aggregate memory of a cluster and can efficiently support the operations performed on them during distributed stochastic gradient descent. We describe the design, implementation, and the use of CMs in Sparkler programs. Through a variety of experiments, we demonstrate that Sparkler is faster than Spark by 4x to 21x, with bigger advantages for larger problems. Equally importantly, we show that this can be done without imposing any changes to the ease of programming. We argue that Sparkler provides a convenient and efficient extension to Spark for solving matrix factorization problems on very large datasets.

#index 1962368
#* Choosing the right crowd: expert finding in social networks
#@ Alessandro Bozzon;Marco Brambilla;Stefano Ceri;Matteo Silvestri;Giuliano Vesci
#t 2013
#c 8
#% 730082
#% 939376
#% 956516
#% 1074171
#% 1083720
#% 1130900
#% 1195845
#% 1214668
#% 1292752
#% 1399976
#% 1482395
#% 1536507
#% 1560264
#% 1573506
#% 1642174
#% 1642219
#% 1740999
#% 1746898
#% 1880464
#! Expert selection is an important aspect of many Web applications, e.g., when they aim at matching contents, tasks or advertisement based on user profiles, possibly retrieved from social networks. This paper focuses on selecting experts within the population of social networks, according to the information about the social activities of their users. We consider the following problem: given an expertise need (expressed for instance as a natural language query) and a set of social network members, who are the most knowledgeable people for addressing that need? We considers social networks both as a source of expertise information and as a route to reach expert users, and define models and methods for evaluating people's expertise by considering their profiles and by tracing their activities in social networks. For matching queries to social resources, we use both text analysis and semantic annotation. An extensive set of experiments shows that the analysis of social activities, social relationships, and socially shared contents helps improving the effectiveness of an expert finding system.

#index 1962369
#* Real-time wildfire monitoring using scientific database and linked data technologies
#@ Manolis Koubarakis;Charalambos Kontoes;Stefan Manegold
#t 2013
#c 8
#% 227934
#% 248863
#% 500874
#% 902315
#% 1270142
#% 1296324
#% 1652704
#% 1882103
#% 1895101
#% 1933391
#% 1942739
#! We present a real-time wildfire monitoring service that exploits satellite images and linked geospatial data to detect hotspots and monitor the evolution of fire fronts. The service makes heavy use of scientific database technologies (array databases, SciQL, data vaults) and linked data technologies (ontologies, linked geospatial data, stSPARQL) and is implemented on top of MonetDB and Strabon. The service is now operational at the National Observatory of Athens and has been used during the previous summer by emergency managers monitoring wildfires in Greece.

#index 1962370
#* Efficient multifaceted screening of job applicants
#@ Sameep Mehta;Rakesh Pimplikar;Amit Singh;Lav R. Varshney;Karthik Visweswariah
#t 2013
#c 8
#% 330769
#% 397155
#% 400847
#% 770205
#% 829008
#% 968030
#% 983098
#% 987339
#% 1065811
#% 1183681
#% 1189780
#% 1202349
#% 1312124
#% 1464208
#% 1482244
#% 1646478
#! Built on top of human resources management databases within the enterprise, we present a decision support system for managing and optimizing screening activities during the hiring process in a large organization. The basic idea is to prioritize the efforts of human resource practitioners to focus on candidates that are likely of high quality, that are likely to accept a job offer if made one, and that are likely to remain with the organization for the long term. To do so, the system first individually ranks candidates along several dimensions using a keyword matching algorithm and several bipartite ranking algorithms with univariate loss trained on historical actions. Next, individual rankings are aggregated to derive a single list that is presented to the recruitment team through an interactive portal. The portal supports multiple filters that facilitate effective identification of candidates. We demonstrate the usefulness of our system on data collected from a large organization over several years with business value metrics showing greater hiring yield with less interviews. Similarly, using historical pre-hire data we demonstrate accurate identification of candidates that will have quickly left the organization. The system has been deployed as described in a large globally integrated enterprise.

#index 1962371
#* EXLEngine: executable schema mappings for statistical data processing
#@ Paolo Atzeni;Luigi Bellomarini;Francesca Bugiotti
#t 2013
#c 8
#% 800616
#% 806215
#% 809239
#% 810078
#% 826032
#% 960233
#% 1126564
#% 1206578
#% 1232194
#% 1334268
#% 1335352
#% 1680216
#% 1771511
#! Data processing is the core of any statistical information system. Statisticians are interested in specifying transformations and manipulations of data at a high level, in terms of entities of statistical models such as time series. We illustrate here an experience at the Bank of Italy where (i) a language, EXL, has been defined for the declarative specification of statistical programs, (ii) an approach for the translation of EXL code into executables in various target systems has been developed, and (iii) a concrete implementation, EXLEngine, has been carried out. The approach leverages on schema mappings as an intermediate specification step, in order to facilitate the translation from EXL towards several target systems.

#index 1962372
#* HyperLogLog in practice: algorithmic engineering of a state of the art cardinality estimation algorithm
#@ Stefan Heule;Marc Nunkesser;Alexander Hall
#t 2013
#c 8
#% 519953
#% 723898
#% 937911
#% 954300
#% 1016602
#% 1044487
#% 1136386
#% 1426447
#% 1523824
#% 1880459
#! Cardinality estimation has a wide range of applications and is of particular importance in database systems. Various algorithms have been proposed in the past, and the HyperLogLog algorithm is one of them. In this paper, we present a series of improvements to this algorithm that reduce its memory requirements and significantly increase its accuracy for an important range of cardinalities. We have implemented our proposed algorithm for a system at Google and evaluated it empirically, comparing it to the original HyperLogLog algorithm. Like HyperLogLog, our improved algorithm parallelizes perfectly and computes the cardinality estimate in a single pass.

#index 1962373
#* Entity discovery and annotation in tables
#@ Gianluca Quercini;Chantal Reynaud
#t 2013
#c 8
#% 815283
#% 956564
#% 1016363
#% 1083654
#% 1127393
#% 1152461
#% 1218677
#% 1249547
#% 1288161
#% 1426594
#% 1523913
#% 1526538
#% 1540293
#% 1540327
#% 1587299
#% 1592311
#% 1641552
#% 1770359
#% 1943531
#! The Web is rich of tables (e.g., HTML tables, spreadsheets, Google Fusion Tables) that host a considerable wealth of high-quality relational data. Unlike unstructured texts, tables usually favour the automatic extraction of data because of their regular structure and properties. The data extraction is usually complemented by the annotation of the table, which determines its semantics by identifying a type for each column, the relations between columns, if any, and the entities that occur in each cell. In this paper, we focus on the problem of discovering and annotating entities in tables. More specifically, we describe an algorithm that identifies the rows of a table that contain information on entities of specific types (e.g., restaurant, museum, theatre) derived from an ontology and determines the cells in which the names of those entities occur. We implemented this algorithm while developing a faceted browser over a repository of RDF data on points of interest of cities that we extracted from Google Fusion Tables. We claim that our algorithm complements the existing approaches, which annotate entities in a table based on a pre-compiled reference catalogue that lists the types of a finite set of entities; as a result, they are unable to discover and annotate entities that do not belong to the reference catalogue. Instead, we train our algorithm to look for information on previously unseen entities on the Web so as to annotate them with the correct type.

#index 1962374
#* iPark: identifying parking spaces from trajectories
#@ Bin Yang;Nicolas Fantini;Christian S. Jensen
#t 2013
#c 8
#% 1618279
#% 1642010
#% 1643293
#% 1667221
#! A wide variety of desktop and mobile Web applications involve geo-tagged content, e.g., photos and (micro-) blog postings. Such content, often called User Generated Geo-Content (UGGC), plays an increasingly important role in many applications. However, a great demand also exists for "core" UGGC where the geo-spatial aspect is not just a tag on other content, but is the primary content, e.g., a city street map with up-to-date road construction data. Along these lines, the iPark system aims to turn volumes of GPS data obtained from vehicles into information about the locations of parking spaces, thus enabling effective parking search applications. In particular, we demonstrate how iPark helps ordinary users annotate an existing digital map with two types of parking, on-street parking and parking zones, based on vehicular tracking data.

#index 1962375
#* Limosa: a system for geographic user interest analysis in Twitter
#@ Jan Vosecky;Di Jiang;Wilfred Ng
#t 2013
#c 8
#% 1482254
#% 1746002
#! In this demonstration, we present Limosa, an interactive system for visualization of geographic interests of users in Twitter. The system supports the modeling of comprehensive geographic characteristics of topics discussed in microblogs, both with respect to locations that postings originate from and also locations mentioned within the posting itself. Limosa then provides visualizations of geographic user interests, including the geographic scope of topics, terms, or the semantics associated with specific locations. Using a variety of recommendation strategies, we show that Limosa provides effective news and user recommendations.

#index 1962376
#* Accelerating spatial range queries
#@ Alexandros Stougiannis;Farhan Tauheed;Thomas Heinis;Anastasia Ailamaki
#t 2013
#c 8
#% 252304
#% 427199
#% 462059
#% 480093
#% 481455
#% 765430
#% 797419
#% 875016
#% 1846821
#! It is increasingly common for domain scientists to use computational tools to build and simulate spatial models of the phenomena they are studying. The spatial models they build are more and more detailed as well as dense and are consequently difficult to manage with today's tools. A crucial problem when analyzing spatial models of increasing detail is the scalable execution of range queries. State-of-the-art approaches like the R-Tree perform suboptimally on today's models and do not scale for more dense, future models. The problem is that the amount of overlap in the tree structure increases as a function of the level of detail/density in the model. In this demonstration we showcase ZOOM, a new tool to efficiently execute spatial range queries on increasingly detailed (denser) models. ZOOM is based on FLAT, a novel range query execution approach that effectively decouples the query execution time from the density of the dataset, thereby ensuring efficient query execution. At the core of the demonstration thus is the visualization of the novel query execution strategy of FLAT which we contrast with a visualization of the query execution of the R-Tree.

#index 1962377
#* An efficient layout method for a large collection of geographic data entries
#@ Sarana Nutanong;Marco D. Adelfio;Hanan Samet
#t 2013
#c 8
#% 282169
#% 427326
#% 547437
#% 794881
#% 818938
#% 927035
#% 1035400
#% 1135150
#% 1298864
#% 1511022
#% 1560254
#% 1667270
#% 1770335
#% 1940998
#% 1941070
#! Many spatial applications require the ability to display locations of geographic data entries on an online map. For example, an online photo-sharing service may wish to display photos (as thumbnails) according to where they were taken. Since displaying geographic data entries as thumbnails or icons on a map requires some amount of space, displayed entries can overlap each other. As a result, we may wish to discard less popular or older entries (based on a given measure of importance) so that these more popular or newer entries become more distinct. A straightforward solution is to apply a spatial database extension such as PostGIS (i) to retrieve entries within a given display window; (ii) to discard entries in proximity of a more important one. In this paper, we demonstrate our method for efficiently selecting distinct entries from a large geographical point set. Specifically, our demonstration software presents a voting system built upon an ensemble of interrelated indexes, which is the main novelty of our query processing method. This allows us to efficiently determine the degree of distinctiveness of all entries within a query window using simple index traversal operations rather than expensive spatial operations. The effectiveness of our method in comparison to a traditional spatial query is shown by our experimental results using a real dataset of over 9 million locations. These experimental results show that our proposed method is capable of consistently producing subsecond response times, while the spatial query-based method takes more than 10 seconds on average in a low spatial selectivity setting.

#index 1962378
#* In the Mood4: recommendation by examples
#@ Rubi Boim;Tova Milo
#t 2013
#c 8
#% 1328172
#% 1358747
#% 1471597
#% 1536564
#% 1641999
#! Traditional recommender systems generate personalized recommendations based on a profile that they create for each user. We argue here that such profiles are often too coarse to capture the current user's state of mind and desire. For example, a serious user that usually prefers documentary features may, at the end of a long and tiring conference, be in the mood for a lighter entertaining movie, not captured by her usual profile. As communicating one's state of mind to a system in (key)words may be difficult, we present in this demo Mood4 - a novel plug-in for recommender systems, which allows users to describe their current desire/mood through examples. Mood4 utilizes the user's examples to refine the recommendations generated by a given recommender system, considering several, possibly competing, desired properties of the recommended items set (rating, diversity, coverage). The system uses a novel algorithm, based on a simple geometric representation of the items, which allows for efficient processing and the generation of suitable recommendations even in the absence of semantic information.

#index 1962379
#* YmalDB: a result-driven recommendation system for databases
#@ Marina Drosou;Evaggelia Pitoura
#t 2013
#c 8
#% 893105
#% 894444
#% 915254
#% 1218714
#% 1328119
#% 1482250
#% 1642095
#! To assist users in database exploration, we present the YmalDB system, a database system enhanced with a recommendation functionality. Along with the results of each user query, YmalDB computes and presents to the users additional results, called Ymal (i.e., "You May Also Like") results, that are highly related with the results of their original query. Such results are computed using the most interesting sets of attribute values, called faSets, that appear either in the results of the original query or in the results of an appropriately expanded one. The interestingness of a faSet is based on its frequency both in the query result and in the database.

#index 1962380
#* CrowdSeed: query processing on microblogs
#@ Zhou Zhao;Wilfred Ng;Zhijun Zhang
#t 2013
#c 8
#% 1083641
#% 1581851
#% 1628171
#% 1688516
#% 1770349
#% 1770351
#% 1869838
#! Databases often offer poor answers with respect to judgemental queries such as asking the best among the movies shown in recent months. Processing such queries requires human input for providing missing information in order to clarify uncertainty or inconsistency in queries. Nowadays, it is common to see people seeking answers on micro-blogs through asking or sharing questions with their friends. This can be easily done via smart phones, which diffuse a question to a large number of users through message propagation in microblogs. This trend is important and known as CrowdSearch. Due to conflicting attitudes among crowds, the majority vote is employed as a crowd-wisdom aggregation schema. In this demo, we show the problem of minimizing the monetary cost of a crowdsourced query, given the specified expected accuracy of the aggregated answer. We present CrowdSeed, a system that automatically integrates human input for processing queries imposed on microblogs. We demonstrate the effectiveness and efficiency of our system using real world data, as well as presenting interesting results from a game called "Who is in the CrowdSeed?".

#index 1962381
#* Hive open research network platform
#@ Jung Hyun Kim;Xilun Chen;K. Selçuk Candan;Maria Luisa Sapino
#t 2013
#c 8
#% 428982
#% 1190063
#% 1270277
#% 1564132
#% 1581376
#% 1620204
#% 1919897
#! Did you ever return back from a conference, having met a lot of interesting folks, listened to many inspiring talks, or having your presentation welcomed with a barrage of (of course, constructive!) questions, wishing if only you managed to take record of all these during the event? We are developing the Hive Open Research Network, a social platform for fostering scientific interactions and reducing friction in scientific exchanges and the underlying integrated services supporting content personalization, preview, and social/scientific recommendations. Hive is a conference-centric, but cross-conference platform, where researchers can seed and expand their research networks, keep track of the technical research sessions they are attending, meet new colleagues, share their ideas, ask questions, give and receive comments, or simply keep and/or view records of interactions at a conference they have attended (or wanted to attend, but missed due to other commitments). In its core, Hive leverages dynamically evolving knowledge structures, including user connections, concept maps, co-authorship networks, content from papers and presentations, and contextual knowledge to create and to promote networks of peers. These peer networks support each other explicitly through direct communication or indirectly through collaborative filtering. Hive provides the following online integrated services: a) understanding the personal activity context through access patterns and analysis of user supplied content, b) context-aware resource discovery, including search, presentation, and exploration support within the scientific knowledge structures, and c) peer discovery, and peer driven resource and knowledge sharing and collaborative recommendations.

#index 1962382
#* Tuning in action
#@ Wei Cao;Dennis Shasha
#t 2013
#c 8
#% 893178
#% 1016220
#% 1016221
#% 1022308
#% 1207033
#! Imagine that your database has all the right indexes. Its buffer manager has been tuned to give a high hit ratio, the buffer fits in RAM, and the data is well distributed on disk. You're done, right? Well, no, because the application code might be poorly written. It might include delinquent design patterns. The demoed tuning tool AppSleuth will find those delinquent design patterns but it is the demo visitor's job to fix them.

#index 1962383
#* PostgreSQL anomalous query detector
#@ Bilal Shebaro;Asmaa Sallam;Ashish Kamra;Elisa Bertino
#t 2013
#c 8
#% 306097
#% 507694
#% 737361
#% 844725
#% 1072639
#% 1500550
#% 1573150
#% 1706199
#% 1897970
#! We propose to demonstrate the design, implementation, and the capabilities of an anomaly detection (AD) system integrated with a relational database management system (DBMS). Our AD system is trained by extracting relevant features from the parse-tree representation of the SQL commands, and then uses the DBMS roles as the classes for the bayesian classifier. In the detection phase, the maximum apriori probability role is chosen by the classifier which, if not matching the role associated with the SQL command, raises an alarm. We have implemented such system in the PostgreSQL DBMS, integrated with the statistics collection and the query processing mechanism of the DBMS. During the demonstration, our audience will be given the choice of training our system using either synthetic role-based SQL query traces based on probability sampling, or by entering their own set of training queries. In the subsequent detection mode, the audience can test the detection capabilities of the system by submitting arbitrary SQL commands. We will also allow the audience to generate arbitrary work loads to measure the overhead of the training phase and the detection phase of our AD mechanism on the performance of the DBMS.

#index 1962384
#* Processing XML queries and updates on map/reduce clusters
#@ Nicole Bidoit;Dario Colazzo;Noor Malla;Federico Ulliana;Maurizio Nolè;Carlo Sartiani
#t 2013
#c 8
#% 479465
#% 963669
#% 994015
#% 1181228
#% 1488674
#% 1881369
#% 1912327
#% 1920038
#! In this demo we will showcase a research prototype for processing queries and updates on large XML documents. The prototype is based on the idea of statically and dynamically partitioning the input document, so to distribute the computing load among the machines of a Map/Reduce cluster. Attendees will be able to run predefined queries and updates on documents conforming to the XMark schema, as well as to submit their own queries and updates.

#index 1962385
#* CISC: clustered image search by conceptualization
#@ Kaiqi Zhao;Enxun Wei;Qingyu Sui;Kenny Q. Zhu;Eric Lo
#t 2013
#c 8
#% 780874
#% 1432578
#% 1649028
#% 1649048
#! Clustering of images from search results can improve the user experience of image search. Most of the existing systems use both visual features and surrounding texts as signals for clustering while this paper demonstrates the use of an external knowledge base to make better sense out of the text signals in a prototype system called CISC. Once we understand the semantics of the text better, the result of the clustering is significantly improved. In addition to clustering the images by their semantic entities, our system can also conceptualize each image cluster into a set of concepts to represent the meaning of the cluster.

#index 1962386
#* MinExp-card: limiting data collection using a smart card
#@ Nicolas Anciaux;Walid Bezza;Benjamin Nguyen;Michalis Vazirgiannis
#t 2013
#c 8
#% 443463
#% 739335
#% 874989
#% 993943
#% 1090270
#% 1250752
#! Online services such as social care, tax services, bank loans and many others, request individuals to fill in application forms with hundreds of private data items, in order to calibrate their offer. In practice, far too much data is requested, leading to over data disclosure. As shown in our previous works, avoiding this problem would (1) improve the privacy of the applicants and (2) decrease costs for service providers. We demonstrate here a prototype designed and implemented in partnership with the General Council of Yvelines District in France. The prototype targets forms used to calibrate social care for dependant people. To maintain the privacy of the decision process used to calibrate the social care, we propose a smartcard implementation. We will show that a 50% reduction of the items exposed in application forms can be achieved, explore the quality and scalability of our smartcard implementation, and demonstrate its scope.

#index 1962387
#* PrivComp: a privacy-aware data service composition system
#@ Mahmoud Barhamgi;Djamal Benslimane;Youssef Amghar;Nora Cuppens-Boulahia;Frederic Cuppens
#t 2013
#c 8
#% 232066
#% 765448
#% 864414
#% 1743948
#! In this demo paper, we present a new privacy preserving composition execution system. Our system allows to execute queries over multiple data services without revealing any extra information to any of the involved services. None of involved services (and their providers) is able to infer any information about the data the other services provide beyond what is permitted

#index 1962388
#* ProQua: a system for evaluating logic-based scoring functions on uncertain relational data
#@ Sebastian Lehrack;Sascha Saretz;Christian Winkel
#t 2013
#c 8
#% 1021953
#% 1134501
#% 1312979
#% 1563349
#% 1599873
#% 1610454
#% 1615075
#% 1697247
#% 1919902
#% 1933425
#! ProQua is an innovative probabilistic database system which enables the application of logic-based and weighted similarity conditions on uncertain relation data. In this demonstration paper we describe the interrelations among the main concepts, present an archaeological example scenario and sketch the software architecture of ProQua.

#index 1962389
#* ProvenanceCurious: a tool to infer data provenance from scripts
#@ Mohammad Rezwanul Huq;Peter M. G. Apers;Andreas Wombacher
#t 2013
#c 8
#% 725423
#% 960365
#% 1616855
#% 1882097
#! The increasing data volume and highly complex models used in different domains make it difficult to debug models in cases of anomalies. Data provenance provides scientists sufficient information to investigate their models. In this paper, we propose a tool which can infer fine-grained data provenance based on a given script. The tool is demonstrated using a hydrological model. The tool is also tested successfully handling other scripts in different contexts.

#index 1962390
#* Trust and reputation in and across virtual communities
#@ Nurit Gal-Oz;Ehud Gudes
#t 2013
#c 8
#% 341929
#% 577367
#% 893123
#% 943777
#% 1253108
#% 1280702
#% 1445480
#% 1469900
#% 1531260
#% 1682181
#% 1719267
#! Trust and Reputation systems have become key enablers of positive interaction experiences on the Web. These systems accumulate information regarding activities of people or peers in general, to infer their reputation in some context or within a virtual community. Reputation information improves the quality of interactions between peers and reduces the effect of fraudulent members. In this tutorial we motivate the use of trust and reputation systems and survey some of the important models introduced in the past decade. Among these models, we present our work on the knot model, which deals with communities of strangers. Special attention is given to the way existing models tackle attempts to attack reputation systems. In a dynamic world, a person or a service may be a member of multiple communities and valuable information can be gained by sharing reputation of members among communities. In the second part of the tutorial, we present the CCR model for sharing reputation across virtual communities and address major privacy concerns related to it. In the third part of our talk, we discuss the use of reputation systems in other contexts, such as domain reputation for fighting malware, and outline our research directions on this subject.

#index 1962391
#* The W3C PROV family of specifications for modelling provenance metadata
#@ Paolo Missier;Khalid Belhajjame;James Cheney
#t 2013
#c 8
#% 464891
#% 810115
#% 826032
#% 893189
#% 1174009
#% 1231247
#% 1426581
#% 1486258
#% 1926167
#% 1926170
#% 1926177
#% 1942765
#% 1983024
#! Provenance, a form of structured metadata designed to record the origin or source of information, can be instrumental in deciding whether information is to be trusted, how it can be integrated with other diverse information sources, and how to establish attribution of information to authors throughout its history. The PROV set of specifications, produced by the World Wide Web Consortium (W3C), is designed to promote the publication of provenance information on the Web, and offers a basis for interoperability across diverse provenance management systems. The PROV provenance model is deliberately generic and domain-agnostic, but extension mechanisms are available and can be exploited for modelling specific domains. This tutorial provides an account of these specifications. Starting from intuitive and informal examples that present idiomatic provenance patterns, it progressively introduces the relational model of provenance along with the constraints model for validation of provenance documents, and concludes with example applications that show the extension points in use.

#index 1962392
#* Schema mappings and data examples
#@ Balder ten Cate;Phokion G. Kolaitis;Wang-Chiew Tan
#t 2013
#c 8
#% 333988
#% 378409
#% 809239
#% 810078
#% 824763
#% 1036084
#% 1065125
#% 1127370
#% 1206612
#% 1209667
#% 1215806
#% 1310057
#% 1424594
#% 1581857
#% 1625108
#% 1818419
#! A fundamental task in data integration and data exchange is the design of schema mappings, that is, high-level declarative specifications of the relationship between two database schemas. Several research prototypes and commercial systems have been developed to facilitate schema-mapping design; a common characteristic of these systems is that they produce a schema mapping based on attribute correspondences across schemas solicited from the user via a visual interface. This methodology, however, suffers from certain shortcomings. In the past few years, a fundamentally different methodology to designing and understanding schema mappings has emerged. This new methodology is based on the systematic use of data examples to derive, illustrate, and refine schema mappings. Example-driven schema-mapping design is currently an active area of research in which several different approaches towards using data examples in schema-mapping design have been explored. After a brief overview of the earlier methodology, this tutorial will provide a comprehensive overview of the different ways in which data examples can be used in schema-mapping design. In particular, it will cover the basic concepts, technical results, and prototype systems that have been developed in the past few years, as well as open problems and directions for further research in this area.

