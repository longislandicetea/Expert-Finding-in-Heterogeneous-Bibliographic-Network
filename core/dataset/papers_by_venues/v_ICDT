#index 303884
#* Information integration using logical views
#@ Jeffrey D. Ullman
#t 2000
#c 6

#index 303886
#* Conjunctive query containment revisited
#@ Chandra Chekuri;Anand Rajaraman
#t 2000
#c 6

#index 303887
#* Queries and computation on the Web
#@ Serge Abiteboul;Victor Vianu
#t 2000
#c 6

#index 303889
#* Games and total Datalog ¬ queries
#@ Jörg Flum;Max Kubierschky;Bertram Ludäscher
#t 2000
#c 6

#index 303891
#* Local properties of query languages
#@ Guozhu Dong;Leonid Libkin;Limsoon Wong
#t 2000
#c 6

#index 303893
#* A formula for incorporating weights into scoring rules
#@ Ronald Fagin;Edward L. Wimmers
#t 2000
#c 6

#index 454208
#* Proceedings of the 6th International Conference on Database Theory
#@ Foto N. Afrati;Phokion G. Kolaitis
#t 1997
#c 6

#index 454209
#* Proceedings of the 7th International Conference on Database Theory
#@ Catriel Beeri;Peter Buneman
#t 1999
#c 6

#index 454210
#* Proceedings of the 8th International Conference on Database Theory
#@ Jan Van den Bussche;Victor Vianu
#t 2001
#c 6

#index 454211
#* Proceedings of the 9th International Conference on Database Theory
#@ Diego Calvanese;Maurizio Lenzerini;Rajeev Motwani
#t 2003
#c 6

#index 464705
#* Concurrency Control Theory for Deferred Materialized Views
#@ Akira Kawaguchi;Daniel F. Lieuwen;Inderpal Singh Mumick;Dallan Quass;Kenneth A. Ross
#t 1997
#c 6

#index 464706
#* Selection of Views to Materialize in a Data Warehouse
#@ Himanshu Gupta
#t 1997
#c 6

#index 464707
#* Fine Hierarchies of Generic Computation
#@ Jerzy Tyszkiewicz
#t 1997
#c 6

#index 464708
#* Expressiveness and Complexity of Active Databases
#@ Philippe Picouet;Victor Vianu
#t 1997
#c 6

#index 464709
#* Efficient Complete Local Tests for Conjunctive Query Constraints with Negation
#@ Nam Huyn
#t 1997
#c 6

#index 464710
#* Object-Oriented Database Evolution
#@ Jean-Bernard Lagorce;Arunas Stockus;Emmanuel Waller
#t 1997
#c 6

#index 464711
#* Tractable Iteration Mechanisms for Bag Languages
#@ Latha S. Colby;Leonid Libkin
#t 1997
#c 6

#index 464712
#* Methods and Problems in Data Mining
#@ Heikki Mannila
#t 1997
#c 6

#index 464713
#* A Model Theoretic Approach to Update Rule Programs
#@ Nicole Bidoit;Sofian Maabout
#t 1997
#c 6

#index 464714
#* Discovering All Most Specific Sentences by Randomized Algorithms
#@ Dimitrios Gunopulos;Heikki Mannila;Sanjeev Saluja
#t 1997
#c 6

#index 464715
#* The Complexity of Iterated Belief Revision
#@ Paolo Liberatore
#t 1997
#c 6

#index 464716
#* Correspondence and Translation for Heterogeneous Data
#@ Serge Abiteboul;Sophie Cluet;Tova Milo
#t 1997
#c 6

#index 464717
#* Information Integration Using Logical Views
#@ Jeffrey D. Ullman
#t 1997
#c 6

#index 464718
#* Optimal Allocation of Two-Dimensional Data
#@ Khaled A. S. Abdel-Ghaffar;Amr El Abbadi
#t 1997
#c 6

#index 464719
#* Queries and Computation on the Web
#@ Serge Abiteboul;Victor Vianu
#t 1997
#c 6

#index 464720
#* Querying Semi-Structured Data
#@ Serge Abiteboul
#t 1997
#c 6

#index 464721
#* Total and Partial Well-Founded Datalog Coincide
#@ Jörg Flum;Max Kubierschky;Bertram Ludäscher
#t 1997
#c 6

#index 464722
#* Semantics and Containment with Internal and External Conjunctions
#@ Gösta Grahne;Nicolas Spyratos;Daniel Stamate
#t 1997
#c 6

#index 464723
#* Serializability of Nested Transactions in Multidatabases
#@ Ugur Halici;Budak Arpinar;Asuman Dogac
#t 1997
#c 6

#index 464724
#* Adding Structure to Unstructured Data
#@ Peter Buneman;Susan B. Davidson;Mary F. Fernandez;Dan Suciu
#t 1997
#c 6

#index 464725
#* Local Properties of Query Languages
#@ Guozhu Dong;Leonid Libkin;Limsoon Wong
#t 1997
#c 6

#index 464726
#* Incorporating User Preferences in Multimedia Queries
#@ Ronald Fagin;Edward L. Wimmers
#t 1997
#c 6

#index 464727
#* Conjunctive Query Containment Revisited
#@ Chandra Chekuri;Anand Rajaraman
#t 1997
#c 6

#index 464856
#* Efficient Indexing for Constraint and Temporal Databases
#@ Sridhar Ramaswamy
#t 1997
#c 6

#index 464857
#* Model-Theoretic Minimal Chenge Operators for Constraint Databases
#@ Peter Z. Revesz
#t 1997
#c 6

#index 464858
#* Structural Issues in Active Rule Systems
#@ James Bailey;Guozhu Dong;Kotagiri Ramamohanarao
#t 1997
#c 6

#index 464859
#* Performance of Nearest Neighbor Queries in R-Trees
#@ Apostolos Papadopoulos;Yannis Manolopoulos
#t 1997
#c 6

#index 464860
#* On Topological Elementary Equivalence of Spatial Databases
#@ Bart Kuijpers;Jan Paredaens;Jan Van den Bussche
#t 1997
#c 6

#index 464861
#* Type-Consistency Problems for Queries in Object-Oriented Databases
#@ Yasunori Ishihara;Hiroyuki Seki;Minoru Ito
#t 1997
#c 6

#index 464862
#* Novel Computational Approaches to Information Retrieval and Data Mining (Abstract)
#@ Christos H. Papadimitriou
#t 1999
#c 6
#! The realities and opportunities of the global information environment enable and necessitate new techniques and approaches, they expand the scope and the methodology of database theory. This talk surveys recent results of this nature, by the author and/or several collaborators, in two areas. In information retrieval, spectral methods have been introduced which extract the hidden semantics of a corpus by analysing the eigenvalues of related matrices. In fact, the performance of such methods can be theoretically predicted to be favorable in a certain statistical sense. A different spectral method has also been introduced successfully in the analysis of hypertext so as to identify authoritative sources of information. In data mining --the search for interesting patterns in data-- we argue that a meaningful definition of "interesting" requires consideration of the optimization problem the enterprise is facing. This "microeconomic" view leads quickly to certain novel and interesting computational problems.

#index 464863
#* Schemas for Integration and Translation of Structured and Semi-structured Data
#@ Catriel Beeri;Tova Milo
#t 1999
#c 6
#% 90847
#% 172927
#% 198479
#% 210214
#% 229827
#% 236409
#% 237192
#% 248024
#% 248799
#% 463919
#% 464716
#% 464724
#% 479465
#% 479783
#% 481923
#% 840584
#! With the emergence of the Web as a universal data repository, research has recently focused on data integration and data translation, and a common data model of semistructured data has been established. It is being realized, however, that having a common schema model is also necessary, to support tasks such as query formulation, decomposition and optimization, or declarative specification of data translation. In this paper we elaborate on the theoretical foundations of a middleware schema model. We present expressive and flexible schema definition languages, and investigate properties such as expressive power and the complexity of decision problems that are significant in the context of data translation and integration.

#index 464864
#* Definability and Descriptive Complexity on Databases of Bounded Tree-Width
#@ Martin Grohe;Julian Mariño
#t 1999
#c 6
#% 5804
#% 6787
#% 31482
#% 67883
#% 70756
#% 84546
#% 101944
#% 150197
#% 475714
#% 538954
#% 544229
#% 587453
#% 598376
#! We study the expressive power of various query languages on relational databases of bounded tree-width. Our first theorem says that fixed-point logic with counting captures polynomial time on classes of databases of bounded tree-width. This result should be seen on the background of an important open question of Chandra and Harel [7] asking whether there is a query language capturing polynomial time on unordered databases. Our theorem is a further step in a larger project of extending the scope of databases on which polynomial time can be captured by reasonable query languages. We then prove a general definability theorem stating that each query on a class of databases of bounded tree-width which is definable in monadic second-order logic is also definable in fixed-point logic (or datalog). Furthermore, for each k ≥ 1 the class of databases of tree-width at most k is definable in fixed-point logic. These results have some remarkable consequences concerning the definability of certain classes of graphs. Finally, we show that each database of tree-width at most k can be characterized up to isomorphism in the language Ck+3, the (k + 3)-variable fragment of first-order logic with counting.

#index 464865
#* Increasing the Expressiveness of Analytical Performance Models for Replicated Databases
#@ Matthias Nicola;Matthias Jarke
#t 1999
#c 6
#% 2157
#% 77005
#% 207725
#% 210179
#% 216033
#% 217560
#% 219482
#% 224894
#% 225006
#% 248825
#% 442713
#% 443076
#% 462641
#% 480256
#% 481584
#% 511166
#% 511171
#% 562180
#% 635834
#% 672534
#% 688641
#% 786939
#% 835744
#% 1013863
#% 1013977
#! The vast number of design options in replicated databases requires efficient analytical performance evaluations so that the considerable overhead of simulations or measurements can be focused on a few promising options. A review of existing analytical models in terms of their modeling assumptions, replication schemata considered, and network properties captured, shows that data replication and intersite communication as well as workload patterns should be modeled more accurately. Based on this analysis, we define a new modeling approach named 2RC (2-dimensional replication model with integrated communication). We derive a complete analytical queueing model for 2RC and demonstrate that it is of higher expressiveness than existing models. 2RC also yields a novel bottleneck analysis and permits to evaluate the trade-off between throughput and availability.

#index 464866
#* Description Logics and Their Relationships with Databases
#@ Maurizio Lenzerini
#t 1999
#c 6
#% 58347
#% 111922
#% 205398
#% 227528
#% 237189
#% 244095
#% 248026
#% 248819
#% 264856
#% 266095
#% 406898
#% 442977
#% 464717
#% 464720
#% 464724
#% 556363
#% 562303
#% 674973
#% 1274719
#! Description Logics are logics for representing and reasoning about classes of objects and their relationships. They can be seen as successors of semantic networks and frame systems, and have been investigated for more than a decade under different points of view, in particular, expressive power and computational complexity of reasoning. In this short paper, we introduce Description Logics, we compare Description Logics with Database models, and then discuss how Description Logics can be used for several tasks related to data management, in particular information integration, and semi-structured data modeling.

#index 464867
#* Tableau Techniques for Querying Information Sources through Global Schemas
#@ Gösta Grahne;Alberto O. Mendelzon
#t 1999
#c 6
#% 663
#% 94459
#% 145195
#% 191584
#% 198465
#% 248038
#% 264858
#% 268788
#% 285925
#% 366807
#% 384978
#% 416015
#% 464717
#% 481444
#% 481786
#% 481923
#% 490909
#% 599549
#! The foundational homomorphism techniques introduced by Chandra and Merlin for testing containment of conjunctive queries have recently attracted renewed interest due to their central role in information integration applications. We show that generalizations of the classical tableau representation of conjunctive queries are useful for computing query answers in information integration systems where information sources are modeled as views defined on a virtual global schema. We consider a general situation where sources may or may not be known to be correct and complete. We characterize the set of answers to a global query and give algorithms to compute a finite representation of this possibly infinite set, as well as its certain and possible approximations. We show how to rewrite a global query in terms of the sources in two special cases, and show that one of these is equivalent to the Information Manifold rewriting of Levy et al.

#index 464868
#* Issues Raised by Three Years of Developing PJama: An Orthogonally Persistent Platform for Java
#@ Malcolm P. Atkinson;Mick J. Jordan
#t 1999
#c 6
#% 178643
#% 221392
#% 235914
#% 248823
#% 258457
#% 376102
#% 381886
#% 387159
#% 393784
#% 403195
#% 435151
#% 480099
#% 513152
#% 555039
#% 555040
#% 555167
#% 555168
#% 555169
#% 555173
#% 555175
#! Orthogonal persistence is based on three principles that have been understood for nearly 20 years. PJama is a publically available prototype of a Java platform that supports orthogonal persistence. It is already capable of supporting substantial applications. The experience of applying the principles of orthogonal persistence to the Java programming language is described in the context of PJama. For example, issues arise over achieving orthogonality when there are classes that have a special relationship with the Java Virtual Machine. The treatment of static variables and the definition of reachability for classes and the handling of the keyword transient also pose design problems. The model for checkpointing the state of a computation, including live threads, is analyzed and related to a transactional approach. The problem of dealing with state that is external to the PJama environment is explained and the solutions outlined. The difficult problem of system evolution is identified as a major barrier to deploying orthogonal persistence for the Java language. The predominant focus is on semantic issues, but with concern for reasonably efficient implementation. We take the opportunity throughout the paper and in the conclusions to identify directions for further work.

#index 464869
#* On the Generation of 2-Dimensional Index Workloads
#@ Joseph M. Hellerstein;Lisa Hellerstein;George Kollios
#t 1999
#c 6
#% 86950
#% 102759
#% 137887
#% 153260
#% 164360
#% 191595
#% 213974
#% 237204
#% 248015
#% 248016
#% 248861
#% 252304
#% 365700
#% 427199
#% 462503
#% 481599
#% 481620
#% 481770
#% 482112
#% 503887
#! A large number of database index structures have been proposed over the last two decades, and little consensus has emerged regarding their relative effectiveness. In order to empirically evaluate these indexes, it is helpful to have methodologies for generating random queries for performance testing. In this paper we propose a domain-independent approach to the generation of random queries: choose randomly among all logically distinct queries. We investigate this idea in the context of range queries over 2-dimensional points. We present an algorithm that chooses randomly among logically distinct 2-d range queries. It has constant-time expected performance over uniformly distributed data, and exhibited good performance in experiments over a variety of real and synthetic data sets. We observe nonuniformities in the way randomly chosen logical 2-d range queries are distributed over a variety of spatial properties. This raises questions about the quality of the workloads generated from such queries. We contrast our approach with previous work that generates workloads of random spatial ranges, and we sketch directions for future work on the robust generation of workloads for studying index performance.

#index 464870
#* In Search of the Lost Schema
#@ Stéphane Grumbach;Giansalvatore Mecca
#t 1999
#c 6
#% 47369
#% 95620
#% 179696
#% 194118
#% 237194
#% 244103
#% 248808
#% 248809
#% 248848
#% 275917
#% 369849
#% 384978
#% 435157
#% 458745
#% 479471
#% 504443
#! We study the problem of rediscovering the schema of nested relations that have been encoded as strings for storage purposes. We consider various classes of encoding functions, and consider the markup encodings, which allow to find the schema without knowledge of the encoding function, under reasonable assumptions on the input data. Depending upon the encoding of empty sets, we propose two polynomial on-line algorithms (with different buffer size) solving the schema finding problem. We also prove that with a high probability, both algorithms find the schema after examining a fixed number of tuples, thus leading in practice to a linear time behavior with respect to the database size for wrapping the data. Finally, we show that the proposed techniques are well-suited for practical applications, such as structuring and wrapping HTML pages and Web sites.

#index 464871
#* Decidability of First-Order Logic Queries over Views
#@ James Bailey;Guozhu Dong
#t 1999
#c 6
#% 54225
#% 69283
#% 137871
#% 248026
#% 248038
#% 248039
#% 289266
#% 464717
#% 535516
#% 1499551
#! We study the problem of deciding satisfiability of first order logic queries over views, our aim being to delimit the boundary between the decidable and the undecidable fragments of this language. Views currently occupy a central place in database research, due to their role in applications such as information integration and data warehousing. Our principal result is the identification of an important decidable class of queries over unary conjunctive views. This extends the decidability of the classical class of first order sentences over unary relations (the Löwenheim class). We then demonstrate how extending this class leads to undecidability. In addition to new areas, our work also has relevance to extensions of results for related problems such as query containment, trigger termination, implication of dependencies and reasoning in description logics.

#index 464872
#* An Equational Chase for Path-Conjunctive Queries, Constraints, and Views
#@ Lucian Popa;Val Tannen
#t 1999
#c 6
#% 583
#% 34242
#% 53390
#% 69283
#% 83537
#% 111826
#% 116090
#% 116303
#% 120672
#% 145178
#% 169812
#% 188853
#% 189868
#% 201873
#% 205851
#% 210184
#% 210203
#% 237181
#% 248026
#% 286998
#% 287316
#% 287339
#% 289384
#% 368248
#% 384978
#% 395735
#% 463913
#% 464540
#% 464727
#% 562153
#% 563512
#% 598678
#% 599549
#% 836134
#! We consider the class of path-conjunctive queries and constraints (dependencies) defined over complex values with dictionaries. This class includes the relational conjunctive queries and embedded dependencies, as well as many interesting examples of complex value and oodb queries and integrity constraints. We show that some important classical results on containment, dependency implication, and chasing extend and generalize to this class.

#index 464873
#* Discovering Frequent Closed Itemsets for Association Rules
#@ Nicolas Pasquier;Yves Bastide;Rafik Taouil;Lotfi Lakhal
#t 1999
#c 6
#% 152934
#% 201075
#% 227917
#% 248791
#% 420062
#% 443082
#% 459020
#% 481290
#% 481754
#% 481779
#! In this paper, we address the problem of finding frequent itemsets in a database. Using the closed itemset lattice framework, we show that this problem can be reduced to the problem of finding frequent closed itemsets. Based on this statement, we can construct efficient data mining algorithms by limiting the search space to the closed itemset lattice rather than the subset lattice. Moreover, we show that the set of all frequent closed itemsets suffices to determine a reduced set of association rules, thus addressing another important data mining problem: limiting the number of rules produced without information loss. We propose a new algorithm, called A-Close, using a closure mechanism to find frequent closed itemsets. We realized experiments to compare our approach to the commonly used frequent itemset search approach. Those experiments showed that our approach is very valuable for dense and/or correlated data that represent an important part of existing databases.

#index 464874
#* On Capturing First-Order Topological Properties of Planar Spatial Databases
#@ Bart Kuijpers;Jan Van den Bussche
#t 1999
#c 6
#% 190332
#% 213966
#% 224744
#% 241166
#% 246560
#% 262550
#% 384978
#% 464860
#% 587366
#! Spatial databases are modeled as closed semi-algebraic subsets of the real plane. First-order logic over the reals (expanded with a symbol to address the database) provides a natural language for expressing properties of such databases. Motivated by applications in geographical information systems, this paper investigates the question of which topological properties can be thus expressed.We introduce a novel, two-tiered logic for expressing topological properties, called CL, which is subsumed by first-order logic over the reals. We put forward the question whether the two logics are actually equivalent (when restricting attention to topological properties). We answer this question affirmatively on the class of "region databases." We also prove a general result which further illustrates the power of the logic CL.

#index 464875
#* Optimizing Large Join Queries in Mediation Systems
#@ Ramana Yerneni;Chen Li;Jeffrey D. Ullman;Hector Garcia-Molina
#t 1999
#c 6
#% 554
#% 25998
#% 32877
#% 36683
#% 43161
#% 58376
#% 86949
#% 116303
#% 172041
#% 210166
#% 227987
#% 237203
#% 248859
#% 277327
#% 277328
#% 286916
#% 340302
#% 408396
#% 464700
#% 479452
#% 479938
#% 480430
#% 481429
#% 481923
#% 482115
#% 482116
#% 571091
#% 631868
#% 993494
#% 1014242
#! In data integration systems, queries posed to a mediator need to be translated into a sequence of queries to the underlying data sources. In a heterogeneous environment, with sources of diverse and limited query capabilities, not all the translations are feasible. In this paper, we study the problem of finding feasible and efficient query plans for mediator systems. We consider conjunctive queries on mediators and model the source capabilities through attribute-binding adornments. We use a simple cost model that focuses on the major costs in mediation systems, those involved with sending queries to sources and getting answers back. Under this metric, we develop two algorithms for source query sequencing - one based on a simple greedy strategy and another based on a partitioning scheme. The first algorithm produces optimal plans in some scenarios, and we show a linear bound on its worst case performance when it misses optimal plans. The second algorithm generates optimal plans in more scenarios, while having no bound on the margin by which it misses the optimal plans. We also report on the results of the experiments that study the performance of the two algorithms.

#index 464876
#* On Rectangular Partitionings in Two Dimensions: Algorithms, Complexity, and Applications
#@ S. Muthukrishnan;Viswanath Poosala;Torsten Suel
#t 1999
#c 6
#% 37250
#% 38736
#% 43163
#% 64102
#% 92299
#% 136704
#% 160213
#% 201921
#% 210190
#% 242366
#% 282029
#% 282658
#% 285932
#% 427219
#% 464876
#% 474693
#% 479648
#% 481266
#% 482092
#% 485251
#% 494333
#% 656735
#% 689389
#% 1068782
#% 1080840
#! Partitioning a multi-dimensional data set into rectangular partitions subject to certain constraints is an important problem that arises in many database applications, including histogram-based selectivity estimation, load-balancing, and construction of index structures. While provably optimal and efficient algorithms exist for partitioning one-dimensional data, the multi-dimensional problem has received less attention, except for a few special cases. As a result, the heuristic partitioning techniques that are used in practice are not well understood, and come with no guarantees on the quality of the solution. In this paper, we present algorithmic and complexity-theoretic results for the fundamental problem of partitioning a two-dimensional array into rectangular tiles of arbitrary size in a way that minimizes the number of tiles required to satisfy a given constraint. Our main results are approximation algorithms for several partitioning problems that provably approximate the optimal solutions within small constant factors, and that run in linear or close to linear time. We also establish the NP-hardness of several partitioning problems, therefore it is unlikely that there are efficient, i.e., polynomial time, algorithms for solving these problems exactly. We also discuss a few applications in which partitioning problems arise. One of the applications is the problem of constructing multi-dimensional histograms. Our results, for example, give an efficient algorithm to construct the V-Optimal histograms which are known to be the most accurate histograms in several selectivity estimation problems. Our algorithms are the first to provide guaranteed bounds on the quality of the solution.

#index 464877
#* View Disassembly
#@ Parke Godfrey;Jarek Gryz
#t 1999
#c 6
#% 36683
#% 69272
#% 91344
#% 100597
#% 198465
#% 408396
#% 442714
#% 442946
#% 459283
#% 462954
#% 464039
#% 481916
#% 497938
#% 709684
#! We explore a new form of view rewrite called view disassembly. The objective is to rewrite views in order to "remove" certain sub-views (or unfoldings) of the view. This becomes pertinent for complex views which may defined over other views and which may involve union. Such complex views arise necessarily in environments as data warehousing and mediation over heterogeneous databases. View disassembly can be used for view and query optimization, preserving data security, making use of cached queries and materialized views, and view maintenance. We provide computational complexity results of view disassembly. We show that the optimal rewrites for disassembled views is at least NP-hard. However, we provide good news too. We provide an approximation algorithm that has much better run-time behavior. We show a pertinent class of unfoldings for which their removal always results in a simpler disassembled view than the view itself. We also show the complexity to determine when a collection of unfoldings cover the view definition.

#index 464878
#* Selection of Views to Materialize Under a Maintenance Cost Constraint
#@ Himanshu Gupta;Inderpal Singh Mumick
#t 1999
#c 6
#% 25470
#% 135476
#% 199537
#% 201928
#% 210182
#% 210208
#% 227869
#% 286991
#% 462204
#% 464706
#% 479476
#% 480623
#% 482110
#% 482111
#! A data warehouse stores materialized views derived from one or more sources for the purpose of efficiently implementing decision-support or OLAP queries. One of the most important decisions in designing a data warehouse is the selection of materialized views to be maintained at the warehouse. The goal is to select an appropriate set of views that minimizes total query response time and/or the cost of maintaining the selected views, given a limited amount of resource such as materialization time, storage space, or total view maintenance time. In this article, we develop algorithms to select a set of views to materialize in a data warehouse in order to minimize the total query response time under the constraint of a given total view maintenance time. As the above maintenance-cost view-selection problem is extremely intractable, we tackle some special cases and design approximation algorithms. First, we design an approximation greedy algorithm for the maintenance-cost view-selection problem in OR view graphs, which arise in many practical applications, e.g., data cubes. We prove that the query benefit of the solution delivered by the proposed greedy heuristic is within 63% of that of the optimal solution. Second, we also design an A* heuristic, that delivers an optimal solution, for the general case of AND-OR view graphs. We implemented our algorithms and a performance study of the algorithms shows that the proposed greedy algorithm for OR view graphs almost always delivers an optimal solution.

#index 464879
#* The Data Warehouse of Newsgroups
#@ Himanshu Gupta;Divesh Srivastava
#t 1999
#c 6
#% 1921
#% 46803
#% 68089
#% 68091
#% 135476
#% 137893
#% 164362
#% 169817
#% 199537
#% 210182
#% 214112
#% 281731
#% 326878
#% 408396
#% 464706
#% 481627
#% 1068587
#% 1081214
#! Electronic newsgroups are one of the primary means for the dissemination, exchange and sharing of information. We argue that the current newsgroup model is unsatisfactory, especially when posted articles are relevant to multiple newsgroups. We demonstrate that considerable additional flexibility can be achieved by managing newsgroups in a data warehouse, where each article is a tuple of attribute-value pairs, and each newsgroup is a view on the set of all posted articles. Supporting this paradigm for a large set of newsgroups makes it imperative to efficiently support a very large number of views: this is the key difference between newsgroup data warehouses and conventional data warehouses. We identify two complementary problems concerning the design of such a newsgroup data warehouse. An important design decision that the system needs to make is which newsgroup views to eagerly maintain (i.e., materialize). We demonstrate the intractability of the general newsgroup-selection problem, consider various natural special cases of the problem, and present efficient exact/approximation algorithms and complexity hardness results for them. A second important task concerns the efficient incremental maintenance of the eagerly maintained newsgroups. The newsgroup-maintenance problem for our model of newsgroup definitions is a more general version of the classical point-location problem, and we design an I/O and CPU efficient algorithm for this problem.

#index 464880
#* A Framework for the Investigation of Aggregate Functions in Database Queries
#@ Luca Cabibbo;Riccardo Torlone
#t 1999
#c 6
#% 5379
#% 27056
#% 80217
#% 101937
#% 164376
#% 189868
#% 189870
#% 203159
#% 231873
#% 237188
#% 245655
#% 289370
#% 435157
#% 464372
#% 552970
#% 565497
#% 571170
#! In this paper we present a new approach for studying aggregations in the context of database query languages. Starting from a broad definition of aggregate function, we address our investigation from two different perspectives. We first propose a declarative notion of uniform aggregate function that refers to a family of scalar functions uniformly constructed over a vocabulary of basic operators by a bounded Turing Machine. This notion yields an effective tool to study the effect of the embedding of a class of built-in aggregate functions in a query language. All the aggregate functions most used in practice are included in this classification. We then present an operational notion of aggregate function, by considering a high-order folding constructor, based on structural recursion, devoted to compute numeric aggregations over complex values. We show that numeric folding over a given vocabulary is sometimes not able to compute, by itself, the whole class of uniform aggregate function over the same vocabulary. It turns out however that this limitation can be partially remedied by the restructuring capabilities of a query language.

#index 464881
#* Urn Models and Yao's Formula
#@ Danièle Gardy;Laurent Némirovski
#t 1999
#c 6
#% 117070
#% 205024
#% 320113
#% 321250
#% 481623
#! Yao's formula is one of the basic tools in any situation where one wants to estimate the number of blocks to be read in answer to some query. We show that such situations can be modelized by probabilistic urn models. This allows us to fully characterize the distribution probability of the number of selected blocks under uniformity assumptions, and to consider extensions to non-uniform block probabilities. We also obtain a computationnally efficient approximation of Yao's formula.

#index 464882
#* Answering Queries Using Materialized Views with Disjunctions
#@ Foto N. Afrati;Manolis Gergatsoulis;Theodoros G. Kavalieros
#t 1999
#c 6
#% 23898
#% 36683
#% 114723
#% 188853
#% 194120
#% 198465
#% 198466
#% 231671
#% 237190
#% 248038
#% 408458
#% 464056
#% 464717
#% 464720
#% 464727
#% 672627
#% 707146
#% 1068741
#! We consider the problem of answering datalog queries using materialized views. More specifically, queries are rewritten to refer to views instead of the base relations over which the queries were originally written. Much work has been done on program rewriting that produces an equivalent query. In the context of information integration, though, the importance of using views to infer as many answers as possible has been pointed out. Formally, the problem is: Given a datalog program P is there a datalog program Pv which uses only views as EDB predicates and (i) produces a subset of the answers that P produces and (ii) any other program P′v over the views with property (i) is contained in Pv? In this paper we investigate the problem in the case of disjunctive view definitions.

#index 464883
#* Index Structures for Path Expressions
#@ Tova Milo;Dan Suciu
#t 1999
#c 6
#% 11277
#% 31484
#% 66387
#% 91364
#% 139837
#% 172927
#% 198465
#% 210214
#% 244109
#% 340295
#% 369768
#% 442665
#% 462062
#% 464720
#% 464724
#% 479465
#% 593696
#% 600179

#index 464884
#* On the Orthographic Dimension of Constraint Databases
#@ Stéphane Grumbach;Philippe Rigaux;Luc Segoufin
#t 1999
#c 6
#% 13742
#% 36683
#% 164406
#% 190332
#% 213956
#% 245656
#% 248021
#% 248802
#% 257440
#% 260062
#% 268788
#% 527008
#% 552988
#% 836134
#! One of the most important advantages of constraint databases is their ability to represent and to manipulate data in arbitrary dimension within a uniform framework. Although the complexity of querying such databases by standard means such as first-order queries has been shown to be tractable for reasonable constraints (e.g. polynomial), it depends badly (roughly speaking exponentially) upon the dimension of the data. A precise analysis of the trade-off between the dimension of the input data and the complexity of the queries reveals that the complexity strongly depends upon the use the input makes of its dimensions. We introduce the concept of orthographic dimension, which, for a convex object O, corresponds to the dimension of the (component) objects O1; ..., On, such that O = O1 × ... × On. We study properties of databases with bounded orthographic dimension in a general setting of o-minimal structures, and provide a syntactic characterization of first-order orthographic dimension preserving queries. The main result of the paper concerns linear constraint databases. We prove that orthographic dimension preserving Boolean combination of conjunctive queries can be evaluated independently of the global dimension, with operators limited to the orthographic dimension, in parallel on the components. This results in an extremely efficient optimization mechanism, very easy to use in practical applications.

#index 464885
#* Databases for Tracking Mobile Units in Real Time
#@ Ouri Wolfson;Liqin Jiang;A. Prasad Sistla;Sam Chamberlain;Naphtali Rishe;Minglin Deng
#t 1999
#c 6
#% 16028
#% 100613
#% 164406
#% 168779
#% 191584
#% 194481
#% 245015
#% 380973
#% 384978
#% 461923
#% 464847
#! In this paper we consider databases representing information about moving objects (e.g. vehicles), particularly their location. We address the problems of updating and querying such databases. Specifically, the update problem is to determine when the location of a moving object in the database (namely its database location) should be updated. We answer this question by proposing an information cost model that captures uncertainty, deviation, and communication. Then we analyze dead-reckoning policies, namely policies that update the database location whenever the distance between the actual location and the database location exceeds a given threshold, x. Dead-reckoning is the prevalent approach in military applications, and our cost model enables us to determine the threshold x. Then we consider the problem of processing range queries in the database, and we propose a probabilistic algorithm to solve the problem.

#index 464886
#* Optimal Dynamic Range Searching in Non-replicating Index Structures
#@ Kothuri Venkata Ravi Kanth;Ambuj K. Singh
#t 1999
#c 6
#% 672
#% 32898
#% 63923
#% 68091
#% 68113
#% 86950
#% 88056
#% 201878
#% 227939
#% 237204
#% 248015
#% 248016
#% 281731
#% 321455
#% 370597
#% 411694
#% 427199
#% 464195
#% 481956
#% 482090
#% 494184
#! In this paper, we examine the complexity of multi-dimensional range searching in non-replicating index structures. Such nonreplicating structures achieve low storage costs and fast update times due to lack of multiple copies. We first obtain a lower bound for range searching in non-replicating structures. Assuming a simple tree structure model of an index, we prove that the worst-case time for a query retrieving t out of n data items is Ω((n/b)(d-1)/d + t/b), where d is the data dimensionality and b is the capacity of index nodes. We then propose a new index structure, called the O-tree, that achieves this query time in dynamic environments. Updates are supported in O(logb n) amortized time and exact match queries in O(logb n) worst-case time. This structure improves the query time of the best known non-replicating structure, the divided k-d tree, and is optimal for both queries and updates in non-replicating tree structures.

#index 464887
#* Transactions in Stack, Fork, and Join Composite Systems
#@ Gustavo Alonso;Armin Fessler;Guy Pardon;Hans-Jörg Schek
#t 1999
#c 6
#% 9241
#% 13017
#% 32897
#% 54033
#% 91076
#% 169820
#% 237199
#% 247424
#% 435104
#% 442853
#% 458568
#% 480259
#! Middleware tools are generally used to glue together distributed, heterogeneous systems into a coherent composite whole. Unfortunately, there is no clear conceptual framework in which to reason about transactional correctness in such an environment. This paper is a first attempt at developing such framework. Unlike most existing systems, where concurrent executions are controlled by a centralized scheduler, we will assume that each element in the system has its own independent scheduler receiving input from the schedulers of other elements and producing output for the schedules of yet other elements in the system. In this paper we analyze basic configurations of such composite systems and develop correctness criteria for each case. Moreover, we also show how these ideas can be used to characterize and improve different transaction models such as distributed transactions, sagas, and federated database transactions.

#index 464888
#* When Is ''Nearest Neighbor'' Meaningful?
#@ Kevin S. Beyer;Jonathan Goldstein;Raghu Ramakrishnan;Uri Shaft
#t 1999
#c 6
#% 120270
#% 145895
#% 164360
#% 169940
#% 188519
#% 198573
#% 212689
#% 212690
#% 217292
#% 227856
#% 227937
#% 227939
#% 237187
#% 238545
#% 282552
#% 317468
#% 435141
#% 460862
#% 463414
#% 464195
#% 481620
#% 481941
#! We explore the effect of dimensionality on the "nearest neighbor" problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10-15 dimensions. These results should not be interpreted to mean that high-dimensional indexing is never meaningful; we illustrate this point by identifying some high-dimensional workloads for which this effect does not occur. However, our results do emphasize that the methodology used almost universally in the database literature to evaluate high-dimensional indexing techniques is flawed, and should be modified. In particular, most such techniques proposed in the literature are not evaluated versus simple linear scan, and are evaluated over workloads for which nearest neighbor is not meaningful. Often, even the reported experiments, when analyzed carefully, show that linear scan would outperform the techniques being proposed on the workloads studied in high (10-15) dimensionality!

#index 464889
#* Incremental FO(+,
#@ Chaoyi Pang;Kotagiri Ramamohanarao;Guozhu Dong
#t 1999
#c 6
#% 13016
#% 121208
#% 142461
#% 164381
#% 175965
#% 198470
#% 245234
#% 287398
#% 381377
#% 427218
#% 459286
#% 464536
#% 534163
#! We give incremental algorithms, which support both edge insertions and deletions, for the all-pairs shortest-distance problem (APSD) in weighted undirected graphs. Our algorithms use first-order queries, + (addition) and O(n2) number of tuples, where n is the number of vertices, and have AC0 data complexity for integer weights. Since FO(+,

#index 464890
#* Constraint-based clustering in large databases
#@ Anthony K. H. Tung;Raymond T. Ng;Laks V. S. Lakshmanan;Jiawei Han
#t 2001
#c 6
#% 210173
#% 232716
#% 248785
#% 248792
#% 273899
#% 316709
#% 408396
#% 420082
#% 438137
#% 464890
#% 481281
#% 566128
#! Constrained clustering--finding clusters that satisfy user-specified constraints--is highly desirable in many applications. In this paper, we introduce the constrained clustering problem and show that traditional clustering algorithms (e.g., k-means) cannot handle it. A scalable constraint-clustering algorithm is developed in this study which starts by finding an initial solution that satisfies user-specified constraints and then refines the solution by performing confined object movements under constraints. Our algorithm consists of two phases: pivot movement and deadlock resolution. For both phases, we show that finding the optimal solution is NP-hard. We then propose several heuristics and show how our algorithm can scale up for large data sets using the heuristic of micro-cluster sharing. By experiments, we show the effectiveness and efficiency of the heuristics.

#index 464891
#* Why and Where: A Characterization of Data Provenance
#@ Peter Buneman;Sanjeev Khanna;Wang Chiew Tan
#t 2001
#c 6
#% 36181
#% 137864
#% 201928
#% 210214
#% 281149
#% 291299
#% 384978
#% 462072
#% 463919
#% 632040
#! With the proliferation of database views and curated databases, the issue of data provenance - where a piece of data came from and the process by which it arrived in the database - is becoming increasingly important, especially in scientific databases where understanding provenance is crucial to the accuracy and currency of data. In this paper we describe an approach to computing provenance when the data of interest has been created by a database query. We adopt a syntactic approach and present results for a general data model that applies to relational databases as well as to hierarchical data such as XML. A novel aspect of our work is a distinction between "why" provenance (refers to the source data that had some influence on the existence of the data) and "where" provenance (refers to the location(s) in the source databases from which the data was extracted).

#index 464892
#* On Answering Queries in the Presence of Limited Access Patterns
#@ Chen Li;Edward Y. Chang
#t 2001
#c 6
#% 23902
#% 36181
#% 54225
#% 116303
#% 122396
#% 140410
#% 156703
#% 164364
#% 198465
#% 198466
#% 248859
#% 273912
#% 273923
#% 289266
#% 299968
#% 368248
#% 464717
#% 464875
#% 464882
#% 479783
#% 481786
#% 599549
#% 632086
#% 707146
#! Abstract. In information-integration systems, source relations often have limitations on access patterns to their data; i.e., when one must provide values for certain attributes of a relation in order to retrieve its tuples. In this paper we consider the following fundamental problem: can we compute the complete answer to a query by accessing the relations with legal patterns? The complete answer to a query is the answer that we could compute if we could retrieve all the tuples from the relations. We give algorithms for solving the problem for various classes of queries, including conjunctive queries, unions of conjunctive queries, and conjunctive queries with arithmetic comparisons. We prove the problem is undecidable for datalog queries. If the complete answer to a query cannot be computed, we often need to compute its maximal answer. The second problem we study is, given two conjunctive queries on relations with limited access patterns, how to test whether the maximal answer to the first query is contained in the maximal answer to the second one? We show this problem is decidable using the results of monadic programs.

#index 464893
#* A Semi-monad for Semi-structured Data
#@ Mary F. Fernandez;Jérôme Siméon;Philip Wadler
#t 2001
#c 6
#% 5379
#% 67878
#% 80217
#% 103832
#% 116091
#% 125595
#% 145178
#% 163444
#% 189868
#% 210184
#% 224612
#% 235740
#% 235914
#% 245655
#% 281149
#% 300143
#% 313640
#% 384978
#% 400361
#% 425792
#% 504575
#% 504578
#% 516682
#% 541980
#% 562135
#% 571040
#! This document proposes an algebra for XML Query. The algebra has been submitted to the W3C XML Query Working Group. A novel feature of the algebra is the use of regular-expression types, similar in power to DTDs or XML Schemas, and closely related to Hasoya and Pierce's work on Xduce. The iteration construct is based on the notion of a monad, and involves novel typing rules not encountered elsewhere.

#index 464894
#* Minimizing View Sets without Losing Query-Answering Power
#@ Chen Li;Mayank Bawa;Jeffrey D. Ullman
#t 2001
#c 6
#% 116303
#% 135476
#% 198465
#% 199537
#% 237190
#% 248038
#% 289266
#% 299945
#% 300139
#% 464203
#% 464717
#% 464867
#% 464882
#% 479950
#% 480149
#% 480623
#% 481923
#% 599549
#% 707146
#! The problem of answering queries using views has been studied extensively due to its relevance in a wide variety of data-management applications. In these applications, we often need to select a subset of views to maintain due to limited resources. In this paper, we show that traditional query containment is not a good basis for deciding whether or not a view should be selected. Instead, we should minimize the view set without losing its query-answering power. To formalize this notion, we first introduce the concept of "p-containment." That is, a view set V is p-contained in another view set W, if W can answer all the queries that can be answered by V. We show that p-containment and the traditional query containment are not related. We then discuss how to minimize a view set while retaining its query-answering power. We develop the idea further by considering p-containment of two view sets with respect to a given set of queries, and consider their relationship in terms of maximally-contained rewritings of queries using the views.

#index 464895
#* Algebraic Rewritings for Optimizing Regular Path Queries
#@ Gösta Grahne;Alex Thomo
#t 2001
#c 6
#% 197751
#% 198465
#% 237190
#% 237191
#% 241136
#% 248025
#% 248038
#% 273700
#% 273924
#% 281149
#% 291299
#% 299967
#% 384978
#% 404772
#% 440518
#% 462235
#% 464717
#% 464720
#% 464724
#% 464867
#% 464883
#% 504583
#% 632039
#% 1373483
#! Rewriting queries using views is a powerful technique that has applications in query optimization, data integration, data warehousing etc. Query rewriting in relational databases is by now rather well investigated. However, in the framework of semistructured data the problem of rewriting has received much less attention. In this paper we focus on extracting as much information as possible from algebraic rewritings for the purpose of optimizing regular path queries. The cases when we can find a complete exact rewriting of a query using a set a views are very "ideal." However, there is always information available in the views, even if this information is only partial. We introduce "lower" and "possibility" partial rewritings and provide algorithms for computing them. These rewritings are algebraic in their nature, i.e. we use only the algebraic view definitions for computing the rewritings. This fact makes them a main memory product which can be used for reducing secondary memory and remote access. We give two algorithms for utilizing the partial lower and partial possibility rewritings in the context of query optimization.

#index 464896
#* Orthogonal Range Queries in OLAP
#@ Chung Keung Poon
#t 2001
#c 6
#% 186
#% 1606
#% 1731
#% 11274
#% 37861
#% 64532
#% 68113
#% 69474
#% 227866
#% 268126
#% 289322
#% 319601
#% 319603
#% 321455
#% 461921
#% 464215
#% 479822
#% 631947
#% 682366
#! We study the problem of pre-computing auxillary information to support on-line range queries for the sum and max functions on a datacube. For a d-dimensional datacube with size n in each dimension, we propose a data structure for range max queries with O((4L)d) query time and O((12L2n1/Lγ(n))d) update time where L ∈ {1,..., log n} is a user-controlled parameter and γ(n) is a slow-growing function. (For example, γ(n) ≤ log* n and γ(24110) = 3.) The data structure uses O((6nγ(n))d) storage and can be initialized in time linear to its size. There are three major techniques employed in designing the data structure, namely, a technique for trading query and update times, a technique for trading query time and storage and a technique for extending 1-dimensional data structures to d-dimensional ones. Our techniques are also applicable to range queries over any semi-group and group operation, such as min, sum and count.

#index 464897
#* Mining for Empty Rectangles in Large Data Sets
#@ Jeff Edmonds;Jarek Gryz;Dongming Liang;Renée J. Miller
#t 2001
#c 6
#% 10131
#% 10341
#% 152934
#% 210173
#% 227953
#% 408396
#% 479814
#% 480124
#% 520745
#% 1271849
#! Many data mining approaches focus on the discovery of similar (and frequent) data values in large data sets. We present an alternative, but complementary approach in which we search for empty regions in the data. We consider the problem of finding all maximal empty rectangles in large, two-dimensional data sets. We introduce a novel, scalable algorithm for finding all such rectangles. The algorithm achieves this with a single scan over a sorted data set and requires only a small bounded amount of memory. We also describe an algorithm to find all maximal empty hyper-rectangles in a multi-dimensional space. We consider the complexity of this search problem and present new bounds on the number of maximal empty hyper-rectangles. We briefly overview experimental results obtained by applying our algorithm to a synthetic data set.

#index 464898
#* Expressiveness Issues and Decision Problems for Active Database Event Queries
#@ James Bailey;Szabolcs Mikulás
#t 2001
#c 6
#% 1729
#% 196999
#% 264776
#% 385995
#% 442962
#% 442970
#% 480938
#% 481448
#% 571041
#% 631974
#% 834987
#% 1373474
#! A key facility of active database management systems is their ability to detect and react to the occurrence of events. Such events can be either atomic in nature, or specified using an event algebra to form complex events. An important role of an event algebra is to define the semantics of when events become invalid (event consumption). In this paper, we examine a simple event algebra and provide a logical framework for specification of various consumption policies. We then study the problems of equivalence and implication, identifying a powerful class of complex events for which equivalence is decidable. We then demonstrate how extensions of this class lead to undecidability.

#index 465025
#* Scalar Aggregation in FD-Inconsistent Databases
#@ Marcelo Arenas;Leopoldo E. Bertossi;Jan Chomicki
#t 2001
#c 6
#% 663
#% 102787
#% 156962
#% 264858
#% 272734
#% 273687
#% 464050
#% 519568
#% 591534
#% 598376
#! We consider here scalar aggregation queries in databases that may violate a given set of functional dependencies. We show how to compute consistent answers (answers true in every minimal repair of the database) to such queries. We provide a complete characterization of the computational complexity of this problem. We also show how tractability can be obtained in several special cases (one involves a novel application of the perfect graph theory) and present a practical hybrid query evaluation method.

#index 465026
#* Cost Based Data Dissemination in Broadcast Networks with Disconnection
#@ Bo Xu;Ouri Wolfson;Sam Chamberlain
#t 2001
#c 6
#% 780
#% 4800
#% 77005
#% 114559
#% 124019
#% 131055
#% 210179
#% 223887
#% 227885
#% 235846
#% 281441
#% 281557
#% 413174
#% 464065
#% 483438
#% 503869
#! We consider the problem of data dissemination in a broadcast network. In contrast to previously studied models, broadcasting is among peers, rather than client server. Such a model represents, for example, satellite communication among widely distributed nodes, sensor networks, and mobile ad-hoc networks. We introduce a cost model for data dissemination in peer to peer broadcast networks. The model quantifies the tradeoff between the inconsistency of the data, and its transmission cost; the transmission cost may be given in terms of dollars, energy, or bandwidth. Using the model we first determine the parameters for which eager (i.e. consistent) replication has a lower cost than lazy (i.e. inconsistent) replication. Then we introduce a lazy broadcast policy and compare it with several naive or traditional approaches to solving the problem.

#index 465027
#* Asymptotically Optimal Declustering Schemes for Range Queries
#@ Rakesh K. Sinha;Randeep Bhatia;Chung-Min Chen
#t 2001
#c 6
#% 43179
#% 227856
#% 261738
#% 286962
#% 299983
#% 339622
#% 461922
#% 462233
#% 464718
#% 469603
#% 481109
#% 565263
#% 609704
#% 631955
#% 631956
#% 632069
#% 637794
#! Declustering techniques have been widely adopted in parallel storage systems (e.g. disk arrays) to speed up bulk retrieval of multidimensional data. A declustering scheme distributes data items among multiple devices, thus enabling parallel I/O access and reducing query response time. We measure the performance of any declustering scheme as its worst case additive deviation from the ideal scheme. The goal thus is to design declustering schemes with as small an additive error as possible. We describe a number of declustering schemes with additive error O(log M) for 2-dimensional range queries, where M is the number of disks. These are the first results giving such a strong bound for any value of M. Our second result is a lower bound on the additive error. In 1997, Abdel-Ghaffar and Abbadi showed that except for a few stringent cases, additive error of any 2-dim declustering scheme is at least one. We strengthen this lower bound to Ω((log M)d-1/2 for d-dim schemes and to Ω(log M) for 2-dim schemes, thus proving that the 2-dim schemes described in this paper are (asymptotically) optimal. These results are obtained by establishing a connection to geometric discrepancy, a widely studied area of mathematics. We also present simulation results to evaluate the performance of these schemes in practice.

#index 465028
#* Reasoning about Summarizability in Heterogeneous Multidimensional Schemas
#@ Carlos A. Hurtado;Alberto O. Mendelzon
#t 2001
#c 6
#% 77312
#% 152588
#% 210182
#% 223781
#% 287047
#% 464215
#% 480123
#% 482081
#% 503731
#% 562299
#% 631946
#% 993439
#! In OLAP applications, data are modeled as points in a multidimensional space. Dimensions themselves have structure, described by a schema and an instance; the schema is basically a directed acyclic graph of granularity levels, and the instance consists of a set of elements for each level and mappings between these elements, usually called rollup functions. Current dimension models restrict dimensions in various ways; for example, rollup functions are restricted to be total. We relax these restrictions, yielding what we call heterogeneous schemas, which describe more naturally and cleanly many practical situations. In the context of heterogeneous schemas, the notion of summarizability becomes more complex. An aggregate view defined at some granularity level is summarizable from a set of precomputed views defined at other levels if the rollup functions can be used to compute the first view from the set of views. In order to study summarizability in heterogeneous schemas, we introduce a class of constraints on dimension instances that enrich the semantics of dimension hierarchies, and we show how to use the constraints to characterize and test for summarizability.

#index 465029
#* Expressive Power of SQL
#@ Leonid Libkin
#t 2001
#c 6
#% 42977
#% 55882
#% 84275
#% 145182
#% 159227
#% 188350
#% 189868
#% 217086
#% 231873
#% 235370
#% 245655
#% 257866
#% 289370
#% 289557
#% 303891
#% 307249
#% 307258
#% 384978
#% 399235
#% 562301
#% 587434
#% 587507
#! It is a folk result in database theory that SQL cannot express recursive queries such as reachability; in fact, a new construct was added to SQL3 to overcome this limitation. However, the evidence for this claim is usually given in the form of a reference to a proof that relational algebra cannot express such queries. SQL, on the other hand, in all its implementations has three features that fundamentally distinguish it from relational algebra: namely, grouping, arithmetic operations, and aggregation. In the past few years, most questions about the additional power provided by these features have been answered. This paper surveys those results, and presents new simple and self-contained proofs of the main results on the expressive power of SQL. Somewhat surprisingly, tiny differences in the language definition affect the results in a dramatic way: under some very natural assumptions, it can be proved that SQL cannot define recursive queries, no matter what aggregate functions and arithmetic operations are allowed. But relaxing these assumptions just a tiny bit makes the problem of proving expressivity bounds for SQL as hard as some long-standing open problems in complexity theory.

#index 465030
#* Estimating Range Queries Using Aggregate Data with Integrity Constraints: A Probabilistic Approach
#@ Francesco Buccafurri;Filippo Furfaro;Domenico Saccà
#t 2001
#c 6
#% 201921
#% 210182
#% 210190
#% 223781
#% 227883
#% 242366
#% 248812
#% 259995
#% 273909
#% 411554
#% 464215
#% 482095
#% 482123
#! In fast OLAP applications it is often advantageous to provide approximate answers to range queries in order to achieve very high performances. A possible solution is to inquire summary data rather than the original ones and to perform suitable interpolations. Approximate answers become mandatory in situations where only aggregate data are available. This paper studies the problem of estimating range queries (namely, sum and count) over aggregate data using a probabilistic approach for computing expected value and variance of the answers. The novelty of this approach is the exploitation of possible integrity constraints about the presence of elements in the range that are known to be null or nonnull. Closed formulas for all results are provided, and some interesting applications for query estimations on histograms are discussed.

#index 465031
#* On the Surprising Behavior of Distance Metrics in High Dimensional Spaces
#@ Charu C. Aggarwal;Alexander Hinneburg;Daniel A. Keim
#t 2001
#c 6
#% 227939
#% 237187
#% 248796
#% 280452
#% 427199
#% 435141
#% 462239
#% 464888
#% 479649
#% 480132
#! In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a efficiency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used Lk norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric (L1 norm) is consistently more preferable than the Euclidean distance metric (L2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lk norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.

#index 465032
#* Query Evaluation via Tree-Decompositions
#@ Jörg Flum;Markus Frick;Martin Grohe
#t 2001
#c 6
#% 451
#% 93660
#% 101921
#% 101944
#% 139753
#% 145687
#% 150197
#% 191611
#% 210028
#% 219474
#% 248033
#% 273683
#% 384978
#% 408638
#% 464727
#% 593867
#% 598376
#% 993437
#% 1273786
#! A number of efficient methods for evaluating first-order and monadicsecond order queries on finite relational structures are based on tree-decompositions of structures or queries. We systematically study these methods. In the first-part of the paper we consider tree-like structures. We generalize a theorem of Courcelle [7] by showing that on such structures a monadic second-order formula (with free first-order and second-order variables) can be evaluated in time linear in the structure size plus the size of the output. In the second part we study treelike formulas. We generalize the notions of acyclicity and bounded tree-width from conjunctive queries to arbitrary first-order formulas in a straightforward way and analyze the complexity of evaluating formulas of these fragments. Moreover, we show that the acyclic and bounded tree-width fragments have the same expressive power as the well-known guarded fragment and the finite-variable fragments of first-order logic, respectively.

#index 465033
#* Parallelizing the Data Cube
#@ Frank K. H. A. Dehne;Todd Eavis;Susanne E. Hambrusch;Andrew Rau-Chaplin
#t 2001
#c 6
#% 2833
#% 113694
#% 144423
#% 210182
#% 213791
#% 227880
#% 232620
#% 248023
#% 273916
#% 276705
#% 282930
#% 289334
#% 420053
#% 420069
#% 470561
#% 479450
#% 481948
#% 481951
#% 560666
#% 595409
#% 617012
#% 637791
#! This paper presents a general methodology for the efficient parallelization of existing data cube construction algorithms. We describe two different partitioning strategies, one for top-down and one for bottom-up cube algorithms. Both partitioning strategies assign subcubes to individual processors in such a way that the loads assigned to the processors are balanced. Our methods reduce inter-processor communication overhead by partitioning the load in advance instead of computing each individual group-by in parallel as is done in previous parallel approaches. In fact, after the initial load distribution phase, each processor can compute its assigned subcube without any communication with the other processors. Our methods enable code reuse by permitting the use of existing sequential (external memory) data cube algorithms for the subcube computations on each processor. This supports the transfer of optimized sequential data cube code to a parallel setting. The bottom-up partitioning strategy balances the number of single attribute external memory sorts made by each processor. The top-down strategy partitions a weighted tree in which weights reflect algorithm specific cost measures like estimated group-by sizes. Both partitioning approaches can be implemented on any shared disk type parallel machine composed of p processors connected via an interconnection fabric and with access to a shared parallel disk array. Experimental results presented show that our partitioning strategies generate a close to optimal load balance between processors.

#index 465034
#* Towards Aggregated Answers for Semistructured Data
#@ Holger Meuss;Klaus U. Schulz;François Bry
#t 2001
#c 6
#% 3463
#% 36683
#% 163445
#% 236416
#% 237191
#% 268797
#% 273701
#% 275423
#% 281149
#% 281150
#% 291299
#% 299976
#% 308463
#% 339373
#% 356855
#% 408396
#% 442887
#% 464883
#% 479465
#% 504571
#% 593867
#% 599549
#% 1783150

#index 465035
#* FUN: An Efficient Algorithm for Mining Functional and Embedded Dependencies
#@ Noel Novelli;Rosine Cicchetti
#t 2001
#c 6
#% 6710
#% 6714
#% 18398
#% 23878
#% 89751
#% 125557
#% 129570
#% 169324
#% 169370
#% 189872
#% 232136
#% 250473
#% 267604
#% 279167
#% 287295
#% 287798
#% 289446
#% 387089
#% 420062
#% 458762
#% 462214
#% 462472
#% 464199
#% 464203
#% 464837
#% 464873
#% 481290
#! Discovering functional dependencies from existing databases is an important technique strongly required in database design and administration tools. Investigated for long years, such an issue has been recently addressed with a data mining viewpoint, in a novel and more efficient way by following from principles of level-wise algorithms. In this paper, we propose a new characterization of minimal functional dependencies which provides a formal framework simpler than previous proposals. The algorithm, defined for enforcing our approach has been implemented and experimented. It is more efficient (in whatever configuration of original data) than the best operational solution (according to our knowledge): the algorithm Tane. Moreover, our approach also performs (without additional execution time) the mining of embedded functional dependencies, i.e. dependencies holding for a subset of the attribute set initially considered (e.g. for materialized views widely used in particular for managing data warehouses).

#index 465036
#* Axiomatization of Frequent Sets
#@ Toon Calders;Jan Paredaens
#t 2001
#c 6
#% 3034
#% 42485
#% 73571
#% 152934
#% 167626
#% 273899
#% 300120
#% 481290
#% 1290137
#! In data mining association rules are very popular. Most of the algorithms in the literature for finding association rules start by searching for frequent itemsets. The itemset mining algorithms typically interleave brute force counting of frequencies with a meta-phase for pruning parts of the search space. The knowledge acquired in the counting phases can be represented by frequent set expressions. A frequent set expression is a pair containing an itemset and a frequency indicating that the frequency of that itemset is greater than or equal to the given frequency. A system of frequent sets is a collection of such expressions. We give an axiomatization for these systems. This axiomatization characterizes complete systems. A system is complete when it explicitly contains all information that it logically implies. Every system of frequent sets has a unique completion. The completion of a system actually represents the knowledge that maximally can be derived in the meta-phase.

#index 465037
#* Subsumption for XML types
#@ Gabriel M. Kuper;Jérôme Siméon
#t 2001
#c 6
#% 40983
#% 214091
#% 246333
#% 248799
#% 273702
#% 273922
#% 281149
#% 299942
#% 299944
#% 300143
#% 300157
#% 313564
#% 390132
#% 462235
#% 464724
#% 464863
#% 479465
#% 479956
#% 504445
#% 504575
#% 799999
#! XML data is often used (validated, stored, queried, etc) with respect to different types. Understanding the relationship between these types can provide important information for manipulating this data. We propose a notion of subsumption for XML to capture such relationships. Subsumption relies on a syntactic mapping between types, and can be used for facilitating validation and query processing. We study the properties of subsumption, in particular the notion of the greatest lower bound of two schemas, and show how this can be used as a guide for selecting a storage structure. While less powerful than inclusion, subsumption generalizes several other mechanisms for reusing types, notably extension and refinement from XML Schema, and subtyping.

#index 465038
#* On Optimizing Nearest Neighbor Queries in High-Dimensional Data Spaces
#@ Stefan Berchtold;Christian Böhm;Daniel A. Keim;Florian Krebs;Hans-Peter Kriegel
#t 2001
#c 6
#% 86950
#% 102772
#% 121989
#% 131061
#% 169940
#% 201876
#% 227856
#% 227939
#% 237187
#% 238764
#% 248017
#% 248831
#% 317313
#% 317380
#% 435141
#% 463414
#% 464195
#% 479649
#% 481956
#% 527026
#! Nearest-neighbor queries in high-dimensional space are of high importance in various applications, especially in content-based indexing of multimedia data. For an optimization of the query processing, accurate models for estimating the query processing costs are needed. In this paper, we propose a new cost model for nearest neighbor queries in high-dimensional space, which we apply to enhance the performance of high-dimensional index structures. The model is based on new insights into effects occurring in high-dimensional space and provides a closed formula for the processing costs of nearest neighbor queries depending on the dimensionality, the block size and the database size. From the wide range of possible applications of our model, we select two interesting samples: First, we use the model to prove the known linear complexity of the nearest neighbor search problem in high-dimensional space, and second, we provide a technique for optimizing the block size. For data of medium dimensionality, the optimized block size allows significant speed-ups of the query processing time when compared to traditional block sizes and to the linear scan.

#index 465039
#* On Decidability and Complexity of Description Logics with Uniqueness Constraints
#@ Vitaliy L. Khizder;David Toman;Grant E. Weddell
#t 2001
#c 6
#% 33376
#% 58347
#% 87891
#% 114579
#% 135873
#% 175746
#% 248024
#% 264856
#% 273686
#% 299943
#% 442879
#% 442977
#% 459240
#% 459291
#% 464719
#% 711915
#! We establish the equivalence of: (1) the logical implication problem for a description logic dialect called DLClass that includes a concept constructor for expressing uniqueness constraints, (2) the logical implication problem for path functional dependencies (PFDs), and (3) the problem of answering queries in deductive databases with limited use of successor functions. As a consequence, we settle an open problem concerning lower bounds for the PFD logical implication problem and show that a regularity condition for DLClass that ensures low order polynomial time decidability for its logical implication problem is tight.

#index 465040
#* The Dynamic Complexity of Transitive Closure Is In DynTC0
#@ William Hesse
#t 2001
#c 6
#% 17288
#% 126317
#% 183611
#% 188352
#% 245652
#% 249173
#% 562156
#% 562302
#! This paper presents a fully dynamic algorithm for maintaining the transitive closure of a directed graph. All updates and queries can be computed by constant depth threshold circuits of polynomial size (TC0 circuits). This places transitive closure in the dynamic complexity class DynTC0, and implies that transitive closure can be maintained in databases using updates written in a first order query language plus counting operators, while keeping the size of the database polynomial in the size of the graph.

#index 465041
#* A Theory of Transactions on Recoverable Search Trees
#@ Seppo Sippu;Eljas Soisalon-Soininen
#t 2001
#c 6
#% 6716
#% 9241
#% 36118
#% 91076
#% 114582
#% 116063
#% 116085
#% 116087
#% 137940
#% 166215
#% 194942
#% 201869
#% 247424
#% 286929
#% 403195
#% 480589
#% 481256
#% 571093
#! We consider transactions running on a database that consists of records with unique totally-ordered keys and is organized as a sparse primary search tree such as a B-tree index on disk storage.We extend the classical read-write model of transactions by considering inserts, deletes and key-range scans and by distinguishing between four types of transaction states: forward-rolling, committed, backward-rolling, and rolled-back transactions. A search-tree transaction is modelled as a two-level transaction containing structure modifications as open nested subtran-sactions that can commit even though the parent transaction aborts. Isolation conditions are defined for search-tree transactions with nested structure modifications that guarantee the structural consistency of the search tree, a required isolation level (including phantom prevention) for database operations, and recoverability for structure modifications and database operations.

#index 465042
#* Deciding Termination of Query Evaluation in Transitive-Closure Logics for Constraint Databases
#@ Floris Geerts;Bart Kuijpers
#t 2003
#c 6
#% 147801
#% 169733
#% 190332
#% 299972
#% 299973
#% 299974
#% 330091
#% 335402
#% 410601
#% 472881
#% 505561
#% 565499
#% 569214
#! We study extensions of first-order logic over the reals with different types of transitive-closure operators as query languages for constraint databases that can be described by Boolean combinations of polynomial inequalities over the reals. We are in particular interested in deciding the termination of the evaluation of queries expressible in these transitive-closure logics. It turns out that termination is undecidable in general. However, we show that the termination of the transitive closure of a continuous function graph in the two-dimensional plane, viewed as a binary relation over the reals, is decidable, and even expressible in first-order logic over the reals. Based on this result, we identify a particular transitive-closure logic for which termination of query evaluation is decidable and which is more expressive than first-order logic over the reals. Furthermore, we can define a guarded fragment in which exactly the terminating queries of this language are expressible.

#index 465043
#* Decidable Containment of Recursive Queries
#@ Diego Calvanese;Giuseppe De Giacomo;Moshe Y. Vardi
#t 2003
#c 6
#% 3997
#% 36181
#% 53400
#% 54225
#% 140410
#% 154067
#% 164371
#% 166208
#% 198473
#% 210176
#% 210214
#% 211987
#% 237191
#% 244095
#% 248025
#% 261741
#% 283052
#% 285926
#% 289266
#% 291299
#% 292677
#% 384978
#% 464056
#% 464883
#% 495632
#% 599549
#% 835733
#% 1373561
#% 1499552
#! One of the most important reasoning tasks on queries is checking containment, i.e., verifying whether one query yields necessarily a subset of the result of another one. Query containment, is crucial in several contexts, such as query optimization, query reformulation, knowledge-base verification, information integration, integrity checking, and cooperative answering. Containment is undecidable in general for Datalog, the fundamental language for expressing recursive queries. On the other hand, it is known that containment between monadic Datalog queries and between Datalog queries and unions of conjunctive queries are decidable. It is also known that containment between unions of conjunctive two-way regular path queries (UC2RPQs), which are queries used in the context of semistructured data models containing a limited form of recursion in the form of transitive closure, is decidable. In this paper we combine the automata-theoretic techniques at the base of these two decidability results to show that containment of Datalog in UC2RPQs is decidable in 2EXPTIME.

#index 465044
#* Probabilistic Interval XML
#@ Edward Hung;Lise Getoor;V. S. Subrahmanian
#t 2003
#c 6
#% 44876
#% 73571
#% 102767
#% 181038
#% 209725
#% 235023
#% 236416
#% 259487
#% 348163
#% 442830
#% 458522
#% 480102
#% 553830
#% 664841
#% 718342
#% 993985
#! Interest in XML databases has been growing over the last few years. In this paper, we study the problem of incorporating probabilistic information into XML databases. We propose the Probabilistic Interval XML (PIXml for short) data model in this paper. Using this data model, users can express probabilistic information within XML markups. In addition, we provide two alternative formal model-theoretic semantics for PIXml data. The first semantics is a "global" semantics which is relatively intuitive, but is not directly amenable to computation. The second semantics is a "local" semantics which is more amenable to computation. We prove several results linking the two semantics together. To our knowledge, this is the first formal model theoretic semantics for probabilistic interval XML. We then provide an operational semantics that may be used to compute answers to queries and that is correct for a large class of probabilistic instances.

#index 465045
#* Characterizing the Temporal and Semantic Coherency of Broadcast-Based Data Dissemination
#@ Evaggelia Pitoura;Panos K. Chrysanthis;Krithi Ramamritham
#t 2003
#c 6
#% 8194
#% 9241
#% 32884
#% 137870
#% 151529
#% 273893
#% 279165
#% 286256
#% 286967
#% 443127
#% 479961
#% 480941
#% 481777
#% 553989
#% 615031
#% 635795
#% 661499
#% 1013748
#! In this paper, we develop a general theory of temporal and semantic coherency for an extended client/server architecture in which the server broadcasts items of interest to a large number of clients without a specific client request. Such architectures are part of an increasing number of emerging applications in wireless mobile computing systems; they also provide a scalable means to deliver information in web-based applications, for example in publish-subscribe systems. We introduce various forms of temporal and semantic coherency applicable to such architectures and present a framework to precisely define protocols for enforcing them.

#index 465046
#* Auditing Sum Queries
#@ Francesco M. Malvestuto;Mauro Mezzini
#t 2003
#c 6
#% 28709
#% 37869
#% 67453
#% 299970
#% 415044
#% 442730
#% 1014207
#! In an on-line statistical database, the query system should leave unanswered queries asking for sums that could lead to the disclosure of confidential data. To check that, every sum query and previously answered sum queries should be audited. We show that, under a suitable query-overlap restriction, an auditing procedure can be efficiently worked out using flow-network computation.

#index 465047
#* Bioinformatics Adventures in Database Research
#@ Jinyan Li;See-Kiong Ng;Limsoon Wong
#t 2003
#c 6
#% 136350
#% 163444
#% 189868
#% 190581
#% 210349
#% 245655
#% 248791
#% 280409
#% 411622
#% 416030
#% 420062
#% 463919
#% 466426
#% 478615
#% 481290
#% 565765
#% 589353
#% 618434
#% 736843
#% 772131
#% 993241
#% 1478493
#! Informatics has helped launch molecular biology into the genomic era. It appears certain that informatics will remain a major contributor to molecular biology in the post-genome era.We discuss here data integration and datamining in bioinformatics, as well as the role that database theory played in these topics. We also describe LIMS as a third key topic in bioinformatics where advances in database system and theory can be very relevant.

#index 465048
#* New Rewritings and Optimizations for Regular Path Queries
#@ Gösta Grahne;Alex Thomo
#t 2003
#c 6
#% 197751
#% 198465
#% 241136
#% 248025
#% 273700
#% 273924
#% 291299
#% 292677
#% 299967
#% 404772
#% 464867
#% 464895
#% 479465
#% 504583
#% 632039
#! All the languages for querying semistructured data and the web use as an integral part regular expressions. Based on practical observations, finding the paths that satisfy those regular expressions is very expensive. In this paper, we introduce the "maximal partial rewritings" (MPR's) for regular path queries using views. The MPR's are always exact and more useful for the optimization of the regular path queries than other rewritings from previously known methods. We develop an algorithm for computing MPR's and prove, through a complexity theoretic analysis, that our algorithm is essentially optimal. Also, we present query answering algorithms that utilize exact partial rewritings for regular path queries and conjunctive regular path queries respectively.

#index 465049
#* Optimal Range Max Datacube for Fixed Dimensions
#@ Chung Keung Poon
#t 2003
#c 6
#% 37861
#% 64532
#% 68113
#% 69474
#% 211575
#% 227866
#% 275072
#% 289322
#% 319601
#% 333977
#% 458821
#% 461921
#% 464215
#% 464896
#% 479822
#% 480323
#% 480801
#% 487692
#% 498538
#% 565500
#% 594028
#% 598375
#% 604653
#% 617881
#% 631947
#! We present a new data structure to support orthogonal range max queries on a datacube. For a d-dimensional datacube with size n in each dimension where d 驴 c3 log log n/ log(log* n), our structure requires O(c1d) query time and O((c2n)d) storage where c1, c2 and c3 are constants independent of d and n; and log* n is the minimum number of repeated logarithms it takes to reduce the value n to at most 2. Hence our data structure is asymptotically optimal when d is fixed, i.e., a constant independent of n.

#index 465050
#* Containment of Conjunctive Queries with Safe Negation
#@ Fang Wei;Georg Lausen
#t 2003
#c 6
#% 36181
#% 164364
#% 198465
#% 237181
#% 248025
#% 248038
#% 289266
#% 368248
#% 443460
#% 464717
#% 481128
#% 572311
#% 599549
#! We consider the problem of query containment for conjunctive queries with safe negated subgoals (CQ卢s). We propose a new method for the containment test of CQ卢s. Comparing to the previous known approach, which always requires an exponential number of canonical databases to be verified to prove that Q1 驴 Q2, the algorithm proposed in this paper exploits the containment mappings of their positive counterparts, and terminates once the specified test succeeds. We show that in the worst case, the algorithm has the same performance as the one proposed in previous work. We also extend our algorithm to unions of CQ卢s in a natural way. Due to the close relation between query containment and answering queries using views, we give some notes on considering answering queries using views when both queries and views have safe negated subgoals.

#index 465051
#* Containment for XPath Fragments under DTD Constraints
#@ Peter T. Wood
#t 2003
#c 6
#% 70235
#% 248026
#% 273924
#% 287339
#% 299944
#% 333989
#% 348170
#% 378393
#% 400361
#% 536288
#% 564264
#% 584866
#% 584879
#% 584936
#% 993437
#! The containment and equivalence problems for various fragments of XPath have been studied by a number of authors. For some fragments, deciding containment (and even minimisation) has been shown to be in PTIME, while for minor extensions containment has been shown to be CONP-complete. When containment is with respect to trees satisfying a set of constraints (such as a schema or DTD), the problem seems to be more difficult. For example, containment under DTDs is CONP-complete for an XPath fragment denoted XP{[ ]} for which containment is in PTIME. It is also undecidable for a larger class of XPath queries when the constraints are so-called simple XPath integrity constraints (SXICs). In this paper, we show that containment is decidable for an important fragment of XPath, denoted XP{[ ], *, //}, when the constraints are DTDs. We also identify XPath fragments for which containment under DTDs can be decided in PTIME.

#index 465052
#* Condensed Representation of Database Repairs for Consistent Query Answering
#@ Jef Wijsen
#t 2003
#c 6
#% 583
#% 273687
#% 384978
#% 464915
#% 465025
#% 488620
#% 490501
#% 519568
#% 536287
#% 727668
#! Repairing a database means bringing the database in accordance with a given set of integrity constraints by applying modifications that are as small as possible. In the seminal work of Arenas et al. on query answering in the presence of inconsistency, the possible modifications considered are deletions and insertions of tuples. Unlike earlier work, we also allow tuple updates as a repair primitive. Update-based repairing is advantageous, because it allows rectifying an error within a tuple without deleting the tuple, thereby preserving other consistent values in the tuple. At the center of the paper is the problem of query answering in the presence of inconsistency relative to this refined repair notion. Given a query, a trustable answer is obtained by intersecting the query answers on all repaired versions of the database. The problem arising is that, in general, a database can be repaired in infinitely many ways. A positive result is that for conjunctive queries and full dependencies, there exists a condensed representation of all repairs that permits computing trustable query answers.

#index 465053
#* Reformulation of XML Queries and Constraints
#@ Alin Deutsch;Val Tannen
#t 2003
#c 6
#% 583
#% 83228
#% 198465
#% 273700
#% 273922
#% 273924
#% 283052
#% 287336
#% 299967
#% 309851
#% 330627
#% 333935
#% 333965
#% 378409
#% 384978
#% 398263
#% 461897
#% 462214
#% 480317
#% 480657
#% 480822
#% 481444
#% 481923
#% 562454
#% 564416
#% 572307
#% 712339
#% 716433
#! We state and solve the query reformulation problem for XML publishing in a general setting that allows mixed (XML and relational) storage for the proprietary data and exploits redundancies (materialized views, indexes and caches) to enhance performance. The correspondence between published and proprietary schemas is specified by views in both directions, and the same algorithm performs rewriting-with-views, composition-with-views, or the combined effect of both, unifying the Global-As-View and Local-As-View approaches to data integration. We prove a completeness theorem which guarantees that under certain conditions, our algorithm will find a minimal reformulation if one exists. Moreover, we identify conditions when this algorithm achieves optimal complexity bounds. We solve the reformulation problem for constraints by exploiting a reduction to the problem of query reformulation.

#index 465054
#* Typechecking Top-Down Uniform Unranked Tree Transducers
#@ Wim Martens;Frank Neven
#t 2003
#c 6
#% 55690
#% 70235
#% 175597
#% 241160
#% 273702
#% 299942
#% 299944
#% 333857
#% 342438
#% 345757
#% 401124
#% 408396
#% 427027
#% 427874
#% 562313
#% 562461
#% 571040
#% 600179
#% 653704
#% 1373610
#! We investigate the typechecking problem for XML queries: statically verifying that every answer to a query conforms to a given output schema, for inputs satisfying a given input schema. As typechecking quickly turns undecidable for query languages capable of testing equality of data values, we return to the limited framework where we abstract XML documents as labeled ordered trees. We focus on simple top-down recursive transformations motivated by XSLT and structural recursion on trees. We parameterize the problem by several restrictions on the transformations (deleting, non-deleting, bounded width) and consider both tree automata and DTDs as output schemas. The complexity of the typechecking problems in this scenario range from PTIME to EXPTIME.

#index 465055
#* Nearest Neighbors Can Be Found Efficiently If the Dimension Is Small Relative to the Input Size
#@ Michiel Hagedoorn
#t 2003
#c 6
#% 56637
#% 68089
#% 158405
#% 190611
#% 201893
#% 213673
#% 249321
#% 249322
#% 264161
#% 336579
#% 379423
#% 464888
#% 480304
#% 481947
#% 522277
#% 632043
#% 656800
#% 729437
#! We consider the problem of nearest-neighbor search for a set of n data points in d-dimensional Euclidean space. We propose a simple, practical data structure, which is basically a directed acyclic graph in which each node has at most two outgoing arcs. We analyze the performance of this data structure for the setting in which the n data points are chosen independently from a d-dimensional ball under the uniform distribution. In the average case, for fixed dimension d, we achieve a query time of O(log2 n) using only O(n) storage space. For variable dimension, both the query time and the storage space are multiplied with a dimension-dependent factor that is at most exponential in d. This is an improvement over previously known time-space tradeoffs, which all have a super-exponential factor of at least d驴 (d) either in the query time or in the storage space. Our data structure can be stored efficiently in secondary memory: In a standard secondary-memory model, for fixed dimension d, we achieve average-case bounds of O((log2 n)/B + log n) query time and O(N) storage space, where B is the block-size parameter and N = n/B. Our data structure is not limited to Euclidean space; its definition generalizes to all possible choices of query objects, data objects, and distance functions.

#index 465056
#* An Efficient Indexing Scheme for Multi-dimensional Moving Objects
#@ Khaled M. Elbassioni;Amr Elmasry;Ibrahim Kamel
#t 2003
#c 6
#% 13742
#% 41684
#% 83322
#% 86950
#% 144870
#% 152954
#% 153260
#% 237205
#% 248028
#% 273706
#% 299979
#% 300174
#% 427199
#% 458765
#% 461923
#% 493207
#! We consider the problem of indexing a set of objects moving in d-dimensional space along linear trajectories. A simple disk-based indexing scheme is proposed to efficiently answer queries of the form: report all objects that will pass between two given points within a specified time interval. Our scheme is based on mapping the objects to a dual space, where queries about moving objects translate into polyhedral queries concerning their speeds and initial locations. We then present a simple method for answering such polyhedral queries, based on partitioning the space into disjoint regions and using a B-tree to index the points in each region. By appropriately selecting the boundaries of each region, we can guarantee an average search time that almost matches a known lower bound for the problem. Specifically, for a fixed d, if the coordinates of a given set of N points are statistically independent, the proposed technique answers polyhedral queries, on the average, in O((N/B)1 - 1/d 驴 (logB N)1/d + K/B) I/O's using O(N/B) space, where B is the block size, and K is the number of reported points. Our approach is novel in that, while it provides a theoretical upper bound on the average query time, it avoids the use of complicated data structures, making it an effective candidate for practical applications.

#index 465057
#* Data Exchange: Semantics and Query Answering
#@ Ronald Fagin;Phokion G. Kolaitis;Renée J. Miller;Lucian Popa
#t 2003
#c 6
#% 583
#% 24989
#% 83228
#% 198465
#% 230142
#% 248038
#% 264858
#% 283052
#% 285926
#% 287339
#% 287733
#% 289384
#% 321466
#% 378409
#% 464716
#% 465057
#% 480134
#% 488620
#% 572311
#% 993981
#! Data exchange is the problem of taking data structured under a source schema and creating an instance of a target schema that reflects the source data as accurately as possible. In this paper, we address foundational and algorithmic issues related to the semantics of data exchange and to query answering in the context of data exchange. These issues arise because, given a source instance, there may be many target instances that satisfy the constraints of the data exchange problem. We give an algebraic specification that selects, among all solutions to the data exchange problem, a special class of solutions that we call universal. A universal solution has no more and no less data than required for data exchange and it represents the entire space of possible solutions. We then identify fairly general, and practical, conditions that guarantee the existence of a universal solution and yield algorithms to compute a canonical universal solution efficiently. We adopt the notion of "certain answers" in indefinite databases for the semantics for query answering in data exchange. We investigate the computational complexity of computing the certain answers in this context and also study the problem of computing the certain answers of target queries by simply evaluating them on a canonical universal solution.

#index 465058
#* On Reasoning about Structural Equality in XML: A Description Logic Approach
#@ David Toman;Grant E. Weddell
#t 2003
#c 6
#% 33376
#% 87891
#% 91073
#% 101435
#% 114579
#% 114723
#% 135873
#% 175746
#% 210169
#% 231786
#% 235018
#% 252608
#% 330627
#% 333855
#% 378395
#% 378411
#% 398243
#% 404772
#% 459291
#% 464701
#% 465039
#% 472862
#% 475381
#% 533987
#% 1289168
#! We define a boolean complete description logic dialect called DLFDreg that can be used to reason about structural equality in semistructured ordered data in the presence of document type definitions. This application depends on the novel ability of DLFDreg to express functional dependencies over sets of possibly infinite feature paths defined by regular languages. We also present a decision procedure for the associated logical implication problem. The procedure underlies a mapping of such problems to satisfiability problems of DatalognS驴, 卢 and in turn to the Ackermann case of the decision problem.

#index 465059
#* Incremental Validation of XML Documents
#@ Yannis Papakonstantinou;Victor Vianu
#t 2003
#c 6
#% 85022
#% 175440
#% 181367
#% 198470
#% 245652
#% 248799
#% 262724
#% 264462
#% 289261
#% 299944
#% 355864
#% 393907
#% 411621
#% 464863
#% 473142
#% 516204
#% 520144
#% 526825
#% 545382
#! We investigate the incremental validation of XML documents with respect to DTDs and XML Schemas, under updates consisting of element tag renamings, insertions and deletions. DTDs are modeled as extended context-free grammars and XML Schemas are abstracted as "specialized DTDs", allowing to decouple element types from element tags. For DTDs, we exhibit an O(m log n) incremental validation algorithm using an auxiliary structure of size O(n), where n is the size of the document and m the number of updates. For specialized DTDs, we provide an O(m log2 n) incremental algorithm, again using an auxiliary structure of size O(n). This is a significant improvement over brute-force re-validation from scratch.

#index 465060
#* CRB-Tree: An Efficient Indexing Scheme for Range-Aggregate Queries
#@ Sathish Govindarajan;Pankaj K. Agarwal;Lars Arge
#t 2003
#c 6
#% 37861
#% 41684
#% 69474
#% 210182
#% 252304
#% 300849
#% 333874
#% 411694
#% 458821
#% 458858
#% 465010
#% 479822
#% 480323
#% 548489
#% 560840
#% 562982
#% 565462
#% 580214
#! We propose a newindexing scheme, called the CRB-tree, for efficiently answering range-aggregate queries. The range-aggregate problem is defined as follows: Given a set of weighted points in Rd, compute the aggregate of weights of points that lie inside a d-dimensional query rectangle. In this paper we focus on range-COUNT, SUM, AVG aggregates. First, we develop an indexing scheme for answering two-dimensional range-COUNT queries that uses O(N/B) disk blocks and answers a query in O(logB N) I/Os, where N is the number of input points and B is the disk block size. This is the first optimal index structure for the 2D range-COUNT problem. The index can be extended to obtain a near-linear-size structure for answering range-SUM queries using O(logB N) I/Os. We also obtain similar bounds for rectangle-intersection aggregate queries, in which the input is a set of weighted rectangles and a query asks to compute the aggregate of the weights of those input rectangles that overlap with the query rectangle. This result immediately improves a recent result on temporal-aggregate queries. Our indexing scheme can be dynamized and extended to higher dimensions. Finally, we demonstrate the practical efficiency of our index by comparing its performance against kdB-tree. For a dataset of around 100 million points, the CRB-tree query time is 8-10 times faster than the kdB-tree query time. Furthermore, unlike other indexing schemes, the query performance of CRB-tree is oblivious to the distribution of the input points and placement, shape and size of the query rectangle.

#index 465061
#* Processing XML Streams with Deterministic Automata
#@ Todd J. Green;Gerome Miklau;Makoto Onizuka;Dan Suciu
#t 2003
#c 6
#% 70370
#% 291299
#% 300153
#% 300179
#% 321327
#% 333982
#% 342372
#% 382963
#% 462235
#% 464724
#% 479465
#% 479806
#% 480296
#% 504573
#% 570880
#% 659987
#% 659995
#% 740916
#! We consider the problem of evaluating a large number of XPath expressions on an XML stream. Our main contribution consists in showing that Deterministic Finite Automata (DFA) can be used effectively for this problem: in our experiments we achieve a throughput of about 5.4MB/s, independent of the number of XPath expressions (up to 1,000,000 in our tests). The major problem we face is that of the size of the DFA. Since the number of states grows exponentially with the number of XPath expressions, it was previously believed that DFAs cannot be used to process large sets of expressions. We make a theoretical analysis of the number of states in the DFA resulting from XPath expressions, and consider both the case when it is constructed eagerly, and when it is constructed lazily. Our analysis indicates that, when the automaton is constructed lazily, and under certain assumptions about the structure of the input XML data, the number of states in the lazy DFA is manageable. We also validate experimentally our findings, on both synthetic and real XML data sets.

#index 465062
#* Typing Graph-Manipulation Operations
#@ Jan Hidders
#t 2003
#c 6
#% 319
#% 27043
#% 67245
#% 84990
#% 139177
#% 164415
#% 230556
#% 275423
#% 286831
#% 287631
#% 322880
#% 416030
#% 442887
#% 443410
#% 464720
#% 512863
#% 518018
#% 704167
#% 1081098
#! We present a graph-based data model called GDM where database instances and database schemas are described by certain types of labeled graphs called instance graphs and schema graphs. For this data model we introduce two graph-manipulation operations, an addition and a deletion, that are based on pattern matching and can be represented in a graphical way. For these operations it is investigated if they can be typed such that it is guaranteed for well-typed operations that the result belongs to a certain database schema graph, and what the complexity of deciding this well-typedness is.

#index 465063
#* Open Problems in Data-Sharing Peer-to-Peer Systems
#@ Neil Daswani;Hector Garcia-Molina;Beverly Yang
#t 2003
#c 6
#% 261357
#% 316798
#% 319849
#% 330620
#% 330621
#% 340175
#% 340176
#% 345086
#% 349973
#% 360802
#% 401980
#% 414381
#% 414382
#% 438231
#% 446421
#% 496156
#% 496286
#% 496291
#% 505869
#% 528350
#% 612645
#% 636008
#% 636009
#% 963874
#! In a Peer-To-Peer (P2P) system, autonomous computers pool their resources (e.g., files, storage, compute cycles) in order to inexpensively handle tasks that would normally require large costly servers. The scale of these systems, their "open nature," and the lack of centralized control pose difficult performance and security challenges. Much research has recently focused on tackling some of these challenges; in this paper, we propose future directions for research in P2P systems, and highlight problems that have not yet been studied in great depth. We focus on two particular aspects of P2P systems - search and security - and suggest several open and important research problems for the community to address.

#index 465064
#* Approximations in Database Systems
#@ Yannis E. Ioannidis
#t 2003
#c 6
#% 210190
#% 333947
#% 379238
#% 479648
#% 479984
#% 480810
#% 482092
#% 572308
#! The need for approximations of information has become very critical in the recent past. From traditional query optimization to newer functionality like user feedback and knowledge discovery, data management systems require quick delivery of approximate data in order to serve their goals. There are several techniques that have been proposed to solve the problem, each with its own strengths and weaknesses. In this paper, we take a look at some of the most important data approximation problems and attempt to put them in a common framework and identify their similarities and differences. We then hint on some open and challenging problems that we believe are worth investigating.

#index 465065
#* Structural Properties of XPath Fragments
#@ Michael Benedikt;Wenfei Fan;Gabriel M. Kuper
#t 2003
#c 6
#% 187659
#% 289335
#% 299942
#% 299976
#% 333856
#% 333989
#% 378393
#% 390685
#% 397374
#% 427874
#% 487257
#% 536288
#% 570879
#% 579726
#% 993939
#! We study structural properties of each of the main sublanguages of XPath [8] commonly used in practice. First, we characterize the expressive power of these language fragments in terms of both logics and tree patterns. Second, we investigate closure properties, focusing on the ability to perform basic Boolean operations while remaining within the fragment. We give a complete picture of the closure properties of these fragments, treating XPath expressions both as functions of arbitrary nodes in a document tree, and as functions that are applied only at the root of the tree. Finally, we provide sound and complete axiom systems and normal forms for several of these fragments. These results are useful for simplification of XPath expressions and optimization of XML queries.

#index 465066
#* Database Interrogation Using Conjunctive Queries
#@ Michal Bielecki;Jan Van den Bussche
#t 2003
#c 6
#% 36683
#% 67453
#% 245988
#% 262716
#% 287297
#% 303884
#% 384978
#% 490480
#% 939496
#! We consider a scenario where a client communicates with a database server by posing boolean conjunctive queries, or more generally, counts of conjunctive queries. We investigate to what extent features such as quantification, negation, or non-equalities are important in such a setting. We also investigate the difference between a setting where the client can pose an adaptive sequence of queries, and a setting where the client must pose a fixed combination of queries.

#index 465067
#* On the Difficulty of Finding Optimal Relational Decompositions for XML Workloads: A Complexity Theoretic Perspective
#@ Rajasekar Krishnamurthy;Venkatesan T. Chakaravarthy;Jeffrey F. Naughton
#t 2003
#c 6
#% 273922
#% 289266
#% 347186
#% 408396
#% 479956
#% 504574
#% 560661
#% 650962
#! A key problem that arises in the context of storing XML documents in relational databases is that of finding an optimal relational decomposition for a given set of XML documents and a given set of XML queries over those documents. While there have been a number of ad hoc solutions proposed for this problem, to our knowledge this paper represents a first step toward formalizing the problem and studying its complexity. It turns out that to even define what one means by an optimal decomposition, one first needs to specify an algorithm to translate XML queries to relational queries, and a cost model to evaluate the quality of the resulting relational queries. By examining an interesting problem embedded in choosing a relational decomposition, we show that choices of different translation algorithms and cost models result in very different complexities for the resulting optimization problems. Our results suggest that, contrary to the trend in previous work, the eventual development of practical algorithms for finding relational decompositions for XML workloads will require judicious choices of cost models and translation algorithms, rather than an exclusive focus on the decomposition problem in isolation.

#index 465068
#* Generating Relations from XML Documents
#@ Sara Cohen;Yaron Kanza;Yehoshua Sagiv
#t 2003
#c 6
#% 248809
#% 285926
#% 287667
#% 300157
#% 333843
#% 333845
#% 416015
#% 462228
#% 479772
#% 501026
#% 562464
#% 993437
#! This paper discusses several mechanisms for creating relations out of XML documents. A relation generator consists of two parts: (1) a tuple of path expressions and (2) an index indicating which path expressions may not be assigned the null value. Evaluating a relation generator involves finding tuples of nodes that satisfy the path expressions and are related to one another in a meaningful fashion. Different semantics for evaluation are given that take into account the possible presence of incomplete information. The complexity of generating relations from documents is analyzed and evaluation algorithms are described.

#index 465190
#* Containment of Aggregate Queries
#@ Sara Cohen;Werner Nutt;Yehoshua Sagiv
#t 2003
#c 6
#% 36683
#% 137867
#% 190638
#% 198465
#% 210208
#% 227949
#% 237190
#% 248034
#% 248038
#% 273696
#% 273698
#% 279164
#% 299945
#% 333869
#% 464056
#% 480149
#% 481923
#% 482081
#% 484859
#% 564419
#% 572311
#% 803609
#% 1227358
#! The problem of deciding containment of aggregate queries is investigated. Containment is reduced to equivalence for queries with expandable aggregation functions. Many common aggregation functions, such as max, cntd (count distinct), count, sum, avg, median and stdev (standard deviation) are shown to be expandable. It is shown that even in the presence of integrity constraints, containment can be reduced to equivalence. For conjunctive count and sum-queries, simpler characterizations for containment are given, that do not require checking equivalence. These results are built upon in order to solve the problem of finding maximally-contained sets of rewritings for conjunctive count-queries.

#index 564260
#* Abstract Interpretation of Active Rules and its Use in Termination Analysis
#@ James Bailey;Lobel Crnogorac;Kotagiri Ramamohanarao;Harald Søndergaard
#t 1997
#c 6

#index 564264
#* XPath Containment in the Presence of Disjunction, DTDs, and Variables
#@ Frank Neven;Thomas Schwentick
#t 2003
#c 6
#% 3997
#% 6242
#% 309525
#% 378393
#% 401124
#% 427027
#% 427874
#% 465051
#% 465065
#% 473117
#% 536288
#% 545382
#% 770338
#% 993939
#% 993940
#% 1395000
#! XPath is a simple language for navigating an XML tree and returning a set of answer nodes. The focus in this paper is on the complexity of the containment problem for various fragments of XPath. In addition to the basic operations (child, descendant, filter, and wildcard), we consider disjunction, DTDs and variables. W.r.t. variables we study two semantics: (1) the value of variables is given by an outer context; (2) the value of variables is defined existentially. We establish an almost complete classification of the complexity of the containment problem w.r.t. these fragments.

#index 565496
#* A Formal Foundation for Distributed Workflow Execution Based on State Charts
#@ Dirk Wodtke;Gerhard Weikum
#t 1997
#c 6

#index 565497
#* Expressive Power of Unary Counters
#@ Michael Benedikt;H. Jerome Keisler
#t 1997
#c 6

#index 565498
#* Adding For-Loops to First-Order Logic
#@ Frank Neven;Martin Otto;Jerzy Tyszkiewicz;Jan Van den Bussche
#t 1999
#c 6
#% 187081
#% 277320
#% 384978
#% 399066
#% 544229
#! We study the query language BQL: the extension of the relational algebra with for-loops. We also study FO(FOR): the extension of first-order logic with a for-loop variant of the partial fixpoint operator. In contrast to the known situation with query languages which include while-loops instead of for-loops, BQL and FO(FOR) are not equivalent. Among the topics we investigate are: the precise relationship between BQL and FO(FOR); inflationary versus non-inflationary iteration; the relationship with logics that have the ability to count; and nested versus unnested loops.

#index 565499
#* Query Languages for Constraint Databases: First-Order Logic, Fixed-Points, and Convex Hulls
#@ Stephan Kreutzer
#t 2001
#c 6
#% 24108
#% 238399
#% 245656
#% 248022
#% 257440
#% 268788
#% 299973
#% 299974
#% 505561
#% 553832
#% 1562065
#! We define various extensions of first-order logic on linear as well as polynomial constraint databases. First, we extend first-order logic by a convex closure operator and show this logic, FO(conv), to be closed and to have Ptime data-complexity. We also show that a weak form of multiplication is definable in this language and prove the equivalence between this language and the multiplication part of PFOL. We then extend FO(conv) by fixed-point operators to get a query languages expressive enough to capture Ptime. In the last part of the paper we lift the results to polynomial constraint databases.

#index 565500
#* Flexible Data Cubes for Online Aggregation
#@ Mirek Riedewald;Divyakant Agrawal;Amr El Abbadi
#t 2001
#c 6
#% 210182
#% 227866
#% 248040
#% 248805
#% 259995
#% 273902
#% 273916
#% 420053
#% 458821
#% 462204
#% 464706
#% 479476
#% 479822
#% 617881
#% 631947
#! Applications like Online Analytical Processing depend heavily on the ability to quickly summarize large amounts of information. Techniques were proposed recently that speed up aggregate range queries on MOLAP data cubes by storing pre-computed aggregates. These approaches try to handle data cubes of any dimensionality by dealing with all dimensions at the same time and treat the different dimensions uniformly. The algorithms are typically complex, and it is difficult to prove their correctness and to analyze their performance. We present a new technique to generate Iterative Data Cubes (IDC) that addresses these problems. The proposed approach provides a modular framework for combining one-dimensional aggregation techniques to create space-optimal high-dimensional data cubes. A large variety of cost tradeoffs for high-dimensional IDC can be generated, making it easy to find the right configuration based on the application requirements.

#index 1179996
#* Automatic verification of database-driven systems: a new frontier
#@ Victor Vianu
#t 2009
#c 6
#% 1729
#% 29439
#% 33586
#% 101955
#% 158068
#% 169697
#% 185412
#% 188086
#% 213957
#% 248013
#% 261269
#% 289415
#% 295410
#% 321054
#% 342119
#% 384978
#% 425200
#% 431006
#% 489794
#% 519432
#% 565496
#% 570649
#% 571038
#% 571991
#% 576091
#% 577343
#% 581901
#% 630964
#% 653685
#% 765514
#% 769518
#% 786859
#% 801675
#% 810053
#% 824702
#% 874885
#% 875055
#% 888014
#% 888015
#% 942360
#% 982402
#% 1063731
#% 1072645
#% 1081077
#% 1099727
#% 1180017
#% 1392367
#% 1396853
#% 1415338
#% 1599270
#% 1656081
#! We describe a novel approach to verification of software systems centered around an underlying database. Instead of applying general-purpose techniques with only partial guarantees of success, it identifies restricted but reasonably expressive classes of applications and properties for which sound and complete verification can be performed in a fully automatic way. This leverages the emergence of high-level specification tools for database-centered applications that not only allow fast prototyping and improved programmer productivity but, as a side effect, provide convenient targets for automatic verification. We present theoretical and practical results on verification of database-driven systems. The results are quite encouraging and suggest that, unlike arbitrary software systems, significant classes of database-driven systems may be amenable to automatic verification. This relies on a novel marriage of database and model checking techniques, of relevance to both the database and the computer aided verification communities.

#index 1179997
#* Datalog±: a unified approach to ontologies and integrity constraints
#@ Andrea Calì;Georg Gottlob;Thomas Lukasiewicz
#t 2009
#c 6
#% 16
#% 583
#% 36683
#% 53385
#% 71306
#% 73005
#% 189739
#% 191611
#% 287339
#% 378409
#% 384978
#% 464203
#% 465053
#% 490909
#% 576116
#% 591778
#% 598678
#% 598679
#% 599549
#% 801668
#% 826032
#% 869463
#% 893166
#% 992962
#% 1063724
#% 1289425
#% 1409909
#% 1416180
#! We report on a recently introduced family of expressive extensions of Datalog, called Datalog±, which is a new framework for representing ontological axioms in form of integrity constraints, and for query answering under such constraints. Datalog± is derived from Datalog by allowing existentially quantified variables in rule heads, and by enforcing suitable properties in rule bodies, to ensure decidable and efficient query answering. We first present different languages in the Datalog± family, providing tight complexity bounds for all cases but one (where we have a low complexity AC0 upper bound). We then show that such languages are general enough to capture the most common tractable ontology languages. In particular, we show that the DL-Lite family of description logics and F-Logic Lite are expressible in Datalog±. We finally show how stratified negation can be added to Datalog± while keeping ontology querying tractable in the data complexity. Datalog± is a natural and very general framework that can be successfully employed in different contexts such as data integration and exchange. This survey mainly summarizes two recent papers.

#index 1179998
#* Repair checking in inconsistent databases: algorithms and complexity
#@ Foto N. Afrati;Phokion G. Kolaitis
#t 2009
#c 6
#% 183411
#% 273687
#% 378409
#% 420072
#% 465053
#% 480499
#% 809239
#% 810019
#% 814475
#% 826032
#% 833132
#% 874880
#% 879041
#% 912245
#% 981634
#% 1025173
#% 1661426
#% 1661438
#! Managing inconsistency in databases has long been recognized as an important problem. One of the most promising approaches to coping with inconsistency in databases is the framework of database repairs, which has been the topic of an extensive investigation over the past several years. Intuitively, a repair of an inconsistent database is a consistent database that differs from the given inconsistent database in a minimal way. So far, most of the work in this area has addressed the problem of obtaining the consistent answers to a query posed on an inconsistent database. Repair checking is the following decision problem: given two databases r and r', is r' a repair of r? Although repair checking is a fundamental algorithmic problem about inconsistent databases, it has not received as much attention as consistent query answering. In this paper, we give a polynomial-time algorithm for subset-repair checking under integrity constraints that are the union of a weakly acyclic set of local-as-view (LAV) tuple-generating dependencies and a set of equality-generating dependencies. This result significantly generalizes earlier work for subset-repair checking when the integrity constraints are the union of an acyclic set of inclusion dependencies and a set of functional dependencies. We also give a polynomial-time algorithm for symmetric-difference repair checking, when the integrity constraints form a weakly acyclic set of LAV tgds. After this, we establish a number of complexity-theoretic results that delineate the boundary between tractability and intractability for the repair-checking problem. Specifically, we show that the aforementioned tractability results are optimal; in particular, subset-repair checking for arbitrary weakly acyclic sets of tuple-generating dependencies is a coNP-complete problem. We also study cardinality-based repairs and show that cardinality-repair checking is coNP-complete for various classes of integrity constraints encountered in database design and data exchange.

#index 1179999
#* Consistent query answering under primary keys: a characterization of tractable queries
#@ Jef Wijsen
#t 2009
#c 6
#% 273687
#% 289424
#% 384978
#% 576116
#% 778122
#% 810020
#% 814475
#% 833132
#% 838543
#% 949372
#% 1063725
#% 1408532
#% 1661426
#% 1700140
#% 1728684
#! This article deals with consistent query answering to conjunctive queries under primary key constraints. The repairs of an inconsistent database db are obtained by selecting a maximum number of tuples from db without ever selecting two tuples that agree on their primary key. For a Boolean conjunctive query q, we are interested in the following question: does there exist a Boolean first-order query &phis; such that for every database db, &phis; evaluates to true on db if and only if q evaluates to true on every repair of db? We address this problem for acyclic conjunctive queries in which no relation name occurs more than once. Our results improve previous solutions that are based on Fuxman-Miller join graphs.

#index 1180000
#* On approximating optimum repairs for functional dependency violations
#@ Solmaz Kolahi;Laks V. S. Lakshmanan
#t 2009
#c 6
#% 94459
#% 150128
#% 217812
#% 273687
#% 341672
#% 384978
#% 465052
#% 582130
#% 752741
#% 810019
#% 810020
#% 814475
#% 833132
#% 874888
#% 879041
#% 949372
#% 1022228
#% 1046452
#% 1058256
#% 1063725
#% 1661426
#% 1673674
#% 1733291
#! We study the problem of repairing an inconsistent database that violates a set of functional dependencies by making the smallest possible value modifications. For an inconsistent database, we define an optimum repair as a database that satisfies the functional dependencies, and minimizes, among all repairs, a distance measure that depends on the number of corrections made in the database and the weights of tuples modified. We show that like other versions of the repair problem, checking the existence of a repair within a certain distance of a database is NP-complete. We also show that finding a constant-factor approximation for the optimum repair for any set of functional dependencies is NP-hard. Furthermore, there is a small constant and a set of functional dependencies, for which finding an approximate solution for the optimum repair within the factor of that constant is also NP-hard. Then we present an approximation algorithm that for a fixed set of functional dependencies and an arbitrary input inconsistent database, produces a repair whose distance to the database is within a constant factor of the optimum repair distance. We finally show how the approximation algorithm can be used in data cleaning using a recent extension to functional dependencies, called conditional functional dependencies.

#index 1180001
#* Structural characterizations of schema-mapping languages
#@ Balder ten Cate;Phokion G. Kolaitis
#t 2009
#c 6
#% 9255
#% 167155
#% 237190
#% 248038
#% 378409
#% 809239
#% 821606
#% 826032
#% 850730
#% 893094
#% 1054485
#% 1063712
#! Schema mappings are declarative specifications that describe the relationship between two database schemas. In recent years, there has been an extensive study of schema mappings and of their applications to several different data inter-operability tasks, including applications to data exchange and data integration. Schema mappings are expressed in some logical formalism that is typically a fragment of first-order logic or a fragment of second-order logic. These fragments are chosen because they possess certain desirable structural properties, such as existence of universal solutions or closure under target homomorphisms. In this paper, we turn the tables and focus on the following question: can we characterize the various schema-mapping languages in terms of structural properties possessed by the schema mappings specified in these languages? We obtain a number of characterizations of schema mappings specified by source-to-target (s-t) dependencies, including characterizations of schema mappings specified by LAV (local-as-view) s-t tgds, schema mappings specified by full s-t tgds, and schema mappings specified by arbitrary s-t tgds. These results shed light on schema-mapping languages from a new perspective and, more importantly, demarcate the properties of schema mappings that can be used to reason about them in data inter-operability applications.

#index 1180002
#* Query languages for data exchange: beyond unions of conjunctive queries
#@ Marcelo Arenas;Pablo Barceló;Juan Reutter
#t 2009
#c 6
#% 583
#% 663
#% 378409
#% 384978
#% 591778
#% 598376
#% 778122
#% 801676
#% 801691
#% 806215
#% 809239
#% 823106
#% 826032
#% 874879
#% 874882
#% 1044476
#% 1063723
#% 1063724
#! The class of unions of conjunctive queries (UCQ) has been shown to be particularly well-behaved for data exchange; its certain answers can be computed in polynomial time (in terms of data complexity). However, this is not the only class with this property; the certain answers to any Datalog program can also can be computed in polynomial time. The problem is that both UCQ and Datalog do not allow negated atoms, as adding an unrestricted form of negation to these languages yields to intractability. In this paper, we propose a language called DatalogC(≠) that extends Datalog with a restricted form of negation, and study some of its fundamental properties. In particular, we show that the certain answers to a DatalogC(≠) program can be computed in polynomial time (in terms of data complexity), and that every union of conjunctive queries with at most one inequality or negated relational atom per disjunct, can be efficiently rewritten as a DatalogC(≠) program in the context of data exchange. Furthermore, we show that this is also the case for a syntactic restriction of the class of unions of conjunctive queries with at most two inequalities per disjunct. This syntactic restriction is given by two conditions that are optimal, in the sense that computing certain answers becomes intractable if one removes any of them. Finally, we provide a thorough analysis of the combined complexity of computing certain answers to DatalogC(≠) programs and other related query languages. In particular, we show that this problem is Exptime-complete for DatalogC(≠), even if one restricts to conjunctive queries with single inequalities, which is a fragment of DatalogC(≠) by the result mentioned above. Furthermore, we show that the combined complexity is CoNexptime-complete for the case of conjunctive queries with k inequalities, for every k ≥ 2.

#index 1180003
#* Querying data sources that export infinite sets of views
#@ Bogdan Cautis;Alin Deutsch;Nicola Onose
#t 2009
#c 6
#% 54225
#% 156703
#% 198465
#% 213982
#% 273912
#% 278830
#% 378407
#% 384978
#% 411569
#% 459241
#% 462501
#% 465053
#% 465057
#% 482116
#% 632086
#% 765447
#% 801698
#% 857502
#% 871765
#% 1688298
#% 1700141
#! We study the problem of querying data sources that accept only a limited set of queries, such as sources accessible by Web services which can implement very large (potentially infinite) families of queries. We revisit a classical setting in which the application queries are conjunctive queries and the source accepts families of conjunctive queries specified as the expansions of a (potentially recursive) Datalog program. We say that query Q is expressible by the program P if it is equivalent to some expansion of P. Q is supported by P if it has an equivalent rewriting using some finite set of P's expansions. We present the first study of expressibility and support for sources that satisfy integrity constraints, which is generally the case in practice.

#index 1180004
#* Optimal splitters for database partitioning with size bounds
#@ Kenneth A. Ross;John Cieslewicz
#t 2009
#c 6
#% 77937
#% 172913
#% 201921
#% 210190
#% 235242
#% 248014
#% 248820
#% 252304
#% 340670
#% 414915
#% 464062
#% 479648
#% 479984
#% 480442
#% 480596
#% 480608
#% 480966
#% 481775
#% 494333
#% 824697
#% 1015256
#! Partitioning is an important step in several database algorithms, including sorting, aggregation, and joins. Partitioning is also fundamental for dividing work into equal-sized (or balanced) parallel subtasks. In this paper, we aim to find, materialize and maintain a set of partitioning elements (splitters) for a data set. Unlike traditional partitioning elements, our splitters define both inequality and equality partitions, which allows us to bound the size of the inequality partitions. We provide an algorithm for determining an optimal set of splitters from a sorted data set and show that it has time complexity O(k lg2 N), where k is the number of splitters requested and N is the size of the data set. We show how the algorithm can be extended to pairs of tables, so that joins can be partitioned into work units that have balanced cost. We demonstrate experimentally (a) that finding the optimal set of splitters can be done efficiently, and (b) that using the precomputed splitters can improve the time to sort a data set by up to 76%, with particular benefits in the presence of a few heavy hitters.

#index 1180005
#* Efficient data structures for range-aggregate queries on trees
#@ Hao Yuan;Mikhail J. Atallah
#t 2009
#c 6
#% 186
#% 166791
#% 227866
#% 235941
#% 312399
#% 397360
#% 460796
#% 498538
#% 604653
#% 769697
#% 898157
#% 1080803
#% 1080822
#% 1114764
#% 1206573
#! Graph-theoretic aggregation problems have been considered both in OLAP (grid graph) and XML (tree). This paper gives new results for MIN aggregation in a tree, where we want the MIN in a query subtree consisting of the nodes reachable from a node u along paths of length ≤ k (u and k are query parameters). The same problem is well solved when the aggregation is SUM rather than MIN, but the solutions rely on additive inverses for the "+" operator, and they fail for the MIN aggregation which is the topic of this paper. For the directed (rooted tree) case, we give an O(n) space, constant query time solution. For the undirected case, the space complexity is O(n log n) and the query time is O(log n).

#index 1180006
#* Faster join-projects and sparse matrix multiplications
#@ Rasmus Resen Amossen;Rasmus Pagh
#t 2009
#c 6
#% 23614
#% 41684
#% 243624
#% 268777
#% 269157
#% 427210
#% 819511
#% 874899
#% 993437
#! Computing an equi-join followed by a duplicate eliminating projection is conventionally done by performing the two operations in serial. If some join attribute is projected away the intermediate result may be much larger than both the input and the output, and the computation could therefore potentially be performed faster by a direct procedure that does not produce such a large intermediate result. We present a new algorithm that has smaller intermediate results on worst-case inputs, and in particular is more efficient in both the RAM and I/O model. It is easy to see that join-project where the join attributes are projected away is equivalent to boolean matrix multiplication. Our results can therefore also be interpreted as improved sparse, output-sensitive matrix multiplication.

#index 1180007
#* A compositional query algebra for second-order logic and uncertain databases
#@ Christoph Koch
#t 2009
#c 6
#% 663
#% 102787
#% 205154
#% 384978
#% 435157
#% 598376
#% 778122
#% 893167
#% 960293
#% 977013
#% 992830
#% 1022206
#% 1063719
#% 1127376
#% 1206717
#% 1206987
#! World-set algebra is a variable-free query language for uncertain databases. It constitutes the core of the query language implemented in MayBMS, an uncertain database system. This paper shows that world-set algebra captures exactly second-order logic over finite structures, or equivalently, the polynomial hierarchy. The proofs also imply that world-set algebra is closed under composition, a previously open problem.

#index 1180008
#* A logical account of uncertain databases based on linear logic
#@ Sungwoo Park;Seung-won Hwang
#t 2009
#c 6
#% 663
#% 2984
#% 7051
#% 29844
#% 32879
#% 64413
#% 68140
#% 102787
#% 117869
#% 137865
#% 139593
#% 264858
#% 277070
#% 287333
#% 291859
#% 442692
#% 442740
#% 504491
#% 864394
#% 893167
#! A formal semantics of uncertain databases typically takes an algebraic approach by mapping an uncertain database to a set of relational databases, or possible worlds. We present a new semantics for uncertain databases which takes a logical approach by translating uncertain databases into logical theories. A characteristic feature of our semantics is that it uses linear logic, instead of propositional logic, as its logical foundation. Linear logic lends itself well to a logical interpretation of uncertain information because unlike propositional logic, it treats logical formulae not as persistent facts but as consumable resources. We motivate our development by arguing that propositional logic is inadequate as a logical foundation for uncertain databases. As the main result, we show that our semantics is faithful to the operational account of uncertain databases in the algebraic approach.

#index 1180009
#* A compositional framework for complex queries over uncertain data
#@ Michaela Götz;Christoph Koch
#t 2009
#c 6
#% 663
#% 101922
#% 215225
#% 235023
#% 265692
#% 384978
#% 465044
#% 808858
#% 864417
#% 1016201
#% 1063523
#% 1063719
#% 1063720
#% 1063721
#% 1068580
#% 1127376
#% 1206717
#! The ability to flexibly compose confidence computation with the operations of relational algebra is an important feature of probabilistic database query languages. Computing confidences is computationally hard, however, and has to be approximated in practice. In a compositional query language, even very small errors caused by approximation can lead to an entirely incorrect result: A selection operation on an approximated probability can incorrectly keep or drop a tuple even if the probability value has been approximated to a very narrow confidence interval. In this paper, we study the query evaluation problem for compositional query languages for probabilistic databases with particular focus on providing overall result quality guarantees in the face of approximate intermediate results. We present a framework for evaluating compositional queries based on a new representation system that can capture uncertainty about probabilities. More specifically, we consider probability intervals instead of exact probabilities, interpreting tuples obtained by selection on approximate values as unreliable. We study the complexity of query evaluation over our new model. We present efficient confidence computation algorithms which compute bounds that are close to tight for important classes. For deciding a selection predicate, we show that no efficient randomized algorithm exists unless BPP⊃NP. Still we are able to efficiently guess robust predicates with a good error bound. Putting all these pieces together in our framework, we evaluate queries using a decomposition into a relational algebra plan and an approximation plan. The latter allows to successively improve accuracy and error bounds, while the relational algebra plan only has to be executed once.

#index 1180010
#* Incremental XPath evaluation
#@ Henrik Björklund;Wouter Gelade;Marcel Marquardt;Wim Martens
#t 2009
#c 6
#% 152928
#% 245652
#% 742056
#% 745467
#% 765488
#% 791181
#% 804841
#% 805907
#% 810045
#% 814648
#% 838429
#% 893136
#% 942357
#% 958231
#% 976988
#% 976991
#% 1063733
#% 1129529
#% 1408526
#! We study the problem of incrementally maintaining an XPath query on an XML database under updates. The updates we consider are node insertion, node deletion, and node relabeling. Our main results are that downward XPath queries can be incrementally maintained in time O(depth(D) · poly(Q)) and conjunctive forward XPath queries in time O(depth(D)· log(width(D))·poly(Q)), where D is the size of the database, Q the size of the query, and depth(D) and width(D) are the nesting depth and maximum number of siblings in the database, respectively. The auxiliary data structures for maintenance are linear in D and polynomial in Q in all these cases.

#index 1180011
#* Efficient asymmetric inclusion between regular expression types
#@ Dario Colazzo;Giorgio Ghelli;Carlo Sartiani
#t 2009
#c 6
#% 172925
#% 498538
#% 893098
#% 894435
#% 1130846
#% 1408540
#% 1661444
#% 1725654
#! The inclusion of Regular Expressions (REs) is the kernel of any subtype checking algorithm for XML schema languages. XML applications would benefit from the extension of REs with interleaving and counting, but this is not feasible in general, since inclusion is EXPSPACE-complete for such extended REs. In [9] we introduced a notion of "conflict-free REs", which are extended REs with excellent complexity behaviour, including a cubic inclusion algorithm [9] and linear membership [10]. Conflict-free REs have interleaving and counting, but the complexity is tamed by the "conflict-free" limitations, which have been found to be satisfied by the vast majority of the content models published on the Web. However, the most important use of subtype checking is in the context of type-cheching of XML manipulation languges. A type checker works by testing the inclusion of inferred subtypes in declared supertypes. The conflict-free restriction, while quite harmless for the human-defined supertype, is far too restrictive for the inferred subtype, whose shape is difficult to constrain. We show here that the PTIME inclusion algorithm can be actually extended to deal with totally unrestricted REs with counting and interleaving in the subtype position, provided that the supertype is conflict-free. This is exactly the expressive power that we need in order to use subtyping inside type-checking algorithms, and the cost of this generalized algorithm is only quadratic, which is as good as the best algorithm we have for the symmetric case (see [5]). The result is extremely surprising, since we had previously found that asymmetric inclusion becomes NP-hard as soon as the candidate subtype is enriched with binary intersection, a generalization that looked much more innocent than what we achieve here.

#index 1180012
#* How big must complete XML query languages be?
#@ Clemens Ley;Michael Benedikt
#t 2009
#c 6
#% 24752
#% 101955
#% 310016
#% 399031
#% 414920
#% 505765
#% 510128
#% 587495
#% 801669
#% 821610
#% 850728
#% 1063714
#% 1407292
#% 1700125
#! Marx and de Rijke have shown that the navigational core of the w3c XML query language XPath is not first-order complete -- that is it cannot express every query definable in firstorder logic over the navigational predicates. How can one extend XPath to get a first-order complete language? Marx has shown that Conditional XPath -- an extension of XPath with an "Until" operator -- is first order complete. The completeness argument makes essential use of the presence of upward axes in Conditional XPath. We examine whether it is possible to get "forward-only" languages that are first-order complete for XML Boolean queries. It is easy to see that a variant of the temporal logic CTL* is first-order complete; the variant has path quantifiers for downward, leftward and rightward paths, while along a path one can check arbitrary formulas of linear temporal logic (LTL). This language has two major disadvantages: it requires path quantification in both horizontal directions (in particular, it requires looking backward at the prior siblings of a node), and it requires the consideration of formulas of LTL of arbitrary complexity on vertical paths. This last is in contrast with Marx's Conditional XPath, which requires only the checking of a single Until operator on a path. We investigate whether either of these restrictions can be eliminated. Our main results are negative ones. We show that if we restrict our CTL* language by having an until operator in only one horizontal direction, then we lose completeness. We also show that no restriction to a "small" subset of LTL along vertical paths is sufficient for first order completeness. Smallness here means of bounded "Until Depth", a measure of complexity of LTL formulas defined by Etessami and Wilke. In particular, it follows from our work that Conditional XPath with only forward axes is not expressively complete; this extends results proved by Rabinovich and Maoz in the context of infinite unordered trees.

#index 1180013
#* Towards a theory of search queries
#@ George H. L. Fletcher;Jan Van den Bussche;Dirk Van Gucht;Stijn Vansummeren
#t 2009
#c 6
#% 213970
#% 338753
#% 368248
#% 384978
#% 387427
#% 399235
#% 562289
#% 801677
#% 822573
#% 874876
#% 893119
#% 949369
#% 960234
#% 960237
#% 1063539
#% 1696286
#% 1711122
#! The need to manage diverse information sources has triggered the rise of very loosely structured data models, known as "dataspace models." Such information management systems must allow querying in simple ways, mostly by a form of searching. Motivated by these developments, we propose a theory of search queries in a general model of dataspaces. In this model, a dataspace is a collection of data objects, where each data object is a collection of data items. Basic search queries are expressed using filters on data items, following the basic model of boolean search in information retrieval. We characterise semantically the class of queries that can be expressed by searching. We apply our theory to classical relational databases, where we connect search queries to the known class of fully generic queries, and to dataspaces where data items are formed by attribute--value pairs. We also extend our theory to a more powerful, associative form of searching where one can ask for objects that are similar to objects satisfying given search conditions. Such associative search queries are shown to correspond to a very limited kind of joins. Specifically, we show that the basic search language extended with associative search can define exactly the queries definable in a restricted fragment of the semijoin algebra working on an explicit relational representation of the dataspace.

#index 1180014
#* Reconcilable differences
#@ Todd J. Green;Zachary G. Ives;Val Tannen
#t 2009
#c 6
#% 58377
#% 70370
#% 137867
#% 152928
#% 190638
#% 198465
#% 273696
#% 289266
#% 378062
#% 384978
#% 411554
#% 464056
#% 565457
#% 572311
#% 599549
#% 801769
#% 803609
#% 871765
#% 874884
#% 938789
#% 938799
#% 976987
#% 1022258
#% 1180021
#! Exact query reformulation using views in positive relational languages is well understood, and has a variety of applications in query optimization and data sharing. Generalizations to larger fragments of the relational algebra (RA) --- specifically, support for the difference operator --- would increase the options available for query reformulation, and also apply to view adaptation (updating a materialized view in response to a modified view definition) and view maintenance. Unfortunately, most questions about queries become undecidable in the presence of difference/negation. We present a novel way of managing this difficulty via an excursion through a non-standard semantics, Z-relations, where tuples are annotated with positive or negative integers. We show that under Z-semantics RA queries have a normal form as a single difference of positive queries and this leads to the decidability of equivalence. In most real-world settings with difference, it is possible to convert the queries to this normal form. We give a sound and complete algorithm that explores all reformulations of an RA query (under Z-semantics) using a set of RA views, finitely bounding the search space with a simple and natural cost model. We investigate related complexity questions, and we also extend our results to queries with built-in predicates. Z-relations are interesting in their own right because they capture updates and data uniformly. However, our algorithm turns out to be sound and complete also for bag semantics, albeit necessarily only for a subclass of RA. This subclass turns out to be quite large and covers generously the applications of interest to us. We also show a subclass of RA where reformulation and evaluation under Z-semantics can be combined with duplicate elimination to obtain the answer under set semantics.

#index 1180015
#* Automatic construction of simple artifact-based business processes
#@ Christian Fritz;Richard Hull;Jianwen Su
#t 2009
#c 6
#% 35562
#% 38228
#% 60745
#% 62653
#% 189636
#% 190332
#% 245656
#% 266168
#% 314843
#% 321426
#% 323377
#% 348131
#% 632068
#% 770373
#% 786874
#% 807689
#% 824702
#% 982526
#% 1062932
#% 1068659
#% 1137809
#% 1180017
#% 1250631
#% 1289241
#% 1395966
#% 1397963
#% 1415590
#% 1709213
#! Almost all medium- and large-scale businesses rely on electronic workflow systems to manage their business processes. A key challenge is to enable the easy re-use and modification of these workflow schemas and their piece-parts, so that they can be adapted to new business situations. This paper describes an approach for automatic construction (and thus, evolution) of a workflow schema that satisfies a specified condition (or "goal"), starting from a set of basic building block services (or "tasks"). We use a workflow model based on "business artifacts", which represent key (real or conceptual) business entities, and include both the business-relevant data about them and a specification of their lifecycle, that is, how they can evolve over time as they move through the workflow as the result of services being applied to them. This paper uses a declarative form of artifact-centric workflow. The services are non-deterministic, which corresponds to the intuition that humans performing the services may rely on information that is not modeled within the framework. We study the problem of, given a goal to be achieved, automatically finding the "maximal" workflow schema that has the following property: every execution is either complete or can be completed, and every complete execution satisfies the goal. We also study a complimentary problem, in which exception-handling is used to deal with executions that would otherwise not complete successfully. These problems are non-trivial because the workflow services are non-deterministic. This paper provides a general framework for studying these problems, and shows a tight relationship between workflow systems specified using logics that permit quantifier elimination and the ability to construct maximal schemas with the desired properties. The paper then studies a restricted setting to provide insights into complexity issues. Even in the restricted setting, the problem of testing properties of maximal workflows is PSPACE-complete.

#index 1180016
#* TOP-K projection queries for probabilistic business processes
#@ Daniel Deutch;Tova Milo
#t 2009
#c 6
#% 71306
#% 86957
#% 114677
#% 496116
#% 835628
#% 893117
#% 1016201
#% 1022204
#% 1022252
#% 1063522
#% 1063720
#% 1127379
#% 1206929
#% 1408536
#% 1688305
#% 1707157
#% 1707651
#% 1709206
#! A Business Process (BP) consists of some business activities undertaken by one or more organizations in pursuit of some business goal. Tools for querying and analyzing BP specifications are extremely valuable for companies. In particular, given a BP specification, identifying the top-k flows that are most likely to occur in practice, out of those satisfying a given query criteria, is crucial for various applications such as personalized advertizement and BP web-site design. This paper studies, for the first time, top-k query evaluation for queries with projection in this context. We analyze the complexity of the problem for different classes of distribution functions for the flows likelihood, and provide efficient (PTIME) algorithms whenever possible. Furthermore, we show an interesting application of our algorithms to the analysis of BP execution traces (logs), for recovering missing information about the run-time process behavior, that has not been recorded in the logs.

#index 1180017
#* Automatic verification of data-centric business processes
#@ Alin Deutsch;Richard Hull;Fabio Patrizi;Victor Vianu
#t 2009
#c 6
#% 101955
#% 213957
#% 266168
#% 321054
#% 348131
#% 431006
#% 445446
#% 570649
#% 581901
#% 615784
#% 630964
#% 632068
#% 769518
#% 770373
#% 807689
#% 810053
#% 874885
#% 875055
#% 888014
#% 888015
#% 942360
#% 982402
#% 982526
#% 1062932
#% 1063731
#% 1099727
#% 1137809
#% 1180015
#% 1392367
#% 1395966
#% 1396853
#% 1415338
#% 1415590
#% 1709213
#! We formalize and study business process systems that are centered around "business artifacts", or simply "artifacts". Artifacts are used to represent (real or conceptual) key business entities, including both their data schema and lifecycles. The lifecycle of an artifact type specifies the possible sequencings of services that can be applied to an artifact of this type as it progresses through the business process. The artifact-centric approach was introduced by IBM, and has been used to achieve substantial savings when performing business transformations. In this paper, artifacts carry attribute records and internal state relations (holding sets of tuples) that services can consult and update. In addition, services can access an underlying database and can introduce new values from an infinite domain, thus modeling external inputs or partially specified processes described by pre-and-post conditions. The lifecycles associate services to the artifacts using declarative, condition-action style rules. We consider the problem of statically verifying whether all runs of an artifact system satisfy desirable correctness properties expressed in a first-order extension of linear-time temporal logic. We map the boundaries of decidability for the verification problem and provide its complexity. The technical challenge to static verification stems from the presence of data from an infinite domain, yielding an infinite-state system. While much work has been done lately in the verification community on model checking specialized classes of infinite-state systems, the available results do not transfer to our framework, and this remains a difficult problem. We identify an expressive class of artifact systems for which verification is nonetheless decidable. The complexity of verification is PSPACE-complete, which is no worse than classical finite-state model checking. This investigation builds upon previous work on verification of data-driven Web services and ASM transducers, while addressing significant new technical challenges raised by the artifact model.

#index 1180018
#* Tight results for clustering and summarizing data streams
#@ Sudipto Guha
#t 2009
#c 6
#% 3426
#% 232768
#% 238182
#% 274152
#% 287391
#% 338344
#% 338425
#% 397389
#% 399763
#% 410958
#% 479648
#% 481266
#% 578388
#% 580668
#% 656775
#% 823463
#% 824685
#% 866990
#% 982754
#% 989609
#% 1016154
#% 1817010
#! In this paper we investigate algorithms and lower bounds for summarization problems over a single pass data stream. In particular we focus on histogram construction and K-center clustering. We provide a simple framework that improves upon all previous algorithms on these problems in either the space bound, the approximation factor or the running time. The framework uses a notion of "streamstrapping" where summaries created for the initial prefixes of the data are used to develop better approximation algorithms. We also prove the first non-trivial lower bounds for these problems. We show that the stricter requirement that if an algorithm accurately approximates the error of every bucket or every cluster produced by it, then these upper bounds are almost the best possible. This property of accurate estimation is true of all known upper bounds on these problems.

#index 1180019
#* Analysis of sampling techniques for association rule mining
#@ Venkatesan T. Chakaravarthy;Vinayaka Pandit;Yogish Sabharwal
#t 2009
#c 6
#% 152934
#% 481290
#% 481779
#% 577261
#% 614619
#% 867053
#! In this paper, we present a comprehensive theoretical analysis of the sampling technique for the association rule mining problem. Most of the previous works have concentrated only on the empirical evaluation of the effectiveness of sampling for the step of finding frequent itemsets. To the best of our knowledge, a theoretical framework to analyze the quality of the solutions obtained by sampling has not been studied. Our contributions are two-fold. First, we present the notions of ε-close frequent itemset mining and ε-close association rule mining that help assess the quality of the solutions obtained by sampling. Secondly, we show that both the frequent items mining and association rule mining problems can be solved satisfactorily with a sample size that is independent of both the number of transactions size and the number of items. Let θ be the required support, ε the closeness parameter, and 1/h the desired bound on the probability of failure. We show that the sampling based analysis succeeds in solving both ε-close frequent itemset mining and ε-close association rule mining with a probability of at least (1 - 1/h) with a sample of size S = O(1/ε2θ [Δ + log h/(1 - ε)θ]), where Δ is the maximum number of items present in any transaction. Thus, we establish that it is possible to speed up the entire process of association rule mining for massive databases by working with a small sample while retaining any desired degree of accuracy. Our work gives a comprehensive explanation for the well known empirical successes of sampling for association rule mining.

#index 1180020
#* The average-case complexity of counting distinct elements
#@ David P. Woodruff
#t 2009
#c 6
#% 2833
#% 115608
#% 190611
#% 233698
#% 238182
#% 278835
#% 282942
#% 299989
#% 386006
#% 411355
#% 492912
#% 519953
#% 749451
#% 818434
#% 833351
#% 859116
#% 874904
#% 894646
#% 1029125
#% 1039656
#% 1056443
#% 1061648
#% 1916580
#! We continue the study of approximating the number of distinct elements in a data stream of length n to within a (1 ± ε) factor. It is known that if the stream may consist of arbitrary data arriving in an arbitrary order, then any 1-pass algorithm requires Ω(1/ε2) bits of space to perform this task. To try to bypass this lower bound, the problem was recently studied in a model in which the stream may consist of arbitrary data, but it arrives to the algorithm in a random order. However, even in this model an Ω(1/ε2) lower bound was established. This is because the adversary can still choose the data arbitrarily. This leaves open the possibility that the problem is only hard under a pathological choice of data, which would be of little practical relevance. We study the average-case complexity of this problem under certain distributions. Namely, we study the case when each successive stream item is drawn independently and uniformly at random from an unknown subset of d items for an unknown value of d. This captures the notion of random uncorrelated data. For a wide range of values of d and n, we design a 1-pass algorithm that bypasses the Ω(1/ε2) lower bound that holds in the adversarial and random-order models, thereby showing that this model admits more space-efficient algorithms. Moreover, the update time of our algorithm is optimal. Despite these positive results, for a certain range of values of d and n we show that estimating the number of distinct elements requires Ω(1/ε2) bits of space even in this model. Our lower bound subsumes previous bounds, showing that even for natural choices of data the problem is hard.

#index 1180021
#* Containment of conjunctive queries on annotated relations
#@ Todd J. Green
#t 2009
#c 6
#% 663
#% 122396
#% 137867
#% 140410
#% 162828
#% 190638
#% 215225
#% 228817
#% 248034
#% 273696
#% 289266
#% 318704
#% 378401
#% 443429
#% 464722
#% 599549
#% 757932
#% 801769
#% 803609
#% 874883
#% 874884
#% 960293
#% 976987
#% 977012
#% 1022258
#% 1063709
#% 1063736
#% 1180014
#% 1206732
#! We study containment and equivalence of (unions of) conjunctive queries on relations annotated with elements of a commutative semiring. Such relations and the semantics of positive relational queries on them were introduced in a recent paper as a generalization of set semantics, bag semantics, incomplete databases, and databases annotated with various kinds of provenance information. We obtain positive decidability results and complexity characterizations for databases with lineage, why-provenance, and provenance polynomial annotations, for both conjunctive queries and unions of conjunctive queries. At least one of these results is surprising given that provenance polynomial annotations seem "more expressive" than bag semantics and under the latter, containment of unions of conjunctive queries is known to be undecidable. The decision procedures rely on interesting variations on the notion of containment mappings. We also show that for any positive semiring (a very large class) and conjunctive queries without self-joins, equivalence is the same as isomorphism.

#index 1180022
#* Optimizing user views for workflows
#@ Olivier Biton;Susan B. Davidson;Sanjeev Khanna;Sudeepa Roy
#t 2009
#c 6
#% 410276
#% 504161
#% 600539
#% 803468
#% 825661
#% 832825
#% 896027
#% 1022327
#% 1042649
#% 1063593
#% 1206750
#% 1728161
#! A technique called user views has recently been proposed to focus user attention on relevant information in response to provenance queries over workflow executions [1, 2]: Given user input on what modules in the workflow specification are relevant to the user, a user view is a concise representation that clusters together modules to create a small number of composite modules (or clusters) such that (1) each composite module in a user view contains at most one relevant (atomic) module, thus assuming the "meaning" of that module; and (2) no control or data dependencies (either direct or indirect) are introduced (soundness) or removed (completeness) between relevant modules. The goal is to find a user view with a smallest number of composite modules. We show that for workflow specifications that are general graphs, regardless of the number of distinct modules in the input workflow and the structure of interaction between them, there always exists a user view of size at most (2k--1 -- k)2 + k, where k is the number of relevant modules. Moreover, a good user view with at most (2k--1 -- k)2 + k clusters can be computed in polynomial time in the size of the graph. We also show that this upper bound is tight. Thus in general graphs, the number of composite modules can be exponentially large in k even in an optimum user view for the specification. We also give a characterization of a good user view in terms of structural properties of each cluster in the user view. However, for series-parallel workflow graphs, we show that there is always a user-view with at most 2k -- 3 composite modules; further, there exist series-parallel graphs where every user view requires at least 2k -- 3 composite modules. Such graphs capture the structure of many scientific and other workflows that we have encountered in practice. For this class of graphs, we give a simple, linear time algorithm for constructing an optimum user view for a given specification.

#index 1424585
#* Increasing representational power and scaling reasoning in probabilistic databases
#@ Amol Deshpande
#t 2010
#c 6
#% 850430
#% 1000502
#% 1179162
#% 1200291
#% 1217181
#% 1270256
#% 1273915
#% 1279353
#% 1289560
#% 1291123
#% 1417109
#! Increasing numbers of real-world application domains are generating data that is inherently noisy, incomplete, and probabilistic in nature. Statistical analysis and probabilistic inference, widely used in those domains, often introduce additional layers of uncertainty. Examples include sensor data analysis, data integration and information extraction on the Web, social network analysis, and scientific and biomedical data management. Managing and querying such data requires us to combine the tools and the techniques from a variety of disciplines including databases, first-order logic, and probabilistic reasoning. There has been much work at the intersection of these research areas in recent years. The work on probabilistic databases has made great advances in efficiently executing SQL and inference queries over large-scale uncertain datasets [2, 1]. The research in first-order probabilistic models like probabilistic relational models [5], Markov logic networks [10] etc. (see Getoor and Taskar [6] for a comprehensive overview), and the work on lifted inference [9, 3, 8, 11] has resulted in several techniques for efficiently integrating first-order logic and probabilistic reasoning. In this talk, I will present some of the foundations of large-scale probabilistic data management, and the challenges in scaling the representational power and the reasoning capabilities of probabilistic databases. I will use the PrDB probabilistic data management system being developed at the University of Maryland as a case study for this purpose [4, 7, 12]. Unlike the other recent work on probabilistic databases, PrDB is designed to represent uncertain data with rich correlation structures, and it uses probabilistic graphical models as the basic representation model. I will discuss how PrDB supports compact specification of uncertainties at different abstraction levels, from "schema-level" uncertainties that apply to entire relations to "tuple-specific" uncertainties that apply to a specific tuple or a specific set of tuples; I will also discuss how this relates to the work on first-order probabilistic models. Query evaluation in PrDB can be formulated as inference in appropriately constructed graphical models, and I will briefly present some of the key novel techniques that we have developed for efficient query evaluation, and their relationship to recent work on efficient lifted inference. I will conclude with a discussion of some of the open research challenges moving forward.

#index 1424586
#* From polynomial time queries to graph structure theory
#@ Martin Grohe
#t 2010
#c 6
#% 1068068
#% 1068080
#% 1259254
#% 1259269
#% 1700136
#! In a fundamental article on query languages for relational databases, Chandra and Harel [2] asked in 1982 whether there is a language that expresses precisely those queries which can be answered in polynomial time. Gurevich [10] later rephrased the question in the language of finite model theory, asking whether there is a logic that captures polynomial time. Despite serious efforts in the late 1980s and the 1990s, the question is still wide open. It is considered to be one of the main open problems in database theory and finite model theory. Recently, there has been a renewed interest in the question. New languages have been proposed [1, 4, 5] and old ones reconsidered [3, 12], and a number of partial results stating that certain languages capture polynomial time on large and natural classes of structures have been obtained [6, 8, 9, 11]. My talk will be a survey of the state of the art in the "quest for a logic capturing polynomial time." The focus will be on positive results for restricted classes of structures. This will lead us on an excursion to modern graph structure theory, and in particular to Robertson and Seymour's Graph Minor Theory [13]. Besides the references cited in the abstract, the following list contains a reference to the short survey [7].

#index 1424587
#* Provenance for database transformations
#@ Pierre Fraigniaud
#t 2010
#c 6
#! Network representations play an important role in many domains of computer science, ranging from data structures and graph algorithms, to parallel and distributed computing, and communication networks. Traditional network representations are usually global in nature. That is, in order to retrieve useful information, one must access a global data structure representing the entire network, even if the desired information is solely local, pertaining to only a few nodes. In contrast, the notion of informative labeling schemes suggests the use of a local representation of the network. The principle is to associate a label with each node, selected in a way that enables to infer information about any two nodes directly from their labels, without using any additional sources of information. Hence in essence, this method bases the entire representation on the set of labels alone. Obviously, labels of unrestricted size can be used to encode any desired information, including in particular the entire graph structure. The focus is thus on informative labeling schemes which use labels as short as possible. This talk will introduce the notion of informative labeling scheme to the audience, and will survey some of the important results achieved in this context. In particular, we will focus on the design of compact adjacency-, ancestry-, and distance-labeling schemes for trees. These schemes find applications in various contexts, including the design of small universal graphs, and the design of small universal posets. We will actually specifically emphasis the importance of ancestry- labeling scheme for the design of compact such schemes finds applications in XML search engines. In this context, even small improvements in the label size are important, and we will survey the most recent results in this domain. The interested person is referred to the following paper, that includes pointers to many of the most important references on informative labeling schemes: Pierre Fraigniaud and Amos Korman, Compact Ancestry Labeling Schemes for XML Trees, in ACM-SIAM Symposium on Discrete Algorithms (SODA), 2010.

#index 1424588
#* Foundations of SPARQL query optimization
#@ Michael Schmidt;Michael Meier;Georg Lausen
#t 2010
#c 6
#% 801677
#% 824755
#% 826032
#% 851283
#% 857502
#% 956573
#% 1022236
#% 1044477
#% 1055731
#% 1063724
#% 1127402
#% 1127431
#% 1127610
#% 1152438
#% 1152440
#% 1223424
#% 1328190
#% 1655424
#% 1655429
#% 1914019
#! We study fundamental aspects related to the efficient processing of the SPARQL query language for RDF, proposed by the W3C to encode machine-readable information in the Semantic Web. Our key contributions are (i) a complete complexity analysis for all operator fragments of the SPARQL query language, which -- as a central result -- shows that the SPARQL operator Optional alone is responsible for the PSpace-completeness of the evaluation problem, (ii) a study of equivalences over SPARQL algebra, including both rewriting rules like filter and projection pushing that are well-known from relational algebra optimization as well as SPARQL-specific rewriting schemes, and (iii) an approach to the semantic optimization of SPARQL queries, built on top of the classical chase algorithm. While studied in the context of a theoretically motivated set semantics, almost all results carry over to the official, bag-based semantics and therefore are of immediate practical relevance.

#index 1424589
#* Efficient processing of 3-sided range queries with probabilistic guarantees
#@ A. Kaporis;A. N. Papadopoulos;S. Sioutas;K. Tsakalidis;K. Tsichlas
#t 2010
#c 6
#% 210355
#% 252304
#% 273714
#% 281731
#% 341100
#% 378388
#% 484843
#% 571296
#% 875023
#% 1016202
#% 1336222
#% 1698239
#% 1703218
#! This work studies the problem of 2-dimensional searching for the 3-sided range query of the form [a, b] x (-∞, c] in both main and external memory, by considering a variety of input distributions. A dynamic linear main memory solution is proposed, which answers 3-sided queries in O(log n + t) worst case time and scales with O (log log n) expected with high probability update time, under continuous μ-random distributions of the x and y coordinates, where n is the current number of stored points and t is the size of the query output. Our expected update bound constitutes a considerable improvement over the O(log n) update time bound achieved by the classic Priority Search Tree of McCreight [23], as well as over the Fusion Priority Search Tree of Willard [30], which requires O(log n/log log n) time for all operations. Moreover, we externalize this solution, gaining O(logB n + t/B) worst case and O(logBlogn) amortized expected with high probability I/Os for query and update operations respectively, where B is the disk block size. Then, combining the Modified Priority Search Tree [27] with the Priority Search Tree [23], we achieve a query time of O(log log n + t) expected with high probability and an update time of O(log log n) expected with high probability, under the assumption that the x-coordinates are continuously drawn from a smooth distribution and the y-coordinates are continuously drawn from a more restricted class of distributions. The total space is linear. Finally, we externalize this solution, obtaining a dynamic data structure that answers 3-sided queries in O(logB log n + t/B) I/Os expected with high probability, and it can be updated in O(logB log n) I/Os amortized expected with high probability and consumes O(n/B) space, under the same assumptions.

#index 1424590
#* A theoretical study of 'Snapshot Isolation'
#@ Ragnar Normann;Lene T. Østby
#t 2010
#c 6
#% 201869
#% 336201
#% 757988
#% 783784
#% 814649
#% 923676
#% 1124990
#! Snapshot Isolation is a popular and efficient protocol for concurrency control. In this paper we discuss Snapshot Isolation in view of the classical theory for transaction processing. In addition to summarizing previous research we prove that the set SI of histories that may be generated by Snapshot Isolation is incomparable to final state, view and conflict serializability, that SI is monotone, and that schedules generated by Snapshot Isolation are strict and thus have good properties with respect to recoverability.

#index 1424591
#* Aggregate queries for discrete and continuous probabilistic XML
#@ Serge Abiteboul;T.-H. Hubert Chan;Evgeny Kharlamov;Werner Nutt;Pierre Senellart
#t 2010
#c 6
#% 265692
#% 430773
#% 654487
#% 800547
#% 871765
#% 977012
#% 986483
#% 991156
#% 993985
#% 1016178
#% 1022204
#% 1063522
#% 1063720
#% 1063722
#% 1131141
#% 1200291
#% 1217138
#% 1291113
#% 1291120
#% 1408537
#% 1688305
#! Sources of data uncertainty and imprecision are numerous. A way to handle this uncertainty is to associate probabilistic annotations to data. Many such probabilistic database models have been proposed, both in the relational and in the semi-structured setting. The latter is particularly well adapted to the management of uncertain data coming from a variety of automatic processes. An important problem, in the context of probabilistic XML databases, is that of answering aggregate queries (count, sum, avg, etc.), which has received limited attention so far. In a model unifying the various (discrete) semi-structured probabilistic models studied up to now, we present algorithms to compute the distribution of the aggregation values (exploiting some regularity properties of the aggregate functions) and probabilistic moments (especially, expectation and variance) of this distribution. We also prove the intractability of some of these problems and investigate approximation techniques. We finally extend the discrete model to a continuous one, in order to take into account continuous data values, such as measurements from sensor networks, and present algorithms to compute distribution functions and moments for various classes of continuous distributions of data values.

#index 1424592
#* Querying parse trees of stochastic context-free grammars
#@ Sara Cohen;Benny Kimelfeld
#t 2010
#c 6
#% 190443
#% 247892
#% 279755
#% 345710
#% 397375
#% 478452
#% 479535
#% 545382
#% 578771
#% 741360
#% 786577
#% 816169
#% 817472
#% 850524
#% 881539
#% 939341
#% 939555
#% 977012
#% 977013
#% 993985
#% 1016201
#% 1022204
#% 1063522
#% 1063719
#% 1063720
#% 1134141
#% 1217141
#% 1330545
#% 1699599
#! Stochastic context-free grammars (SCFGs) have long been recognized as useful for a large variety of tasks including natural language processing, morphological parsing, speech recognition, information extraction, Web-page wrapping and even analysis of RNA. A string and an SCFG jointly represent a probabilistic interpretation of the meaning of the string, in the form of a (possibly infinite) probability space of parse trees. The problem of evaluating a query over this probability space is considered under the conventional semantics of querying a probabilistic database. For general SCFGs, extremely simple queries may have results that include irrational probabilities. But, for a large subclass of SCFGs (that includes all the standard studied subclasses of SCFGs) and the language of tree-pattern queries with projection (and child/descendant edges), it is shown that query results have rational probabilities with a polynomial-size bit representation and, more importantly, an efficient query-evaluation algorithm is presented.

#index 1424593
#* Probabilistic data exchange
#@ Ronald Fagin;Benny Kimelfeld;Phokion G. Kolaitis
#t 2010
#c 6
#% 215225
#% 219471
#% 265692
#% 806215
#% 810098
#% 826032
#% 850730
#% 893189
#% 976987
#% 977012
#% 977013
#% 977014
#% 992830
#% 1016201
#% 1022206
#% 1022258
#% 1022259
#% 1039063
#% 1063522
#% 1063534
#% 1063719
#% 1063720
#% 1190673
#% 1206732
#% 1408537
#% 1496496
#% 1688305
#! The work reported here lays the foundations of data exchange in the presence of probabilistic data. This requires rethinking the very basic concepts of traditional data exchange, such as solution, universal solution, and the certain answers of target queries. We develop a framework for data exchange over probabilistic databases, and make a case for its coherence and robustness. This framework applies to arbitrary schema mappings, and finite or countably infinite probability spaces on the source and target instances. After establishing this framework and formulating the key concepts, we study the application of the framework to a concrete and practical setting where probabilistic databases are compactly encoded by means of annotations formulated over random Boolean variables. In this setting, we study the problems of testing for the existence of solutions and universal solutions, materializing such solutions, and evaluating target queries (for unions of conjunctive queries) in both the exact sense and the approximate sense. For each of the problems, we carry out a complexity analysis based on properties of the annotation, in various classes of dependencies. Finally, we show that the framework and results easily and completely generalize to allow not only the data, but also the schema mapping itself to be probabilistic.

#index 1424594
#* Synthesizing view definitions from data
#@ Anish Das Sarma;Aditya Parameswaran;Hector Garcia-Molina;Jennifer Widom
#t 2010
#c 6
#% 302747
#% 341672
#% 376266
#% 572311
#% 598376
#% 835018
#% 893114
#% 1063711
#% 1092519
#% 1217187
#% 1839809
#! Given a database instance and a corresponding view instance, we address the view definitions problem (VDP): Find the most succinct and accurate view definition, when the view query is restricted to a specific family of queries. We study the tradeoffs among succintness, level of approximation, and the family of queries through algorithms and complexity results. For each family of queries, we address three variants of the VDP: (1) Does there exist an exact view definition, and if so find it. (2) Find the best view definition, i.e., one as close to the input view instance as possible, and as succinct as possible. (3) Find an approximate view definition that satisfies an input approximation threshold, and is as succinct as possible.

#index 1424595
#* k-jump strategy for preserving privacy in micro-data disclosure
#@ Wen Ming Liu;Lingyu Wang;Lei Zhang
#t 2010
#c 6
#% 299970
#% 443463
#% 765449
#% 800515
#% 801690
#% 809244
#% 810011
#% 857493
#% 874989
#% 881546
#% 937550
#% 1015140
#% 1022247
#% 1103937
#% 1700133
#% 1700134
#% 1707132
#! In disclosing micro-data with sensitive attributes, the goal is usually two fold. First, the data utility of disclosed data should be maximized for analysis purposes. Second, the private information contained in such data must be limited to an acceptable level. Recent studies show that adversarial inferences using knowledge about a disclosure algorithm can usually render the algorithm unsafe. In this paper, we show that an existing unsafe algorithm can be transformed into a large family of distinct safe algorithms, namely, k-jump algorithms. We prove that the data utility of different k-jump algorithms is generally incomparable. Therefore, a secret choice can be made among all k-jump algorithms to eliminate adversarial inferences while improving the data utility of disclosed micro-data.

#index 1424596
#* Bag equivalence of XPath queries
#@ Sara Cohen;Yaacov Y. Weiss
#t 2010
#c 6
#% 190638
#% 465051
#% 733593
#% 801769
#% 938789
#% 976991
#% 1211652
#! When a query is evaluated under bag semantics, each answer is returned as many times as it has derivations. Bag semantics has long been recognized as important, especially when aggregation functions will be applied to query results. This paper is the first to focus on bag semantics for XPath queries. In particular, the problem of bag-equivalence of a large class of XPath queries (modeled as tree patterns) is explored. The queries can contain unions, branching, label wildcards, the vertical child and descendant axes, the horizontal following, following-sibling and immediately-following sibling axes, as well as positional (i.e., first and last) axes. Equivalence characterizations are provided, and their complexity is analyzed. As the descendent axis involves a recursive relationship, this paper is also the first to address bag equivalence over recursive queries, in any setting.

#index 1424597
#* Composition with target constraints
#@ Marcelo Arenas;Ronald Fagin;Alan Nash
#t 2010
#c 6
#% 188350
#% 378409
#% 465053
#% 465057
#% 488616
#% 765540
#% 778122
#% 801691
#% 809239
#% 809249
#% 850730
#% 857502
#% 893094
#% 912245
#% 976995
#% 976997
#% 997492
#% 1015302
#% 1022258
#% 1039063
#% 1044476
#% 1063710
#% 1063712
#% 1063723
#% 1063724
#% 1132129
#% 1152349
#% 1179998
#% 1180001
#% 1180003
#% 1200406
#% 1215806
#% 1217137
#% 1232194
#% 1985061
#! It is known that the composition of schema mappings, each specified by source-to-target tgds (st-tgds), can be specified by a second-order tgd (SO tgd). We consider the question of what happens when target constraints are allowed. Specifically, we consider the question of specifying the composition of standard schema mappings (those specified by st-tgds, target egds, and a weakly-acyclic set of target tgds). We show that SO tgds, even with the assistance of arbitrary source constraints and target constraints, cannot specify in general the composition of two standard schema mappings. Therefore, we introduce source-to-target second-order dependencies (st-SO dependencies), which are similar to SO tgds, but allow equations in the conclusion. We show that st-SO dependencies (along with target egds and target tgds) are sufficient to express the composition of every finite sequence of standard schema mappings, and further, every st-SO dependency specifies such a composition. In addition to this expressive power, we show that st-SO dependencies enjoy other desirable properties. In particular, they have a polynomial-time chase that generates a universal solution. This universal solution can be used to find the certain answers to unions of conjunctive queries in polynomial time. It is easy to show that the composition of an arbitrary number of standard schema mappings is equivalent to the composition of only two standard schema mappings. We show that surprisingly, the analogous result holds also for schema mappings specified by just st-tgds (no target constraints). That is, the composition of an arbitrary number of such schema mappings is equivalent to the composition of only two such schema mappings. This is proven by showing that every SO tgd is equivalent to an unnested SO tgd (one where there is no nesting of function symbols). The language of unnested SO tgds is quite natural, and we show that unnested SO tgds are capable of specifying the composition of an arbitrary number of schema mappings, each specified by st-tgds. Similarly, we prove unnesting results for st-SO dependencies, with the same types of consequences. The collapsing result for SO tgds gives us two alternative ways to deal with the composition of multiple schema mappings specified by st-tgds. First, we can replace the composition by a single schema mapping, specified by an unnested SO tgd. Second, we can replace the composition by the composition of only two schema mappings, each specified by st-tgds. A similar comment holds for the composition of standard schema mappings.

#index 1424598
#* Answering non-monotonic queries in relational data exchange
#@ André Hernich
#t 2010
#c 6
#% 264858
#% 384978
#% 801691
#% 806215
#% 809239
#% 826032
#% 850730
#% 874879
#% 874881
#% 874882
#% 912245
#% 976995
#% 1039061
#% 1039063
#% 1063722
#% 1063723
#% 1063724
#% 1180002
#% 1215806
#% 1348452
#% 1488993
#! Relational data exchange deals with translating a relational database instance over some source schema into a relational database instance over some target schema, according to a schema mapping that specifies the relationship between the source data and the target data. Various semantics for answering queries against the target schema exist, each of them suitable for a certain class of queries, and with respect to certain schema mappings. However, for each of these semantics, there are examples that show that it leads to counter-intuitive answers, or that it does not respect logical equivalence of schema mappings. In this article, we study query answering semantics for deductive databases in the context of relational data exchange. Furthermore, we propose a new semantics, called GCWA*-answers semantics, which seems to be well-suited with respect to a number of schema mappings, including schema mappings defined by st-tgds and egds. We show that the GCWA*-answers semantics coincides with the classical certain answers semantics on monotonic queries, and we further explore the data complexity of computing the GCWA*-answers to non-monotonic queries. In particular, we identify a class of schema mappings for which the GCWA*-answers to universal queries can be computed from the core of the universal solutions in polynomial time (data complexity).

#index 1424599
#* On the tradeoff between mapping and querying power in XML data exchange
#@ Shun'ichi Amano;Claire David;Leonid Libkin;Filip Murlak
#t 2010
#c 6
#% 332166
#% 772031
#% 801691
#% 809239
#% 826032
#% 850730
#% 865766
#% 866986
#% 874882
#% 960233
#% 976995
#% 993981
#% 1021195
#% 1039061
#% 1063722
#% 1063723
#% 1106486
#% 1180002
#% 1215806
#% 1217117
#% 1217139
#% 1348452
#% 1408529
#! In XML data exchange, a schema mapping specifies rules for restructuring a source document under the target schema, and queries over the target document must be answered in a way consistent with the source information. Mapping rules and queries in this scenario are typically based on various kinds of tree patterns. Patterns with downward navigation have been studied, and tractable classes of mappings and queries have been isolated. In this paper we extend schema mappings and queries with general tree patterns that include horizontal navigation and data-value comparisons, and study their impact on the tractability of the query answering problem. Our main results state that, in the nutshell, extending the tractable cases for downward patterns with expressive schema mappings is harmless, but adding new features to queries quickly leads to intractability even for very simple schema mapping.

#index 1424600
#* The complexity of rooted phylogeny problems
#@ Manuel Bodirsky;Jens K. Mueller
#t 2010
#c 6
#% 190340
#% 201269
#% 248033
#% 268708
#% 282020
#% 305564
#% 335852
#% 384978
#% 733125
#% 802982
#% 944139
#% 1172186
#% 1669590
#! Several computational problems in phylogenetic reconstruction can be formulated as restrictions of the following general problem: given a formula in conjunctive normal form where the atomic formulas are rooted triples, is there a rooted binary tree that satisfies the formula? If the formulas do not contain disjunctions and negations, the problem becomes the famous rooted triple consistency problem, which can be solved in polynomial time by an algorithm of Aho, Sagiv, Szymanski, and Ullman. If the clauses in the formulas are restricted to disjunctions of negated triples, Ng, Steel, and Wormald showed that the problem remains NP-complete. We systematically study the computational complexity of the problem for all such restrictions of the clauses in the input formula. For certain restricted disjunctions of triples we present an algorithm that has sub-quadratic running time and is asymptotically as fast as the fastest known algorithm for the rooted triple consistency problem. We also show that any restriction of the general rooted phylogeny problem that does not fall into our tractable class is NP-complete, using known results about the complexity of Boolean constraint satisfaction problems. Finally, we present a pebble game argument that shows that the rooted triple consistency problem (and also all generalizations studied in this paper) cannot be solved by Datalog.

#index 1424601
#* A greedy algorithm for constructing a low-width generalized hypertree decomposition
#@ Kaoru Katayama;Tatsuro Okawara;Yuka Ito
#t 2010
#c 6
#% 273683
#% 333865
#% 563132
#% 727988
#% 798967
#% 847068
#% 942358
#% 1000772
#% 1091908
#% 1152055
#% 1224352
#% 1273786
#! We propose a greedy algorithm which, given a hypergraph H and a positive integer k, produces a hypertree decomposition of width less than or equal to 3k -- 1, or determines that H does not have a generalized hypertree-width less than k. The running time of this algorithm is O(mk+2n), where m is the number of hyperedges and n is the number of vertices. If k is a constant, it is polynomial. The concepts of (generalized) hypertree decomposition and (generalized) hypertree-width were introduced by Gottlob et al. Many important NP-complete problems in database theory or artificial intelligence are polynomially solvable for classes of instances associated with hypergraphs of bounded hypertree-width. Gottlob et al. also developed a polynomial time algorithm det-k-decomp which, given a hypergraph H and a constant k, computes a hypertree decomposition of width less than or equal to k if the hypertree-width of H is less than or equal to k. The running time of det-k-decomp is O(m2kn2) in the worst case, where m and n are the number of hyperedges and the number of vertices, respectively. The proposed algorithm is faster than this. The key step of our algorithm is checking whether a set of hyperedges is an obstacle to a hypergraph having low generalized hypertree-width. We call such a local hyper-graph structure a k-hyperconnected set. If a hypergraph contains a k-hyperconnected set with a size of at least 2k, it has hypertree-width of at least k. Adler et al. propose another obstacle called a k-hyperlinked set. We discuss the difference between the two concepts with examples.

#index 1424602
#* Static analysis of schema-mappings ensuring oblivious termination
#@ Bruno Marnette;Floris Geerts
#t 2010
#c 6
#% 384978
#% 806215
#% 809238
#% 809239
#% 810078
#% 826031
#% 826032
#% 874882
#% 912245
#% 976995
#% 1039063
#% 1063723
#% 1063724
#% 1217115
#% 1217122
#% 1217124
#% 1328190
#% 1328193
#% 1661430
#! A schema-mapping is a high level specification of a data-exchange setting where a set of source-to-target dependencies is used to realize basic operations from source to target relations (such as copy, selection, join or union) while the target schema is subject to a set of target constraints (such as inclusion dependencies or key constraints). In this paper, we consider strong schema-mappings that allow for additional constraints such as source dependencies on the source schema and target-to-source dependencies from the target relations back to the source. Furthermore, strong schema-mappings may include disjunctive dependencies. We argue that this extension is desirable when the source instance is to provide both a lower and upper bound on the information that a target instance can have. We first focus on the implication problem for strong schema-mappings which is to determine whether a given constraint δ is logically implied by the set Σ of constraints (denoted by σ &vDash; δ). After providing complete characterizations for this problem in terms of universal solutions (while supporting equality constraints), we introduce criteria of termination, denoted by TOC, DTOC and MTOC, that allow the efficient computation of universal solutions for standard constraints, disjunctive constraints, and when the source instance is assumed to be immutable (i.e., it is master data), respectively. We obtain decision procedures for the implication problem, provided that Σ satisfies these termination conditions, and give the corresponding complexity bounds. As an immediate application we revisit the problems of determinacy, relative information completeness and variations thereof, all for strong schema-mappings. Indeed, by viewing them as implication problems we obtain efficient decision procedures when the relevant termination conditions are satisfied. We then focus on the problem of deciding whether source-to-target constraints in a strong schema-mapping are already implied by the embedded (standard) schema-mapping. This problem is important if one wants to use target-to-source constraints in standard data-exchange tools. Since no such constraints are logically implied by standard schema-mappings (and hence the results established earlier are of no use), we provide an alternative semantics for implication. More specifically, we want the constraint to be satisfied by every solution corresponding to the output of a standard data-exchange tool. We consider three semantics based on universal solutions, cores and CWA-solutions, respectively. Decidability of the implication of general (resp. safe) target-to-source constraints is shown for the CWA-based semantics (resp. core-semantics).

#index 1424603
#* Mapping polymorphism
#@ Ryan Wisnesky;Mauricio A. Hernández;Lucian Popa
#t 2010
#c 6
#% 273688
#% 283052
#% 378409
#% 480134
#% 492755
#% 572328
#% 800497
#% 810078
#% 826032
#% 848315
#% 850730
#% 875029
#% 893093
#% 893094
#% 893193
#% 960233
#% 976998
#% 993981
#% 997492
#% 1015302
#% 1063710
#% 1200406
#% 1206803
#% 1232194
#! We examine schema mappings from a type-theoretic perspective and aim to facilitate and formalize the reuse of mappings. Starting with the mapping language of Clio, we present a type-checking algorithm such that typable mappings are necessarily satisfiable. We add type variables to the schema language and present a theory of polymorphism, including a sound and complete type inference algorithm and a semantic notion of a principal type of a mapping. Principal types, which intuitively correspond to the minimum amount of schema structure required by the mappings, have an important application for mapping reuse. Concretely, we show that mappings can be reused, with the same semantics, on any schemas as long as these schemas are expansions (i.e., subtypes) of the principal types.

#index 1424604
#* Composing local-as-view mappings: closure and applications
#@ Patricia C. Arocena;Ariel Fuxman;Reneé J. Miller
#t 2010
#c 6
#% 248038
#% 328429
#% 378409
#% 384978
#% 481923
#% 572307
#% 824736
#% 826032
#% 850730
#% 893093
#% 912245
#% 927032
#% 997492
#% 1015302
#% 1054485
#% 1063710
#% 1180001
#% 1217116
#% 1328194
#% 1434929
#! Schema mapping composition is a fundamental operation in schema management and data exchange. The mapping composition problem has been extensively studied for a number of mapping languages, most notably source-to-target tuple-generating dependencies (s-t tgds). An important class of s-t tgds are local-as-view (LAV) tgds. This class of mappings is prevalent in practical data integration and exchange systems, and recent work by ten Cate and Kolaitis shows that such mappings possess desirable structural properties. It is known that s-t tgds are not closed under composition. That is, given two mappings expressed with s-t tgds, their composition may not be definable by any set of s-t tgds (and, in general, may not be expressible in first-order logic). Despite their importance and extensive use in data integration and exchange systems, the closure properties of LAV composition remained open to date. The most important contribution of this paper is to show that LAV tgds are closed under composition, and provide an algorithm to directly compute the composition. An important application of our composition result is that it helps to understand if given a LAV mapping Mst from schema S to schema T, and a LAV mapping Mts from schema T back to S, the composition of Mst and Mts is able to recover the information in any instance of S. Arenas et al. formalized this notion and showed that general s-t tgds mappings always have a recovery. Hence, a LAV mapping always has a recovery. However, the problem of testing whether a given Mts is a recovery of Mst is known to be undecidable for general s-t tgds. In contrast, in this paper we show the tractability of the problem for LAV mappings, and give a polynomial-time algorithm to solve it.

#index 1424605
#* Data correspondence, exchange and repair
#@ Gösta Grahne;Adrian Onet
#t 2010
#c 6
#% 273687
#% 378409
#% 384978
#% 464867
#% 465057
#% 809239
#% 850730
#% 874880
#% 879041
#% 912245
#% 1179998
#% 1180001
#% 1215806
#% 1217116
#% 1661426
#% 1661438
#% 1705010
#! Checking the correspondence between two or more database instances and enforcing it is a procedure widely used in practice without however having been explored from a theoretical perspective. In this paper we formally introduce the data correspondence setting and its associated computational problems: checking the existence of solutions, and verifying whether a candidate is a solution, for three distinct types of solutions. Data correspondence is a generalization of data exchange and peer data exchange, and a special case of repairing inconsistent databases. We introduce a new class of dependencies, called semi-LAV, that properly includes both LAV and full dependencies, while retaining tractability for peer data exchange, data correspondence, and database repairs. We also introduce the concept of Σ-satisfying homomorphisms. This new type of homomorphism, together with recent advances, is essential in achieving tractability, while at the same time allowing a large class of dependencies, namely the aforementioned semi-LAV ones. We also show the intractability for a series of problems in the case of weakly acyclic tuple generating dependencies. This implies that many tractability results for weakly acyclic dependencies do not carry over to data correspondence; in these new settings we need to restrict the dependencies a bit further, yielding our semi-LAV dependencies.

#index 1424606
#* Forward-XPath and extended register automata on data-trees
#@ Diego Figueira
#t 2010
#c 6
#% 330274
#% 814648
#% 874877
#% 874910
#% 888015
#% 1039062
#% 1063734
#% 1217135
#% 1266894
#% 1673664
#! We consider a fragment of XPath named 'forward-XPath', which contains all descendant and rightwards sibling axes as well as data equality and inequality tests. The satisfiability problem for forward-XPath in the presence of DTDs and even of primary key constraints is shown here to be decidable. To show decidability we introduce a model of alternating automata on data trees that can move downwards and rightwards in the tree, have one register for storing data and compare them for equality, and have the ability to (1) non-deterministically guess a data value and store it, and (2) quantify universally over the set of data values seen so far during the run. This model extends the work of Jurdzi&nacute;ski and Lazi&cacute;. Decidability of the finitary non-emptiness problem for this model is obtained by a direct reduction to a well-structured transition system, contrary to previous approaches.

#index 1424607
#* On the aggregation problem for synthesized web services
#@ Ting Deng;Wenfei Fan;Leonid Libkin;Yinghui Wu
#t 2010
#c 6
#% 297770
#% 321054
#% 388196
#% 630964
#% 740213
#% 753306
#% 754120
#% 770373
#% 786874
#% 799801
#% 800007
#% 805892
#% 824702
#% 874885
#% 942360
#% 996295
#% 1063731
#% 1063732
#% 1180015
#% 1180017
#% 1195119
#% 1389438
#% 1415590
#% 1702399
#! The paper formulates and investigates the aggregation problem for synthesized mediators of Web services (SWMs). An SWM is a finite-state transducer defined in terms of templates for component services. Upon receiving an artifact, an SWM selects a set of available services from a library to realize its templates, and invokes those services to operate on the artifact, in parallel; it produces a numeric value as output (e.g., the total price of a package) by applying synthesis rules. Given an SWM, a library and an input artifact, the aggregation problem is to find a mapping from the component templates of the SWM to available services in the library that maximizes (or minimizes) the output. As opposed to the composition syntheses of Web services, the aggregation problem aims to optimize the realization of a given mediator, to best serve the users' need. We analyze this problem, and show that its complexity depends on the underlying graph structure of the mediator: while it is undecidable when such graphs contain even very simple cycles, it is solvable in single-exponential time (in the size of the specification) for SWMs whose underlying graphs are acyclic. We prove several results of this kind, with matching lower bounds (NP and PSPACE), and analyze restrictions that lead to polynomial-time solutions.

#index 1538769
#* Tractability in probabilistic databases
#@ Dan Suciu
#t 2011
#c 6
#% 442830
#% 864417
#% 893168
#% 976984
#% 992830
#% 993985
#% 1016201
#% 1022259
#% 1036075
#% 1063521
#% 1063523
#% 1111133
#% 1127378
#% 1206962
#% 1206987
#% 1207234
#% 1217176
#% 1291116
#% 1291119
#% 1291120
#% 1328151
#% 1426461
#! Probabilistic databases are motivated by a large and diverse set of applications that need to query and process uncertain data. Uncertain and probabilistic data arises in RFID systems [22], information extraction [12], data cleaning [1], scientific data management [17], biomedical data integration [9], business intelligence [14], approximate schema mappings [10], data deduplication [13]. All these applications have large collections of data, where some, or most individual data items are uncertain.

#index 1538770
#* On provenance and privacy
#@ Susan B. Davidson;Sanjeev Khanna;Sudeepa Roy;Julia Stoyanovich;Val Tannen;Yi Chen
#t 2011
#c 6
#% 344639
#% 379248
#% 393844
#% 576110
#% 576761
#% 659990
#% 742048
#% 765449
#% 765450
#% 832825
#% 874892
#% 937550
#% 956511
#% 960261
#% 1014464
#% 1022252
#% 1041522
#% 1074831
#% 1121279
#% 1153476
#% 1164737
#% 1180022
#% 1193149
#% 1206679
#% 1206750
#% 1217125
#% 1217188
#% 1375910
#% 1414540
#% 1426581
#% 1448946
#% 1468475
#% 1523877
#% 1720925
#% 1728161
#! Provenance in scientific workflows is a double-edged sword. On the one hand, recording information about the module executions used to produce a data item, as well as the parameter settings and intermediate data items passed between module executions, enables transparency and reproducibility of results. On the other hand, a scientific workflow often contains private or confidential data and uses proprietary modules. Hence, providing exact answers to provenance queries over all executions of the workflow may reveal private information. In this paper we discuss privacy concerns in scientific workflows -- data, module, and structural privacy - and frame several natural questions: (i) Can we formally analyze data, module, and structural privacy, giving provable privacy guarantees for an unlimited/bounded number of provenance queries? (ii) How can we answer search and structural queries over repositories of workflow specifications and their executions, providing as much information as possible to the user while still guaranteeing privacy? We then highlight some recent work in this area and point to several directions for future work.

#index 1538771
#* The PADS project: an overview
#@ Kathleen Fisher;David Walker
#t 2011
#c 6
#% 65894
#% 310908
#% 319343
#% 428348
#% 461269
#% 809123
#% 912487
#% 997040
#% 1015354
#% 1024194
#% 1172465
#% 1310061
#% 1344678
#% 1373703
#% 1410422
#% 1426198
#! The goal of the PADS project, which started in 2001, is to make it easier for data analysts to extract useful information from ad hoc data files. This paper does not report new results, but rather gives an overview of the project and how it helps bridge the gap between the unmanaged world of ad hoc data and the managed world of typed programming languages and databases. In particular, the paper reviews the design of PADS data description languages, describes the generated parsing tools and discusses the importance of meta-data. It also sketches the formal semantics, discusses useful tools and how can they can be generated automatically from PADS descriptions, and describes an inferencing system that can learn useful PADS descriptions from positive examples of the data format.

#index 1538772
#* Efficient reasoning about data trees via integer linear programming
#@ Claire David;Leonid Libkin;Tony Tan
#t 2011
#c 6
#% 289328
#% 333841
#% 398752
#% 401092
#% 401124
#% 476530
#% 509964
#% 545382
#% 576106
#% 579716
#% 630965
#% 643569
#% 734789
#% 742056
#% 888014
#% 982402
#% 1021195
#% 1039061
#% 1086124
#% 1106486
#% 1173493
#% 1181329
#% 1198377
#% 1217135
#% 1373460
#% 1425586
#% 1511867
#% 1718501
#% 1916593
#! Data trees provide a standard abstraction of XML documents with data values: they are trees whose nodes, in addition to the usual labels, can carry labels from an infinite alphabet (data). Therefore, one is interested in decidable formalisms for reasoning about data trees. While some are known -- such as the two-variable logic -- they tend to be of very high complexity, and most decidability proofs are highly nontrivial. We are therefore interested in reasonable complexity formalisms as well as better techniques for proving decidability. Here we show that many decidable formalisms for data trees are subsumed -- fully or partially -- by the power of tree automata together with set constraints and linear constraints on cardinalities of various sets of data values. All these constraints can be translated into instances of integer linear programming, giving us an NP bound on the complexity of the reasoning tasks. We prove that this bound, as well as the key encoding technique, remain very robust, and allow the addition of features such as counting of paths and patterns, and even a concise encoding of constraints, without increasing the complexity. We also relate our results to several reasoning tasks over XML documents, such as satisfiability of schemas and data dependencies and satisfiability of the two-variable logic.

#index 1538773
#* Generating, sampling and counting subclasses of regular tree languages
#@ Timos Antonopoulos;Floris Geerts;Wim Martens;Frank Neven
#t 2011
#c 6
#% 70235
#% 103957
#% 159532
#% 169726
#% 183092
#% 227530
#% 262724
#% 281764
#% 343978
#% 397407
#% 498205
#% 848763
#% 894435
#% 949370
#% 988839
#% 1001430
#% 1021195
#% 1022285
#% 1055754
#% 1105409
#% 1127694
#% 1217138
#% 1217202
#% 1223426
#% 1232519
#% 1370257
#% 1426465
#% 1504036
#% 1722176
#! To experimentally validate learning and approximation algorithms for XML Schema Definitions (XSDs), we need algorithms to generate uniformly at random a corpus of XSDs as well as a similarity measure to compare how close the generated XSD resembles the target schema. In this paper, we provide the formal foundation for such a testbed. We adopt similarity measures based on counting the number of common and different trees in the two languages, and we develop the necessary machinery for computing them. We use the formalism of extended DTDs (EDTDs) to represent the unranked regular tree languages. In particular, we obtain an efficient algorithm to count the number of trees up to a certain size in an unambiguous EDTD. The latter class of unambiguous EDTDs encompasses the more familiar classes of single-type, restrained competition and bottom-up deterministic EDTDs. The single-type EDTDs correspond precisely to the core of XML Schema, while the others are strictly more expressive. We also show how constraints on the shape of allowed trees can be incorporated. As we make use of a translation into a well-known formalism for combinatorial specifications, we get for free a sampling procedure to draw members of any unambiguous EDTD. When dropping the restriction to unambiguous EDTDs, i.e. taking the full class of EDTDs into account, we show that the counting problem becomes #P-complete and provide an approximation algorithm. Finally, we discuss uniform generation of single-type EDTDs, i.e., the formal abstraction of XSDs. To this end, we provide an algorithm to generate k-occurrence automata (k-OAs) uniformly at random and show how this leads to uniform generation of single-type EDTDs.

#index 1538774
#* View update translation for XML
#@ Iovka Boneva;Anne-Cécile Caron;Benoît Groz;Yves Roos;Sophie Tison;Sławek Staworko
#t 2011
#c 6
#% 664
#% 24750
#% 43031
#% 99458
#% 183302
#% 286901
#% 287000
#% 288571
#% 397366
#% 416007
#% 547142
#% 799800
#% 875010
#% 891734
#% 894437
#% 942354
#% 948929
#% 957975
#% 1015275
#% 1105438
#% 1153334
#% 1196553
#% 1255692
#% 1266687
#% 1266689
#% 1384857
#% 1405946
#% 1493211
#% 1728678
#! We study the problem of update translation for views on XML documents. More precisely, given an XML view definition and a user defined view update program, find a source update program that translates the view update without side effects on the view. Additionally, we require the translation to be defined on all possible source documents; this corresponds to Hegner's notion of uniform translation. The existence of such translation would allow to update XML views without the need of materialization. The class of views we consider can remove parts of the document and rename nodes. Our update programs define the simultaneous application of a collection of atomic update operations among insertion/deletion of a subtree and node renaming. Such update programs are compatible with the XQuery Update Facility (XQUF) snapshot semantics. Both views and update programs are represented by recognizable tree languages. We present as a proof of concept a small fragment of XQUF that can be expressed by our update programs, thus allows for update propagation. Two settings for the update problem are studied: without source constraints, where all source updates are allowed, and with source constraints, where there is a restricted set of authorized source updates. Using tree automata techniques, we establish that without constraints, all view updates are uniformly translatable and the translation is tractable. In presence of constraints, not all view updates are uniformly translatable. However, we introduce a reasonable restriction on update programs for which uniform translation with constraints becomes possible.

#index 1538775
#* Querying probabilistic business processes for sub-flows
#@ Daniel Deutch
#t 2011
#c 6
#% 54223
#% 71306
#% 238399
#% 424283
#% 492610
#% 599777
#% 824806
#% 893117
#% 977012
#% 1022204
#% 1063731
#% 1063736
#% 1127379
#% 1127389
#% 1134141
#% 1180016
#% 1206929
#% 1217138
#% 1523864
#! This paper studies top-k query evaluation for an important class of probabilistic semi-structured data: nested DAGs (Directed Acyclic Graphs) that describe possible execution flows of Business Processes (BPs for short). We consider queries with projection, that select portions (sub-flows) of the execution flows that interest the user and are most likely to occur at run-time. Retrieving common sub-flows is crucial for various applications such as targeted advertisement and BP optimization. Sub-flows are ranked here by the sum of likelihood of EX-flows in which they appear, in contrast to the max-of-likelihood semantics studied in previous work; we show that while sum semantics is more natural, it makes query evaluation much more challenging. We study the problem for BPs and queries of varying classes and present efficient query evaluation algorithms whenever possible.

#index 1538776
#* Artifact systems with data dependencies and arithmetic
#@ Elio Damaggio;Alin Deutsch;Victor Vianu
#t 2011
#c 6
#% 266168
#% 289384
#% 321054
#% 348131
#% 384978
#% 431006
#% 445446
#% 465057
#% 562314
#% 570649
#% 581901
#% 604673
#% 615784
#% 630964
#% 632068
#% 769518
#% 770373
#% 807689
#% 834987
#% 888014
#% 888015
#% 942360
#% 982402
#% 982526
#% 1062932
#% 1063724
#% 1068329
#% 1099727
#% 1100590
#% 1103257
#% 1180017
#% 1260125
#% 1270568
#% 1335442
#% 1392367
#% 1395966
#% 1396853
#% 1415338
#% 1415590
#% 1417933
#% 1426453
#% 1562020
#% 1580599
#% 1709213
#! We revisit the static verification problem for data centric business processes, specified in a variant of IBM's "business artifact" model. Artifacts are records of variables that correspond to business-relevant objects and are updated by a set of services equipped with pre-and-post conditions, that implement business process tasks. The verification problem consists in statically checking whether all runs of an artifact system satisfy desirable properties expressed in a firstorder extension of linear-time temporal logic. In previous work we identified the class of guarded artifact systems and properties, for which verification is decidable. However, the results suffer from an important limitation: they fail in the presence of even very simple data dependencies or arithmetic, both crucial to real-life business processes. In this paper, we extend the artifact model and verification results to alleviate this limitation. We identify a practically significant class of business artifacts with data dependencies and arithmetic, for which verification is decidable. The technical machinery needed to establish the results is fundamentally different from our previous work. While the worst-case complexity of verification is non-elementary, we identify various realistic restrictions yielding more palatable upper bounds.

#index 1538777
#* Comparing workflow specification languages: a matter of views
#@ Serge Abiteboul;Pierre Bourhis;Victor Vianu
#t 2011
#c 6
#% 29439
#% 101955
#% 185412
#% 213957
#% 261269
#% 266168
#% 321054
#% 333855
#% 348131
#% 369768
#% 378411
#% 384978
#% 445446
#% 562314
#% 630964
#% 643569
#% 769518
#% 770373
#% 817690
#% 888014
#% 942360
#% 982526
#% 985981
#% 1063731
#% 1072645
#% 1100590
#% 1173493
#% 1180015
#% 1180017
#% 1326580
#% 1335442
#% 1415590
#% 1709213
#! We address the problem of comparing the expressiveness of workflow specification formalisms using a notion of view of a workflow. Views allow to compare widely different workflow systems by mapping them to a common representation capturing the observables relevant to the comparison. Using this framework, we compare the expressiveness of several workflow specification mechanisms, including automata, temporal constraints, and pre-and-post conditions, with XML and relational databases as underlying data models. One surprising result shows the considerable power of static constraints to simulate apparently much richer workflow control mechanisms.

#index 1538778
#* Relaxed notions of schema mapping equivalence revisited
#@ Reinhard Pichler;Emanuel Sallinger;Vadim Savenkov
#t 2011
#c 6
#% 583
#% 140410
#% 289384
#% 591778
#% 765540
#% 806215
#% 826032
#% 850730
#% 893089
#% 927032
#% 1015302
#% 1063712
#% 1063724
#% 1217115
#% 1217116
#% 1270567
#% 1328190
#% 1328194
#% 1328201
#% 1424597
#! Recently, two relaxed notions of equivalence of schema mappings have been introduced, which provide more potential of optimizing schema mappings than logical equivalence: data exchange (DE) equivalence and conjunctive query (CQ) equivalence. In this work, we systematically investigate these notions of equivalence for mappings consisting of s-t tgds and target egds and/or target tgds. We prove that both CQ- and DE-equivalence are undecidable and so are some important optimization tasks (like detecting if some dependency is redundant). However, we also identify an important difference between the two notions of equivalence: CQ-equivalence remains undecidable even if the schema mappings consist of s-t tgds and target dependencies in the form of key dependencies only. In contrast, DE-equivalence is decidable for schema mappings with s-t tgds and target dependencies in the form of functional and inclusion dependencies with terminating chase property.

#index 1538779
#* Solutions in XML data exchange
#@ Mikołaj Bojańczyk;Leszek A. Kołodziejczyk;Filip Murlak
#t 2011
#c 6
#% 332166
#% 427027
#% 570877
#% 809239
#% 826032
#% 848763
#% 850730
#% 865766
#% 960233
#% 1039061
#% 1039062
#% 1106486
#% 1106498
#% 1215806
#% 1217117
#% 1408529
#! The task of XML data exchange is to restructure a document conforming to a source schema under a target schema according to certain mapping rules. The rules are typically expressed as source-to-target dependencies using various kinds of patterns, involving horizontal and vertical navigation, as well as data comparisons. The target schema imposes complex conditions on the structure of solutions, possibly inconsistent with the mapping rules. In consequence, for some source documents there may be no solutions. We investigate three problems: deciding if all documents of the source schema can be mapped to a document of the target schema (absolute consistency), deciding if a given document of the source schema can be mapped (solution existence), and constructing a solution for a given source document (solution building). We show that the complexity of absolute consistency is rather high in general, but within the polynomial hierarchy for bounded depth schemas. The combined complexity of solution existence and solution building behaves similarly, but the data complexity turns out to be very low. In addition to this we show that even for much more expressive mapping rules, based on MSO definable queries, absolute consistency is decidable and data complexity of solution existence is polynomial.

#index 1538780
#* Simplifying schema mappings
#@ Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini;Moshe Y. Vardi
#t 2011
#c 6
#% 32904
#% 198465
#% 237191
#% 241144
#% 248038
#% 289266
#% 291299
#% 299967
#% 378409
#% 384978
#% 464717
#% 464720
#% 464867
#% 472877
#% 599549
#% 731485
#% 801691
#% 806215
#% 809235
#% 809239
#% 826032
#% 850730
#% 874913
#% 893089
#% 912245
#% 1015302
#% 1022349
#% 1054485
#% 1063712
#% 1063723
#% 1180001
#% 1215806
#% 1217116
#% 1217140
#% 1270567
#% 1328201
#% 1424597
#% 1424604
#% 1426443
#% 1426463
#% 1426464
#% 1426466
#! A schema mapping is a formal specification of the relationship holding between the databases conforming to two given schemas, called source and target, respectively. While in the general case a schema mapping is specified in terms of assertions relating two queries in some given language, various simplified forms of mappings, in particular LAV and GAV, have been considered, based on desirable properties that these forms enjoy. Recent works propose methods for transforming schema mappings to logically equivalent ones of a simplified form. In many cases, this transformation is impossible, and one might be interested in finding simplifications based on a weaker notion, namely logical implication, rather than equivalence. More precisely, given a schema mapping M, find a simplified (LAV, or GAV) schema mapping M' such that M' logically implies M. In this paper we formally introduce this problem, and study it in a variety of cases, providing techniques and complexity bounds. The various cases we consider depend on three parameters: the simplified form to achieve (LAV, or GAV), the type of schema mapping considered (sound, or exact), and the query language used in the schema mapping specification (conjunctive queries and variants over relational databases, or regular path queries and variants over graph databases). Notably, this is the first work on comparing schema mappings for graph databases.

#index 1538781
#* On the equivalence of distributed systems with queries and communication
#@ Serge Abiteboul;Balder ten Cate;Yannis Katsis
#t 2011
#c 6
#% 129217
#% 264263
#% 333989
#% 535150
#% 733595
#% 801671
#% 1116555
#% 1129529
#% 1217123
#% 1217135
#% 1477071
#% 1688304
#! Distributed data management systems consist of peers that store, exchange and process data in order to collaboratively achieve a common goal, such as evaluate some query. We study the equivalence of such systems. We model a distributed system by a collection of Active XML documents, i.e., trees augmented with function calls for performing tasks such as sending, receiving and querying data. As our model is quite general, the equivalence problem turns out to be undecidable. However, we exhibit several restrictions of the model, for which equivalence can be effectively decided. We also study the computational complexity of the equivalence problem, and present an axiomatization of equivalence, in the form of a set of equivalence-preserving rewrite rules allowing us to optimize a system by rewriting it into an equivalent, but possibly more efficient system.

#index 1538782
#* Two-variable logic and key constraints on data words
#@ Matthias Niewerth;Thomas Schwentick
#t 2011
#c 6
#% 283011
#% 578964
#% 742566
#% 888014
#% 1086124
#% 1173493
#% 1181329
#% 1328528
#% 1424606
#% 1529987
#% 1682381
#% 1700125
#! The paper introduces key constraints for data words and shows that it is decidable whether, for a given two-variable sentence &phiv; that can refer to the successor relation on positions and a set Κ of key constraints, there is a data string w that satisfies &phiv; and respects Κ. Here, the formula is allowed to refer to the successor relation but not to the linear order on the positions of the word. As a byproduct, a self-contained exposition of an algorithm that decides satisfiability of such formulas (without key constraints) in 2-nexptime is given.

#index 1538783
#* Satisfiability algorithms for conjunctive queries over trees
#@ James Cheney
#t 2011
#c 6
#% 384978
#% 487257
#% 839147
#% 865766
#% 927031
#% 938017
#% 963218
#% 980990
#% 1016139
#% 1039062
#% 1092015
#% 1106486
#% 1129529
#% 1268676
#% 1415274
#% 1523876
#% 1666188
#% 1718234
#! We investigate the satisfiability problem for conjunctions of constraints over ordered, unranked trees, including child, descendant, following-sibling, root, leaf, and first/last child constraints. We introduce new, symbolic approaches based on graph transformations, which simplify and check the consistency of a problem first, and delay blind search as long as possible. We prove correctness and termination for these algorithms. We also analyze the complexity of important special cases: binary and κ-ary intersection of certain classes of XPath expressions. Our main complexity result is that binary intersection (for positive, simple navigational XPath over all axes) is tractable for expressions with a bounded number of changes in direction in the path, which is typically small.

#index 1538784
#* Knowledge compilation meets database theory: compiling queries to decision diagrams
#@ Abhay Jha;Dan Suciu
#t 2011
#c 6
#% 3873
#% 101852
#% 182588
#% 227703
#% 261793
#% 285330
#% 289266
#% 317105
#% 442363
#% 599549
#% 936786
#% 976984
#% 976987
#% 977013
#% 992830
#% 1111133
#% 1180021
#% 1272349
#% 1343690
#% 1343724
#% 1372680
#% 1426461
#% 1523890
#! The goal of Knowledge Compilation is to represent a Boolean expression in a format in which it can answer a range of online-queries in PTIME. The online-query of main interest to us is model counting, because of its application to query evaluation on probabilistic databases, but other online-queries can be supported as well such as testing for equivalence, testing for implication, etc. In this paper we study the following problem. Given a database query q, decide whether its lineage can be compiled efficiently into a given target language. We consider four target languages, of strictly increasing expressive power(when the size of compilation is constrained to be polynomial in the input size): Read-Once Boolean formulae, OBDD, FBDD and d-DNNF. For each target, we study the class of database queries that admit polynomial size representation: these queries can also be evaluated in PTIME over probabilistic databases. When queries are restricted to conjunctive queries without self-joins, it was known that these four classes collapse to the class of hierarchical queries, which is also the class of PTIME queries over probabilistic databases. Our main result in this paper is that, in the case of Unions of Conjunctive Queries (UCQ), these classes form a strict hierarchy. Thus, unlike conjunctive queries without self-joins, the expressive power of UCQ differs considerably w.r.t. these target compilation languages. Moreover, we give a complete characterization of the first two target languages, based on the query's syntax.

#index 1538785
#* On the optimal approximation of queries using tractable propositional languages
#@ Robert Fink;Dan Olteanu
#t 2011
#c 6
#% 663
#% 39702
#% 204396
#% 289266
#% 386158
#% 414902
#% 480810
#% 960293
#% 976987
#% 992830
#% 1111133
#% 1200291
#% 1217176
#% 1253069
#% 1272349
#% 1523890
#% 1594634
#! This paper investigates the problem of approximating conjunctive queries without self-joins on probabilistic databases by lower and upper bounds that can be computed more efficiently. We study this problem via an indirection: Given a propositional formula &phis;, find formulas in a more restricted language that are greatest lower bound and least upper bound, respectively, of &phis;. We study bounds in the languages of read-once formulas, where every variable occurs at most once, and of read-once formulas in disjunctive normal form. We show equivalences of syntactic and model-theoretic characterisations of optimal bounds for unate formulas, and present algorithms that can enumerate them with polynomial delay. Such bounds can be computed by queries expressed using first-order queries extended with transitive closure and a special choice construct. Besides probabilistic databases, these results can also benefit the problem of approximate query evaluation in relational databases, since the bounds expressed by queries can be computed in polynomial combined complexity.

#index 1538786
#* (Approximate) uncertain skylines
#@ Peyman Afshani;Pankaj K. Agarwal;Lars Arge;Kasper Green Larsen;Jeff M. Phillips
#t 2011
#c 6
#% 1606
#% 2115
#% 10624
#% 190611
#% 288976
#% 321455
#% 465167
#% 480671
#% 654480
#% 654487
#% 824728
#% 875012
#% 943612
#% 992830
#% 993954
#% 1022203
#% 1058620
#% 1063485
#% 1064307
#% 1206866
#% 1206892
#% 1206893
#% 1217143
#% 1328116
#% 1328151
#% 1328153
#% 1523894
#% 1594651
#! Given a set of points with uncertain locations, we consider the problem of computing the probability of each point lying on the skyline, that is, the probability that it is not dominated by any other input point. If each point's uncertainty is described as a probability distribution over a discrete set of locations, we improve the best known exact solution. We also suggest why we believe our solution might be optimal. Next, we describe simple, near-linear time approximation algorithms for computing the probability of each point lying on the skyline. In addition, some of our methods can be adapted to construct data structures that can efficiently determine the probability of a query point lying on the skyline.

#index 1538787
#* Relative expressive power of navigational querying on graphs
#@ George H. L. Fletcher;Marc Gyssens;Dirk Leinders;Jan Van den Bussche;Dirk Van Gucht;Stijn Vansummeren;Yuqing Wu
#t 2011
#c 6
#% 31484
#% 261741
#% 291299
#% 338753
#% 384978
#% 390685
#% 465065
#% 562454
#% 654454
#% 824798
#% 845350
#% 850728
#% 927031
#% 935898
#% 1019798
#% 1600267
#! Motivated by both established and new applications, we study navigational query languages for graphs (binary relations). The simplest language has only the two operators union and composition, together with the identity relation. We make more powerful languages by adding any of the following operators: intersection; set difference; projection; coprojection; converse; transitive closure; and the diversity relation. All these operators map binary relations to binary relations. We compare the expressive power of all resulting languages. We do this not only for general path queries (queries where the result may be any binary relation) but also for boolean or yes/no queries (expressed by the nonemptiness of an expression). For both cases, we present the complete Hasse diagram of relative expressiveness. In particular, the Hasse diagram for boolean queries contains nontrivial separations and a few surprising collapses.

#index 1538788
#* Complexity of higher-order queries
#@ Huy Vu;Michael Benedikt
#t 2011
#c 6
#% 58346
#% 121631
#% 164382
#% 199806
#% 213982
#% 248037
#% 342829
#% 411557
#% 464540
#% 587394
#% 591778
#% 598376
#% 912238
#% 1385106
#% 1426445
#% 1426454
#! While relational algebra and calculus are a well-established foundation for classical database query languages, it is less clear what the analog is for higher-order functions, such as query transformations. Here we study a natural way to add higher-order functionality to query languages, by adding database query operators to the λ-calculus as constants. This framework, which we refer to as λ-embedded query languages, was introduced in [BPV10]. That work had a restricted focus: the containment and equivalence problems for query-to-query functions, in the case where only positive relational operators are allowed as constants. In this work we take an in-depth look at the most basic issue for such languages: the evaluation problem. We give a full picture of the complexity of evaluation for λ-embedded query languages, looking at a number of variations: with negation and without; with only relational algebra operators, and also with a recursion mechanism in the form of a query iteration operator; in a strongly-typed calculus as well as a weakly-typed one. We give tight bounds on both the combined complexity and the query complexity of evaluation in all these settings, in the process explaining connections with Datalog and prior work on λ-calculus evaluation.

#index 1538789
#* Conjunctive queries determinacy and rewriting
#@ Daniel Pasailă
#t 2011
#c 6
#% 198465
#% 198466
#% 227886
#% 237190
#% 248038
#% 273912
#% 303884
#% 303886
#% 333964
#% 479452
#% 481916
#% 976986
#% 1044476
#% 1661430
#% 1698887
#% 1914701
#! The problem of whether a query Q can be answered using a set of views V studies the possibility of computing Q when only the answers to the given set of views are available. In information-theoretic terms, we say that V determines Q iff for any two databases D1, D2, V(D1) = V(D2) implies Q(D1) = Q(D2). In the case that V determines Q, we also study the existence of equivalent rewritings of Q in terms of V in a specific rewriting language. Having a view language ν and a query language Q we say that ν-to-Q determinacy is decidable if there is an algorithm which, given a view V ε ν and a query Q ε Q, outputs whether V determines Q. We focus on the case where the views and the query are defined by subclasses of conjunctive queries and investigate in which cases V determines Q and the existence of equivalent rewritings of Q in terms of V. We define the class of CQcgraph queries as binary CQ queries whose body, if viewed as an undirected graph, is connected. Next, we establish necessary conditions for determinacy in the CQcgraph-to-CQcgraph case. We also show that CQchain-to-CQcgraph determinacy is decidable, extending the previous decidability result for CQchain-to-CQchain, where CQchain denotes the class of binary CQ queries whose body is a simple path between the two free variables. Finally, we provide an algorithm which, starting with a set of CQcgraph views V and an integer κ, generates a set of CQcgraph queries that are determined by V and have their size bounded by κ.

#index 1538790
#* Faster query answering in probabilistic databases using read-once functions
#@ Sudeepa Roy;Vittorio Perduca;Val Tannen
#t 2011
#c 6
#% 663
#% 697
#% 82144
#% 138422
#% 150635
#% 215225
#% 228817
#% 235023
#% 265692
#% 288984
#% 480102
#% 874987
#% 893167
#% 976984
#% 976987
#% 977013
#% 1016201
#% 1022258
#% 1111133
#% 1133732
#% 1206717
#% 1206987
#% 1217176
#% 1343550
#% 1343724
#% 1372709
#% 1417383
#% 1426461
#% 1426581
#% 1523890
#% 1728680
#! A boolean expression is in read-once form if each of its variables appears exactly once. When the variables denote independent events in a probability space, the probability of the event denoted by the whole expression in read-once form can be computed in polynomial time (whereas the general problem for arbitrary expressions is #P-complete). Known approaches to checking read-once property seem to require putting these expressions in disjunctive normal form. In this paper, we tell a better story for a large subclass of boolean event expressions: those that are generated by conjunctive queries without self-joins and on tuple-independent probabilistic databases. We first show that given a tuple-independent representation and the provenance graph of an SPJ query plan without self-joins, we can, without using the DNF of a result event expression, efficiently compute its co-occurrence graph. From this, the read-once form can already, if it exists, be computed efficiently using existing techniques. Our second and key contribution is a complete, efficient, and simple to implement algorithm for computing the read-once forms (whenever they exist) directly, using a new concept, that of co-table graph, which can be significantly smaller than the cooccurrence graph.

#index 1538791
#* The complexity of evaluating tuple generating dependencies
#@ Reinhard Pichler;Sebastian Skritek
#t 2011
#c 6
#% 583
#% 248038
#% 289384
#% 303886
#% 338450
#% 378409
#% 384978
#% 427161
#% 598376
#% 643572
#% 801691
#% 806215
#% 809239
#% 826032
#% 850730
#% 857282
#% 874879
#% 874882
#% 893089
#% 893094
#% 960233
#% 993981
#% 1022258
#% 1039063
#% 1063722
#% 1152349
#% 1180001
#% 1297644
#% 1328125
#% 1972413
#! Dependencies have played an important role in database design for many years. More recently, they have also turned out to be central to data integration and data exchange. In this work we concentrate on tuple generating dependencies (tgds) which enforce the presence of certain tuples in a database instance if certain other tuples are already present. Previous complexity results in data integration and data exchange mainly referred to the data complexity. In this work, we study the query complexity and combined complexity of a fundamental problem related to tgds, namely checking if a given tgd is satisfied by a database instance. We also address an important variant of this problem which deals with updates (by inserts or deletes) of a database: Here we have to check if all previously satisfied tgds are still satisfied after an update. We show that the query complexity and combined complexity of these problems are much higher than the data complexity. However, we also prove sufficient conditions on the tgds to reduce this high complexity.

#index 1538792
#* Detecting and exploiting near-sortedness for efficient relational query evaluation
#@ Sagi Ben-Moshe;Yaron Kanza;Eldar Fischer;Arie Matsliah;Mani Fischer;Carl Staelin
#t 2011
#c 6
#% 248824
#% 311810
#% 347235
#% 902757
#% 993168
#! Many relational operations are best performed when the relations are stored sorted over the relevant attributes (e.g. the common attributes in a natural join operation). However, generally relations are not stored sorted because it is expensive to maintain them this way (and impossible whenever there is more than one relevant sort key). Still, many times relations turn out to be nearly-sorted, where most tuples are close to their place in the order. This state can result from "leftover sortedness", where originally sorted relations were updated, or were combined into interim results when evaluating a complex query. It can also result from weak correlations between attribute values. Currently, nearly-sorted relations are treated the same as unsorted relations, and when relational operations are evaluated for them, a generic algorithm is used. Yet, many operations can be computed more efficiently by an algorithm that exploits this near-ordering. However, to consistently benefit from using such algorithms the system should also refrain from using the wrong algorithm for relations which happen not to be sorted at all. Thus, an efficient test is required, i.e., a very fast approximation algorithm for establishing whether a given relation is sufficiently nearly-sorted. In this paper, we provide the theoretical foundations for improving query evaluation over possibly nearly-sorted relations. First we formally define what it means for a relation to be nearly-sorted, and show how operations over such relations, such as natural join, set operations and sorting, can be executed significantly more efficiently using an algorithm that we provide. If a relation is nearly-sorted enough, then it can be sorted using two sequential reads of the relation, and writing no intermediate data to disk. We then construct efficient probabilistic tests for approximating the degree of the near-sortedness of a relation without having to read an entire file. The role of our algorithms in a database management system setting is illustrated as soon as the theoretical foundation is laid out. Finally, we outline factors that relate to practical implementations of our algorithms. We show how our test can be enhanced to provide an approximation rather than just a yes-no answer, and discuss its implementability in reallife scenarios where some sparseness may be present in the database files (e.g. if they were created using a B*-tree approach). We also show how our sort can benefit distributed systems and systems that use a solid-state drive, which may very well become prevalent in the near future.

#index 1538793
#* Data cleaning and query answering with matching dependencies and matching functions
#@ Leopoldo Bertossi;Solmaz Kolahi;Laks V. S. Lakshmanan
#t 2011
#c 6
#% 663
#% 58354
#% 94459
#% 101951
#% 109995
#% 191581
#% 273687
#% 279167
#% 287313
#% 527112
#% 582130
#% 727668
#% 752741
#% 874882
#% 879041
#% 893105
#% 913783
#% 949372
#% 1054484
#% 1063722
#% 1063725
#% 1129527
#% 1201863
#% 1328143
#% 1394993
#% 1401744
#% 1538793
#% 1661426
#! Matching dependencies were recently introduced as declarative rules for data cleaning and entity resolution. Enforcing a matching dependency on a database instance identifies the values of some attributes for two tuples, provided that the values of some other attributes are sufficiently similar. Assuming the existence of matching functions for making two attributes values equal, we formally introduce the process of cleaning an instance using matching dependencies, as a chase-like procedure. We show that matching functions naturally introduce a lattice structure on attribute domains, and a partial order of semantic domination between instances. Using the latter, we define the semantics of clean query answering in terms of certain/possible answers as the greatest lower bound/least upper bound of all possible answers obtained from the clean instances. We show that clean query answering is intractable in some cases. Then we study queries that behave monotonically w.r.t. semantic domination order, and show that we can provide an under/over approximation for clean answers to monotone queries. Moreover, non-monotone positive queries can be relaxed into monotone queries.

#index 1661425
#* Proceedings of the 11th international conference on Database Theory
#@ Thomas Schwentick;Dan Suciu
#t 2007
#c 6

#index 1661426
#* Consistent query answering: five easy pieces
#@ Jan Chomicki
#t 2007
#c 6
#% 663
#% 2655
#% 69272
#% 101956
#% 102787
#% 264858
#% 273687
#% 287313
#% 335500
#% 342829
#% 350105
#% 384978
#% 400992
#% 473009
#% 519568
#% 536447
#% 576116
#% 582130
#% 598376
#% 599549
#% 630971
#% 727668
#% 752741
#% 783532
#% 810019
#% 810020
#% 826007
#% 826032
#% 833132
#% 838543
#% 864417
#% 879041
#% 880394
#% 885862
#% 903332
#% 1279213
#% 1347336
#% 1661438
#% 1673662
#% 1673673
#% 1673674
#% 1683883
#% 1700140
#% 1705005
#% 1705008
#% 1728672
#% 1728681
#% 1728683
#% 1728684
#% 1733291
#! Consistent query answering (CQA) is an approach to querying inconsistent databases without repairing them first. This invited talk introduces the basics of CQA, and discusses selected issues in this area. The talk concludes with a summary of other relevant work and an outline of potential future research topics.

#index 1661427
#* Ask a better question, get a better answer a new approach to private data analysis
#@ Cynthia Dwork
#t 2007
#c 6
#% 67453
#% 287794
#% 576110
#% 576111
#% 809245
#% 810028
#% 1670071
#% 1732708
#% 1740518
#! Cryptographic techniques for reasoning about information leakage have recently been brought to bear on the classical problem of statistical disclosure control – revealing accurate statistics about a population while preserving the privacy of individuals. This new perspective has been invaluable in guiding the development of a powerful approach to private data analysis, founded on precise mathematical definitions, and yielding algorithms with provable, meaningful, privacy guarantees.

#index 1661428
#* Beauty and the beast: the theory and practice of information integration
#@ Laura Haas
#t 2007
#c 6
#% 116303
#% 273894
#% 287463
#% 287733
#% 309133
#% 316873
#% 323980
#% 340302
#% 344448
#% 378409
#% 420072
#% 479452
#% 479756
#% 480134
#% 481614
#% 481923
#% 572314
#% 644182
#% 753124
#% 765455
#% 769242
#% 782759
#% 809239
#% 810078
#% 826032
#% 867054
#% 874876
#% 875064
#% 875066
#% 875067
#% 880387
#% 891573
#% 1016220
#! Information integration is becoming a critical problem for businesses and individuals alike. Data volumes are sky-rocketing, and new sources and types of information are proliferating. This paper briefly reviews some of the key research accomplishments in information integration (theory and systems), then describes the current state-of-the-art in commercial practice, and the challenges (still) faced by CIOs and application developers. One critical challenge is choosing the right combination of tools and technologies to do the integration. Although each has been studied separately, we lack a unified (and certainly, a unifying) understanding of these various approaches to integration. Experience with a variety of integration projects suggests that we need a broader framework, perhaps even a theory, which explicitly takes into account requirements on the result of the integration, and considers the entire end-to-end integration process.

#index 1661429
#* Approximate data exchange
#@ Michel de Rougemont;Adrien Vieilleribière
#t 2007
#c 6
#% 256686
#% 288469
#% 379449
#% 414770
#% 414935
#% 465057
#% 600179
#% 616528
#% 801670
#% 801676
#% 806218
#% 809235
#% 888055
#! We introduce approximate data exchange, by relaxing classical data exchange problems such as Consistency and Typechecking to their approximate versions based on Property Testing. It provides a natural framework for consistency and safety questions, which first considers approximate solutions and then exact solutions obtained with a Corrector. We consider a model based on transducers of words and trees, and study ε-Consistency, i.e., the problem of deciding whether a given source instance I is ε-close to a source I′, whose image by a transducer is also ε-close to a target schema. We prove that ε-Consistency has an ε-tester, i.e. can be solved by looking at a constant fraction of the input I. We also show that ε-Typechecking on words can be solved in polynomial time, whereas the exact problem is PSPACE-complete. Moreover, data exchange settings can be composed when they are close.

#index 1661430
#* Determinacy and rewriting of conjunctive queries using views: a progress report
#@ Alan Nash;Luc Segoufin;Victor Vianu
#t 2007
#c 6
#% 198465
#% 198466
#% 248038
#% 273698
#% 286901
#% 299945
#% 378407
#% 378410
#% 384978
#% 464717
#% 464727
#% 481916
#% 564416
#% 809238
#! Suppose we are given a set of exact conjunctive views V and a conjunctive query Q. Suppose we wish to answer Q using V, but the classical test for the existence of a conjunctive rewriting of Q using V answers negatively. What can we conclude: (i) there is no way Q can be answered using V, or (ii) a more powerful rewriting language may be needed. This has been an open question, with conventional wisdom favoring (i). Surprisingly, we show that the right answer is actually (ii). That is, even if V provides enough information to answer Q, it may not be possible to rewrite Q in terms of V using just conjunctive queries – in fact, no monotonic language is sufficiently powerful. We also exhibit several well-behaved classes of conjunctive views and queries for which conjunctive rewritings remain sufficient. This continues a previous investigation of rewriting and its connection to semantic determinacy, for various query and view languages.

#index 1661431
#* Compact samples for data dissemination
#@ Tova Milo;Assaf Sagi;Elad Verbin
#t 2007
#c 6
#% 164166
#% 237196
#% 333938
#% 408396
#% 419687
#% 446419
#% 546058
#% 546223
#% 661476
#% 793890
#% 824766
#% 960136
#% 963598
#% 963884
#% 1849768
#! We consider data dissemination in a peer-to-peer network, where each user wishes to obtain some subset of the available information objects. In most of the modern algorithms for such data dissemination, the users periodically obtain samples of peer IDs (possibly with some summary of their content). They then use the samples for connecting to other peers and downloading data pieces from them. For a set O of information objects, we call a sample of peers, containing at least k possible providers for each object o∈O, a k-sample. In order to balance the load, the k-samples should be fair, in the sense that for every object, its providers should appear in the sample with equal probability. Also, since most algorithms send fresh samples frequently, the size of the k-samples should be as small as possible, to minimize communication overhead. We describe in this paper two novel techniques for generating fair and small k-samples in a P2P setting. The first is based on a particular usage of uniform sampling and has the advantage that it allows to build on standard P2P uniform sampling tools. The second is based on non-uniform sampling and requires more particular care, but is guaranteed to generate the smallest possible fair k-sample. The two algorithms exploit available dependencies between information objects to reduce the sample size, and are proved, both theoretically and experimentally, to be extremely effective.

#index 1661432
#* Privacy in GLAV information integration
#@ Alan Nash;Alin Deutsch
#t 2007
#c 6
#% 94459
#% 283052
#% 378409
#% 384978
#% 398263
#% 464717
#% 464867
#% 465057
#% 465066
#% 488620
#% 490489
#% 576111
#% 765432
#% 765447
#% 765449
#% 809238
#% 824718
#% 1015329
#% 1700133
#% 1700137
#! We define and study formal privacy guarantees for information integration systems, where sources are related to a public schema by mappings given by source-to-target dependencies which express inclusion of unions of conjunctive queries with equality. This generalizes previous privacy work in the global-as-view publishing scenario and covers local-as-view as well as combinations of the two. We concentrate on logical security, where malicious users have the same level of access as legitimate users: they can issue queries against the global schema which are answered under “certain answers” semantics and then use unlimited computational power and external knowledge on the results of the queries to guess the result of a secret query (“the secret”) on one or more of the sources, which are not directly accessible. We do not address issues of physical security, which include how to prevent users from gaining unauthorized access to the data. We define both absolute guarantees: how safe is the secret? and relative guarantees: how much of the secret is additionally disclosed when the mapping is extended, for example to allow new data sources or new relationships between an existing data source and the global schema? We provide algorithms for checking whether these guarantees hold and undecidability results for related, stronger guarantees.

#index 1661433
#* Unlocking keys for XML trees
#@ Sven Hartmann;Sebastian Link
#t 2007
#c 6
#% 55899
#% 273689
#% 321051
#% 332151
#% 384978
#% 398752
#% 428149
#% 465051
#% 491695
#% 562455
#% 564264
#% 578570
#% 630971
#% 733268
#% 733593
#% 742566
#% 771227
#% 801700
#% 804840
#% 814648
#% 826029
#% 1221201
#% 1712426
#! We review key constraints in the context of XML as introduced by Buneman et al. We show that one of the proposed inference rules is not sound in general, and the axiomatisation proposed for XML keys is incomplete even if key paths are simple. Therefore, the axiomatisation and also the implication problem for XML keys are still unsolved. We propose a set of inference rules that is indeed sound and complete for the implication of XML keys with simple key paths. Our completeness proof enables us to characterise the implication of XML keys in terms of the reachability problem of nodes in a digraph. This results in a quadratic time algorithm for deciding XML key implication, and shows that reasoning for XML keys is practically efficient.

#index 1661434
#* Characterization of the interaction of XML functional dependencies with DTDs
#@ Łucja Kot;Walker White
#t 2007
#c 6
#% 7348
#% 248024
#% 267604
#% 273689
#% 287295
#% 333855
#% 384978
#% 458850
#% 564264
#% 733268
#% 742566
#% 771227
#% 809236
#% 823637
#% 877241
#% 1656308
#! With the rise of XML as a standard model of data exchange, XML functional dependencies (XFDs) have become important to areas such as key analysis, document normalization, and data integrity. XFDs are more complicated than relational functional dependencies because the set of XFDs satisfied by an XML document depends not only on the document values, but also the tree structure and corresponding DTD. In particular, constraints imposed by DTDs may alter the implications from a base set of XFDs, and may even be inconsistent with a set of XFDs. In this paper we examine the interaction between XFDs and DTDs. We present a sound and complete axiomatization for XFDs, both alone and in the presence of certain classes of DTDs; we show that these DTD classes induce an axiomatic hierarchy. We also give efficient implication algorithms for those classes of DTDs that do not use disjunction or nesting.

#index 1661435
#* Axiomatizing the logical core of XPath 2.0
#@ Balder ten Cate;Maarten Marx
#t 2007
#c 6
#% 357150
#% 384978
#% 717512
#% 850728
#% 874909
#% 888014
#% 993939
#% 1673664
#! The first aim of this paper is to present the logical core of XPath 2.0: a logically clean, decidable fragment, which includes most navigational features of XPath 2.0 (complex counting conditions and data joins are not supported, as they lead to undecidability). The second aim is to provide a list of equations completely axiomatizing query equivalence in this language (i.e., all other query equivalences can be derived from these).

#index 1661436
#* Query evaluation on a database given by a random graph
#@ Nilesh Dalvi
#t 2007
#c 6
#% 33547
#% 216970
#% 248038
#% 333986
#% 378409
#% 572311
#% 576111
#% 598376
#% 599549
#% 678484
#% 765449
#% 810017
#% 824718
#% 864412
#% 874988
#% 1543751
#% 1700137
#! We consider random graphs, and their extensions to random structures, with edge probabilities of the form βn−−α, where n is the number of vertices, α, β are fixed and α 1 (α arity – 1 for structures of higher arity). We consider conjunctive properties over these random graphs, and investigate the problem of computing their asymptotic conditional probabilities. This provides us a novel approach to dealing with uncertainty in databases, with applications to data privacy and other database problems.

#index 1661437
#* The limits of querying ontologies
#@ Riccardo Rosati
#t 2007
#c 6
#% 663
#% 11817
#% 205398
#% 230142
#% 248026
#% 263136
#% 378409
#% 384978
#% 519438
#% 598376
#% 665856
#% 665873
#% 754129
#% 874914
#% 1250550
#% 1269453
#% 1289408
#! We study query answering in Description Logics (DLs). In particular, we consider conjunctive queries, unions of conjunctive queries, and their extensions with safe negation or inequality, which correspond to well-known classes of relational algebra queries. We provide a set of decidability, undecidability and complexity results for answering queries of the above languages over various classes of Description Logics knowledge bases. In general, such results show that extending standard reasoning tasks in DLs to answering relational queries is unfeasible in many DLs, even in inexpressive ones. In particular: (i) answering even simple conjunctive queries is undecidable in some very expressive DLs in which standard DL reasoning is decidable; (ii) in DLs where answering (unions of) conjunctive queries is decidable, adding the possibility of expressing safe negation or inequality leads in general to undecidability of query answering, even in DLs of very limited expressiveness. We also highlight the negative consequences of these results for the integration of ontologies and rules. We believe that these results have important implications for ontology-based information access, in particular for the design of query languages for ontologies.

#index 1661438
#* Complexity of consistent query answering in databases under cardinality-based and incremental repair semantics
#@ Andrei Lopatenko;Leopoldo Bertossi
#t 2007
#c 6
#% 42986
#% 131559
#% 175440
#% 214247
#% 273687
#% 292675
#% 443378
#% 449224
#% 465052
#% 473009
#% 540034
#% 576116
#% 582130
#% 724872
#% 752741
#% 838392
#% 857282
#% 1347336
#% 1661438
#% 1673673
#% 1673674
#% 1700140
#% 1707166
#! A database D may be inconsistent wrt a given set IC of integrity constraints. Consistent Query Answering (CQA) is the problem of computing from D the answers to a query that are consistent wrt IC. Consistent answers are invariant under all the repairs of D, i.e. the consistent instances that minimally depart from D. Three classes of repair have been considered in the literature: those that minimize set-theoretically the set of tuples in the symmetric difference; those that minimize the changes of attribute values, and those that minimize the cardinality of the set of tuples in the symmetric difference. The latter class has not been systematically investigated. In this paper we obtain algorithmic and complexity theoretic results for CQA under this cardinality-based repair semantics. We do this in the usual, static setting, but also in a dynamic framework where a consistent database is affected by a sequence of updates, which may make it inconsistent. We also establish comparative results with the other two kinds of repairs in the dynamic case.

#index 1661439
#* World-set decompositions: expressiveness and efficient algorithms
#@ Lyublena Antova;Christoph Koch;Dan Olteanu
#t 2007
#c 6
#% 663
#% 94459
#% 248038
#% 366807
#% 384978
#% 479754
#% 752741
#% 768808
#% 783532
#% 801692
#% 810019
#% 864417
#% 893167
#% 1016201
#% 1673673
#! Uncertain information is commonplace in real-world data management scenarios. An important challenge in this context is the ability to represent large sets of possible instances (worlds) while supporting efficient storage and processing. The recent formalism of world-set decompositions (WSDs) provides a space-efficient representation for uncertain data that also supports scalable processing. WSDs are complete for finite world-sets in that they can represent any finite set of possible worlds. For possibly infinite world-sets, we show that a natural generalization of WSDs precisely captures the expressive power of c-tables. We then show that several important problems are efficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we give a polynomial-time algorithm for factorizing WSDs, i.e. an efficient algorithm for minimizing such representations.

#index 1661440
#* On the expressiveness of implicit provenance in query and update languages
#@ Peter Buneman;James Cheney;Stijn Vansummeren
#t 2007
#c 6
#% 189868
#% 318704
#% 384978
#% 464891
#% 566114
#% 617859
#% 803468
#% 825661
#% 864469
#% 875015
#% 893167
#% 1016204
#! Information concerning the origin of data (that is, its provenance) is important in many areas, especially scientific recordkeeping. Currently, provenance information must be maintained explicitly, by added effort of the database maintainer. Since such maintenance is tedious and error-prone, it is desirable to provide support for provenance in the database system itself. In order to provide such support, however, it is important to provide a clear explanation of the behavior and meaning of existing database operations, both queries and updates, with respect to provenance. In this paper we take the view that a query or update implicitly defines a provenance mapping linking components of the output to the originating components in the input. Our key result is that the proposed semantics are expressively complete relative to natural classes of queries that explicitly manipulate provenance.

#index 1661441
#* Trajectory databases: data models, uncertainty and complete query languages
#@ Bart Kuijpers;Walied Othman
#t 2007
#c 6
#% 292679
#% 417825
#% 500896
#% 527176
#% 527321
#% 562450
#! Moving objects produce trajectories. We describe a data model for trajectories and trajectory samples and an efficient way of modeling uncertainty via beads for trajectory samples. We study transformations for which important physical properties of trajectories, such as speed, are invariant. We also determine which transformations preserve beads. We give conceptually easy first-order complete query languages and computationally complete query languages for trajectory databases, which allow to talk directly about speed and beads. The queries expressible in these languages are invariant under speed- and bead-preserving transformations.

#index 1661442
#* Complexity of typechecking XML views of relational databases
#@ Piotr Wieczorek
#t 2007
#c 6
#% 309851
#% 384978
#% 575379
#% 630965
#% 643569
#% 826034
#% 942356
#! The typechecking problem for transformations of relational data into tree data is the following: given a TreeQL program P (called transformation), and a DTD d (called output type), decide whether for every database instance D the result of the transformation P of D is of a type consistent with d (see [2]). TreeQL programs with projection-free conjunctive queries and DTDs with arbitrary regular expressions are considered here. A non-elementary upper bound for the typechecking problem is given in [2] (although in a more general setting, where equality and negation in projection-free conjunctive queries and additional universal integrity constraints are allowed). In this paper we show that the typechecking problem is in coNEXPTIME. As an intermediate step we consider the following problem, which can be formulated in a language independent of XML notions. Given a set of triples of the form (ϕ, k, j), where ϕ is a projection-free conjunctive query and k, j are natural numbers, decide whether there exists a database D such that for each triple (ϕ, k, j) in the set, there exists a natural number α, such that there are exactly k + j * α tuples satisfying the query ϕ in D. Our main technical contribution consists of a NEXPTIME algorithm for the last problem.

#index 1661443
#* Exact XML type checking in polynomial time
#@ Sebastian Maneth;Thomas Perst;Helmut Seidl
#t 2007
#c 6
#% 175597
#% 241162
#% 287482
#% 427027
#% 493543
#% 572328
#% 630965
#% 736877
#% 740776
#% 743615
#% 790313
#% 801670
#% 809259
#% 826034
#% 994015
#% 1700119
#! Stay macro tree transducers (smtts) are an expressive formalism for reasoning about XSLT-like document transformations. Here, we consider the exact type checking problem for smtts. While the problem is decidable, the involved technique of inverse type inference is known to have exponential worst-case complexity (already for top-down transformations without parameters). We present a new adaptive type checking algorithm based on forward type inference through exact characterizations of output languages. The new algorithm correctly type-checks all call-by-value smtts. Given that the output type is specified by a deterministic automaton, the algorithm is polynomial-time whenever the transducer uses only few parameters and visits every input node only constantly often. Our new approach can also be generalized from smtts to stay macro forest transducers which additionally support concatenation as built-in output operation.

#index 1661444
#* Optimizing schema languages for XML: numerical constraints and interleaving
#@ Wouter Gelade;Wim Martens;Frank Neven
#t 2007
#c 6
#% 70235
#% 172925
#% 175597
#% 262724
#% 273922
#% 291299
#% 299944
#% 318331
#% 342898
#% 390964
#% 480822
#% 490905
#% 547133
#% 572328
#% 600179
#% 772031
#% 782853
#% 809236
#% 835398
#% 848763
#% 893098
#% 894435
#% 942356
#% 1016148
#% 1068354
#% 1388689
#% 1673671
#% 1718404
#! The presence of a schema offers many advantages in processing, translating, querying, and storage of XML data. Basic decision problems like equivalence, inclusion, and non-emptiness of intersection of schemas form the basic building blocks for schema optimization and integration, and algorithms for static analysis of transformations. It is thereby paramount to establish the exact complexity of these problems. Most common schema languages for XML can be adequately modeled by some kind of grammar with regular expressions at right-hand sides. In this paper, we observe that apart from the usual regular operators of union, concatenation and Kleene-star, schema languages also allow numerical occurrence constraints and interleaving operators. Although the expressiveness of these operators remain within the regular languages, their presence or absence has significant impact on the complexity of the basic decision problems. We present a complete overview of the complexity of the basic decision problems for DTDs, XSDs and Relax NG with regular expressions incorporating numerical occurrence constraints and interleaving. We also discuss chain regular expressions and the complexity of the schema simplification problem incorporating the new operators.

#index 1661445
#* Database query processing using finite cursor machines
#@ Martin Grohe;Yuri Gurevich;Dirk Leinders;Nicole Schweikardt;Jerzy Tyszkiewicz;Jan Van den Bussche
#t 2007
#c 6
#% 210169
#% 210761
#% 278835
#% 289425
#% 293720
#% 342387
#% 378388
#% 387508
#% 465061
#% 480296
#% 570879
#% 654476
#% 654477
#% 771325
#% 778122
#% 785130
#% 801685
#% 809240
#% 809253
#% 809255
#% 822573
#% 857282
#% 993437
#% 1016170
#% 1327758
#% 1676013
#! We introduce a new abstract model of database query processing, finite cursor machines, that incorporates certain data streaming aspects. The model describes quite faithfully what happens in so-called “one-pass” and “two-pass query processing”. Technically, the model is described in the framework of abstract state machines. Our main results are upper and lower bounds for processing relational algebra queries in this model, specifically, queries of the semijoin fragment of the relational algebra.

#index 1661446
#* Constant-memory validation of streaming XML documents against DTDs
#@ Luc Segoufin;Cristina Sirangelo
#t 2007
#c 6
#% 378392
#% 772032
#% 1669571
#! In this paper we investigate the problem of validating, with constant memory, streaming XML documents with respect to a DTD. Such constant memory validations can only be performed for some but not all DTDs. This paper gives a non trivial interesting step towards characterizing those DTDs for which a constant-memory on-line algorithm exists.

#index 1661447
#* Preferentially annotated regular path queries
#@ Gösta Grahne;Alex Thomo;William Wadge
#t 2007
#c 6
#% 291299
#% 378409
#% 731407
#% 810549
#% 1700139
#% 1728709
#! In this paper, we introduce preferential regular path queries. These are regular path queries whose symbols are annotated with preference weights for “scaling” up or down the intrinsic importance of matching a symbol against a (semistructured) database edge label. Annotated regular path queries are expressed syntactically as annotated regular expressions. We interpret these expressions in a uniform semiring framework, which allows different semantics specializations for the same syntactic annotations. For our preference queries, we study three important aspects: (1) (progressive) query answering (2) (certain) query answering in LAV data-integration systems, and (3) query containment and equivalence. In all of these, we obtain important positive results, which encourage the use of our preference framework for enhanced querying of semistructured databases.

#index 1661448
#* Combining incompleteness and ranking in tree queries
#@ Benny Kimelfeld;Yehoshua Sagiv
#t 2007
#c 6
#% 39702
#% 172933
#% 273703
#% 333845
#% 340914
#% 479803
#% 480819
#% 772025
#% 824703
#% 874894
#% 893149
#% 1015317
#% 1721851
#% 1733564
#! In many cases, users may want to consider incomplete answers to their queries. Often, however, there is an overwhelming number of such answers, even if subsumed answers are ignored and only maximal ones are considered. Therefore, it is important to rank answers according to their degree of incompleteness and, moreover, this ranking should be combined with other, conventional ranking techniques that are already in use (e.g., the relevance of answers to keywords). Query evaluation should take the ranking into account by computing answers incrementally, i.e., in ranked order. In particular, the evaluation process should generate the top-k answers efficiently. We show how a semantics for incomplete answers to tree queries can be combined with common ranking techniques. In our approach, answers are rewarded for relevancy and penalized for incompleteness, where the user specifies the appropriate quantum. An incremental algorithm for evaluating tree queries is given. This algorithm enumerates in ranked order with polynomial delay, under query-and-data complexity. Our results are couched in terms of a formal framework that captures a variety of data models (e.g., relational, semistructured and XML).

#index 1661449
#* Structural recursion on ordered trees and list-based complex objects
#@ Edward L. Robertson;Lawrence V. Saxton;Dirk Van Gucht;Stijn Vansummeren
#t 2007
#c 6
#% 61616
#% 115515
#% 142194
#% 145410
#% 154314
#% 164378
#% 189868
#% 206988
#% 228660
#% 245655
#% 272193
#% 275304
#% 384978
#% 435157
#% 464690
#% 464711
#% 464893
#% 544723
#% 571040
#% 737783
#% 809241
#% 1700124
#! XML query languages need to provide some mechanism to inspect and manipulate nodes at all levels of an input tree. In this paper we investigate the expressive power provided in this regard by structural recursion. We show that the combination of vertical recursion down a tree combined with horizontal recursion across a list of trees gives rise to a robust class of transformations: it captures the class of all primitive recursive queries. Since queries are expected to be computable in at most polynomial time for all practical purposes, we next identify a restriction of structural recursion that captures the polynomial time queries. Although this restriction is semantical in nature, and therefore undecidable, we provide an effective syntax. We also give corresponding results for list-based complex objects.

#index 1661450
#* Combining temporal logics for querying XML documents
#@ Marcelo Arenas;Pablo Barceló;Leonid Libkin
#t 2007
#c 6
#% 1729
#% 24752
#% 29440
#% 401124
#% 460560
#% 473125
#% 476334
#% 483543
#% 494344
#% 502885
#% 512876
#% 543009
#% 545382
#% 579727
#% 587321
#% 587363
#% 733593
#% 733595
#% 776459
#% 804841
#% 821563
#% 821610
#% 850728
#% 874898
#% 874910
#% 1015274
#% 1016139
#% 1673670
#% 1675930
#% 1700118
#! Close relationships between XML navigation and temporal logics have been discovered recently, in particular between logics LTL and CTL* and XPath navigation, and between the μ-calculus and navigation based on regular expressions. This opened up the possibility of bringing model-checking techniques into the field of XML, as documents are naturally represented as labeled transition systems. Most known results of this kind, however, are limited to Boolean or unary queries, which are not always sufficient for complex querying tasks. Here we present a technique for combining temporal logics to capture n-ary XML queries expressible in two yardstick languages: FO and MSO. We show that by adding simple terms to the language, and combining a temporal logic for words together with a temporal logic for unary tree queries, one obtains logics that select arbitrary tuples of elements, and can thus be used as building blocks in complex query languages. We present general results on the expressiveness of such temporal logics, study their model-checking properties, and relate them to some common XML querying tasks.

#index 1661451
#* Commutativity analysis in XML update languages
#@ Giorgio Ghelli;Kristoffer Rose;Jérôme Siméon
#t 2007
#c 6
#% 268764
#% 291189
#% 333979
#% 481128
#% 733593
#% 738953
#% 826029
#% 1015272
#% 1718234
#! A common approach to XML updates is to extend XQuery with update operations. This approach results in very expressive languages which are convenient for users but are difficult to reason about. Deciding whether two expressions can commute has numerous applications from view maintenance to rewriting-based optimizations. Unfortunately, commutativity is undecidable in most recent XML update languages. In this paper, we propose a conservative analysis for an expressive XML update language that can be used to determine whether two expressions commute. The approach relies on a form of path analysis that computes upper bounds for the nodes that are accessed or modified in a given update expression. Our main result is a commutativity theorem that can be used to identify commuting expressions.

#index 1661452
#* Containment of conjunctive queries over databases with null values
#@ Carles Farré;Werner Nutt;Ernest Teniente;Toni Urpí
#t 2007
#c 6
#% 36181
#% 129572
#% 137867
#% 140410
#% 198465
#% 230142
#% 248032
#% 289266
#% 378393
#% 464717
#% 464872
#% 465043
#% 465190
#% 481128
#% 599549
#% 1124990
#% 1273817
#! We study containment of conjunctive queries that are evaluated over databases that may contain tuples with null values. We assume the semantics of SQL for single block queries with a SELECT DISTINCT clause. This problem (“null containment” for short) is different from containment over databases without null values and sometimes more difficult. We show that null-containment for boolean conjunctive queries is NP-complete while it is $\mathit\Pi^{\rm P}_{2}$-complete for queries with distinguished variables. However, if no relation symbol is allowed to appear more than twice, then null-containment is polynomial, as it is for databases without nulls. If we add a unary test predicate IS NULL, as it is available in SQL, then containment becomes $\mathit\Pi^{\rm P}_{2}$-hard for boolean queries, while it remains in $\mathit\Pi^{\rm P}_{2}$ for arbitrary queries.

#index 1661453
#* Some algorithmic improvements for the containment problem of conjunctive queries with negation
#@ Michel Leclère;Marie-Laure Mugnier
#t 2007
#c 6
#% 368248
#% 464717
#% 465050
#% 481128
#% 572311
#% 599549
#% 943546
#! Query containment is a fundamental problem of databases. Given two queries q1 and q2, it asks whether the set of answers to q1 is included in the set of answers to q2 for any database. In this paper, we investigate this problem for conjunctive queries with negated subgoals. We use graph homomorphism as the core notion, which leads us to extend the results presented in [Ull97] and [WL03]. First, we exhibit sufficient (but not necessary) conditions for query containment based on special subgraphs of q2, which generalize that proposed in [WL03]. As a corollary, we obtain a case where the time complexity of the problem decreases. From a practical viewpoint, these properties can be exploited in algorithms, as shown in the paper. Second, we propose an algorithm based on the exploration of a space of graphs, which improves existing algorithms.

#index 1700117
#* Proceedings of the 10th international conference on Database Theory
#@ Thomas Eiter;Leonid Libkin
#t 2005
#c 6

#index 1700118
#* Model checking for database theoreticians
#@ Moshe Y. Vardi
#t 2005
#c 6
#% 2991
#% 3645
#% 9241
#% 34773
#% 64414
#% 65904
#% 109626
#% 136347
#% 145228
#% 158068
#% 166762
#% 172932
#% 175122
#% 191573
#% 196419
#% 215675
#% 218112
#% 234819
#% 239406
#% 285967
#% 291299
#% 297770
#% 298915
#% 309909
#% 321054
#% 336018
#% 336201
#% 384978
#% 399032
#% 403195
#% 427027
#% 452266
#% 475381
#% 479031
#% 489894
#% 494344
#% 502091
#% 502751
#% 502889
#% 509477
#% 509675
#% 510287
#% 511392
#% 512032
#% 516652
#% 529740
#% 541448
#% 541929
#% 542094
#% 542116
#% 543673
#% 545382
#% 576091
#% 587365
#% 587479
#% 587583
#% 593767
#% 630964
#% 801675
#% 836134
#% 1068329
#% 1068595
#% 1080888
#% 1388770
#! Algorithmic verification is one of the most successful applications of automated reasoning in computer science. In algorithmic verification one uses algorithmic techniques to establish the correctness of the system under verification with respect to a given property. Model checking is an algorithmic-verification technique that is based on a small number of key ideas, tying together graph theory, automata theory, and logic. In this self-contained talk I will describe how this “holy trinity” gave rise to algorithmic-verification tools, and discuss its applicability to database verification.

#index 1700119
#* The design space of type checkers for XML transformation languages
#@ Anders Møller;Michael I. Schwartzbach
#t 2005
#c 6
#% 285169
#% 314266
#% 342438
#% 349956
#% 545382
#% 572328
#% 630965
#% 722731
#% 722732
#% 723587
#% 736877
#% 737663
#% 740776
#% 799999
#% 904699
#% 1389603
#% 1389619
#% 1704129
#! We survey work on statically type checking XML transformations, covering a wide range of notations and ambitions. The concept of type may vary from idealizations of DTD to full-blown XML Schema or even more expressive formalisms. The notion of transformation may vary from clean and simple transductions to domain-specific languages or integration of XML in general-purpose programming languages. Type annotations can be either explicit or implicit, and type checking ranges from exact decidability to pragmatic approximations. We characterize and evaluate existing tools in this design space, including a recent result of the authors providing practical type checking of full unannotated XSLT 1.0 stylesheets given general DTDs that describe the input and output languages.

#index 1700120
#* Semantics of data streams and operators
#@ David Maier;Jin Li;Peter Tucker;Kristin Tufte;Vassilis Papadimos
#t 2005
#c 6
#% 201897
#% 378388
#% 578391
#% 726621
#% 765404
#% 771230
#% 783783
#% 801694
#% 853011
#% 979303
#% 993949
#% 1015279
#% 1015296
#% 1016157
#% 1016170
#! What does a data stream mean? Much of the extensive work on query operators and query processing for data streams has proceeded without the benefit of an answer to this question. While such imprecision may be tolerable when dealing with simple cases, such as flat data, guaranteed physical order and element-wise operations, it can lead to ambiguities when dealing with nested data, disordered streams and windowed operators. We propose reconstitution functions to make the denotation and representation of data streams more precise, and use these functions to investigate the connection between monotonicity and non-blocking behavior of stream operators. We also touch on a reconstitution function for XML data. Other aspects of data stream semantics we consider are the use of punctuation to delineate finite subsets of a stream, adequacy of descriptions of stream disorder, and the formal specification of windowed operators.

#index 1700121
#* Conjunctive query evaluation by search tree revisited
#@ Albert Atserias
#t 2005
#c 6
#% 263371
#% 292675
#% 317733
#% 321058
#% 414951
#% 464727
#% 535150
#% 593867
#% 599549
#% 656686
#% 993437
#! The most natural and perhaps most frequently used method for testing membership of an individual tuple into a conjunctive query is based on search trees. We investigate the question of evaluating conjunctive queries with a time-bound guarantee that is measured as a function of the size of the minimal search tree. We provide an algorithm that, given a database D, a conjunctive query Q, and a tuple t, tests whether Q(t) holds in D in time bounded by (sn)O(logk) (sn)O(loglog n), where n is the size of the domain of the database, k is the number of bound variables of the conjunctive query, and s is the size of the optimal search tree. In many cases of interest, this bound is significantly smaller than the nO(k) bound provided by the naive search-tree method. Moreover, our algorithm has the advantage of guaranteeing the bound for any given conjunctive query. In particular, it guarantees the bound for queries that admit an equivalent form that is much easier to evaluate, even when finding such a form is an NP-hard task. Concrete examples include the conjunctive queries that can be non-trivially folded into a conjunctive query of bounded size or bounded treewidth. All our results translate to the context of constraint-satisfaction problems via the well-publicized correspondence between both frameworks.

#index 1700122
#* Which XML schemas admit 1-pass preorder typing?
#@ Wim Martens;Frank Neven;Thomas Schwentick
#t 2005
#c 6
#% 70235
#% 262724
#% 299944
#% 378392
#% 465054
#% 465059
#% 545382
#% 562461
#% 572328
#% 600179
#% 765274
#% 801670
#% 835398
#% 835409
#% 848763
#% 976789
#! It is shown that the class of regular tree languages admitting one-pass preorder typing is exactly the class defined by restrained competition tree grammars introduced by Murata et al. [14]. In a streaming context, the former is the largest class of XSDs where every element in a document can be typed when its opening tag is met. The main technical machinery consists of semantical characterizations of restrained competition grammars and their subclasses. In particular, they can be characterized in terms of the context of nodes, closure properties, allowed patterns and guarded DTDs. It is further shown that deciding whether a schema is restrained competition is tractable. Deciding whether a schema is equivalent to a restrained competition tree grammar, or one of its subclasses, is much more difficult: it is complete for EXPTIME. We show that our semantical characterizations allow for easy optimization and minimization algorithms. Finally, we relate the notion of one-pass preorder typing to the existing XML Schema standard.

#index 1700123
#* The pipelined set cover problem
#@ Kamesh Munagala;Shivnath Babu;Rajeev Motwani;Jennifer Widom
#t 2005
#c 6
#% 203146
#% 217812
#% 248821
#% 249985
#% 256685
#% 285924
#% 287461
#% 288449
#% 300179
#% 347263
#% 378388
#% 378397
#% 453556
#% 464044
#% 479938
#% 480803
#% 480944
#% 654497
#% 765435
#% 781735
#% 993959
#% 1373609
#! A classical problem in query optimization is to find the optimal ordering of a set of possibly correlated selections. We provide an abstraction of this problem as a generalization of set cover called pipelined set cover, where the sets are applied sequentially to the elements to be covered and the elements covered at each stage are discarded. We show that several natural heuristics for this NP-hard problem, such as the greedy set-cover heuristic and a local-search heuristic, can be analyzed using a linear-programming framework. These heuristics lead to efficient algorithms for pipelined set cover that can be applied to order possibly correlated selections in conventional database systems as well as data-stream processing systems. We use our linear-programming framework to show that the greedy and local-search algorithms are 4-approximations for pipelined set cover. We extend our analysis to minimize the lp-norm of the costs paid by the sets, where p ≥ 2 is an integer, to examine the improvement in performance when the total cost has increasing contribution from initial sets in the pipeline. Finally, we consider the online version of pipelined set cover and present a competitive algorithm with a logarithmic performance guarantee. Our analysis framework may be applicable to other problems in query optimization where it is important to account for correlations.

#index 1700124
#* Well-definedness and semantic type-checking in the nested relational calculus and XQuery
#@ Jan Van den Bussche;Dirk Van Gucht;Stijn Vansummeren
#t 2005
#c 6
#% 189868
#% 237181
#% 299942
#% 384978
#% 465054
#% 562461
#% 571038
#% 575379
#% 643569
#% 700905
#% 801670
#! Two natural decision problems regarding the XML query language XQuery are well-definedness and semantic type-checking. We study these problems in the setting of a relational fragment of XQuery. We show that well-definedness and semantic type-checking are undecidable, even in the positive-existential case. Nevertheless, for a “pure” variant of XQuery, in which no identification is made between an item and the singleton containing that item, the problems become decidable. We also consider the analogous problems in the setting of the nested relational calculus.

#index 1700125
#* First order paths in ordered trees
#@ Maarten Marx
#t 2005
#c 6
#% 263136
#% 357150
#% 465065
#% 544561
#% 587436
#% 801669
#% 801686
#% 993939
#! We give two sufficient conditions on XPath like languages for having first order expressivity, meaning that every first order definable set of paths in an ordered node-labeled tree is definable in that XPath language. They are phrased in terms of expansions of navigational (sometimes called “Core”) XPath. Adding either complementation, or the more elegant conditional paths is sufficient. A conditional path is an axis relation of the form (one_step_axis::n[F])+, denoting the transitive closure of the relation expressed by one_step_axis::n[F]. As neither is expressible in navigational XPath we also give characterizations in terms of first order logic of the answer sets and the sets of paths navigational XPath can define. The first in terms of a suitable two variable fragment, the second in terms of unions of conjunctive queries.

#index 1700126
#* An abstract framework for generating maximal answers to queries
#@ Sara Cohen;Yehoshua Sagiv
#t 2005
#c 6
#% 39702
#% 172933
#% 191611
#% 213983
#% 273703
#% 289321
#% 333845
#% 333862
#% 397373
#% 408396
#% 458861
#% 465068
#% 527112
#% 576099
#% 765408
#% 993437
#! A framework for modeling query semantics as graph properties is presented. In this framework, a single definition of a query automatically gives rise to several semantics for evaluating that query under varying degrees of incomplete information. For example, defining natural joins automatically gives rise to full disjunctions. Two of the proposed semantics have incremental-polynomial-time query-evaluation algorithms for all types of queries that can be defined in this framework. Thus, the proposed framework generalizes previous definitions of semantics for incomplete information and improves previous complexity results for query evaluation.

#index 1700127
#* Optimal distributed declustering using replication
#@ Keith B. Frikken
#t 2005
#c 6
#% 43179
#% 164363
#% 286962
#% 299983
#% 303086
#% 325289
#% 328142
#% 378390
#% 462233
#% 462779
#% 464718
#% 465027
#% 564098
#% 565263
#% 632069
#% 730011
#! A common technique for improving performance for database query retrieval is to decluster the database among multiple disks so that retrievals can be parallelized. In this paper we focus on answering range queries over a multidimensional database, where each of its dimensions are divided uniformly to obtain tiles which are placed on different disks; there has been a significant amount of research for determining how to place the records on disks to minimize the retrieval time. Recently, the idea of using replication (i.e., placing records on more than one disk) to improve performance has been introduced. When using replication there are two goals: i) to minimize the retrieval time and ii) to minimize the scheduling overhead it takes to determine which disk obtains a specific record when processing a query. The previously known replicated declustering schemes with low retrieval times are randomized; and one of the primary advantages of randomized schemes is that they balance the load evenly among the disks for large queries with high probability. In this paper we introduce a new class of replicated placement schemes called the shift schemes that are: i) deterministic, ii) have retrieval performance that is comparable to the randomized schemes, iii) have a strictly optimal retrieval time for all large queries, and iv) have a more efficient query scheduling algorithm than those for the randomized placements. Furthermore, we display experimental results that suggest that the shift schemes have stronger average performance (in terms of retrieval times) than the randomized schemes.

#index 1700128
#* When is nearest neighbors indexable?
#@ Uri Shaft;Raghu Ramakrishnan
#t 2005
#c 6
#% 86950
#% 227939
#% 237204
#% 411694
#% 427199
#% 435141
#% 464195
#% 464888
#% 479973
#% 480093
#% 481956
#% 711097
#% 714747
#! In this paper, we consider whether traditional index structures are effective in processing unstable nearest neighbors workloads. It is known that under broad conditions, nearest neighbors workloads become unstable–distances between data points become indistinguishable from each other. We complement this earlier result by showing that if the workload for your application is unstable, you are not likely to be able to index it efficiently using (almost all known) multidimensional index structures. For a broad class of data distributions, we prove that these index structures will do no better than a linear scan of the data as dimensionality increases. Our result has implications for how experiments should be designed on index structures such as R-Trees, X-Trees and SR-Trees: Simply put, experiments trying to establish that these index structures scale with dimensionality should be designed to establish cross-over points, rather than to show that the methods scale to an arbitrary number of dimensions. In other words, experiments should seek to establish the dimensionality of the dataset at which the proposed index structure deteriorates to linear scan, for each data distribution of interest; that linear scan will eventually dominate is a given. An important problem is to analytically characterize the rate at which index structures degrade with increasing dimensionality, because the dimensionality of a real data set may well be in the range that a particular method can handle. The results in this paper can be regarded as a step towards solving this problem. Although we do not characterize the rate at which a structure degrades, our techniques allow us to reason directly about a broad class of index structures, rather than the geometry of the nearest neighbors problem, in contrast to earlier work.

#index 1700129
#* Nonmaterialized motion information in transport networks
#@ Hu Cao;Ouri Wolfson
#t 2005
#c 6
#% 93666
#% 333953
#% 480306
#% 482123
#% 580300
#% 720076
#% 729853
#% 765187
#! The traditional way of representing motion in 3D space-time uses a trajectory, i.e. a sequence of (x,y,t) points. Such a trajectory may be produced by periodic sampling of a Global Positioning System (GPS) receiver. The are two problems with this representation of motion. First, imprecision due to errors (e.g. GPS receivers often produce off-the-road locations), and second, space complexity due to a large number of samplings. We examine an alternative representation, called a nonmaterialized trajectory, which addresses both problems by taking advantage of the a priori knowledge that the motion occurs on a transport network.

#index 1700130
#* Algorithms for the database layout problem
#@ Gagan Aggarwal;Tomás Feder;Rajeev Motwani;Rina Panigrahy;An Zhu
#t 2005
#c 6
#% 43171
#% 261208
#% 297941
#% 300164
#% 340595
#% 566573
#% 571066
#! We present a formal analysis of the database layout problem, i.e., the problem of determining how database objects such as tables and indexes are assigned to disk drives. Optimizing this layout has a direct impact on the I/O performance of the entire system. The traditional approach of striping each object across all available disk drives is aimed at optimizing I/O parallelism; however, it is suboptimal when queries co-access two or more database objects, e.g., during a merge join of two tables, due to the increase in random disk seeks. We adopt an existing model, which takes into account both the benefit of I/O parallelism and the overhead due to random disk accesses, in the context of a query workload which includes co-access of database objects. The resulting optimization problem is intractable in general and we employ techniques from approximation algorithms to present provable performance guarantees. We show that while optimally exploiting I/O parallelism alone suggests uniformly striping data objects (even for heterogeneous files and disks), optimizing random disk access alone would assign each data object to a single disk drive. This confirms the intuition that the two effects are in tension with each other. We provide approximation algorithms in an attempt to optimize the trade-off between the two effects. We show that our algorithm achieves the best possible approximation ratio.

#index 1700131
#* Approximately dominating representatives
#@ Vladlen Koltun;Christos H. Papadimitriou
#t 2005
#c 6
#% 86755
#% 248010
#% 275929
#% 278831
#% 289148
#% 303893
#% 333847
#% 333854
#% 480671
#% 593993
#% 866783
#! We propose and investigate from the algorithmic standpoint a novel form of fuzzy query called approximately dominating representatives or ADRs. The ADRs of a multidimensional point set consist of a few points guaranteed to contain an approximate optimum of any monotone Lipschitz continuous combining function of the dimensions. ADRs can be computed by appropriately post-processing Pareto, or “skyline,” queries [14,1]. We show that the problem of minimizing the number of points returned, for a user-specified desired approximation, can be solved in polynomial time in two dimensions; for three and more it is NP-hard but has a polynomial-time logarithmic approximation. Finally, we present a polynomial-time, constant factor approximation algorithm for three dimensions.

#index 1700132
#* On horn axiomatizations for sequential data
#@ José L. Balcázar;Gemma Casas-Garriga
#t 2005
#c 6
#% 181339
#% 204396
#% 310494
#% 384416
#% 463903
#% 745515
#! We propose a notion of deterministic association rules for ordered data. We prove that our proposed rules can be formally justified by a purely logical characterization, namely, a natural notion of empirical Horn approximation for ordered data which involves background Horn conditions; these ensure the consistency of the propositional theory obtained with the ordered context. The main proof resorts to a concept lattice model in the framework of Formal Concept Analysis, but adapted to ordered contexts. We also discuss a general method to mine these rules that can be easily incorporated into any algorithm for mining closed sequences, of which there are already some in the literature.

#index 1700133
#* Privacy in database publishing
#@ Alin Deutsch;Yannis Papakonstantinou
#t 2005
#c 6
#% 9261
#% 94459
#% 378410
#% 384978
#% 398263
#% 464717
#% 464867
#% 465053
#% 465066
#% 576111
#% 765447
#% 765449
#% 826031
#% 1015329
#! We formulate and study a privacy guarantee to data owners, who share information with clients by publishing views of a proprietary database. The owner identi.es the sensitive proprietary data using a secret query against the proprietary database. Given an extra view, the privacy guarantee ensures that potential attackers will not learn any information about the secret that could not already be obtained from the existing views. We de.ne “learning” as the modi.cation of the attacker's a-priori probability distribution on the set of possible secrets. We assume arbitrary a-priori distributions (including distributions that correlate the existence of particular tuples) and solve the problem when secret and views are expressed as unions of conjunctive queries with non-equalities, under integrity constraints. We consider guarantees (a) for given view extents (b) for given domain of the secret and (c) independent of the domain and extents.

#index 1700134
#* Anonymizing tables
#@ Gagan Aggarwal;Tomás Feder;Krishnaram Kenthapadi;Rajeev Motwani;Rina Panigrahy;Dilys Thomas;An Zhu
#t 2005
#c 6
#% 43022
#% 159516
#% 248030
#% 300184
#% 333876
#% 512307
#% 576110
#% 576111
#% 576761
#% 801690
#! We consider the problem of releasing tables from a relational database containing personal records, while ensuring individual privacy and maintaining data integrity to the extent possible. One of the techniques proposed in the literature is k-anonymization. A release is considered k-anonymous if the information for each person contained in the release cannot be distinguished from at least k–1 other persons whose information also appears in the release. In the k-Anonymityproblem the objective is to minimally suppress cells in the table so as to ensure that the released version is k-anonymous. We show that the k-Anonymity problem is NP-hard even when the attribute values are ternary. On the positive side, we provide an O(k)-approximation algorithm for the problem. This improves upon the previous best-known O(klog k)-approximation. We also give improved positive results for the interesting cases with specific values of k — in particular, we give a 1.5-approximation algorithm for the special case of 2-Anonymity, and a 2-approximation algorithm for 3-Anonymity.

#index 1700135
#* Authorization views and conditional query containment
#@ Zheng Zhang;Alberto O. Mendelzon
#t 2005
#c 6
#% 36181
#% 123118
#% 198465
#% 248038
#% 289266
#% 378409
#% 378410
#% 462501
#% 464056
#% 464867
#% 465053
#% 481923
#% 488476
#% 599549
#% 630963
#% 765447
#! A recent proposal for database access control consists of defining “authorization views” that specify the accessible data, and declaring a query valid if it can be completely rewritten using the views. Unlike traditional work in query rewriting using views, the rewritten query needs to be equivalent to the original query only over the set of database states that agree with a given set of materializations for the authorization views. With this motivation, we study conditional query containment, i.e. , containment over states that agree on a set of materialized views. We give an algorithm to test conditional containment of conjunctive queries with respect to a set of materialized conjunctive views. We show the problem is ${\it \Pi}^{p}_{2}$-complete. Based on the algorithm, we give a test for a query to be conditionally authorized given a set of materialized authorization views.

#index 1700136
#* PTIME queries revisited
#@ Alan Nash;Jeff Remmel;Victor Vianu
#t 2005
#c 6
#% 6787
#% 197949
#% 384978
#% 404772
#% 587453
#% 598376
#! The existence of a language expressing precisely the PTIME queries on arbitrary structures remains the central open problem in the theory of database query languages. As it turns out, two variants of this question have been formulated. Surprisingly, despite the importance of the problem, the relationship between these variants has not been systematically explored. A first contribution of the present paper is to revisit the basic definitions and clarify the connection between these two variants. We then investigate two relaxations to the original problem that appear as tempting alternatives in the absence of a language for the PTIME queries. The first consists in trying to express the PTIME queries using a richer language that can also express queries beyond PTIME, but for which there exists a query processor evaluating all PTIME queries in PTIME. The second approach, studied by many researchers, is to focus on PTIME properties on restricted sets of graphs. Our results are mostly negative, and point out limitations to both approaches. Finally, we turn to a natural class of languages that we call finitely generated, whose syntax is obtained by applying a fixed set of constructors to a given set of building blocks. We identify a broad class of such languages that cannot express all the PTIME queries.

#index 1700137
#* Asymptotic conditional probabilities for conjunctive queries
#@ Nilesh Dalvi;Gerome Miklau;Dan Suciu
#t 2005
#c 6
#% 215225
#% 572311
#% 765449
#% 1016201
#% 1543751
#! We study the asymptotic probabilities of conjunctive queries on random graphs.We consider a probabilistic model where the expected graph size remains constant independent of the number of vertices. While it has been known that a convergence law holds for conjunctive queries under this model, we focus on the calculation of conditional probabilities. This has direct applications to database problems like query-view security, i.e. evaluating the probability of a sensitive query given the knowledge of a set of published views. We prove that a convergence law holds for conditional probabilities of conjunctive queries and we give a procedure for calculating the conditional probabilities.

#index 1700138
#* Magic sets and their application to data integration
#@ Wolfgang Faber;Gianluigi Greco;Nicola Leone
#t 2005
#c 6
#% 11797
#% 86943
#% 93766
#% 94456
#% 101623
#% 103704
#% 125134
#% 164365
#% 171033
#% 176471
#% 196695
#% 210207
#% 235018
#% 268771
#% 342829
#% 452865
#% 464915
#% 576101
#% 576116
#% 749088
#% 835980
#% 880394
#% 1279213
#% 1279214
#% 1347336
#! We propose a generalization of the well-known Magic Sets technique to Datalog¬ programs with (possibly unstratified) negation under stable model semantics. Our technique produces a new program whose evaluation is generally more efficient (due to a smaller instantiation), while preserving soundness under cautious reasoning. Importantly, if the original program is consistent, then full query-equivalence is guaranteed for both brave and cautious reasoning, which turn out to be sound and complete. In order to formally prove the correctness of our Magic Sets transformation, we introduce a novel notion of modularity for Datalog¬ under the stable model semantics, which is relevant per se. We prove that a module can be evaluated independently from the rest of the program, while preserving soundness under cautious reasoning. For consistent programs, both soundness and completeness are guaranteed for brave reasoning and cautious reasoning as well. Our Magic Sets optimization constitutes an effective method for enhancing the performance of data-integration systems in which query-answering is carried out by means of cautious reasoning over Datalog¬ programs. In fact, preliminary results of experiments in the EU project INFOMIX, show that Magic Sets are fundamental for the scalability of the system.

#index 1700139
#* View-based query processing: on the relationship between rewriting, answering and losslessness
#@ Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini;Moshe Y. Vardi
#t 2005
#c 6
#% 64284
#% 198465
#% 210214
#% 237190
#% 237191
#% 248038
#% 248819
#% 261741
#% 268708
#% 281149
#% 291299
#% 299945
#% 299967
#% 378407
#% 378410
#% 404772
#% 443460
#% 464717
#% 464720
#% 464867
#% 464883
#% 464894
#% 480670
#% 572311
#% 576097
#% 587566
#% 632039
#% 731485
#! As a result of the extensive research in view-based query processing, three notions have been identi.ed as fundamental, namely rewriting, answering, and losslessness. Answering amounts to computing the tuples satisfying the query in all databases consistent with the views. Rewriting consists in first reformulating the query in terms of the views and then evaluating the rewriting over the view extensions. Losslessness holds if we can answer the query by solely relying on the content of the views. While the mutual relationship between these three notions is easy to identify in the case of conjunctive queries, the terrain of notions gets considerably more complicated going beyond such a query class. In this paper, we revisit the notions of answering, rewriting, and losslessness and clarify their relationship in the setting of semistructured databases, and in particular for the basic query class in this setting, i.e., two-way regular path queries. Our .rst result is a clean explanation of the relationship between answering and rewriting, in which we characterize rewriting as a “linear approximations” of query answering. We show that applying this linear approximation to the constraint-satisfaction framework yields an elegant automata-theoretic approach to query rewriting. As for losslessness, we show that there are indeed two distinct interpretations for this

#index 1700140
#* First-order query rewriting for inconsistent databases
#@ Ariel D. Fuxman;Renée J. Miller
#t 2005
#c 6
#% 196410
#% 248038
#% 264858
#% 273687
#% 288946
#% 465057
#% 572311
#% 576116
#% 1279213
#% 1279214
#% 1347336
#! We consider the problem of retrieving consistent answers over databases that might be inconsistent with respect to some given integrity constraints. In particular, we concentrate on sets of constraints that consist of key dependencies. Most of the existing work has focused on identifying intractable cases of this problem. In contrast, in this paper we give an algorithm that computes the consistent answers for a large and practical class of conjunctive queries. Given a query q, the algorithm returns a first-order query Q (called a query rewriting) such that for every (potentially inconsistent) database I, the consistent answers for q can be obtained by evaluating Q directly on I.

#index 1700141
#* Rewriting queries using views with access patterns under integrity constraints
#@ Alin Deutsch;Bertram Ludäscher;Alan Nash
#t 2005
#c 6
#% 198466
#% 237190
#% 273912
#% 277327
#% 289266
#% 299968
#% 384978
#% 398263
#% 464892
#% 465053
#% 465057
#% 481923
#% 572311
#% 599549
#% 726626
#% 752773
#% 801698
#% 938575
#% 1015271
#! We study the problem of rewriting queries using views in the presence of access patterns, integrity constraints, disjunction, and negation. We provide asymptotically optimal algorithms for finding minimal containing and maximal contained rewritings and for deciding whether an exact rewriting exists. We show that rewriting queries using views in this case reduces (a) to rewriting queries with access patterns and constraints without views and also (b) to rewriting queries using views under constraints without access patterns. We show how to solve (a) directly and how to reduce (b) to rewriting queries under constraints only (semantic optimization). These reductions provide two separate routes to a unified solution for all three problems, based on an extension of the relational chase theory to queries and constraints with disjunction and negation. We also handle equality and arithmetic comparisons.

#index 1700142
#* Optimal workload-based weighted wavelet synopses
#@ Yossi Matias;Daniel Urieli
#t 2005
#c 6
#% 248820
#% 248822
#% 257637
#% 259995
#% 273901
#% 273902
#% 293714
#% 333955
#% 397389
#% 465162
#% 480306
#% 480471
#% 654460
#% 801684
#% 1188997
#! In recent years wavelets were shown to be effective data synopses. We are concerned with the problem of finding efficiently wavelet synopses for massive data sets, in situations where information about query workload is available. We present linear time, I/O optimal algorithms for building optimal workload-based wavelet synopses for point queries. The synopses are based on a novel construction of weighted inner-products and use weighted wavelets that are adapted to those products. The synopses are optimal in the sense that the subset of retained coefficients is the best possible for the bases in use with respect to either the mean-squared absolute or relative errors. For the latter, this is the first optimal wavelet synopsis even for the regular, non-workload-based case. Experimental results demonstrate the advantage obtained by the new optimal wavelet synopses, as well as the robustness of the synopses to deviations in the actual query workload.

#index 1700143
#* Selecting and using views to compute aggregate queries
#@ Foto Afrati;Rada Chirkova
#t 2005
#c 6
#% 27056
#% 137867
#% 199537
#% 210182
#% 247577
#% 248034
#% 273696
#% 333964
#% 420053
#% 430773
#% 451768
#% 462204
#% 464717
#% 465010
#% 480158
#% 481604
#% 481951
#% 482081
#% 484859
#% 630967
#% 1609990
#! We consider a workload of aggregate queries and investigate the problem of selecting views that (1) provide equivalent rewritings for all queries, and (2) are optimal, in that the cost of evaluating the query workload is minimized. We consider conjunctive views and rewritings, with or without aggregation; in each rewriting, only one view contributes to computing the aggregated query output. We look at query rewriting using existing views and at view selection. In the query-rewriting problem, we give su.cient and necessary conditions for a rewriting to exist. For view selection, we prove complexity results. Finally, we give algorithms for obtaining rewritings and selecting views.

#index 1700144
#* Efficient computation of frequent and top-k elements in data streams
#@ Ahmed Metwally;Divyakant Agrawal;Amr El Abbadi
#t 2005
#c 6
#% 225914
#% 326337
#% 492912
#% 548479
#% 569754
#% 576119
#% 642409
#% 730046
#% 993960
#% 1015293
#! We propose an integrated approach for solving both problems of finding the most popular k elements, and finding frequent elements in a data stream. Our technique is efficient and exact if the alphabet under consideration is small. In the more practical large alphabet case, our solution is space efficient and reports both top-k and frequent elements with tight guarantees on errors. For general data distributions, our top-k algorithm can return a set of k′ elements, where k′ ≈ k, which are guaranteed to be the top-k' elements; and we use minimal space for calculating frequent elements. For realistic Zipfian data, our space requirement for the frequent elements problem decreases dramatically with the parameter of the distribution; and for top-k queries, we ensure that only the top-k elements, in the correct order, are reported. Our experiments show significant space reductions with no loss in accuracy.

#index 1818405
#* Proceedings of the 15th International Conference on Database Theory
#@ Alin Deutsch
#t 2012
#c 6
#! The papers in this volume were presented at the 15th International Conference on Database Theory (ICDT'12), held in Berlin, Germany, March 26-28, 2012. Starting in 2009, ICDT was held jointly with the EDBT (Extending Database Technology) conference. EDBT took place on March 27-29, 2012. The joint conference also included a series of affiliated workshops, held on March 30, 2012. In response to the Call for Papers, 60 submissions were received by the submission deadline of July 29, 2011. All were submitted electronically through the EasyChair conference management tool. EasyChair was also used for the virtual Program Committee meeting, whose deliberations where held exclusively electronically. The Program Committee selected 22 papers for presentation. Among them, the Program Committee selected the paper "Learning Schema Mappings", by Balder Ten Cate, Victor Dalmau and Phokion Kolaitis for the ICDT Best Paper Award, and the paper "Validating XML documents in the Streaming Model with External Memory", by Christian Konrad and Frederic Magniez, for the ICDT Best Newcomer Paper Award. In addition, there were four ICDT/EDBT keynote speakers: Michael Carey (UC Irvine), Wenfei Fan (University of Edinburgh), Erich Graedel (RWTH Aachen University) and Alon Halevy (Google).

#index 1818406
#* Graph pattern matching revised for social network analysis
#@ Wenfei Fan
#t 2012
#c 6
#% 194127
#% 197751
#% 205419
#% 219211
#% 288990
#% 300176
#% 369768
#% 378391
#% 378409
#% 408396
#% 464883
#% 593696
#% 641958
#% 654452
#% 656242
#% 722530
#% 754058
#% 835906
#% 864462
#% 893106
#% 960276
#% 976785
#% 1023420
#% 1047390
#% 1111124
#% 1127600
#% 1189225
#% 1214643
#% 1217208
#% 1328183
#% 1372657
#% 1407271
#% 1413162
#% 1424586
#% 1426443
#% 1451193
#% 1506210
#% 1523818
#% 1523898
#% 1560413
#% 1581923
#% 1594585
#% 1668636
#% 1818411
#! Graph pattern matching is fundamental to social network analysis. Traditional techniques are subgraph isomorphism and graph simulation. However, these notions often impose too strong a topological constraint on graphs to find meaningful matches. Worse still, graphs in the real world are typically large, with millions of nodes and billions of edges. It is often prohibitively expensive to compute matches in such graphs. With these comes the need for revising the notions of graph pattern matching and for developing techniques of querying large graphs, to effectively and efficiently identify social communities or groups. This paper aims to provide an overview of recent advances in the study of graph pattern matching in social networks. (1) We present several revisions of the traditional notions of graph pattern matching to find sensible matches in social networks. (2) We provide boundedness analyses of incremental graph pattern matching, in response to frequent updates to social networks. (3) To cope with large real-life graphs, we propose a framework of query preserving graph compression, which retains only information necessary for answering a certain class of queries of users' choice. (4) We also address pattern matching in distributed graphs, and in particular, advocate the use of partial evaluation techniques. Finally, we identify directions for future research.

#index 1818407
#* On the data complexity of consistent query answering
#@ Balder ten Cate;Gaëlle Fontaine;Phokion G. Kolaitis
#t 2012
#c 6
#% 273687
#% 384978
#% 404719
#% 465053
#% 576116
#% 786665
#% 806215
#% 826032
#% 833132
#% 874879
#% 879041
#% 912245
#% 949372
#% 976995
#% 1039062
#% 1039063
#% 1179998
#% 1280391
#% 1347336
#% 1424605
#% 1426459
#% 1464322
#% 1552660
#% 1661426
#% 1679459
#! The framework of database repairs is a principled approach to managing inconsistency in databases. In particular, the consistent answers of a query on an inconsistent database provide sound semantics and the guarantee that the values obtained are those returned by the query on every repair of the given inconsistent database. In this paper, we carry out a systematic investigation of the data complexity of the consistent answers of conjunctive queries for set-based repairs and with respect to classes of constraints that, in recent years, have been extensively studied in the context of data exchange and data integration. Our results, which range from polynomial-time computability to undecidability, complement or improve on earlier work, and provide a fairly comprehensive picture of the data complexity of consistent query answering. We also address the problem of finding a "representative" or "useful" repair of an inconsistent database. To this effect, we introduce the notion of a universal repair, as well as relaxations of it, and then apply it to the investigation of the data complexity of consistent query answering.

#index 1818408
#* Validating XML documents in the streaming model with external memory
#@ Christian Konrad;Fr&#233/d&#233/ric Magniez
#t 2012
#c 6
#% 238182
#% 278835
#% 378392
#% 427027
#% 771386
#% 809255
#% 847113
#% 874901
#% 958231
#% 963308
#% 1141470
#% 1181328
#% 1426276
#% 1661446
#% 1718398
#! We study the problem of validating XML documents of size N against general DTDs in the context of streaming algorithms. The starting point of this work is a well-known space lower bound. There are XML documents and DTDs for which p-pass streaming algorithms require Ω(N/p) space. We show that when allowing access to external memory, there is a deterministic streaming algorithm that solves this problem with memory space O(log2 N), a constant number of auxiliary read/write streams, and O(log N) total number of passes on the XML document and auxiliary streams. An important intermediate step of this algorithm is the computation of the First-Child-Next-Sibling (FCNS) encoding of the initial XML document in a streaming fashion. We study this problem independently, and we also provide memory efficient streaming algorithms for decoding an XML document given in its FCNS encoding. Furthermore, validating XML documents encoding binary trees in the usual streaming model without external memory can be done with sublinear memory. There is a one-pass algorithm using O(√N log N) space, and a bidirectional two-pass algorithm using O(log2 N) space performing this task.

#index 1818409
#* Highly expressive query languages for unordered data trees
#@ Serge Abiteboul;Pierre Bourhis;Victor Vianu
#t 2012
#c 6
#% 91364
#% 122392
#% 187081
#% 261370
#% 384978
#% 545382
#% 778122
#% 907142
#% 912238
#% 942354
#% 985981
#% 1270568
#% 1270570
#% 1335442
#% 1538777
#% 1673666
#% 1682381
#! We study highly expressive query languages for unordered data trees, using as formal vehicles Active XML and extensions of languages in the while family. All languages may be seen as adding some form of control on top of a set of basic pattern queries. The results highlight the impact and interplay of different factors: the expressive power of basic queries, the embedding of computation into data (as in Active XML), and the use of deterministic vs. nondeterministic control. All languages are Turing complete, but not necessarily query complete in the sense of Chandra and Harel. Indeed, we show that some combinations of features yield serious limitations, analogous to FOk definability in the relational context. On the other hand, the limitations come with benefits such as the existence of powerful normal forms. Other languages are "almost" complete, but fall short because of subtle limitations reminiscent of the copy elimination problem in object databases.

#index 1818410
#* Deciding twig-definability of node selecting tree automata
#@ Timos Antonopoulos;Dag Hovland;Wim Martens;Frank Neven
#t 2012
#c 6
#% 70235
#% 241166
#% 275308
#% 299976
#% 344425
#% 401124
#% 427027
#% 587580
#% 733593
#% 733595
#% 742056
#% 804841
#% 806858
#% 814648
#% 821563
#% 826029
#% 850728
#% 892715
#% 894435
#% 1039062
#% 1129529
#% 1181329
#% 1224353
#% 1265349
#% 1344895
#% 1426465
#% 1511886
#% 1582162
#! Node selecting tree automata (NSTAs) constitute a general formalism defining unary queries over trees. Basically, a node is selected by an NSTA when it is visited in a selecting state during an accepting run. We consider twig patterns as an abstraction of XPath. Since the queries definable by NSTAs form a strict superset of twig-definable queries, we study the complexity of the problem to decide whether the query by a given NSTA is twig-definable. In particular, we obtain that the latter problem is EXPTIME-complete. In addition, we show that it is also EXPTIME-complete to decide whether the query by a given NSTA is definable by a node selecting string automaton.

#index 1818411
#* Regular path queries on graphs with data
#@ Leonid Libkin;Domagoj Vrgoč
#t 2012
#c 6
#% 32904
#% 175464
#% 248025
#% 268797
#% 291299
#% 292269
#% 292677
#% 464895
#% 509964
#% 562454
#% 576102
#% 769518
#% 814648
#% 905847
#% 941832
#% 945025
#% 985981
#% 1019798
#% 1039062
#% 1063733
#% 1173493
#% 1181329
#% 1206916
#% 1217135
#% 1218646
#% 1223424
#% 1425586
#% 1426443
#% 1523818
#% 1552657
#% 1566271
#% 1581833
#% 1682381
#! Graph data models received much attention lately due to applications in social networks, semantic web, biological databases and other areas. Typical query languages for graph databases retrieve their topology, while actual data stored in them is usually queried using standard relational mechanisms. Our goal is to develop techniques that combine these two modes of querying, and give us query languages that can ask questions about both data and topology. As the basic querying mechanism we consider regular path queries, with the key difference that conditions on paths between nodes now talk not only about labels but also specify how data changes along the path. Paths that combine edge labels with data values are closely related to data words, so for stating conditions in queries, we look at several data-word formalisms developed recently. We show that many of them immediately lead to intractable data complexity for graph queries, with the notable exception of register automata, which can specify many properties of interest, and have NLogspace data and Pspace combined complexity. As register automata themselves are not easy to use in querying, we define two types of extensions of regular expressions that are more user-friendly, and develop query evaluation techniques for them. For one class, regular expressions with memory, we achieve the same bounds as for automata, and for the other class, regular expressions with equality, we also obtain tractable combined complexity of query evaluation. In addition, we show that results extends to analogs of conjunctive regular path queries.

#index 1818412
#* Deciding eventual consistency for a simple class of relational transducer networks
#@ Tom J. Ameloot;Jan Van den Bussche
#t 2012
#c 6
#% 321054
#% 384978
#% 630964
#% 874885
#% 942360
#% 1111848
#% 1172464
#% 1180017
#% 1246527
#% 1426442
#% 1472960
#% 1581842
#% 1581843
#% 1696824
#! Networks of relational transducers can serve as a formal model for declarative networking, focusing on distributed database querying applications. In declarative networking, a crucial property is eventual consistency, meaning that the final output does not depend on the message delays and re-orderings caused by the network. Here, we show that eventual consistency is decidable when the transducers satisfy some syntactic restrictions, some of which have also been considered in earlier work on automated verification of relational transducers. This simple class of transducer net-works computes exactly all distributed queries expressible by unions of conjunctive queries with negation.

#index 1818413
#* Win-move is coordination-free (sometimes)
#@ Daniel Zinn;Todd J. Green;Bertram Ludäscher
#t 2012
#c 6
#% 64410
#% 93791
#% 94458
#% 101646
#% 103705
#% 140410
#% 154317
#% 196695
#% 248029
#% 268781
#% 384978
#% 417546
#% 464721
#% 515555
#% 752806
#% 1246527
#% 1472960
#% 1581836
#% 1581842
#% 1581843
#% 1818412
#! In a recent paper by Hellerstein [15], a tight relationship was conjectured between the number of strata of a Datalog¬ program and the number of "coordination stages" required for its distributed computation. Indeed, Ameloot et al. [9] showed that a query can be computed by a coordination-free relational transducer network iff it is monotone, thus answering in the affirmative a variant of Hellerstein's CALM conjecture, based on a particular definition of coordination-free computation. In this paper, we present three additional models for declarative networking. In these variants, relational transducers have limited access to the way data is distributed. This variation allows transducer networks to compute more queries in a coordination-free manner: e.g., a transducer can check whether a ground atom A over the input schema is in the "scope" of the local node, and then send either A or ¬A to other nodes. We show the surprising result that the query given by the well-founded semantics of the unstratifiable win-move program is coordination-free in some of the models we consider. We also show that the original transducer network model [9] and our variants form a strict hierarchy of classes of coordination-free queries. Finally, we identify different syntactic fragments of Datalog∀¬¬, called semi-monotone programs, which can be used as declarative network programming languages, whose distributed computation is guaranteed to be eventually consistent and coordination-free.

#index 1818414
#* A normal form for preventing redundant tuples in relational databases
#@ Hugh Darwen;C. J. Date;Ronald Fagin
#t 2012
#c 6
#% 12849
#% 117901
#% 241515
#% 275941
#% 286860
#% 287316
#% 287339
#% 287754
#% 411570
#% 527109
#% 607468
#% 643940
#% 804840
#% 836134
#% 983074
#% 1369370
#! We introduce a new normal form, called essential tuple normal form (ETNF), for relations in a relational database where the constraints are given by functional dependencies and join dependencies. ETNF lies strictly between fourth normal form and fifth normal form (5NF, also known as projection-join normal form). We show that ETNF, although strictly weaker than 5NF, is exactly as effective as 5NF in eliminating redundancy of tuples. Our definition of ETNF is semantic, in that it is defined in terms of tuple redundancy. We give a syntactic characterization of ETNF, which says that a relation schema is in ETNF if and only if it is in Boyce-Codd normal form and some component of every explicitly declared join dependency of the schema is a superkey.

#index 1818415
#* Finding optimal probabilistic generators for XML collections
#@ Serge Abiteboul;Yael Amsterdamer;Daniel Deutch;Tova Milo;Pierre Senellart
#t 2012
#c 6
#% 248809
#% 273702
#% 300157
#% 398752
#% 413602
#% 741093
#% 848763
#% 881936
#% 891559
#% 893098
#% 894435
#% 949370
#% 1021195
#% 1022285
#% 1055754
#% 1063720
#% 1072645
#% 1127389
#% 1134141
#% 1291113
#% 1326580
#% 1424591
#% 1426465
#% 1523864
#% 1538772
#% 1538773
#! We study the problem of, given a corpus of XML documents and its schema, finding an optimal (generative) probabilistic model, where optimality here means maximizing the likelihood of the particular corpus to be generated. Focusing first on the structure of documents, we present an efficient algorithm for finding the best generative probabilistic model, in the absence of constraints. We further study the problem in the presence of integrity constraints, namely key, inclusion, and domain constraints. We study in this case two different kinds of generators. First, we consider a continuation-test generator that performs, while generating documents, tests of schema satisfiability; these tests prevent from generating a document violating the constraints but, as we will see, they are computationally expensive. We also study a restart generator that may generate an invalid document and, when this is the case, restarts and tries again. Finally, we consider the injection of data values into the structure, to obtain a full XML document. We study different approaches for generating these values.

#index 1818416
#* Learning twig and path queries
#@ Sławek Staworko;Piotr Wieczorek
#t 2012
#c 6
#% 31215
#% 71516
#% 180945
#% 229810
#% 278109
#% 289372
#% 312423
#% 464883
#% 479338
#% 515615
#% 564264
#% 570877
#% 600552
#% 733593
#% 742056
#% 745462
#% 809235
#% 809236
#% 911086
#% 1058704
#% 1110929
#% 1217187
#% 1370257
#% 1373479
#% 1378363
#% 1424594
#% 1426468
#% 1456293
#% 1499980
#% 1728758
#% 1728766
#! We investigate the problem of learning XML queries, path queries and twig queries, from examples given by the user. A learning algorithm takes on the input a set of XML documents with nodes annotated by the user and returns a query that selects the nodes in a manner consistent with the annotation. We study two learning settings that differ with the types of annotations. In the first setting the user may only indicate required nodes that the query must select (i.e., positive examples). In the second, more general, setting, the user may also indicate forbidden nodes that the query must not select (i.e., negative examples). The query may or may not select any node with no annotation. We formalize what it means for a class of queries to be learnable. One requirement is the existence of a learning algorithm that is sound i.e., always returning a query consistent with the examples given by the user. Furthermore, the learning algorithm should be complete i.e., able to produce every query with sufficiently rich examples. Other requirements involve tractability of the learning algorithm and its robustness to nonessential examples. We identify practical classes of Boolean and unary, path and twig queries that are learnable from positive examples. We also show that adding negative examples to the picture renders learning unfeasible.

#index 1818417
#* Bounded repairability for regular tree languages
#@ Gabriele Puppis;Cristian Riveros;Sławek Staworko
#t 2012
#c 6
#% 70235
#% 257873
#% 273687
#% 378392
#% 600179
#% 826007
#% 942354
#% 949370
#% 1026963
#% 1105438
#% 1179998
#% 1181332
#% 1278888
#% 1426465
#% 1538773
#% 1611362
#% 1638635
#% 1683883
#% 1728672
#! We consider the problem of repairing unranked trees (e.g., XML documents) satisfying a given restriction specification R (e.g., a DTD) into unranked trees satisfying a given target specification T. Specifically, we focus on the question of whether one can get from any tree in a regular language R to some tree in another regular language T with a finite, uniformly bounded, number of edit operations (i.e., deletions and insertions of nodes). We give effective characterizations of the pairs of specifications R and T for which such a uniform bound exists, and we study the complexity of the problem under different representations of the regular tree languages (e.g., non-deterministic stepwise automata, deterministic stepwise automata, DTDs). Finally, we point out some connections with the analogous problem for regular languages of words, which was previously studied in [6].

#index 1818418
#* On the complexity of query answering over incomplete XML documents
#@ Amélie Gheerbrant;Leonid Libkin;Tony Tan
#t 2012
#c 6
#% 663
#% 94459
#% 230142
#% 235018
#% 248038
#% 291299
#% 321327
#% 366807
#% 378409
#% 576116
#% 865766
#% 866986
#% 1039061
#% 1183377
#% 1206717
#% 1426460
#% 1481057
#% 1541335
#% 1552649
#% 1552660
#% 1581823
#% 1615075
#! Previous studies of incomplete XML documents have identified three main sources of incompleteness -- in structural information, data values, and labeling -- and addressed data complexity of answering analogs of unions of conjunctive queries under the open world assumption. It is known that structural incompleteness leads to intractability, while incompleteness in data values and labeling still permits efficient computation of certain answers. The goal of this paper is to provide a complete picture of the complexity of query answering over incomplete XML documents. We look at more expressive languages, at other semantic assumptions, and at both data and combined complexity of query answering, to see whether some well-behaving tractable classes have been missed. To incorporate non-positive features into query languages, we look at gentle ways of introducing negation via inequalities and/or Boolean combinations of positive queries, as well as the analog of relational calculus. We also look at the closed world assumption which, due to the hierarchical structure of XML, has two variations. For all combinations of languages and semantics of incompleteness we determine data and combined complexity of computing certain answers. We show that structural incompleteness leads to intractability under all assumptions, while by dropping it we can recover efficient evaluation algorithms for some queries that go beyond those previously studied.

#index 1818419
#* Learning schema mappings
#@ Balder ten Cate;Víctor Dalmau;Phokion G. Kolaitis
#t 2012
#c 6
#% 697
#% 26125
#% 73369
#% 73372
#% 108287
#% 114739
#% 131685
#% 157162
#% 180945
#% 187083
#% 198345
#% 211440
#% 303886
#% 321058
#% 333988
#% 348187
#% 378409
#% 449515
#% 450951
#% 451056
#% 543735
#% 572314
#% 599549
#% 716343
#% 809239
#% 810078
#% 824763
#% 826032
#% 936820
#% 1002374
#% 1036084
#% 1136225
#% 1206612
#% 1206961
#% 1209667
#% 1215806
#% 1272373
#% 1272374
#% 1274724
#% 1310057
#% 1424594
#% 1426466
#% 1493601
#% 1581857
#! A schema mapping is a high-level specification of the relationship between a source schema and a target schema. Recently, a line of research has emerged that aims at deriving schema mappings automatically or semi-automatically with the help of data examples, i.e., pairs consisting of a source instance and a target instance that depict, in some precise sense, the intended behavior of the schema mapping. Several different uses of data examples for deriving, refining, or illustrating a schema mapping have already been proposed and studied. In this paper, we use the lens of computational learning theory to systematically investigate the problem of obtaining algorithmically a schema mapping from data examples. Our aim is to leverage the rich body of work on learning theory in order to develop a framework for exploring the power and the limitations of the various algorithmic methods for obtaining schema mappings from data examples. We focus on GAV schema mappings, that is, schema mappings specified by GAV (Global-As-View) constraints. GAV constraints are the most basic and the most widely supported language for specifying schema mappings. We present an efficient algorithm for learning GAV schema mappings using Angluin's model of exact learning with membership and equivalence queries. This is optimal, since we show that neither membership queries nor equivalence queries suffice, unless the source schema consists of unary relations only. We also obtain results concerning the learnability of schema mappings in the context of Valiant's well known PAC (Probably-Approximately-Correct) learning model. Finally, as a byproduct of our work, we show that there is no efficient algorithm for approximating the shortest GAV schema mapping fitting a given set of examples, unless the source schema consists of unary relations only.

#index 1818420
#* Combining dependent annotations for relational algebra
#@ Egor V. Kostylev;Peter Buneman
#t 2012
#c 6
#% 663
#% 215225
#% 228817
#% 318704
#% 361445
#% 384978
#% 464891
#% 476414
#% 810115
#% 976987
#% 1328109
#% 1581829
#% 1592794
#! Annotation is some form of data that is added to an existing database. It could be additional data that for whatever reason cannot be stored in the original database, or it could be some form of metadata such as comments, probabilities, timestamps that are not normally regarded part of the basic database design. It has recently been observed that, in order to determine how annotations should be propagated through database queries, we need to have some structure on them. Although various forms of annotation have been considered in some detail, each form has been considered in isolation. In this paper we consider what happens when different forms of annotation are combined. We show that there are many cases in which annotations, for quite natural reasons, depend on one another. What is the appropriate structure to place on such annotations? We provide a method for combining different forms and provide a normal form which is useful in deciding whether two or more combined annotations are equivalent.

#index 1818421
#* Representation systems for data exchange
#@ Gösta Grahne;Adrian Onet
#t 2012
#c 6
#% 663
#% 94459
#% 289384
#% 366807
#% 384978
#% 465057
#% 576100
#% 801676
#% 874882
#% 927032
#% 976995
#% 997492
#% 1063712
#% 1063724
#% 1217116
#% 1270567
#% 1347328
#% 1424597
#% 1424598
#% 1424604
#% 1424605
#% 1433975
#% 1562961
#% 1581822
#% 1581823
#! The notion of representation systems describes structures that are algebraically closed under queries. It has recently been realized that representation systems are highly relevant also in the context of data exchange. We extend the notion of representation system to encompass data exchange mappings and their composition. Seen through this lens, two major classes of representation systems emerge, namely homomorphic data exchange systems and strong data exchange systems. The homomorphic "OWA" systems encompass the "classical" part of data exchange. Reasoning is modulo homomorphic equivalence (CQ-equivalence), and only unions of conjunctive queries and monotone data exchange mappings are supported. We then develop some new technical tools that allow us to prove that there is a class of '"CWA" strong representation systems in which reasoning is modulo isomorphic equivalence. These systems are based on conditional tables, and they support first order queries and non-monotonic data exchange mappings specified by a large class of second order dependencies. We achieve this by showing that, under a CWA-interpretation, conditional tables are chaseable with the aforementioned class of second order dependencies, and that the class is closed under composition in the CWA-setting. We also introduce a stricter notion of composability, and show that the class of (first order) source-to-target tuple generating dependencies is closed under the stricter notion of composability.

#index 1818422
#* Computing universal models under guarded TGDs
#@ André Hernich
#t 2012
#c 6
#% 583
#% 129217
#% 287316
#% 287339
#% 378409
#% 416034
#% 576116
#% 599549
#% 749088
#% 778122
#% 806215
#% 826032
#% 874914
#% 992962
#% 1039063
#% 1063724
#% 1217115
#% 1217122
#% 1279214
#% 1328190
#% 1413141
#% 1424602
#% 1511857
#% 1523803
#% 1523844
#% 1562977
#! A universal model of a database D and a set Σ of integrity constraints is a database that extends D, satisfies Σ, and is most general in the sense that it contains sound and complete information. Universal models have a number of applications including answering conjunctive queries, and deciding containment of conjunctive queries, with respect to databases with integrity constraints. Furthermore, they are used in slightly modified form as solutions in data exchange. In general, it is undecidable whether a database possesses a universal model, but in the past few years researchers identified various settings where this problem is decidable, and even efficiently solvable. This paper focuses on computing universal models under finite sets of guarded TGDs, non-conflicting keys, and negative constraints. Such constraints generalize inclusion dependencies, and were recently shown to be expressive enough to capture certain members of the DL-Lite family of description logics. The main result is an algorithm that, given a database without null values and a finite set Σ of such constraints, decides whether there is a universal model, and if so, outputs such a model. If Σ is fixed, the algorithm runs in polynomial time. The algorithm can be extended to cope with databases containing nulls; however, in this case, polynomial running time can be guaranteed only for databases with bounded block size.

#index 1818423
#* Dynamic definability
#@ Erich Grädel;Sebastian Siebertz
#t 2012
#c 6
#% 28120
#% 59792
#% 84275
#% 175440
#% 188352
#% 198470
#% 245652
#% 248036
#% 275310
#% 417543
#% 464536
#% 464689
#% 465040
#% 473142
#% 574872
#% 587329
#% 778122
#% 1707166
#! We investigate the logical resources required to maintain knowledge about a property of a finite structure that undergoes an ongoing series of local changes such as insertion or deletion of tuples to basic relations. Our framework is closely related to the Dyn-FO-framework of Patnaik and Immerman and the FOIES-framework of Dong, Libkin, Su and Wong, and also builds on work of Weber and Schwentick. We assume that the dynamic process starts with an arbitrary, nonempty structure, but in contrast to previous work, we assume that, in general, structures are unordered. We show how to modify known dynamic algorithms for symmetric reachability, bipartiteness, k-edge connectivity and more, to work also without an order and with dynamic processes starting at an arbitrary graph. A history independent dynamic system (also called deterministic or memoryless) is one that maintains all auxiliary information independent of the update order. In 1997, Dong and Su posed the problem whether there exist history independent dynamic systems with FO-updates for symmetric reachability or bipartiteness. We give a positive answer to this question. We further show that there is a history independent system for tree isomorphism with FO+C-updates. On the other hand we show that on unordered structures first-order logic is too weak to maintain enough information to answer the equal cardinality query and the tree isomorphism query dynamically.

#index 1818424
#* On the tractability of query compilation and bounded treewidth
#@ Abhay Jha;Dan Suciu
#t 2012
#c 6
#% 31482
#% 44876
#% 150115
#% 285330
#% 321058
#% 330538
#% 388024
#% 976987
#% 1027276
#% 1111133
#% 1196918
#% 1217176
#% 1300360
#% 1343690
#% 1343724
#% 1372709
#% 1426461
#% 1523890
#% 1538784
#% 1538790
#% 1615075
#% 1674531
#% 1675285
#% 1985057
#! We consider the problem of computing the probability of a Boolean function, which generalizes the model counting problem. Given an OBDD for such a function, its probability can be computed in linear time in the size of the OBDD. In this paper we investigate the connection between treewidth and the size of the OBDD. Bounded treewidth has proven to be applicable to many graph problems, which are NP-hard in general but become tractable on graphs with bounded treewidth. However, it is less well understood how bounded treewidth can be used for the probability computation problem of a Boolean function. We introduce a new notion of treewidth of a Boolean function, called the expression treewidth, as the smallest treewidth of any DAG-expression representing the function. Our new notion of bounded treewidth includes some previously known tractable cases: all read-once Boolean functions, and all functions having a bounded treewidth of the primal graph or of the incidence graph also have a bounded expression treewidth. We show that bounded expression treewidth implies the existence of a polynomial size OBDD, and that bounded expression pathwidth implies the existence of a constant-width OBDD. We also show a converse of the latter result: constant-width OBDD imply bounded expression pathwidth. We then study the implications of these results to query compilation, where the Boolean function is the lineage of a fixed query on varying input databases. We give a syntactic characterizations of all UCQ≠ queries that admit a polynomial size OBDD, showing that these are precisely inversion-free queries with unrestricted use of ≠. It was previously known that inversion-free queries characterize precisely those UCQ queries that have a polynomial size OBDD, and that these also have a constant width OBDD: in contrast, inversion-free queries with ≠ have polynomial-width OBDD, thus using the full power of OBDD. Finally, we show that in the case of UCQ, the four classes studied in this paper collapse: bounded expression pathwidth, bounded expression treewidth, constant-width OBDD, and polynomial size OBDD.

#index 1818425
#* Equivalence and minimization of conjunctive queries under combined semantics
#@ Rada Chirkova
#t 2012
#c 6
#% 137867
#% 190638
#% 223781
#% 279164
#% 300138
#% 458768
#% 465190
#% 599549
#% 874883
#% 874884
#% 1211652
#% 1217136
#% 1383775
#% 1573184
#! The problems of query containment, equivalence, and minimization are fundamental problems in the context of query processing and optimization. In their classic work [2] published in 1977, Chandra and Merlin solved the three problems for the language of conjunctive queries (CQ queries) on relational data, under the "set-semantics" assumption for query evaluation. While the results of [2] have been very influential in database research, it was recognized long ago that the set semantics does not correspond to the semantics of the standard commercial query language SQL. Alternative semantics, called bag and bag-set semantics, have been studied since 1993; Chaudhuri and Vardi in [5] outlined necessary and sufficient conditions for equivalence of CQ queries under these semantics. (The problems of containment of CQ bag and bag-set queries remain open to this day.) More recently, Cohen [7, 8] introduced a formalism for treating (generalizations of) CQ queries evaluated under each of set, bag, and bag-set semantics uniformly as special cases of the more general combined semantics. This formalism provides tools for studying broader classes of practical SQL queries, specifically important types of queries that arise in on-line analytical processing (OLAP). Cohen in [8] provides a sufficient condition for equivalence of (generalizations of) combined-semantics CQ queries, as well as sufficient and necessary equivalence conditions for several proper sublanguages of the query language of [8]. To the best of our knowledge, no results on minimization of CQ queries beyond set-semantics queries have been reported in the literature. Our goal in this paper is to continue the study of equivalence and minimization of CQ queries. We consider the problems of (i) finding minimized versions of combined-semantics CQ queries, and of (ii) determining whether two CQ queries are combined-semantics equivalent. We continue the tradition of [2, 5, 8] of studying these problems using the tool of containment between queries. We extend the containment, equivalence, and minimization results of [2] to general combined-semantics CQ queries, and show the limitations of each extension. We show that the minimization approach of [2] can be extended to general CQ queries without limitations. We also propose a necessary and sufficient condition for equivalence of queries belonging to a large natural sublanguage of combined-semantics CQ queries; this sublanguage encompasses (but is not limited to) all set, bag, and bag-set queries. Our equivalence and minimization results, as well as our general sufficient condition for containment of combined-semantics CQ queries, reduce correctly to the special cases reported in [5] for bag and bag-set semantics. Our containment and equivalence conditions also properly generalize the results of [8], provided the latter are restricted to the language of (combined-semantics) CQ queries.

#index 1818426
#* Parallel skyline queries
#@ Foto N. Afrati;Paraschos Koutris;Dan Suciu;Jeffrey D. Ullman
#t 2012
#c 6
#% 100803
#% 144423
#% 288976
#% 465167
#% 519659
#% 806212
#% 824670
#% 963669
#% 968270
#% 1022225
#% 1063486
#% 1136091
#% 1206998
#% 1267293
#% 1328060
#% 1372690
#% 1472960
#% 1484141
#% 1581836
#% 1581853
#% 1688253
#! In this paper, we design and analyze parallel algorithms for skyline queries. The skyline of a multidimensional set consists of the points for which no other point exists that is at least as good along every dimension. As a framework for parallel computation, we use both the MP model proposed in (Koutris and Suciu, PODS 2011), which requires that the data is perfectly load-balanced, and a variation of the model in (Afrati and Ullman, EDBT 2010), the GMP model, which demands weaker load balancing constraints. In addition to load balancing, we want to minimize the number of blocking steps, where all processors must wait and synchronize. We propose a 2-step algorithm in the MP model for any dimension of the dataset, as well a 1-step algorithm for the case of 2 and 3 dimensions. Moreover, we present a 1-step algorithm in the GMP model for any number of dimensions.

#index 1818427
#* Factorised representations of query results: size bounds and readability
#@ Dan Olteanu;Jakub Závodný
#t 2012
#c 6
#% 663
#% 273683
#% 338450
#% 339937
#% 414902
#% 847068
#% 976987
#% 992830
#% 1070611
#% 1083337
#% 1111133
#% 1141493
#% 1164915
#% 1197989
#% 1253069
#% 1581836
#% 1615075
#! We introduce a representation system for relational data based on algebraic factorisation using distributivity of product over union and commutativity of product and union. We give two characterisations of conjunctive queries based on factorisations of their results whose nesting structure is defined by so-called factorisation trees. The first characterisation concerns sizes of factorised representations. For any query, we derive a size bound that is asymptotically tight within our class of factorisations. We also characterise the queries by tight bounds on the readability of the provenance of result tuples and define syntactically the class of queries with bounded readability.

#index 1818428
#* Differentially private summaries for sparse data
#@ Graham Cormode;Cecilia Procopiuc;Divesh Srivastava;Thanh T. L. Tran
#t 2012
#c 6
#% 397426
#% 492912
#% 646233
#% 977011
#% 1014727
#% 1029084
#% 1198224
#% 1206678
#% 1217148
#% 1217156
#% 1217237
#% 1369276
#% 1426454
#% 1426456
#% 1496267
#% 1523886
#% 1670071
#! Differential privacy is fast becoming the method of choice for releasing data under strong privacy guarantees. A standard mechanism is to add noise to the counts in contingency tables derived from the dataset. However, when the dataset is sparse in its underlying domain, this vastly increases the size of the published data, to the point of making the mechanism infeasible. We propose a general framework to overcome this problem. Our approach releases a compact summary of the noisy data with the same privacy guarantee and with similar utility. Our main result is an efficient method for computing the summary directly from the input data, without materializing the vast noisy data. We instantiate this general framework for several summarization methods. Our experiments show that this is a highly practical solution: The summaries are up to 1000 times smaller, and can be computed in less than 1% of the time compared to standard methods. Finally, our framework works with various data transformations, such as wavelets or sketches.

#index 1855830
#* Proceedings of the 2012 Joint EDBT/ICDT Workshops
#@ Divesh Srivastava;Ismail Ari
#t 2012
#c 6
#! The EDBT/ICDT 2012 joint conference takes place on March 26-30, 2012 in Berlin, Germany. As in previous years, we host a number of high-quality workshops that will be held on the last day of the conference. This year, there is a collection of six EDBT/ICDT 2012 workshops whose themes cover important and timely aspects of databases and their applications, which complement the main conference program.

#index 1959469
#* The DNA query language DNAQL
#@ Robert Brijder;Joris J. M. Gillis;Jan Van den Bussche
#t 2013
#c 6
#% 299944
#% 344133
#% 544435
#% 562316
#% 631928
#% 817674
#% 829166
#% 976998
#% 1082169
#% 1425615
#% 1602703
#% 1615075
#% 1623808
#% 1667559
#% 1746500
#% 1905894
#! This paper presents an exposition of the authors' past and present work on the query language DNAQL for querying databases in DNA. In DNA computing, data is represented and stored in DNA molecules. Accordingly, a logical data model is defined that models complexes of DNA molecules in a graph-oriented fashion. Next, a set of formal operations on DNA complexes is defined, much in the spirit of the operations of the relational algebra in the relational data model. These operations model laboratory operations on DNA in solution. Their combination leads to the query language DNAQL; but in order for programs to be well-defined on prescribed types of inputs, a type system is superimposed on the language. Finally a correspondence is shown between well-typed DNAQL programs and programs in a relational-algebra query language.

#index 1959470
#* Enumerating with constant delay the answers to a query
#@ Luc Segoufin
#t 2013
#c 6
#% 292675
#% 344155
#% 384978
#% 494764
#% 982414
#% 986480
#% 1042142
#% 1181329
#% 1211511
#% 1493234
#% 1494846
#% 1521663
#% 1552186
#% 1582162
#% 1608485
#% 1682389
#% 1914832
#% 1916596
#! We survey recent results about enumerating with constant delay the answers to a query over a database. More precisely, we focus on the case when enumeration can be achieved with a preprocessing running in time linear in the size of the database, followed by an enumeration process outputting the answers one by one with constant time between any consecutive outputs. We survey classes of databases and classes of queries for which this is possible. We also mention related problems such as computing the number of answers or sampling the set of answers.

#index 1959471
#* A personal perspective on keyword search over data graphs
#@ Yehoshua Sagiv
#t 2013
#c 6
#% 213983
#% 262096
#% 397418
#% 660011
#% 750863
#% 824693
#% 874894
#% 893149
#% 993987
#% 1015258
#% 1058253
#% 1063539
#% 1206910
#% 1227614
#% 1426613
#% 1482251
#% 1581841
#% 1642112
#% 1673660
#% 1693898
#% 1959490
#! Theoretical and practical issues pertaining to keyword search over data graphs are discussed. A formal model and algorithms for enumerating answers (by operating directly on the data graph) are described. Various aspects of a system are explained, including the object-connector-property data model, how it is used to construct a data graph from an XML document, how to deal with redundancies in the source data, what are duplicate answers, implementation and GUI. An approach to ranking that combines textual relevance with semantic considerations is described. It is argued that search over data graphs is inherently a two-dimensional process, where the goal is not just to find particular content but also to collect information on how the desired data may be semantically connected.

#index 1959472
#* A theory of pricing private data
#@ Chao Li;Daniel Yang Li;Gerome Miklau;Dan Suciu
#t 2013
#c 6
#% 211193
#% 572311
#% 576110
#% 757953
#% 1381029
#% 1426418
#% 1429646
#% 1478165
#% 1584793
#% 1647859
#% 1740518
#% 1770132
#% 1783961
#% 1783980
#% 1783983
#% 1784892
#% 1859930
#! Personal data has value to both its owner and to institutions who would like to analyze it. Privacy mechanisms protect the owner's data while releasing to analysts noisy versions of aggregate query results. But such strict protections of individual's data have not yet found wide use in practice. Instead, Internet companies, for example, commonly provide free services in return for valuable sensitive information from users, which they exploit and sometimes sell to third parties. As the awareness of the value of the personal data increases, so has the drive to compensate the end user for her private information. The idea of monetizing private data can improve over the narrower view of hiding private data, since it empowers individuals to control their data through financial means. In this paper we propose a theoretical framework for assigning prices to noisy query answers, as a function of their accuracy, and for dividing the price amongst data owners who deserve compensation for their loss of privacy. Our framework adopts and extends key principles from both differential privacy and query pricing in data markets. We identify essential properties of the price function and micro-payments, and characterize valid solutions.

#index 1959473
#* Fast learning of restricted regular expressions and DTDs
#@ Dominik D. Freydenberger;Timo Kötzing
#t 2013
#c 6
#% 410276
#% 425025
#% 894435
#% 1022285
#% 1053733
#% 1178389
#% 1370257
#% 1456293
#% 1504036
#! We study the problem of generalizing from a finite sample to a language taken from a predefined language class. The two language classes we consider are subsets of the regular languages and have significance in the specification of XML documents (the classes corresponding to so called chain regular expressions, Chares, and to single occurrence regular expressions, Sores). The previous literature gave a number of algorithms for generalizing to Sores providing a trade off between quality of the solution and speed. Furthermore, a fast but non-optimal algorithm for generalizing to Chares is known. For each of the two language classes we give an efficient algorithm returning a minimal generalization from the given finite sample to an element of the fixed language class; such generalizations are called descriptive. In this sense, both our algorithms are optimal.

#index 1959474
#* Which DTDs are streaming bounded repairable?
#@ Pierre Bourhis;Gabriele Puppis;Cristian Riveros
#t 2013
#c 6
#% 257873
#% 378392
#% 826007
#% 848763
#% 894435
#% 956600
#% 1131403
#% 1611362
#% 1638635
#% 1718404
#% 1818417
#! Integrity constraint management concerns both checking whether data is valid and taking action to restore correctness when invalid data is discovered. In XML the notion of valid data can be captured by schema languages such as Document Type Definitions (DTDs) and more generally XML schemas. DTDs have the property that constraint checking can be done in streaming fashion. In this paper we consider when the corresponding action to restore validity -- repair -- can be done in streaming fashion. We formalize this as the problem of determining, given a DTD, whether or not a streaming procedure exists that transforms an input document so as to satisfy the DTD, using a number of edits independent of the document. We show that this problem is decidable. In fact, we show the decidability of a more general problem, allowing a more general class of schemas than DTDs, and requiring a repair procedure that works only for documents that are already known to satisfy another class of constraints. The decision procedure relies on a new analysis of the structure of DTDs, reducing to a novel notion of game played on pushdown systems associated with the schemas.

#index 1959475
#* XML compression via DAGs
#@ Markus Lohrey;Sebastian Maneth;Eric Noeth
#t 2013
#c 6
#% 300153
#% 386158
#% 427027
#% 562461
#% 587580
#% 588042
#% 937828
#% 942354
#% 954493
#% 1015266
#% 1015275
#% 1058258
#% 1272315
#% 1572714
#% 1701403
#% 1792884
#% 1880442
#! Unranked trees can be represented using their minimal dag (directed acyclic graph). For XML this achieves high compression ratios due to their repetitive mark up. Unranked trees are often represented through first child/next sibling (fcns) encoded binary trees. We study the difference in size (= number of edges) of minimal dag versus minimal dag of the fcns encoded binary tree. One main finding is that the size of the dag of the binary tree can never be smaller than the square root of the size of the minimal dag, and that there are examples that match this bound. We introduce a new combined structure, the hybrid dag, which is guaranteed to be smaller than (or equal in size to) both dags. Interestingly, we find through experiments that last child/previous sibling encodings are much better for XML compression via dags, than fcns encodings. This is because optional elements are more likely to appear towards the end of child sequences.

#index 1959476
#* Structural tractability of counting of solutions to conjunctive queries
#@ Arnaud Durand;Stefan Mengel
#t 2013
#c 6
#% 331899
#% 338450
#% 427161
#% 778122
#% 809166
#% 847068
#% 927017
#% 1000772
#% 1061964
#% 1224352
#% 1357840
#% 1493584
#% 1680797
#% 1888495
#% 1914832
#! In this paper we explore the problem of counting solutions to conjunctive queries. We consider a parameter called the quantified star size of a formula &phiv; which measures how the free variables are spread in &phiv;. We show that for conjunctive queries that admit nice decomposition properties (such as being of bounded treewidth or generalized hypertree width) bounded quantified star size exactly characterizes the classes of queries for which counting the number of solutions is tractable. This also allows us to fully characterize the conjunctive queries for which counting the solutions is tractable in the case of bounded arity. To illustrate the applicability of our results, we also show that computing the quantified star size of a formula is possible in time nO(k) for queries of generalized hypertree width k. Furthermore, quantified star size is even fixed parameter tractable parameterized by some other width measures, while it is W[1]-hard for generalized hypertree width and thus unlikely to be fixed parameter tractable. We finally show how to compute an approximation of quantified star size in polynomial time where the approximation ratio depends on the width of the input.

#index 1959477
#* Recursive queries on trees and data trees
#@ Serge Abiteboul;Pierre Bourhis;Anca Muscholl;Zhilin Wu
#t 2013
#c 6
#% 384978
#% 401124
#% 733595
#% 1039062
#% 1106486
#% 1106498
#% 1181329
#% 1538781
#% 1888838
#! The analysis of datalog programs over relational structures has been studied in depth, most notably the problem of containment. The analysis problems that have been considered were shown to be undecidable with the exception of (i) containment of arbitrary programs in nonrecursive ones, (ii) containment of monadic programs, and (iii) emptiness. In this paper, we are concerned with a much less studied problem, the analysis of datalog programs over data trees. We show that the analysis of datalog programs is more complex for data trees than for arbitrary structures. In particular, we prove that the three aforementioned problems are undecidable for data trees. But in practice, data trees (e.g., XML trees) are often of bounded depth. We prove that all three problems are decidable over bounded depth data trees. Another contribution of the paper is the study of a new form of automata called pattern automata, that are essentially equivalent to linear datalog programs. We use pattern automata to show that the emptiness problem for linear monadic datalog programs with data value inequalities is decidable over arbitrary data trees.

#index 1959478
#* On optimum left-to-right strategies for active context-free games
#@ Henrik Björklund;Martin Schuster;Thomas Schwentick;Joscha Kulbatzki
#t 2013
#c 6
#% 654485
#% 806211
#% 809260
#! Active context-free games are two-player games on strings over finite alphabets with one player trying to rewrite the input string to match a target specification. These games have been investigated in the context of exchanging Active XML (AXML) data. While it was known that the rewriting problem is undecidable in general, it is shown here that it is EXPSPACE-complete to decide for a given context-free game, whether all safely rewritable strings can be safely rewritten in a left-to-right manner, a problem that was previously considered by Abiteboul et al. Furthermore, it is shown that the corresponding problem for games with finite replacement languages is EXPTIME-complete.

#index 1959479
#* Walk logic as a framework for path query languages on graph databases
#@ Jelle Hellings;Bart Kuijpers;Jan Van den Bussche;Xiaowang Zhang
#t 2013
#c 6
#% 241166
#% 384978
#% 630968
#% 655430
#% 657754
#% 731485
#% 778122
#% 1019798
#% 1060757
#% 1264206
#% 1334766
#% 1426443
#% 1494848
#% 1566271
#% 1746861
#% 1764508
#% 1770126
#% 1932717
#! Motivated by the current interest in languages for expressing path queries to graph databases, this paper proposes to investigate Walk Logic (WL): the extension of first-order logic on finite graphs with the possibility to explicitly quantify over walks. WL can serve as a unifying framework for path query languages. To support this claim, WL is compared in expressive power with various established query languages for graphs, such as first-order logic extended with reachability; the monadic second-order logic of graphs; hybrid computation tree logic; and regular path queries. WL also serves as a framework to investigate the following natural questions: Is quantifying over walks more powerful than quantifying over paths (walks without repeating nodes) only? Is quantifying over infinite walks more powerful than quantifying over finite walks only? WL model checking is decidable, but determining the precise complexity remains an open problem.

#index 1959480
#* Querying graph databases with XPath
#@ Leonid Libkin;Wim Martens;Domagoj Vrgoč
#t 2013
#c 6
#% 32904
#% 268797
#% 292677
#% 390685
#% 742056
#% 778122
#% 814648
#% 850728
#% 874910
#% 985981
#% 1019798
#% 1024476
#% 1206916
#% 1218646
#% 1223424
#% 1224353
#% 1266687
#% 1426443
#% 1497253
#% 1511885
#% 1523818
#% 1538787
#% 1552657
#% 1582162
#% 1770126
#% 1791183
#% 1818406
#% 1818411
#% 1888487
#! XPath plays a prominent role as an XML navigational language due to several factors, including its ability to express queries of interest, its close connection to yardstick database query languages (e.g., first-order logic), and the low complexity of query evaluation for many fragments. Another common database model---graph databases---also requires a heavy use of navigation in queries; yet it largely adopts a different approach to querying, relying on reachability patterns expressed with regular constraints. Our goal here is to investigate the behavior and applicability of XPath-like languages for querying graph databases, concentrating on their expressiveness and complexity of query evaluation. We are particularly interested in a model of graph data that combines navigation through graphs with querying data held in the nodes, such as, for example, in a social network scenario. As navigational languages, we use analogs of core and regular XPath and augment them with various tests on data values. We relate these languages to first-order logic, its transitive closure extensions, and finite-variable fragments thereof, proving several capture results. In addition, we describe their relative expressive power. We then show that they behave very well computationally: they have a low-degree polynomial combined complexity, which becomes linear for several fragments. Furthermore, we introduce new types of tests for XPath languages that let them capture first-order logic with data comparisons and prove that the low complexity bounds continue to apply to such extended languages. Therefore, XPath-like languages seem to be very well-suited to query graphs.

#index 1959481
#* Definability problems for graph query languages
#@ Timos Antonopoulos;Frank Neven;Frédéric Servais
#t 2013
#c 6
#% 197751
#% 248025
#% 384978
#% 419948
#% 562309
#% 562454
#% 976987
#% 1111697
#% 1209667
#% 1224938
#% 1370257
#% 1426443
#% 1456293
#% 1493567
#% 1538787
#% 1581833
#% 1770126
#% 1818411
#% 1959479
#! Given a graph, a relation on its nodes, and a query language Q of interest, we study the Q-definability problem which amounts to deciding whether there exists a query in Q defining precisely the given relation over the given graph. Previous research has identified the complexity of FO- and CQ-definability. In this paper, we consider the definability problem for regular paths and conjunctive regular path queries (CRPQs) over labelled graphs.

#index 1959482
#* Algebraic structures for capturing the provenance of SPARQL queries
#@ Floris Geerts;Grigoris Karvounarakis;Vassilis Christophides;Irini Fundulaki
#t 2013
#c 6
#% 976987
#% 977141
#% 1022258
#% 1063736
#% 1092014
#% 1154625
#% 1180021
#% 1206861
#% 1218627
#% 1223424
#% 1288164
#% 1300978
#% 1332386
#% 1374392
#% 1424588
#% 1426581
#% 1536933
#% 1581829
#% 1581837
#% 1716777
#% 1942759
#! We show that the evaluation of SPARQL algebra queries on various notions of annotated RDF graphs can be seen as particular cases of the evaluation of these queries on RDF graphs annotated with elements of so-called spm-semirings. Spm-semirings extend semirings, used for positive relational algebra queries on annotated relational data, with a new operator to capture the semantics of the non-monotone SPARQL operator OPTIONAL. Furthermore, spm-semiring-based annotations ensure that desired SPARQL query equivalences hold when querying annotated RDF. In addition to introducing spm-semirings, we study their properties and provide an alternative characterization of these structures in terms of semirings with an embedded boolean algebra (or seba-structure for short). This characterization allows to construct spm-semirings and to identify a universal object in the class of spm-semirings. Finally, we show that this universal object provides a concise provenance representation and can be used to evaluate SPARQL queries on arbitrary spm-semiring-annotated RDF graphs.

#index 1959483
#* A propagation model for provenance views of public/private workflows
#@ Susan B. Davidson;Tova Milo;Sudeepa Roy
#t 2013
#c 6
#% 576761
#% 742048
#% 824727
#% 864412
#% 893101
#% 956511
#% 1022247
#% 1029084
#% 1083653
#% 1121279
#% 1189362
#% 1193149
#% 1217156
#% 1268484
#% 1370254
#% 1414540
#% 1523888
#% 1538770
#% 1543133
#% 1581817
#% 1581832
#% 1581862
#% 1588627
#% 1592343
#% 1595893
#% 1606068
#% 1635928
#% 1647861
#% 1728180
#% 1740518
#! We study the problem of concealing functionality of a proprietary or private module when provenance information is shown over repeated executions of a workflow which contains both public and private modules. Our approach is to use provenance views to hide carefully chosen subsets of data over all executions of the workflow to ensure Γ-privacy: for each private module and each input x, the module's output f(x) is indistinguishable from Γ--1 other possible values given the visible data in the workflow executions. We show that Γ-privacy cannot be achieved simply by combining solutions for individual private modules; data hiding must also be propagated through public modules. We then examine how much additional data must be hidden and when it is safe to stop propagating data hiding. The answer depends strongly on the workflow topology as well as the behavior of public modules on the visible data. In particular, for a class of workflows (which include the common tree and chain workflows), taking private solutions for each private module, augmented with a public closure that is upstream-downstream safe, ensures Γ-privacy. We define these notions formally and show that the restrictions are necessary. We also study the related optimization problems of minimizing the amount of hidden data.

#index 1959484
#* Annotations are relative
#@ Peter Buneman;Egor V. Kostylev;Stijn Vansummeren
#t 2013
#c 6
#% 318704
#% 332909
#% 810115
#% 864469
#% 912238
#% 976987
#% 1022258
#% 1328109
#% 1581829
#% 1592794
#% 1716777
#% 1818420
#! Most systems that have been developed for annotation of data assume a two-level structure in which annotation is superimposed on, and separate from, the data. However there are many cases in which an annotation may itself be annotated. For example threads in e-mail and newsgroups allow the imposition of one comment on another; belief annotations can be compounded; and valid time, regarded as an annotation can be freely mixed with belief annotations (at time t1, B1 believed that at time t2, B2 believed that...). In this paper we describe a hierarchical model of annotation in which there is no absolute distinction between annotation and data. First, we introduce a term model for annotations and, in order to express the fact that an annotation may apply to two or more data values with some shared structure, we provide a simple schema for annotation hierarchies. We then look at how queries can be applied to such hierarchies; in particular we ask the usual question of how annotations should propagate through queries. We take the view that the query together with schema describes a level in the hierarchy: everything below this level is treated as data to which the query should be applied; everything above it is annotation which should, according to certain rules, be propagated with the query. We also examine the representation of annotation hierarchies in conventional relational structures and describe a technique for annotating datalog programs.

#index 1959485
#* Schema mappings and data exchange for graph databases
#@ Pablo Barceló;Jorge Pérez;Juan Reutter
#t 2013
#c 6
#% 197751
#% 291299
#% 299967
#% 378409
#% 562451
#% 765258
#% 765540
#% 809239
#% 809249
#% 814648
#% 826032
#% 850730
#% 874879
#% 997492
#% 1015302
#% 1039061
#% 1215806
#% 1217117
#% 1426443
#% 1433975
#% 1497253
#% 1538780
#% 1538787
#% 1541335
#% 1581822
#% 1581833
#% 1592783
#% 1764508
#! Data exchange and schema mapping management have received little attention so far in the graph database scenario, and tools developed in this context for relational databases have significant drawbacks in the context of graph-structured data. In this paper we embark on the study of interoperability issues for graph databases, including schema mappings, data exchange and certain answers computation. We start by analyzing different possibilities for specifying mappings in graph databases. Our mapping languages are based on the most typical graph databases queries, ranging from regular path queries to conjunctions of nested regular expressions. They subsume all previously considered mapping languages, and let one express many data exchange scenarios in the graph database context. We study the problems of materializing solutions and query answering, in particular, the problem of computing universal representatives and certain answers for various classes of mappings. We show that both problems are difficult with respect to combined complexity, and that for the latter problem, even data complexity is high for some very simple mappings and queries. We then identify relevant classes of mappings and queries for which the problems of materializing solutions and query answering can be solved efficiently.

#index 1959486
#* Containment of pattern-based queries over data trees
#@ Claire David;Amélie Gheerbrant;Leonid Libkin;Wim Martens
#t 2013
#c 6
#% 230142
#% 248032
#% 333841
#% 378409
#% 398752
#% 545382
#% 570877
#% 572311
#% 643569
#% 733593
#% 742056
#% 865766
#% 866986
#% 888014
#% 909447
#% 1021195
#% 1039061
#% 1044440
#% 1086124
#% 1106486
#% 1106498
#% 1217117
#% 1481057
#% 1541335
#% 1552649
#% 1592794
#% 1661443
#% 1818418
#! We study static analysis, in particular the containment problem, for analogs of conjunctive queries over XML documents. The problem has been studied for queries based on arbitrary patterns, not necessarily following the tree structure of documents. However, many applications force the syntactic shape of queries to be tree-like, as they are based on proper tree patterns. This renders previous results, crucially based on having non-tree-like features, inapplicable. Thus, we investigate static analysis of queries based on proper tree patterns. We go beyond simple navigational conjunctive queries in two ways: we look at unions and Boolean combinations of such queries as well and, crucially, all our queries handle data stored in documents, i.e., we deal with containment over data trees. We start by giving a general Πp2 upper bound on the containment of conjunctive queries and Boolean combinations for patterns that involve all types of navigation through documents. We then show matching hardness for conjunctive queries with all navigation, or their Boolean combinations with the simplest form of navigation. After that we look at cases when containment can be witnessed by homomorphisms of analogs of tableaux. These include conjunctive queries and their unions over child and next-sibling axes; however, we show that not all cases of containment can be witnessed by homomorphisms. We look at extending tree patterns used in queries in three possible ways: with wildcard, with schema information, and with data value comparisons. The first one is relatively harmless, the second one tends to increase complexity by an exponential, and the last one quickly leads to undecidability.

#index 1959487
#* Access patterns and integrity constraints revisited
#@ Vince Bárány;Michael Benedikt;Pierre Bourhis
#t 2013
#c 6
#% 198466
#% 273912
#% 342359
#% 384978
#% 465057
#% 726626
#% 801698
#% 874914
#% 976986
#% 1217122
#% 1426418
#% 1537995
#% 1538789
#% 1700141
#% 1880450
#! We consider which queries are answerable in the presence of access restrictions and integrity constraints, and which portions of the schema are accessible in the presence of access restrictions and constraints. Unlike prior work, we focus on integrity constraint languages that subsume inclusion dependencies. We also use a semantic definition of answerability: a query is answerable if the accessible information is sufficient to determine its truth value. We show that answerability is decidable for the class of guarded dependencies, which includes all inclusion dependencies, and also for constraints given in the guarded fragment of first-order logic. We also show that answerable queries have "query plans" in a restricted language. We give corresponding results for extractability of portions of the schema. Our results relate querying with limited access patterns, determinacy-vs-rewriting, and analysis of guarded constraints.

#index 1959488
#* Using the crowd for top-k and group-by queries
#@ Susan B. Davidson;Sanjeev Khanna;Tova Milo;Sudeepa Roy
#t 2013
#c 6
#% 190611
#% 1265149
#% 1538161
#% 1573368
#% 1581851
#% 1628171
#% 1730733
#% 1746896
#% 1770349
#% 1770351
#% 1846744
#% 1869838
#% 1880463
#! Group-by and top-k are fundamental constructs in database queries. However, the criteria used for grouping and ordering certain types of data -- such as unlabeled photos clustered by the same person ordered by age -- are difficult to evaluate by machines. In contrast, these tasks are easy for humans to evaluate and are therefore natural candidates for being crowd-sourced. We study the problem of evaluating top-k and group-by queries using the crowd to answer either type or value questions. Given two data elements, the answer to a type question is "yes" if the elements have the same type and therefore belong to the same group or cluster; the answer to a value question orders the two data elements. The assumption here is that there is an underlying ground truth, but that the answers returned by the crowd may sometimes be erroneous. We formalize the problems of top-k and group-by in the crowd-sourced setting, and give efficient algorithms that are guaranteed to achieve good results with high probability. We analyze the crowd-sourced cost of these algorithms in terms of the total number of type and value questions, and show that they are essentially the best possible. We also show that fewer questions are needed when values and types are correlated, or when the error model is one in which the error decreases as the distance between the two elements in the sorted order increases.

#index 1959489
#* Certain and possible XPath answers
#@ Sara Cohen;Yaacov Y. Weiss
#t 2013
#c 6
#% 238556
#% 252029
#% 378391
#% 379344
#% 654442
#% 874894
#% 1039061
#% 1058704
#% 1127409
#% 1206703
#% 1217186
#% 1217187
#% 1279271
#% 1328076
#% 1424594
#% 1426460
#% 1426503
#% 1728766
#% 1818416
#! Formulating an XPath query over an XML document is a difficult chore for a non-expert user. This paper introduces a novel approach to ease the querying process. Instead of specifying a query, the user simply marks positive examples χ+ of nodes that fit her information need. She may also mark negative examples χ− of undesirable nodes. A deductive method, to suggest additional nodes that will interest the user, is developed in this paper. To be precise, a node y is a certain answer if every query returning all positive examples χ+, and not returning any negative example from χ−, must also return y. Similarly, y is a possible answer if there exists a query returning χ+ and y, while not returning any node in χ−. Thus, y is likely to be of interest to the user if y is a certain answer, and unlikely to be of interest if y is not even a possible answer. The complexity of finding certain and possible answers, with respect to various classes of XPath, is studied. It is shown that for a wide variety of XPath queries (including child and descendant axes, wildcards, branching and attribute constraints), certain and possible answers can be found efficiently, provided that χ+ and χ− are of bounded size. To prove this result a novel algorithm is developed.

#index 1959490
#* Extracting minimum-weight tree patterns from a schema with neighborhood constraints
#@ Benny Kimelfeld;Yehoshua Sagiv
#t 2013
#c 6
#% 464863
#% 660011
#% 838492
#% 857282
#% 874894
#% 993987
#% 1016135
#% 1063539
#% 1127413
#% 1206760
#% 1217198
#% 1217259
#% 1223425
#% 1288160
#% 1581841
#% 1972413
#! The task of formulating queries is greatly facilitated when they can be generated automatically from some given data values, schema concepts or both (e.g., names of particular entities and XML tags). This automation is the basis of various database applications, such as keyword search and interactive query formulation. Usually, automatic query generation is realized by finding a set of small tree patterns that contain some given labels. More formally, the computational problem at hand is to find top-k patterns, that is, k minimum-weight tree patterns that contain a given bag of labels, conform to the schema, and are non-redundant. A plethora of systems and research papers include a component that deals with this problem. This paper presents an algorithm for this problem, with complexity guarantees, that allows nontrivial schema constraints and, hence, avoids generating patterns that cannot be instantiated. Specifically, this paper shows that for schemas with certain types of neighborhood constraints, the problem is fixed-parameter tractable (FPT), the parameter being the size of the given bag of labels. As machinery, an adaptation of Lawler-Murty's procedure is developed. This adaptation reduces a top-k problem, over an infinite space of solutions, to a prefix-constrained optimization problem. It is shown how to cast the problem of top-k patterns in this adaptation. A solution is developed for the corresponding prefix-constrained optimization problem, and it uses an algorithm for finding a (single) minimum-weight tree pattern. This algorithm generalizes an earlier work by handling leaf constraints (i.e., which labels may, must or should not be leaves). It all boils down to a reduction showing that, under a language for neighborhood constraints, finding top-k patterns is FPT if a certain variant of exact cover is FPT.

#index 1959491
#* On optimal differentially private mechanisms for count-range queries
#@ Chen Zeng;Jin-Yi Cai;Pinyan Lu;Jeffrey F. Naughton
#t 2013
#c 6
#% 576110
#% 963241
#% 1029084
#% 1061644
#% 1198224
#% 1414540
#% 1426322
#% 1426454
#% 1426455
#% 1426456
#% 1521655
#% 1523886
#% 1670071
#% 1740518
#! While there is a large and growing body of literature on differentially private mechanisms for answering various classes of queries, to the best of our knowledge "count-range" queries have not been studied. These are a natural class of queries that ask "is the number of rows in a relation satisfying a given predicate between two integers θ1 and θ2?" Such queries can be viewed as a simple form of SQL "having" queries. We begin by developing a provably optimal differentially private mechansim for count-range queries for a single consumer. For count queries (in contrast to countrange queries), Ghosh et al. [9] have provided a differentially private mechanism that simultaneously maximizes utility for multiple consumers. This raises the question of whether such a mechanism exists for count-range queries. We prove that the answer is no --- for count range queries, no such mechanism exists. However, perhaps surprisingly, we prove that such a mechanism does exist for "threshold" queries, which are simply count-range queries for which either θ1 = 0 or θ2 = +∞. Furthermore, we prove that this mechanism is a two-approximation for general count-range queries.

#index 1959492
#* Optimal error of query sets under the differentially-private matrix mechanism
#@ Chao Li;Gerome Miklau
#t 2013
#c 6
#% 576110
#% 963241
#% 977011
#% 1022246
#% 1061644
#% 1198227
#% 1214684
#% 1426322
#% 1426328
#% 1426329
#% 1426454
#% 1496267
#% 1521653
#% 1521654
#% 1523886
#% 1581864
#% 1595893
#% 1627567
#% 1730731
#% 1732708
#% 1740518
#% 1770520
#% 1791638
#% 1846816
#% 1880452
#! A common goal of privacy research is to release synthetic data that satisfies a formal privacy guarantee and can be used by an analyst in place of the original data. To achieve reasonable accuracy, a synthetic data set must be tuned to support a specified set of queries accurately, sacrificing fidelity for other queries. This work considers methods for producing synthetic data under differential privacy and investigates what makes a set of queries "easy" or "hard" to answer. We consider answering sets of linear counting queries using the matrix mechanism [18], a recent differentially-private mechanism that can reduce error by adding complex correlated noise adapted to a specified workload. Our main result is a novel lower bound on the minimum total error required to simultaneously release answers to a set of workload queries. The bound reveals that the hardness of a query workload is related to the spectral properties of the workload when it is represented in matrix form. The bound is most informative for (ε δ)-differential privacy but also applies to ε-differential privacy.

#index 1959493
#* Private decayed predicate sums on streams
#@ Jean Bolot;Nadia Fawaz;S. Muthukrishnan;Aleksandar Nikolov;Nina Taft
#t 2013
#c 6
#% 379445
#% 576112
#% 1426323
#% 1484081
#% 1489408
#% 1740518
#! In many monitoring applications, recent data is more important than distant data. How does this affect privacy of data analysis? We study a general class of data analyses --- predicate sums --- in this context. Formally, we study the problem of estimating predicate sums privately, for sliding windows and other decay models. While we require accuracy in analysis with respect to the decayed sums, we still want differential privacy for the entire past. This is challenging because window sums are not monotonic or even near-monotonic as the problems studied previously [DPNR10]. We present accurate ε-differentially private algorithms for decayed sums. For window and exponential decay sums, our algorithms are accurate up to additive 1/ε and polylog terms in the range of the computed function; for polynomial decay sums which are technically more challenging because partial solutions do not compose easily, our algorithms incur additional relative error. Our algorithm for polynomial decay sums generalizes to arbitrary decay sum functions. The algorithm crucially relies on our solution for the window sum problem as a subroutine. Further, we show lower bounds, tight within polylog factors and tight with respect to the dependence on the probability of error. Our results are obtained via a natural dyadic tree we maintain, but the crux is we treat the tree data structure in non-uniform manner. We also extend our study and consider the "dual" question of maintaining conventional running sums on the entire data thus far, but when privacy constraints expire with time. We define a new model of privacy with expiration and consider the problems of designing accurate running sum and linear map algorithms in this model. Now the goal is to design algorithms whose accuracy guarantees scale with the size of the privacy window. We reduce running sum with a privacy window W to window sum without privacy expiration, and characterize the accuracy of output perturbation for general linear maps with privacy window W.

