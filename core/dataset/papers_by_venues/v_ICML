#index 454232
#* Proceedings of the Fourteenth International Conference on Machine Learning
#@ Douglas H. Fisher
#t 1997
#c 19

#index 454233
#* Proceedings of the Fifteenth International Conference on Machine Learning
#@ Jude W. Shavlik
#t 1998
#c 19

#index 454234
#* Proceedings of the Sixteenth International Conference on Machine Learning
#@ Ivan Bratko;Saso Dzeroski
#t 1999
#c 19

#index 454235
#* Proceedings of the Seventeenth International Conference on Machine Learning
#@ Pat Langley
#t 2000
#c 19

#index 454236
#* Proceedings of the Eighteenth International Conference on Machine Learning
#@ Carla E. Brodley;Andrea Pohoreckyj Danyluk
#t 2001
#c 19

#index 454237
#* Proceedings of the Nineteenth International Conference on Machine Learning
#@ Claude Sammut;Achim G. Hoffmann
#t 2002
#c 19

#index 464267
#* Composite Kernels for Hypertext Categorisation
#@ Thorsten Joachims;Nello Cristianini;John Shawe-Taylor
#t 2001
#c 19

#index 464268
#* Toward Optimal Active Learning through Sampling Estimation of Error Reduction
#@ Nicholas Roy;Andrew McCallum
#t 2001
#c 19

#index 464269
#* Evolutionary Search, Stochastic Policies with Memory, and Reinforcement Learning with Hidden State
#@ Matthew R. Glickman;Katia P. Sycara
#t 2001
#c 19

#index 464270
#* A procedure for unsupervised lexicon learning
#@ Anand Venkataraman
#t 2001
#c 19

#index 464271
#* A Generalized Kalman Filter for Fixed Point Approximation and Efficient Temporal Difference Learning
#@ David Choi;Benjamin Van Roy
#t 2001
#c 19

#index 464272
#* A Unified Loss Function in Bayesian Framework for Support Vector Regression
#@ Wei Chu;S. Sathiya Keerthi;Chong Jin Ong
#t 2001
#c 19

#index 464273
#* Some Theoretical Aspects of Boosting in the Presence of Noisy Data
#@ Wenxin Jiang
#t 2001
#c 19

#index 464274
#* Learning an Agent's Utility Function by Observing Behavior
#@ Urszula Chajewska;Daphne Koller;Dirk Ormoneit
#t 2001
#c 19

#index 464275
#* Application of Fuzzy Similarity-Based Fractal Dimensions to Characterize Medical Time Series
#@ Manish Sarkar;Tze-Yun Leong
#t 2001
#c 19

#index 464276
#* Visual Development and the Acquisition of Binocular Disparity Sensitivities
#@ Melissa Dominguez;Robert A. Jacobs
#t 2001
#c 19

#index 464277
#* An Efficient Approach for Approximating Multi-dimensional Range Queries and Nearest Neighbor Classification in Large Datasets
#@ Carlotta Domeniconi;Dimitrios Gunopulos
#t 2001
#c 19

#index 464278
#* Expectation Maximization for Weakly Labeled Data
#@ Yuri A. Ivanov;Bruce Blumberg;Alex Pentland
#t 2001
#c 19

#index 464279
#* An Improved Predictive Accuracy Bound for Averaging Classifiers
#@ John Langford;Matthias Seeger;Nimrod Megiddo
#t 2001
#c 19

#index 464280
#* Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers
#@ Bianca Zadrozny;Charles Elkan
#t 2001
#c 19

#index 464281
#* Convergence of Gradient Dynamics with a Variable Learning Rate
#@ Michael H. Bowling;Manuela M. Veloso
#t 2001
#c 19

#index 464282
#* Exploration Control in Reinforcement Learning using Optimistic Model Selection
#@ Jeremy L. Wyatt
#t 2001
#c 19

#index 464283
#* Friend-or-Foe Q-learning in General-Sum Games
#@ Michael L. Littman
#t 2001
#c 19

#index 464284
#* Relevance Feedback using Support Vector Machines
#@ Harris Drucker;Behzad Shahrary;David Gibbon
#t 2001
#c 19

#index 464285
#* Learning to Generate Fast Signal Processing Implementations
#@ Bryan Singer;Manuela M. Veloso
#t 2001
#c 19

#index 464286
#* Ridge Regression Confidence Machine
#@ Ilia Nouretdinov;Thomas Melluish;Volodya Vovk
#t 2001
#c 19

#index 464287
#* Estimating a Kernel Fisher Discriminant in the Presence of Label Noise
#@ Neil D. Lawrence;Bernhard Schölkopf
#t 2001
#c 19

#index 464288
#* Using the Genetic Algorithm to Reduce the Size of a Nearest-Neighbor Classifier and to Select Relevant Attributes
#@ Antonin Rozsypal;Miroslav Kubat
#t 2001
#c 19

#index 464289
#* Feature Construction with Version Spaces for Biochemical Applications
#@ Stefan Kramer;Luc De Raedt
#t 2001
#c 19

#index 464290
#* Breeding Decision Trees Using Evolutionary Techniques
#@ Athanassios Papagelis;Dimitrios Kalles
#t 2001
#c 19

#index 464291
#* Constrained K-means Clustering with Background Knowledge
#@ Kiri Wagstaff;Claire Cardie;Seth Rogers;Stefan Schrödl
#t 2001
#c 19

#index 464292
#* Repairing Faulty Mixture Models using Density Estimation
#@ Peter Sand;Andrew W. Moore
#t 2001
#c 19

#index 464293
#* Comprehensible Interpretation of Relief's Estimates
#@ Marko Robnik-Sikonja;Igor Kononenko
#t 2001
#c 19

#index 464294
#* Incremental Maximization of Non-Instance-Averaging Utility Functions with Applications to Knowledge Discovery Problems
#@ Tobias Scheffer;Stefan Wrobel
#t 2001
#c 19

#index 464295
#* Reinforcement Learning in Dynamic Environments using Instantiated Information
#@ Marco Wiering
#t 2001
#c 19

#index 464296
#* Scaling Reinforcement Learning toward RoboCup Soccer
#@ Peter Stone;Richard S. Sutton
#t 2001
#c 19

#index 464297
#* Learning with the Set Covering Machine
#@ Mario Marchand;John Shawe-Taylor
#t 2001
#c 19

#index 464298
#* Adjusting the Outputs of a Classifier to New a Priori Probabilities May Significantly Improve Classification Accuracy: Evidence from a multi-class problem in remote sensing
#@ Patrice Latinne;Marco Saerens;Christine Decaestecker
#t 2001
#c 19

#index 464299
#* Latent Semantic Kernels
#@ Nello Cristianini;John Shawe-Taylor;Huma Lodhi
#t 2001
#c 19

#index 464300
#* Pairwise Comparison of Hypotheses in Evolutionary Learning
#@ Krzysztof Krawiec
#t 2001
#c 19

#index 464301
#* Bias Correction in Classification Tree Construction
#@ Alin Dobra;Johannes Gehrke
#t 2001
#c 19

#index 464302
#* Some Sparse Approximation Bounds for Regression Problems
#@ Tong Zhang
#t 2001
#c 19

#index 464303
#* Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density
#@ Amy McGovern;Andrew G. Barto
#t 2001
#c 19

#index 464304
#* Learning Probabilistic Models of Relational Structure
#@ Lise Getoor;Nir Friedman;Daphne Koller;Benjamin Taskar
#t 2001
#c 19

#index 464305
#* Some Greedy Algorithms for Sparse Nonlinear Regression
#@ Prasanth B. Nair;Arindam Choudhury;Andy J. Keane
#t 2001
#c 19

#index 464306
#* A Multi-Agent Policy-Gradient Approach to Network Routing
#@ Nigel Tao;Jonathan Baxter;Lex Weaver
#t 2001
#c 19

#index 464307
#* Round Robin Rule Learning
#@ Johannes Fürnkranz
#t 2001
#c 19

#index 464433
#* Direct Policy Search using Paired Statistical Tests
#@ Malcolm J. A. Strens;Andrew W. Moore
#t 2001
#c 19

#index 464434
#* Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data
#@ John D. Lafferty;Andrew McCallum;Fernando C. N. Pereira
#t 2001
#c 19

#index 464435
#* Structured Prioritised Sweeping
#@ Richard Dearden
#t 2001
#c 19

#index 464436
#* Multiple-Instance Learning of Real-Valued Data
#@ Robert Amar;Daniel R. Dooly;Sally A. Goldman;Qi Zhang
#t 2001
#c 19

#index 464437
#* On No-Regret Learning, Fictitious Play, and Nash Equilibrium
#@ Amir Jafari;Amy R. Greenwald;David Gondek;Gunes Ercal
#t 2001
#c 19

#index 464438
#* Off-Policy Temporal Difference Learning with Function Approximation
#@ Doina Precup;Richard S. Sutton;Sanjoy Dasgupta
#t 2001
#c 19

#index 464439
#* Learnability of Augmented Naive Bayes in Nonimal Domains
#@ Huajie Zhang;Charles X. Ling
#t 2001
#c 19

#index 464440
#* Boosting Neighborhood-Based Classifiers
#@ Marc Sebban;Richard Nock;Stéphane Lallich
#t 2001
#c 19

#index 464441
#* Efficient algorithms for decision tree cross-validation
#@ Hendrik Blockeel;Jan Struyf
#t 2001
#c 19

#index 464442
#* Symmetry in Markov Decision Processes and its Implications for Single Agent and Multiagent Learning
#@ Martin Zinkevich;Tucker R. Balch
#t 2001
#c 19

#index 464443
#* Using EM to Learn 3D Models of Indoor Environments with Mobile Robots
#@ Yufeng Liu;Rosemary Emery;Deepayan Chakrabarti;Wolfram Burgard;Sebastian Thrun
#t 2001
#c 19

#index 464444
#* Feature selection for high-dimensional genomic microarray data
#@ Eric P. Xing;Michael I. Jordan;Richard M. Karp
#t 2001
#c 19

#index 464445
#* WBCsvm: Weighted Bayesian Classification based on Support Vector Machines
#@ Thomas Gärtner;Peter A. Flach
#t 2001
#c 19

#index 464446
#* Bayesian approaches to failure prediction for disk drives
#@ Greg Hamerly;Charles Elkan
#t 2001
#c 19

#index 464447
#* Smoothed Bootstrap and Statistical Data Cloning for Classifier Evaluation
#@ Gregory Shakhnarovich;Ran El-Yaniv;Yoram Baram
#t 2001
#c 19

#index 464448
#* Scalable Internal-State Policy-Gradient Methods for POMDPs
#@ Douglas Aberdeen;Jonathan Baxter
#t 2002
#c 19

#index 464449
#* Linkage and Autocorrelation Cause Feature Selection Bias in Relational Learning
#@ David Jensen;Jennifer Neville
#t 2002
#c 19

#index 464450
#* A New Statistical Approach to Personal Name Extraction
#@ Zheng Chen;Liu Wenyin;Feng Zhang
#t 2002
#c 19

#index 464451
#* Cranking: Combining Rankings Using Conditional Probability Models on Permutations
#@ Guy Lebanon;John D. Lafferty
#t 2002
#c 19

#index 464452
#* Is Combining Classifiers Better than Selecting the Best One
#@ Saso Dzeroski;Bernard Zenko
#t 2002
#c 19

#index 464453
#* An Analysis of Functional Trees
#@ Joao Gama
#t 2002
#c 19

#index 464454
#* Modeling Auction Price Uncertainty Using Boosting-based Conditional Density Estimation
#@ Robert E. Schapire;Peter Stone;David A. McAllester;Michael L. Littman;János Csirik
#t 2002
#c 19

#index 464455
#* A Boosted Maximum Entropy Model for Learning Text Chunking
#@ Seong-Bae Park;Byoung-Tak Zhang
#t 2002
#c 19

#index 464456
#* Separating Skills from Preference: Using Learning to Program by Reward
#@ Daniel G. Shapiro;Pat Langley
#t 2002
#c 19

#index 464457
#* Adaptive View Validation: A First Step Towards Automatic View Detection
#@ Ion Muslea;Steven Minton;Craig A. Knoblock
#t 2002
#c 19

#index 464458
#* Stock Trading System Using Reinforcement Learning with Cooperative Agents
#@ Jangmin O;Jae Won Lee;Byoung-Tak Zhang
#t 2002
#c 19

#index 464459
#* Modeling for Optimal Probability Prediction
#@ Yong Wang;Ian H. Witten
#t 2002
#c 19

#index 464460
#* Hierarchically Optimal Average Reward Reinforcement Learning
#@ Mohammad Ghavamzadeh;Sridhar Mahadevan
#t 2002
#c 19

#index 464461
#* Learning to Fly by Controlling Dynamic Instabilities
#@ David Stirling
#t 2002
#c 19

#index 464462
#* Investigating the Maximum Likelihood Alternative to TD(lambda)
#@ Fletcher Lu;Relu Patrascu;Dale Schuurmans
#t 2002
#c 19

#index 464463
#* Action Refinement in Reinforcement Learning by Probability Smoothing
#@ Thomas G. Dietterich;Dídac Busquets;Ramon López de Mántaras;Carles Sierra
#t 2002
#c 19

#index 464464
#* Learning k-Reversible Context-Free Grammars from Positive Structural Examples
#@ Tim Oates;Devina Desai;Vinay Bhat
#t 2002
#c 19

#index 464465
#* Incorporating Prior Knowledge into Boosting
#@ Robert E. Schapire;Marie Rochery;Mazin G. Rahim;Narendra Gupta
#t 2002
#c 19

#index 464466
#* Active + Semi-supervised Learning = Robust Multi-View Learning
#@ Ion Muslea;Steven Minton;Craig A. Knoblock
#t 2002
#c 19

#index 464467
#* Sufficient Dimensionality Reduction - A novel Analysis Method
#@ Amir Globerson;Naftali Tishby
#t 2002
#c 19

#index 464468
#* Transformation-Based Regression
#@ Björn Bringmann;Stefan Kramer;Friedrich Neubarth;Hannes Pirker;Gerhard Widmer
#t 2002
#c 19

#index 464469
#* Sparse Bayesian Learning for Regression and Classification using Markov Chain Monte Carlo
#@ Shien-Shin Tham;Arnaud Doucet;Kotagiri Ramamohanarao
#t 2002
#c 19

#index 464470
#* Using Abstract Models of Behaviours to Automatically Generate Reinforcement Learning Hierarchies
#@ Malcolm R. K. Ryan
#t 2002
#c 19

#index 464471
#* Qualitative reverse engineering
#@ Dorian Suc;Ivan Bratko
#t 2002
#c 19

#index 464472
#* Issues in Classifier Evaluation using Optimal Cost Curves
#@ Kai Ming Ting
#t 2002
#c 19

#index 464473
#* Univariate Polynomial Inference by Monte Carlo Message Length Approximation
#@ Leigh J. Fitzgibbon;David L. Dowe;Lloyd Allison
#t 2002
#c 19

#index 464603
#* Mining Both Positive and Negative Association Rules
#@ Xindong Wu;Chengqi Zhang;Shichao Zhang
#t 2002
#c 19

#index 464604
#* Exploiting Relations Among Concepts to Acquire Weakly Labeled Training Data
#@ Joseph Bockhorst;Mark Craven
#t 2002
#c 19

#index 464605
#* Anytime Interval-Valued Outputs for Kernel Machines: Fast Support Vector Machine Classification via Distance Geometry
#@ Dennis DeCoste
#t 2002
#c 19

#index 464606
#* Learning Decision Trees Using the Area Under the ROC Curve
#@ César Ferri;Peter A. Flach;José Hernández-Orallo
#t 2002
#c 19

#index 464607
#* PolicyBlocks: An Algorithm for Creating Useful Macro-Actions in Reinforcement Learning
#@ Marc Pickett;Andrew G. Barto
#t 2002
#c 19

#index 464608
#* From Instance-level Constraints to Space-Level Constraints: Making the Most of Prior Knowledge in Data Clustering
#@ Dan Klein;Sepandar D. Kamvar;Christopher D. Manning
#t 2002
#c 19

#index 464609
#* Learning from Scarce Experience
#@ Leonid Peshkin;Christian R. Shelton
#t 2002
#c 19

#index 464610
#* Exact model averaging with naive Bayesian classifiers
#@ Denver Dash;Gregory F. Cooper
#t 2002
#c 19

#index 464611
#* MMIHMM: Maximum Mutual Information Hidden Markov Models
#@ Nuria Oliver;Ashutosh Garg
#t 2002
#c 19

#index 464612
#* The Perceptron Algorithm with Uneven Margins
#@ Yaoyong Li;Hugo Zaragoza;Ralf Herbrich;John Shawe-Taylor;Jaz S. Kandola
#t 2002
#c 19

#index 464613
#* Competitive Analysis of the Explore/Exploit Tradeoff
#@ John Langford;Martin Zinkevich;Sham Kakade
#t 2002
#c 19

#index 464614
#* Randomized Variable Elimination
#@ David J. Stracuzzi;Paul E. Utgoff
#t 2002
#c 19

#index 464615
#* Diffusion Kernels on Graphs and Other Discrete Input Spaces
#@ Risi Imre Kondor;John D. Lafferty
#t 2002
#c 19

#index 464616
#* Non-Disjoint Discretization for Naive-Bayes Classifiers
#@ Ying Yang;Geoffrey I. Webb
#t 2002
#c 19

#index 464617
#* Descriptive Induction through Subgroup Discovery: A Case Study in a Medical Domain
#@ Dragan Gamberger;Nada Lavrac
#t 2002
#c 19

#index 464618
#* Fast Minimum Training Error Discretization
#@ Tapio Elomaa;Juho Rousu
#t 2002
#c 19

#index 464619
#* Graph-Based Relational Concept Learning
#@ Jesus A. Gonzalez;Lawrence B. Holder;Diane J. Cook
#t 2002
#c 19

#index 464620
#* Learning to Share Distributed Probabilistic Beliefs
#@ Christopher Leckie;Kotagiri Ramamohanarao
#t 2002
#c 19

#index 464621
#* Content-Based Image Retrieval Using Multiple-Instance Learning
#@ Qi Zhang;Sally A. Goldman;Wei Yu;Jason Fritts
#t 2002
#c 19

#index 464622
#* Reinforcement Learning and Shaping: Encouraging Intended Behaviors
#@ Adam Laud;Gerald DeJong
#t 2002
#c 19

#index 464623
#* Feature Subset Selection and Inductive Logic Programming
#@ Érick Alphonse;Stan Matwin
#t 2002
#c 19

#index 464624
#* Approximately Optimal Approximate Reinforcement Learning
#@ Sham Kakade;John Langford
#t 2002
#c 19

#index 464625
#* Inducing Process Models from Continuous Data
#@ Pat Langley;Javier Sánchez;Ljupco Todorovski;Saso Dzeroski
#t 2002
#c 19

#index 464626
#* Learning Decision Rules by Randomized Iterative Local Search
#@ Michael Chisholm;Prasad Tadepalli
#t 2002
#c 19

#index 464627
#* On generalization bounds, projection profile, and margin distribution
#@ Ashutosh Garg;Sariel Har-Peled;Dan Roth
#t 2002
#c 19

#index 464628
#* Model-based Hierarchical Average-reward Reinforcement Learning
#@ Sandeep Seri;Prasad Tadepalli
#t 2002
#c 19

#index 464629
#* A Fast Dual Algorithm for Kernel Logistic Regression
#@ S. Sathiya Keerthi;Kaibo Duan;Shirish K. Shevade;Aun N. Poo
#t 2002
#c 19

#index 464630
#* Discriminative Feature Selection via Multiclass Variable Memory Markov Model
#@ Noam Slonim;Gill Bejerano;Shai Fine;Naftali Tishby
#t 2002
#c 19

#index 464631
#* Semi-supervised Clustering by Seeding
#@ Sugato Basu;Arindam Banerjee;Raymond J. Mooney
#t 2002
#c 19

#index 464632
#* Statistical Behavior and Consistency of Support Vector Machines, Boosting, and Beyond
#@ Tong Zhang
#t 2002
#c 19

#index 464633
#* Multi-Instance Kernels
#@ Thomas Gärtner;Peter A. Flach;Adam Kowalczyk;Alex J. Smola
#t 2002
#c 19

#index 464634
#* Finding an Optimal Gain-Ratio Subset-Split Test for a Set-Valued Attribute in Decision Tree Induction
#@ Fumio Takechi;Einoshin Suzuki
#t 2002
#c 19

#index 464635
#* IEMS - The Intelligent Email Sorter
#@ Elisabeth Crawford;Judy Kay;Eric McCreath
#t 2002
#c 19

#index 464636
#* Discovering Hierarchy in Reinforcement Learning with HEXQ
#@ Bernhard Hengst
#t 2002
#c 19

#index 464637
#* Combining Trainig Set and Test Set Bounds
#@ John Langford
#t 2002
#c 19

#index 464638
#* Markov Chain Monte Carlo Sampling using Direct Search Optimization
#@ Malcolm J. A. Strens;Mark Bernhardt;Nicholas Everett
#t 2002
#c 19

#index 464639
#* Pruning Improves Heuristic Search for Cost-Sensitive Learning
#@ Valentina Bayer Zubek;Thomas G. Dietterich
#t 2002
#c 19

#index 464640
#* Kernels for Semi-Structured Data
#@ Hisashi Kashima;Teruo Koyanagi
#t 2002
#c 19

#index 464641
#* Partially Supervised Classification of Text Documents
#@ Bing Liu;Wee Sun Lee;Philip S. Yu;Xiaoli Li
#t 2002
#c 19

#index 464642
#* Learning Spatial and Temporal Correlation for Navigation in a 2-Dimensional Continuous World
#@ Anand Panangadan;Michael G. Dyer
#t 2002
#c 19

#index 464643
#* A Necessary Condition of Convergence for Reinforcement Learning with Function Approximation
#@ Artur Merke;Ralf Schoknecht
#t 2002
#c 19

#index 464644
#* An Alternate Objective Function for Markovian Fields
#@ Sham Kakade;Yee Whye Teh;Sam T. Roweis
#t 2002
#c 19

#index 464773
#* Integrating Experimentation and Guidance in Relational Reinforcement Learning
#@ Kurt Driessens;Saso Dzeroski
#t 2002
#c 19

#index 464774
#* Syllables and other String Kernel Extensions
#@ Craig Saunders;Hauke Tschach;John Shawe-Taylor
#t 2002
#c 19

#index 464775
#* Representational Upper Bounds of Bayesian Networks
#@ Huajie Zhang;Charles X. Ling
#t 2002
#c 19

#index 464776
#* Towards "Large Margin" Speech Recognizers by Boosting and Discriminative Training
#@ Carsten Meyer;Peter Beyerlein
#t 2002
#c 19

#index 464777
#* Combining Labeled and Unlabeled Data for MultiClass Text Categorization
#@ Rayid Ghani
#t 2002
#c 19

#index 464778
#* Algorithm-Directed Exploration for Model-Based Reinforcement Learning in Factored MDPs
#@ Carlos Guestrin;Relu Patrascu;Dale Schuurmans
#t 2002
#c 19

#index 464779
#* Constraint-based Learning of Long Relational Concepts
#@ Jacques Ales Bianchetti;Céline Rouveirol;Michèle Sebag
#t 2002
#c 19

#index 464780
#* Using Unlabelled Data for Text Classification through Addition of Cluster Parameters
#@ Bhavani Raskutti;Herman L. Ferrá;Adam Kowalczyk
#t 2002
#c 19

#index 464781
#* Refining the Wrapper Approach - Smoothed Error Estimates for Feature Selection
#@ Loo-Nin Teow;Haifeng Liu;Hwee Tou Ng;Eric Yap
#t 2002
#c 19

#index 464782
#* How to Make Stacking Better and Faster While Also Taking Care of an Unknown Weakness
#@ Alexander K. Seewald
#t 2002
#c 19

#index 464783
#* Interpreting and Extending Classical Agglomerative Clustering Algorithms using a Model-Based approach
#@ Sepandar D. Kamvar;Dan Klein;Christopher D. Manning
#t 2002
#c 19

#index 464784
#* Learning word normalization using word suffix and context from unlabeled data
#@ Dunja Mladenic
#t 2002
#c 19

#index 465736
#* Preventing "Overfitting" of Cross-Validation Data
#@ Andrew Y. Ng
#t 1997
#c 19

#index 465737
#* On Learning From Multi-Instance Examples: Empirical Evaluation of a Theoretical Approach
#@ Peter Auer
#t 1997
#c 19

#index 465738
#* Improving Minority Class Prediction Using Case-Specific Feature Weights
#@ Claire Cardie;Nicholas Nowe
#t 1997
#c 19

#index 465739
#* Hierarchical Explanation-Based Reinforcement Learning
#@ Prasad Tadepalli;Thomas G. Dietterich
#t 1997
#c 19

#index 465740
#* Probabilistic Linear Tree
#@ Joao Gama
#t 1997
#c 19

#index 465741
#* Efficient Feature Selection in Conceptual Clustering
#@ Mark Devaney;Ashwin Ram
#t 1997
#c 19

#index 465742
#* A Comparative Study of Inductive Logic Programming Methods for Software Fault Prediction
#@ William W. Cohen;Premkumar T. Devanbu
#t 1997
#c 19

#index 465743
#* Automatic Rule Acquisition for Spelling Correction
#@ Lidia Mangu;Eric Brill
#t 1997
#c 19

#index 465744
#* The Effective Size of a Neural Network: A Principal Component Approach
#@ David W. Opitz
#t 1997
#c 19

#index 465745
#* A Bayesian Approach to Model Learning in Non-Markovian Environments
#@ Nobuo Suematsu;Akira Hayashi;Shigang Li
#t 1997
#c 19

#index 465746
#* Boosting the margin: A new explanation for the effectiveness of voting methods
#@ Robert E. Schapire;Yoav Freund;Peter Barlett;Wee Sun Lee
#t 1997
#c 19

#index 465747
#* Hierarchically Classifying Documents Using Very Few Words
#@ Daphne Koller;Mehran Sahami
#t 1997
#c 19

#index 465748
#* Machine Learning by Function Decomposition
#@ Blaz Zupan;Marko Bohanec;Ivan Bratko;Janez Demsar
#t 1997
#c 19

#index 465749
#* Improving Regressors using Boosting Techniques
#@ Harris Drucker
#t 1997
#c 19

#index 465750
#* FONN: Combining First Order Logic with Connectionist Learning
#@ Marco Botta;Attilio Giordana;Roberto Piola
#t 1997
#c 19

#index 465751
#* Using output codes to boost multiclass learning problems
#@ Robert E. Schapire
#t 1997
#c 19

#index 465752
#* Pessimistic decision tree pruning based Continuous-time
#@ Yishay Mansour
#t 1997
#c 19

#index 465753
#* The Effects of Training Set Size on Decision Tree Complexity
#@ Tim Oates;David Jensen
#t 1997
#c 19

#index 465754
#* A Comparative Study on Feature Selection in Text Categorization
#@ Yiming Yang;Jan O. Pedersen
#t 1997
#c 19

#index 465755
#* Knowledge Acquisition form Examples Vis Multiple Models
#@ Pedro Domingos
#t 1997
#c 19

#index 465756
#* Feature Engineering and Classifier Selection: A Case Study in Venusian Volcano Detection
#@ Lars Asker;Richard Maclin
#t 1997
#c 19

#index 465757
#* ARCCHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods
#@ Filippo Menczer
#t 1997
#c 19

#index 465758
#* An adaptation of Relief for attribute estimation in regression
#@ Marko Robnik-Sikonja;Igor Kononenko
#t 1997
#c 19

#index 465759
#* Reinforcement Learning in POMDPs with Function Approximation
#@ Hajime Kimura;Kazuteru Miyazaki;Shigenobu Kobayashi
#t 1997
#c 19

#index 465760
#* Instance Pruning Techniques
#@ D. Randall Wilson;Tony R. Martinez
#t 1997
#c 19

#index 465761
#* Learning Goal-Decomposition Rules using Exercises
#@ Chandra Reddy;Prasad Tadepalli
#t 1997
#c 19

#index 465762
#* Learning Belief Networks in the Presence of Missing Values and Hidden Variables
#@ Nir Friedman
#t 1997
#c 19

#index 465763
#* The Canonical Distortion Measure for Vector Quantization and Function Approximation
#@ Jonathan Baxter
#t 1997
#c 19

#index 465764
#* Stacking Bagged and Dagged Models
#@ Kai Ming Ting;Ian H. Witten
#t 1997
#c 19

#index 465765
#* Learning String Edit Distance
#@ Eric Sven Ristad;Peter N. Yianilos
#t 1997
#c 19

#index 465890
#* PAC Learning with Constant-Partition Classification Noise and Applications to Decision Tree Induction
#@ Scott E. Decatur
#t 1997
#c 19

#index 465891
#* Efficient Locally Weighted Polynomial Regression Predictions
#@ Andrew W. Moore;Jeff Schneider;Kan Deng
#t 1997
#c 19

#index 465892
#* Using Optimal Dependency-Trees for Combinational Optimization
#@ Shumeet Baluja;Scott Davies
#t 1997
#c 19

#index 465893
#* Expected Mistake Bound Model for On-Line Reinforcement Learning
#@ Claude-Nicolas Fiechter
#t 1997
#c 19

#index 465894
#* Integrating Feature Construction with Multiple Classifiers in Decision Tree Induction
#@ Ricardo Vilalta;Larry A. Rendell
#t 1997
#c 19

#index 465895
#* A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization
#@ Thorsten Joachims
#t 1997
#c 19

#index 465896
#* Declarative Bias in Equation Discovery
#@ Ljupco Todorovski;Saso Dzeroski
#t 1997
#c 19

#index 465897
#* Functional Models for Regression Tree Leaves
#@ Luís Torgo
#t 1997
#c 19

#index 465898
#* Exponentiated Gradient Methods for Reinforcement Learning
#@ Doina Precup;Richard S. Sutton
#t 1997
#c 19

#index 465899
#* Characterizing the generalization performance of model selection strategies
#@ Dale Schuurmans;Lyle H. Ungar;Dean P. Foster
#t 1997
#c 19

#index 465900
#* On the Decomposition of Polychotomies into Dichotomies
#@ Eddy Mayoraz;Miguel Moreira
#t 1997
#c 19

#index 465901
#* Predicting Multiprocessor Memory Access Patterns with Learning Models
#@ M. F. Sakr;Steven P. Levitan;Donald M. Chiarulli;Bill G. Horne;C. Lee Giles
#t 1997
#c 19

#index 465902
#* Robot Learning From Demonstration
#@ Christopher G. Atkeson;Stefan Schaal
#t 1997
#c 19

#index 465903
#* Why Experimentation can be better than "Perfect Guidance"
#@ Tobias Scheffer;Russell Greiner;Christian Darken
#t 1997
#c 19

#index 465904
#* Bayesian Network Classification with Continuous Attributes: Getting the Best of Both Discretization and Parametric Fitting
#@ Nir Friedman;Moisés Goldszmidt;Thomas J. Lee
#t 1998
#c 19

#index 465905
#* On Feature Selection: Learning with Exponentially Many Irrelevant Features as Training Examples
#@ Andrew Y. Ng
#t 1998
#c 19

#index 465906
#* Collaborative Filtering Using Weighted Majority Prediction Algorithms
#@ Atsuyoshi Nakamura;Naoki Abe
#t 1998
#c 19

#index 465907
#* Q2: Memory-Based Active Learning for Optimizing Noisy Continuous Functions
#@ Andrew W. Moore;Jeff G. Schneider;Justin A. Boyan;Mary S. Lee
#t 1998
#c 19

#index 465908
#* Local Cascade Generalization
#@ Joao Gama
#t 1998
#c 19

#index 465909
#* Learning to Locate an Object in 3D Space from a Sequence of Camera Images
#@ Dimitris Margaritis;Sebastian Thrun
#t 1998
#c 19

#index 465910
#* Value Function Based Production Scheduling
#@ Jeff G. Schneider;Justin A. Boyan;Andrew W. Moore
#t 1998
#c 19

#index 465911
#* A Supra-Classifier Architecture for Scalable Knowledge Reuse
#@ Kurt D. Bollacker;Joydeep Ghosh
#t 1998
#c 19

#index 465912
#* Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets
#@ Stephen D. Bay
#t 1998
#c 19

#index 465913
#* Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm
#@ Junling Hu;Michael P. Wellman
#t 1998
#c 19

#index 465914
#* An Information-Theoretic Definition of Similarity
#@ Dekang Lin
#t 1998
#c 19

#index 465915
#* Multi-criteria Reinforcement Learning
#@ Zoltán Gábor;Zsolt Kalmár;Csaba Szepesvári
#t 1998
#c 19

#index 465916
#* Multiple-Instance Learning for Natural Scene Classification
#@ Oded Maron;Aparna Lakshmi Ratan
#t 1998
#c 19

#index 465917
#* KnightCap: A Chess Programm That Learns by Combining TD(lambda) with Game-Tree Search
#@ Jonathan Baxter;Andrew Tridgell;Lex Weaver
#t 1998
#c 19

#index 465918
#* Using Learning for Approximation in Stochastic Processes
#@ Daphne Koller;Raya Fratkina
#t 1998
#c 19

#index 465919
#* Multistrategy Learning for Information Extraction
#@ Dayne Freitag
#t 1998
#c 19

#index 465920
#* Teaching an Agent to Test Students
#@ Gheorghe Tecuci;Harry Keeling
#t 1998
#c 19

#index 465921
#* Automatic Segmentation of Continuous Trajectories with Invariance to Nonlinear Warpings of Time
#@ Lawrence K. Saul
#t 1998
#c 19

#index 465922
#* Generating Accurate Rule Sets Without Global Optimization
#@ Eibe Frank;Ian H. Witten
#t 1998
#c 19

#index 465923
#* A Learning Rate Analysis of Reinforcement Learning Algorithms in Finite-Horizon
#@ Frédérick Garcia;Seydina M. Ndiaye
#t 1998
#c 19

#index 465924
#* Structural Machine Learning with Galois Lattice and Graphs
#@ Michael Liquiere;Jean Sallantin
#t 1998
#c 19

#index 465925
#* Genetic Programming and Deductive-Inductive Learning: A Multi-Strategy Approach
#@ Ricardo Aler;Daniel Borrajo;Pedro Isasi
#t 1998
#c 19

#index 465926
#* An Analysis of Direct Reinforcement Learning in Non-Markovian Domains
#@ Mark D. Pendrith;Michael McGarity
#t 1998
#c 19

#index 465927
#* A Process-Oriented Heuristic for Model Selection
#@ Pedro Domingos
#t 1998
#c 19

#index 465928
#* Learning Collaborative Information Filters
#@ Daniel Billsus;Michael J. Pazzani
#t 1998
#c 19

#index 465929
#* A Case Study in the Use of Theory Revision in Requirements Validation
#@ T. L. McCluskey;M. M. West
#t 1998
#c 19

#index 465930
#* Theory Refinement of Bayesian Networks with Hidden Variables
#@ Sowmya Ramachandran;Raymond J. Mooney
#t 1998
#c 19

#index 465931
#* Learning Sorting and Decision Trees with POMDPs
#@ Blai Bonet;Héctor Geffner
#t 1998
#c 19

#index 466062
#* Bayesian Classifiers Are Large Margin Hyperplanes in a Hilbert Space
#@ Nello Cristianini;John Shawe-Taylor;Peter Sykacek
#t 1998
#c 19

#index 466063
#* An Experimental Evaluation of Coevolutive Concept Learning
#@ Cosimo Anglano;Attilio Giordana;Giuseppe Lo Bello;Lorenza Saitta
#t 1998
#c 19

#index 466064
#* Well-Behaved Borgs, Bolos, and Berserkers
#@ Diana F. Gordon
#t 1998
#c 19

#index 466065
#* A Neural Network Model for Prognostic Prediction
#@ W. Nick Street
#t 1998
#c 19

#index 466066
#* The MAXQ Method for Hierarchical Reinforcement Learning
#@ Thomas G. Dietterich
#t 1998
#c 19

#index 466067
#* Learning First-Order Acyclic Horn Programs from Entailment
#@ Chandra Reddy;Prasad Tadepalli
#t 1998
#c 19

#index 466068
#* Using a Permutation Test for Attribute Selection in Decision Trees
#@ Eibe Frank;Ian H. Witten
#t 1998
#c 19

#index 466069
#* Using Eligibility Traces to Find the Best Memoryless Policy in Partially Observable Markov Decision Processes
#@ John Loch;Satinder P. Singh
#t 1998
#c 19

#index 466070
#* Intra-Option Learning about Temporally Abstract Actions
#@ Richard S. Sutton;Doina Precup;Satinder P. Singh
#t 1998
#c 19

#index 466071
#* RL-TOPS: An Architecture for Modularity and Re-Use in Reinforcement Learning
#@ Malcolm R. K. Ryan;Mark D. Pendrith
#t 1998
#c 19

#index 466072
#* Heading in the Right Direction
#@ Hagit Shatkay;Leslie Pack Kaelbling
#t 1998
#c 19

#index 466073
#* Top-Down Induction of Clustering Trees
#@ Hendrik Blockeel;Luc De Raedt;Jan Ramon
#t 1998
#c 19

#index 466074
#* A Fast, Bottom-Up Decision Tree Pruning Algorithm with Near-Optimal Generalization
#@ Michael J. Kearns;Yishay Mansour
#t 1998
#c 19

#index 466075
#* Near-Optimal Reinforcement Learning in Polynominal Time
#@ Michael J. Kearns;Satinder P. Singh
#t 1998
#c 19

#index 466076
#* The Problem with Noise and Small Disjuncts
#@ Gary M. Weiss;Haym Hirsh
#t 1998
#c 19

#index 466077
#* Relational Reinforcement Learning
#@ Saso Dzeroski;Luc De Raedt;Hendrik Blockeel
#t 1998
#c 19

#index 466078
#* Improving Text Classification by Shrinkage in a Hierarchy of Classes
#@ Andrew McCallum;Ronald Rosenfeld;Tom M. Mitchell;Andrew Y. Ng
#t 1998
#c 19

#index 466079
#* Stochastic Resonance with Adaptive Fuzzy Systems
#@ Sanya Mitaim;Bart Kosko
#t 1998
#c 19

#index 466080
#* Solving a Huge Number of Similar Tasks: A Combination of Multi-Task Learning and a Hierarchical Bayesian Approach
#@ Tom Heskes
#t 1998
#c 19

#index 466081
#* Ridge Regression Learning Algorithm in Dual Variables
#@ Craig Saunders;Alexander Gammerman;Volodya Vovk
#t 1998
#c 19

#index 466082
#* Evolving Structured Programs with Hierarchical Instructions and Skip Nodes
#@ Rafal Salustowicz;Jürgen Schmidhuber
#t 1998
#c 19

#index 466083
#* Refining Initial Points for K-Means Clustering
#@ Paul S. Bradley;Usama M. Fayyad
#t 1998
#c 19

#index 466084
#* Feature Selection via Concave Minimization and Support Vector Machines
#@ Paul S. Bradley;O. L. Mangasarian
#t 1998
#c 19

#index 466085
#* Coevolutionary Learning: A Case Study
#@ Hugues Juillé;Jordan B. Pollack
#t 1998
#c 19

#index 466086
#* The Case against Accuracy Estimation for Comparing Induction Algorithms
#@ Foster J. Provost;Tom Fawcett;Ron Kohavi
#t 1998
#c 19

#index 466087
#* The Kernel-Adatron Algorithm: A Fast and Simple Learning Procedure for Support Vector Machines
#@ Thilo-Thomas Frieß;Nello Cristianini;Colin Campbell
#t 1998
#c 19

#index 466088
#* An Analysis of Actor/Critic Algorithms Using Eligibility Traces: Reinforcement Learning with Imperfect Value Function
#@ Hajime Kimura;Shigenobu Kobayashi
#t 1998
#c 19

#index 466089
#* A Randomized ANOVA Procedure for Comparing Performance Curves
#@ Justus H. Piater;Paul R. Cohen;Xiaoqin Zhang;Michael Atighetchi
#t 1998
#c 19

#index 466090
#* Finite-Time Regret Bounds for the Multiarmed Bandit Problem
#@ Nicolò Cesa-Bianchi;Paul Fischer
#t 1998
#c 19

#index 466091
#* Classification Using Phi-Machines and Constructive Function Approximation
#@ Doina Precup;Paul E. Utgoff
#t 1998
#c 19

#index 466092
#* Learning a Language-Independent Representation for Terms from a Partially Aligned Corpus
#@ Michael L. Littman;Fan Jiang;Greg A. Keim
#t 1998
#c 19

#index 466093
#* An Investigation of Transformation-Based Learning in Discourse
#@ Ken Samuel;Sandra Carberry;K. Vijay-Shanker
#t 1998
#c 19

#index 466094
#* On the Power of Decision Lists
#@ Richard Nock;Pascal Jappy
#t 1998
#c 19

#index 466095
#* Query Learning Strategies Using Boosting and Bagging
#@ Naoki Abe;Hiroshi Mamitsuka
#t 1998
#c 19

#index 466096
#* Discriminant Trees
#@ Joao Gama
#t 1999
#c 19

#index 466097
#* Boosting a Strong Learner: Evidence Against the Minimum Margin
#@ Michael Bonnell Harries
#t 1999
#c 19

#index 466098
#* A Minimum Risk Metric for Nearest Neighbor Classification
#@ Enrico Blanzieri;Francesco Ricci
#t 1999
#c 19

#index 466099
#* Detecting Motifs from Sequences
#@ Yuh-Jyh Hu;Suzanne B. Sandmeyer;Dennis F. Kibler
#t 1999
#c 19

#index 466100
#* Hierarchical Optimization of Policy-Coupled Semi-Markov Decision Processes
#@ Gang Wang;Sridhar Mahadevan
#t 1999
#c 19

#index 466101
#* Feature Engineering for Text Classification
#@ Sam Scott;Stan Matwin
#t 1999
#c 19

#index 466229
#* Combining Statistical Learning with a Knowledge-Based Approach - A Case Study in Intensive Care Monitoring
#@ Katharina Morik;Peter Brockhausen;Thorsten Joachims
#t 1999
#c 19

#index 466230
#* Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping
#@ Andrew Y. Ng;Daishi Harada;Stuart J. Russell
#t 1999
#c 19

#index 466231
#* Active Learning for Natural Language Parsing and Information Extraction
#@ Cynthia A. Thompson;Mary Elaine Califf;Raymond J. Mooney
#t 1999
#c 19

#index 466232
#* Monte Carlo Hidden Markov Models: Learning Non-Parametric Models of Partially Observable Stochastic Processes
#@ Sebastian Thrun;John C. Langford;Dieter Fox
#t 1999
#c 19

#index 466233
#* Instance-Family Abstraction in Memory-Based Language Learning
#@ Antal van den Bosch
#t 1999
#c 19

#index 466234
#* Making Better Use of Global Discretization
#@ Eibe Frank;Ian H. Witten
#t 1999
#c 19

#index 466235
#* Least-Squares Temporal Difference Learning
#@ Justin A. Boyan
#t 1999
#c 19

#index 466236
#* Correcting Noisy Data
#@ Choh-Man Teng
#t 1999
#c 19

#index 466237
#* Distributed Robotic Learning: Adaptive Behavior Acquisition for Distributed Autonomous Swimming Robot in Real World
#@ Daisuke Iijima;Wenwei Yu;Hiroshi Yokoi;Yukinori Kakazu
#t 1999
#c 19

#index 466238
#* Learning to Ride a Bicycle using Iterated Phantom Induction
#@ Mark Brodie;Gerald DeJong
#t 1999
#c 19

#index 466239
#* Large Margin Trees for Induction and Transduction
#@ Donghui Wu;Kristin P. Bennett;Nello Cristianini;John Shawe-Taylor
#t 1999
#c 19

#index 466240
#* The Alternating Decision Tree Learning Algorithm
#@ Yoav Freund;Llew Mason
#t 1999
#c 19

#index 466241
#* Approximation Via Value Unification
#@ Paul E. Utgoff;David J. Stracuzzi
#t 1999
#c 19

#index 466242
#* Implicit Imitation in Multiagent Reinforcement Learning
#@ Bob Price;Craig Boutilier
#t 1999
#c 19

#index 466243
#* Hierarchical Models for Screening of Iron Deficiency Anemia
#@ Igor V. Cadez;Christine E. McLaren;Padhraic Smyth;Geoffrey J. McLachlan
#t 1999
#c 19

#index 466244
#* Attribute Dependencies, Understandability and Split Selection in Tree Based Models
#@ Marko Robnik-Sikonja;Igor Kononenko
#t 1999
#c 19

#index 466245
#* Combining Error-Driven Pruning and Classification for Partial Parsing
#@ Claire Cardie;Scott Mardis;David Pierce
#t 1999
#c 19

#index 466246
#* Expected Error Analysis for Model Selection
#@ Tobias Scheffer;Thorsten Joachims
#t 1999
#c 19

#index 466247
#* Simple DFA are Polynomially Probably Exactly Learnable from Simple Examples
#@ Rajesh Parekh;Vasant G. Honavar
#t 1999
#c 19

#index 466248
#* Learning Hierarchical Performance Knowledge by Observation
#@ Michael van Lent;John E. Laird
#t 1999
#c 19

#index 466249
#* Experiments with Noise Filtering in a Medical Domain
#@ Dragan Gamberger;Nada Lavrac;Ciril Groselj
#t 1999
#c 19

#index 466250
#* Using Reinforcement Learning to Spider the Web Efficiently
#@ Jason Rennie;Andrew McCallum
#t 1999
#c 19

#index 466251
#* An Region-Based Learning Approach to Discovering Temporal Structures in Data
#@ Wei Zhang
#t 1999
#c 19

#index 466252
#* Lazy Bayesian Rules: A Lazy Semi-Naive Bayesian Learning Technique Competitive to Boosting Decision Trees
#@ Zijian Zheng;Geoffrey I. Webb;Kai Ming Ting
#t 1999
#c 19

#index 466253
#* A Hybrid Lazy-Eager Approach to Reducing the Computation and Memory Requirements of Local Parametric Learning Algorithms
#@ Yuanhui Zhou;Carla E. Brodley
#t 1999
#c 19

#index 466254
#* Learning Discriminatory and Descriptive Rules by an Inductive Logic Programming System
#@ Maziar Palhang;Arcot Sowmya
#t 1999
#c 19

#index 466255
#* Sonar-Based Mapping of Large-Scale Mobile Robot Environments using EM
#@ Wolfram Burgard;Dieter Fox;Hauke Jans;Christian Matenar;Sebastian Thrun
#t 1999
#c 19

#index 466256
#* Abstracting from Robot Sensor Data using Hidden Markov Models
#@ Laura Firoiu;Paul R. Cohen
#t 1999
#c 19

#index 466257
#* An Accelerated Chow and Liu Algorithm: Fitting Tree Distributions to High-Dimensional Sparse Data
#@ Marina Meila
#t 1999
#c 19

#index 466258
#* Machine-Learning Applications of Algorithmic Randomness
#@ Volodya Vovk;Alexander Gammerman;Craig Saunders
#t 1999
#c 19

#index 466259
#* Learning Policies with External Memory
#@ Leonid Peshkin;Nicolas Meuleau;Leslie Pack Kaelbling
#t 1999
#c 19

#index 466260
#* Learning Comprehensible Descriptions of Multivariate Time Series
#@ Mohammed Waleed Kadous
#t 1999
#c 19

#index 466261
#* On Some Misbehaviour of Back-Propagation with Non-Normalized RBFNs and a Solution
#@ Attilio Giordana;Roberto Piola
#t 1999
#c 19

#index 466262
#* Distributed Value Functions
#@ Jeff G. Schneider;Weng-Keen Wong;Andrew W. Moore;Martin A. Riedmiller
#t 1999
#c 19

#index 466263
#* Transductive Inference for Text Classification using Support Vector Machines
#@ Thorsten Joachims
#t 1999
#c 19

#index 466264
#* OPT-KD: An Algorithm for Optimizing Kd-Trees
#@ Douglas A. Talbert;Douglas H. Fisher
#t 1999
#c 19

#index 466265
#* Efficient Non-Linear Control by Combining Q-learning with Local Linear Controllers
#@ Hajime Kimura;Shigenobu Kobayashi
#t 1999
#c 19

#index 466266
#* Feature Selection for Unbalanced Class Distribution and Naive Bayes
#@ Dunja Mladenic;Marko Grobelnik
#t 1999
#c 19

#index 466267
#* GA-based Learning of Context-Free Grammars using Tabular Representations
#@ Yasubumi Sakakibara;Mitsuhiro Kondo
#t 1999
#c 19

#index 466268
#* AdaCost: Misclassification Cost-Sensitive Boosting
#@ Wei Fan;Salvatore J. Stolfo;Junxin Zhang;Philip K. Chan
#t 1999
#c 19

#index 466269
#* Feature Selection as a Preprocessing Step for Hierarchical Clustering
#@ Luis Talavera
#t 1999
#c 19

#index 466394
#* Learning User Evaluation Functions for Adaptive Scheduling Assistance
#@ Melinda T. Gervasio;Wayne Iba;Pat Langley
#t 1999
#c 19

#index 466395
#* Model Selection in Unsupervised Learning with Applications To Document Clustering
#@ Shivakumar Vaithyanathan;Byron Dom
#t 1999
#c 19

#index 466396
#* Associative Reinforcement Learning using Linear Probabilistic Concepts
#@ Naoki Abe;Philip M. Long
#t 1999
#c 19

#index 466397
#* Learning to Optimally Schedule Internet Banner Advertisements
#@ Naoki Abe;Atsuyoshi Nakamura
#t 1999
#c 19

#index 466398
#* Characterizing Model Erros and Differences
#@ Stephen D. Bay;Michael J. Pazzani
#t 2000
#c 19

#index 466399
#* Localizing Policy Gradient Estimates to Action Transition
#@ Gregory Z. Grudic;Lyle H. Ungar
#t 2000
#c 19

#index 466400
#* Selective Voting for Perception-like Online Learning
#@ Yi Li
#t 2000
#c 19

#index 466401
#* A Unifeid Bias-Variance Decomposition and its Applications
#@ Pedro Domingos
#t 2000
#c 19

#index 466402
#* Rates of Convergence for Variable Resolution Schemes in Optimal Control
#@ Rémi Munos;Andrew W. Moore
#t 2000
#c 19

#index 466403
#* Constructive Feature Learning and the Development of Visual Expertise
#@ Justus H. Piater;Roderic A. Grupen
#t 2000
#c 19

#index 466404
#* Mixtures of Factor Analyzers
#@ Geoffrey J. McLachlan;David Peel
#t 2000
#c 19

#index 466405
#* A Boosting Approach to Topic Spotting on Subdialogues
#@ Kary Myers;Michael J. Kearns;Satinder P. Singh;Marilyn A. Walker
#t 2000
#c 19

#index 466406
#* Discovering Homogeneous Regions in Spatial Data through Competition
#@ Slobodan Vucetic;Zoran Obradovic
#t 2000
#c 19

#index 466407
#* An Evolutionary Approach to Evidence-Based Learning of Deterministic Finite Automata
#@ Stefan Veeser
#t 2000
#c 19

#index 466408
#* Detecting Concept Drift with Support Vector Machines
#@ Ralf Klinkenberg;Thorsten Joachims
#t 2000
#c 19

#index 466409
#* Comparing the Minimum Description Length Principle and Boosting in the Automatic Analysis of Discourse
#@ Tadashi Nomoto;Yuji Matsumoto
#t 2000
#c 19

#index 466410
#* Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning
#@ Mark A. Hall
#t 2000
#c 19

#index 466411
#* Shaping in Reinforcement Learning by Changing the Physics of the Problem
#@ Jette Randløv
#t 2000
#c 19

#index 466412
#* A Dynamic Adaptation of AD-trees for Efficient Machine Learning on Large Data Sets
#@ Paul Komarek;Andrew W. Moore
#t 2000
#c 19

#index 466413
#* An Initial Study of an Adaptive Hierarchical Vision System
#@ Marcus A. Maloof
#t 2000
#c 19

#index 466414
#* Feature Subset Selection and Order Identification for Unsupervised Learning
#@ Jennifer G. Dy;Carla E. Brodley
#t 2000
#c 19

#index 466415
#* Experimental Results on Q-Learning for General-Sum Stochastic Games
#@ Junling Hu;Michael P. Wellman
#t 2000
#c 19

#index 466416
#* A Bayesian Approach to Temporal Data Clustering using Hidden Markov Models
#@ Cen Li;Gautam Biswas
#t 2000
#c 19

#index 466417
#* Pseudo-convergent Q-Learning by Competitive Pricebots
#@ Jeffrey O. Kephart;Gerald Tesauro
#t 2000
#c 19

#index 466418
#* Algorithms for Inverse Reinforcement Learning
#@ Andrew Y. Ng;Stuart J. Russell
#t 2000
#c 19

#index 466419
#* Less is More: Active Learning with Support Vector Machines
#@ Greg Schohn;David Cohn
#t 2000
#c 19

#index 466420
#* Machine Learning for Subproblem Selection
#@ Robert Moll;Theodore J. Perkins;Andrew G. Barto
#t 2000
#c 19

#index 466421
#* Combining Reinforcement Learning with a Local Control Algorithm
#@ Jette Randløv;Andrew G. Barto;Michael T. Rosenstein
#t 2000
#c 19

#index 466422
#* Analyzing Relational Learning in the Phase Transition Framework
#@ Attilio Giordana;Lorenza Saitta;Michèle Sebag;Marco Botta
#t 2000
#c 19

#index 466423
#* State-based Classification of Finger Gestures from Electromyographic Signals
#@ Peter Ju;Leslie Pack Kaelbling;Yoram Singer
#t 2000
#c 19

#index 466424
#* Learning Bayesian Networks for Diverse and Varying numbers of Evidence Sets
#@ Zu Whan Kim;Ramakant Nevatia
#t 2000
#c 19

#index 466425
#* X-means: Extending K-means with Efficient Estimation of the Number of Clusters
#@ Dan Pelleg;Andrew W. Moore
#t 2000
#c 19

#index 466426
#* The Space of Jumping Emerging Patterns and Its Incremental Maintenance Algorithms
#@ Jinyan Li;Kotagiri Ramamohanarao;Guozhu Dong
#t 2000
#c 19

#index 466427
#* Clustering the Users of Large Web Sites into Communities
#@ Georgios Paliouras;Christos Papatheodorou;Vangelis Karkaletsis;Constantine D. Spyropoulos
#t 2000
#c 19

#index 466428
#* Locally Weighted Projection Regression: Incremental Real Time Learning in High Dimensional Space
#@ Sethu Vijayakumar;Stefan Schaal
#t 2000
#c 19

#index 466429
#* Why Discretization Works for Naive Bayesian Classifiers
#@ Chun-Nan Hsu;Hung-Ju Huang;Tzu-Tsung Wong
#t 2000
#c 19

#index 466430
#* Learning Subjective Functions with Large Margins
#@ Claude-Nicolas Fiechter;Seth Rogers
#t 2000
#c 19

#index 466431
#* A Divide and Conquer Approach to Learning from Prior Knowledge
#@ Eric Chown;Thomas G. Dietterich
#t 2000
#c 19

#index 466432
#* Induction of Concept Hierarchies from Noisy Data
#@ Blaz Zupan;Ivan Bratko;Marko Bohanec;Janez Demsar
#t 2000
#c 19

#index 466433
#* Bounds on the Generalization Performance of Kernel Machine Ensembles
#@ Theodoros Evgeniou;Luis Perez-Breva;Massimiliano Pontil;Tomaso Poggio
#t 2000
#c 19

#index 466434
#* Challenges of the Email Domain for Text Classification
#@ Jake D. Brutlag;Christopher Meek
#t 2000
#c 19

#index 466435
#* Multi-agent Q-learning and Regression Trees for Automated Pricing Decisions
#@ Manu Sridharan;Gerald Tesauro
#t 2000
#c 19

#index 466436
#* Automatic Identification of Mathematical Concepts
#@ Simon Colton;Alan Bundy;Toby Walsh
#t 2000
#c 19

#index 466558
#* An Approach to Data Reduction and Clustering with Theoretical Guarantees
#@ Partha Niyogi;Narendra Karmarkar
#t 2000
#c 19

#index 466559
#* Using Learning by Discovery to Segment Remotely Sensed Images
#@ Leen-Kiat Soh;Costas Tsatsoulis
#t 2000
#c 19

#index 466560
#* Image Color Constancy Using EM and Cached Statistics
#@ Charles R. Rosenberg
#t 2000
#c 19

#index 466561
#* A Comparative Study of Cost-Sensitive Boosting Algorithms
#@ Kai Ming Ting
#t 2000
#c 19

#index 466562
#* Instance Pruning as an Information Preserving Problem
#@ Marc Sebban;Richard Nock
#t 2000
#c 19

#index 466563
#* Finding Variational Structure in Data by Cross-Entropy Optimization
#@ Matthew Brand
#t 2000
#c 19

#index 466564
#* Incremental Learning in SwiftFile
#@ Richard Segal;Jeffrey O. Kephart
#t 2000
#c 19

#index 466565
#* Learning to Select Text Databases with Neural Nets
#@ Yong S. Choi;Suk I. Yoo
#t 2000
#c 19

#index 466566
#* Discovering the Structure of Partial Differential Equations from Example Behaviour
#@ Ljupco Todorovski;Saso Dzeroski;Ashwin Srinivasan;Jonathan Whiteley;David Gavaghan
#t 2000
#c 19

#index 466567
#* Approximate Dimension Equalization in Vector-based Information Retrieval
#@ Fan Jiang;Michael L. Littman
#t 2000
#c 19

#index 466568
#* Bootstrap Methods for the Cost-Sensitive Evaluation of Classifiers
#@ Dragos D. Margineantu;Thomas G. Dietterich
#t 2000
#c 19

#index 466569
#* Adaptive Resolution Model-Free Reinforcement Learning: Decision Boundary Partitioning
#@ Stuart I. Reynolds
#t 2000
#c 19

#index 466570
#* Classification with Multiple Latent Variable Models using Maximum Entropy Discrimination
#@ Machiel Westerdijk;Wim Wiegerinck
#t 2000
#c 19

#index 466571
#* Generalized Average-Case Analyses of the Nearest Neighbor Algorithm
#@ Seishi Okamoto;Nobuhiro Yugami
#t 2000
#c 19

#index 466572
#* Combining Multiple Learning Strategies for Effective Cross Validation
#@ Yiming Yang;Tom Ault;Thomas Pierce
#t 2000
#c 19

#index 466573
#* Automatically Extracting Features for Concept Learning from the Web
#@ William W. Cohen
#t 2000
#c 19

#index 466574
#* Learning to Probabilistically Identify Authoritative Documents
#@ David Cohn;Huan Chang
#t 2000
#c 19

#index 466575
#* Practical Reinforcement Learning in Continuous Spaces
#@ William D. Smart;Leslie Pack Kaelbling
#t 2000
#c 19

#index 466576
#* Query Learning with Large Margin Classifiers
#@ Colin Campbell;Nello Cristianini;Alex J. Smola
#t 2000
#c 19

#index 466577
#* Dimension Reduction Techniques for Training Polynomial Networks
#@ William M. Campbell;Kari Torkkola;Sreeream V. Balakrishnan
#t 2000
#c 19

#index 466578
#* Efficient Learning Through Evolution: Neural Programming and Internal Reinforcement
#@ Astro Teller;Manuela M. Veloso
#t 2000
#c 19

#index 466579
#* Ideal Theory Refinement under Object Identity
#@ Floriana Esposito;Nicola Fanizzi;Stefano Ferilli;Giovanni Semeraro
#t 2000
#c 19

#index 466580
#* Improving Short-Text Classification using Unlabeled Data for Classification Problems
#@ Sarah Zelikovitz;Haym Hirsh
#t 2000
#c 19

#index 466581
#* Knowledge Representation Issues in Control Knowledge Learning
#@ Ricardo Aler;Daniel Borrajo;Pedro Isasi
#t 2000
#c 19

#index 466582
#* On-line Learning for Humanoid Robot Systems
#@ Jörg Conradt;Gaurav Tevatia;Sethu Vijayakumar;Stefan Schaal
#t 2000
#c 19

#index 466583
#* Bayesian Averaging of Classifiers and the Overfitting Problem
#@ Pedro Domingos
#t 2000
#c 19

#index 466584
#* Learning to Predict Performance from Formula Modeling and Training Data
#@ Bryan Singer;Manuela M. Veloso
#t 2000
#c 19

#index 466585
#* Combining Multiple Perspectives
#@ Bikramit Banerjee;Sandip Debnath;Sandip Sen
#t 2000
#c 19

#index 466586
#* Efficient Mining from Large Databases by Query Learning
#@ Hiroshi Mamitsuka;Naoki Abe
#t 2000
#c 19

#index 466587
#* Learning Horn Expressions with LogAn-H
#@ Roni Khardon
#t 2000
#c 19

#index 466588
#* Mutual Information in Learning Feature Transformations
#@ Kari Torkkola;William M. Campbell
#t 2000
#c 19

#index 466589
#* Duality and Geometry in SVM Classifiers
#@ Kristin P. Bennett;Erin J. Bredensteiner
#t 2000
#c 19

#index 466590
#* Version Space Algebra and its Application to Programming by Demonstration
#@ Tessa A. Lau;Pedro Domingos;Daniel S. Weld
#t 2000
#c 19

#index 466591
#* Model Selection Criteria for Learning Belief Nets: An Empirical Comparison
#@ Tim Van Allen;Russell Greiner
#t 2000
#c 19

#index 466592
#* Classification of Individuals with Complex Structure
#@ Antony F. Bowers;Christophe G. Giraud-Carrier;John W. Lloyd
#t 2000
#c 19

#index 466593
#* Probabilistic DFA Inference using Kullback-Leibler Divergence and Minimality
#@ Franck Thollard;Pierre Dupont;Colin de la Higuera
#t 2000
#c 19

#index 466594
#* Direct Bayes Point Machines
#@ Matthias Rychetsky;John Shawe-Taylor;Manfred Glesner
#t 2000
#c 19

#index 466595
#* Local Expert Autoassociators for Anomaly Detection
#@ Geoffrey G. Towell
#t 2000
#c 19

#index 466596
#* Linear Discriminant Trees
#@ Olcay Taner Yildiz;Ethem Alpaydin
#t 2000
#c 19

#index 466597
#* Sparse Greedy Matrix Approximation for Machine Learning
#@ Alex J. Smola;Bernhard Schökopf
#t 2000
#c 19

#index 466598
#* Algorithm Selection using Reinforcement Learning
#@ Michail G. Lagoudakis;Michael L. Littman
#t 2000
#c 19

#index 466722
#* Meta-Learning by Landmarking Various Learning Algorithms
#@ Bernhard Pfahringer;Hilan Bensusan;Christophe G. Giraud-Carrier
#t 2000
#c 19

#index 466723
#* Acquisition of Stand-up Behavior by a Real Robot using Hierarchical Reinforcement Learning
#@ Jun Morimoto;Kenji Doya
#t 2000
#c 19

#index 466724
#* Knowledge Propagation in Model-based Reinforcement Learning Tasks
#@ Corinna Richter;Jörg Stachowiak
#t 2000
#c 19

#index 466725
#* FeatureBoost: A Meta-Learning Algorithm that Improves Model Robustness
#@ Joseph O'Sullivan;John Langford;Rich Caruana;Avrim Blum
#t 2000
#c 19

#index 466726
#* Multi-Agent Reinforcement Leraning for Traffic Light Control
#@ Marco Wiering
#t 2000
#c 19

#index 466727
#* Meta-Learning for Phonemic Annotation of Corpora
#@ Veronique Hoste;Walter Daelemans;Erik F. Tjong Kim Sang;Steven Gillis
#t 2000
#c 19

#index 466728
#* Relative Loss Bounds for Temporal-Difference Learning
#@ Jürgen Forster;Manfred K. Warmuth
#t 2000
#c 19

#index 466729
#* Learning Declarative Control Rules for Constraint-BAsed Planning
#@ Yi-Cheng Huang;Bart Selman;Henry A. Kautz
#t 2000
#c 19

#index 466730
#* An Algorithm for Distributed Reinforcement Learning in Cooperative Multi-Agent Systems
#@ Martin Lauer;Martin A. Riedmiller
#t 2000
#c 19

#index 466731
#* A Bayesian Framework for Reinforcement Learning
#@ Malcolm J. A. Strens
#t 2000
#c 19

#index 466732
#* "Boosting'' a Positive-Data-Only Learner
#@ Andrew R. Mitchell
#t 2000
#c 19

#index 466733
#* Crafting Papers on Machine Learning
#@ Pat Langley
#t 2000
#c 19

#index 466734
#* Learning in Non-stationary Conditions: A Control Theoretic Approach
#@ Jefferson Coelho;Roderic A. Grupen
#t 2000
#c 19

#index 466735
#* Unpacking Multi-valued Symbolic Features and Classes in Memory-Based Language Learning
#@ Antal van den Bosch;Jakub Zavrel
#t 2000
#c 19

#index 466736
#* Discriminative Reranking for Natural Language Parsing
#@ Michael Collins
#t 2000
#c 19

#index 466737
#* Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers
#@ Erin L. Allwein;Robert E. Schapire;Yoram Singer
#t 2000
#c 19

#index 466738
#* Convergence Problems of General-Sum Multiagent Reinforcement Learning
#@ Michael H. Bowling
#t 2000
#c 19

#index 466739
#* Learning Probabilistic Models for Decision-Theoretic Navigation of Mobile Robots
#@ Daniel Nikovski;Illah R. Nourbakhsh
#t 2000
#c 19

#index 466740
#* Learning to Fly: An Application of Hierarchical Reinforcement Learning
#@ Malcolm R. K. Ryan;Mark Reid
#t 2000
#c 19

#index 466741
#* Online Ensemble Learning: An Empirical Study
#@ Alan Fern;Robert Givan
#t 2000
#c 19

#index 466742
#* Learning Filaments
#@ Geoffrey J. Gordon;Andrew Moore
#t 2000
#c 19

#index 466743
#* A Normative Examination of Ensemble Learning Algorithms
#@ David M. Pennock;Pedrito Maynard-Reid, II;C. Lee Giles;Eric Horvitz
#t 2000
#c 19

#index 466744
#* Lightweight Rule Induction
#@ Sholom M. Weiss;Nitin Indurkhya
#t 2000
#c 19

#index 466745
#* Anomaly Detection over Noisy Data using Learned Probability Distributions
#@ Eleazar Eskin
#t 2000
#c 19

#index 466746
#* Selection of Support Vector Kernel Parameters for Improved Generalization
#@ Loo-Nin Teow;Kia-Fock Loe
#t 2000
#c 19

#index 466747
#* MultiStage Cascading of Multiple Classifiers: One Man's Noise is Another Man's Data
#@ Cenk Kaynak;Ethem Alpaydin
#t 2000
#c 19

#index 466748
#* Learning Chomsky-like Grammars for Biological Sequence Families
#@ Stephen H. Muggleton;Christopher H. Bryant;Ashwin Srinivasan
#t 2000
#c 19

#index 466749
#* A Universal Generalization for Temporal-Difference Learning Using Haar Basis Functions
#@ Susumu Katayama;Hajime Kimura;Shigenobu Kobayashi
#t 2000
#c 19

#index 466750
#* Empirical Bayes for Learning to Learn
#@ Tom Heskes
#t 2000
#c 19

#index 466751
#* Eligibility Traces for Off-Policy Policy Evaluation
#@ Doina Precup;Richard S. Sutton;Satinder P. Singh
#t 2000
#c 19

#index 466752
#* Predicting the Generalization Performance of Cross Validatory Model Selection Criteria
#@ Tobias Scheffer
#t 2000
#c 19

#index 466753
#* TPOT-RL Applied to Network Routing
#@ Peter Stone
#t 2000
#c 19

#index 466754
#* Enhancing the Plausibility of Law Equation Discovery
#@ Takashi Washio;Hiroshi Motoda;Yuji Niwa
#t 2000
#c 19

#index 466755
#* Achieving Efficient and Cognitively Plausible Learning in Backgammon
#@ Scott Sanner;John R. Anderson;Christian Lebiere;Marsha C. Lovett
#t 2000
#c 19

#index 466756
#* Bootstrapping Syntax and Recursion using Alginment-Based Learning
#@ Menno van Zaanen
#t 2000
#c 19

#index 466757
#* An Adaptive Regularization Criterion for Supervised Learning
#@ Dale Schuurmans;Finnegan Southey
#t 2000
#c 19

#index 466758
#* A Quantification of Distance Bias Between Evaluation Metrics In Classification
#@ Ricardo Vilalta;Daniel Oblinger
#t 2000
#c 19

#index 466759
#* Estimating the Generalization Performance of an SVM Efficiently
#@ Thorsten Joachims
#t 2000
#c 19

#index 466760
#* Exploiting the Cost (In)sensitivity of Decision Tree Splitting Criteria
#@ Chris Drummond;Robert C. Holte
#t 2000
#c 19

#index 466761
#* Hidden Strengths and Limitations: An Empirical Investigation of Reinforcement Learning
#@ Gerald DeJong
#t 2000
#c 19

#index 466762
#* Using Error-Correcting Codes for Text Classification
#@ Rayid Ghani
#t 2000
#c 19

#index 466887
#* Support Vector Machine Active Learning with Application sto Text Classification
#@ Simon Tong;Daphne Koller
#t 2000
#c 19

#index 466888
#* Enhancing Supervised Learning with Unlabeled Data
#@ Sally A. Goldman;Yan Zhou
#t 2000
#c 19

#index 466889
#* Partial Linear Trees
#@ Luís Torgo
#t 2000
#c 19

#index 466890
#* Clustering with Instance-level Constraints
#@ Kiri Wagstaff;Claire Cardie
#t 2000
#c 19

#index 466891
#* Learning to Create Customized Authority Lists
#@ Huan Chang;David Cohn;Andrew McCallum
#t 2000
#c 19

#index 466892
#* Maximum Entropy Markov Models for Information Extraction and Segmentation
#@ Andrew McCallum;Dayne Freitag;Fernando C. N. Pereira
#t 2000
#c 19

#index 466893
#* Using Natural Language Processing and discourse Features to Identify Understanding Errors
#@ Marilyn A. Walker;Jeremy H. Wright;Irene Langkilde
#t 2000
#c 19

#index 466894
#* Behavioral Cloning of Student Pilots with Modular Neural Networks
#@ Charles W. Anderson;Bruce A. Draper;David A. Peterson
#t 2000
#c 19

#index 466895
#* A Nonparametric Approach to Noisy and Costly Optimization
#@ Brigham S. Anderson;Andrew W. More;David Cohn
#t 2000
#c 19

#index 466896
#* Discovering Test Set Regularities in Relational Domains
#@ Seán Slattery;Tom M. Mitchell
#t 2000
#c 19

#index 466897
#* Feature Selection and Incremental Learning of Probabilistic Concept Hierarchies
#@ Luis Talavera
#t 2000
#c 19

#index 466898
#* Using Multiple Levels of Learning and Diverse Evidence to Uncover Coordinately Controlled Genes
#@ Mark Craven;David Page;Jude W. Shavlik;Joseph Bockhorst;Jeremy D. Glasner
#t 2000
#c 19

#index 466899
#* Learning Multiple Models for Reward Maximization
#@ Dani Goldberg;Maja J. Mataric
#t 2000
#c 19

#index 466900
#* Learning Curved Multinomial Subfamilies for Natural Language Processing and Information Retrieval
#@ Keith Hall;Thomas Hofmann
#t 2000
#c 19

#index 466901
#* Learning Distributed Representations by Mapping Concepts and Relations into a Linear Space
#@ Alberto Paccanaro;Geoffrey E. Hinton
#t 2000
#c 19

#index 466902
#* An Integrated Connectionist Approach to Reinforcement Learning for Robotic Control
#@ Dean F. Hougen;Maria L. Gini;James R. Slagle
#t 2000
#c 19

#index 466903
#* Obtaining Simplified Rule Bases by Hybrid Learning
#@ Ricardo Bezerra de Andrade e Silva;Teresa Bernarda Ludermir
#t 2000
#c 19

#index 466904
#* Using Knowledge to Speed Learning: A Comparison of Knowledge-based Cascade-correlation and Multi-task Learning
#@ Thomas R. Shultz;Francois Rivest
#t 2000
#c 19

#index 466905
#* Learning Priorities From Noisy Examples
#@ Geoffrey G. Towell;Thomas Petsche;Michael R. Miller
#t 2000
#c 19

#index 466906
#* Disciple-COA: From Agent Programming to Agent Teaching
#@ Mihai Boicu;Gheorghe Tecuci;Dorin Marcu;Michael Bowman;Ping Shyr;Florin Ciucu;Cristian Levcovici
#t 2000
#c 19

#index 466907
#* Complete Cross-Validation for Nearest Neighbor Classifiers
#@ Matthew Mullin;Rahul Sukthankar
#t 2000
#c 19

#index 466908
#* A General Method for Scaling Up Machine Learning Algorithms and its Application to Clustering
#@ Pedro Domingos;Geoff Hulten
#t 2001
#c 19

#index 466909
#* Clustering Continuous Time Series
#@ Paola Sebastiani;Marco Ramoni
#t 2001
#c 19

#index 466910
#* Reinforcement Learning with Bounded Risk
#@ Peter Geibel
#t 2001
#c 19

#index 466911
#* Boosting with Confidence Information
#@ Craig W. Codrington
#t 2001
#c 19

#index 466912
#* Filters, Wrappers and a Boosting-Based Hybrid for Feature Selection
#@ Sanmay Das
#t 2001
#c 19

#index 466913
#* Collaborative Learning and Recommender Systems
#@ Wee Sun Lee
#t 2001
#c 19

#index 466914
#* Convergence rates of the Voting Gibbs classifier, with application to Bayesian feature selection
#@ Andrew Y. Ng;Michael I. Jordan
#t 2001
#c 19

#index 466915
#* Continuous-Time Hierarchical Reinforcement Learning
#@ Mohammad Ghavamzadeh;Sridhar Mahadevan
#t 2001
#c 19

#index 466916
#* Learning to Select Good Title Words: An New Approach based on Reverse Information Retrieval
#@ Rong Jin;Alexander G. Hauptmann
#t 2001
#c 19

#index 466917
#* Learning Embedded Maps of Markov Processes
#@ Yaakov Engel;Shie Mannor
#t 2001
#c 19

#index 466918
#* General Loss Bounds for Universal Sequence Prediction
#@ Marcus Hutter
#t 2001
#c 19

#index 466919
#* Inducing Partially-Defined Instances with Evolutionary Algorithms
#@ Xavier Llorà;Josep Maria Garrell i Guiu
#t 2001
#c 19

#index 466920
#* Improving Probabilistic Grammatical Inference Core Algorithms with Post-processing Techniques
#@ Franck Thollard
#t 2001
#c 19

#index 466921
#* Mixtures of Rectangles: Interpretable Soft Clustering
#@ Dan Pelleg;Andrew W. Moore
#t 2001
#c 19

#index 466922
#* Hypertext Categorization using Hyperlink Patterns and Meta Data
#@ Rayid Ghani;Seán Slattery;Yiming Yang
#t 2001
#c 19

#index 466923
#* Lyapunov-Constrained Action Sets for Reinforcement Learning
#@ Theodore J. Perkins;Andrew G. Barto
#t 2001
#c 19

#index 466924
#* Discovering Communicable Scientific Knowledge from Spatio-Temporal Data
#@ Mark Schwabacher;Pat Langley
#t 2001
#c 19

#index 466925
#* A Theory-Refinement Approach to Information Extraction
#@ Tina Eliassi-Rad;Jude W. Shavlik
#t 2001
#c 19

#index 466926
#* Unsupervised Sequence Segmentation by a Mixture of Switching Variable Memory Markov Sources
#@ Yevgeny Seldin;Gill Bejerano;Naftali Tishby
#t 2001
#c 19

#index 466927
#* Multiple Instance Regression
#@ Soumya Ray;David Page
#t 2001
#c 19

#index 564257
#* On the Existence of Fixed Points for Q-Learning and Sarsa in Partially Observable Domains
#@ Theodore J. Perkins;Mark D. Pendrith
#t 2002
#c 19

#index 564259
#* Feature Selection with Selective Sampling
#@ Huan Liu;Hiroshi Motoda;Lei Yu
#t 2002
#c 19

#index 564273
#* Option Decision Trees with Majority Votes
#@ Ron Kohavi;Clayton Kunz
#t 1997
#c 19

#index 564279
#* An Efficient Boosting Algorithm for Combining Preferences
#@ Yoav Freund;Raj D. Iyer;Robert E. Schapire;Yoram Singer
#t 1998
#c 19

#index 564282
#* Data Reduction Techniques for Instance-Based Learning from Human/Computer Interface Data
#@ Terran Lane;Carla E. Brodley
#t 2000
#c 19

#index 564285
#* The Effect of the Input Density Distribution on Kernel-based Classifiers
#@ Christopher K. I. Williams;Matthias Seeger
#t 2000
#c 19

#index 564288
#* Coupled Clustering: a Method for Detecting Structural Correspondence
#@ Zvika Marx;Ido Dagan;Joachim M. Buhmann
#t 2001
#c 19

#index 565528
#* Pruning Adaptive Boosting
#@ Dragos D. Margineantu;Thomas G. Dietterich
#t 1997
#c 19

#index 565529
#* Learning Symbolic Prototypes
#@ Piew Datta;Dennis F. Kibler
#t 1997
#c 19

#index 565530
#* Learning the Grammar of Dance
#@ Joshua M. Stuart;Elizabeth Bradley
#t 1998
#c 19

#index 565531
#* Employing EM and Pool-Based Active Learning for Text Classification
#@ Andrew McCallum;Kamal Nigam
#t 1998
#c 19

#index 565532
#* Learning to Drive a Bicycle Using Reinforcement Learning and Shaping
#@ Jette Randløv;Preben Alstrøm
#t 1998
#c 19

#index 565533
#* Local Learning for Iterated Time-Series Prediction
#@ Gianluca Bontempi;Mauro Birattari;Hugues Bersini
#t 1999
#c 19

#index 565534
#* Noise-Tolerant Recursive Best-First Induction
#@ Uros Pompe
#t 1999
#c 19

#index 565535
#* Tractable Average-Case Analysis of Naive Bayesian Classifiers
#@ Pat Langley;Stephanie Sage
#t 1999
#c 19

#index 565536
#* Data as Ensembles of Records: Representation and Comparison
#@ Nicholas R. Howe
#t 2000
#c 19

#index 565537
#* Solving the Multiple-Instance Problem: A Lazy Learning Approach
#@ Jun Wang;Jean-Daniel Zucker
#t 2000
#c 19

#index 565538
#* A Column Generation Algorithm For Boosting
#@ Kristin P. Bennet;Ayhan Demiriz;John Shawe-Taylor
#t 2000
#c 19

#index 565539
#* Reinforcement Learning in POMDP's via Direct Gradient Ascent
#@ Jonathan Baxter;Peter L. Bartlett
#t 2000
#c 19

#index 565540
#* Voting Nearest-Neighbor Subclassifiers
#@ Miroslav Kubat;Martin Cooperson, Jr.
#t 2000
#c 19

#index 565541
#* Fixed Points of Approximate Value Iteration and Temporal-Difference Learning
#@ Daniela Pucci de Farias;Benjamin Van Roy
#t 2000
#c 19

#index 565542
#* Hierarchical Unsupervised Learning
#@ Shivakumar Vaithyanathan;Byron Dom
#t 2000
#c 19

#index 565543
#* Boosting Noisy Data
#@ Abba Krieger;Chuan Long;Abraham Wyner
#t 2001
#c 19

#index 565544
#* Average-Reward Reinforcement Learning for Variance Penalized Markov Decision Problems
#@ Makoto Sato;Shigenobu Kobayashi
#t 2001
#c 19

#index 565545
#* Learning from Labeled and Unlabeled Data using Graph Mincuts
#@ Avrim Blum;Shuchi Chawla
#t 2001
#c 19

#index 565546
#* Classification Value Grouping
#@ Colin K. M. Ho
#t 2002
#c 19

#index 565547
#* An epsilon-Optimal Grid-Based Algorithm for Partially Observable Markov Decision Processes
#@ Blai Bonet
#t 2002
#c 19

#index 565548
#* A Unified Decomposition of Ensemble Loss for Predicting Ensemble Performance
#@ Michael Goebel;Patricia J. Riddle;Mike Barley
#t 2002
#c 19

#index 565549
#* Learning the Kernel Matrix with Semi-Definite Programming
#@ Gert R. G. Lanckriet;Nello Christianini;Peter L. Bartlett;Laurent El Ghaoui;Michael I. Jordan
#t 2002
#c 19

#index 565550
#* Coordinated Reinforcement Learning
#@ Carlos Guestrin;Michail G. Lagoudakis;Ronald Parr
#t 2002
#c 19

#index 759335
#* Proceedings of the Twentieth International Conference on Machine Learning: August 21-24, 2003 Washington, Dc USA
#@ Tom Fawcett;Nina Mishra
#t 2004
#c 19

#index 770752
#* Proceedings of the twenty-first international conference on Machine learning
#@ Carla Brodley
#t 2004
#c 19

#index 770753
#* Active learning of label ranking functions
#@ Klaus Brinker
#t 2004
#c 19
#% 116165
#% 169717
#% 236729
#% 276557
#% 308736
#% 347195
#% 393059
#% 450951
#% 451056
#% 466576
#% 466887
#% 539620
#% 855602
#% 1272396
#! The effort necessary to construct labeled sets of examples in a supervised learning scenario is often disregarded, though in many applications, it is a time-consuming and expensive procedure. While this already constitutes a major issue in classification learning, it becomes an even more serious problem when dealing with the more complex target domain of total orders over a set of alternatives. Considering both the pairwise decomposition and the constraint classification technique to represent label ranking functions, we introduce a novel generalization of pool-based active learning to address this problem.

#index 770754
#* Solving large scale linear prediction problems using stochastic gradient descent algorithms
#@ Tong Zhang
#t 2004
#c 19
#% 116794
#% 302390
#% 304485
#% 350337
#% 420507
#% 543924
#% 854636
#! Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings.

#index 770755
#* Hyperplane margin classifiers on the multinomial manifold
#@ Guy Lebanon;John Lafferty
#t 2004
#c 19
#% 466900
#! The assumptions behind linear classifiers for categorical data are examined and reformulated in the context of the multinomial manifold, the simplex of multinomial models furnished with the Riemannian structure induced by the Fisher information. This leads to a new view of hyperplane classifiers which, together with a generalized margin concept, shows how to adapt existing margin-based hyperplane models to multinomial geometry. Experiments show the new classification framework to be effective for text classification, where the categorical structure of the data is modeled naturally within the multinomial family.

#index 770756
#* A comparative study on methods for reducing myopia of hill-climbing search in multirelational learning
#@ Lourdes Peña Castillo;Stefan Wrobel
#t 2004
#c 19
#% 105501
#% 198345
#% 217072
#% 252221
#% 277919
#% 340736
#% 458683
#% 543238
#% 550417
#% 550422
#% 550549
#% 1393861
#! Hill-climbing search is the most commonly used search algorithm in ILP systems because it permits the generation of theories in short running times. However, a well known drawback of this greedy search strategy is its myopia. Macro-operators (or macros for short), a recently proposed technique to reduce the search space explored by exhaustive search, can also be argued to reduce the myopia of hill-climbing search by automatically performing a variable-depth look-ahead in the search space. Surprisingly, macros have not been employed in a greedy learner. In this paper, we integrate macros into a hill-climbing learner. In a detailed comparative study in several domains, we show that indeed a hill-climbing learner using macros performs significantly better than current state-of-the-art systems involving other techniques for reducing myopia, such as fixed-depth look-ahead, template-based look-ahead, beam-search, or determinate literals. In addition, macros, in contrast to some of the other approaches, can be computed fully automatically and do not require user involvement nor special domain properties such as determinacy.

#index 770757
#* Probabilistic score estimation with piecewise logistic regression
#@ Jian Zhang;Yiming Yang
#t 2004
#c 19
#% 169717
#% 190581
#% 219052
#% 420507
#% 458379
#% 464280
#% 465754
#% 577298
#% 642988
#% 729437
#! Well-calibrated probabilities are necessary in many applications like probabilistic frameworks or cost-sensitive tasks. Based on previous success of asymmetric Laplace method in calibrating text classifiers' scores, we propose to use piecewise logistic regression, which is a simple extension of standard logistic regression, as an alternative method in the discriminative family. We show that both methods have the flexibility to be piecewise linear functions in log-odds, but they are based on quite different assumptions. We evaluated asymmetric Laplace method, piecewise logistic regression and standard logistic regression over standard text categorization collections (Reuters-21578 and TRECAP) with three classifiers (SVM, Naive Bayes and Logistic Regression Classifier), and observed that piecewise logistic regression performs significantly better than the other two methods in the log-loss metric.

#index 770758
#* Boosting grammatical inference with confidence oracles
#@ Jean-Christophe Janodet;Richard Nock;Marc Sebban;Henri-Maxime Suchier
#t 2004
#c 19
#% 180945
#% 235377
#% 252011
#% 276511
#% 302391
#% 425019
#% 464401
#% 562952
#! In this paper we focus on the adaptation of boosting to grammatical inference. We aim at improving the performance of state merging algorithms in the presence of noisy data by using, in the update rule, additional information provided by an oracle. This strategy requires the construction of a new weighting scheme that takes into account the confidence in the labels of the examples. We prove that our new framework preserves the theoretical properties of boosting. Using the state merging algorithm RPNI*, we describe an experimental study on various datasets, showing a dramatic improvement of performances.

#index 770759
#* Kernel conditional random fields: representation and clique selection
#@ John Lafferty;Xiaojin Zhu;Yan Liu
#t 2004
#c 19
#% 226495
#% 458379
#% 464434
#% 466263
#% 643004
#% 763697
#% 816181
#% 854636
#% 1673026
#! Kernel conditional random fields (KCRFs) are introduced as a framework for discriminative modeling of graph-structured data. A representer theorem for conditional graphical models is given which shows how kernel conditional random fields arise from risk minimization procedures defined using Mercer kernels on labeled graphs. A procedure for greedily selecting cliques in the dual representation is then proposed, which allows sparse representations. By incorporating kernels and implicit feature spaces into conditional graphical models, the framework enables semi-supervised learning algorithms for structured data through the use of graph kernels. The framework and clique selection methods are demonstrated in synthetic data experiments, and are also applied to the problem of protein secondary structure prediction.

#index 770760
#* Estimating replicability of classifier learning experiments
#@ Remco R. Bouckaert
#t 2004
#c 19
#% 136350
#% 272995
#% 290482
#% 405926
#% 420065
#% 1650665
#! Replicability of machine learning experiments measures how likely it is that the outcome of one experiment is repeated when performed with a different randomization of the data. In this paper, we present an estimator of replicability of an experiment that is efficient. More precisely, the estimator is unbiased and has lowest variance in the class of estimators formed by a linear combination of outcomes of experiments on a given data set.We gathered empirical data for comparing experiments consisting of different sampling schemes and hypothesis tests. Both factors are shown to have an impact on replicability of experiments. The data suggests that sign tests should not be used due to low replicability. Ranked sum tests show better performance, but the combination of a sorted runs sampling scheme with a t-test gives the most desirable performance judged on Type I and II error and replicability.

#index 770761
#* Learning Bayesian network classifiers by maximizing conditional likelihood
#@ Daniel Grossman;Pedro Domingos
#t 2004
#c 19
#% 44876
#% 129987
#% 132779
#% 190581
#% 197387
#% 243728
#% 246831
#% 246832
#% 246834
#% 277480
#% 304919
#% 331909
#% 420054
#% 466591
#% 578681
#% 580510
#! Bayesian networks are a powerful probabilistic representation, and their use for classification has received considerable attention. However, they tend to perform poorly when learned in the standard way. This is attributable to a mismatch between the objective function used (likelihood or a function thereof) and the goal of classification (maximizing accuracy or conditional likelihood). Unfortunately, the computational cost of optimizing structure and parameters for conditional likelihood is prohibitive. In this paper we show that a simple approximation---choosing structures by maximizing conditional likelihood while setting parameters by maximum likelihood---yields good results. On a large suite of benchmark datasets, this approach produces better class probability estimates than naive Bayes, TAN, and generatively-trained Bayesian networks.

#index 770762
#* Online learning of conditionally I.I.D. data
#@ Daniil Ryabko
#t 2004
#c 19
#% 68820
#% 180945
#% 190581
#% 359194
#% 460767
#% 1808874
#! In this work we consider the task of relaxing the i.i.d assumption in online pattern recognition (or classification), aiming to make existing learning algorithms applicable to a wider range of tasks. Online pattern recognition is predicting a sequence of labels based on objects given for each label and on examples (pairs of objects and labels) learned so far. Traditionally, this task is considered under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over under a much weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects only, while the only condition on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold.We find a broad class of learning algorithms for which estimations of the probability of a classification error achieved under the classical i.i.d. assumption can be generalised to the similar estimates for the case of conditionally i.i.d. distributed examples.

#index 770763
#* Support vector machine learning for interdependent and structured output spaces
#@ Ioannis Tsochantaridis;Thomas Hofmann;Thorsten Joachims;Yasemin Altun
#t 2004
#c 19
#% 190581
#% 279755
#% 464434
#% 722816
#% 741115
#% 854636
#! Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment.

#index 770764
#* Surrogate maximization/minimization algorithms for AdaBoost and the logistic regression model
#@ Zhihua Zhang;James T. Kwok;Dit-Yan Yeung
#t 2004
#c 19
#% 226495
#% 276509
#% 276511
#% 425065
#! Surrogate maximization (or minimization) (SM) algorithms are a family of algorithm that can be regarded as a generalization of expectation-maximization (EM) algorithms. There are three major approaches to the construction of surrogate function, all relying on the convexity of some function. In this paper, we solve the boosting problem by proposing SM algorithms for the corresponding optimization problem. Specifically, for AdaBoost, we derive an SM algorithm that can be shown to be identical to the algorithm proposed by Collins et al. (2002) based on Bregman distance. More importantly, for LogitBoost (or logistic boosting), we use several methods to construct different surrogate functions which result in different SM algorithms. By combining multiple methods, we are able to derive an SM algorithm that is also the same as an algorithm derived by Collins et al. (2002). Our approach based on SM algorithms is much simpler and convergence results follow naturally.

#index 770765
#* Learning to track 3D human motion from silhouettes
#@ Ankur Agarwal;Bill Triggs
#t 2004
#c 19
#% 361100
#% 443975
#% 443993
#% 457830
#% 457987
#% 592413
#% 635689
#% 635749
#% 718437
#% 722760
#% 724177
#% 724242
#% 724256
#% 724290
#% 1502515
#% 1562510
#! We describe a sparse Bayesian regression method for recovering 3D human body motion directly from silhouettes extracted from monocular video sequences. No detailed body shape model is needed, and realism is ensured by training on real human motion capture data. The tracker estimates 3D body pose by using Relevance Vector Machine regression to combine a learned autoregressive dynamical model with robust shape descriptors extracted automatically from image silhouettes. We studied several different combination methods, the most effective being to learn a nonlinear observation-update correction based on joint regression with respect to the predicted state and the observations. We demonstrate the method on a 54-parameter full body pose model, both quantitatively using motion capture based test sequences, and qualitatively on a test video sequence.

#index 770766
#* Leveraging the margin more carefully
#@ Nir Krause;Yoram Singer
#t 2004
#c 19
#% 186989
#% 198701
#% 235377
#% 276506
#% 302391
#% 311027
#% 312727
#% 425065
#! Boosting is a popular approach for building accurate classifiers. Despite the initial popular belief, boosting algorithms do exhibit overfitting and are sensitive to label noise. Part of the sensitivity of boosting algorithms to outliers and noise can be attributed to the unboundedness of the margin-based loss functions that they employ. In this paper we describe two leveraging algorithms that build on boosting techniques and employ a bounded loss function of the margin. The first algorithm interleaves the expectation maximization (EM) algorithm with boosting steps. The second algorithm decomposes a non-convex loss into a difference of two convex losses. We prove that both algorithms converge to a stationary point. We also analyze the generalization properties of the algorithms using the Rademacher complexity. We describe experiments with both synthetic data and natural data (OCR and text) that demonstrate the merits of our framework, in particular robustness to outliers.

#index 770767
#* Learning a kernel matrix for nonlinear dimensionality reduction
#@ Kilian Q. Weinberger;Fei Sha;Lawrence K. Saul
#t 2004
#c 19
#% 209961
#% 266426
#% 269213
#% 393059
#% 443790
#% 492792
#% 593047
#% 723241
#% 763697
#% 770839
#% 1502529
#! We investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. Noting that the kernel matrix implicitly maps the data into a nonlinear feature space, we show how to discover a mapping that "unfolds" the underlying manifold from which the data was sampled. The kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors. The main optimization involves an instance of semidefinite programming---a fundamentally different computation than previous algorithms for manifold learning, such as Isomap and locally linear embedding. The optimized kernels perform better than polynomial and Gaussian kernels for problems in manifold learning, but worse for problems in large margin classification. We explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods.

#index 770768
#* Utile distinction hidden Markov models
#@ Daan Wierstra;Marco Wiering
#t 2004
#c 19
#% 111440
#% 154022
#% 384911
#% 466069
#% 702594
#% 1272286
#! This paper addresses the problem of constructing good action selection policies for agents acting in partially observable environments, a class of problems generally known as Partially Observable Markov Decision Processes. We present a novel approach that uses a modification of the well-known Baum-Welch algorithm for learning a Hidden Markov Model (HMM) to predict both percepts and utility in a non-deterministic world. This enables an agent to make decisions based on its previous history of actions, observations, and rewards. Our algorithm, called Utile Distinction Hidden Markov Models (UDHMM), handles the creation of memory well in that it tends to create perceptual and utility distinctions only when needed, while it can still discriminate states based on histories of arbitrary length. The experimental results in highly stochastic problem domains show very good performance.

#index 770769
#* Generalized low rank approximations of matrices
#@ Jieping Ye
#t 2004
#c 19
#% 200694
#% 248798
#% 273699
#% 274703
#% 282481
#% 338442
#% 340309
#% 425010
#% 578399
#% 593842
#% 656665
#% 769911
#! We consider the problem of computing low rank approximations of matrices. The novelty of our approach is that the low rank approximations are on a sequence of matrices. Unlike the problem of low rank approximations of a single matrix, which was well studied in the past, the proposed algorithm in this paper does not admit a closed form solution in general. We did extensive experiments on face image data to evaluate the effectiveness of the proposed algorithm and compare the computed low rank approximations with those obtained from traditional Singular Value Decomposition based method.

#index 770770
#* Feature extraction via generalized uncorrelated linear discriminant analysis
#@ Jieping Ye;Ravi Janardan;Qi Li;Haesun Park
#t 2004
#c 19
#% 80995
#% 212689
#% 581716
#! Feature extraction is important in many applications, such as text and image retrieval, because of high dimensionality. Uncorrelated Linear Discriminant Analysis (ULDA) was recently proposed for feature extraction. The extracted features via ULDA were shown to be statistically uncorrelated, which is desirable for many applications. In this paper, we will first propose the ULDA/QR algorithm to simplify the previous implementation of ULDA. Then we propose the ULDA/GSVD algorithm, based on a novel optimization criterion, to address the singularity problem. It is applicable for undersampled problem, where the data dimension is much larger than the data size, such as text and image retrieval. The novel criterion used in ULDA/GSVD is the perturbed version of the one from ULDA/QR, while surprisingly, the solution to ULDA/GSVD is shown to be independent of the amount of perturbation applied. We did extensive experiments on text and face image data to show the effectiveness of ULDA/GSVD and compare with other popular feature extraction algorithms.

#index 770771
#* Active learning using pre-clustering
#@ Hieu T. Nguyen;Arnold Smeulders
#t 2004
#c 19
#% 169717
#% 252694
#% 311027
#% 341269
#% 420507
#% 446742
#% 464268
#% 466419
#% 466576
#% 565531
#% 722797
#% 815878
#% 1272282
#% 1387560
#% 1775158
#! The paper is concerned with two-class active learning. While the common approach for collecting data in active learning is to select samples close to the classification boundary, better performance can be achieved by taking into account the prior data distribution. The main contribution of the paper is a formal framework that incorporates clustering into active learning. The algorithm first constructs a classifier on the set of the cluster representatives, and then propagates the classification decision to the other samples via a local noise model. The proposed model allows to select the most representative samples as well as to avoid repeatedly labeling samples in the same cluster. During the active learning process, the clustering is adjusted using the coarse-to-fine strategy in order to balance between the advantage of large clusters and the accuracy of the data representation. The results of experiments in image databases show a better performance of our algorithm compared to the current methods.

#index 770772
#* Co-EM support vector learning
#@ Ulf Brefeld;Tobias Scheffer
#t 2004
#c 19
#% 190581
#% 252011
#% 269217
#% 269225
#% 305016
#% 311027
#% 316509
#% 464457
#% 464466
#% 464777
#% 464784
#% 466086
#% 466263
#% 565531
#% 1378224
#! Multi-view algorithms, such as co-training and co-EM, utilize unlabeled data when the available attributes can be split into independent and compatible subsets. Co-EM outperforms co-training for many problems, but it requires the underlying learner to estimate class probabilities, and to learn from probabilistically labeled data. Therefore, co-EM has so far only been studied with naive Bayesian learners. We cast linear classifiers into a probabilistic framework and develop a co-EM version of the Support Vector Machine. We conduct experiments on text classification problems and compare the family of semi-supervised support vector algorithms under different conditions, including violations of the assumptions underlying multi-view learning. For some problems, such as course web page classification, we observe the most accurate results reported so far.

#index 770773
#* Communication complexity as a lower bound for learning in games
#@ Vincent Conitzer;Tuomas Sandholm
#t 2004
#c 19
#% 143652
#% 238182
#% 338466
#% 348821
#% 465913
#% 528018
#% 567883
#% 580519
#% 600560
#% 1279321
#% 1279323
#! A fast-growing body of research in the AI and machine learning communities addresses learning in games, where there are multiple learners with different interests. This research adds to more established research on learning in games conducted in economics. In part because of a clash of fields, there are widely varying requirements on learning algorithms in this domain. The goal of this paper is to demonstrate how communication complexity can be used as a lower bound on the required learning time or cost. Because this lower bound does not assume any requirements on the learning algorithm, it is universal, applying under any set of requirements on the learning algorithm.We characterize exactly the communication complexity of various solution concepts from game theory, namely Nash equilibrium, iterated dominant strategies (both strict and weak), and backwards induction. This gives the tighest lower bounds on learning in games that can be obtained with this method.

#index 770774
#* Margin based feature selection - theory and algorithms
#@ Ran Gilad-Bachrach;Amir Navot;Naftali Tishby
#t 2004
#c 19
#% 126894
#% 197394
#% 234978
#% 235377
#% 243728
#% 449588
#% 722929
#% 722936
#% 1809314
#! Feature selection is the task of choosing a small set out of a given set of features that capture the relevant properties of the data. In the context of supervised classification problems the relevance is determined by the given labels on the training data. A good choice of features is a key for building compact and accurate classifiers. In this paper we introduce a margin based feature selection criterion and apply it to measure the quality of sets of features. Using margins we devise novel selection algorithms for multi-class classification problems and provide theoretical generalization bound. We also study the well known Relief algorithm and show that it resembles a gradient ascent over our margin criterion. We apply our new algorithm to various datasets and show that our new Simba algorithm, which directly optimizes the margin, outperforms Relief.

#index 770775
#* Using relative novelty to identify useful temporal abstractions in reinforcement learning
#@ Özgür Şimşek;Andrew G. Barto
#t 2004
#c 19
#% 124692
#% 270031
#% 272662
#% 286423
#% 384911
#% 458686
#% 464303
#% 464607
#% 464636
#% 466075
#% 706874
#% 711673
#% 715736
#% 722895
#% 729437
#% 1271827
#! We present a new method for automatically creating useful temporal abstractions in reinforcement learning. We argue that states that allow the agent to transition to a different region of the state space are useful subgoals, and propose a method for identifying them using the concept of relative novelty. When such a state is identified, a temporally-extended activity (e.g., an option) is generated that takes the agent efficiently to this state. We illustrate the utility of the method in a number of tasks.

#index 770776
#* A graphical model for protein secondary structure prediction
#@ Wei Chu;Zoubin Ghahramani;David L. Wild
#t 2004
#c 19
#% 471266
#% 715083
#! In this paper, we present a graphical model for protein secondary structure prediction. This model extends segmental semi-Markov models (SSMM) to exploit multiple sequence alignment profiles which contain information from evolutionarily related sequences. A novel parameterized model is proposed as the likelihood function for the SSMM to capture the segmental conformation. By incorporating the information from long range interactions in ß-sheets, this model is capable of carrying out inference on contact maps. The numerical results on benchmark data sets show that incorporating the profiles results in substantial improvements and the generalization performance is promising.

#index 770777
#* Dynamic abstraction in reinforcement learning via clustering
#@ Shie Mannor;Ishai Menache;Amit Hoze;Uri Klein
#t 2004
#c 19
#% 36672
#% 124691
#% 124692
#% 217812
#% 270031
#% 286423
#% 296738
#% 393786
#% 431471
#% 458686
#% 464303
#% 476730
#% 1271827
#% 1272385
#! We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.

#index 770778
#* A pitfall and solution in multi-class feature selection for text classification
#@ George Forman
#t 2004
#c 19
#% 243728
#% 280817
#% 290482
#% 425048
#% 458379
#% 465754
#% 466266
#% 722935
#% 727663
#! Information Gain is a well-known and empirically proven method for high-dimensional feature selection. We found that it and other existing methods failed to produce good results on an industrial text classification problem. On investigating the root cause, we find that a large class of feature scoring methods suffers a pitfall: they can be blinded by a surplus of strongly predictive features for some classes, while largely ignoring features needed to discriminate difficult classes. In this paper we demonstrate this pitfall hurts performance even for a relatively uniform text classification task. Based on this understanding, we present solutions inspired by round-robin scheduling that avoid this pitfall, without resorting to costly wrapper methods. Empirical evaluation on 19 datasets shows substantial improvements.

#index 770779
#* A spatio-temporal extension to Isomap nonlinear dimension reduction
#@ Odest Chadwicke Jenkins;Maja J. Matarić
#t 2004
#c 19
#% 36672
#% 179858
#% 266426
#% 348548
#% 445394
#% 729960
#% 788500
#! We present an extension of Isomap nonlinear dimension reduction (Tenenbaum et al., 2000) for data with both spatial and temporal relationships. Our method, ST-Isomap, augments the existing Isomap framework to consider temporal relationships in local neighborhoods that can be propagated globally via a shortest-path mechanism. Two instantiations of ST-Isomap are presented for sequentially continuous and segmented data. Results from applying ST-Isomap to real-world data collected from human motion performance and humanoid robot teleoperation are also presented.

#index 770780
#* The Bayesian backfitting relevance vector machine
#@ Aaron D'Souza;Sethu Vijayakumar;Stefan Schaal
#t 2004
#c 19
#% 132779
#% 242970
#% 272537
#% 293279
#% 424806
#% 466428
#% 528020
#% 722760
#% 763698
#! Traditional non-parametric statistical learning techniques are often computationally attractive, but lack the same generalization and model selection abilities as state-of-the-art Bayesian algorithms which, however, are usually computationally prohibitive. This paper makes several important contributions that allow Bayesian learning to scale to more complex, real-world learning scenarios. Firstly, we show that backfitting --- a traditional non-parametric, yet highly efficient regression tool --- can be derived in a novel formulation within an expectation maximization (EM) framework and thus can finally be given a probabilistic interpretation. Secondly, we show that the general framework of sparse Bayesian learning and in particular the relevance vector machine (RVM), can be derived as a highly efficient algorithm using a Bayesian version of backfitting at its core. As we demonstrate on several regression and classification benchmarks, Bayesian backfitting offers a compelling alternative to current regression methods, especially when the size and dimensionality of the data challenge computational resources.

#index 770781
#* Learning and discovery of predictive state representations in dynamical systems with reset
#@ Michael R. James;Satinder Singh
#t 2004
#c 19
#% 102136
#% 158924
#% 646958
#% 857087
#% 1271848
#! Predictive state representations (PSRs) are a recently proposed way of modeling controlled dynamical systems. PSR-based models use predictions of observable outcomes of tests that could be done on the system as their state representation, and have model parameters that define how the predictive state representation changes over time as actions are taken and observations noted. Learning PSR-based models requires solving two subproblems: 1) discovery of the tests whose predictions constitute state, and 2) learning the model parameters that define the dynamics. So far, there have been no results available on the discovery subproblem while for the learning subproblem an approximate-gradient algorithm has been proposed (Singh et al., 2003) with mixed results (it works on some domains and not on others). In this paper, we provide the first discovery algorithm and a new learning algorithm for linear PSRs for the special class of controlled dynamical systems that have a reset operation. We provide experimental verification of our algorithms. Finally, we also distinguish our work from prior work by Jaeger (2000) on observable operator models (OOMs).

#index 770782
#* Integrating constraints and metric learning in semi-supervised clustering
#@ Mikhail Bilenko;Sugato Basu;Raymond J. Mooney
#t 2004
#c 19
#% 460812
#% 464291
#% 464608
#% 464631
#% 593940
#% 715706
#% 723241
#% 729913
#% 769881
#! Semi-supervised clustering employs a small amount of labeled data to aid unsupervised learning. Previous work in the area has utilized supervised data in one of two approaches: 1) constraint-based methods that guide the clustering algorithm towards a better grouping of the data, and 2) distance-function learning methods that adapt the underlying similarity metric used by the clustering algorithm. This paper provides new methods for the two approaches as well as presents a new semi-supervised clustering algorithm that integrates both of these techniques in a uniform, principled framework. Experimental results demonstrate that the unified approach produces better clusters than both individual approaches as well as previously proposed semi-supervised clustering algorithms.

#index 770783
#* A MFoM learning approach to robust multiclass multi-label text categorization
#@ Sheng Gao;Wen Wu;Chin-Hui Lee;Tat-Seng Chua
#t 2004
#c 19
#% 190581
#% 280817
#% 311034
#% 344447
#% 402289
#% 642996
#% 1272365
#! We propose a multiclass (MC) classification approach to text categorization (TC). To fully take advantage of both positive and negative training examples, a maximal figure-of-merit (MFoM) learning algorithm is introduced to train high performance MC classifiers. In contrast to conventional binary classification, the proposed MC scheme assigns a uniform score function to each category for each given test sample, and thus the classical Bayes decision rules can now be applied. Since all the MC MFoM classifiers are simultaneously trained, we expect them to be more robust and work better than the binary MFoM classifiers, which are trained separately and are known to give the best TC performance. Experimental results on the Reuters-21578 TC task indicate that the MC MFoM classifiers achieve a micro-averaging F1 value of 0.377, which is significantly better than 0.138, obtained with the binary MFoM classifiers, for the categories with less than 4 training samples. Furthermore, for all 90 categories, most with large training sizes, the MC MFoM classifiers give a micro-averaging F1 value of 0.888, better than 0.884, obtained with the binary MFoM classifiers.

#index 770784
#* Probabilistic tangent subspace: a unified view
#@ Jianguo Lee;Jingdong Wang;Changshui Zhang;Zhaoqi Bian
#t 2004
#c 19
#% 235340
#% 236651
#% 266426
#% 269213
#% 272538
#% 315986
#% 425049
#% 443605
#% 476717
#% 493092
#% 724170
#% 1786835
#! Tangent Distance (TD) is one classical method for invariant pattern classification. However, conventional TD need pre-obtain tangent vectors, which is difficult except for image objects. This paper extends TD to more general pattern classification tasks. The basic assumption is that tangent vectors can be approximately represented by the pattern variations. We propose three probabilistic subspace models to encode the variations: the linear subspace, nonlinear subspace, and manifold subspace models. These three models are addressed in a unified view, namely Probabilistic Tangent Subspace (PTS). Experiments show that PTS can achieve promising classification performance in non-image data sets.

#index 770785
#* Ensembles of nested dichotomies for multi-class problems
#@ Eibe Frank;Stefan Kramer
#t 2004
#c 19
#% 290482
#% 425052
#% 580511
#% 722756
#% 722807
#% 763699
#% 1272365
#! Nested dichotomies are a standard statistical technique for tackling certain polytomous classification problems with logistic regression. They can be represented as binary trees that recursively split a multi-class classification task into a system of dichotomies and provide a statistically sound way of applying two-class learning algorithms to multi-class problems (assuming these algorithms generate class probability estimates). However, there are usually many candidate trees for a given problem and in the standard approach the choice of a particular tree is based on domain knowledge that may not be available in practice. An alternative is to treat every system of nested dichotomies as equally likely and to form an ensemble classifier based on this assumption. We show that this approach produces more accurate classifications than applying C4.5 and logistic regression directly to multi-class problems. Our results also show that ensembles of nested dichotomies produce more accurate classifiers than pairwise classification if both techniques are used with C4.5, and comparable results for logistic regression. Compared to error-correcting output codes, they are preferable if logistic regression is used, and comparable in the case of C4.5. An additional benefit is that they generate class probability estimates. Consequently they appear to be a good general-purpose method for applying binary classifiers to multi-class problems.

#index 770786
#* Gradient LASSO for feature selection
#@ Yongdai Kim;Jinseog Kim
#t 2004
#c 19
#% 235377
#% 274586
#% 304902
#% 425061
#% 722937
#! LASSO (Least Absolute Shrinkage and Selection Operator) is a useful tool to achieve the shrinkage and variable selection simultaneously. Since LASSO uses the L1 penalty, the optimization should rely on the quadratic program (QP) or general non-linear program which is known to be computational intensive. In this paper, we propose a gradient descent algorithm for LASSO. Even though the final result is slightly less accurate, the proposed algorithm is computationally simpler than QP or non-linear program, and so can be applied to large size problems. We provide the convergence rate of the algorithm, and illustrate it with simulated models as well as real data sets.

#index 770787
#* Learning large margin classifiers locally and globally
#@ Kaizhu Huang;Haiqin Yang;Irwin King;Michael R. Lyu
#t 2004
#c 19
#% 80995
#% 190581
#% 390723
#% 722901
#! A new large margin classifier, named Maxi-Min Margin Machine (M4) is proposed in this paper. This new classifier is constructed based on both a "local: and a "global" view of data, while the most popular large margin classifier, Support Vector Machine (SVM) and the recently-proposed important model, Minimax Probability Machine (MPM) consider data only either locally or globally. This new model is theoretically important in the sense that SVM and MPM can both be considered as its special case. Furthermore, the optimization of M4 can be cast as a sequential conic programming problem, which can be solved efficiently. We describe the M4 model definition, provide a clear geometrical interpretation, present theoretical justifications, propose efficient solving methods, and perform a series of evaluations on both synthetic data sets and real world benchmark data sets. Its comparison with SVM and MPM also demonstrates the advantages of our new model.

#index 770788
#* Optimising area under the ROC curve using gradient descent
#@ Alan Herschtal;Bhavani Raskutti
#t 2004
#c 19
#% 190581
#% 309208
#% 458379
#% 723244
#% 1378224
#! This paper introduces RankOpt, a linear binary classifier which optimises the area under the ROC curve (the AUC). Unlike standard binary classifiers, RankOpt adopts the AUC statistic as its objective function, and optimises it directly using gradient descent. The problems with using the AUC statistic as an objective function are that it is non-differentiable, and of complexity O(n2) in the number of data observations. RankOpt uses a differentiable approximation to the AUC which is accurate, and computationally efficient, being of complexity O(n.) This enables the gradient descent to be performed in reasonable time. The performance of RankOpt is compared with a number of other linear binary classifiers, over a number of different classification problems. In almost all cases it is found that the performance of RankOpt is significantly better than the other classifiers tested.

#index 770789
#* Parameter space exploration with Gaussian process trees
#@ Robert B. Gramacy;Herbert K. H. Lee;William G. Macready
#t 2004
#c 19
#% 132697
#% 633485
#! Computer experiments often require dense sweeps over input parameters to obtain a qualitative understanding of their response. Such sweeps can be prohibitively expensive, and are unnecessary in regions where the response is easy predicted; well-chosen designs could allow a mapping of the response with far fewer simulation runs. Thus, there is a need for computationally inexpensive surrogate models and an accompanying method for selecting small designs. We explore a general methodology for addressing this need that uses non-stationary Gaussian processes. Binary trees partition the input space to facilitate non-stationarity and a Bayesian interpretation provides an explicit measure of predictive uncertainty that can be used to guide sampling. Our methods are illustrated on several examples, including a motivating example involving computational fluid dynamics simulation of a NASA reentry vehicle.

#index 770790
#* Bayesian inference for transductive learning of kernel matrix using the Tanner-Wong data augmentation algorithm
#@ Zhihua Zhang;Dit-Yan Yeung;James T. Kwok
#t 2004
#c 19
#% 3084
#% 213009
#% 565549
#% 723238
#! In kernel methods, an interesting recent development seeks to learn a good kernel from empirical data automatically. In this paper, by regarding the transductive learning of the kernel matrix as a missing data problem, we propose a Bayesian hierarchical model for the problem and devise the Tanner-Wong data augmentation algorithm for making inference on the model. The Tanner-Wong algorithm is closely related to Gibbs sampling, and it also bears a strong resemblance to the expectation-maximization (EM) algorithm. For an efficient implementation, we propose a simplified Bayesian hierarchical model and the corresponding Tanner-Wong algorithm. We express the relationship between the kernel on the input space and the kernel on the output space as a symmetric-definite generalized eigenproblem. Based on this eigenproblem, an efficient approach to choosing the base kernel matrices is presented. The effectiveness of our Bayesian model with the Tanner-Wong algorithm is demonstrated through some classification experiments showing promising results.

#index 770791
#* Decision trees with minimal costs
#@ Charles X. Ling;Qiang Yang;Jianning Wang;Shichao Zhang
#t 2004
#c 19
#% 92554
#% 136350
#% 160852
#% 280437
#% 376266
#% 447606
#% 464639
#% 477640
#% 714684
#% 1272369
#% 1289281
#% 1499572
#! We propose a simple, novel and yet effective method for building and testing decision trees that minimizes the sum of the misclassification and test costs. More specifically, we first put forward an original and simple splitting criterion for attribute selection in tree building. Our tree-building algorithm has many desirable properties for a cost-sensitive learning system that must account for both types of costs. Then, assuming that the test cases may have a large number of missing values, we design several intelligent test strategies that can suggest ways of obtaining the missing values at a cost in order to minimize the total cost. We experimentally compare these strategies and C4.5, and demonstrate that our new algorithms significantly outperform C4.5 and its variations. In addition, our algorithm's complexity is similar to that of C4.5, and is much lower than that of previous work. Our work is useful for many diagnostic tasks which must factor in the misclassification and test costs for obtaining missing information.

#index 770792
#* Robust feature induction for support vector machines
#@ Rong Jin;Huan Liu
#t 2004
#c 19
#% 136350
#% 226495
#% 266255
#% 290482
#% 302391
#% 305004
#% 312727
#% 331916
#% 340903
#% 420077
#% 1673026
#! The goal of feature induction is to automatically create nonlinear combinations of existing features as additional input features to improve classification accuracy. Typically, nonlinear features are introduced into a support vector machine (SVM) through a nonlinear kernel function. One disadvantage of such an approach is that the feature space induced by a kernel function is usually of high dimension and therefore will substantially increase the chance of over-fitting the training data. Another disadvantage is that nonlinear features are induced implicitly and therefore are difficult for people to understand which induced features are critical to the classification performance. In this paper, we propose a boosting-style algorithm that can explicitly induces important nonlinear features for SVMs. We present empirical studies with discussion to show that this approach is effective in improving classification accuracy for SVMs. The comparison with an SVM model using nonlinear kernels also indicates that this approach is effective and robust, particularly when the number of training data is small.

#index 770793
#* Generative modeling for continuous non-linearly embedded visual inference
#@ Cristian Sminchisescu;Allan Jepson
#t 2004
#c 19
#% 213552
#% 266616
#% 457830
#% 625173
#% 1502477
#% 1502529
#% 1562510
#! Many difficult visual perception problems, like 3D human motion estimation, can be formulated in terms of inference using complex generative models, defined over high-dimensional state spaces. Despite progress, optimizing such models is difficult because prior knowledge cannot be flexibly integrated in order to reshape an initially designed representation space. Nonlinearities, inherent sparsity of high-dimensional training sets, and lack of global continuity makes dimensionality reduction challenging and low-dimensional search inefficient. To address these problems, we present a learning and inference algorithm that restricts visual tracking to automatically extracted, non-linearly embedded, low-dimensional spaces. This formulation produces a layered generative model with reduced state representation, that can be estimated using efficient continuous optimization methods. Our prior flattening method allows a simple analytic treatment of low-dimensional intrinsic curvature constraints, and allows consistent interpolation operations. We analyze reduced manifolds for human interaction activities, and demonstrate that the algorithm learns continuous generative models that are useful for tracking and for the reconstruction of 3D human motion in monocular video.

#index 770794
#* Incremental learning of linear model trees
#@ Duncan Potts
#t 2004
#c 19
#% 131402
#% 229931
#% 246747
#% 273319
#% 292240
#% 465897
#% 466428
#% 577264
#! A linear model tree is a decision tree with a linear functional model in each leaf. Previous model tree induction algorithms have operated on the entire training set, however there are many situations when an incremental learner is advantageous. In this paper we demonstrate that model trees can be induced incrementally using an algorithm that scales linearly with the number of examples. An incremental node splitting rule is presented, together with incremental methods for stopping the growth of the tree and pruning. Empirical testing in three domains, where the emphasis is on learning a dynamic model of the environment, shows that the algorithm can learn a more accurate approximation from fewer examples than other incremental methods. In addition the induced models are smaller, and the learner requires less prior knowledge about the domain.

#index 770795
#* Lookahead-based algorithms for anytime induction of decision trees
#@ Saher Esmeir;Shaul Markovitch
#t 2004
#c 19
#% 26125
#% 101468
#% 136350
#% 159239
#% 162624
#% 420077
#% 449588
#% 465579
#% 496419
#% 835998
#% 1272290
#% 1279300
#% 1290031
#% 1788186
#! The majority of the existing algorithms for learning decision trees are greedy---a tree is induced top-down, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Furthermore, the greedy algorithms require a fixed amount of time and are not able to generate a better tree if additional time is available. To overcome this problem, we present two lookahead-based algorithms for anytime induction of decision trees, thus allowing tradeoff between tree quality and learning time. The first one is depth-k lookahead, where a larger time allocation permits larger k. The second algorithm uses a novel strategy for evaluating candidate splits; a stochastic version of ID3 is repeatedly invoked to estimate the size of the tree in which each split results, and the one that minimizes the expected size is preferred. Experimental results indicate that for several hard concepts, our proposed approach exhibits good anytime behavior and yields significantly better decision trees when more time is available.

#index 770796
#* Large margin hierarchical classification
#@ Ofer Dekel;Joseph Keshet;Yoram Singer
#t 2004
#c 19
#% 190581
#% 227736
#% 309141
#% 382854
#% 395959
#% 420466
#% 465747
#% 466078
#% 1815223
#! We present an algorithmic framework for supervised classification learning where the set of labels is organized in a predefined hierarchical structure. This structure is encoded by a rooted tree which induces a metric over the label set. Our approach combines ideas from large margin kernel methods and Bayesian analysis. Following the large margin principle, we associate a prototype with each label in the tree and formulate the learning task as an optimization problem with varying margin constraints. In the spirit of Bayesian methods, we impose similarity requirements between the prototypes corresponding to adjacent labels in the hierarchy. We describe new online and batch algorithms for solving the constrained optimization problem. We derive a worst case loss-bound for the online algorithm and provide generalization analysis for its batch counterpart. We demonstrate the merits of our approach with a series of experiments on synthetic, text and speech data.

#index 770797
#* Sequential information bottleneck for finite data
#@ Jaakko Peltonen;Janne Sinkkonen;Samuel Kaski
#t 2004
#c 19
#% 397139
#% 458673
#% 458705
#% 668807
#% 722904
#% 1650298
#! The sequential information bottleneck (sIB) algorithm clusters co-occurrence data such as text documents vs. words. We introduce a variant that models sparse co-occurrence data by a generative process. This turns the objective function of sIB, mutual information, into a Bayes factor, while keeping it intact asymptotically, for non-sparse data. Experimental performance of the new algorithm is comparable to the original sIB for large data sets, and better for smaller, sparse sets.

#index 770798
#* Online and batch learning of pseudo-metrics
#@ Shai Shalev-Shwartz;Yoram Singer;Andrew Y. Ng
#t 2004
#c 19
#% 51749
#% 92233
#% 190581
#% 218982
#% 382854
#% 457926
#% 464612
#% 729437
#! We describe and analyze an online algorithm for supervised learning of pseudo-metrics. The algorithm receives pairs of instances and predicts their similarity according to a pseudo-metric. The pseudo-metrics we use are quadratic forms parameterized by positive semi-definite matrices. The core of the algorithm is an update rule that is based on successive projections onto the positive semi-definite cone and onto half-space constraints imposed by the examples. We describe an efficient procedure for performing these projections, derive a worst case mistake bound on the similarity predictions, and discuss a dual version of the algorithm in which it is simple to incorporate kernel operators. The online algorithm also serves as a building block for deriving a large-margin batch algorithm. We demonstrate the merits of the proposed approach by conducting experiments on MNIST dataset and on document filtering.

#index 770799
#* Testing the significance of attribute interactions
#@ Aleks Jakulin;Ivan Bratko
#t 2004
#c 19
#% 246832
#% 458168
#% 1650316
#! Attribute interactions are the irreducible dependencies between attributes. Interactions underlie feature relevance and selection, the structure of joint probability and classification models: if and only if the attributes interact, they should be connected. While the issue of 2-way interactions, especially of those between an attribute and the label, has already been addressed, we introduce an operational definition of a generalized n-way interaction by highlighting two models: the reductionistic part-to-whole approximation, where the model of the whole is reconstructed from models of the parts, and the holistic reference model, where the whole is modelled directly. An interaction is deemed significant if these two models are significantly different. In this paper, we propose the Kirkwood superposition approximation for constructing part-to-whole approximations. To model data, we do not assume a particular structure of interactions, but instead construct the model by testing for the presence of interactions. The resulting map of significant interactions is a graphical model learned from the data. We confirm that the P-values computed with the assumption of the asymptotic X2 distribution closely match those obtained with the boot-strap.

#index 770800
#* Feature subset selection for learning preferences: a case study
#@ Antonio Bahamonde;Gustavo F. Bayón;Jorge Díez;José Ramón Quevedo;Oscar Luaces;Juan José del Coz;Jaime Alonso;Félix Goyache
#t 2004
#c 19
#% 78888
#% 190581
#% 243728
#% 269217
#% 425048
#% 425058
#% 465583
#% 466430
#% 483062
#% 577224
#% 722931
#% 722939
#% 1272396
#% 1290045
#% 1478815
#% 1814562
#! In this paper we tackle a real world problem, the search of a function to evaluate the merits of beef cattle as meat producers. The independent variables represent a set of live animals' measurements; while the outputs cannot be captured with a single number, since the available experts tend to assess each animal in a relative way, comparing animals with the other partners in the same batch. Therefore, this problem can not be solved by means of regression methods; our approach is to learn the preferences of the experts when they order small groups of animals. Thus, the problem can be reduced to a binary classification, and can be dealt with a Support Vector Machine (SVM) improved with the use of a feature subset selection (FSS) method. We develop a method based on Recursive Feature Elimination (RFE) that employs an adaptation of a metric based method devised for model selection (ADJ). Finally, we discuss the extension of the resulting method to more general settings, and provide a comparison with other possible alternatives.

#index 770801
#* A multiplicative up-propagation algorithm
#@ Jong-Hoon Ahn;Seungjin Choi;Jong-Hoon Oh
#t 2004
#c 19
#% 272532
#! We present a generalization of the nonnegative matrix factorization (NMF), where a multilayer generative network with nonnegative weights is used to approximate the observed nonnegative data. The multilayer generative network with nonnegativity constraints, is learned by a multiplicative uppropagation algorithm, where the weights in each layer are updated in a multiplicative fashion while the mismatch ratio is propagated from the bottom to the top layer. The monotonic convergence of the multiplicative up-propagation algorithm is shown. In contrast to NMF, the multiplicative uppropagation is an algorithm that can learn hierarchical representations, where complex higher-level representations are defined in terms of less complex lower-level representations. The interesting behavior of our algorithm is demonstrated with face image data.

#index 770802
#* Coalition calculation in a dynamic agent environment
#@ Ted Scully;Michael G. Madden;Gerard Lyons
#t 2004
#c 19
#% 274917
#% 302018
#% 378947
#% 379158
#% 641962
#% 643079
#% 643115
#% 659853
#! We consider a dynamic market-place of self-interested agents with differing capabilities. A task to be completed is proposed to the agent population. An agent attempts to form a coalition of agents to perform the task. Before proposing a coalition, the agent must determine the optimal set of agents with whom to enter into a coalition for this task; we refer to this activity as coalition calculation. To determine the optimal coalition, the agent must have a means of calculating the value of any given coalition. Multiple metrics (cost, time, quality etc.) determine the true value of a coalition. However, because of conflicting metrics, differing metric importance and the tendency of metric importance to vary over time, it is difficult to obtain a true valuation of a given coalition. Previous work has not addressed these issues. We present a solution based on the adaptation of a multi-objective optimization evolutionary algorithm. In order to obtain a true valuation of any coalition, we use the concept of Pareto dominance coupled with a distance weighting algorithm. We determine the Pareto optimal set of coalitions and then use an instance-based learning algorithm to select the optimal coalition. We show through empirical evaluation that the proposed technique is capable of eliciting metric importance and adapting to metric variation over time.

#index 770803
#* Efficient hierarchical MCMC for policy search
#@ Malcolm Strens
#t 2004
#c 19
#% 527859
#% 1227446
#! Many inference and optimization tasks in machine learning can be solved by sampling approaches such as Markov Chain Monte Carlo (MCMC) and simulated annealing. These methods can be slow if a single target density query requires many runs of a simulation (or a complete sweep of a training data set). We introduce a hierarchy of MCMC samplers that allow most steps to be taken in the solution space using only a small sample of simulation runs (or training examples). This is shown to accelerate learning in a policy search optimization task.

#index 770804
#* Learning to learn with the informative vector machine
#@ Neil D. Lawrence;John C. Platt
#t 2004
#c 19
#% 203329
#% 236497
#% 269206
#% 469390
#! This paper describes an efficient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task.

#index 770805
#* Kernel-based discriminative learning algorithms for labeling sequences, trees, and graphs
#@ Hisashi Kashima;Yuta Tsuboi
#t 2004
#c 19
#% 464434
#% 464640
#% 464644
#% 466736
#% 466892
#% 722803
#% 731607
#% 755851
#% 854636
#% 855284
#! We introduce a new perceptron-based discriminative learning algorithm for labeling structured data such as sequences, trees, and graphs. Since it is fully kernelized and uses pointwise label prediction, large features, including arbitrary number of hidden variables, can be incorporated with polynomial time complexity. This is in contrast to existing labelers that can handle only features of a small number of hidden variables, such as Maximum Entropy Markov Models and Conditional Random Fields. We also introduce several kernel functions for labeling sequences, trees, and graphs and efficient algorithms for them.

#index 770806
#* Learning to fly by combining reinforcement learning with behavioural cloning
#@ Eduardo F. Morales;Claude Sammut
#t 2004
#c 19
#% 425013
#% 464470
#% 464773
#% 465735
#% 466071
#% 466575
#! Reinforcement learning deals with learning optimal or near optimal policies while interacting with the environment. Application domains with many continuous variables are difficult to solve with existing reinforcement learning methods due to the large search space. In this paper, we use a relational representation to define powerful abstractions that allow us to incorporate domain knowledge and re-use previously learned policies in other similar problems. We also describe how to learn useful actions from human traces using a behavioural cloning approach combined with an exploration phase. Since several conflicting actions may be induced for the same abstract state, reinforcement learning is used to learn an optimal policy over this reduced space. It is shown experimentally how a combination of behavioural cloning and reinforcement learning using a relational representation is powerful enough to learn how to fly an aircraft through different points in space and different turbulence conditions.

#index 770807
#* Diverse ensembles for active learning
#@ Prem Melville;Raymond J. Mooney
#t 2004
#c 19
#% 115608
#% 116165
#% 136350
#% 170649
#% 236729
#% 290482
#% 317062
#% 464268
#% 466095
#% 529191
#% 565531
#% 785413
#% 1272282
#% 1279286
#% 1289273
#% 1478821
#! Query by Committee is an effective approach to selective sampling in which disagreement amongst an ensemble of hypotheses is used to select data for labeling. Query by Bagging and Query by Boosting are two practical implementations of this approach that use Bagging and Boosting, respectively, to build the committees. For effective active learning, it is critical that the committee be made up of consistent hypotheses that are very different from each other. DECORATE is a recently developed method that directly constructs such diverse committees using artificial training data. This paper introduces ACTIVE-DECORATE, which uses DECORATE committees to select good training examples. Extensive experimental results demonstrate that, in general, ACTIVE-DECORATE outperforms both Query by Bagging and Query by Boosting.

#index 770808
#* A Monte Carlo analysis of ensemble classification
#@ Roberto Esposito;Lorenza Saitta
#t 2004
#c 19
#% 36358
#% 73372
#% 209021
#% 214401
#% 276508
#% 443616
#% 451221
#% 464273
#% 562957
#% 562963
#% 1279285
#! In this paper we extend previous results providing a theoretical analysis of a new Monte Carlo ensemble classifier. The framework allows us to characterize the conditions under which the ensemble approach can be expected to outperform the single hypothesis classifier. Moreover, we provide a closed form expression for the distribution of the true ensemble accuracy, as well as of its mean and variance. We then exploit this result in order to analyze the expected error behavior in a particularly interesting case.

#index 770809
#* Towards tight bounds for rule learning
#@ Ulrich Rückert;Stefan Kramer
#t 2004
#c 19
#% 165663
#% 180945
#% 272501
#% 276522
#% 283138
#% 290482
#% 400847
#% 464297
#% 465922
#% 466744
#% 562962
#! While there is a lot of empirical evidence showing that traditional rule learning approaches work well in practice, it is nearly impossible to derive analytical results about their predictive accuracy. In this paper, we investigate rule-learning from a theoretical perspective. We show that the application of McAllester's PAC-Bayesian bound to rule learning yields a practical learning algorithm, which is based on ensembles of weighted rule sets. Experiments with the resulting learning algorithm show not only that it is competitive with state-of-the-art rule learners, but also that its error rate can often be bounded tightly. In fact, the bound turns out to be tighter than one of the "best" bounds for a practical learning scheme known so far (the Set Covering Machine). Finally, we prove that the bound can be further improved by allowing the learner to abstain from uncertain predictions.

#index 770810
#* Text categorization with many redundant features: using aggressive feature selection to make SVMs competitive with C4.5
#@ Evgeniy Gabrilovich;Shaul Markovitch
#t 2004
#c 19
#% 136350
#% 190581
#% 260001
#% 269217
#% 280817
#% 344447
#% 348148
#% 348184
#% 413637
#% 425047
#% 430761
#% 458379
#% 458389
#% 465754
#% 466101
#% 722935
#% 763708
#% 766438
#% 815240
#% 854646
#! Text categorization algorithms usually represent documents as bags of words and consequently have to deal with huge numbers of features. Most previous studies found that the majority of these features are relevant for classification, and that the performance of text categorization with support vector machines peaks when no feature selection is performed. We describe a class of text categorization problems that are characterized with many redundant features. Even though most of these features are relevant, the underlying concepts can be concisely captured using only a few features, while keeping all of them has substantially detrimental effect on categorization accuracy. We develop a novel measure that captures feature redundancy, and use it to analyze a large collection of datasets. We show that for problems plagued with numerous redundant features the performance of C4.5 is significantly superior to that of SVM, while aggressive feature selection allows SVM to beat C4.5 by a narrow margin.

#index 770811
#* Boosting margin based distance functions for clustering
#@ Tomer Hertz;Aharon Bar-Hillel;Daphna Weinshall
#t 2004
#c 19
#% 80995
#% 219845
#% 237663
#% 302391
#% 313959
#% 457926
#% 464291
#% 465746
#% 729437
#! The performance of graph based clustering methods critically depends on the quality of the distance function used to compute similarities between pairs of neighboring nodes. In this paper we learn distance functions by training binary classifiers with margins. The classifiers are defined over the product space of pairs of points and are trained to distinguish whether two points come from the same class or not. The signed margin is used as the distance value. Our main contribution is a distance learning method (DistBoost), which combines boosting hypotheses over the product space with a weak learner based on partitioning the original feature space. Each weak hypothesis is a Gaussian mixture model computed using a semi-supervised constrained EM algorithm, which is trained using both unlabeled and labeled data. We also consider SVM and decision trees boosting as margin based classifiers in the product space. We experimentally compare the margin based distance functions with other existing metric learning methods, and with existing techniques for the direct incorporation of constraints into various clustering algorithms. Clustering performance is measured on some benchmark databases from the UCI repository, a sample from the MNIST database, and a data set of color images of animals. In most cases the DistBoost algorithm significantly and robustly outperformed its competitors.

#index 770812
#* Convergence of synchronous reinforcement learning with linear function approximation
#@ Artur Merke;Ralf Schoknecht
#t 2004
#c 19
#% 15234
#% 207501
#% 238372
#% 393786
#% 449561
#% 464643
#% 527994
#! Synchronous reinforcement learning (RL) algorithms with linear function approximation are representable as inhomogeneous matrix iterations of a special form (Schoknecht & Merke, 2003). In this paper we state conditions of convergence for general inhomogeneous matrix iterations and prove that they are both necessary and sufficient. This result extends the work presented in (Schoknecht & Merke, 2003), where only a sufficient condition of convergence was proved. As the condition of convergence is necessary and sufficient, the new result is suitable to prove convergence and divergence of RL algorithms with function approximation. We use the theorem to deduce a new concise proof of convergence for the synchronous residual gradient algorithm (Baird, 1995). Moreover, we derive a counterexample for which the uniform RL algorithm (Merke & Schoknecht, 2002) diverges. This yields a negative answer to the open question if the uniform RL algorithm converges for arbitrary multiple transitions.

#index 770813
#* Locally linear metric adaptation for semi-supervised clustering
#@ Hong Chang;Dit-Yan Yeung
#t 2004
#c 19
#% 190429
#% 209623
#% 444007
#% 450876
#% 464291
#% 464608
#% 464631
#% 466890
#% 627825
#% 1279446
#! Many supervised and unsupervised learning algorithms are very sensitive to the choice of an appropriate distance metric. While classification tasks can make use of class label information for metric learning, such information is generally unavailable in conventional clustering tasks. Some recent research sought to address a variant of the conventional clustering problem called semi-supervised clustering, which performs clustering in the presence of some background knowledge or supervisory information expressed as pairwise similarity or dissimilarity constraints. However, existing metric learning methods for semi-supervised clustering mostly perform global metric learning through a linear transformation. In this paper, we propose a new metric learning method which performs nonlinear transformation globally but linear transformation locally. In particular, we formulate the learning problem as an optimization problem and present two methods for solving it. Through some toy data sets, we show empirically that our locally linear metric adaptation (LLMA) method can handle some difficult cases that cannot be handled satisfactorily by previous methods. We also demonstrate the effectiveness of our method on some real data sets.

#index 770814
#* Sequential skewing: an improved skewing algorithm
#@ Soumya Ray;David Page
#t 2004
#c 19
#% 136350
#% 1279300
#! This paper extends previous work on the Skewing algorithm, a promising approach that allows greedy decision tree induction algorithms to handle problematic functions such as parity functions with a lower run-time penalty than Lookahead. A deficiency of the previously proposed algorithm is its inability to scale up to high dimensional problems. In this paper, we describe a modified algorithm that scales better with increasing numbers of variables. We present experiments with randomly generated Boolean functions that evaluate the algorithm's response to increasing dimensions. We also evaluate the algorithm on a challenging real world biomedical problem, that of SH3 domain binding. Our results indicate that our algorithm almost always outperforms an information gain-based decision tree learner.

#index 770815
#* Automated hierarchical mixtures of probabilistic principal component analyzers
#@ Ting Su;Jennifer G. Dy
#t 2004
#c 19
#% 251155
#% 278040
#% 304879
#% 345824
#% 349210
#% 443966
#% 466414
#% 466425
#% 722902
#! Many clustering algorithms fail when dealing with high dimensional data. Principal component analysis (PCA) is a popular dimensionality reduction algorithm. However, it assumes a single multivariate Gaussian model, which provides a global linear projection of the data. Mixture of probabilistic principal component analyzers (PPCA) provides a better model to the clustering paradigm. It provides a local linear PCA projection for each multivariate Gaussian cluster component. We extend this model to build hierarchical mixtures of PPCA. Hierarchical clustering provides a flexible representation showing relationships among clusters in various perceptual levels. We introduce an automated hierarchical mixture of PPCA algorithm, which utilizes the integrated classification likelihood as a criterion for splitting and stopping the addition of hierarchical levels. An automated approach requires automated methods for initialization, determining the number of principal component dimensions, and determining when to split clusters. We address each of these in the paper. This automated approach results in a coarse to fine local component model with varying projections and with different number of dimensions for each cluster.

#index 770816
#* Unifying collaborative and content-based filtering
#@ Justin Basilico;Thomas Hofmann
#t 2004
#c 19
#% 124010
#% 173879
#% 202011
#% 220709
#% 260778
#% 266281
#% 283169
#% 301259
#% 330687
#% 465928
#% 578684
#% 1499473
#% 1650569
#! Collaborative and content-based filtering are two paradigms that have been applied in the context of recommender systems and user preference prediction. This paper proposes a novel, unified approach that systematically integrates all available training information such as past user-item ratings as well as attributes of items or users to learn a prediction function. The key ingredient of our method is the design of a suitable kernel or similarity function between user-item pairs that allows simultaneous generalization across the user and item dimensions. We propose an on-line algorithm (JRank) that generalizes perceptron learning. Experimental results on the EachMovie data set show significant improvements over standard approaches.

#index 770817
#* Delegating classifiers
#@ César Ferri;Peter Flach;José Hernández-Orallo
#t 2004
#c 19
#% 132938
#% 277919
#% 290482
#% 328946
#% 331909
#% 349550
#% 379342
#% 449508
#% 449566
#% 465922
#% 549438
#% 580510
#! A sensible use of classifiers must be based on the estimated reliability of their predictions. A cautious classifier would delegate the difficult or uncertain predictions to other, possibly more specialised, classifiers. In this paper we analyse and develop this idea of delegating classifiers in a systematic way. First, we design a two-step scenario where a first classifier chooses which examples to classify and delegates the difficult examples to train a second classifier. Secondly, we present an iterated scenario involving an arbitrary number of chained classifiers. We compare these scenarios to classical ensemble methods, such as bagging and boosting. We show experimentally that our approach is not far behind these methods in terms of accuracy, but with several advantages: (i) improved efficiency, since each classifier learns from fewer examples than the previous one; (ii) improved comprehensibility, since each classification derives from a single classifier; and (iii) the possibility to simplify the overall multi-classifier by removing the parts that lead to delegation.

#index 770818
#* Approximate inference by Markov chains on union spaces
#@ Max Welling;Michal Rosen-Zvi;Yee Whye Teh
#t 2004
#c 19
#% 176830
#% 1271994
#! A standard method for approximating averages in probabilistic models is to construct a Markov chain in the product space of the random variables with the desired equilibrium distribution. Since the number of configurations in this space grows exponentially with the number of random variables we often need to represent the distribution with samples. In this paper we show that if one is interested in averages over single variables only, an alternative Markov chain defined on the much smaller "union space", which can be evolved exactly, becomes feasible. The transition kernel of this Markov chain is based on conditional distributions for pairs of variables and we present ways to approximate them using approximate inference algorithms such as mean field, factorized neighbors and belief propagation. Robustness to these approximations and error bounds on the estimates follow from stability analysis for Markov chains. We also present ideas on a new class of algorithms that iterate between increasingly accurate estimates for conditional and marginal distributions. Experiments validate the proposed methods.

#index 770819
#* Redundant feature elimination for multi-class problems
#@ Annalisa Appice;Michelangelo Ceci;Simon Rawles;Peter Flach
#t 2004
#c 19
#% 73374
#% 90146
#% 103051
#% 126894
#% 136350
#% 169659
#% 191680
#% 235377
#% 243727
#% 280817
#% 290482
#% 311034
#% 392781
#% 458261
#% 466410
#% 856251
#! We consider the problem of eliminating redundant Boolean features for a given data set, where a feature is redundant if it separates the classes less well than another feature or set of features. Lavra&ccaron; et al. proposed the algorithm REDUCE that works by pairwise comparison of features, i.e., it eliminates a feature if it is redundant with respect to another feature. Their algorithm operates in an ILP setting and is restricted to two-class problems. In this paper we improve their method and extend it to multiple classes. Central to our approach is the notion of a neighbourhood of examples: a set of examples of the same class where the number of different features between examples is relatively small. Redundant features are eliminated by applying a revised version of the REDUCE method to each pair of neighbourhoods of different class. We analyse the performance of our method on a range of data sets.

#index 770820
#* C4.5 competence map: a phase transition-inspired approach
#@ Nicolas Baskiotis;Michèle Sebag
#t 2004
#c 19
#% 697
#% 73372
#% 136350
#% 156186
#% 169653
#% 180945
#% 190581
#% 290482
#% 314784
#% 425004
#% 458672
#% 466722
#% 722941
#% 723257
#! How to determine a priori whether a learning algorithm is suited to a learning problem instance is a major scientific and technological challenge. A first step toward this goal, inspired by the Phase Transition (PT) paradigm developed in the Constraint Satisfaction domain, is presented in this paper.Based on the PT paradigm, extensive and principled experiments allow for constructing the Competence Map associated to a learning algorithm, describing the regions where this algorithm on average fails or succeeds. The approach is illustrated on the long and widely used C4.5 algorithm. A non trivial failure region in the landscape of k-term DNF languages is observed and some interpretations are offered for the experimental results.

#index 770821
#* A needle in a haystack: local one-class optimization
#@ Koby Crammer;Gal Chechik
#t 2004
#c 19
#% 382854
#% 722810
#% 855602
#! This paper addresses the problem of finding a small and coherent subset of points in a given data. This problem, sometimes referred to as one-class or set covering, requires to find a small-radius ball that covers as many data points as possible. It rises naturally in a wide range of applications, from finding gene-modules to extracting documents' topics, where many data points are irrelevant to the task at hand, or in applications where only positive examples are available. Most previous approaches to this problem focus on identifying and discarding a possible set of outliers. In this paper we adopt an opposite approach which directly aims to find a small set of coherently structured regions, by using a loss function that focuses on local properties of the data. We formalize the learning task as an optimization problem using the Information-Bottleneck principle. An algorithm to solve this optimization problem is then derived and analyzed. Experiments on gene expression data and a text document corpus demonstrate the merits of our approach.

#index 770822
#* Model selection via the AUC
#@ Saharon Rosset
#t 2004
#c 19
#% 723244
#% 1279288
#! We present a statistical analysis of the AUC as an evaluation criterion for classification scoring models. First, we consider significance tests for the difference between AUC scores of two algorithms on the same test set. We derive exact moments under simplifying assumptions and use them to examine approximate practical methods from the literature. We then compare AUC to empirical misclassification error when the prediction goal is to minimize future error rate. We show that the AUC may be preferable to empirical error even in this case and discuss the tradeoff between approximation error and estimation error underlying this phenomenon.

#index 770823
#* Bellman goes relational
#@ Kristian Kersting;Martijn Van Otterlo;Luc De Raedt
#t 2004
#c 19
#% 44624
#% 236730
#% 382569
#% 384911
#% 425013
#% 644560
#% 816084
#% 1279355
#% 1289241
#! Motivated by the interest in relational reinforcement learning, we introduce a novel relational Bellman update operator called REBEL. It employs a constraint logic programming language to compactly represent Markov decision processes over relational domains. Using REBEL, a novel value iteration algorithm is developed in which abstraction (over states and actions) plays a major role. This framework provides new insights into relational reinforcement learning. Convergence results as well as experiments are presented.

#index 770824
#* Bias and variance in value function estimation
#@ Shie Mannor;Duncan Simester;Peng Sun;John N. Tsitsiklis
#t 2004
#c 19
#% 58474
#% 215664
#% 266287
#% 270290
#% 425075
#% 466731
#! We consider the bias and variance of value function estimation that are caused by using an empirical model instead of the true model. We analyze these bias and variance for Markov processes from a classical (frequentist) statistical point of view, and in a Bayesian setting. Using a second order approximation, we provide explicit expressions for the bias and variance in terms of the transition counts and the reward statistics. We present supporting experiments with artificial Markov chains and with a large transactional database provided by a mail-order catalog firm.

#index 770825
#* Learning to cluster using local neighborhood structure
#@ Rómer Rosales;Kannan Achan;Brendan Frey
#t 2004
#c 19
#% 44876
#% 313959
#% 594009
#% 765552
#% 1672995
#% 1810385
#% 1848680
#! This paper introduces an approach for clustering/classification which is based on the use of local, high-order structure present in the data. For some problems, this local structure might be more relevant for classification than other measures of point similarity used by popular unsupervised and semi-supervised clustering methods. Under this approach, changes in the class label are associated to changes in the local properties of the data. Using this idea, we also pursue to learn how to cluster given examples of clustered data (including from different datasets). We make these concepts formal by presenting a probability model that captures their fundamentals and show that in this setting, learning to cluster is a well defined and tractable task. Based on probabilistic inference methods, we then present an algorithm for computing the posterior probability distribution of class labels for each data point. Experiments in the domain of spatial grouping and functional gene classification are used to illustrate and test these concepts.

#index 770826
#* Entropy-based criterion in categorical clustering
#@ Tao Li;Sheng Ma;Mitsunori Ogihara
#t 2004
#c 19
#% 115608
#% 245365
#% 280419
#% 314054
#% 374580
#% 413618
#% 420081
#% 479659
#% 730050
#! Entropy-type measures for the heterogeneity of clusters have been used for a long time. This paper studies the entropy-based criterion in clustering categorical data. It first shows that the entropy-based criterion can be derived in the formal framework of probabilistic clustering models and establishes the connection between the criterion and the approach based on dissimilarity co-efficients. An iterative Monte-Carlo procedure is then presented to search for the partitions minimizing the criterion. Experiments are conducted to show the effectiveness of the proposed procedure.

#index 770827
#* SVM-based generalized multiple-instance learning via approximate box counting
#@ Qingping Tao;Stephen Scott;N. V. Vinodchandran;Thomas Takeo Osugi
#t 2004
#c 19
#% 58608
#% 101898
#% 224755
#% 269217
#% 272527
#% 333328
#% 458667
#% 464621
#% 465916
#% 466927
#% 565537
#% 722913
#! The multiple-instance learning (MIL) model has been very successful in application areas such as drug discovery and content-based image-retrieval. Recently, a generalization of this model and an algorithm for this generalization were introduced, showing significant advantages over the conventional MIL model in certain application areas. Unfortunately, this algorithm is inherently inefficient, preventing scaling to high dimensions. We reformulate this algorithm using a kernel for a support vector machine, reducing its time complexity from exponential to polynomial. Computing the kernel is equivalent to counting the number of axis-parallel boxes in a discrete, bounded space that contain at least one point from each of two multisets P and Q. We show that this problem is #P-complete, but then give a fully polynomial randomized approximation scheme (FPRAS) for it. Finally, we empirically evaluate our kernel.

#index 770828
#* Tractable learning of large Bayes net structures from sparse data
#@ Anna Goldenberg;Andrew Moore
#t 2004
#c 19
#% 101217
#% 152934
#% 197387
#% 316709
#% 481290
#% 577271
#% 669247
#% 727667
#% 1272326
#% 1650278
#% 1650289
#% 1650569
#! This paper addresses three questions. Is it useful to attempt to learn a Bayesian network structure with hundreds of thousands of nodes? How should such structure search proceed practically? The third question arises out of our approach to the second: how can Frequent Sets (Agrawal et al., 1993), which are extremely popular in the area of descriptive data mining, be turned into a probabilistic model?Large sparse datasets with hundreds of thousands of records and attributes appear in social networks, warehousing, supermarket transactions and web logs. The complexity of structural search made learning of factored probabilistic models on such datasets unfeasible. We propose to use Frequent Sets to significantly speed up the structural search. Unlike previous approaches, we not only cache n-way sufficient statistics, but also exploit their local structure. We also present an empirical evaluation of our algorithm applied to several massive datasets.

#index 770829
#* Linearized cluster assignment via spectral ordering
#@ Chris Ding;Xiaofeng He
#t 2004
#c 19
#% 152663
#% 313959
#% 466675
#% 478768
#% 629653
#% 724227
#! Spectral clustering uses eigenvectors of the Laplacian of the similarity matrix. They are most conveniently applied to 2-way clustering problems. When applying to multi-way clustering, either the 2-way spectral clustering is recursively applied or an embedding to spectral space is done and some other methods are used to cluster the points. Here we propose and study a K-way cluster assignment method. The method transforms the problem to find valleys and peaks of a 1-D quantity called cluster crossing, which measures the symmetric cluster overlap across a cut point along a linear ordering of the data points. The method can either determine K clusters in one shot or recursively split a current cluster into several smaller ones. We show that a linear ordering based on a distance sensitive objective has a continuous solution which is the eigenvector of the Laplacian, showing the close relationship between clustering and ordering. The method relies on the connectivity matrix constructed as the truncated spectral expansion of the similarity matrix, useful for revealing cluster structure. The method is applied to newsgroups to illustrate introduced concepts; experiments show it outperforms the recursive 2-way clustering and the standard K-means clustering.

#index 770830
#* K-means clustering via principal component analysis
#@ Chris Ding;Xiaofeng He
#t 2004
#c 19
#% 36672
#% 114667
#% 224113
#% 304932
#% 375388
#% 466083
#% 627408
#% 696472
#% 729437
#! Principal component analysis (PCA) is a widely used statistical technique for unsupervised dimension reduction. K-means clustering is a commonly used data clustering for performing unsupervised learning tasks. Here we prove that principal components are the continuous solutions to the discrete cluster membership indicators for K-means clustering. New lower bounds for K-means objective function are derived, which is the total variance minus the eigenvalues of the data covariance matrix. These results indicate that unsupervised dimension reduction is closely related to unsupervised learning. Several implications are discussed. On dimension reduction, the result provides new insights to the observed effectiveness of PCA-based data reductions, beyond the conventional noise-reduction explanation that PCA, via singular value decomposition, provides the best low-dimensional linear approximation of the data. On learning, the result suggests effective techniques for K-means data clustering. DNA gene expression and Internet newsgroups are analyzed to illustrate our results. Experiments indicate that the new bounds are within 0.5-1.5% of the optimal values.

#index 770831
#* A fast iterative algorithm for fisher discriminant using heterogeneous kernels
#@ Glenn Fung;Murat Dundar;Jinbo Bi;Bharat Rao
#t 2004
#c 19
#% 80995
#% 190581
#% 292664
#% 309208
#% 342598
#% 376266
#% 384950
#% 529216
#% 577213
#% 735256
#% 763697
#! We propose a fast iterative classification algorithm for Kernel Fisher Discriminant (KFD) using heterogeneous kernel models. In contrast with the standard KFD that requires the user to predefine a kernel function, we incorporate the task of choosing an appropriate kernel into the optimization problem to be solved. The choice of kernel is defined as a linear combination of kernels belonging to a potentially large family of different positive semidefinite kernels. The complexity of our algorithm does not increase significantly with respect to the number of kernels on the kernel family. Experiments on several benchmark datasets demonstrate that generalization performance of the proposed algorithm is not significantly different from that achieved by the standard KFD in which the kernel parameters have been tuned using cross validation. We also present results on a real-life colon cancer dataset that demonstrate the efficiency of the proposed method.

#index 770832
#* Sparse cooperative Q-learning
#@ Jelle R. Kok;Nikos Vlassis
#t 2004
#c 19
#% 124691
#% 164502
#% 266286
#% 274912
#% 384911
#% 466262
#% 565550
#% 578694
#% 746746
#! Learning in multiagent systems suffers from the fact that both the state and the action space scale exponentially with the number of agents. In this paper we are interested in using Q-learning to learn the coordinated actions of a group of cooperative agents, using a sparse representation of the joint state-action space of the agents. We first examine a compact representation in which the agents need to explicitly coordinate their actions only in a predefined set of states. Next, we use a coordination-graph approach in which we represent the Q-values by value rules that specify the coordination dependencies of the agents at particular states. We show how Q-learning can be efficiently applied to learn a coordinated policy for the agents in the above framework. We demonstrate the proposed method on the predator-prey domain, and we compare it with other related multiagent Q-learning methods.

#index 770833
#* Adaptive cognitive orthotics: combining reinforcement learning and constraint-based temporal reasoning
#@ Matthew Rudary;Satinder Singh;Martha E. Pollack
#t 2004
#c 19
#% 107137
#% 384911
#% 466394
#% 466430
#% 736897
#% 817553
#% 1271967
#% 1272032
#% 1290042
#! Reminder systems support people with impaired prospective memory and/or executive function, by providing them with reminders of their functional daily activities. We integrate temporal constraint reasoning with reinforcement learning (RL) to build an adaptive reminder system and in a simulated environment demonstrate that it can personalize to a user and adapt to both short- and long-term changes. In addition to advancing the application domain, our integrated algorithm contributes to research on temporal constraint reasoning by showing how RL can select an optimal policy from amongst a set of temporally consistent ones, and it contributes to the work on RL by showing how temporal constraint reasoning can be used to dramatically reduce the space of actions from which an RL agent needs to learn.

#index 770834
#* A maximum entropy approach to species distribution modeling
#@ Steven J. Phillips;Miroslav Dudík;Robert E. Schapire
#t 2004
#c 19
#% 133258
#% 190434
#% 211044
#% 226495
#% 425065
#% 815864
#% 854813
#% 1673039
#! We study the problem of modeling species geographic distributions, a critical problem in conservation biology. We propose the use of maximum-entropy techniques for this problem, specifically, sequential-update algorithms that can handle a very large number of features. We describe experiments comparing maxent with a standard distribution-modeling tool, called GARP, on a dataset containing observation data for North American breeding birds. We also study how well maxent performs as a function of the number of training examples and training time, analyze the use of regularization to avoid overfitting when the number of examples is small, and explore the interpretability of models constructed using maxent.

#index 770835
#* Learning probabilistic motion models for mobile robots
#@ Austin I. Eliazar;Ronald Parr
#t 2004
#c 19
#% 82083
#% 290714
#% 580300
#! Machine learning methods are often applied to the problem of learning a map from a robot's sensor data, but they are rarely applied to the problem of learning a robot's motion model. The motion model, which can be influenced by robot idiosyncrasies and terrain properties, is a crucial aspect of current algorithms for Simultaneous Localization and Mapping (SLAM). In this paper we concentrate on generating the correct motion model for a robot by applying EM methods in conjunction with a current SLAM algorithm. In contrast to previous calibration approaches, we not only estimate the mean of the motion, but also the interdependencies between motion terms, and the variances in these terms. This can be used to provide a more focused proposal distribution to a particle filter used in a SLAM algorithm, which can reduce the resources needed for localization while decreasing the chance of losing track of the robot's position. We validate this approach by recovering a good motion model despite initialization with a poor one. Further experiments validate the generality of the learned model in similar circumstances.

#index 770836
#* Solving cluster ensemble problems by bipartite graph partitioning
#@ Xiaoli Zhang Fern;Carla E. Brodley
#t 2004
#c 19
#% 115608
#% 274612
#% 309128
#% 313959
#% 342621
#% 579655
#% 722902
#% 727903
#% 729918
#! A critical problem in cluster ensemble research is how to combine multiple clusterings to yield a final superior clustering result. Leveraging advanced graph partitioning techniques, we solve this problem by reducing it to a graph partitioning problem. We introduce a new reduction method that constructs a bipartite graph from a given cluster ensemble. The resulting graph models both instances and clusters of the ensemble simultaneously as vertices in the graph. Our approach retains all of the information provided by a given ensemble, allowing the similarity among instances and the similarity among clusters to be considered collectively in forming the final clustering. Further, the resulting graph partitioning problem can be solved efficiently. We empirically evaluate the proposed approach against two commonly used graph formulations and show that it is more robust and achieves comparable or better performance in comparison to its competitors.

#index 770837
#* Links between perceptrons, MLPs and SVMs
#@ Ronan Collobert;Samy Bengio
#t 2004
#c 19
#% 33917
#% 61477
#% 92148
#% 190581
#% 269217
#% 361100
#% 476873
#% 493292
#% 729437
#! We propose to study links between three important classification algorithms: Perceptrons, Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs). We first study ways to control the capacity of Perceptrons (mainly regularization parameters and early stopping), using the margin idea introduced with SVMs. After showing that under simple conditions a Perceptron is equivalent to an SVM, we show it can be computationally expensive in time to train an SVM (and thus a Perceptron) with stochastic gradient descent, mainly because of the margin maximization term in the cost function. We then show that if we remove this margin maximization term, the learning rate or the use of early stopping can still control the margin. These ideas are extended afterward to the case of MLPs. Moreover, under some assumptions it also appears that MLPs are a kind of mixture of SVMs, maximizing the margin in the hidden layer space. Finally, we present a very simple MLP based on the previous findings, which yields better performances in generalization and speed than the other models.

#index 770838
#* Nonparametric classification with polynomial MPMC cascades
#@ Sander M. Bohte;Markus Breitenbach;Gregory Z. Grudic
#t 2004
#c 19
#% 94926
#% 276506
#% 400847
#% 722901
#% 1271831
#! A new class of nonparametric algorithms for high-dimensional binary classification is proposed using cascades of low dimensional polynomial structures. Construction of polynomial cascades is based on Minimax Probability Machine Classification (MPMC), which results in direct estimates of classification accuracy, and provides a simple stopping criteria that does not require expensive cross-validation measures. This Polynomial MPMC Cascade (PMC) algorithm is constructed in linear time with respect to the input space dimensionality, and linear time in the number of examples, making it a potentially attractive alternative to algorithms like support vector machines and standard MPMC. Experimental evidence is given showing that, compared to state-of-the-art classifiers, PMCs are competitive; inherently fast to compute; not prone to overfitting; and generally yield accurate estimates of the maximum error rate on unseen data.

#index 770839
#* A kernel view of the dimensionality reduction of manifolds
#@ Jihun Ham;Daniel D. Lee;Sebastian Mika;Bernhard Schölkopf
#t 2004
#c 19
#% 266426
#% 316228
#% 464615
#% 593047
#% 635713
#! We interpret several well-known algorithms for dimensionality reduction of manifolds as kernel methods. Isomap, graph Laplacian eigenmap, and locally linear embedding (LLE) all utilize local neighborhood information to construct a global embedding of the manifold. We show how all three algorithms can be described as kernel PCA on specially constructed Gram matrices, and illustrate the similarities and differences between the algorithms with representative examples.

#index 770840
#* Predictive automatic relevance determination by expectation propagation
#@ Yuan (Alan) Qi;Thomas P. Minka;Rosalind W. Picard;Zoubin Ghahramani
#t 2004
#c 19
#% 132676
#% 331916
#% 360691
#% 528330
#% 722929
#% 857429
#! In many real-world classification problems the input contains a large number of potentially irrelevant features. This paper proposes a new Bayesian framework for determining the relevance of input features. This approach extends one of the most successful Bayesian methods for feature selection and sparse learning, known as Automatic Relevance Determination (ARD). ARD finds the relevance of features by optimizing the model marginal likelihood, also known as the evidence. We show that this can lead to overfitting. To address this problem, we propose Predictive ARD based on estimating the predictive performance of the classifier. While the actual leave-one-out predictive performance is generally very costly to compute, the expectation propagation (EP) algorithm proposed by Minka provides an estimate of this predictive performance as a side-effect of its iterations. We exploit this in our algorithm to do feature selection, and to select data points in a sparse Bayesian kernel classifier. Moreover, we provide two other improvements to previous algorithms, by replacing Laplace's approximation with the generally more accurate EP, and by incorporating the fast optimization algorithm proposed by Faul and Tipping. Our experiments show that our method based on the EP estimate of predictive performance is more accurate on test data than relevance determination by optimizing the evidence.

#index 770841
#* Take a walk and cluster genes: a TSP-based approach to optimal rearrangement clustering
#@ Sharlee Climer;Weixiong Zhang
#t 2004
#c 19
#% 52889
#% 318032
#% 542590
#% 584676
#! Cluster analysis is a fundamental problem and technique in many areas related to machine learning. In this paper, we consider rearrangement clustering, which is the problem of finding sets of objects that share common or similar features by arranging the rows (objects) of a matrix (specifying object features) in such a way that adjacent objects are similar to each other (based on a similarity measure of the features) so as to maximize the overall similarity. Based on formulating this problem as the Traveling Salesman Problem (TSP), we develop a new TSP-based optimal clustering algorithm called TSPCluster. We overcome a flaw that is inherent in previous approaches by relaxing restrictions on dissimilarities between clusters. Our new algorithm has three important features: finding the optimal k clusters for a given k, automatically detecting cluster borders, and ascertaining a set of most viable clustering results that make good balances among maximizing the overall similarity within clusters and dissimilarity between clusters. We apply TSPCluster to cluster and display ~500 genes of flowering plant Arabidopsis which are regulated under various abiotic stress conditions. We compare TSPCluster to the bond energy algorithm and two existing clustering algorithms. Our TSPCluster code is available at (Climer & Zhang, 2004).

#index 770842
#* Relational sequential inference with reliable observations
#@ Alan Fern;Robert Givan
#t 2004
#c 19
#% 36683
#% 75936
#% 226437
#% 464434
#% 479726
#% 724234
#% 736898
#% 853859
#% 1272377
#% 1279354
#% 1650403
#! We present a trainable sequential-inference technique for processes with large state and observation spaces and relational structure. Our method assumes "reliable observations", i.e. that each process state persists long enough to be reliably inferred from the observations it generates. We introduce the idea of a "state-inference function" (from observation sequences to underlying hidden states) for representing knowledge about a process and develop an efficient sequential-inference algorithm, utilizing this function, that is correct for processes that generate reliable observations consistent with the state-inference function. We describe a representation for state-inference functions in relational domains and give a corresponding supervised learning algorithm. Experiments, in relational video interpretation, show that our technique provides significantly improved accuracy and speed relative to a variety of recent, hand-coded, non-trainable systems.

#index 770843
#* A theoretical characterization of linear SVM-based feature selection
#@ Douglas Hardin;Ioannis Tsamardinos;Constantin F. Aliferis
#t 2004
#c 19
#% 190581
#% 243728
#% 390723
#% 425048
#% 722929
#% 729990
#% 734919
#% 1650674
#% 1861219
#! Most prevalent techniques in Support Vector Machine (SVM) feature selection are based on the intuition that the weights of features that are close to zero are not required for optimal classification. In this paper we show that indeed, in the sample limit, the irrelevant variables (in a theoretical and optimal sense) will be given zero weight by a linear SVM, both in the soft and the hard margin case. However, SVM-based methods have certain theoretical disadvantages too. We present examples where the linear SVM may assign zero weights to strongly relevant variables (i.e., variables required for optimal estimation of the distribution of the target variable) and where weakly relevant features (i.e., features that are superfluous for optimal feature selection given other features) may get non-zero weights. We contrast and theoretically compare with Markov-Blanket based feature selection algorithms that do not have such disadvantages in a broad class of distributions and could also be used for causal discovery.

#index 770844
#* Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data
#@ Charles Sutton;Khashayar Rohanimanesh;Andrew McCallum
#t 2004
#c 19
#% 75936
#% 179800
#% 211044
#% 246836
#% 279755
#% 292235
#% 464434
#% 466892
#% 643004
#% 715096
#% 715615
#% 716892
#% 740916
#% 816081
#% 816181
#% 853697
#% 854813
#% 1272356
#% 1279274
#% 1279275
#% 1650318
#% 1650403
#! In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data.

#index 770845
#* Bayesian haplo-type inference via the dirichlet process
#@ Eric Xing;Roded Sharan;Michael I. Jordan
#t 2004
#c 19
#% 397647
#% 451128
#% 674186
#! The problem of inferring haplotypes from genotypes of single nucleotide polymorphisms (SNPs) is essential for the understanding of genetic variation within and among populations, with important applications to the genetic analysis of disease propensities and other complex traits. The problem can be formulated as a mixture model, where the mixture components correspond to the pool of haplotypes in the population. The size of this pool is unknown; indeed, knowing the size of the pool would correspond to knowing something significant about the genome and its history. Thus methods for fitting the genotype mixture must crucially address the problem of estimating a mixture with an unknown number of mixture components. In this paper we present a Bayesian approach to this problem based on a nonparametric prior known as the Dirichlet process. The model also incorporates a likelihood that captures statistical errors in the haplotype/genotype relationship. We apply our approach to the analysis of both simulated and real genotype data, and compare to extant methods.

#index 770846
#* Multiple kernel learning, conic duality, and the SMO algorithm
#@ Francis R. Bach;Gert R. G. Lanckriet;Michael I. Jordan
#t 2004
#c 19
#% 269217
#% 269218
#% 416838
#% 425040
#% 466084
#% 763697
#% 856251
#! While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes.

#index 770847
#* Learning and evaluating classifiers under sample selection bias
#@ Bianca Zadrozny
#t 2004
#c 19
#% 17144
#% 136350
#% 269217
#% 341700
#% 361100
#% 466759
#% 727925
#% 1289281
#! Classifier learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classifier learning methods are affected by it. We also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias.

#index 770848
#* Multi-task feature and kernel selection for SVMs
#@ Tony Jebara
#t 2004
#c 19
#% 203329
#% 236497
#% 267027
#% 527857
#% 565549
#% 1271814
#! We compute a common feature selection or kernel selection configuration for multiple support vector machines (SVMs) trained on different yet inter-related datasets. The method is advantageous when multiple classification tasks and differently labeled datasets exist over a common input space. Different datasets can mutually reinforce a common choice of representation or relevant features for their various classifiers. We derive a multi-task representation learning approach using the maximum entropy discrimination formalism. The resulting convex algorithms maintain the global solution properties of support vector machines. However, in addition to multiple SVM classification/regression parameters they also jointly estimate an optimal subset of features or optimal combination of kernels. Experiments are shown on standardized datasets.

#index 770849
#* A hierarchical method for multi-class support vector machines
#@ Volkan Vural;Jennifer G. Dy
#t 2004
#c 19
#% 80995
#% 190581
#% 309141
#% 310560
#% 328945
#% 562950
#! We introduce a framework, which we call Divide-by-2 (DB2), for extending support vector machines (SVM) to multi-class problems. DB2 offers an alternative to the standard one-against-one and one-against-rest algorithms. For an N class problem, DB2 produces an N − 1 node binary decision tree where nodes represent decision boundaries formed by N − 1 SVM binary classifiers. This tree structure allows us to present a generalization and a time complexity analysis of DB2. Our analysis and related experiments show that, DB2 is faster than one-against-one and one-against-rest algorithms in terms of testing time, significantly faster than one-against-rest in terms of training time, and that the cross-validation accuracy of DB2 is comparable to these two methods.

#index 770850
#* Training conditional random fields via gradient tree boosting
#@ Thomas G. Dietterich;Adam Ashenfelter;Yaroslav Bulatov
#t 2004
#c 19
#% 136350
#% 464434
#% 466892
#% 479726
#% 1673026
#! Conditional Random Fields (CRFs; Lafferty, McCallum, & Pereira, 2001) provide a flexible and powerful model for learning to assign labels to elements of sequences in such applications as part-of-speech tagging, text-to-speech mapping, protein and DNA sequence analysis, and information extraction from web pages. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features. This paper describes a new method for training CRFs by applying Friedman's (1999) gradient tree boosting method. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees. Regression trees are learned by stage-wise optimizations similar to Adaboost, but with the objective of maximizing the conditional likelihood P(Y|X) of the CRF model. By growing regression trees, interactions among features are introduced only as needed, so although the parameter space is potentially immense, the search algorithm does not explicitly consider the large space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially like previous algorithms based on iterative scaling and gradient descent.

#index 770851
#* Semi-supervised learning using randomized mincuts
#@ Avrim Blum;John Lafferty;Mugizi Robert Rwebangira;Rajashekar Reddy
#t 2004
#c 19
#% 103951
#% 143314
#% 348535
#% 431293
#% 443790
#% 565545
#% 593975
#% 749440
#! In many application domains there is a large amount of unlabeled data but only a very limited amount of labeled training data. One general approach that has been explored for utilizing this unlabeled data is to construct a graph on all the data points based on distance relationships among examples, and then to use the known labels to perform some type of graph partitioning. One natural partitioning to use is the minimum cut that agrees with the labeled data (Blum & Chawla, 2001), which can be thought of as giving the most probable label assignment if one views labels as generated according to a Markov Random Field on the graph. Zhu et al. (2003) propose a cut based on a relaxation of this field, and Joachims (2003) gives an algorithm based on finding an approximate min-ratio cut.In this paper, we extend the mincut approach by adding randomness to the graph structure. The resulting algorithm addresses several short-comings of the basic mincut approach, and can be given theoretical justification from both a Markov random field perspective and from sample complexity considerations. In cases where the graph does not have small cuts for a given classification problem, randomization may not help. However, our experiments on several datasets show that when the structure of the graph supports small cuts, this can result in highly accurate classifiers with good accuracy/coverage tradeoffs. In addition, we are able to achieve good performance with a very simple graph-construction procedure.

#index 770852
#* Apprenticeship learning via inverse reinforcement learning
#@ Pieter Abbeel;Andrew Y. Ng
#t 2004
#c 19
#% 78916
#% 190581
#% 465735
#% 465902
#% 466230
#% 466418
#% 661886
#% 770852
#! We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.

#index 770853
#* An information theoretic analysis of maximum likelihood mixture estimation for exponential families
#@ Arindam Banerjee;Inderjit Dhillon;Joydeep Ghosh;Srujana Merugu
#t 2004
#c 19
#% 115608
#% 277483
#% 425021
#% 562954
#% 722934
#% 1650729
#! An important task in unsupervised learning is maximum likelihood mixture estimation (MLME) for exponential families. In this paper, we prove a mathematical equivalence between this MLME problem and the rate distortion problem for Bregman divergences. We also present new theoretical results in rate distortion theory for Bregman divergences. Further, an analysis of the problems as a trade-off between compression and preservation of information is presented that yields the information bottleneck method as an interesting special case.

#index 770854
#* Ensemble selection from libraries of models
#@ Rich Caruana;Alexandru Niculescu-Mizil;Geoff Crew;Alex Ksikes
#t 2004
#c 19
#% 132938
#% 209021
#% 243728
#% 283145
#% 466583
#% 466725
#% 580510
#% 1272365
#! We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using different learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area. Experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection.

#index 770855
#* Gaussian process classification for segmenting and annotating sequences
#@ Yasemin Altun;Thomas Hofmann;Alexander J. Smola
#t 2004
#c 19
#% 268069
#% 450245
#% 464434
#% 466597
#% 466892
#% 577213
#% 669219
#% 715096
#% 722816
#% 854636
#% 857429
#% 1650793
#% 1860734
#! Many real-world classification tasks involve the prediction of multiple, inter-dependent class labels. A prototypical case of this sort deals with prediction of a sequence of labels for a sequence of observations. Such problems arise naturally in the context of annotating and segmenting observation sequences. This paper generalizes Gaussian Process classification to predict multiple labels by taking dependencies between neighboring labels into account. Our approach is motivated by the desire to retain rigorous probabilistic semantics, while overcoming limitations of parametric methods like Conditional Random Fields, which exhibit conceptual and computational difficulties in high-dimensional input spaces. Experiments on named entity recognition and pitch accent prediction tasks demonstrate the competitiveness of our approach.

#index 770856
#* Distribution kernels based on moments of counts
#@ Corinna Cortes;Mehryar Mohri
#t 2004
#c 19
#% 116149
#% 190581
#% 197394
#% 292340
#% 741066
#! Many applications in text and speech processing require the analysis of distributions of variable-length sequences. We recently introduced a general kernel framework, rational kernels, to extend kernel methods to the analysis of such variable-length sequences or more generally weighted automata. These kernels are efficient to compute and have been successfully used in applications such as spoken-dialog classification using Support Vector Machines.However, the rational kernels previously introduced do not fully encompass distributions over alternate sequences. Prior similarity measures between two weighted automata are based only on the expected counts of co-occurring subsequences and ignore similarities (or dissimilarities) in higher order moments of the distributions of these counts.In this paper, we introduce a new family of rational kernels, moment kernels, that precisely exploit this additional information. These kernels are distribution kernels based on moments of counts of strings. We describe efficient algorithms to compute moment kernels and apply them to several difficult spoken-dialog classification tasks. Our experiments show that using the second moment of the counts of n-gram sequences consistently improves the classification accuracy in these tasks.

#index 770857
#* Feature selection, L1 vs. L2 regularization, and rotational invariance
#@ Andrew Y. Ng
#t 2004
#c 19
#% 173879
#% 272381
#% 277483
#% 397153
#% 450888
#% 1650569
#! We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn "well,") grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invariant algorithm---including logistic regression with L2 regularization, SVMs, and neural networks trained by backpropagation---has a worst case sample complexity that grows at least linearly in the number of irrelevant features.

#index 770858
#* Improving SVM accuracy by training on auxiliary data sources
#@ Pengcheng Wu;Thomas G. Dietterich
#t 2004
#c 19
#% 252011
#% 304876
#% 309208
#% 444027
#% 640416
#% 1854922
#! The standard model of supervised learning assumes that training and test data are drawn from the same underlying distribution. This paper explores an application in which a second, auxiliary, source of data is available drawn from a different distribution. This auxiliary data is more plentiful, but of significantly lower quality, than the training and test data. In the SVM framework, a training example has two roles: (a) as a data point to constrain the learning process and (b) as a candidate support vector that can form part of the definition of the classifier. The paper considers using the auxiliary data in either (or both) of these roles. This auxiliary data framework is applied to a problem of classifying images of leaves of maple and oak trees using a kernel derived from the shapes of the leaves. Experiments show that when the training data set is very small, training with auxiliary data can produce large improvements in accuracy, even when the auxiliary data is significantly different from the training (and test) data. The paper also introduces techniques for adjusting the kernel scores of the auxiliary data points to make them more comparable to the training data points.

#index 770859
#* The multiple multiplicative factor model for collaborative filtering
#@ Benjamin Marlin;Richard S. Zemel
#t 2004
#c 19
#% 173879
#% 272381
#% 277483
#% 397153
#% 450888
#% 1650569
#! We describe a class of causal, discrete latent variable models called Multiple Multiplicative Factor models (MMFs). A data vector is represented in the latent space as a vector of factors that have discrete, non-negative expression levels. Each factor proposes a distribution over the data vector. The distinguishing feature of MMFs is that they combine the factors' proposed distributions multiplicatively, taking into account factor expression levels. The product formulation of MMFs allow factors to specialize to a subset of the items, while the causal generative semantics mean MMFs can readily accommodate missing data. This makes MMFs distinct from both directed models with mixture semantics and undirected product models. In this paper we present empirical results from the collaborative filtering domain showing that a binary/multinomial MMF model matches the performance of the best existing models while learning an interesting latent space description of the users.

#index 770860
#* Decentralized detection and classification using kernel methods
#@ XuanLong Nguyen;Martin J. Wainwright;Michael I. Jordan
#t 2004
#c 19
#% 304917
#% 365897
#! We consider the problem of decentralized detection under constraints on the number of bits that can be transmitted by each sensor. In contrast to most previous work, in which the joint distribution of sensor observations is assumed to be known, we address the problem when only a set of empirical samples is available. We propose a novel algorithm using the framework of empirical risk minimization and marginalized kernels, and analyze its computational and statistical properties both theoretically and empirically. We provide an efficient implementation of the algorithm, and demonstrate its performance on both simulated and real data sets.

#index 770861
#* Variational methods for the Dirichlet process
#@ David M. Blei;Michael I. Jordan
#t 2004
#c 19
#% 303620
#% 450275
#! Variational inference methods, including mean field methods and loopy belief propagation, have been widely used for approximate probabilistic inference in graphical models. While often less accurate than MCMC, variational methods provide a fast deterministic approximation to marginal and conditional probabilities. Such approximations can be particularly useful in high dimensional problems where sampling methods are too slow to be effective. A limitation of current methods, however, is that they are restricted to parametric probabilistic models. MCMC does not have such a limitation; indeed, MCMC samplers have been developed for the Dirichlet process (DP), a nonparametric distribution on distributions (Ferguson, 1973) that is the cornerstone of Bayesian nonparametric statistics (Escobar & West, 1995; Neal, 2000). In this paper, we develop a mean-field variational approach to approximate inference for the Dirichlet process, where the approximate posterior is based on the truncated stick-breaking construction (Ishwaran & James, 2001). We compare our approach to DP samplers for Gaussian DP mixture models.

#index 770862
#* P3VI: a partitioned, prioritized, parallel value iterator
#@ David Wingate;Kevin D. Seppi
#t 2004
#c 19
#% 160859
#% 181627
#% 363744
#% 384911
#% 425075
#% 425080
#% 703029
#! We present an examination of the state-of-the-art for using value iteration to solve large-scale discrete Markov Decision Processes. We introduce an architecture which combines three independent performance enhancements (the intelligent prioritization of computation, state partitioning, and massively parallel processing) into a single algorithm. We show that each idea improves performance in a different way, meaning that algorithm designers do not have to trade one improvement for another. We give special attention to parallelization issues, discussing how to efficiently partition states, distribute partitions to processors, minimize message passing and ensure high scalability. We present experimental results which demonstrate that this approach solves large problems in reasonable time.

#index 770863
#* Learning low dimensional predictive representations
#@ Matthew Rosencrantz;Geoff Gordon;Sebastian Thrun
#t 2004
#c 19
#% 179940
#% 702594
#% 857087
#% 1279471
#% 1650702
#! Predictive state representations (PSRs) have recently been proposed as an alternative to partially observable Markov decision processes (POMDPs) for representing the state of a dynamical system (Littman et al., 2001). We present a learning algorithm that learns a PSR from observational data. Our algorithm produces a variant of PSRs called transformed predictive state representations (TPSRs). We provide an efficient principal-components-based algorithm for learning a TPSR, and show that TPSRs can perform well in comparison to Hidden Markov Models learned with Baum-Welch in a real world robot tracking task for low dimensional representations and long prediction horizons.

#index 770864
#* Learning random walk models for inducing word dependency distributions
#@ Kristina Toutanova;Christopher D. Manning;Andrew Y. Ng
#t 2004
#c 19
#% 208934
#% 268079
#% 278099
#% 282905
#% 340899
#% 515356
#% 708948
#% 740902
#% 742218
#% 756253
#% 786511
#% 817554
#% 818061
#% 1289272
#% 1478822
#! Many NLP tasks rely on accurately estimating word dependency probabilities P(ω1|ω2), where the words w1 and w2 have a particular relationship (such as verb-object). Because of the sparseness of counts of such dependencies, smoothing and the ability to use multiple sources of knowledge are important challenges. For example, if the probability P(N|V) of noun N being the subject of verb V is high, and V takes similar objects to V', and V' is synonymous to V", then we want to conclude that P(N|V") should also be reasonably high---even when those words did not cooccur in the training data.To capture these higher order relationships, we propose a Markov chain model, whose stationary distribution is used to give word probability estimates. Unlike the manually defined random walks used in some link analysis algorithms, we show how to automatically learn a rich set of parameters for the Markov chain's transition probabilities. We apply this model to the task of prepositional phrase attachment, obtaining an accuracy of 87.54%.

#index 770865
#* Learning with non-positive kernels
#@ Cheng Soon Ong;Xavier Mary;Stéphane Canu;Alexander J. Smola
#t 2004
#c 19
#% 190581
#% 274636
#% 646000
#! In this paper we show that many kernel methods can be adapted to deal with indefinite kernels, that is, kernels which are not positive semidefinite. They do not satisfy Mercer's condition and they induce associated functional spaces called Reproducing Kernel Kre&icaron;n Spaces (RKKS), a generalization of Reproducing Kernel Hilbert Spaces (RKHS).Machine learning in RKKS shares many "nice" properties of learning in RKHS, such as orthogonality and projection. However, since the kernels are indefinite, we can no longer minimize the loss, instead we stabilize it. We show a general representer theorem for constrained stabilization and prove generalization bounds by computing the Rademacher averages of the kernel class. We list several examples of indefinite kernels and investigate regularization methods to solve spline interpolation. Some preliminary experiments with indefinite kernels for spline smoothing are reported for truncated spectral factorization, Landweber-Fridman iterations, and MR-II.

#index 770866
#* Learning associative Markov networks
#@ Ben Taskar;Vassil Chatalbashev;Daphne Koller
#t 2004
#c 19
#% 248810
#% 266215
#% 464434
#% 592345
#% 593940
#% 732537
#% 1650403
#! Markov networks are extensively used to model complex sequential, spatial, and relational interactions in fields as diverse as image processing, natural language analysis, and bioinformatics. However, inference and learning in general Markov networks is intractable. In this paper, we focus on learning a large subclass of such models (called associative Markov networks) that are tractable or closely approximable. This subclass contains networks of discrete variables with K labels each and clique potentials that favor the same labels for all variables in the clique. Such networks capture the "guilt by association" pattern of reasoning present in many domains, in which connected ("associated") variables tend to have the same label. Our approach exploits a linear programming relaxation for the task of finding the best joint assignment in such networks, which provides an approximate quadratic program (QP) for the problem of learning a margin-maximizing Markov network. We show that for associative Markov network over binary-valued variables, this approximate QP is guaranteed to return an optimal parameterization for Markov networks of arbitrary topology. For the nonbinary case, optimality is not guaranteed, but the relaxation produces good solutions in practice. Experimental results with hypertext and newswire classification show significant advantages over standard approaches.

#index 770867
#* Interpolation-based Q-learning
#@ Csaba Szepesvári;William D. Smart
#t 2004
#c 19
#% 203598
#% 203602
#% 304312
#% 425072
#% 496114
#% 763696
#! We consider a variant of Q-learning in continuous state spaces under the total expected discounted cost criterion combined with local function approximation methods. Provided that the function approximator satisfies certain interpolation properties, the resulting algorithm is shown to converge with probability one. The limit function is shown to satisfy a fixed point equation of the Bellman type, where the fixed point operator depends on the stationary distribution of the exploration policy and the function approximation method. The basic algorithm is extended in several ways. In particular, a variant of the algorithm is obtained that is shown to converge in probability to the optimal Q function. Preliminary computer simulations are presented that confirm the validity of the approach.

#index 770868
#* Extensions of marginalized graph kernels
#@ Pierre Mahé;Nobuhisa Ueda;Tatsuya Akutsu;Jean-Luc Perret;Jean-Philippe Vert
#t 2004
#c 19
#! Positive definite kernels between labeled graphs have recently been proposed. They enable the application of kernel methods, such as support vector machines, to the analysis and classification of graphs, for example, chemical compounds. These graph kernels are obtained by marginalizing a kernel between paths with respect to a random walk model on the graph vertices along the edges. We propose two extensions of these graph kernels, with the double goal to reduce their computation time and increase their relevance as measure of similarity between graphs. First, we propose to modify the label of each vertex by automatically adding information about its environment with the use of the Morgan algorithm. Second, we suggest a modification of the random walk model to prevent the walk from coming back to a vertex that was just visited. These extensions are then tested on benchmark experiments of chemical compounds classification, with promising results.

#index 770869
#* Learning first-order rules from data with multiple parts: applications on mining chemical compound data
#@ Cholwich Nattee;Sukree Sinthupinyo;Masayuki Numao;Takashi Okada
#t 2004
#c 19
#% 217072
#% 224755
#% 260151
#% 272527
#% 449508
#% 464633
#! Inductive learning of first-order theory based on examples has serious bottleneck in the enormous hypothesis search space needed, making existing learning approaches perform poorly when compared to the propositional approach. Moreover, in order to choose the appropiate candidates, all Inductive Logic Programming (ILP) systems only use quantitive information, e.g. number of examples covered and length of rules, which is insufficient for search space having many similar candidates. This paper introduces a novel approach to improve ILP by incorporating the qualitative information into the search heuristics by focusing only on a kind of data where one instance consists of several parts, as well as relations among parts. This approach aims to find the hypothesis describing each class by using both individual and relational characteristics of parts of examples. This kind of data can be found in various domains, especially in representing chemical compound structure. Each compound is composed of atoms as parts, and bonds as relations between two atoms. We apply the proposed approach for discovering rules describing the activity of compounds from their structures from two real-world datasets: mutagenicity in nitroaromatic compounds and dopamine antagonist compounds. The results were compared to the existing method using ten-fold cross validation, and we found that the proposed method significantly produced more accurate results in prediction.

#index 770870
#* Authorship verification as a one-class classification problem
#@ Moshe Koppel;Jonathan Schler
#t 2004
#c 19
#% 219052
#% 318412
#% 344447
#% 458379
#% 578558
#% 722811
#% 735077
#% 855602
#% 1558464
#! In the authorship verification problem, we are given examples of the writing of a single author and are asked to determine if given long texts were or were not written by this author. We present a new learning-based method for adducing the "depth of difference" between two example sets and offer evidence that this method solves the authorship verification problem with very high accuracy. The underlying idea is to test the rate of degradation of the accuracy of learned models as the best features are iteratively dropped from the learning process.

#index 840834
#* Proceedings of the 22nd international conference on Machine learning
#@ Saso Dzeroski;Luc De Raedt;Stefan Wrobel
#t 2005
#c 19
#! This volume, which is also available online fromhttp://www.machinelearning.org, contains the papers accepted forpresentation at ICML-2005, the 22nd lnternational Conference onMachine Learning, which was held at the University of Bonn inGermany from August 7 to August 11, 2005. ICML is the annualconference of the lnternational Machine Learning Society (IMLS),and forms an international forum for the discussion andpresentation of the latest results in the field of machinelearning. This year, ICML was co-located with the 15thlnternational Conference on Inductive Logic Programming (ILP-2005),the proceedings of which are published by Springer Verlag in aseparate volume.The papers in this volume were selected on the basis of athorough review process. In the first round of reviewing, threeprogram committee members produced individual reviews for a paper.Authors then had the opportunity to view those reviews and submitan author's reply to the reviewers. Led by the responsible areachair, the reviewers then engaged in a discussion about the paper,ultimately leading to the decision by the program chairs. In sum,of the 491 papers that were initially submitted, 62 were acceptedimmediately, and a further 81 were conditionally accepted andreconsidered after resubmission in a second round of reviewing. Ofthose 81 conditionally accepted papers, 72 were finally accepted,leading to a total of 134 accepted papers, which translates into anacceptance rate of 27.3 %. The author reply was a new feature ofICML this year, while the option of working with conditionalaccepts has already become a tradition.In addition to the presentations of the accepted papers, theICML program included several other features. On the first and lastday of the conference, 11 workshops and 6 tutorials on currenttopics of machine learning were held. For many of these,proceedings and/or presentation materials are available online fromthe ICML website. The other days of the conference each featured aninvited talk by a prominent researcher as a program highlight. Wewere delighted that Johannes Gehrke of Cornell University, MichaelJordan of the University of California at Berkeley, and GerhardWidmer of the University of Linz in Austria, agreed to deliver aninvited talk. The abstracts of their talks are also published aspart of these proceedings.Continuing a long standing tradition at ICML, all paperspresented in a talk at the conference were also exhibited atevening poster sessions, giving everyone ample time to discuss theresults in depth. In order to emphasize the co-location withILP-2005, the program contained joint elements in both invitedspeakers, paper sessions, poster sessions, and tutorials. As usual,the scientific program was complemented by a social program, thistime featuring an excursion to the scenic surroundings of the cityof Bonn.During the conference best paper and best student paper awardswere presented, the former being sponsored by NICTA, the later bythe Machine Learning Journal.

#index 840835
#* Exploration and apprenticeship learning in reinforcement learning
#@ Pieter Abbeel;Andrew Y. Ng
#t 2005
#c 19
#% 69418
#% 78916
#% 126926
#% 366058
#% 393786
#% 425075
#% 466575
#% 495933
#% 661886
#% 722895
#% 770852
#! We consider reinforcement learning in systems with unknown dynamics. Algorithms such as E3 (Kearns and Singh, 2002) learn near-optimal policies by using "exploration policies" to drive the system towards poorly modeled states, so as to encourage exploration. But this makes these algorithms impractical for many systems; for example, on an autonomous helicopter, overly aggressive exploration may well result in a crash. In this paper, we consider the apprenticeship learning setting in which a teacher demonstration of the task is available. We show that, given the initial demonstration, no explicit exploration is necessary, and we can attain near-optimal performance (compared to the teacher) simply by repeatedly executing "exploitation policies" that try to maximize rewards. In finite-state MDPs, our algorithm scales polynomially in the number of states; in continuous-state linear dynamical systems, it scales polynomially in the dimension of the state. These results are proved using a martingale construction over relative losses.

#index 840836
#* Active learning for Hidden Markov Models: objective functions and algorithms
#@ Brigham Anderson;Andrew Moore
#t 2005
#c 19
#% 95730
#% 116165
#% 132697
#% 170649
#% 236729
#% 464268
#% 549447
#% 715096
#% 1289266
#% 1650401
#% 1650568
#% 1762173
#! Hidden Markov Models (HMMs) model sequential data in many fields such as text/speech processing and biosignal analysis. Active learning algorithms learn faster and/or better by closing the data-gathering loop, i.e., they choose the examples most informative with respect to their learning objectives. We introduce a framework and objective functions for active learning in three fundamental HMM problems: model learning, state estimation, and path estimation. In addition, we describe a new set of algorithms for efficiently finding optimal greedy queries using these objective functions. The algorithms are fast, i.e., linear in the number of time steps to select the optimal query and we present empirical results showing that these algorithms can significantly reduce the need for labelled training data.

#index 840837
#* Tempering for Bayesian C&RT
#@ Nicos Angelopoulos;James Cussens
#t 2005
#c 19
#% 830659
#% 1289453
#! This paper concerns the experimental assessment of tempering as a technique for improving Bayesian inference for C&RT models. Full Bayesian inference requires the computation of a posterior over all possible trees. Since exact computation is not possible Markov chain Monte Carlo (MCMC) methods are used to produce an approximation. C&RT posteriors have many local modes: tempering aims to prevent the Markov chain getting stuck in these modes. Our results show that a clear improvement is achieved using tempering.

#index 840838
#* Fast condensed nearest neighbor rule
#@ Fabrizio Angiulli
#t 2005
#c 19
#% 307100
#% 420138
#% 1861143
#! We present a novel algorithm for computing a training set consistent subset for the nearest neighbor decision rule. The algorithm, called FCNN rule, has some desirable properties. Indeed, it is order independent, and has subquadratic worst case time complexity, while it requires few iterations to converge, and it is likely to select points very close to the decision boundary. We compare the FCNN rule with state of the art competence preservation algorithms on large multidimensional training sets, showing that it outperforms existing methods in terms of learning speed and learning scaling behavior, and in terms of size of the model, while it guarantees a comparable prediction accuracy.

#index 840839
#* Predictive low-rank decomposition for kernel methods
#@ Francis R. Bach;Michael I. Jordan
#t 2005
#c 19
#% 292664
#% 466597
#% 564285
#% 722815
#% 722887
#% 743284
#% 763697
#! Low-rank matrix decompositions are essential tools in the application of kernel methods to large-scale learning problems. These decompositions have generally been treated as black boxes---the decomposition of the kernel matrix that they deliver is independent of the specific learning task at hand---and this is a potentially significant source of inefficiency. In this paper, we present an algorithm that can exploit side information (e.g., classification labels, regression responses) in the computation of low-rank decompositions for kernel matrices. Our algorithm has the same favorable scaling as state-of-the-art methods such as incomplete Cholesky decomposition---it is linear in the number of data points and quadratic in the rank of the approximation. We present simulation results that show that our algorithm yields decompositions of significantly smaller rank than those found by incomplete Cholesky decomposition.

#index 840840
#* Multi-way distributional clustering via pairwise interactions
#@ Ron Bekkerman;Ran El-Yaniv;Andrew McCallum
#t 2005
#c 19
#% 262059
#% 309128
#% 397139
#% 469422
#% 528174
#% 722930
#% 722934
#% 729918
#% 748465
#% 769928
#% 770797
#% 770799
#% 778215
#% 788043
#! We present a novel unsupervised learning scheme that simultaneously clusters variables of several types (e.g., documents, words and authors) based on pairwise interactions between the types, as observed in co-occurrence data. In this scheme, multiple clustering systems are generated aiming at maximizing an objective function that measures multiple pairwise mutual information between cluster variables. To implement this idea, we propose an algorithm that interleaves top-down clustering of some variables and bottom-up clustering of the other variables, with a local optimization correction routine. Focusing on document clustering we present an extensive empirical study of two-way, three-way and four-way applications of our scheme using six real-world datasets including the 20 News-groups (20NG) and the Enron email collection. Our multi-way distributional clustering (MDC) algorithms consistently and significantly outperform previous state-of-the-art information theoretic clustering algorithms.

#index 840841
#* Error limiting reductions between classification tasks
#@ Alina Beygelzimer;Varsha Dani;Tom Hayes;John Langford;Bianca Zadrozny
#t 2005
#c 19
#% 697
#% 84306
#% 136350
#% 209021
#% 235377
#% 272518
#% 276516
#% 280437
#% 302391
#% 465751
#% 577298
#% 580687
#% 722756
#% 727925
#% 1272365
#! We introduce a reduction-based model for analyzing supervised learning tasks. We use this model to devise a new reduction from multi-class cost-sensitive classification to binary classification with the following guarantee: If the learned binary classifier has error rate at most ε then the cost-sensitive classifier has cost at most 2ε times the expected sum of costs of all possible lables. Since cost-sensitive classification can embed any bounded loss finite choice supervised learning task, this result shows that any such task can be solved using a binary classification oracle. Finally, we present experimental results showing that our new reduction outperforms existing algorithms for multi-class cost-sensitive learning.

#index 840842
#* Multi-instance tree learning
#@ Hendrik Blockeel;David Page;Ashwin Srinivasan
#t 2005
#c 19
#% 217072
#% 224755
#% 252221
#% 449588
#! We introduce a novel algorithm for decision tree learning in the multi-instance setting as originally defined by Dietterich et al. It differs from existing multi-instance tree learners in a few crucial, well-motivated details. Experiments on synthetic and real-life datasets confirm the beneficial effect of these differences and show that the resulting system outperforms the existing multi-instance decision tree learners.

#index 840843
#* Action respecting embedding
#@ Michael Bowling;Ali Ghodsi;Dana Wilkinson
#t 2005
#c 19
#% 266426
#% 272544
#% 304931
#% 723241
#% 770767
#% 1289493
#% 1502529
#! Dimensionality reduction is the problem of finding a low-dimensional representation of high-dimensional input data. This paper examines the case where additional information is known about the data. In particular, we assume the data are given in a sequence with action labels associated with adjacent data points, such as might come from a mobile robot. The goal is a variation on dimensionality reduction, where the output should be a representation of the input data that is both low-dimensional and respects the actions (i.e., actions correspond to simple transformations in the output representation). We show how this variation can be solved with a semidefinite program. We evaluate the technique in a synthetic, robot-inspired domain, demonstrating qualitatively superior representations and quantitative improvements on a data prediction task.

#index 840844
#* Clustering through ranking on manifolds
#@ Markus Breitenbach;Gregory Z. Grudic
#t 2005
#c 19
#% 316709
#% 336073
#% 373996
#! Clustering aims to find useful hidden structures in data. In this paper we present a new clustering algorithm that builds upon the consistency method (Zhou, et.al., 2003), a semi-supervised learning technique with the property of learning very smooth functions with respect to the intrinsic structure revealed by the data. Other methods, e.g. Spectral Clustering, obtain good results on data that reveals such a structure. However, unlike Spectral Clustering, our algorithm effectively detects both global and within-class outliers, and the most representative examples in each class. Furthermore, we specify an optimization framework that estimates all learning parameters, including the number of clusters, directly from data. Finally, we show that the learned cluster-models can be used to add previously unseen points to clusters without re-learning the original cluster model. Encouraging experimental results are obtained on a number of real world problems.

#index 840845
#* Reducing overfitting in process model induction
#@ Will Bridewell;Narges Bani Asadi;Pat Langley;Ljupčo Todorovski
#t 2005
#c 19
#% 1116
#% 109848
#% 135552
#% 136350
#% 197064
#% 209021
#% 348816
#% 464625
#% 466754
#% 1269500
#! In this paper, we review the paradigm of inductive process modeling, which uses background knowledge about possible component processes to construct quantitative models of dynamical systems. We note that previous methods for this task tend to overfit the training data, which suggests ensemble learning as a likely response. However, such techniques combine models in ways that reduce comprehensibility, making their output much less accessible to domain scientists. As an alternative, we introduce a new approach that induces a set of process models from different samples of the training data and uses them to guide a final search through the space of model structures. Experiments with synthetic and natural data suggest this method reduces error and decreases the chance of including unnecessary processes in the model. We conclude by discussing related work and suggesting directions for additional research.

#index 840846
#* Learning to rank using gradient descent
#@ Chris Burges;Tal Shaked;Erin Renshaw;Ari Lazier;Matt Deeds;Nicole Hamilton;Greg Hullender
#t 2005
#c 19
#% 309095
#% 376266
#% 476873
#% 734915
#! We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.

#index 840847
#* Learning class-discriminative dynamic Bayesian networks
#@ John Burge;Terran Lane
#t 2005
#c 19
#% 6199
#% 101213
#% 107414
#% 109042
#% 129987
#% 197387
#% 205380
#% 351595
#% 528154
#% 578681
#% 716892
#% 770761
#% 892526
#! In many domains, a Bayesian network's topological structure is not known a priori and must be inferred from data. This requires a scoring function to measure how well a proposed network topology describes a set of data. Many commonly used scores such as BD, BDE, BDEU, etc., are not well suited for class discrimination. Instead, scores such as the class-conditional likelihood (CCL) should be employed. Unfortunately, CCL does not decompose and its application to large domains is not feasible. We introduce a decomposable score, approximate conditional likelihood (ACL) that is capable of identifying class discriminative structures. We show that dynamic Bayesian networks (DBNs) trained with ACL have classification efficacies competitive to those trained with CCL on a set of simulated data experiments. We also show that ACL-trained DBNs outperform BDE-trained DBNs, Gaussian naïve Bayes networks and support vector machines within a neuroscience domain too large for CCL.

#index 840848
#* Recognition and reproduction of gestures using a probabilistic framework combining PCA, ICA and HMM
#@ Sylvain Calinon;Aude Billard
#t 2005
#c 19
#% 298346
#% 729917
#% 770852
#% 1860500
#! This paper explores the issue of recognizing, generalizing and reproducing arbitrary gestures. We aim at extracting a representation that encapsulates only the key aspects of the gesture and discards the variability intrinsic to each person's motion. We compare a decomposition into principal components (PCA) and independent components (ICA) as a first step of preprocessing in order to decorrelate and denoise the data, as well as to reduce the dimensionality of the dataset to make this one tractable. In a second stage of processing, we explore the use of a probabilistic encoding through continuous Hidden Markov Models (HMMs), as a way to encapsulate the sequential nature and intrinsic variability of the motions in stochastic finite state automata. Finally, the method is validated in a humanoid robot to reproduce a variety of gestures performed by a human demonstrator.

#index 840849
#* Predicting probability distributions for surf height using an ensemble of mixture density networks
#@ Michael Carney;Pádraig Cunningham;Jim Dowling;Ciaran Lee
#t 2005
#c 19
#% 138308
#% 209021
#% 361100
#% 387299
#% 465749
#! There is a range of potential applications of Machine Learning where it would be more useful to predict the probability distribution for a variable rather than simply the most likely value for that variable. In meteorology and in finance it is often important to know the probability of a variable falling within (or outside) different ranges. In this paper we consider the prediction of surf height with the objective of predicting if it will fall within a given 'surfable' range. Prediction problems such as this are considerably more difficult if the distribution of the phenomenon is significantly different from a normal distribution. This is the case with the surf data we have studied. To address this we use an ensemble of mixture density networks to predict the probability density function. Our evaluation shows that this is an effective solution. We also describe a web-based application that presents these predictions in a usable manner.

#index 840850
#* Hedged learning: regret-minimization with learning experts
#@ Yu-Han Chang;Leslie Pack Kaelbling
#t 2005
#c 19
#% 465913
#% 466075
#% 593734
#! In non-cooperative multi-agent situations, there cannot exist a globally optimal, yet opponent-independent learning algorithm. Regret-minimization over a set of strategies optimized for potential opponent models is proposed as a good framework for deciding how to behave in such situations. Using longer playing horizons and experts that learn as they play, the regret-minimization framework can be extended to overcome several shortcomings of earlier approaches to the problem of multi-agent learning.

#index 840851
#* Variational Bayesian image modelling
#@ Li Cheng;Feng Jiao;Dale Schuurmans;Shaojun Wang
#t 2005
#c 19
#% 128024
#% 277483
#% 303620
#% 349211
#% 443734
#% 444028
#% 580307
#% 721176
#! We present a variational Bayesian framework for performing inference, density estimation and model selection in a special class of graphical models---Hidden Markov Random Fields (HMRFs). HMRFs are particularly well suited to image modelling and in this paper, we apply them to the problem of image segmentation. Unfortunately, HMRFs are notoriously hard to train and use because the exact inference problems they create are intractable. Our main contribution is to introduce an efficient variational approach for performing approximate inference of the Bayesian formulation of HMRFs, which we can then apply to the density estimation and model selection problems that arise when learning image models from data. With this variational approach, we can conveniently tackle the problem of image segmentation. We present experimental results which show that our technique outperforms recent HMRF-based segmentation methods on real world images.

#index 840852
#* Preference learning with Gaussian processes
#@ Wei Chu;Zoubin Ghahramani
#t 2005
#c 19
#% 169777
#% 268069
#% 360691
#% 450245
#% 466430
#% 770753
#% 770800
#% 771846
#! In this paper, we propose a probabilistic kernel approach to preference learning based on Gaussian processes. A new likelihood function is proposed to capture the preference relations in the Bayesian framework. The generalized formulation is also applicable to tackle many multiclass problems. The overall approach has the advantages of Bayesian methods for model selection and probabilistic prediction. Experimental results compared against the constraint classification approach on several benchmark datasets verify the usefulness of this algorithm.

#index 840853
#* New approaches to support vector ordinal regression
#@ Wei Chu;S. Sathiya Keerthi
#t 2005
#c 19
#% 190581
#% 269218
#% 856251
#% 1898704
#! In this paper, we propose two new support vector approaches for ordinal regression, which optimize multiple thresholds to define parallel discriminant hyperplanes for the ordinal scales. Both approaches guarantee that the thresholds are properly ordered at the optimal solution. The size of these optimization problems is linear in the number of training samples. The SMO algorithm is adapted for the resulting optimization problems; it is extremely easy to implement and scales efficiently as a quadratic function of the number of examples. The results of numerical experiments on benchmark datasets verify the usefulness of these approaches.

#index 840854
#* A general regression technique for learning transductions
#@ Corinna Cortes;Mehryar Mohri;Jason Weston
#t 2005
#c 19
#% 3134
#% 18262
#% 190581
#% 408470
#% 466081
#% 722887
#% 770763
#% 771848
#% 809163
#! The problem of learning a transduction, that is a string-to-string mapping, is a common problem arising in natural language processing and computational biology. Previous methods proposed for learning such mappings are based on classification techniques. This paper presents a new and general regression technique for learning transductions and reports the results of experiments showing its effectiveness. Our transduction learning consists of two phases: the estimation of a set of regression coefficients and the computation of the pre-image corresponding to this set of coefficients. A novel and conceptually cleaner formulation of kernel dependency estimation provides a simple framework for estimating the regression coefficients, and an efficient algorithm for computing the pre-image from the regression coefficients extends the applicability of kernel dependency estimation to output sequences. We report the results of a series of experiments illustrating the application of our regression technique for learning transductions.

#index 840855
#* Learning to compete, compromise, and cooperate in repeated general-sum games
#@ Jacob W. Crandall;Michael A. Goodrich
#t 2005
#c 19
#% 348821
#% 366058
#% 464283
#% 465913
#% 580519
#% 1289221
#! Learning algorithms often obtain relatively low average payoffs in repeated general-sum games between other learning agents due to a focus on myopic best-response and one-shot Nash equilibrium (NE) strategies. A less myopic approach places focus on NEs of the repeated game, which suggests that (at the least) a learning agent should possess two properties. First, an agent should never learn to play a strategy that produces average payoffs less than the minimax value of the game. Second, an agent should learn to cooperate/compromise when beneficial. No learning algorithm from the literature is known to possess both of these properties. We present a reinforcement learning algorithm (M-Qubed) that provably satisfies the first property and empirically displays (in self play) the second property in a wide range of games.

#index 840856
#* Learning as search optimization: approximate large margin methods for structured prediction
#@ Hal Daumé, III;Daniel Marcu
#t 2005
#c 19
#% 174161
#% 302390
#% 464434
#% 466892
#% 722814
#% 722822
#% 722924
#% 770763
#% 770844
#% 770855
#% 788082
#% 816181
#% 854636
#% 938667
#! Mappings to structured output spaces (strings, trees, partitions, etc.) are typically learned using extensions of classification algorithms to simple graphical structures (eg., linear chains) in which search and parameter estimation can be performed exactly. Unfortunately, in many complex problems, it is rare that exact search or parameter estimation is tractable. Instead of learning exact models and searching via heuristic means, we embrace this difficulty and treat the structured output problem in terms of approximate search. We present a framework for learning as search optimization, and two parameter updates with convergence the-orems and bounds. Empirical evidence shows that our integrated approach to learning and decoding can outperform exact models at smaller computational cost.

#index 840857
#* Multimodal oriented discriminant analysis
#@ Fernando De la Torre;Takeo Kanade
#t 2005
#c 19
#% 58584
#% 80995
#% 97411
#% 235340
#% 275107
#% 277483
#% 324288
#% 724227
#% 729437
#% 970381
#! Linear discriminant analysis (LDA) has been an active topic of research during the last century. However, the existing algorithms have several limitations when applied to visual data. LDA is only optimal for Gaussian distributed classes with equal covariance matrices, and only classes-1 features can be extracted. On the other hand, LDA does not scale well to high dimensional data (overfitting), and it cannot handle optimally multimodal distributions. In this paper, we introduce Multimodal Oriented Discriminant Analysis (MODA), a LDA extension which can overcome these drawbacks. A new formulation and several novelties are proposed:• An optimal dimensionality reduction for multimodal Gaussian classes with different covariances is derived. The new criteria allows for extracting more than classes-1 features.• A covariance approximation is introduced to improve generalization and avoid over-fitting when dealing with high dimensional data.• A linear time iterative majorization method is suggested in order to find a local optimum.Several synthetic and real experiments on face recognition show that MODA outperform existing linear techniques.

#index 840858
#* A practical generalization of Fourier-based learning
#@ Adam Drake;Dan Ventura
#t 2005
#c 19
#% 148216
#% 156699
#% 243163
#% 312083
#! This paper presents a search algorithm for finding functions that are highly correlated with an arbitrary set of data. The functions found by the search can be used to approximate the unknown function that generated the data. A special case of this approach is a method for learning Fourier representations. Empirical results demonstrate that on typical real-world problems the most highly correlated functions can be found very quickly, while combinations of these functions provide good approximations of the unknown function.

#index 840859
#* Combining model-based and instance-based learning for first order regression
#@ Kurt Driessens;Sašo Džeroski
#t 2005
#c 19
#% 92533
#% 92555
#% 209023
#% 229931
#% 252221
#% 288885
#% 317313
#% 333786
#% 347711
#% 466077
#% 771946
#! The introduction of relational reinforcement learning and the RRL algorithm gave rise to the development of several first order regression algorithms. So far, these algorithms have employed either a model-based approach or an instance-based approach. As a consequence, they suffer from the typical drawbacks of model-based learning such as coarse function approximation or those of lazy learning such as high computational intensity.In this paper we develop a new regression algorithm that combines the strong points of both approaches and tries to avoid the normally inherent draw-backs. By combining model-based and instance-based learning, we produce an incremental first order regression algorithm that is both computationally efficient and produces better predictions earlier in the learning experiment.

#index 840860
#* Reinforcement learning with Gaussian processes
#@ Yaakov Engel;Shie Mannor;Ron Meir
#t 2005
#c 19
#% 135414
#% 266287
#% 366058
#% 393786
#% 770824
#! Gaussian Process Temporal Difference (GPTD) learning offers a Bayesian solution to the policy evaluation problem of reinforcement learning. In this paper we extend the GPTD framework by addressing two pressing issues, which were not adequately treated in the original GPTD paper (Engel et al., 2003). The first is the issue of stochasticity in the state transitions, and the second is concerned with action selection and policy improvement. We present a new generative model for the value function, deduced from its relation with the discounted return. We derive a corresponding on-line algorithm for learning the posterior moments of the value Gaussian process. We also present a SARSA based extension of GPTD, termed GPSARSA, that allows the selection of actions and the gradual improvement of policies without requiring a world-model.

#index 840861
#* Experimental comparison between bagging and Monte Carlo ensemble classification
#@ Roberto Esposito;Lorenza Saitta
#t 2005
#c 19
#% 36358
#% 209021
#% 770808
#% 1279285
#! Properties of ensemble classification can be studied using the framework of Monte Carlo stochastic algorithms. Within this framework it is also possible to define a new ensemble classifier, whose accuracy probability distribution can be computed exactly. This paper has two goals: first, an experimental comparison between the theoretical predictions and experimental results; second, a systematic comparison between bagging and Monte Carlo ensemble classification.

#index 840862
#* Supervised clustering with support vector machines
#@ Thomas Finley;Thorsten Joachims
#t 2005
#c 19
#% 280492
#% 310516
#% 464291
#% 725438
#% 740995
#% 749492
#% 763697
#% 765548
#% 769881
#% 770763
#% 770782
#% 770866
#% 815329
#% 815876
#! Supervised clustering is the problem of training a clustering algorithm to produce desirable clusterings: given sets of items and complete clusterings over these sets, we learn how to cluster future sets of items. Example applications include noun-phrase coreference clustering, and clustering news articles by whether they refer to the same topic. In this paper we present an SVM algorithm that trains a clustering algorithm by adapting the item-pair similarity measure. The algorithm may optimize a variety of different clustering functions to a variety of clustering performance measures. We empirically evaluate the algorithm for noun-phrase and news article clustering.

#index 840863
#* Optimal assignment kernels for attributed molecular graphs
#@ Holger Fröhlich;Jörg K. Wegner;Florian Sieker;Andreas Zell
#t 2005
#c 19
#% 296571
#% 464289
#% 722887
#% 731608
#% 763697
#! We propose a new kernel function for attributed molecular graphs, which is based on the idea of computing an optimal assignment from the atoms of one molecule to those of another one, including information on neighborhood, membership to a certain structural element and other characteristics for each atom. As a byproduct this leads to a new class of kernel functions. We demonstrate how the necessary computations can be carried out efficiently. Compared to marginalized graph kernels our method in some cases leads to a significant reduction of the prediction error. Further improvement can be gained, if expert knowledge is combined with our method. We also investigate a reduced graph representation of molecules by collapsing certain structural elements, like e.g. rings, into a single node of the molecular graph.

#index 840864
#* Closed-form dual perturb and combine for tree-based models
#@ Pierre Geurts;Louis Wehenkel
#t 2005
#c 19
#% 136350
#% 157825
#% 209021
#% 256615
#% 312727
#% 314785
#% 357521
#% 400847
#% 420054
#% 580511
#% 722357
#! This paper studies the aggregation of predictions made by tree-based models for several perturbed versions of the attribute vector of a test case. A closed-form approximation of this scheme combined with cross-validation to tune the level of perturbation is proposed. This yields soft-tree models in a parameter free way. and preserves their interpretability. Empirical evaluations, on classification and regression problems, show that accuracy and bias/variance tradeoff are improved significantly at the price of an acceptable computational overhead. The method is further compared and combined with tree bagging.

#index 840865
#* Hierarchic Bayesian models for kernel learning
#@ Mark Girolami;Simon Rogers
#t 2005
#c 19
#% 303620
#% 425061
#% 528020
#% 705252
#% 715096
#% 722760
#% 739899
#% 743284
#% 763697
#% 770790
#% 770831
#% 770846
#% 830749
#! The integration of diverse forms of informative data by learning an optimal combination of base kernels in classification or regression problems can provide enhanced performance when compared to that obtained from any single data source. We present a Bayesian hierarchical model which enables kernel learning and present effective variational Bayes estimators for regression and classification. Illustrative experiments demonstrate the utility of the proposed method. Matlab code replicating results reported is available at http://www.dcs.gla.ac.uk/~srogers/kernel_comb.html.

#index 840866
#* Online feature selection for pixel classification
#@ Karen Glocer;Damian Eads;James Theiler
#t 2005
#c 19
#% 124073
#% 243728
#% 397688
#% 444051
#% 722929
#% 722937
#% 722943
#% 744799
#! Online feature selection (OFS) provides an efficient way to sort through a large space of features, particularly in a scenario where the feature space is large and features take a significant amount of memory to store. Image processing operators, and especially combinations of image processing operators, provide a rich space of potential features for use in machine learning for image processing tasks but they are expensive to generate and store. In this paper we apply OFS to the problem of edge detection in grayscale imagery. We use a standard data set and compare our results to those obtained with traditional edge detectors, as well as with results obtained more recently using "statistical edge detection." We compare several different OFS approaches, including hill climbing, best first search, and grafting.

#index 840867
#* Learning strategies for story comprehension: a reinforcement learning approach
#@ Eugene Grois;David C. Wilkins
#t 2005
#c 19
#% 126926
#% 172505
#% 196896
#% 280088
#% 309126
#% 786549
#% 853665
#% 853667
#% 1272286
#% 1272292
#% 1700507
#! This paper describes the use of machine learning to improve the performance of natural language question answering systems. We present a model for improving story comprehension through inductive generalization and reinforcement learning, based on classified examples. In the process, the model selects the most relevant and useful pieces of lexical information to be used by the inference procedure. We compare our approach to three prior non-learning systems, and evaluate the conditions under which learning is effective. We demonstrate that a learning-based approach can improve upon "matching and extraction"-only techniques.

#index 840868
#* Near-optimal sensor placements in Gaussian processes
#@ Carlos Guestrin;Andreas Krause;Ajit Paul Singh
#t 2005
#c 19
#% 115608
#% 138548
#% 193136
#% 336597
#% 739899
#% 1016178
#! When monitoring spatial phenomena, which are often modeled as Gaussian Processes (GPs), choosing sensor locations is a fundamental task. A common strategy is to place sensors at the points of highest entropy (variance) in the GP model. We propose a mutual information criteria, and show that it produces better placements. Furthermore, we prove that finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1 -- 1/e) of the optimum by exploiting the submodularity of our criterion. This algorithm is extended to handle local structure in the GP, yielding significant speedups. We demonstrate the advantages of our approach on two real-world data sets.

#index 840869
#* Robust one-class clustering using hybrid global and local search
#@ Gunjan Gupta;Joydeep Ghosh
#t 2005
#c 19
#% 329562
#% 464195
#% 469425
#% 770821
#% 855602
#! Unsupervised learning methods often involve summarizing the data using a small number of parameters. In certain domains, only a small subset of the available data is relevant for the problem. One-Class Classification or One-Class Clustering attempts to find a useful subset by locating a dense region in the data. In particular, a recently proposed algorithm called One-Class Information Ball (OC-IB) shows the advantage of modeling a small set of highly coherent points as opposed to pruning outliers. We present several modifications to OC-IB and integrate it with a global search that results in several improvements such as deterministic results, optimality guarantees, control over cluster size and extension to other cost functions. Empirical studies yield significantly better results on various real and artificial data.

#index 840870
#* Statistical and computational analysis of locality preserving projection
#@ Xiaofei He;Deng Cai;Wanli Min
#t 2005
#c 19
#% 313959
#% 643008
#% 766418
#% 791402
#% 835741
#! Recently, several manifold learning algorithms have been proposed, such as ISOMAP (Tenenbaum et al., 2000), Locally Linear Embedding (Roweis & Saul, 2000), Laplacian Eigenmap (Belkin & Niyogi, 2001), Locality Preserving Projection (LPP) (He & Niyogi, 2003), etc. All of them aim at discovering the meaningful low dimensional structure of the data space. In this paper, we present a statistical analysis of the LPP algorithm. Different from Principal Component Analysis (PCA) which obtains a subspace spanned by the largest eigenvectors of the global covariance matrix, we show that LPP obtains a subspace spanned by the smallest eigenvectors of the local covariance matrix. We applied PCA and LPP to real world document clustering task. Experimental results show that the performance can be significantly improved in the subspace, and especially LPP works much better than PCA.

#index 840871
#* Intrinsic dimensionality estimation of submanifolds in Rd
#@ Matthias Hein;Jean-Yves Audibert
#t 2005
#c 19
#% 1705532
#! We present a new method to estimate the intrinsic dimensionality of a submanifold M in Rd from random samples. The method is based on the convergence rates of a certain U-statistic on the manifold. We solve at least partially the question of the choice of the scale of the data. Moreover the proposed method is easy to implement, can handle large data sets and performs very well even for small sample sizes. We compare the proposed method to two standard estimators on several artificial as well as real data sets.

#index 840872
#* Bayesian hierarchical clustering
#@ Katherine A. Heller;Zoubin Ghahramani
#t 2005
#c 19
#% 476708
#% 528008
#! We present a novel algorithm for agglomerative hierarchical clustering based on evaluating marginal likelihoods of a probabilistic model. This algorithm has several advantages over traditional distance-based agglomerative clustering algorithms. (1) It defines a probabilistic model of the data which can be used to compute the predictive distribution of a test point and the probability of it belonging to any of the existing clusters in the tree. (2) It uses a model-based criterion to decide on merging clusters rather than an ad-hoc distance metric. (3) Bayesian hypothesis testing is used to decide which merges are advantageous and to output the recommended depth of the tree. (4) The algorithm can be interpreted as a novel fast bottom-up approximate inference method for a Dirichlet process (i.e. countably infinite) mixture model (DPM). It provides a new lower bound on the marginal likelihood of a DPM by summing over exponentially many clusterings of the data in polynomial time. We describe procedures for learning the model hyperpa-rameters, computing the predictive distribution, and extensions to the algorithm. Experimental results on synthetic and real-world data sets demonstrate useful properties of the algorithm.

#index 840873
#* Online learning over graphs
#@ Mark Herbster;Massimiliano Pontil;Lisa Wainer
#t 2005
#c 19
#% 216480
#% 302390
#% 304824
#% 464615
#% 466576
#% 466887
#% 765552
#! We apply classic online learning techniques similar to the perceptron algorithm to the problem of learning a function defined on a graph. The benefit of our approach includes simple algorithms and performance guarantees that we naturally interpret in terms of structural properties of the graph, such as the algebraic connectivity or the diameter of the graph. We also discuss how these methods can be modified to allow active learning on a graph. We present preliminary experiments with encouraging results.

#index 840874
#* Adapting two-class support vector classification methods to many class problems
#@ Simon I. Hill;Arnaud Doucet
#t 2005
#c 19
#% 269221
#% 342598
#% 425043
#% 429569
#% 722756
#% 722758
#% 722761
#% 722807
#% 722816
#% 732385
#% 763699
#% 856251
#% 1272365
#% 1860941
#! A geometric construction is presented which is shown to be an effective tool for understanding and implementing multi-category support vector classification. It is demonstrated how this construction can be used to extend many other existing two-class kernel-based classification methodologies in a straightforward way while still preserving attractive properties of individual algorithms. Reducing training times through incorporating the results of pairwise classification is also discussed and experimental results presented.

#index 840875
#* A martingale framework for concept change detection in time-varying data streams
#@ Shen-Shyang Ho
#t 2005
#c 19
#% 654489
#% 785380
#! In a data streaming setting, data points are observed one by one. The concepts to be learned from the data points may change infinitely often as the data is streaming. In this paper, we extend the idea of testing exchangeability online (Vovk et al., 2003) to a martingale framework to detect concept changes in time-varying data streams. Two martingale tests are developed to detect concept changes using: (i) martingale values, a direct consequence of the Doob's Maximal Inequality, and (ii) the martingale difference, justified using the Hoeffding-Azuma Inequality. Under some assumptions, the second test theoretically has a lower probability than the first test of rejecting the null hypothesis, "no concept change in the data stream", when it is in fact correct. Experiments show that both martingale tests are effective in detecting concept changes in time-varying data streams simulated using two synthetic data sets and three benchmark data sets.

#index 840876
#* Multi-class protein fold recognition using adaptive codes
#@ Eugene Ie;Jason Weston;William Stafford Noble;Christina Leslie
#t 2005
#c 19
#% 397654
#% 466737
#% 562950
#% 763699
#% 770763
#% 815896
#% 830744
#% 1272365
#! We develop a novel multi-class classification method based on output codes for the problem of classifying a sequence of amino acids into one of many known protein structural classes, called folds. Our method learns relative weights between one-vs-all classifiers and encodes information about the protein structural hierarchy for multi-class prediction. Our code weighting approach significantly improves on the standard one-vs-all method for the fold recognition problem. In order to compare against widely used methods in protein sequence analysis, we also test nearest neighbor approaches based on the PSI-BLAST algorithm. Our code weight learning algorithm strongly outperforms these PSI-BLAST methods on every structure recognition problem we consider.

#index 840877
#* Learning approximate preconditions for methods in hierarchical plans
#@ Okhtay Ilghami;Héctor Muñoz-Avila;Dana S. Nau;David W. Aha
#t 2005
#c 19
#% 289949
#% 341644
#% 465761
#% 466248
#% 466729
#% 495942
#% 771604
#% 1478805
#! A significant challenge in developing planning systems for practical applications is the difficulty of acquiring the domain knowledge needed by such systems. One method for acquiring this knowledge is to learn it from plan traces, but this method typically requires a huge number of plan traces to converge. In this paper, we show that the problem with slow convergence can be circumvented by having the learner generate solution plans even before the planning domain is completely learned. Our empirical results show that these improvements reduce the size of the training set that is needed to find correct answers to a large percentage of planning problems in the test set.

#index 840878
#* Evaluating machine learning for information extraction
#@ Neil Ireson;Fabio Ciravegna;Mary Elaine Califf;Dayne Freitag;Nicholas Kushmerick;Alberto Lavelli
#t 2005
#c 19
#% 707780
#% 709066
#! Comparative evaluation of Machine Learning (ML) systems used for Information Extraction (IE) has suffered from various inconsistencies in experimental procedures. This paper reports on the results of the Pascal Challenge on Evaluating Machine Learning for Information Extraction, which provides a standardised corpus, set of tasks, and evaluation methodology. The challenge is described and the systems submitted by the ten participants are briefly introduced and their performance is analysed.

#index 840879
#* Learn to weight terms in information retrieval using category information
#@ Rong Jin;Joyce Y. Chai;Luo Si
#t 2005
#c 19
#% 46803
#% 120104
#% 218982
#% 262096
#% 280850
#% 300542
#% 306504
#% 340899
#% 340948
#% 397128
#% 397129
#% 420507
#% 448725
#! How to assign appropriate weights to terms is one of the critical issues in information retrieval. Many term weighting schemes are unsupervised. They are either based on the empirical observation in information retrieval, or based on generative approaches for language modeling. As a result, the existing term weighting schemes are usually insufficient in distinguishing informative words from the uninformative ones, which is crucial to the performance of information retrieval. In this paper, we present supervised term weighting schemes that automatically learn term weights based on the correlation between word frequency and category information of documents. Empirical studies with the ImageCLEF dataset have indicated that the proposed methods perform substantially better than the state-of-the-art approaches for term weighting and other alternatives that exploit category information for information retrieval.

#index 840880
#* A smoothed boosting algorithm using probabilistic output codes
#@ Rong Jin;Jian Zhang
#t 2005
#c 19
#% 266255
#% 304936
#% 312727
#% 331916
#% 424997
#% 464273
#% 465751
#% 520224
#% 1272365
#! AdaBoost.OC has shown to be an effective method in boosting "weak" binary classifiers for multi-class learning. It employs the Error Correcting Output Code (ECOC) method to convert a multi-class learning problem into a set of binary classification problems, and applies the AdaBoost algorithm to solve them efficiently. In this paper, we propose a new boosting algorithm that improves the AdaBoost.OC algorithm in two aspects: 1) It introduces a smoothing mechanism into the boosting algorithm to alleviate the potential overfitting problem with the AdaBoost algorithm, and 2) It introduces a probabilistic coding scheme to generate binary codes for multiple classes such that training errors can be efficiently reduced. Empirical studies with seven UCI datasets have indicated that the proposed boosting algorithm is more robust and effective than the AdaBoost.OC algorithm for multi-class learning.

#index 840881
#* Efficient discriminative learning of Bayesian network classifier via boosted augmented naive Bayes
#@ Yushi Jing;Vladimir Pavlović;James M. Rehg
#t 2005
#c 19
#% 129987
#% 246832
#% 252009
#% 578681
#% 766670
#% 770761
#% 799040
#! The use of Bayesian networks for classification problems has received significant recent attention. Although computationally efficient, the standard maximum likelihood learning method tends to be suboptimal due to the mismatch between its optimization criteria (data likelihood) and the actual goal for classification (label prediction). Recent approaches to optimizing the classification performance during parameter or structure learning show promise, but lack the favorable computational properties of maximum likelihood learning. In this paper we present the Boosted Augmented Naive Bayes (BAN) classifier. We show that a combination of discriminative data-weighting with generative training of intermediate models can yield a computationally efficient method for discriminative parameter learning and structure selection.

#index 840882
#* A support vector method for multivariate performance measures
#@ Thorsten Joachims
#t 2005
#c 19
#% 116149
#% 194284
#% 197394
#% 340904
#% 425031
#% 464606
#% 466229
#% 564279
#% 769875
#% 769882
#% 770763
#% 770788
#! This paper presents a Support Vector Method for optimizing multivariate nonlinear performance measures like the F1-score. Taking a multivariate prediction approach, we give an algorithm with which such multivariate SVMs can be trained in polynomial time for large classes of potentially non-linear performance measures, in particular ROCArea and all measures that can be computed from the contingency table. The conventional classification SVM arises as a special case of our method.

#index 840883
#* Error bounds for correlation clustering
#@ Thorsten Joachims;John Hopcroft
#t 2005
#c 19
#% 290830
#% 331989
#% 342621
#% 460812
#% 593926
#% 656762
#% 675366
#% 749492
#% 765548
#% 815876
#% 1809449
#! This paper presents a learning theoretical analysis of correlation clustering (Bansal et al., 2002). In particular, we give bounds on the error with which correlation clustering recovers the correct partition in a planted partition model (Condon & Karp, 2001; McSherry, 2001). Using these bounds, we analyze how the accuracy of correlation clustering scales with the number of clusters and the sparsity of the graph. We also propose a statistical test that analyzes the significance of the clustering found by correlation clustering.

#index 840884
#* Interactive learning of mappings from visual percepts to actions
#@ Sébastien Jodogne;Justus H. Piater
#t 2005
#c 19
#% 111440
#% 136350
#% 227526
#% 266288
#% 313140
#% 366058
#% 393786
#% 425080
#% 589983
#% 635689
#% 644560
#% 702594
#% 713072
#% 838742
#! We introduce flexible algorithms that can automatically learn mappings from images to actions by interacting with their environment. They work by introducing an image classifier in front of a Reinforcement Learning algorithm. The classifier partitions the visual space according to the presence or absence of highly informative local descriptors. The image classifier is incrementally refined by selecting new local descriptors when perceptual aliasing is detected. Thus, we reduce the visual input domain down to a size manageable by Reinforcement Learning, permitting us to learn direct percept-to-action mappings. Experimental results on a continuous visual navigation task illustrate the applicability of the framework.

#index 840885
#* A causal approach to hierarchical decomposition of factored MDPs
#@ Anders Jonsson;Andrew Barto
#t 2005
#c 19
#% 75936
#% 272662
#% 286423
#% 458686
#% 464303
#% 464607
#% 464636
#% 466915
#% 495933
#% 1271827
#% 1289239
#% 1290041
#% 1650297
#% 1673002
#! We present Variable Influence Structure Analysis, an algorithm that dynamically performs hierarchical decomposition of factored Markov decision processes. Our algorithm determines causal relationships between state variables and introduces temporally-extended actions that cause the values of state variables to change. Each temporally-extended action corresponds to a subtask that is significantly easier to solve than the overall task. Results from experiments show great promise in scaling to larger tasks.

#index 840886
#* A comparison of tight generalization error bounds
#@ Matti Kääriäinen;John Langford
#t 2005
#c 19
#% 209021
#% 269217
#% 276526
#% 431293
#! We investigate the empirical applicability of several bounds (a number of which are new) on the true error rate of learned classifiers which hold whenever the examples are chosen independently at random from a fixed distribution.The collection of tricks we use includes:1. A technique using unlabeled data for a tight derandomization of randomized bounds.2. A tight form of the progressive validation bound.3. The exact form of the test set bound.The bounds are implemented in the semibound package and are freely available.

#index 840887
#* Generalized LARS as an effective feature selection tool for text classification with SVMs
#@ S. Sathiya Keerthi
#t 2005
#c 19
#% 458379
#% 465754
#% 722930
#% 722935
#! In this paper we generalize the LARS feature selection method to the linear SVM model, derive an efficient algorithm for it, and empirically demonstrate its usefulness as a feature selection tool for text classification.

#index 840888
#* Ensembles of biased classifiers
#@ Rinat Khoussainov;Andreas Heß;Nicholas Kushmerick
#t 2005
#c 19
#% 73372
#% 198701
#% 209021
#% 235377
#% 269218
#% 277919
#% 290482
#% 424996
#% 770817
#% 837668
#% 1271973
#% 1289463
#! We propose a novel ensemble learning algorithm called Triskel, which has two interesting features. First, Triskel learns an ensemble of classifiers, each biased to have high precision on instances from a single class (as opposed to, for example, boosting, where the ensemble members are biased to maximise accuracy over a subset of instances from all classes). Second, the ensemble members' voting weights are assigned so that certain pairs of biased classifiers outweigh the rest of the ensemble, if their predictions agree. Our experiments demonstrate that Triskel often outperforms boosting, in terms of both accuracy and training time. We also present an ROC analysis, which shows that Triskel's iterative structure corresponds to a sequence of nested ROC spaces. The analysis predicts that Triskel works best when there are concavities in the ROC curves; this prediction agrees with our empirical results.

#index 840889
#* Computational aspects of Bayesian partition models
#@ Mikko Koivisto;Kismat Sood
#t 2005
#c 19
#% 44876
#% 129987
#% 197387
#% 289947
#% 414761
#% 448202
#% 763715
#% 1271900
#% 1650705
#% 1650783
#! The conditional distribution of a discrete variable y, given another discrete variable x, is often specified by assigning one multinomial distribution to each state of x. The cost of this rich parametrization is the loss of statistical power in cases where the data actually fits a model with much fewer parameters. In this paper, we consider a model that partitions the state space of x into disjoint sets, and assigns a single Dirichlet-multinomial to each set. We treat the partition as an unknown variable which is to be integrated away when the interest is in a coarser level task, e.g., variable selection or classification. Based on two different computational approaches, we present two exact algorithms for integration over partitions. Respective complexity bounds are derived in terms of detailed problem characteristics, including the size of the data and the size of the state space of x. Experiments on synthetic data demonstrate the applicability of the algorithms.

#index 840890
#* Learning the structure of Markov logic networks
#@ Stanley Kok;Pedro Domingos
#t 2005
#c 19
#% 26722
#% 197387
#% 226437
#% 226495
#% 246831
#% 396021
#% 449508
#% 496116
#% 568785
#% 577271
#% 729913
#% 816181
#% 1269496
#% 1271843
#% 1650403
#% 1673026
#! Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. In this paper we develop an algorithm for learning the structure of MLNs from relational databases, combining ideas from inductive logic programming (ILP) and feature induction in Markov networks. The algorithm performs a beam or shortest-first search of the space of clauses, guided by a weighted pseudo-likelihood measure. This requires computing the optimal weights for each candidate structure, but we show how this can be done efficiently. The algorithm can be used to learn an MLN from scratch, or to refine an existing knowledge base. We have applied it in two real-world domains, and found that it outperforms using off-the-shelf ILP systems to learn the MLN structure, as well as pure ILP, purely probabilistic and purely knowledge-based approaches.

#index 840891
#* Using additive expert ensembles to cope with concept drift
#@ Jeremy Z. Kolter;Marcus A. Maloof
#t 2005
#c 19
#% 81507
#% 165663
#% 170405
#% 204531
#% 226674
#% 232319
#% 235377
#% 262903
#% 266792
#% 342639
#% 451055
#% 722905
#% 727880
#! We consider online learning where the target concept can change over time. Previous work on expert prediction algorithms has bounded the worst-case performance on any subsequence of the training data relative to the performance of the best expert. However, because these "experts" may be difficult to implement, we take a more general approach and bound performance relative to the actual performance of any online learner on this single subsequence. We present the additive expert ensemble algorithm AddExp, a new, general method for using any online learner for drifting concepts. We adapt techniques for analyzing expert prediction algorithms to prove mistake and loss bounds for a discrete and a continuous version of AddExp. Finally, we present pruning methods and empirical results for data sets with concept drift.

#index 840892
#* Semi-supervised graph clustering: a kernel approach
#@ Brian Kulis;Sugato Basu;Inderjit Dhillon;Raymond Mooney
#t 2005
#c 19
#% 309208
#% 313959
#% 460812
#% 464291
#% 464608
#% 732539
#% 769881
#% 769935
#% 1279294
#! Semi-supervised clustering algorithms aim to improve clustering results using limited supervision. The supervision is generally given as pairwise constraints; such constraints are natural for graphs, yet most semi-supervised clustering algorithms are designed for data represented as vectors. In this paper, we unify vector-based and graph-based approaches. We show that a recently-proposed objective function for semi-supervised clustering based on Hidden Markov Random Fields, with squared Euclidean distance and a certain class of constraint penalty functions, can be expressed as a special case of the weighted kernel k-means objective. A recent theoretical connection between kernel k-means and several graph clustering objectives enables us to perform semi-supervised clustering of data given either as vectors or as a graph. For vector data, the kernel approach also enables us to find clusters with non-linear boundaries in the input data space. Furthermore, we show that recent work on spectral learning (Kamvar et al., 2003) may be viewed as a special case of our formulation. We empirically show that our algorithm is able to outperform current state-of-the-art semi-supervised algorithms on both vector-based and graph-based data sets.

#index 840893
#* A brain computer interface with online feedback based on magnetoencephalography
#@ Thomas Navin Lal;Michael Schröder;N. Jeremy Hill;Hubert Preissl;Thilo Hinterberger;Jürgen Mellinger;Martin Bogdan;Wolfgang Rosenstiel;Thomas Hofmann;Niels Birbaumer;Bernhard Schölkopf
#t 2005
#c 19
#% 197394
#% 207376
#% 425048
#% 829211
#! The aim of this paper is to show that machine learning techniques can be used to derive a classifying function for human brain signal data measured by magnetoencephalography (MEG), for the use in a brain computer interface (BCI). This is especially helpful for evaluating quickly whether a BCI approach based on electroencephalography, on which training may be slower due to lower signal-to-noise ratio, is likely to succeed. We apply RCE and regularized SVMs to the experimental data of ten healthy subjects performing a motor imagery task. Four subjects were able to use a trained classifier to write a short name. Further analysis gives evidence that the proposed imagination task is suboptimal for the possible extension to a multiclass interface. To the best of our knowledge this paper is the first working online MEG-based BCI and is therefore a "proof of concept".

#index 840894
#* Relating reinforcement learning performance to classification performance
#@ John Langford;Bianca Zadrozny
#t 2005
#c 19
#% 280088
#% 384911
#% 393786
#% 464624
#% 466075
#% 495927
#% 727925
#% 788097
#% 1705511
#! We prove a quantitative connection between the expected sum of rewards of a policy and binary classification performance on created subproblems. This connection holds without any unobservable assumptions (no assumption of independence, small mixing time, fully observable states, or even hidden states) and the resulting statement is independent of the number of states or actions. The statement is critically dependent on the size of the rewards and prediction performance of the created classifiers.We also provide some general guidelines for obtaining good classification performance on the created subproblems. In particular, we discuss possible methods for generating training examples for a classifier learning algorithm.

#index 840895
#* PAC-Bayes risk bounds for sample-compressed Gibbs classifiers
#@ François Laviolette;Mario Marchand
#t 2005
#c 19
#% 115608
#% 201259
#% 302395
#% 431293
#% 562940
#% 722896
#% 722916
#! We extend the PAC-Bayes theorem to the sample-compression setting where each classifier is represented by two independent sources of information: a compression set which consists of a small subset of the training data, and a message string of the additional information needed to obtain a classifier. The new bound is obtained by using a prior over a data-independent set of objects where each object gives a classifier only when the training data is provided. The new PAC-Bayes theorem states that a Gibbs classifier defined on a posterior over sample-compressed classifiers can have a smaller risk bound than any such (deterministic) sample-compressed classifier.

#index 840896
#* Heteroscedastic Gaussian process regression
#@ Quoc V. Le;Alex J. Smola;Stéphane Canu
#t 2005
#c 19
#% 272516
#% 277516
#% 722815
#% 763697
#% 788036
#% 872759
#! This paper presents an algorithm to estimate simultaneously both mean and variance of a non parametric regression problem. The key point is that we are able to estimate variance locally unlike standard Gaussian Process regression or SVMs. This means that our estimator adapts to the local noise. The problem is cast in the setting of maximum a posteriori estimation in exponential families. Unlike previous work, we obtain a convex optimization problem which can be solved via Newton's method.

#index 840897
#* Predicting relative performance of classifiers from samples
#@ Rui Leite;Pavel Brazdil
#t 2005
#c 19
#% 168280
#% 191910
#% 280406
#% 359837
#% 431102
#% 466722
#! This paper is concerned with the problem of predicting relative performance of classification algorithms. It focusses on methods that use results on small samples and discusses the shortcomings of previous approaches. A new variant is proposed that exploits, as some previous approaches, meta-learning. The method requires that experiments be conducted on few samples. The information gathered is used to identify the nearest learning curve for which the sampling procedure was carried out fully. This in turn permits to generate a prediction regards the relative performance of algorithms. Experimental evaluation shows that the method competes well with previous approaches and provides quite good and practical solution to this problem.

#index 840898
#* Logistic regression with an auxiliary data source
#@ Xuejun Liao;Ya Xue;Lawrence Carin
#t 2005
#c 19
#% 115608
#% 132697
#% 311027
#% 727925
#% 770847
#% 770858
#! To achieve good generalization in supervised learning, the training and testing examples are usually required to be drawn from the same source distribution. In this paper we propose a method to relax this requirement in the context of logistic regression. Assuming Dp and Da are two sets of examples drawn from two mismatched distributions, where Da are fully labeled and Dp partially labeled, our objective is to complete the labels of Dp. We introduce an auxiliary variable μ for each example in Da to reflect its mismatch with Dp. Under an appropriate constraint the μ's are estimated as a byproduct, along with the classifier. We also present an active learning approach for selecting the labeled examples in Dp. The proposed algorithm, called "Migratory-Logit" or M-Logit, is demonstrated successfully on simulated as well as real data sets.

#index 840899
#* Predicting protein folds with structural repeats using a chain graph model
#@ Yan Liu;Eric P. Xing;Jaime Carbonell
#t 2005
#c 19
#% 328320
#% 464434
#% 471266
#% 770759
#% 770776
#% 1650633
#! Protein fold recognition is a key step towards inferring the tertiary structures from amino-acid sequences. Complex folds such as those consisting of interacting structural repeats are prevalent in proteins involved in a wide spectrum of biological functions. However, extant approaches often perform inadequately due to their inability to capture long-range interactions between structural units and to handle low sequence similarities across proteins (under 25% identity). In this paper, we propose a chain graph model built on a causally connected series of segmentation conditional random fields (SCRFs) to address these issues. Specifically, the SCRF model captures long-range interactions within recurring structural units and the Bayesian network backbone decomposes cross-repeat interactions into locally computable modules consisting of repeat-specific SCRFs and a model for sequence motifs. We applied this model to predict β-helices and leucine-rich repeats, and found it significantly outperforms extant methods in predictive accuracy and/or computational efficiency.

#index 840900
#* Unsupervised evidence integration
#@ Philip M. Long;Vinay Varadan;Sarah Gilman;Mark Treshock;Rocco A. Servedio
#t 2005
#c 19
#% 564279
#% 717093
#! Many biological propositions can be supported by a variety of different types of evidence. It is often useful to collect together large numbers of such propositions, together with the evidence supporting them, into databases to be used in other analyses. Methods that automatically make preliminary choices about which propositions to include can be helpful, if they are accurate enough. This can involve weighing evidence of varying strength.We describe a method for learning a scoring function to weigh evidence of different types. The algorithm evaluates each source of evidence by the extent to which other sources tend to support it. The details are guided by a probabilistic formulation of the problem, building on previous theoretical work. We evaluate our method by applying it to predict protein-protein interactions in yeast, and using synthetic data.

#index 840901
#* Naive Bayes models for probability estimation
#@ Daniel Lowd;Pedro Domingos
#t 2005
#c 19
#% 44876
#% 197387
#% 205391
#% 232117
#% 246831
#% 338609
#% 420515
#% 722753
#% 1650569
#% 1650579
#% 1650783
#! Naive Bayes models have been widely used for clustering and classification. However, they are seldom used for general probabilistic learning and inference (i.e., for estimating and computing arbitrary joint, conditional and marginal distributions). In this paper we show that, for a wide range of benchmark datasets, naive Bayes models learned using EM have accuracy and learning time comparable to Bayesian networks with context-specific independence. Most significantly, naive Bayes inference is orders of magnitude faster than Bayesian network inference using Gibbs sampling and belief propagation. This makes naive Bayes models a very attractive alternative to Bayesian networks for general probability estimation, particularly in large or real-time domains.

#index 840902
#* ROC confidence bands: an empirical evaluation
#@ Sofus A. Macskassy;Foster Provost;Saharon Rosset
#t 2005
#c 19
#% 290482
#% 466086
#% 466568
#% 642988
#% 723244
#% 840902
#% 1378224
#! This paper is about constructing confidence bands around ROC curves. We first introduce to the machine learning community three band-generating methods from the medical field, and evaluate how well they perform. Such confidence bands represent the region where the "true" ROC curve is expected to reside, with the designated confidence level. To assess the containment of the bands we begin with a synthetic world where we know the true ROC curve---specifically, where the class-conditional model scores are normally distributed. The only method that attains reasonable containment out-of-the-box produces non-parametric, "fixed-width" bands (FWBs). Next we move to a context more appropriate for machine learning evaluations: bands that with a certain confidence level will bound the performance of the model on future data. We introduce a correction to account for the larger uncertainty, and the widened FWBs continue to have reasonable containment. Finally, we assess the bands on 10 relatively large benchmark data sets. We conclude by recommending these FWBs, noting that being non-parametric they are especially attractive for machine learning studies, where the score distributions (1) clearly are not normal, and (2) even for the same data set vary substantially from learning method to learning method.

#index 840903
#* Modeling word burstiness using the Dirichlet distribution
#@ Rasmus E. Madsen;David Kauchak;Charles Elkan
#t 2005
#c 19
#% 165110
#% 280819
#% 321635
#% 344447
#% 458369
#% 570316
#% 642976
#% 722904
#% 742513
#% 817455
#! Multinomial distributions are often used to model text documents. However, they do not capture well the phenomenon that words in a document tend to appear in bursts: if a word appears once, it is more likely to appear again. In this paper, we propose the Dirichlet compound multinomial model (DCM) as an alternative to the multinomial. The DCM model has one additional degree of freedom, which allows it to capture burstiness. We show experimentally that the DCM is substantially better than the multinomial at modeling text data, measured by perplexity. We also show using three standard document collections that the DCM leads to better classification than the multinomial model. DCM performance is comparable to that obtained with multiple heuristic changes to the multinomial model.

#index 840904
#* Proto-value functions: developmental reinforcement learning
#@ Sridhar Mahadevan
#t 2005
#c 19
#% 384911
#% 393786
#% 458686
#% 527994
#% 593842
#% 732552
#% 734920
#! This paper presents a novel framework called proto-reinforcement learning (PRL), based on a mathematical model of a proto-value function: these are task-independent basis functions that form the building blocks of all value functions on a given state space manifold. Proto-value functions are learned not from rewards, but instead from analyzing the topology of the state space. Formally, proto-value functions are Fourier eigenfunctions of the Laplace-Beltrami diffusion operator on the state space manifold. Proto-value functions facilitate structural decomposition of large state spaces, and form geodesically smooth orthonormal basis functions for approximating any value function. The theoretical basis for proto-value functions combines insights from spectral graph theory, harmonic analysis, and Riemannian manifolds. Proto-value functions enable a novel generation of algorithms called representation policy iteration, unifying the learning of representation and behavior.

#index 840905
#* The cross entropy method for classification
#@ Shie Mannor;Dori Peleg;Reuven Rubinstein
#t 2005
#c 19
#% 53981
#% 201259
#% 269218
#% 309208
#% 351094
#% 390723
#% 450895
#% 466084
#% 722817
#% 722894
#% 770427
#% 856251
#! We consider support vector machines for binary classification. As opposed to most approaches we use the number of support vectors (the "L0 norm") as a regularizing term instead of the L1 or L2 norms. In order to solve the optimization problem we use the cross entropy method to search over the possible sets of support vectors. The algorithm consists of solving a sequence of efficient linear programs. We report experiments where our method produces generalization errors that are similar to support vector machines, while using a considerably smaller number of support vectors.

#index 840906
#* Bounded real-time dynamic programming: RTDP with monotone upper bounds and performance guarantees
#@ H. Brendan McMahan;Maxim Likhachev;Geoffrey J. Gordon
#t 2005
#c 19
#% 181627
#% 194647
#% 337981
#% 393786
#% 788098
#% 1272055
#% 1279387
#! MDPs are an attractive formalization for planning, but realistic problems often have intractably large state spaces. When we only need a partial policy to get from a fixed start state to a goal, restricting computation to states relevant to this task can make much larger problems tractable. We introduce a new algorithm, Bounded RTDP, which can produce partial policies with strong performance guarantees while only touching a fraction of the state space, even on problems where other algorithms would have to visit the full state space. To do so, Bounded RTDP maintains both upper and lower bounds on the optimal value function. The performance of Bounded RTDP is greatly aided by the introduction of a new technique to efficiently find suitable upper bounds; this technique can also be used to provide informed initialization to a wide range of other planning algorithms.

#index 840907
#* Comparing clusterings: an axiomatic view
#@ Marina Meilǎ
#t 2005
#c 19
#% 17485
#% 115608
#% 650937
#! This paper views clusterings as elements of a lattice. Distances between clusterings are analyzed in their relationship to the lattice. From this vantage point, we first give an axiomatic characterization of some criteria for comparing clusterings, including the variation of information and the unadjusted Rand index. Then we study other distances between partitions w.r.t these axioms and prove an impossibility result: there is no "sensible" criterion for comparing clusterings that is simultaneously (1) aligned with the lattice of partitions, (2) convexely additive, and (3) bounded.

#index 840908
#* Weighted decomposition kernels
#@ Sauro Menchetti;Fabrizio Costa;Paolo Frasconi
#t 2005
#c 19
#% 342604
#% 458667
#% 722803
#% 722908
#% 727896
#% 731607
#% 743284
#% 769891
#% 771841
#% 771848
#% 1855717
#! We introduce a family of kernels on discrete data structures within the general class of decomposition kernels. A weighted decomposition kernel (WDK) is computed by dividing objects into substructures indexed by a selector. Two substructures are then matched if their selectors satisfy an equality predicate, while the importance of the match is determined by a probability kernel on local distributions fitted on the substructures. Under reasonable assumptions, a WDK can be computed efficiently and can avoid combinatorial explosion of the feature space. We report experimental evidence that the proposed kernel is highly competitive with respect to more complex state-of-the-art methods on a set of problems in bioinformatics.

#index 840909
#* High speed obstacle avoidance using monocular vision and reinforcement learning
#@ Jeff Michels;Ashutosh Saxena;Andrew Y. Ng
#t 2005
#c 19
#% 163348
#% 305085
#% 325689
#% 384911
#% 424094
#% 527859
#% 818481
#! We consider the task of driving a remote control car at high speeds through unstructured outdoor environments. We present an approach in which supervised learning is first used to estimate depths from single monocular images. The learning algorithm can be trained either on real camera images labeled with ground-truth distances to the closest obstacles, or on a training set consisting of synthetic graphics images. The resulting algorithm is able to learn monocular vision cues that accurately estimate the relative depths of obstacles in a scene. Reinforcement learning/policy search is then applied within a simulator that renders synthetic scenes. This learns a control policy that selects a steering direction as a function of the vision system's output. We present results evaluating the predictive ability of the algorithm both on held out test data, and in actual autonomous driving experiments.

#index 840910
#* Dynamic preferences in multi-criteria reinforcement learning
#@ Sriraam Natarajan;Prasad Tadepalli
#t 2005
#c 19
#% 188153
#% 203604
#% 251784
#% 252183
#% 383470
#% 464274
#% 464306
#% 465915
#% 466418
#% 466753
#% 578692
#% 613339
#% 763707
#% 770852
#% 1650613
#! The current framework of reinforcement learning is based on maximizing the expected returns based on scalar rewards. But in many real world situations, tradeoffs must be made among multiple objectives. Moreover, the agent's preferences between different objectives may vary with time. In this paper, we consider the problem of learning in the presence of time-varying preferences among multiple objectives, using numeric weights to represent their importance. We propose a method that allows us to store a finite number of policies, choose an appropriate policy for any weight vector and improve upon it. The idea is that although there are infinitely many weight vectors, they may be well-covered by a small number of optimal policies. We show this empirically in two domains: a version of the Buridan's ass problem and network routing.

#index 840911
#* Learning first-order probabilistic models with combining rules
#@ Sriraam Natarajan;Prasad Tadepalli;Eric Altendorf;Thomas G. Dietterich;Alan Fern;Angelo Restificar
#t 2005
#c 19
#% 147677
#% 246835
#% 484593
#% 790446
#% 1271905
#% 1650727
#! First-order probabilistic models allow us to model situations in which a random variable in the first-order model may have a large and varying numbers of parent variables in the ground ("unrolled") model. One approach to compactly describing such models is to independently specify the probability of a random variable conditioned on each individual parent (or small sets of parents) and then combine these conditional distributions via a combining rule (e.g., Noisy-OR). This paper presents algorithms for learning with combining rules. Specifically, algorithms based on gradient descent and expectation maximization are derived, implemented, and evaluated on synthetic data and on a real-world task. The results demonstrate that the algorithms are able to learn the parameters of both the individual parent-target distributions and the combining rules.

#index 840912
#* An efficient method for simplifying support vector machines
#@ DucDung Nguyen;TuBao Ho
#t 2005
#c 19
#% 116149
#% 190581
#% 197394
#% 304931
#% 309208
#% 344993
#% 420077
#% 458379
#% 592108
#% 1860543
#! In this paper we describe a new method to reduce the complexity of support vector machines by reducing the number of necessary support vectors included in their solutions. The reduction process iteratively selects two nearest support vectors belonging to the same class and replaces them by a newly constructed vector. Through the analysis of relation between vectors in the input and feature spaces, we present the construction of new vectors that requires to find the unique maximum point of a one-variable function on the interval (0, 1), not to minimize a function of many variables with local minimums in former reduced set methods. Experimental results on real life datasets show that the proposed method is effective in reducing number of support vectors and preserving machine's generalization performance.

#index 840913
#* Predicting good probabilities with supervised learning
#@ Alexandru Niculescu-Mizil;Rich Caruana
#t 2005
#c 19
#% 464280
#% 577298
#! We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.

#index 840914
#* Recycling data for multi-agent learning
#@ Santiago Ontañón;Enric Plaza
#t 2005
#c 19
#% 209021
#% 465912
#% 490445
#% 755461
#! Learning agents can improve performance cooperating with other agents, particularly learning agents forming a committee outperform individual agents. This "ensemble effect" is well known for multi-classifier systems in Machine Learning. However, multi-classifier systems assume all data is known to all classifiers while we focus on agents that learn from cases (examples) that are owned and stored individually. In this article we focus on how individual agents can engage in bargaining activities that improve the performance of both individual agents and the committee. The agents are capable of self-evaluation and determining that some data used for learning is unnecessary. This "refuse" data can then be exploited by other agents that might found some part of it profitable to improve their performance. The experiments we performed show that this approach improves both individual and committee performance and we analyze how these results in terms of the "ensemble effect".

#index 840915
#* A graphical model for chord progressions embedded in a psychoacoustic space
#@ Jean-François Paiement;Douglas Eck;Samy Bengio;David Barber
#t 2005
#c 19
#% 730140
#! Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term dependencies in music. In this paper, a distributed representation for chords is designed such that Euclidean distances roughly correspond to psychoacoustic dissimilarities. Parameters in the graphical models are learnt with the EM algorithm and the classical Junction Tree algorithm. Various model architectures are compared in terms of conditional out-of-sample likelihood. Both perceptual and statistical evidence show that binary trees related to meter are well suited to capture chord dependencies.

#index 840916
#* Q-learning of sequential attention for visual object recognition from informative local descriptors
#@ Lucas Paletta;Gerald Fritz;Christin Seifert
#t 2005
#c 19
#% 124691
#% 334612
#% 760805
#% 1250189
#! This work provides a framework for learning sequential attention in real-world visual object recognition, using an architecture of three processing stages. The first stage rejects irrelevant local descriptors based on an information theoretic saliency measure, providing candidates for foci of interest (FOI). The second stage investigates the information in the FOI using a codebook matcher and providing weak object hypotheses. The third stage integrates local information via shifts of attention, resulting in chains of descriptor-action pairs that characterize object discrimination. A Q-learner adapts then from explorative search and evaluative feedback from entropy decreases on the attention sequences, eventually prioritizing shifts that lead to a geometry of descriptor-action scanpaths that is highly discriminative with respect to object recognition. The methodology is successfully evaluated on indoors (COIL-20 database) and outdoors (TSG-20 database) imagery, demonstrating significant impact by learning, outperforming standard local descriptor based methods both in recognition accuracy and processing time.

#index 840917
#* Discriminative versus generative parameter and structure learning of Bayesian network classifiers
#@ Franz Pernkopf;Jeff Bilmes
#t 2005
#c 19
#% 44876
#% 115608
#% 126227
#% 205380
#% 243728
#% 246832
#% 376266
#% 388024
#% 528154
#% 578681
#% 709350
#% 715701
#% 770761
#% 1378256
#! In this paper, we compare both discriminative and generative parameter learning on both discriminatively and generatively structured Bayesian network classifiers. We use either maximum likelihood (ML) or conditional maximum likelihood (CL) to optimize network parameters. For structure learning, we use either conditional mutual information (CMI), the explaining away residual (EAR), or the classification rate (CR) as objective functions. Experiments with the naive Bayes classifier (NB), the tree augmented naive Bayes classifier (TAN), and the Bayesian multinet have been performed on 25 data sets from the UCI repository (Merz et al., 1997) and from (Kohavi & John, 1997). Our empirical study suggests that discriminative structures learnt using CR produces the most accurate classifiers on almost half the data sets. This approach is feasible, however, only for rather small problems since it is computationally expensive. Discriminative parameter learning produces on average a better classifier than ML parameter learning.

#index 840918
#* Optimizing abstaining classifiers using ROC analysis
#@ Tadeusz Pietraszek
#t 2005
#c 19
#% 266280
#% 272995
#% 290482
#% 770817
#! Classifiers that refrain from classification in certain cases can significantly reduce the misclassification cost. However, the parameters for such abstaining classifiers are often set in a rather ad-hoc manner. We propose a method to optimally build a specific type of abstaining binary classifiers using ROC analysis. These classifiers are built based on optimization criteria in the following three models: cost-based, bounded-abstention and bounded-improvement. We demonstrate the usage and applications of these models to effectively reduce misclassification cost in real classification systems. The method has been validated with a ROC building algorithm and cross-validation on 15 UCI KDD datasets.

#index 840919
#* Independent subspace analysis using geodesic spanning trees
#@ Barnabás Póczos;András Lõrincz
#t 2005
#c 19
#% 106318
#% 176172
#% 190861
#% 272373
#% 299264
#% 722887
#% 734933
#% 856933
#% 1759699
#! A novel algorithm for performing Independent Subspace Analysis, the estimation of hidden independent subspaces is introduced. This task is a generalization of Independent Component Analysis. The algorithm works by estimating the multi-dimensional differential entropy. The estimation utilizes minimal geodesic spanning trees matched to the sample points. Numerical studies include (i) illustrative examples, (ii) a generalization of the cocktail-party problem to songs played by bands, and (iii) an example on mixed independent subspaces, where subspaces have dependent sources, which are pairwise independent.

#index 840920
#* A model for handling approximate, noisy or incomplete labeling in text classification
#@ Ganesh Ramakrishnan;Krishna Prasad Chitrapura;Raghu Krishnapuram;Pushpak Bhattacharyya
#t 2005
#c 19
#% 213009
#% 280437
#% 280819
#% 311027
#% 318412
#% 458369
#% 458379
#% 1499584
#! We introduce a Bayesian model, BayesANIL, that is capable of estimating uncertainties associated with the labeling process. Given a labeled or partially labeled training corpus of text documents, the model estimates the joint distribution of training documents and class labels by using a generalization of the Expectation Maximization algorithm. The estimates can be used in standard classification models to reduce error rates. Since uncertainties in the labeling are taken into account, the model provides an elegant mechanism to deal with noisy labels. We provide an intuitive modification to the EM iterations by re-estimating the empirical. distribution in order to reinforce feature values in unlabeled data and to reduce the influence of noisily labeled examples. Considerable improvement in the classification accuracies of two popular classification algorithms on standard labeled data-sets with and without artificially introduced noise, as well as in the presence and absence of unlabeled data, indicates that this may be a promising method to reduce the burden of manual labeling.

#index 840921
#* Healing the relevance vector machine through augmentation
#@ Carl Edward Rasmussen;Joaquin Quiñonero-Candela
#t 2005
#c 19
#% 304961
#% 450245
#% 466597
#% 722760
#% 857421
#! The Relevance Vector Machine (RVM) is a sparse approximate Bayesian kernel method. It provides full predictive distributions for test cases. However, the predictive uncertainties have the unintuitive property, that they get smaller the further you move away from the training cases. We give a thorough analysis. Inspired by the analogy to non-degenerate Gaussian Processes, we suggest augmentation to solve the problem. The purpose of the resulting model, RVM*, is primarily to corroborate the theoretical and experimental analysis. Although RVM* could be used in practical applications, it is no longer a truly sparse model. Experiments show that sparsity comes at the expense of worse predictive. distributions.

#index 840922
#* Supervised versus multiple instance learning: an empirical comparison
#@ Soumya Ray;Mark Craven
#t 2005
#c 19
#% 44876
#% 224755
#% 256756
#% 269217
#% 272527
#% 378173
#% 396021
#% 449508
#% 464621
#% 464633
#% 465916
#% 565537
#% 707541
#% 770827
#% 1378224
#! We empirically study the relationship between supervised and multiple instance (MI) learning. Algorithms to learn various concepts have been adapted to the MI representation. However, it is also known that concepts that are PAC-learnable with one-sided noise can be learned from MI data. A relevant question then is: how well do supervised learners do on MI data? We attempt to answer this question by looking at a cross section of MI data sets from various domains coupled with a number of learning algorithms including Diverse Density, Logistic Regression, nonlinear Support Vector Machines and FOIL. We consider a supervised and MI version of each learner. Several interesting conclusions emerge from our work: (1) no MI algorithm is superior across all tested domains, (2) some MI algorithms are consistently superior to their supervised counterparts, (3) using high false-positive costs can improve a supervised learner's performance in MI domains, and (4) in several domains, a supervised algorithm is superior to any MI algorithm we tested.

#index 840923
#* Generalized skewing for functions with continuous and nominal attributes
#@ Soumya Ray;David Page
#t 2005
#c 19
#% 136350
#% 209021
#% 235377
#% 723244
#% 770814
#% 1279300
#% 1290031
#! This paper extends previous work on skewing, an approach to problematic functions in decision tree induction. The previous algorithms were applicable only to functions of binary variables. In this paper, we extend skewing to directly handle functions of continuous and nominal variables. We present experiments with randomly generated functions and a number of real world datasets to evaluate the algorithm's accuracy. Our results indicate that our algorithm almost always outperforms an Information Gain-based decision tree learner.

#index 840924
#* Fast maximum margin matrix factorization for collaborative prediction
#@ Jasson D. M. Rennie;Nathan Srebro
#t 2005
#c 19
#% 338443
#% 420507
#% 465928
#% 734592
#% 766422
#% 770859
#% 1705537
#! Maximum Margin Matrix Factorization (MMMF) was recently suggested (Srebro et al., 2005) as a convex, infinite dimensional alternative to low-rank approximations and standard factor models. MMMF can be formulated as a semi-definite programming (SDP) and learned using standard SDP solvers. However, current SDP solvers can only handle MMMF problems on matrices of dimensionality up to a few hundred. Here, we investigate a direct gradient-based optimization method for MMMF and demonstrate it on large collaborative prediction problems. We compare against results obtained by Marlin (2004) and find that MMMF substantially outperforms all nine methods he tested.

#index 840925
#* Coarticulation: an approach for generating concurrent plans in Markov decision processes
#@ Khashayar Rohanimanesh;Sridhar Mahadevan
#t 2005
#c 19
#% 289947
#% 377895
#% 384911
#% 424793
#% 565550
#% 711673
#% 712581
#% 1271882
#% 1650369
#! We study an approach for performing concurrent activities in Markov decision processes (MDPs) based on the coarticulation framework. We assume that the agent has multiple degrees of freedom (DOF) in the action space which enables it to perform activities simultaneously. We demonstrate that one natural way for generating concurrency in the system is by coarticulating among the set of learned activities available to the agent. In general due to the multiple DOF in the system, often there exists a redundant set of admissible sub-optimal policies associated with each learned activity. Such flexibility enables the agent to concurrently commit to several subgoals according to their priority levels, given a new task defined in terms of a set of prioritized subgoals. We present efficient approximate algorithms for computing such policies and for generating concurrent plans. We also evaluate our approach in a simulated domain.

#index 840926
#* Why skewing works: learning difficult Boolean functions with greedy tree learners
#@ Bernard Rosell;Lisa Hellerstein;Soumya Ray;David Page
#t 2005
#c 19
#% 115608
#% 136350
#% 156522
#% 580688
#% 770814
#% 1279300
#! We analyze skewing, an approach that has been empirically observed to enable greedy decision tree learners to learn "difficult" Boolean functions, such as parity, in the presence of irrelevant variables. We prove tha, in an idealized setting, for any function and choice of skew parameters, skewing finds relevant variables with probability 1. We present experiments exploring how different parameter choices affect the success of skewing in empirical settings. Finally, we analyze a variant of skewing called Sequential Skewing.

#index 840927
#* Integer linear programming inference for conditional random fields
#@ Dan Roth;Wen-tau Yih
#t 2005
#c 19
#% 302390
#% 464434
#% 816181
#% 853697
#% 854636
#% 939845
#% 1250184
#% 1289530
#! Inference in Conditional Random Fields and Hidden Markov Models is done using the Viterbi algorithm, an efficient dynamic programming algorithm. In many cases, general (non-local and non-sequential) constraints may exist over the output sequence, but cannot be incorporated and exploited in a natural way by this inference procedure. This paper proposes a novel inference procedure based on integer linear programming (ILP) and extends CRF models to naturally and efficiently support general constraint structures. For sequential constraints, this procedure reduces to simple linear programming as the inference process. Experimental evidence is supplied in the context of an important NLP problem, semantic role labeling.

#index 840928
#* Learning hierarchical multi-category text classification models
#@ Juho Rousu;Craig Saunders;Sandor Szedmak;John Shawe-Taylor
#t 2005
#c 19
#% 309141
#% 465747
#% 466078
#% 763708
#% 770763
#% 770796
#% 783478
#! We present a kernel-based algorithm for hierarchical text classification where the documents are allowed to belong to more than one category at a time. The classification model is a variant of the Maximum Margin Markov Network framework, where the classification hierarchy is represented as a Markov tree equipped with an exponential family defined on the edges. We present an efficient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classification hierarchies consisting of hundreds of nodes. The algorithm's predictive accuracy is competitive with other recently introduced hierarchical multi-category or multilabel classification learning algorithms.

#index 840929
#* Expectation maximization algorithms for conditional likelihoods
#@ Jarkko Salojärvi;Kai Puolamäki;Samuel Kaski
#t 2005
#c 19
#% 128653
#% 304919
#% 458673
#% 715701
#% 757953
#! We introduce an expectation maximization-type (EM) algorithm for maximum likelihood optimization of conditional densities. It is applicable to hidden variable models where the distributions are from the exponential family. The algorithm can alternatively be viewed as automatic step size selection for gradient ascent, where the amount of computation is traded off to guarantees that each step increases the likelihood. The tradeoff makes the algorithm computationally more feasible than the earlier conditional EM. The method gives a theoretical basis for extended Baum Welch algorithms used in discriminative hidden Markov models in speech recognition, and compares favourably with the current best method in the experiments.

#index 840930
#* Estimating and computing density based distance metrics
#@  Sajama;Alon Orlitsky
#t 2005
#c 19
#% 565545
#% 770851
#% 1672995
#% 1673021
#! Density-based distance metrics have applications in semi-supervised learning, nonlinear interpolation and clustering. We consider density-based metrics induced by Riemannian manifold structures and estimate them using kernel density estimators for the underlying data distribution. We lower bound the rate of convergence of these plug-in path-length estimates and hence of the metric, as the sample size increases. We present an upper bound on the rate of convergence of all estimators of the metric. We also show that the metric can be consistently computed using the shortest path algorithm on a suitably constructed graph on the data samples and lower bound the convergence rate of the computation error. We present experiments illustrating the use of the metrics for semi-supervised classification and non-linear interpolation.

#index 840931
#* Supervised dimensionality reduction using mixture models
#@  Sajama;Alon Orlitsky
#t 2005
#c 19
#% 97428
#% 304919
#% 466588
#% 722936
#% 763698
#! Given a classification problem, our goal is to find a low-dimensional linear transformation of the feature vectors which retains information needed to predict the class labels. We present a method based on maximum conditional likelihood estimation of mixture models. Use of mixture models allows us to approximate the distributions to any desired accuracy while use of conditional likelihood as the contrast function ensures that the selected subspace retains maximum possible mutual information between feature vectors and class labels. Classification experiments using Gaussian mixture components show that this method compares favorably to related dimension reduction techniques. Other distributions belonging to the exponential family can be used to reduce dimensions when data is of a special type, for example binary or integer valued data. We provide an EM-like algorithm for model estimation and present visualization experiments using Gaussian and Bernoulli mixture models.

#index 840932
#* Object correspondence as a machine learning problem
#@ Bernhard Schölkopf;Florian Steinke;Volker Blanz
#t 2005
#c 19
#% 117665
#% 235349
#% 247010
#% 279841
#% 309208
#% 317002
#% 340355
#% 403085
#% 436725
#% 457597
#% 662829
#% 771053
#% 781498
#% 855495
#! We propose machine learning methods for the estimation of deformation fields that transform two given objects into each other, thereby establishing a dense point to point correspondence. The fields are computed using a modified support vector machine containing a penalty enforcing that points of one object will be mapped to "similar" points on the other one. Our system, which contains little engineering or domain knowledge, delivers state of the art performance. We present application results including close to photorealistic morphs of 3D head models.

#index 840933
#* Analysis and extension of spectral methods for nonlinear dimensionality reduction
#@ Fei Sha;Lawrence K. Saul
#t 2005
#c 19
#% 209961
#% 519951
#% 593047
#% 723241
#% 1502529
#! Many unsupervised algorithms for nonlinear dimensionality reduction, such as locally linear embedding (LLE) and Laplacian eigenmaps, are derived from the spectral decompositions of sparse matrices. While these algorithms aim to preserve certain proximity relations on average, their embeddings are not explicitly designed to preserve local features such as distances or angles. In this paper, we show how to construct a low dimensional embedding that maximally preserves angles between nearby data points. The embedding is derived from the bottom eigenvectors of LLE and/or Laplacian eigenmaps by solving an additional (but small) problem in semidefinite programming, whose size is independent of the number of data points. The solution obtained by semidefinite programming also yields an estimate of the data's intrinsic dimensionality. Experimental results on several data sets demonstrate the merits of our approach.

#index 840934
#* Non-negative tensor factorization with applications to statistics and computer vision
#@ Amnon Shashua;Tamir Hazan
#t 2005
#c 19
#% 109394
#% 225606
#% 316143
#% 400277
#% 415697
#% 415756
#% 457831
#% 1650298
#% 1762405
#! We derive algorithms for finding a non-negative n-dimensional tensor factorization (n-NTF) which includes the non-negative matrix factorization (NMF) as a particular case when n = 2. We motivate the use of n-NTF in three areas of data analysis: (i) connection to latent class models in statistics, (ii) sparse image coding in computer vision, and (iii) model selection problems. We derive a "direct" positive-preserving gradient descent algorithm and an alternating scheme based on repeated multiple rank-1 problems.

#index 840935
#* Fast inference and learning in large-state-space HMMs
#@ Sajid M. Siddiqi;Andrew W. Moore
#t 2005
#c 19
#% 592062
#! For Hidden Markov Models (HMMs) with fully connected transition models, the three fundamental problems of evaluating the likelihood of an observation sequence, estimating an optimal state sequence for the observations, and learning the model parameters, all have quadratic time complexity in the number of states. We introduce a novel class of non-sparse Markov transition matrices called Dense-Mostly-Constant (DMC) transition matrices that allow us to derive new algorithms for solving the basic HMM problems in sub-quadratic time. We describe the DMC HMM model and algorithms and attempt to convey some intuition for their usage. Empirical results for these algorithms show dramatic speedups for all three problems. In terms of accuracy, the DMC model yields strong results and outperforms the baseline algorithms even in domains known to violate the DMC assumption.

#index 840936
#* New d-separation identification results for learning continuous latent variable models
#@ Ricardo Silva;Richard Scheines
#t 2005
#c 19
#% 32361
#% 297171
#% 722900
#% 788084
#% 1650407
#% 1673043
#! Learning the structure of graphical models is an important task, but one of considerable difficulty when latent variables are involved. Because conditional independences using hidden variables cannot be directly observed, one has to rely on alternative methods to identify the d-separations that define the graphical structure. This paper describes new distribution-free techniques for identifying d-separations in continuous latent variable models when non-linear dependencies are allowed among hidden variables.

#index 840937
#* Identifying useful subgoals in reinforcement learning by local graph partitioning
#@ Özgür Şimşek;Alicia P. Wolfe;Andrew G. Barto
#t 2005
#c 19
#% 124692
#% 270031
#% 286423
#% 313959
#% 443723
#% 458686
#% 464303
#% 464607
#% 464636
#% 706874
#% 711673
#% 729437
#% 770775
#% 770777
#% 1271827
#! We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs---those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek---states that lie between two densely-connected regions of the state space while producing an algorithm with low computational cost.

#index 840938
#* Beyond the point cloud: from transductive to semi-supervised learning
#@ Vikas Sindhwani;Partha Niyogi;Mikhail Belkin
#t 2005
#c 19
#% 429570
#% 466263
#% 715258
#% 746307
#! Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly semi-supervised setting however, a learning machine has access to labeled and unlabeled examples and must make predictions on data points never encountered before. In this paper, we show how to turn transductive and standard supervised learning algorithms into semi-supervised learners. We construct a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS). These norms allow us to warp the structure of the RKHS to reflect the underlying geometry of the data. We derive explicit formulas for the corresponding new kernels. Our approach demonstrates state of the art performance on a variety of classification tasks.

#index 840939
#* Active learning for sampling in time-series experiments with application to gene expression analysis
#@ Rohit Singh;Nathan Palmer;David Gifford;Bonnie Berger;Ziv Bar-Joseph
#t 2005
#c 19
#% 191696
#% 714351
#% 729917
#% 830379
#% 1016178
#% 1673023
#! Many time-series experiments seek to estimate some signal as a continuous function of time. In this paper, we address the sampling problem for such experiments: determining which time-points ought to be sampled in order to minimize the cost of data collection. We restrict our attention to a growing class of experiments which measure multiple signals at each time-point and where raw materials/observations are archived initially, and selectively analyzed later, this analysis being the more expensive step. We present an active learning algorithm for iteratively choosing time-points to sample, using the uncertainty in the quality of the currently estimated time-dependent curve as the objective function. Using simulated data as well as gene expression data, we show that our algorithm performs well, and can significantly reduce experimental cost without loss of information.

#index 840940
#* Compact approximations to Bayesian predictive distributions
#@ Edward Snelson;Zoubin Ghahramani
#t 2005
#c 19
#% 232214
#% 424806
#% 424808
#% 715096
#% 722761
#! We provide a general framework for learning precise, compact, and fast representations of the Bayesian predictive distribution for a model. This framework is based on minimizing the KL divergence between the true predictive density and a suitable compact approximation. We consider various methods for doing this, both sampling based approximations, and deterministic approximations such as expectation propagation. These methods are tested on a mixture of Gaussians model for density estimation and on binary linear classification, with both synthetic data sets for visualization and several real data sets. Our results show significant reductions in prediction time and memory footprint.

#index 840941
#* Large scale genomic sequence SVM classifiers
#@ Sören Sonnenburg;Gunnar Rätsch;Bernhard Schölkopf
#t 2005
#c 19
#% 197394
#% 269217
#% 269218
#% 309208
#% 397654
#% 451933
#% 469390
#% 1860761
#! In genomic sequence analysis tasks like splice site recognition or promoter identification, large amounts of training sequences are available, and indeed needed to achieve sufficiently high classification performances. In this work we study two recently proposed and successfully used kernels, namely the Spectrum kernel and the Weighted Degree kernel (WD). In particular, we suggest several extensions using Suffix Trees and modifications of an SMO-like SVM training algorithm in order to accelerate the training of the SVMs and their evaluation on test sequences. Our simulations show that for the spectrum kernel and WD kernel, large scale SVM training can be accelerated by factors of 20 and 4 times, respectively, while using much less memory (e.g. no kernel caching). The evaluation on new sequences is often several thousand times faster using the new techniques (depending on the number of Support Vectors). Our method allows us to train on sets as large as one million sequences.

#index 840942
#* A theoretical analysis of Model-Based Interval Estimation
#@ Alexander L. Strehl;Michael L. Littman
#t 2005
#c 19
#% 135414
#% 270016
#% 363744
#% 384911
#% 425075
#% 465893
#% 722895
#% 785489
#! Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents the first theoretical analysis of MBIE, proving its efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less "online" cousins from the literature.

#index 840943
#* Explanation-Augmented SVM: an approach to incorporating domain knowledge into SVM learning
#@ Qiang Sun;Gerald DeJong
#t 2005
#c 19
#% 269211
#% 376266
#% 425049
#% 430757
#% 576214
#% 812488
#% 1809459
#% 1810960
#! We introduce a novel approach to incorporating domain knowledge into Support Vector Machines to improve their example efficiency. Domain knowledge is used in an Explanation Based Learning fashion to build justifications or explanations for why the training examples are assigned their given class labels. Explanations bias the large margin classifier through the interaction of training examples and domain knowledge. We develop a new learning algorithm for this Explanation-Augmented SVM (EA-SVM). It naturally extends to imperfect knowledge, a stumbling block to conventional EBL. Experimental results confirm desirable properties predicted by the analysis and demonstrate the approach on three domains.

#index 840944
#* Unifying the error-correcting and output-code AdaBoost within the margin framework
#@ Yijun Sun;Sinisa Todorovic;Jian Li;Dapeng Wu
#t 2005
#c 19
#% 136350
#% 266255
#% 276516
#% 299255
#% 312727
#% 465751
#% 562950
#% 722756
#% 793251
#% 1272365
#! In this paper, we present a new interpretation of AdaBoost.ECC and AdaBoost.OC. We show that AdaBoost.ECC performs stage-wise functional gradient descent on a cost function, defined in the domain of margin values, and that AdaBoost.OC is a shrinkage version of AdaBoost.ECC. These findings strictly explain some properties of the two algorithms. The gradient-minimization formulation of AdaBoost.ECC allows us to derive a new algorithm, referred to as AdaBoost.SECC, by explicitly exploiting shrinkage as regularization in AdaBoost.ECC. Experiments on diverse databases confirm our theoretical findings. Empirical results show that AdaBoost.SECC performs significantly better than AdaBoost.ECC and AdaBoost.OC.

#index 840945
#* Finite time bounds for sampling based fitted value iteration
#@ Csaba Szepesvári;Rémi Munos
#t 2005
#c 19
#% 203598
#% 387653
#% 393786
#% 408435
#% 495927
#% 563266
#% 722806
#% 770867
#! In this paper we consider sampling based fitted value iteration for discounted, large (possibly infinite) state space, finite action Markovian Decision Problems where only a generative model of the transition probabilities and rewards is available. At each step the image of the current estimate of the optimal value function under a Monte-Carlo approximation to the Bellman-operator is projected onto some function space. PAC-style bounds on the weighted Lp-norm approximation error are obtained as a function of the covering number and the approximation power of the function space, the iteration number and the sample size.

#index 840946
#* TD(λ) networks: temporal-difference networks with eligibility traces
#@ Brian Tanner;Richard S. Sutton
#t 2005
#c 19
#% 106674
#% 425076
#% 449561
#% 651665
#% 788097
#% 840958
#% 1289489
#! Temporal-difference (TD) networks have been introduced as a formalism for expressing and learning grounded world knowledge in a predictive form (Sutton & Tanner, 2005). Like conventional TD(0) methods, the learning algorithm for TD networks uses 1-step backups to train prediction units about future events. In conventional TD learning, the TD(λ) algorithm is often used to do more general multi-step backups of future predictions. In our work, we introduce a generalization of the 1-step TD network specification that is based on the TD(λ) learning algorithm, creating TD(λ) networks. We present experimental results that show TD(λ) networks can learn solutions in more complex environments than TD networks. We also show that in problems that can be solved by TD networks, TD(λ) networks generally learn solutions much faster than their 1-step counterparts. Finally, we present an analysis of our algorithm that shows that the computational cost of TD(λ) networks is only slightly more than that of TD networks.

#index 840947
#* Learning structured prediction models: a large margin approach
#@ Ben Taskar;Vassil Chatalbashev;Daphne Koller;Carlos Guestrin
#t 2005
#c 19
#% 443975
#% 729437
#% 757953
#% 770763
#% 770866
#% 827631
#% 830820
#% 854636
#! We consider large margin estimation in a broad range of prediction models where inference involves solving combinatorial optimization problems, for example, weighted graph-cuts or matchings. Our goal is to learn parameters such that inference using the model reproduces correct answers on the training data. Our method relies on the expressive power of convex optimization problems to compactly capture inference or solution optimality in structured prediction models. Directly embedding this structure within the learning formulation produces concise convex problems for efficient estimation of very complex and diverse models. We describe experimental results on a matching task, disulfide connectivity prediction, showing significant improvements over state-of-the-art methods.

#index 840948
#* Learning discontinuities with products-of-sigmoids for switching between local models
#@ Marc Toussaint;Sethu Vijayakumar
#t 2005
#c 19
#% 169358
#% 272374
#% 273319
#% 319464
#% 466428
#% 857094
#% 1396003
#! Sensorimotor data from many interesting physical interactions comprises discontinuities. While existing locally weighted learning approaches aim at learning smooth functions, we propose a model that learns how to switch discontinuously between local models. The local responsibilities, usually represented by Gaussian kernels, are learned by a product of local sigmoidal classifiers that can represent complex shaped and sharply bounded regions. Local models are incrementally added. A locality prior constrains them to learn only local data---which is the key ingredient for incremental learning with local models.

#index 840949
#* Core Vector Regression for very large regression problems
#@ Ivor W. Tsang;James T. Kwok;Kimo T. Lai
#t 2005
#c 19
#% 269217
#% 269218
#% 302406
#% 347211
#% 450263
#% 466597
#% 592108
#% 722757
#% 722758
#% 772202
#% 1861308
#! In this paper, we extend the recently proposed Core Vector Machine algorithm to the regression setting by generalizing the underlying minimum enclosing ball problem. The resultant Core Vector Regression (CVR) algorithm can be used with any linear/nonlinear kernels and can obtain provably approximately optimal solutions. Its asymptotic time complexity is linear in the number of training patterns m, while its space complexity is independent of m. Experiments show that CVR has comparable performance with SVR, but is much faster and produces much fewer support vectors on very large data sets. It is also successfully applied to large 3D point sets in computer graphics for the modeling of implicit surfaces.

#index 840950
#* Propagating distributions on a hypergraph by dual information regularization
#@ Koji Tsuda
#t 2005
#c 19
#% 115608
#% 213009
#% 268069
#% 833012
#! In the information regularization framework by Corduneanu and Jaakkola (2005), the distributions of labels are propagated on a hypergraph for semi-supervised learning. The learning is efficiently done by a Blahut-Arimoto-like two step algorithm, but, unfortunately, one of the steps cannot be solved in a closed form. In this paper, we propose a dual version of information regularization, which is considered as more natural in terms of information geometry. Our learning algorithm has two steps, each of which can be solved in a closed form. Also it can be naturally applied to exponential family distributions such as Gaussians. In experiments, our algorithm is applied to protein classification based on a metabolic network and known functional categories.

#index 840951
#* Hierarchical Dirichlet model for document classification
#@ Sriharsha Veeramachaneni;Diego Sona;Paolo Avesani
#t 2005
#c 19
#% 309141
#% 344447
#% 387427
#% 420528
#% 466078
#% 466501
#% 479817
#% 482113
#% 722904
#% 754076
#% 754105
#% 788066
#% 1289267
#% 1387537
#! The proliferation of text documents on the web as well as within institutions necessitates their convenient organization to enable efficient retrieval of information. Although text corpora are frequently organized into concept hierarchies or taxonomies, the classification of the documents into the hierarchy is expensive in terms human effort. We present a novel and simple hierarchical Dirichlet generative model for text corpora and derive an efficient algorithm for the estimation of model parameters and the unsupervised classification of text documents into a given hierarchy. The class conditional feature means are assumed to be inter-related due to the hierarchical Bayesian structure of the model. We show that the algorithm provides robust estimates of the classification parameters by performing smoothing or regularization. We present experimental evidence on real web data that our algorithm achieves significant gains in accuracy over simpler models.

#index 840952
#* Implicit surface modelling as an eigenvalue problem
#@ Christian Walder;Olivier Chapelle;Bernhard Schölkopf
#t 2005
#c 19
#! We discuss the problem of fitting an implicit shape model to a set of points sampled from a co-dimension one manifold of arbitrary topology. The method solves a non-convex optimisation problem in the embedding function that defines the implicit by way of its zero level set. By assuming that the solution is a mixture of radial basis functions of varying widths we attain the globally optimal solution by way of an equivalent eigenvalue problem, without using or constructing as an intermediate step the normal vectors of the manifold at each data point. We demonstrate the system on two and three dimensional data, with examples of missing data interpolation and set operations on the resultant shapes.

#index 840953
#* New kernels for protein structural motif discovery and function classification
#@ Chang Wang;Stephen D. Scott
#t 2005
#c 19
#% 269217
#% 832639
#! We present new, general-purpose kernels for protein structure analysis, and describe how to apply them to structural motif discovery and function classification. Experiments show that our new methods are faster than conventional techniques, are capable of finding structural motifs, and are very effective in function classification. In addition to strong cross-validation results, we found possible new oxidoreductases and cytochrome P450 reductases and a possible new structural motif in cytochrome P450 reductases.

#index 840954
#* Exploiting syntactic, semantic and lexical regularities in language modeling via directed Markov random fields
#@ Shaojun Wang;Shaomin Wang;Russell Greiner;Dale Schuurmans;Li Cheng
#t 2005
#c 19
#% 211044
#% 252472
#% 329569
#% 722904
#% 741046
#% 741360
#% 823319
#! We present a directed Markov random field (MRF) model that combines n-gram models, probabilistic context free grammars (PCFGs) and probabilistic latent semantic analysis (PLSA) for the purpose of statistical language modeling. Even though the composite directed MRF model potentially has an exponential number of loops and becomes a context sensitive grammar, we are nevertheless able to estimate its parameters in cubic time using an efficient modified EM method, the generalized inside-outside algorithm, which extends the inside-outside algorithm to incorporate the effects of the n-gram and PLSA language models. We generalize various smoothing techniques to alleviate the sparseness of n-gram counts in cases where there are hidden variables. We also derive an analogous algorithm to calculate the probability of initial subsequence of a sentence, generated by the composite language model. Our experimental results on the Wall Street Journal corpus show that we obtain significant reductions in perplexity compared to the state-of-the-art baseline trigram model with Good-Turing and Kneser-Ney smoothings.

#index 840955
#* Bayesian sparse sampling for on-line reward optimization
#@ Tao Wang;Daniel Lizotte;Michael Bowling;Dale Schuurmans
#t 2005
#c 19
#% 170386
#% 277396
#% 277516
#% 310835
#% 360691
#% 361729
#% 384911
#% 393786
#% 464282
#% 466075
#% 466731
#% 527859
#% 715337
#% 722923
#% 1271956
#% 1273918
#% 1289278
#% 1650283
#! We present an efficient "sparse sampling" technique for approximating Bayes optimal decision making in reinforcement learning, addressing the well known exploration versus exploitation tradeoff. Our approach combines sparse sampling with Bayesian exploration to achieve improved decision making while controlling computational cost. The idea is to grow a sparse lookahead tree, intelligently, by exploiting information in a Bayesian posterior---rather than enumerate action branches (standard sparse sampling) or compensate myopically (value of perfect information). The outcome is a flexible, practical technique for improving action selection in simple reinforcement learning scenarios.

#index 840956
#* Learning predictive representations from a history
#@ Eric Wiewiora
#t 2005
#c 19
#% 272652
#% 286423
#% 651665
#% 770781
#% 770863
#% 788097
#% 857087
#! Predictive State Representations (PSRs) have shown a great deal of promise as an alternative to Markov models. However, learning a PSR from a single stream of data generated from an environment remains a challenge. In this work, we present a formalism of PSRs and the domains they model. This formalization suggests an algorithm for learning PSRs that will (almost surely) converge to a globally optimal model given sufficient training data.

#index 840957
#* Incomplete-data classification using logistic regression
#@ David Williams;Xuejun Liao;Ya Xue;Lawrence Carin
#t 2005
#c 19
#% 492792
#% 723238
#% 724154
#% 729437
#! A logistic regression classification algorithm is developed for problems in which the feature vectors may be missing data (features). Single or multiple imputation for the missing data is avoided by performing analytic integration with an estimated conditional density function (conditioned on the non-missing data). Conditional density functions are estimated using a Gaussian mixture model (GMM), with parameter estimation performed using both expectation maximization (EM) and Variational Bayesian EM (VB-EM). Using widely available real data, we demonstrate the general advantage of the VB-EM GMM estimation for handling incomplete data, vis-à-vis the EM algorithm. Moreover, it is demonstrated that the approach proposed here is generally superior to standard imputation procedures.

#index 840958
#* Learning predictive state representations in dynamical systems without reset
#@ Britton Wolfe;Michael R. James;Satinder Singh
#t 2005
#c 19
#% 203602
#% 651665
#% 770781
#% 770863
#% 788097
#! Predictive state representations (PSRs) are a recently-developed way to model discrete-time, controlled dynamical systems. We present and describe two algorithms for learning a PSR model: a Monte Carlo algorithm and a temporal difference (TD) algorithm. Both of these algorithms can learn models for systems without requiring a reset action as was needed by the previously available general PSR-model learning algorithm. We present empirical results that compare our two algorithms and also compare their performance with that of existing algorithms, including an EM algorithm for learning POMDP models.

#index 840959
#* Linear Asymmetric Classifier for cascade detectors
#@ Jianxin Wu;Matthew D. Mullin;James M. Rehg
#t 2005
#c 19
#% 247889
#% 424081
#% 466561
#% 592267
#% 736300
#% 765520
#% 1271973
#% 1502470
#! The detection of faces in images is fundamentally a rare event detection problem. Cascade classifiers provide an efficient computational solution, by leveraging the asymmetry in the distribution of faces vs. non-faces. Training a cascade classifier in turn requires a solution for the following subproblems: Design a classifier for each node in the cascade with very high detection rate but only moderate false positive rate. While there are a few strategies in the literature for indirectly addressing this asymmetric node learning goal, none of them are based on a satisfactory theoretical framework. We present a mathematical characterization of the node-learning problem and describe an effective closed form approximation to the optimal solution, which we call the Linear Asymmetric Classifier (LAC). We first use AdaBoost or AsymBoost to select features, and use LAC to learn a linear discriminant function to achieve the node learning goal. Experimental results on face detection show that LAC can improve the detection performance in comparison to standard methods. We also show that Fisher Discriminant Analysis on the features selected by AdaBoost yields better performance than AdaBoost itself.

#index 840960
#* Building Sparse Large Margin Classifiers
#@ Mingrui Wu;Bernhard Schölkopf;Gökhan Bakir
#t 2005
#c 19
#% 73441
#% 190581
#% 269225
#% 361100
#% 425040
#% 722760
#% 722918
#% 734919
#% 1861262
#! This paper presents an approach to build Sparse Large Margin Classifiers (SLMC) by adding one more constraint to the standard Support Vector Machine (SVM) training problem. The added constraint explicitly controls the sparseness of the classifier and an approach is provided to solve the formulated problem. When considering the dual of this problem. it can be seen that building an SLMC is equivalent to constructing an SVM with a modified kernel function. Further analysis of this kernel function indicates that the proposed approach essentially finds a discriminating subspace that can be spanned by a small number of vectors, and in this subspace different classes of data are linearly well separated. Experimental results over several classification benchmarks show that in most cases the proposed approach outperforms the state-of-art sparse learning algorithms.

#index 840961
#* Dirichlet enhanced relational learning
#@ Zhao Xu;Volker Tresp;Kai Yu;Shipeng Yu;Hans-Peter Kriegel
#t 2005
#c 19
#% 234797
#% 252472
#% 392781
#% 398840
#% 496116
#% 722914
#% 731606
#% 766451
#% 1705028
#! We apply nonparametric hierarchical Bayesian modelling to relational learning. In a hierarchical Bayesian approach, model parameters can be "personalized", i.e., owned by entities or relationships, and are coupled via a common prior distribution. Flexibility is added in a nonparametric hierarchical Bayesian approach, such that the learned knowledge can be truthfully represented. We apply our approach to a medical domain where we form a nonparametric hierarchical Bayesian model for relations involving hospitals, patients, procedures and diagnosis. The experiments show that the additional flexibility in a nonparametric hierarchical Bayes approach results in a more accurate model of the dependencies between procedures and diagnosis and gives significantly improved estimates of the probabilities of future procedures.

#index 840962
#* Learning Gaussian processes from multiple tasks
#@ Kai Yu;Volker Tresp;Anton Schwaighofer
#t 2005
#c 19
#% 236497
#% 277516
#% 420507
#% 458379
#% 722904
#% 723239
#% 763708
#% 769886
#% 770804
#! We consider the problem of multi-task learning, that is, learning multiple related functions. Our approach is based on a hierarchical Bayesian framework, that exploits the equivalence between parametric linear models and nonparametric Gaussian processes (GPs). The resulting models can be learned easily via an EM-algorithm. Empirical studies on multi-label text categorization suggest that the presented models allow accurate solutions of these multi-task problems.

#index 840963
#* Augmenting naive Bayes for ranking
#@ Harry Zhang;Liangxiao Jiang;Jiang Su
#t 2005
#c 19
#% 136350
#% 246831
#% 246832
#% 290482
#% 321055
#% 342611
#% 349550
#% 466086
#% 577298
#% 578681
#% 580510
#% 770761
#% 799040
#% 1378224
#! Naive Bayes is an effective and efficient learning algorithm in classification. In many applications, however, an accurate ranking of instances based on the class probability is more desirable. Unfortunately, naive Bayes has been found to produce poor probability estimates. Numerous techniques have been proposed to extend naive Bayes for better classification accuracy, of which selective Bayesian classifiers (SBC) (Langley & Sage, 1994), tree-augmented naive Bayes (TAN) (Friedman et al., 1997), NBTree (Kohavi, 1996), boosted naive Bayes (Elkan, 1997), and AODE (Webb et al., 2005) achieve remarkable improvement over naive Bayes in terms of classification accuracy. An interesting question is: Do these techniques also produce accurate ranking? In this paper, we first conduct a systematic experimental study on their efficacy for ranking. Then, we propose a new approach to augmenting naive Bayes for generating accurate ranking, called hidden naive Bayes (HNB). In an HNB, a hidden parent is created for each attribute to represent the influences from all other attributes, and thus a more accurate ranking is expected. HNB inherits the structural simplicity of naive Bayes and can be easily learned without structure learning. Our experiments show that HNB outperforms naive Bayes, SBC, boosted naive Bayes, NBTree, and TAN significantly, and performs slightly better than AODE in ranking.

#index 840964
#* A new Mallows distance based metric for comparing clusterings
#@ Ding Zhou;Jia Li;Hongyuan Zha
#t 2005
#c 19
#% 54215
#% 650937
#% 718437
#! Despite of the large number of algorithms developed for clustering, the study on comparing clustering results is limited. In this paper, we propose a measure for comparing clustering results to tackle two issues insufficiently addressed or even overlooked by existing methods: (a) taking into account the distance between cluster representatives when assessing the similarity of clustering results; (b) constructing a unified framework for defining a distance based on either hard or soft clustering and ensuring the triangle inequality under the definition. Our measure is derived from a complete and globally optimal matching between clusters in two clustering results. It is shown that the distance is an instance of the Mallows distance---a metric between probability distributions in statistics. As a result, the defined distance inherits desirable properties from the Mallows distance. Experiments show that our clustering distance measure successfully handles cases difficult for other measures.

#index 840965
#* Learning from labeled and unlabeled data on a directed graph
#@ Dengyong Zhou;Jiayuan Huang;Bernhard Schölkopf
#t 2005
#c 19
#% 290830
#% 313959
#% 433902
#% 722914
#% 723885
#! We propose a general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph including the directionality of the edges is considered. The time complexity of the algorithm derived from this framework is nearly linear due to recently developed numerical techniques. In the absence of labeled instances, this framework can be utilized as a spectral clustering method for directed graphs, which generalizes the spectral clustering approach for undirected graphs. We have applied our framework to real-world web classification problems and obtained encouraging results.

#index 840966
#* 2D Conditional Random Fields for Web information extraction
#@ Jun Zhu;Zaiqing Nie;Ji-Rong Wen;Bo Zhang;Wei-Ying Ma
#t 2005
#c 19
#% 73441
#% 211044
#% 338741
#% 464434
#% 466892
#% 724344
#% 729978
#% 766464
#% 770844
#% 805846
#% 805896
#% 815924
#% 816181
#% 854636
#% 854813
#% 938708
#% 1650403
#% 1759438
#% 1810385
#! The Web contains an abundance of useful semistructured information about real world objects, and our empirical study shows that strong sequence characteristics exist for Web information about objects of the same type across different Web sites. Conditional Random Fields (CRFs) are the state of the art approaches taking the sequence characteristics to do better labeling. However, as the information on a Web page is two-dimensionally laid out, previous linear-chain CRFs have their limitations for Web information extraction. To better incorporate the two-dimensional neighborhood interactions, this paper presents a two-dimensional CRF model to automatically extract object information from the Web. We empirically compare the proposed model with existing linear-chain CRF models for product information extraction, and the results show the effectiveness of our model.

#index 840967
#* Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning
#@ Xiaojin Zhu;John Lafferty
#t 2005
#c 19
#% 203341
#% 311027
#% 565545
#% 732552
#% 770767
#% 1808946
#! Graph-based methods for semi-supervised learning have recently been shown to be promising for combining labeled and unlabeled data in classification problems. However, inference for graph-based methods often does not scale well to very large data sets, since it requires inversion of a large matrix or solution of a large linear program. Moreover, such approaches are inherently transductive, giving predictions for only those points in the unlabeled set, and not for an arbitrary test point. In this paper a new approach is presented that preserves the strengths of graph-based semi-supervised learning while overcoming the limitations of scalability and non-inductive inference, through a combination of generative mixture models and discriminative regularization using the graph Laplacian. Experimental results show that this approach preserves the accuracy of purely graph-based transductive methods when the data has "manifold structure," and at the same time achieves inductive learning with significantly reduced computational cost.

#index 840968
#* Large margin non-linear embedding
#@ Alexander Zien;Joaquin Quiñonero Candela
#t 2005
#c 19
#% 60576
#% 361100
#% 466263
#% 765552
#! It is common in classification methods to first place data in a vector space and then learn decision boundaries. We propose reversing that process: for fixed decision boundaries, we "learn" the location of the data. This way we (i) do not need a metric (or even stronger structure) - pairwise dissimilarities suffice; and additionally (ii) produce low-dimensional embeddings that can be analyzed visually. We achieve this by combining an entropy-based embedding method with an entropy-based version of semi-supervised logistic regression. We present results for clustering and semi-supervised classification.

#index 875944
#* Proceedings of the 23rd international conference on Machine learning
#@ William Cohen;Andrew Moore
#t 2006
#c 19
#! This volume, which is also available from http://www.machinelearning.org, the home page of the International Machine Learning Society, contains the technical papers accepted for presentation at ICML-2006, the 23rd International Conference on Machine Learning. ICML is an international forum for presentation and discussion of the latest results in the field of machine learning. This year, ICML was held at Carnegie Mellon University, in Pittsburgh, Pennsylvania, and was co-located with COLT-2006, the 19th Annual Conference on Computational Learning Theory.Coincidentally, Carnegie Mellon University was also the venue for the first ICML---the First Machine Learning Workshop, which was held in 1980. Instead of proceedings, a book was published (Machine Learning: an Artificial Intelligence Approach, ed. Michalski, Carbonell, and Mitchell, Morgan Kaufman, 1983) containing sixteen research papers, and also a "comprehensive bibliography" of the field of machine learning, as it stood in 1983. This bibliography contained 572 entries.In 2006, no less than 548 papers were submitted to ICML---nearly as many as were in the "comprehensive bibliography" published with the papers from the first ICML. These papers were subjected to a thorough review process. In the first round of reviewing, every paper received three reviews by program committee members. Authors were then given an opportunity to view the first-round reviews and respond to them. Led by a Senior Program Committee member, the reviewers then engaged in a discussion of the paper, leading finally to a decision by the Senior Program Committee member in charge of the paper. Papers could be accepted, rejected, or conditionally accepted; the 36 conditionally accepted papers were subject to an additional final round of review by the Senior Program Committee. Of the 548 submissions, 140 were accepted for publication, an acceptance rate of 25.5%.In addition to the technical talks, ICML-2006 also included seven tutorials and eleven workshops, which were held before and after the conference, respectively. Authors presented their papers both orally and in a poster session, allowing time for detailed discussions with any interested attendees of the conference. Each day of the main conference included an invited talk by a prominent researcher. We were very fortunate to be able to host David Haussler, of the University of California at Santa Cruz; Robert Schapire, of Princeton University; and Mandyam V. Srinivasan, of the Australian National University.

#index 875945
#* Using inaccurate models in reinforcement learning
#@ Pieter Abbeel;Morgan Quigley;Andrew Y. Ng
#t 2006
#c 19
#% 69418
#% 203450
#% 229940
#% 351418
#% 384911
#% 393786
#% 465902
#% 466723
#% 875945
#% 1250215
#! In the model-based policy search approach to reinforcement learning (RL), policies are found using a model (or "simulator") of the Markov decision process. However, for high-dimensional continuous-state tasks, it can be extremely difficult to build an accurate model, and thus often the algorithm returns a policy that works in simulation but not in real-life. The other extreme, model-free RL, tends to require infeasibly large numbers of real-life trials. In this paper, we present a hybrid algorithm that requires only an approximate model, and only a small number of real-life trials. The key idea is to successively "ground" the policy evaluations using real-life trials, but to rely on the approximate model to suggest local changes. Our theoretical results show that this algorithm achieves near-optimal performance in the real system, even when the model is only approximate. Empirical results also demonstrate that---when given only a crude model and a small number of real-life trials---our algorithm can obtain near-optimal performance in the real system.

#index 875946
#* Algorithms for portfolio management based on the Newton method
#@ Amit Agarwal;Elad Hazan;Satyen Kale;Robert E. Schapire
#t 2006
#c 19
#% 10463
#% 45843
#% 116178
#% 214399
#% 284715
#% 722907
#% 723920
#% 810933
#% 850011
#% 1272037
#% 1674795
#! We experimentally study on-line investment algorithms first proposed by Agarwal and Hazan and extended by Hazan et al. which achieve almost the same wealth as the best constant-rebalanced portfolio determined in hindsight. These algorithms are the first to combine optimal logarithmic regret bounds with efficient deterministic computability. They are based on the Newton method for offline optimization which, unlike previous approaches, exploits second order information. After analyzing the algorithm using the potential function introduced by Agarwal and Hazan, we present extensive experiments on actual financial data. These experiments confirm the theoretical advantage of our algorithms, which yield higher returns and run considerably faster than previous algorithms with optimal regret. Additionally, we perform financial analysis using mean-variance calculations and the Sharpe ratio.

#index 875947
#* Higher order learning with graphs
#@ Sameer Agarwal;Kristin Branson;Serge Belongie
#t 2006
#c 19
#% 138468
#% 213687
#% 278658
#% 313959
#% 479659
#% 765552
#% 812453
#% 812579
#% 840934
#% 1667706
#% 1676331
#% 1838081
#! Recently there has been considerable interest in learning with higher order relations (i.e., three-way or higher) in the unsupervised and semi-supervised settings. Hypergraphs and tensors have been proposed as the natural way of representing these relations and their corresponding algebra as the natural tools for operating on them. In this paper we argue that hypergraphs are not a natural representation for higher order relations, indeed pairwise as well as higher order relations can be handled using graphs. We show that various formulations of the semi-supervised and the unsupervised learning problem on hypergraphs result in the same graph theoretic problem and can be analyzed using existing tools.

#index 875948
#* Ranking on graph data
#@ Shivani Agarwal
#t 2006
#c 19
#% 190581
#% 269217
#% 269218
#% 420077
#% 722805
#% 734915
#% 757953
#% 765552
#% 840938
#% 840965
#% 1272396
#% 1705503
#! In ranking, one is given examples of order relationships among objects, and the goal is to learn from these examples a real-valued ranking function that induces a ranking or ordering over the object space. We consider the problem of learning such a ranking function when the data is represented as a graph, in which vertices correspond to objects and edges encode similarities between objects. Building on recent developments in regularization theory for graphs and corresponding Laplacian-based methods for classification, we develop an algorithmic framework for learning ranking functions on graph data. We provide generalization guarantees for our algorithms via recent results based on the notion of algorithmic stability, and give experimental evidence of the potential benefits of our framework.

#index 875949
#* Robust probabilistic projections
#@ Cédric Archambeau;Nicolas Delannay;Michel Verleysen
#t 2006
#c 19
#% 272536
#% 277483
#% 278040
#% 424831
#% 457911
#% 1862524
#! Principal components and canonical correlations are at the root of many exploratory data mining techniques and provide standard pre-processing tools in machine learning. Lately, probabilistic reformulations of these methods have been proposed (Roweis, 1998; Tipping & Bishop, 1999b; Bach & Jordan, 2005). They are based on a Gaussian density model and are therefore, like their non-probabilistic counterpart, very sensitive to atypical observations. In this paper, we introduce robust probabilistic principal component analysis and robust probabilistic canonical correlation analysis. Both are based on a Student-t density model. The resulting probabilistic reformulations are more suitable in practice as they handle outliers in a natural way. We compute maximum likelihood estimates of the parameters by means of the EM algorithm.

#index 875950
#* A DC-programming algorithm for kernel selection
#@ Andreas Argyriou;Raphael Hauser;Charles A. Micchelli;Massimiliano Pontil
#t 2006
#c 19
#% 299012
#% 425040
#% 743284
#% 763697
#% 770846
#% 891549
#% 1705523
#! We address the problem of learning a kernel for a given supervised learning task. Our approach consists in searching within the convex hull of a prescribed set of basic kernels for one which minimizes a convex regularization functional. A unique feature of this approach compared to others in the literature is that the number of basic kernels can be infinite. We only require that they are continuously parameterized. For example, the basic kernels could be isotropic Gaussians with variance in a prescribed interval or even Gaussians parameterized by multiple continuous parameters. Our work builds upon a formulation involving a minimax optimization problem and a recently proposed greedy algorithm for learning the kernel. Although this optimization problem is not convex, it belongs to the larger class of DC (difference of convex functions) programs. Therefore, we apply recent results from DC optimization theory to create a new algorithm for learning the kernel. Our experimental results on benchmark data sets show that this algorithm outperforms a previously proposed method.

#index 875951
#* Relational temporal difference learning
#@ Nima Asgharbeygi;David Stracuzzi;Pat Langley
#t 2006
#c 19
#% 126860
#% 169359
#% 280409
#% 304312
#% 333786
#% 384911
#% 465917
#% 1271827
#% 1272286
#% 1279355
#% 1289241
#% 1718449
#! We introduce relational temporal difference learning as an effective approach to solving multi-agent Markov decision problems with large state spaces. Our algorithm uses temporal difference reinforcement to learn a distributed value function represented over a conceptual hierarchy of relational predicates. We present experiments using two domains from the General Game Playing repository, in which we observe that our system achieves higher learning rates than non-relational methods. We also discuss related work and directions for future research.

#index 875952
#* A new approach to data driven clustering
#@ Arik Azran;Zoubin Ghahramani
#t 2006
#c 19
#% 296738
#% 313959
#% 594009
#% 765261
#! We consider the problem of clustering in its most basic form where only a local metric on the data space is given. No parametric statistical model is assumed, and the number of clusters is learned from the data. We introduce, analyze and demonstrate a novel approach to clustering where data points are viewed as nodes of a graph, and pairwise similarities are used to derive a transition probability matrix P for a Markov random walk between them. The algorithm automatically reveals structure at increasing scales by varying the number of steps taken by this random walk. Points are represented as rows of Pt, which are the t-step distributions of the walk starting at that point; these distributions are then clustered using a KL-minimizing iterative algorithm. Both the number of clusters, and the number of steps that 'best reveal' it, are found by optimizing spectral properties of P.

#index 875953
#* Agnostic active learning
#@ Maria-Florina Balcan;Alina Beygelzimer;John Langford
#t 2006
#c 19
#% 26125
#% 61140
#% 170649
#% 243163
#% 387653
#% 450951
#% 451056
#% 476744
#% 543755
#% 722888
#% 1661927
#% 1705517
#! We state and analyze the first active learning algorithm which works in the presence of arbitrary forms of noise. The algorithm, A2 (for Agnostic Active), relies only upon the assumption that the samples are drawn i.i.d. from a fixed distribution. We show that A2 achieves an exponential improvement (i.e., requires only O (ln 1/ε) samples to find an ε-optimal classifier) over the usual sample complexity of supervised learning, for several settings considered before in the realizable case. These include learning threshold classifiers and learning homogeneous linear separators with respect to an input distribution which is uniform over the unit sphere.

#index 875954
#* On a theory of learning with similarity functions
#@ Maria-Florina Balcan;Avrim Blum
#t 2006
#c 19
#% 73372
#% 82156
#% 190581
#% 197394
#% 302390
#% 387653
#% 390723
#% 402289
#% 743284
#% 763697
#% 835998
#% 836495
#% 1860761
#! Kernel functions have become an extremely popular tool in machine learning, with an attractive theory as well. This theory views a kernel as implicitly mapping data points into a possibly very high dimensional space, and describes a kernel function as being good for a given learning problem if data is separable by a large margin in that implicit space. However, while quite elegant, this theory does not directly correspond to one's intuition of a good kernel as a good similarity function. Furthermore, it may be difficult for a domain expert to use the theory to help design an appropriate kernel for the learning task at hand since the implicit mapping may not be easy to calculate. Finally, the requirement of positive semi-definiteness may rule out the most natural pairwise similarity functions for the given problem domain.In this work we develop an alternative, more general theory of learning with similarity functions (i.e., sufficient conditions for a similarity function to allow one to learn well) that does not require reference to implicit spaces, and does not require the function to be positive semi-definite (or even symmetric). Our results also generalize the standard theory in the sense that any good kernel function under the usual definition can be shown to also be a good similarity function under our definition (though with some loss in the parameters). In this way, we provide the first steps towards a theory of kernels that describes the effectiveness of a given kernel function in terms of natural similarity-based properties.

#index 875955
#* On Bayesian bounds
#@ Arindam Banerjee
#t 2006
#c 19
#% 115608
#% 165663
#% 187104
#% 203297
#% 227736
#% 232319
#% 232728
#% 235377
#% 276511
#% 302390
#% 431293
#% 722896
#% 1809072
#! We show that several important Bayesian bounds studied in machine learning, both in the batch as well as the online setting, arise by an application of a simple compression lemma. In particular, we derive (i) PAC-Bayesian bounds in the batch setting, (ii) Bayesian log-loss bounds and (iii) Bayesian bounded-loss bounds in the online setting using the compression lemma. Although every setting has different semantics for prior, posterior and loss, we show that the core bound argument is the same. The paper simplifies our understanding of several important and apparently disparate results, as well as brings to light a powerful tool for developing similar arguments for other methods.

#index 875956
#* Convex optimization techniques for fitting sparse Gaussian graphical models
#@ Onureena Banerjee;Laurent El Ghaoui;Alexandre d'Aspremont;Georges Natsoulis
#t 2006
#c 19
#% 131165
#% 263310
#% 771626
#% 803567
#% 1300083
#! We consider the problem of fitting a large-scale covariance matrix to multivariate Gaussian data in such a way that the inverse is sparse, thus providing model selection. Beginning with a dense empirical covariance matrix, we solve a maximum likelihood problem with an l1-norm penalty term added to encourage sparsity in the inverse. For models with tens of nodes, the resulting problem can be solved using standard interior-point algorithms for convex optimization, but these methods scale poorly with problem size. We present two new algorithms aimed at solving problems with a thousand nodes. The first, based on Nesterov's first-order algorithm, yields a rigorous complexity estimate for the problem, with a much better dependence on problem size than interior-point methods. Our second algorithm uses block coordinate descent, updating row/columns of the covariance matrix sequentially. Experiments with genomic data show that our method is able to uncover biologically interpretable connections among genes.

#index 875957
#* Cover trees for nearest neighbor
#@ Alina Beygelzimer;Sham Kakade;John Langford
#t 2006
#c 19
#% 317313
#% 347264
#% 723894
#% 749529
#% 858170
#% 875957
#! We present a tree data structure for fast nearest neighbor operations in general n-point metric spaces (where the data set consists of n points). The data structure requires O(n) space regardless of the metric's structure yet maintains all performance properties of a navigating net (Krauthgamer & Lee, 2004b). If the point set has a bounded expansion constant c, which is a measure of the intrinsic dimensionality, as defined in (Karger & Ruhl, 2002), the cover tree data structure can be constructed in O (c6n log n) time. Furthermore, nearest neighbor queries require time only logarithmic in n, in particular O (c12 log n) time. Our experimental results show speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets.

#index 875958
#* Graph model selection using maximum likelihood
#@ Ivona Bezáková;Adam Kalai;Rahul Santhanam
#t 2006
#c 19
#% 300078
#% 300079
#% 344037
#% 344708
#% 446426
#% 720278
#% 723920
#% 969400
#! In recent years, there has been a proliferation of theoretical graph models, e.g., preferential attachment and small-world models, motivated by real-world graphs such as the Internet topology. To address the natural question of which model is best for a particular data set, we propose a model selection criterion for graph models. Since each model is in fact a probability distribution over graphs, we suggest using Maximum Likelihood to compare graph models and select their parameters. Interestingly, for the case of graph models, computing likelihoods is a difficult algorithmic task. However, we design and implement MCMC algorithms for computing the maximum likelihood for four popular models: a power-law random graph model, a preferential attachment model, a small-world model, and a uniform random graph model. We hope that this novel use of ML will objectify comparisons between graph models.

#index 875959
#* Dynamic topic models
#@ David M. Blei;John D. Lafferty
#t 2006
#c 19
#% 235061
#% 722904
#% 788043
#% 788094
#% 836717
#! A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.

#index 875960
#* Predictive search distributions
#@ Edwin V. Bonilla;Christopher K. I. Williams;Felix V. Agakov;John Cavazos;John Thomson;Michael F. P. O'Boyle
#t 2006
#c 19
#% 191680
#% 236497
#% 392766
#% 437616
#% 464434
#% 809828
#! Estimation of Distribution Algorithms (EDAs) are a popular approach to learn a probability distribution over the "good" solutions to a combinatorial optimization problem. Here we consider the case where there is a collection of such optimization problems with learned distributions, and where each problem can be characterized by some vector of features. Now we can define a machine learning problem to predict the distribution of good solutions q(s|x) for a new problem with features x, where s denotes a solution. This predictive distribution is then used to focus the search. We demonstrate the utility of our method on a compiler optimization task where the goal is to find a sequence of code transformations to make the code run fastest. Results on a set of 12 different benchmarks on two distinct architectures show that our approach consistently leads to significant improvements in performance.

#index 875961
#* Learning predictive state representations using non-blind policies
#@ Michael Bowling;Peter McCracken;Michael James;James Neufeld;Dana Wilkinson
#t 2006
#c 19
#% 770781
#% 770863
#% 788097
#% 840956
#% 840958
#! Predictive state representations (PSRs) are powerful models of non-Markovian decision processes that differ from traditional models (e.g., HMMs, POMDPs) by representing state using only observable quantities. Because of this, PSRs can be learned solely using data from interaction with the process. The majority of existing techniques, though, explicitly or implicitly require that this data be gathered using a blind policy, where actions are selected independently of preceding observations. This is a severe limitation for practical learning of PSRs. We present two methods for fixing this limitation in most of the existing PSR algorithms: one when the policy is known and one when it is not. We then present an efficient optimization for computing good exploration policies to be used when learning a PSR. The exploration policies, which are not blind, significantly lower the amount of data needed to build an accurate model, thus demonstrating the importance of non-blind policies.

#index 875962
#* Efficient co-regularised least squares regression
#@ Ulf Brefeld;Thomas Gärtner;Tobias Scheffer;Stefan Wrobel
#t 2006
#c 19
#% 252011
#% 316509
#% 466081
#% 770772
#% 785334
#% 961134
#% 1289496
#% 1705507
#! In many applications, unlabelled examples are inexpensive and easy to obtain. Semi-supervised approaches try to utilise such examples to reduce the predictive error. In this paper, we investigate a semi-supervised least squares regression algorithm based on the co-learning approach. Similar to other semi-supervised algorithms, our base algorithm has cubic runtime complexity in the number of unlabelled examples. To be able to handle larger sets of unlabelled examples, we devise a semi-parametric variant that scales linearly in the number of unlabelled examples. Experiments show a significant error reduction by co-regularisation and a large runtime improvement for the semi-parametric approximation. Last but not least, we propose a distributed procedure that can be applied without collecting all data at a single site.

#index 875963
#* Semi-supervised learning for structured output variables
#@ Ulf Brefeld;Tobias Scheffer
#t 2006
#c 19
#% 252011
#% 279755
#% 316509
#% 466263
#% 722816
#% 741115
#% 770759
#% 840882
#% 1699580
#! The problem of learning a mapping between input and structured, interdependent output variables covers sequential, spatial, and relational learning as well as predicting recursive structures. Joint feature representations of the input and output variables have paved the way to leveraging discriminative learners such as SVMs to this class of problems. We address the problem of semi-supervised learning in joint input output spaces. The co-training approach is based on the principle of maximizing the consensus among multiple independent hypotheses; we develop this principle into a semi-supervised support vector learning algorithm for joint input output spaces and arbitrary loss functions. Experiments investigate the benefit of semi-supervised structured models in terms of accuracy and F1 score.

#index 875964
#* Fast nonparametric clustering with Gaussian blurring mean-shift
#@ Miguel Á. Carreira-Perpiñán
#t 2006
#c 19
#% 3084
#% 70370
#% 91780
#% 304932
#% 313959
#% 318133
#% 349208
#% 443894
#% 726727
#% 975136
#% 1393416
#! We revisit Gaussian blurring mean-shift (GBMS), a procedure that iteratively sharpens a dataset by moving each data point according to the Gaussian mean-shift algorithm (GMS). (1) We give a criterion to stop the procedure as soon as clustering structure has arisen and show that this reliably produces image segmentations as good as those of GMS but much faster. (2) We prove that GBMS has convergence of cubic order with Gaussian clusters (much faster than GMS's, which is of linear order) and that the local principal component converges last, which explains the powerful clustering and denoising properties of GBMS. (3) We show a connection with spectral clustering that suggests GBMS is much faster. (4) We further accelerate GBMS by interleaving connected-components and blurring steps, achieving 2x--4x speedups without introducing an approximation error. In summary, our accelerated GBMS is a simple, fast, nonparametric algorithm that achieves segmentations of state-of-the-art quality.

#index 875965
#* An empirical comparison of supervised learning algorithms
#@ Rich Caruana;Alexandru Niculescu-Mizil
#t 2006
#c 19
#% 190581
#% 209021
#% 314784
#% 400847
#% 424997
#% 464280
#% 577298
#% 580510
#% 723244
#% 769882
#% 840913
#% 926881
#! A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the effect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods.

#index 875966
#* Robust Euclidean embedding
#@ Lawrence Cayton;Sanjoy Dasgupta
#t 2006
#c 19
#% 80432
#% 443975
#! We derive a robust Euclidean embedding procedure based on semidefinite programming that may be used in place of the popular classical multidimensional scaling (cMDS) algorithm. We motivate this algorithm by arguing that cMDS is not particularly robust and has several other deficiencies. General-purpose semidefinite programming solvers are too memory intensive for medium to large sized applications, so we also describe a fast subgradient-based implementation of the robust algorithm. Additionally, since cMDS is often used for dimensionality reduction, we provide an in-depth look at reducing dimensionality with embedding procedures. In particular, we show that it is NP-hard to find optimal low-dimensional embeddings under a variety of cost functions.

#index 875967
#* Hierarchical classification: combining Bayes with SVM
#@ Nicolò Cesa-Bianchi;Claudio Gentile;Luca Zaniboni
#t 2006
#c 19
#% 197394
#% 309141
#% 309208
#% 420528
#% 465747
#% 466078
#% 466501
#% 770796
#% 840913
#% 840928
#% 961135
#% 1700085
#! We study hierarchical classification in the general case when an instance could belong to more than one class node in the underlying taxonomy. Experiments done in previous work showed that a simple hierarchy of Support Vectors Machines (SVM) with a top-down evaluation scheme has a surprisingly good performance on this kind of task. In this paper, we introduce a refined evaluation scheme which turns the hierarchical SVM classifier into an approximator of the Bayes optimal classifier with respect to a simple stochastic model for the labels. Experiments on synthetic datasets, generated according to this stochastic model, show that our refined algorithm outperforms the simple hierarchical SVM. On real-world data, however, the advantage brought by our approach is a bit less clear. We conjecture this is due to a higher noise rate for the training labels in the low levels of the taxonomy.

#index 875968
#* A continuation method for semi-supervised SVMs
#@ Olivier Chapelle;Mingmin Chi;Alexander Zien
#t 2006
#c 19
#% 190581
#% 197394
#% 304876
#% 416549
#% 466263
#% 840938
#% 876050
#% 961195
#! Semi-Supervised Support Vector Machines (S3VMs) are an appealing method for using unlabeled data in classification: their objective function favors decision boundaries which do not cut clusters. However their main problem is that the optimization problem is non-convex and has many local minima, which often results in suboptimal performances. In this paper we propose to use a global optimization technique known as continuation to alleviate this problem. Compared to other algorithms minimizing the same objective function, our continuation method often leads to lower test errors.

#index 875969
#* A regularization framework for multiple-instance learning
#@ Pak-Ming Cheung;James T. Kwok
#t 2006
#c 19
#% 224755
#% 272527
#% 416553
#% 464436
#% 464633
#% 465916
#% 466927
#% 565537
#% 576520
#% 722913
#% 770827
#% 771844
#% 840922
#% 1860548
#! This paper focuses on kernel methods for multi-instance learning. Existing methods require the prediction of the bag to be identical to the maximum of those of its individual instances. However, this is too restrictive as only the sign is important in classification. In this paper, we provide a more complete regularization framework for MI learning by allowing the use of different loss functions between the outputs of a bag and its associated instances. This is especially important as we generalize this for multi-instance regression. Moreover, both bag and instance information can now be directly used in the optimization. Instead of using heuristics to solve the resultant non-linear optimization problem, we use the constrained concave-convex procedure which has well-studied convergence properties. Experiments on both classification and regression data sets show that the proposed method leads to improved performance.

#index 875970
#* Trading convexity for scalability
#@ Ronan Collobert;Fabian Sinz;Jason Weston;Léon Bottou
#t 2006
#c 19
#% 190581
#% 269218
#% 304876
#% 307097
#% 466263
#% 476873
#% 576520
#% 734919
#% 763708
#% 770766
#% 1861160
#! Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide scalability advantages over convexity. We show how concave-convex programming can be applied to produce (i) faster SVMs where training errors are no longer support vectors, and (ii) much faster Transductive SVMs.

#index 875971
#* Learning algorithms for online principal-agent problems (and selling goods online)
#@ Vincent Conitzer;Nikesh Garera
#t 2006
#c 19
#% 232319
#% 379485
#% 453487
#% 593734
#% 723936
#% 754134
#% 754140
#% 754148
#% 788099
#% 1250120
#% 1250124
#% 1269397
#% 1289498
#! In a principal-agent problem, a principal seeks to motivate an agent to take a certain action beneficial to the principal, while spending as little as possible on the reward. This is complicated by the fact that the principal does not know the agent's utility function (or type). We study the online setting where at each round, the principal encounters a new agent, and the principal sets the rewards anew. At the end of each round, the principal only finds out the action that the agent took, but not his type. The principal must learn how to set the rewards optimally. We show that this setting generalizes the setting of selling a digital good online.We study and experimentally compare three main approaches to this problem. First, we show how to apply a standard bandit algorithm to this setting. Second, for the case where the distribution of agent types is fixed (but unknown to the principal), we introduce a new gradient ascent algorithm. Third, for the case where the distribution of agents' types is fixed, and the principal has a prior belief (distribution) over a limited class of type distributions, we study a Bayesian approach.

#index 875972
#* Dealing with non-stationary environments using context detection
#@ Bruno C. da Silva;Eduardo W. Basso;Ana L. C. Bazzan;Paulo M. Engel
#t 2006
#c 19
#% 160859
#% 286423
#% 449447
#% 788097
#% 890329
#% 1272286
#! In this paper we introduce RL-CD, a method for solving reinforcement learning problems in non-stationary environments. The method is based on a mechanism for creating, updating and selecting one among several partial models of the environment. The partial models are incrementally built according to the system's capability of making predictions regarding a given sequence of observations. We propose, formalize and show the efficiency of this method both in a simple non-stationary environment and in a noisy scenario. We show that RL-CD performs better than two standard reinforcement learning algorithms and that it has advantages over methods specifically designed to cope with non-stationarity. Finally, we present known limitations of the method and future works.

#index 875973
#* Locally adaptive classification piloted by uncertainty
#@ Juan Dai;Shuicheng Yan;Xiaoou Tang;James T. Kwok
#t 2006
#c 19
#% 235342
#% 324288
#% 562956
#% 592108
#% 656665
#% 791401
#% 798509
#% 840948
#! Locally adaptive classifiers are usually superior to the use of a single global classifier. However, there are two major problems in designing locally adaptive classifiers. First, how to place the local classifiers, and, second, how to combine them together. In this paper, instead of placing the classifiers based on the data distribution only, we propose a responsibility mixture model that uses the uncertainty associated with the classification at each training sample. Using this model, the local classifiers are placed near the decision boundary where they are most effective. A set of local classifiers are then learned to form a global classifier by maximizing an estimate of the probability that the samples will be correctly classified with a nearest neighbor classifier. Experimental results on both artificial and real-world data sets demonstrate its superiority over traditional algorithms.

#index 875974
#* The relationship between Precision-Recall and ROC curves
#@ Jesse Davis;Mark Goadrich
#t 2006
#c 19
#% 57485
#% 70370
#% 279755
#% 310519
#% 464606
#% 466086
#% 564279
#% 770788
#% 840882
#% 840890
#% 1045379
#% 1269496
#% 1289459
#% 1289482
#% 1291574
#% 1378224
#! Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.

#index 875975
#* Discriminative cluster analysis
#@ Fernando De la Torre;Takeo Kanade
#t 2006
#c 19
#% 36672
#% 61438
#% 80995
#% 93251
#% 97411
#% 224113
#% 296738
#% 313959
#% 625065
#% 770830
#% 812501
#% 836707
#% 840857
#! Clustering is one of the most widely used statistical tools for data analysis. Among all existing clustering techniques, k-means is a very popular method because of its ease of programming and because it accomplishes a good trade-off between achieved performance and computational complexity. However, k-means is prone to local minima problems, and it does not scale too well with high dimensional data sets. A common approach to dealing with high dimensional data is to cluster in the space spanned by the principal components (PC). In this paper, we show the benefits of clustering in a low dimensional discriminative space rather than in the PC space (generative). In particular, we propose a new clustering algorithm called Discriminative Cluster Analysis (DCA). DCA jointly performs dimensionality reduction and clustering. Several toy and real examples show the benefits of DCA versus traditional PCA+k-means clustering. Additionally, a new matrix formulation is proposed and connections with related techniques such as spectral graph methods and linear discriminant analysis are provided.

#index 875976
#* Collaborative prediction using ensembles of Maximum Margin Matrix Factorizations
#@ Dennis DeCoste
#t 2006
#c 19
#% 283169
#% 458666
#% 465928
#% 578684
#% 734592
#% 770859
#% 771838
#% 840924
#% 1390165
#! Fast gradient-based methods for Maximum Margin Matrix Factorization (MMMF) were recently shown to have great promise (Rennie & Srebro, 2005), including significantly outperforming the previous state-of-the-art methods on some standard collaborative prediction benchmarks (including MovieLens). In this paper, we investigate ways to further improve the performance of MMMF, by casting it within an ensemble approach. We explore and evaluate a variety of alternative ways to define such ensembles. We show that our resulting ensembles can perform significantly better than a single MMMF model, along multiple evaluation metrics. In fact, we find that ensembles of partially trained MMMF models can sometimes even give better predictions in total training time comparable to a single MMMF model.

#index 875977
#* Learning the structure of Factored Markov Decision Processes in reinforcement learning problems
#@ Thomas Degris;Olivier Sigaud;Pierre-Henri Wuillemin
#t 2006
#c 19
#% 75936
#% 90041
#% 246747
#% 277494
#% 314843
#% 384911
#% 449529
#% 449588
#% 464778
#% 702594
#% 771849
#% 1272002
#% 1290041
#% 1650297
#% 1650705
#! Recent decision-theoric planning algorithms are able to find optimal solutions in large problems, using Factored Markov Decision Processes (FMDPs). However, these algorithms need a perfect knowledge of the structure of the problem. In this paper, we propose SDYNA, a general framework for addressing large reinforcement learning problems by trial-and-error and with no initial knowledge of their structure. SDYNA integrates incremental planning algorithms based on FMDPs with supervised learning techniques building structured representations of the problem. We describe SPITI, an instantiation of SDYNA, that uses incremental decision tree induction to learn the structure of a problem combined with an incremental version of the Structured Value Iteration algorithm. We show that SPITI can build a factored representation of a reinforcement learning problem and may improve the policy faster than tabular reinforcement learning algorithms by exploiting the generalization property of decision tree induction algorithms.

#index 875978
#* Efficient learning of Naive Bayes classifiers under class-conditional classification noise
#@ François Denis;Christophe Nicolas Magnan;Liva Ralaivola
#t 2006
#c 19
#% 246831
#% 276500
#% 836539
#% 1279298
#% 1699593
#! We address the problem of efficiently learning Naive Bayes classifiers under class-conditional classification noise (CCCN). Naive Bayes classifiers rely on the hypothesis that the distributions associated to each class are product distributions. When data is subject to CCC-noise, these conditional distributions are themselves mixtures of product distributions. We give analytical formulas which makes it possible to identify them from data subject to CCCN. Then, we design a learning algorithm based on these formulas able to learn Naive Bayes classifiers under CCCN. We present results on artificial datasets and datasets extracted from the UCI repository database. These results show that CCCN can be efficiently and successfully handled.

#index 875979
#* Learning user preferences for sets of objects
#@ Marie desJardins;Eric Eaton;Kiri L. Wagstaff
#t 2006
#c 19
#% 262112
#% 414514
#% 578710
#% 642975
#% 729437
#% 734915
#% 818584
#% 840846
#% 840852
#% 926881
#% 1250331
#% 1269444
#% 1269456
#% 1272396
#! Most work on preference learning has focused on pairwise preferences or rankings over individual items. In this paper, we present a method for learning preferences over sets of items. Our learning method takes as input a collection of positive examples---that is, one or more sets that have been identified by a user as desirable. Kernel density estimation is used to estimate the value function for individual items, and the desired set diversity is estimated from the average set diversity observed in the collection. Since this is a new learning problem, we introduce a new evaluation methodology and evaluate the learning method on two data collections: synthetic blocks-world data and a new real-world music data collection that we have gathered.

#index 875980
#* R1-PCA: rotational invariant L1-norm principal component analysis for robust subspace factorization
#@ Chris Ding;Ding Zhou;Xiaofeng He;Hongyuan Zha
#t 2006
#c 19
#% 224113
#% 444001
#% 576580
#% 753005
#% 770830
#% 770857
#% 812400
#! Principal component analysis (PCA) minimizes the sum of squared errors (L2-norm) and is sensitive to the presence of outliers. We propose a rotational invariant L1-norm PCA (R1-PCA). R1-PCA is similar to PCA in that (1) it has a unique global solution, (2) the solution are principal eigenvectors of a robust covariance matrix (re-weighted to soften the effects of outliers), (3) the solution is rotational invariant. These properties are not shared by the L1-norm PCA. A new subspace iteration algorithm is given to compute R1-PCA efficiently. Experiments on several real-life datasets show R1-PCA can effectively handle outliers. We extend R1-norm to K-means clustering and show that L1-norm K-means leads to poor results while R1-K-means outperforms standard K-means.

#index 875981
#* Clustering documents with an exponential-family approximation of the Dirichlet compound multinomial distribution
#@ Charles Elkan
#t 2006
#c 19
#% 261550
#% 722904
#% 826918
#% 837639
#% 840903
#% 916785
#% 1014679
#! The Dirichlet compound multinomial (DCM) distribution, also called the multivariate Polya distribution, is a model for text documents that takes into account burstiness: the fact that if a word occurs once in a document, it is likely to occur repeatedly. We derive a new family of distributions that are approximations to DCM distributions and constitute an exponential family, unlike DCM distributions. We use these so-called EDCM distributions to obtain insights into the properties of DCM distributions, and then derive an algorithm for EDCM maximum-likelihood training that is many times faster than the corresponding method for DCM distributions. Next, we investigate expectation-maximization with EDCM components and deterministic annealing as a new clustering algorithm for documents. Experiments show that the new algorithm is competitive with the best methods in the literature, and superior from the point of view of finding models with low perplexity.

#index 875982
#* A graphical model for predicting protein molecular function
#@ Barbara E. Engelhardt;Michael I. Jordan;Steven E. Brenner
#t 2006
#c 19
#! We present a simple statistical model of molecular function evolution to predict protein function. The model description encodes general knowledge of how molecular function evolves within a phylogenetic tree based on the proteins' sequence. Inputs are a phylogeny for a set of evolutionarily related protein sequences and any available function characterizations for those proteins. Posterior probabilities for each protein are used to predict the molecular function of that protein. We present results from applying our model to three protein families, and compare our prediction results on the extant proteins to other available protein function prediction methods. For the deaminase family, our method achieves 93.9% where related methods BLAST achieves 72.7%, GOtcha achieves 87.9%, and Orthostrapper achieves 72.7% in prediction accuracy.

#index 875983
#* Qualitative reinforcement learning
#@ Arkady Epshteyn;Gerald DeJong
#t 2006
#c 19
#% 318485
#% 384911
#% 466230
#% 770852
#% 840835
#% 875983
#% 1650329
#% 1650353
#! When the transition probabilities and rewards of a Markov Decision Process are specified exactly, the problem can be solved without any interaction with the environment. When no such specification is available, the agent's only recourse is a long and potentially dangerous exploration. We present a framework which allows the expert to specify imprecise knowledge of transition probabilities in terms of stochastic dominance constraints. Our algorithm can be used to find optimal policies for qualitatively specified problems, or, when no such solution is available, to decrease the required amount of exploration. The algorithm's behavior is demonstrated on simulations of two classic problems: mountain car ascent and cart pole balancing.

#index 875984
#* Online multiclass learning by interclass hypothesis sharing
#@ Michael Fink;Shai Shalev-Shwartz;Yoram Singer;Shimon Ullman
#t 2006
#c 19
#% 190581
#% 235377
#% 722903
#% 725437
#% 760805
#% 770763
#% 770796
#% 854636
#% 1502472
#! We describe a general framework for online multiclass learning based on the notion of hypothesis sharing. In our framework sets of classes are associated with hypotheses. Thus, all classes within a given set share the same hypothesis. This framework includes as special cases commonly used constructions for multiclass categorization such as allocating a unique hypothesis for each class and allocating a single common hypothesis for all classes. We generalize the multiclass Perceptron to our framework and derive a unifying mistake bound analysis. Our construction naturally extends to settings where the number of classes is not known in advance but, rather, is revealed along the online learning process. We demonstrate the merits of our approach by comparing it to previous methods on both synthetic and natural datasets.

#index 875985
#* Regression with the optimised combination technique
#@ Jochen Garcke
#t 2006
#c 19
#% 185955
#% 342599
#% 398278
#! We consider the sparse grid combination technique for regression, which we regard as a problem of function reconstruction in some given function space. We use a regularised least squares approach, discretised by sparse grids and solved using the so-called combination technique, where a certain sequence of conventional grids is employed. The sparse grid solution is then obtained by addition of the partial solutions with combination co-efficients dependent on the involved grids. This approach shows instabilities in certain situations and is not guaranteed to converge with higher discretisation levels. In this article we apply the recently introduced optimised combination technique, which repairs these instabilities. Now the combination coefficients also depend on the function to be reconstructed, resulting in a non-linear approximation method which achieves very competitive results. We show that the computational complexity of the improved method still scales only linear in regard to the number of data.

#index 875986
#* A note on mixtures of experts for multiclass responses: approximation rate and Consistent Bayesian Inference
#@ Yang Ge;Wenxin Jiang
#t 2006
#c 19
#% 168831
#% 169358
#% 293367
#% 314707
#% 853734
#! We report that mixtures of m multinomial logistic regression can be used to approximate a class of 'smooth' probability models for multiclass responses. With bounded second derivatives of log-odds, the approximation rate is O(m-2/s) in Hellinger distance or O(m-4/s) in Kullback-Leibler divergence. Here s = dim(x) is the dimension of the input space (or the number of predictors). With the availability of training data of size n, we also show that 'consistency' in multiclass regression and classification can be achieved, simultaneously for all classes, when posterior based inference is performed in a Bayesian framework. Loosely speaking, such 'consistency' refers to performance being often close to the best possible for large n. Consistency can be achieved either by taking m = mn, or by taking m to be uniformly distributed among {1, ...,mn} according to the prior, where 1 &pr; mn &pr; na in order as n grows, for some a ∈ (0, 1).

#index 875987
#* The rate adapting poisson model for information retrieval and object recognition
#@ Peter V. Gehler;Alex D. Holub;Max Welling
#t 2006
#c 19
#% 272536
#% 350323
#% 450888
#% 458673
#% 492962
#% 643056
#% 722904
#% 788043
#% 812535
#% 836904
#% 1650298
#% 1650387
#! Probabilistic modelling of text data in the bag-of-words representation has been dominated by directed graphical models such as pLSI, LDA, NMF, and discrete PCA. Recently, state of the art performance on visual object recognition has also been reported using variants of these models. We introduce an alternative undirected graphical model suitable for modelling count data. This "Rate Adapting Poisson" (RAP) model is shown to generate superior dimensionally reduced representations for subsequent retrieval or classification. Models are trained using contrastive divergence while inference of latent topical representations is efficiently achieved through a simple matrix multiplication.

#index 875988
#* Kernelizing the output of tree-based methods
#@ Pierre Geurts;Louis Wehenkel;Florence d'Alché-Buc
#t 2006
#c 19
#% 209021
#% 458687
#% 464615
#% 466073
#% 723238
#% 769935
#% 833016
#% 833069
#% 840854
#% 840947
#% 866298
#% 1717612
#! We extend tree-based methods to the prediction of structured outputs using a kernelization of the algorithm that allows one to grow trees as soon as a kernel can be defined on the output space. The resulting algorithm, called output kernel trees (OK3), generalizes classification and regression trees as well as tree-based ensemble methods in a principled way. It inherits several features of these methods such as interpretability, robustness to irrelevant variables, and input scalability. When only the Gram matrix over the outputs of the learning sample is given, it learns the output kernel as a function of inputs. We show that the proposed algorithm works well on an image reconstruction task and on a biological network inference problem.

#index 875989
#* Nightmare at test time: robust learning by feature deletion
#@ Amir Globerson;Sam Roweis
#t 2006
#c 19
#% 389586
#% 465754
#% 757953
#% 763697
#% 770774
#% 1289457
#! When constructing a classifier from labeled data, it is important not to assign too much weight to any single input feature, in order to increase the robustness of the classifier. This is particularly important in domains with nonstationary feature distributions or with input sensor failures. A common approach to achieving such robustness is to introduce regularization which spreads the weight more evenly between the features. However, this strategy is very generic, and cannot induce robustness specifically tailored to the classification task at hand. In this work, we introduce a new algorithm for avoiding single feature over-weighting by analyzing robustness using a game theoretic formalization. We develop classifiers which are optimally resilient to deletion of features in a minimax sense, and show how to construct such classifiers using quadratic programming. We illustrate the applicability of our methods on spam filtering and handwritten digit recognition tasks, where feature deletion is indeed a realistic noise model.

#index 875990
#* A choice model with infinitely many latent features
#@ Dilan Görür;Frank Jäkel;Carl Edward Rasmussen
#t 2006
#c 19
#! Elimination by aspects (EBA) is a probabilistic choice model describing how humans decide between several options. The options from which the choice is made are characterized by binary features and associated weights. For instance, when choosing which mobile phone to buy the features to consider may be: long lasting battery, color screen, etc. Existing methods for inferring the parameters of the model assume pre-specified features. However, the features that lead to the observed choices are not always known. Here, we present a non-parametric Bayesian model to infer the features of the options and the corresponding weights from choice data. We use the Indian buffet process (IBP) as a prior over the features. Inference using Markov chain Monte Carlo (MCMC) in conjugate IBP models has been previously described. The main contribution of this paper is an MCMC algorithm for the EBA model that can also be used in inference for other non-conjugate IBP models---this may broaden the use of IBP priors considerably.

#index 875991
#* Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks
#@ Alex Graves;Santiago Fernández;Faustino Gomez;Jürgen Schmidhuber
#t 2006
#c 19
#% 237658
#% 361100
#% 395470
#% 450291
#% 464434
#% 476873
#% 722892
#% 788670
#% 856275
#% 1579211
#% 1762951
#! Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.

#index 875992
#* Practical solutions to the problem of diagonal dominance in kernel document clustering
#@ Derek Greene;Pádraig Cunningham
#t 2006
#c 19
#% 266426
#% 390723
#% 393059
#% 458667
#% 629666
#% 722902
#% 722925
#% 785508
#% 832727
#! In supervised kernel methods, it has been observed that the performance of the SVM classifier is poor in cases where the diagonal entries of the Gram matrix are large relative to the off-diagonal entries. This problem, referred to as diagonal dominance, often occurs when certain kernel functions are applied to sparse high-dimensional data, such as text corpora. In this paper we investigate the implications of diagonal dominance for unsupervised kernel methods, specifically in the task of document clustering. We propose a selection of strategies for addressing this issue, and evaluate their effectiveness in producing more accurate and stable clusterings.

#index 875993
#* Fast transpose methods for kernel learning on sparse data
#@ Patrick Haffner
#t 2006
#c 19
#% 190581
#% 197394
#% 211044
#% 251625
#% 269217
#% 269218
#% 458379
#% 465754
#% 741425
#% 771848
#% 817422
#% 916781
#% 1558464
#! Kernel-based learning algorithms, such as Support Vector Machines (SVMs) or Perceptron, often rely on sequential optimization where a few examples are added at each iteration. Updating the kernel matrix usually requires matrix-vector multiplications. We propose a new method based on transposition to speedup this computation on sparse data. Instead of dot-products over sparse feature vectors, our computation incrementally merges lists of training examples and minimizes access to the data. Caching and shrinking are also optimized for sparsity. On very large natural language tasks (tagging, translation, text classification) with sparse feature representations, a 20 to 80-fold speedup over LIBSVM is observed using the same SMO algorithm. Theory and experiments explain what type of sparsity structure is needed for this approach to work, and why its adaptation to Maxent sequential optimization is inefficient.

#index 875994
#* An analysis of graph cut size for transductive learning
#@ Steve Hanneke
#t 2006
#c 19
#% 163897
#% 164166
#% 214077
#% 214078
#% 283980
#% 311496
#% 410276
#% 565545
#% 749440
#% 770851
#% 777978
#% 778077
#% 845510
#% 1272044
#! I consider the setting of transductive learning of vertex labels in graphs, in which a graph with n vertices is sampled according to some unknown distribution; there is a true labeling of the vertices such that each vertex is assigned to exactly one of k classes, but the labels of only some (random) subset of the vertices are revealed to the learner. The task is then to find a labeling of the remaining (unlabeled) vertices that agrees as much as possible with the true labeling. Several existing algorithms are based on the assumption that adjacent vertices are usually labeled the same. In order to better understand algorithms based on this assumption, I derive data-dependent bounds on the fraction of mislabeled vertices, based on the number (or total weight) of edges between vertices differing in predicted label (i.e., the size of the cut).

#index 875995
#* Learning a kernel function for classification with small training samples
#@ Tomer Hertz;Aharon Bar Hillel;Daphna Weinshall
#t 2006
#c 19
#% 236495
#% 267027
#% 302391
#% 565549
#% 593654
#% 718392
#% 770811
#% 784995
#% 836706
#% 840962
#% 866297
#% 1272365
#! When given a small sample, we show that classification with SVM can be considerably enhanced by using a kernel function learned from the training data prior to discrimination. This kernel is also shown to enhance retrieval based on data similarity. Specifically, we describe KernelBoost - a boosting algorithm which computes a kernel function as a combination of 'weak' space partitions. The kernel learning method naturally incorporates domain knowledge in the form of unlabeled data (i.e. in a semi-supervised or transductive settings), and also in the form of labeled samples from relevant related problems (i.e. in a learning-to-learn scenario). The latter goal is accomplished by learning a single kernel function for all classes. We show comparative evaluations of our method on datasets from the UCI repository. We demonstrate performance enhancement on two challenging tasks: digit classification with kernel SVM, and facial image retrieval based on image similarity as measured by the learnt kernel.

#index 875996
#* Looping suffix tree-based inference of partially observable hidden state
#@ Michael P. Holmes;Charles Lee Isbell, Jr
#t 2006
#c 19
#% 100336
#% 150145
#% 158924
#% 451047
#% 702594
#% 788096
#% 1269515
#! We present a solution for inferring hidden state from sensorimotor experience when the environment takes the form of a POMDP with deterministic transition and observation functions. Such environments can appear to be arbitrarily complex and non-deterministic on the surface, but are actually deterministic with respect to the unobserved underlying state. We show that there always exists a finite history-based representation that fully captures the unobserved world state, allowing for perfect prediction of action effects. This representation takes the form of a looping prediction suffix tree (PST). We derive a sound and complete algorithm for learning a looping PST from a sufficient sample of sensorimotor experience. We also give empirical illustrations of the advantages conferred by this approach, and characterize the approximations to the looping PST that are made by existing algorithms such as Variable Length Markov Models, Utile Suffix Memory and Causal State Splitting Reconstruction.

#index 875997
#* Batch mode active learning and its application to medical image classification
#@ Steven C. H. Hoi;Rong Jin;Jianke Zhu;Michael R. Lyu
#t 2006
#c 19
#% 42240
#% 116165
#% 236729
#% 420077
#% 446680
#% 464268
#% 466419
#% 466576
#% 466887
#% 565531
#% 775375
#% 818209
#% 1855337
#! The goal of active learning is to select the most informative examples for manual labeling. Most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration. This could be inefficient since the classification model has to be retrained for every labeled example. In this paper, we present a framework for "batch mode active learning" that applies the Fisher information matrix to select a number of informative examples simultaneously. The key computational challenge is how to efficiently identify the subset of unlabeled examples that can result in the largest reduction in the Fisher information. To resolve this challenge, we propose an efficient greedy algorithm that is based on the property of submodular functions. Our empirical studies with five UCI datasets and one real-world medical image classification show that the proposed batch mode active learning algorithm is more effective than the state-of-the-art algorithms for active learning.

#index 875998
#* Ranking individuals by group comparisons
#@ Tzu-Kuo Huang;Chih-Jen Lin;Ruby C. Weng
#t 2006
#c 19
#% 226495
#% 722756
#% 815864
#% 1272365
#! This paper proposes new approaches to rank individuals from their group competition results. Many real-world problems are of this type. For example, ranking players from team games is important in some sports. We propose an exponential model to solve such problems. To estimate individual rankings through the proposed model we introduce two convex minimization formulas with easy and efficient solution procedures. Experiments on real bridge records and multi-class classification demonstrate the viability of the proposed model.

#index 875999
#* Hidden process models
#@ Rebecca A. Hutchinson;Tom M. Mitchell;Indrayana Rustandi
#t 2006
#c 19
#% 541077
#% 768668
#% 961183
#! We introduce Hidden Process Models (HPMs), a class of probabilistic models for multivariate time series data. The design of HPMs has been motivated by the challenges of modeling hidden cognitive processes in the brain, given functional Magnetic Resonance Imaging (fMRI) data. fMRI data is sparse, high-dimensional, non-Markovian, and often involves prior knowledge of the form "hidden event A occurs n times within the interval [t,t′]." HPMs provide a generalization of the widely used General Linear Model approaches to fMRI analysis, and HPMs can also be viewed as a subclass of Dynamic Bayes Networks.

#index 876000
#* Estimating relatedness via data compression
#@ Brendan Juba
#t 2006
#c 19
#% 26125
#% 73372
#% 88325
#% 145224
#% 234979
#% 236497
#% 267044
#% 453188
#% 453575
#% 739899
#% 1271814
#! We show that it is possible to use data compression on independently obtained hypotheses from various tasks to algorithmically provide guarantees that the tasks are sufficiently related to benefit from multitask learning. We give uniform bounds in terms of the empirical average error for the true average error of the n hypotheses provided by deterministic learning algorithms drawing independent samples from a set of n unknown computable task distributions over finite sets.

#index 876001
#* Automatic basis function construction for approximate dynamic programming and reinforcement learning
#@ Philipp W. Keller;Shie Mannor;Doina Precup
#t 2006
#c 19
#% 203596
#% 331911
#% 366058
#% 425076
#% 425080
#% 449561
#% 729964
#! We address the problem of automatically constructing basis functions for linear approximation of the value function of a Markov Decision Process (MDP). Our work builds on results by Bertsekas and Castañon (1989) who proposed a method for automatically aggregating states to speed up value iteration. We propose to use neighborhood component analysis (Goldberger et al., 2005), a dimensionality reduction technique created for supervised learning, in order to map a high-dimensional state space to a low-dimensional space, based on the Bellman error, or on the temporal difference (TD) error. We then place basis function in the lower-dimensional space. These are added as new features for the linear function approximator. This approach is applied to a high-dimensional inventory control problem.

#index 876002
#* Personalized handwriting recognition via biased regularization
#@ Wolf Kienzle;Kumar Chellapilla
#t 2006
#c 19
#% 345826
#% 466087
#% 640455
#% 658773
#% 743284
#% 1558464
#% 1860941
#! We present a new approach to personalized handwriting recognition. The problem, also known as writer adaptation, consists of converting a generic (user-independent) recognizer into a personalized (user-dependent) one, which has an improved recognition rate for a particular user. The adaptation step usually involves user-specific samples, which leads to the fundamental question of how to fuse this new information with that captured by the generic recognizer. We propose adapting the recognizer by minimizing a regularized risk functional (a modified SVM) where the prior knowledge from the generic recognizer enters through a modified regularization term. The result is a simple personalization framework with very good practical properties. Experiments on a 100 class real-world data set show that the number of errors can be reduced by over 40% with as few as five user samples per character.

#index 876003
#* Optimal kernel selection in Kernel Fisher discriminant analysis
#@ Seung-Jean Kim;Alessandro Magnani;Stephen Boyd
#t 2006
#c 19
#% 209961
#% 577213
#% 578414
#% 743284
#% 757953
#% 763697
#% 769930
#% 770831
#% 770846
#% 789030
#% 832903
#% 1861472
#! In Kernel Fisher discriminant analysis (KFDA), we carry out Fisher linear discriminant analysis in a high dimensional feature space defined implicitly by a kernel. The performance of KFDA depends on the choice of the kernel; in this paper, we consider the problem of finding the optimal kernel, over a given convex set of kernels. We show that this optimal kernel selection problem can be reformulated as a tractable convex optimization problem which interior-point methods can solve globally and efficiently. The kernel selection method is demonstrated with some UCI machine learning benchmark examples.

#index 876004
#* Pareto optimal linear classification
#@ Seung-Jean Kim;Alessandro Magnani;Sikandar Samar;Stephen Boyd;Johan Lim
#t 2006
#c 19
#% 722901
#% 727925
#% 757953
#% 785369
#% 793241
#% 840902
#% 840959
#% 876004
#! We consider the problem of choosing a linear classifier that minimizes misclassification probabilities in two-class classification, which is a bi-criterion problem, involving a trade-off between two objectives. We assume that the class-conditional distributions are Gaussian. This assumption makes it computationally tractable to find Pareto optimal linear classifiers whose classification capabilities are inferior to no other linear ones. The main purpose of this paper is to establish several robustness properties of those classifiers with respect to variations and uncertainties in the distributions. We also extend the results to kernel-based classification. Finally, we show how to carry out trade-off analysis empirically with a finite number of given labeled data.

#index 876005
#* Fast particle smoothing: if I had a million particles
#@ Mike Klaas;Mark Briers;Nando de Freitas;Arnaud Doucet;Simon Maskell;Dustin Lang
#t 2006
#c 19
#% 31225
#% 44876
#% 457719
#% 724162
#% 724253
#% 1271978
#! We propose efficient particle smoothing methods for generalized state-spaces models. Particle smoothing is an expensive O(N2) algorithm, where N is the number of particles. We overcome this problem by integrating dual tree recursions and fast multipole techniques with forward-backward smoothers, a new generalized two-filter smoother and a maximum a posteriori (MAP) smoother. Our experiments show that these improvements can substantially increase the practicality of particle smoothing.

#index 876006
#* Autonomous shaping: knowledge transfer in reinforcement learning
#@ George Konidaris;Andrew Barto
#t 2006
#c 19
#% 68238
#% 160859
#% 181627
#% 203608
#% 305086
#% 366058
#% 383916
#% 418632
#% 464607
#% 466230
#% 565532
#% 647111
#% 677519
#% 707761
#% 840904
#% 1269498
#% 1271996
#! We introduce the use of learned shaping rewards in reinforcement learning tasks, where an agent uses prior experience on a sequence of tasks to learn a portable predictor that estimates intermediate rewards, resulting in accelerated learning in later tasks that are related but distinct. Such agents can be trained on a sequence of relatively easy tasks in order to develop a more informative measure of reward that can be transferred to improve performance on more difficult tasks without requiring a hand coded shaping function. We use a rod positioning task to show that this significantly improves performance even after a very brief training period.

#index 876007
#* Data association for topic intensity tracking
#@ Andreas Krause;Jure Leskovec;Carlos Guestrin
#t 2006
#c 19
#% 246832
#% 246836
#% 262043
#% 271083
#% 309096
#% 309100
#% 342279
#% 528327
#% 577220
#% 722904
#% 1289566
#% 1673032
#! We present a unified model of what was traditionally viewed as two separate tasks: data association and intensity tracking of multiple topics over time. In the data association part, the task is to assign a topic (a class) to each data point, and the intensity tracking part models the bursts and changes in intensities of topics over time. Our approach to this problem combines an extension of Factorial Hidden Markov models for topic intensity tracking with exponential order statistics for implicit data association. Experiments on text and email datasets show that the interplay of classification and topic intensity tracking improves the accuracy of both classification and intensity tracking. Even a little noise in topic assignments can mislead the traditional algorithms. However, our approach detects correct topic intensities even with 30% topic noise.

#index 876008
#* Learning low-rank kernel matrices
#@ Brian Kulis;Mátyás Sustik;Inderjit Dhillon
#t 2006
#c 19
#% 31225
#% 238376
#% 722815
#% 743284
#% 763697
#% 770767
#% 840839
#% 840892
#! Kernel learning plays an important role in many machine learning tasks. However, algorithms for learning a kernel matrix often scale poorly, with running times that are cubic in the number of data points. In this paper, we propose efficient algorithms for learning low-rank kernel matrices; our algorithms scale linearly in the number of data points and quadratically in the rank of the kernel. We introduce and employ Bregman matrix divergences for rank-deficient matrices---these divergences are natural for our problem since they preserve the rank as well as positive semi-definiteness of the kernel matrix. Special cases of our framework yield faster algorithms for various existing kernel learning problems. Experimental results demonstrate the effectiveness of our algorithms in learning both low-rank and full-rank kernels.

#index 876009
#* Local distance preservation in the GP-LVM through back constraints
#@ Neil D. Lawrence;Joaquin Quiñonero-Candela
#t 2006
#c 19
#% 257039
#% 266426
#% 278040
#% 770767
#% 771053
#% 836721
#% 840968
#% 916787
#% 1034759
#! The Gaussian process latent variable model (GP-LVM) is a generative approach to nonlinear low dimensional embedding, that provides a smooth probabilistic mapping from latent to data space. It is also a non-linear generalization of probabilistic PCA (PPCA) (Tipping & Bishop, 1999). While most approaches to non-linear dimensionality methods focus on preserving local distances in data space, the GP-LVM focusses on exactly the opposite. Being a smooth mapping from latent to data space, it focusses on keeping things apart in latent space that are far apart in data space. In this paper we first provide an overview of dimensionality reduction techniques, placing the emphasis on the kind of distance relation preserved. We then show how the GP-LVM can be generalized, through back constraints, to additionally preserve local distances. We give illustrative experiments on common data sets.

#index 876010
#* Simpler knowledge-based support vector machines
#@ Quoc V. Le;Alex J. Smola;Thomas Gärtner
#t 2006
#c 19
#% 264923
#% 360691
#% 576520
#% 722894
#% 722898
#% 722909
#% 739899
#% 793236
#% 872759
#! If appropriately used, prior knowledge can significantly improve the predictive accuracy of learning algorithms or reduce the amount of training data needed. In this paper we introduce a simple method to incorporate prior knowledge in support vector machines by modifying the hypothesis space rather than the optimization problem. The optimization problem is amenable to solution by the constrained concave convex procedure, which finds a local optimum. The paper discusses different kinds of prior knowledge and demonstrates the applicability of the approach in some characteristic experiments.

#index 876011
#* Using query-specific variance estimates to combine Bayesian classifiers
#@ Chi-Hoon Lee;Russ Greiner;Shaojun Wang
#t 2006
#c 19
#% 44876
#% 129987
#% 156186
#% 209021
#% 229513
#% 235377
#% 246832
#% 277480
#% 376266
#% 424997
#% 528319
#% 729437
#! Many of today's best classification results are obtained by combining the responses of a set of base classifiers to produce an answer for the query. This paper explores a novel "query specific" combination rule: After learning a set of simple belief network classifiers, we produce an answer to each query by combining their individual responses, using weights based inversely on their respective variances around their responses. These variances are based on the uncertainty of the network parameters, which in turn depend on the training datasample. In essence, this variance quantifies the base classifier's confidence of its response to this query. Our experimental results show that these "mixture-using-variance belief net classifiers" MUVS work effectively, especially when the base classifiers are learned using balanced bootstrap samples and when their results are combined using James-Stein shrinkage. We also found that our variance-based combination rule performed better than both bagging and AdaBoost, even on the set of base classifiers produced by AdaBoost itself. Finally, this framework is extremely efficient, as both the learning and the classification components require only straight-line code.

#index 876012
#* A probabilistic model for text kernels
#@ Alain Lehmann;John Shawe-Taylor
#t 2006
#c 19
#% 190581
#% 304917
#% 425047
#% 458379
#% 722803
#% 743284
#! This paper explores several kernels in the context of text classification. A novel view of how documents might have been created is introduced and kernels are derived from this framework. The relations between these kernels as well as to the Gaussian kernel are discussed. Moreover, the popular tf-idf weighting scheme will be derived as a natural consequence. Finally, the kernels have been evaluated on the Reuters Corpus Volume I newswire database to assess their quality in a topic classification application.

#index 876013
#* Efficient MAP approximation for dense energy functions
#@ Marius Leordeanu;Martial Hebert
#t 2006
#c 19
#% 506302
#% 576520
#% 724344
#% 836724
#% 1632801
#% 1650318
#% 1810398
#% 1815753
#! We present an efficient method for maximizing energy functions with first and second order potentials, suitable for MAP labeling estimation problems that arise in undirected graphical models. Our approach is to relax the integer constraints on the solution in two steps. First we efficiently obtain the relaxed global optimum following a procedure similar to the iterative power method for finding the largest eigenvector of a matrix. Next, we map the relaxed optimum on a simplex and show that the new energy obtained has a certain optimal bound. Starting from this energy we follow an efficient coordinate ascent procedure that is guaranteed to increase the energy at every step and converge to a solution that obeys the initial integral constraints. We also present a sufficient condition for ascent procedures that guarantees the increase in energy at every step.

#index 876014
#* Nonstationary kernel combination
#@ Darrin P. Lewis;Tony Jebara;William Stafford Noble
#t 2006
#c 19
#% 269218
#% 311027
#% 328374
#% 466263
#% 565549
#% 749653
#% 833065
#% 833088
#! The power and popularity of kernel methods stem in part from their ability to handle diverse forms of structured inputs, including vectors, graphs and strings. Recently, several methods have been proposed for combining kernels from heterogeneous data sources. However, all of these methods produce stationary combinations; i.e., the relative weights of the various kernels do not vary among input examples. This article proposes a method for combining multiple kernels in a nonstationary fashion. The approach uses a large-margin latent-variable generative model within the maximum entropy discrimination (MED) framework. Latent parameter estimation is rendered tractable by variational bounds and an iterative optimization procedure. The classifier we use is a log-ratio of Gaussian mixtures, in which each component is implicitly mapped via a Mercer kernel function. We show that the support vector machine is a special case of this model. In this approach, discriminative parameter estimation is feasible via a fast sequential minimal optimization algorithm. Empirical results are presented on synthetic data, several benchmarks, and on a protein function annotation task.

#index 876015
#* Region-based value iteration for partially observable Markov decision processes
#@ Hui Li;Xuejun Liao;Lawrence Carin
#t 2006
#c 19
#% 92301
#% 252183
#% 695957
#% 788098
#% 1279358
#% 1478842
#! An approximate region-based value iteration (RBVI) algorithm is proposed to find the optimal policy for a partially observable Markov decision process (POMDP). The proposed RBVI approximates the true polyhedral partition of the belief simplex with an ellipsoidal partition, such that the optimal value function is linear in each of the ellipsoidal regions. The position and shape of each region, as well as the gradient (alpha-vector) of the optimal value function in the region, are parameterized explicitly, and are estimated via efficient expectation maximization (EM) and variational Bayesian EM (VBEM), based on a set of selected sample belief points. The RBVI maintains a much smaller number of alpha-vectors than point-based methods and yields a more parsimonious representation that approximates the true value function in the maximum likelihood (ML) sense. The results on benchmark problems show that the proposed RBVI is comparable in performance to state-of-the-art algorithms, despite of the small number of alpha-vectors that are used.

#index 876016
#* Multiclass boosting with repartitioning
#@ Ling Li
#t 2006
#c 19
#% 190581
#% 191910
#% 276516
#% 465751
#% 722756
#% 840944
#% 1272365
#! A multiclass classification problem can be reduced to a collection of binary problems with the aid of a coding matrix. The quality of the final solution, which is an ensemble of base classifiers learned on the binary problems, is affected by both the performance of the base learner and the error-correcting ability of the coding matrix. A coding matrix with strong error-correcting ability may not be overall optimal if the binary problems are too hard for the base learner. Thus a trade-off between error-correcting and base learning should be sought. In this paper, we propose a new multiclass boosting algorithm that modifies the coding matrix according to the learning ability of the base learner. We show experimentally that our algorithm is very efficient in optimizing the multiclass margin cost, and outperforms existing multiclass algorithms such as AdaBoost.ECC and one-vs-one. The improvement is especially significant when the base learner is not very powerful.

#index 876017
#* Pachinko allocation: DAG-structured mixture models of topic correlations
#@ Wei Li;Andrew McCallum
#t 2006
#c 19
#% 340951
#% 722904
#! Latent Dirichlet allocation (LDA) and other related topic models are increasingly popular tools for summarization and manifold discovery in discrete data. However, LDA does not capture correlations between topics. In this paper, we introduce the pachinko allocation model (PAM), which captures arbitrary, nested, and possibly sparse correlations between topics using a directed acyclic graph (DAG). The leaves of the DAG represent individual words in the vocabulary, while each interior node represents a correlation among its children, which may be words or other interior nodes (topics). PAM provides a flexible alternative to recent work by Blei and Lafferty (2006), which captures correlations only between pairs of topics. Using text data from newsgroups, historic NIPS proceedings and other research paper corpora, we show improved performance of PAM in document classification, likelihood of held-out data, the ability to support finer-grained topics, and topical keyword coherence.

#index 876018
#* Spectral clustering for multi-type relational data
#@ Bo Long;Zhongfei (Mark) Zhang;Xiaoyun Wú;Philip S. Yu
#t 2006
#c 19
#% 148149
#% 280819
#% 313959
#% 342621
#% 342659
#% 466675
#% 495929
#% 566189
#% 578670
#% 643009
#% 729437
#% 729918
#% 769928
#% 770829
#% 823328
#% 823343
#% 823396
#% 1289267
#! Clustering on multi-type relational data has attracted more and more attention in recent years due to its high impact on various important applications, such as Web mining, e-commerce and bioinformatics. However, the research on general multi-type relational data clustering is still limited and preliminary. The contribution of the paper is three-fold. First, we propose a general model, the collective factorization on related matrices, for multi-type relational data clustering. The model is applicable to relational data with various structures. Second, under this model, we derive a novel algorithm, the spectral relational clustering, to cluster multi-type interrelated data objects simultaneously. The algorithm iteratively embeds each type of data objects into low dimensional spaces and benefits from the interactions among the hidden structures of different types of data objects. Extensive experiments demonstrate the promise and effectiveness of the proposed algorithm. Third, we show that the existing spectral clustering algorithms can be considered as the special cases of the proposed model and algorithm. This demonstrates the good theoretic generality of the proposed model and algorithm.

#index 876019
#* Combined central and subspace clustering for computer vision applications
#@ Le Lu;René Vidal
#t 2006
#c 19
#% 278040
#% 729437
#% 837622
#% 1502529
#% 1562503
#! Central and subspace clustering methods are at the core of many segmentation problems in computer vision. However, both methods fail to give the correct segmentation in many practical scenarios, e.g., when data points are close to the intersection of two subspaces or when two cluster centers in different subspaces are spatially close. In this paper, we address these challenges by considering the problem of clustering a set of points lying in a union of subspaces and distributed around multiple cluster centers inside each subspace. We propose a generalization of Kmeans and Ksubspaces that clusters the data by minimizing a cost function that combines both central and subspace distances. Experiments on synthetic data compare our algorithm favorably against four other clustering methods. We also test our algorithm on computer vision problems such as face clustering with varying illumination and video shot segmentation of dynamic scenes.

#index 876020
#* Fast direct policy evaluation using multiscale analysis of Markov diffusion processes
#@ Mauro Maggioni;Sridhar Mahadevan
#t 2006
#c 19
#% 31225
#% 431471
#% 593047
#% 734920
#! Policy evaluation is a critical step in the approximate solution of large Markov decision processes (MDPs), typically requiring O(|S|3) to directly solve the Bellman system of |S| linear equations (where |S| is the state space size in the discrete case, and the sample size in the continuous case). In this paper we apply a recently introduced multiscale framework for analysis on graphs to design a faster algorithm for policy evaluation. For a fixed policy π, this framework efficiently constructs a multiscale decomposition of the random walk Pπ associated with the policy π. This enables efficiently computing medium and long term state distributions, approximation of value functions, and the direct computation of the potential operator (I - γPπ)-1 needed to solve Bellman's equation. We show that even a preliminary non-optimized version of the solver competes with highly optimized iterative techniques, requiring in many cases a complexity of O(|S|).

#index 876021
#* Pruning in ordered bagging ensembles
#@ Gonzalo Martínez-Muñoz;Alberto Suárez
#t 2006
#c 19
#% 209021
#% 312727
#% 312728
#% 329533
#% 379341
#% 400847
#% 400985
#% 465755
#% 520224
#% 565528
#% 580511
#% 581138
#% 770808
#% 1378374
#% 1408623
#! We present a novel ensemble pruning method based on reordering the classifiers obtained from bagging and then selecting a subset for aggregation. Ordering the classifiers generated in bagging makes it possible to build subensembles of increasing size by including first those classifiers that are expected to perform best when aggregated. Ensemble pruning is achieved by halting the aggregation process before all the classifiers generated are included into the ensemble. Pruned subensembles containing between 15% and 30% of the initial pool of classifiers, besides being smaller, improve the generalization performance of the full bagging ensemble in the classification problems investigated.

#index 876022
#* Learning high-order MRF priors of color images
#@ Julian J. McAuley;Tibério S. Caetano;Alex J. Smola;Matthias O. Franz
#t 2006
#c 19
#% 261293
#% 323242
#% 812582
#% 836674
#! In this paper, we use large neighborhood Markov random fields to learn rich prior models of color images. Our approach extends the monochromatic Fields of Experts model (Roth & Black, 2005a) to color images. In the Fields of Experts model, the curse of dimensionality due to very large clique sizes is circumvented by parameterizing the potential functions according to a product of experts. We introduce simplifications to the original approach by Roth and Black which allow us to cope with the increased clique size (typically 3x3x3 or 5x5x3 pixels) of color images. Experimental results are presented for image denoising which evidence improvements over state-of-the-art monochromatic image priors.

#index 876023
#* The uniqueness of a good optimum for K-means
#@ Marina Meilă
#t 2006
#c 19
#% 25998
#% 593926
#% 763861
#% 770830
#% 771391
#% 840907
#! If we have found a "good" clustering C of a data set, can we prove that C is not far from the (unknown) best clustering Copt of these data? Perhaps surprisingly, the answer to this question is sometimes yes. When "goodness" is measured by the distortion of K-means clustering, this paper proves spectral bounds on the distance d(C, Copt). The bounds exist in the case when the data admits a low distortion clustering.

#index 876024
#* Kernel information embeddings
#@ Roland Memisevic
#t 2006
#c 19
#% 115608
#% 266426
#% 722942
#% 821881
#% 857076
#! We describe a family of embedding algorithms that are based on nonparametric estimates of mutual information (MI). Using Parzen window estimates of the distribution in the joint (input, embedding)-space, we derive a MI-based objective function for dimensionality reduction that can be optimized directly with respect to a set of latent data representatives. Various types of supervision signal can be introduced within the framework by replacing plain MI with several forms of conditional MI. Examples of the semi-(un)supervised algorithms that we obtain this way are a new model for manifold alignment, and a new type of embedding method that performs 'conditional dimensionality reduction'.

#index 876025
#* Generalized spectral bounds for sparse LDA
#@ Baback Moghaddam;Yair Weiss;Shai Avidan
#t 2006
#c 19
#% 3084
#% 36698
#% 243727
#% 243728
#% 425048
#% 722929
#% 762666
#% 1300083
#! We present a discrete spectral framework for the sparse or cardinality-constrained solution of a generalized Rayleigh quotient. This NP-hard combinatorial optimization problem is central to supervised learning tasks such as sparse LDA, feature selection and relevance ranking for classification. We derive a new generalized form of the Inclusion Principle for variational eigenvalue bounds, leading to exact and optimal sparse linear discriminants using branch-and-bound search. An efficient greedy (approximate) technique is also presented. The generalization performance of our sparse LDA algorithms is demonstrated with real-world UCI ML benchmarks and compared to a leading SVM-based gene selection algorithm for cancer classification.

#index 876026
#* Learning to impersonate
#@ Moni Naor;Guy N. Rothblum
#t 2006
#c 19
#% 697
#% 15404
#% 70165
#% 131687
#% 163021
#% 164838
#% 170405
#% 176024
#% 180945
#% 222437
#% 280430
#% 354287
#% 616292
#% 785093
#% 836546
#% 876026
#% 1810124
#! Consider Alice and Bob, who have some shared secret which helps Alice to identify Bob-impersonators, and Eve, who does not know their secret. Eve wants to impersonate Bob and "fool" Alice. If Eve is computationally unbounded, how long does she need to observe Bob before she can impersonate him? What is a good strategy for Eve? If (cryptographic) one-way functions exist, an efficient Eve cannot impersonate even very simple Bobs, but if they do not exist, can Eve learn to impersonate any efficient Bob?We formalize these questions in a new computational learning model, which we believe captures a wide variety of natural learning tasks, and tightly bound the number of observations Eve makes in terms of the secret's entropy. We then show that if one-way functions do not exist, then an efficient Eve can learn to impersonate any efficient Bob nearly as well as an unbounded Eve.For the full version of this work see (Naor & Rothblum, 2006).

#index 876027
#* Online decoding of Markov models under latency constraints
#@ Mukund Narasimhan;Paul Viola;Michael Shilman
#t 2006
#c 19
#% 211044
#% 420060
#% 420064
#% 445344
#% 464434
#% 466892
#% 643004
#% 763414
#% 840856
#% 1250184
#% 1650403
#! The Viterbi algorithm is an efficient and optimal method for decoding linear-chain Markov Models. However, the entire input sequence must be observed before the labels for any time step can be generated, and therefore Viterbi cannot be directly applied to online/interactive/streaming scenarios without incurring significant (possibly unbounded) latency. A widely used approach is to break the input stream into fixed-size windows, and apply Viterbi to each window. Larger windows lead to higher accuracy, but result in higher latency.We propose several alternative algorithms to the fixed-sized window decoding approach. These approaches compute a certainty measure on predicted labels that allows us to trade off latency for expected accuracy dynamically, without having to choose a fixed window size up front. Not surprisingly, this more principled approach gives us a substantial improvement over choosing a fixed window. We show the effectiveness of the approach for the task of spotting semi-structured information in large documents. When compared to full Viterbi, the approach suffers a 0.1 percent error degradation with a average latency of 2.6 time steps (versus the potentially infinite latency of Viterbi). When compared to fixed windows Viterbi, we achieve a 40x reduction in error and 6x reduction in latency.

#index 876028
#* Learning hierarchical task networks by observation
#@ Negin Nejati;Pat Langley;Tolga Konik
#t 2006
#c 19
#% 57896
#% 150994
#% 356978
#% 368328
#% 723395
#% 732423
#% 961150
#% 1272286
#! Knowledge-based planning methods offer benefits over classical techniques, but they are time consuming and costly to construct. There has been research on learning plan knowledge from search, but this can take substantial computer time and may even fail to find solutions on complex tasks. Here we describe another approach that observes sequences of operators taken from expert solutions to problems and learns hierarchical task networks from them. The method has similarities to previous algorithms for explanation-based learning, but differs in its ability to acquire hierarchical structures and in the generality of learned conditions. These increase the method's capability to transfer learned knowledge to other problems and supports the acquisition of recursive procedures. After presenting the learning algorithm, we report experiments that compare its abilities to other techniques on two planning domains. In closing, we review related work and directions for future research.

#index 876029
#* Reinforcement learning for optimized trade execution
#@ Yuriy Nevmyvaka;Yi Feng;Michael Kearns
#t 2006
#c 19
#% 376266
#% 379009
#% 435523
#% 754154
#% 836270
#! We present the first large-scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets. Our experiments are based on 1.5 years of millisecond time-scale limit order data from NASDAQ, and demonstrate the promise of reinforcement learning methods to market microstructure problems. Our learning algorithm introduces and exploits a natural "low-impact" factorization of the state space.

#index 876030
#* Concept boundary detection for speeding up SVMs
#@ Navneet Panda;Edward Y. Chang;Gang Wu
#t 2006
#c 19
#% 190581
#% 209021
#% 269213
#% 269217
#% 310547
#% 420125
#% 466597
#% 479973
#% 722815
#% 729940
#% 1558464
#! Support Vector Machines (SVMs) suffer from an O(n2) training cost, where n denotes the number of training instances. In this paper, we propose an algorithm to select boundary instances as training data to substantially reduce n. Our proposed algorithm is motivated by the result of (Burges, 1999) that, removing non-support vectors from the training set does not change SVM training results. Our algorithm eliminates instances that are likely to be non-support vectors. In the concept-independent preprocessing step of our algorithm, we prepare nearest-neighbor lists for training instances. In the concept-specific sampling step, we can then effectively select useful training data for each target concept. Empirical studies show our algorithm to be effective in reducing n, outperforming other competing downsampling algorithms without significantly compromising testing accuracy.

#index 876031
#* The support vector decomposition machine
#@ Francisco Pereira;Geoffrey Gordon
#t 2006
#c 19
#% 236497
#% 425048
#% 757953
#% 768668
#% 828046
#% 1232015
#% 1558464
#! In machine learning problems with tens of thousands of features and only dozens or hundreds of independent training examples, dimensionality reduction is essential for good learning performance. In previous work, many researchers have treated the learning problem in two separate phases: first use an algorithm such as singular value decomposition to reduce the dimensionality of the data set, and then use a classification algorithm such as naïve Bayes or support vector machines to learn a classifier. We demonstrate that it is possible to combine the two goals of dimensionality reduction and classification into a single learning objective, and present a novel and efficient algorithm which optimizes this objective directly. We present experimental results in fMRI analysis which show that we can achieve better learning performance and lower-dimensional representations than two-phase approaches can.

#index 876032
#* An analytic solution to discrete Bayesian reinforcement learning
#@ Pascal Poupart;Nikos Vlassis;Jesse Hoey;Kevin Regan
#t 2006
#c 19
#% 135414
#% 183499
#% 266287
#% 284108
#% 366058
#% 466731
#% 715337
#% 840955
#% 1272075
#% 1289556
#% 1650283
#! Reinforcement learning (RL) was originally proposed as a framework to allow agents to learn in an online fashion as they interact with their environment. Existing RL algorithms come short of achieving this goal because the amount of exploration required is often too costly and/or too time consuming for online learning. As a result, RL is mostly used for offline learning in simulated environments. We propose a new algorithm, called BEETLE, for effective online learning that is computationally efficient while minimizing the amount of exploration. We take a Bayesian model-based approach, framing RL as a partially observable Markov decision process. Our two main contributions are the analytical derivation that the optimal value function is the upper envelope of a set of multivariate polynomials, and an efficient point-based value iteration algorithm that exploits this simple parameterization.

#index 876033
#* MISSL: multiple-instance semi-supervised learning
#@ Rouhollah Rahmani;Sally A. Goldman
#t 2006
#c 19
#% 272527
#% 464621
#% 465916
#% 565545
#% 578794
#% 770851
#% 771844
#% 812449
#% 840452
#% 840965
#% 840967
#% 1289496
#! There has been much work on applying multiple-instance (MI) learning to content-based image retrieval (CBIR) where the goal is to rank all images in a known repository using a small labeled data set. Most existing MI learning algorithms are non-transductive in that the images in the repository serve only as test data and are not used in the learning process. We present MISSL (Multiple-Instance Semi-Supervised Learning) that transforms any MI problem into an input for a graph-based single-instance semi-supervised learning method that encodes the MI aspects of the problem simultaneously working at both the bag and point levels. Unlike most prior MI learning algorithms, MISSL makes use of the unlabeled data.

#index 876034
#* Constructing informative priors using transfer learning
#@ Rajat Raina;Andrew Y. Ng;Daphne Koller
#t 2006
#c 19
#% 198058
#% 236495
#% 236497
#% 770804
#% 840962
#% 916788
#! Many applications of supervised learning require good generalization from limited labeled data. In the Bayesian setting, we can try to achieve this goal by using an informative prior over the parameters, one that encodes useful domain knowledge. Focusing on logistic regression, we present an algorithm for automatically constructing a multivariate Gaussian prior with a full covariance matrix for a given supervised learning task. This prior relaxes a commonly used but overly simplistic independence assumption, and allows parameters to be dependent. The algorithm uses other "similar" learning problems to estimate the covariance of pairs of individual parameters. We then use a semidefinite program to combine these estimates and learn a good prior for the current learning task. We apply our methods to binary text classification, and demonstrate a 20 to 40% test error reduction over a commonly used prior.

#index 876035
#* CN = CPCN
#@ Liva Ralaivola;François Denis;Christophe Nicolas Magnan
#t 2006
#c 19
#% 697
#% 180945
#% 451057
#% 465890
#% 593786
#% 656741
#% 961143
#! We address the issue of the learnability of concept classes under three classification noise models in the probably approximately correct framework. After introducing the Class-Conditional Classification Noise (CCCN) model, we investigate the problem of the learnability of concept classes under this particular setting and we show that concept classes that are learnable under the well-known uniform classification noise (CN) setting are also CCCN-learnable, which gives CN = CCCN. We then use this result to prove the equality between the set of concept classes that are CN-learnable and the set of concept classes that are learnable in the Constant Partition Classification Noise (CPCN) setting, or, in other words, we show that CN = CPCN.

#index 876036
#* Maximum margin planning
#@ Nathan D. Ratliff;J. Andrew Bagnell;Martin A. Zinkevich
#t 2006
#c 19
#% 1528
#% 78916
#% 363744
#% 770754
#% 770852
#% 840947
#% 1674795
#! Imitation learning of sequential, goal-directed behavior by standard supervised techniques is often difficult. We frame learning such behaviors as a maximum margin structured prediction problem over a space of policies. In this approach, we learn mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior. Further, we demonstrate a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference. Although the technique is general, it is particularly relevant in problems where A* and dynamic programming approaches make learning policies tractable in problems beyond the limitations of a QP formulation. We demonstrate our approach applied to route planning for outdoor mobile robots, where the behavior a designer wishes a planner to execute is often clear, while specifying cost functions that engender this behavior is a much more difficult task.

#index 876037
#* Quadratic programming relaxations for metric labeling and Markov random field MAP estimation
#@ Pradeep Ravikumar;John Lafferty
#t 2006
#c 19
#% 44876
#% 342116
#% 344568
#% 382586
#% 593940
#% 803614
#% 889176
#% 1810398
#% 1815753
#! Quadratic program relaxations are proposed as an alternative to linear program relaxations and tree reweighted belief propagation for the metric labeling or MAP estimation problem. An additional convex relaxation of the quadratic approximation is shown to have additive approximation guarantees that apply even when the graph weights have mixed sign or do not come from a metric. The approximations are extended in a manner that allows tight variational relaxations of the MAP problem, although they generally involve non-convex optimization. Experiments carried out on synthetic data show that the quadratic approximations can be more accurate and computationally efficient than the linear programming and propagation based alternatives.

#index 876038
#* Categorization in multiple category systems
#@ Jean-Michel Renders;Eric Gaussier;Cyril Goutte;Francois Pacull;Gabriela Csurka
#t 2006
#c 19
#% 132938
#% 252011
#% 344447
#% 457933
#% 458379
#% 824956
#% 1667696
#! We explore the situation in which documents have to be categorized into more than one category system, a situation we refer to as multiple-view categorization. More particularly, we address the case where two different categorizers have already been built based on non-necessarily identical training sets, each one labeled using one category system. On the top of these categorizers considered as black-boxes, we propose some algorithms able to exploit a third training set containing a few examples annotated in both category systems. Such a situation arises for example in large companies where incoming mails have to be routed to several departments, each one relying on its own category system. We focus here on exploiting possible dependencies between category systems in order to refine the categorization decisions made by categorizers trained independently on different category systems. After a description of the multiple categorization problem, we present several possible solutions, based either on a categorization or reweighting approach, and compare them on real data. Lastly, we show how the multimedia categorization problem can be cast as a multiple categorization problem and assess our methods in this framework.

#index 876039
#* How boosting the margin can also boost classifier complexity
#@ Lev Reyzin;Robert E. Schapire
#t 2006
#c 19
#% 26125
#% 235377
#% 266255
#% 299255
#% 302391
#% 563256
#% 646003
#% 1499573
#! Boosting methods are known not to usually overfit training data even as the size of the generated classifiers becomes large. Schapire et al. attempted to explain this phenomenon in terms of the margins the classifier achieves on training examples. Later, however, Breiman cast serious doubt on this explanation by introducing a boosting algorithm, arc-gv, that can generate a higher margins distribution than AdaBoost and yet performs worse. In this paper, we take a close look at Breiman's compelling but puzzling results. Although we can reproduce his main finding, we find that the poorer performance of arc-gv can be explained by the increased complexity of the base classifiers it uses, an explanation supported by our experiments and entirely consistent with the margins theory. Thus, we find maximizing the margins is desirable, but not necessarily at the expense of other factors, especially base-classifier complexity.

#index 876040
#* Combining discriminative features to infer complex trajectories
#@ David A. Ross;Simon Osindero;Richard S. Zemel
#t 2006
#c 19
#% 409857
#% 450888
#% 464434
#% 718442
#% 792896
#% 812352
#% 812582
#% 824957
#% 857094
#! We propose a new model for the probabilistic estimation of continuous state variables from a sequence of observations, such as tracking the position of an object in video. This mapping is modeled as a product of dynamics experts (features relating the state at adjacent time-steps) and observation experts (features relating the state to the image sequence). Individual features are flexible in that they can switch on or off at each time-step depending on their inferred relevance (or on additional side information), and discriminative in that they need not model the full generative likelihood of the data. When trained conditionally, this permits the inclusion of a broad range of rich features (for example, features relying on observations from multiple time-steps), and allows the relevance of features to be learned from labeled sequences.

#index 876041
#* Sequential update of ADtrees
#@ Josep Roure;Andrew W. Moore
#t 2006
#c 19
#% 129987
#% 458346
#% 466412
#% 483084
#% 577271
#% 770828
#% 1272326
#! Ingcreasingly, data-mining algorithms must deal with databases that continuously grow over time. These algorithms must avoid repeatedly scanning their databases. When database attributes are symbolic, ADtrees have already shown to be efficient structures to store sufficient statistics in main memory and to accelerate the mining process in batch environments. Here we present an efficient method to sequentially update ADtrees that is suitable for incremental environments.

#index 876042
#* Predictive linear-Gaussian models of controlled stochastic dynamical systems
#@ Matthew Rudary;Satinder Singh
#t 2006
#c 19
#% 17145
#% 677787
#% 788097
#% 840958
#! We introduce the controlled predictive linear-Gaussian model (cPLG), a model that uses predictive state to model discrete-time dynamical systems with real-valued observations and vector-valued actions. This extends the PLG, an uncontrolled model recently introduced by Rudary et al. (2005). We show that the cPLG subsumes controlled linear dynamical systems (LDS, also called Kalman filter models) of equal dimension, but requires fewer parameters. We also introduce the predictive linear-quadratic Gaussian problem, a cost-minimization problem based on the cPLG that we show is equivalent to linear-quadratic Gaussian problems (LQG, sometimes called LQR). We present an algorithm to estimate cPLG parameters from data, and show that our algorithm is a consistent estimation procedure. Finally, we present empirical results suggesting that our algorithm performs favorably compared to expectation maximization on controlled LDS models.

#index 876043
#* A statistical approach to rule learning
#@ Ulrich Rückert;Stefan Kramer
#t 2006
#c 19
#% 277919
#% 283138
#% 465922
#% 466744
#% 770809
#! We present a new, statistical approach to rule learning. Doing so, we address two of the problems inherent in traditional rule learning: The computational hardness of finding rule sets with low training error and the need for capacity control to avoid over-fitting. The chosen representation involves weights attached to rules. Instead of optimizing the error rate directly, we optimize for rule sets that have large margin and low variance. This can be formulated as a convex optimization problem allowing for efficient computation. Given the representation and the optimization procedure, we effectively yield weighted clauses in a CNF-like representation. To avoid overfitting, we propose a model selection strategy that utilizes a novel concentration inequality. Empirical tests show that the system is competitive with existing rule learning algorithms and that its flexible learning bias can be adjusted to improve predictive accuracy considerably.

#index 876044
#* Efficient inference on sequence segmentation models
#@ Sunita Sarawagi
#t 2006
#c 19
#% 722822
#% 840856
#% 874707
#% 939977
#! Sequence segmentation is a flexible and highly accurate mechanism for modeling several applications. Inference on segmentation models involves dynamic programming computations that in the worst case can be cubic in the length of a sequence. In contrast, typical sequence labeling models require linear time. We remove this limitation of segmentation models vis-a-vis sequential models by designing a succinct representation of potentials common across overlapping segments. We exploit such potentials to design efficient inference algorithms that are both analytically shown to have a lower complexity and empirically found to be comparable to sequential models for typical extraction tasks.

#index 876045
#* Cost-sensitive learning with conditional Markov networks
#@ Prithviraj Sen;Lise Getoor
#t 2006
#c 19
#% 280437
#% 342611
#% 420507
#% 453479
#% 464434
#% 770763
#% 800505
#% 837848
#% 1289281
#% 1650403
#% 1815596
#! There has been a recent, growing interest in classification and link prediction in structured domains. Methods such as CRFs (Lafferty et al., 2001) and RMNs (Taskar et al., 2002) support flexible mechanisms for modeling correlations due to the link structure. In addition, in many structured domains, there is an interesting structure in the risk or cost function associated with different misclassifications. There is a rich tradition of cost-sensitive learning applied to unstructured (IID) data. Here we propose a general framework which can capture correlations in the link structure and handle structured cost functions. We present a novel cost-sensitive structured classifier based on Maximum Entropy principles that directly determines the cost-sensitive classification. We contrast this with an approach which employs a standard 0/1 loss structured classifier followed by minimization of the expected cost of misclassification. We demonstrate the utility of our proposed classifier with experiments on both synthetic and real-world data.

#index 876046
#* Feature value acquisition in testing: a sequential batch test algorithm
#@ Victor S. Sheng;Charles X. Ling
#t 2006
#c 19
#% 92554
#% 136350
#% 160852
#% 280437
#% 342611
#% 376266
#% 447606
#% 464639
#% 477640
#% 735358
#% 770791
#% 785338
#% 785413
#% 829982
#% 1272282
#% 1272369
#% 1289281
#% 1499572
#% 1673023
#! In medical diagnosis, doctors often have to order sets of medical tests in sequence in order to make an accurate diagnosis of patient diseases. While doing so they have to make a trade-off between the cost of the tests and possible misdiagnosis. In this paper, we use cost-sensitive learning to model this process. We assume that test examples (new patients) may contain missing values, and their actual values can be acquired at cost (similar to doing medical tests) in order to reduce misclassification errors (misdiagnosis). We propose a novel Sequential Batch Test algorithm that can acquire sets of attribute values in sequence, similar to sets of medical tests ordered by doctors in sequence. The goal of our algorithm is to minimize the total cost (i.e., the trade-off) of acquiring attribute values and misclassifications. We demonstrate the effectiveness of our algorithm, and show that it outperforms previous methods significantly. Our algorithm can be readily applied in real-world diagnosis tasks. A case study on the heart disease is given in the paper.

#index 876047
#* Permutation invariant SVMs
#@ Pannagadatta K. Shivaswamy;Tony Jebara
#t 2006
#c 19
#% 25998
#% 190581
#% 197394
#% 213165
#% 420077
#% 476872
#% 722803
#% 722810
#% 770868
#% 778279
#! We extend Support Vector Machines to input spaces that are sets by ensuring that the classifier is invariant to permutations of sub-elements within each input. Such permutations include reordering of scalars in an input vector, re-orderings of tuples in an input matrix or re-orderings of general objects (in Hilbert spaces) within a set as well. This approach induces permutational invariance in the classifier which can then be directly applied to unusual set-based representations of data. The permutation invariant Support Vector Machine alternates the Hungarian method for maximum weight matching within the maximum margin learning procedure. We effectively estimate and apply permutations to the input data points to maximize classification margin while minimizing data radius. This procedure has a strong theoretical justification via well established error probability bounds. Experiments are shown on character recognition, 3D object recognition and various UCI datasets.

#index 876048
#* Bayesian learning of measurement and structural models
#@ Ricardo Silva;Richard Scheines
#t 2006
#c 19
#% 770861
#% 840936
#% 859292
#% 861853
#% 961141
#! We present a Bayesian search algorithm for learning the structure of latent variable models of continuous variables. We stress the importance of applying search operators designed especially for the parametric family used in our models. This is performed by searching for subsets of the observed variables whose covariance matrix can be represented as a sum of a matrix of low rank and a diagonal matrix of residuals. The resulting search procedure is relatively efficient, since the main search operator has a branch factor that grows linearly with the number of variables. The resulting models are often simpler and give a better fit than models based on generalizations of factor analysis or those derived from standard hill-climbing methods.

#index 876049
#* An intrinsic reward mechanism for efficient exploration
#@ Özgür Şimşek;Andrew G. Barto
#t 2006
#c 19
#% 90041
#% 103431
#% 157736
#% 160859
#% 286423
#% 464303
#% 464636
#% 466075
#% 647111
#% 715337
#% 770775
#% 770777
#% 840937
#! How should a reinforcement learning agent act if its sole purpose is to efficiently learn an optimal policy for later use? In other words, how should it explore, to be able to exploit later? We formulate this problem as a Markov Decision Process by explicitly modeling the internal state of the agent and propose a principled heuristic for its solution. We present experimental results in a number of domains, also exploring the algorithm's use for learning a policy for a skill given its reward function---an important but neglected component of skill discovery.

#index 876050
#* Deterministic annealing for semi-supervised kernel machines
#@ Vikas Sindhwani;S. Sathiya Keerthi;Olivier Chapelle
#t 2006
#c 19
#% 25442
#% 304876
#% 443948
#% 466263
#% 875968
#% 879624
#! An intuitive approach to utilizing unlabeled data in kernel-based classification algorithms is to simply treat unknown labels as additional optimization variables. For margin-based loss functions, one can view this approach as attempting to learn low-density separators. However, this is a hard optimization problem to solve in typical semi-supervised settings where unlabeled data is abundant. The popular Transductive SVM algorithm is a label-switching-retraining procedure that is known to be susceptible to local minima. In this paper, we present a global optimization framework for semi-supervised Kernel machines where an easier problem is parametrically deformed to the original hard problem and minimizers are smoothly tracked. Our approach is motivated from deterministic annealing techniques and involves a sequence of convex optimization problems that are exactly and efficiently solved. We present empirical results on several synthetic and real world datasets that demonstrate the effectiveness of our approach.

#index 876051
#* Feature subset selection bias for classification learning
#@ Surendra K. Singhi;Huan Liu
#t 2006
#c 19
#% 272995
#% 290482
#% 307109
#% 425048
#% 464449
#% 580511
#% 722929
#% 722935
#% 729437
#% 770847
#% 796212
#! Feature selection is often applied to high-dimensional data prior to classification learning. Using the same training dataset in both selection and learning can result in so-called feature subset selection bias. This bias putatively can exacerbate data over-fitting and negatively affect classification performance. However, in current practice separate datasets are seldom employed for selection and learning, because dividing the training data into two datasets for feature selection and classifier learning respectively reduces the amount of data that can be used in either task. This work attempts to address this dilemma. We formalize selection bias for classification learning, analyze its statistical properties, and study factors that affect selection bias, as well as how the bias impacts classification learning via various experiments. This research endeavors to provide illustration and explanation why the bias may not cause negative impact in classification as much as expected in regression.

#index 876052
#* Classifying EEG for brain-computer interfaces: learning optimal filters for dynamical system features
#@ Le Song;Julien Epps
#t 2006
#c 19
#% 840893
#! Classification of multichannel EEG recordings during motor imagination has been exploited successfully for brain-computer interfaces (BCI). In this paper, we consider EEG signals as the outputs of a networked dynamical system (the cortex), and exploit novel features from the collective dynamics of the system for classification. Herein, we also propose a new framework for learning optimal filters automatically from the data, by employing a Fisher ratio criterion. Experimental evaluations comparing the proposed dynamical system features with the CSP and the AR features reveal their competitive performance during classification. Results also show the benefits of employing the spatial and the temporal filters optimized using the proposed learning approach.

#index 876053
#* An investigation of computational and informational limits in Gaussian mixture clustering
#@ Nathan Srebro;Gregory Shakhnarovich;Sam Roweis
#t 2006
#c 19
#% 338392
#% 527854
#% 593926
#% 771391
#% 785121
#% 1705530
#% 1705531
#! We investigate under what conditions clustering by learning a mixture of spherical Gaussians is (a) computationally tractable; and (b) statistically possible. We show that using principal component projection greatly aids in recovering the clustering using EM; present empirical evidence that even using such a projection, there is still a large gap between the number of samples needed to recover the clustering using EM, and the number of samples needed without computational restrictions; and characterize the regime in which such a gap exists.

#index 876054
#* Bayesian pattern ranking for move prediction in the game of Go
#@ David Stern;Ralf Herbrich;Thore Graepel
#t 2006
#c 19
#% 322884
#% 341865
#% 348582
#% 715096
#! We investigate the problem of learning to predict moves in the board game of Go from game records of expert players. In particular, we obtain a probability distribution over legal moves for professional play in a given position. This distribution has numerous applications in computer Go, including serving as an efficient stand-alone Go player. It would also be effective as a move selector and move sorter for game tree search and as a training tool for Go players. Our method has two major components: a) a pattern extraction scheme for efficiently harvesting patterns of given size and shape from expert game records and b) a Bayesian learning algorithm (in two variants) that learns a distribution over the values of a move given a board position based on the local pattern context. The system is trained on 181,000 expert games and shows excellent prediction performance as indicated by its ability to perfectly predict the moves made by professional Go players in 34% of test positions.

#index 876055
#* PAC model-free reinforcement learning
#@ Alexander L. Strehl;Lihong Li;Eric Wiewiora;John Langford;Michael L. Littman
#t 2006
#c 19
#% 124691
#% 162953
#% 181627
#% 305085
#% 384911
#% 425075
#% 722895
#% 763696
#% 840942
#! For a Markov Decision Process with finite state (size S) and action spaces (size A per state), we propose a new algorithm---Delayed Q-Learning. We prove it is PAC, achieving near optimal performance except for Õ(SA) timesteps using O(SA) space, improving on the Õ(S2 A) bounds of best previous algorithms. This result proves efficient reinforcement learning is possible without learning a model of the MDP from experience. Learning takes place from a single continuous thread of experience---no resets nor parallel sampling is used. Beyond its smaller storage and experience requirements, Delayed Q-learning's per-experience computation cost is much less than that of previous PAC algorithms.

#index 876056
#* Experience-efficient learning in associative bandit problems
#@ Alexander L. Strehl;Chris Mesterharm;Michael L. Littman;Haym Hirsh
#t 2006
#c 19
#% 697
#% 164838
#% 170386
#% 376266
#% 465893
#% 562935
#% 727925
#% 1289281
#! We formalize the associative bandit problem framework introduced by Kaelbling as a learning-theory problem. The learning environment is modeled as a k-armed bandit where arm payoffs are conditioned on an observable input selected on each trial. We show that, if the payoff functions are constrained to a known hypothesis class, learning can be performed efficiently with respect to the VC dimension of this class. We formally reduce the problem of PAC classification to the associative bandit problem, producing an efficient algorithm for any hypothesis class for which efficient classification algorithms are known. We demonstrate the approach empirically on a scalable concept class.

#index 876057
#* Full Bayesian network classifiers
#@ Jiang Su;Harry Zhang
#t 2006
#c 19
#% 44876
#% 107414
#% 129987
#% 136350
#% 197387
#% 246832
#% 269218
#% 277480
#% 290482
#% 400980
#% 580510
#% 799040
#% 856251
#% 1650705
#% 1650783
#% 1650785
#! The structure of a Bayesian network (BN) encodes variable independence. Learning the structure of a BN, however, is typically of high computational complexity. In this paper, we explore and represent variable independence in learning conditional probability tables (CPTs), instead of in learning structure. A full Bayesian network is used as the structure and a decision tree is learned for each CPT. The resulting model is called full Bayesian network classifiers (FBCs). In learning an FBC, learning the decision trees for CPTs captures essentially both variable independence and context-specific independence. We present a novel, efficient decision tree learning, which is also effective in the context of FBC learning. In our experiments, the FBC learning algorithm demonstrates better performance in both classification and ranking compared with other state-of-the-art learning algorithms. In addition, its reduced effort on structure learning makes its time complexity quite low as well.

#index 876058
#* Local Fisher discriminant analysis for supervised dimensionality reduction
#@ Masashi Sugiyama
#t 2006
#c 19
#% 80995
#% 578414
#% 593047
#! Dimensionality reduction is one of the important preprocessing steps in high-dimensional data analysis. In this paper, we consider the supervised dimensionality reduction problem where samples are accompanied with class labels. Traditional Fisher discriminant analysis is a popular and powerful method for this purpose. However, it tends to give undesired results if samples in some class form several separate clusters, i.e., multimodal. In this paper, we propose a new dimensionality reduction method called local Fisher discriminant analysis (LFDA), which is a localized variant of Fisher discriminant analysis. LFDA takes local structure of the data into account so the multimodal data can be embedded appropriately. We also show that LFDA can be extended to non-linear dimensionality reduction scenarios by the kernel trick.

#index 876059
#* Iterative RELIEF for feature weighting
#@ Yijun Sun;Jian Li
#t 2006
#c 19
#% 169659
#% 465583
#% 770774
#% 833529
#! We propose a series of new feature weighting algorithms, all stemming from a new interpretation of RELIEF as an online algorithm that solves a convex optimization problem with a margin-based objective function. The new interpretation explains the simplicity and effectiveness of RELIEF, and enables us to identify some of its weaknesses. We offer an analytic solution to mitigate these problems. We extend the newly proposed algorithm to handle multiclass problems by using a new multiclass margin definition. To reduce computational costs, an online learning algorithm is also developed. Convergence theorems of the proposed algorithms are presented. Some experiments based on the UCI and microarray datasets are performed to demonstrate the effectiveness of the proposed algorithms.

#index 876060
#* Multiclass reduced-set support vector machines
#@ Benyang Tang;Dominic Mazzoni
#t 2006
#c 19
#% 269218
#% 420077
#% 422115
#% 1702640
#% 1860543
#% 1860941
#% 1861425
#! There are well-established methods for reducing the number of support vectors in a trained binary support vector machine, often with minimal impact on accuracy. We show how reduced-set methods can be applied to multiclass SVMs made up of several binary SVMs, with significantly better results than reducing each binary SVM independently. Our approach is based on Burges' approach that constructs each reduced-set vector as the pre-image of a vector in kernel space, but we extend this by recomputing the SVM weights and bias optimally using the original SVM objective function. This leads to greater accuracy for a binary reduced-set SVM, and also allows vectors to be "shared" between multiple binary SVMs for greater multiclass accuracy with fewer reduced-set vectors. We also propose computing pre-images using differential evolution, which we have found to be more robust than gradient descent alone. We show experimental results on a variety of problems and find that this new approach is consistently better than previous multiclass reduced-set methods, sometimes with a dramatic difference.

#index 876061
#* Fast and space efficient string kernels using suffix arrays
#@ Choon Hui Teo;S. V. N. Vishwanathan
#t 2006
#c 19
#% 115467
#% 143306
#% 235941
#% 351094
#% 722803
#% 751623
#% 987672
#! String kernels which compare the set of all common substrings between two given strings have recently been proposed by Vishwanathan & Smola (2004). Surprisingly, these kernels can be computed in linear time and linear space using annotated suffix trees. Even though, in theory, the suffix tree based algorithm requires O(n) space for an n length string, in practice at least 40n bytes are required -- 20n bytes for storing the suffix tree, and an additional 20n bytes for the annotation. This large memory requirement coupled with poor locality of memory access, inherent due to the use of suffix trees, means that the performance of the suffix tree based algorithm deteriorates on large strings. In this paper, we describe a new linear time yet space efficient and scalable algorithm for computing string kernels, based on suffix arrays. Our algorithm is a) faster and easier to implement, b) on the average requires only 19n bytes of storage, and c) exhibits strong locality of memory access. We show that our algorithm can be extended to perform linear time prediction on a test string, and present experiments to validate our claims.

#index 876062
#* Bayesian regression with input noise for high dimensional data
#@ Jo-Anne Ting;Aaron D'Souza;Stefan Schaal
#t 2006
#c 19
#% 40640
#% 229931
#% 702350
#% 770780
#! This paper examines high dimensional regression with noise-contaminated input and output data. Goals of such learning problems include optimal prediction with noiseless query points and optimal system identification. As a first step, we focus on linear regression methods, since these can be easily cast into nonlinear learning problems with locally weighted learning approaches. Standard linear regression algorithms generate biased regression estimates if input noise is present and suffer numerically when the data contains redundancy and irrelevancy. Inspired by Factor Analysis Regression, we develop a variational Bayesian algorithm that is robust to ill-conditioned data, automatically detects relevant features, and identifies input and output noise -- all in a computationally efficient way. We demonstrate the effectiveness of our techniques on synthetic data and on a system identification task for a rigid body dynamics model of a robotic vision head. Our algorithm performs 10 to 70% better than previously suggested methods.

#index 876063
#* Probabilistic inference for solving discrete and continuous state Markov Decision Processes
#@ Marc Toussaint;Amos Storkey
#t 2006
#c 19
#% 246836
#% 366058
#% 496267
#% 715096
#% 716892
#% 1272002
#% 1272356
#% 1289564
#% 1290041
#% 1650589
#! Inference in Markov Decision Processes has recently received interest as a means to infer goals of an observed action, policy recognition, and also as a tool to compute policies. A particularly interesting aspect of the approach is that any existing inference technique in DBNs now becomes available for answering behavioral question--including those on continuous, factorial, or hierarchical state representations. Here we present an Expectation Maximization algorithm for computing optimal policies. Unlike previous approaches we can show that this actually optimizes the discounted expected future return for arbitrary reward functions and without assuming an ad hoc finite total time. The algorithm is generic in that any inference technique can be utilized in the E-step. We demonstrate this for exact inference on a discrete maze and Gaussian belief state propagation in continuous stochastic optimal control problems.

#index 876064
#* Clustering graphs by weighted substructure mining
#@ Koji Tsuda;Taku Kudo
#t 2006
#c 19
#% 629708
#% 729938
#% 785396
#% 814009
#! Graph data is getting increasingly popular in, e.g., bioinformatics and text processing. A main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of indicators of all possible subgraphs, the dimensionality gets too large for usual statistical methods. We propose an efficient method for learning a binomial mixture model in this feature space. Combining the l1 regularizer and the data structure called DFS code tree, the MAP estimate of non-zero parameters are computed efficiently by means of the EM algorithm. Our method is applied to the clustering of RNA graphs, and is compared favorably with graph kernels and the spectral graph distance.

#index 876065
#* Active sampling for detecting irrelevant features
#@ Sriharsha Veeramachaneni;Emanuele Olivetti;Paolo Avesani
#t 2006
#c 19
#% 116165
#% 132697
#% 466887
#% 629616
#% 1272282
#% 1673023
#% 1673582
#! The general approach for automatically driving data collection using information from previously acquired data is called active learning. Traditional active learning addresses the problem of choosing the unlabeled examples for which the class labels are queried with the goal of learning a classifier. In contrast we address the problem of active feature sampling for detecting useless features. We propose a strategy to actively sample the values of new features on class-labeled examples, with the objective of feature relevance assessment. We derive an active feature sampling algorithm from an information theoretic and statistical formulation of the problem. We present experimental results on synthetic, UCI and real world datasets to demonstrate that our active sampling algorithm can provide accurate estimates of feature relevance with lower data acquisition costs than random sampling and other previously proposed sampling algorithms.

#index 876066
#* Accelerated training of conditional random fields with stochastic gradient methods
#@ S. V. N. Vishwanathan;Nicol N. Schraudolph;Mark W. Schmidt;Kevin P. Murphy
#t 2006
#c 19
#% 169353
#% 299914
#% 344568
#% 450291
#% 464434
#% 580307
#% 816181
#% 853697
#% 854636
#% 1223727
#% 1223735
#% 1562516
#% 1927565
#! We apply Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, to the training of Conditional Random Fields (CRFs). On several large data sets, the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS, the leading method reported to date. We report results for both exact and inexact inference techniques.

#index 876067
#* Topic modeling: beyond bag-of-words
#@ Hanna M. Wallach
#t 2006
#c 19
#% 722904
#! Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the "bag-of-words" assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful.

#index 876068
#* Label propagation through linear neighborhoods
#@ Fei Wang;Changshui Zhang
#t 2006
#c 19
#% 466263
#! A novel semi-supervised learning approach is proposed based on a linear neighborhood model, which assumes that each data point can be linearly reconstructed from its neighborhood. Our algorithm, named Linear Neighborhood Propagation (LNP), can propagate the labels from the labeled points to the whole dataset using these linear neighborhoods with sufficient smoothness. We also derive an easy way to extend LNP to out-of-sample data. Promising experimental results are presented for synthetic data, digit and text classification tasks.

#index 876069
#* Two-dimensional solution path for support vector regression
#@ Gang Wang;Dit-Yan Yeung;Frederick H. Lochovsky
#t 2006
#c 19
#% 190581
#% 771845
#% 793245
#% 872759
#! Recently, a very appealing approach was proposed to compute the entire solution path for support vector classification (SVC) with very low extra computational cost. This approach was later extended to a support vector regression (SVR) model called ε-SVR. However, the method requires that the error parameter ε be set a priori, which is only possible if the desired accuracy of the approximation can be specified in advance. In this paper, we show that the solution path for ε-SVR is also piecewise linear with respect to ε. We further propose an efficient algorithm for exploring the two-dimensional solution space defined by the regularization and error parameters. As opposed to the algorithm for SVC, our proposed algorithm for ε-SVR initializes the number of support vectors to zero and then increases it gradually as the algorithm proceeds. As such, a good regression function possessing the sparseness property can be obtained after only a few iterations.

#index 876070
#* Totally corrective boosting algorithms that maximize the margin
#@ Manfred K. Warmuth;Jun Liao;Gunnar Rätsch
#t 2006
#c 19
#% 235377
#% 266255
#% 276509
#% 276511
#% 299255
#% 302391
#% 331916
#% 451055
#% 565538
#% 722762
#% 793251
#% 916798
#! We consider boosting algorithms that maintain a distribution over a set of examples. At each iteration a weak hypothesis is received and the distribution is updated. We motivate these updates as minimizing the relative entropy subject to linear constraints. For example AdaBoost constrains the edge of the last hypothesis w.r.t. the updated distribution to be at most γ = 0. In some sense, AdaBoost is "corrective" w.r.t. the last hypothesis. A cleaner boosting method is to be "totally corrective": the edges of all past hypotheses are constrained to be at most γ, where γ is suitably adapted.Using new techniques, we prove the same iteration bounds for the totally corrective algorithms as for their corrective versions. Moreover with adaptive γ, the algorithms provably maximizes the margin. Experimentally, the totally corrective versions return smaller convex combinations of weak hypotheses than the corrective ones and are competitive with LPBoost, a totally corrective boosting algorithm with no regularization, for which there is no iteration bound known.

#index 876071
#* Inference with the Universum
#@ Jason Weston;Ronan Collobert;Fabian Sinz;Léon Bottou;Vladimir Vapnik
#t 2006
#c 19
#% 116149
#% 190581
#% 191866
#% 229507
#% 493092
#% 763708
#% 837668
#! In this paper we study a new framework introduced by Vapnik (1998) and Vapnik (2006) that is an alternative capacity concept to the large margin approach. In the particular case of binary classification, we are given a set of labeled examples, and a collection of "non-examples" that do not belong to either class of interest. This collection, called the Universum, allows one to encode prior knowledge by representing meaningful concepts in the same domain as the problem at hand. We describe an algorithm to leverage the Universum by maximizing the number of observed contradictions, and show experimentally that this approach delivers accuracy improvements over using labeled data alone.

#index 876072
#* Kernel Predictive Linear Gaussian models for nonlinear stochastic dynamical systems
#@ David Wingate;Satinder Singh
#t 2006
#c 19
#% 361100
#% 724162
#% 743284
#% 1759704
#! The recent Predictive Linear Gaussian model (or PLG) improves upon traditional linear dynamical system models by using a predictive representation of state, which makes consistent parameter estimation possible without any loss of modeling power and while using fewer parameters. In this paper we extend the PLG to model stochastic, nonlinear dynamical systems by using kernel methods. With a Gaussian kernel, the model admits closed form solutions to the state update equations due to conjugacy between the dynamics and the state representation. We also explore an efficient sigma-point approximation to the state updates, and show how all of the model parameters can be learned directly from data (and can be learned on-line with the Kernel Recursive Least-Squares algorithm). We empirically compare the model and its approximation to the original PLG and discuss their relative advantages.

#index 876073
#* Predictive state representations with options
#@ Britton Wolfe;Satinder Singh
#t 2006
#c 19
#% 286423
#% 466066
#% 720089
#% 770781
#% 788097
#% 840958
#% 1289468
#% 1650589
#! Recent work on predictive state representation (PSR) models has focused on using predictions of the outcomes of open-loop action sequences as state. These predictions answer questions of the form "What is the probability of seeing observation sequence o1, o2, ..., oN if the agent takes action sequence a1, a2, ..., aN from some given history?" We would like to ask more expressive questions in our representation of state, such as "If I behave according to some policy until I terminate, what will be my last observation?" We extend the linear PSR framework to answer questions like these about options -- temporally extended, closed-loop courses of action -- bounding the size of the linear PSR needed to model questions about a certain class of options. We introduce a hierarchical PSR (HPSR) that can make predictions about both options and primitive action sequences and show empirical results from learning HPSRs in simple domains.

#index 876074
#* Fast time series classification using numerosity reduction
#@ Xiaopeng Xi;Eamonn Keogh;Christian Shelton;Li Wei;Chotirat Ann Ratanamahatana
#t 2006
#c 19
#% 218435
#% 316709
#% 465760
#% 572113
#% 737331
#% 779029
#% 783521
#% 799396
#% 800574
#% 824705
#% 844343
#% 853064
#% 940369
#% 993965
#% 1702634
#% 1705293
#! Many algorithms have been proposed for the problem of time series classification. However, it is clear that one-nearest-neighbor with Dynamic Time Warping (DTW) distance is exceptionally difficult to beat. This approach has one weakness, however; it is computationally too demanding for many realtime applications. One way to mitigate this problem is to speed up the DTW calculations. Nonetheless, there is a limit to how much this can help. In this work, we propose an additional technique, numerosity reduction, to speed up one-nearest-neighbor DTW. While the idea of numerosity reduction for nearest-neighbor classifiers has a long history, we show here that we can leverage off an original observation about the relationship between dataset size and DTW constraints to produce an extremely compact dataset with little or no loss in accuracy. We test our ideas with a comprehensive set of experiments, and show that it can efficiently produce extremely fast accurate classifiers.

#index 876075
#* A duality view of spectral methods for dimensionality reduction
#@ Lin Xiao;Jun Sun;Stephen Boyd
#t 2006
#c 19
#% 209961
#% 266426
#% 593047
#% 723241
#% 757953
#% 770839
#% 787101
#% 790049
#% 840933
#% 1502529
#% 1759699
#! We present a unified duality view of several recently emerged spectral methods for nonlinear dimensionality reduction, including Isomap, locally linear embedding, Laplacian eigenmaps, and maximum variance unfolding. We discuss the duality theory for the maximum variance unfolding problem, and show that other methods are directly related to either its primal formulation or its dual formulation, or can be interpreted from the optimality conditions. This duality framework reveals close connections between these seemingly quite different algorithms. In particular, it resolves the myth about these methods in using either the top eigenvectors of a dense matrix, or the bottom eigenvectors of a sparse matrix --- these two eigenspaces are exactly aligned at primal-dual optimality.

#index 876076
#* Bayesian multi-population haplotype inference via a hierarchical dirichlet process mixture
#@ Eric P. Xing;Kyung-Ah Sohn;Michael I. Jordan;Yee-Whye Teh
#t 2006
#c 19
#% 742454
#% 770845
#! Uncovering the haplotypes of single nucleotide polymorphisms and their population demography is essential for many biological and medical applications. Methods for haplotype inference developed thus far---including methods based on coalescence, finite and infinite mixtures, and maximal parsimony---ignore the underlying population structure in the genotype data. As noted by Pritchard (2001), different populations can share certain portion of their genetic ancestors, as well as have their own genetic components through migration and diversification. In this paper, we address the problem of multi-population haplotype inference. We capture cross-population structure using a nonparametric Bayesian prior known as the hierarchical Dirichlet process (HDP) (Teh et al., 2006), conjoining this prior with a recently developed Bayesian methodology for haplotype phasing known as DP-Haplotyper (Xing et al., 2004). We also develop an efficient sampling algorithm for the HDP based on a two-level nested Pólya urn scheme. We show that our model outperforms extant algorithms on both simulated and real biological data.

#index 876077
#* Discriminative unsupervised learning of structured predictors
#@ Linli Xu;Dana Wilkinson;Finnegan Southey;Dale Schuurmans
#t 2006
#c 19
#% 252472
#% 279755
#% 389155
#% 464434
#% 722816
#% 757953
#% 770763
#% 1269502
#! We present a new unsupervised algorithm for training structured predictors that is discriminative, convex, and avoids the use of EM. The idea is to formulate an unsupervised version of structured learning methods, such as maximum margin Markov networks, that can be trained via semidefinite programming. The result is a discriminative training criterion for structured predictors (like hidden Markov models) that remains unsupervised and does not create local minima. To reduce training cost, we reformulate the training procedure to mitigate the dependence on semidefinite programming, and finally propose a heuristic procedure that avoids semidefinite programming entirely. Experimental results show that the convex discriminative procedure can produce better conditional models than conventional Baum-Welch (EM) training.

#index 876078
#* Semi-supervised nonlinear dimensionality reduction
#@ Xin Yang;Haoying Fu;Hongyuan Zha;Jesse Barlow
#t 2006
#c 19
#% 216483
#% 723241
#% 790049
#% 812416
#! The problem of nonlinear dimensionality reduction is considered. We focus on problems where prior information is available, namely, semi-supervised dimensionality reduction. It is shown that basic nonlinear dimensionality reduction algorithms, such as Locally Linear Embedding (LLE), Isometric feature mapping (ISOMAP), and Local Tangent Space Alignment (LTSA), can be modified by taking into account prior information on exact mapping of certain data points. The sensitivity analysis of our algorithms shows that prior information will improve stability of the solution. We also give some insight on what kind of prior information best improves the solution. We demonstrate the usefulness of our algorithm by synthetic and real life examples.

#index 876079
#* Null space versus orthogonal linear discriminant analysis
#@ Jieping Ye;Tao Xiong
#t 2006
#c 19
#% 49252
#% 80995
#% 212689
#% 235342
#% 627829
#% 729437
#% 789030
#% 791368
#% 1828409
#% 1861142
#! Dimensionality reduction is an important pre-processing step for many applications. Linear Discriminant Analysis (LDA) is one of the well known methods for supervised dimensionality reduction. However, the classical LDA formulation requires the nonsingularity of scatter matrices involved. For undersampled problems, where the data dimension is much larger than the sample size, all scatter matrices are singular and classical LDA fails. Many extensions, including null space based LDA (NLDA), orthogonal LDA (OLDA), etc, have been proposed in the past to overcome this problem. In this paper, we present a computational and theoretical analysis of NLDA and OLDA. Our main result shows that under a mild condition which holds in many applications involving high-dimensional data, NLDA is equivalent to OLDA. We have performed extensive experiments on various types of data and results are consistent with our theoretical analysis. The presented analysis and experimental results provide further insight into several LDA based algorithms.

#index 876080
#* Active learning via transductive experimental design
#@ Kai Yu;Jinbo Bi;Volker Tresp
#t 2006
#c 19
#% 132697
#% 187651
#% 236729
#% 311027
#% 642998
#% 714351
#% 722943
#% 735256
#% 757953
#% 840868
#% 1272282
#% 1650569
#! This paper considers the problem of selecting the most informative experiments x to get measurements y for learning a regression model y = f(x). We propose a novel and simple concept for active learning, transductive experimental design, that explores available unmeasured experiments (i.e., unlabeled data) and has a better scalability in comparison with classic experimental design methods. Our in-depth analysis shows that the new method tends to favor experiments that are on the one side hard-to-predict and on the other side representative for the rest of the experiments. Efficient optimization of the new design problem is achieved through alternating optimization and sequential greedy search. Extensive experimental results on synthetic problems and three real-world tasks, including questionnaire design for preference learning, active learning for text categorization, and spatial sensor placement, highlight the advantages of the proposed approaches.

#index 876081
#* Collaborative ordinal regression
#@ Shipeng Yu;Kai Yu;Volker Tresp;Hans-Peter Kriegel
#t 2006
#c 19
#% 309095
#% 715096
#% 840846
#% 840852
#% 840924
#% 840962
#% 891549
#% 1272396
#! Ordinal regression has become an effective way of learning user preferences, but most research focuses on single regression problems. In this paper we introduce collaborative ordinal regression, where multiple ordinal regression tasks are handled simultaneously. Rather than modeling each task individually, we explore the dependency between ranking functions through a hierarchical Bayesian model and assign a common Gaussian Process (GP) prior to all individual functions. Empirical studies show that our collaborative model outperforms the individual counterpart in preference learning applications.

#index 876082
#* Block-quantized kernel matrix for fast spectral embedding
#@ Kai Zhang;James T. Kwok
#t 2006
#c 19
#% 54221
#% 266426
#% 304932
#% 313959
#% 338442
#% 564285
#% 722815
#% 722887
#% 732552
#% 812619
#% 916799
#! Eigendecomposition of kernel matrix is an indispensable procedure in many learning and vision tasks. However, the cubic complexity O(N3) is impractical for large problem, where N is the data size. In this paper, we propose an efficient approach to solve the eigendecomposition of the kernel matrix W. The idea is to approximate W with W that is composed of m2 constant blocks. The eigenvectors of W, which can be solved in O(m3) time, is then used to recover the eigenvectors of the original kernel matrix. The complexity of our method is only O(mN + m3), which scales more favorably than state-of-the-art low rank approximation and sampling based approaches (O(m2N + m3)), and the approximation quality can be controlled conveniently. Our method demonstrates encouraging scaling behaviors in experiments of image segmentation (by spectral clustering) and kernel principal component analysis.

#index 876083
#* Statistical debugging: simultaneous identification of multiple bugs
#@ Alice X. Zheng;Michael I. Jordan;Ben Liblit;Mayur Naik;Alex Aiken
#t 2006
#c 19
#% 231941
#% 342621
#% 581051
#% 807094
#% 809099
#% 823217
#! We describe a statistical approach to software debugging in the presence of multiple bugs. Due to sparse sampling issues and complex interaction between program predicates, many generic off-the-shelf algorithms fail to select useful bug predictors. Taking inspiration from bi-clustering algorithms, we propose an iterative collective voting scheme for the program runs and predicates. We demonstrate successful debugging results on several real world programs and a large debugging benchmark suite.

#index 876084
#* Efficient lazy elimination for averaged one-dependence estimators
#@ Fei Zheng;Geoffrey I. Webb
#t 2006
#c 19
#% 246832
#% 290482
#% 312728
#% 321059
#% 458259
#% 486328
#% 486797
#% 799040
#% 1269504
#% 1674146
#% 1699581
#! Semi-naive Bayesian classifiers seek to retain the numerous strengths of naive Bayes while reducing error by relaxing the attribute independence assumption. Backwards Sequential Elimination (BSE) is a wrapper technique for attribute elimination that has proved effective at this task. We explore a new technique, Lazy Elimination (LE), which eliminates highly related attribute-values at classification time without the computational overheads inherent in wrapper techniques. We analyze the effect of LE and BSE on a state-of-the-art semi-naive Bayesian algorithm Averaged One-Dependence Estimators (AODE). Our experiments show that LE significantly reduces bias and error without undue computation, while BSE significantly reduces bias but not error, with high training time complexity. In the context of AODE, LE has a significant advantage over BSE in both computational efficiency and error.

#index 983803
#* Proceedings of the 24th international conference on Machine learning
#@ Zoubin Ghahramani
#t 2007
#c 19
#! This volume contains the papers accepted to the 24th International Conference on Machine Learning (ICML 2007), which was held at Oregon State University in Corvalis, Oregon, from June 20th to 24th, 2007. ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at: http://www.machinelearning.org. This year there were 522 submissions to ICML. There was a very thorough review process, in which each paper was reviewed by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. For the first time this year there were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Also for the first time, authors were asked to submit a list of changes with their final accepted papers, which was checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 150 papers were accepted to ICML this year, including a very small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 29%. ICML attracts submissions from machine learning researchers around the globe. The 150 accepted papers this year were geographically distributed as follows: 66 papers had a first author from the US, 32 from Europe, 19 from China or Hong Kong, 11 from Canada, 6 from India, 5 each from Australia and Japan, 3 from Israel, and 1 each from Korea, Russia and Taiwan. In addition to the main program of accepted papers, which includes both a talk and poster presentation for each paper, the ICML program included 3 workshops and 8 tutorials on machine learning topics which are currently of broad interest. We were also extremely pleased to have David Heckerman (Microsoft Research), Joshua Tenenbaum (Massachussetts Institute of Technology), and Bernhard Schölkopf (Max Planck Institute for Biological Cybernetics) as the invited speakers this year. Thanks to sponsorship by the Machine Learning Journal, we were able to award a number of outstanding student paper prizes. We were fortunate this year that ICML was co-located with the International Conference on Inductive Logic Programming (ILP 2007). ICML and ILP held joint sessions on the first day of ICML 2007.

#index 983804
#* Quantum clustering algorithms
#@ Esma Aïmeur;Gilles Brassard;Sébastien Gambs
#t 2007
#c 19
#% 237859
#% 249170
#% 271221
#% 321455
#% 325357
#% 450951
#% 451056
#% 785078
#! By the term "quantization", we refer to the process of using quantum mechanics in order to improve a classical algorithm, usually by making it go faster. In this paper, we initiate the idea of quantizing clustering algorithms by using variations on a celebrated quantum algorithm due to Grover. After having introduced this novel approach to unsupervised learning, we illustrate it with a quantized version of three standard algorithms: divisive clustering, k-medians and an algorithm for the construction of a neighbourhood graph. We obtain a significant speedup compared to the classical approach.

#index 983805
#* Learning random walks to rank nodes in graphs
#@ Alekh Agarwal;Soumen Chakrabarti
#t 2007
#c 19
#% 115608
#% 268079
#% 577224
#% 577329
#% 722805
#% 827631
#% 840846
#% 840965
#% 875948
#% 879619
#% 881457
#% 1674801
#% 1705503
#! Ranking nodes in graphs is of much recent interest. Edges, via the graph Laplacian, are used to encourage local smoothness of node scores in SVM-like formulations with generalization guarantees. In contrast, Page-rank variants are based on Markovian random walks. For directed graphs, there is no simple known correspondence between these views of scoring/ranking. Recent scalable algorithms for learning the Pagerank transition probabilities do not have generalization guarantees. In this paper we show some correspondence results between the Laplacian and the Pagerank approaches, and give new generalization guarantees for the latter. We enhance the Pagerank-learning approaches to use an additive margin. We also propose a general framework for rank-sensitive score-learning, and apply it to Laplacian smoothing. Experimental results are promising.

#index 983806
#* Uncovering shared structures in multiclass classification
#@ Yonatan Amit;Michael Fink;Nathan Srebro;Shimon Ullman
#t 2007
#c 19
#% 420507
#% 722816
#% 757953
#% 770796
#% 875984
#% 884155
#% 975142
#! This paper suggests a method for multiclass learning with many classes by simultaneously learning shared characteristics common to the classes, and predictors for the classes in terms of these characteristics. We cast this as a convex optimization problem, using trace-norm regularization and study gradient-based optimization both for the linear case and the kernelized setting.

#index 983807
#* Two-view feature generation model for semi-supervised learning
#@ Rie Kubota Ando;Tong Zhang
#t 2007
#c 19
#% 252011
#% 311027
#% 916788
#! We consider a setting for discriminative semi-supervised learning where unlabeled data are used with a generative model to learn effective feature representations for discriminative training. Within this framework, we revisit the two-view feature generation model of co-training and prove that the optimum predictor can be expressed as a linear combination of a few features constructed from unlabeled data. From this analysis, we derive methods that employ two views but are very different from co-training. Experiments show that our approach is more robust than co-training and EM, under various data generation conditions.

#index 983808
#* Scalable training of L1-regularized log-linear models
#@ Galen Andrew;Jianfeng Gao
#t 2007
#c 19
#% 190265
#% 251365
#% 466736
#% 770857
#% 854813
#% 855283
#% 939353
#! The L-BFGS limited-memory quasi-Newton method is the algorithm of choice for optimizing the parameters of large-scale log-linear models with L2 regularization, but it cannot be used for an L1-regularized loss due to its non-differentiability whenever some parameter is zero. Efficient algorithms have been proposed for this task, but they are impractical when the number of parameters is very large. We present an algorithm Orthant-Wise Limited-memory Quasi-Newton (OWL-QN), based on L-BFGS, that can efficiently optimize the L1-regularized log-likelihood of log-linear models with millions of parameters. In our experiments on a parse reranking task, our algorithm was several orders of magnitude faster than an alternative algorithm, and substantially faster than L-BFGS on the analogous L2-regularized problem. We also present a proof that OWL-QN is guaranteed to converge to a globally optimal parameter vector.

#index 983809
#* Multiclass core vector machine
#@ S. Asharaf;M. Narasimha Murty;S. K. Shevade
#t 2007
#c 19
#% 33917
#% 466597
#% 760985
#% 803575
#% 829021
#% 855602
#% 972162
#% 1860941
#! Even though several techniques have been proposed in the literature for achieving multiclass classification using Support Vector Machine(SVM), the scalability aspect of these approaches to handle large data sets still needs much of exploration. Core Vector Machine(CVM) is a technique for scaling up a two class SVM to handle large data sets. In this paper we propose a Multiclass Core Vector Machine(MCVM). Here we formulate the multiclass SVM problem as a Quadratic Programming(QP) problem defining an SVM with vector valued output. This QP problem is then solved using the CVM technique to achieve scalability to handle large data sets. Experiments done with several large synthetic and real world data sets show that the proposed MCVM technique gives good generalization performance as that of SVM at a much lesser computational expense. Further, it is observed that MCVM scales well with the size of the data set.

#index 983810
#* The rendezvous algorithm: multiclass semi-supervised learning with Markov random walks
#@ Arik Azran
#t 2007
#c 19
#% 313959
#% 466263
#% 593047
#% 833913
#% 883861
#% 1502529
#! We consider the problem of multiclass classification where both labeled and unlabeled data points are given. We introduce and demonstrate a new approach for estimating a distribution over the missing labels where data points are viewed as nodes of a graph, and pairwise similarities are used to derive a transition probability matrix P for a Markov random walk between them. The algorithm associates each point with a particle which moves between points according to P. Labeled points are set to be absorbing states of the Markov random walk, and the probability of each particle to be absorbed by the different labeled points, as the number of steps increases, is then used to derive a distribution over the associated missing label. A computationally efficient algorithm to implement this is derived and demonstrated on both real and artificial data sets, including a numerical comparison with other methods.

#index 983811
#* Focused crawling with scalable ordinal regression solvers
#@ Rashmin Babaria;J. Saketha Nath;Krishnan S;Sivaramakrishnan K R;Chiranjib Bhattacharyya;M. N. Murty
#t 2007
#c 19
#% 210173
#% 269218
#% 281251
#% 290830
#% 309145
#% 330599
#% 348138
#% 480309
#% 840853
#% 878570
#% 881533
#! In this paper we propose a novel, scalable, clustering based Ordinal Regression formulation, which is an instance of a Second Order Cone Program (SOCP) with one Second Order Cone (SOC) constraint. The main contribution of the paper is a fast algorithm, CB-OR, which solves the proposed formulation more eficiently than general purpose solvers. Another main contribution of the paper is to pose the problem of focused crawling as a large scale Ordinal Regression problem and solve using the proposed CB-OR. Focused crawling is an efficient mechanism for discovering resources of interest on the web. Posing the problem of focused crawling as an Ordinal Regression problem avoids the need for a negative class and topic hierarchy, which are the main drawbacks of the existing focused crawling methods. Experiments on large synthetic and benchmark datasets show the scalability of CB-OR. Experiments also show that the proposed focused crawler outperforms the state-of-the-art.

#index 983812
#* Learning distance function by coding similarity
#@ Aharon Bar Hillel;Daphna Weinshall
#t 2007
#c 19
#% 115608
#% 219845
#% 443975
#% 457926
#% 465914
#% 593654
#% 729437
#% 770782
#% 770811
#% 829025
#% 883981
#% 1809406
#! We consider the problem of learning a similarity function from a set of positive equivalence constraints, i.e. 'similar' point pairs. We define the similarity in information theoretic terms, as the gain in coding length when shifting from independent encoding of the pair to joint encoding. Under simple Gaussian assumptions, this formulation leads to a non-Mahalanobis similarity function which is efficient and simple to learn. This function can be viewed as a likelihood ratio test, and we show that the optimal similarity-preserving projection of the data is a variant of Fisher Linear Discriminant. We also show that under some naturally occurring sampling conditions of equivalence constraints, this function converges to a known Mahalanobis distance (RCA). The suggested similarity function exhibits superior performance over alternative Mahalanobis distances learnt from the same data. Its superiority is demonstrated in the context of image retrieval and graph based clustering, using a large number of data sets.

#index 983813
#* Structural alignment based kernels for protein structure classification
#@ Sourangshu Bhattacharya;Chiranjib Bhattacharyya;Nagasuma Chandra
#t 2007
#c 19
#% 45753
#% 382586
#% 458667
#% 469390
#% 793247
#% 840953
#! Structural alignments are the most widely used tools for comparing proteins with low sequence similarity. The main contribution of this paper is to derive various kernels on proteins from structural alignments, which do not use sequence information. Central to the kernels is a novel alignment algorithm which matches substructures of fixed size using spectral graph matching techniques. We derive positive semi-definite kernels which capture the notion of similarity between substructures. Using these as base more sophisticated kernels on protein structures are proposed. To empirically evaluate the kernels we used a 40% sequence non-redundant structures from 15 different SCOP superfamilies. The kernels when used with SVMs show competitive performance with CE, a state of the art structure comparison program.

#index 983814
#* Discriminative learning for differing training and test distributions
#@ Steffen Bickel;Michael Brückner;Tobias Scheffer
#t 2007
#c 19
#% 770847
#% 961246
#% 998622
#% 1289281
#% 1579122
#! We address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution---problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. We formulate the general problem of learning under covariate shift as an integrated optimization problem. We derive a kernel logistic regression classifier for differing training and test distributions.

#index 983815
#* Solving multiclass support vector machines with LaRank
#@ Antoine Bordes;Léon Bottou;Patrick Gallinari;Jason Weston
#t 2007
#c 19
#% 269218
#% 722816
#% 722903
#% 763699
#% 827631
#% 829043
#% 840947
#% 854636
#% 916781
#% 1000452
#% 1699617
#% 1860941
#! Optimization algorithms for large margin multiclass recognizers are often too costly to handle ambitious problems with structured outputs and exponential numbers of classes. Optimization algorithms that rely on the full gradient are not effective because, unlike the solution, the gradient is not sparse and is very large. The LaRank algorithm sidesteps this difficulty by relying on a randomized exploration inspired by the perceptron algorithm. We show that this approach is competitive with gradient based optimizers on simple multiclass problems. Furthermore, a single LaRank pass over the training examples delivers test error rates that are nearly as good as those of the final solution.

#index 983816
#* Efficiently computing minimax expected-size confidence regions
#@ Brent Bryan;H. Brendan McMahan;Chad M. Schafer;Jeff Schneider
#t 2007
#c 19
#% 176299
#% 997945
#! Given observed data and a collection of parameterized candidate models, a 1 -- α confidence region in parameter space provides useful insight as to those models which are a good fit to the data, all while keeping the probability of incorrect exclusion below α. With complex models, optimally precise procedures (those with small expected size) are, in practice, difficult to derive; one solution is the Minimax Expected-Size (MES) confidence procedure. The key computational problem of MES is computing a minimax equilibria to a certain zero-sum game. We show that this game is convex with bilinear payoffs, allowing us to apply any convex game solver, including linear programming. Exploiting the sparsity of the matrix, along with using fast linear programming software, allows us to compute approximate minimax expected-size confidence regions orders of magnitude faster than previously published methods. We test these approaches by estimating parameters for a cosmological model.

#index 983817
#* Multiple instance learning for sparse positive bags
#@ Razvan C. Bunescu;Raymond J. Mooney
#t 2007
#c 19
#% 190581
#% 224755
#% 269218
#% 464621
#% 464633
#% 466263
#% 576520
#% 707541
#% 840922
#% 961195
#! We present a new approach to multiple instance learning (MIL) that is particularly effective when the positive bags are sparse (i.e. contain few positive instances). Unlike other SVM-based MIL methods, our approach more directly enforces the desired constraint that at least one of the instances in a positive bag is positive. Using both artificial and real-world data, we experimentally demonstrate that our approach achieves greater accuracy than state-of-the-art MIL methods when positive bags are sparse, and performs competitively when they are not. In particular, our approach is the best performing method for image region classification.

#index 983818
#* Cluster analysis of heterogeneous rank data
#@ Ludwig M. Busse;Peter Orbanz;Joachim M. Buhmann
#t 2007
#c 19
#% 443948
#% 450535
#% 464451
#% 805798
#! Cluster analysis of ranking data, which occurs in consumer questionnaires, voting forms or other inquiries of preferences, attempts to identify typical groups of rank choices. Empirically measured rankings are often incomplete, i.e. different numbers of filled rank positions cause heterogeneity in the data. We propose a mixture approach for clustering of heterogeneous rank data. Rankings of different lengths can be described and compared by means of a single probabilistic model. A maximum entropy approach avoids hidden assumptions about missing rank positions. Parameter estimators and an efficient EM algorithm for unsupervised inference are derived for the ranking mixture model. Experiments on both synthetic data and real-world data demonstrate significantly improved parameter estimates on heterogeneous data when the incomplete rankings are included in the inference process.

#index 983819
#* Feature selection in a kernel space
#@ Bin Cao;Dou Shen;Jian-Tao Sun;Qiang Yang;Zheng Chen
#t 2007
#c 19
#% 3084
#% 80995
#% 169659
#% 304931
#% 465583
#% 465754
#% 466084
#% 697311
#% 722929
#% 722943
#% 770774
#% 818217
#% 840960
#% 857439
#% 876059
#% 899958
#% 940319
#! We address the problem of feature selection in a kernel space to select the most discriminative and informative features for classification and data analysis. This is a difficult problem because the dimension of a kernel space may be infinite. In the past, little work has been done on feature selection in a kernel space. To solve this problem, we derive a basis set in the kernel space as a first step for feature selection. Using the basis set, we then extend the margin-based feature selection algorithms that are proven effective even when many features are dependent. The selected features form a subspace of the kernel space, in which different state-of-the-art classification algorithms can be applied for classification. We conduct extensive experiments over real and simulated data to compare our proposed method with four baseline algorithms. Both theoretical analysis and experimental results validate the effectiveness of our proposed method.

#index 983820
#* Learning to rank: from pairwise approach to listwise approach
#@ Zhe Cao;Tao Qin;Tie-Yan Liu;Ming-Feng Tsai;Hang Li
#t 2007
#c 19
#% 169777
#% 269217
#% 272510
#% 309095
#% 387427
#% 464451
#% 564279
#% 577224
#% 766414
#% 840846
#% 879588
#% 987227
#% 987240
#! The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative filtering, and many other applications. Several methods for learning to rank have been proposed, which take object pairs as 'instances' in learning. We refer to them as the pairwise approach in this paper. Although the pairwise approach offers advantages, it ignores the fact that ranking is a prediction task on list of objects. The paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as 'instances' in learning. The paper proposes a new probabilistic method for the approach. Specifically it introduces two probability models, respectively referred to as permutation probability and top k probability, to define a listwise loss function for learning. Neural Network and Gradient Descent are then employed as model and algorithm in the learning method. Experimental results on information retrieval show that the proposed listwise approach performs better than the pairwise approach.

#index 983821
#* Local similarity discriminant analysis
#@ Luca Cazzanti;Maya R. Gupta
#t 2007
#c 19
#% 5182
#% 115608
#% 140588
#% 302402
#% 304899
#% 305012
#% 316780
#% 443948
#% 443975
#% 476717
#% 722813
#% 881895
#% 883981
#% 889311
#% 940279
#% 1815873
#! We propose a local, generative model for similarity-based classification. The method is applicable to the case that only pairwise similarities between samples are available. The classifier models the local class-conditional distribution using a maximum entropy estimate and empirical moment constraints. The resulting exponential class conditional-distributions are combined with class prior probabilities and misclassification costs to form the local similarity discriminant analysis (local SDA) classifier. We compare the performance of local SDA to a non-local version, to the local nearest centroid classifier, the nearest centroid classifier, k-NN, and to the recently-developed potential support vector machine (PSVM). Results show that local SDA is competitive with k-NN and the computationally-demanding PSVM while offering the advantages of a generative classifier.

#index 983822
#* Direct convex relaxations of sparse SVM
#@ Antoni B. Chan;Nuno Vasconcelos;Gert R. G. Lanckriet
#t 2007
#c 19
#% 190581
#% 243727
#% 425048
#% 466084
#% 481708
#% 722929
#% 722932
#% 722938
#% 722943
#% 754400
#% 757953
#% 846429
#! Although support vector machines (SVMs) for binary classification give rise to a decision rule that only relies on a subset of the training data points (support vectors), it will in general be based on all available features in the input space. We propose two direct, novel convex relaxations of a non-convex sparse SVM formulation that explicitly constrains the cardinality of the vector of feature weights. One relaxation results in a quadratically-constrained quadratic program (QCQP), while the second is based on a semidefinite programming (SDP) relaxation. The QCQP formulation can be interpreted as applying an adaptive soft-threshold on the SVM hyperplane, while the SDP formulation learns a weighted inner-product (i.e. a kernel) that results in a sparse hyperplane. Experimental results show an increase in sparsity while conserving the generalization performance compared to a standard as well as a linear programming SVM.

#index 983823
#* Minimum reference set based feature selection for small sample classifications
#@ Xue-wen Chen;Jong Cheol Jeong
#t 2007
#c 19
#% 96692
#% 143194
#% 351831
#% 425040
#% 425048
#% 466084
#% 722898
#% 722929
#% 722939
#% 722943
#% 934582
#% 959434
#% 1810179
#% 1861143
#! We address feature selection problems for classification of small samples and high dimensionality. A practical example is microarray-based cancer classification problems, where sample size is typically less than 100 and number of features is several thousands or higher. One of the commonly used methods in addressing this problem is recursive feature elimination (RFE) method, which utilizes the generalization capability embedded in support vector machines and is thus suitable for small samples problems. We propose a novel method using minimum reference set (MRS) generated by the nearest neighbor rule. MRS is the set of minimum number of samples that correctly classify all the training samples. It is related to structural risk minimization principle and thus leads to good generalization. The proposed MRS based method is compared to RFE method with several real datasets, and experimental results show that the MRS method produces better classification performance.

#index 983824
#* Learning to compress images and videos
#@ Li Cheng;S. V. N. Vishwanathan
#t 2007
#c 19
#% 593047
#% 724246
#% 771073
#% 961218
#! We present an intuitive scheme for lossy color-image compression: Use the color information from a few representative pixels to learn a model which predicts color on the rest of the pixels. Now, storing the representative pixels and the image in grayscale suffice to recover the original image. A similar scheme is also applicable for compressing videos, where a single model can be used to predict color on many consecutive frames, leading to better compression. Existing algorithms for colorization -- the process of adding color to a grayscale image or video sequence -- are tedious, and require intensive human-intervention. We bypass these limitations by using a graph-based inductive semi-supervised learning module for colorization, and a simple active learning strategy to choose the representative pixels. Experiments on a wide variety of images and video sequences demonstrate the efficacy of our algorithm.

#index 983825
#* Magnitude-preserving ranking algorithms
#@ Corinna Cortes;Mehryar Mohri;Ashish Rastogi
#t 2007
#c 19
#% 564279
#% 722805
#% 840853
#% 1674802
#% 1705503
#% 1705505
#! This paper studies the learning problem of ranking when one wishes not just to accurately predict pairwise ordering but also preserve the magnitude of the preferences or the difference between ratings, a problem motivated by its key importance in the design of search engines, movie recommendation, and other similar ranking systems. We describe and analyze several algorithms for this problem and give stability bounds for their generalization error, extending previously known stability results to non-bipartite ranking and magnitude of preference-preserving algorithms. We also report the results of experiments comparing these algorithms on several datasets and compare these results with those obtained using an algorithm minimizing the pairwise misranking error and standard regression.

#index 983826
#* Full regularization path for sparse principal component analysis
#@ Alexandre d'Aspremont;Francis R. Bach;Laurent El Ghaoui
#t 2007
#c 19
#% 187651
#% 319234
#% 876025
#! Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all numbers of non zero coefficients, with complexity O(n3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n3). We show on toy examples and biological data that our algorithm does provide globally optimal solutions in many cases.

#index 983827
#* Kernel selection forl semi-supervised kernel machines
#@ Guang Dai;Dit-Yan Yeung
#t 2007
#c 19
#% 425040
#% 466263
#% 763697
#% 765552
#% 829031
#% 840938
#% 840965
#% 875968
#% 876050
#% 961218
#% 1705523
#! Existing semi-supervised learning methods are mostly based on either the cluster assumption or the manifold assumption. In this paper, we propose an integrated regularization framework for semi-supervised kernel machines by incorporating both the cluster assumption and the manifold assumption. Moreover, it supports kernel learning in the form of kernel selection. The optimization problem involves joint optimization over all the labeled and unlabeled data points, a convex set of basic kernels, and a discrete space of unknown labels for the unlabeled data. When the manifold assumption is incorporated, graph Laplacian kernels are used as the basic kernels for learning an optimal convex combination of graph Laplacian kernels. Comparison with related methods on the USPS data set shows very promising results.

#index 983828
#* Boosting for transfer learning
#@ Wenyuan Dai;Qiang Yang;Gui-Rong Xue;Yong Yu
#t 2007
#c 19
#% 116149
#% 235377
#% 236497
#% 465746
#% 466263
#% 496419
#% 770847
#% 770858
#% 840898
#% 1272110
#% 1290055
#! Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund & Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model.

#index 983829
#* Intractability and clustering with constraints
#@ Ian Davidson;S. S. Ravi
#t 2007
#c 19
#% 272284
#% 388196
#% 464291
#% 464631
#% 466890
#% 715529
#% 769881
#! Clustering with constraints is a developing area of machine learning. Various papers have used constraints to enforce particular clusterings, seed clustering algorithms and even learn distance functions which are then used for clustering. We present intractability results for some constraint combinations and illustrate both formally and experimentally the implications of these results for using constraints with clustering.

#index 983830
#* Information-theoretic metric learning
#@ Jason V. Davis;Brian Kulis;Prateek Jain;Suvrit Sra;Inderjit S. Dhillon
#t 2007
#c 19
#% 209623
#% 227736
#% 382854
#% 457926
#% 769881
#% 770798
#% 812372
#% 829027
#% 852092
#% 876008
#% 963196
#! In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function. We express this problem as a particular Bregman optimization problem---that of minimizing the LogDet divergence subject to linear constraints. Our resulting algorithm has several advantages over existing methods. First, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. Second, it is fast and scalable. Unlike most existing methods, no eigenvalue computations or semi-definite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of metric learning for nearest neighbor classification, as well as on standard data sets.

#index 983831
#* An integrated approach to feature invention and model construction for drug activity prediction
#@ Jesse Davis;Vítor Santos Costa;Soumya Ray;David Page
#t 2007
#c 19
#% 37628
#% 190581
#% 224755
#% 260151
#% 345862
#% 466927
#% 707541
#% 883330
#% 1250568
#% 1269484
#% 1699582
#! We present a new machine learning approach for 3D-QSAR, the task of predicting binding affinities of molecules to target proteins based on 3D structure. Our approach predicts binding affinity by using regression on substructures discovered by relational learning. We make two contributions to the state-of-the-art. First, we use multiple-instance (MI) regression, which represents a molecule as a set of 3D conformations, to model activity. Second, the relational learning component employs the "Score As You Use" (SAYU) method to select substructures for their ability to improve the regression model. This is the first application of SAYU to multiple-instance, real-valued prediction. We evaluate our approach on three tasks and demonstrate that (i) SAYU outperforms standard coverage measures when selecting features for regression, (ii) the MI representation improves accuracy over standard single feature-vector encodings and (iii) combining SAYU with MI regression is more accurate for 3D-QSAR than either approach by itself.

#index 983832
#* Percentile optimization in uncertain Markov decision processes with application to efficient exploration
#@ Erick Delage;Shie Mannor
#t 2007
#c 19
#% 270831
#% 318485
#% 363744
#% 466075
#% 722895
#% 810882
#% 840942
#% 951076
#% 959522
#% 1023856
#% 1650283
#! Markov decision processes are an effective tool in modeling decision-making in uncertain dynamic environments. Since the parameters of these models are typically estimated from data, learned from experience, or designed by hand, it is not surprising that the actual performance of a chosen strategy often significantly differs from the designer's initial expectations due to unavoidable model uncertainty. In this paper, we present a percentile criterion that captures the trade-off between optimistic and pessimistic points of view on MDP with parameter uncertainty. We describe tractable methods that take parameter uncertainty into account in the process of decision making. Finally, we propose a cost-effective exploration strategy when it is possible to invest (money, time or computation efforts) in actions that will reduce the uncertainty in the parameters.

#index 983833
#* Unsupervised prediction of citation influences
#@ Laura Dietz;Steffen Bickel;Tobias Scheffer
#t 2007
#c 19
#% 290830
#% 329569
#% 466574
#% 528169
#% 722904
#% 788094
#% 840961
#% 874462
#! Publication repositories contain an abundance of information about the evolution of scientific research areas. We address the problem of creating a visualization of a research area that describes the flow of topics between papers, quantifies the impact that papers have on each other, and helps to identify key contributions. To this end, we devise a probabilistic topic model that explains the generation of documents; the model incorporates the aspects of topical innovation and topical inheritance via citations. We evaluate the model's ability to predict the strength of influence of citations against manually rated citations.

#index 983834
#* Non-isometric manifold learning: analysis and an algorithm
#@ Piotr Dollár;Vincent Rabaud;Serge Belongie
#t 2007
#c 19
#% 305005
#% 424080
#% 476569
#% 476872
#% 723241
#% 871628
#% 876075
#! In this work we take a novel view of nonlinear manifold learning. Usually, manifold learning is formulated in terms of finding an embedding or 'unrolling' of a manifold into a lower dimensional space. Instead, we treat it as the problem of learning a representation of a nonlinear, possibly non-isometric manifold that allows for the manipulation of novel points. Central to this view of manifold learning is the concept of generalization beyond the training data. Drawing on concepts from supervised learning, we establish a framework for studying the problems of model assessment, model complexity, and model selection for manifold learning. We present an extension of a recent algorithm, Locally Smooth Manifold Learning (LSML), and show it has good generalization properties. LSML learns a representation of a manifold or family of related manifolds and can be used for computing geodesic distances, finding the projection of a point onto a manifold, recovering a manifold from points corrupted by noise, generating novel points on a manifold, and more.

#index 983835
#* Hierarchical maximum entropy density estimation
#@ Miroslav Dudik;David M. Blei;Robert E. Schapire
#t 2007
#c 19
#% 466078
#% 757953
#% 855283
#% 876034
#% 1271814
#% 1674770
#! We study the problem of simultaneously estimating several densities where the datasets are organized into overlapping groups, such as a hierarchy. For this problem, we propose a maximum entropy formulation, which systematically incorporates the groups and allows us to share the strength of prediction across similar datasets. We derive general performance guarantees, and show how some previous approaches, such as hierarchical shrinkage and hierarchical priors, can be derived as special cases. We demonstrate the proposed technique on synthetic data and in a real-world application to modeling the geographic distributions of species hierarchically grouped in a taxonomy. Specifically, we model the geographic distributions of species in the Australian wet tropics and Northeast New South Wales. In these regions, small numbers of samples per species significantly hinder effective prediction. Substantial benefits are obtained by combining information across taxonomic groups.

#index 983836
#* CarpeDiem: an algorithm for the fast evaluation of SSL classifiers
#@ Roberto Esposito;Daniele P. Radicioni
#t 2007
#c 19
#% 70370
#% 95730
#% 103493
#% 464434
#% 466892
#% 479726
#% 840935
#% 849839
#% 854636
#% 1663474
#! In this paper we present a novel algorithm, CarpeDiem. It significantly improves on the time complexity of Viterbi algorithm, preserving the optimality of the result. This fact has consequences on Machine Learning systems that use Viterbi algorithm during learning or classification. We show how the algorithm applies to the Supervised Sequential Learning task and, in particular, to the HMPerceptron algorithm. We illustrate CarpeDiem in full details, and provide experimental results that support the proposed approach.

#index 983837
#* Manifold-adaptive dimension estimation
#@ Amir massoud Farahmand;Csaba Szepesvári;Jean-Yves Audibert
#t 2007
#c 19
#% 840871
#% 1674765
#! Intuitively, learning should be easier when the data points lie on a low-dimensional submanifold of the input space. Recently there has been a growing interest in algorithms that aim to exploit such geometrical properties of the data. Oftentimes these algorithms require estimating the dimension of the manifold first. In this paper we propose an algorithm for dimension estimation and study its finite-sample behaviour. The algorithm estimates the dimension locally around the data points using nearest neighbor techniques and then combines these local estimates. We show that the rate of convergence of the resulting estimate is independent of the dimension of the input space and hence the algorithm is "manifold-adaptive". Thus, when the manifold supporting the data is low dimensional, the algorithm can be exponentially more efficient than its counterparts that are not exploiting this property. Our computer experiments confirm the obtained theoretical results.

#index 983838
#* Combining online and offline knowledge in UCT
#@ Sylvain Gelly;David Silver
#t 2007
#c 19
#% 90041
#% 384911
#% 425053
#% 449561
#% 1274923
#% 1289220
#% 1404135
#% 1665148
#! The UCT algorithm learns a value function online using sample-based search. The TD(λ) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions in the UCT algorithm. First, the offline value function is used as a default policy during Monte-Carlo simulation. Second, the UCT value function is combined with a rapid online estimate of action values. Third, the offline value function is used as prior knowledge in the UCT search tree. We evaluate these algorithms in 9 x 9 Go against GnuGo 3.7.10. The first algorithm performs better than UCT with a random simulation policy, but surprisingly, worse than UCT with a weaker, handcrafted simulation policy. The second algorithm outperforms UCT altogether. The third algorithm outperforms UCT with handcrafted prior knowledge. We combine these algorithms in MoGo, the world's strongest 9 x 9 Go program. Each technique significantly improves MoGo's playing strength.

#index 983839
#* Robust non-linear dimensionality reduction using successive 1-dimensional Laplacian Eigenmaps
#@ Samuel Gerber;Tolga Tasdizen;Ross Whitaker
#t 2007
#c 19
#% 114910
#% 132779
#% 169026
#% 234978
#% 266426
#% 313959
#% 361100
#% 492796
#% 577290
#% 593047
#% 769911
#% 770767
#% 790049
#% 848111
#% 855573
#% 1667615
#! Non-linear dimensionality reduction of noisy data is a challenging problem encountered in a variety of data analysis applications. Recent results in the literature show that spectral decomposition, as used for example by the Laplacian Eigenmaps algorithm, provides a powerful tool for non-linear dimensionality reduction and manifold learning. In this paper, we discuss a significant shortcoming of these approaches, which we refer to as the repeated eigendirections problem. We propose a novel approach that combines successive 1-dimensional spectral embeddings with a data advection scheme that allows us to address this problem. The proposed method does not depend on a non-linear optimization scheme; hence, it is not prone to local minima. Experiments with artificial and real data illustrate the advantages of the proposed method over existing approaches. We also demonstrate that the approach is capable of correctly learning manifolds corrupted by significant amounts of noise.

#index 983840
#* Gradient boosting for kernelized output spaces
#@ Pierre Geurts;Louis Wehenkel;Florence d'Alché-Buc
#t 2007
#c 19
#% 235377
#% 425063
#% 448194
#% 829043
#% 833069
#% 840854
#% 840947
#% 866298
#% 875988
#! A general framework is proposed for gradient boosting in supervised learning problems where the loss function is defined using a kernel over the output space. It extends boosting in a principled way to complex output spaces (images, text, graphs etc.) and can be applied to a general class of base learners working in kernelized output spaces. Empirical results are provided on three problems: a regression problem, an image completion task and a graph prediction problem. In these experiments, the framework is combined with tree-based base learners, which have interesting algorithmic properties. The results show that gradient boosting significantly improves these base learners and provides competitive results with other tree-based ensemble methods based on randomization.

#index 983841
#* Bayesian actor-critic algorithms
#@ Mohammad Ghavamzadeh;Yaakov Engel
#t 2007
#c 19
#% 384911
#% 393786
#% 743284
#% 840860
#! We present a new actor-critic learning model in which a Bayesian class of non-parametric critics, using Gaussian process temporal difference learning is used. Such critics model the state-action value function as a Gaussian process, allowing Bayes' rule to be used in computing the posterior distribution over state-action value functions, conditioned on the observed data. Appropriate choices of the prior covariance (kernel) between state-action values and of the parametrization of the policy allow us to obtain closed-form expressions for the posterior distribution of the gradient of the average discounted return with respect to the policy parameters. The posterior mean, which serves as our estimate of the policy gradient, is used to update the policy, while the posterior covariance allows us to gauge the reliability of the update.

#index 983842
#* Exponentiated gradient algorithms for log-linear structured prediction
#@ Amir Globerson;Terry Y. Koo;Xavier Carreras;Michael Collins
#t 2007
#c 19
#% 115608
#% 227736
#% 269218
#% 425065
#% 464434
#% 816181
#% 846432
#% 876066
#% 939343
#% 1249462
#% 1845612
#! Conditional log-linear models are a commonly used method for structured prediction. Efficient learning of parameters in these models is therefore an important problem. This paper describes an exponentiated gradient (EG) algorithm for training such models. EG is applied to the convex dual of the maximum likelihood objective; this results in both sequential and parallel update algorithms, where in the sequential algorithm parameters are updated in an online fashion. We provide a convergence proof for both algorithms. Our analysis also simplifies previous results on EG for max-margin models, and leads to a tighter bound on convergence rates. Experiments on a large-scale parsing task show that the proposed algorithm converges much faster than conjugate-gradient and L-BFGS approaches both in terms of optimization objective and test error.

#index 983843
#* Best of both: a hybridized centroid-medoid clustering heuristic
#@ Nizar Grira;Michael E. Houle
#t 2007
#c 19
#% 36672
#% 80995
#% 190611
#% 304590
#% 338344
#% 729437
#% 830362
#! Although each iteration of the popular k-Means clustering heuristic scales well to larger problem sizes, it often requires an unacceptably-high number of iterations to converge to a solution. This paper introduces an enhancement of k-Means in which local search is used to accelerate convergence without greatly increasing the average computational cost of the iterations. The local search involves a carefully-controlled number of swap operations resembling those of the more robust k-Medoids clustering heuristic. We show empirically that the proposed method improves convergence results when compared to standard k-Means.

#index 983844
#* Recovering temporally rewiring networks: a model-based approach
#@ Fan Guo;Steve Hanneke;Wenjie Fu;Eric P. Xing
#t 2007
#c 19
#% 788083
#% 833120
#% 868095
#% 906571
#% 1396216
#! A plausible representation of relational information among entities in dynamic systems such as a living cell or a social community is a stochastic network which is topologically rewiring and semantically evolving over time. While there is a rich literature on modeling static or temporally invariant networks, much less has been done toward modeling the dynamic processes underlying rewiring networks, and on recovering such networks when they are not observable. We present a class of hidden temporal exponential random graph models (htERGMs) to study the yet unexplored topic of modeling and recovering temporally rewiring networks from time series of node attributes such as activities of social actors or expression levels of genes. We show that one can reliably infer the latent time-specific topologies of the evolving networks from the observation. We report empirical results on both synthetic data and a Drosophila lifecycle gene expression data set, in comparison with a static counterpart of htERGM.

#index 983845
#* Efficient inference with cardinality-based clique potentials
#@ Rahul Gupta;Ajit A. Diwan;Sunita Sarawagi
#t 2007
#c 19
#% 25998
#% 248810
#% 344568
#% 413869
#% 732537
#% 769942
#% 770866
#% 772862
#% 876037
#% 938708
#% 939376
#% 939641
#% 1650403
#% 1730598
#! Many collective labeling tasks require inference on graphical models where the clique potentials depend only on the number of nodes that get a particular label. We design efficient inference algorithms for various families of such potentials. Our algorithms are exact for arbitrary cardinality-based clique potentials on binary labels and for max-like and majority-like clique potentials on multiple labels. Moving towards more complex potentials, we show that inference becomes NP-hard even on cliques with homogeneous Potts potentials. We present a 13/15-approximation algorithm with runtime sub-quadratic in the clique size. In contrast, the best known previous guarantee for graphs with Potts potentials is only 0.5. We perform empirical comparisons on real and synthetic data, and show that our proposed methods are an order of magnitude faster than the well-known Tree-based re-parameterization (TRW) and graph-cut algorithms.

#index 983846
#* Sparse probabilistic classifiers
#@ Romain Hérault;Yves Grandvalet
#t 2007
#c 19
#% 424723
#% 466561
#% 669214
#% 757953
#% 846432
#% 875970
#% 959454
#% 961154
#% 961255
#% 961274
#% 1271973
#% 1861282
#! The scores returned by support vector machines are often used as a confidence measures in the classification of new examples. However, there is no theoretical argument sustaining this practice. Thus, when classification uncertainty has to be assessed, it is safer to resort to classifiers estimating conditional probabilities of class labels. Here, we focus on the ambiguity in the vicinity of the boundary decision. We propose an adaptation of maximum likelihood estimation, instantiated on logistic regression. The model outputs proper conditional probabilities into a user-defined interval and is less precise elsewhere. The model is also sparse, in the sense that few examples contribute to the solution. The computational efficiency is thus improved compared to logistic regression. Furthermore, preliminary experiments show improvements over standard logistic regression and performances similar to support vector machines.

#index 983847
#* Supervised clustering of streaming data for email batch detection
#@ Peter Haider;Ulf Brefeld;Tobias Scheffer
#t 2007
#c 19
#% 460812
#% 464291
#% 465765
#% 578388
#% 662751
#% 749492
#% 776254
#% 829043
#% 840862
#% 850014
#! We address the problem of detecting batches of emails that have been created according to the same template. This problem is motivated by the desire to filter spam more effectively by exploiting collective information about entire batches of jointly generated messages. The application matches the problem setting of supervised clustering, because examples of correct clusterings can be collected. Known decoding procedures for supervised clustering are cubic in the number of instances. When decisions cannot be reconsidered once they have been made --- owing to the streaming nature of the data --- then the decoding problem can be solved in linear time. We devise a sequential decoding procedure and derive the corresponding optimization problem of supervised clustering. We study the impact of collective attributes of email batches on the effectiveness of recognizing spam emails.

#index 983848
#* A bound on the label complexity of agnostic active learning
#@ Steve Hanneke
#t 2007
#c 19
#% 66937
#% 81849
#% 145224
#% 156184
#% 875953
#% 1661927
#% 1862651
#! We study the label complexity of pool-based active learning in the agnostic PAC model. Specifically, we derive general bounds on the number of label requests made by the A2 algorithm proposed by Balcan, Beygelzimer & Langford (Balcan et al., 2006). This represents the first nontrivial general-purpose upper bound on label complexity in the agnostic PAC model.

#index 983849
#* Learning nonparametric kernel matrices from pairwise constraints
#@ Steven C. H. Hoi;Rong Jin;Michael R. Lyu
#t 2007
#c 19
#% 269218
#% 464291
#% 464615
#% 647057
#% 757953
#% 763697
#% 829025
#% 876008
#% 881474
#! Many kernel learning methods have to assume parametric forms for the target kernel functions, which significantly limits the capability of kernels in fitting diverse patterns. Some kernel learning methods assume the target kernel matrix to be a linear combination of parametric kernel matrices. This assumption again importantly limits the flexibility of the target kernel matrices. The key challenge with nonparametric kernel learning arises from the difficulty in linking the nonparametric kernels to the input patterns. In this paper, we resolve this problem by introducing the graph Laplacian of the observed data as a regularizer when optimizing the kernel matrix with respect to the pairwise constraints. We formulate the problem into Semi-Definite Programs (SDP), and propose an efficient algorithm to solve the SDP problem. The extensive evaluation on clustering with pairwise constraints shows that the proposed nonparametric kernel learning method is more effective than other state-of-the-art kernel learning techniques.

#index 983850
#* Parameter learning for relational Bayesian networks
#@ Manfred Jaeger
#t 2007
#c 19
#% 246835
#% 417753
#% 484593
#% 496116
#% 711139
#% 731606
#% 840911
#% 850430
#% 1271905
#% 1289565
#% 1650727
#! We present a method for parameter learning in relational Bayesian networks (RBNs). Our approach consists of compiling the RBN model into a computation graph for the likelihood function, and to use this likelihood graph to perform the necessary computations for a gradient ascent likelihood optimization procedure. The method can be applied to all RBN models that only contain differentiable combining rules. This includes models with non-decomposable combining rules, as well as models with weighted combinations or nested occurrences of combining rules. Experimental results on artificial random graph data explores the feasibility of the approach both for complete and incomplete data.

#index 983851
#* Bayesian compressive sensing and projection optimization
#@ Shihao Ji;Lawrence Carin
#t 2007
#c 19
#% 115608
#% 132697
#% 236497
#% 274586
#% 528020
#% 722760
#% 873582
#% 1815896
#% 1815965
#% 1816250
#% 1858236
#! This paper introduces a new problem for which machine-learning tools may make an impact. The problem considered is termed "compressive sensing", in which a real signal of dimension N is measured accurately based on K real measurements. This is achieved under the assumption that the underlying signal has a sparse representation in some basis (e.g., wavelets). In this paper we demonstrate how techniques developed in machine learning, specifically sparse Bayesian regression and active learning, may be leveraged to this new problem. We also point out future research directions in compressive sensing of interest to the machine-learning community.

#index 983852
#* Constructing basis functions from directed graphs for value function approximation
#@ Jeff Johns;Sridhar Mahadevan
#t 2007
#c 19
#% 384911
#% 734920
#% 840965
#% 876001
#% 1250345
#% 1269758
#% 1275167
#! Basis functions derived from an undirected graph connecting nearby samples from a Markov decision process (MDP) have proven useful for approximating value functions. The success of this technique is attributed to the smoothness of the basis functions with respect to the state space geometry. This paper explores the properties of bases created from directed graphs which are a more natural fit for expressing state connectivity. Digraphs capture the effect of non-reversible MDPs whose value functions may not be smooth across adjacent states. We provide an analysis using the Dirichlet sum of the directed graph Laplacian to show how the smoothness of the basis functions is affected by the graph's invariant distribution. Experiments in discrete and continuous MDPs with non-reversible actions demonstrate a significant improvement in the policies learned using directed graph bases.

#index 983853
#* Most likely heteroscedastic Gaussian process regression
#@ Kristian Kersting;Christian Plagemann;Patrick Pfaff;Wolfram Burgard
#t 2007
#c 19
#% 269206
#% 272516
#% 840896
#% 872759
#% 891549
#% 931214
#% 1051445
#% 1672479
#! This paper presents a novel Gaussian process (GP) approach to regression with input-dependent noise rates. We follow Goldberg et al.'s approach and model the noise variance using a second GP in addition to the GP governing the noise-free output value. In contrast to Goldberg et al., however, we do not use a Markov chain Monte Carlo method to approximate the posterior noise variance but a most likely noise approach. The resulting model is easy to implement and can directly be used in combination with various existing extensions of the standard GPs such as sparse approximations. Extensive experiments on both synthetic and real-world data, including a challenging perception problem in robotics, show the effectiveness of most likely heteroscedastic GP regression.

#index 983854
#* Neighbor search with global geometry: a minimax message passing algorithm
#@ Kye-Hyeon Kim;Seungjin Choi
#t 2007
#c 19
#% 444007
#% 593047
#% 765261
#% 765552
#% 770813
#% 832334
#% 840967
#% 876068
#% 919460
#% 1810385
#! Neighbor search is a fundamental task in machine learning, especially in classification and retrieval. Efficient nearest neighbor search methods have been widely studied, with their emphasis on data structures but most of them did not consider the underlying global geometry of a data set. Recent graph-based semi-supervised learning methods capture the global geometry, but suffer from scalability and parameter tuning problems. In this paper we present a (nearest) neighbor search method where the underlying global geometry is incorporated and the parameter tuning is not required. To this end, we introduce deterministic walks as a deterministic counterpart of Markov random walks, leading us to use the minimax distance as a global dissimilarity measure. Then we develop a message passing algorithm for efficient minimax distance calculation, which scales linearly in both time and space. Empirical study reveals the useful behavior of the method in image retrieval and semi-supervised learning.

#index 983855
#* A recursive method for discriminative mixture learning
#@ Minyoung Kim;Vladimir Pavlovic
#t 2007
#c 19
#% 246832
#% 272518
#% 469390
#% 520224
#% 578681
#% 840881
#% 840917
#% 969308
#% 1391301
#% 1650623
#% 1702640
#! We consider the problem of learning density mixture models for classification. Traditional learning of mixtures for density estimation focuses on models that correctly represent the density at all points in the sample space. Discriminative learning, on the other hand, aims at representing the density at the decision boundary. We introduce a novel discriminative learning method for mixtures of generative models. Unlike traditional discriminative learning methods that often resort to computationally demanding gradient search optimization, the proposed method is highly efficient as it reduces to generative learning of individual mixture components on weighted data. Hence it is particularly suited to domains with complex component models, such as hidden Markov models or Bayesian networks in general, that are usually too complex for effective gradient search. We demonstrate the benefits of the proposed method in a comprehensive set of evaluations on time-series sequence classification problems.

#index 983856
#* Infinite mixtures of trees
#@ Sergey Kirshner;Padhraic Smyth
#t 2007
#c 19
#% 70370
#% 129987
#% 205164
#% 450456
#% 722753
#% 854012
#! Finite mixtures of tree-structured distributions have been shown to be efficient and effective in modeling multivariate distributions. Using Dirichlet processes, we extend this approach to allow countably many tree-structured mixture components. The resulting Bayesian framework allows us to deal with the problem of selecting the number of mixture components by computing the posterior distribution over the number of components and integrating out the components by Bayesian model averaging. We apply the proposed framework to identify the number and the properties of predominant precipitation patterns in historical archives of climate data.

#index 983857
#* Local dependent components
#@ Arto Klami;Samuel Kaski
#t 2007
#c 19
#% 304879
#% 528174
#% 729918
#% 743284
#% 827225
#% 850420
#% 875949
#% 893386
#! We introduce a mixture of probabilistic canonical correlation analyzers model for analyzing local correlations, or more generally mutual statistical dependencies, in cooccurring data pairs. The model extends the traditional canonical correlation analysis and its probabilistic interpretation in three main ways. First, a full Bayesian treatment enables analysis of small samples (large p, small n, a crucial problem in bioinformatics, for instance), and rigorous estimation of the degree of dependency and independency. Secondly, the mixture formulation generalizes the method from global linearity to the more reasonable assumption of different kinds of dependencies for different kinds of data. As a third novel extension the method decomposes the variation in the data into shared and data set-specific components.

#index 983858
#* Statistical predicate invention
#@ Stanley Kok;Pedro Domingos
#t 2007
#c 19
#% 44876
#% 333797
#% 466078
#% 729918
#% 769954
#% 794859
#% 840890
#% 840961
#% 844322
#% 850430
#% 876018
#% 1000502
#! We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.

#index 983859
#* Kernelizing PLS, degrees of freedom, and efficient model selection
#@ Nicole Krämer;Mikio L. Braun
#t 2007
#c 19
#% 722809
#% 1742155
#! Kernelizing partial least squares (PLS), an algorithm which has been particularly popular in chemometrics, leads to kernel PLS which has several interesting properties, including a sub-cubic runtime for learning, and an iterative construction of directions which are relevant for predicting the outputs. We show that the kernelization of PLS introduces interesting properties not found in ordinary PLS, giving novel insights into the workings of kernel PLS and the connections to kernel ridge regression and conjugate gradient descent methods. Furthermore, we show how to correctly define the degrees of freedom for kernel PLS and how to efficiently compute an unbiased estimate. Finally, we address the practical problem of model selection. We demonstrate how to use the degrees of freedom estimate to perform effective model selection, and discuss how to implement crossvalidation schemes efficiently.

#index 983860
#* Nonmyopic active learning of Gaussian processes: an exploration-exploitation approach
#@ Andreas Krause;Carlos Guestrin
#t 2007
#c 19
#% 115608
#% 829029
#% 840868
#% 875953
#% 877257
#% 891549
#! When monitoring spatial phenomena, such as the ecological condition of a river, deciding where to make observations is a challenging task. In these settings, a fundamental question is when an active learning, or sequential design, strategy, where locations are selected based on previous measurements, will perform significantly better than sensing at an a priori specified set of locations. For Gaussian Processes (GPs), which often accurately model spatial phenomena, we present an analysis and efficient algorithms that address this question. Central to our analysis is a theoretical bound which quantifies the performance difference between active and a priori design strategies. We consider GPs with unknown kernel parameters and present a nonmyopic approach for trading off exploration, i.e., decreasing uncertainty about the model parameters, and exploitation, i.e., near-optimally selecting observations when the parameters are (approximately) known. We discuss several exploration strategies, and present logarithmic sample complexity bounds for the exploration phase. We then extend our algorithm to handle nonstationary GPs exploiting local structure in the model. We also present extensive empirical evaluation on several real-world problems.

#index 983861
#* On one method of non-diagonal regularization in sparse Bayesian learning
#@ Dmitry Kropotov;Dmitry Vetrov
#t 2007
#c 19
#% 190434
#% 272995
#% 360691
#% 424806
#% 528020
#% 528330
#% 722760
#% 891559
#% 906491
#! In the paper we propose a new type of regularization procedure for training sparse Bayesian methods for classification. Transforming Hessian matrix of log-likelihood function to diagonal form with further regularization of its eigenvectors allows us to optimize evidence explicitly as a product of one-dimensional integrals. The process of automatic regularization coefficients determination then converges in one iteration. We show how to use the proposed approach for Gaussian and Laplace priors. Both algorithms show comparable performance with the state-of-the-art Relevance Vector Machines (RVM) but require less time for training and produce more sparse decision rules (in terms of degrees of freedom).

#index 983862
#* Online kernel PCA with entropic matrix updates
#@ Dima Kuzmin;Manfred K. Warmuth
#t 2007
#c 19
#% 227736
#% 235377
#% 266426
#% 722762
#% 722905
#% 734924
#% 757953
#% 829027
#% 963257
#% 1674796
#% 1705525
#! A number of updates for density matrices have been developed recently that are motivated by relative entropy minimization problems. The updates involve a softmin calculation based on matrix logs and matrix exponentials. We show that these updates can be kernelized. This is important because the bounds provable for these algorithms are logarithmic in the feature dimension (provided that the 2-norm of feature vectors is bounded by a constant). The main problem we focus on is the kernelization of an online PCA algorithm which belongs to this family of updates.

#index 983863
#* An empirical evaluation of deep architectures on problems with many factors of variation
#@ Hugo Larochelle;Dumitru Erhan;Aaron Courville;James Bergstra;Yoshua Bengio
#t 2007
#c 19
#% 425049
#% 450888
#% 891060
#% 1502411
#! Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.

#index 983864
#* Hierarchical Gaussian process latent variable models
#@ Neil D. Lawrence;Andrew J. Moore
#t 2007
#c 19
#% 44876
#% 251155
#% 257039
#% 349210
#% 443972
#% 771053
#% 836721
#% 836729
#% 876009
#% 883872
#% 891549
#% 916787
#% 1275085
#% 1275152
#! The Gaussian process latent variable model (GP-LVM) is a powerful approach for probabilistic modelling of high dimensional data through dimensional reduction. In this paper we extend the GP-LVM through hierarchies. A hierarchical model (such as a tree) allows us to express conditional independencies in the data as well as the manifold structure. We first introduce Gaussian process hierarchies through a simple dynamical model, we then extend the approach to a more complex hierarchy which is applied to the visualisation of human motion data sets.

#index 983865
#* Learning a meta-level prior for feature relevance from multiple related tasks
#@ Su-In Lee;Vassil Chatalbashev;David Vickrey;Daphne Koller
#t 2007
#c 19
#% 132676
#% 236495
#% 236497
#% 452991
#% 466750
#% 702350
#% 823311
#% 829014
#% 840962
#% 875984
#% 876034
#% 938695
#% 1128929
#! In many prediction tasks, selecting relevant features is essential for achieving good generalization performance. Most feature selection algorithms consider all features to be a priori equally likely to be relevant. In this paper, we use transfer learning---learning on an ensemble of related tasks---to construct an informative prior on feature relevance. We assume that features themselves have meta-features that are predictive of their relevance to the prediction task, and model their relevance as a function of the meta-features using hyperparameters (called meta-priors). We present a convex optimization algorithm for simultaneously learning the meta-priors and feature weights from an ensemble of related prediction tasks which share a similar relevance structure. Our approach transfers the "meta-priors" among different tasks, which makes it possible to deal with settings where tasks have nonoverlapping features or the relevance of the features vary over the tasks. We show that learning feature relevance improves performance on two real data sets which illustrate such settings: (1) predicting ratings in a collaborative filtering task, and (2) distinguishing arguments of a verb in a sentence.

#index 983866
#* Scalable modeling of real graphs using Kronecker multiplication
#@ Jure Leskovec;Christos Faloutsos
#t 2007
#c 19
#% 283833
#% 867050
#% 1673564
#! Given a large, real graph, how can we generate a synthetic graph that matches its properties, i.e., it has similar degree distribution, similar (small) diameter, similar spectrum, etc? We propose to use "Kronecker graphs", which naturally obey all of the above properties, and we present KronFit, a fast and scalable algorithm for fitting the Kronecker graph generation model to real networks. A naive approach to fitting would take super-exponential time. In contrast, KronFit takes linear time, by exploiting the structure of Kronecker product and by using sampling. Experiments on large real and synthetic graphs show that KronFit indeed mimics very well the patterns found in the target graphs. Once fitted, the model parameters and the resulting synthetic graphs can be used for anonymization, extrapolations, and graph summarization.

#index 983867
#* Support cluster machine
#@ Bin Li;Mingmin Chi;Jianping Fan;Xiangyang Xue
#t 2007
#c 19
#% 210173
#% 269217
#% 269218
#% 342598
#% 466419
#% 722757
#% 729940
#% 771841
#% 803575
#% 856251
#% 905170
#% 961202
#! For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.

#index 983868
#* A transductive framework of distance metric learning by spectral dimensionality reduction
#@ Fuxin Li;Jian Yang;Jue Wang
#t 2007
#c 19
#% 476872
#% 593047
#% 763697
#% 770839
#% 839977
#% 840938
#% 852092
#% 855495
#% 855573
#% 866297
#% 884027
#% 961204
#! Distance metric learning and nonlinear dimensionality reduction are two interesting and active topics in recent years. However, the connection between them is not thoroughly studied yet. In this paper, a transductive framework of distance metric learning is proposed and its close connection with many nonlinear spectral dimensionality reduction methods is elaborated. Furthermore, we prove a representer theorem for our framework, linking it with function estimation in an RKHS, and making it possible for generalization to unseen test samples. In our framework, it suffices to solve a sparse eigenvalue problem, thus datasets with 105 samples can be handled. Finally, experiment results on synthetic data, several UCI databases and the MNIST handwritten digit database are shown.

#index 983869
#* Adaptive dimension reduction using discriminant analysis and K-means clustering
#@ Chris Ding;Tao Li
#t 2007
#c 19
#% 252836
#% 342621
#% 342659
#% 469422
#% 527853
#% 629648
#% 729437
#% 765518
#% 766434
#% 769928
#% 770830
#% 875975
#% 876079
#% 881468
#% 1828410
#! We combine linear discriminant analysis (LDA) and K-means clustering into a coherent framework to adaptively select the most discriminative subspace. We use K-means clustering to generate class labels and use LDA to do subspace selection. The clustering process is thus integrated with the subspace selection process and the data are then simultaneously clustered while the feature subspaces are selected. We show the rich structure of the general LDA-Km framework by examining its variants and their relationships to earlier approaches. Relations among PCA, LDA, K-means are clarified. Extensive experimental results on real-world datasets show the effectiveness of our approach.

#index 983870
#* Large-scale RLSC learning without agony
#@ Wenye Li;Kin-Hong Lee;Kwong-Sak Leung
#t 2007
#c 19
#% 2115
#% 323921
#% 342598
#% 415336
#% 416502
#% 466597
#% 716271
#% 722815
#% 1274917
#! The advances in kernel-based learning necessitate the study on solving a large-scale non-sparse positive definite linear system. To provide a deterministic approach, recent researches focus on designing fast matrix-vector multiplication techniques coupled with a conjugate gradient method. Instead of using the conjugate gradient method, our paper proposes to use a domain decomposition approach in solving such a linear system. Its convergence property and speed can be understood within von Neumann's alternating projection framework. We will report signi ficant and consistent improvements in convergence speed over the conjugate gradient method when the approach is applied to recent machine learning problems.

#index 983871
#* A novel orthogonal NMF-based belief compression for POMDPs
#@ Xin Li;William K. W. Cheung;Jiming Liu;Zhili Wu
#t 2007
#c 19
#% 706380
#% 831339
#% 881468
#% 1272055
#% 1272075
#% 1279358
#! High dimensionality of POMDP's belief state space is one major cause that makes the underlying optimal policy computation intractable. Belief compression refers to the methodology that projects the belief state space to a low-dimensional one to alleviate the problem. In this paper, we propose a novel orthogonal non-negative matrix factorization (O-NMF) for the projection. The proposed O-NMF not only factors the belief state space by minimizing the reconstruction error, but also allows the compressed POMDP formulation to be efficiently computed (due to its orthogonality) in a value-directed manner so that the value function will take same values for corresponding belief states in the original and compressed state spaces. We have tested the proposed approach using a number of benchmark problems and the empirical results confirms its effectiveness in achieving substantial computational cost saving in policy computation.

#index 983872
#* A permutation-augmented sampler for DP mixture models
#@ Percy Liang;Michael I. Jordan;Ben Taskar
#t 2007
#c 19
#% 235439
#% 528004
#% 770845
#% 840872
#% 1275202
#! We introduce a new inference algorithm for Dirichlet process mixture models. While Gibbs sampling and variational methods focus on local moves, the new algorithm makes more global moves. This is done by introducing a permutation of the data points as an auxiliary variable. The algorithm is a blocked sampler which alternates between sampling the clustering and sampling the permutation. The key to the efficiency of this approach is that it is possible to use dynamic programming to consider all exponentially many clusterings consistent with a given permutation. We also show that random projections can be used to effectively sample the permutation. The result is a stochastic hill-climbing algorithm that yields burn-in times significantly smaller than those of collapsed Gibbs sampling.

#index 983873
#* Quadratically gated mixture of experts for incomplete data classification
#@ Xuejun Liao;Hui Li;Lawrence Carin
#t 2007
#c 19
#% 169358
#% 269217
#% 840957
#% 876015
#! We introduce quadratically gated mixture of experts (QGME), a statistical model for multi-class nonlinear classification. The QGME is formulated in the setting of incomplete data, where the data values are partially observed. We show that the missing values entail joint estimation of the data manifold and the classifier, which allows adaptive imputation during classifier learning. The expectation maximization (EM) algorithm is derived for joint likelihood maximization, with adaptive imputation performed analytically in the E-step. The performance of QGME is evaluated on three benchmark data sets and the results show that the QGME yields significant improvements over competing methods.

#index 983874
#* Trust region Newton methods for large-scale logistic regression
#@ Chih-Jen Lin;Ruby C. Weng;S. Sathiya Keerthi
#t 2007
#c 19
#% 73441
#% 116149
#% 226495
#% 324931
#% 416797
#% 763708
#% 829007
#% 854813
#% 881477
#! Large-scale logistic regression arises in many applications such as document classification and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also compare it with linear SVM implementations.

#index 983875
#* Relational clustering by symmetric convex coding
#@ Bo Long;Zhongfei (Mark) Zhang;Xiaoyun Wu;Philip S. Yu
#t 2007
#c 19
#% 148149
#% 202286
#% 274612
#% 281214
#% 313959
#% 342621
#% 342659
#% 466675
#% 578670
#% 724227
#% 729918
#% 769928
#% 823343
#% 823395
#% 823396
#% 881468
#% 1279489
#! Relational data appear frequently in many machine learning applications. Relational data consist of the pairwise relations (similarities or dissimilarities) between each pair of implicit objects, and are usually stored in relation matrices and typically no other knowledge is available. Although relational clustering can be formulated as graph partitioning in some applications, this formulation is not adequate for general relational data. In this paper, we propose a general model for relational clustering based on symmetric convex coding. The model is applicable to all types of relational data and unifies the existing graph partitioning formulation. Under this model, we derive two alternative bound optimization algorithms to solve the symmetric convex coding under two popular distance functions, Euclidean distance and generalized I-divergence. Experimental evaluation and theoretical analysis show the effectiveness and great potential of the proposed model and algorithms.

#index 983876
#* Discriminant analysis in correlation similarity measure space
#@ Yong Ma;Shihong Lao;Erina Takikawa;Masato Kawade
#t 2007
#c 19
#% 80995
#% 190581
#% 235342
#% 315986
#% 722760
#% 811718
#% 829025
#% 837621
#% 838851
#% 855563
#% 900163
#% 1861871
#! Correlation is one of the most widely used similarity measures in machine learning like Euclidean and Mahalanobis distances. However, compared with proposed numerous discriminant learning algorithms in distance metric space, only a very little work has been conducted on this topic using correlation similarity measure. In this paper, we propose a novel discriminant learning algorithm in correlation measure space, Correlation Discriminant Analysis (CDA). In this framework, based on the definitions of within-class correlation and between-class correlation, the optimum transformation can be sought for to maximize the difference between them, which is in accordance with good classification performance empirically. Under different cases of the transformation, different implementations of the algorithm are given. Extensive empirical evaluations of CDA demonstrate its advantage over alternative methods.

#index 983877
#* Adaptive mesh compression in 3D computer graphics using multiscale manifold learning
#@ Sridhar Mahadevan
#t 2007
#c 19
#% 58636
#% 91027
#% 196994
#% 274612
#% 308558
#% 313959
#% 765552
#% 791394
#% 876020
#! This paper investigates compression of 3D objects in computer graphics using manifold learning. Spectral compression uses the eigenvectors of the graph Laplacian of an object's topology to adaptively compress 3D objects. 3D compression is a challenging application domain: object models can have 105 vertices, and reliably computing the basis functions on large graphs is numerically challenging. In this paper, we introduce a novel multiscale manifold learning approach to 3D mesh compression using diffusion wavelets, a general extension of wavelets to graphs with arbitrary topology. Unlike the "global" nature of Laplacian bases, diffusion wavelet bases are compact, and multiscale in nature. We decompose large graphs using a fast graph partitioning method, and combine local multiscale wavelet bases computed on each subgraph. We present results showing that multiscale diffusion wavelets bases are superior to the Laplacian bases for adaptive compression of large 3D objects.

#index 983878
#* Simple, robust, scalable semi-supervised learning via expectation regularization
#@ Gideon S. Mann;Andrew McCallum
#t 2007
#c 19
#% 266292
#% 464434
#% 464465
#% 466263
#% 740953
#% 833913
#% 840967
#% 854813
#% 879624
#% 916788
#% 938713
#% 939375
#% 939380
#% 939527
#% 1269487
#! Although semi-supervised learning has been an active area of research, its use in deployed applications is still relatively rare because the methods are often difficult to implement, fragile in tuning, or lacking in scalability. This paper presents expectation regularization, a semi-supervised learning method for exponential family parametric models that augments the traditional conditional label-likelihood objective function with an additional term that encourages model predictions on unlabeled data to match certain expectations---such as label priors. The method is extremely easy to implement, scales as well as logistic regression, and can handle non-independent features. We present experiments on five different data sets, showing accuracy improvements over other semi-supervised methods.

#index 983879
#* Automatic shaping and decomposition of reward functions
#@ Bhaskara Marthi
#t 2007
#c 19
#% 178906
#% 286423
#% 393786
#% 464296
#% 464622
#% 466230
#% 466262
#% 578674
#% 647256
#% 876006
#% 1250384
#% 1271827
#% 1271996
#% 1272002
#% 1650589
#! This paper investigates the problem of automatically learning how to restructure the reward function of a Markov decision process so as to speed up reinforcement learning. We begin by describing a method that learns a shaped reward function given a set of state and temporal abstractions. Next, we consider decomposition of the per-timestep reward in multieffector problems, in which the overall agent can be decomposed into multiple units that are concurrently carrying out various tasks. We show by example that to find a good reward decomposition, it is often necessary to first shape the rewards appropriately. We then give a function approximation algorithm for solving both problems together. Standard reinforcement learning algorithms can be augmented with our methods, and we show experimentally that in each case, significantly faster learning results.

#index 983880
#* Asymmetric boosting
#@ Hamed Masnadi-Shirazi;Nuno Vasconcelos
#t 2007
#c 19
#% 235377
#% 280437
#% 466268
#% 466561
#% 729437
#% 765522
#% 808760
#% 1705261
#! A cost-sensitive extension of boosting, denoted as asymmetric boosting, is presented. Unlike previous proposals, the new algorithm is derived from sound decision-theoretic principles, which exploit the statistical interpretation of boosting to determine a principled extension of the boosting loss. Similarly to AdaBoost, the cost-sensitive extension minimizes this loss by gradient descent on the functional space of convex combinations of weak learners, and produces large margin detectors. It is shown that asymmetric boosting is fully compatible with AdaBoost, in the sense that it becomes the latter when errors are weighted equally. Experimental evidence is provided to demonstrate the claims of cost-sensitivity and large margin. The algorithm is also applied to the computer vision problem of face detection, where it is shown to outperform a number of previous heuristic proposals for cost-sensitive boosting (AdaCost, CSB0, CSB1, CSB2, asymmetric-AdaBoost, AdaC1, AdaC2 and AdaC3).

#index 983881
#* Linear and nonlinear generative probabilistic class models for shape contours
#@ Graham McNeill;Sethu Vijayakumar
#t 2007
#c 19
#% 611523
#% 724346
#% 744797
#% 883849
#% 891559
#% 1335352
#! We introduce a robust probabilistic approach to modeling shape contours based on a lowdimensional, nonlinear latent variable model. In contrast to existing techniques that use objective functions in data space without explicit noise models, we are able to extract complex shape variation from noisy data. Most approaches to learning shape models slide observed data points around fixed contours and hence, require a correctly labeled 'reference shape' to prevent degenerate solutions. In our method, unobserved curves are reparameterized to explain the fixed data points, so this problem does not arise. The proposed algorithms are suitable for use with arbitrary basis functions and are applicable to both open and closed shapes; their effectiveness is demonstrated through illustrative examples, quantitative assessment on benchmark data sets and a visualization task.

#index 983882
#* Bottom-up learning of Markov logic network structure
#@ Lilyana Mihalkova;Raymond J. Mooney
#t 2007
#c 19
#% 44876
#% 174161
#% 266215
#% 396021
#% 398846
#% 449508
#% 840890
#% 850430
#% 1057202
#% 1718473
#! Markov logic networks (MLNs) are a statistical relational model that consists of weighted firstorder clauses and generalizes first-order logic and Markov networks. The current state-of-the-art algorithm for learning MLN structure follows a top-down paradigm where many potential candidate structures are systematically generated without considering the data and then evaluated using a statistical measure of their fit to the data. Even though this existing algorithm outperforms an impressive array of benchmarks, its greedy search is susceptible to local maxima or plateaus. We present a novel algorithm for learning MLN structure that follows a more bottom-up approach to address this problem. Our algorithm uses a "propositional" Markov network learning method to construct "template" networks that guide the construction of candidate clauses. Our algorithm significantly improves accuracy and learning time over the existing topdown approach in three real-world domains.

#index 983883
#* Mixtures of hierarchical topics with Pachinko allocation
#@ David Mimno;Wei Li;Andrew McCallum
#t 2007
#c 19
#% 722904
#% 788094
#% 876017
#% 876067
#! The four-level pachinko allocation model (PAM) (Li & McCallum, 2006) represents correlations among topics using a DAG structure. It does not, however, represent a nested hierarchy of topics, with some topical word distributions representing the vocabulary that is shared among several more specific topics. This paper presents hierarchical PAM---an enhancement that explicitly represents a topic hierarchy. This model can be seen as combining the advantages of hLDA's topical hierarchy representation with PAM's ability to mix multiple leaves of the topic hierarchy. Experimental results show improvements in likelihood of held-out documents, as well as mutual information between automatically-discovered topics and humangenerated categories such as journals.

#index 983884
#* Three new graphical models for statistical language modelling
#@ Andriy Mnih;Geoffrey Hinton
#t 2007
#c 19
#% 450888
#% 722928
#% 748738
#% 891060
#% 939879
#! The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models.

#index 983885
#* Fast and effective kernels for relational learning from texts
#@ Alessandro Moschitti;Fabio Massimo Zanzotto
#t 2007
#c 19
#% 198058
#% 262096
#% 269217
#% 278107
#% 722926
#% 742218
#% 796230
#% 815896
#% 939551
#% 1280237
#% 1665151
#% 1718473
#! In this paper, we define a family of syntactic kernels for automatic relational learning from pairs of natural language sentences. We provide an efficient computation of such models by optimizing the dynamic programming algorithm of the kernel evaluation. Experiments with Support Vector Machines and the above kernels show the effectiveness and efficiency of our approach on two very important natural language tasks, Textual Entailment Recognition and Question Answering.

#index 983886
#* Dimensionality reduction and generalization
#@ Sofia Mosci;Lorenzo Rosasco;Alessandro Verri
#t 2007
#c 19
#% 269226
#% 765552
#% 829023
#% 854051
#% 944004
#% 948120
#% 1815607
#! In this paper we investigate the regularization property of Kernel Principal Component Analysis (KPCA), by studying its application as a preprocessing step to supervised learning problems. We show that performing KPCA and then ordinary least squares on the projected data, a procedure known as kernel principal component regression (KPCR), is equivalent to spectral cut-off regularization, the regularization parameter being exactly the number of principal components to keep. Using probabilistic estimates for integral operators we can prove error estimates for KPCR and propose a parameter choice procedure allowing to prove consistency of the algorithm.

#index 983887
#* Unsupervised estimation for noisy-channel models
#@ Markos Mylonakis;Khalil Sima'an;Rebecca Hwa
#t 2007
#c 19
#% 95732
#% 464434
#% 529682
#% 579944
#% 742092
#% 756512
#% 815902
#% 816170
#% 840577
#% 939654
#% 940004
#! Shannon's Noisy-Channel model, which describes how a corrupted message might be reconstructed, has been the corner stone for much work in statistical language and speech processing. The model factors into two components: a language model to characterize the original message and a channel model to describe the channel's corruptive process. The standard approach for estimating the parameters of the channel model is unsupervised Maximum-Likelihood of the observation data, usually approximated using the Expectation-Maximization (EM) algorithm. In this paper we show that it is better to maximize the joint likelihood of the data at both ends of the noisy-channel. We derive a corresponding bi-directional EM algorithm and show that it gives better performance than standard EM on two tasks: (1) translation using a probabilistic lexicon and (2) adaptation of a part-of-speech tagger between related languages.

#index 983888
#* Revisiting probabilistic models for clustering with pair-wise constraints
#@ Blaine Nelson;Ira Cohen
#t 2007
#c 19
#% 464291
#% 466890
#% 724203
#% 769881
#% 812399
#% 840892
#! We revisit recently proposed algorithms for probabilistic clustering with pair-wise constraints between data points. We evaluate and compare existing techniques in terms of robustness to misspecified constraints. We show that the technique that strictly enforces the given constraints, namely the chunklet model, produces poor results even under a small number of misspecified constraints. We further show that methods that penalize constraint violation are more robust to misspecified constraints but have undesirable local behaviors. Based on this evaluation, we propose a new learning technique, extending the chunklet model to allow soft constraints represented by an intuitive measure of confidence in the constraint.

#index 983889
#* Comparisons of sequence labeling algorithms and extensions
#@ Nam Nguyen;Yunsong Guo
#t 2007
#c 19
#% 216079
#% 304935
#% 464434
#% 614037
#% 722816
#% 770854
#% 829043
#% 854636
#% 874707
#% 1202000
#! In this paper, we survey the current state-of-art models for structured learning problems, including Hidden Markov Model (HMM), Conditional Random Fields (CRF), Averaged Perceptron (AP), Structured SVMs (SVMstruct), Max Margin Markov Networks (M3N), and an integration of search and learning algorithm (SEARN). With all due tuning efforts of various parameters of each model, on the data sets we have applied the models to, we found that SVMstruct enjoys better performance compared with the others. In addition, we also propose a new method which we call the Structured Learning Ensemble (SLE) to combine these structured learning models. Empirical results show that our SLE algorithm provides more accurate solutions compared with the best results of the individual models.

#index 983890
#* Multi-task learning for sequential data via iHMMs and the nested Dirichlet process
#@ Kai Ni;Lawrence Carin;David Dunson
#t 2007
#c 19
#% 236497
#% 476708
#% 961246
#% 1759404
#! A new hierarchical nonparametric Bayesian model is proposed for the problem of multitask learning (MTL) with sequential data. Sequential data are typically modeled with a hidden Markov model (HMM), for which one often must choose an appropriate model structure (number of states) before learning. Here we model sequential data from each task with an infinite hidden Markov model (iHMM), avoiding the problem of model selection. The MTL for iHMMs is implemented by imposing a nested Dirichlet process (nDP) prior on the base distributions of the iHMMs. The nDP-iHMM MTL method allows us to perform task-level clustering and data-level clustering simultaneously, with which the learning for individual iHMMs is enhanced and between-task similarities are learned. Learning and inference for the nDP-iHMM MTL are based on a Gibbs sampler. The effectiveness of the framework is demonstrated using synthetic data as well as real music data.

#index 983891
#* Regression on manifolds using kernel dimension reduction
#@ Jens Nilsson;Fei Sha;Michael I. Jordan
#t 2007
#c 19
#% 593047
#% 722936
#% 763698
#% 770839
#% 840931
#% 840933
#% 876078
#% 961218
#! We study the problem of discovering a manifold that best preserves information relevant to a nonlinear regression. Solving this problem involves extending and uniting two threads of research. On the one hand, the literature on sufficient dimension reduction has focused on methods for finding the best linear subspace for nonlinear regression; we extend this to manifolds. On the other hand, the literature on manifold learning has focused on unsupervised dimensionality reduction; we extend this to the supervised setting. Our approach to solving the problem involves combining the machinery of kernel dimension reduction with Laplacian eigenmaps. Specifically, we optimize cross-covariance operators in kernel feature spaces that are induced by the normalized graph Laplacian. The result is a highly flexible method in which no strong assumptions are made on the regression function or on the distribution of the covariates. We illustrate our methodology on the analysis of global temperature data and image manifolds.

#index 983892
#* Learning state-action basis functions for hierarchical MDPs
#@ Sarah Osentoski;Sridhar Mahadevan
#t 2007
#c 19
#% 274612
#% 286423
#% 466751
#% 840904
#% 876001
#% 983852
#% 1250345
#% 1269758
#% 1279356
#! This paper introduces a new approach to action-value function approximation by learning basis functions from a spectral decomposition of the state-action manifold. This paper extends previous work on using Laplacian bases for value function approximation by using the actions of the agent as part of the representation when creating basis functions. The approach results in a nonlinear learned representation particularly suited to approximating action-value functions, without incurring the wasteful duplication of state bases in previous work. We discuss two techniques to create state-action graphs: off-policy and on-policy. We show that these graphs have a greater expressive power and have better performance over state-based Laplacian basis functions in domains modeled as Semi-Markov Decision Processes (SMDPs). We present a simple graph partitioning method to scale the approach to large discrete MDPs.

#index 983893
#* A fast linear separability test by projection of positive points on subspaces
#@ Yogananda A P;M Narasimha Murthy;Lakshmi Gopal
#t 2007
#c 19
#% 55384
#% 63424
#% 212297
#% 269217
#% 309208
#% 729437
#% 803575
#% 1861659
#! A geometric and non parametric procedure for testing if two finite set of points are linearly separable is proposed. The Linear Separability Test is equivalent to a test that determines if a strictly positive point h 0 exists in the range of a matrix A (related to the points in the two finite sets). The algorithm proposed in the paper iteratively checks if a strictly positive point exists in a subspace by projecting a strictly positive vector with equal co-ordinates (p), on the subspace. At the end of each iteration, the subspace is reduced to a lower dimensional subspace. The test is completed within r ≤ min(n, d + 1) steps, for both linearly separable and non separable problems (r is the rank of A, n is the number of points and d is the dimension of the space containing the points). The worst case time complexity of the algorithm is O(nr3) and space complexity of the algorithm is O(nd). A small review of some of the prominent algorithms and their time complexities is included. The worst case computational complexity of our algorithm is lower than the worst case computational complexity of Simplex, Perceptron, Support Vector Machine and Convex Hull Algorithms, if d2/3.

#index 983894
#* Multi-armed bandit problems with dependent arms
#@ Sandeep Pandey;Deepayan Chakrabarti;Deepak Agarwal
#t 2007
#c 19
#% 132697
#% 363744
#% 425053
#! We provide a framework to exploit dependencies among arms in multi-armed bandit problems, when the dependencies are in the form of a generative model on clusters of arms. We find an optimal MDP-based policy for the discounted reward case, and also give an approximation of it with formal error guarantee. We discuss lower bounds on regret in the undiscounted reward scenario, and propose a general two-level bandit policy for it. We propose three different instantiations of our general policy and provide theoretical justifications of how the regret of the instantiated policies depend on the characteristics of the clusters. Finally, we empirically demonstrate the efficacy of our policies on large-scale real-world and synthetic data, and show that they significantly outperform classical policies designed for bandits with independent arms.

#index 983895
#* Learning for efficient retrieval of structured data with noisy queries
#@ Charles Parker;Alan Fern;Prasad Tadepalli
#t 2007
#c 19
#% 169940
#% 194192
#% 479649
#% 731409
#% 751601
#% 770763
#% 777419
#% 875957
#% 1250578
#% 1688286
#! Increasingly large collections of structured data necessitate the development of efficient, noise-tolerant retrieval tools. In this work, we consider this issue and describe an approach to learn a similarity function that is not only accurate, but that also increases the effectiveness of retrieval data structures. We present an algorithm that uses functional gradient boosting to maximize both retrieval accuracy and the retrieval efficiency of vantage point trees. We demonstrate the effectiveness of our approach on two datasets, including a moderately sized real-world dataset of folk music.

#index 983896
#* Analyzing feature generation for value-function approximation
#@ Ronald Parr;Christopher Painter-Wakefield;Lihong Li;Michael Littman
#t 2007
#c 19
#% 203596
#% 229931
#% 449561
#% 496267
#% 520224
#% 707761
#% 734920
#% 876001
#! We analyze a simple, Bellman-error-based approach to generating basis functions for value-function approximation. We show that it generates orthogonal basis functions that provably tighten approximation error bounds. We also illustrate the use of this approach in the presence of noise on some sample problems.

#index 983897
#* Reinforcement learning by reward-weighted regression for operational space control
#@ Jan Peters;Stefan Schaal
#t 2007
#c 19
#% 225838
#% 406382
#% 418204
#% 576218
#% 856763
#% 1272286
#! Many robot control problems of practical importance, including operational space control, can be reformulated as immediate reward reinforcement learning problems. However, few of the known optimization or reinforcement learning algorithms can be used in online learning control for robots, as they are either prohibitively slow, do not scale to interesting domains of complex robots, or require trying out policies generated by random search, which are infeasible for a physical system. Using a generalization of the EM-base reinforcement learning framework suggested by Dayan & Hinton, we reduce the problem of learning with immediate rewards to a reward-weighted regression problem with an adaptive, integrated reward transformation for faster convergence. The resulting algorithm is efficient, learns smoothly without dangerous jumps in solution space, and works well in applications of complex high degree-of-freedom robots.

#index 983898
#* Tracking value function dynamics to improve reinforcement learning with piecewise linear function approximation
#@ Chee Wee Phua;Robert Fitch
#t 2007
#c 19
#% 12422
#% 203596
#% 384911
#% 425076
#% 425080
#% 734920
#% 846426
#% 866319
#% 878156
#% 1250563
#% 1271827
#% 1271971
#! Reinforcement learning algorithms can become unstable when combined with linear function approximation. Algorithms that minimize the mean-square Bellman error are guaranteed to converge, but often do so slowly or are computationally expensive. In this paper, we propose to improve the convergence speed of piecewise linear function approximation by tracking the dynamics of the value function with the Kalman filter using a random-walk model. We cast this as a general framework in which we implement the TD, Q-Learning and MAXQ algorithms for different domains, and report empirical results demonstrating improved learning speed over previous methods.

#index 983899
#* Self-taught learning: transfer learning from unlabeled data
#@ Rajat Raina;Alexis Battle;Honglak Lee;Benjamin Packer;Andrew Y. Ng
#t 2007
#c 19
#% 236497
#% 267036
#% 304917
#% 311027
#% 770857
#% 784995
#% 793248
#% 812600
#% 836686
#% 883972
#% 883981
#% 916788
#! We present a new machine learning framework called "self-taught learning" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.

#index 983900
#* Online discovery of similarity mappings
#@ Alexander Rakhlin;Jacob Abernethy;Peter L. Bartlett
#t 2007
#c 19
#% 165663
#% 425048
#% 850011
#% 869231
#% 871302
#% 1396688
#! We consider the problem of choosing, sequentially, a map which assigns elements of a set A to a few elements of a set B. On each round, the algorithm suffers some cost associated with the chosen assignment, and the goal is to minimize the cumulative loss of these choices relative to the best map on the entire sequence. Even though the offline problem of finding the best map is provably hard, we show that there is an equivalent online approximation algorithm, Randomized Map Prediction (RMP), that is efficient and performs nearly as well. While drawing upon results from the "Online Prediction with Expert Advice" setting, we show how RMP can be utilized as an online approach to several standard batch problems. We apply RMP to online clustering as well as online feature selection and, surprisingly, RMP often outperforms the standard batch algorithms on these problems.

#index 983901
#* More efficiency in multiple kernel learning
#@ Alain Rakotomamonjy;Francis Bach;Stéphane Canu;Yves Grandvalet
#t 2007
#c 19
#% 263850
#% 310557
#% 416838
#% 425040
#% 763697
#% 770846
#% 829031
#% 832903
#% 927910
#% 961190
#! An efficient and general multiple kernel learning (MKL) algorithm has been recently proposed by Sonnenburg et al. (2006). This approach has opened new perspectives since it makes the MKL approach tractable for large-scale problems, by iteratively using existing support vector machine code. However, it turns out that this iterative algorithm needs several iterations before converging towards a reasonable solution. In this paper, we address the MKL problem through an adaptive 2-norm regularization formulation. Weights on each kernel matrix are included in the standard SVM empirical risk minimization problem with a l1 constraint to encourage sparsity. We propose an algorithm for solving this problem and provide an new insight on MKL algorithms based on block 1-norm regularization by showing that the two approaches are equivalent. Experimental results show that the resulting algorithm converges rapidly and its efficiency compares favorably to other MKL algorithms.

#index 983902
#* Graph clustering with network structure indices
#@ Matthew J. Rattigan;Marc Maier;David Jensen
#t 2007
#c 19
#% 23614
#% 495944
#% 769935
#% 881491
#% 1376302
#! Graph clustering has become ubiquitous in the study of relational data sets. We examine two simple algorithms: a new graphical adaptation of the k-medoids algorithm and the Girvan-Newman method based on edge betweenness centrality. We show that they can be effective at discovering the latent groups or communities that are defined by the link structure of a graph. However, both approaches rely on prohibitively expensive computations, given the size of modern relational data sets. Network structure indices (NSIs) are a proven technique for indexing network structure and efficiently finding short paths. We show how incorporating NSIs into these graph clustering algorithms can overcome these complexity limitations. We also present promising quantitative and qualitative evaluations of the modified algorithms on synthetic and real data sets.

#index 983903
#* Restricted Boltzmann machines for collaborative filtering
#@ Ruslan Salakhutdinov;Andriy Mnih;Geoffrey Hinton
#t 2007
#c 19
#% 397153
#% 450888
#% 770859
#% 828046
#% 891060
#% 1650298
#! Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.

#index 983904
#* Sample compression bounds for decision trees
#@ Mohak Shah
#t 2007
#c 19
#% 26125
#% 252042
#% 272501
#% 276528
#% 387653
#% 466074
#% 562962
#% 722909
#% 803574
#% 837668
#% 926881
#% 948004
#% 980058
#! We propose a formulation of the Decision Tree learning algorithm in the Compression settings and derive tight generalization error bounds. In particular, we propose Sample Compression and Occam's Razor bounds. We show how such bounds, unlike the VC dimension or Rademacher complexities based bounds, are more general and can also perform a margin-sparsity trade-off to obtain better classifers. Potentially, these risk bounds can also guide the model selection process and replace traditional pruning strategies.

#index 983905
#* Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
#@ Shai Shalev-Shwartz;Yoram Singer;Nathan Srebro
#t 2007
#c 19
#% 269217
#% 269218
#% 302390
#% 309208
#% 382854
#% 722815
#% 757953
#% 770754
#% 881477
#% 961152
#% 961159
#% 1674795
#% 1759695
#% 1815223
#! We describe and analyze a simple and effective iterative algorithm for solving the optimization problem cast by Support Vector Machines (SVM). Our method alternates between stochastic gradient descent steps and projection steps. We prove that the number of iterations required to obtain a solution of accuracy ε is Õ(1/ε). In contrast, previous analyses of stochastic gradient descent methods require Ω (1/ε2) iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/λ, where λ is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is Õ (d/(λε)), where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach can seamlessly be adapted to employ non-linear kernels while working solely on the primal objective function. We demonstrate the efficiency and applicability of our approach by conducting experiments on large text classification problems, comparing our solver to existing state-of-the-art SVM solvers. For example, it takes less than 5 seconds for our solver to converge when solving a text classification problem from Reuters Corpus Volume 1 (RCV1) with 800,000 training examples.

#index 983906
#* A dependence maximization view of clustering
#@ Le Song;Alex Smola;Arthur Gretton;Karsten M. Borgwardt
#t 2007
#c 19
#% 266426
#% 318793
#% 592143
#% 722815
#% 755402
#% 763698
#% 769935
#% 770830
#% 836707
#% 876023
#% 876075
#% 1250438
#% 1673681
#% 1860974
#! We propose a family of clustering algorithms based on the maximization of dependence between the input variables and their cluster labels, as expressed by the Hilbert-Schmidt Independence Criterion (HSIC). Under this framework, we unify the geometric, spectral, and statistical dependence views of clustering, and subsume many existing algorithms as special cases (e.g. k-means and spectral clustering). Distinctive to our framework is that kernels can also be applied on the labels, which can endow them with particular structures. We also obtain a perturbation bound on the change in k-means clustering.

#index 983907
#* Supervised feature selection via dependence estimation
#@ Le Song;Alex Smola;Arthur Gretton;Karsten M. Borgwardt;Justin Bedo
#t 2007
#c 19
#% 425048
#% 465583
#% 722798
#% 722929
#% 722943
#% 763698
#% 846429
#% 906248
#% 1673681
#! We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.

#index 983908
#* Sparse eigen methods by D.C. programming
#@ Bharath K. Sriperumbudur;David A. Torres;Gert R. G. Lanckriet
#t 2007
#c 19
#% 209961
#% 299012
#% 416553
#% 576520
#% 722760
#% 722943
#! Eigenvalue problems are rampant in machine learning and statistics and appear in the context of classification, dimensionality reduction, etc. In this paper, we consider a cardinality constrained variational formulation of generalized eigenvalue problem with sparse principal component analysis (PCA) as a special case. Using l1-norm approximation to the cardinality constraint, previous methods have proposed both convex and non-convex solutions to the sparse PCA problem. In contrast, we propose a tighter approximation that is related to the negative log-likelihood of a Student's t-distribution. The problem is then framed as a d.c. (difference of convex functions) program and is solved as a sequence of locally convex programs. We show that the proposed method not only explains more variance with sparse loadings on the principal directions but also has better scalability compared to other methods. We demonstrate these results on a collection of datasets of varying dimensionality, two of which are high-dimensional gene datasets where the goal is to find few relevant genes that explain as much variance as possible.

#index 983909
#* Learning to solve game trees
#@ David Stern;Ralf Herbrich;Thore Graepel
#t 2007
#c 19
#% 241
#% 1697
#% 52797
#% 98073
#% 174161
#% 243725
#% 348582
#% 739899
#% 837649
#% 876054
#! We apply probability theory to the task of proving whether a goal can be achieved by a player in an adversarial game. Such problems are solved by searching the game tree. We view this tree as a graphical model which yields a distribution over the (Boolean) outcome of the search before it terminates. Experiments show that a best-first search algorithm guided by this distribution explores a similar number of nodes as Proof-Number Search to solve Go problems. Knowledge is incorporated into search by using domain-specific models to provide prior distributions over the values of leaf nodes of the game tree. These are surrogate for the unexplored parts of the tree. The parameters of these models can be learned from previous search trees. Experiments on Go show that the speed of problem solving can be increased by orders of magnitude by this technique but care must be taken to avoid over-fitting.

#index 983910
#* Robust mixtures in the presence of measurement errors
#@ Jianyong Sun;Ata Kabán;Somak Raychaudhury
#t 2007
#c 19
#% 303620
#% 345829
#% 424831
#% 875949
#% 906551
#% 1377410
#! We develop a mixture-based approach to robust density modeling and outlier detection for experimental multivariate data that includes measurement error information. Our model is designed to infer atypical measurements that are not due to errors, aiming to retrieve potentially interesting peculiar objects. Since exact inference is not possible in this model, we develop a tree-structured variational EM solution. This compares favorably against a fully factorial approximation scheme, approaching the accuracy of a Markov-Chain-EM, while maintaining computational simplicity. We demonstrate the benefits of including measurement errors in the model, in terms of improved outlier detection rates in varying measurement uncertainty conditions. We then use this approach for detecting peculiar quasars from an astrophysical survey, given photometric measurements with errors.

#index 983911
#* A kernel-based causal learning algorithm
#@ Xiaohai Sun;Dominik Janzing;Bernhard Schölkopf;Kenji Fukumizu
#t 2007
#c 19
#% 129497
#% 129987
#% 266426
#% 297171
#% 400980
#% 722798
#% 722815
#% 722887
#% 722900
#% 763698
#% 879181
#% 961258
#% 1673681
#! We describe a causal learning method, which employs measuring the strength of statistical dependences in terms of the Hilbert-Schmidt norm of kernel-based cross-covariance operators. Following the line of the common faithfulness assumption of constraint-based causal learning, our approach assumes that a variable Z is likely to be a common effect of X and Y, if conditioning on Z increases the dependence between X and Y. Based on this assumption, we collect "votes" for hypothetical causal directions and orient the edges by the majority principle. In most experiments with known causal structures, our method provided plausible results and outperformed the conventional constraint-based PC algorithm.

#index 983912
#* Piecewise pseudolikelihood for efficient training of conditional random fields
#@ Charles Sutton;Andrew McCallum
#t 2007
#c 19
#% 464434
#% 466892
#% 709066
#% 770844
#% 816186
#% 840856
#% 855119
#% 876066
#% 902646
#% 1261597
#% 1289530
#! Discriminative training of graphical models can be expensive if the variables have large cardinality, even if the graphical structure is tractable. In such cases, pseudolikelihood is an attractive alternative, because its running time is linear in the variable cardinality, but on some data its accuracy can be poor. Piecewise training (Sutton & McCallum, 2005) can have better accuracy but does not scale as well in the variable cardinality. In this paper, we introduce piecewise pseudolikelihood, which retains the computational efficiency of pseudolikelihood but can have much better accuracy. On several benchmark NLP data sets, piecewise pseudolikelihood has better accuracy than standard pseudolikelihood, and in many cases nearly equivalent to maximum likelihood, with five to ten times less training time than batch CRF training.

#index 983913
#* On the role of tracking in stationary environments
#@ Richard S. Sutton;Anna Koop;David Silver
#t 2007
#c 19
#% 183499
#% 449561
#% 876006
#% 1274923
#! It is often thought that learning algorithms that track the best solution, as opposed to converging to it, are important only on nonstationary problems. We present three results suggesting that this is not so. First we illustrate in a simple concrete example, the Black and White problem, that tracking can perform better than any converging algorithm on a stationary problem. Second, we show the same point on a larger, more realistic problem, an application of temporal difference learning to computer Go. Our third result suggests that tracking in stationary problems could be important for metalearning research (e.g., learning to learn, feature selection, transfer). We apply a metalearning algorithm for step-size adaptation, IDBD (Sutton, 1992a), to the Black and White problem, showing that meta-learning has a dramatic long-term effect on performance whereas, on an analogous converging problem, meta-learning has only a small second-order effect. This small result suggests a way of eventually overcoming a major obstacle to meta-learning research: the lack of an independent methodology for task selection.

#index 983914
#* Cross-domain transfer for reinforcement learning
#@ Matthew E. Taylor;Peter Stone
#t 2007
#c 19
#% 203602
#% 384911
#% 769576
#% 926881
#% 1250572
#% 1250585
#% 1665160
#% 1693747
#! A typical goal for transfer learning algorithms is to utilize knowledge gained in a source task to learn a target task faster. Recently introduced transfer methods in reinforcement learning settings have shown considerable promise, but they typically transfer between pairs of very similar tasks. This work introduces Rule Transfer, a transfer algorithm that first learns rules to summarize a source task policy and then leverages those rules to learn faster in a target task. This paper demonstrates that Rule Transfer can effectively speed up learning in Keepaway, a benchmark RL problem in the robot soccer domain, based on experience from source tasks in the gridworld domain. We empirically show, through the use of three distinct transfer metrics, that Rule Transfer is effective across these domains.

#index 983915
#* Incremental Bayesian networks for structure prediction
#@ Ivan Titov;James Henderson
#t 2007
#c 19
#% 130878
#% 277467
#% 277507
#% 708948
#% 715394
#% 716892
#% 742218
#% 816167
#% 938665
#% 939353
#% 939917
#% 1269536
#! We propose a class of graphical models appropriate for structure prediction problems where the model structure is a function of the output structure. Incremental Sigmoid Belief Networks (ISBNs) avoid the need to sum over the possible model structures by using directed arcs and incrementally specifying the model structure. Exact inference in such directed models is not tractable, but we derive two efficient approximations based on mean field methods, which prove effective in artificial experiments. We then demonstrate their effectiveness on a benchmark natural language parsing task, where they achieve state-of-the-art accuracy. Also, the model which is a closer approximation to an ISBN has better parsing accuracy, suggesting that ISBNs are an appropriate abstract model of structure prediction tasks.

#index 983916
#* Classifying matrices with a spectral regularization
#@ Ryota Tomioka;Kazuyuki Aihara
#t 2007
#c 19
#% 757953
#% 1674771
#! We propose a method for the classification of matrices. We use a linear classifier with a novel regularization scheme based on the spectral l1-norm of its coefficient matrix. The spectral regularization not only provides a principled way of complexity control but also enables automatic determination of the rank of the coefficient matrix. Using the Linear Matrix Inequality technique, we formulate the inference task as a single convex optimization problem. We apply our method to the motor-imagery EEG classification problem. The method not only improves upon conventional methods in the classification performance but also determines a subspace in the signal that concentrates discriminative information without any additional feature extraction step. The method can be easily generalized to regression problems by changing the loss function. Connections to other methods are also discussed.

#index 983917
#* Approximate maximum margin algorithms with rules controlled by the number of mistakes
#@ Petroula Tsampouka;John Shawe-Taylor
#t 2007
#c 19
#% 269217
#% 309208
#% 425046
#% 722814
#% 881477
#% 1665161
#% 1699646
#! We present a family of incremental Perceptron-like algorithms (PLAs) with margin in which both the "effective" learning rate, defined as the ratio of the learning rate to the length of the weight vector, and the misclassification condition are entirely controlled by rules involving (powers of) the number of mistakes. We examine the convergence of such algorithms in a finite number of steps and show that under some rather mild conditions there exists a limit of the parameters involved in which convergence leads to classification with maximum margin. An experimental comparison of algorithms belonging to this family with other large margin PLAs and decomposition SVMs is also presented.

#index 983918
#* Simpler core vector machines with enclosing balls
#@ Ivor W. Tsang;Andras Kocsor;James T. Kwok
#t 2007
#c 19
#% 760985
#% 803575
#% 840949
#% 881477
#% 915284
#% 915287
#% 916781
#% 959454
#% 961152
#% 961190
#% 1274887
#% 1699634
#! The core vector machine (CVM) is a recent approach for scaling up kernel methods based on the notion of minimum enclosing ball (MEB). Though conceptually simple, an efficient implementation still requires a sophisticated numerical solver. In this paper, we introduce the enclosing ball (EB) problem where the ball's radius is fixed and thus does not have to be minimized. We develop efficient (1 + e)-approximation algorithms that are simple to implement and do not require any numerical solver. For the Gaussian kernel in particular, a suitable choice of this (fixed) radius is easy to determine, and the center obtained from the (1 + e)-approximation of this EB problem is close to the center of the corresponding MEB. Experimental results show that the proposed algorithm has accuracies comparable to the other large-scale SVM implementations, but can handle very large data sets and is even faster than the CVM in general.

#index 983919
#* Entire regularization paths for graph data
#@ Koji Tsuda
#t 2007
#c 19
#% 243728
#% 299985
#% 629708
#% 769951
#% 1663621
#! Graph data such as chemical compounds and XML documents are getting more common in many application domains. A main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of indicators of all possible subgraph patterns, the dimensionality gets too large for usual statistical methods. We propose an efficient method to select a small number of salient patterns by regularization path tracking. The generation of useless patterns is minimized by progressive extension of the search space. In experiments, it is shown that our technique is considerably more efficient than a simpler approach based on frequent substructure mining.

#index 983920
#* Discriminative Gaussian process latent variable model for classification
#@ Raquel Urtasun;Trevor Darrell
#t 2007
#c 19
#% 840968
#% 876009
#% 876058
#% 891549
#% 943904
#! Supervised learning is difficult with high dimensional input spaces and very small training sets, but accurate classification may be possible if the data lie on a low-dimensional manifold. Gaussian Process Latent Variable Models can discover low dimensional manifolds given only a small number of examples, but learn a latent space without regard for class labels. Existing methods for discriminative manifold learning (e.g., LDA, GDA) do constrain the class distribution in the latent space, but are generally deterministic and may not generalize well with limited training data. We introduce a method for Gaussian Process Classification using latent variable models trained with discriminative priors over the latent space, which can learn a discriminative latent space from a small training set.

#index 983921
#* Experimental perspectives on learning from imbalanced data
#@ Jason Van Hulse;Taghi M. Khoshgoftaar;Amri Napolitano
#t 2007
#c 19
#% 136350
#% 296161
#% 400847
#% 765523
#% 926881
#% 1271973
#% 1272000
#% 1708211
#! We present a comprehensive suite of experimentation on the subject of learning from imbalanced data. When classes are imbalanced, many learning algorithms can suffer from the perspective of reduced performance. Can data sampling be used to improve the performance of learners built from imbalanced data? Is the effectiveness of sampling related to the type of learner? Do the results change if the objective is to optimize different performance metrics? We address these and other issues in this work, showing that sampling in many cases will improve classifier performance.

#index 983922
#* Learning from interpretations: a rooted kernel for ordered hypergraphs
#@ Gabriel Wachman;Roni Khardon
#t 2007
#c 19
#% 175383
#% 217072
#% 309208
#% 449508
#% 464289
#% 464612
#% 727896
#% 769891
#% 840863
#% 857307
#% 876064
#% 961252
#% 1250568
#! The paper presents a kernel for learning from ordered hypergraphs, a formalization that captures relational data as used in Inductive Logic Programming (ILP). The kernel generalizes previous approaches to graph kernels in calculating similarity based on walks in the hypergraph. Experiments on challenging chemical datasets demonstrate that the kernel outperforms existing ILP methods, and is competitive with state-of-the-art graph kernels. The experiments also demonstrate that the encoding of graph data can affect performance dramatically, a fact that can be useful beyond kernel methods.

#index 983923
#* A kernel path algorithm for support vector machines
#@ Gang Wang;Dit-Yan Yeung;Frederick H. Lochovsky
#t 2007
#c 19
#% 763697
#% 770846
#% 793245
#% 829029
#% 866297
#% 876069
#% 961190
#% 1860761
#! The choice of the kernel function which determines the mapping between the input space and the feature space is of crucial importance to kernel methods. The past few years have seen many efforts in learning either the kernel function or the kernel matrix. In this paper, we address this model selection issue by learning the hyperparameter of the kernel function for a support vector machine (SVM). We trace the solution path with respect to the kernel hyperparameter without having to train the model multiple times. Given a kernel hyperparameter value and the optimal solution obtained for that value, we find that the solutions of the neighborhood hyperparameters can be calculated exactly. However, the solution path does not exhibit piecewise linearity and extends nonlinearly. As a result, the breakpoints cannot be computed in advance. We propose a method to approximate the breakpoints. Our method is both efficient and general in the sense that it can be applied to many kernel functions in common use.

#index 983924
#* Dirichlet aggregation: unsupervised learning towards an optimal metric for proportional data
#@ Hua-Yan Wang;Hongbin Zha;Hong Qin
#t 2007
#c 19
#% 120270
#% 325683
#% 329569
#% 406493
#% 592028
#% 722904
#% 760805
#% 770859
#% 784995
#% 788043
#% 875987
#% 889076
#% 1730621
#! Proportional data (normalized histograms) have been frequently occurring in various areas, and they could be mathematically abstracted as points residing in a geometric simplex. A proper distance metric on this simplex is of importance in many applications including classification and information retrieval. In this paper, we develop a novel framework to learn an optimal metric on the simplex. Major features of our approach include: 1) its flexibility to handle correlations among bins/dimensions; 2) widespread applicability without being limited to ad hoc backgrounds; and 3) a "real" global solution in contrast to existing traditional local approaches. The technical essence of our approach is to fit a parametric distribution to the observed empirical data in the simplex. The distribution is parameterized by affinities between simplex vertices, which is learned via maximizing likelihood of observed data. Then, these affinities induce a metric on the simplex, defined as the earth mover's distance equipped with ground distances derived from simplex vertex affinities.

#index 983925
#* Transductive regression piloted by inter-manifold relations
#@ Huan Wang;Shuicheng Yan;Thomas Huang;Jianzhuang Liu;Xiaoou Tang
#t 2007
#c 19
#% 252011
#% 593047
#% 770767
#% 787098
#% 875962
#! In this paper, we present a novel semisupervised regression algorithm working on multiclass data that may lie on multiple manifolds. Unlike conventional manifold regression algorithms that do not consider the class distinction of samples, our method introduces the class information to the regression process and tries to exploit the similar configurations shared by the label distribution of multi-class data. To utilize the correlations among data from different classes, we develop a cross-manifold label propagation process and employ labels from different classes to enhance the regression performance. The interclass relations are coded by a set of intermanifold graphs and a regularization item is introduced to impose inter-class smoothness on the possible solutions. In addition, the algorithm is further extended with the kernel trick for predicting labels of the out-of-sample data even without class information. Experiments on both synthesized data and real world problems validate the effectiveness of the proposed framework for semisupervised regression.

#index 983926
#* Multifactor Gaussian process models for style-content separation
#@ Jack M. Wang;David J. Fleet;Aaron Hertzmann
#t 2007
#c 19
#% 251365
#% 269223
#% 308505
#% 316143
#% 436429
#% 457831
#% 581093
#% 627860
#% 771029
#% 771053
#% 815955
#% 836683
#% 836721
#% 857076
#% 875377
#% 883872
#% 891549
#% 916787
#% 943904
#! We introduce models for density estimation with multiple, hidden, continuous factors. In particular, we propose a generalization of multilinear models using nonlinear basis functions. By marginalizing over the weights, we obtain a multifactor form of the Gaussian process latent variable model. In this model, each factor is kernelized independently, allowing nonlinear mappings from any particular factor to the data. We learn models for human locomotion data, in which each pose is generated by factors representing the person's identity, gait, and the current state of motion. We demonstrate our approach using time-series prediction, and by synthesizing novel animation from the model.

#index 983927
#* Hybrid huberized support vector machines for microarray classification
#@ Li Wang;Ji Zhu;Hui Zou
#t 2007
#c 19
#% 190581
#% 197394
#% 269207
#% 420077
#% 425048
#% 466084
#! The large number of genes and the relatively small number of samples are typical characteristics for microarray data. These characteristics pose challenges for both sample classification and relevant gene selection. The support vector machine (SVM) is a widely used classification technique, and previous studies have demonstrated its superior classification performance in microarray analysis. However, a major limitation is that the SVM can not perform automatic gene selection. To overcome this limitation, we propose the hybrid huberized support vector machine (HHSVM). The HHSVM uses the huberized hinge loss function and the elastic-net penalty. It has two major benefits: 1. automatic gene selection; 2. the grouping effect, where highly correlated genes tend to be selected/removed together. We also develop an efficient algorithm that computes the entire regularized solution path for HHSVM. We have applied our method to real microarray data and achieved promising results.

#index 983928
#* On learning with dissimilarity functions
#@ Liwei Wang;Cheng Yang;Jufu Feng
#t 2007
#c 19
#% 246120
#% 302391
#% 304899
#% 316780
#% 380342
#% 446720
#% 476717
#% 616088
#% 722813
#% 729344
#% 875954
#% 893456
#% 918579
#% 1855352
#! We study the problem of learning a classification task in which only a dissimilarity function of the objects is accessible. That is, data are not represented by feature vectors but in terms of their pairwise dissimilarities. We investigate the sufficient conditions for dissimilarity functions to allow building accurate classifiers. Our results have the advantages that they apply to unbounded dissimilarities and are invariant to order-preserving transformations. The theory immediately suggests a learning paradigm: construct an ensemble of decision stumps each depends on a pair of examples, then find a convex combination of them to achieve a large margin. We next develop a practical algorithm called Dissimilarity based Boosting (DBoost) for learning with dissimilarity functions under the theoretical guidance. Experimental results demonstrate that DBoost compares favorably with several existing approaches on a variety of databases and under different conditions.

#index 983929
#* Winnowing subspaces
#@ Manfred K. Warmuth
#t 2007
#c 19
#% 82156
#% 201259
#% 224113
#% 227736
#% 266789
#% 304824
#% 378462
#% 451055
#% 722762
#% 829027
#% 963257
#% 983862
#% 1396701
#% 1674796
#! We generalize the Winnow algorithm for learning disjunctions to learning subspaces of low rank. Subspaces are represented by symmetric projection matrices. The online algorithm maintains its uncertainty about the hidden low rank projection matrix as a symmetric positive definite matrix. This matrix is updated using a version of the Matrix Exponentiated Gradient algorithm that is based on matrix exponentials and matrix logarithms. As in the case of the Winnow algorithm, the bounds are logarithmic in the dimension n of the problem, but linear in the rank r of the hidden subspace. We show that the algorithm can be adapted to handle arbitrary matrices of any dimension via a reduction.

#index 983930
#* What is decreased by the max-sum arc consistency algorithm?
#@ Tomáš Werner
#t 2007
#c 19
#% 30449
#% 44876
#% 131561
#% 325295
#% 419951
#% 669525
#% 751442
#% 757932
#% 889176
#% 975169
#% 1223200
#% 1275309
#% 1809993
#% 1815753
#% 1845478
#! Inference tasks in Markov random fields (MRFs) are closely related to the constraint satisfaction problem (CSP) and its soft generalizations. In particular, MAP inference in MRF is equivalent to the weighted (maxsum) CSP. A well-known tool to tackle CSPs are arc consistency algorithms, a.k.a. relaxation labeling. A promising approach to MAP inference in MRFs is linear programming relaxation solved by sequential treereweighted message passing (TRW-S). There is a not widely known algorithm equivalent to TRW-S, max-sum diffusion, which is slower but very simple. We give two theoretical results. First, we show that arc consistency algorithms and max-sum diffusion become the same thing if formulated in an abstractalgebraic way. Thus, we argue that max-sum arc consistency algorithm or max-sum relaxation labeling is a more suitable name for max-sum diffusion. Second, we give a criterion that strictly decreases during these algorithms. It turns out that every class of equivalent problems contains a unique problem that is minimal w.r.t. this criterion.

#index 983931
#* Multi-task reinforcement learning: a hierarchical Bayesian approach
#@ Aaron Wilson;Alan Fern;Soumya Ray;Prasad Tadepalli
#t 2007
#c 19
#% 266287
#% 384911
#% 466731
#% 840955
#% 876006
#% 1274860
#% 1650283
#! We consider the problem of multi-task reinforcement learning, where the agent needs to solve a sequence of Markov Decision Processes (MDPs) chosen randomly from a fixed but unknown distribution. We model the distribution over MDPs using a hierarchical Bayesian infinite mixture model. For each novel MDP, we use the previously learned distribution as an informed prior for modelbased Bayesian reinforcement learning. The hierarchical Bayesian framework provides a strong prior that allows us to rapidly infer the characteristics of new environments based on previous environments, while the use of a nonparametric model allows us to quickly adapt to environments we have not encountered before. In addition, the use of infinite mixtures allows for the model to automatically learn the number of underlying MDP components. We evaluate our approach and show that it leads to significant speedups in convergence to an optimal policy after observing only a small number of tasks.

#index 983932
#* Beamforming using the relevance vector machine
#@ David Wipf;Srikantan Nagarajan
#t 2007
#c 19
#% 132676
#% 135331
#% 304879
#% 722760
#% 980659
#! Beamformers are spatial filters that pass source signals in particular focused locations while suppressing interference from elsewhere. The widely-used minimum variance adaptive beamformer (MVAB) creates such filters using a sample covariance estimate; however, the quality of this estimate deteriorates when the sources are correlated or the number of samples n is small. Herein, a modified beamformer is derived that replaces this problematic sample covariance with a robust maximum likelihood estimate obtained using the relevance vector machine (RVM), a Bayesian method for learning sparse models from possibly overcomplete feature sets. We prove that this substitution has the natural ability to remove the undesirable effects of correlations or limited data. When n becomes large and assuming uncorrelated sources, this method reduces to the exact MVAB. Simulations using direction-of-arrival data support these conclusions. Additionally, RVMs can potentially enhance a variety of traditional signal processing methods that rely on robust sample covariance estimates.

#index 983933
#* Learning to combine distances for complex representations
#@ Adam Woznica;Alexandros Kalousis;Melanie Hilario
#t 2007
#c 19
#% 224755
#% 392781
#% 428413
#% 550387
#% 763697
#% 770798
#% 799754
#% 915322
#% 961249
#% 1250593
#% 1705523
#! The k-Nearest Neighbors algorithm can be easily adapted to classify complex objects (e.g. sets, graphs) as long as a proper dissimilarity function is given over an input space. Both the representation of the learning instances and the dissimilarity employed on that representation should be determined on the basis of domain knowledge. However, even in the presence of domain knowledge, it can be far from obvious which complex representation should be used or which dissimilarity should be applied on the chosen representation. In this paper we present a framework that allows to combine different complex representations of a given learning problem and/or different dissimilarities defined on these representations. We build on ideas developed previously on metric learning for vectorial data. We demonstrate the utility of our method in domains in which the learning instances are represented as sets of vectors by learning how to combine different set distance measures.

#index 983934
#* Local learning projections
#@ Mingrui Wu;Kai Yu;Shipeng Yu;Bernhard Schölkopf
#t 2007
#c 19
#% 143194
#% 235342
#% 278040
#% 743284
#% 791402
#% 876058
#! This paper presents a Local Learning Projection (LLP) approach for linear dimensionality reduction. We first point out that the well known Principal Component Analysis (PCA) essentially seeks the projection that has the minimal global estimation error. Then we propose a dimensionality reduction algorithm that leads to the projection with the minimal local estimation error, and elucidate its advantages for classification tasks. We also indicate that LLP keeps the local information in the sense that the projection value of each point can be well estimated based on its neighbors and their projection values. Experimental results are provided to validate the effectiveness of the proposed algorithm.

#index 983935
#* On learning linear ranking functions for beam search
#@ Yuehua Xu;Alan Fern
#t 2007
#c 19
#% 186989
#% 224755
#% 408396
#% 840856
#% 1275082
#% 1705502
#! Beam search is used to maintain tractability in large search spaces at the expense of completeness and optimality. We study supervised learning of linear ranking functions for controlling beam search. The goal is to learn ranking functions that allow for beam search to perform nearly as well as unconstrained search while gaining computational efficiency. We first study the computational complexity of the learning problem, showing that even for exponentially large search spaces the general consistency problem is in NP. We also identify tractable and intractable subclasses of the learning problem. Next, we analyze the convergence of recently proposed and modified online learning algorithms. We first provide a counter-example to an existing convergence result and then introduce alternative notions of "margin" that do imply convergence. Finally, we study convergence properties for ambiguous training data.

#index 983936
#* Modeling changing dependency structure in multivariate time series
#@ Xiang Xuan;Kevin Murphy
#t 2007
#c 19
#% 388024
#% 866470
#% 875956
#% 884071
#% 1757641
#% 1762061
#! We show how to apply the efficient Bayesian changepoint detection techniques of Fearnhead in the multivariate setting. We model the joint density of vector-valued observations using undirected Gaussian graphical models, whose structure we estimate. We show how we can exactly compute the MAP segmentation, as well as how to draw perfect samples from the posterior over segmentations, simultaneously accounting for uncertainty about the number and location of changepoints, as well as uncertainty about the covariance structure. We illustrate the technique by applying it to financial data and to bee tracking data.

#index 983937
#* The matrix stick-breaking process for flexible multi-task learning
#@ Ya Xue;David Dunson;Lawrence Carin
#t 2007
#c 19
#% 236497
#% 267027
#% 766451
#% 961246
#! In multi-task learning our goal is to design regression or classification models for each of the tasks and appropriately share information between tasks. A Dirichlet process (DP) prior can be used to encourage task clustering. However, the DP prior does not allow local clustering of tasks with respect to a subset of the feature vector without making independence assumptions. Motivated by this problem, we develop a new multitask-learning prior, termed the matrix stick-breaking process (MSBP), which encourages cross-task sharing of data. However, the MSBP allows separate clustering and borrowing of information for the different feature components. This is important when tasks are more closely related for certain features than for others. Bayesian inference proceeds by a Gibbs sampling algorithm and the approach is illustrated using a simulated example and a multi-national application.

#index 983938
#* Map building without localization by dimensionality reduction techniques
#@ Takehisa Yairi
#t 2007
#c 19
#% 229084
#% 266426
#% 267315
#% 470942
#% 578744
#% 580300
#% 840843
#% 1275152
#% 1279237
#! This paper proposes a new map building framework for mobile robot named Localization-Free Mapping by Dimensionality Reduction (LFMDR). In this framework, the robot map building is interpreted as a problem of reconstructing the 2-D coordinates of objects so that they maximally preserve the local proximity of the objects in the space of robot's observation history. Not only traditional linear PCA but also recent manifold learning techniques can be used for solving this problem. In contrast to the SLAM framework, LFMDR framework does not require localization procedures nor explicit measurement and motion models. In the latter part of this paper, we will demonstrate "visibility-only" and "bearing-only" localization-free mappings which are derived by applying LFMDR framework to the visibility and bearing measurements respectively.

#index 983939
#* Asymptotic Bayesian generalization error when training and test distributions are different
#@ Keisuke Yamazaki;Motoaki Kawanabe;Sumio Watanabe;Masashi Sugiyama;Klaus-Robert Müller
#t 2007
#c 19
#% 341682
#% 425031
#% 715178
#% 856208
#% 961139
#% 1074038
#! In supervised learning, we commonly assume that training and test data are sampled from the same distribution. However, this assumption can be violated in practice and then standard machine learning techniques perform poorly. This paper focuses on revealing and improving the performance of Bayesian estimation when the training and test distributions are different. We formally analyze the asymptotic Bayesian generalization error and establish its upper bound under a very general setting. Our important finding is that lower order terms---which can be ignored in the absence of the distribution change---play an important role under the distribution change. We also propose a novel variant of stochastic complexity which can be used for choosing an appropriate model and hyper-parameters under a particular distribution change.

#index 983940
#* Least squares linear discriminant analysis
#@ Jieping Ye
#t 2007
#c 19
#% 80995
#% 212689
#% 235342
#% 729437
#% 841687
#% 844346
#% 961176
#% 961218
#! Linear Discriminant Analysis (LDA) is a well-known method for dimensionality reduction and classification. LDA in the binaryclass case has been shown to be equivalent to linear regression with the class label as the output. This implies that LDA for binary-class classifications can be formulated as a least squares problem. Previous studies have shown certain relationship between multivariate linear regression and LDA for the multi-class case. Many of these studies show that multivariate linear regression with a specific class indicator matrix as the output can be applied as a preprocessing step for LDA. However, directly casting LDA as a least squares problem is challenging for the multi-class case. In this paper, a novel formulation for multivariate linear regression is proposed. The equivalence relationship between the proposed least squares formulation and LDA for multi-class classifications is rigorously established under a mild condition, which is shown empirically to hold in many applications involving high-dimensional data. Several LDA extensions based on the equivalence relationship are discussed.

#index 983941
#* Discriminant kernel and regularization parameter learning via semidefinite programming
#@ Jieping Ye;Jianhui Chen;Shuiwang Ji
#t 2007
#c 19
#% 209961
#% 269218
#% 309208
#% 443790
#% 578414
#% 757953
#% 763697
#% 770831
#% 770846
#% 770848
#% 829029
#% 829031
#% 832903
#% 857439
#% 875950
#% 876003
#% 876014
#% 961190
#% 1861629
#! Regularized Kernel Discriminant Analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. The performance of RKDA depends on the selection of kernels. In this paper, we consider the problem of learning an optimal kernel over a convex set of kernels. We show that the kernel learning problem can be formulated as a semidefinite program (SDP) in the binary-class case. We further extend the SDP formulation to the multi-class case. It is based on a key result established in this paper, that is, the multi-class kernel learning problem can be decomposed into a set of binary-class kernel learning problems. In addition, we propose an approximation scheme to reduce the computational complexity of the multi-class SDP formulation. The performance of RKDA also depends on the value of the regularization parameter. We show that this value can be learned automatically in the framework. Experimental results on benchmark data sets demonstrate the efficacy of the proposed SDP formulations.

#index 983942
#* Robust multi-task learning with t-processes
#@ Shipeng Yu;Volker Tresp;Kai Yu
#t 2007
#c 19
#% 236497
#% 303620
#% 723239
#% 769886
#% 840962
#% 876081
#% 891549
#% 916788
#! Most current multi-task learning frameworks ignore the robustness issue, which means that the presence of "outlier" tasks may greatly reduce overall system performance. We introduce a robust framework for Bayesian multitask learning, t-processes (TP), which are a generalization of Gaussian processes (GP) for multi-task learning. TP allows the system to effectively distinguish good tasks from noisy or outlier tasks. Experiments show that TP not only improves overall system performance, but can also serve as an indicator for the "informativeness" of different tasks.

#index 983943
#* On the value of pairwise constraints in classification and consistency
#@ Jian Zhang;Rong Yan
#t 2007
#c 19
#% 237663
#% 464291
#% 724344
#% 769881
#% 836707
#% 852097
#! In this paper we consider the problem of classification in the presence of pairwise constraints, which consist of pairs of examples as well as a binary variable indicating whether they belong to the same class or not. We propose a method which can effectively utilize pairwise constraints to construct an estimator of the decision boundary, and we show that the resulting estimator is sign-insensitive consistent with respect to the optimal linear decision boundary. We also study the asymptotic variance of the estimator and extend the method to handle both labeled and pairwise examples in a natural way. Several experiments on simulated datasets and real world classification datasets are conducted. The results not only verify the theoretical properties of the proposed method but also demonstrate its practical value in applications.

#index 983944
#* Maximum margin clustering made practical
#@ Kai Zhang;Ivor W. Tsang;James T. Kwok
#t 2007
#c 19
#% 36672
#% 269218
#% 292664
#% 313959
#% 345829
#% 735256
#% 757953
#% 763697
#% 876077
#% 1269502
#! Maximum margin clustering (MMC) is a recent large margin unsupervised learning approach that has often outperformed conventional clustering methods. Computationally, it involves non-convex optimization and has to be relaxed to different semidefinite programs (SDP). However, SDP solvers are computationally very expensive and only small data sets can be handled by MMC so far. To make MMC more practical, we avoid SDP relaxations and propose in this paper an efficient approach that performs alternating optimization directly on the original non-convex problem. A key step to avoid premature convergence is on the use of SVR with the Laplacian loss, instead of SVM with the hinge loss, in the inner optimization subproblem. Experiments on a number of synthetic and real-world data sets demonstrate that the proposed approach is often more accurate, much faster and can handle much larger data sets.

#index 983945
#* Nonlinear independent component analysis with minimal nonlinear distortion
#@ Kun Zhang;Laiwan Chan
#t 2007
#c 19
#% 2219
#% 190861
#% 281707
#% 593039
#% 792899
#% 829034
#% 961205
#% 1757198
#% 1860500
#% 1860752
#! Nonlinear ICA may not result in nonlinear blind source separation, since solutions to nonlinear ICA are highly non-unique. In practice, the nonlinearity in the data generation procedure is usually not strong. Thus it is reasonable to select the solution with the mixing procedure close to linear. In this paper we propose to solve nonlinear ICA with the "minimal nonlinear distortion" principle. This is achieved by incorporating a regularization term to minimize the mean square error between the mixing mapping and the best-fitting linear one. As an application, the proposed method helps to identify linear, non-Gaussian, and acyclic causal models when mild nonlinearity exists in the data generation procedure. Using this method to separate daily returns of a set of stocks, we successfully identify their linear causal relations. The resulting causal relations give some interesting insights into the stock market.

#index 983946
#* Optimal dimensionality of metric space for classification
#@ Wei Zhang;Xiangyang Xue;Zichen Sun;Yue-Fei Guo;Hong Lu
#t 2007
#c 19
#% 80995
#% 209623
#% 235342
#% 791402
#% 812578
#% 812580
#% 876058
#% 1289483
#! In many real-world applications, Euclidean distance in the original space is not good due to the curse of dimensionality. In this paper, we propose a new method, called Discriminant Neighborhood Embedding (DNE), to learn an appropriate metric space for classification given finite training samples. We define a discriminant adjacent matrix in favor of classification task, i.e., neighboring samples in the same class are squeezed but those in different classes are separated as far as possible. The optimal dimensionality of the metric space can be estimated by spectral analysis in the proposed method, which is of great significance for high-dimensional patterns. Experiments with various datasets demonstrate the effectiveness of our method.

#index 983947
#* Conditional random fields for multi-agent reinforcement learning
#@ Xinhua Zhang;Douglas Aberdeen;S. V. N. Vishwanathan
#t 2007
#c 19
#% 124687
#% 258937
#% 464434
#% 466262
#% 496272
#% 527987
#% 528006
#% 565550
#% 739899
#% 788036
#% 788065
#% 1272385
#% 1699598
#! Conditional random fields (CRFs) are graphical models for modeling the probability of labels given the observations. They have traditionally been trained with using a set of observation and label pairs. Underlying all CRFs is the assumption that, conditioned on the training data, the labels are independent and identically distributed (iid). In this paper we explore the use of CRFs in a class of temporal learning algorithms, namely policy-gradient reinforcement learning (RL). Now the labels are no longer iid. They are actions that update the environment and affect the next observation. From an RL point of view, CRFs provide a natural way to model joint actions in a decentralized Markov decision process. They define how agents can communicate with each other to choose the optimal joint action. Our experiments include a synthetic network alignment problem, a distributed sensor network, and road traffic control; clearly outperforming RL methods which do not model the proper joint policy.

#index 983948
#* Spectral feature selection for supervised and unsupervised learning
#@ Zheng Zhao;Huan Liu
#t 2007
#c 19
#% 415694
#% 464615
#% 592143
#% 720010
#% 722929
#% 763697
#% 771842
#% 796212
#% 916789
#! Feature selection aims to reduce dimensionality for building comprehensible learning models with good generalization performance. Feature selection algorithms are largely studied separately according to the type of learning: supervised or unsupervised. This work exploits intrinsic properties underlying supervised and unsupervised feature selection algorithms, and proposes a unified framework for feature selection based on spectral graph theory. The proposed framework is able to generate families of algorithms for both supervised and unsupervised feature selection. And we show that existing powerful algorithms such as ReliefF (supervised) and Laplacian Score (unsupervised) are special cases of the proposed framework. To the best of our knowledge, this work is the first attempt to unify supervised and unsupervised feature selection, and enable their joint study under a general framework. Experiments demonstrated the efficacy of the novel algorithms derived from the framework.

#index 983949
#* Spectral clustering and transductive learning with multiple views
#@ Dengyong Zhou;Christopher J. C. Burges
#t 2007
#c 19
#% 252011
#% 290830
#% 313959
#% 464267
#% 723885
#% 840965
#% 881557
#% 905823
#% 912202
#% 1396666
#% 1674764
#! We consider spectral clustering and transductive inference for data with multiple views. A typical example is the web, which can be described by either the hyperlinks between web pages or the words occurring in web pages. When each view is represented as a graph, one may convexly combine the weight matrices or the discrete Laplacians for each graph, and then proceed with existing clustering or classification techniques. Such a solution might sound natural, but its underlying principle is not clear. Unlike this kind of methodology, we develop multiview spectral clustering via generalizing the normalized cut from a single view to multiple views. We further build multiview transductive inference on the basis of multiview spectral clustering. Our framework leads to a mixture of Markov chains defined on every graph. The experimental evaluation on real-world web classification demonstrates promising results that validate our method.

#index 983950
#* On the relation between multi-instance learning and semi-supervised learning
#@ Zhi-Hua Zhou;Jun-Ming Xu
#t 2007
#c 19
#% 224755
#% 252011
#% 272527
#% 304876
#% 311027
#% 464436
#% 465737
#% 465916
#% 466263
#% 466888
#% 466927
#% 565537
#% 771844
#% 799384
#% 840922
#% 875969
#% 875970
#% 876033
#% 902511
#% 1269502
#% 1289496
#! Multi-instance learning and semi-supervised learning are different branches of machine learning. The former attempts to learn from a training set consists of labeled bags each containing many unlabeled instances; the latter tries to exploit abundant unlabeled instances when learning with a small number of labeled examples. In this paper, we establish a bridge between these two branches by showing that multi-instance learning can be viewed as a special case of semi-supervised learning. Based on this recognition, we propose the MissSVM algorithm which addresses multi-instance learning using a special semi-supervised support vector machine. Experiments show that solving multi-instance problems from the view of semi-supervised learning is feasible, and the MissSVM algorithm is competitive with state-of-the-art multi-instance learning algorithms.

#index 983951
#* Dynamic hierarchical Markov random fields and their application to web data extraction
#@ Jun Zhu;Zaiqing Nie;Bo Zhang;Ji-Rong Wen
#t 2007
#c 19
#% 73441
#% 277467
#% 304963
#% 450888
#% 464304
#% 464434
#% 492962
#% 528013
#% 641989
#% 770844
#% 805846
#% 829016
#% 832596
#% 836836
#% 840966
#% 881505
#% 943869
#% 1502489
#% 1854528
#! Hierarchical models have been extensively studied in various domains. However, existing models assume fixed model structures or incorporate structural uncertainty generatively. In this paper, we propose Dynamic Hierarchical Markov Random Fields (DHMRFs) to incorporate structural uncertainty in a discriminative manner. DHMRFs consist of two parts -- structure model and class label model. Both are defined as exponential family distributions. Conditioned on observations, DHMRFs relax the independence assumption as made in directed models. As exact inference is intractable, a variational method is developed to learn parameters and to find the MAP model structure and label assignment. We apply the model to a real-world web data extraction task, which automatically extracts product items for sale on the Web. The results show promise.

#index 983952
#* Transductive support vector machines for structured variables
#@ Alexander Zien;Ulf Brefeld;Tobias Scheffer
#t 2007
#c 19
#% 466263
#% 770759
#% 770763
#% 840938
#% 875963
#% 876077
#% 959454
#! We study the problem of learning kernel machines transductively for structured output variables. Transductive learning can be reduced to combinatorial optimization problems over all possible labelings of the unlabeled data. In order to scale transductive learning to structured variables, we transform the corresponding non-convex, combinatorial, constrained optimization problems into continuous, unconstrained optimization problems. The discrete optimization parameters are eliminated and the resulting differentiable problems can be optimized efficiently. We study the effectiveness of the generalized TSVM on multiclass classification and label-sequence learning problems empirically.

#index 983953
#* Multiclass multiple kernel learning
#@ Alexander Zien;Cheng Soon Ong
#t 2007
#c 19
#% 143238
#% 757953
#% 763697
#% 770763
#% 770831
#% 770846
#% 829029
#% 832903
#% 833065
#% 833540
#% 876003
#% 891549
#% 906370
#% 948114
#! In many applications it is desirable to learn from several kernels. "Multiple kernel learning" (MKL) allows the practitioner to optimize over linear combinations of kernels. By enforcing sparse coefficients, it also generalizes feature selection to kernel selection. We propose MKL for joint feature maps. This provides a convenient and principled way for MKL with multiclass problems. In addition, we can exploit the joint feature map to learn kernels on output spaces. We show the equivalence of several different primal formulations including different regularizers. We present several optimization methods, and compare a convex quadratically constrained quadratic program (QCQP) and two semi-infinite linear programs (SILPs) on toy data, showing that the SILPs are faster than the QCQP. We then demonstrate the utility of our method by applying the SILP to three real world datasets.

#index 984113
#* Hybrid huberized support vector machines for microarray classification
#@ Li Wang;Ji Zhu;Hui Zou
#t 2007
#c 19
#% 190581
#% 197394
#% 269207
#% 420077
#% 425048
#% 466084
#! The large number of genes and the relatively small number of samples are typical characteristics for microarray data. These characteristics pose challenges for both sample classification and relevant gene selection. The support vector machine (SVM) is a widely used classification technique, and previous studies have demonstrated its superior classification performance in microarray analysis. However, a major limitation is that the SVM can not perform automatic gene selection. To overcome this limitation, we propose the hybrid huberized support vector machine (HHSVM). The HHSVM uses the huberized hinge loss function and the elastic-net penalty. It has two major benefits: 1. automatic gene selection; 2. the grouping effect, where highly correlated genes tend to be selected/removed together. We also develop an efficient algorithm that computes the entire regularized solution path for HHSVM. We have applied our method to real microarray data and achieved promising results.

#index 1073871
#* Proceedings of the 25th international conference on Machine learning
#@ William Cohen;Andrew McCallum;Sam Roweis
#t 2008
#c 19
#! This volume contains the papers accepted to the 25th International Conference on Machine Learning (ICML 2008). ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at http://www.machinelearning.org. This year, ICML was held July 5..9 at the University of Helsinki, in Helsinki, Finland, and was co-located with COLT-2008, the 21st Annual Conference on Computational Learning Theory, and UAI-2008, the 24th Conference on Uncertainty in Artificial Intelligence. No less than 583 papers were submitted to ICML 2008. There was a very thorough review process, in which each paper was reviewed double-blind by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. There were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Some papers were checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 158 papers were accepted to ICML this year, including a small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 27%. ICML authors presented their papers both orally and in a poster session, allowing time for detailed discussions with any interested attendees of the conference. Each day of the main conference included one or two invited talks by a prominent researcher. We were very fortunate to be able to host Michael Collins, of the Massachusetts Institute of Technology; Andrew Ng, of Stanford University; and Luc De Raedt, of the Katholieke Universiteit Leuven, and John Winn of Microsoft Research Cambridge. In addition to the technical talks, ICML- 2008 also included nine tutorials held before the main conference, presented by Alex Smola, Arthur Gretton, and Kenji Fukumizu; Bert Kappen and Marc Toussaint; Neil Lawrence; MartinWainwright; Ralf Herbrich and Thore Graepel; Andreas Krause and Carlos Guestrin; Shai Shalev-Shwartz and Yoram Singer; Rob Fergus; and Matthias Seeger. This year our workshops were organized jointly with COLT and UAI as part of a special "overlap day," consisting of eleven workshops selected and arranged collaboratively by the respective workshop chairs of the three conferences. This day provided a rich opportunity for interaction among the attendees of the conferences. This year, ICML enlarged its award offerings to match several other well-established conferences. We hope these will help build our community, celebrate our advances, and encourage applications and long-term thinking. In addition to our previously traditional "Best Paper" and "Best Student Paper" awards, we also gave awards for "Best Application Paper" and "10-year Best Paper" (for the best paper of ICML 1998, optionally given in conjunction with a co-located conference). We thank the Machine Learning Journal for sponsoring some of our paper awards.

#index 1073872
#* Gaussian process product models for nonparametric nonstationarity
#@ Ryan Prescott Adams;Oliver Stegle
#t 2008
#c 19
#% 272516
#% 351347
#% 715096
#% 840896
#% 891549
#! Stationarity is often an unrealistic prior assumption for Gaussian process regression. One solution is to predefine an explicit nonstationary covariance function, but such covariance functions can be difficult to specify and require detailed prior knowledge of the nonstationarity. We propose the Gaussian process product model (GPPM) which models data as the pointwise product of two latent Gaussian processes to nonparametrically infer nonstationary variations of amplitude. This approach differs from other nonparametric approaches to covariance function inference in that it operates on the outputs rather than the inputs, resulting in a significant reduction in computational cost and required data for inference. We present an approximate inference scheme using Expectation Propagation. This variational approximation yields convenient GP hyperparameter selection and compact approximate predictive distributions.

#index 1073873
#* Sequence kernels for predicting protein essentiality
#@ Cyril Allauzen;Mehryar Mohri;Ameet Talwalkar
#t 2008
#c 19
#% 197394
#% 408470
#% 722803
#% 771848
#% 793247
#% 823318
#% 833088
#% 833551
#% 1402057
#% 1558464
#! The problem of identifying the minimal gene set required to sustain life is of crucial importance in understanding cellular mechanisms and designing therapeutic drugs. This work describes several kernel-based solutions for predicting essential genes that outperform existing models while using less training data. Our first solution is based on a semi-manually designed kernel derived from the Pfam database, which includes several Pfam domains. We then present novel and general domain-based sequence kernels that capture sequence similarity with respect to several domains made of large sets of protein sequences. We show how to deal with the large size of the problem -- several thousands of domains with individual domains sometimes containing thousands of sequences -- by representing and efficiently computing these kernels using automata. We report results of extensive experiments demonstrating that they compare favorably with the Pfam kernel in predicting protein essentiality, while requiring no manual tuning.

#index 1073874
#* Hierarchical kernel stick-breaking process for multi-task image analysis
#@ Qi An;Chunping Wang;Ivo Shterev;Eric Wang;Lawrence Carin;David B. Dunson
#t 2008
#c 19
#% 770830
#% 770861
#% 856933
#% 961246
#% 1828427
#! The kernel stick-breaking process (KSBP) is employed to segment general imagery, imposing the condition that patches (small blocks of pixels) that are spatially proximate are more likely to be associated with the same cluster (segment). The number of clusters is not set a priori and is inferred from the hierarchical Bayesian model. Further, KSBP is integrated with a shared Dirichlet process prior to simultaneously model multiple images, inferring their inter-relationships. This latter application may be useful for sorting and learning relationships between multiple images. The Bayesian inference algorithm is based on a hybrid of variational Bayesian analysis and local sampling. In addition to providing details on the model and associated inference framework, example results are presented for several image-analysis problems.

#index 1073875
#* Graph kernels between point clouds
#@ Francis R. Bach
#t 2008
#c 19
#% 409857
#% 443975
#% 722803
#% 743284
#% 770846
#% 833065
#% 840863
#% 889167
#% 929717
#% 940272
#% 961270
#% 1021422
#! Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow one to use kernel methods for such objects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on probabilistic graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.

#index 1073876
#* Bolasso: model consistent Lasso estimation through the bootstrap
#@ Francis R. Bach
#t 2008
#c 19
#% 209021
#% 961223
#% 1074378
#! We consider the least-square linear regression problem with regularization by the l1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository.

#index 1073877
#* Learning all optimal policies with multiple criteria
#@ Leon Barrett;Srini Narayanan
#t 2008
#c 19
#% 72237
#% 188153
#% 252183
#% 384911
#% 465915
#% 466418
#% 644560
#% 763707
#% 770852
#% 840910
#! We describe an algorithm for learning in the presence of multiple criteria. Our technique generalizes previous approaches in that it can learn optimal policies for all linear preference assignments over the multiple reward criteria at once. The algorithm can be viewed as an extension to standard reinforcement learning for MDPs where instead of repeatedly backing up maximal expected rewards, we back up the set of expected rewards that are maximal for some set of linear preferences (given by a weight vector, w). We present the algorithm along with a proof of correctness showing that our solution gives the optimal policy for any linear preference function. The solution reduces to the standard value iteration algorithm for a specific weight vector, w.

#index 1073878
#* Multiple instance ranking
#@ Charles Bergeron;Jed Zaretzki;Curt Breneman;Kristin P. Bennett
#t 2008
#c 19
#% 224755
#% 465916
#% 466927
#% 577224
#% 743284
#% 829020
#! This paper introduces a novel machine learning model called multiple instance ranking (MIRank) that enables ranking to be performed in a multiple instance learning setting. The motivation for MIRank stems from the hydrogen abstraction problem in computational chemistry, that of predicting the group of hydrogen atoms from which a hydrogen is abstracted (removed) during metabolism. The model predicts the preferred hydrogen group within a molecule by ranking the groups, with the ambiguity of not knowing which hydrogen atom within the preferred group is actually abstracted. This paper formulates MIRank in its general context and proposes an algorithm for solving MIRank problems using successive linear programming. The method outperforms multiple instance classification models on several real and synthetic datasets.

#index 1073879
#* Multi-task learning for HIV therapy screening
#@ Steffen Bickel;Jasmina Bogojeska;Thomas Lengauer;Tobias Scheffer
#t 2008
#c 19
#% 723239
#% 769886
#% 770858
#% 840962
#% 961246
#% 983814
#% 1072369
#! We address the problem of learning classifiers for a large number of tasks. We derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task. Our work is motivated by the problem of predicting the outcome of a therapy attempt for a patient who carries an HIV virus with a set of observed genetic properties. Such predictions need to be made for hundreds of possible combinations of drugs, some of which use similar biochemical mechanisms. Multi-task learning enables us to make predictions even for drug combinations with few or no training examples and substantially improves the overall prediction accuracy.

#index 1073880
#* Nonnegative matrix factorization via rank-one downdate
#@ Michael Biggs;Ali Ghodsi;Stephen Vavasis
#t 2008
#c 19
#% 149575
#% 224113
#% 321053
#% 731279
#% 793248
#% 1021506
#% 1038899
#% 1073880
#% 1650298
#! Nonnegative matrix factorization (NMF) was popularized as a tool for data mining by Lee and Seung in 1999. NMF attempts to approximate a matrix with nonnegative entries by a product of two low-rank matrices, also with nonnegative entries. We propose an algorithm called rank-one downdate (R1D) for computing an NMF that is partly motivated by the singular value decomposition. This algorithm computes the dominant singular values and vectors of adaptively determined sub-matrices of a matrix. On each iteration, R1D extracts a rank-one submatrix from the original matrix according to an objective function. We establish a theoretical result that maximizing this objective function corresponds to correctly classifying articles in a nearly separable corpus. We also provide computational experiments showing the success of this method in identifying features in realistic datasets. The method is also much faster than other NMF routines.

#index 1073881
#* Strategy evaluation in extensive games with importance sampling
#@ Michael Bowling;Michael Johanson;Neil Burch;Duane Szafron
#t 2008
#c 19
#% 1250598
#% 1269795
#% 1279308
#! Typically agent evaluation is done through Monte Carlo estimation. However, stochastic agent decisions and stochastic outcomes can make this approach inefficient, requiring many samples for an accurate estimate. We present a new technique that can be used to simultaneously evaluate many strategies while playing a single strategy in the context of an extensive game. This technique is based on importance sampling, but utilizes two new mechanisms for significantly reducing variance in the estimates. We demonstrate its effectiveness in the domain of poker, where stochasticity makes traditional evaluation problematic.

#index 1073882
#* Actively learning level-sets of composite functions
#@ Brent Bryan;Jeff Schneider
#t 2008
#c 19
#% 132697
#% 450951
#% 451056
#% 840868
#% 891549
#% 983853
#% 1093931
#! Scientists frequently have multiple types of experiments and data sets on which they can test the validity of their parameterized models and locate plausible regions for the model parameters. By examining multiple data sets, scientists can obtain inferences which typically are much more informative than the deductions derived from each of the data sources independently. Several standard data combination techniques result in target functions which are a weighted sum of the observed data sources. Thus, computing constraints on the plausible regions of the model parameter space can be formulated as finding a level set of a target function which is the sum of observable functions. We propose an active learning algorithm for this problem which selects both a sample (from the parameter space) and an observable function upon which to compute the next sample. Empirical tests on synthetic functions and on real data for an eight parameter cosmological model show that our algorithm significantly reduces the number of samples required to identify the desired level-set.

#index 1073883
#* Sparse Bayesian nonparametric regression
#@ François Caron;Arnaud Doucet
#t 2008
#c 19
#% 416518
#% 721164
#% 722760
#% 856731
#% 857387
#% 1762839
#! One of the most common problems in machine learning and statistics consists of estimating the mean response Xβ from a vector of observations y assuming y = Xβ + ε where X is known, β is a vector of parameters of interest and ε a vector of stochastic errors. We are particularly interested here in the case where the dimension K of β is much higher than the dimension of y. We propose some flexible Bayesian models which can yield sparse estimates of β. We show that as K → ∞ these models are closely related to a class of Lévy processes. Simulations demonstrate that our models outperform significantly a range of popular alternatives.

#index 1073884
#* An empirical evaluation of supervised learning in high dimensions
#@ Rich Caruana;Nikos Karampatziakis;Ainur Yessenalina
#t 2008
#c 19
#% 302390
#% 400847
#% 424997
#% 577298
#% 840913
#% 875965
#% 881477
#% 916781
#% 983905
#! In this paper we perform an empirical evaluation of supervised learning on high-dimensional data. We evaluate performance on three metrics: accuracy, AUC, and squared loss and study the effect of increasing dimensionality on the performance of the learning algorithms. Our findings are consistent with previous studies for problems of relatively low dimension, but suggest that as dimensionality increases the relative performance of the learning algorithms changes. To our surprise, the method that performs consistently well across all dimensions is random forests, followed by neural nets, boosted trees, and SVMs.

#index 1073885
#* Fast support vector machine training and classification on graphics processors
#@ Bryan Catanzaro;Narayanan Sundaram;Kurt Keutzer
#t 2008
#c 19
#% 197394
#% 247889
#% 269217
#% 269218
#% 443790
#% 450263
#% 856251
#% 881547
#% 916790
#% 961187
#% 963669
#% 1000504
#% 1861727
#! Recent developments in programmable, highly parallel Graphics Processing Units (GPUs) have enabled high performance implementations of machine learning algorithms. We describe a solver for Support Vector Machine training running on a GPU, using the Sequential Minimal Optimization algorithm and an adaptive first and second order working set selection heuristic, which achieves speedups of 9-35x over LIBSVM running on a traditional processor. We also present a GPU-based system for SVM classification which achieves speedups of 81-138x over LIBSVM (5-24x over our own CPU based SVM classifier).

#index 1073886
#* Fast nearest neighbor retrieval for bregman divergences
#@ Lawrence Cayton
#t 2008
#c 19
#% 281750
#% 317313
#% 528164
#% 635694
#% 722904
#% 748465
#% 762054
#% 763708
#% 875957
#% 916785
#% 991199
#% 1396684
#% 1667698
#% 1677690
#% 1775636
#! We present a data structure enabling efficient nearest neighbor (NN) retrieval for bregman divergences. The family of bregman divergences includes many popular dissimilarity measures including KL-divergence (relative entropy), Mahalanobis distance, and Itakura-Saito divergence. These divergences present a challenge for efficient NN retrieval because they are not, in general, metrics, for which most NN data structures are designed. The data structure introduced in this work shares the same basic structure as the popular metric ball tree, but employs convexity properties of bregman divergences in place of the triangle inequality. Experiments demonstrate speedups over brute-force search of up to several orders of magnitude.

#index 1073887
#* Nearest hyperdisk methods for high-dimensional classification
#@ Hakan Cevikalp;Bill Triggs;Robi Polikar
#t 2008
#c 19
#% 266426
#% 466589
#% 732387
#% 743284
#% 760805
#% 784525
#% 836776
#% 885516
#% 1718531
#% 1860128
#! In high-dimensional classification problems it is infeasible to include enough training samples to cover the class regions densely. Irregularities in the resulting sparse sample distributions cause local classifiers such as Nearest Neighbors (NN) and kernel methods to have irregular decision boundaries. One solution is to "fill in the holes" by building a convex model of the region spanned by the training samples of each class and classifying examples based on their distances to these approximate models. Methods of this kind based on affine and convex hulls and bounding hyperspheres have already been studied. Here we propose a method based on the bounding hyperdisk of each class - the intersection of the affine hull and the smallest bounding hypersphere of its training samples. We argue that in many cases hyperdisks are preferable to affine and convex hulls and hyperspheres: they bound the classes more tightly than affine hulls or hyperspheres while avoiding much of the sample overfitting and computational complexity that is inherent in high-dimensional convex hulls. We show that the hyperdisk method can be kernelized to provide nonlinear classifiers based on non-Euclidean distance metrics. Experiments on several classification problems show promising results.

#index 1073888
#* Learning to sportscast: a test of grounded language acquisition
#@ David L. Chen;Raymond J. Mooney
#t 2008
#c 19
#% 85153
#% 722803
#% 722927
#% 741071
#% 815902
#% 939364
#% 939615
#% 939890
#% 995522
#% 1195343
#% 1250196
#% 1265076
#% 1269812
#! We present a novel commentator system that learns language from sportscasts of simulated soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. We also present a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on. Human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries.

#index 1073889
#* Training SVM with indefinite kernels
#@ Jianhui Chen;Jieping Ye
#t 2008
#c 19
#% 143238
#% 190581
#% 304899
#% 309208
#% 341269
#% 393059
#% 443790
#% 464615
#% 722813
#% 732531
#% 757953
#% 763697
#% 770865
#% 791406
#% 796230
#% 961190
#% 983901
#% 989644
#! Similarity matrices generated from many applications may not be positive semidefinite, and hence can't fit into the kernel machine framework. In this paper, we study the problem of training support vector machines with an indefinite kernel. We consider a regularized SVM formulation, in which the indefinite kernel matrix is treated as a noisy observation of some unknown positive semidefinite one (proxy kernel) and the support vectors and the proxy kernel can be computed simultaneously. We propose a semi-infinite quadratically constrained linear program formulation for the optimization, which can be solved iteratively to find a global optimum solution. We further propose to employ an additional pruning strategy, which significantly improves the efficiency of the algorithm, while retaining the convergence property of the algorithm. In addition, we show the close relationship between the proposed formulation and multiple kernel learning. Experiments on a collection of benchmark data sets demonstrate the efficiency and effectiveness of the proposed algorithm.

#index 1073890
#* Learning for control from multiple demonstrations
#@ Adam Coates;Pieter Abbeel;Andrew Y. Ng
#t 2008
#c 19
#% 40640
#% 229931
#% 465902
#% 466418
#% 770852
#% 875945
#% 876036
#% 1025201
#% 1073890
#% 1275169
#% 1650767
#% 1781811
#! We consider the problem of learning to follow a desired trajectory when given a small number of demonstrations from a sub-optimal expert. We present an algorithm that (i) extracts the---initially unknown---desired trajectory from the sub-optimal expert's demonstrations and (ii) learns a local model suitable for control along the learned trajectory. We apply our algorithm to the problem of autonomous helicopter flight. In all cases, the autonomous helicopter's performance exceeds that of our expert helicopter pilot's demonstrations. Even stronger, our results significantly extend the state-of-the-art in autonomous helicopter aerobatics. In particular, our results include the first autonomous tic-tocs, loops and hurricane, vastly superior performance on previously performed aerobatic maneuvers (such as in-place flips and rolls), and a complete airshow, which requires autonomous transitions between these and various other maneuvers.

#index 1073891
#* Spectral clustering with inconsistent advice
#@ Tom Coleman;James Saunderson;Anthony Wirth
#t 2008
#c 19
#% 236829
#% 313959
#% 466890
#% 765548
#% 805786
#% 961185
#% 1279294
#! Clustering with advice (often known as constrained clustering) has been a recent focus of the data mining community. Success has been achieved incorporating advice into the k-means and spectral clustering frameworks. Although the theory community has explored inconsistent advice, it has not yet been incorporated into spectral clustering. Extending work of De Bie and Cristianini, we set out a framework for finding minimum normalised cuts, subject to inconsistent advice.

#index 1073892
#* A unified architecture for natural language processing: deep neural networks with multitask learning
#@ Ronan Collobert;Jason Weston
#t 2008
#c 19
#% 236497
#% 466263
#% 742230
#% 815893
#% 858036
#% 916788
#% 929717
#% 939947
#% 940010
#% 961269
#% 1344885
#! We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.

#index 1073893
#* Autonomous geometric precision error estimation in low-level computer vision tasks
#@ Andrés Corrada-Emmanuel;Howard Schultz
#t 2008
#c 19
#% 296530
#% 332007
#% 391572
#% 457690
#% 719273
#% 891559
#% 1815965
#! Errors in map-making tasks using computer vision are sparse. We demonstrate this by considering the construction of digital elevation models that employ stereo matching algorithms to triangulate real-world points. This sparsity, coupled with a geometric theory of errors recently developed by the authors, allows for autonomous agents to calculate their own precision independently of ground truth. We connect these developments with recent advances in the mathematics of sparse signal reconstruction or compressed sensing. The theory presented here extends the autonomy of 3-D model reconstructions discovered in the 1990s to their errors.

#index 1073894
#* Stability of transductive regression algorithms
#@ Corinna Cortes;Mehryar Mohri;Dmitry Pechyony;Ashish Rastogi
#t 2008
#c 19
#% 425058
#% 722805
#% 837668
#% 1674764
#! This paper uses the notion of algorithmic stability to derive novel generalization bounds for several families of transductive regression algorithms, both by using convexity and closed-form solutions. Our analysis helps compare the stability of these algorithms. It suggests that several existing algorithms might not be stable but prescribes a technique to make them stable. It also reports the results of experiments with local transductive regression demonstrating the benefit of our stability bounds for model selection, in particular for determining the radius of the local neighborhood used by the algorithm.

#index 1073895
#* A rate-distortion one-class model and its applications to clustering
#@ Koby Crammer;Partha Pratim Talukdar;Fernando Pereira
#t 2008
#c 19
#% 115608
#% 382854
#% 397139
#% 722810
#% 770821
#% 805885
#% 855602
#% 915269
#! In one-class classification we seek a rule to find a coherent subset of instances similar to a few positive examples in a large pool of instances. The problem can be formulated and analyzed naturally in a rate-distortion framework, leading to an efficient algorithm that compares well with two previous one-class methods. The model can be also be extended to remove background clutter in clustering to improve cluster purity.

#index 1073896
#* Fast Gaussian process methods for point process intensity estimation
#@ John P. Cunningham;Krishna V. Shenoy;Maneesh Sahani
#t 2008
#c 19
#% 757953
#% 823146
#% 891549
#% 916784
#% 916792
#! Point processes are difficult to analyze because they provide only a sparse and noisy observation of the intensity function driving the process. Gaussian Processes offer an attractive framework within which to infer underlying intensity functions. The result of this inference is a continuous function defined across time that is typically more amenable to analytical efforts. However, a naive implementation will become computationally infeasible in any problem of reasonable size, both in memory and run time requirements. We demonstrate problem specific methods for a class of renewal processes that eliminate the memory burden and reduce the solve time by orders of magnitude.

#index 1073897
#* Self-taught clustering
#@ Wenyuan Dai;Qiang Yang;Gui-Rong Xue;Yong Yu
#t 2008
#c 19
#% 36672
#% 115608
#% 236497
#% 413610
#% 464291
#% 464631
#% 729918
#% 760805
#% 769881
#% 770846
#% 770858
#% 812535
#% 840862
#% 876034
#% 916780
#% 983829
#% 983888
#% 983899
#! This paper focuses on a new clustering task, called self-taught clustering. Self-taught clustering is an instance of unsupervised transfer learning, which aims at clustering a small collection of target unlabeled data with the help of a large amount of auxiliary unlabeled data. The target and auxiliary data can be different in topic distribution. We show that even when the target data are not sufficient to allow effective learning of a high quality feature representation, it is possible to learn the useful features with the help of the auxiliary data on which the target data can be clustered effectively. We propose a co-clustering based self-taught clustering algorithm to tackle this problem, by clustering the target and auxiliary data simultaneously to allow the feature representation from the auxiliary data to influence the target data through a common set of features. Under the new data representation, clustering on the target data can be improved. Our experiments on image clustering show that our algorithm can greatly outperform several state-of-the-art clustering methods when utilizing irrelevant unlabeled auxiliary data.

#index 1073898
#* Hierarchical sampling for active learning
#@ Sanjoy Dasgupta;Daniel Hsu
#t 2008
#c 19
#% 170649
#% 236729
#% 466419
#% 722904
#% 875953
#% 907554
#% 983848
#% 1396656
#% 1396658
#! We present an active learning scheme that exploits cluster structure in data.

#index 1073899
#* Learning to classify with missing and corrupted features
#@ Ofer Dekel;Ohad Shamir
#t 2008
#c 19
#% 101898
#% 269217
#% 757953
#% 875989
#% 1272365
#% 1815223
#! After a classifier is trained using a machine learning algorithm and put to use in a real world system, it often faces noise which did not appear in the training data. Particularly, some subset of features may be missing or may become corrupted. We present two novel machine learning techniques that are robust to this type of classification-time noise. First, we solve an approximation to the learning problem using linear programming. We analyze the tightness of our approximation and prove statistical risk bounds for this approach. Second, we define the online-learning variant of our problem, address this variant using a modified Perceptron, and obtain a statistical learning algorithm using an online-to-batch technique. We conclude with a set of experiments that demonstrate the effectiveness of our algorithms.

#index 1073900
#* Maximum likelihood rule ensembles
#@ Krzysztof Dembczyński;Wojciech Kotłowski;Roman Słowiński
#t 2008
#c 19
#% 235377
#% 277919
#% 283138
#% 312727
#% 466744
#% 961134
#! We propose a new rule induction algorithm for solving classification problems via probability estimation. The main advantage of decision rules is their simplicity and good interpretability. While the early approaches to rule induction were based on sequential covering, we follow an approach in which a single decision rule is treated as a base classifier in an ensemble. The ensemble is built by greedily minimizing the negative loglikelihood which results in estimating the class conditional probability distribution. The introduced approach is compared with other decision rule induction algorithms such as SLIPPER, LRI and RuleFit.

#index 1073901
#* Learning from incomplete data with infinite imputations
#@ Uwe Dick;Peter Haider;Tobias Scheffer
#t 2008
#c 19
#% 840957
#% 948114
#% 961180
#% 983873
#% 1705523
#! We address the problem of learning decision functions from training data in which some attribute values are unobserved. This problem can arise, for instance, when training data is aggregated from multiple sources, and some sources record only a subset of attributes. We derive a generic joint optimization problem in which the distribution governing the missing values is a free parameter. We show that the optimal solution concentrates the density mass on finitely many imputations, and provide a corresponding algorithm for learning from incomplete data. We report on empirical results on benchmark data, and on the email spam application that motivates our work.

#index 1073902
#* An object-oriented representation for efficient reinforcement learning
#@ Carlos Diuk;Andre Cohen;Michael L. Littman
#t 2008
#c 19
#% 363744
#% 384911
#% 1073943
#% 1269772
#% 1271827
#% 1279355
#! Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the well-known Taxi domain, plus a real-life videogame.

#index 1073903
#* Optimizing estimated loss reduction for active sampling in rank learning
#@ Pinar Donmez;Jaime G. Carbonell
#t 2008
#c 19
#% 262112
#% 411762
#% 464268
#% 466887
#% 577224
#% 734915
#% 770753
#% 770771
#% 823360
#% 879588
#% 1387560
#% 1665126
#! Learning to rank is becoming an increasingly popular research area in machine learning. The ranking problem aims to induce an ordering or preference relations among a set of instances in the input space. However, collecting labeled data is growing into a burden in many rank applications since labeling requires eliciting the relative ordering over the set of alternatives. In this paper, we propose a novel active learning framework for SVM-based and boosting-based rank learning. Our approach suggests sampling based on maximizing the estimated loss differential over unlabeled data. Experimental results on two benchmark corpora show that the proposed model substantially reduces the labeling effort, and achieves superior performance rapidly with as much as 30% relative improvement over the margin-based sampling baseline.

#index 1073904
#* Reinforcement learning with limited reinforcement: using Bayes risk for active learning in POMDPs
#@ Finale Doshi;Joelle Pineau;Nicholas Roy
#t 2008
#c 19
#% 466418
#% 466731
#% 817553
#% 876032
#% 1275174
#% 1279358
#% 1289461
#! Partially Observable Markov Decision Processes (POMDPs) have succeeded in planning domains that require balancing actions that increase an agent's knowledge and actions that increase an agent's reward. Unfortunately, most POMDPs are defined with a large number of parameters which are difficult to specify only from domain knowledge. In this paper, we present an approximation approach that allows us to treat the POMDP model parameters as additional hidden state in a "model-uncertainty" POMDP. Coupled with model-directed queries, our planner actively learns good policies. We demonstrate our approach on several POMDP problems.

#index 1073905
#* Confidence-weighted linear classification
#@ Mark Dredze;Koby Crammer;Fernando Pereira
#t 2008
#c 19
#% 722761
#% 757953
#% 763708
#% 801566
#% 881513
#% 891549
#% 961152
#% 1390165
#% 1558464
#% 1699617
#! We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and covariance of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training.

#index 1073906
#* Efficient projections onto the l1-ball for learning in high dimensions
#@ John Duchi;Shai Shalev-Shwartz;Yoram Singer;Tushar Chandra
#t 2008
#c 19
#% 1451
#% 227736
#% 410276
#% 425052
#% 763708
#% 770857
#% 961191
#% 983905
#% 1014657
#% 1845612
#! We describe efficient algorithms for projecting a vector onto the l1-ball. We present two methods for projection. The first performs exact projection in O(n) expected time, where n is the dimension of the space. The second works on vectors k of whose elements are perturbed outside the l1-ball, projecting in O(k log(n)) time. This setting is especially useful for online learning in sparse feature spaces such as text categorization applications. We demonstrate the merits and effectiveness of our algorithms in numerous batch and online learning tasks. We show that variants of stochastic gradient projection methods augmented with our efficient projection procedures outperform interior point methods, which are considered state-of-the-art optimization techniques. We also show that in online settings gradient updates with l1 projections outperform the exponentiated gradient algorithm while obtaining models with high degrees of sparsity.

#index 1073907
#* Pointwise exact bootstrap distributions of cost curves
#@ Charles Dugas;David Gadoury
#t 2008
#c 19
#% 310519
#% 840902
#% 893461
#! Cost curves have recently been introduced as an alternative or complement to ROC curves in order to visualize binary classifiers performance. Of importance to both cost and ROC curves is the computation of confidence intervals along with the curves themselves so that the reliability of a classifier's performance can be assessed. Computing confidence intervals for the difference in performance between two classifiers allows the determination of whether one classifier performs significantly better than another. A simple procedure to obtain confidence intervals for costs or the difference between two costs, under various operating conditions, is to perform bootstrap resampling of the test set. In this paper, we derive exact bootstrap distributions for these values and use these dstributions to obtain confidence intervals, under various operating conditions. Performances of these confidence intervals are measured in terms of coverage accuracies. Simulations show excellent results.

#index 1073908
#* Polyhedral classifier for target detection: a case study: colorectal cancer
#@ M. Murat Dundar;Matthias Wolf;Sarang Lakare;Marcos Salganicoff;Vikas C. Raykar
#t 2008
#c 19
#% 190581
#% 302406
#% 722811
#% 735256
#% 736300
#% 881563
#! In this study we introduce a novel algorithm for learning a polyhedron to describe the target class. The proposed approach takes advantage of the limited subclass information made available for the negative samples and jointly optimizes multiple hyperplane classifiers each of which is designed to classify positive samples from a subclass of the negative samples. The flat faces of the polyhedron provides robustness whereas multiple faces contributes to the flexibility required to deal with complex datasets. Apart from improving the prediction accuracy of the system, the proposed polyhedral classifier also provides run-time speedups as a by-product when executed in a cascaded framework in real-time. We evaluate the performance of the proposed technique on a real-world Colon dataset both in terms of prediction accuracy and online execution speed.

#index 1073909
#* Active reinforcement learning
#@ Arkady Epshteyn;Adam Vogel;Gerald DeJong
#t 2008
#c 19
#% 318485
#% 363744
#% 384911
#% 425075
#% 431470
#% 711934
#% 722895
#% 840835
#% 840942
#% 875945
#% 1650283
#! When the transition probabilities and rewards of a Markov Decision Process (MDP) are known, an agent can obtain the optimal policy without any interaction with the environment. However, exact transition probabilities are difficult for experts to specify. One option left to an agent is a long and potentially costly exploration of the environment. In this paper, we propose another alternative: given initial (possibly inaccurate) specification of the MDP, the agent determines the sensitivity of the optimal policy to changes in transitions and rewards. It then focuses its exploration on the regions of space to which the optimal policy is most sensitive. We show that the proposed exploration strategy performs well on several control and planning problems.

#index 1073910
#* Training structural SVMs when exact inference is intractable
#@ Thomas Finley;Thorsten Joachims
#t 2008
#c 19
#% 44876
#% 464434
#% 575676
#% 763708
#% 772862
#% 812487
#% 829043
#% 840862
#% 840882
#% 840927
#% 854636
#% 875967
#% 876066
#% 905168
#% 975178
#% 1387907
#% 1502489
#% 1558464
#! While discriminative training (e.g., CRF, structural SVM) holds much promise for machine translation, image segmentation, and clustering, the complex inference these applications require make exact training intractable. This leads to a need for approximate training methods. Unfortunately, knowledge about how to perform efficient and effective approximate training is limited. Focusing on structural SVMs, we provide and explore algorithms for two different classes of approximate training algorithms, which we call undergenerating (e.g., greedy) and overgenerating (e.g., relaxations) algorithms. We provide a theoretical and empirical analysis of both types of approximate trained structural SVMs, focusing on fully connected pairwise Markov random fields. We find that models trained with overgenerating methods have theoretic advantages over undergenerating methods, are empirically robust relative to their undergenerating brethren, and relaxed trained models favor non-fractional predictions from relaxed predictors.

#index 1073911
#* An HDP-HMM for systems with state persistence
#@ Emily B. Fox;Erik B. Sudderth;Michael I. Jordan;Alan S. Willsky
#t 2008
#c 19
#% 1073911
#! The hierarchical Dirichlet process hidden Markov model (HDP-HMM) is a flexible, nonparametric model which allows state spaces of unknown size to be learned from data. We demonstrate some limitations of the original HDP-HMM formulation (Teh et al., 2006), and propose a sticky extension which allows more robust learning of smoothly varying dynamics. Using DP mixtures, this formulation also allows learning of more complex, multimodal emission distributions. We further develop a sampling algorithm that employs a truncated approximation of the DP to jointly resample the full state sequence, greatly improving mixing rates. Via extensive experiments with synthetic data and the NIST speaker diarization database, we demonstrate the advantages of our sticky extension, and the utility of the HDP-HMM in real-world applications.

#index 1073912
#* Optimized cutting plane algorithm for support vector machines
#@ Vojtěch Franc;Soeren Sonnenburg
#t 2008
#c 19
#% 197394
#% 269217
#% 309208
#% 881477
#% 916790
#% 959454
#% 961187
#% 983874
#% 983905
#% 989644
#! We have developed a new Linear Support Vector Machine (SVM) training algorithm called OCAS. Its computational effort scales linearly with the sample size. In an extensive empirical evaluation OCAS significantly outperforms current state of the art SVM solvers, like SVMlight, SVMperf and BMRM, achieving speedups of over 1,000 on some datasets over SVMlight and 20 over SVMperf, while obtaining the same precise Support Vector solution. OCAS even in the early optimization steps shows often faster convergence than the so far in this domain prevailing approximative methods SGD and Pegasos. Effectively parallelizing OCAS we were able to train on a dataset of size 15 million examples (itself about 32GB in size) in just 671 seconds --- a competing string kernel SVM required 97,484 seconds to train on 10 million examples sub-sampled from this dataset.

#index 1073913
#* Stopping conditions for exact computation of leave-one-out error in support vector machines
#@ Vojtěch Franc;Pavel Laskov;Klaus-Robert Müller
#t 2008
#c 19
#% 190581
#% 269217
#% 269218
#% 310557
#% 331916
#% 466759
#% 757953
#% 856251
#% 961202
#% 1558464
#% 1861361
#! We propose a new stopping condition for a Support Vector Machine (SVM) solver which precisely reflects the objective of the Leave-One-Out error computation. The stopping condition guarantees that the output on an intermediate SVM solution is identical to the output of the optimal SVM solution with one data point excluded from the training set. A simple augmentation of a general SVM training algorithm allows one to use a stopping criterion equivalent to the proposed sufficient condition. A comprehensive experimental evaluation of our method shows consistent speedup of the exact LOO computation by our method, up to the factor of 13 for the linear kernel. The new algorithm can be seen as an example of constructive guidance of an optimization algorithm towards achieving the best attainable expected risk at optimal computational cost.

#index 1073914
#* Reinforcement learning in the presence of rare events
#@ Jordan Frank;Shie Mannor;Doina Precup
#t 2008
#c 19
#% 366058
#% 393786
#% 449561
#% 464438
#% 466751
#% 770427
#% 951076
#% 959577
#% 961203
#% 1272385
#! We consider the task of reinforcement learning in an environment in which rare significant events occur independently of the actions selected by the controlling agent. If these events are sampled according to their natural probability of occurring, convergence of conventional reinforcement learning algorithms is likely to be slow, and the learning algorithms may exhibit high variance. In this work, we assume that we have access to a simulator, in which the rare event probabilities can be artificially altered. Then, importance sampling can be used to learn with this simulation data. We introduce algorithms for policy evaluation, using both tabular and function approximation representations of the value function. We prove that in both cases, the reinforcement learning algorithms converge. In the tabular case, we also analyze the bias and variance of our approach compared to TD-learning. We evaluate empirically the performance of the algorithm on random Markov Decision Processes, as well as on a large network planning task.

#index 1073915
#* Memory bounded inference in topic models
#@ Ryan Gomes;Max Welling;Pietro Perona
#t 2008
#c 19
#% 722904
#% 760805
#% 784995
#! What type of algorithms and statistical techniques support learning from very large datasets over long stretches of time? We address this question through a memory bounded version of a variational EM algorithm that approximates inference in a topic model. The algorithm alternates two phases: "model building" and "model compression" in order to always satisfy a given memory constraint. The model building phase expands its internal representation (the number of topics) as more data arrives through Bayesian model selection. Compression is achieved by merging data-items in clumps and only caching their sufficient statistics. Empirically, the resulting algorithm is able to handle datasets that are orders of magnitude larger than the standard batch version.

#index 1073916
#* Localized multiple kernel learning
#@ Mehmet Gönen;Ethem Alpaydin
#t 2008
#c 19
#% 294964
#% 304305
#% 328374
#% 450263
#% 565765
#% 763697
#% 770846
#% 832903
#% 876014
#% 961190
#% 983901
#% 1042787
#% 1390365
#! Recently, instead of selecting a single kernel, multiple kernel learning (MKL) has been proposed which uses a convex combination of kernels, where the weight of each kernel is optimized during training. However, MKL assigns the same weight to a kernel over the whole input space. In this paper, we develop a localized multiple kernel learning (LMKL) algorithm using a gating model for selecting the appropriate kernel function locally. The localizing gating model and the kernel-based classifier are coupled and their optimization is done in a joint manner. Empirical results on ten benchmark and two bioinformatics data sets validate the applicability of our approach. LMKL achieves statistically similar accuracy results compared with MKL by storing fewer support vectors. LMKL can also combine multiple copies of the same kernel function localized in different parts. For example, LMKL with multiple linear kernels gives better accuracy results than using a single linear kernel on bioinformatics data sets.

#index 1073917
#* No-regret learning in convex games
#@ Geoffrey J. Gordon;Amy Greenwald;Casey Marks
#t 2008
#c 19
#% 197394
#% 276498
#% 712704
#% 1226698
#% 1705542
#! Quite a bit is known about minimizing different kinds of regret in experts problems, and how these regret types relate to types of equilibria in the multiagent setting of repeated matrix games. Much less is known about the possible kinds of regret in online convex programming problems (OCPs), or about equilibria in the analogous multiagent setting of repeated convex games. This gap is unfortunate, since convex games are much more expressive than matrix games, and since many important machine learning problems can be expressed as OCPs. In this paper, we work to close this gap: we analyze a spectrum of regret types which lie between external and swap regret, along with their corresponding equilibria, which lie between coarse correlated and correlated equilibrium. We also analyze algorithms for minimizing these regret types. As examples of our framework, we derive algorithms for learning correlated equilibria in polyhedral convex games and extensive-form correlated equilibria in extensive-form games. The former is exponentially more efficient than previous algorithms, and the latter is the first of its type.

#index 1073918
#* Boosting with incomplete information
#@ Gholamreza Haffari;Yang Wang;Shaojun Wang;Greg Mori;Feng Jiao
#t 2008
#c 19
#% 235377
#% 299255
#% 302391
#% 350323
#% 425065
#% 760805
#% 836717
#% 836904
#% 855110
#% 883819
#% 939917
#% 961180
#% 1074344
#! In real-world machine learning problems, it is very common that part of the input feature vector is incomplete: either not available, missing, or corrupted. In this paper, we present a boosting approach that integrates features with incomplete information and those with complete information to form a strong classifier. By introducing hidden variables to model missing information, we form loss functions that combine fully labeled data with partially labeled data to effectively learn normalized and unnormalized models. The primal problems of the proposed optimization problems with these loss functions are provided to show their close relationship and the motivations behind them. We use auxiliary functions to bound the change of the loss functions and derive explicit parameter update rules for the learning algorithms. We demonstrate encouraging results on two real-world problems --- visual object recognition in computer vision and named entity recognition in natural language processing --- to show the effectiveness of the proposed boosting approach.

#index 1073919
#* Grassmann discriminant analysis: a unifying view on subspace-based learning
#@ Jihun Hamm;Daniel D. Lee
#t 2008
#c 19
#% 80995
#% 224113
#% 274703
#% 336073
#% 393059
#% 443708
#% 458001
#% 593505
#% 722813
#% 734914
#% 770865
#% 796230
#% 850013
#% 865323
#% 940204
#% 975160
#% 1022958
#! In this paper we propose a discriminant learning framework for problems in which data consist of linear subspaces instead of vectors. By treating subspaces as basic elements, we can make learning algorithms adapt naturally to the problems with linear invariant structures. We propose a unifying view on the subspace-based learning method by formulating the problems on the Grassmann manifold, which is the set of fixed-dimensional linear subspaces of a Euclidean space. Previous methods on the problem typically adopt an inconsistent strategy: feature extraction is performed in the Euclidean space while non-Euclidean distances are used. In our approach, we treat each sub-space as a point in the Grassmann space, and perform feature extraction and classification in the same space. We show feasibility of the approach by using the Grassmann kernel functions such as the Projection kernel and the Binet-Cauchy kernel. Experiments with real image databases show that the proposed method performs well compared with state-of-the-art algorithms.

#index 1073920
#* Modified MMI/MPE: a direct evaluation of the margin in speech recognition
#@ Georg Heigold;Thomas Deselaers;Ralf Schlüter;Hermann Ney
#t 2008
#c 19
#% 190581
#% 715701
#! In this paper we show how common speech recognition training criteria such as the Minimum Phone Error criterion or the Maximum Mutual Information criterion can be extended to incorporate a margin term. Different margin-based training algorithms have been proposed to refine existing training algorithms for general machine learning problems. However, for speech recognition, some special problems have to be addressed and all approaches proposed either lack practical applicability or the inclusion of a margin term enforces significant changes to the underlying model, e.g. the optimization algorithm, the loss function, or the parameterization of the model. In our approach, the conventional training criteria are modified to incorporate a margin term. This allows us to do large-margin training in speech recognition using the same efficient algorithms for accumulation and optimization and to use the same software as for conventional discriminative training. We show that the proposed criteria are equivalent to Support Vector Machines with suitable smooth loss functions, approximating the non-smooth hinge loss function or the hard error (e.g. phone error). Experimental results are given for two different tasks: the rather simple digit string recognition task Sietill which severely suffers from overfitting and the large vocabulary European Parliament Plenary Sessions English task which is supposed to be dominated by the risk and the generalization does not seem to be such an issue.

#index 1073921
#* Statistical models for partial membership
#@ Katherine A. Heller;Sinead Williamson;Zoubin Ghahramani
#t 2008
#c 19
#% 115182
#% 374537
#% 722904
#% 739899
#% 1742154
#! We present a principled Bayesian framework for modeling partial memberships of data points to clusters. Unlike a standard mixture model which assumes that each data point belongs to one and only one mixture component, or cluster, a partial membership model allows data points to have fractional membership in multiple clusters. Algorithms which assign data points partial memberships to clusters can be useful for tasks such as clustering genes based on microarray data (Gasch & Eisen, 2002). Our Bayesian Partial Membership Model (BPM) uses exponential family distributions to model each cluster, and a product of these distibtutions, with weighted parameters, to model each datapoint. Here the weights correspond to the degree to which the datapoint belongs to each cluster. All parameters in the BPM are continuous, so we can use Hybrid Monte Carlo to perform inference and learning. We discuss relationships between the BPM and Latent Dirichlet Allocation, Mixed Membership models, Exponential Family PCA, and fuzzy clustering. Lastly, we show some experimental results and discuss nonparametric extensions to our model.

#index 1073922
#* Active kernel learning
#@ Steven C. H. Hoi;Rong Jin
#t 2008
#c 19
#% 464615
#% 466887
#% 763697
#% 875997
#% 876008
#% 983849
#% 1269754
#! Identifying the appropriate kernel function/matrix for a given dataset is essential to all kernel-based learning techniques. A number of kernel learning algorithms have been proposed to learn kernel functions or matrices from side information (e.g., either labeled examples or pairwise constraints). However, most previous studies are limited to "passive" kernel learning in which side information is provided beforehand. In this paper we present a framework of Active Kernel Learning (AKL) that actively identifies the most informative pairwise constraints for kernel learning. The key challenge of active kernel learning is how to measure the informativeness of an example pair given its class label is unknown. To this end, we propose a min-max approach for active kernel learning that selects the example pair that results in a large classification margin regardless of its assigned class label. We furthermore approximate the related optimization problem into a convex programming problem. We evaluate the effectiveness of the proposed algorithm by comparing it to two other implementations of active kernel learning. Empirical study with nine datasets on semi-supervised data clustering shows that the proposed algorithm is more effective than its competitors.

#index 1073923
#* A dual coordinate descent method for large-scale linear SVM
#@ Cho-Jui Hsieh;Kai-Wei Chang;Chih-Jen Lin;S. Sathiya Keerthi;S. Sundararajan
#t 2008
#c 19
#% 116149
#% 131165
#% 269217
#% 269218
#% 466087
#% 592108
#% 722903
#% 770754
#% 772202
#% 829007
#% 856251
#% 881477
#% 983815
#% 983905
#% 1074360
#% 1117688
#% 1558464
#% 1860545
#! In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines (SVM) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent method for linear SVM with L1-and L2-loss functions. The proposed method is simple and reaches an ε-accurate solution in O(log(1/ε)) iterations. Experiments indicate that our method is much faster than state of the art solvers such as Pegasos, TRON, SVMperf, and a recent primal coordinate descent implementation.

#index 1073924
#* Discriminative structure and parameter learning for Markov logic networks
#@ Tuyen N. Huynh;Raymond J. Mooney
#t 2008
#c 19
#% 73441
#% 568785
#% 770857
#% 840890
#% 850430
#% 875974
#% 961262
#% 983808
#% 983882
#% 1014647
#% 1026555
#% 1250568
#% 1250579
#% 1269496
#% 1699582
#! Markov logic networks (MLNs) are an expressive representation for statistical relational learning that generalizes both first-order logic and graphical models. Existing methods for learning the logical structure of an MLN are not discriminative; however, many relational learning problems involve specific target predicates that must be inferred from given background information. We found that existing MLN methods perform very poorly on several such ILP benchmark problems, and we present improved discriminative methods for learning MLN clauses and weights that outperform existing MLN and traditional ILP methods.

#index 1073925
#* Causal modelling combining instantaneous and lagged effects: an identifiable model based on non-Gaussianity
#@ Aapo Hyvärinen;Shohei Shimizu;Patrik O. Hoyer
#t 2008
#c 19
#% 176172
#% 297171
#% 961205
#! Causal analysis of continuous-valued variables typically uses either autoregressive models or linear Gaussian Bayesian networks with instantaneous effects. Estimation of Gaussian Bayesian networks poses serious identifiability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. We show that such a non-Gaussian model is identifiable without prior knowledge of network structure, and we propose an estimation method shown to be consistent. This approach also points out how neglecting instantaneous effects can lead to completely wrong estimates of the autoregressive coefficients.

#index 1073926
#* Hierarchical model-based reinforcement learning: R-max + MAXQ
#@ Nicholas K. Jong;Peter Stone
#t 2008
#c 19
#% 160859
#% 286423
#% 384911
#% 431471
#% 464628
#% 466075
#% 722895
#% 770775
#% 890241
#% 1084047
#% 1271827
#% 1290041
#! Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their underlying structure. Model-based algorithms, which provided the first finite-time convergence guarantees for reinforcement learning, may also play an important role in coping with the relative scarcity of data in large environments. In this paper, we introduce an algorithm that fully integrates modern hierarchical and model-learning methods in the standard reinforcement learning setting. Our algorithm, R-maxq, inherits the efficient model-based exploration of the R-max algorithm and the opportunities for abstraction provided by the MAXQ framework. We analyze the sample complexity of our algorithm, and our experiments in a standard simulation environment illustrate the advantages of combining hierarchies and models.

#index 1073927
#* Efficient bandit algorithms for online multiclass prediction
#@ Sham M. Kakade;Shai Shalev-Shwartz;Ambuj Tewari
#t 2008
#c 19
#% 92233
#% 227736
#% 302390
#% 451055
#% 593734
#% 722903
#% 813744
#% 866538
#% 875984
#% 961152
#% 1000328
#! This paper introduces the Banditron, a variant of the Perceptron [Rosenblatt, 1958], for the multiclass bandit setting. The multiclass bandit setting models a wide range of practical supervised learning applications where the learner only receives partial feedback (referred to as "bandit" feedback, in the spirit of multi-armed bandit models) with respect to the true label (e.g. in many web applications users often only provide positive "click" feedback which does not necessarily fully disclose a true label). The Banditron has the ability to learn in a multiclass classification setting with the "bandit" feedback which only reveals whether or not the prediction made by the algorithm was correct or not (but does not necessarily reveal the true label). We provide (relative) mistake bounds which show how the Banditron enjoys favorable performance, and our experiments demonstrate the practicality of the algorithm. Furthermore, this paper pays close attention to the important special case when the data is linearly separable --- a problem which has been exhaustively studied in the full information setting yet is novel in the bandit setting.

#index 1073928
#* Large scale manifold transduction
#@ Michael Karlen;Jason Weston;Ayse Erkan;Ronan Collobert
#t 2008
#c 19
#% 466263
#% 840938
#% 879624
#% 929717
#% 961195
#% 961218
#! We show how the regularizer of Transductive Support Vector Machines (TSVM) can be trained by stochastic gradient descent for linear models and multi-layer architectures. The resulting methods can be trained online, have vastly superior training and testing speed to existing TSVM algorithms, can encode prior knowledge in the network architecture, and obtain competitive error rates. We then go on to propose a natural generalization of the TSVM loss function that takes into account neighborhood and manifold information directly, unifying the two-stage Low Density Separation method into a single criterion, and leading to state-of-the-art results.

#index 1073929
#* Non-parametric policy gradients: a unified treatment of propositional and relational domains
#@ Kristian Kersting;Kurt Driessens
#t 2008
#c 19
#% 124687
#% 252221
#% 266288
#% 322913
#% 333786
#% 384911
#% 393786
#% 565550
#% 702594
#% 720778
#% 770823
#% 770850
#% 771946
#% 827868
#% 829011
#% 840859
#% 961160
#% 1000502
#% 1272386
#% 1274930
#% 1665139
#% 1699601
#! Policy gradient approaches are a powerful instrument for learning how to interact with the environment. Existing approaches have focused on propositional and continuous domains only. Without extensive feature engineering, it is difficult - if not impossible - to apply them within structured domains, in which e.g. there is a varying number of objects and relations among them. In this paper, we describe a non-parametric policy gradient approach - called NPPG - that overcomes this limitation. The key idea is to apply Friedmann's gradient boosting: policies are represented as a weighted sum of regression models grown in an stage-wise optimization. Employing off-the-shelf regression learners, NPPG can deal with propositional, continuous, and relational domains in a unified way. Our experimental results show that it can even improve on established results.

#index 1073930
#* ICA and ISA using Schweizer-Wolff measure of dependence
#@ Sergey Kirshner;Barnabás Póczos
#t 2008
#c 19
#% 176172
#% 276877
#% 722887
#% 734930
#% 792898
#% 840919
#% 925382
#% 961280
#% 1860500
#! We propose a new algorithm for independent component and independent subspace analysis problems. This algorithm uses a contrast based on the Schweizer-Wolff measure of pairwise dependence (Schweizer & Wolff, 1981), a non-parametric measure computed on pairwise ranks of the variables. Our algorithm frequently outperforms state of the art ICA methods in the normal setting, is significantly more robust to outliers in the mixed signals, and performs well even in the presence of noise. Our method can also be used to solve independent subspace analysis (ISA) problems by grouping signals recovered by ICA methods. We provide an extensive empirical evaluation using simulated, sound, and image data.

#index 1073931
#* Unsupervised rank aggregation with distance-based models
#@ Alexandre Klementiev;Dan Roth;Kevin Small
#t 2008
#c 19
#% 137599
#% 266477
#% 464451
#% 728195
#% 818080
#% 956542
#% 983818
#! The need to meaningfully combine sets of rankings often comes up when one deals with ranked data. Although a number of heuristic and supervised learning approaches to rank aggregation exist, they require domain knowledge or supervised ranked data, both of which are expensive to acquire. In order to address these limitations, we propose a mathematical and algorithmic framework for learning to aggregate (partial) rankings without supervision. We instantiate the framework for the cases of combining permutations and combining top-k lists, and propose a novel metric for the latter. Experiments in both scenarios demonstrate the effectiveness of the proposed formalism.

#index 1073932
#* On partial optimality in multi-label MRFs
#@ Pushmeet Kohli;Alexander Shekhovtsov;Carsten Rother;Vladimir Kolmogorov;Philip Torr
#t 2008
#c 19
#% 44876
#% 344568
#% 575676
#% 732537
#% 771023
#% 836801
#% 889176
#% 975169
#% 975178
#% 1405983
#% 1667612
#% 1730598
#% 1828398
#! We consider the problem of optimizing multilabel MRFs, which is in general NP-hard and ubiquitous in low-level computer vision. One approach for its solution is to formulate it as an integer linear programming and relax the integrality constraints. The approach we consider in this paper is to first convert the multi-label MRF into an equivalent binary-label MRF and then to relax it. The resulting relaxation can be efficiently solved using a maximum flow algorithm. Its solution provides us with a partially optimal labelling of the binary variables. This partial labelling is then easily transferred to the multi-label problem. We study the theoretical properties of the new relaxation and compare it with the standard one. Specifically, we compare tightness, and characterize a subclass of problems where the two relaxations coincide. We propose several combined algorithms based on the technique and demonstrate their performance on challenging computer vision problems.

#index 1073933
#* Space-indexed dynamic programming: learning to follow trajectories
#@ J. Zico Kolter;Adam Coates;Andrew Y. Ng;Yi Gu;Charles DuHadway
#t 2008
#c 19
#% 62914
#% 69418
#% 363744
#% 365039
#% 390351
#% 998545
#! We consider the task of learning to accurately follow a trajectory in a vehicle such as a car or helicopter. A number of dynamic programming algorithms such as Differential Dynamic Programming (DDP) and Policy Search by Dynamic Programming (PSDP), can efficiently compute non-stationary policies for these tasks --- such policies in general are well-suited to trajectory following since they can easily generate different control actions at different times in order to follow the trajectory. However, a weakness of these algorithms is that their policies are time-indexed, in that they apply different policies depending on the current time. This is problematic since 1) the current time may not correspond well to where we are along the trajectory and 2) the uncertainty over states can prevent these algorithms from finding any good policies at all. In this paper we propose a method for space-indexed dynamic programming that overcomes both these difficulties. We begin by showing how a dynamical system can be rewritten in terms of a spatial index variable (i.e., how far along the trajectory we are) rather than as a function of time. We then use these space-indexed dynamical systems to derive space-indexed version of the DDP and PSDP algorithms. Finally, we show that these algorithms perform well on a variety of control tasks, both in simulation and on real systems.

#index 1073934
#* The skew spectrum of graphs
#@ Risi Kondor;Karsten M. Borgwardt
#t 2008
#c 19
#% 66635
#% 147411
#% 731607
#% 833065
#% 844291
#% 881523
#% 915350
#% 1558464
#! The central issue in representing graph-structured data instances in learning algorithms is designing features which are invariant to permuting the numbering of the vertices. We present a new system of invariant graph features which we call the skew spectrum of graphs. The skew spectrum is based on mapping the adjacency matrix of any (weigted, directed, unlabeled) graph to a function on the symmetric group and computing bispectral invariants. The reduced form of the skew spectrum is computable in O(n3) time, and experiments show that on several benchmark datasets it can outperform state of the art graph kernels.

#index 1073935
#* Fast estimation of first-order clause coverage through randomization and maximum likelihood
#@ Ondřej Kuželka;Filip Železný
#t 2008
#c 19
#% 217072
#% 420713
#% 425004
#% 748639
#% 835212
#% 883333
#% 961264
#% 1165100
#% 1271843
#! In inductive logic programming, θ-subsumption is a widely used coverage test. Unfortunately, testing θ-subsumption is NP-complete, which represents a crucial efficiency bottleneck for many relational learners. In this paper, we present a probabilistic estimator of clause coverage, based on a randomized restarted search strategy. Under a distribution assumption, our algorithm can estimate clause coverage without having to decide subsumption for all examples. We implement this algorithm in program ReCovEr. On generated graph data and real-world datasets, we show that ReCovEr provides reasonably accurate estimates while achieving dramatic runtimes improvements compared to a state-of-the-art algorithm.

#index 1073936
#* Query-level stability and generalization in learning to rank
#@ Yanyan Lan;Tie-Yan Liu;Tao Qin;Zhiming Ma;Hang Li
#t 2008
#c 19
#% 387427
#% 411762
#% 577224
#% 722805
#% 734915
#% 840846
#% 879588
#% 983820
#% 1039843
#% 1705503
#! This paper is concerned with the generalization ability of learning to rank algorithms for information retrieval (IR). We point out that the key for addressing the learning problem is to look at it from the viewpoint of query. We define a number of new concepts, including query-level loss, query-level risk, and query-level stability. We then analyze the generalization ability of learning to rank algorithms by giving query-level generalization bounds to them using query-level stability as a tool. Such an analysis is very helpful for us to derive more advanced algorithms for IR. We apply the proposed theory to the existing algorithms of Ranking SVM and IRSVM. Experimental results on the two algorithms verify the correctness of the theoretical analysis.

#index 1073937
#* Modeling interleaved hidden processes
#@ Niels Landwehr
#t 2008
#c 19
#% 246836
#% 292004
#% 292235
#% 843360
#% 1000452
#% 1275113
#% 1403133
#! Hidden Markov models assume that observations in time series data stem from some hidden process that can be compactly represented as a Markov chain. We generalize this model by assuming that the observed data stems from multiple hidden processes, whose outputs interleave to form the sequence of observations. Exact inference in this model is NP-hard. However, a tractable and effective inference algorithm is obtained by extending structured approximate inference methods used in factorial hidden Markov models. The proposed model is evaluated in an activity recognition domain, where multiple activities interleave and together generate a stream of sensor observations. It is shown to be more accurate than a standard hidden Markov model in this domain.

#index 1073938
#* Exploration scavenging
#@ John Langford;Alexander Strehl;Jennifer Wortman
#t 2008
#c 19
#% 11948
#% 170389
#% 411762
#% 425053
#% 593734
#% 956547
#% 961172
#% 963333
#! We examine the problem of evaluating a policy in the contextual bandit setting using only observations collected during the execution of another policy. We show that policy evaluation can be impossible if the exploration policy chooses actions based on the side information provided at each time step. We then propose and prove the correctness of a principled method for policy evaluation which works when this is not the case, even when the exploration policy is deterministic, as long as each action is explored sufficiently often. We apply this general technique to the problem of offline evaluation of internet advertising policies. Although our theoretical results hold only when the exploration policy chooses ads independent of side information, an assumption that is typically violated by commercial systems, we show how clever uses of the theory provide non-trivial and realistic applications. We also provide an empirical demonstration of the effectiveness of our techniques on real ad placement data.

#index 1073939
#* Classification using discriminative restricted Boltzmann machines
#@ Hugo Larochelle;Yoshua Bengio
#t 2008
#c 19
#% 92145
#% 450888
#% 682451
#% 875987
#% 883830
#% 891060
#% 983863
#% 983903
#% 989599
#% 1061434
#% 1250575
#% 1455666
#! Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.

#index 1073940
#* Transfer of samples in batch reinforcement learning
#@ Alessandro Lazaric;Marcello Restelli;Andrea Bonarini
#t 2008
#c 19
#% 788055
#% 829011
#% 840937
#% 1014676
#% 1024771
#% 1274897
#! The main objective of transfer in reinforcement learning is to reduce the complexity of learning the solution of a target task by effectively reusing the knowledge retained from solving a set of source tasks. In this paper, we introduce a novel algorithm that transfers samples (i.e., tuples 〈s, a, s', r〉) from source to target tasks. Under the assumption that tasks have similar transition models and reward functions, we propose a method to select samples from the source tasks that are mostly similar to the target task, and, then, to use them as input for batch reinforcement-learning algorithms. As a result, the number of samples an agent needs to collect from the target task to learn its solution is reduced. We empirically show that, following the proposed approach, the transfer of samples is effective in reducing the learning complexity, even when some source tasks are significantly different from the target task.

#index 1073941
#* Local likelihood modeling of temporal text streams
#@ Guy Lebanon;Yang Zhao
#t 2008
#c 19
#% 105562
#% 170405
#% 279755
#% 342600
#% 355810
#% 763708
#% 879596
#% 939346
#% 1040832
#! Temporal text data is often generated by a time-changing process or distribution. Such a drift in the underlying distribution cannot be captured by stationary likelihood techniques. We consider the application of local likelihood methods to generative and conditional modeling of temporal document sequences. We examine the asymptotic bias and variance and present an experimental study using the RCV1 dataset containing a temporal sequence of Reuters news stories.

#index 1073942
#* A worst-case comparison between temporal difference and residual gradient with linear function approximation
#@ Lihong Li
#t 2008
#c 19
#% 3084
#% 203600
#% 227736
#% 363744
#% 384911
#% 393786
#% 449561
#% 465898
#% 734920
#% 1860015
#! Residual gradient (RG) was proposed as an alternative to TD(0) for policy evaluation when function approximation is used, but there exists little formal analysis comparing them except in very limited cases. This paper employs techniques from online learning of linear functions and provides a worst-case (non-probabilistic) analysis to compare these two types of algorithms when linear function approximation is used. No statistical assumptions are made on the sequence of observations, so the analysis applies to non-Markovian and even adversarial domains as well. In particular, our results suggest that RG may result in smaller temporal differences, while TD(0) is more likely to yield smaller prediction errors. These phenomena can be observed even in two simple Markov chain examples that are non-adversarial.

#index 1073943
#* Knows what it knows: a framework for self-aware learning
#@ Lihong Li;Michael L. Littman;Thomas J. Walsh
#t 2008
#c 19
#% 697
#% 170649
#% 177539
#% 232728
#% 314872
#% 425075
#% 431296
#% 451055
#% 495933
#% 722895
#% 749132
#% 876056
#% 949573
#% 961177
#% 1269772
#% 1815581
#! We introduce a learning framework that combines elements of the well-known PAC and mistake-bound models. The KWIK (knows what it knows) framework was designed particularly for its utility in learning settings where active exploration can impact the training examples the learner is exposed to, as is true in reinforcement-learning and active-learning problems. We catalog several KWIK-learnable classes and open problems.

#index 1073944
#* Pairwise constraint propagation by semidefinite programming for semi-supervised classification
#@ Zhenguo Li;Jianzhuang Liu;Xiaoou Tang
#t 2008
#c 19
#% 464291
#% 464608
#% 466890
#% 722902
#% 743284
#% 757953
#% 769881
#% 770782
#% 840892
#% 961218
#% 983849
#% 1269773
#% 1279294
#% 1455666
#! We consider the general problem of learning from both pairwise constraints and unlabeled data. The pairwise constraints specify whether two objects belong to the same class or not, known as the must-link constraints and the cannot-link constraints. We propose to learn a mapping that is smooth over the data graph and maps the data onto a unit hypersphere, where two must-link objects are mapped to the same point while two cannot-link objects are mapped to be orthogonal. We show that such a mapping can be achieved by formulating a semidefinite programming problem, which is convex and can be solved globally. Our approach can effectively propagate pairwise constraints to the whole data set. It can be directly applied to multi-class classification and can handle data labels, pairwise constraints, or a mixture of them in a unified framework. Promising experimental results are presented for classification tasks on a variety of synthetic and real data sets.

#index 1073945
#* An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators
#@ Percy Liang;Michael I. Jordan
#t 2008
#c 19
#% 464434
#% 883830
#% 961199
#% 1250575
#! Statistical and computational concerns have motivated parameter estimators based on various forms of likelihood, e.g., joint, conditional, and pseudolikelihood. In this paper, we present a unified framework for studying these estimators, which allows us to compare their relative (statistical) efficiencies. Our asymptotic analysis suggests that modeling more of the data tends to reduce variance, but at the cost of being more sensitive to model misspecification. We present experiments validating our analysis.

#index 1073946
#* Structure compilation: trading structure for features
#@ Percy Liang;Hal Daumé, III;Dan Klein
#t 2008
#c 19
#% 464434
#% 703975
#% 722806
#% 881511
#% 894635
#% 1289530
#! Structured models often achieve excellent performance but can be slow at test time. We investigate structure compilation, where we replace structure with features, which are often computationally simpler but unfortunately statistically more complex. We analyze this tradeoff theoretically and empirically on three natural language processing tasks. We also introduce a simple method to transfer predictive power from structure to features via unlabeled data, while incurring a minimal statistical penalty.

#index 1073947
#* ManifoldBoost: stagewise function approximation for fully-, semi- and un-supervised learning
#@ Nicolas Loeff;David Forsyth;Deepak Ramachandran
#t 2008
#c 19
#% 466263
#% 1455666
#! We describe a manifold learning framewor that naturally accommodates supervised learning, partially supervised learning and unsupervised clustering as particular cases. Our method chooses a function by minimizing loss subject to a manifold regularization penalty. This augmented cost is minimized using a greedy, stagewise, functional minimization procedure, as in Gradientboost. Each stage of boosting is fast and efficient. We demonstrate our approach using both radial basis function approximations and trees. The performance of our method is at the state of the art on many standard semi-supervised learning benchmarks, and we produce results for large scale datasets.

#index 1073948
#* Random classification noise defeats all convex potential boosters
#@ Philip M. Long;Rocco A. Servedio
#t 2008
#c 19
#% 198701
#% 235377
#% 302391
#% 312727
#% 331916
#% 425019
#% 446682
#% 562952
#% 850010
#% 1014681
#% 1478814
#% 1705506
#! A broad class of boosting algorithms can be interpreted as performing coordinate-wise gradient descent to minimize some potential function of the margins of a data set. This class includes AdaBoost, LogitBoost, and other widely used and well-studied boosters. In this paper we show that for a broad class of convex potential functions, any such boosting algorithm is highly susceptible to random classification noise. We do this by showing that for any such booster and any nonzero random classification noise rate η, there is a simple data set of examples which is efficiently learnable by such a booster if there is no noise, but which cannot be learned to accuracy better than 1/2 if there is random classification noise at rate η. This negative result is in contrast with known branching program based boosters which do not fall into the convex potential function framework and which can provably learn to high accuracy in the presence of random classification noise.

#index 1073949
#* Uncorrelated multilinear principal component analysis through successive variance maximization
#@ Haiping Lu;Konstantinos N. Plataniotis;Anastasios N. Venetsanopoulos
#t 2008
#c 19
#% 315986
#% 316150
#% 727684
#% 769911
#% 812492
#% 846431
#% 848112
#% 940356
#% 1022958
#% 1302072
#% 1786347
#% 1862054
#! Tensorial data are frequently encountered in various machine learning tasks today and dimensionality reduction is one of their most important applications. This paper extends the classical principal component analysis (PCA) to its multilinear version by proposing a novel unsupervised dimensionality reduction algorithm for tensorial data, named as uncorrelated multilinear PCA (UMPCA). UMPCA seeks a tensor-to-vector projection that captures most of the variation in the original tensorial input while producing uncorrelated features through successive variance maximization. We evaluate the UMPCA on a second-order tensorial problem, face recognition, and the experimental results show its superiority, especially in low-dimensional spaces, through the comparison with three other PCA-based algorithms.

#index 1073950
#* A reproducing kernel Hilbert space framework for pairwise time series distances
#@ Zhengdong Lu;Todd K. Leen;Yonghong Huang;Deniz Erdogmus
#t 2008
#c 19
#% 420077
#% 562954
#% 763697
#% 788036
#% 891549
#% 916785
#! A good distance measure for time series needs to properly incorporate the temporal structure, and should be applicable to sequences with unequal lengths. In this paper, we propose a distance measure as a principled solution to the two requirements. Unlike the conventional feature vector representation, our approach represents each time series with a summarizing smooth curve in a reproducing kernel Hilbert space (RKHS), and therefore translate the distance between time series into distances between curves. Moreover we propose to learn the kernel of this RKHS from a population of time series with discrete observations using Gaussian process-based non-parametric mixed-effect models. Experiments on two vastly different real-world problems show that the proposed distance measure leads to improved classification accuracy over the conventional distance measures.

#index 1073951
#* On-line discovery of temporal-difference networks
#@ Takaki Makino;Toshihisa Takagi
#t 2008
#c 19
#% 770781
#% 770863
#% 840946
#% 840958
#% 857087
#% 875961
#% 1289489
#! We present an algorithm for on-line, incremental discovery of temporal-difference (TD) networks. The key contribution is the establishment of three criteria to expand a node in TD network: a node is expanded when the node is well-known, independent, and has a prediction error that requires further explanation. Since none of these criteria requires centralized calculation operations, they are easily computed in a parallel and distributed manner, and scalable for bigger problems compared to other discovery methods of predictive state representations. Through computer experiments, we demonstrate the empirical effectiveness of our algorithm.

#index 1073952
#* Nonextensive entropic kernels
#@ André F. T. Martins;Mário A. T. Figueiredo;Pedro M. Q. Aguiar;Noah A. Smith;Eric P. Xing
#t 2008
#c 19
#% 115608
#% 771841
#% 794860
#% 818235
#% 829033
#% 916785
#% 1810152
#% 1814838
#! Positive definite kernels on probability measures have been recently applied in structured data classification problems. Some of these kernels are related to classic information theoretic quantities, such as mutual information and the Jensen-Shannon divergence. Meanwhile, driven by recent advances in Tsallis statistics, nonextensive generalizations of Shannon's information theory have been proposed. This paper bridges these two trends. We introduce the Jensen-Tsallis q-difference, a generalization of the Jensen-Shannon divergence. We then define a new family of nonextensive mutual information kernels, which allow weights to be assigned to their arguments, and which includes the Boolean, Jensen-Shannon, and linear kernels as particular cases. We illustrate the performance of these kernels on text categorization tasks.

#index 1073953
#* Automatic discovery and transfer of MAXQ hierarchies
#@ Neville Mehta;Soumya Ray;Prasad Tadepalli;Thomas Dietterich
#t 2008
#c 19
#% 286423
#% 458686
#% 464303
#% 464607
#% 464636
#% 465739
#% 578674
#% 890241
#% 961214
#% 1271827
#! We present an algorithm, HI-MAT (Hierarchy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by applying dynamic Bayesian network models to a successful trajectory from a source reinforcement learning task. HI-MAT discovers subtasks by analyzing the causal and temporal relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consistent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empirically that HI-MAT constructs compact hierarchies that are comparable to manually-engineered hierarchies and facilitate significant speedup in learning when transferred to a target task.

#index 1073954
#* Rank minimization via online learning
#@ Raghu Meka;Prateek Jain;Constantine Caramanis;Inderjit S. Dhillon
#t 2008
#c 19
#% 109653
#% 235377
#% 263387
#% 770767
#% 836524
#% 840839
#% 850011
#% 876008
#% 1080960
#% 1674795
#% 1815826
#! Minimum rank problems arise frequently in machine learning applications and are notoriously difficult to solve due to the non-convex nature of the rank objective. In this paper, we present the first online learning approach for the problem of rank minimization of matrices over polyhedral sets. In particular, we present two online learning algorithms for rank minimization - our first algorithm is a multiplicative update method based on a generalized experts framework, while our second algorithm is a novel application of the online convex programming framework (Zinkevich, 2003). In the latter, we flip the role of the decision maker by making the decision maker search over the constraint space instead of feasible points, as is usually the case in online convex programming. A salient feature of our online learning approach is that it allows us to give provable approximation guarantees for the rank minimization problem over polyhedral sets. We demonstrate the effectiveness of our methods on synthetic examples, and on the real-life application of low-rank kernel learning.

#index 1073955
#* An analysis of reinforcement learning with function approximation
#@ Francisco S. Melo;Sean P. Meyn;M. Isabel Ribeiro
#t 2008
#c 19
#% 83929
#% 203598
#% 226878
#% 319765
#% 331911
#% 393786
#% 425072
#% 464438
#% 564257
#% 647069
#% 751025
#% 770867
#! We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of Q-learning when combined with function approximation, extending the analysis of TD-learning in (Tsitsiklis & Van Roy, 1996a) to stochastic control settings. We identify conditions under which such approximate methods converge with probability 1. We conclude with a brief discussion on the general applicability of our results and compare them with several related works.

#index 1073956
#* Empirical Bernstein stopping
#@ Volodymyr Mnih;Csaba Szepesvári;Jean-Yves Audibert
#t 2008
#c 19
#% 229959
#% 314829
#% 466908
#% 562952
#% 563266
#% 564834
#! Sampling is a popular way of scaling up machine learning algorithms to large datasets. The question often is how many samples are needed. Adaptive stopping algorithms monitor the performance in an online fashion and they can stop early, saving valuable resources. We consider problems where probabilistic guarantees are desired and demonstrate how recently-introduced empirical Bernstein bounds can be used to design stopping rules that are efficient. We provide upper bounds on the sample complexity of the new rules, as well as empirical results on model selection and boosting in the filtering setting.

#index 1073957
#* Efficiently solving convex relaxations for MAP estimation
#@ M. Pawan Kumar;P. H. S. Torr
#t 2008
#c 19
#% 16602
#% 205305
#% 256814
#% 271300
#% 836724
#% 889176
#% 1730598
#% 1815753
#! The problem of obtaining the maximum a posteriori (MAP) estimate of a discrete random field is of fundamental importance in many areas of Computer Science. In this work, we build on the tree reweighted message passing (TRW) framework of (Kolmogorov, 2006; Wainwright et al., 2005). TRW iteratively optimizes the Lagrangian dual of a linear programming relaxation for MAP estimation. We show how the dual formulation of TRW can be extended to include cycle inequalities (Barahona & Mahjoub, 1986) and some recently proposed second order cone (SOC) constraints (Kumar et al., 2007). We propose efficient iterative algorithms for solving the resulting duals. Similar to the method described in (Kolmogorov, 2006), these algorithms are guaranteed to converge. We test our approach on a large set of synthetic data, as well as real data. Our experiments show that the additional constraints (i.e. cycle inequalities and SOC constraints) provide better results in cases where the TRW framework fails (namely MAP estimation for non-submodular energy functions).

#index 1073958
#* On the hardness of finding symmetries in Markov decision processes
#@ Shravan Matthur Narayanamurthy;Balaraman Ravindran
#t 2008
#c 19
#% 112912
#% 115513
#% 464442
#% 599556
#% 655325
#% 777133
#% 777317
#% 836038
#% 1478746
#! In this work we address the question of finding symmetries of a given MDP. We show that the problem is Isomorphism Complete, that is, the problem is polynomially equivalent to verifying whether two graphs are isomorphic. Apart from the theoretical importance of this result it has an important practical application. The reduction presented can be used together with any off-the-shelf Graph Isomorphism solver, which performs well in the average case, to find symmetries of an MDP. In fact, we present results of using NAutY (the best Graph Isomorphism solver currently available), to find symmetries of MDPs.

#index 1073959
#* Bayes optimal classification for decision trees
#@ Siegfried Nijssen
#t 2008
#c 19
#% 136350
#% 232136
#% 376266
#% 466483
#% 466583
#% 989624
#% 1289453
#! We present an algorithm for exact Bayes optimal classification from a hypothesis space of decision trees satisfying leaf constraints. Our contribution is that we reduce this classification problem to the problem of finding a rule-based classifier with appropriate weights. We show that these rules and weights can be computed in linear time from the output of a modified frequent itemset mining algorithm, which means that we can compute the classifier in practice, despite the exponential worst-case complexity. In experiments we compare the Bayes optimal predictions with those of the maximum a posteriori hypothesis.

#index 1073960
#* A decoupled approach to exemplar-based unsupervised learning
#@ Sebastian Nowozin;Gökhan Bakir
#t 2008
#c 19
#% 318133
#% 349208
#% 443894
#% 757953
#% 845502
#% 876043
#% 916785
#% 1856420
#% 1860543
#! A recent trend in exemplar based unsupervised learning is to formulate the learning problem as a convex optimization problem. Convexity is achieved by restricting the set of possible prototypes to training exemplars. In particular, this has been done for clustering, vector quantization and mixture model density estimation. In this paper we propose a novel algorithm that is theoretically and practically superior to these convex formulations. This is possible by posing the unsupervised learning problem as a single convex "master problem" with non-convex subproblems. We show that for the above learning tasks the subproblems are extremely well-behaved and can be solved efficiently.

#index 1073961
#* Cost-sensitive multi-class classification from probability estimates
#@ Deirdre B. O'Brien;Maya R. Gupta;Robert M. Gray
#t 2008
#c 19
#% 280437
#% 331909
#% 420054
#% 464280
#% 577298
#% 840913
#! For two-class classification, it is common to classify by setting a threshold on class probability estimates, where the threshold is determined by ROC curve analysis. An analog for multi-class classification is learning a new class partitioning of the multiclass probability simplex to minimize empirical misclassification costs. We analyze the interplay between systematic errors in the class probability estimates and cost matrices for multiclass classification. We explore the effect on the class partitioning of five different transformations of the cost matrix. Experiments on benchmark datasets with naive Bayes and quadratic discriminant analysis show the effectiveness of learning a new partition matrix compared to previously proposed methods.

#index 1073962
#* The projectron: a bounded kernel-based Perceptron
#@ Francesco Orabona;Joseph Keshet;Barbara Caputo
#t 2008
#c 19
#% 458685
#% 722817
#% 961152
#% 1042610
#% 1674794
#% 1705518
#% 1759695
#% 1815223
#! We present a discriminative online algorithm with a bounded memory growth, which is based on the kernel-based Perceptron. Generally, the required memory of the kernel-based Perceptron for storing the online hypothesis is not bounded. Previous work has been focused on discarding part of the instances in order to keep the memory bounded. In the proposed algorithm the instances are not discarded, but projected onto the space spanned by the previous online hypothesis. We derive a relative mistake bound and compare our algorithm both analytically and empirically to the state-of-the-art Forgetron algorithm (Dekel et al, 2007). The first variant of our algorithm, called Projectron, outperforms the Forgetron. The second variant, called Projectron++, outperforms even the Perceptron.

#index 1073963
#* Learning dissimilarities by ranking: from SDP to QP
#@ Hua Ouyang;Alex Gray
#t 2008
#c 19
#% 209623
#% 209961
#% 272510
#% 577224
#% 763697
#% 829029
#% 829031
#% 1558464
#! We consider the problem of learning dissimilarities between points via formulations which preserve a specified ordering between points rather than the numerical values of the dissimilarities. Dissimilarity ranking (d-ranking) learns from instances like "A is more similar to B than C is to D" or "The distance between E and F is larger than that between G and H". Three formulations of d-ranking problems are presented and new algorithms are presented for two of them, one by semidefinite programming (SDP) and one by quadratic programming (QP). Among the novel capabilities of these approaches are out-of-sample prediction and scalability to large problems.

#index 1073964
#* A distance model for rhythms
#@ Jean-François Paiement;Yves Grandvalet;Samy Bengio;Douglas Eck
#t 2008
#c 19
#% 450268
#% 729437
#% 843645
#% 1112718
#! Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper, we introduce a model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases.

#index 1073965
#* On the chance accuracies of large collections of classifiers
#@ Mark Palatucci;Andrew Carlson
#t 2008
#c 19
#% 307109
#% 451139
#% 466068
#% 578689
#% 722929
#% 768668
#! We provide a theoretical analysis of the chance accuracies of large collections of classifiers. We show that on problems with small numbers of examples, some classifier can perform well by random chance, and we derive a theorem to explicitly calculate this accuracy. We use this theorem to provide a principled feature selection criterion for sparse, high-dimensional problems. We evaluate this method on microarray and fMRI datasets and show that it performs very close to the optimal accuracy obtained from an oracle. We also show that on the fMRI dataset this technique chooses relevant features successfully while another state-of-the-art method, the False Discovery Rate (FDR), completely fails at standard significance levels.

#index 1073966
#* An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning
#@ Ronald Parr;Lihong Li;Gavin Taylor;Christopher Painter-Wakefield;Michael L. Littman
#t 2008
#c 19
#% 203596
#% 366058
#% 449561
#% 466235
#% 496267
#% 734920
#% 876001
#% 983896
#% 1014677
#% 1275167
#% 1478746
#! We show that linear value-function approximation is equivalent to a form of linear model approximation. We then derive a relationship between the model-approximation error and the Bellman error, and show how this relationship can guide feature selection for model improvement and/or value-function improvement. We also show how these results give insight into the behavior of existing feature-selection algorithms.

#index 1073967
#* Learning to learn implicit queries from gaze patterns
#@ Kai Puolamäki;Antti Ajanki;Samuel Kaski
#t 2008
#c 19
#% 169781
#% 236495
#% 236497
#% 267043
#% 272510
#% 320432
#% 340936
#% 387427
#% 424989
#% 429833
#% 731615
#% 738969
#% 805200
#% 818220
#% 818221
#% 879566
#% 884026
#% 916788
#% 1004294
#! In the absence of explicit queries, an alternative is to try to infer users' interests from implicit feedback signals, such as clickstreams or eye tracking. The interests, formulated as an implicit query, can then be used in further searches. We formulate this task as a probabilistic model, which can be interpreted as a kind of transfer or meta-learning. The probabilistic model is demonstrated to outperform an earlier kernel-based method in a small-scale information retrieval task.

#index 1073968
#* Multi-task compressive sensing with Dirichlet process priors
#@ Yuting Qi;Dehong Liu;David Dunson;Lawrence Carin
#t 2008
#c 19
#% 274586
#% 360691
#% 722760
#% 770861
#% 873582
#% 1298331
#% 1759741
#% 1762152
#% 1815896
#% 1815965
#! Compressive sensing (CS) is an emerging £eld that, under appropriate conditions, can signi£cantly reduce the number of measurements required for a given signal. In many applications, one is interested in multiple signals that may be measured in multiple CS-type measurements, where here each signal corresponds to a sensing "task". In this paper we propose a novel multitask compressive sensing framework based on a Bayesian formalism, where a Dirichlet process (DP) prior is employed, yielding a principled means of simultaneously inferring the appropriate sharing mechanisms as well as CS inversion for each task. A variational Bayesian (VB) inference algorithm is employed to estimate the full posterior on the model parameters.

#index 1073969
#* Estimating labels from label proportions
#@ Novi Quadrianto;Alex J. Smola;Tiberio S. Caetano;Quoc V. Le
#t 2008
#c 19
#% 722909
#% 864390
#% 983878
#% 989644
#% 1117019
#% 1674770
#% 1674771
#! Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, also with known label proportions. This problem appears in areas like e-commerce, spam filtering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice.

#index 1073970
#* Learning diverse rankings with multi-armed bandits
#@ Filip Radlinski;Robert Kleinberg;Thorsten Joachims
#t 2008
#c 19
#% 262112
#% 297675
#% 387427
#% 416988
#% 425053
#% 577224
#% 642975
#% 818262
#% 818266
#% 823348
#% 829028
#% 840846
#% 879566
#% 879567
#% 879618
#% 987226
#% 987313
#% 989628
#% 1035577
#! Algorithms for learning to rank Web documents usually assume a document's relevance is independent of other documents. This leads to learned ranking functions that produce rankings with redundant results. In contrast, user studies have shown that diversity at high ranks is often preferred. We present two online learning algorithms that directly learn a diverse ranking of documents based on users' clicking behavior. We show that these algorithms minimize abandonment, or alternatively, maximize the probability that a relevant document is found in the top k positions of a ranking. Moreover, one of our algorithms asymptotically achieves optimal worst-case performance even if users' interests change.

#index 1073971
#* Semi-supervised learning of compact document representations with deep networks
#@ Marc' Aurelio Ranzato;Martin Szummer
#t 2008
#c 19
#% 169781
#% 476873
#% 722904
#% 875987
#% 891060
#% 1650298
#! Finding good representations of text documents is crucial in information retrieval and classification systems. Today the most popular document representation is based on a vector of word counts in the document. This representation neither captures dependencies between related words, nor handles synonyms or polysemous words. In this paper, we propose an algorithm to learn text document representations based on semi-supervised autoencoders that are stacked to form a deep network. The model can be trained efficiently on partially labeled corpora, producing very compact representations of documents, while retaining as much class information and joint word statistics as possible. We show that it is advantageous to exploit even a few labeled samples during training.

#index 1073972
#* Message-passing for graph-structured linear programs: proximal projections, convergence and rounding schemes
#@ Pradeep Ravikumar;Alekh Agarwal;Martin J. Wainwright
#t 2008
#c 19
#% 197978
#% 344568
#% 382586
#% 382680
#% 382854
#% 803614
#% 876037
#% 883899
#% 937711
#% 1810398
#% 1815753
#! A large body of past work has focused on the first-order tree-based LP relaxation for the MAP problem in Markov random fields. This paper develops a family of super-linearly convergent LP solvers based on proximal minimization schemes using Bregman divergences that exploit the underlying graphical structure, and so scale well to large problems. All of our algorithms have a double-loop character, with the outer loop corresponding to the proximal sequence, and an inner loop of cyclic Bregman divergences used to compute each proximal update. The inner loop updates are distributed and respect the graph structure, and thus can be cast as message-passing algorithms. We establish various convergence guarantees for our algorithms, illustrate their performance, and also present rounding schemes with provable optimality guarantees.

#index 1073973
#* Bayesian multiple instance learning: automatic feature selection and inductive transfer
#@ Vikas C. Raykar;Balaji Krishnapuram;Jinbo Bi;Murat Dundar;R. Bharat Rao
#t 2008
#c 19
#% 224755
#% 236497
#% 272527
#% 565537
#% 722760
#% 840922
#! We propose a novel Bayesian multiple instance learning (MIL) algorithm. This algorithm automatically identifies the relevant feature subset, and utilizes inductive transfer when learning multiple (conceptually related) classifiers. Experimental results indicate that the proposed MIL method is more accurate than previous MIL algorithms and selects a much smaller set of useful features. Inductive transfer further improves the accuracy of the classifier as compared to learning each task individually.

#index 1073974
#* Online kernel selection for Bayesian reinforcement learning
#@ Joseph Reisinger;Peter Stone;Risto Miikkulainen
#t 2008
#c 19
#% 384911
#% 722818
#% 768632
#% 840860
#% 891549
#% 891559
#% 961164
#% 1223289
#% 1269760
#! Kernel-based Bayesian methods for Reinforcement Learning (RL) such as Gaussian Process Temporal Difference (GPTD) are particularly promising because they rigorously treat uncertainty in the value function and make it easy to specify prior knowledge. However, the choice of prior distribution significantly affects the empirical performance of the learning agent, and little work has been done extending existing methods for prior model selection to the online setting. This paper develops Replacing-Kernel RL, an online model selection method for GPTD using sequential Monte-Carlo methods. Replacing-Kernel RL is compared to standard GPTD and tile-coding on several RL domains, and is shown to yield significantly better asymptotic performance for many different kernel families. Furthermore, the resulting kernels capture an intuitively useful notion of prior state covariance that may nevertheless be difficult to capture manually.

#index 1073975
#* The dynamic hierarchical Dirichlet process
#@ Lu Ren;David B. Dunson;Lawrence Carin
#t 2008
#c 19
#% 722904
#% 770861
#% 875959
#% 1761568
#! The dynamic hierarchical Dirichlet process (dHDP) is developed to model the time-evolving statistical properties of sequential data sets. The data collected at any time point are represented via a mixture associated with an appropriate underlying model, in the framework of HDP. The statistical properties of data collected at consecutive time points are linked via a random parameter that controls their probabilistic similarity. The sharing mechanisms of the time-evolving data are derived, and a relatively simple Markov Chain Monte Carlo sampler is developed. Experimental results are presented to demonstrate the model.

#index 1073976
#* Closed-form supervised dimensionality reduction with generalized linear models
#@ Irina Rish;Genady Grabarnik;Guillermo Cecchi;Francisco Pereira;Geoffrey J. Gordon
#t 2008
#c 19
#% 840931
#% 876031
#! We propose a family of supervised dimensionality reduction (SDR) algorithms that combine feature extraction (dimensionality reduction) with learning a predictive model in a unified optimization framework, using data- and class-appropriate generalized linear models (GLMs), and handling both classification and regression problems. Our approach uses simple closed-form update rules and is provably convergent. Promising empirical results are demonstrated on a variety of high-dimensional datasets.

#index 1073977
#* Bi-level path following for cross validated solution of kernel quantile regression
#@ Saharon Rosset
#t 2008
#c 19
#% 211801
#% 768632
#% 793245
#% 876069
#% 959470
#% 961168
#% 961178
#% 989670
#! Modeling of conditional quantiles requires specification of the quantile being estimated and can thus be viewed as a parameterized predictive modeling problem. Quantile loss is typically used, and it is indeed parameterized by a quantile parameter. In this paper we show how to follow the path of cross validated solutions to regularized kernel quantile regression. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on data. This algorithm allows us to efficiently solve the whole family of bi-level problems.

#index 1073978
#* The Group-Lasso for generalized linear models: uniqueness of solutions and efficient algorithms
#@ Volker Roth;Bernd Fischer
#t 2008
#c 19
#% 35402
#% 961265
#% 1815597
#! The Group-Lasso method for finding important explanatory factors suffers from the potential non-uniqueness of solutions and also from high computational costs. We formulate conditions for the uniqueness of Group-Lasso solutions which lead to an easily implementable test procedure that allows us to identify all potentially active groups. These results are used to derive an efficient algorithm that can deal with input dimensions in the millions and can approximate the solution path efficiently. The derived methods are applied to large-scale learning problems where they exhibit excellent performance and where the testing procedure helps to avoid misinterpretations of the solutions.

#index 1073979
#* Robust matching and recognition using context-dependent kernels
#@ Hichem Sahbi;Jean-Yves Audibert;Jaonary Rabarisoa;Renaud Keriven
#t 2008
#c 19
#% 116149
#% 120270
#% 227526
#% 280826
#% 309208
#% 457915
#% 469390
#% 640416
#% 724232
#% 731607
#% 734914
#% 760805
#% 812323
#% 812495
#% 812569
#% 816170
#% 891559
#% 961270
#% 1860548
#! The success of kernel methods including support vector machines (SVMs) strongly depends on the design of appropriate kernels. While initially kernels were designed in order to handle fixed-length data, their extension to unordered, variable-length data became more than necessary for real pattern recognition problems such as object recognition and bioinformatics. We focus in this paper on object recognition using a new type of kernel referred to as "context-dependent". Objects, seen as constellations of local features (interest points, regions, etc.), are matched by minimizing an energy function mixing (1) a fidelity term which measures the quality of feature matching, (2) a neighborhood criterion which captures the object geometry and (3) a regularization term. We will show that the fixed-point of this energy is a "context-dependent" kernel ("CDK") which also satisfies the Mercer condition. Experiments conducted on object recognition show that when plugging our kernel in SVMs, we clearly outperform SVMs with "context-free" kernels.

#index 1073980
#* Privacy-preserving reinforcement learning
#@ Jun Sakuma;Shigenobu Kobayashi;Rebecca N. Wright
#t 2008
#c 19
#% 366058
#% 466262
#% 743280
#% 771917
#% 810588
#% 823389
#% 874166
#% 948086
#% 963800
#% 1068712
#% 1411057
#! We consider the problem of distributed reinforcement learning (DRL) from private perceptions. In our setting, agents' perceptions, such as states, rewards, and actions, are not only distributed but also should be kept private. Conventional DRL algorithms can handle multiple agents, but do not necessarily guarantee privacy preservation and may not guarantee optimality. In this work, we design cryptographic solutions that achieve optimal policies without requiring the agents to share their private information.

#index 1073981
#* On the quantitative analysis of deep belief networks
#@ Ruslan Salakhutdinov;Iain Murray
#t 2008
#c 19
#% 424845
#% 450888
#% 875987
#% 891060
#% 983903
#% 1061434
#% 1815596
#% 1815597
#! Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.

#index 1073982
#* Bayesian probabilistic matrix factorization using Markov chain Monte Carlo
#@ Ruslan Salakhutdinov;Andriy Mnih
#t 2008
#c 19
#% 132681
#% 151214
#% 303620
#% 770859
#% 840924
#% 1650298
#! Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction accuracy than PMF models trained using MAP estimation.

#index 1073983
#* Accurate max-margin training for structured output spaces
#@ Sunita Sarawagi;Rahul Gupta
#t 2008
#c 19
#% 277467
#% 344568
#% 464434
#% 722903
#% 827631
#% 829043
#% 881477
#% 939343
#% 939977
#% 961193
#% 983815
#! Tsochantaridis et al. (2005) proposed two formulations for maximum margin training of structured spaces: margin scaling and slack scaling. While margin scaling has been extensively used since it requires the same kind of MAP inference as normal structured prediction, slack scaling is believed to be more accurate and better-behaved. We present an efficient variational approximation to the slack scaling method that solves its inference bottleneck while retaining its accuracy advantage over margin scaling. We further argue that existing scaling approaches do not separate the true labeling comprehensively while generating violating constraints. We propose a new max-margin trainer PosLearn that generates violators to ensure separation at each position of a decomposable loss function. Empirical results on real datasets illustrate that PosLearn can reduce test error by up to 25% over margin scaling and 10% over slack scaling. Further, PosLearn violators can be generated more efficiently than slack violators; for many structured tasks the time required is just twice that of MAP inference.

#index 1073984
#* Fast incremental proximity search in large graphs
#@ Purnamrita Sarkar;Andrew W. Moore;Amit Prakash
#t 2008
#c 19
#% 348173
#% 577273
#% 730089
#% 956551
#% 989646
#% 1016176
#% 1061639
#! In this paper we investigate two aspects of ranking problems on large graphs. First, we augment the deterministic pruning algorithm in Sarkar and Moore (2007) with sampling techniques to compute approximately correct rankings with high probability under random walk based proximity measures at query time. Second, we prove some surprising locality properties of these proximity measures by examining the short term behavior of random walks. The proposed algorithm can answer queries on the fly without caching any information about the entire graph. We present empirical results on a 600, 000 node author-word-citation graph from the Citeseer domain on a single CPU machine where the average query processing time is around 4 seconds. We present quantifiable link prediction tasks. On most of them our techniques outperform Personalized Pagerank, a well-known diffusion based proximity measure.

#index 1073985
#* Inverting the Viterbi algorithm: an abstract framework for structure design
#@ Michael Schnall-Levin;Leonid Chindelevitch;Bonnie Berger
#t 2008
#c 19
#% 906390
#! Probabilistic grammatical formalisms such as hidden Markov models (HMMs) and stochastic context-free grammars (SCFGs) have been extensively studied and widely applied in a number of fields. Here, we introduce a new algorithmic problem on HMMs and SCFGs that arises naturally from protein and RNA design, and which has not been previously studied. The problem can be viewed as an inverse to the one solved by the Viterbi algorithm on HMMs or by the CKY algorithm on SCFGs. We study this problem theoretically and obtain the first algorithmic results. We prove that the problem is NP-complete, even for a 3-letter emission alphabet, via a reduction from 3-SAT, a result that has implications for the hardness of RNA secondary structure design. We then develop a number of approaches for making the problem tractable. In particular, for HMMs we develop a branch-and-bound algorithm, which can be shown to have fixed-parameter tractable worst-case running time, exponential in the number of states of the HMM but linear in the length of the structure. We also show how to cast the problem as a Mixed Integer Linear Program.

#index 1073986
#* Compressed sensing and Bayesian experimental design
#@ Matthias W. Seeger;Hannes Nickisch
#t 2008
#c 19
#% 528330
#% 722760
#% 983851
#% 1074364
#% 1760042
#% 1815896
#% 1815965
#! We relate compressed sensing (CS) with Bayesian experimental design and provide a novel efficient approximate method for the latter, based on expectation propagation. In a large comparative study about linearly measuring natural images, we show that the simple standard heuristic of measuring wavelet coefficients top-down systematically outperforms CS methods using random measurements; the sequential projection optimisation approach of (Ji & Carin, 2007) performs even worse. We also show that our own approximate Bayesian method is able to learn measurement filters on full images efficiently which outperform the wavelet heuristic. To our knowledge, ours is the first successful attempt at "learning compressed sensing" for images of realistic size. In contrast to common CS methods, our framework is not restricted to sparse signals, but can readily be applied to other notions of signal complexity or noise models. We give concrete ideas how our method can be scaled up to large signal representations.

#index 1073987
#* Multi-classification by categorical features via clustering
#@ Yevgeny Seldin;Naftali Tishby
#t 2008
#c 19
#% 115608
#% 302395
#% 593043
#% 803574
#% 828046
#% 1396663
#% 1396693
#! We derive a generalization bound for multi-classification schemes based on grid clustering in categorical parameter product spaces. Grid clustering partitions the parameter space in the form of a Cartesian product of partitions for each of the parameters. The derived bound provides a means to evaluate clustering solutions in terms of the generalization power of a built-on classifier. For classification based on a single feature the bound serves to find a globally optimal classification rule. Comparison of the generalization power of individual features can then be used for feature ranking. Our experiments show that in this role the bound is much more precise than mutual information or normalized correlation indices.

#index 1073988
#* SVM optimization: inverse dependence on training set size
#@ Shai Shalev-Shwartz;Nathan Srebro
#t 2008
#c 19
#% 697
#% 269217
#% 269218
#% 722909
#% 881477
#% 983905
#% 1861002
#! We discuss how the runtime of SVM optimization should decrease as the size of the training data increases. We present theoretical and empirical results demonstrating how a simple subgradient descent approach indeed displays such behavior, at least for linear kernels.

#index 1073989
#* Data spectroscopy: learning mixture models using eigenspaces of convolution operators
#@ Tao Shi;Mikhail Belkin;Bin Yu
#t 2008
#c 19
#% 269226
#% 564285
#% 593926
#% 1705530
#! In this paper we develop a spectral framework for estimating mixture distributions, specifically Gaussian mixture models. In physics, spectroscopy is often used for the identification of substances through their spectrum. Treating a kernel function K(x, y) as "light" and the sampled data as "substance", the spectrum of their interaction (eigenvalues and eigenvectors of the kernel matrix K) unveils certain aspects of the underlying parametric distribution p, such as the parameters of a Gaussian mixture. Our approach extends the intuitions and analyses underlying the existing spectral techniques, such as spectral clustering and Kernel Principal Components Analysis (KPCA). We construct algorithms to estimate parameters of Gaussian mixture models, including the number of mixture components, their means and covariance matrices, which are important in many practical applications. We provide a theoretical framework and show encouraging experimental results.

#index 1073990
#* A generalization of Haussler's convolution kernel: mapping kernel
#@ Kilho Shin;Tetsuji Kuboyama
#t 2008
#c 19
#% 222804
#% 245500
#% 288885
#% 289193
#% 309208
#% 444031
#% 464640
#% 830744
#% 840908
#% 846095
#% 938668
#! Haussler's convolution kernel provides a successful framework for engineering new positive semidefinite kernels, and has been applied to a wide range of data types and applications. In the framework, each data object represents a finite set of finer grained components. Then, Haussler's convolution kernel takes a pair of data objects as input, and returns the sum of the return values of the predetermined primitive positive semidefinite kernel calculated for all the possible pairs of the components of the input data objects. On the other hand, the mapping kernel that we introduce in this paper is a natural generalization of Haussler's convolution kernel, in that the input to the primitive kernel moves over a predetermined subset rather than the entire cross product. Although we have plural instances of the mapping kernel in the literature, their positive semidefiniteness was investigated in case-by-case manners, and worse yet, was sometimes incorrectly concluded. In fact, there exists a simple and easily checkable necessary and sufficient condition, which is generic in the sense that it enables us to investigate the positive semidefiniteness of an arbitrary instance of the mapping kernel. This is the first paper that presents and proves the validity of the condition. In addition, we introduce two important instances of the mapping kernel, which we refer to as the size-of-index-structure-distribution kernel and the editcost-distribution kernel. Both of them are naturally derived from well known (dis)similarity measurements in the literature (e.g. the maximum agreement tree, the edit distance), and are reasonably expected to improve the performance of the existing measures by evaluating their distributional features rather than their peak (maximum/minimum) features.

#index 1073991
#* mStruct: a new admixture model for inference of population structure in light of both genetic admixing and allele mutations
#@ Suyash Shringarpure;Eric P. Xing
#t 2008
#c 19
#% 303620
#% 722904
#% 906262
#% 1038926
#% 1673048
#! Traditional methods for analyzing population structure, such as the Structure program, ignore the influence of mutational effects. We propose mStruct, an admixture of population-specific mixtures of inheritance models, that addresses the task of structure inference and mutation estimation jointly through a hierarchical Bayesian framework, and a variational algorithm for inference. We validated our method on synthetic data, and used it to analyze the HGDP-CEPH cell line panel of microsatellites used in (Rosenberg et al., 2002) and the HGDP SNP data used in (Conrad et al., 2006). A comparison of the structural maps of world populations estimated by mStruct and Structure is presented, and we also report potentially interesting mutation patterns in world populations estimated by mStruct, which is not possible by Structure.

#index 1073992
#* Expectation-maximization for sparse and non-negative PCA
#@ Christian D. Sigg;Joachim M. Buhmann
#t 2008
#c 19
#% 278040
#% 709571
#% 906232
#% 935262
#% 979617
#% 983826
#% 983908
#! We study the problem of finding the dominant eigenvector of the sample covariance matrix, under additional constraints on the vector: a cardinality constraint limits the number of non-zero elements, and non-negativity forces the elements to have equal sign. This problem is known as sparse and non-negative principal component analysis (PCA), and has many applications including dimensionality reduction and feature selection. Based on expectation-maximization for probabilistic PCA, we present an algorithm for any combination of these constraints. Its complexity is at most quadratic in the number of dimensions of the data. We demonstrate significant improvements in performance and computational efficiency compared to other constrained PCA algorithms, on large data sets from biology and computer vision. Finally, we show the usefulness of non-negative sparse PCA for unsupervised feature selection in a gene clustering task.

#index 1073993
#* Sample-based learning and search with permanent and transient memories
#@ David Silver;Richard S. Sutton;Martin Müller
#t 2008
#c 19
#% 90041
#% 366058
#% 425053
#% 449561
#% 453701
#% 983838
#% 983913
#% 1274923
#% 1289220
#% 1665148
#! We present a reinforcement learning architecture, Dyna-2, that encompasses both sample-based learning and sample-based search, and that generalises across states during both learning and search. We apply Dyna-2 to high performance Computer Go. In this domain the most successful planning methods are based on sample-based search algorithms, such as UCT, in which states are treated individually, and the most successful learning methods are based on temporal-difference learning algorithms, such as Sarsa, in which linear function approximation is used. In both cases, an estimate of the value function is formed, but in the first case it is transient, computed and then discarded after each move, whereas in the second case it is more permanent, slowly accumulating over many moves and games. The idea of Dyna-2 is for the transient planning memory and the permanent learning memory to remain separate, but for both to be based on linear function approximation and both to be updated by Sarsa. To apply Dyna-2 to 9x9 Computer Go, we use a million binary features in the function approximator, based on templates matching small fragments of the board. Using only the transient memory, Dyna-2 performed at least as well as UCT. Using both memories combined, it significantly outperformed UCT. Our program based on Dyna-2 achieved a higher rating on the Computer Go Online Server than any handcrafted or traditional search based program.

#index 1073994
#* An RKHS for multi-view learning and manifold co-regularization
#@ Vikas Sindhwani;David S. Rosenberg
#t 2008
#c 19
#% 252011
#% 563274
#% 840938
#% 875962
#% 961218
#! Inspired by co-training, many multi-view semi-supervised kernel methods implement the following idea: find a function in each of multiple Reproducing Kernel Hilbert Spaces (RKHSs) such that (a) the chosen functions make similar predictions on unlabeled examples, and (b) the average prediction given by the chosen functions performs well on labeled examples. In this paper, we construct a single RKHS with a data-dependent "co-regularization" norm that reduces these approaches to standard supervised learning. The reproducing kernel for this RKHS can be explicitly derived and plugged into any kernel method, greatly extending the theoretical and algorithmic scope of coregularization. In particular, with this development, the Rademacher complexity bound for co-regularization given in (Rosenberg & Bartlett, 2007) follows easily from wellknown results. Furthermore, more refined bounds given by localized Rademacher complexity can also be easily applied. We propose a co-regularization based algorithmic alternative to manifold regularization (Belkin et al., 2006; Sindhwani et al., 2005a) that leads to major empirical improvements on semi-supervised tasks. Unlike the recently proposed transductive approach of (Yu et al., 2008), our RKHS formulation is truly semi-supervised and naturally extends to unseen test data.

#index 1073995
#* The asymptotics of semi-supervised learning in discriminative probabilistic models
#@ Nataliya Sokolovska;Olivier Cappé;François Yvon
#t 2008
#c 19
#% 311027
#% 464434
#% 883830
#% 938713
#% 939527
#% 983878
#% 1455666
#% 1672995
#! Semi-supervised learning aims at taking advantage of unlabeled data to improve the efficiency of supervised learning procedures. For discriminative models however, this is a challenging task. In this contribution, we introduce an original methodology for using unlabeled data through the design of a simple semi-supervised objective function. We prove that the corresponding semi-supervised estimator is asymptotically optimal. The practical consequences of this result are discussed for the case of the logistic regression model.

#index 1073996
#* Tailoring density estimation via reproducing kernel moment matching
#@ Le Song;Xinhua Zhang;Alex Smola;Arthur Gretton;Bernhard Schölkopf
#t 2008
#c 19
#% 280402
#% 325683
#% 633253
#% 722798
#% 722909
#% 771841
#% 778078
#% 1674771
#% 1828389
#! Moment matching is a popular means of parametric density estimation. We extend this technique to nonparametric estimation of mixture models. Our approach works by embedding distributions into a reproducing kernel Hilbert space, and performing moment matching in that space. This allows us to tailor density estimators to a function class of interest (i.e., for which we would like to compute expectations). We show our density estimation approach is useful in applications such as message compression in graphical models, and image classification and retrieval.

#index 1073997
#* Detecting statistical interactions with additive groves of trees
#@ Daria Sorokina;Rich Caruana;Mirek Riedewald;Daniel Fink
#t 2008
#c 19
#% 33182
#% 458360
#% 722929
#% 769939
#% 770799
#% 881571
#! Discovering additive structure is an important step towards understanding a complex multi-dimensional function because it allows the function to be expressed as the sum of lower-dimensional components. When variables interact, however, their effects are not additive and must be modeled and interpreted simultaneously. We present a new approach for the problem of interaction detection. Our method is based on comparing the performance of unrestricted and restricted prediction models, where restricted models are prevented from modeling an interaction in question. We show that an additive model-based regression ensemble, Additive Groves, can be restricted appropriately for use with this framework, and thus has the right properties for accurately detecting variable interactions.

#index 1073998
#* Metric embedding for kernel classification rules
#@ Bharath K. Sriperumbudur;Omer A. Lang;Gert R. G. Lanckriet
#t 2008
#c 19
#% 299012
#% 763720
#% 770798
#! In this paper, we consider a smoothing kernel based classification rule and propose an algorithm for optimizing the performance of the rule by learning the bandwidth of the smoothing kernel along with a data-dependent distance metric. The data-dependent distance metric is obtained by learning a function that embeds an arbitrary metric space into a Euclidean space while minimizing an upper bound on the resubstitution estimate of the error probability of the kernel classification rule. By restricting this embedding function to a reproducing kernel Hilbert space, we reduce the problem to solving a semidefinite program and show the resulting kernel classification rule to be a variation of the k-nearest neighbor rule. We compare the performance of the kernel rule (using the learned data-dependent distance metric) to state-of-the-art distance metric learning algorithms (designed for k-nearest neighbor classification) on some benchmark datasets. The results show that the proposed rule has either better or as good classification accuracy as the other metric learning algorithms.

#index 1073999
#* Discriminative parameter learning for Bayesian networks
#@ Jiang Su;Harry Zhang;Charles X. Ling;Stan Matwin
#t 2008
#c 19
#% 44876
#% 197387
#% 246832
#% 290482
#% 578681
#% 770761
#% 1269497
#! Bayesian network classifiers have been widely used for classification problems. Given a fixed Bayesian network structure, parameters learning can take two different approaches: generative and discriminative learning. While generative parameter learning is more efficient, discriminative parameter learning is more effective. In this paper, we propose a simple, efficient, and effective discriminative parameter learning method, called Discriminative Frequency Estimate (DFE), which learns parameters by discriminatively computing frequencies from data. Empirical studies show that the DFE algorithm integrates the advantages of both generative and discriminative learning: it performs as well as the state-of-the-art discriminative parameter learning method ELR in accuracy, but is significantly more efficient.

#index 1074000
#* A least squares formulation for canonical correlation analysis
#@ Liang Sun;Shuiwang Ji;Jieping Ye
#t 2008
#c 19
#% 302906
#% 393059
#% 722887
#% 729437
#% 855563
#% 891559
#% 902459
#% 961218
#% 983908
#% 983940
#! Canonical Correlation Analysis (CCA) is a well-known technique for finding the correlations between two sets of multi-dimensional variables. It projects both sets of variables into a lower-dimensional space in which they are maximally correlated. CCA is commonly applied for supervised dimensionality reduction, in which one of the multi-dimensional variables is derived from the class label. It has been shown that CCA can be formulated as a least squares problem in the binaryclass case. However, their relationship in the more general setting remains unclear. In this paper, we show that, under a mild condition which tends to hold for high-dimensional data, CCA in multi-label classifications can be formulated as a least squares problem. Based on this equivalence relationship, we propose several CCA extensions including sparse CCA using 1-norm regularization. Experiments on multi-label data sets confirm the established equivalence relationship. Results also demonstrate the effectiveness of the proposed CCA extensions.

#index 1074001
#* Apprenticeship learning using linear programming
#@ Umar Syed;Michael Bowling;Robert E. Schapire
#t 2008
#c 19
#% 3084
#% 137425
#% 363744
#% 770852
#% 876036
#! In apprenticeship learning, the goal is to learn a policy in a Markov decision process that is at least as good as a policy demonstrated by an expert. The difficulty arises in that the MDP's true reward function is assumed to be unknown. We show how to frame apprenticeship learning as a linear programming problem, and show that using an off-the-shelf LP solver to solve this problem results in a substantial improvement in running time over existing methods---up to two orders of magnitude faster in our experiments. Additionally, our approach produces stationary policies, while all existing methods for apprenticeship learning output policies that are "mixed", i.e. randomized combinations of stationary policies. The technique used is general enough to convert any mixed policy to a stationary policy.

#index 1074002
#* Composite kernel learning
#@ Marie Szafranski;Yves Grandvalet;Alain Rakotomamonjy
#t 2008
#c 19
#% 263850
#% 304818
#% 393059
#% 425040
#% 722929
#% 757953
#% 763697
#% 770846
#% 961190
#% 983901
#% 993910
#% 1677750
#! The Support Vector Machine (SVM) is an acknowledged powerful tool for building classifiers, but it lacks flexibility, in the sense that the kernel is chosen prior to learning. Multiple Kernel Learning (MKL) enables to learn the kernel, from an ensemble of basis kernels, whose combination is optimized in the learning process. Here, we propose Composite Kernel Learning to address the situation where distinct components give rise to a group structure among kernels. Our formulation of the learning problem encompasses several setups, putting more or less emphasis on the group structure. We characterize the convexity of the learning problem, and provide a general wrapper algorithm for computing solutions. Finally, we illustrate the behavior of our method on multi-channel data where groups correpond to channels.

#index 1074003
#* The many faces of optimism: a unifying approach
#@ István Szita;András Lőrincz
#t 2008
#c 19
#% 270016
#% 284108
#% 307102
#% 384911
#% 425075
#% 466075
#% 466731
#% 713108
#% 840942
#% 1133454
#% 1289278
#! The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. "Optimism in the face of uncertainty" and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.

#index 1074004
#* ν-support vector machine as conditional value-at-risk minimization
#@ Akiko Takeda;Masashi Sugiyama
#t 2008
#c 19
#% 116149
#% 190581
#% 197394
#% 855583
#% 872759
#! The ν-support vector classification (ν-SVC) algorithm was shown to work well and provide intuitive interpretations, e.g., the parameter ν roughly specifies the fraction of support vectors. Although ν corresponds to a fraction, it cannot take the entire range between 0 and 1 in its original form. This problem was settled by a non-convex extension of ν-SVC and the extended method was experimentally shown to generalize better than original ν-SVC. However, its good generalization performance and convergence properties of the optimization algorithm have not been studied yet. In this paper, we provide new theoretical insights into these issues and propose a novel ν-SVC algorithm that has guaranteed generalization performance and convergence properties.

#index 1074005
#* Training restricted Boltzmann machines using approximations to the likelihood gradient
#@ Tijmen Tieleman
#t 2008
#c 19
#% 130878
#% 450888
#% 492962
#% 784864
#% 875987
#% 891060
#% 983863
#% 983903
#% 1073981
#! A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple.

#index 1074006
#* A semiparametric statistical approach to model-free policy evaluation
#@ Tsuyoshi Ueno;Motoaki Kawanabe;Takeshi Mori;Shin-ichi Maeda;Shin Ishii
#t 2008
#c 19
#% 1435
#% 3084
#% 203596
#% 384911
#% 393786
#% 734920
#% 793249
#% 951076
#% 1699598
#! Reinforcement learning (RL) methods based on least-squares temporal difference (LSTD) have been developed recently and have shown good practical performance. However, the quality of their estimation has not been well elucidated. In this article, we discuss LSTD-based policy evaluation from the new view-point of semiparametric statistical inference. In fact, the estimator can be obtained from a particular estimating function which guarantees its convergence to the true value asymptotically, without specifying a model of the environment. Based on these observations, we 1) analyze the asymptotic variance of an LSTD-based estimator, 2) derive the optimal estimating function with the minimum asymptotic estimation variance, and 3) derive a suboptimal estimator to reduce the computational burden in obtaining the optimal estimating function.

#index 1074007
#* Topologically-constrained latent variable models
#@ Raquel Urtasun;David J. Fleet;Andreas Geiger;Jovan Popović;Trevor J. Darrell;Neil D. Lawrence
#t 2008
#c 19
#% 770767
#% 771053
#% 812356
#% 876009
#% 883872
#% 891549
#% 916787
#% 1034759
#% 1083027
#% 1502487
#! In dimensionality reduction approaches, the data are typically embedded in a Euclidean latent space. However for some data sets this is inappropriate. For example, in human motion data we expect latent spaces that are cylindrical or a toroidal, that are poorly captured with a Euclidean space. In this paper, we present a range of approaches for embedding data in a non-Euclidean latent space. Our focus is the Gaussian Process latent variable model. In the context of human motion modeling this allows us to (a) learn models with interpretable latent directions enabling, for example, style/content separation, and (b) generalise beyond the data set enabling us to learn transitions between motion styles even though such transitions are not present in the data.

#index 1074008
#* Beam sampling for the infinite hidden Markov model
#@ Jurgen Van Gael;Yunus Saatci;Yee Whye Teh;Zoubin Ghahramani
#t 2008
#c 19
#% 891549
#! The infinite hidden Markov model is a non-parametric extension of the widely used hidden Markov model. Our paper introduces a new inference algorithm for the infinite Hidden Markov model called beam sampling. Beam sampling combines slice sampling, which limits the number of states considered at each time step to a finite number, with dynamic programming, which samples whole state trajectories efficiently. Our algorithm typically outperforms the Gibbs sampler and is more robust. We present applications of iHMM inference using the beam sampler on changepoint detection and text prediction problems.

#index 1074009
#* Extracting and composing robust features with denoising autoencoders
#@ Pascal Vincent;Hugo Larochelle;Yoshua Bengio;Pierre-Antoine Manzagol
#t 2008
#c 19
#% 65443
#% 92190
#% 190433
#% 451937
#% 812582
#% 891060
#% 983863
#% 1527440
#% 1856288
#! Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.

#index 1074010
#* Prediction with expert advice for the Brier game
#@ Vladimir Vovk;Fedor Zhdanov
#t 2008
#c 19
#% 81507
#% 165663
#% 232319
#% 235377
#% 262903
#% 871302
#% 1705513
#! We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee turns out to be rather tight on these data sets, especially in the case of the more extensive tennis data.

#index 1074011
#* Sparse multiscale gaussian process regression
#@ Christian Walder;Kwang In Kim;Bernhard Schölkopf
#t 2008
#c 19
#% 450245
#% 722798
#% 916792
#! Most existing sparse Gaussian process (g.p.) models seek computational advantages by basing their computations on a set of m basis functions that are the covariance function of the g.p. with one of its two inputs fixed. We generalise this for the case of Gaussian covariance function, by basing our computations on m Gaussian basis functions with arbitrary diagonal covariance matrices (or length scales). For a fixed number of basis functions and any given criteria, this additional flexibility permits approximations no worse and typically better than was previously possible. We perform gradient based optimisation of the marginal likelihood, which costs O(m2n) time where n is the number of data points, and compare the method to various other sparse g.p. methods. Although we focus on g.p. regression, the central idea is applicable to all kernel based algorithms, and we also provide some results for the support vector machine (s.v.m.) and kernel ridge regression (k.r.r.). Our approach outperforms the other methods, particularly for the case of very few basis functions, i. e. a very high sparsity ratio.

#index 1074012
#* Manifold alignment using Procrustes analysis
#@ Chang Wang;Sridhar Mahadevan
#t 2008
#c 19
#% 593047
#% 840904
#% 902496
#% 1275191
#! In this paper we introduce a novel approach to manifold alignment, based on Procrustes analysis. Our approach differs from "semi-supervised alignment" in that it results in a mapping that is defined everywhere - when used with a suitable dimensionality reduction method - rather than just on the training data points. We describe and evaluate our approach both theoretically and experimentally, providing results showing useful knowledge transfer from one domain to another. Novel applications of our method including cross-lingual information retrieval and transfer learning in Markov decision processes are presented.

#index 1074013
#* Dirichlet component analysis: feature extraction for compositional data
#@ Hua-Yan Wang;Qiang Yang;Hong Qin;Hongbin Zha
#t 2008
#c 19
#% 266426
#% 449570
#% 722904
#% 983924
#% 1673021
#! We consider feature extraction (dimensionality reduction) for compositional data, where the data vectors are constrained to be positive and constant-sum. In real-world problems, the data components (variables) usually have complicated "correlations" while their total number is huge. Such scenario demands feature extraction. That is, we shall de-correlate the components and reduce their dimensionality. Traditional techniques such as the Principle Component Analysis (PCA) are not suitable for these problems due to unique statistical properties and the need to satisfy the constraints in compositional data. This paper presents a novel approach to feature extraction for compositional data. Our method first identifies a family of dimensionality reduction projections that preserve all relevant constraints, and then finds the optimal projection that maximizes the estimated Dirichlet precision on projected data. It reduces the compositional data to a given lower dimensionality while the components in the lower-dimensional space are de-correlated as much as possible. We develop theoretical foundation of our approach, and validate its effectiveness on some synthetic and real-world datasets.

#index 1074014
#* Adaptive p-posterior mixture-model kernels for multiple instance learning
#@ Hua-Yan Wang;Qiang Yang;Hongbin Zha
#t 2008
#c 19
#% 169777
#% 224755
#% 393059
#% 464621
#% 464633
#% 565537
#% 875995
#% 983817
#% 983899
#% 983924
#% 983950
#% 1274898
#% 1673021
#! In multiple instance learning (MIL), how the instances determine the bag-labels is an essential issue, both algorithmically and intrinsically. In this paper, we show that the mechanism of how the instances determine the bag-labels is different for different application domains, and does not necessarily obey the traditional assumptions of MIL. We therefore propose an adaptive framework for MIL that adapts to different application domains by learning the domain-specific mechanisms merely from labeled bags. Our approach is especially attractive when we are encountered with novel application domains, for which the mechanisms may be different and unknown. Specifically, we exploit mixture models to represent the composition of each bag and an adaptable kernel function to represent the relationship between the bags. We validate on synthetic MIL datasets that the kernel function automatically adapts to different mechanisms of how the instances determine the bag-labels. We also compare our approach with state-of-the-art MIL techniques on real-world benchmark datasets.

#index 1074015
#* Graph transduction via alternating minimization
#@ Jun Wang;Tony Jebara;Shih-Fu Chang
#t 2008
#c 19
#% 205305
#% 466263
#% 565545
#% 840938
#% 961218
#% 983878

#index 1074016
#* On multi-view active learning and the combination with semi-supervised learning
#@ Wei Wang;Zhi-Hua Zhou
#t 2008
#c 19
#% 170649
#% 236729
#% 252011
#% 266396
#% 311027
#% 387653
#% 466888
#% 529191
#% 722797
#% 770807
#% 875953
#% 879447
#% 926881
#% 1272126
#% 1289496
#% 1396658
#% 1705517
#! Multi-view learning has become a hot topic during the past few years. In this paper, we first characterize the sample complexity of multi-view active learning. Under the α-expansion assumption, we get an exponential improvement in the sample complexity from usual Õ(1/ε) to Õ(log 1/ε), requiring neither strong assumption on data distribution such as the data is distributed uniformly over the unit sphere in Rd nor strong assumption on hypothesis class such as linear separators through the origin. We also give an upper bound of the error rate when the α-expansion assumption does not hold. Then, we analyze the combination of multi-view active learning and semi-supervised learning and get a further improvement in the sample complexity. Finally, we study the empirical behavior of the two paradigms, which verifies that the combination of multi-view active learning and semi-supervised learning is efficient.

#index 1074017
#* Fast solvers and efficient implementations for distance metric learning
#@ Kilian Q. Weinberger;Lawrence K. Saul
#t 2008
#c 19
#% 209623
#% 393059
#% 476717
#% 757953
#% 812372
#% 983830
#! In this paper we study how to improve nearest neighbor classification by learning a Mahalanobis distance metric. We build on a recently proposed framework for distance metric learning known as large margin nearest neighbor (LMNN) classification. Our paper makes three contributions. First, we describe a highly efficient solver for the particular instance of semidefinite programming that arises in LMNN classification; our solver can handle problems with billions of large margin constraints in a few hours. Second, we show how to reduce both training and testing times using metric ball trees; the speedups from ball trees are further magnified by learning low dimensional representations of the input space. Third, we show how to learn different Mahalanobis distance metrics in different parts of the input space. For large data sets, the use of locally adaptive distance metrics leads to even lower error rates.

#index 1074018
#* Deep learning via semi-supervised embedding
#@ Jason Weston;Frédéric Ratle;Ronan Collobert
#t 2008
#c 19
#% 236497
#% 593047
#% 840938
#% 857292
#% 884076
#% 891060
#% 916788
#% 929717
#% 961195
#% 961218
#% 1148255
#! We show how nonlinear embedding algorithms popular for use with shallow semi-supervised learning techniques such as kernel methods can be applied to deep multilayer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.

#index 1074019
#* Efficiently learning linear-linear exponential family predictive representations of state
#@ David Wingate;Satinder Singh
#t 2008
#c 19
#% 136358
#% 1093835
#% 1699598
#! Exponential Family PSR (EFPSR) models capture stochastic dynamical systems by representing state as the parameters of an exponential family distribution over a shortterm window of future observations. They are appealing from a learning perspective because they are fully observed (meaning expressions for maximum likelihood do not involve hidden quantities), but are still expressive enough to both capture existing models and predict new models. While maximum-likelihood learning algorithms for EFPSRs exist, they are not computationally feasible. We present a new, computationally efficient, learning algorithm based on an approximate likelihood function. The algorithm can be interpreted as attempting to induce stationary distributions of observations, features and states which match their empirically observed counterparts. The approximate likelihood, and the idea of matching stationary distributions, may apply to other models.

#index 1074020
#* Fully distributed EM for very large datasets
#@ Jason Wolfe;Aria Haghighi;Dan Klein
#t 2008
#c 19
#% 370075
#% 722904
#% 740915
#% 763708
#% 788088
#% 963669
#% 1756872
#! In EM and related algorithms, E-step computations distribute easily, because data items are independent given parameters. For very large data sets, however, even storing all of the parameters in a single node for the M-step can be impractical. We present a framework that fully distributes the entire EM procedure. Each node interacts only with parameters relevant to its data, sending messages to other nodes along a junction-tree topology. We demonstrate improvements over a MapReduce topology, on two tasks: word alignment and topic modeling.

#index 1074021
#* Listwise approach to learning to rank: theory and algorithm
#@ Fen Xia;Tie-Yan Liu;Jue Wang;Wensheng Zhang;Hang Li
#t 2008
#c 19
#% 309095
#% 387427
#% 420142
#% 564279
#% 757953
#% 766414
#% 793240
#% 840846
#% 983820
#% 987226
#% 987241
#% 1039843
#% 1674802
#! This paper aims to conduct a study on the listwise approach to learning to rank. The listwise approach learns a ranking function by taking individual lists as instances and minimizing a loss function defined on the predicted list and the ground-truth list. Existing work on the approach mainly focused on the development of new algorithms; methods such as RankCosine and ListNet have been proposed and good performances by them have been observed. Unfortunately, the underlying theory was not sufficiently studied so far. To amend the problem, this paper proposes conducting theoretical analysis of learning to rank algorithms through investigations on the properties of the loss functions, including consistency, soundness, continuity, differentiability, convexity, and efficiency. A sufficient condition on consistency for ranking is given, which seems to be the first such result obtained in related research. The paper then conducts analysis on three loss functions: likelihood loss, cosine loss, and cross entropy loss. The latter two were used in RankCosine and ListNet. The use of the likelihood loss leads to the development of a new listwise method called ListMLE, whose loss function offers better properties, and also leads to better experimental results.

#index 1074022
#* Democratic approximation of lexicographic preference models
#@ Fusun Yaman;Thomas J. Walsh;Michael L. Littman;Marie desJardins
#t 2008
#c 19
#% 382680
#% 449559
#% 961136
#! Previous algorithms for learning lexicographic preference models (LPMs) produce a "best guess" LPM that is consistent with the observations. Our approach is more democratic: we do not commit to a single LPM. Instead, we approximate the target using the votes of a collection of consistent LPMs. We present two variations of this method---variable voting and model voting---and empirically show that these democratic algorithms outperform the existing methods. We also introduce an intuitive yet powerful learning bias to prune some of the possible LPMs. We demonstrate how this learning bias can be used with variable and model voting and show that the learning bias improves the learning curve significantly, especially when the number of observations is small.

#index 1074023
#* Preconditioned temporal difference learning
#@ Hengshuai Yao;Zhi-Qiang Liu
#t 2008
#c 19
#% 203596
#% 331911
#% 384911
#% 431472
#% 449561
#% 466235
#% 616105
#% 647197
#% 1250563
#% 1271971
#! This paper extends many of the recent popular policy evaluation algorithms to a generalized framework that includes least-squares temporal difference (LSTD) learning, least-squares policy evaluation (LSPE) and a variant of incremental LSTD (iLSTD). The basis of this extension is a preconditioning technique that solves a stochastic model equation. This paper also studies three significant issues of the new framework: it presents a new rule of step-size that can be computed online, provides an iterative way to apply preconditioning, and reduces the complexity of related algorithms to near that of temporal difference (TD) learning.

#index 1074024
#* A quasi-Newton approach to non-smooth convex optimization
#@ Jin Yu;S. V. N. Vishwanathan;Simon Günter;Nicol N. Schraudolph
#t 2008
#c 19
#% 298699
#% 881477
#% 983808
#% 989644
#! We extend the well-known BFGS quasi-Newton method and its limited-memory variant LBFGS to the optimization of non-smooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: The local quadratic model, the identification of a descent direction, and the Wolfe line search conditions. We apply the resulting subLBFGS algorithm to L2-regularized risk minimization with binary hinge loss, and its direction-finding component to L1-regularized risk minimization with logistic loss. In both settings our generic algorithms perform comparable to or better than their counterparts in specialized state-of-the-art solvers.

#index 1074025
#* Predicting diverse subsets using structural SVMs
#@ Yisong Yue;Thorsten Joachims
#t 2008
#c 19
#% 46803
#% 262112
#% 297675
#% 642975
#% 783478
#% 818266
#% 829043
#% 875979
#% 879618
#% 987221
#% 987226
#% 1073910
#% 1073970
#% 1264133
#! In many retrieval tasks, one important goal involves retrieving a diverse set of results (e.g., documents covering a wide range of topics for a search query). First of all, this reduces redundancy, effectively showing more information with the presented results. Secondly, queries are often ambiguous at some level. For example, the query "Jaguar" can refer to many different topics (such as the car or feline). A set of documents with high topic diversity ensures that fewer users abandon the query because no results are relevant to them. Unlike existing approaches to learning retrieval functions, we present a method that explicitly trains to diversify results. In particular, we formulate the learning problem of predicting diverse subsets and derive a training method based on structural SVMs.

#index 1074026
#* Improved Nyström low-rank approximation and error analysis
#@ Kai Zhang;Ivor W. Tsang;James T. Kwok
#t 2008
#c 19
#% 114667
#% 266426
#% 292664
#% 338442
#% 443984
#% 466597
#% 564285
#% 593842
#% 722815
#% 722887
#% 732552
#% 840839
#% 876082
#% 916799
#% 1386291
#! Low-rank matrix approximation is an effective tool in alleviating the memory and computational burdens of kernel methods and sampling, as the mainstream of such algorithms, has drawn considerable attention in both theory and practice. This paper presents detailed studies on the Nyström sampling scheme and in particular, an error analysis that directly relates the Nyström approximation quality with the encoding powers of the landmark points in summarizing the data. The resultant error bound suggests a simple and efficient sampling scheme, the k-means clustering algorithm, for Nyström low-rank approximation. We compare it with state-of-the-art approaches that range from greedy schemes to probabilistic sampling. Our algorithm achieves significant performance gains in a number of supervised/unsupervised learning tasks including kernel PCA and least squares SVM.

#index 1074027
#* Estimating local optimums in EM algorithm over Gaussian mixture model
#@ Zhenjie Zhang;Bing Tian Dai;Anthony K. H. Tung
#t 2008
#c 19
#% 213010
#% 857416
#% 915359
#% 1051482
#! EM algorithm is a very popular iteration-based method to estimate the parameters of Gaussian Mixture Model from a large observation set. However, in most cases, EM algorithm is not guaranteed to converge to the global optimum. Instead, it stops at some local optimums, which can be much worse than the global optimum. Therefore, it is usually required to run multiple procedures of EM algorithm with different initial configurations and return the best solution. To improve the efficiency of this scheme, we propose a new method which can estimate an upper bound on the logarithm likelihood of the local optimum, based on the current configuration after the latest EM iteration. This is accomplished by first deriving some region bounding the possible locations of local optimum, followed by some upper bound estimation on the maximum likelihood. With this estimation, we can terminate an EM algorithm procedure if the estimated local optimum is definitely worse than the best solution seen so far. Extensive experiments show that our method can effectively and efficiently accelerate conventional multiple restart EM algorithm.

#index 1074028
#* Efficient multiclass maximum margin clustering
#@ Bin Zhao;Fei Wang;Changshui Zhang
#t 2008
#c 19
#% 269226
#% 313959
#% 420495
#% 466675
#% 722816
#% 729437
#% 763708
#% 829043
#% 881477
#% 983944
#% 1074028
#% 1269502
#! This paper presents a cutting plane algorithm for multiclass maximum margin clustering (MMC). The proposed algorithm constructs a nested sequence of successively tighter relaxations of the original MMC problem, and each optimization problem in this sequence could be efficiently solved using the constrained concave-convex procedure (CCCP). Experimental evaluations on several real world datasets show that our algorithm converges much faster than existing MMC methods with guaranteed accuracy, and can thus handle much larger datasets efficiently.

#index 1074029
#* Laplace maximum margin Markov networks
#@ Jun Zhu;Eric P. Xing;Bo Zhang
#t 2008
#c 19
#% 464279
#% 464434
#% 721164
#% 770763
#% 957325
#% 983808
#% 983822
#% 983842
#% 1014647
#! We propose Laplace max-margin Markov networks (LapM3N), and a general class of Bayesian M3N (BM3N) of which the LapM3N is a special case with sparse structural bias, for robust structured prediction. BM3N generalizes extant structured prediction rules based on point estimator to a Bayes-predictor using a learnt distribution of rules. We present a novel Structured Maximum Entropy Discrimination (SMED) formalism for combining Bayesian and max-margin learning of Markov networks for structured prediction, and our approach subsumes the conventional M3N as a special case. An efficient learning algorithm based on variational inference and standard convex-optimization solvers for M3N, and a generalization bound are offered. Our method outperforms competing ones on both synthetic and real OCR data.

#index 1211689
#* Proceedings of the 26th Annual International Conference on Machine Learning
#@ Andrea Danyluk;Léon Bottou;Michael Littman
#t 2009
#c 19

#index 1211690
#* Archipelago: nonparametric Bayesian semi-supervised learning
#@ Ryan Prescott Adams;Zoubin Ghahramani
#t 2009
#c 19
#% 268069
#% 889295
#% 891549
#% 916792
#% 1274924
#! Semi-supervised learning (SSL), is classification where additional unlabeled data can be used to improve accuracy. Generative approaches are appealing in this situation, as a model of the data's probability density can assist in identifying clusters. Nonparametric Bayesian methods, while ideal in theory due to their principled motivations, have been difficult to apply to SSL in practice. We present a nonparametric Bayesian method that uses Gaussian processes for the generative model, avoiding many of the problems associated with Dirichlet process mixture models. Our model is fully generative and we take advantage of recent advances in Markov chain Monte Carlo algorithms to provide a practical inference method. Our method compares favorably to competing approaches on synthetic and real-world multi-class data.

#index 1211691
#* Tractable nonparametric Bayesian inference in Poisson processes with Gaussian process intensities
#@ Ryan Prescott Adams;Iain Murray;David J. C. MacKay
#t 2009
#c 19
#% 891549
#% 1073896
#! The inhomogeneous Poisson process is a point process that has varying intensity across its domain (usually time or space). For nonparametric Bayesian modeling, the Gaussian process is a useful way to place a prior distribution on this intensity. The combination of a Poisson process and GP is known as a Gaussian Cox process, or doubly-stochastic Poisson process. Likelihood-based inference in these models requires an intractable integral over an infinite-dimensional random function. In this paper we present the first approach to Gaussian Cox processes in which it is possible to perform inference without introducing approximations or finitedimensional proxy distributions. We call our method the Sigmoidal Gaussian Cox Process, which uses a generative model for Poisson data to enable tractable inference via Markov chain Monte Carlo. We compare our methods to competing methods on synthetic data and apply it to several real-world data sets.

#index 1211692
#* Route kernels for trees
#@ Fabio Aiolli;Giovanni Da San Martino;Alessandro Sperduti
#t 2009
#c 19
#% 304917
#% 464640
#% 571903
#% 815896
#% 983661
#% 1019151
#% 1073990
#% 1665151
#! Almost all tree kernels proposed in the literature match substructures without taking into account their relative positioning with respect to one another. In this paper, we propose a novel family of kernels which explicitly focus on this type of information. Specifically, after defining a family of tree kernels based on routes between nodes, we present an efficient implementation for a member of this family. Experimental results on four different datasets show that our method is able to reach state of the art performances, obtaining in some cases performances better than computationally more demanding tree kernels.

#index 1211693
#* Incorporating domain knowledge into topic modeling via Dirichlet Forest priors
#@ David Andrzejewski;Xiaojin Zhu;Mark Craven
#t 2009
#c 19
#% 39186
#% 722904
#% 876017
#% 1085668
#% 1270678
#! Users of topic modeling methods often have knowledge about the composition of words that should have high or low probability in various topics. We incorporate such domain knowledge using a novel Dirichlet Forest prior in a Latent Dirichlet Allocation framework. The prior is a mixture of Dirichlet tree distributions with special structures. We present its construction, and inference via collapsed Gibbs sampling. Experiments on synthetic and real datasets demonstrate our model's ability to follow and generalize beyond user-specified domain knowledge.

#index 1211694
#* Grammatical inference as a principal component analysis problem
#@ Raphaël Bailly;François Denis;Liva Ralaivola
#t 2009
#c 19
#% 301800
#% 466593
#% 466716
#% 1137679
#% 1665132
#% 1674780
#! One of the main problems in probabilistic grammatical inference consists in inferring a stochastic language, i.e. a probability distribution, in some class of probabilistic models, from a sample of strings independently drawn according to a fixed unknown target distribution p. Here, we consider the class of rational stochastic languages composed of stochastic languages that can be computed by multiplicity automata, which can be viewed as a generalization of probabilistic automata. Rational stochastic languages p have a useful algebraic characterization: all the mappings up: v → p(uv) lie in a finite dimensional vector subspace Vp* of the vector space ℝ 〈〈Σ〉〉 composed of all real-valued functions defined over Σ*. Hence, a first step in the grammatical inference process can consist in identifying the subspace Vp*. In this paper, we study the possibility of using Principal Component Analysis to achieve this task. We provide an inference algorithm which computes an estimate of this space and then build a multiplicity automaton which computes an estimate of the target distribution. We prove some theoretical properties of this algorithm and we provide results from numerical simulations that confirm the relevance of our approach.

#index 1211695
#* Curriculum learning
#@ Yoshua Bengio;Jérôme Louradour;Ronan Collobert;Jason Weston
#t 2009
#c 19
#% 78641
#% 360718
#% 416849
#% 648739
#% 682451
#% 891060
#% 983863
#% 983903
#% 1073892
#% 1074009
#% 1074018
#% 1302905
#! Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).

#index 1211696
#* Importance weighted active learning
#@ Alina Beygelzimer;Sanjoy Dasgupta;John Langford
#t 2009
#c 19
#% 170649
#% 466095
#% 727925
#% 757953
#% 803574
#% 875953
#% 983848
#% 1073898
#% 1661927
#% 1705517
#! We present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions. Our algorithm uses importance weighting to correct sampling bias, and by controlling the variance, we are able to give rigorous label complexity bounds for the learning process.

#index 1211697
#* Split variational inference
#@ Guillaume Bouchard;Onno Zoeter
#t 2009
#c 19
#% 115608
#% 272505
#% 272514
#% 277470
#% 424845
#% 528330
#! We propose a deterministic method to evaluate the integral of a positive function based on soft-binning functions that smoothly cut the integral into smaller integrals that are easier to approximate. In combination with mean-field approximations for each individual sub-part this leads to a tractable algorithm that alternates between the optimization of the bins and the approximation of the local integrals. We introduce suitable choices for the binning functions such that a standard mean field approximation can be extended to a split mean field approximation without the need for extra derivations. The method can be seen as a revival of the ideas underlying the mixture mean field approach. The latter can be obtained as a special case by taking soft-max functions for the binning.

#index 1211698
#* Predictive representations for policy gradient in POMDPs
#@ Abdeslam Boularias;Brahim Chaib-draa
#t 2009
#c 19
#% 464448
#% 565539
#% 715178
#% 715676
#% 840956
#% 1073951
#% 1650314
#! We consider the problem of estimating the policy gradient in Partially Observable Markov Decision Processes (POMDPs) with a special class of policies that are based on Predictive State Representations (PSRs). We compare PSR policies to Finite-State Controllers (FSCs), which are considered as a standard model for policy gradient methods in POMDPs. We present a general Actor-Critic algorithm for learning both FSCs and PSR policies. The critic part computes a value function that has as variables the parameters of the policy. These latter parameters are gradually updated to maximize the value function. We show that the value function is polynomial for both FSCs and PSR policies, with a potentially smaller degree in the case of PSR policies. Therefore, the value function of a PSR policy can have less local optima than the equivalent FSC, and consequently, the gradient algorithm is more likely to converge to a global optimal solution.

#index 1211699
#* Online feature elicitation in interactive optimization
#@ Craig Boutilier;Kevin Regan;Paolo Viappiani
#t 2009
#c 19
#% 44625
#% 170649
#% 211440
#% 236729
#% 450951
#% 451056
#% 763719
#% 879183
#% 1784525
#! Most models of utility elicitation in decision support and interactive optimization assume a predefined set of "catalog" features over which user preferences are expressed. However, users may differ in the features over which they are most comfortable expressing their preferences. In this work we consider the problem of feature elicitation: a user's utility function is expressed using features whose definitions (in terms of "catalog" features) are unknown. We cast this as a problem of concept learning, but whose goal is to identify only enough about the concept to enable a good decision to be recommended. We describe computational procedures for identifying optimal alternatives w.r.t. minimax regret in the presence of concept uncertainty; and describe several heuristic query strategies that focus on reduction of relevant concept uncertainty.

#index 1211700
#* Spectral clustering based on the graph p-Laplacian
#@ Thomas Bühler;Matthias Hein
#t 2009
#c 19
#% 313959
#% 961185
#% 995140
#% 1014651
#% 1676331
#! We present a generalized version of spectral clustering using the graph p-Laplacian, a nonlinear generalization of the standard graph Laplacian. We show that the second eigenvector of the graph p-Laplacian interpolates between a relaxation of the normalized and the Cheeger cut. Moreover, we prove that in the limit as p → 1 the cut found by thresholding the second eigenvector of the graph p-Laplacian converges to the optimal Cheeger cut. Furthermore, we provide an efficient numerical scheme to compute the second eigenvector of the graph p-Laplacian. The experiments show that the clustering found by p-spectral clustering is at least as good as normal spectral clustering, but often leads to significantly better results.

#index 1211701
#* Active learning for directed exploration of complex systems
#@ Michael C. Burl;Esther Wang
#t 2009
#c 19
#% 132697
#% 170649
#% 190581
#% 408670
#% 722797
#% 735357
#% 763705
#% 770789
#% 770847
#% 840868
#% 876065
#% 1665154
#! Physics-based simulation codes are widely used in science and engineering to model complex systems that would be infeasible to study otherwise. Such codes provide the highest-fidelity representation of system behavior, but are often so slow to run that insight into the system is limited. For example, conducting an exhaustive sweep over a d-dimensional input parameter space with k-steps along each dimension requires kd simulation trials (translating into kd CPU-days for one of our current simulations). An alternative is directed exploration in which the next simulation trials are cleverly chosen at each step. Given the results of previous trials, supervised learning techniques (SVM, KDE, GP) are applied to build up simplified predictive models of system behavior. These models are then used within an active learning framework to identify the most valuable trials to run next. Several active learning strategies are examined including a recently-proposed information-theoretic approach. Performance is evaluated on a set of thirteen synthetic oracles, which serve as surrogates for the more expensive simulations and enable the experiments to be replicated by other researchers.

#index 1211702
#* Optimized expected information gain for nonlinear dynamical systems
#@ Alberto Giovanni Busetto;Cheng Soon Ong;Joachim M. Buhmann
#t 2009
#c 19
#% 757953
#% 927337
#! This paper addresses the problem of active model selection for nonlinear dynamical systems. We propose a novel learning approach that selects the most informative subset of time-dependent variables for the purpose of Bayesian model inference. The model selection criterion maximizes the expected Kullback-Leibler divergence between the prior and the posterior probabilities over the models. The proposed strategy generalizes the standard D-optimal design, which is obtained from a uniform prior with Gaussian noise. In addition, our approach allows us to determine an information halting criterion for model identification. We illustrate the benefits of our approach by differentiating between 18 published biochemical models of the TOR signaling pathway, a model selection problem in systems biology. By generating pivotal selection experiments, our strategy outperforms the standard Aoptimal, D-optimal and E-optimal sequential design techniques.

#index 1211703
#* Probabilistic dyadic data analysis with local and global consistency
#@ Deng Cai;Xuanhui Wang;Xiaofei He
#t 2009
#c 19
#% 280819
#% 304908
#% 313959
#% 317525
#% 329569
#% 458379
#% 643008
#% 722904
#% 788094
#% 835741
#% 840967
#% 876017
#% 961218
#% 1055681
#% 1130899
#! Dyadic data arises in many real world applications such as social network analysis and information retrieval. In order to discover the underlying or hidden structure in the dyadic data, many topic modeling techniques were proposed. The typical algorithms include Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA). The probability density functions obtained by both of these two algorithms are supported on the Euclidean space. However, many previous studies have shown naturally occurring data may reside on or close to an underlying submanifold. We introduce a probabilistic framework for modeling both the topical and geometrical structure of the dyadic data that explicitly takes into account the local manifold structure. Specifically, the local manifold structure is modeled by a graph. The graph Laplacian, analogous to the Laplace-Beltrami operator on manifolds, is applied to smooth the probability density functions. As a result, the obtained probabilistic distributions are concentrated around the data manifold. Experimental results on real data sets demonstrate the effectiveness of the proposed approach.

#index 1211704
#* Structure learning of Bayesian networks using constraints
#@ Cassio P. de Campos;Zhi Zeng;Qiang Ji
#t 2009
#c 19
#% 89748
#% 129987
#% 197387
#% 416706
#% 722900
#% 763715
#% 893460
#% 1672992
#! This paper addresses exact learning of Bayesian network structure from data and expert's knowledge based on score functions that are decomposable. First, it describes useful properties that strongly reduce the time and memory costs of many known methods such as hill-climbing, dynamic programming and sampling variable orderings. Secondly, a branch and bound algorithm is presented that integrates parameter and structural constraints with data in a way to guarantee global optimality with respect to the score function. It is an any-time procedure because, if stopped, it provides the best current solution and an estimation about how far it is from the global solution. We show empirically the advantages of the properties and the constraints, and the applicability of the algorithm to large data sets (up to one hundred variables) that cannot be handled by other current methods (limited to around 30 variables).

#index 1211705
#* Robust bounds for classification via selective sampling
#@ Nicolò Cesa-Bianchi;Claudio Gentile;Francesco Orabona
#t 2009
#c 19
#% 96688
#% 236729
#% 722906
#% 875953
#% 961135
#% 961177
#% 1000326
#% 1042610
#% 1073943
#% 1073962
#% 1396658
#% 1705517
#! We introduce a new algorithm for binary classification in the selective sampling protocol. Our algorithm uses Regularized Least Squares (RLS) as base classifier, and for this reason it can be efficiently run in any RKHS. Unlike previous margin-based semi-supervised algorithms, our sampling condition hinges on a simultaneous upper bound on bias and variance of the RLS estimate under a simple linear label noise model. This fact allows us to prove performance bounds that hold for an arbitrary sequence of instances. In particular, we show that our sampling strategy approximates the margin of the Bayes optimal classifier to any desired accuracy ε by asking Õ (d/ε2) queries (in the RKHS case d is replaced by a suitable spectral quantity). While these are the standard rates in the fully supervised i.i.d. case, the best previously known result in our harder setting was Õ (d3/ε4). Preliminary experiments show that some of our algorithms also exhibit a good practical performance.

#index 1211706
#* Multi-view clustering via canonical correlation analysis
#@ Kamalika Chaudhuri;Sham M. Kakade;Karen Livescu;Karthik Sridharan
#t 2009
#c 19
#% 252011
#% 460760
#% 527854
#% 593926
#% 967461
#% 983807
#% 1141475
#% 1396661
#% 1705530
#% 1705531
#! Clustering data in high dimensions is believed to be a hard problem in general. A number of efficient clustering algorithms developed in recent years address this problem by projecting the data into a lower-dimensional subspace, e.g. via Principal Components Analysis (PCA) or random projections, before clustering. Here, we consider constructing such projections using multiple views of the data, via Canonical Correlation Analysis (CCA). Under the assumption that the views are un-correlated given the cluster label, we show that the separation conditions required for the algorithm to be successful are significantly weaker than prior results in the literature. We provide results for mixtures of Gaussians and mixtures of log concave distributions. We also provide empirical support from audio-visual speaker clustering (where we desire the clusters to correspond to speaker ID) and from hierarchical Wikipedia document clustering (where one view is the words in the document and the other is the link structure).

#index 1211707
#* A convex formulation for learning shared structures from multiple tasks
#@ Jianhui Chen;Lei Tang;Jun Liu;Jieping Ye
#t 2009
#c 19
#% 176588
#% 236497
#% 723239
#% 757953
#% 763708
#% 770804
#% 829014
#% 840962
#% 916788
#% 961246
#% 983806
#% 983942
#% 1128929
#% 1271814
#! Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. In this paper, we consider the problem of learning shared structures from multiple related tasks. We present an improved formulation (iASO) for multi-task learning based on the non-convex alternating structure optimization (ASO) algorithm, in which all tasks are related by a shared feature representation. We convert iASO, a non-convex formulation, into a relaxed convex one, which is, however, not scalable to large data sets due to its complex constraints. We propose an alternating optimization (cASO) algorithm which solves the convex relaxation efficiently, and further show that cASO converges to a global optimum. In addition, we present a theoretical condition, under which cASO can find a globally optimal solution to iASO. Experiments on several benchmark data sets confirm our theoretical analysis.

#index 1211708
#* Learning kernels from indefinite similarities
#@ Yihua Chen;Maya R. Gupta;Benjamin Recht
#t 2009
#c 19
#% 5182
#% 304899
#% 443948
#% 732531
#% 757953
#% 770865
#% 796230
#% 889311
#% 983821
#% 1073889
#% 1232033
#! Similarity measures in many real applications generate indefinite similarity matrices. In this paper, we consider the problem of classification based on such indefinite similarities. These indefinite kernels can be problematic for standard kernel-based algorithms as the optimization problems become non-convex and the underlying theory is invalidated. In order to adapt kernel methods for similarity-based learning, we introduce a method that aims to simultaneously find a reproducing kernel Hilbert space based on the given similarities and train a classifier with good generalization in that space. The method is formulated as a convex optimization problem. We propose a simplified version that can reduce overfitting and whose associated convex conic program can be solved efficiently. We compare the proposed simplified version with six other methods on a collection of real data sets.

#index 1211709
#* Matrix updates for perceptron training of continuous density hidden Markov models
#@ Chih-Chieh Cheng;Fei Sha;Lawrence K. Saul
#t 2009
#c 19
#% 302390
#% 722814
#% 854636
#% 1767700
#% 1767753
#! In this paper, we investigate a simple, mistake-driven learning algorithm for discriminative training of continuous density hidden Markov models (CD-HMMs). Most CD-HMMs for automatic speech recognition use multivariate Gaussian emission densities (or mixtures thereof) parameterized in terms of their means and covariance matrices. For discriminative training of CD-HMMs, we reparameterize these Gaussian distributions in terms of positive semidefinite matrices that jointly encode their mean and covariance statistics. We show how to explore the resulting parameter space in CDHMMs with perceptron-style updates that minimize the distance between Viterbi decodings and target transcriptions. We experiment with several forms of updates, systematically comparing the effects of different matrix factorizations, initializations, and averaging schemes on phone accuracies and convergence rates. We present experimental results for context-independent CD-HMMs trained in this way on the TIMIT speech corpus. Our results show that certain types of perceptron training yield consistently significant and rapid reductions in phone error rates.

#index 1211710
#* Decision tree and instance-based learning for label ranking
#@ Weiwei Cheng;Jens Hühn;Eyke Hüllermeier
#t 2009
#c 19
#% 92533
#% 136350
#% 156421
#% 847120
#% 858155
#% 961134
#% 961252
#% 1093383
#! The label ranking problem consists of learning a model that maps instances to total orders over a finite set of predefined labels. This paper introduces new methods for label ranking that complement and improve upon existing approaches. More specifically, we propose extensions of two methods that have been used extensively for classification and regression so far, namely instance-based learning and decision tree induction. The unifying element of the two methods is a procedure for locally estimating predictive probability models for label rankings.

#index 1211711
#* Learning dictionaries of stable autoregressive models for audio scene analysis
#@ Youngmin Cho;Lawrence K. Saul
#t 2009
#c 19
#% 274586
#% 917950
#% 1132481
#% 1185857
#% 1238191
#! In this paper, we explore an application of basis pursuit to audio scene analysis. The goal of our work is to detect when certain sounds are present in a mixed audio signal. We focus on the regime where out of a large number of possible sources, a small but unknown number combine and overlap to yield the observed signal. To infer which sounds are present, we decompose the observed signal as a linear combination of a small number of active sources. We cast the inference as a regularized form of linear regression whose sparse solutions yield decompositions with few active sources. We characterize the acoustic variability of individual sources by autoregressive models of their time domain waveforms. When we do not have prior knowledge of the individual sources, the coefficients of these autoregressive models must be learned from audio examples. We analyze the dynamical stability of these models and show how to estimate stable models by substituting a simple convex optimization for a difficult eigenvalue problem. We demonstrate our approach by learning dictionaries of musical notes and using these dictionaries to analyze polyphonic recordings of piano, cello, and violin.

#index 1211712
#* Exploiting sparse Markov and covariance structure in multiresolution models
#@ Myung Jin Choi;Venkat Chandrasekaran;Alan S. Willsky
#t 2009
#c 19
#% 31225
#% 875956
#% 1188704
#% 1760836
#! We consider Gaussian multiresolution (MR) models in which coarser, hidden variables serve to capture statistical dependencies among the finest scale variables. Tree-structured MR models have limited modeling capabilities, as variables at one scale are forced to be uncorrelated with each other conditioned on other scales. We propose a new class of Gaussian MR models that capture the residual correlations within each scale using sparse covariance structure. Our goal is to learn a tree-structured graphical model connecting variables across different scales, while at the same time learning sparse structure for the conditional covariance within each scale conditioned on other scales. This model leads to an efficient, new inference algorithm that is similar to multipole methods in computational physics.

#index 1211713
#* Nonparametric estimation of the precision-recall curve
#@ Stéphan Clémençon;Nicolas Vayatis
#t 2009
#c 19
#% 57485
#% 279755
#% 840902
#% 875974
#! The Precision-Recall (PR) curve is a widely used visual tool to evaluate the performance of scoring functions in regards to their capacities to discriminate between two populations. The purpose of this paper is to examine both theoretical and practical issues related to the statistical estimation of PR curves based on classification data. Consistency and asymptotic normality of the empirical counterpart of the PR curve in sup norm are rigorously established. Eventually, the issue of building confidence bands in the PR space is considered and a specific resampling procedure based on a smoothed and truncated version of the empirical distribution of the data is promoted. Arguments of theoretical and computational nature are presented to explain why such a bootstrap is preferable to a "naive" bootstrap in this setup.

#index 1211714
#* EigenTransfer: a unified framework for transfer learning
#@ Wenyuan Dai;Ou Jin;Gui-Rong Xue;Qiang Yang;Yong Yu
#t 2009
#c 19
#% 116149
#% 127850
#% 169777
#% 236497
#% 313959
#% 466263
#% 770858
#% 876034
#% 983899
#% 1272110
#! This paper proposes a general framework, called EigenTransfer, to tackle a variety of transfer learning problems, e.g. cross-domain learning, self-taught learning, etc. Our basic idea is to construct a graph to represent the target transfer learning task. By learning the spectra of a graph which represents a learning task, we obtain a set of eigenvectors that reflect the intrinsic structure of the task graph. These eigenvectors can be used as the new features which transfer the knowledge from auxiliary data to help classify target data. Given an arbitrary non-transfer learner (e.g. SVM) and a particular transfer learning task, EigenTransfer can produce a transfer learner accordingly for the target transfer learning task. We apply EigenTransfer on three different transfer learning tasks, cross-domain learning, cross-category learning and self-taught learning, to demonstrate its unifying ability, and show through experiments that EigenTransfer can greatly outperform several representative non-transfer learners.

#index 1211715
#* Fitting a graph to vector data
#@ Samuel I. Daitch;Jonathan A. Kelner;Daniel A. Spielman
#t 2009
#c 19
#% 136350
#% 593047
#% 656762
#% 765261
#% 799040
#% 840938
#% 876057
#% 1026602
#% 1074015
#! We introduce a measure of how well a combinatorial graph fits a collection of vectors. The optimal graphs under this measure may be computed by solving convex quadratic programs and have many interesting properties. For vectors in d dimensional space, the graphs always have average degree at most 2(d + 1), and for vectors in 2 dimensions they are always planar. We compute these graphs for many standard data sets and show that they can be used to obtain good solutions to classification, regression and clustering problems.

#index 1211716
#* Unsupervised search-based structured prediction
#@ Hal Daumé, III
#t 2009
#c 19
#% 95730
#% 464434
#% 527859
#% 740915
#% 740916
#% 757830
#% 812336
#% 840841
#% 840947
#% 938713
#% 983561
#% 1202000
#% 1271211
#% 1271213
#% 1299529
#! We describe an adaptation and application of a search-based structured prediction algorithm "Searn" to unsupervised learning problems. We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high-quality un-supervised shift-reduce parsing model. We additionally show a close connection between unsupervised Searn and expectation maximization. Finally, we demonstrate the efficacy of a semi-supervised extension. The key idea that enables this is an application of the predict-self idea for unsupervised learning.

#index 1211717
#* Deep transfer via second-order Markov logic
#@ Jesse Davis;Pedro Domingos
#t 2009
#c 19
#% 65345
#% 129969
#% 226495
#% 333797
#% 392781
#% 840890
#% 983858
#% 1269766
#% 1415862
#% 1650403
#% 1699582
#! Standard inductive learning requires that training and test instances come from the same distribution. Transfer learning seeks to remove this restriction. In shallow transfer, test instances are from the same domain, but have a different distribution. In deep transfer, test instances are from a different domain entirely (i.e., described by different predicates). Humans routinely perform deep transfer, but few learning systems, if any, are capable of it. In this paper we propose an approach based on a form of second-order Markov logic. Our algorithm discovers structural regularities in the source domain in the form of Markov logic formulas with predicate variables, and instantiates these formulas with predicates from the target domain. Using this approach, we have successfully transferred learned knowledge among molecular biology, social network and Web domains. The discovered patterns include broadly useful properties of predicates, like symmetry and transitivity, and relations among predicates, such as various forms of homophily.

#index 1211718
#* Analytic moment-based Gaussian process filtering
#@ Marc Peter Deisenroth;Marco F. Huber;Uwe D. Hanebeck
#t 2009
#c 19
#% 274216
#% 304898
#% 878156
#% 891549
#% 1650568
#! We propose an analytic moment-based filter for nonlinear stochastic dynamic systems modeled by Gaussian processes. Exact expressions for the expected value and the covariance matrix are provided for both the prediction step and the filter step, where an additional Gaussian assumption is exploited in the latter case. Our filter does not require further approximations. In particular, it avoids finite-sample approximations. We compare the filter to a variety of Gaussian filters, that is, the EKF, the UKF, and the recent GP-UKF proposed by Ko et al. (2007).

#index 1211719
#* Good learners for evil teachers
#@ Ofer Dekel;Ohad Shamir
#t 2009
#c 19
#% 264164
#% 309208
#% 763708
#% 983905
#% 1039674
#% 1073899
#% 1083692
#% 1117687
#! We consider a supervised machine learning scenario where labels are provided by a heterogeneous set of teachers, some of which are mediocre, incompetent, or perhaps even malicious. We present an algorithm, built on the SVM framework, that explicitly attempts to cope with low-quality and malicious teachers by decreasing their influence on the learning process. Our algorithm does not receive any prior information on the teachers, nor does it resort to repeated labeling (where each example is labeled by multiple teachers). We provide a theoretical analysis of our algorithm and demonstrate its merits empirically. Finally, we present a second algorithm with promising empirical results but without a formal analysis.

#index 1211720
#* A scalable framework for discovering coherent co-clusters in noisy data
#@ Meghana Deodhar;Gunjan Gupta;Joydeep Ghosh;Hyuk Cho;Inderjit Dhillon
#t 2009
#c 19
#% 397382
#% 397632
#% 469422
#% 722934
#% 729918
#% 765518
#% 772864
#% 778215
#% 839669
#% 864476
#% 906367
#% 906512
#% 915269
#% 1014670
#% 1111931
#% 1206649
#! Clustering problems often involve datasets where only a part of the data is relevant to the problem, e.g., in microarray data analysis only a subset of the genes show cohesive expressions within a subset of the conditions/features. The existence of a large number of non-informative data points and features makes it challenging to hunt for coherent and meaningful clusters from such datasets. Additionally, since clusters could exist in different subspaces of the feature space, a co-clustering algorithm that simultaneously clusters objects and features is often more suitable as compared to one that is restricted to traditional "one-sided" clustering. We propose Robust Overlapping Co-Clustering (ROCC), a scalable and very versatile framework that addresses the problem of efficiently mining dense, arbitrarily positioned, possibly overlapping co-clusters from large, noisy datasets. ROCC has several desirable properties that make it extremely well suited to a number of real life applications.

#index 1211721
#* The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning
#@ Carlos Diuk;Lihong Li;Bethany R. Leffler
#t 2009
#c 19
#% 697
#% 75936
#% 131686
#% 164838
#% 232319
#% 363744
#% 425075
#% 495933
#% 722895
#% 763718
#% 876055
#% 961197
#% 1073943
#% 1269760
#% 1269772
#% 1272002
#% 1450170
#! The purpose of this paper is three-fold. First, we formalize and study a problem of learning probabilistic concepts in the recently proposed KWIK framework. We give details of an algorithm, known as the Adaptive k-Meteorologists Algorithm, analyze its sample-complexity upper bound, and give a matching lower bound. Second, this algorithm is used to create a new reinforcement-learning algorithm for factored-state problems that enjoys significant improvement over the previous state-of-the-art algorithm. Finally, we apply the Adaptive k-Meteorologists Algorithm to remove a limiting assumption in an existing reinforcement-learning algorithm. The effectiveness of our approaches is demonstrated empirically in a couple benchmark domains as well as a robotics navigation problem.

#index 1211722
#* Proximal regularization for online and batch learning
#@ Chuong B. Do;Quoc V. Le;Chuan-Sheng Foo
#t 2009
#c 19
#% 192645
#% 881477
#% 906274
#% 983905
#% 989644
#% 1000325
#! Many learning algorithms rely on the curvature (in particular, strong convexity) of regularized objective functions to provide good theoretical performance guarantees. In practice, the choice of regularization penalty that gives the best testing set performance may result in objective functions with little or even no curvature. In these cases, algorithms designed specifically for regularized objectives often either fail completely or require some modification that involves a substantial compromise in performance. We present new online and batch algorithms for training a variety of supervised learning models (such as SVMs, logistic regression, structured prediction models, and CRFs) under conditions where the optimal choice of regularization parameter results in functions with low curvature. We employ a technique called proximal regularization, in which we solve the original learning problem via a sequence of modified optimization tasks whose objectives are chosen to have greater curvature than the original problem. Theoretically, our algorithms achieve low regret bounds in the online setting and fast convergence in the batch setting. Experimentally, our algorithms improve upon state-of-the-art techniques, including Pegasos and bundle methods, on medium and large-scale SVM and structured learning tasks.

#index 1211723
#* Large margin training for hidden Markov models with partially observed states
#@ Trinh-Minh-Tri Do;Thierry Artières
#t 2009
#c 19
#% 770763
#% 881477
#% 989644
#% 1008104
#% 1025481
#! Large margin learning of Continuous Density HMMs with a partially labeled dataset has been extensively studied in the speech and handwriting recognition fields. Yet due to the non-convexity of the optimization problem, previous works usually rely on severe approximations so that it is still an open problem. We propose a new learning algorithm that relies on non-convex optimization and bundle methods and allows tackling the original optimization problem as is. It is proved to converge to a solution with accuracy ε with a rate O (1/ε). We provide experimental results gained on speech and handwriting recognition that demonstrate the potential of the method.

#index 1211724
#* Accelerated sampling for the Indian Buffet Process
#@ Finale Doshi-Velez;Zoubin Ghahramani
#t 2009
#c 19
#% 324288
#% 336073
#% 875990
#% 995371
#% 1124431
#% 1344738
#! We often seek to identify co-occurring hidden features in a set of observations. The Indian Buffet Process (IBP) provides a non-parametric prior on the features present in each observation, but current inference techniques for the IBP often scale poorly. The collapsed Gibbs sampler for the IBP has a running time cubic in the number of observations, and the uncollapsed Gibbs sampler, while linear, is often slow to mix. We present a new linear-time collapsed Gibbs sampler for conjugate likelihood models and demonstrate its efficacy on large real-world datasets.

#index 1211725
#* Accounting for burstiness in topic models
#@ Gabriel Doyle;Charles Elkan
#t 2009
#c 19
#% 251365
#% 722904
#% 812535
#% 840903
#% 875981
#% 876017
#! Many different topic models have been used successfully for a variety of applications. However, even state-of-the-art topic models suffer from the important flaw that they do not capture the tendency of words to appear in bursts; it is a fundamental property of language that if a word is used once in a document, it is more likely to be used again. We introduce a topic model that uses Dirichlet compound multinomial (DCM) distributions to model this burstiness phenomenon. On both text and non-text datasets, the new model achieves better held-out likelihood than standard latent Dirichlet allocation (LDA). It is straightforward to incorporate the DCM extension into topic models that are more complex than LDA.

#index 1211726
#* Domain adaptation from multiple sources via auxiliary classifiers
#@ Lixin Duan;Ivor W. Tsang;Dong Xu;Tat-Seng Chua
#t 2009
#c 19
#% 466263
#% 732385
#% 770858
#% 829014
#% 840938
#% 903632
#% 961218
#% 997093
#% 1117687
#% 1130817
#% 1261539
#! We propose a multiple source domain adaptation method, referred to as Domain Adaptation Machine (DAM), to learn a robust decision function (referred to as target classifier) for label prediction of patterns from the target domain by leveraging a set of pre-computed classifiers (referred to as auxiliary/source classifiers) independently learned with the labeled patterns from multiple source domains. We introduce a new data-dependent regularizer based on smoothness assumption into Least-Squares SVM (LS-SVM), which enforces that the target classifier shares similar decision values with the auxiliary classifiers from relevant source domains on the unlabeled patterns of the target domain. In addition, we employ a sparsity regularizer to learn a sparse target classifier. Comprehensive experiments on the challenging TRECVID 2005 corpus demonstrate that DAM outperforms the existing multiple source domain adaptation methods for video concept detection in terms of effectiveness and efficiency.

#index 1211727
#* Boosting with structural sparsity
#@ John Duchi;Yoram Singer
#t 2009
#c 19
#% 425065
#% 646003
#% 763708
#% 829018
#% 876070
#% 961223
#% 1014647
#% 1014657
#! We derive generalizations of AdaBoost and related gradient-based coordinate descent methods that incorporate sparsity-promoting penalties for the norm of the predictor that is being learned. The end result is a family of coordinate descent algorithms that integrate forward feature induction and back-pruning through regularization and give an automatic stopping criterion for feature induction. We study penalties based on the l1, l2, and l∞ norms of the predictor and introduce mixed-norm penalties that build upon the initial penalties. The mixed-norm regularizers facilitate structural sparsity in parameter space, which is a useful property in multiclass prediction and other related tasks. We report empirical results that demonstrate the power of our approach in building accurate and structurally sparse models.

#index 1211728
#* Learning to segment from a few well-selected training images
#@ Alireza Farhangfar;Russell Greiner;Csaba Szepesvári
#t 2009
#c 19
#% 197922
#% 236729
#% 457979
#% 464268
#% 466419
#% 466576
#% 722797
#% 724344
#% 770771
#% 951455
#% 1073903
#% 1149156
#% 1272282
#% 1274885
#% 1387560
#% 1775158
#! We address the task of actively learning a segmentation system: given a large number of unsegmented images, and access to an oracle that can segment a given image, decide which images to provide, to quickly produce a segmenter (here, a discriminative random field) that is accurate over this distribution of images. We extend the standard models for active learner to define a system for this task that first selects the image whose expected label will reduce the uncertainty of the other unlabeled images the most, and then after greedily selects, from the pool of unsegmented images, the most informative image. The results of our experiments, over two real-world datasets (segmenting brain tumors within magnetic resonance images; and segmenting the sky in real images) show that training on very few informative images (here, as few as 2) can produce a segmenter that is as good as training on the entire dataset.

#index 1211729
#* GAODE and HAODE: two proposals based on AODE to deal with continuous variables
#@ M. Julia Flores;José A. Gámez;Ana M. Martínez;José M. Puerta
#t 2009
#c 19
#% 304305
#% 321059
#% 424851
#% 799040
#% 919561
#% 926881
#% 961134
#% 1023380
#! AODE (Aggregating One-Dependence Estimators) is considered one of the most interesting representatives of the Bayesian classifiers, taking into account not only the low error rate it provides but also its efficiency. Until now, all the attributes in a dataset have had to be nominal to build an AODE classifier or they have had to be previously discretized. In this paper, we propose two different approaches in order to deal directly with numeric attributes. One of them uses conditional Gaussian networks to model a dataset exclusively with numeric attributes; and the other one keeps the superparent on each model discrete and uses univariate Gaussians to estimate the probabilities for the numeric attributes and multinomial distributions for the categorical ones, it also being able to model hybrid datasets. Both of them obtain competitive results compared to AODE, the latter in particular being a very attractive alternative to AODE in numeric datasets.

#index 1211730
#* A majorization-minimization algorithm for (multiple) hyperparameter learning
#@ Chuan-Sheng Foo;Chuong B. Do;Andrew Y. Ng
#t 2009
#c 19
#% 132676
#% 190434
#% 360691
#% 425040
#% 476874
#% 576520
#% 721164
#% 722760
#% 855508
#% 906274
#% 906491
#% 1854567
#! We present a general Bayesian framework for hyperparameter tuning in L2-regularized supervised learning models. Paradoxically, our algorithm works by first analytically integrating out the hyperparameters from the model. We find a local optimum of the resulting non-convex optimization problem efficiently using a majorization-minimization (MM) algorithm, in which the non-convex problem is reduced to a series of convex L2-regularized parameter estimation tasks. The principal appeal of our method is its simplicity: the updates for choosing the L2-regularized subproblems in each step are trivial to implement (or even perform by hand), and each subproblem can be efficiently solved by adapting existing solvers. Empirical results on a variety of supervised learning models show that our algorithm is competitive with both grid-search and gradient-based algorithms, but is more efficient and far easier to implement.

#index 1211731
#* Dynamic mixed membership blockmodel for evolving networks
#@ Wenjie Fu;Le Song;Eric P. Xing
#t 2009
#c 19
#% 875959
#% 876017
#% 1117695
#% 1673048
#! In a dynamic social or biological environment, interactions between the underlying actors can undergo large and systematic changes. Each actor can assume multiple roles and their degrees of affiliation to these roles can also exhibit rich temporal phenomena. We propose a state space mixed membership stochastic blockmodel which can track across time the evolving roles of the actors. We also derive an efficient variational inference procedure for our model, and apply it to the Enron email networks, and rewiring gene regulatory networks of yeast. In both cases, our model reveals interesting dynamical roles of the actors.

#index 1211732
#* Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property
#@ Rahul Garg;Rohit Khandekar
#t 2009
#c 19
#% 187651
#% 224113
#% 416518
#% 917281
#% 1193355
#% 1815826
#% 1815896
#% 1816485
#% 1816940
#! We present an algorithm for finding an s-sparse vector x that minimizes the square-error ∥y -- Φx∥2 where Φ satisfies the restricted isometry property (RIP), with isometric constant δ2s GraDeS (Gradient Descent with Sparsification) iteratively updates x as: [EQUATION] where γ 1 and Hs sets all but s largest magnitude coordinates to zero. GraDeS converges to the correct solution in constant number of iterations. The condition δ2s near-linear time algorithm is known. In comparison, the best condition under which a polynomial-time algorithm is known, is δ2s Our Matlab implementation of GraDeS outperforms previously proposed algorithms like Subspace Pursuit, StOMP, OMP, and Lasso by an order of magnitude. Curiously, our experiments also uncovered cases where L1-regularized regression (Lasso) fails but GraDeS finds the correct solution.

#index 1211733
#* Sequential Bayesian prediction in the presence of changepoints
#@ Roman Garnett;Michael A. Osborne;Stephen J. Roberts
#t 2009
#c 19
#% 135968
#% 891549
#% 1060234
#! We introduce a new sequential algorithm for making robust predictions in the presence of changepoints. Unlike previous approaches, which focus on the problem of detecting and locating changepoints, our algorithm focuses on the problem of making predictions even when such changes might be present. We introduce nonstationary covariance functions to be used in Gaussian process prediction that model such changes, then proceed to demonstrate how to effectively manage the hyperparameters associated with those covariance functions. By using Bayesian quadrature, we can integrate out the hyperparameters, allowing us to calculate the marginal predictive distribution. Furthermore, if desired, the posterior distribution over putative changepoint locations can be calculated as a natural byproduct of our prediction algorithm.

#index 1211734
#* PAC-Bayesian learning of linear classifiers
#@ Pascal Germain;Alexandre Lacasse;François Laviolette;Mario Marchand
#t 2009
#c 19
#% 431293
#% 722896
#% 803574
#% 875955
#! We present a general PAC-Bayes theorem from which all known PAC-Bayes risk bounds are obtained as particular cases. We also propose different learning algorithms for finding linear classifiers that minimize these bounds. These learning algorithms are generally competitive with both AdaBoost and the SVM.

#index 1211735
#* Fast evolutionary maximum margin clustering
#@ Fabian Gieseke;Tapio Pahikkala;Oliver Kramer
#t 2009
#c 19
#% 36672
#% 292664
#% 413449
#% 716271
#% 849269
#% 983944
#% 1074028
#% 1150790
#% 1269502
#! The maximum margin clustering approach is a recently proposed extension of the concept of support vector machines to the clustering problem. Briefly stated, it aims at finding an optimal partition of the data into two classes such that the margin induced by a subsequent application of a support vector machine is maximal. We propose a method based on stochastic search to address this hard optimization problem. While a direct implementation would be infeasible for large data sets, we present an efficient computational shortcut for assessing the "quality" of intermediate solutions. Experimental results show that our approach outperforms existing methods in terms of clustering accuracy.

#index 1211736
#* Dynamic analysis of multiagent Q-learning with ε-greedy exploration
#@ Eduardo Rodrigues Gomes;Ryszard Kowalczyk
#t 2009
#c 19
#% 266286
#% 299643
#% 384911
#% 431572
#% 643166
#% 773383
#% 825512
#% 830736
#% 1026746
#% 1074351
#% 1084371
#% 1156068
#% 1225267
#% 1274878
#! The development of mechanisms to understand and model the expected behaviour of multiagent learners is becoming increasingly important as the area rapidly find application in a variety of domains. In this paper we present a framework to model the behaviour of Q-learning agents using the ε-greedy exploration mechanism. For this, we analyse a continuous-time version of the Q-learning update rule and study how the presence of other agents and the ε-greedy mechanism affect it. We then model the problem as a system of difference equations which is used to theoretically analyse the expected behaviour of the agents. The applicability of the framework is tested through experiments in typical games selected from the literature.

#index 1211737
#* Bayesian inference for Plackett-Luce ranking models
#@ John Guiver;Edward Snelson
#t 2009
#c 19
#% 330769
#% 1024555
#! This paper gives an efficient Bayesian method for inferring the parameters of a Plackett-Luce ranking model. Such models are parameterised distributions over rankings of a finite set of objects, and have typically been studied and applied within the psychometric, sociometric and econometric literature. The inference scheme is an application of Power EP (expectation propagation). The scheme is robust and can be readily applied to large scale data sets. The inference algorithm extends to variations of the basic Plackett-Luce model, including partial rankings. We show a number of advantages of the EP approach over the traditional maximum likelihood method. We apply the method to aggregate rankings of NASCAR racing drivers over the 2002 season, and also to rankings of movie genres.

#index 1211738
#* Bayesian clustering for email campaign detection
#@ Peter Haider;Tobias Scheffer
#t 2009
#c 19
#% 321059
#% 799040
#% 840872
#% 983847
#! We discuss the problem of clustering elements according to the sources that have generated them. For elements that are characterized by independent binary attributes, a closed-form Bayesian solution exists. We derive a solution for the case of dependent attributes that is based on a transformation of the instances into a space of independent feature functions. We derive an optimization problem that produces a mapping into a space of independent binary feature vectors; the features can reflect arbitrary dependencies in the input space. This problem setting is motivated by the application of spam filtering for email service providers. Spam traps deliver a real-time stream of messages known to be spam. If elements of the same campaign can be recognized reliably, entire spam and phishing campaigns can be contained. We present a case study that evaluates Bayesian clustering for this application.

#index 1211739
#* Efficient learning algorithms for changing environments
#@ Elad Hazan;C. Seshadhri
#t 2009
#c 19
#% 232728
#% 266792
#% 722905
#% 871302
#% 875946
#% 991153
#% 1014650
#% 1674795
#! We study online learning in an oblivious changing environment. The standard measure of regret bounds the difference between the cost of the online learner and the best decision in hindsight. Hence, regret minimizing algorithms tend to converge to the static best optimum, clearly a suboptimal behavior in changing environments. On the other hand, various metrics proposed to strengthen regret and allow for more dynamic algorithms produce inefficient algorithms. We propose a different performance metric which strengthens the standard metric of regret and measures performance with respect to a changing comparator. We then describe a series of data-streaming-based reductions which transform algorithms for minimizing (standard) regret into adaptive algorithms albeit incurring only poly-logarithmic computational overhead. Using this reduction, we obtain efficient low adaptive-regret algorithms for the problem of online convex optimization. This can be applied to various learning scenarios, i.e. online portfolio selection, for which we describe experimental results showing the advantage of adaptivity.

#index 1211740
#* Hoeffding and Bernstein races for selecting policies in evolutionary direct policy search
#@ Verena Heidrich-Meisner;Christian Igel
#t 2009
#c 19
#% 229959
#% 384911
#% 477228
#% 490637
#% 575666
#% 803700
#% 961164
#% 1044118
#% 1055264
#% 1073956
#% 1074369
#% 1183589
#% 1225859
#% 1295890
#% 1688409
#! Uncertainty arises in reinforcement learning from various sources, and therefore it is necessary to consider statistics based on several roll-outs for evaluating behavioral policies. We add an adaptive uncertainty handling based on Hoeffding and empirical Bernstein races to the CMA-ES, a variable metric evolution strategy proposed for direct policy search. The uncertainty handling adjusts individually the number of episodes considered for the evaluation of a policy. The performance estimation is kept just accurate enough for a sufficiently good ranking of candidate policies, which is in turn sufficient for the CMA-ES to find better solutions. This increases the learning speed as well as the robustness of the algorithm.

#index 1211741
#* Partially supervised feature selection with regularized linear models
#@ Thibault Helleputte;Pierre Dupont
#t 2009
#c 19
#% 425040
#% 425048
#% 722929
#% 722943
#% 983927
#% 984113
#% 999695
#% 1041316
#% 1861282
#! This paper addresses feature selection techniques for classification of high dimensional data, such as those produced by microarray experiments. Some prior knowledge may be available in this context to bias the selection towards some dimensions (genes) a priori assumed to be more relevant. We propose a feature selection method making use of this partial supervision. It extends previous works on embedded feature selection with linear models including regularization to enforce sparsity. A practical approximation of this technique reduces to standard SVM learning with iterative rescaling of the inputs. The scaling factors depend here on the prior knowledge but the final selection may depart from it. Practical results on several microarray data sets show the benefits of the proposed approach in terms of the stability of the selected gene lists with improved classification performances.

#index 1211742
#* Learning with structured sparsity
#@ Junzhou Huang;Tong Zhang;Dimitris Metaxas
#t 2009
#c 19
#% 1074378
#% 1188997
#% 1815826
#% 1815965
#% 1816940
#! This paper investigates a new learning formulation called structured sparsity, which is a natural extension of the standard sparsity concept in statistical learning and compressive sensing. By allowing arbitrary structures on the feature set, this concept generalizes the group sparsity idea. A general theory is developed for learning with structured sparsity, based on the notion of coding complexity associated with the structure. Moreover, a structured greedy algorithm is proposed to efficiently solve the structured sparsity problem. Experiments demonstrate the advantage of structured sparsity over standard sparsity.

#index 1211743
#* Learning linear dynamical systems without sequence information
#@ Tzu-Kuo Huang;Jeff Schneider
#t 2009
#c 19
#% 770767
#% 891559
#! Virtually all methods of learning dynamic systems from data start from the same basic assumption: that the learning algorithm will be provided with a sequence, or trajectory, of data generated from the dynamic system. In this paper we consider the case where the data is not sequenced. The learning algorithm is presented a set of data points from the system's operation but with no temporal ordering. The data are simply drawn as individual disconnected points. While making this assumption may seem absurd at first glance, we observe that many scientific modeling tasks have exactly this property. In this paper we restrict our attention to learning linear, discrete time models. We propose several algorithms for learning these models based on optimizing approximate likelihood functions and test the methods on several synthetic data sets.

#index 1211744
#* Group lasso with overlap and graph lasso
#@ Laurent Jacob;Guillaume Obozinski;Jean-Philippe Vert
#t 2009
#c 19
#% 274586
#% 961223
#% 1073978
#% 1074378
#% 1379069
#! We propose a new penalty function which, when used as regularization for empirical risk minimization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of co-variates defined a priori, or a set of covariates which tend to be connected to each other when a graph of covariates is given. We study theoretical properties of the estimator, and illustrate its behavior on simulated and breast cancer gene expression data.

#index 1211745
#* Graph construction and b-matching for semi-supervised learning
#@ Tony Jebara;Jun Wang;Shih-Fu Chang
#t 2009
#c 19
#% 565545
#% 945194
#% 1034714
#% 1074015
#% 1133457
#% 1164839
#% 1665186
#! Graph based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems. A crucial step in graph based SSL methods is the conversion of data into a weighted graph. However, most of the SSL literature focuses on developing label inference algorithms without extensively studying the graph building method and its effect on performance. This article provides an empirical study of leading semi-supervised methods under a wide range of graph construction algorithms. These SSL inference algorithms include the Local and Global Consistency (LGC) method, the Gaussian Random Field (GRF) method, the Graph Transduction via Alternating Minimization (GTAM) method as well as other techniques. Several approaches for graph construction, sparsification and weighting are explored including the popular k-nearest neighbors method (kNN) and the b-matching method. As opposed to the greedily constructed kNN graph, the b-matched graph ensures each node in the graph has the same number of edges and produces a balanced or regular graph. Experimental results on both artificial data and real benchmark datasets indicate that b-matching produces more robust graphs and therefore provides significantly better prediction accuracy without any significant change in computation time.

#index 1211746
#* Trajectory prediction: learning to map situations to robot trajectories
#@ Nikolay Jetchev;Marc Toussaint
#t 2009
#c 19
#% 27853
#% 190429
#% 203167
#% 263522
#% 393059
#% 771638
#% 840848
#% 876006
#! Trajectory planning and optimization is a fundamental problem in articulated robotics. Algorithms used typically for this problem compute optimal trajectories from scratch in a new situation. In effect, extensive data is accumulated containing situations together with the respective optimized trajectories - but this data is in practice hardly exploited. The aim of this paper is to learn from this data. Given a new situation we want to predict a suitable trajectory which only needs minor refinement by a conventional optimizer. Our approach has two essential ingredients. First, to generalize from previous situations to new ones we need an appropriate situation descriptor - we propose a sparse feature selection approach to find such well-generalizing features of situations. Second, the transfer of previously optimized trajectories to a new situation should not be made in joint angle space - we propose a more efficient task space transfer of old trajectories to new situations. Experiments on a simulated humanoid reaching problem show that we can predict reasonable motion prototypes in new situations for which the refinement is much faster than an optimization from scratch.

#index 1211747
#* An accelerated gradient method for trace norm minimization
#@ Shuiwang Ji;Jieping Ye
#t 2009
#c 19
#% 803567
#% 840924
#% 983806
#% 983916
#% 1074373
#% 1077627
#% 1128929
#% 1232035
#% 1302843
#% 1302853
#% 1379069
#% 1556166
#% 1716082
#! We consider the minimization of a smooth loss function regularized by the trace norm of the matrix variable. Such formulation finds applications in many machine learning tasks including multi-task learning, matrix classification, and matrix completion. The standard semidefinite programming formulation for this problem is computationally expensive. In addition, due to the non-smooth nature of the trace norm, the optimal first-order black-box method for solving such class of problems converges as O(1/√k), where k is the iteration counter. In this paper, we exploit the special structure of the trace norm, based on which we propose an extended gradient algorithm that converges as O(1/k). We further propose an accelerated gradient algorithm, which achieves the optimal convergence rate of O(1/k2) for smooth problems. Experiments on multi-task learning problems demonstrate the efficiency of the proposed algorithms.

#index 1211748
#* A novel lexicalized HMM-based learning framework for web opinion miningNOTE FROM ACM: A Joint ACM Conference Committee has been determined that the authors of this article violated ACM's publication policy on simultaneous submissions. Therefore ACM has shut off access to this paper.
#@ Wei Jin;Hung Hay Ho
#t 2009
#c 19
#% 722308
#% 755861
#% 769892
#% 815915
#% 829973
#% 907489
#% 939896
#% 1035591
#! NOTE FROM ACM: A Joint ACM Conference Committee has been determined that the authors of this article violated ACM's publication policy on simultaneous submissions. Therefore ACM has shut off access to this paper.

#index 1211749
#* Orbit-product representation and correction of Gaussian belief propagation
#@ Jason K. Johnson;Vladimir Y. Chernyak;Michael Chertkov
#t 2009
#c 19
#% 856762
#% 961206
#% 1311983
#% 1810399
#% 1815596
#% 1816322
#! We present a new view of Gaussian belief propagation (GaBP) based on a representation of the determinant as a product over orbits of a graph. We show that the GaBP determinant estimate captures totally backtracking orbits of the graph and consider how to correct this estimate. We show that the missing orbits may be grouped into equivalence classes corresponding to backtrackless orbits and the contribution of each equivalence class is easily determined from the GaBP solution. Furthermore, we demonstrate that this multiplicative correction factor can be interpreted as the determinant of a backtrackless adjacency matrix of the graph with edge weights based on GaBP. Finally, an efficient method is proposed to compute a truncated correction factor including all backtrackless orbits up to a specified length.

#index 1211750
#* A Bayesian approach to protein model quality assessment
#@ Hetunandan Kamisetty;Christopher J. Langmead
#t 2009
#c 19
#% 6199
#% 443635
#% 577224
#% 840846
#% 980535
#% 983820
#% 1074021
#% 1387915
#% 1387916
#! Given multiple possible models b1, b2, ... bn for a protein structure, a common sub-task in in-silico Protein Structure Prediction is ranking these models according to their quality. Extant approaches use MLE estimates of parameters ri to obtain point estimates of the Model Quality. We describe a Bayesian alternative to assessing the quality of these models that builds an MRF over the parameters of each model and performs approximate inference to integrate over them. Hyperparameters w are learnt by optimizing a list-wise loss function over training data. Our results indicate that our Bayesian approach can significantly outperform MLE estimates and that optimizing the hyper-parameters can further improve results.

#index 1211751
#* Learning prediction suffix trees with Winnow
#@ Nikos Karampatziakis;Dexter Kozen
#t 2009
#c 19
#% 67056
#% 222437
#% 226674
#% 227736
#% 229807
#% 298269
#% 466074
#% 722816
#% 1818269
#! Prediction suffix trees (PSTs) are a popular tool for modeling sequences and have been successfully applied in many domains such as compression and language modeling. In this work we adapt the well studied Winnow algorithm to the task of learning PSTs. The proposed algorithm automatically grows the tree, so that it provably remains competitive with any fixed PST determined in hindsight. At the same time we prove that the depth of the tree grows only logarithmically with the number of mistakes made by the algorithm. Finally, we empirically demonstrate its effectiveness in two different tasks.

#index 1211752
#* Boosting products of base classifiers
#@ Balázs Kégl;Róbert Busa-Fekete
#t 2009
#c 19
#% 235377
#% 283138
#% 302391
#% 736300
#% 926881
#% 1499571
#! In this paper we show how to boost products of simple base learners. Similarly to trees, we call the base learner as a subroutine but in an iterative rather than recursive fashion. The main advantage of the proposed method is its simplicity and computational efficiency. On benchmark datasets, our boosted products of decision stumps clearly outperform boosted trees, and on the MNIST dataset the algorithm achieves the second best result among no-domain-knowledge algorithms after deep belief nets. As a second contribution, we present an improved base learner for nominal features and show that boosting the product of two of these new subset indicator base learners solves the maximum margin matrix factorization problem used to formalize the collaborative filtering task. On a small benchmark dataset, we get experimental results comparable to the semi-definite-programming-based solution but at a much lower computational cost.

#index 1211753
#* Learning Markov logic network structure via hypergraph lifting
#@ Stanley Kok;Pedro Domingos
#t 2009
#c 19
#% 26722
#% 44876
#% 333797
#% 769954
#% 840890
#% 850430
#% 915340
#% 983858
#% 983882
#% 1000502
#% 1073924
#% 1223507
#% 1250579
#% 1270261
#! Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. Learning MLN structure from a relational database involves learning the clauses and weights. The state-of-the-art MLN structure learners all involve some element of greedily generating candidate clauses, and are susceptible to local optima. To address this problem, we present an approach that directly utilizes the data in constructing candidates. A relational database can be viewed as a hypergraph with constants as nodes and relations as hyperedges. We find paths of true ground atoms in the hypergraph that are connected via their arguments. To make this tractable (there are exponentially many paths in the hypergraph), we lift the hypergraph by jointly clustering the constants to form higherlevel concepts, and find paths in it. We variabilize the ground atoms in each path, and use them to form clauses, which are evaluated using a pseudo-likelihood measure. In our experiments on three real-world datasets, we find that our algorithm outperforms the state-of-the-art approaches.

#index 1211754
#* Near-Bayesian exploration in polynomial time
#@ J. Zico Kolter;Andrew Y. Ng
#t 2009
#c 19
#% 363744
#% 425075
#% 466731
#% 495933
#% 722895
#% 840955
#% 876032
#% 876055
#% 1133454
#% 1211754
#% 1650283
#! We consider the exploration/exploitation problem in reinforcement learning (RL). The Bayesian approach to model-based RL offers an elegant solution to this problem, by considering a distribution over possible models and acting to maximize expected reward; unfortunately, the Bayesian solution is intractable for all but very restricted cases. In this paper we present a simple algorithm, and prove that with high probability it is able to perform ε-close to the true (intractable) optimal Bayesian policy after some small (polynomial in quantities describing the system) number of time steps. The algorithm and analysis are motivated by the so-called PAC-MDP approach, and extend such results into the setting of Bayesian RL. In this setting, we show that we can achieve lower sample complexity bounds than existing algorithms, while using an exploration strategy that is much greedier than the (extremely cautious) exploration of PAC-MDP algorithms.

#index 1211755
#* Regularization and feature selection in least-squares temporal difference learning
#@ J. Zico Kolter;Andrew Y. Ng
#t 2009
#c 19
#% 15462
#% 203596
#% 384911
#% 425076
#% 449561
#% 734920
#% 770857
#% 876001
#% 983896
#% 1211755
#% 1223289
#% 1250563
#% 1672995
#% 1861993
#! We consider the task of reinforcement learning with linear value function approximation. Temporal difference algorithms, and in particular the Least-Squares Temporal Difference (LSTD) algorithm, provide a method for learning the parameters of the value function, but when the number of features is large this algorithm can over-fit to the data and is computationally expensive. In this paper, we propose a regularization framework for the LSTD algorithm that overcomes these difficulties. In particular, we focus on the case of l1 regularization, which is robust to irrelevant features and also serves as a method for feature selection. Although the l1 regularized LSTD solution cannot be expressed as a convex optimization problem, we present an algorithm similar to the Least Angle Regression (LARS) algorithm that can efficiently compute the optimal solution. Finally, we demonstrate the performance of the algorithm experimentally.

#index 1211756
#* The graphlet spectrum
#@ Risi Kondor;Nino Shervashidze;Karsten M. Borgwardt
#t 2009
#c 19
#% 66635
#% 265373
#% 833065
#% 844291
#% 915350
#% 983919
#% 1073875
#% 1073934
#! Current graph kernels suffer from two limitations: graph kernels based on counting particular types of subgraphs ignore the relative position of these subgraphs to each other, while graph kernels based on algebraic methods are limited to graphs without node labels. In this paper we present the graphlet spectrum, a system of graph invariants derived by means of group representation theory that capture information about the number as well as the position of labeled subgraphs in a given graph. In our experimental evaluation the graphlet spectrum outperforms state-of-the-art graph kernels.

#index 1211757
#* Rule learning with monotonicity constraints
#@ Wojciech Kotłowski;Roman Słowiński
#t 2009
#c 19
#% 59121
#% 182684
#% 252020
#% 272541
#% 399790
#% 425033
#% 580511
#% 926881
#% 958632
#% 1090441
#% 1176984
#% 1412268
#! In classification with monotonicity constraints, it is assumed that the class label should increase with increasing values on the attributes. In this paper we aim at formalizing the approach to learning with monotonicity constraints from statistical point of view. Motivated by the statistical analysis, we present an algorithm for learning rule ensembles. The algorithm first "monotonizes" the data using a nonparametric classification procedure and then generates a rule ensemble consistent with the training set. The procedure is justified by a theoretical analysis and verified in a computational experiment.

#index 1211758
#* Multiple indefinite kernel learning with mixed norm regularization
#@ Matthieu Kowalski;Marie Szafranski;Liva Ralaivola
#t 2009
#c 19
#% 116149
#% 197394
#% 722909
#% 763697
#% 770846
#% 983901
#! We address the problem of learning classifiers using several kernel functions. On the contrary to many contributions in the field of learning from different sources of information using kernels, we here do not assume that the kernels used are positive definite. The learning problem that we are interested in involves a misclassification loss term and a regularization term that is expressed by means of a mixed norm. The use of a mixed norm allows us to enforce some sparsity structure, a particular case of which is, for instance, the Group Lasso. We solve the convex problem by employing proximal minimization algorithms, which can be viewed as refined versions of gradient descent procedures capable of naturally dealing with nondifferentiability. A numerical simulation on a Uci dataset shows the modularity of our approach.

#index 1211759
#* On sampling-based approximate spectral decomposition
#@ Sanjiv Kumar;Mehryar Mohri;Ameet Talwalkar
#t 2009
#c 19
#% 593842
#% 656665
#% 732552
#% 847159
#% 916799
#% 1074026
#! This paper addresses the problem of approximate singular value decomposition of large dense matrices that arises naturally in many machine learning applications. We discuss two recently introduced sampling-based spectral decomposition techniques: the Nyström and the Column-sampling methods. We present a theoretical comparison between the two methods and provide novel insights regarding their suitability for various applications. We then provide experimental results motivated by this theory. Finally, we propose an efficient adaptive sampling technique to select informative columns from the original matrix. This novel technique outperforms standard sampling methods on a variety of datasets.

#index 1211760
#* Learning spectral graph transformations for link prediction
#@ Jérôme Kunegis;Andreas Lommatzsch
#t 2009
#c 19
#% 420515
#% 464615
#% 729027
#% 730089
#% 754098
#% 783521
#% 823388
#% 975021
#% 1190129
#! We present a unified framework for learning link prediction and edge weight prediction functions in large networks, based on the transformation of a graph's algebraic spectrum. Our approach generalizes several graph kernels and dimensionality reduction methods and provides a method to estimate their parameters efficiently. We show how the parameters of these prediction functions can be learned by reducing the problem to a one-dimensional regression problem whose runtime only depends on the method's reduced rank and that can be inspected visually. We derive variants that apply to undirected, weighted, unweighted, unipartite and bipartite graphs. We evaluate our method experimentally using examples from social networks, collaborative filtering, trust networks, citation networks, authorship graphs and hyperlink networks.

#index 1211761
#* Block-wise construction of acyclic relational features with monotone irreducibility and relevancy properties
#@ Ondřej Kuželka;Filip železný
#t 2009
#c 19
#% 340736
#% 400847
#% 420087
#% 840863
#% 850431
#% 857307
#% 926881
#% 993437
#% 1271830
#! We describe an algorithm for constructing a set of acyclic conjunctive relational features by combining smaller conjunctive blocks. Unlike traditional level-wise approaches which preserve the monotonicity of frequency, our block-wise approach preserves a form of monotonicity of the irreducibility and relevancy feature properties, which are important in propositionalization employed in the context of classification learning. With pruning based on these properties, our block-wise approach efficiently scales to features including tens of first-order literals, far beyond the reach of state-of-the art propositionalization or inductive logic programming systems.

#index 1211762
#* Generalization analysis of listwise learning-to-rank algorithms
#@ Yanyan Lan;Tie-Yan Liu;Zhiming Ma;Hang Li
#t 2009
#c 19
#% 646000
#% 722909
#% 734915
#% 983820
#% 1039843
#% 1073936
#% 1074021
#% 1705503
#% 1810705
#! This paper presents a theoretical framework for ranking, and demonstrates how to perform generalization analysis of listwise ranking algorithms using the framework. Many learning-to-rank algorithms have been proposed in recent years. Among them, the listwise approach has shown higher empirical ranking performance when compared to the other approaches. However, there is no theoretical study on the listwise approach as far as we know. In this paper, we propose a theoretical framework for ranking, which can naturally describe various listwise learning-to-rank algorithms. With this framework, we prove a theorem which gives a generalization bound of a listwise ranking algorithm, on the basis of Rademacher Average of the class of compound functions. The compound functions take listwise loss functions as outer functions and ranking models as inner functions. We then compute the Rademacher Averages for existing listwise algorithms of ListMLE, ListNet, and RankCosine. We also discuss the tightness of the bounds in different situations with regard to the list length and transformation function.

#index 1211763
#* Approximate inference for planning in stochastic relational worlds
#@ Tobias Lang;Marc Toussaint
#t 2009
#c 19
#% 225838
#% 425074
#% 528340
#% 770823
#% 876063
#% 1000502
#% 1269827
#% 1272161
#% 1274869
#! Relational world models that can be learned from experience in stochastic domains have received significant attention recently. However, efficient planning using these models remains a major issue. We propose to convert learned noisy probabilistic relational rules into a structured dynamic Bayesian network representation. Predicting the effects of action sequences using approximate inference allows for planning in complex worlds. We evaluate the effectiveness of our approach for online planning in a 3D simulated blocksworld with an articulated manipulator and realistic physics. Empirical results show that our method can solve problems where existing methods fail.

#index 1211764
#* Learning nonlinear dynamic models
#@ John Langford;Ruslan Salakhutdinov;Tong Zhang
#t 2009
#c 19
#% 235061
#% 284558
#% 464624
#% 1202000
#% 1760888
#! We present a novel approach for learning nonlinear dynamic models, which leads to a new set of tools capable of solving problems that are otherwise difficult. We provide theory showing this new approach is consistent for models with long range structure, and apply the approach to motion capture and high-dimensional video data, yielding results superior to standard alternatives.

#index 1211765
#* Non-linear matrix factorization with Gaussian processes
#@ Neil D. Lawrence;Raquel Urtasun
#t 2009
#c 19
#% 304879
#% 840924
#% 875976
#% 891549
#% 916787
#% 989626
#% 1038334
#% 1073982
#% 1083671
#! A popular approach to collaborative filtering is matrix factorization. In this paper we develop a non-linear probabilistic matrix factorization using Gaussian process latent variable models. We use stochastic gradient descent (SGD) to optimize the model. SGD allows us to apply Gaussian processes to data sets with millions of observations without approximate methods. We apply our approach to benchmark movie recommender data sets. The results show better than previous state-of-the-art performance.

#index 1211766
#* Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations
#@ Honglak Lee;Roger Grosse;Rajesh Ranganath;Andrew Y. Ng
#t 2009
#c 19
#% 450888
#% 784995
#% 812301
#% 883810
#% 883972
#% 883981
#% 891060
#% 983899
#% 1042825
#% 1074018
#% 1211799
#! There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.

#index 1211767
#* Transfer learning for collaborative filtering via a rating-matrix generative model
#@ Bin Li;Qiang Yang;Xiangyang Xue
#t 2009
#c 19
#% 173879
#% 236497
#% 330687
#% 495929
#% 528156
#% 668807
#% 770848
#% 844369
#% 881468
#% 983899
#% 1271814
#! Cross-domain collaborative filtering solves the sparsity problem by transferring rating knowledge across multiple domains. In this paper, we propose a rating-matrix generative model (RMGM) for effective cross-domain collaborative filtering. We first show that the relatedness across multiple rating matrices can be established by finding a shared implicit cluster-level rating matrix, which is next extended to a cluster-level rating model. Consequently, a rating matrix of any related task can be viewed as drawing a set of users and items from a user-item joint mixture model as well as drawing the corresponding ratings from the cluster-level rating model. The combination of these two models gives the RMGM, which can be used to fill the missing ratings for both existing and new users. A major advantage of RMGM is that it can share the knowledge by pooling the rating data from multiple tasks even when the users and items of these tasks do not overlap. We evaluate the RMGM empirically on three real-world collaborative filtering data sets to show that RMGM can outperform the individual models trained separately.

#index 1211768
#* ABC-boost: adaptive base class boost for multi-class classification
#@ Ping Li
#t 2009
#c 19
#% 73372
#% 198701
#% 235377
#% 302391
#% 311034
#% 722756
#% 793240
#% 1074039
#% 1674802
#! We propose abc-boost (adaptive base class boost) for multi-class classification and present abc-mart, an implementation of abc-boost, based on the multinomial logit model. The key idea is that, at each boosting iteration, we adaptively and greedily choose a base class. Our experiments on public datasets demonstrate the improvement of abc-mart over the original mart algorithm.

#index 1211769
#* Semi-supervised learning using label mean
#@ Yu-Feng Li;James T. Kwok;Zhi-Hua Zhou
#t 2009
#c 19
#% 304876
#% 466263
#% 763697
#% 770846
#% 829043
#% 833913
#% 961190
#% 961218
#% 1074345
#% 1286820
#! Semi-Supervised Support Vector Machines (S3VMs) typically directly estimate the label assignments for the unlabeled instances. This is often inefficient even with recent advances in the efficient training of the (supervised) SVM. In this paper, we show that S3VMs, with knowledge of the means of the class labels of the unlabeled data, is closely related to the supervised SVM with known labels on all the unlabeled data. This motivates us to first estimate the label means of the unlabeled data. Two versions of the meanS3VM, which work by maximizing the margin between the label means, are proposed. The first one is based on multiple kernel learning, while the second one is based on alternating optimization. Experiments show that both of the proposed algorithms achieve highly competitive and sometimes even the best performance as compared to the state-of-the-art semi-supervised learners. Moreover, they are more efficient than existing S3VMs.

#index 1211770
#* Learning from measurements in exponential families
#@ Percy Liang;Michael I. Jordan;Dan Klein
#t 2009
#c 19
#% 464268
#% 464434
#% 940031
#% 983878
#% 1014647
#% 1073969
#% 1073986
#% 1074125
#! Given a model family and a set of unlabeled examples, one could either label specific examples or state general constraints---both provide information about the desired model. In general, what is the most cost-effective way to learn? To address this question, we introduce measurements, a general class of mechanisms for providing information about a target model. We present a Bayesian decision-theoretic framework, which allows us to both integrate diverse measurements and choose new measurements to make. We use a variational inference algorithm, which exploits exponential family duality. The merits of our approach are demonstrated on two sequence labeling tasks.

#index 1211771
#* Blockwise coordinate descent procedures for the multi-task lasso, with applications to neural semantic basis discovery
#@ Han Liu;Mark Palatucci;Jian Zhang
#t 2009
#c 19
#% 397854
#% 1042631
#% 1128929
#! We develop a cyclical blockwise coordinate descent algorithm for the multi-task Lasso that efficiently solves problems with thousands of features and tasks. The main result shows that a closed-form Winsorization operator can be obtained for the sup-norm penalized least squares regression. This allows the algorithm to find solutions to very large-scale problems far more efficiently than existing methods. This result complements the pioneering work of Friedman, et al. (2007) for the single-task Lasso. As a case study, we use the multi-task Lasso as a variable selector to discover a semantic basis for predicting human neural activation. The learned solution outperforms the standard basis for this task on the majority of test participants, while requiring far fewer assumptions about cognitive neuroscience. We demonstrate how this learned basis can yield insights into how the brain represents the meanings of words.

#index 1211772
#* Efficient Euclidean projections in linear time
#@ Jun Liu;Jieping Ye
#t 2009
#c 19
#% 410276
#% 757953
#% 770857
#% 961191
#% 1014657
#% 1073906
#% 1815965
#! We consider the problem of computing the Euclidean projection of a vector of length n onto a closed convex set including the l1 ball and the specialized polyhedra employed in (Shalev-Shwartz & Singer, 2006). These problems have played building block roles in solving several l1-norm based sparse learning problems. Existing methods have a worst-case time complexity of O(n log n). In this paper, we propose to cast both Euclidean projections as root finding problems associated with specific auxiliary functions, which can be solved in linear time via bisection. We further make use of the special structure of the auxiliary functions, and propose an improved bisection algorithm. Empirical studies demonstrate that the proposed algorithms are much more efficient than the competing ones for computing the projections.

#index 1211773
#* Topic-link LDA: joint models of topic and author community
#@ Yan Liu;Alexandru Niculescu-Mizil;Wojciech Gryc
#t 2009
#c 19
#% 249110
#% 303620
#% 420495
#% 705252
#% 722904
#% 788094
#% 840961
#% 867050
#% 875959
#% 983833
#% 1055681
#% 1289476
#! Given a large-scale linked document collection, such as a collection of blog posts or a research literature archive, there are two fundamental problems that have generated a lot of interest in the research community. One is to identify a set of high-level topics covered by the documents in the collection; the other is to uncover and analyze the social network of the authors of the documents. So far these problems have been viewed as separate problems and considered independently from each other. In this paper we argue that these two problems are in fact inter-dependent and should be addressed together. We develop a Bayesian hierarchical approach that performs topic modeling and author community discovery in one unified framework. The effectiveness of our model is demonstrated on two blog data sets in different domains and one research paper citation data from CiteSeer.

#index 1211774
#* Geometry-aware metric learning
#@ Zhengdong Lu;Prateek Jain;Inderjit S. Dhillon
#t 2009
#c 19
#% 464615
#% 593047
#% 763697
#% 770767
#% 794860
#% 829029
#% 840938
#% 855573
#% 983830
#% 1074377
#! In this paper, we introduce a generic framework for semi-supervised kernel learning. Given pair-wise (dis-)similarity constraints, we learn a kernel matrix over the data that respects the provided side-information as well as the local geometry of the data. Our framework is based on metric learning methods, where we jointly model the metric/kernel over the data along with the underlying manifold. Furthermore, we show that for some important parameterized forms of the underlying manifold model, we can estimate the model parameters and the kernel matrix efficiently. Our resulting algorithm is able to incorporate local geometry into the metric learning task; at the same time it can handle a wide class of constraints. Finally, our algorithm is fast and scalable -- unlike most of the existing methods, it is able to exploit the low dimensional manifold structure and does not require semi-definite programming. We demonstrate wide applicability and effectiveness of our framework by applying to various machine learning tasks such as semi-supervised classification, colored dimensionality reduction, manifold alignment etc. On each of the tasks our method performs competitively or better than the respective state-of-the-art method.

#index 1211775
#* Identifying suspicious URLs: an application of large-scale online learning
#@ Justin Ma;Lawrence K. Saul;Stefan Savage;Geoffrey M. Voelker
#t 2009
#c 19
#% 274189
#% 956559
#% 961152
#% 1014521
#% 1020403
#% 1042610
#% 1072120
#% 1073905
#% 1073962
#% 1164774
#% 1214746
#! This paper explores online learning approaches for detecting malicious Web sites (those involved in criminal scams) using lexical and host-based features of the associated URLs. We show that this application is particularly appropriate for online algorithms as the size of the training data is larger than can be efficiently processed in batch and because the distribution of features that typify malicious URLs is changing continuously. Using a real-time system we developed for gathering URL features, combined with a real-time source of labeled URLs from a large Web mail provider, we demonstrate that recently-developed online algorithms can be as accurate as batch techniques, achieving classification accuracies up to 99% over a balanced data set.

#index 1211776
#* Online dictionary learning for sparse coding
#@ Julien Mairal;Francis Bach;Jean Ponce;Guillermo Sapiro
#t 2009
#c 19
#% 263850
#% 274189
#% 274586
#% 983899
#% 1301803
#% 1302899
#% 1759275
#% 1815739
#% 1856288
#% 1856571
#! Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.

#index 1211777
#* Proto-predictive representation of states with simple recurrent temporal-difference networks
#@ Takaki Makino
#t 2009
#c 19
#% 92135
#% 96697
#% 203678
#% 356892
#% 770781
#% 770863
#% 840946
#% 840958
#% 857087
#% 1042867
#% 1073951
#% 1269515
#% 1289489
#! We propose a new neural network architecture, called Simple Recurrent Temporal-Difference Networks (SR-TDNs), that learns to predict future observations in partially observable environments. SR-TDNs incorporate the structure of simple recurrent neural networks (SRNs) into temporal-difference (TD) networks to use proto-predictive representation of states. Although they deviate from the principle of predictive representations to ground state representations on observations, they follow the same learning strategy as TD networks, i.e., applying TD-learning to general predictions. Simulation experiments revealed that SR-TDNs can correctly represent states with an incomplete set of core tests (question networks), and consequently, SR-TDNs have better on-line learning capacity than TD networks in various environments.

#index 1211778
#* Sparse Gaussian graphical models with unknown block structure
#@ Benjamin M. Marlin;Kevin P. Murphy
#t 2009
#c 19
#% 61079
#% 722754
#% 738776
#% 771626
#% 875956
#% 891559
#% 1074353
#% 1125541
#% 1223124
#! Recent work has shown that one can learn the structure of Gaussian Graphical Models by imposing an L1 penalty on the precision matrix, and then using efficient convex optimization methods to find the penalized maximum likelihood estimate. This is similar to performing MAP estimation with a prior that prefers sparse graphs. In this paper, we use the stochastic block model as a prior. This prefer graphs that are blockwise sparse, but unlike previous work, it does not require that the blocks or groups be specified a priori. The resulting problem is no longer convex, but we devise an efficient variational Bayes algorithm to solve it. We show that our method has better test set likelihood on two different datasets (motion capture and gene expression) compared to independent L1, and can match the performance of group L1 using manually created groups.

#index 1211779
#* Polyhedral outer approximations with application to natural language parsing
#@ André F. T. Martins;Noah A. Smith;Eric P. Xing
#t 2009
#c 19
#% 10129
#% 341672
#% 464434
#% 575676
#% 757329
#% 770763
#% 770866
#% 840856
#% 840927
#% 850430
#% 854636
#% 939919
#% 961152
#% 1073910
#% 1249462
#% 1249474
#% 1261540
#% 1264733
#% 1271211
#% 1272209
#% 1815223
#! Recent approaches to learning structured predictors often require approximate inference for tractability; yet its effects on the learned model are unclear. Meanwhile, most learning algorithms act as if computational cost was constant within the model class. This paper sheds some light on the first issue by establishing risk bounds for max-margin learning with LP relaxed inference and addresses the second issue by proposing a new paradigm that attempts to penalize "time-consuming" hypotheses. Our analysis relies on a geometric characterization of the outer polyhedra associated with the LP relaxation. We then apply these techniques to the problem of dependency parsing, for which a concise LP formulation is provided that handles non-local output features. A significant improvement is shown over arc-factored models.

#index 1211780
#* Partial order embedding with multiple kernels
#@ Brian McFee;Gert Lanckriet
#t 2009
#c 19
#% 464291
#% 763697
#% 770767
#% 770782
#% 778279
#% 884076
#! We consider the problem of embedding arbitrary objects (e.g., images, audio, documents) into Euclidean space subject to a partial order over pair-wise distances. Partial order constraints arise naturally when modeling human perception of similarity. Our partial order framework enables the use of graph-theoretic tools to more efficiently produce the embedding, and exploit global structure within the constraint set. We present an embedding algorithm based on semidefinite programming, which can be parameterized by multiple kernels to yield a unified space from heterogeneous features.

#index 1211781
#* Bandit-based optimization on graphs with application to library performance tuning
#@ Frédéric de Mesmay;Arpad Rimmel;Yevgen Voronenko;Markus Püschel
#t 2009
#c 19
#% 425053
#% 722922
#% 801935
#% 970933
#% 983838
#% 1138542
#% 1204882
#% 1269575
#% 1404135
#% 1664996
#% 1665148
#! The problem of choosing fast implementations for a class of recursive algorithms such as the fast Fourier transforms can be formulated as an optimization problem over the language generated by a suitably defined grammar. We propose a novel algorithm that solves this problem by reducing it to maximizing an objective function over the sinks of a directed acyclic graph. This algorithm valuates nodes using Monte-Carlo and grows a subgraph in the most promising directions by considering local maximum k-armed bandits. When used inside an adaptive linear transform library, it cuts down the search time by an order of magnitude compared to the existing algorithm. In some cases, the performance of the implementations found is also increased by up to 10% which is of considerable practical importance since it consequently improves the performance of all applications using the library.

#index 1211782
#* Deep learning from temporal coherence in video
#@ Hossein Mobahi;Ronan Collobert;Jason Weston
#t 2009
#c 19
#% 190581
#% 220995
#% 274894
#% 278012
#% 450248
#% 642768
#% 775298
#% 812372
#% 840843
#% 929717
#% 961285
#% 1074018
#% 1119142
#% 1502411
#! This work proposes a learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the same object or objects. This coherence is used as a supervisory signal over the unlabeled data, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and face recognition tasks.

#index 1211783
#* Regression by dependence minimization and its application to causal inference in additive noise models
#@ Joris Mooij;Dominik Janzing;Jonas Peters;Bernhard Schölkopf
#t 2009
#c 19
#% 73441
#% 297171
#% 722798
#% 891549
#% 961205
#% 1673681
#! Motivated by causal inference problems, we propose a novel method for regression that minimizes the statistical dependence between regressors and residuals. The key advantage of this approach to regression is that it does not assume a particular distribution of the noise, i.e., it is non-parametric with respect to the noise distribution. We argue that the proposed regression method is well suited to the task of causal inference in additive noise models. A practical disadvantage is that the resulting optimization problem is generally non-convex and can be difficult to solve. Nevertheless, we report good results on one of the tasks of the NIPS 2008 Causality Challenge, where the goal is to distinguish causes from effects in pairs of statistically dependent variables. In addition, we propose an algorithm for efficiently inferring causal models from observational data for more than two variables. The required number of regressions and independence tests is quadratic in the number of variables, which is a significant improvement over the simple method that tests all possible DAGs.

#index 1211784
#* Learning complex motions by sequencing simpler motion templates
#@ Gerhard Neumann;Wolfgang Maass;Jan Peters
#t 2009
#c 19
#% 229931
#% 286423
#% 570003
#% 829011
#% 1699601
#! Abstraction of complex, longer motor tasks into simpler elemental movements enables humans and animals to exhibit motor skills which have not yet been matched by robots. Humans intuitively decompose complex motions into smaller, simpler segments. For example when describing simple movements like drawing a triangle with a pen, we can easily name the basic steps of this movement. Surprisingly, such abstractions have rarely been used in artificial motor skill learning algorithms. These algorithms typically choose a new action (such as a torque or a force) at a very fast time-scale. As a result, both policy and temporal credit assignment problem become unnecessarily complex - often beyond the reach of current machine learning methods. We introduce a new framework for temporal abstractions in reinforcement learning (RL), i.e. RL with motion templates. We present a new algorithm for this framework which can learn high-quality policies by making only few abstract decisions.

#index 1211785
#* Convex variational Bayesian inference for large scale generalized linear models
#@ Hannes Nickisch;Matthias W. Seeger
#t 2009
#c 19
#% 269189
#% 415339
#% 576520
#% 705252
#% 856731
#% 992949
#% 1073986
#% 1074364
#! We show how variational Bayesian inference can be implemented for very large generalized linear models. Our relaxation is proven to be a convex problem for any log-concave model. We provide a generic double loop algorithm for solving this relaxation on models with arbitrary super-Gaussian potentials. By iteratively decoupling the criterion, most of the work can be done by solving large linear systems, rendering our algorithm orders of magnitude faster than previously proposed solvers for the same problem. We evaluate our method on problems of Bayesian active learning for large binary classification models, and show how to address settings with many candidates and sequential inclusion steps.

#index 1211786
#* Solution stability in linear programming relaxations: graph partitioning and unsupervised learning
#@ Sebastian Nowozin;Stefanie Jegelka
#t 2009
#c 19
#% 13742
#% 61552
#% 80635
#% 137029
#% 144404
#% 382586
#% 460812
#% 840862
#% 840883
#% 896856
#% 937551
#% 1034723
#! We propose a new method to quantify the solution stability of a large class of combinatorial optimization problems arising in machine learning. As practical example we apply the method to correlation clustering, clustering aggregation, modularity clustering, and relative performance significance clustering. Our method is extensively motivated by the idea of linear programming relaxations. We prove that when a relaxation is used to solve the original clustering problem, then the solution stability calculated by our method is conservative, that is, it never overestimates the solution stability of the true, unrelaxed problem. We also demonstrate how our method can be used to compute the entire path of optimal solutions as the optimization problem is increasingly perturbed. Experimentally, our method is shown to perform well on a number of benchmark problems.

#index 1211787
#* Nonparametric factor analysis with beta process priors
#@ John Paisley;Lawrence Carin
#t 2009
#c 19
#% 722760
#% 1073991
#% 1403232
#! We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.

#index 1211788
#* Unsupervised hierarchical modeling of locomotion styles
#@ Wei Pan;Lorenzo Torresani
#t 2009
#c 19
#% 95730
#% 303620
#% 308505
#% 398425
#% 398426
#% 436429
#% 771053
#% 816038
#% 983926
#% 1074007
#% 1346985
#! This paper describes an unsupervised learning technique for modeling human locomotion styles, such as distinct related activities (e.g. running and striding) or variations of the same motion performed by different subjects. Modeling motion styles requires identifying the common structure in the motions and detecting style-specific characteristics. We propose an algorithm that learns a hierarchical model of styles from unlabeled motion capture data by exploiting the cyclic property of human locomotion. We assume that sequences with the same style contain locomotion cycles generated by noisy, temporally warped versions of a single latent cycle. We model these style-specific latent cycles as random variables drawn from a common "parent" cycle distribution, representing the structure shared by all motions. Given these hierarchical priors, the algorithm learns, in a completely unsupervised fashion, temporally aligned latent cycle distributions, each modeling a specific locomotion style, and computes for each example the style label posterior distribution, the segmentation into cycles, and the temporal warping with respect to the latent cycles. We demonstrate the flexibility of the model on several application problems such as style clustering, animation, style blending, and filling in of missing data.

#index 1211789
#* Binary action search for learning continuous-action control policies
#@ Jason Pazis;Michail G. Lagoudakis
#t 2009
#c 19
#% 252329
#% 384911
#% 425077
#% 734920
#% 771849
#% 829011
#% 1787847
#% 1860225
#! Reinforcement Learning methods for controlling stochastic processes typically assume a small and discrete action space. While continuous action spaces are quite common in real-world problems, the most common approach still employed in practice is coarse discretization of the action space. This paper presents a novel method, called Binary Action Search, for realizing continuousaction policies by searching efficiently the entire action range through increment and decrement modifications to the values of the action variables according to an internal binary policy defined over an augmented state space. The proposed approach essentially approximates any continuous action space to arbitrary resolution and can be combined with any discrete-action reinforcement learning algorithm for learning continuous-action policies. Binary Action Search eliminates the restrictive modification steps of Adaptive Action Modification and requires no temporal action locality in the domain. Our approach is coupled with two well-known reinforcement learning algorithms (Least-Squares Policy Iteration and Fitted Q-Iteration) and its use and properties are thoroughly investigated and demonstrated on the continuous state-action Inverted Pendulum, Double Integrator, and Car on the Hill domains.

#index 1211790
#* Detecting the direction of causal time series
#@ Jonas Peters;Dominik Janzing;Arthur Gretton;Bernhard Schölkopf
#t 2009
#c 19
#% 13453
#% 961205
#% 1673681
#! We propose a method that detects the true direction of time series, by fitting an autoregressive moving average model to the data. Whenever the noise is independent of the previous samples for one ordering of the observations, but dependent for the opposite ordering, we infer the former direction to be the true one. We prove that our method works in the population case as long as the noise of the process is not normally distributed (for the latter case, the direction is not identifiable). A new and important implication of our result is that it confirms a fundamental conjecture in causal reasoning --- if after regression the noise is independent of signal for one direction and dependent for the other, then the former represents the true causal direction --- in the case of time series. We test our approach on two types of data: simulated data sets conforming to our modeling assumptions, and real world EEG time series. Our method makes a decision for a significant fraction of both data sets, and these decisions are mostly correct. For real world data, our approach outperforms alternative solutions to the problem of time direction recovery.

#index 1211791
#* Constraint relaxation in approximate linear programs
#@ Marek Petrik;Shlomo Zilberstein
#t 2009
#c 19
#% 384911
#% 393786
#% 477283
#% 734920
#% 739715
#% 1021595
#% 1272002
#! Approximate Linear Programming (ALP) is a reinforcement learning technique with nice theoretical properties, but it often performs poorly in practice. We identify some reasons for the poor quality of ALP solutions in problems where the approximation induces virtual loops. We then introduce two methods for improving solution quality. One method rolls out selected constraints of the ALP, guided by the dual information. The second method is a relaxation of the ALP, based on external penalty methods. The latter method is applicable in domains in which rolling out constraints is impractical. Both approaches show promising empirical results for simple benchmark problems as well as for a realistic blood inventory management problem.

#index 1211792
#* Multi-class image segmentation using conditional random fields and global classification
#@ Nils Plath;Marc Toussaint;Shinichi Nakajima
#t 2009
#c 19
#% 443975
#% 464434
#% 748636
#% 829043
#% 883972
#% 945194
#% 961190
#% 964950
#% 990309
#% 1275085
#% 1667698
#! A key aspect of semantic image segmentation is to integrate local and global features for the prediction of local segment labels. We present an approach to multi-class segmentation which combines two methods for this integration: a Conditional Random Field (CRF) which couples to local image features and an image classification method which considers global features. The CRF follows the approach of Reynolds & Murphy (2007) and is based on an unsupervised multi scale pre-segmentation of the image into patches, where patch labels correspond to the random variables of the CRF. The output of the classifier is used to constraint this CRF. We demonstrate and compare the approach on a standard semantic segmentation data set.

#index 1211793
#* Learning when to stop thinking and do something!
#@ Barnabás Póczos;Yasin Abbasi-Yadkori;Csaba Szepesvári;Russell Greiner;Nathan Sturtevant
#t 2009
#c 19
#% 124687
#% 384911
#% 447606
#% 793249
#% 1073956
#% 1272375
#% 1272385
#! An anytime algorithm is capable of returning a response to the given task at essentially any time; typically the quality of the response improves as the time increases. Here, we consider the challenge of learning when we should terminate such algorithms on each of a sequence of iid tasks, to optimize the expected average reward per unit time. We provide a system for addressing this challenge, which combines the global optimizer Cross-Entropy method with local gradient ascent. This paper theoretically investigates how far the estimated gradient is from the true gradient, then empirically demonstrates that this system is effective by applying it to a toy problem, as well as on a real-world face detection task.

#index 1211794
#* Independent factor topic models
#@ Duangmanee (Pew) Putthividhya;Hagai T. Attias;Srikantan Nagarajan
#t 2009
#c 19
#% 705252
#% 722904
#% 856731
#! Topic models such as Latent Dirichlet Allocation (LDA) and Correlated Topic Model (CTM) have recently emerged as powerful statistical tools for text document modeling. In this paper, we improve upon CTM and propose Independent Factor Topic Models (IFTM) which use linear latent variable models to uncover the hidden sources of correlation between topics. There are 2 main contributions of this work. First, by using a sparse source prior model, we can directly visualize sparse patterns of topic correlations. Secondly, the conditional independence assumption implied in the use of latent source variables allows the objective function to factorize, leading to a fast Newton-Raphson based variational inference algorithm. Experimental results on synthetic and real data show that IFTM runs on average 3--5 times faster than CTM, while giving competitive performance as measured by perplexity and loglikelihood of held-out data.

#index 1211795
#* An efficient sparse metric learning in high-dimensional space via l1-penalized log-determinant regularization
#@ Guo-Jun Qi;Jinhui Tang;Zheng-Jun Zha;Tat-Seng Chua;Hong-Jiang Zhang
#t 2009
#c 19
#% 983830
#% 983946
#% 1074353
#% 1164188
#! This paper proposes an efficient sparse metric learning algorithm in high dimensional space via an l1-penalized log-determinant regularization. Compare to the most existing distance metric learning algorithms, the proposed algorithm exploits the sparsity nature underlying the intrinsic high dimensional feature space. This sparsity prior of learning distance metric serves to regularize the complexity of the distance model especially in the "less example number p and high dimension d" setting. Theoretically, by analogy to the covariance estimation problem, we find the proposed distance learning algorithm has a consistent result at rate O (√m2 log d)/n) to the target distance matrix with at most m nonzeros per row. Moreover, from the implementation perspective, this l1-penalized log-determinant formulation can be efficiently optimized in a block coordinate descent fashion which is much faster than the standard semi-definite programming which has been widely adopted in many other advanced distance learning algorithms. We compare this algorithm with other state-of-the-art ones on various datasets and competitive results are obtained.

#index 1211796
#* Sparse higher order conditional random fields for improved sequence labeling
#@ Xian Qian;Xiaoqian Jiang;Qi Zhang;Xuanjing Huang;Lide Wu
#t 2009
#c 19
#% 464434
#% 815924
#% 840927
#% 854636
#! In real sequence labeling tasks, statistics of many higher order features are not sufficient due to the training data sparseness, very few of them are useful. We describe Sparse Higher Order Conditional Random Fields (SHO-CRFs), which are able to handle local features and sparse higher order features together using a novel tractable exact inference algorithm. Our main insight is that states and transitions with same potential functions can be grouped together, and inference is performed on the grouped states and transitions. Though the complexity is not polynomial, SHO-CRFs are still efficient in practice because of the feature sparseness. Experimental results on optical character recognition and Chinese organization name recognition show that with the same higher order feature set, SHO-CRFs significantly outperform previous approaches.

#index 1211797
#* An efficient projection for l1, ∞ regularization
#@ Ariadna Quattoni;Xavier Carreras;Michael Collins;Trevor Darrell
#t 2009
#c 19
#% 770857
#% 829008
#% 873584
#% 883971
#% 961270
#% 983905
#% 1073906
#% 1450428
#! In recent years the l1, ∞ norm has been proposed for joint regularization. In essence, this type of regularization aims at extending the l1 framework for learning sparse models to a setting where the goal is to learn a set of jointly sparse models. In this paper we derive a simple and effective projected gradient method for optimization of l1, ∞ regularized problems. The main challenge in developing such a method resides on being able to compute efficient projections to the l1, ∞ ball. We present an algorithm that works in O(n log n) time and O(n) memory where n is the number of parameters. We test our algorithm in a multi-task image annotation problem. Our results show that l1, ∞ leads to better performance than both l2 and l1 regularization and that it is is effective in discovering jointly sparse solutions.

#index 1211798
#* Nearest neighbors in high-dimensional data: the emergence and influence of hubs
#@ Miloš Radovanović;Alexandros Nanopoulos;Mirjana Ivanović
#t 2009
#c 19
#% 730019
#% 835018
#% 891559
#% 982755
#% 992326
#% 1073884
#! High dimensionality can pose severe difficulties, widely recognized as different aspects of the curse of dimensionality. In this paper we study a new aspect of the curse pertaining to the distribution of k-occurrences, i.e., the number of times a point appears among the k nearest neighbors of other points in a data set. We show that, as dimensionality increases, this distribution becomes considerably skewed and hub points emerge (points with very high k-occurrences). We examine the origin of this phenomenon, showing that it is an inherent property of high-dimensional vector space, and explore its influence on applications based on measuring distances in vector spaces, notably classification, clustering, and information retrieval.

#index 1211799
#* Large-scale deep unsupervised learning using graphics processors
#@ Rajat Raina;Anand Madhavan;Andrew Y. Ng
#t 2009
#c 19
#% 450888
#% 770857
#% 815796
#% 891060
#% 931298
#% 963669
#% 983808
#% 983899
#% 1062525
#% 1064222
#% 1073885
#% 1073971
#% 1211766
#% 1309131
#! The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.

#index 1211800
#* The Bayesian group-Lasso for analyzing contingency tables
#@ Sudhir Raman;Thomas J. Fuchs;Peter J. Wild;Edgar Dahl;Volker Roth
#t 2009
#c 19
#% 1073978
#% 1074364
#! Group-Lasso estimators, useful in many applications, suffer from lack of meaningful variance estimates for regression coefficients. To overcome such problems, we propose a full Bayesian treatment of the Group-Lasso, extending the standard Bayesian Lasso, using hierarchical expansion. The method is then applied to Poisson models for contingency tables using a highly efficient MCMC algorithm. The simulated experiments validate the performance of this method on artificial datasets with known ground-truth. When applied to a breast cancer dataset, the method demonstrates the capability of identifying the differences in interactions patterns of marker proteins between different patient groups.

#index 1211801
#* Supervised learning from multiple experts: whom to trust when everyone lies a bit
#@ Vikas C. Raykar;Shipeng Yu;Linda H. Zhao;Anna Jerebko;Charles Florin;Gerardo Hermosillo Valadez;Luca Bogoni;Linda Moy
#t 2009
#c 19
#% 124467
#% 269195
#% 1083692
#% 1264744
#! We describe a probabilistic approach for supervised learning when we have multiple experts/annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of the actual hidden labels. Experimental results indicate that the proposed method is superior to the commonly used majority voting baseline.

#index 1211802
#* Surrogate regret bounds for proper losses
#@ Mark D. Reid;Robert C. Williamson
#t 2009
#c 19
#% 735254
#% 916785
#% 1071503
#% 1816287
#% 1860570
#! We present tight surrogate regret bounds for the class of proper (i.e., Fisher consistent) losses. The bounds generalise the margin-based bounds due to Bartlett et al. (2006). The proof uses Taylor's theorem and leads to new representations for loss and regret and a simple proof of the integral representation of proper losses. We also present a different formulation of a duality result of Bregman divergences which leads to a simple demonstration of the convexity of composite losses using canonical link functions.

#index 1211803
#* Learning structurally consistent undirected probabilistic graphical models
#@ Sushmita Roy;Terran Lane;Margaret Werner-Washburne
#t 2009
#c 19
#% 115608
#% 272520
#% 722754
#% 961197
#% 1060829
#% 1269485
#% 1269873
#% 1650289
#! In many real-world domains, undirected graphical models such as Markov random fields provide a more natural representation of the statistical dependency structure than directed graphical models. Unfortunately, structure learning of undirected graphs using likelihood-based scores remains difficult because of the intractability of computing the partition function. We describe a new Markov random field structure learning algorithm, motivated by canonical parameterization of Abbeel et al. We provide computational improvements on their parameterization by learning per-variable canonical factors, which makes our algorithm suitable for domains with hundreds of nodes. We compare our algorithm against several algorithms for learning undirected and directed models on simulated and real datasets from biology. Our algorithm frequently outperforms existing algorithms, producing higher-quality structures, suggesting that enforcing consistency during structure learning is beneficial for learning undirected graphs.

#index 1211804
#* Ranking interesting subgroups
#@ Stefan Rueping
#t 2009
#c 19
#% 232126
#% 546694
#% 577224
#% 768667
#% 799765
#% 823351
#% 961134
#% 1130811
#% 1289454
#! Subgroup discovery is the task of identifying the top k patterns in a database with most significant deviation in the distribution of a target attribute Y. Subgroup discovery is a popular approach for identifying interesting patterns in data, because it combines statistical significance with an understandable representation of patterns as a logical formula. However, it is often a problem that some subgroups, even if they are statistically highly significant, are not interesting to the user. We present an approach based on the work on ranking Support Vector Machines that ranks subgroups with respect to the user's concept of interestingness, and finds more interesting subgroups. This approach can significantly increase the quality of the subgroups.

#index 1211805
#* Function factorization using warped Gaussian processes
#@ Mikkel N. Schmidt
#t 2009
#c 19
#% 891549
#% 1070136
#% 1073872
#! We introduce a new approach to non-linear regression called function factorization, that is suitable for problems where an output variable can reasonably be modeled by a number of multiplicative interaction terms between non-linear functions of the inputs. The idea is to approximate a complicated function on a high-dimensional space by the sum of products of simpler functions on lower-dimensional subspaces. Function factorization can be seen as a generalization of matrix and tensor factorization methods, in which the data are approximated by the sum of outer products of vectors. We present a non-parametric Bayesian approach to function factorization where the priors over the factorizing functions are warped Gaussian processes, and we do inference using Hamiltonian Markov chain Monte Carlo. We demonstrate the superior predictive performance of the method on a food science data set compared to Gaussian process regression and tensor factorization using PARAFAC and GEMANOVA models.

#index 1211806
#* Stochastic methods for l1 regularized loss minimization
#@ Shai Shalev-Shwartz;Ambuj Tewari
#t 2009
#c 19
#% 131165
#% 227736
#% 309208
#% 420507
#% 425020
#% 451055
#% 725437
#% 818171
#% 983905
#% 1014657
#% 1039678
#% 1073906
#% 1073988
#% 1814726
#% 1845612
#! We describe and analyze two stochastic methods for l1 regularized loss minimization problems, such as the Lasso. The first method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature/example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.

#index 1211807
#* Structure preserving embedding
#@ Blake Shaw;Tony Jebara
#t 2009
#c 19
#% 385505
#% 593047
#% 765247
#% 1073910
#! Structure Preserving Embedding (SPE) is an algorithm for embedding graphs in Euclidean space such that the embedding is low-dimensional and preserves the global topological properties of the input graph. Topology is preserved if a connectivity algorithm, such as k-nearest neighbors, can easily recover the edges of the input graph from only the coordinates of the nodes after embedding. SPE is formulated as a semidefinite program that learns a low-rank kernel matrix constrained by a set of linear inequalities which captures the connectivity structure of the input graph. Traditional graph embedding algorithms do not preserve structure according to our definition, and thus the resulting visualizations can be misleading or less informative. SPE provides significant improvements in terms of visualization and lossless compression of graphs, outperforming popular methods such as spectral embedding and Laplacian eigen-maps. We find that many classical graphs and networks can be properly embedded using only a few dimensions. Furthermore, introducing structure preserving constraints into dimensionality reduction algorithms produces more accurate representations of high-dimensional data.

#index 1211808
#* Monte-Carlo simulation balancing
#@ David Silver;Gerald Tesauro
#t 2009
#c 19
#% 124687
#% 283236
#% 348585
#% 983838
#% 1270060
#% 1274923
#% 1665148
#! In this paper we introduce the first algorithms for efficiently learning a simulation policy for Monte-Carlo search. Our main idea is to optimise the balance of a simulation policy, so that an accurate spread of simulation outcomes is maintained, rather than optimising the direct strength of the simulation policy. We develop two algorithms for balancing a simulation policy by gradient descent. The first algorithm optimises the balance of complete simulations, using a policy gradient algorithm; whereas the second algorithm optimises the balance over every two steps of simulation. We compare our algorithms to reinforcement learning and supervised learning algorithms for maximising the strength of the simulation policy. We test each algorithm in the domain of 5 x 5 and 6 x 6 Computer Go, using a softmax policy that is parameterised by weights for a hundred simple patterns. When used in a simple Monte-Carlo search, the policies learnt by simulation balancing achieved significantly better performance, with half the mean squared error of a uniform random policy, and similar overall performance to a sophisticated Go engine.

#index 1211809
#* Uncertainty sampling and transductive experimental design for active dual supervision
#@ Vikas Sindhwani;Prem Melville;Richard D. Lawrence
#t 2009
#c 19
#% 722797
#% 799753
#% 876080
#% 987202
#% 1014679
#% 1074125
#% 1197791
#% 1214749
#% 1220999
#! Dual supervision refers to the general setting of learning from both labeled examples as well as labeled features. Labeled features are naturally available in tasks such as text classification where it is frequently possible to provide domain knowledge in the form of words that associate strongly with a class. In this paper, we consider the novel problem of active dual supervision, or, how to optimally query an example and feature labeling oracle to simultaneously collect two different forms of supervision, with the objective of building the best classifier in the most cost effective manner. We apply classical uncertainty and experimental design based active learning schemes to graph/kernel-based dual supervision models. Empirical studies confirm the potential of these schemes to significantly reduce the cost of acquiring labeled data for training high-quality models.

#index 1211810
#* Hilbert space embeddings of conditional distributions with applications to dynamical systems
#@ Le Song;Jonathan Huang;Alex Smola;Kenji Fukumizu
#t 2009
#c 19
#% 763698
#% 788036
#% 829043
#% 840854
#% 891549
#% 945186
#% 1073969
#% 1758781
#! In this paper, we extend the Hilbert space embedding approach to handle conditional distributions. We derive a kernel estimate for the conditional embedding, and show its connection to ordinary embeddings. Conditional embeddings largely extend our ability to manipulate distributions in Hilbert spaces, and as an example, we derive a nonparametric method for modeling dynamical systems where the belief state of the system is maintained as a conditional embedding. Our method is very general in terms of both the domains and the types of distributions that it can handle, and we demonstrate the effectiveness of our method in various dynamical systems. We expect that conditional embeddings will have wider applications beyond modeling dynamical systems.

#index 1211811
#* Multi-assignment clustering for Boolean data
#@ Andreas P. Streich;Mario Frank;David Basin;Joachim M. Buhmann
#t 2009
#c 19
#% 44876
#% 152934
#% 342328
#% 577514
#% 763861
#% 978023
#% 1052927
#% 1064394
#% 1066243
#% 1128848
#% 1663644
#! Conventional clustering methods typically assume that each data item belongs to a single cluster. This assumption does not hold in general. In order to overcome this limitation, we propose a generative method for clustering vectorial data, where each object can be assigned to multiple clusters. Using a deterministic annealing scheme, our method decomposes the observed data into the contributions of individual clusters and infers their parameters. Experiments on synthetic Boolean data show that our method achieves higher accuracy in the source parameter estimation and superior cluster stability compared to state-of-the-art approaches. We also apply our method to an important problem in computer security known as role mining. Experiments on real-world access control data show performance gains in generalization to new employees against other multi-assignment methods. In challenging situations with high noise levels, our approach maintains its good performance, while alternative state-of-the-art techniques lack robustness.

#index 1211812
#* A least squares formulation for a class of generalized eigenvalue problems in machine learning
#@ Liang Sun;Shuiwang Ji;Jieping Ye
#t 2009
#c 19
#% 317525
#% 393059
#% 443790
#% 465754
#% 875947
#% 891559
#% 961218
#% 983940
#% 1074000
#% 1083698
#% 1164191
#% 1298976
#% 1742155
#! Many machine learning algorithms can be formulated as a generalized eigenvalue problem. One major limitation of such formulation is that the generalized eigenvalue problem is computationally expensive to solve especially for large-scale problems. In this paper, we show that under a mild condition, a class of generalized eigenvalue problems in machine learning can be formulated as a least squares problem. This class of problems include classical techniques such as Canonical Correlation Analysis (CCA), Partial Least Squares (PLS), and Linear Discriminant Analysis (LDA), as well as Hypergraph Spectral Learning (HSL). As a result, various regularization techniques can be readily incorporated into the formulation to improve model sparsity and generalization ability. In addition, the least squares formulation leads to efficient and scalable implementations based on the iterative conjugate gradient type algorithms. We report experimental results that confirm the established equivalence relationship. Results also demonstrate the efficiency and effectiveness of the equivalent least squares formulations on large-scale problems.

#index 1211813
#* A simpler unified analysis of budget perceptrons
#@ Ilya Sutskever
#t 2009
#c 19
#% 197394
#% 795802
#% 813744
#% 871302
#% 961152
#% 983905
#% 1000326
#% 1042610
#% 1073988
#% 1080960
#% 1705518
#! The kernel Perceptron is an appealing online learning algorithm that has a drawback: whenever it makes an error it must increase its support set, which slows training and testing if the number of errors is large. The Forgetron and the Randomized Budget Perceptron algorithms overcome this problem by restricting the number of support vectors the Perceptron is allowed to have. These algorithms have regret bounds whose proofs are dissimilar. In this paper we propose a unified analysis of both of these algorithms by observing that the way in which they remove support vectors can be seen as types of L2-regularization. By casting these algorithms as instances of online convex optimization problems and applying a variant of Zinkevich's theorem for noisy and incorrect gradient, we can bound the regret of these algorithms more easily than before. Our bounds are similar to the existing ones, but the proofs are less technical.

#index 1211814
#* Fast gradient-descent methods for temporal-difference learning with linear function approximation
#@ Richard S. Sutton;Hamid Reza Maei;Doina Precup;Shalabh Bhatnagar;David Silver;Csaba Szepesvári;Eric Wiewiora
#t 2009
#c 19
#% 61475
#% 124695
#% 203596
#% 226878
#% 286423
#% 299643
#% 425076
#% 449561
#% 464438
#% 1046338
#% 1250563
#% 1274923
#% 1404139
#! Sutton, Szepesvári and Maei (2009) recently introduced the first temporal-difference learning algorithm compatible with both linear function approximation and off-policy training, and whose complexity scales only linearly in the size of the function approximator. Although their gradient temporal difference (GTD) algorithm converges reliably, it can be very slow compared to conventional linear TD (on on-policy problems where TD is convergent), calling into question its practical utility. In this paper we introduce two new related algorithms with better convergence rates. The first algorithm, GTD2, is derived and proved convergent just as GTD was, but uses a different objective function and converges significantly faster (but still not as fast as conventional TD). The second new algorithm, linear TD with gradient correction, or TDC, uses the same update rule as conventional TD except for an additional term which is initially zero. In our experiments on small test problems and in a Computer Go application with a million features, the learning rate of this algorithm was comparable to that of conventional TD. This algorithm appears to extend linear TD to off-policy learning with no penalty in performance while only doubling computational requirements.

#index 1211815
#* Optimistic initialization and greediness lead to polynomial time learning in factored MDPs
#@ István Szita;András Lőrincz
#t 2009
#c 19
#% 314843
#% 466075
#% 495933
#% 527994
#% 578696
#% 840942
#% 1074003
#% 1180828
#% 1211815
#% 1269772
#% 1272002
#% 1289278
#% 1290041
#! In this paper we propose an algorithm for polynomial-time reinforcement learning in factored Markov decision processes (FMDPs). The factored optimistic initial model (FOIM) algorithm, maintains an empirical model of the FMDP in a conventional way, and always follows a greedy policy with respect to its model. The only trick of the algorithm is that the model is initialized optimistically. We prove that with suitable initialization (i) FOIM converges to the fixed point of approximate value iteration (AVI); (ii) the number of steps when the agent makes non-near-optimal decisions (with respect to the solution of AVI) is polynomial in all relevant quantities; (iii) the per-step costs of the algorithm are also polynomial. To our best knowledge, FOIM is the first algorithm with these properties.

#index 1211816
#* Discriminative k-metrics
#@ Arthur Szlam;Guillermo Sapiro
#t 2009
#c 19
#% 308899
#% 345850
#% 884187
#% 983830
#% 1073939
#% 1759275
#! The k q-flats algorithm is a generalization of the popular k-means algorithm where q dimensional best fit affine sets replace centroids as the cluster prototypes. In this work, a modification of the k q-flats framework for pattern classification is introduced. The basic idea is to replace the original reconstruction only energy, which is optimized to obtain the k affine spaces, by a new energy that incorporates discriminative terms. This way, the actual classification task is introduced as part of the design and optimization. The presentation of the proposed framework is complemented with experimental results, showing that the method is computationally very efficient and gives excellent results on standard supervised learning benchmarks.

#index 1211817
#* Kernelized value function approximation for reinforcement learning
#@ Gavin Taylor;Ronald Parr
#t 2009
#c 19
#% 458685
#% 466235
#% 840860
#% 891559
#% 1073966
#% 1861993
#! A recent surge in research in kernelized approaches to reinforcement learning has sought to bring the benefits of kernelized machine learning techniques to reinforcement learning. Kernelized reinforcement learning techniques are fairly new and different authors have approached the topic with different assumptions and goals. Neither a unifying view nor an understanding of the pros and cons of different approaches has yet emerged. In this paper, we offer a unifying view of the different approaches to kernelized value function approximation for reinforcement learning. We show that, except for different approaches to regularization, Kernelized LSTD (KLSTD) is equivalent to a modelbased approach that uses kernelized regression to find an approximate reward and transition model, and that Gaussian Process Temporal Difference learning (GPTD) returns a mean value function that is equivalent to these other approaches. We also discuss the relationship between our modelbased approach and the earlier Gaussian Processes in Reinforcement Learning (GPRL). Finally, we decompose the Bellman error into the sum of transition error and reward error terms, and demonstrate through experiments that this decomposition can be helpful in choosing regularization parameters.

#index 1211818
#* Factored conditional restricted Boltzmann Machines for modeling motion style
#@ Graham W. Taylor;Geoffrey E. Hinton
#t 2009
#c 19
#% 92145
#% 308505
#% 450888
#% 816039
#% 891060
#% 983903
#% 983926
#% 1073981
#% 1074005
#% 1527440
#! The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factor the three-way weight tensor implied by the multiplicative model, reducing the number of parameters from O(N3) to O(N2). The result is an efficient, compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM, our model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly among them.

#index 1211819
#* Using fast weights to improve persistent contrastive divergence
#@ Tijmen Tieleman;Geoffrey Hinton
#t 2009
#c 19
#% 130878
#% 277483
#% 303620
#% 450888
#% 492962
#% 891060
#% 1073939
#% 1074005
#% 1502411
#! The most commonly used learning algorithm for restricted Boltzmann machines is contrastive divergence which starts a Markov chain at a data point and runs the chain for only a few iterations to get a cheap, low variance estimate of the sufficient statistics under the model. Tieleman (2008) showed that better learning can be achieved by estimating the model's statistics using a small set of persistent "fantasy particles" that are not reinitialized to data points after each weight update. With sufficiently small weight updates, the fantasy particles represent the equilibrium distribution accurately but to explain why the method works with much larger weight updates it is necessary to consider the interaction between the weight updates and the Markov chain. We show that the weight updates force the Markov chain to mix fast, and using this insight we develop an even faster mixing chain that uses an auxiliary set of "fast weights" to implement a temporary overlay on the energy landscape. The fast weights learn rapidly but also decay rapidly and do not contribute to the normal energy landscape that defines the model.

#index 1211820
#* Structure learning with independent non-identically distributed data
#@ Robert E. Tillman
#t 2009
#c 19
#% 297171
#% 650939
#% 722900
#% 961266
#! There are well known algorithms for learning the structure of directed and undirected graphical models from data, but nearly all assume that the data consists of a single i.i.d. sample. In contexts such as fMRI analysis, data may consist of an ensemble of independent samples from a common data generating mechanism which may not have identical distributions. Pooling such data can result in a number of well known statistical problems so each sample must be analyzed individually, which offers no increase in power due to the presence of multiple samples. We show how existing constraint based methods can be modified to learn structure from the aggregate of such data in a statistically sound manner. The prescribed method is simple to implement and based on existing statistical methods employed in metaanalysis and other areas, but works surprisingly well in this context where there are increased concerns due to issues such as retesting. We report results for directed models, but the method given is just as applicable to undirected models.

#index 1211821
#* Robot trajectory optimization using approximate inference
#@ Marc Toussaint
#t 2009
#c 19
#% 46437
#% 130135
#% 225838
#% 528330
#% 715096
#% 716892
#! The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, reproduces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot.

#index 1211822
#* Ranking with ordered weighted pairwise classification
#@ Nicolas Usunier;David Buffoni;Patrick Gallinari
#t 2009
#c 19
#% 40313
#% 577224
#% 722816
#% 734915
#% 829043
#% 879588
#% 983815
#% 983820
#% 987226
#% 987241
#% 1035577
#% 1073936
#% 1074064
#% 1077150
#% 1674802
#! In ranking with the pairwise classification approach, the loss associated to a predicted ranked list is the mean of the pairwise classification losses. This loss is inadequate for tasks like information retrieval where we prefer ranked lists with high precision on the top of the list. We propose to optimize a larger class of loss functions for ranking, based on an ordered weighted average (OWA) (Yager, 1988) of the classification losses. Convex OWA aggregation operators range from the max to the mean depending on their weights, and can be used to focus on the top ranked elements as they give more weight to the largest losses. When aggregating hinge losses, the optimization problem is similar to the SVM for interdependent output spaces. Moreover, we show that OWA aggregates of margin-based classification losses have good generalization properties. Experiments on the Letor 3.0 benchmark dataset for information retrieval validate our approach.

#index 1211823
#* More generality in efficient multiple kernel learning
#@ Manik Varma;Bodla Rakesh Babu
#t 2009
#c 19
#% 349215
#% 425040
#% 763697
#% 769930
#% 770846
#% 829029
#% 883371
#% 961190
#% 983808
#% 983822
#% 983907
#% 983953
#% 1705523
#! Recent advances in Multiple Kernel Learning (MKL) have positioned it as an attractive tool for tackling many supervised learning tasks. The development of efficient gradient descent based optimization schemes has made it possible to tackle large scale problems. Simultaneously, MKL based algorithms have achieved very good results on challenging real world applications. Yet, despite their successes, MKL approaches are limited in that they focus on learning a linear combination of given base kernels. In this paper, we observe that existing MKL formulations can be extended to learn general kernel combinations subject to general regularization. This can be achieved while retaining all the efficiency of existing large scale optimization algorithms. To highlight the advantages of generalized kernel learning, we tackle feature selection problems on benchmark vision and UCI databases. It is demonstrated that the proposed formulation can lead to better results not only as compared to traditional MKL but also as compared to state-of-the-art wrapper and filter methods for feature selection.

#index 1211824
#* Information theoretic measures for clusterings comparison: is a correction for chance necessary?
#@ Nguyen Xuan Vinh;Julien Epps;James Bailey
#t 2009
#c 19
#% 115608
#% 579655
#% 722902
#% 829039
#% 840907
#% 1041370
#% 1254952
#! Information theoretic based measures form a fundamental class of similarity measures for comparing clusterings, beside the class of pair-counting based and set-matching based measures. In this paper, we discuss the necessity of correction for chance for information theoretic based measures for clusterings comparison. We observe that the baseline for such measures, i.e. average value between random partitions of a data set, does not take on a constant value, and tends to have larger variation when the ratio between the number of data points and the number of clusters is small. This effect is similar in some other non-information theoretic based measures such as the well-known Rand Index. Assuming a hypergeometric model of randomness, we derive the analytical formula for the expected mutual information value between a pair of clusterings, and then propose the adjusted version for several popular information theoretic based measures. Some examples are given to demonstrate the need and usefulness of the adjusted measures.

#index 1211825
#* Model-free reinforcement learning as mixture learning
#@ Nikos Vlassis;Marc Toussaint
#t 2009
#c 19
#% 225838
#% 266287
#% 277483
#% 384911
#% 393786
#% 466069
#% 564257
#% 722889
#% 842579
#% 876063
#% 1034794
#% 1073955
#% 1275174
#% 1342587
#% 1650588
#! We cast model-free reinforcement learning as the problem of maximizing the likelihood of a probabilistic mixture model via sampling, addressing both the infinite and finite horizon cases. We describe a Stochastic Approximation EM algorithm for likelihood maximization that, in the tabular case, is equivalent to a non-bootstrapping optimistic policy iteration algorithm like Sarsa(1) that can be applied both in MDPs and POMDPs. On the theoretical side, by relating the proposed stochastic EM algorithm to the family of optimistic policy iteration algorithms, we provide new tools that permit the design and analysis of algorithms in that family. On the practical side, preliminary experiments on a POMDP problem demonstrated encouraging results.

#index 1211826
#* BoltzRank: learning to maximize expected ranking gain
#@ Maksims N. Volkovs;Richard S. Zemel
#t 2009
#c 19
#% 309095
#% 387427
#% 734915
#% 840846
#% 983820
#% 987240
#% 987241
#% 1035577
#! Ranking a set of retrieved documents according to their relevance to a query is a popular problem in information retrieval. Methods that learn ranking functions are difficult to optimize, as ranking performance is typically judged by metrics that are not smooth. In this paper we propose a new listwise approach to learning to rank. Our method creates a conditional probability distribution over rankings assigned to documents for a given query, which permits gradient ascent optimization of the expected value of some performance measure. The rank probabilities take the form of a Boltzmann distribution, based on an energy function that depends on a scoring function composed of individual and pairwise potentials. Including pairwise potentials is a novel contribution, allowing the model to encode regularities in the relative scores of documents; existing models assign scores at test time based only on individual documents, with no pairwise constraints between documents. Experimental results on the LETOR3.0 data set show that our method out-performs existing learning approaches to ranking.

#index 1211827
#* K-means in space: a radiation sensitivity evaluation
#@ Kiri L. Wagstaff;Benjamin Bornstein
#t 2009
#c 19
#% 88219
#% 280463
#% 282954
#% 299535
#% 443984
#% 466083
#% 963195
#% 989098
#% 1800248
#! Spacecraft increasingly employ onboard data analysis to inform further data collection and prioritization decisions. However, many spacecraft operate in high-radiation environments in which the reliability of dataintensive computation is not known. This paper presents the first study of radiation sensitivity for k-means clustering. Our key findings are 1) k-means data structures differ in sensitivity, which is not determined solely by the amount of memory exposed; 2) no special radiation protection is needed below a data-set-dependent radiation threshold, enabling the use of faster, smaller, and cheaper onboard memory; and 3) subsampling improves radiation tolerance slightly, but the use of kd-trees unfortunately reduces tolerance. Our conclusions can help tailor k-means for use in future high-radiation environments.

#index 1211828
#* Evaluation methods for topic models
#@ Hanna M. Wallach;Iain Murray;Ruslan Salakhutdinov;David Mimno
#t 2009
#c 19
#% 424845
#% 722904
#% 788094
#% 876017
#% 876067
#! A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.

#index 1211829
#* Feature hashing for large scale multitask learning
#@ Kilian Weinberger;Anirban Dasgupta;John Langford;Alex Smola;Josh Attenberg
#t 2009
#c 19
#% 479973
#% 643568
#! Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case --- multitask learning with hundreds of thousands of tasks.

#index 1211830
#* Herding dynamical weights to learn
#@ Max Welling
#t 2009
#c 19
#% 130878
#% 237646
#% 276509
#% 281756
#% 443994
#% 450888
#% 577995
#% 788083
#% 829017
#% 1074005
#! A new "herding" algorithm is proposed which directly converts observed moments into a sequence of pseudo-samples. The pseudo-samples respect the moment constraints and may be used to estimate (unobserved) quantities of interest. The procedure allows us to sidestep the usual approach of first learning a joint model (which is intractable) and then sampling from that model (which can easily get stuck in a local mode). Moreover, the algorithm is fully deterministic, avoiding random number generation) and does not need expensive operations such as exponentiation.

#index 1211831
#* A stochastic memoizer for sequence data
#@ Frank Wood;Cédric Archambeau;Jan Gasthaus;Lancelot James;Yee Whye Teh
#t 2009
#c 19
#% 722928
#% 939624
#% 1116726
#! We propose an unbounded-depth, hierarchical, Bayesian nonparametric model for discrete sequence data. This model can be estimated from a single training sequence, yet shares statistical strength between subsequent symbol predictive distributions in such a way that predictive performance generalizes well. The model builds on a specific parameterization of an unbounded-depth hierarchical Pitman-Yor process. We introduce analytic marginalization steps (using coagulation operators) to reduce this model to one that can be represented in time and space linear in the length of the training sequence. We show how to perform inference in such a model without truncation approximation and introduce fragmentation operators necessary to do predictive inference. We demonstrate the sequence memoizer by using it as a language model, achieving state-of-the-art results.

#index 1211832
#* Optimal reverse prediction: a unified perspective on supervised, unsupervised and semi-supervised learning
#@ Linli Xu;Martha White;Dale Schuurmans
#t 2009
#c 19
#% 252011
#% 304931
#% 313959
#% 466263
#% 769935
#% 770830
#% 828246
#% 840938
#% 891559
#% 939375
#% 961218
#% 983417
#% 1156095
#% 1705508
#! Training principles for unsupervised learning are often derived from motivations that appear to be independent of supervised learning. In this paper we present a simple unification of several supervised and unsupervised training principles through the concept of optimal reverse prediction: predict the inputs from the target labels, optimizing both over model parameters and any missing labels. In particular, we show how supervised least squares, principal components analysis, k-means clustering and normalized graph-cut can all be expressed as instances of the same training principle. Natural forms of semi-supervised regression and classification are then automatically derived, yielding semi-supervised learning algorithms for regression and classification that, surprisingly, are novel and refine the state of the art. These algorithms can all be combined with standard regularizers and made non-linear via kernels.

#index 1211833
#* Non-monotonic feature selection
#@ Zenglin Xu;Rong Jin;Jieping Ye;Michael R. Lyu;Irwin King
#t 2009
#c 19
#% 310498
#% 361100
#% 425048
#% 466084
#% 722929
#% 722938
#% 722943
#% 763697
#% 770846
#% 770857
#% 846429
#% 872759
#% 961190
#% 983822
#% 983901
#% 983907
#! We consider the problem of selecting a subset of m most informative features where m is the number of required features. This feature selection problem is essentially a combinatorial optimization problem, and is usually solved by an approximation. Conventional feature selection methods address the computational challenge in two steps: (a) ranking all the features by certain scores that are usually computed independently from the number of specified features m, and (b) selecting the top m ranked features. One major shortcoming of these approaches is that if a feature f is chosen when the number of specified features is m, it will always be chosen when the number of specified features is larger than m. We refer to this property as the "monotonic" property of feature selection. In this work, we argue that it is important to develop efficient algorithms for non-monotonic feature selection. To this end, we develop an algorithm for non-monotonic feature selection that approximates the related combinatorial optimization problem by a Multiple Kernel Learning (MKL) problem. We also present a strategy that derives a discrete solution from the approximate solution of MKL, and show the performance guarantee for the derived discrete solution when compared to the global optimal solution for the related combinatorial optimization problem. An empirical study with a number of benchmark data sets indicates the promising performance of the proposed framework compared with several state-of-the-art approaches for feature selection.

#index 1211834
#* Online learning by ellipsoid method
#@ Liu Yang;Rong Jin;Jieping Ye
#t 2009
#c 19
#% 67056
#% 227736
#% 304824
#% 425020
#% 425046
#% 722814
#% 722816
#% 722903
#% 840873
#% 875984
#% 961152
#% 983806
#% 1674790
#% 1759695
#! In this work, we extend the ellipsoid method, which was originally designed for convex optimization, for online learning. The key idea is to approximate by an ellipsoid the classification hypotheses that are consistent with all the training examples received so far. This is in contrast to most online learning algorithms where only a single classifier is maintained at each iteration. Efficient algorithms are presented for updating both the centroid and the positive definite matrix of ellipsoid given a misclassified example. In addition to the classical ellipsoid method, an improved version for online learning is also presented. Mistake bounds for both ellipsoid methods are derived. Evaluation with the USPS dataset and three UCI data-sets shows encouraging results when comparing the proposed online learning algorithm to two state-of-the-art online learners.

#index 1211835
#* Stochastic search using the natural gradient
#@ Sun Yi;Daan Wierstra;Tom Schaul;Jürgen Schmidhuber
#t 2009
#c 19
#% 203594
#% 235332
#% 258937
#% 449980
#% 846487
#% 1025316
#% 1044118
#% 1225776
#% 1665183
#! To optimize unknown 'fitness' functions, we present Natural Evolution Strategies, a novel algorithm that constitutes a principled alternative to standard stochastic search methods. It maintains a multinormal distribution on the set of solution candidates. The Natural Gradient is used to update the distribution's parameters in the direction of higher expected fitness, by efficiently calculating the inverse of the exact Fisher information matrix whereas previous methods had to use approximations. Other novel aspects of our method include optimal fitness baselines and importance mixing, a procedure adjusting batches with minimal numbers of fitness evaluations. The algorithm yields competitive results on a number of benchmarks.

#index 1211836
#* Learning structural SVMs with latent variables
#@ Chun-Nam John Yu;Thorsten Joachims
#t 2009
#c 19
#% 71176
#% 196811
#% 576520
#% 577224
#% 770763
#% 815329
#% 815876
#% 840862
#% 875970
#% 884049
#% 983820
#% 983952
#% 1126419
#% 1264133
#! We present a large-margin formulation and algorithm for structured output prediction that allows the use of latent variables. Our proposal covers a large range of application problems, with an optimization problem that can be solved efficiently using Concave-Convex Programming. The generality and performance of the approach is demonstrated through three applications including motiffinding, noun-phrase coreference resolution, and optimizing precision at k in information retrieval.

#index 1211837
#* Piecewise-stationary bandit problems with side observations
#@ Jia Yuan Yu;Shie Mannor
#t 2009
#c 19
#% 165663
#% 266792
#% 416988
#% 425053
#% 871302
#% 961172
#! We consider a sequential decision problem where the rewards are generated by a piecewise-stationary distribution. However, the different reward distributions are unknown and may change at unknown instants. Our approach uses a limited number of side observations on past rewards, but does not require prior knowledge of the frequency of changes. In spite of the adversarial nature of the reward process, we provide an algorithm whose regret, with respect to the baseline with perfect knowledge of the distributions and the changes, is O(k log(T)), where k is the number of changes up to time T. This is in contrast to the case where side observations are not available, and where the regret is at least Ω(√T).

#index 1211838
#* Large-scale collaborative prediction using a nonparametric random effects model
#@ Kai Yu;John Lafferty;Shenghuo Zhu;Yihong Gong
#t 2009
#c 19
#% 770804
#% 840924
#% 840962
#% 876081
#% 983903
#% 1073982
#! A nonparametric model is introduced that allows multiple related regression tasks to take inputs from a common data space. Traditional transfer learning models can be inappropriate if the dependence among the outputs cannot be fully resolved by known input-specific and task-specific predictors. The proposed model treats such output responses as conditionally independent, given known predictors and appropriate unobserved random effects. The model is nonparametric in the sense that the dimensionality of random effects is not specified a priori but is instead determined from data. An approach to estimating the model is presented uses an EM algorithm that is efficient on a very large scale collaborative prediction problem. The obtained prediction accuracy is competitive with state-of-the-art results.

#index 1211839
#* Robust feature extraction via information theoretic learning
#@ Xiao-Tong Yuan;Bao-Gang Hu
#t 2009
#c 19
#% 80995
#% 722942
#% 875966
#% 889150
#% 913838
#% 940088
#% 961218
#% 1034713
#% 1761564
#! In this paper, we present a robust feature extraction framework based on information-theoretic learning. Its formulated objective aims at simultaneously maximizing the Renyi's quadratic information potential of features and the Renyi's cross information potential between features and class labels. This objective function reaps the advantages in robustness from both redescending M-estimator and manifold regularization, and can be efficiently optimized via half-quadratic optimization in an iterative manner. In addition, the popular algorithms LPP, SRDA and LapRLS for feature extraction are all justified to be the special cases within this framework. Extensive comparison experiments on several real-world data sets, with contaminated features or labels, well validate the encouraging gain in algorithmic robustness from this proposed framework.

#index 1211840
#* Interactively optimizing information retrieval systems as a dueling bandits problem
#@ Yisong Yue;Thorsten Joachims
#t 2009
#c 19
#% 813744
#% 879567
#% 946521
#% 1073970
#% 1074025
#% 1074092
#% 1083633
#% 1130811
#% 1415710
#! We present an on-line learning framework tailored towards real-time learning from observed user behavior in search engines and other information retrieval systems. In particular, we only require pairwise comparisons which were shown to be reliably inferred from implicit feedback (Joachims et al., 2007; Radlinski et al., 2008b). We will present an algorithm with theoretical guarantees as well as simulation results.

#index 1211841
#* Compositional noisy-logical learning
#@ Alan Yuille;Songfeng Zheng
#t 2009
#c 19
#% 44876
#% 351459
#% 787639
#% 876039
#! We describe a new method for learning the conditional probability distribution of a binary-valued variable from labelled training examples. Our proposed Compositional Noisy-Logical Learning (CNLL) approach learns a noisy-logical distribution in a compositional manner. CNLL is an alternative to the well-known AdaBoost algorithm which performs coordinate descent on an alternative error measure. We describe two CNLL algorithms and test their performance compared to AdaBoost on two types of problem: (i) noisy-logical data (such as noisy exclusive-or), and (ii) four standard datasets from the UCI repository. Our results show that we outperform AdaBoost while using significantly fewer weak classifiers, thereby giving a more transparent classifier suitable for knowledge extraction.

#index 1211842
#* Discovering options from example trajectories
#@ Peng Zang;Peng Zhou;David Minnen;Charles Isbell
#t 2009
#c 19
#% 272662
#% 286423
#% 384911
#% 464303
#% 464607
#% 464636
#% 466066
#% 468476
#% 770777
#% 961214
#% 1073953
#! We present a novel technique for automated problem decomposition to address the problem of scalability in reinforcement learning. Our technique makes use of a set of near-optimal trajectories to discover options and incorporates them into the learning process, dramatically reducing the time it takes to solve the underlying problem. We run a series of experiments in two different domains and show that our method offers up to 30 fold speedup over the baseline.

#index 1211843
#* Learning instance specific distances using metric propagation
#@ De-Chuan Zhan;Ming Li;Yu-Feng Li;Zhi-Hua Zhou
#t 2009
#c 19
#% 836778
#% 915338
#% 961218
#% 983946
#% 1073944
#% 1781594
#! In many real-world applications, such as image retrieval, it would be natural to measure the distances from one instance to others using instance specific distance which captures the distinctions from the perspective of the concerned instance. However, there is no complete framework for learning instance specific distances since existing methods are incapable of learning such distances for test instance and unlabeled data. In this paper, we propose the Isd method to address this issue. The key of Isd is metric propagation, that is, propagating and adapting metrics of individual labeled examples to individual unlabeled instances. We formulate the problem into a convex optimization framework and derive efficient solutions. Experiments show that Isd can effectively learn instance specific distances for labeled as well as unlabeled instances. The metric propagation scheme can also be used in other scenarios.

#index 1211844
#* Prototype vector machine for large scale semi-supervised learning
#@ Kai Zhang;James T. Kwok;Bahram Parvin
#t 2009
#c 19
#% 269218
#% 466263
#% 564285
#% 840967
#% 961195
#% 961218
#% 1074026
#! Practical data mining rarely falls exactly into the supervised learning scenario. Rather, the growing amount of unlabeled data poses a big challenge to large-scale semi-supervised learning (SSL). We note that the computational intensiveness of graph-based SSL arises largely from the manifold or graph regularization, which in turn lead to large models that are difficult to handle. To alleviate this, we proposed the prototype vector machine (PVM), a highly scalable, graph-based algorithm for large-scale SSL. Our key innovation is the use of "prototypes vectors" for efficient approximation on both the graph-based regularizer and model representation. The choice of prototypes are grounded upon two important criteria: they not only perform effective low-rank approximation of the kernel matrix, but also span a model suffering the minimum information loss compared with the complete model. We demonstrate encouraging performance and appealing scaling properties of the PVM on a number of machine learning benchmark data sets.

#index 1211846
#* Learning non-redundant codebooks for classifying complex objects
#@ Wei Zhang;Akshat Surve;Xiaoli Fern;Thomas Dietterich
#t 2009
#c 19
#% 46803
#% 136350
#% 209021
#% 224755
#% 262059
#% 272995
#% 397139
#% 722930
#% 722934
#% 755467
#% 760805
#% 836746
#% 836905
#% 848115
#% 853308
#% 1035698
#% 1117008
#% 1137062
#% 1667696
#! Codebook-based representations are widely employed in the classification of complex objects such as images and documents. Most previous codebook-based methods construct a single codebook via clustering that maps a bag of low-level features into a fixed-length histogram that describes the distribution of these features. This paper describes a simple yet effective framework for learning multiple non-redundant codebooks that produces surprisingly good results. In this framework, each codebook is learned in sequence to extract discriminative information that was not captured by preceding codebooks and their corresponding classifiers. We apply this framework to two application domains: visual object categorization and document classification. Experiments on large classification tasks show substantial improvements in performance compared to a single codebook or codebooks learned in a bagging style.

#index 1211847
#* Multi-instance learning by treating instances as non-I.I.D. samples
#@ Zhi-Hua Zhou;Yu-Yin Sun;Yu-Feng Li
#t 2009
#c 19
#% 5182
#% 224755
#% 272527
#% 464436
#% 464621
#% 464633
#% 466927
#% 565537
#% 731607
#% 771844
#% 840842
#% 840922
#% 844291
#% 850166
#% 875969
#% 876033
#% 902511
#% 983950
#% 1074014
#% 1274898
#% 1397097
#! Previous studies on multi-instance learning typically treated instances in the bags as independently and identically distributed. The instances in a bag, however, are rarely independent in real tasks, and a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits relations among instances. In this paper, we propose two simple yet effective methods. In the first method, we explicitly map every bag to an undirected graph and design a graph kernel for distinguishing the positive and negative bags. In the second method, we implicitly construct graphs by deriving affinity matrices and propose an efficient graph kernel considering the clique information. The effectiveness of the proposed methods are validated by experiments.

#index 1211848
#* MedLDA: maximum margin supervised topic models for regression and classification
#@ Jun Zhu;Amr Ahmed;Eric P. Xing
#t 2009
#c 19
#% 269217
#% 642990
#% 722816
#% 722904
#% 768632
#% 1074029
#% 1250575
#! Supervised topic models utilize document's side information for discovering predictive low dimensional representations of documents; and existing models apply likelihood-based estimation. In this paper, we present a max-margin supervised topic model for both continuous and categorical response variables. Our approach, the maximum entropy discrimination latent Dirichlet allocation (MedLDA), utilizes the max-margin principle to train supervised topic models and estimate predictive topic representations that are arguably more suitable for prediction. We develop efficient variational methods for posterior inference and demonstrate qualitatively and quantitatively the advantages of MedLDA over likelihood-based topic models on movie review and 20 Newsgroups data sets.

#index 1211849
#* On primal and dual sparsity of Markov networks
#@ Jun Zhu;Eric P. Xing
#t 2009
#c 19
#% 464434
#% 721164
#% 722760
#% 770763
#% 1073906
#% 1074029
#% 1117681
#% 1385995
#! Sparsity is a desirable property in high dimensional learning. The l1-norm regularization can lead to primal sparsity, while max-margin methods achieve dual sparsity. Combining these two methods, an l1-norm max-margin Markov network (l1-M3N) can achieve both types of sparsity. This paper analyzes its connections to the Laplace max-margin Markov network (LapM3N), which inherits the dual sparsity of max-margin models but is pseudo-primal sparse, and to a novel adaptive M3N (AdapM3N). We show that the l1-M3N is an extreme case of the LapM3N, and the l1-M3N is equivalent to an AdapM3N. Based on this equivalence we develop a robust EM-style algorithm for learning an l1-M3N. We demonstrate the advantages of the simultaneously (pseudo-) primal and dual sparse models over the ones which enjoy either primal or dual sparsity on both synthetic and real data sets.

#index 1211850
#* SimpleNPKL: simple non-parametric kernel learning
#@ Jinfeng Zhuang;Ivor W. Tsang;Steven C. H. Hoi
#t 2009
#c 19
#% 228992
#% 464615
#% 757953
#% 763697
#% 840938
#% 841693
#% 875957
#% 876008
#% 881474
#% 961193
#% 983849
#% 1073922
#% 1074017
#! Previous studies of Non-Parametric Kernel (NPK) learning usually reduce to solving some Semi-Definite Programming (SDP) problem by a standard SDP solver. However, time complexity of standard interior-point SDP solvers could be as high as O(n6.5). Such intensive computation cost prohibits NPK learning applicable to real applications, even for data sets of moderate size. In this paper, we propose an efficient approach to NPK learning from side information, referred to as SimpleNPKL, which can efficiently learn non-parametric kernels from large sets of pairwise constraints. In particular, we show that the proposed SimpleNPKL with linear loss has a closed-form solution that can be simply computed by the Lanczos algorithm. Moreover, we show that the SimpleNPKL with square hinge loss can be re-formulated as a saddle-point optimization task, which can be further solved by a fast iterative algorithm. In contrast to the previous approaches, our empirical results show that our new technique achieves the same accuracy, but is significantly more efficient and scalable.

#index 1211851
#* Invited talk: Can learning kernels help performance?
#@ Corinna Cortes
#t 2009
#c 19

#index 1211852
#* Invited talk: Drifting games, boosting and online learning
#@ Yoav Freund
#t 2009
#c 19

#index 1211853
#* Workshop summary: Seventh annual workshop on Bayes applications
#@ John Mark Agosta;Russell Almond;Dennis Buede;Marek J. Druzdzel;Judy Goldsmith;Silja Renooij
#t 2009
#c 19

#index 1211855
#* Workshop summary: Automated interpretation and modelling of cell images
#@ Robert F. Murphy;Chun-Nan Hsu;Loris Nanni
#t 2009
#c 19

#index 1211856
#* Workshop summary: Workshop on learning feature hierarchies
#@ Kay Yu;Ruslan Salakhutdinov;Yann LeCun;Geoff Hinton;Yoshua Bengio
#t 2009
#c 19

#index 1211857
#* Workshop summary: Results of the 2009 reinforcement learning competition
#@ David Wingate;Carlos Diuk;Lihong Li;Matthew Taylor;Jordan Frank
#t 2009
#c 19

#index 1211859
#* Workshop summary: The fourth workshop on evaluation methods for machine learning
#@ Chris Drummond;Nathalie Japkowicz;William Klement;Sofus Macskassy
#t 2009
#c 19

#index 1211860
#* Workshop summary: On-line learning with limited feedback
#@ Jean-Yves Audibert;Peter Auer;Alessandro Lazaric;Remi Munos;Daniil Ryabko;Csaba Szepesvari
#t 2009
#c 19

#index 1211861
#* Workshop summary: Numerical mathematics in machine learning
#@ Matthias Seeger;Suvrit Sra;John P. Cunningham
#t 2009
#c 19

#index 1211863
#* Workshop summary: Abstraction in reinforcement learning
#@ Ozgur Simsek
#t 2009
#c 19

#index 1211864
#* Workshop summary: Sparse methods for music audio
#@ Douglas Eck;Dan Ellis;Philippe Hamel
#t 2009
#c 19

#index 1211865
#* Tutorial summary: Reductions in machine learning
#@ Alina Beygelzimer;John Langford;Bianca Zadrozny
#t 2009
#c 19

#index 1211866
#* Tutorial summary: Convergence of natural dynamics to equilibria
#@ Eyal Even-Dar;Vahab Mirrokni
#t 2009
#c 19

#index 1211867
#* Tutorial summary: Learning with dependencies between several response variables
#@ Volker Tresp;Kai Yu
#t 2009
#c 19

#index 1211868
#* Tutorial summary: Survey of boosting from an optimization perspective
#@ Manfred K. Warmuth;S.V.N. Vishwanathan
#t 2009
#c 19

#index 1211870
#* Tutorial summary: The neuroscience of reinforcement learning
#@ Yael Niv
#t 2009
#c 19

#index 1211871
#* Tutorial summary: Machine learning in IR: recent successes and new opportunities
#@ Paul Bennett;Misha Bilenko;Kevyn Collins-Thompson
#t 2009
#c 19

#index 1211872
#* Tutorial summary: Active learning
#@ Sanjoy Dasgupta;John Langford
#t 2009
#c 19

#index 1211873
#* Tutorial summary: Large social and information networks: opportunities for ML
#@ Jure Leskovec
#t 2009
#c 19

#index 1211874
#* Tutorial summary: Structured prediction for natural language processing
#@ Noah Smith
#t 2009
#c 19

