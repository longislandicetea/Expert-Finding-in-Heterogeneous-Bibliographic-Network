#index 1035567
#* Proceedings of the 2008 International Conference on Web Search and Data Mining
#@ Marc Najork;Andrei Broder;Soumen Chakrabarti
#t 2008
#c 2
#! WSDM was announced at WWW 2007 in Banff in May 2007 and thereafter on several electronic bulletin boards. Abstracts were sought by 30th July and full paper submissions by the 6th August. Despite the rather short notice and tight deadlines, we received 151 submissions from around the world. With the help of the steering committee we decided on novel reviewing system and a two-tier technical program committee was formed There were 52 regular program committee members. Each paper was first reviewed by at least three regular PC members. After this phase was completed, we retained about 60 papers with the highest scores for a second round of evaluation by a senior program committee with 11 members. Each retained paper was reviewed by two senior PC members, who strove to ensure that all regular PC members had a consistent view of the contributions of the paper (although their opinions could, of course, differ quantitatively) and had written clear, well-justified and useful reviews for the authors. In many cases, the senior PCs effectively made accept/reject decisions. The final decision was made by the PC chairs who took into account all the scores and comments, novelty, technical depth, elegance, practical application, impact, and presentation. Notifications of acceptance of 24 full papers were sent out on 20th October Overall, we are pleased with the quality and mix of the papers we accepted. Most are solidly practical papers with extensive experimental evaluation while a few are of a more theoretical nature, but we believe all of them have the potential to significantly influence the practice of Web search and mining in coming years. The acceptance ratio of 24/151 = 16 percent is consistent with the leading ACM and IEEE conferences in similar or related areas. For the first ever WSDM conference, we decided to have only a single track of full-length papers and not have short papers, poster papers, or demos, although this might change over time

#index 1035568
#* Web information management: past, present and future
#@ Hector Garcia-Molina
#t 2008
#c 2
#! In this talk I will give a brief retrospective on Web Information Management, and will discuss some of the key challenges for the future. I will not give a survey of all work in the area; instead I will give my personal perspective based on work in the InfoLab at Stanford. In particular, I will touch on our lab's work on crawling, indexing, ranking, personalization, and more recently on spam detection and social networking.

#index 1035569
#* Machine reading at web scale
#@ Oren Etzioni
#t 2008
#c 2
#% 830520
#% 1275182

#index 1035570
#* Crawl ordering by search impact
#@ Sandeep Pandey;Christopher Olston
#t 2008
#c 2
#% 268079
#% 268087
#% 281251
#% 300139
#% 330604
#% 330609
#% 341672
#% 348137
#% 577302
#% 577330
#% 754060
#% 754088
#% 785089
#% 805879
#% 813734
#% 869534
#% 928355
#% 956536
#! We study how to prioritize the fetching of new pages under the objective of maximizing the quality of search results. In particular, our objective is to fetch new pages that have the most impact, where the impact of a page is equal to the number of times the page appears in the top K search results for queries, for some constant K, e.g., K = 10. Since the impact of a page depends on its relevance score for queries, which in turn depends on the page content, the main difficulty lies in estimating the impact of the page before actually fetching it. Hence, impact must be estimated based on the limited information that is available prior to fetching page content, e.g., the URL string, number of in-links, referring anchortext We formally characterize this problem and study its hardness. We leverage our formalism to design a new impact-driven crawling policy, and demonstrate its effectiveness using real world data. Our technique ensures that the crawler acquires content relevant to "tail topics" that are obscure but of interest to some users, rather than just redundantly accumulating content on popular topics.

#index 1035571
#* On placing skips optimally in expectation
#@ Flavio Chierichetti;Silvio Lattanzi;Federico Mari;Alessandro Panconesi
#t 2008
#c 2
#% 69316
#% 140389
#% 213786
#% 1667821
#% 1739411
#! We study the problem of optimal skip placement in an inverted list. Assuming the query distribution to be known in advance, we formally prove that an optimal skip placement can be computed quite efficiently. Our best algorithm runs in time O (n log n), n being the length of the list. The placement is optimal in the sense that it minimizes the expected time to process a query. Our theoretical results are matched by experiments with a real corpus, showing that substantial savings can be obtained with respect to the traditional skip placement strategy, that of placing consecutive skips, each spanning √n many locations.

#index 1035572
#* Disorder inequality: a combinatorial approach to nearest neighbor search
#@ Navin Goyal;Yury Lifshits;Hinrich Schütze
#t 2008
#c 2
#% 220711
#% 232764
#% 249238
#% 249321
#% 279755
#% 281166
#% 281209
#% 300078
#% 342827
#% 347264
#% 415033
#% 452563
#% 654466
#% 731409
#% 749529
#% 763708
#% 765262
#% 787562
#% 799242
#% 818223
#% 858170
#% 866740
#% 866778
#% 869471
#% 869482
#% 875957
#% 879625
#% 898309
#% 956504
#% 983805
#% 1077150
#% 1407672
#% 1914424
#% 1914425
#! We say that an algorithm for nearest neighbor search is combinatorial if only direct comparisons between two pairwise similarity values are allowed. Combinatorial algorithms for nearest neighbor search have two important advantages: (1) they do not map similarity values to artificial distance values and do not use the triangle inequality for the latter, and (2) they work for arbitrarily complicated data representations and similarity functions. In this paper we introduce a special property of the similarity function on a set S that leads to efficient combinatorial algorithms for S. The disorder constant D(S) of a set S is defined to ensure the following inequality: if x is the a'th most similar object to z and y is the b'th most similar object to z, then x is among the D(S) (a + b) most similar objects to y. Assuming that disorder is small we present the first two known combinatorial algorithms for nearest neighbors whose query time has logarithmic dependence on the size of S. The first one, called Ranwalk, is a randomized zero-error algorithm that always returns the exact nearest neighbor. It uses space quadratic in the input size in preprocessing, but is very efficient in query processing. The second algorithm, called Arwalk, uses near-linear space. It uses random choices in preprocessing, but the query processing is essentially deterministic. For an arbitrary query q, there is only a small probability that the chosen data structure does not support q Finally, we show that for the Reuters corpus average disorder is indeed quite small and that Ranwalk efficiently computes the nearest neighbor in most cases.

#index 1035573
#* Beyond basic faceted search
#@ Ori Ben-Yitzhak;Nadav Golbandi;Nadav Har'El;Ronny Lempel;Andreas Neumann;Shila Ofek-Koifman;Dafna Sheinwald;Eugene Shekita;Benjamin Sznajder;Sivan Yogev
#t 2008
#c 2
#% 227868
#% 239162
#% 301234
#% 413120
#% 464215
#% 479450
#% 481951
#% 809250
#% 824732
#% 838540
#% 993998
#% 1688264
#% 1711122
#! This paper extends traditional faceted search to support richer information discovery tasks over more complex data models. Our first extension adds exible, dynamic business intelligence aggregations to the faceted application, enabling users to gain insight into their data that is far richer than just knowing the quantities of documents belonging to each facet. We see this capability as a step toward bringing OLAP capabilities, traditionally supported by databases over relational data, to the domain of free-text queries over metadata-rich content. Our second extension shows how one can efficiently extend a faceted search engine to support correlated facets - a more complex information model in which the values associated with a document across multiple facets are not independent. We show that by reducing the problem to a recently solved tree-indexing scenario, data with correlated facets can be efficiently indexed and retrieved

#index 1035574
#* Entropy of search logs: how hard is search? with personalization? with backoff?
#@ Qiaozhu Mei;Kenneth Church
#t 2008
#c 2
#% 268114
#% 298221
#% 340948
#% 419404
#% 420123
#% 577224
#% 577329
#% 643057
#% 740898
#% 748738
#% 754126
#% 807320
#% 818207
#% 818259
#% 818335
#% 823348
#% 869501
#% 881540
#% 918842
#% 943042
#% 943837
#% 1810646
#! How many pages are there on the Web? 5B? 20B? More? Less? Big bets on clusters in the clouds could be wiped out if a small cache of a few million urls could capture much of the value. Language modeling techniques are applied to MSN's search logs to estimate entropy. The perplexity is surprisingly small: millions, not billions. Entropy is a powerful tool for sizing challenges and opportunities. How hard is search? How hard are query suggestion mechanisms like auto-complete? How much does personalization help? All these difficult questions can be answered by estimation of entropy from search logs. What is the potential opportunity for personalization? In this paper, we propose a new way to personalize search, personalization with backoff. If we have relevant data for a particular user, we should use it. But if we don't, back off to larger and larger classes of similar users. As a proof of concept, we use the first few bytes of the IP address to define classes. The coefficients of each backoff class are estimated with an EM algorithm. Ideally, classes would be defined by market segments, demographics and surrogate variables such as time and geography

#index 1035575
#* Fast learning of document ranking functions with the committee perceptron
#@ Jonathan L. Elsas;Vitor R. Carvalho;Jaime G. Carbonell
#t 2008
#c 2
#% 92233
#% 268079
#% 269217
#% 302390
#% 340936
#% 387427
#% 411762
#% 425046
#% 466400
#% 577224
#% 722924
#% 734915
#% 750863
#% 766409
#% 815924
#% 818239
#% 823317
#% 840846
#% 879565
#% 879588
#% 879619
#% 881513
#% 907546
#% 907614
#% 976952
#% 987200
#% 987226
#! This paper presents a new variant of the perceptron algorithm using selective committee averaging (or voting). We apply this agorithm to the problem of learning ranking functions for document retrieval, known as the "Learning to Rank" problem. Most previous algorithms proposed to address this problem focus on minimizing the number of misranked document pairs in the training set. The committee perceptron algorithm improves upon existing solutions by biasing the final solution towards maximizing an arbitrary rank-based performance metrics. This method performs comparably or better than two state-of-the-art rank learning algorithms, and also provides significant training time improvements over those methods, showing over a 45-fold reduction in training time compared to ranking SVM

#index 1035576
#* Ranking web sites with real user traffic
#@ Mark R. Meiss;Filippo Menczer;Santo Fortunato;Alessandro Flammini;Alessandro Vespignani
#t 2008
#c 2
#% 186340
#% 268079
#% 290830
#% 308769
#% 309748
#% 309749
#% 330609
#% 399058
#% 400726
#% 402266
#% 438193
#% 503228
#% 565488
#% 641956
#% 754060
#% 805890
#% 805895
#% 807346
#% 869534
#% 879567
#% 956583
#% 967455
#! We analyze the traffic-weighted Web host graph obtained from a large sample of real Web users over about seven months. A number of interesting structural properties are revealed by this complex dynamic network, some in line with the well-studied boolean link host graph and others pointing to important differences. We find that while search is directly involved in a surprisingly small fraction of user clicks, it leads to a much larger fraction of all sites visited. The temporal traffic patterns display strong regularities, with a large portion of future requests being statistically predictable by past ones. Given the importance of topological measures such as PageRank in modeling user navigation, as well as their role in ranking sites for Web search, we use the traffic data to validate the PageRank random surfing model. The ranking obtained by the actual frequency with which a site is visited by users differs significantly from that approximated by the uniform surfing/teleportation behavior modeled by PageRank, especially for the most important sites. To interpret this finding, we consider each of the fundamental assumptions underlying PageRank and show how each is violated by actual user behavior

#index 1035577
#* SoftRank: optimizing non-smooth rank metrics
#@ Michael Taylor;John Guiver;Stephen Robertson;Tom Minka
#t 2008
#c 2
#% 309095
#% 476873
#% 577224
#% 783474
#% 829028
#% 840846
#% 879567
#% 907546
#% 983820
#! We address the problem of learning large complex ranking functions. Most IR applications use evaluation metrics that depend only upon the ranks of documents. However, most ranking functions generate document scores, which are sorted to produce a ranking. Hence IR metrics are innately non-smooth with respect to the scores, due to the sort. Unfortunately, many machine learning algorithms require the gradient of a training objective in order to perform the optimization of the model parameters,and because IR metrics are non-smooth,we need to find a smooth proxy objective that can be used for training. We present a new family of training objectives that are derived from the rank distributions of documents, induced by smoothed scores. We call this approach SoftRank. We focus on a smoothed approximation to Normalized Discounted Cumulative Gain (NDCG), called SoftNDCG and we compare it with three other training objectives in the recent literature. We present two main results. First, SoftRank yields a very good way of optimizing NDCG. Second, we show that it is possible to achieve state of the art test set NDCG results by optimizing a soft NDCG objective on the training set with a different discount function

#index 1035578
#* An experimental comparison of click position-bias models
#@ Nick Craswell;Onno Zoeter;Michael Taylor;Bill Ramsey
#t 2008
#c 2
#% 577224
#% 818221
#% 824716
#% 879565
#% 956546
#% 999292
#% 1250379
#% 1682438
#! Search engine click logs provide an invaluable source of relevance information, but this information is biased. A key source of bias is presentation order: the probability of click is influenced by a document's position in the results page. This paper focuses on explaining that bias, modelling how probability of click depends on position. We propose four simple hypotheses about how position bias might arise. We carry out a large data-gathering effort, where we perturb the ranking of a major search engine, to see how clicks are affected. We then explore which of the four hypotheses best explains the real-world position effects, and compare these to a simple logistic regression model. The data are not well explained by simple position models, where some users click indiscriminately on rank 1 or there is a simple decay of attention over ranks. A â聙聵cascade' model, where users view results from top to bottom and leave as soon as they see a worthwhile document, is our best explanation for position bias in early ranks

#index 1035579
#* A scalable pattern mining approach to web graph compression with communities
#@ Gregory Buehrer;Kumar Chellapilla
#t 2008
#c 2
#% 152934
#% 243166
#% 249110
#% 249321
#% 255137
#% 268079
#% 281214
#% 290830
#% 300120
#% 310514
#% 311808
#% 438553
#% 443393
#% 479973
#% 481290
#% 481779
#% 656242
#% 754117
#% 755402
#% 794132
#% 824711
#% 898311
#% 956540
#% 993960
#! A link server is a system designed to support efficient implementations of graph computations on the web graph. In this work, we present a compression scheme for the web graph specifically designed to accommodate community queries and other random access algorithms on link servers. We use a frequent pattern mining approach to extract meaningful connectivity formations. Our Virtual Node Miner achieves graph compression without sacrificing random access by generating virtual nodes from frequent itemsets in vertex adjacency lists. The mining phase guarantees scalability by bounding the pattern mining complexity to O(E log E). We facilitate global mining, relaxing the requirement for the graph to be sorted by URL, enabling discovery for both inter-domain as well as intra-domain patterns. As a consequence, the approach allows incremental graph updates. Further, it not only facilitates but can also expedite graph computations such as PageRank and local random walks by implementing them directly on the compressed graph. We demonstrate the effectiveness of the proposed approach on several publicly available large web graph data sets. Experimental results indicate that the proposed algorithm achieves a 10- to 15-fold compression on most real word web graph data sets

#index 1035580
#* Collaboration over time: characterizing and modeling network evolution
#@ Jian Huang;Ziming Zhuang;Jia Li;C. Lee Giles
#t 2008
#c 2
#% 190581
#% 614036
#% 771919
#% 823342
#% 824797
#% 881523
#% 915252
#% 918685
#% 955712
#% 1250581
#% 1663664
#! A formal type of scientific and academic collaboration is coauthorship which can be represented by a coauthorship network. Coauthorship networks are among some of the largest social networks and offer us the opportunity to study the mechanisms underlying large-scale real world networks. We construct such a network for the Computer Science field covering research collaborations from 1980 to 2005, based on a large dataset of 451,305 papers authored by 283,174 distinct researchers. By mining this network, we first present a comprehensive study of the network statistical properties for a longitudinal network at the overall network level as well as for the intermediate community level. Major observations are that the database community is the best connected while the AI community is the most assortative, and that the Computer Science field as a whole shows a collaboration pattern more similar to Mathematics than to Biology. Moreover, the small world phenomenon and the scale-free degree distribution accompany the growth of the network. To study the individual collaborations, we propose a novel stochastic model, Stochastic Poisson model with Optimization Tree (Spot)to efficiently predict any increment of collaboration based on the local neighborhood structure. Spot models the non-stationary Poisson process by maximizing the log-likelihood with a tree structure. Empirical results show that Spot outperforms Support Vector Regression by better fitting collaboration records and predicting the rate of collaboration

#index 1035581
#* Preferential behavior in online groups
#@ Lars Backstrom;Ravi Kumar;Cameron Marlow;Jasmine Novak;Andrew Tomkins
#t 2008
#c 2
#% 259567
#% 260769
#% 297541
#% 343142
#% 446941
#% 573896
#% 607948
#% 771919
#% 783835
#% 791903
#% 823403
#% 847218
#% 860115
#% 881460
#! Online communities in the form of message boards, listservs, and newsgroups continue to represent a considerable amount of the social activity on the Internet. Every year thousands of groups ourish while others decline into relative obscurity; likewise, millions of members join a new community every year, some of whom will come to manage or moderate the conversation while others simply sit by the sidelines and observe. These processes of group formation, growth, and dissolution are central in social science, and in an online venue they have ramifications for the design and development of community software In this paper we explore a large corpus of thriving online communities. These groups vary widely in size, moderation and privacy, and cover an equally diverse set of subject matter. We present a broad range of descriptive statistics of these groups. Using metadata from groups, members, and individual messages, we identify users who post and are replied-to frequently by multiple group members; we classify these high-engagement users based on the longevity of their engagements. We show that users who will go on to become long-lived, highly-engaged users experience significantly better treatment than other users from the moment they join the group, well before there is an opportunity for them to develop a long-standing relationship with members of the group We present a simple model explaining long-term heavy engagement as a combination of user-dependent and group-dependent factors. Using this model as an analytical tool, we show that properties of the user alone are sufficient to explain 95% of all memberships, but introducing a small amount of per-group information dramatically improves our ability to model users belonging to multiple groups.

#index 1035582
#* Connectivity structure of bipartite graphs via the KNC-plot
#@ Ravi Kumar;Andrew Tomkins;Erik Vee
#t 2008
#c 2
#% 194127
#% 249110
#% 281214
#% 283833
#% 309749
#% 310514
#% 480635
#% 492912
#% 577337
#% 593743
#% 824711
#% 869485
#% 956540
#! In this paper we introduce the k-neighbor connectivity plot, or KNC-plot, as a tool to study the macroscopic connectiv-ity structure of sparse bipartite graphs. Given a bipartite graph G = (U, V, E), we say that two nodes in U are k-neighbors if there exist at least k distinct length-two paths between them; this defines a k-neighborhood graph on U where the edges are given by the k-neighbor relation. For example, in a bipartite graph of users and interests, two users are k-neighbors if they have at least k common interests. The KNC-plot shows the degradation of connectivity of the graph as a function of k. We show that this tool provides an effective and interpretable high-level characterization of the connectivity of a bipartite graph However, naive algorithms to compute the KNC-plot are inefficient for k 1. We give an efficient and practical algorithm that runs in sub-quadratic time O(|E|2-1/k) and is a non-trivial improvement over the obvious quadratic-time algorithms for this problem. We prove significant improvements in this runtime for graphs with power-law degree distributions, and give a different algorithm with near-linear runtime when V grows slowly as a function of the size of the graph We compute the KNC-plot of four large real-world bipartite graphs, and discuss the structural properties of these graphs that emerge. We conclude that the KNC-plot represents a useful and practical tool for macroscopic analysis of large bipartite graphs.

#index 1035583
#* Deep classifier: automatically categorizing search results into large-scale hierarchies
#@ Dikan Xing;Gui-Rong Xue;Qiang Yang;Yong Yu
#t 2008
#c 2
#% 115006
#% 246831
#% 268079
#% 297550
#% 309141
#% 318412
#% 413615
#% 466501
#% 571073
#% 642986
#% 643026
#% 783478
#% 818224
#% 829975
#% 857482
#! Organizing Web search results into hierarchical categories facilitates users' browsing through Web search results, especially for ambiguous queries where the potential results are mixed together. Previous methods on search result classification are usually based on pre-training a classification model on some fixed and shallow hierarchical categories, where only the top-two-level categories of a Web taxonomy is used. Such classification methods may be too coarse for users to browse, since most search results would be classified into only two or three shallow categories. Instead, a deep hierarchical classifier must provide many more categories. However, the performance of such classifiers is usually limited because their classification effectiveness can deteriorate rapidly at the third or fourth level of a hierarchy. In this paper, we propose a novel algorithm known as Deep Classifier to classify the search results into detailed hierarchical categories with higher effectiveness than previous approaches. Given the search results in response to a query, the algorithm first prunes a wide-ranged hierarchy into a narrow one with the help of some Web directories. Different strategies are proposed to select the training data by utilizing the hierarchical structures. Finally, a discriminative naíve Bayesian classifier is developed to perform efficient and effective classification. As a result, the algorithm can provide more meaningful and specific class labels for search result browsing than shallow style of classification. We conduct experiments to show that the Deep Classifier can achieve significant improvement over state-of-the-art algorithms. In addition, with sufficient off-line preparation, the efficiency of the proposed algorithm is suitable for online application

#index 1035584
#* Personal name classification in web queries
#@ Dou Shen;Toby Walkery;Zijian Zhengy;Qiang Yangz;Ying Li
#t 2008
#c 2
#% 190581
#% 279755
#% 296646
#% 310567
#% 318412
#% 330617
#% 375017
#% 458379
#% 464450
#% 735077
#% 787639
#% 805878
#% 815068
#% 818275
#% 838408
#% 851481
#% 855112
#% 855114
#% 869484
#% 869550
#% 894253
#% 939641
#! Personal names are an important kind of Web queries in Web search, and yet they are special in many ways. Strategies for retrieving information on personal names should therefore be different from the strategies for other types of queries. To improve the search quality for personal names, a first step is to detect whether a query is a personal name. Despite the importance of this problem, relatively little previous research has been done on this topic. Since Web queries are usually short, conventional supervised machine-learning algorithms cannot be applied directly. An alternative is to apply some heuristic rules coupled with name-term dictionaries. However, when the dictionaries are small, this method tends to make false negatives; when the dictionaries are large, it tends to generate false positives. A more serious problem is that this method cannot provide a good trade-off between precision and recall. To solve these problems, we propose an approach based on the construction of probabilistic name-term dictionaries and personal name grammars, and use this algorithm to predict the probability of a query to be a personal name. In this paper, we develop four different methods for building probabilistic name-term dictionaries in which a term is assigned with a probability value of the term being a name term. We compared our approach with baseline algorithms such as dictionary-based look-up methods and supervised classification algorithms including logistic regression and SVM on some manually labeled test sets. The results validate the effectiveness of our approach, whose F1 value is more than 79.8%, which outperforms the best baseline by more than 11.3%

#index 1035585
#* Understanding temporal aspects in document classification
#@ Fernando Mourão;Leonardo Rocha;Renata Araújo;Thierson Couto;Marcos Gonçalves;Wagner Meira, Jr.
#t 2008
#c 2
#% 124004
#% 194284
#% 219049
#% 260001
#% 269217
#% 275837
#% 287292
#% 288285
#% 344447
#% 406493
#% 433674
#% 445382
#% 577283
#% 722935
#% 730034
#% 879596
#% 881477
#% 960414
#% 1055665
#! Due to the increasing amount of information present on the Web, Automatic Document Classification (ADC) has become an important research topic. ADC usually follows a standard supervised learning strategy, where we first build a model using preclassified documents and then use it to classify new unseen documents. One major challenge for ADC in many scenarios is that the characteristics of the documents and the classes to which they belong may change over time. However, most of the current techniques for ADC are applied without taking into account the temporal evolution of the collection of documents In this work, we perform a detailed study of the temporal evolution in the ADC, introducing an analysis methodology. We discuss that temporal evolution may be explained by three factors: 1) class distribution; 2) term distribution; and 3) class similarity. We employ metrics and experimental strategies capable of isolating each of these factors in order to analyze them separately, using two very different document collections: the ACM Digital Library and the Medline medical collections. Moreover, we present some preliminary results of potential gains that could be obtained by varying the training set to find the ideal size that minimizes the time effects. We show that by using just 69% of the ACM database, we are able to have an accuracy of 89.76%, and with only 25% of the Medline, an accuracy of 87.57%, which means gains of up to 20% in accuracy with much smaller training sets

#index 1035586
#* On ranking controversies in wikipedia: models and evaluation
#@ Ba-Quy Vuong;Ee-Peng Lim;Aixin Sun;Minh-Tam Le;Hady Wirawan Lauw;Kuiyu Chang
#t 2008
#c 2
#% 309095
#% 768632
#% 954955
#% 956520
#% 961568
#% 961697
#% 1019083
#% 1168662
#! Wikipedia 1 is a very large and successful Web 2.0 example. As the number of Wikipedia articles and contributors grows at a very fast pace, there are also increasing disputes occurring among the contributors. Disputes often happen in articles with controversial content. They also occur frequently among contributors who are "aggressive" or controversial in their personalities. In this paper, we aim to identify controversial articles in Wikipedia. We propose three models, namely the Basic model and two Controversy Rank (CR) models. These models draw clues from collaboration and edit history instead of interpreting the actual articles or edited content. While the Basic model only considers the amount of disputes within an article, the two Controversy Rank models extend the former by considering the relationships between articles and contributors. We also derived enhanced versions of these models by considering the age of articles. Our experiments on a collection of 19,456 Wikipedia articles shows that the Controversy Rank models can more effectively determine controversial articles compared to the Basic and other baseline models

#index 1035587
#* Finding high-quality content in social media
#@ Eugene Agichtein;Carlos Castillo;Debora Donato;Aristides Gionis;Gilad Mishne
#t 2008
#c 2
#% 211044
#% 290830
#% 448194
#% 662755
#% 730082
#% 742219
#% 747910
#% 754098
#% 811339
#% 818221
#% 846184
#% 847218
#% 854646
#% 879565
#% 879593
#% 918842
#% 956516
#% 956517
#% 956545
#% 987357
#% 1019165
#! The quality of user-generated content varies drastically from excellent to abuse and spam. As the availability of such content increases, the task of identifying high-quality content sites based on user contributions --social media sites -- becomes increasingly important. Social media in general exhibit a rich variety of information sources: in addition to the content itself, there is a wide array of non-content information available, such as links between items and explicit quality ratings from members of the community. In this paper we investigate methods for exploiting such community feedback to automatically identify high quality content. As a test case, we focus on Yahoo! Answers, a large community question/answering portal that is particularly rich in the amount and types of content and social interactions available in it. We introduce a general classification framework for combining the evidence from different sources of information, that can be tuned automatically for a given social media type and quality definition. In particular, for the community question/answering domain, we show that our system is able to separate high-quality items from the rest with an accuracy close to that of humans

#index 1035588
#* Can social bookmarking improve web search?
#@ Paul Heymann;Georgia Koutrika;Hector Garcia-Molina
#t 2008
#c 2
#% 27049
#% 168969
#% 340928
#% 643069
#% 754088
#% 855601
#% 878624
#% 881054
#% 905319
#% 956515
#% 956536
#% 956544
#% 958000
#% 967260
#! Social bookmarking is a recent phenomenon which has the potential to give us a great deal of data about pages on the web. One major question is whether that data can be used to augment systems like web search. To answer this question, over the past year we have gathered what we believe to be the largest dataset from a social bookmarking site yet analyzed by academic researchers. Our dataset represents about forty million bookmarks from the social bookmarking site del.icio.us. We contribute a characterization of posts to del.icio. us: how many bookmarks exist (about 115 million), how fast is it growing, and how active are the URLs being posted about (quite active). We also contribute a characterization of tags used by bookmarkers. We found that certain tags tend to gravitate towards certain domains, and vice versa. We also found that tags occur in over 50 percent of the pages that they annotate, and in only 20 percent of cases do they not occur in the page text, backlink page text, or forward link page text of the pages they annotate. We conclude that social bookmarking can provide search data not currently provided by other sources, though it may currently lack the size and distribution of tags necessary to make a significant impact

#index 1035589
#* Identifying the influential bloggers in a community
#@ Nitin Agarwal;Huan Liu;Lei Tang;Philip S. Yu
#t 2008
#c 2
#% 282905
#% 310514
#% 577217
#% 577360
#% 729923
#% 794513
#% 823332
#% 869594
#% 869640
#% 881051
#% 912461
#% 918842
#% 932742
#% 936341
#% 957992
#! Blogging becomes a popular way for a Web user to publish information on the Web. Bloggers write blog posts, share their likes and dislikes, voice their opinions, provide suggestions, report news, and form groups in Blogosphere. Bloggers form their virtual communities of similar interests. Activities happened in Blogosphere affect the external world. One way to understand the development on Blogosphere is to find influential blog sites. There are many non-influential blog sites which form the "the long tail". Regardless of a blog site being influential or not, there are influential bloggers. Inspired by the high impact of the influentials in a physical community, we study a novel problem of identifying influential bloggers at a blog site. Active bloggers are not necessarily influential. Influential bloggers can impact fellow bloggers in various ways. In this paper, we discuss the challenges of identifying influential bloggers, investigate what constitutes influential bloggers, present a preliminary model attempting to quantify an influential blogger, and pave the way for building a robust model that allows for finding various types of the influentials. To illustrate these issues, we conduct experiments with data from a real-world blog site, evaluate multi-facets of the problem of identifying influential bloggers, and discuss unique challenges. We conclude with interesting findings and future work

#index 1035590
#* Opinion spam and analysis
#@ Nitin Jindal;Bing Liu
#t 2008
#c 2
#% 451536
#% 577355
#% 616528
#% 679843
#% 769892
#% 807297
#% 815915
#% 818223
#% 854646
#% 869469
#% 869471
#% 879600
#% 907490
#% 912202
#% 936239
#% 939896
#% 956518
#% 956523
#% 956559
#% 963698
#% 1117058
#% 1250376
#% 1663483
#! Evaluative texts on the Web have become a valuable source of opinions on products, services, events, individuals, etc. Recently, many researchers have studied such opinion sources as product reviews, forum posts, and blogs. However, existing research has been focused on classification and summarization of opinions using natural language processing and data mining techniques. An important issue that has been neglected so far is opinion spam or trustworthiness of online opinions. In this paper, we study this issue in the context of product reviews, which are opinion rich and are widely used by consumers and product manufacturers. In the past two years, several startup companies also appeared which aggregate opinions from product reviews. It is thus high time to study spam in reviews. To the best of our knowledge, there is still no published study on this topic, although Web spam and email spam have been investigated extensively. We will see that opinion spam is quite different from Web spam and email spam, and thus requires different detection techniques. Based on the analysis of 5.8 million reviews and 2.14 million reviewers from amazon.com, we show that opinion spam in reviews is widespread. This paper analyzes such spam activities and presents some novel techniques to detect them

#index 1035591
#* A holistic lexicon-based approach to opinion mining
#@ Xiaowen Ding;Bing Liu;Philip S. Yu
#t 2008
#c 2
#% 118040
#% 577246
#% 577355
#% 723399
#% 746885
#% 755835
#% 769892
#% 805873
#% 815915
#% 848644
#% 854646
#% 855279
#% 855282
#% 907489
#% 939346
#% 939634
#% 939848
#% 939896
#% 983579
#% 983583
#% 983599
#% 987340
#% 1250238
#% 1250367
#% 1261566
#% 1299640
#% 1700552
#% 1708349
#! One of the important types of information on the Web is the opinions expressed in the user generated content, e.g., customer reviews of products, forum posts, and blogs. In this paper, we focus on customer reviews of products. In particular, we study the problem of determining the semantic orientations (positive, negative or neutral) of opinions expressed on product features in reviews. This problem has many applications, e.g., opinion mining, summarization and search. Most existing techniques utilize a list of opinion (bearing) words (also called opinion lexicon) for the purpose. Opinion words are words that express desirable (e.g., great, amazing, etc.) or undesirable (e.g., bad, poor, etc) states. These approaches, however, all have some major shortcomings. In this paper, we propose a holistic lexicon-based approach to solving the problem by exploiting external evidences and linguistic conventions of natural language expressions. This approach allows the system to handle opinion words that are context dependent, which cause major difficulties for existing algorithms. It also deals with many special words, phrases and language constructs which have impacts on opinions based on their linguistic patterns. It also has an effective function for aggregating multiple conflicting opinion words in a sentence. A system, called Opinion Observer, based on the proposed technique has been implemented. Experimental results using a benchmark product review data set and some additional reviews show that the proposed technique is highly effective. It outperforms existing methods significantly

#index 1035592
#* An empirical analysis of sponsored search performance in search engine advertising
#@ Anindya Ghose;Sha Yang
#t 2008
#c 2
#% 330705
#% 590523
#% 739648
#% 740191
#% 770059
#% 868473
#! The phenomenon of sponsored search advertising â聙聯 where advertisers pay a fee to Internet search engines to be displayed alongside organic (non-sponsored) web search results â聙聯 is gaining ground as the largest source of revenues for search engines. Despite the growth of search advertising, we have little understanding of how consumers respond to contextual and sponsored search advertising on the Internet. Using a unique panel dataset of several hundred keywords collected from a large nationwide retailer that advertises on Google, we empirically model the relationship between different metrics such as click-through rates, conversion rates, bid prices and keyword ranks. Our paper proposes a novel framework and data to better understand what drives these differences. We use a Hierarchical Bayesian modeling framework and estimate the model using Markov Chain Monte Carlo (MCMC) methods. We empirically estimate the impact of keyword attributes on consumer search and purchase behavior as well as on firms' decision-making behavior on bid prices and ranks. We find that the presence of retailer-specific information in the keyword increases click-through rates, and the presence of brand-specific information in the keyword increases conversion rates. We also demonstrate that as suggested by anecdotal evidence, search engines like Google factor in both the auction bid price as well as prior click-through rates before allotting a final rank to an advertisement. To the best of our knowledge, this is the first study that uses real world data from an advertiser and jointly estimates the effect of sponsored search advertising at a keyword level on consumer search, click and purchase behavior in electronic markets

#index 1035593
#* Advertising keyword suggestion based on concept hierarchy
#@ Yifan Chen;Gui-Rong Xue;Yong Yu
#t 2008
#c 2
#% 144029
#% 158687
#% 280849
#% 281480
#% 298183
#% 309095
#% 387427
#% 447948
#% 465747
#% 479817
#% 565898
#% 642999
#% 665561
#% 735138
#% 754076
#% 818265
#% 869484
#% 869501
#% 972307
#! The increasing growth of the World Wide Web constantly enlarges the revenue generated by search engine advertising. Advertisers bid on keywords associated with their products to display their ads on the search result pages. Keyword suggestion methods are proposed to fill the gap between the keywords chosen by advertisers and the popular queries, through finding new relevant keywords according to some statistical information (for example, the keyword co-occurrence). However, there is little effort taking semantic information, such as concept hierarchy, into account. In this paper, we propose a novel keyword suggestion method that fully exploits the semantic knowledge among concept hierarchy. Given a keyword, we first match it with some relevant concepts. Then the relevant concepts are used with their hierarchy to fertilize the meanings of the keywords. Finally new keywords are suggested according to the concept information rather than the statistical co-occurrence of the keyword itself. Experimental results show that our proposed method can successfully provide suggestion that meets the accuracy and coverage requirements

#index 1166469
#* Challenges in building large-scale information retrieval systems: invited talk
#@ Jeffrey Dean
#t 2009
#c 2
#! Building and operating large-scale information retrieval systems used by hundreds of millions of people around the world provides a number of interesting challenges. Designing such systems requires making complex design tradeoffs in a number of dimensions, including (a) the number of user queries that must be handled per second and the response latency to these requests, (b) the number and size of various corpora that are searched, (c) the latency and frequency with which documents are updated or added to the corpora, and (d) the quality and cost of the ranking algorithms that are used for retrieval. In this talk I will discuss the evolution of Google's hardware infrastructure and information retrieval systems and some of the design challenges that arise from ever-increasing demands in all of these dimensions. I will also describe how we use various pieces of distributed systems infrastructure when building these retrieval systems. Finally, I will describe some future challenges and open research problems in this area.

#index 1166470
#* Online social networks: modeling and mining: invited talk
#@ Ravi Kumar
#t 2009
#c 2
#! Online social networks have become major and driving phenomena on the Web. In this talk, we will address key modeling and algorithmic questions related to large online social networks. From the modeling perspective, we raise the question of whether there is a generative model for network evolution. The availability of time-stamped data makes it possible to study this question at an extremely fine granularity. We exhibit a simple, natural model that leads to synthetic networks with properties similar to the online ones. From an algorithmic viewpoint, we focus on data mining challenges posed by the magnitude of data in these networks. In particular, we examine topics related to influence and correlation in user activities and compressibility of such networks.

#index 1166471
#* Harvesting, searching, and ranking knowledge on the web: invited talk
#@ Gerhard Weikum
#t 2009
#c 2
#% 805850
#% 824695
#% 830520
#% 881505
#% 956501
#% 956564
#% 987276
#% 1019061
#% 1022234
#% 1022235
#% 1022288
#% 1055735
#% 1092530
#% 1102245
#% 1127402
#% 1166537
#% 1183369
#% 1206687
#% 1206702
#% 1206817
#% 1275182
#% 1300591
#% 1409954
#! There are major trends to advance the functionality of search engines to a more expressive semantic level (e.g., [2, 4, 6, 7, 8, 9, 13, 14, 18]). This is enabled by employing large-scale information extraction [1, 11, 20] of entities and relationships from semistructured as well as natural-language Web sources. In addition, harnessing Semantic-Web-style ontologies [22] and reaching into Deep-Web sources [16] can contribute towards a grand vision of turning the Web into a comprehensive knowledge base that can be efficiently searched with high precision. This talk presents ongoing research towards this objective, with emphasis on our work on the YAGO knowledge base [23, 24] and the NAGA search engine [14] but also covering related projects. YAGO is a large collection of entities and relational facts that are harvested from Wikipedia and WordNet with high accuracy and reconciled into a consistent RDF-style "semantic" graph. For further growing YAGO from Web sources while retaining its high quality, pattern-based extraction is combined with logic-based consistency checking in a unified framework [25]. NAGA provides graph-template-based search over this data, with powerful ranking capabilities based on a statistical language model for graphs. Advanced queries and the need for ranking approximate matches pose efficiency and scalability challenges that are addressed by algorithmic and indexing techniques [15, 17]. YAGO is publicly available and has been imported into various other knowledge-management projects including DB-pedia. YAGO shares many of its goals and methodologies with parallel projects along related lines. These include Avatar [19], Cimple/DBlife [10, 21], DBpedia [3], Know-ItAll/TextRunner [12, 5], Kylin/KOG [26, 27], and the Libra technology [18, 28] (and more). Together they form an exciting trend towards providing comprehensive knowledge bases with semantic search capabilities.

#index 1166473
#* Diversifying search results
#@ Rakesh Agrawal;Sreenivas Gollapudi;Alan Halverson;Samuel Ieong
#t 2009
#c 2
#% 217812
#% 262112
#% 309095
#% 642975
#% 805841
#% 805863
#% 872020
#% 879618
#% 879686
#% 1026428
#% 1073970
#% 1074133
#% 1077150
#% 1206662
#! We study the problem of answering ambiguous web queries in a setting where there exists a taxonomy of information, and that both queries and documents may belong to more than one category according to this taxonomy. We present a systematic approach to diversifying results that aims to minimize the risk of dissatisfaction of the average user. We propose an algorithm that well approximates this objective in general, and is provably optimal for a natural special case. Furthermore, we generalize several classical IR metrics, including NDCG, MRR, and MAP, to explicitly account for the value of diversification. We demonstrate empirically that our algorithm scores higher in these generalized metrics compared to results produced by commercial search engines.

#index 1166492
#* Discovering and using groups to improve personalized search
#@ Jaime Teevan;Meredith Ringel Morris;Steve Bush
#t 2009
#c 2
#% 144076
#% 246877
#% 309095
#% 399057
#% 754099
#% 754126
#% 805877
#% 807924
#% 818259
#% 838547
#% 933280
#% 955930
#% 956552
#% 998795
#% 1004296
#% 1019163
#% 1035574
#% 1047489
#% 1047490
#% 1074090
#% 1132900
#% 1674834
#! Personalized Web search takes advantage of information about an individual to identify the most relevant results for that person. A challenge for personalization lies in collecting user profiles that are rich enough to do this successfully. One way an individual's profile can be augmented is by using data from other people. To better understand whether groups of people can be used to benefit personalized search, we explore the similarity of query selection, desktop information, and explicit relevance judgments across people grouped in different ways. The groupings we explore fall along two dimensions: the longevity of the group members' relationship, and how explicitly the group is formed. We find that some groupings provide valuable insight into what members consider relevant to queries related to the group focus, but that it can be difficult to identify valuable groups implicitly. Building on these findings, we explore an algorithm to "groupize" (versus "personalize") Web search results that leads to a significant improvement in result ranking on group-relevant queries.

#index 1166507
#* Adaptive subjective triggers for opinionated document retrieval
#@ Kazuhiro Seki;Kuniaki Uehara
#t 2009
#c 2
#% 279755
#% 340901
#% 389155
#% 406493
#% 466863
#% 789959
#% 818219
#% 832271
#% 843728
#% 879585
#% 956510
#% 1019145
#% 1035589
#% 1035590
#% 1035591
#% 1074094
#% 1074102
#% 1074168
#! This paper proposes a novel application of a statistical language model to opinionated document retrieval targeting weblogs (blogs). In particular, we explore the use of the trigger model---originally developed for incorporating distant word dependencies---in order to model the characteristics of personal opinions that cannot be properly modeled by standard n-grams. Our primary assumption is that there are two constituents to form a subjective opinion. One is the subject of the opinion or the object that the opinion is about, and the other is a subjective expression; the former is regarded as a triggering word and the latter as a triggered word. We automatically identify those subjective trigger patterns to build a language model from a corpus of product customer reviews. Experimental results on the TREC Blog Track test collections show that, when used for reranking initial search results, our proposed model significantly improves opinionated document retrieval by over 20% in MAP. In addition, we report on an experiment on dynamic adaptation of the model to a given query, which is found effective for most of difficult queries categorized under politics and organizations.

#index 1166508
#* Query by document
#@ Yin Yang;Nilesh Bansal;Wisam Dakka;Panagiotis Ipeirotis;Nick Koudas;Dimitris Papadias
#t 2009
#c 2
#% 35937
#% 118726
#% 118728
#% 252328
#% 279755
#% 281480
#% 348173
#% 406493
#% 420487
#% 420500
#% 742250
#% 746927
#% 748499
#% 855200
#% 874505
#% 956564
#% 960263
#% 1016177
#% 1019189
#% 1022269
#% 1022338
#% 1215396
#! We are experiencing an unprecedented increase of content contributed by users in forums such as blogs, social networking sites and microblogging services. Such abundance of content complements content on web sites and traditional media forums such as news papers, news and financial streams, and so on. Given such plethora of information there is a pressing need to cross reference information across textual services. For example, commonly we read a news item and we wonder if there are any blogs reporting related content or vice versa. In this paper, we present techniques to automate the process of cross referencing online information content. We introduce methodologies to extract phrases from a given "query document" to be used as queries to search interfaces with the goal to retrieve content related to the query document. In particular, we consider two techniques to extract and score key phrases. We also consider techniques to complement extracted phrases with information present in external sources such as Wikipedia and introduce an algorithm called RelevanceRank for this purpose. We discuss both these techniques in detail and provide an experimental study utilizing a large number of human judges from Amazons's Mechanical Turk service. Detailed experiments demonstrate the effectiveness and efficiency of the proposed techniques for the task of automating retrieval of documents related to a query document.

#index 1166509
#* Wikipedia pages as entry points for book search
#@ Marijn Koolen;Gabriella Kazai;Nick Craswell
#t 2009
#c 2
#% 118726
#% 283833
#% 290830
#% 309749
#% 330787
#% 375017
#% 397126
#% 818255
#% 838532
#% 879665
#% 987222
#% 987333
#% 1077038
#% 1131070
#% 1131084
#% 1415728
#% 1415729
#% 1415731
#! A lot of the world's knowledge is stored in books, which, as a result of recent mass-digitisation efforts, are increasingly available online. Search engines, such as Google Books, provide mechanisms for searchers to enter this vast knowledge space using queries as entry points. In this paper, we view Wikipedia as a summary of this world knowledge and aim to use this resource to guide users to relevant books. Thus, we investigate possible ways of using Wikipedia as an intermediary between the user's query and a collection of books being searched. We experiment with traditional query expansion techniques, exploiting Wikipedia articles as rich sources of information that can augment the user's query. We then propose a novel approach based on link distance in an extended Wikipedia graph: we associate books with Wikipedia pages that cite these books and use the link distance between these nodes and the pages that match the user query as an estimation of a book's relevance to the query. Our results show that a) classical query expansion using terms extracted from query pages leads to increased precision, and b) link distance between query and book pages in Wikipedia provides a good indicator of relevance that can boost the retrieval score of relevant books in the result ranking of a book search engine.

#index 1166510
#* Clustering the tagged web
#@ Daniel Ramage;Paul Heymann;Christopher D. Manning;Hector Garcia-Molina
#t 2009
#c 2
#% 118771
#% 218992
#% 219053
#% 262045
#% 280819
#% 281251
#% 348165
#% 348173
#% 375017
#% 465754
#% 642990
#% 643069
#% 649674
#% 722904
#% 751818
#% 766430
#% 766433
#% 766447
#% 813043
#% 869525
#% 879587
#% 905224
#% 956544
#% 967260
#% 987205
#% 995468
#% 1035588
#% 1055743
#% 1077150
#% 1667787
#! Automatically clustering web pages into semantic groups promises improved search and browsing on the web. In this paper, we demonstrate how user-generated tags from large-scale social bookmarking websites such as del.icio.us can be used as a complementary data source to page text and anchor text for improving automatic clustering of web pages. This paper explores the use of tags in 1) K-means clustering in an extended vector space model that includes tags as well as page text and 2) a novel generative clustering algorithm based on latent Dirichlet allocation that jointly models text and tags. We evaluate the models by comparing their output to an established web directory. We find that the naive inclusion of tagging data improves cluster quality versus page text alone, but a more principled inclusion can substantially improve the quality of all models with a statistically significant absolute F-score increase of 4%. The generative model outperforms K-means with another 8% F-score increase.

#index 1166511
#* Classifying tags using open content resources
#@ Simon Overell;Börkur Sigurbjörnsson;Roelof van Zwol
#t 2009
#c 2
#% 452641
#% 874536
#% 956564
#% 987205
#% 1016372
#% 1055704
#% 1707940
#% 1916063
#! Tagging has emerged as a popular means to annotate on-line objects such as bookmarks, photos and videos. Tags vary in semantic meaning and can describe different aspects of a media object. Tags describe the content of the media as well as locations, dates, people and other associated meta-data. Being able to automatically classify tags into semantic categories allows us to understand better the way users annotate media objects and to build tools for viewing and browsing the media objects. In this paper we present a generic method for classifying tags using third party open content resources, such as Wikipedia and the Open Directory. Our method uses structural patterns that can be extracted from resource meta-data. We describe the implementation of our method on Wikipedia using WordNet categories as our classification schema and ground truth. Two structural patterns found in Wikipedia are used for training and classification: categories and templates. We apply our system to classifying Flickr tags. Compared to a WordNet baseline our method increases the coverage of the Flickr vocabulary by 115%. We can classify many important entities that are not covered by WordNet, such as, London Eye, Big Island, Ronaldinho, geocaching and wii.

#index 1166512
#* Cross-language query classification using web search for exogenous knowledge
#@ Xuerui Wang;Andrei Broder;Evgeniy Gabrilovich;Vanja Josifovski;Bo Pang
#t 2009
#c 2
#% 219096
#% 262870
#% 306506
#% 786534
#% 807744
#% 807745
#% 815902
#% 818313
#% 832331
#% 916788
#% 939570
#% 955500
#% 987221
#% 1051798
#% 1055769
#% 1130910
#% 1261539
#% 1272110
#! The non-English Web is growing at phenomenal speed, but available language processing tools and resources are predominantly English-based. Taxonomies are a case in point: while there are plenty of commercial and non-commercial taxonomies for the English Web, taxonomies for other languages are either not available or of arguable quality. Given that building comprehensive taxonomies for each language is prohibitively expensive, it is natural to ask whether existing English taxonomies can be leveraged, possibly via machine translation, to enable text processing tasks in other languages. Our experimental results confirm that the answer is affirmative with respect to at least one task. In this study we focus on query classification, which is essential for understanding the user intent both in Web search and in online advertising. We propose a robust method for classifying non-English queries into an English taxonomy, using an existing English text classifier and off-the-shelf machine translation systems. In particular, we show that by considering the Web search results in the query's original language as additional sources of information, we can alleviate the effect of erroneous machine translation. Empirical evaluation on query sets in languages as diverse as Chinese and Russian yields very encouraging results; consequently, we believe that our approach is also applicable to many additional languages.

#index 1166513
#* Improving music genre classification using collaborative tagging data
#@ Ling Chen;Phillip Wright;Wolfgang Nejdl
#t 2009
#c 2
#% 248810
#% 643010
#% 855601
#% 869482
#% 869504
#% 869525
#% 869548
#% 879625
#% 956515
#% 956544
#% 961278
#% 1044454
#% 1650403
#! As a fundamental and critical component of music information retrieval (MIR) systems, music genre classification has attracted considerable research attention. Automatically classifying music by genre is, however, a challenging problem due to the fact that music is an evolving art. While most of the existing work categorizes music using features extracted from music audio signals, in this paper, we propose to exploit the semantic information embedded in tags supplied by users of social networking websites. Particularly, we consider the tag information by creating a graph of tracks so that tracks are neighbors if they are similar in terms of their associated tags. Two classification methods based on the track graph are developed. The first one employs a classification scheme which simultaneously considers the audio content and neighborhood of tracks. In contrast, the second one is a two-level classifier which initializes genre label for unknown tracks using their audio content, and then iteratively updates the genres considering the influence from their neighbors. A set of optimizing strategies are designed for the purpose of further enhancing the quality of the two-level classifier. Extensive experiments are conducted on real-world data collected from Last.fm. Promising experimental results demonstrate the benefit of using tags for accurate music genre classification.

#index 1166514
#* Information arbitrage across multi-lingual Wikipedia
#@ Eytan Adar;Michael Skinner;Daniel S. Weld
#t 2009
#c 2
#% 466078
#% 572314
#% 786389
#% 868096
#% 1019061
#% 1055735
#% 1083705
#% 1270363
#! The rapid globalization of Wikipedia is generating a parallel, multi-lingual corpus of unprecedented scale. Pages for the same topic in many different languages emerge both as a result of manual translation and independent development. Unfortunately, these pages may appear at different times, vary in size, scope, and quality. Furthermore, differential growth rates cause the conceptual mapping between articles in different languages to be both complex and dynamic. These disparities provide the opportunity for a powerful form of information arbitrage--leveraging articles in one or more languages to improve the content in another. Analyzing four large language domains (English, Spanish, French, and German), we present Ziggurat, an automated system for aligning Wikipedia infoboxes, creating new infoboxes as necessary, filling in missing information, and detecting discrepancies between parallel pages. Our method uses self-supervised learning and our experiments demonstrate the method's feasibility, even in the absence of dictionaries.

#index 1166515
#* Measuring the similarity between implicit semantic relations using web search engines
#@ Danushka Bollegala;Yutaka Matsuo;Mitsuru Ishizuka
#t 2009
#c 2
#% 65345
#% 144023
#% 342630
#% 406493
#% 722917
#% 722926
#% 741083
#% 747738
#% 747893
#% 756964
#% 778732
#% 786515
#% 815868
#% 823312
#% 902089
#% 938706
#% 939540
#% 1250378
#% 1273708
#% 1289532
#! Measuring the similarity between implicit semantic relations is an important task in information retrieval and natural language processing. For example, consider the situation where you know an entity-pair (e.g. Google, YouTube), between which a particular relation holds (e.g. acquisition), and you are interested in retrieving other entity-pairs for which the same relation holds (e.g. Yahoo, Inktomi). Existing keyword-based search engines cannot be directly applied in this case because in keyword-based search, the goal is to retrieve documents that are relevant to the words used in the query -- not necessarily to the relations implied by a pair of words. Accurate measurement of relational similarity is an important step in numerous natural language processing tasks such as identification of word analogies, and classification of noun-modifier pairs. We propose a method that uses Web search engines to efficiently compute the relational similarity between two pairs of words. Our method consists of three components: representing the various semantic relations that exist between a pair of words using automatically extracted lexical patterns, clustering the extracted lexical patterns to identify the different semantic relations implied by them, and measuring the similarity between different semantic relations using an inter-cluster correlation matrix. We propose a pattern extraction algorithm to extract a large number of lexical patterns that express numerous semantic relations. We then present an efficient clustering algorithm to cluster the extracted lexical patterns. Finally, we measure the relational similarity between word-pairs using inter-cluster correlation. We evaluate the proposed method in a relation classification task. Experimental results on a dataset covering multiple relation types show a statistically significant improvement over the current state-of-the-art relational similarity measures.

#index 1166516
#* A model for fast web mining prototyping
#@ Álvaro Pereira;Ricardo Baeza-Yates;Nivio Ziviani;Jesús Bisbal
#t 2009
#c 2
#% 236409
#% 320930
#% 384978
#% 451536
#% 464825
#% 584881
#% 823438
#% 926881
#% 936239
#% 1015257
#% 1023420
#% 1055708
#! Web mining is a computation intensive task, even after the mining tool itself has been developed. Most mining software are developed ad-hoc and usually are not scalable nor reused for other mining tasks. The objective of this paper is to present a model for fast Web mining prototyping, referred to as WIM -- Web Information Mining. The underlying conceptual model of WIM provides its users with a level of abstraction appropriate for prototyping and experimentation throughout the Web data mining task. Abstracting from the idiosyncrasies of raw Web data representations facilitates the inherently iterative mining process. We present the WIM conceptual model, its associated algebra, and the WIM tool software architecture, which implements the WIM model. We also illustrate how the model can be applied to real Web data mining tasks. The experimentation of WIM in real use cases has shown to significantly facilitate Web mining prototyping.

#index 1166517
#* Efficient multiple-click models in web search
#@ Fan Guo;Chao Liu;Yi Min Wang
#t 2009
#c 2
#% 296646
#% 577224
#% 590523
#% 783482
#% 805200
#% 818207
#% 818221
#% 823348
#% 879565
#% 879567
#% 881540
#% 946521
#% 956546
#% 989628
#% 1004294
#% 1035578
#% 1055676
#% 1074092
#% 1682438
#! Many tasks that leverage web search users' implicit feedback rely on a proper and unbiased interpretation of user clicks. Previous eye-tracking experiments and studies on explaining position-bias of user clicks provide a spectrum of hypotheses and models on how an average user examines and possibly clicks web documents returned by a search engine with respect to the submitted query. In this paper, we attempt to close the gap between previous work, which studied how to model a single click, and the reality that multiple clicks on web documents in a single result page are not uncommon. Specifically, we present two multiple-click models: the independent click model (ICM) which is reformulated from previous work, and the dependent click model (DCM) which takes into consideration dependencies between multiple clicks. Both models can be efficiently learned with linear time and space complexities. More importantly, they can be incrementally updated as new click logs flow in. These are well-demanded properties in reality. We systematically evaluate the two models on click logs obtained in July 2008 from a major commercial search engine. The data set, after preprocessing, contains over 110 thousand distinct queries and 8.8 million query sessions. Extensive experimental studies demonstrate the gain of modeling multiple clicks and their dependencies. Finally, we note that since our experimental setup does not rely on tweaking search result rankings, it can be easily adopted by future studies.

#index 1166518
#* Characterizing the influence of domain expertise on web search behavior
#@ Ryen W. White;Susan T. Dumais;Jaime Teevan
#t 2009
#c 2
#% 309767
#% 345262
#% 378486
#% 581916
#% 751596
#% 807420
#% 810774
#% 845310
#% 854636
#% 879565
#% 879663
#% 933280
#% 956495
#% 987224
#% 1047297
#% 1074071
#% 1074190
#% 1275193
#! Domain experts search differently than people with little or no domain knowledge. Previous research suggests that domain experts employ different search strategies and are more successful in finding what they are looking for than non-experts. In this paper we present a large-scale, longitudinal, log-based analysis of the effect of domain expertise on web search behavior in four different domains (medicine, finance, law, and computer science). We characterize the nature of the queries, search sessions, web sites visited, and search success for users identified as experts and non-experts within these domains. Large-scale analysis of real-world interactions allows us to understand how expertise relates to vocabulary, resource use, and search task under more realistic search conditions than has been possible in previous small-scale studies. Building upon our analysis we develop a model to predict expertise based on search behavior, and describe how knowledge about domain expertise can be used to present better results and query suggestions to users and to help non-experts gain expertise.

#index 1166519
#* Quality-aware collaborative question answering: methods and evaluation
#@ Maggy Anastasia Suryanto;Ee Peng Lim;Aixin Sun;Roger H. L. Chiang
#t 2009
#c 2
#% 211044
#% 262096
#% 290830
#% 676170
#% 730082
#% 750863
#% 755394
#% 766482
#% 838397
#% 838398
#% 879570
#% 879592
#% 879593
#% 956516
#% 956517
#% 956550
#% 987357
#% 1019165
#% 1035587
#% 1055718
#! Community Question Answering (QA) portals contain questions and answers contributed by hundreds of millions of users. These databases of questions and answers are of great value if they can be used directly to answer questions from any user. In this research, we address this collaborative QA task by drawing knowledge from the crowds in community QA portals such as Yahoo! Answers. Despite their popularity, it is well known that answers in community QA portals have unequal quality. We therefore propose a quality-aware framework to design methods that select answers from a community QA portal considering answer quality in addition to answer relevance. Besides using answer features for determining answer quality, we introduce several other quality-aware QA methods using answer quality derived from the expertise of answerers. Such expertise can be question independent or question dependent. We evaluate our proposed methods using a database of 95K questions and 537K answers obtained from Yahoo! Answers. Our experiments have shown that answer quality can improve QA performance significantly. Furthermore, question dependent expertise based methods are shown to outperform methods using answer features only. It is also found that there are also good answers not among the best answers identified by Yahoo! Answers users.

#index 1166520
#* A new visual search interface for web browsing
#@ Songhua Xu;Tao Jin;Francis C. M. Lau
#t 2009
#c 2
#% 118771
#% 144023
#% 214711
#% 218992
#% 259946
#% 272915
#% 309157
#% 309531
#% 343127
#% 344930
#% 346549
#% 378525
#% 434613
#% 640268
#% 643008
#% 801438
#% 856220
#! We introduce a new visual search interface for search engines. The interface is a user-friendly and informative graphical front-end for organizing and presenting search results in the form of topic groups. Such a semantics-oriented search result presentation is in contrast with conventional search interfaces which present search results according to the physical structures of the information. Given a user query, our interface first retrieves relevant online materials via a third-party search engine. And then we analyze the semantics of search results to detect latent topics in the result set. Once the topics are detected, we map the search result pages into topic clusters. According to the topic clustering result, we divide the available screen space for our visual interface into multiple topic displaying regions, one for each topic. For each topic's displaying region, we summarize the information contained in the search results under the corresponding topic so that only key messages will be displayed. With this new visual search interface, users are conveyed the key information in the search results expediently. With the key information, users can navigate to the final, desired results with less effort and time than conventional searching. Supplementary materials for this paper are available at http://www.cs.hku.hk/~songhua/visualsearch/.

#index 1166521
#* Mining user web search activity with layered bayesian networks or how to capture a click in its context
#@ Benjamin Piwowarski;Georges Dupret;Rosie Jones
#t 2009
#c 2
#% 284796
#% 293364
#% 449294
#% 590523
#% 766454
#% 823348
#% 838547
#% 874705
#% 879565
#% 881540
#% 956495
#% 987211
#% 1275193
#! Mining user web search activity potentially has a broad range of applications including web result pre-fetching, automatic search query reformulation, click spam detection, estimation of document relevance and prediction of user satisfaction. This analysis is difficult because the data recorded by search engines while users interact with them, although abundant, is very noisy. In this work, we explore the utility of mining search behavior of users, represented by observed variables including the time the user spends on the page, and whether the user reformulated his or her query. As a case study, we examine the contribution this data makes to predicting the relevance of a document in the absence of document content models. To this end, we first propose a method for grouping the interactions of a particular user according to the different tasks he or she undertakes. With each task corresponding to a distinct information need, we then propose a Bayesian Network to holistically model these interactions. The aim is to identify distinct patterns of search behaviors. Finally, we join these patterns to a list of custom features and we use gradient boosted decision trees to predict the relevance of a set of query document pairs for which we have relevance assessments. The experimental results confirm the potential of our model, with significant improvements in precision for predicting the relevance of documents based on a model of the user's search and click behavior, over a baseline model using only click and query features, with no Bayesian Network input.

#index 1166522
#* Generating labels from clicks
#@ R. Agrawal;A. Halverson;K. Kenthapadi;N. Mishra;P. Tsaparas
#t 2009
#c 2
#% 180945
#% 330769
#% 577224
#% 734915
#% 801673
#% 805200
#% 805798
#% 818221
#% 823348
#% 840846
#% 875002
#% 881515
#% 907550
#% 946521
#% 954948
#% 954949
#% 987209
#% 987222
#% 989628
#% 1035577
#% 1063473
#% 1074061
#% 1074085
#% 1074092
#% 1112365
#% 1130814
#% 1250379
#% 1684470
#! The ranking function used by search engines to order results is learned from labeled training data. Each training point is a (query, URL) pair that is labeled by a human judge who assigns a score of Perfect, Excellent, etc., depending on how well the URL matches the query. In this paper, we study whether clicks can be used to automatically generate good labels. Intuitively, documents that are clicked (resp., skipped) in aggregate can indicate relevance (resp., lack of relevance). We give a novel way of transforming clicks into weighted, directed graphs inspired by eye-tracking studies and then devise an objective function for finding cuts in these graphs that induce a good labeling. In its full generality, the problem is NP-hard, but we show that, in the case of two labels, an optimum labeling can be found in linear time. For the more general case, we propose heuristic solutions. Experiments on real click logs show that click-based labels align with the opinion of a panel of judges, especially as the consensus of the panel grows stronger.

#index 1166523
#* Integration of news content into web results
#@ Fernando Diaz
#t 2009
#c 2
#% 124004
#% 310567
#% 340901
#% 342961
#% 366058
#% 397161
#% 466731
#% 565531
#% 575568
#% 642982
#% 765412
#% 766408
#% 818281
#% 869500
#% 952142
#% 956521
#% 960414
#% 983894
#% 987221
#% 1061652
#% 1073970
#% 1074092
#% 1074093
#% 1074360
#% 1269878
#% 1392432
#! Aggregated search refers to the integration of content from specialized corpora or verticals into web search results. Aggregation improves search when the user has vertical intent but may not be aware of or desire vertical search. In this paper, we address the issue of integrating search results from a news vertical into web search results. News is particularly challenging because, given a query, the appropriate decision---to integrate news content or not---changes with time. Our system adapts to news intent in two ways. First, by inspecting the dynamics of the news collection and query volume, we can track development of and interest in topics. Second, by using click feedback, we can quickly recover from system errors. We define several click-based metrics which allow a system to be monitored and tuned without annotator effort.

#index 1166524
#* Mining common topics from multiple asynchronous text streams
#@ Xiang Wang;Kai Zhang;Xiaoming Jin;Dou Shen
#t 2009
#c 2
#% 262042
#% 262043
#% 280819
#% 309096
#% 577220
#% 769967
#% 818215
#% 823344
#% 824666
#% 869516
#% 875959
#% 876007
#% 876017
#% 881498
#% 983883
#% 989650
#! Text streams are becoming more and more ubiquitous, in the forms of news feeds, weblog archives and so on, which result in a large volume of data. An effective way to explore the semantic as well as temporal information in text streams is topic mining, which can further facilitate other knowledge discovery procedures. In many applications, we are facing multiple text streams which are related to each other and share common topics. The correlation among these streams can provide more meaningful and comprehensive clues for topic mining than those from each individual stream. However, it is nontrivial to explore the correlation with the existence of asynchronism among multiple streams, i.e. documents from different streams about the same topic may have different timestamps, which remains unsolved in the context of topic mining. In this paper, we formally address this problem and put forward a novel algorithm based on the generative topic model. Our algorithm consists of two alternate steps: the first step extracts common topics from multiple streams based on the adjusted timestamps by the second step; the second step adjusts the timestamps of the documents according to the time distribution of the discovered topics by the first step. We perform these two steps alternately and a monotone convergence of our objective function is guaranteed. The effectiveness and advantage of our approach were justified by extensive empirical studies on two real data sets consisting of six research paper streams and two news article streams, respectively.

#index 1166525
#* Predicting the readability of short web summaries
#@ Tapas Kanungo;David Orr
#t 2009
#c 2
#% 287253
#% 296375
#% 340948
#% 342740
#% 448194
#% 577224
#% 577377
#% 747910
#% 766414
#% 840846
#% 853819
#% 879593
#% 956648
#% 987209
#% 1035587
#% 1742077
#! Readability is a crucial presentation attribute that web summarization algorithms consider while generating a querybaised web summary. Readability quality also forms an important component in real-time monitoring of commercial search-engine results since readability of web summaries impacts clickthrough behavior, as shown in recent studies, and thus impacts user satisfaction and advertising revenue. The standard approach to computing the readability is to first collect a corpus of random queries and their corresponding search result summaries, and then each summary is then judged by a human for its readabilty quality. An average readability score is then reported. This process is time consuming and expensive. Besides, the manual evaluation process can not be used in the real-time summary generation process. In this paper we propose a machine learning approach to the problem. We use the corpus as described above and extract summary features that we think may characterize readability. We then estimate a model (gradient boosted decision tree) that predicts human judgments given the features. This model can then be used in real time to estimate the readability of new (unseen) web search summaries and also be used in the summary generation process. We present results on approximately 5000 editorial judgments collected over the course of a year and show examples where the model predicts the quality well and where it disagrees with human judgments. We compare the results of the model to previous models of readability, most notably Collins-Thompson-Callan, Fog and Flesch-Kincaid, and see that our model shows substantially better correlation with editorial judgments as measured by Pearson's correlation coefficient. The learning algorithm also provides us with the relative importance of the features used.

#index 1166526
#* Effective latent space graph-based re-ranking model with global consistency
#@ Hongbo Deng;Michael R. Lyu;Irwin King
#t 2009
#c 2
#% 262096
#% 268079
#% 280819
#% 290830
#% 387427
#% 397129
#% 466574
#% 750863
#% 766409
#% 805896
#% 818241
#% 818266
#% 838528
#% 840846
#% 879568
#% 879570
#% 879575
#% 881457
#% 913206
#% 918017
#% 983805
#% 987253
#% 1055681
#% 1055685
#% 1055711
#% 1055712
#% 1176887
#% 1392465
#! Recently the re-ranking algorithms have been quite popular for web search and data mining. However, one of the issues is that those algorithms treat the content and link information individually. Inspired by graph-based machine learning algorithms, we propose a novel and general framework to model the re-ranking algorithm, by regularizing the smoothness of ranking scores over the graph, along with a regularizer on the initial ranking scores (which are obtained by the base ranker). The intuition behind the model is the global consistency over the graph: similar entities are likely to have the same ranking scores with respect to a query. Our approach simultaneously incorporates the content with other explicit or implicit link information in a latent space graph. Then an effective unified re-ranking algorithm is performed on the graph with respect to the query. To illustrate our methodology, we apply the framework to literature retrieval and expert finding applications on DBLP bibliography data. We compare the proposed method with the initial language model method and another PageRank-style re-ranking method. Also, we evaluate the proposed method with varying graphs and settings. Experimental results show that the improvement in our proposed method is consistent and promising.

#index 1166527
#* Top-k aggregation using intersections of ranked inputs
#@ Ravi Kumar;Kunal Punera;Torsten Suel;Sergei Vassilvitskii
#t 2009
#c 2
#% 157880
#% 198335
#% 212665
#% 228097
#% 262099
#% 278831
#% 340886
#% 340887
#% 397608
#% 480330
#% 631988
#% 643566
#% 765466
#% 781169
#% 805864
#% 839358
#% 879611
#% 893126
#% 987216
#% 1015265
#% 1077150
#% 1404894
#! There has been considerable past work on efficiently computing top k objects by aggregating information from multiple ranked lists of these objects. An important instance of this problem is query processing in search engines: One has to combine information from several different posting lists (rankings) of web pages (objects) to obtain the top k web pages to answer user queries. Two particularly well-studied approaches to achieve efficiency in top-k aggregation include early-termination algorithms (e.g., TA and NRA) and preaggregation of some of the input lists. However, there has been little work on a rigorous treatment of combining these approaches. We generalize the TA and NRA algorithms to the case when preaggregated intersection lists are available in addition to the original lists. We show that our versions of TA and NRA continue to remain "instance optimal," a very strong optimality notion that is a highlight of the original TA and NRA algorithms. Using an index of millions of web pages and real-world search engine queries, we empirically characterize the performance gains offered by our new algorithms. We show that the practical benefits of intersection lists can be fully realized only with an early-termination algorithm.

#index 1166528
#* Is Wikipedia link structure different?
#@ Jaap Kamps;Marijn Koolen
#t 2009
#c 2
#% 281214
#% 283833
#% 290830
#% 309151
#% 309749
#% 340928
#% 397126
#% 590525
#% 642992
#% 818255
#% 823342
#% 838460
#% 878916
#% 937549
#% 961564
#% 987251
#% 1415731
#! In this paper, we investigate the difference between Wikipedia and Web link structure with respect to their value as indicators of the relevance of a page for a given topic of request. Our experimental evidence is from two IR test-collections: the .GOV collection used at the TREC Web tracks and the Wikipedia XML Corpus used at INEX. We first perform a comparative analysis of Wikipedia and .GOV link structure and then investigate the value of link evidence for improving search on Wikipedia and on the .GOV domain. Our main findings are: First, Wikipedia link structure is similar to the Web, but more densely linked. Second, Wikipedia's outlinks behave similar to inlinks and both are good indicators of relevance, whereas on the Web the inlinks are more important. Third, when incorporating link evidence in the retrieval model, for Wikipedia the global link evidence fails and we have to take the local context into account.

#index 1166529
#* Less is more: sampling the neighborhood graph makes SALSA better and faster
#@ Marc Najork;Sreenivas Gollapudi;Rina Panigrahy
#t 2009
#c 2
#% 255170
#% 282905
#% 290830
#% 309779
#% 311808
#% 322884
#% 340147
#% 411762
#% 801674
#% 824711
#% 987251
#% 1019074
#% 1130812
#% 1404192
#% 1415743
#! In this paper, we attempt to improve the effectiveness and the efficiency of query-dependent link-based ranking algorithms such as HITS, MAX and SALSA. All these ranking algorithms view the results of a query as nodes in the web graph, expand the result set to include neighboring nodes, and compute scores on the induced neighborhood graph. In previous work it was shown that SALSA in particular is substantially more effective than query-independent link-based ranking algorithms such as PageRank. In this work, we show that whittling down the neighborhood graph through consistent sampling of nodes and edges makes SALSA and its cousins both faster (more efficient) and better (more effective). We offer a hypothesis as to why "less is more", i.e. why using a reduced graph improves performance.

#index 1166530
#* Camera brand congruence in the Flickr social graph
#@ Adish Singla;Ingmar Weber
#t 2009
#c 2
#% 577217
#% 847347
#% 881054
#% 881460
#% 949164
#% 956578
#% 987205
#% 1002007
#% 1055704
#% 1055737
#% 1055763
#% 1127458
#% 1669913
#! Given that my friends on Flickr use cameras of brand X, am I more likely to also use a camera of brand X? Given that one of these friends changes her brand, am I likely to do the same? These are the kind of questions addressed in this work. Direct applications involve personalized advertising in social networks. For our study we crawled a complete connected component of the Flickr friendship graph with a total of 67M edges and 3.9M users. Camera brands and models were assigned to users and time slots according to the model specific meta data pertaining to their images taken during these time slots. Similarly, we used, where provided in a user's profile, information about a user's geographic location and the groups joined on Flickr. Our main findings are the following. First, a pair of friends on Flickr has a significantly higher probability of being congruent, i.e., using the same brand, compared to two random users (27% vs. 19%). Second, the degree of congruence goes up for pairs of friends (i) in the same country (29%), (ii) who both only have very few friends (30%), and (iii) with a very high cliqueness (38%). Third, given that a user changes her camera model between March-May 2007 and March-May 2008, high cliqueness friends are more likely than random users to do the same (54% vs. 48%). Fourth, users using high-end cameras are far more loyal to their brand than users using point-and-shoot cameras, with a probability of staying with the same brand of 60% vs 33%, given that a new camera is bought. Fifth, these "expert" users' brand congruence reaches 66% (!) for high cliqueness friends. To the best of our knowledge this is the first time that the phenomenon of brand congruence is studied for hundreds of thousands of users and over a period of two years.

#index 1166531
#* Finding text reuse on the web
#@ Michael Bendersky;W. Bruce Croft
#t 2009
#c 2
#% 262096
#% 268079
#% 290830
#% 340883
#% 340901
#% 347225
#% 348173
#% 577321
#% 730070
#% 742399
#% 766408
#% 818223
#% 818262
#% 823344
#% 832271
#% 838508
#% 843716
#% 879600
#% 939939
#% 956541
#% 987341
#% 1035587
#% 1055708
#% 1074122
#% 1415722
#! With the overwhelming number of reports on similar events originating from different sources on the web, it is often hard, using existing web search paradigms, to find the original source of "facts", statements, rumors, and opinions, and to track their development. Several techniques have been previously proposed for detecting such text reuse between different sources, however these techniques have been tested against relatively small and homogeneous TREC collections. In this work, we test the feasibility of text reuse detection techniques in the setting of web search. In addition to text reuse detection, we develop a novel technique that addresses the unique challenges of finding original sources on the web, such as defining a timeline. We also explore the use of link analysis for identifying reliable and relevant reports. Our experimental results show that the proposed techniques can operate on the scale of the web, are significantly more accurate than standard web search for finding text reuse, and provide a richer representation for tracking the information flow.

#index 1166532
#* Speeding up algorithms on compressed web graphs
#@ Chinmay Karande;Kumar Chellapilla;Reid Andersen
#t 2009
#c 2
#% 194127
#% 268079
#% 281214
#% 290830
#% 309779
#% 502368
#% 577329
#% 641979
#% 740507
#% 754117
#% 794132
#% 805897
#% 869485
#% 957995
#% 1002377
#% 1035579
#! A variety of lossless compression schemes have been proposed to reduce the storage requirements of web graphs. One successful approach is virtual node compression [7], in which often-used patterns of links are replaced by links to virtual nodes, creating a compressed graph that succinctly represents the original. In this paper, we show that several important classes of web graph algorithms can be extended to run directly on virtual node compressed graphs, such that their running times depend on the size of the compressed graph rather than the original. These include algorithms for link analysis, estimating the size of vertex neighborhoods, and a variety of algorithms based on matrix-vector products and random walks. Similar speed-ups have been obtained previously for classical graph algorithms like shortest paths and maximum bipartite matching. We measure the performance of our modified algorithms on several publicly available web graph datasets, and demonstrate significant empirical speedups that nearly match the compression ratios.

#index 1166533
#* The web changes everything: understanding the dynamics of web content
#@ Eytan Adar;Jaime Teevan;Susan T. Dumais;Jonathan L. Elsas
#t 2009
#c 2
#% 232912
#% 344447
#% 344929
#% 397161
#% 480136
#% 577370
#% 754058
#% 754102
#% 778477
#% 834610
#% 978374
#% 987211
#% 1047435
#% 1055715
#% 1124069
#% 1656339
#% 1684791
#! The Web is a dynamic, ever changing collection of information. This paper explores changes in Web content by analyzing a crawl of 55,000 Web pages, selected to represent different user visitation patterns. Although change over long intervals has been explored on random (and potentially unvisited) samples of Web pages, little is known about the nature of finer grained changes to pages that are actively consumed by users, such as those in our sample. We describe algorithms, analyses, and models for characterizing changes in Web content, focusing on both time (by using hourly and sub-hourly crawls) and structure (by looking at page-, DOM-, and term-level changes). Change rates are higher in our behavior-based sample than found in previous work on randomly sampled pages, with a large portion of pages changing more than hourly. Detailed content and structure analyses identify stable and dynamic content within each page. The understanding of Web change we develop in this paper has implications for tools designed to help people interact with dynamic Web content, such as search engines, advertising, and Web browsers.

#index 1172627
#* Searching and exploring controlled vocabularies
#@ Alasdair J. G. Gray;Norman Gray;Iadh Ounis
#t 2009
#c 2
#% 27049
#% 280849
#% 298183
#% 519567
#! Within most domains of discourse, there exists different terminology used by distinct sub-groups. Often the terms used can be, or have already been, organised into controlled vocabularies which can be encoded into SKOS, a W3C standard for representing vocabularies. This terminology can then be used to help users to search for and discover resources. This requires a search mechanism to go from a user-supplied string to a vocabulary concept. In this paper, we discuss the issues encountered in developing a web service for searching and exploring the concepts in SKOS encoded astronomical vocabularies. Our prototype service takes in a query and responds with the concepts which are the "best match". It then supports the user in exploring the concepts' formal definition and alternative forms, as well as their relationship to other concepts. When we add mappings between the concepts in different vocabularies (where available), these further enrich the explorations of vocabulary concepts.

#index 1172628
#* Terminological cleansing for improved information retrieval based on ontological terms
#@ Antonio Jimeno-Yepes;Rafael Berlanga-Llavori;Dietrich Rebholz-Schuhmann
#t 2009
#c 2
#% 144029
#% 169729
#% 194301
#% 218978
#% 262067
#% 262084
#% 262096
#% 280851
#% 340901
#% 375017
#% 533966
#% 643003
#% 757423
#% 810637
#% 818240
#% 834876
#% 838530
#% 1041733
#! Ontologies are frequently used in information retrieval being their main applications the expansion of queries, semantic indexing of documents and the organization of search results. However, the optimization of an ontology to perform information retrieval tasks is still unclear. In this paper, we propose an ontology query model to analyze the usefulness of ontologies in effectively performing document searches. Moreover, we propose a series of heuristic techniques that optimize ontologies for information retrieval tasks. Preliminary results demonstrate that current domain ontologies provide enough information to support user requests and that ontologies might be improved with simple methods.

#index 1172629
#* Faceted search and retrieval based on semantically annotated product family ontology
#@ Soon Chong Johnson Lim;Ying Liu;Wing Bun Lee
#t 2009
#c 2
#% 614038
#% 730048
#% 730148
#% 748499
#% 766439
#% 780158
#% 780804
#% 805861
#% 817501
#% 818273
#% 828969
#% 832382
#% 839929
#% 905217
#% 939335
#% 949029
#% 956572
#% 991554
#% 996200
#% 997144
#% 1001362
#% 1008815
#% 1019104
#% 1026932
#% 1026943
#% 1069832
#% 1074095
#% 1345576
#% 1703085
#! With the advent of various services and applications of Semantic Web, semantic annotation had emerged as an important research area. The use of semantically annotated ontology had been evident in numerous information processing and retrieval tasks. One of such tasks is utilizing the semantically annotated ontology in product design which is able to suggest many important applications that are critical to aid various design related tasks. However, ontology development in design engineering remains a time consuming and tedious task that demands tremendous human efforts. In the context of product family design, management of different product information that features efficient indexing, update, navigation, search and retrieval across product families is both desirable and challenging. This paper attempts to address this issue by proposing an information management and retrieval framework based on the semantically annotated product family ontology. Particularly, we propose a document profile (DP) model to suggest semantic tags for annotation purpose. Using a case study of digital camera families, we illustrate how the faceted search and retrieval of product information can be accomplished based on the semantically annotated camera family ontology. Lastly, we briefly discuss some further research and application in design decision support, e.g. commonality and variety, based on the semantically annotated product family ontology.

#index 1172630
#* A hybrid approach to item recommendation in folksonomies
#@ Robert Wetzker;Winfried Umbrath;Alan Said
#t 2009
#c 2
#% 734590
#% 769895
#% 801785
#% 813966
#% 879594
#% 1035588
#% 1074117
#% 1074129
#% 1127455
#% 1650298
#% 1667787
#! In this paper we consider the problem of item recommendation in collaborative tagging communities, so called folksonomies, where users annotate interesting items with tags. Rather than following a collaborative filtering or annotation-based approach to recommendation, we extend the probabilistic latent semantic analysis (PLSA) approach and present a unified recommendation model which evolves from item user and item tag co-occurrences in parallel. The inclusion of tags reduces known collaborative filtering problems related to overfitting and allows for higher quality recommendations. Experimental results on a large snapshot of the delicious bookmarking service show the scalability of our approach and an improved recommendation quality compared to two-mode collaborative or annotation based methods.

#index 1172631
#* Combining named entities and tags for novel sentence detection
#@ Yi Zhang;Flora S. Tsai
#t 2009
#c 2
#% 287196
#% 397133
#% 575313
#% 577297
#% 643014
#% 766444
#% 766460
#% 786511
#% 838537
#% 882738
#% 893432
#% 982760
#% 1043035
#! Novel sentence detection aims at identifying novel information from an incoming stream of sentences. Our research applies named entity recognition (NER) and part-of-speech (POS) tagging on sentence-level novelty detection and proposes a mixed method to utilize these two techniques. Furthermore, we discuss the performance when setting different history sentence sets. Experimental results of different approaches on TREC'04 Novelty Track show that our new combined method outperforms some other novelty detection methods in terms of precision and recall. The experimental observations of each approach are also discussed.

#index 1172632
#* Query independent measures of annotation and annotator impact
#@ James Lanagan;Alan F. Smeaton
#t 2009
#c 2
#% 220708
#% 220711
#% 309150
#% 766456
#% 1303064
#% 1783083
#% 1858377
#! The modern-day web-user plays a far more active role in the creation of content for the web as a whole. In this paper we present Annoby, a free-text annotation system built to give users a more interactive experience of the events of the Rugby World Cup 2007. Annotations can be used for query-independent ranking of both the annotations and the original recorded video footage (or documents) which has been annotated, based on the social interactions of a community of users. We present two algorithms, AuthorRank and MessageRank, designed to take advantage of these interactions so as to provide a means of ranking documents by their social impact.

#index 1172633
#* Evaluation of semantic events for legal case retrieval
#@ K. Tamsin Maxwell;Jon Oberlander;Victor Lavrenko
#t 2009
#c 2
#% 894066
#% 939748
#% 1249504
#! Legal argument is based on the facts of a case as well as legal issues, concepts and factors. We assess the feasibility of using semantic events extracted from court judgements to adequately represent the legal concepts and factual content of cases for legal information retrieval (IR). Results of a preliminary study show extracted events are attributed with 74% accuracy, 72% legal importance, and representing 86% of sentence meaning.

#index 1172634
#* Collaborative annotation for context-aware retrieval
#@ Stefano Mizzaro;Elena Nazzi;Luca Vassena
#t 2009
#c 2
#% 549295
#% 1047345
#% 1093779
#! We discuss how collaborative annotations can be exploited to simplify and improve the management of context and resources in the context-aware retrieval field. We apply this approach to our Context Aware Browser, a general purpose solution to Web content perusal by means of mobile devices, based on the user's context. Instead of relying on a pool of experts and on a rigid categorization, as it is usually done in the context-aware field, our solution allows the crowd of users to model, control and manage the contextual knowledge through collaboration and participation. We propose two models and we outline an example of application.

#index 1172635
#* Long, often quite boring, notes of meetings
#@ Maarten Marx
#t 2009
#c 2
#% 378409
#% 481923
#% 824798
#% 893089
#% 909444
#% 1055754
#% 1411897
#! Meeting notes are documents which contain lots of structure. This structure is often implicit in layout and reserved words. On the other hand, since meetings tend to occur regularly and are repeated for long periods of time, this structure is often (semi-)formalized. This makes these documents suitable for automatic semantic annotation efforts. We describe the annotation we performed on the notes of more than 20 years of Dutch parliamentary debates. We annotated every word spoken in parliament with 1) the speaker, 2) her party at the time of speaking, 3) her role/function in parliament and 4) the iso-date. These annotations yield numerous new ways of searching, browsing, mining and summarizing these documents. Meetings are always too long, whence so are their verbatim notes. But of course they contain valuable information and notes have to be consulted from time to time. In this paper we show that semantic annotation can make finding things easier, and more fun.

#index 1355015
#* Proceedings of the third ACM international conference on Web search and data mining
#@ Brian D. Davison;Torsten Suel;Nick Craswell;Bing Liu
#t 2010
#c 2
#! We welcome you to the Third ACM International Conference on Web Search and Data Mining (WSDM) held February 3--6, 2010 in New York City. As a premier conference in the field, WSDM 2010 provides a highly competitive forum for reporting the latest developments in the research and application of Web search and data mining. We are pleased to present the proceedings of the conference as its published record. WSDM (pronounced "wisdom") is a young conference for research in the areas of search and retrieval, Web mining, economics implications, and in depth analysis of accuracy and performance of search and mining systems. Although it is only in its third year, it has already witnessed significant growth. As evidence of that, WSDM 2010 received a record 290 submissions, representing a 70% increase compared to WSDM 2009. The conference accepted 45 papers (15.5%). The authors of submitted papers come from 35 countries and regions. Authors of accepted papers are from 11 countries.

#index 1355016
#* Leveraging temporal dynamics of document content in relevance ranking
#@ Jonathan L. Elsas;Susan T. Dumais
#t 2010
#c 2
#% 118741
#% 255137
#% 411762
#% 577370
#% 642992
#% 730070
#% 750863
#% 754058
#% 766408
#% 805878
#% 810054
#% 879639
#% 960414
#% 987257
#% 1055715
#% 1055875
#% 1124069
#% 1166533
#% 1183221
#% 1270766
#% 1392437
#! Many web documents are dynamic, with content changing in varying amounts at varying frequencies. However, current document search algorithms have a static view of the document content, with only a single version of the document in the index at any point in time. In this paper, we present the first published analysis of using the temporal dynamics of document content to improve relevance ranking. We show that there is a strong relationship between the amount and frequency of content change and relevance. We develop a novel probabilistic document ranking algorithm that allows differential weighting of terms based on their temporal characteristics. By leveraging such content dynamics we show significant performance improvements for navigational queries.

#index 1355017
#* Towards recency ranking in web search
#@ Anlei Dong;Yi Chang;Zhaohui Zheng;Gilad Mishne;Jing Bai;Ruiqiang Zhang;Karolina Buchner;Ciya Liao;Fernando Diaz
#t 2010
#c 2
#% 411762
#% 564279
#% 577220
#% 577224
#% 765412
#% 805848
#% 810054
#% 824716
#% 983820
#% 987203
#% 1052713
#% 1166523
#% 1176882
#% 1227620
#% 1270766
#% 1783036
#! In web search, recency ranking refers to ranking documents by relevance which takes freshness into account. In this paper, we propose a retrieval system which automatically detects and responds to recency sensitive queries. The system detects recency sensitive queries using a high precision classifier. The system responds to recency sensitive queries by using a machine learned ranking model trained for such queries. We use multiple recency features to provide temporal evidence which effectively represents document recency. Furthermore, we propose several training methodologies important for training recency sensitive rankers. Finally, we develop new evaluation metrics for recency sensitive queries. Our experiments demonstrate the efficacy of the proposed approaches.

#index 1355018
#* Ranking mechanisms in twitter-like forums
#@ Anish Das Sarma;Atish Das Sarma;Sreenivas Gollapudi;Rina Panigrahy
#t 2010
#c 2
#% 1183200
#! We study the problem of designing a mechanism to rank items in forums by making use of the user reviews such as thumb and star ratings. We compare mechanisms where forum users rate individual posts and also mechanisms where the user is asked to perform a pairwise comparison and state which one is better. The main metric used to evaluate a mechanism is the ranking accuracy vs the cost of reviews, where the cost is measured as the average number of reviews used per post. We show that for many reasonable probability models, there is no thumb (or star) based ranking mechanism that can produce approximately accurate rankings with bounded number of reviews per item. On the other hand we provide a review mechanism based on pairwise comparisons which achieves approximate rankings with bounded cost. We have implemented a system, shoutvelocity, which is a twitter-like forum but items (i.e., tweets in Twitter) are rated by using comparisons. For each new item the user who posts the item is required to compare two previous entries. This ensures that over a sequence of n posts, we get at least n comparisons requiring one review per item on average. Our mechanism uses this sequence of comparisons to obtain a ranking estimate. It ensures that every item is reviewed at least once and winning entries are reviewed more often to obtain better estimates of top items.

#index 1355019
#* Learning concept importance using a weighted dependence model
#@ Michael Bendersky;Donald Metzler;W. Bruce Croft
#t 2010
#c 2
#% 46803
#% 169780
#% 262092
#% 262096
#% 411762
#% 577224
#% 766409
#% 818262
#% 976952
#% 987226
#% 987229
#% 987231
#% 987356
#% 1055706
#% 1055856
#% 1074081
#% 1074098
#% 1074112
#% 1227608
#% 1227636
#% 1227647
#% 1227747
#% 1268491
#% 1715627
#! Modeling query concepts through term dependencies has been shown to have a significant positive effect on retrieval performance, especially for tasks such as web search, where relevance at high ranks is particularly critical. Most previous work, however, treats all concepts as equally important, an assumption that often does not hold, especially for longer, more complex queries. In this paper, we show that one of the most effective existing term dependence models can be naturally extended by assigning weights to concepts. We demonstrate that the weighted dependence model can be trained using existing learning-to-rank techniques, even with a relatively small number of training queries. Our study compares the effectiveness of both endogenous (collection-based) and exogenous (based on external sources) features for determining concept importance. To test the weighted dependence model, we perform experiments on both publicly available TREC corpora and a proprietary web corpus. Our experimental results indicate that our model consistently and significantly outperforms both the standard bag-of-words model and the unweighted term dependence model, and that combining endogenous and exogenous features generally results in the best retrieval effectiveness.

#index 1355020
#* Query reformulation using anchor text
#@ Van Dang;Bruce W. Croft
#t 2010
#c 2
#% 268079
#% 298183
#% 310567
#% 330617
#% 340901
#% 577224
#% 643069
#% 730090
#% 748465
#% 748619
#% 754125
#% 869501
#% 879567
#% 939973
#% 987272
#% 1074112
#% 1074170
#% 1130855
#% 1181094
#% 1227592
#% 1227647
#% 1712595
#! Query reformulation techniques based on query logs have been studied as a method of capturing user intent and improving retrieval effectiveness. The evaluation of these techniques has primarily, however, focused on proprietary query logs and selected samples of queries. In this paper, we suggest that anchor text, which is readily available, can be an effective substitute for a query log and study the effectiveness of a range of query reformulation techniques (including log-based stemming, substitution, and expansion) using standard TREC collections. Our results show that log-based query reformulation techniques are indeed effective with standard collections, but expansion is a much safer form of query modification than word substitution. We also show that using anchor text as a simulated query log is as least as effective as a real log for these techniques.

#index 1355021
#* Tagging human knowledge
#@ Paul Heymann;Andreas Paepcke;Hector Garcia-Molina
#t 2010
#c 2
#% 348165
#% 855601
#% 881054
#% 956515
#% 1065406
#% 1074117
#% 1074164
#% 1131211
#% 1170223
#! A fundamental premise of tagging systems is that regular users can organize large collections for browsing and other tasks using uncontrolled vocabularies. Until now, that premise has remained relatively unexamined. Using library data, we test the tagging approach to organizing a collection. We find that tagging systems have three major large scale organizational features: consistency, quality, and completeness. In addition to testing these features, we present results suggesting that users produce tags similar to the topics designed by experts, that paid tagging can effectively supplement tags in a tagging system, and that information integration may be possible across tagging systems.

#index 1355022
#* Precomputing search features for fast and accurate query classification
#@ Venkatesh Ganti;Arnd Christian König;Xiao Li
#t 2010
#c 2
#% 279755
#% 378388
#% 492912
#% 569754
#% 765529
#% 816392
#% 844287
#% 879581
#% 956641
#% 987221
#% 987275
#% 1074093
#% 1130910
#% 1214736
#% 1227616
#% 1227678
#% 1269907
#% 1392432
#! Query intent classification is crucial for web search and advertising. It is known to be challenging because web queries contain less than three words on average, and so provide little signal to base classification decisions on. At the same time, the vocabulary used in search queries is vast: thus, classifiers based on word-occurrence have to deal with a very sparse feature space, and often require large amounts of training data. Prior efforts to address the issue of feature sparseness augmented the feature space using features computed from the results obtained by issuing the query to be classified against a web search engine. However, these approaches induce high latency, making them unacceptable in practice. In this paper, we propose a new class of features that realizes the benefit of search-based features without high latency. These leverage co-occurrence between the query keywords and tags applied to documents in search results, resulting in a significant boost to web query classification accuracy. By pre-computing the tag incidence for a suitably chosen set of keyword-combinations, we are able to generate the features online with low latency and memory requirements. We evaluate the accuracy of our approach using a large corpus of real web queries in the context of commercial search.

#index 1355023
#* I tag, you tag: translating tags for advanced user models
#@ Robert Wetzker;Carsten Zimmermann;Christian Bauckhage;Sahin Albayrak
#t 2010
#c 2
#% 268079
#% 855601
#% 869504
#% 881054
#% 955010
#% 956515
#% 1035588
#% 1055739
#% 1074070
#% 1074115
#% 1074117
#% 1127455
#% 1127456
#% 1127482
#% 1130827
#% 1131211
#% 1166510
#% 1172630
#% 1190119
#% 1214694
#% 1215464
#% 1227638
#% 1227644
#% 1415717
#% 1655418
#% 1667787
#! Collaborative tagging services (folksonomies) have been among the stars of the Web 2.0 era. They allow their users to label diverse resources with freely chosen keywords (tags). Our studies of two real-world folksonomies unveil that individual users develop highly personalized vocabularies of tags. While these meet individual needs and preferences, the considerable differences between personal tag vocabularies (personomies) impede services such as social search or customized tag recommendation. In this paper, we introduce a novel user-centric tag model that allows us to derive mappings between personal tag vocabularies and the corresponding folksonomies. Using these mappings, we can infer the meaning of user-assigned tags and can predict choices of tags a user may want to assign to new items. Furthermore, our translational approach helps in reducing common problems related to tag ambiguity, synonymous tags, or multilingualism. We evaluate the applicability of our method in tag recommendation and tag-based social search. Extensive experiments show that our translational model improves the prediction accuracy in both scenarios.

#index 1355024
#* Pairwise interaction tensor factorization for personalized tag recommendation
#@ Steffen Rendle;Lars Schmidt-Thieme
#t 2010
#c 2
#% 316143
#% 734592
#% 840924
#% 1074117
#% 1083671
#% 1127455
#% 1130816
#% 1156304
#% 1176909
#% 1176959
#% 1214666
#% 1214694
#% 1417104
#% 1667787
#! Tagging plays an important role in many recent websites. Recommender systems can help to suggest a user the tags he might want to use for tagging a specific item. Factorization models based on the Tucker Decomposition (TD) model have been shown to provide high quality tag recommendations outperforming other approaches like PageRank, FolkRank, collaborative filtering, etc. The problem with TD models is the cubic core tensor resulting in a cubic runtime in the factorization dimension for prediction and learning. In this paper, we present the factorization model PITF (Pairwise Interaction Tensor Factorization) which is a special case of the TD model with linear runtime both for learning and prediction. PITF explicitly models the pairwise interactions between users, items and tags. The model is learned with an adaption of the Bayesian personalized ranking (BPR) criterion which originally has been introduced for item recommendation. Empirically, we show on real world datasets that this model outperforms TD largely in runtime and even can achieve better prediction quality. Besides our lab experiments, PITF has also won the ECML/PKDD Discovery Challenge 2009 for graph-based tag recommendation.

#index 1355025
#* fLDA: matrix factorization through latent dirichlet allocation
#@ Deepak Agarwal;Bee-Chung Chen
#t 2010
#c 2
#% 220709
#% 283169
#% 397155
#% 722904
#% 805841
#% 823392
#% 840924
#% 881537
#% 989580
#% 1000502
#% 1073982
#% 1083671
#% 1083696
#% 1116993
#% 1190066
#% 1211765
#% 1211838
#% 1214623
#% 1214642
#% 1270334
#! We propose fLDA, a novel matrix factorization method to predict ratings in recommender system applications where a "bag-of-words" representation for item meta-data is natural. Such scenarios are commonplace in web applications like content recommendation, ad targeting and web search where items are articles, ads and web pages respectively. Because of data sparseness, regularization is key to good predictive accuracy. Our method works by regularizing both user and item factors simultaneously through user features and the bag of words associated with each item. Specifically, each word in an item is associated with a discrete latent factor often referred to as the topic of the word; item topics are obtained by averaging topics across all words in an item. Then, user rating on an item is modeled as user's affinity to the item's topics where user affinity to topics (user factors) and topic assignments to words in items (item factors) are learned jointly in a supervised fashion. To avoid overfitting, user and item factors are regularized through Gaussian linear regression and Latent Dirichlet Allocation (LDA) priors respectively. We show our model is accurate, interpretable and handles both cold-start and warm-start scenarios seamlessly through a single model. The efficacy of our method is illustrated on benchmark datasets and a new dataset from Yahoo! Buzz where fLDA provides superior predictive accuracy in cold-start scenarios and is comparable to state-of-the-art methods in warm-start scenarios. As a by-product, fLDA also identifies interesting topics that explains user-item interactions. Our method also generalizes a recently proposed technique called supervised LDA (sLDA) to collaborative filtering applications. While sLDA estimates item topic vectors in a supervised fashion for a single regression, fLDA incorporates multiple regressions (one for each user) in estimating the item factors.

#index 1355026
#* Coupled semi-supervised learning for information extraction
#@ Andrew Carlson;Justin Betteridge;Richard C. Wang;Estevam R. Hruschka, Jr.;Tom M. Mitchell
#t 2010
#c 2
#% 236497
#% 252011
#% 278107
#% 283180
#% 301241
#% 504443
#% 722926
#% 756964
#% 817462
#% 939602
#% 940010
#% 1023420
#% 1176941
#% 1183375
#% 1209696
#% 1264744
#% 1264778
#% 1264788
#% 1275192
#% 1338552
#% 1338685
#% 1705508
#! We consider the problem of semi-supervised learning to extract categories (e.g., academic fields, athletes) and relations (e.g., PlaysSport(athlete, sport)) from web pages, starting with a handful of labeled training examples of each category or relation, plus hundreds of millions of unlabeled web documents. Semi-supervised training using only a few labeled examples is typically unreliable because the learning task is underconstrained. This paper pursues the thesis that much greater accuracy can be achieved by further constraining the learning task, by coupling the semi-supervised training of many extractors for different categories and relations. We characterize several ways in which the training of category and relation extractors can be coupled, and present experimental results demonstrating significantly improved accuracy as a result.

#index 1355027
#* Adapting information bottleneck method for automatic construction of domain-oriented sentiment lexicon
#@ Weifu Du;Songbo Tan;Xueqi Cheng;Xiaochun Yun
#t 2010
#c 2
#% 115608
#% 722308
#% 746885
#% 769892
#% 815915
#% 838521
#% 854646
#% 855282
#% 939348
#% 939848
#% 939896
#% 1019180
#% 1200477
#% 1251688
#% 1261566
#% 1261670
#% 1280783
#% 1292694
#! Domain-oriented sentiment lexicons are widely used for fine-grained sentiment analysis on reviews; therefore, the automatic construction of domain-oriented sentiment lexicon is a fundamental and important task for sentiment analysis research. Most of existing construction approaches take only the kind of relationships between words into account, which makes them have a lot of room for improvement. This paper proposes an adapted information bottleneck method for the construction of domain-oriented sentiment lexicon. This approach can naturally make full use of the mutual reinforcement between documents and words by fusing three kinds of relationships either from words to documents or from words to words; either homogeneous or heterogeneous; either within-domain or cross-domain. The experimental results demonstrate that proposed method could dramatically improve the accuracy of the baseline approach on the construction of out-of-domain sentiment lexicon.

#index 1355028
#* Data-oriented content query system: searching for data into text on the web
#@ Mianwei Zhou;Tao Cheng;Kevin Chen-Chuan Chang
#t 2010
#c 2
#% 730022
#% 754068
#% 805883
#% 854668
#% 869535
#% 1022234
#% 1127426
#% 1206687
#% 1206702
#% 1261582
#! As the Web provides rich data embedded in the immense contents inside pages, we witness many ad-hoc efforts for exploiting fine granularity information across Web text, such as Web information extraction, typed-entity search, and question answering. To unify and generalize these efforts, this paper proposes a general search system--Data-oriented Content Query System(DoCQS)--to search directly into document contents for finding relevant values of desired data types. Motivated by the current limitations, we start by distilling the essential capabilities needed by such content querying. The capabilities call for a conceptually relational model, upon which we design a powerful Content Query Language (CQL). For efficient processing, we design novel index structures and query processing algorithms. We evaluate our proposal over two concrete domains of realistic Web corpora, demonstrating that our query language is rather flexible and expressive, and our query processing is efficient with reasonable index overhead.

#index 1355029
#* Corroborating information from disagreeing views
#@ Alban Galland;Serge Abiteboul;Am&#233/lie Marian;Pierre Senellart
#t 2010
#c 2
#% 268079
#% 273687
#% 330616
#% 577330
#% 810020
#% 854668
#% 855601
#% 874971
#% 889107
#% 989682
#% 1077150
#% 1289516
#% 1328156
#% 1668093
#! We consider a set of views stating possibly conflicting facts. Negative facts in the views may come, e.g., from functional dependencies in the underlying database schema. We want to predict the truth values of the facts. Beyond simple methods such as voting (typically rather accurate), we explore techniques based on "corroboration", i.e., taking into account trust in the views. We introduce three fixpoint algorithms corresponding to different levels of complexity of an underlying probabilistic model. They all estimate both truth values of facts and trust in the views. We present experimental studies on synthetic and real-world data. This analysis illustrates how and in which context these methods improve corroboration results over baseline methods. We believe that corroboration can serve in a wide range of applications such as source selection in the semantic Web, data quality assessment or semantic annotation cleaning in social networks. This work sets the bases for a wide range of techniques for solving these more complex problems.

#index 1355030
#* Ranking with query-dependent loss for web search
#@ Jiang Bian;Tie-Yan Liu;Tao Qin;Hongyuan Zha
#t 2010
#c 2
#% 236497
#% 262096
#% 288541
#% 340899
#% 387427
#% 411762
#% 577224
#% 590523
#% 642975
#% 642982
#% 734915
#% 754059
#% 805878
#% 840846
#% 857180
#% 879581
#% 907517
#% 983820
#% 987228
#% 987326
#% 1074021
#% 1074065
#! Queries describe the users' search intent and therefore they play an essential role in the context of ranking for information retrieval and Web search. However, most of existing approaches for ranking do not explicitly take into consideration the fact that queries vary significantly along several dimensions and entail different treatments regarding the ranking models. In this paper, we propose to incorporate query difference into ranking by introducing query-dependent loss functions. In the context of Web search, query difference is usually represented as different query categories; and, queries are usually classified according to search intent such as navigational, informational and transactional queries. Based on the observation that such kind of query categorization has high correlation with the user's different expectation on the result accuracy on different rank positions, we develop position-sensitive query-dependent loss functions exploring such kind of query categorization. Beyond the simple learning method that builds ranking functions with pre-defined query categorization, we further propose a new method that learns both ranking functions and query categorization simultaneously. We apply the query-dependent loss functions to two particular ranking algorithms, RankNet and ListMLE. Experimental results demonstrate that query-dependent loss functions can be exploited to significantly improve the accuracy of learned ranking functions. We also show that the ranking function jointly learned with query categorization can achieve better performance than that learned with pre-defined query categorization. Finally, we provide analysis and conduct additional experiments to gain deeper understanding on the advantages of ranking with query-dependent loss functions over other query-dependent ranking approaches and query-independent approaches.

#index 1355031
#* IntervalRank: isotonic regression with listwise and pairwise constraints
#@ Taesup Moon;Alex Smola;Yi Chang;Zhaohui Zheng
#t 2010
#c 2
#% 73441
#% 577224
#% 734915
#% 757953
#% 840846
#% 983820
#% 987228
#% 987240
#% 987241
#% 1035577
#% 1074021
#% 1211722
#% 1292737
#% 1354575
#% 1817412
#! Ranking a set of retrieved documents according to their relevance to a given query has become a popular problem at the intersection of web search, machine learning, and information retrieval. Recent work on ranking focused on a number of different paradigms, namely, pointwise, pairwise, and list-wise approaches. Each of those paradigms focuses on a different aspect of the dataset while largely ignoring others. The current paper shows how a combination of them can lead to improved ranking performance and, moreover, how it can be implemented in log-linear time. The basic idea of the algorithm is to use isotonic regression with adaptive bandwidth selection per relevance grade. This results in an implicitly-defined loss function which can be minimized efficiently by a subgradient descent procedure. Experimental results show that the resulting algorithm is competitive on both commercial search engine data and publicly available LETOR data sets.

#index 1355032
#* An optimization framework for query recommendation
#@ Aris Anagnostopoulos;Luca Becchetti;Carlos Castillo;Aristides Gionis
#t 2010
#c 2
#% 728105
#% 757527
#% 805200
#% 1130868
#% 1130878
#% 1173699
#% 1190094
#% 1599267
#% 1712595
#! Query recommendation is an integral part of modern search engines. The goal of query recommendation is to facilitate users while searching for information. Query recommendation also allows users to explore concepts related to their information needs. In this paper, we present a formal treatment of the problem of query recommendation. In our framework we model the querying behavior of users by a probabilistic reformula- tion graph, or query-flow graph [Boldi et al. CIKM 2008]. A sequence of queries submitted by a user can be seen as a path on this graph. Assigning score values to queries allows us to define suitable utility functions and to consider the expected utility achieved by a reformulation path on the query-flow graph. Providing recommendations can be seen as adding shortcuts in the query-flow graph that "nudge" the reformulation paths of users, in such a way that users are more likely to follow paths with larger expected utility. We discuss in detail the most important questions that arise in the proposed framework. In particular, we provide examples of meaningful utility functions to optimize, we discuss how to estimate the effect of recommendations on the reformulation probabilities, we address the complexity of the optimization problems that we consider, we suggest efficient algorithmic solutions, and we validate our models and algorithms with extensive experimentation. Our techniques can be applied to other scenarios where user behavior can be modeled as a Markov process.

#index 1355033
#* Improving quality of training data for learning to rank using click-through data
#@ Jingfang Xu;Chuanliang Chen;Gu Xu;Hang Li;Elbio Renato Torres Abib
#t 2010
#c 2
#% 208931
#% 262105
#% 464434
#% 577224
#% 734915
#% 818221
#% 818262
#% 823348
#% 869536
#% 876037
#% 987222
#% 987226
#% 987241
#% 1074134
#% 1130811
#% 1130854
#% 1166522
#% 1173702
#% 1173703
#% 1227581
#! In information retrieval, relevance of documents with respect to queries is usually judged by humans, and used in evaluation and/or learning of ranking functions. Previous work has shown that certain level of noise in relevance judgments has little effect on evaluation, especially for comparison purposes. Recently learning to rank has become one of the major means to create ranking models in which the models are automatically learned from the data derived from a large number of relevance judgments. As far as we know, there was no previous work about quality of training data for learning to rank, and this paper tries to study the issue. Specifically, we address three problems. Firstly, we show that the quality of training data labeled by humans has critical impact on the performance of learning to rank algorithms. Secondly, we propose detecting relevance judgment errors using click-through data accumulated at a search engine. Two discriminative models, referred to as sequential dependency model and full dependency model, are proposed to make the detection. Both models consider the conditional dependency of relevance labels and thus are more powerful than the conditionally independent model previously proposed for other tasks. Finally, we verify that using training data in which the errors are detected and corrected by our method, we can improve the performance of learning to rank algorithms.

#index 1355034
#* A model to estimate intrinsic document relevance from the clickthrough logs of a web search engine
#@ Georges Dupret;Ciya Liao
#t 2010
#c 2
#% 411762
#% 577224
#% 590523
#% 766472
#% 818221
#% 879565
#% 879567
#% 946521
#% 956495
#% 987228
#% 1035578
#% 1074092
#% 1130868
#% 1166517
#% 1166521
#% 1190055
#% 1227581
#% 1275193
#! We propose a new model to interpret the clickthrough logs of a web search engine. This model is based on explicit assumptions on the user behavior. In particular, we draw conclusions on a document relevance by observing the user behavior after he examined the document and not based on whether a user clicks or not a document url. This results in a model based on intrinsic relevance, as opposed to perceived relevance. We use the model to predict document relevance and then use this as feature for a "Learning to Rank" machine learning algorithm. Comparing the ranking functions obtained by training the algorithm with and without the new feature we observe surprisingly good results. This is particularly notable given that the baseline we use is the heavily optimized ranking function of a leading commercial search engine. A deeper analysis shows that the new feature is particularly helpful for non navigational queries and queries with a large abandonment rate or a large average number of queries per session. This is important because these types of query is considered to be the most difficult to solve.

#index 1355035
#* Large scale query log analysis of re-finding
#@ Sarah K. Tyler;Jaime Teevan
#t 2010
#c 2
#% 186340
#% 194299
#% 233808
#% 247268
#% 284796
#% 643057
#% 655487
#% 766447
#% 805898
#% 818259
#% 832099
#% 859488
#% 881544
#% 919706
#% 954970
#% 956552
#% 987211
#% 987212
#% 1047435
#% 1047436
#% 1074071
#% 1083899
#% 1089474
#% 1392483
#! Although Web search engines are targeted towards helping people find new information, people regularly use them to re-find Web pages they have seen before. Researchers have noted the existence of this phenomenon, but relatively little is understood about how re-finding behavior differs from the finding of new information. This paper dives deeply into the differences via analysis of three large-scale data sources: 1) query logs (queries, clicks, result impressions), 2) Web browsing logs (URL visits), and 3) a daily Web crawl (page content). It appears that people learn valuable information about the pages they find that helps them re-find what they are looking for later; compared to the initial finding query, re-finding queries are typically shorter, and rank the re-found URL higher. While many instances of re-finding probably serve as a type of bookmark for a known URL, others seem to represent the resumption of a previous task; results clicked at the end of a session are more likely than those at the beginning to be re-found during a later session, while re-finding is more likely to happen at the beginning of a session than at the end. Additionally, we observe differences in cross-session and intra-session re-finding that may indicate different types of re-finding tasks. Our findings suggest there is a rich opportunity for search engines to take advantage of re-finding behavior as a means to improve the search experience.

#index 1355036
#* Anatomy of the long tail: ordinary people with extraordinary tastes
#@ Sharad Goel;Andrei Broder;Evgeniy Gabrilovich;Bo Pang
#t 2010
#c 2
#% 306468
#% 323131
#% 918842
#% 987358
#% 1130852
#! The success of "infinite-inventory" retailers such as Amazon.com and Netflix has been ascribed to a "long tail" phenomenon. To wit, while the majority of their inventory is not in high demand, in aggregate these "worst sellers," unavailable at limited-inventory competitors, generate a significant fraction of total revenue. The long tail phenomenon, however, is in principle consistent with two fundamentally different theories. The first, and more popular hypothesis, is that a majority of consumers consistently follow the crowds and only a minority have any interest in niche content; the second hypothesis is that everyone is a bit eccentric, consuming both popular and specialty products. Based on examining extensive data on user preferences for movies, music, Web search, and Web browsing, we find overwhelming support for the latter theory. However, the observed eccentricity is much less than what is predicted by a fully random model whereby every consumer makes his product choices independently and proportional to product popularity; so consumers do indeed exhibit at least some a priori propensity toward either the popular or the exotic. Our findings thus suggest an additional factor in the success of infinite-inventory retailers, namely, that tail availability may boost head sales by offering consumers the convenience of "one-stop shopping" for both their mainstream and niche interests. This hypothesis is further supported by our theoretical analysis that presents a simple model in which shared inventory stores, such as Amazon Marketplace, gain a clear advantage by satisfying tail demand, helping to explain the emergence and increasing popularity of such retail arrangements. Hence, we believe that the return-on-investment (ROI) of niche products goes beyond direct revenue, extending to second-order gains associated with increased consumer satisfaction and repeat patronage. More generally, our findings call into question the conventional wisdom that specialty products only appeal to a minority of consumers.

#index 1355037
#* Inferring search behaviors using partially observable Markov (POM) model
#@ Kuansan Wang;Nikolas Gloy;Xiaolong Li
#t 2010
#c 2
#% 310567
#% 330617
#% 577224
#% 731615
#% 737637
#% 752177
#% 805200
#% 823348
#% 856751
#% 879565
#% 879567
#% 946521
#% 954948
#% 954949
#% 956495
#% 956546
#% 987222
#% 1014390
#% 1035578
#% 1053505
#% 1055713
#% 1074092
#% 1074107
#% 1093793
#% 1125901
#% 1130811
#% 1190055
#% 1190056
#% 1190074
#% 1214757
#% 1269878
#! This article describes an application of the partially observable Markov (POM) model to the analysis of a large scale commercial web search log. Mathematically, POM is a variant of the hidden Markov model in which all the hidden state transitions do not necessarily emit observable events. This property of POM is used to model, as the hidden process, a common search behavior that users would read and skip search results, leaving no observable user actions to record in the search logs. The Markov nature of the model further lends support to cope with the facts that a single observed sequence can be probabilistically associated with many hidden sequences that have variable lengths, and the search results can be read in various temporal orders that are not necessarily reflected in the observed sequence of user actions. To tackle the implementation challenges accompanying the flexibility and analytic powers of POM, we introduce segmental Viterbi algorithm based on segmental decoding and Viterbi training to train the POM model parameters and apply them to uncover hidden processes from the search logs. To validate the model, the latent variables modeling the browsing patterns on the search result page are compared with the experimental data of the eye tracking stu-dies. The close agreements suggest that the search logs do contain rich information of user behaviors in browsing the search result page even though they are not directly observable, and that using POM to understand these sophisticated search behaviors is a promising approach.

#index 1355038
#* Beyond DCG: user behavior as a predictor of a successful search
#@ Ahmed Hassan;Rosie Jones;Kristina Lisa Klinkner
#t 2010
#c 2
#% 296646
#% 411762
#% 805200
#% 823348
#% 874705
#% 879566
#% 879567
#% 943049
#% 987263
#% 1055760
#% 1130811
#% 1130868
#% 1130878
#% 1166521
#% 1190055
#% 1227582
#% 1227680
#% 1415709
#! Web search engines are traditionally evaluated in terms of the relevance of web pages to individual queries. However, relevance of web pages does not tell the complete picture, since an individual query may represent only a piece of the user's information need and users may have different information needs underlying the same queries. In this work, we address the problem of predicting user search goal success by modeling user behavior. We show empirically that user behavior alone can give an accurate picture of the success of the user's web search goals, without considering the relevance of the documents displayed. In fact, our experiments show that models using user behavior are more predictive of goal success than those using document relevance. We build novel sequence models incorporating time distributions for this task and our experiments show that the sequence and time distribution models are more accurate than static models based on user behavior, or predictions based on document relevance.

#index 1355039
#* Measuring the reusability of test collections
#@ Ben Carterette;Evgeniy Gabrilovich;Vanja Josifovski;Donald Metzler
#t 2010
#c 2
#% 262102
#% 375017
#% 766409
#% 857180
#% 879650
#% 907496
#% 987199
#% 987200
#% 987201
#% 997473
#% 1019125
#% 1019154
#% 1130865
#% 1226913
#! While test collection construction is a time-consuming and expensive process, the true cost is amortized by reusing the collection over hundreds or thousands of experiments. Some of these experiments may involve systems that retrieve documents not judged during the initial construction phase, and some of these systems may be "hard" to evaluate: depending on which judgments are missing and which judged documents were retrieved, the experimenter's confidence in an evaluation could potentially be very low. We propose two methods for quantifying the reusability of a test collection for evaluating new systems. The proposed methods provide simple yet highly effective tests for determining whether an existing set of judgments is useful for evaluating a new system. Empirical evaluations using TREC datasets confirm the usefulness of our proposed reusability measures. In particular, we show that our methods can reliably estimate confidence intervals that are indicative of collection reusability.

#index 1355040
#* Learning influence probabilities in social networks
#@ Amit Goyal;Francesco Bonchi;Laks V.S. Lakshmanan
#t 2010
#c 2
#% 342596
#% 466086
#% 577217
#% 729923
#% 754098
#% 846184
#% 879628
#% 907373
#% 956512
#% 989613
#% 1067866
#% 1083624
#% 1190127
#% 1214641
#% 1214702
#% 1676017
#! Recently, there has been tremendous interest in the phenomenon of influence propagation in social networks. The studies in this area assume they have as input to their problems a social graph with edges labeled with probabilities of influence between users. However, the question of where these probabilities come from or how they can be computed from real social network data has been largely ignored until now. Thus it is interesting to ask whether from a social graph and a log of actions by its users, one can build models of influence. This is the main problem attacked in this paper. In addition to proposing models and algorithms for learning the model parameters and for testing the learned models to make predictions, we also develop techniques for predicting the time by which a user may be expected to perform an action. We validate our ideas and techniques using the Flickr data set consisting of a social graph with 1.3M nodes, 40M edges, and an action log consisting of 35M tuples referring to 300K distinct actions. Beyond showing that there is genuine influence happening in a real social network, we show that our techniques have excellent prediction performance.

#index 1355041
#* You are who you know: inferring user profiles in online social networks
#@ Alan Mislove;Bimal Viswanath;Krishna P. Gummadi;Peter Druschel
#t 2010
#c 2
#% 736155
#% 755402
#% 802710
#% 869485
#% 989600
#% 1002007
#% 1127763
#% 1190108
#! Online social networks are now a popular way for users to connect, express themselves, and share content. Users in today's online social networks often post a profile, consisting of attributes like geographic location, interests, and schools attended. Such profile information is used on the sites as a basis for grouping users, for sharing content, and for suggesting users who may benefit from interaction. However, in practice, not all users provide these attributes. In this paper, we ask the question: given attributes for some fraction of the users in an online social network, can we infer the attributes of the remaining users? In other words, can the attributes of users, in combination with the social network graph, be used to predict the attributes of another user in the network? To answer this question, we gather fine-grained data from two social networks and try to infer user profile attributes. We find that users with common attributes are more likely to be friends and often form dense communities, and we propose a method of inferring user attributes that is inspired by previous approaches to detecting communities in social networks. Our results show that certain user attributes can be inferred with high accuracy when given information on as little as 20% of the users.

#index 1355042
#* TwitterRank: finding topic-sensitive influential twitterers
#@ Jianshu Weng;Ee-Peng Lim;Jing Jiang;Qi He
#t 2010
#c 2
#% 268079
#% 290830
#% 348173
#% 722904
#% 729923
#% 1040837
#% 1083687
#% 1676017
#% 1814838
#! This paper focuses on the problem of identifying influential users of micro-blogging services. Twitter, one of the most notable micro-blogging services, employs a social-networking model called "following", in which each user can choose who she wants to "follow" to receive tweets from without requiring the latter to give permission first. In a dataset prepared for this study, it is observed that (1) 72.4% of the users in Twitter follow more than 80% of their followers, and (2) 80.5% of the users have 80% of users they are following follow them back. Our study reveals that the presence of "reciprocity" can be explained by phenomenon of homophily. Based on this finding, TwitterRank, an extension of PageRank algorithm, is proposed to measure the influence of users in Twitter. TwitterRank measures the influence taking both the topical similarity between users and the link structure into account. Experimental results show that TwitterRank outperforms the one Twitter currently uses and other related algorithms, including the original PageRank and Topic-sensitive PageRank.

#index 1355043
#* Folks in Folksonomies: social link prediction from shared metadata
#@ Rossano Schifanella;Alain Barrat;Ciro Cattuto;Benjamin Markines;Filippo Menczer
#t 2010
#c 2
#% 67565
#% 445515
#% 465914
#% 730089
#% 807657
#% 855601
#% 881054
#% 881523
#% 885452
#% 946524
#% 1002007
#% 1026892
#% 1055739
#% 1065415
#% 1080078
#% 1083675
#% 1190119
#% 1215456
#% 1215484
#! Web 2.0 applications have attracted a considerable amount of attention because their open-ended nature allows users to create lightweight semantic scaffolding to organize and share content. To date, the interplay of the social and semantic components of social media has been only partially explored. Here we focus on Flickr and Last.fm, two social media systems in which we can relate the tagging activity of the users with an explicit representation of their social network. We show that a substantial level of local lexical and topical alignment is observable among users who lie close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local alignment between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar topical interests are more likely to be friends, and therefore semantic similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on the Last.fm data set, confirming that the social network constructed from semantic similarity captures actual friendship more accurately than Last.fm's suggestions based on listening patterns.

#index 1355044
#* GeoFolk: latent spatial semantics in web 2.0 social media
#@ Sergej Sizov
#t 2010
#c 2
#% 722904
#% 868088
#% 881498
#% 946524
#% 1055704
#% 1055743
#% 1074117
#% 1077150
#% 1079882
#% 1810980
#! We describe an approach for multi-modal characterization of social media by combining text features (e.g. tags as a prominent example of short, unstructured text labels) with spatial knowledge (e.g. geotags and coordinates of images and videos). Our model-based framework GeoFolk combines these two aspects in order to construct better algorithms for content management, retrieval, and sharing. The approach is based on multi-modal Bayesian models which allow us to integrate spatial semantics of social media in a well-formed, probabilistic manner. We systematically evaluate the solution on a subset of Flickr data, in characteristic scenarios of tag recommendation, content classification, and clustering. Experimental results show that our method outperforms baseline techniques that are based on one of the aspects alone. The approach described in this contribution can also be used in other domains such as Geoweb retrieval.

#index 1355045
#* Learning similarity metrics for event identification in social media
#@ Hila Becker;Mor Naaman;Luis Gravano
#t 2010
#c 2
#% 201889
#% 210173
#% 239588
#% 262042
#% 262043
#% 309131
#% 310516
#% 445316
#% 575570
#% 577263
#% 722902
#% 729913
#% 735078
#% 766444
#% 844289
#% 915242
#% 926881
#% 961134
#% 983830
#% 987205
#% 987219
#% 997189
#% 1035587
#% 1035588
#% 1055773
#% 1074117
#% 1077150
#% 1127407
#% 1133031
#% 1190086
#% 1213625
#% 1217162
#% 1292518
#% 1561708
#! Social media sites (e.g., Flickr, YouTube, and Facebook) are a popular distribution outlet for users looking to share their experiences and interests on the Web. These sites host substantial amounts of user-contributed materials (e.g., photographs, videos, and textual content) for a wide variety of real-world events of different type and scale. By automatically identifying these events and their associated user-contributed social media documents, which is the focus of this paper, we can enable event browsing and search in state-of-the-art search engines. To address this problem, we exploit the rich "context" associated with social media content, including user-provided annotations (e.g., title, tags) and automatically generated information (e.g., content creation time). Using this rich context, which includes both textual and non-textual features, we can define appropriate document similarity metrics to enable online clustering of media to events. As a key contribution of this paper, we explore a variety of techniques for learning multi-feature similarity metrics for social media documents in a principled manner. We evaluate our techniques on large-scale, real-world datasets of event images from Flickr. Our evaluation results suggest that our approach identifies events, and their associated social media documents, more effectively than the state-of-the-art strategies on which we build.

#index 1355046
#* Early online identification of attention gathering items in social media
#@ Michael Mathioudakis;Nick Koudas;Peter Marbach
#t 2010
#c 2
#% 268079
#% 290830
#% 340147
#% 577220
#% 824666
#% 876007
#% 989601
#% 1022269
#% 1269909
#! Activity in social media such as blogs, micro-blogs, social networks, etc is manifested via interaction that involves text, images, links and other information items. Naturally, some items attract more attention than others, expressed with large volumes of linking, commenting or tagging activity, to name a few examples. Moreover, high attention can be indicative of emerging events, breaking news or generally indicate information items of interest to a vast set of people. The numbers associated with digital social activity are astonishing: in excess of millions of blog posts, tweets and forums updates per day, millions of tags in photos, news articles or blogs. Being able to identify information items that gather much attention in such a real time information collective is a challenging task. In this paper, we consider the problem of early online identification of items that gather a lot of attention in social media. We model social media activity using ISIS, a stochastic model for Interacting Streaming Information Sources, that intuitively captures the concept of attention gathering information items. Given the challenge of the information overload characterizing digital social activity, we present sequential statistical tests that enable early identification of attention gathering items. This effectively reduces the set of items one has to monitor in real time in order to identify pieces of information attracting a lot of attention. Experiments on real data demonstrate the utility of our model, as well as the efficiency and effectiveness of the proposed sequential statistical tests.

#index 1355047
#* Evolution of two-sided markets
#@ Ravi Kumar;Yury Lifshits;Andrew Tomkins
#t 2010
#c 2
#% 729923
#% 881460
#% 959072
#% 963248
#% 963337
#! Two-sided markets arise when two different types of users may realize gains by interacting with one another through one or more platforms or mediators. We initiate a study of the evolution of such markets. We present an empirical analysis of the value accruing to members of each side of the market, based on the presence of the other side. We codify the range of value curves into a general theoretical model, characterize the equilibrium states of two-sided markets in our model, and prove that each platform will converge to one of these equilibria. We give some early experimental results of the stability of two-sided markets, and close with a theoretical treatment of the formation of different kinds of coalitions in such markets.

#index 1355048
#* A novel click model and its applications to online advertising
#@ Zeyuan Allen Zhu;Weizhu Chen;Tom Minka;Chenguang Zhu;Zheng Chen
#t 2010
#c 2
#% 577224
#% 715096
#% 818221
#% 879565
#% 879567
#% 946521
#% 956546
#% 1035578
#% 1074092
#% 1166517
#% 1190055
#% 1190056
#% 1190057
#% 1190077
#% 1190079
#! Recent advances in click model have positioned it as an attractive method for representing user preferences in web search and online advertising. Yet, most of the existing works focus on training the click model for individual queries, and cannot accurately model the tail queries due to the lack of training data. Simultaneously, most of the existing works consider the query, url and position, neglecting some other important attributes in click log data, such as the local time. Obviously, the click through rate is different between daytime and midnight. In this paper, we propose a novel click model based on Bayesian network, which is capable of modeling the tail queries because it builds the click model on attribute values, with those values being shared across queries. We called our work General Click Model (GCM) as we found that most of the existing works can be special cases of GCM by assigning different parameters. Experimental results on a large-scale commercial advertisement dataset show that GCM can significantly and consistently lead to better results as compared to the state-of-the-art works.

#index 1355049
#* Adaptive weighing designs for keyword value computation
#@ John W. Byers;Michael Mitzenmacher;Georgios Zervas
#t 2010
#c 2
#% 17509
#% 217814
#% 868445
#% 963358
#% 987262
#% 1074101
#% 1735205
#! Attributing a dollar value to a keyword is an essential part of running any profitable search engine advertising campaign. When an advertiser has complete control over the interaction with and monetization of each user arriving on a given keyword, the value of that term can be accurately tracked. However, in many instances, the advertiser may monetize arrivals indirectly through one or more third parties. In such cases, it is typical for the third party to provide only coarse-grained reporting: rather than report each monetization event, users are aggregated into larger channels and the third party reports aggregate information such as total daily revenue for each channel. Examples of third parties that use channels include Amazon and Google AdSense. In such scenarios, the number of channels is generally much smaller than the number of keywords whose value per click (VPC) we wish to learn. However, the advertiser has flexibility as to how to assign keywords to channels over time. We introduce the channelization problem: how do we adaptively assign keywords to channels over the course of multiple days to quickly obtain accurate VPC estimates of all keywords? We relate this problem to classical results in weighing design, devise new adaptive algorithms for this problem, and quantify the performance of these algorithms experimentally. Our results demonstrate that adaptive weighing designs that exploit statistics of term frequency, variability in VPCs across keywords, and flexible channel assignments over time provide the best estimators of keyword VPCs.

#index 1355050
#* Automatic generation of bid phrases for online advertising
#@ Sujith Ravi;Andrei Broder;Evgeniy Gabrilovich;Vanja Josifovski;Sandeep Pandey;Bo Pang
#t 2010
#c 2
#% 240026
#% 333679
#% 336329
#% 529158
#% 740915
#% 816173
#% 817596
#% 818265
#% 850108
#% 869484
#% 881477
#% 938729
#% 972307
#% 987262
#% 990182
#% 1035593
#% 1040857
#% 1055677
#% 1066587
#% 1074101
#% 1130910
#% 1190106
#% 1190181
#% 1190182
#% 1270653
#! One of the most prevalent online advertising methods is textual advertising. To produce a textual ad, an advertiser must craft a short creative (the text of the ad) linking to a landing page, which describes the product or service being promoted. Furthermore, the advertiser must associate the creative to a set of manually chosen bid phrases representing those Web search queries that should trigger the ad. For efficiency, given a landing page, the bid phrases are often chosen first, and then for each bid phrase the creative is produced using a template. Nevertheless, an ad campaign (e.g., for a large retailer) might involve thousands of landing pages and tens or hundreds of thousands of bid phrases, hence the entire process is very laborious. Our study aims towards the automatic construction of online ad campaigns: given a landing page, we propose several algorithmic methods to generate bid phrases suitable for the given input. Such phrases must be both relevant (that is, reflect the content of the page) and well-formed (that is, likely to be used as queries to a Web search engine). To this end, we use a two phase approach. First, candidate bid phrases are generated by a number of methods, including a (mono-lingual) translation model capable of generating phrases contained within the text of the input as well as previously "unseen" phrases. Second, the candidates are ranked in a probabilistic framework using both the translation model, which favors relevant phrases, as well as a bid phrase language model, which favors well-formed phrases. Empirical evaluation based on a real-life corpus of advertiser-created landing pages and associated bid phrases confirms the value of our approach, which successfully re-generates many of the human-crafted bid phrases and performs significantly better than a pure text extraction method.

#index 1355051
#* Personalized click prediction in sponsored search
#@ Haibin Cheng;Erick Cantú-Paz
#t 2010
#c 2
#% 211044
#% 879567
#% 990182
#% 1029264
#% 1035578
#% 1055694
#% 1055713
#% 1214728
#! Sponsored search is a multi-billion dollar business that generates most of the revenue for search engines. Predicting the probability that users click on ads is crucial to sponsored search because the prediction is used to influence ranking, filtering, placement, and pricing of ads. Ad ranking, filtering and placement have a direct impact on the user experience, as users expect the most useful ads to rank high and be placed in a prominent position on the page. Pricing impacts the advertisers' return on their investment and revenue for the search engine. The objective of this paper is to present a framework for the personalization of click models in sponsored search. We develop user-specific and demographic-based features that reflect the click behavior of individuals and groups. The features are based on observations of search and click behaviors of a large number of users of a commercial search engine. We add these features to a baseline non-personalized click model and perform experiments on offline test sets derived from user logs as well as on live traffic. Our results demonstrate that the personalized models significantly improve the accuracy of click prediction.

#index 1355052
#* Improving ad relevance in sponsored search
#@ Dustin Hillard;Stefan Schroedl;Eren Manavoglu;Hema Raghavan;Chirs Leggetter
#t 2010
#c 2
#% 280851
#% 311034
#% 340901
#% 411762
#% 590523
#% 730065
#% 740915
#% 818265
#% 869484
#% 869501
#% 956546
#% 980527
#% 987262
#% 989668
#% 1040857
#% 1055694
#% 1056829
#% 1074101
#% 1130909
#% 1130910
#% 1190055
#% 1214754
#% 1227723
#% 1246499
#% 1292738
#! We describe a machine learning approach for predicting sponsored search ad relevance. Our baseline model incorporates basic features of text overlap and we then extend the model to learn from past user clicks on advertisements. We present a novel approach using translation models to learn user click propensity from sparse click logs. Our relevance predictions are then applied to multiple sponsored search applications in both offline editorial evaluations and live online user tests. The predicted relevance score is used to improve the quality of the search page in three areas: filtering low quality ads, more accurate ranking for ads, and optimized page placement of ads to reduce prominent placement of low relevance ads. We show significant gains across all three tasks.

#index 1355053
#* Revisiting globally sorted indexes for efficient document retrieval
#@ Fan Zhang;Shuming Shi;Hao Yan;Ji-Rong Wen
#t 2010
#c 2
#% 157880
#% 198335
#% 212665
#% 228097
#% 262099
#% 268079
#% 278831
#% 290703
#% 292684
#% 330707
#% 340886
#% 340887
#% 387427
#% 397608
#% 463737
#% 480330
#% 631988
#% 643566
#% 730065
#% 805862
#% 805864
#% 867054
#% 879611
#% 893126
#% 907504
#% 1015265
#% 1019139
#% 1019182
#% 1074067
#% 1130876
#% 1166527
#% 1227595
#% 1387547
#% 1404894
#! There has been a large amount of research on efficient document retrieval in both IR and web search areas. One important technique to improve retrieval efficiency is early termination, which speeds up query processing by avoiding scanning the entire inverted lists. Most early termination techniques first build new inverted indexes by sorting the inverted lists in the order of either the term-dependent information, e.g., term frequencies or term IR scores, or the term-independent information, e.g., static rank of the document; and then apply appropriate retrieval strategies on the resulting indexes. Although the methods based only on the static rank have been shown to be ineffective for the early termination, there are still many advantages of using the methods based on term-independent information. In this paper, we propose new techniques to organize inverted indexes based on the term-independent information beyond static rank and study the new retrieval strategies on the resulting indexes. We perform a detailed experimental evaluation on our new techniques and compare them with the existing approaches. Our results on the TREC GOV and GOV2 data sets show that our techniques can improve query efficiency significantly.

#index 1355054
#* Learning URL patterns for webpage de-duplication
#@ Hema Swetha Koppula;Krishna P. Leela;Amit Agarwal;Krishna Prasad Chitrapura;Sachin Garg;Amit Sasturkar
#t 2010
#c 2
#% 235941
#% 347225
#% 449588
#% 577330
#% 616528
#% 728115
#% 838469
#% 879600
#% 956504
#% 956507
#% 963669
#% 1083644
#% 1190189
#% 1292753
#! Presence of duplicate documents in the World Wide Web adversely affects crawling, indexing and relevance, which are the core building blocks of web search. In this paper, we present a set of techniques to mine rules from URLs and utilize these rules for de-duplication using just URL strings without fetching the content explicitly. Our technique is composed of mining the crawl logs and utilizing clusters of similar pages to extract transformation rules, which are used to normalize URLs belonging to each cluster. Preserving each mined rule for de-duplication is not efficient due to the large number of such rules. We present a machine learning technique to generalize the set of rules, which reduces the resource footprint to be usable at web-scale. The rule extraction techniques are robust against web-site specific URL conventions. We compare the precision and scalability of our approach with recent efforts in using URLs for de-duplication. Experimental results demonstrate that our approach achieves 2 times more reduction in duplicates with only half the rules compared to the most recent previous approach. Scalability of the framework is demonstrated by performing a large scale evaluation on a set of 3 Billion URLs, implemented using the MapReduce framework.

#index 1355055
#* On compressing the textual web
#@ Paolo Ferragina;Giovanni Manzini
#t 2010
#c 2
#% 236763
#% 255137
#% 290703
#% 346149
#% 387427
#% 482658
#% 578337
#% 654447
#% 656262
#% 745499
#% 754117
#% 762054
#% 794132
#% 807657
#% 813701
#% 823468
#% 869500
#% 936965
#% 956506
#% 960181
#% 963669
#% 978157
#% 985947
#% 987208
#% 987259
#% 1019138
#% 1035579
#% 1044490
#% 1054227
#% 1055710
#% 1074068
#% 1128430
#% 1164001
#% 1164901
#% 1190061
#% 1190095
#% 1214643
#% 1227595
#% 1392439
#% 1404879
#% 1810034
#! Nowadays we know how to effectively compress most basic components of any modern search engine, such as, the graphs arising from the Web structure and/or its usage, the posting lists, and the dictionary of terms. But we are not aware of any study which has deeply addressed the issue of compressing the raw Web pages. Many Web applications use simple compression algorithms--- e.g. gzip, or word-based Move-to-Front or Huffman coders-and conclude that, even compressed, raw data take more space than Inverted Lists. In this paper we investigate two typical scenarios of use of data compression for large Web collections. In the first scenario, the compressed pages are stored on disk and we only need to support the fast scanning of large parts of the compressed collection (such as for map-reduce paradigms). In the second scenario, we consider the fast access to individual pages of the compressed collection that is distributed among the RAMs of many PCs (such as for search engines and miners). For the first scenario, we provide a thorough experimental comparison among state-of-the-art compressors thus indicating pros and cons of the available solutions. For the second scenario, we compare known compressed-storage solutions with the new algorithmic technology of compressed self-indexes [NM07]. Our results show that Web pages are more compressible than expected and, consequently, that some common beliefs in this area should be reconsidered. Our results are novel for the large spectrum of tested approaches and the size of datasets, and provide a threefold contribution: a non-trivial baseline for designing new compressed-storage solutions, a guide for software developers faced with Web-page storage, and a natural complement to the recent figures on InvertedList-compression achieved by [Yan et al, sigir 09 and www 09].

#index 1355056
#* A sketch-based distance oracle for web-scale graphs
#@ Atish Das Sarma;Sreenivas Gollapudi;Marc Najork;Rina Panigrahy
#t 2010
#c 2
#% 249183
#% 255137
#% 379482
#% 410276
#% 781865
#% 787540
#% 789667
#% 793252
#% 813786
#% 975021
#% 1042320
#% 1074066
#% 1083711
#% 1155317
#% 1215445
#! We study the fundamental problem of computing distances between nodes in large graphs such as the web graph and social networks. Our objective is to be able to answer distance queries between pairs of nodes in real time. Since the standard shortest path algorithms are expensive, our approach moves the time-consuming shortest-path computation offline, and at query time only looks up precomputed values and performs simple and fast computations on these precomputed values. More specifically, during the offline phase we compute and store a small "sketch" for each node in the graph, and at query-time we look up the sketches of the source and destination nodes and perform a simple computation using these two sketches to estimate the distance.

#index 1355057
#* Early exit optimizations for additive machine learned ranking systems
#@ B. Barla Cambazoglu;Hugo Zaragoza;Olivier Chapelle;Jiang Chen;Ciya Liao;Zhaohui Zheng;Jon Degenhardt
#t 2010
#c 2
#% 157880
#% 169817
#% 194247
#% 198335
#% 209021
#% 213786
#% 228097
#% 278831
#% 340886
#% 387427
#% 397608
#% 410276
#% 464605
#% 577302
#% 736300
#% 818229
#% 840846
#% 879588
#% 879611
#% 907546
#% 987226
#% 987251
#% 1166527
#% 1227628
#% 1674802
#! Some commercial web search engines rely on sophisticated machine learning systems for ranking web documents. Due to very large collection sizes and tight constraints on query response times, online efficiency of these learning systems forms a bottleneck. An important problem in such systems is to speedup the ranking process without sacrificing much from the quality of results. In this paper, we propose optimization strategies that allow short-circuiting score computations in additive learning systems. The strategies are evaluated over a state-of-the-art machine learning system and a large, real-life query log, obtained from Yahoo!. By the proposed strategies, we are able to speedup the score computations by more than four times with almost no loss in result quality.

#index 1355058
#* SBotMiner: large scale search bot detection
#@ Fang Yu;Yinglian Xie;Qifa Ke
#t 2010
#c 2
#% 326522
#% 568291
#% 889653
#% 904250
#% 956518
#% 981649
#% 983467
#% 990375
#% 1020409
#% 1020410
#% 1029754
#% 1072118
#% 1084472
#% 1114743
#% 1125899
#% 1166530
#% 1166531
#% 1216356
#% 1298833
#% 1387554
#% 1468421
#! In this paper, we study search bot traffic from search engine query logs at a large scale. Although bots that generate search traffic aggressively can be easily detected, a large number of distributed, low rate search bots are difficult to identify and are often associated with malicious attacks. We present SBotMiner, a system for automatically identifying stealthy, low-rate search bot traffic from query logs. Instead of detecting individual bots, our approach captures groups of distributed, coordinated search bots. Using sampled data from two different months, SBotMiner identifies over 123 million bot-related pageviews, accounting for 3.8% of total traffic. Our in-depth analysis shows that a large fraction of the identified bot traffic may be associated with various malicious activities such as phishing attacks or vulnerability exploits. This finding suggests that detecting search bot traffic holds great promise to detect and stop attacks early on.

#index 1355059
#* Gathering and ranking photos of named entities with high precision, high recall, and diversity
#@ Bilyana Taneva;Mouna Kacimi;Gerhard Weikum
#t 2010
#c 2
#% 290482
#% 319464
#% 321455
#% 592073
#% 760805
#% 883870
#% 956564
#% 990307
#% 996177
#% 1038781
#% 1040539
#% 1055701
#% 1071087
#% 1074095
#% 1092530
#% 1119142
#% 1190089
#% 1190131
#% 1217263
#% 1275182
#% 1405994
#% 1409954
#% 1857848
#! Knowledge-sharing communities like Wikipedia and automated extraction methods like those of DBpedia enable the construction of large machine-processible knowledge bases with relational facts about entities. These endeavors lack multimodal data like photos and videos of people and places. While photos of famous entities are abundant on the Internet, they are much harder to retrieve for less popular entities such as notable computer scientists or regionally interesting churches. Querying the entity names in image search engines yields large candidate lists, but they often have low precision and unsatisfactory recall. Our goal is to populate a knowledge base with photos of named entities, with high precision, high recall, and diversity of photos for a given entity. We harness relational facts about entities for generating expanded queries to retrieve different candidate lists from image search engines. We use a weighted voting method to determine better rankings of an entity's photos. Appropriate weights are dependent on the type of entity (e.g., scientist vs. politician) and automatically computed from a small set of training entities. We also exploit visual similarity measures based on SIFT features, for higher diversity in the final rankings. Our experiments with photos of persons and landmarks show significant improvements of ranking measures like MAP and NDCG, and also for diversity-aware ranking.

#index 1355060
#* Boilerplate detection using shallow text features
#@ Christian Kohlschütter;Peter Fankhauser;Wolfgang Nejdl
#t 2010
#c 2
#% 348180
#% 577323
#% 729939
#% 800178
#% 807298
#% 821872
#% 869466
#% 907512
#% 956499
#% 1016333
#% 1019075
#% 1055709
#% 1130926
#% 1190152
#% 1190217
#% 1394469
#! In addition to the actual content Web pages consist of navigational elements, templates, and advertisements. This boilerplate text typically is not related to the main content, may deteriorate search precision and thus needs to be detected properly. In this paper, we analyze a small set of shallow text features for classifying the individual text elements in a Web page. We compare the approach to complex, state-of-the-art techniques and show that competitive accuracy can be achieved, at almost no cost. Moreover, we derive a simple and plausible stochastic model for describing the boilerplate creation process. With the help of our model, we also quantify the impact of boilerplate removal to retrieval performance and show significant improvements over the baseline. Finally, we extend the principled approach by straight-forward heuristics, achieving a remarkable detection accuracy.

#index 1536496
#* Proceedings of the fourth ACM international conference on Web search and data mining
#@ Irwin King;Wolfgang Nejdl;Hang Li
#t 2011
#c 2
#! Welcome to the Fourth ACM International Conference on Web Search and Data Mining (WSDM 2011) held on February 9-12, 2011, in Hong Kong. As the premier ACM conference in the field, WSDM 2011 offers a highly competitive forum for reporting the latest developments in websearch, social search and data mining. We are pleased to present the proceedings of the conference as its published record. Although it is only in its fourth year, WSDM has already witnessed significant growth. We received a record 372 submissions, representing a 22% increase compared to WSDM 2010. 19 Senior PC members and 134 PC members conducted reviews to the submissions. The conference accepted 83 papers (22.3% acceptance rate). Among these, 32 papers were selected for oral and poster presentations and 51 papers were selected for poster only presentations. The authors of submitted papers were from 35 countries and regions, authors of accepted papers are from 13 countries and regions. The quality of accepted papers is very high, making WSDM a first tier conference in computer science.

#index 1536497
#* Crowdsourcing for search and data mining
#@ Vitor R. Carvalho;Matthew Lease;Emine Yilmaz
#t 2011
#c 2
#! The advent of crowdsourcing is revolutionizing data annotation, evaluation, and other traditionally manual-labor intensive processes by dramatically reducing the time, cost, and effort involved. This in turn is driving a disruptive shift in search and data mining methodology in areas such as: Evaluation: the Cranfield paradigm for search evaluation requires manually assessing document relevance to search queries. Recent work on stochastic evaluation has reduced but not removed this need for manual assessment. Supervised Learning: while traditional costs associated with data annotation have driven recent machine learning work (e.g. Learning to Rank) toward greater use of unsupervised and semi-supervised methods, the emergence of crowdsourcing has made labeled data far easier to acquire, thereby driving a potential resurgence in fully-supervised methods. Applications: Crowdsourcing has introduced exciting new opportunities to integrate human labor into automated systems: handling difficult cases where automation fails, exploiting the breadth of backgrounds, geographic dispersion, real-time response, etc.

#index 1536498
#* User modeling for web applications
#@ David Carmel;Vanja Josifovski;Yoelle Maarek
#t 2011
#c 2
#! Users have taken a more and more central role in the Web. Their role is both explicit, as they become more savvy, they have more expectations, and new interactive features keep appearing, and implicit, as their actions are monitored at various levels of granularity for various needs from live traffic evaluation for usage data mining to improve ranking, spelling etc. In a few years, most Web applications will have the ability to successfully adapt to both the explicit and implicit needs and tastes of their users. Such adaptation requires the ability to model the user's personal goals, interests, preferences and knowledge, and to apply this model while users interact with various applications. While adaptive applications that are based on user modeling have attracted the attention of multiple communities, from AI to UI, there is no forum that specifically focuses on user modeling and adaptive applications in the Web domain. This workshop will focus on user modeling and the usage of these models in Web applications. The emphasis of the workshop will be on modeling techniques that scale for the Web. User modeling might be based on explicit and implicit user feedback gathered from variety of sources such as sign-on information, clickthrough data, user previous queries, social network, purchases, and real-world activity. Adaptive Web based applications include search personalization, advertisement targeting, recommendation systems, social networks, on-line shopping, etc.

#index 1536499
#* Crowdsourcing 101: putting the WSDM of crowds to work for you
#@ Omar Alonso;Matthew Lease
#t 2011
#c 2
#! Crowdsourcing has emerged in recent years as an exciting new avenue for leveraging the tremendous potential and resources of today's digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained particular attention as active online market places for reaching and tapping into this glut of a still largely under-utilized workforce. Crowdsourcing offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite understanding of principles, platforms, and best practices. This tutorial will introduce the opportunities and challenges of crowdsourcing while discussing the three issues above. This will provide attendees with a basic foundation to begin applying crowdsourcing in the context of their own particular tasks.

#index 1536500
#* Introduction to display advertising: a half-day tutorial
#@ Andrei Broder;Vanja Josifovski;Jayavel Shanmugasundaram
#t 2011
#c 2
#! Display advertising is one of the two major advertising channels on the web (in addition to search advertising). Display advertising on the Web is usually done by graphical ads placed on the publishers' Web pages. There is no explicit user query, and the ad selection is performed based on the page where the ad is placed (contextual targeting) or user's past activities (behavioral targeting). In both cases, sophisticated text analysis and learning algorithms are needed to provide relevant ads to the user. In this tutorial we will overview the display advertising marketplace, and technologies that power the display advertising platforms.

#index 1536501
#* Exploiting statistical and relational information on the web and in social media
#@ Lise Getoor;Lilyana Mihalkova
#t 2011
#c 2
#% 464304
#% 577356
#% 754098
#% 850430
#% 956546
#% 956552
#% 961268
#% 989578
#% 1130868
#% 1130878
#% 1190108
#% 1190128
#% 1267762
#% 1279353
#% 1289560
#% 1451235
#% 1650403
#! The popularity of Web 2.0, characterized by a proliferation of social media sites, and Web 3.0, with more richly semantically annotated objects and relationships, brings to light a variety of important prediction, ranking, and extraction tasks. The input to these tasks is often best seen as a (noisy) multi-relational graph, such as the click graph, defined by user interactions with Web sites; and the social graph, defined by friendships and affiliations on social media sites. This tutorial will provide an overview of statistical relational learning and inference techniques, motivating and illustrating them using web and social media applications. We will start by briefly surveying some of the sources of statistical and relational information on the web and in social media and will then dedicate most of the tutorial time to an introduction to representations and techniques for learning and reasoning with multi-relational information, viewing them through the lens of web and social media domains. We will end with a discussion of current trends and related fields, such as privacy in social networks.

#index 1536502
#* Web retrieval: the role of users
#@ Ricardo Baeza-Yates;Yoelle Maarek
#t 2011
#c 2
#! Web retrieval methods have evolved through three major steps in the last decade or so. They started from standard documentcentric IR in the early days of the Web, then made a major step forward by leveraging the structure of the Web, using link analysis techniques in both crawling and ranking challenges. A more recent, no less important but maybe more discrete step forward, has been to enter the user in this equation in two ways: (1) implicitly, through the analysis of usage data captured by query logs, and session and click information in general, the goal being to improve ranking as well as to measure user's happiness and engagement; (2) explicitly, by offering novel interactive features; the goal here being to better answer users' needs. In this tutorial, we will cover the user-related challenges associated with the implicit and explicit role of users in Web retrieval. We will review and discuss challenges associated with two types of activities, namely: Usage data analysis and metrics - It is critical to monitor how users interact with Web retrieval systems, as this implicit relevant feedback aggregated at a large scale can approximate quite accurately the level of success of a given feature. Here we have to consider not only clicks statistics but also the time spent in a page, the number of actions per session, etc. User interaction - Given the intrinsic problems posed by the Web, the key challenge for the user is to conceive a good query, one that leads to a manageable and relevant answer. The retrieval system must complete search requests fast and give back relevant results, even for poorly formulated queries. Web retrieval engines thus interact with the user at two key stages, each associated with its own challenges: (1) Expressing a query: Human beings have needs or tasks to accomplish, which are frequently not easy to express as 'queries'. Queries are just a reflection of human needs and are thus, by definition, imperfect. The issue here is for the engine both to assist the user in reflecting this need and to capture the intent behind the query even if the information is incomplete or poorly expressed. (2) Interpreting and using results: Even if the user is able to perfectly express a query, the answer might be split over thousands or millions of Web pages or not exist at all. In this context, numerous questions need to be addressed. Examples include: How do we handle a large answer? How do we select or maybe synthesize the documents that really are of interest to the user? Even in the case of a single document candidate, the document itself could be large. How do we browse such documents efficiently? How to help the user take advantage of results, and possibly combine with applications to perform the task that drove the query? The goal of this tutorial is to teach the key principles and technologies behind the activities and challenges briefly outlined above, bring new understanding and insights to the attendees, and hopefully foster future research. A previous version of this tutorial was offered at the ACM SIGIR

#index 1536503
#* Mining billion-node graphs: patterns, generators and tools
#@ Christos Faloutsos
#t 2011
#c 2
#! What do graphs look like? How do they evolve over time? How to handle a graph with a billion nodes? We present a comprehensive list of static and temporal laws, and some recent observations on real graphs (like, e.g., "eigenSpokes"). For generators, we describe some recent ones, which naturally match all of the known properties of real graphs. Finally, for tools, we present "oddBall" for discovering anomalies and patterns, as well as an overview of the PEGASUS system which is designed for handling Billion-node graphs, running on top of the "hadoop" system.

#index 1536504
#* Who uses web search for what: and how
#@ Ingmar Weber;Alejandro Jaimes
#t 2011
#c 2
#% 349447
#% 401405
#% 443984
#% 561315
#% 590523
#% 727917
#% 766447
#% 956495
#% 956508
#% 987211
#% 1043040
#% 1130878
#% 1166521
#% 1355037
#% 1399989
#% 1450833
#% 1450845
#% 1450894
#% 1482368
#% 1537504
#% 1682429
#! We analyze a large query log of 2.3 million anonymous registered users from a web-scale U.S. search engine in order to jointly analyze their on-line behavior in terms of who they might be (demographics), what they search for (query topics), and how they search (session analysis). We examine basic demographics from registration information provided by the users, augmented with U.S. census data, analyze basic session statistics, classify queries into types (navigational, informational, transactional) based on click entropy, classify queries into topic categories, and cluster users based on the queries they issued. We then examine the resulting clusters in terms of demographics and search behavior. Our analysis of the data suggests that there are important differences in search behavior across different demographic groups in terms of the topics they search for, and how they search (e.g., white conservatives are those likely to have voted republican, mostly white males, who search for business, home, and gardening related topics; Baby Boomers tend to be primarily interested in Finance and a large fraction of their sessions consist of simple navigational queries related to online banking, etc.). Finally, we examine regional search differences, which seem to correlate with differences in local industries (e.g., gambling related queries are highest in Las Vegas and lowest in Salt Lake City; searches related to actors are about three times higher in L.A. than in any other region).

#index 1536505
#* Personalizing web search using long term browsing history
#@ Nicolaas Matthijs;Filip Radlinski
#t 2011
#c 2
#% 296646
#% 306468
#% 309095
#% 413615
#% 577224
#% 637576
#% 729625
#% 754126
#% 766479
#% 771571
#% 805877
#% 818224
#% 818259
#% 832349
#% 838547
#% 869536
#% 907495
#% 907515
#% 946521
#% 956495
#% 956552
#% 995516
#% 1019113
#% 1022764
#% 1074071
#% 1074113
#% 1093778
#% 1130811
#% 1192483
#% 1357833
#% 1450912
#! Personalizing web search results has long been recognized as an avenue to greatly improve the search experience. We present a personalization approach that builds a user interest profile using users' complete browsing behavior, then uses this model to rerank web results. We show that using a combination of content and previously visited websites provides effective personalization. We extend previous work by proposing a number of techniques for filtering previously viewed content that greatly improve the user model used for personalization. Our approaches are compared to previous work in offline experiments and are evaluated against unpersonalized web search in large scale online tests. Large improvements are found in both cases.

#index 1536506
#* #TwitterSearch: a comparison of microblog search and web search
#@ Jaime Teevan;Daniel Ramage;Merredith Ringel Morris
#t 2011
#c 2
#% 80398
#% 323131
#% 590523
#% 722904
#% 879587
#% 905319
#% 987211
#% 987224
#% 1035588
#% 1040837
#% 1055697
#% 1074070
#% 1074159
#% 1130878
#% 1132927
#% 1166510
#% 1190135
#% 1194140
#% 1227616
#% 1355297
#% 1379671
#% 1384287
#% 1399976
#% 1474638
#% 1742093
#! Social networking Web sites are not just places to maintain relationships; they can also be valuable information sources. However, little is known about how and why people search socially-generated content. In this paper we explore search behavior on the popular microblogging/social networking site Twitter. Using analysis of large-scale query logs and supplemental qualitative data, we observe that people search Twitter to find temporally relevant information (e.g., breaking news, real-time content, and popular trends) and information related to people (e.g., content directed at the searcher, information about people of interest, and general sentiment and opinion). Twitter queries are shorter, more popular, and less likely to evolve as part of a session than Web queries. It appears people repeat Twitter queries to monitor the associated search results, while changing and developing Web queries to learn about a topic. The results returned from the different corpora support these different uses, with Twitter results including more social chatter and social events, and Web results containing more basic facts and navigational content. We discuss the implications of these findings for the design of next-generation Web search tools that incorporate social media.

#index 1536507
#* Identifying topical authorities in microblogs
#@ Aditya Pal;Scott Counts
#t 2011
#c 2
#% 94368
#% 282905
#% 413571
#% 722904
#% 729923
#% 847218
#% 956516
#% 963669
#% 1019165
#% 1035587
#% 1083720
#% 1130900
#% 1355042
#% 1392432
#% 1482364
#% 1702614
#! Content in microblogging systems such as Twitter is produced by tens to hundreds of millions of users. This diversity is a notable strength, but also presents the challenge of finding the most interesting and authoritative authors for any given topic. To address this, we first propose a set of features for characterizing social media authors, including both nodal and topical metrics. We then show how probabilistic clustering over this feature space, followed by a within-cluster ranking procedure, can yield a final list of top authors for a given topic. We present results across several topics, along with results from a user study confirming that our method finds authors who are significantly more interesting and authoritative than those resulting from several baseline conditions. Additionally our algorithm is computationally feasible in near real-time scenarios making it an attractive alternative for capturing the rapidly changing dynamics of microblogs.

#index 1536508
#* Correcting for missing data in information cascades
#@ Eldar Sadikov;Montserrat Medina;Jure Leskovec;Hector Garcia-Molina
#t 2011
#c 2
#% 577217
#% 729923
#% 754107
#% 881526
#% 937549
#% 949164
#% 1190127
#% 1200324
#% 1399992
#% 1400003
#! Transmission of infectious diseases, propagation of information, and spread of ideas and influence through social networks are all examples of diffusion. In such cases we say that a contagion spreads through the network, a process that can be modeled by a cascade graph. Studying cascades and network diffusion is challenging due to missing data. Even a single missing observation in a sequence of propagation events can significantly alter our inferences about the diffusion process. We address the problem of missing data in information cascades. Specifically, given only a fraction C' of the complete cascade C, our goal is to estimate the properties of the complete cascade C, such as its size or depth. To estimate the properties of C, we first formulate k-tree model of cascades and analytically study its properties in the face of missing data. We then propose a numerical method that given a cascade model and observed cascade C' can estimate properties of the complete cascade C. We evaluate our methodology using information propagation cascades in the Twitter network (70 million nodes and 2 billion edges), as well as information cascades arising in the blogosphere. Our experiments show that the k-tree model is an effective tool to study the effects of missing data in cascades. Most importantly, we show that our method (and the k-tree model) can accurately estimate properties of the complete cascade C even when 90% of the data is missing.

#index 1536509
#* Everyone's an influencer: quantifying influence on twitter
#@ Eytan Bakshy;Jake M. Hofman;Winter A. Mason;Duncan J. Watts
#t 2011
#c 2
#% 729923
#% 739487
#% 754107
#% 832271
#% 949164
#% 1047347
#% 1083692
#% 1130857
#% 1222654
#% 1252624
#% 1264744
#% 1355042
#% 1384254
#% 1399992
#! In this paper we investigate the attributes and relative influence of 1.6M Twitter users by tracking 74 million diffusion events that took place on the Twitter follower graph over a two month interval in 2009. Unsurprisingly, we find that the largest cascades tend to be generated by users who have been influential in the past and who have a large number of followers. We also find that URLs that were rated more interesting and/or elicited more positive feelings by workers on Mechanical Turk were more likely to spread. In spite of these intuitive results, however, we find that predictions of which particular user or URL will generate large cascades are relatively unreliable. We conclude, therefore, that word-of-mouth diffusion can only be harnessed reliably by targeting large numbers of potential influencers, thereby capturing average effects. Finally, we consider a family of hypothetical marketing strategies, defined by the relative cost of identifying versus compensating potential "influencers." We find that although under some circumstances, the most influential users are also the most cost-effective, under a wide range of plausible assumptions the most cost-effective performance can be realized using "ordinary influencers"---individuals who exert average or even less-than-average influence.

#index 1536510
#* A comparative analysis of cascade measures for novelty and diversity
#@ Charles L.A. Clarke;Nick Craswell;Ian Soboroff;Azin Ashkan
#t 2011
#c 2
#% 262105
#% 262112
#% 411762
#% 642975
#% 766409
#% 818266
#% 879618
#% 879630
#% 907493
#% 1035578
#% 1073970
#% 1074133
#% 1095876
#% 1166473
#% 1263586
#% 1263587
#% 1292528
#% 1366523
#% 1400011
#% 1482378
#! Traditional editorial effectiveness measures, such as nDCG, remain standard for Web search evaluation. Unfortunately, these traditional measures can inappropriately reward redundant information and can fail to reflect the broad range of user needs that can underlie a Web query. To address these deficiencies, several researchers have recently proposed effectiveness measures for novelty and diversity. Many of these measures are based on simple cascade models of user behavior, which operate by considering the relationship between successive elements of a result list. The properties of these measures are still poorly understood, and it is not clear from prior research that they work as intended. In this paper we examine the properties and performance of cascade measures with the goal of validating them as tools for measuring effectiveness. We explore their commonalities and differences, placing them in a unified framework; we discuss their theoretical difficulties and limitations, and compare the measures experimentally, contrasting them against traditional measures and against other approaches to measuring novelty. Data collected by the TREC 2009 Web Track is used as the basis for our experimental comparison. Our results indicate that these measures reward systems that achieve an balance between novelty and overall precision in their result lists, as intended. Nonetheless, other measures provide insights not captured by the cascade measures, and we suggest that future evaluation efforts continue to report a variety of measures.

#index 1536511
#* Understanding and predicting personal navigation
#@ Jaime Teevan;Daniel J. Liebling;Gayathri Ravichandran Geetha
#t 2011
#c 2
#% 194299
#% 590523
#% 642982
#% 754059
#% 754126
#% 805878
#% 818221
#% 818259
#% 832099
#% 832349
#% 838547
#% 881544
#% 907515
#% 907556
#% 956552
#% 987211
#% 1043040
#% 1047435
#% 1074071
#% 1089474
#% 1131030
#% 1190094
#% 1195879
#% 1280759
#% 1355035
#% 1376118
#% 1392483
#% 1682429
#! This paper presents an algorithm that predicts with very high accuracy which Web search result a user will click for one sixth of all Web queries. Prediction is done via a straightforward form of personalization that takes advantage of the fact that people often use search engines to re-find previously viewed resources. In our approach, an individual's past navigational behavior is identified via query log analysis and used to forecast identical future navigational behavior by the same individual. We compare the potential value of personal navigation with general navigation identified using aggregate user behavior. Although consistent navigational behavior across users can be useful for identifying a subset of navigational queries, different people often use the same queries to navigate to different resources. This is true even for queries comprised of unambiguous company names or URLs and typically thought of as navigational. We build an understanding of what personal navigation looks like, and identify ways to improve its coverage and accuracy by taking advantage of people's consistency over time and across groups of individuals.

#index 1536512
#* Quality-biased ranking of web documents
#@ Michael Bendersky;W. Bruce Croft;Yanlei Diao
#t 2011
#c 2
#% 218982
#% 262096
#% 268079
#% 290830
#% 309150
#% 397126
#% 433999
#% 577224
#% 818241
#% 818255
#% 818262
#% 838472
#% 840846
#% 869471
#% 869534
#% 976952
#% 1019074
#% 1074107
#% 1166525
#% 1355019
#% 1356185
#% 1392507
#% 1415722
#% 1415741
#! Many existing retrieval approaches do not take into account the content quality of the retrieved documents, although link-based measures such as PageRank are commonly used as a form of document prior. In this paper, we present the quality-biased ranking method that promotes documents containing high-quality content, and penalizes low-quality documents. The quality of the document content can be determined by its readability, layout and ease-of-navigation, among other factors. Accordingly, instead of using a single estimate for document quality, we consider multiple content-based features that are directly integrated into a state-of- the-art retrieval method. These content-based features are easy to compute, store and retrieve, even for large web collections. We use several query sets and web collections to empirically evaluate the performance of our quality-biased retrieval method. In each case, our method consistently improves by a large margin the retrieval performance of text-based and link-based retrieval methods that do not take into account the quality of the document content.

#index 1536513
#* Ranking from pairs and triplets: information quality, evaluation methods and query complexity
#@ Kira Radinsky;Nir Ailon
#t 2011
#c 2
#% 262112
#% 272510
#% 411762
#% 577224
#% 766409
#% 963243
#% 987241
#% 1015625
#% 1039607
#% 1047347
#% 1066718
#% 1074021
#% 1074137
#% 1093383
#% 1095876
#% 1195893
#% 1211762
#% 1227641
#% 1250605
#% 1292528
#% 1355018
#% 1385985
#% 1415710
#! Obtaining judgments from human raters is a vital part in the design of search engines' evaluation. Today, a discrepancy exists between judgment acquisition from raters (training phase) and use of the responses for retrieval evaluation (evaluation phase). This discrepancy is due to the inconsistency between the representation of the information in both phases. During training, raters are requested to provide a relevance score for an individual result in the context of a query, whereas the evaluation is performed on ordered lists of search results, with the results' relative position (compared to other results) taken into account. As an alternative to the practice of learning to rank using relevance judgments for individual search results, more and more focus has recently been diverted to the theory and practice of learning from answers to combinatorial questions about sets of search results. That is, users, during training, are asked to rank small sets (typically pairs). Human rater responses to questions about the relevance of individual results are first compared to their responses to questions about the relevance of pairs of results. We empirically show that neither type of response can be deduced from the other, and that the added context created when results are shown together changes the raters' evaluation process. Since pairwise judgments are directly related to ranking, we conclude they are more accurate for that purpose. We go beyond pairs to show that triplets do not contain significantly more information than pairs for the purpose of measuring statistical preference. These two results establish good stability properties of pairwise comparisons for the purpose of learning to rank. We further analyze different scenarios, in which results of varying quality are added as "decoys". A recurring source of worry in papers focusing on pairwise comparison is the quadratic number of pairs in a set of results. Which preferences do we choose to solicit from paid raters? Can we provably eliminate a quadratic cost? We employ results from statistical learning theory to show that the quadratic cost can be provably eliminated in certain cases. More precisely, we show that in order to obtain a ranking in which each element is an average of O(n/C) positions away from its position in the optimal ranking, one needs to sample O(nC2) pairs uniformly at random, for any C 0. We also present an active learning algorithm which samples the pairs adaptively, and conjecture that it provides additional improvement.

#index 1536514
#* Bing dialog model: intent, knowledge and user interaction
#@ Harry Shum
#t 2011
#c 2
#! The decade-old Internet search outcomes, manifested in the form of "ten blue links," are no longer sufficient for Internet users. Many studies have shown that when users are ushered off the conventional search result pages through blue links, their needs are often partially met at best in a "hit-or-miss" fashion. To tackle this challenge, we have designed Bing, Microsoft's decision engine, to not just navigate users to a landing page through a blue link but to continue engaging with users to clarify intent and facilitate task completion. Underlying this new paradigm is the Bing Dialog Model that consists of three building blocks: an indexing system that comprehensively collects information from the web and systematically harvests knowledge, an intent model that statistically infers user intent and predicts next action, and an interaction model that elicits user intent through mathematically optimized presentations of web information and domain knowledge that matches user needs. In this talk, I'll describe Bing Dialog Model in details and demonstrate it in action through some innovative features since the launch of www.Bing.com.

#index 1536515
#* We feel fine and searching the emotional web
#@ Sepandar D. Kamvar;Jonathan Harris
#t 2011
#c 2
#% 270633
#% 466892
#% 844513
#% 871013
#% 938687
#% 1127964
#! We present We Feel Fine, an emotional search engine and web-based artwork whose mission is to collect the world's emotions to help people better understand themselves and others. We Feel Fine continuously crawls blogs, microblogs, and social networking sites, extracting sentences that include the words "I feel" or "I am feeling", as well as the gender, age, and location of the people authoring those sentences. The We Feel Fine search interface allows users to search or browse over the resulting sentence-level index, asking questions such as "How did young people in Ohio feel when Obama was elected?" While most research in sentiment analysis focuses on algorithms for extraction and classification of sentiment about given topics, we focus instead on building an interface that provides an engaging means of qualitative exploration of emotional data, and a flexible data collection and serving architecture that enables an ecosystem of data analysis applications. We use our observations on the usage of We Feel Fine to suggest a class of visualizations called Experiential Data Visualization, which focus on immersive item-level interaction with data. We also discuss the implications of such visualizations for crowdsourcing qualitative research in the social sciences.

#index 1536516
#* On the selection of tags for tag clouds
#@ Petros Venetis;Georgia Koutrika;Hector Garcia-Molina
#t 2011
#c 2
#% 218992
#% 281186
#% 311027
#% 765464
#% 766433
#% 955013
#% 956649
#% 1083629
#% 1173697
#% 1181246
#% 1387574
#! We examine the creation of a tag cloud for exploring and understanding a set of objects (e.g., web pages, documents). In the first part of our work, we present a formal system model for reasoning about tag clouds. We then present metrics that capture the structural properties of a tag cloud, and we briefly present a set of tag selection algorithms that are used in current sites (e.g., del.icio.us, Flickr, Technorati) or that have been described in recent work. In order to evaluate the results of these algorithms, we devise a novel synthetic user model. This user model is specifically tailored for tag cloud evaluation and assumes an "ideal" user. We evaluate the algorithms under this user model, as well as the model itself, using two datasets: CourseRank (a Stanford social tool containing information about courses) and del.icio.us (a social bookmarking site). The results yield insights as to when and why certain selection schemes work best.

#index 1536517
#* Efficient indexing of repeated n-grams
#@ Samuel Huston;Alistair Moffat;W. Bruce Croft
#t 2011
#c 2
#% 290703
#% 479973
#% 818262
#% 944350
#% 963669
#% 978157
#% 1023422
#% 1074122
#% 1181094
#% 1215321
#% 1227596
#% 1450913
#! The identification of repeated n-gram phrases in text has many practical applications, including authorship attribution, text reuse identification, and plagiarism detection. We consider methods for finding the repeated n-grams in text corpora, with emphasis on techniques that can be effectively scaled across a cluster of processors to handle very large amounts of text. We compare our proposed method to existing techniques using the 1.5 TB TREC ClueWeb-B text collection, using both single-processor and multi-processor approaches. The experiments show that our method offers an important tradeoff between speed and temporary storage space, and provides an alternative to previous approaches that scales almost linearly in the length of the sequence, is largely independent of n, and provides a uniform workload balance across the set of available processors.

#index 1536518
#* Batch query processing for web search engines
#@ Shuai Ding;Josh Attenberg;Ricardo Baeza-Yates;Torsten Suel
#t 2011
#c 2
#% 212665
#% 262099
#% 290703
#% 292684
#% 333932
#% 340888
#% 379372
#% 387427
#% 481290
#% 577302
#% 659994
#% 765678
#% 805864
#% 867054
#% 963669
#% 983467
#% 987215
#% 987216
#% 987275
#% 993962
#% 1015265
#% 1055710
#% 1074067
#% 1083323
#% 1166469
#% 1166527
#% 1190095
#% 1190098
#% 1306948
#% 1348337
#% 1399951
#% 1399964
#% 1400012
#% 1451182
#! Large web search engines are now processing billions of queries per day. Most of these queries are interactive in nature, requiring a response in fractions of a second. However, there are also a number of important scenarios where large batches of queries are submitted for various web mining and system optimization tasks that do not require an immediate response. Given the significant cost of executing search queries over billions of web pages, it is a natural question to ask if such batches of queries can be more efficiently executed than interactive queries. In this paper, we motivate and discuss the problem of batch query processing in search engines, identify basic mechanisms for improving the performance of such queries, and provide a preliminary experimental evaluation of the proposed techniques. Our conclusion is that significant cost reductions are possible by using specialized mechanisms for executing batch queries in Web search engines.

#index 1536519
#* Detecting duplicate web documents using clickthrough data
#@ Filip Radlinski;Paul N. Bennett;Emine Yilmaz
#t 2011
#c 2
#% 255137
#% 262112
#% 280437
#% 345087
#% 347225
#% 375017
#% 544011
#% 577224
#% 590523
#% 642975
#% 766472
#% 805200
#% 851306
#% 879618
#% 946521
#% 989628
#% 1035578
#% 1074025
#% 1074121
#% 1077150
#% 1166473
#% 1166522
#% 1190093
#% 1227591
#% 1250379
#% 1263586
#% 1292528
#% 1301004
#% 1312812
#% 1415710
#% 1450881
#% 1450895
#! The web contains many duplicate and near-duplicate documents. Given that user satisfaction is negatively affected by redundant information in search results, a significant amount of research has been devoted to developing duplicate detection algorithms. However, most such algorithms rely solely on document content to detect duplication, ignoring the fact that a primary goal of duplicate detection is to identify documents that contain redundant information with respect to a particular user query. Similarly, although query-dependent result diversification algorithms compute a query-dependent ranking, they tend to do so on the basis of a query-independent content similarity score. In this paper, we bridge the gap between query-dependent redundancy and query-independent duplication by showing how user click behavior following a query provides evidence about the relative novelty of web documents. While most previous work on interpreting user clicks on search results has assumed that they reflect just result relevance, we show that clicks also provide information about duplication between web documents since users consider search results in the context of previously seen documents. Moreover, we find that duplication explains a substantial amount of presentation bias observed in clicking behavior. We identify three distinct types of redundancy that commonly occur on the web and show how click data can be used to detect these different types.

#index 1536520
#* KMV-peer: a robust and adaptive peer-selection algorithm
#@ Yosi Mass;Yehoshua Sagiv;Michal Shmueli-Scheuer
#t 2011
#c 2
#% 309095
#% 333854
#% 340175
#% 505869
#% 519953
#% 578337
#% 610851
#% 643012
#% 768521
#% 879606
#% 907503
#% 960250
#% 983664
#% 987277
#% 1074091
#% 1091645
#% 1118259
#% 1132145
#% 1290542
#% 1292508
#% 1292751
#% 1392444
#% 1482388
#! The problem of fully decentralized search over many collections is considered. The objective is to approximate the results of centralized search (namely, using a central index) while controlling the communication cost and involving only a small number of collections. The proposed solution is couched in a peer-to-peer (P2P) network, but can also be applied in other setups. Peers publish per-term summaries of their collections. Specifically, for each term, the range of document scores is divided into intervals; and for each interval, a KMV (K Minimal Values) synopsis of its documents is created. A new peer-selection algorithm uses the KMV synopses and two scoring functions in order to adaptively rank the peers, according to the relevance of their documents to a given query. The proposed method achieves high-quality results while meeting the above criteria of efficiency. In particular, experiments are done on two large, real-world datasets; one is blogs and the other is web data. These experiments show that the algorithm outperforms the state-of-the-art approaches and is robust over different collections, various scoring functions and multi-term queries.

#index 1536521
#* Understanding temporal query dynamics
#@ Anagha Kulkarni;Jaime Teevan;Krysta M. Svore;Susan T. Dumais
#t 2011
#c 2
#% 257634
#% 577220
#% 577370
#% 655487
#% 730070
#% 754058
#% 765412
#% 766408
#% 766447
#% 805839
#% 810054
#% 879639
#% 956509
#% 956552
#% 960414
#% 1055715
#% 1074071
#% 1130999
#% 1166523
#% 1166533
#% 1183221
#% 1270766
#% 1338638
#% 1355016
#% 1355017
#% 1467831
#! Web search is strongly influenced by time. The queries people issue change over time, with some queries occasionally spiking in popularity (e.g., earthquake) and others remaining relatively constant (e.g., youtube). The documents indexed by the search engine also change, with some documents always being about a particular query (e.g., the Wikipedia page on earthquakes is about the query earthquake) and others being about the query only at a particular point in time (e.g., the New York Times is only about earthquakes following a major seismic activity). The relationship between documents and queries can also change as people's intent changes (e.g., people sought different content for the query earthquake before the Haitian earthquake than they did after). In this paper, we explore how queries, their associated documents, and the query intent change over the course of 10 weeks by analyzing query log data, a daily Web crawl, and periodic human relevance judgments. We identify several interesting features by which changes to query popularity can be classified, and show that presence of these features, when accompanied by changes in result content, can be a good indicator of change in query intent.

#index 1536522
#* Patterns of temporal variation in online media
#@ Jaewon Yang;Jure Leskovec
#t 2011
#c 2
#% 224113
#% 273704
#% 406493
#% 577276
#% 577360
#% 578400
#% 662750
#% 754107
#% 795273
#% 805839
#% 823332
#% 869516
#% 876007
#% 956509
#% 989613
#% 989650
#% 1040837
#% 1127609
#% 1183303
#% 1214630
#% 1214671
#% 1378458
#! Online content exhibits rich temporal dynamics, and diverse realtime user generated content further intensifies this process. However, temporal patterns by which online content grows and fades over time, and by which different pieces of content compete for attention remain largely unexplored. We study temporal patterns associated with online content and how the content's popularity grows and fades over time. The attention that content receives on the Web varies depending on many factors and occurs on very different time scales and at different resolutions. In order to uncover the temporal dynamics of online content we formulate a time series clustering problem using a similarity metric that is invariant to scaling and shifting. We develop the K-Spectral Centroid (K-SC) clustering algorithm that effectively finds cluster centroids with our similarity measure. By applying an adaptive wavelet-based incremental approach to clustering, we scale K-SC to large data sets. We demonstrate our approach on two massive datasets: a set of 580 million Tweets, and a set of 170 million blog posts and news media articles. We find that K-SC outperforms the K-means clustering algorithm in finding distinct shapes of time series. Our analysis shows that there are six main temporal shapes of attention of online content. We also present a simple model that reliably predicts the shape of attention by using information about only a small number of participants. Our analyses offer insight into common temporal patterns of the content on theWeb and broaden the understanding of the dynamics of human attention.

#index 1536523
#* Using graded-relevance metrics for evaluating community QA answer selection
#@ Tetsuya Sakai;Daisuke Ishikawa;Noriko Kando;Yohei Seki;Kazuko Kuriyama;Chin-Yew Lin
#t 2011
#c 2
#% 411762
#% 879593
#% 879630
#% 940039
#% 950043
#% 956516
#% 1035587
#% 1055679
#% 1166519
#% 1179994
#% 1227599
#% 1251648
#! Community Question Answering (CQA) sites such as Yahoo! Answers have emerged as rich knowledge resources for information seekers. However, answers posted to CQA sites can be irrelevant, incomplete, redundant, incorrect, biased, ill-formed or even abusive. Hence, automatic selection of "good" answers for a given posted question is a practical research problem that will help us manage the quality of accumulated knowledge. One way to evaluate answer selection systems for CQA would be to use the Best Answers (BAs) that are readily available from the CQA sites. However, BAs may be biased, and even if they are not, there may be other good answers besides BAs. To remedy these two problems, we propose system evaluation methods that involve multiple answer assessors and graded-relevance information retrieval metrics. Our main findings from experiments using the NTCIR-8 CQA task data are that, using our evaluation methods, (a) we can detect many substantial differences between systems that would have been overlooked by BA-based evaluation; and (b) we can better identify hard questions (i.e. those that are handled poorly by many systems and therefore require focussed investigation) compared to BAbased evaluation. We therefore argue that our approach is useful for building effective CQA answer selection systems despite the cost of manual answer assessments.

#index 1536524
#* Mining social images with distance metric learning for automated image tagging
#@ Pengcheng Wu;Steven Chu-Hong Hoi;Peilin Zhao;Ying He
#t 2011
#c 2
#% 80995
#% 318785
#% 457912
#% 642989
#% 760805
#% 780804
#% 812486
#% 829025
#% 884027
#% 884044
#% 975105
#% 983830
#% 983905
#% 1000325
#% 1038781
#% 1071084
#% 1074095
#% 1131836
#% 1250593
#% 1279781
#% 1386134
#! With the popularity of various social media applications, massive social images associated with high quality tags have been made available in many social media web sites nowadays. Mining social images on the web has become an emerging important research topic in web search and data mining. In this paper, we propose a machine learning framework for mining social images and investigate its application to automated image tagging. To effectively discover knowledge from social images that are often associated with multimodal contents (including visual images and textual tags), we propose a novel Unified Distance Metric Learning (UDML) scheme, which not only exploits both visual and textual contents of social images, but also effectively unifies both inductive and transductive metric learning techniques in a systematic learning framework. We further develop an efficient stochastic gradient descent algorithm for solving the UDML optimization task and prove the convergence of the algorithm. By applying the proposed technique to the automated image tagging task in our experiments, we demonstrate that our technique is empirically effective and promising for mining social images towards some real applications.

#index 1536525
#* Dynamic relationship and event discovery
#@ Anish Das Sarma;Alpa Jain;Cong Yu
#t 2011
#c 2
#% 266216
#% 301241
#% 458630
#% 577220
#% 754068
#% 824666
#% 987218
#% 989650
#% 1214671
#% 1269909
#% 1275182
#% 1355045
#% 1400018
#! This paper studies the problem of dynamic relationship and event discovery. A large body of previous work on relation extraction focuses on discovering predefined and static relationships between entities. In contrast, we aim to identify temporally defined (e.g., co-bursting) relationships that are not predefined by an existing schema, and we identify the underlying time constrained events that lead to these relationships. The key challenges in identifying such events include discovering and verifying dynamic connections among entities, and consolidating binary dynamic connections into events consisting of a set of entities that are connected at a given time period. We formalize this problem and introduce an efficient end-to-end pipeline as a solution. In particular, we introduce two formal notions, global temporal constraint cluster and local temporal constraint cluster, for detecting dynamic events. We further design efficient algorithms for discovering such events from a large graph of dynamic relationships. Finally, detailed experiments on real data show the effectiveness of our proposed solution.

#index 1536526
#* Joint training for open-domain extraction on the web: exploiting overlap when supervision is limited
#@ Rahul Gupta;Sunita Sarawagi
#t 2011
#c 2
#% 252011
#% 301241
#% 938708
#% 939376
#% 939641
#% 940004
#% 1166537
#% 1211745
#% 1328133
#% 1328199
#% 1355026
#% 1417097
#% 1417383
#% 1650352
#! We consider the problem of jointly training structured models for extraction from multiple web sources whose records enjoy partial content overlap. This has important applications in open-domain extraction, e.g. a user materializing a table of interest from multiple relevant unstructured sources; or a site like Freebase augmenting an incomplete relation by extracting more rows from web sources. Such applications require extraction over arbitrary domains, so one cannot use a pre-trained extractor or demand a huge labeled dataset. We propose to overcome this lack of supervision by using content overlap across the related web sources. Existing methods of exploiting overlap have been developed under settings that do not generalize easily to the scale and diversity of overlap seen on Web sources. We present an agreement-based learning framework that jointly trains the models by biasing them to agree on the agreement regions, i.e. shared text segments. We present alternatives within our framework to trade-off tractability, robustness to noise, and extent of agreement enforced; and propose a scheme of partitioning agreement regions that leads to efficient training while maximizing overall accuracy. Further, we present a principled scheme to discover low-noise agreement regions in unlabeled data across multiple sources. Through extensive experiments over 58 different extraction domains, we establish that our framework provides significant boosts over uncoupled training, and scores over alternatives such as collective inference, staged training, and multi-view learning.

#index 1536527
#* Scalable knowledge harvesting with high precision and high recall
#@ Ndapandula Nakashole;Martin Theobald;Gerhard Weikum
#t 2011
#c 2
#% 152934
#% 211247
#% 258595
#% 301241
#% 459006
#% 504443
#% 850430
#% 956564
#% 1022288
#% 1055735
#% 1183369
#% 1190065
#% 1190118
#% 1206687
#% 1206862
#% 1270341
#% 1275182
#% 1278124
#% 1291356
#% 1354118
#% 1355026
#% 1372745
#% 1409954
#% 1426449
#% 1467732
#% 1471591
#% 1496780
#% 1518201
#% 1698594
#! Harvesting relational facts from Web sources has received great attention for automatically constructing large knowledge bases. Stateof-the-art approaches combine pattern-based gathering of fact candidates with constraint-based reasoning. However, they still face major challenges regarding the trade-offs between precision, recall, and scalability. Techniques that scale well are susceptible to noisy patterns that degrade precision, while techniques that employ deep reasoning for high precision cannot cope with Web-scale data. This paper presents a scalable system, called PROSPERA, for high-quality knowledge harvesting. We propose a new notion of ngram-itemsets for richer patterns, and use MaxSat-based constraint reasoning on both the quality of patterns and the validity of fact candidates.We compute pattern-occurrence statistics for two benefits: they serve to prune the hypotheses space and to derive informative weights of clauses for the reasoner. The paper shows how to incorporate these building blocks into a scalable architecture that can parallelize all phases on a Hadoop-based distributed platform. Our experiments with the ClueWeb09 corpus include comparisons to the recent ReadTheWeb experiment. We substantially outperform these prior results in terms of recall, with the same precision, while having low run-times.

#index 1536528
#* Mining named entities with temporally correlated bursts from multilingual web news streams
#@ Alexander Kotov;ChengXiang Zhai;Richard Sproat
#t 2011
#c 2
#% 155349
#% 309096
#% 577220
#% 729943
#% 741114
#% 805839
#% 823405
#% 854584
#% 876007
#% 939770
#% 989650
#% 990166
#% 1019079
#% 1083732
#% 1166524
#% 1810819
#! In this work, we study a new text mining problem of discovering named entities with temporally correlated bursts of mention counts in multiple multilingual Web news streams. Mining named entities with temporally correlated bursts of mention counts in multilingual text streams has many interesting and important applications, such as identification of the latent events, attracting the attention of on-line media in different countries, and valuable linguistic knowledge in the form of transliterations. While mining "bursty" terms in a single text stream has been studied before, the problem of detecting terms with temporally correlated bursts in multilingual Web streams raises two new challenges: (i) correlated terms in multiple streams may have bursts that are of different orders of magnitude in their intensity and (ii) bursts of correlated terms may be separated by time gaps. We propose a two-stage method for mining items with temporally correlated bursts from multiple data streams, which addresses both challenges. In the first stage of the method, the temporal behavior of different entities is normalized by modeling them with the Markov-Modulated Poisson Process. In the second stage, a dynamic programming algorithm is used to discover correlated bursts of different items, that can be potentially separated by time gaps. We evaluated our method with the task of discovering transliterations of named entities from multilingual Web news streams. Experimental results indicate that our method can not only effectively discover named entities with correlated bursts in multilingual Web news streams, but also outperforms two state-of-the-art baseline methods for unsupervised discovery of transliterations in static text collections.

#index 1536529
#* Dynamic ranked retrieval
#@ Christina Brandt;Thorsten Joachims;Yisong Yue;Jacob Bank
#t 2011
#c 2
#% 118728
#% 262112
#% 342707
#% 642975
#% 742666
#% 783506
#% 818209
#% 1051038
#% 1055719
#% 1073970
#% 1074025
#% 1074081
#% 1074133
#% 1077150
#% 1083721
#% 1130853
#% 1166473
#% 1214650
#% 1227623
#% 1312812
#% 1355032
#% 1450845
#% 1450874
#! We present a theoretically well-founded retrieval model for dynamically generating rankings based on interactive user feedback. Unlike conventional rankings that remain static after the query was issued, dynamic rankings allow and anticipate user activity, thus providing a way to combine the otherwise contradictory goals of result diversification and high recall. We develop a decision-theoretic framework to guide the design and evaluation of algorithms for this interactive retrieval setting. Furthermore, we propose two dynamic ranking algorithms, both of which are computationally efficient. We prove that these algorithms provide retrieval performance that is guaranteed to be at least as good as the optimal static ranking algorithm. In empirical evaluations, dynamic ranking shows substantial improvements in retrieval performance over conventional static rankings.

#index 1536530
#* Optimizing two-dimensional search results presentation
#@ Flavio Chierichetti;Ravi Kumar;Prabhakar Raghavan
#t 2011
#c 2
#% 272816
#% 411762
#% 452152
#% 464572
#% 794149
#% 879686
#% 954948
#% 1035578
#% 1048694
#% 1053505
#% 1095876
#% 1171607
#% 1171610
#% 1355037
#% 1376021
#% 1384641
#% 1678005
#! Classic search engine results are presented as an ordered list of documents and the problem of presentation trivially reduces to ordering documents by their scores. This is because users scan a list presentation from top to bottom. This leads to natural list optimization measures such as the discounted cumulative gain (DCG) and the rank-biased precision (RBP). Increasingly, search engines are using two-dimensional results presentations; image and shopping search results are long-standing examples. The simplistic heuristic used in practice is to place images by row-major order in the matrix presentation. However, a variety of evidence suggests that users' scan of pages is not in this matrix order. In this paper we (1) view users' scan of a results page as a Markov chain, which yields DCG and RBP as special cases for linear lists; (2) formulate, study, and develop solutions for the problem of inferring the Markov chain from click logs; (3) from these inferred Markov chains, empirically validate folklore phenomena (e.g., the "golden triangle" of user scans in two dimensions); and (4) develop and experimentally compare algorithms for optimizing user utility in matrix presentations. The theory and algorithms extend naturally beyond matrix presentations.

#index 1536531
#* Result enrichment in commerce search using browse trails
#@ Debmalya Panigrahi;Sreenivas Gollapudi
#t 2011
#c 2
#% 169806
#% 280839
#% 310567
#% 325001
#% 330617
#% 348155
#% 479973
#% 548479
#% 569754
#% 869501
#% 879567
#% 894253
#% 956495
#% 987222
#% 999292
#% 1055676
#% 1074147
#% 1130852
#% 1130854
#% 1214738
#% 1227578
#% 1227610
#% 1227648
#% 1275193
#% 1426566
#! Commerce search engines have become popular in recent years, as users increasingly search for (and buy) products on the web. In response to an user query, they surface links to products in their catalog (or index) that match the requirements specified in the query. Often, few or no product in the catalog matches the user query exactly, and the search engine is forced to return a set of products that partially match the query. This paper considers the problem of choosing a set of products in response to an user query, so as to ensure maximum user satisfaction. We call this the result enrichment problem in commerce search. The challenge in result enrichment is two-fold: the search engine needs to estimate the extent to which a user genuinely cares about an attribute that she has specified in a query; then, it must display products in the catalog that match the user requirement on the important attributes, but have a similar but possibly non-identical value on the less important ones. To this end, we propose a technique for measuring the importance of individual attribute values and the similarity between different values of an attribute. A novelty of our approach is that we use entire browse trails, rather than just clickthrough rates, in this estimation algorithm. We develop a model for this problem, propose an algorithm to solve it, and support our theoretical findings via experiments conducted on actual user data.

#index 1536532
#* Identifying task-based sessions in search engine query logs
#@ Claudio Lucchese;Salvatore Orlando;Raffaele Perego;Fabrizio Silvestri;Gabriele Tolomei
#t 2011
#c 2
#% 194299
#% 284796
#% 286069
#% 296646
#% 320839
#% 406493
#% 449294
#% 590523
#% 754059
#% 805878
#% 823348
#% 838547
#% 948375
#% 950658
#% 1130868
#% 1130878
#% 1195920
#% 1275012
#% 1275285
#% 1348059
#% 1348355
#% 1417245
#! The research challenge addressed in this paper is to devise effective techniques for identifying task-based sessions, i.e. sets of possibly non contiguous queries issued by the user of a Web Search Engine for carrying out a given task. In order to evaluate and compare different approaches, we built, by means of a manual labeling process, a ground-truth where the queries of a given query log have been grouped in tasks. Our analysis of this ground-truth shows that users tend to perform more than one task at the same time, since about 75% of the submitted queries involve a multi-tasking activity. We formally define the Task-based Session Discovery Problem (TSDP) as the problem of best approximating the manually annotated tasks, and we propose several variants of well known clustering algorithms, as well as a novel efficient heuristic algorithm, specifically tuned for solving the TSDP. These algorithms also exploit the collaborative knowledge collected by Wiktionary and Wikipedia for detecting query pairs that are not similar from a lexical content point of view, but actually semantically related. The proposed algorithms have been evaluated on the above ground-truth, and are shown to perform better than state-of-the-art approaches, because they effectively take into account the multi-tasking behavior of users.

#index 1536533
#* Recommender systems with social regularization
#@ Hao Ma;Dengyong Zhou;Chao Liu;Michael R. Lyu;Irwin King
#t 2011
#c 2
#% 173879
#% 330687
#% 397153
#% 452563
#% 643007
#% 734592
#% 734593
#% 734594
#% 766449
#% 790459
#% 818216
#% 840924
#% 879627
#% 987197
#% 987198
#% 989580
#% 1001279
#% 1055681
#% 1073982
#% 1074061
#% 1083671
#% 1130901
#% 1214666
#% 1227602
#% 1227603
#% 1275183
#% 1287226
#% 1292592
#% 1355025
#% 1385585
#% 1400002
#% 1451209
#% 1650569
#! Although Recommender Systems have been comprehensively analyzed in the past decade, the study of social-based recommender systems just started. In this paper, aiming at providing a general method for improving recommender systems by incorporating social network information, we propose a matrix factorization framework with social regularization. The contributions of this paper are four-fold: (1) We elaborate how social network information can benefit recommender systems; (2) We interpret the differences between social-based recommender systems and trust-aware recommender systems; (3) We coin the term Social Regularization to represent the social constraints on recommender systems, and we systematically illustrate how to design a matrix factorization objective function with social regularization; and (4) The proposed method is quite general, which can be easily extended to incorporate other contextual information, like social tags, etc. The empirical analysis on two large datasets demonstrates that our approaches outperform other state-of-the-art methods.

#index 1536534
#* Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms
#@ Lihong Li;Wei Chu;John Langford;Xuanhui Wang
#t 2011
#c 2
#% 170386
#% 384911
#% 416988
#% 425053
#% 466751
#% 722906
#% 876056
#% 1073938
#% 1190057
#% 1318590
#% 1399999
#% 1482363
#! Contextual bandit algorithms have become popular for online recommendation systems such as Digg, Yahoo! Buzz, and news recommendation in general. Offline evaluation of the effectiveness of new algorithms in these applications is critical for protecting online user experiences but very challenging due to their "partial-label" nature. Common practice is to create a simulator which simulates the online environment for the problem at hand and then run an algorithm against this simulator. However, creating simulator itself is often difficult and modeling bias is usually unavoidably introduced. In this paper, we introduce a replay methodology for contextual bandit algorithm evaluation. Different from simulator-based approaches, our method is completely data-driven and very easy to adapt to different applications. More importantly, our method can provide provably unbiased evaluations. Our empirical results on a large-scale news article recommendation dataset collected from Yahoo! Front Page conform well with our theoretical results. Furthermore, comparisons between our offline replay and online bucket evaluation of several contextual bandit algorithms show accuracy and effectiveness of our offline evaluation method.

#index 1536535
#* Efficient online ad serving in a display advertising exchange
#@ Kevin Lang;Joaquin Delgado;Dongming Jiang;Bhaskar Ghosh;Shirshanka Das;Amita Gajewar;Swaroop Jagadish;Arathi Seshan;Chavdar Botev;Michael Bindeberger-Ortega;Sunil Nagaraj;Raymie Stata
#t 2011
#c 2
#% 271199
#% 333938
#% 342372
#% 379482
#% 397375
#% 410276
#% 567463
#% 745450
#% 800534
#% 1127386
#% 1217208
#% 1328112
#% 1336429
#% 1426643
#% 1768383
#! We introduce and formalize a novel constrained path optimization problem that is the heart of the real-time ad serving task in the Yahoo! (formerly RightMedia) Display Advertising Exchange. In the Exchange, the ad server's task for each display opportunity is to compute, with low latency, an optimal valid path through a directed graph representing the business arrangements between the hundreds of thousands of business entities that are participating in the Exchange. These entities include not only publishers and advertisers, but also intermediate entities called "ad networks" which have delegated their ad serving responsibilities to the Exchange. Path optimality is determined by the payment to the publisher, and is affected by an advertiser's bid and also by the revenue-sharing agreements between the entities in the chosen path leading back to the publisher. Path validity is determined by constraints which focus on the following three issues: 1) suitability of the opportunity's web page and its publisher 2)suitability of the user who is currently viewing that web page, and 3) suitability of a candidate ad and its advertiser. Because the Exchange's constrained path optimization task is novel, there are no published algorithms for it. This paper describes two different algorithms that have both been successfully used in the actual Yahoo! ad server. The first algorithm has the advantage of being extremely simple, while the second is more robust thanks to its polynomial worst-case running time. In both cases, meeting latency caps has required that the basic algorithms be improved by optimizations; we will describe a candidate ordering scheme and a pre-computation scheme that have both been effective in reducing latency in the real ad serving system that serves over ten billion ad calls per day.

#index 1536536
#* Trend analysis model: trend consists of temporal words, topics, and timestamps
#@ Noriaki Kawamae
#t 2011
#c 2
#% 220711
#% 577220
#% 722904
#% 769892
#% 875959
#% 876017
#% 881498
#% 881529
#% 907489
#% 939896
#% 989623
#% 1214666
#% 1400078
#% 1450856
#% 1482243
#% 1536565
#! This paper presents a topic model that identifies interpretable low dimensional components in time-stamped data for capturing the evolution of trends. Unlike other models for time-stamped data, our proposal, the trend analysis model (TAM), focuses on the difference between temporal words and other words in each document to detect topic evolution over time. TAM introduces a latent trend class variable into each document and a latent switch variable into each token for handling these differences. The trend class has a probability distribution over temporal words, topics, and a continuous distribution over time, where each topic is responsible for generating words. The latter class uses a document specific probabilistic distribution to judge which variable each word comes from for generating words in each token. Accordingly, TAM can explain which topic co-occurrence pattern will appear at any given time, and represents documents of similar content and timestamp as sharing the same trend class. Therefore, TAM projects them on a latent space of trend dimensionality and allows us to predict the temporal evolution of words and topics in document collections. Experiments on various data sets show that the proposed model can capture interpretable low dimensionality sets of topics and timestamps, take advantage of previous models, and is useful as a generative model in the analysis of the evolution of trends.

#index 1536537
#* Topical semantics of twitter links
#@ Michael J. Welch;Uri Schonfeld;Dan He;Junghoo Cho
#t 2011
#c 2
#% 281214
#% 282905
#% 309749
#% 310514
#% 577329
#% 577360
#% 754060
#% 754107
#% 1016177
#% 1040837
#% 1080077
#% 1246987
#% 1355018
#% 1355042
#% 1470582
#! Twitter, a micro-blogging platform with an estimated 20 million unique monthly visitors and over 100 million registered users, offers an abundance of rich, structured data at a rate exceeding 600 tweets per second. Recent efforts to leverage this social data to rank users by quality and topical relevance have largely focused on the "follow" relationship. Twitter's data offers additional implicit relationships between users, however, such as "retweets" and "mentions". In this paper we investigate the semantics of the follow and retweet relationships. Specifically, we show that the transitivity of topical relevance is better preserved over retweet links, and that retweeting a user is a significantly stronger indicator of topical interest than following him. We demonstrate these properties by ranking users with two variants of the PageRank algorithm; one based on the follows sub-graph and one based on the implicit retweet sub-graph. We perform a user study to assess the topical relevance of the resulting top-ranked users.

#index 1536538
#* Evaluating the visual quality of web pages using a computational aesthetic approach
#@ Ou Wu;Yunfei Chen;Bing Li;Weiming Hu
#t 2011
#c 2
#% 269217
#% 311872
#% 330765
#% 727925
#% 754078
#% 762422
#% 768632
#% 786793
#% 940986
#% 943995
#% 1014991
#% 1055638
#% 1083692
#% 1129483
#% 1129490
#% 1130926
#% 1148278
#% 1190089
#% 1190132
#% 1223195
#% 1262955
#% 1269106
#% 1558464
#% 1698382
#! Current Web mining explores useful and valuable information (content) online for users. However, there is scant research on the overall visual aspect of Web pages, even though visual elements such as aesthetics significantly influence user experience. A beautiful and well-laid out Web page greatly facilitates users' accessing and enhances browsing experiences.We use "visual quality (VisQ)" to denote the aesthetics of Web pages. In this paper, a computational aesthetics approach is proposed to learn the evaluation model for the visual quality of Web pages. First, a Web page layout extraction algorithm (V-LBE) is introduced to partition a Web page into major layout blocks. Then, regarding a Web page as a semi-structured image, features (e.g., layout,visual complexity, colorfulness) known to significantly affect the visual quality of a Web page are extracted to construct a feature vector. We present a multi-cost-sensitive learning for visual quality classification and a multi-value regression for visual quality score assignment. Our experiments compare the extracted features and conclude that the Web page's layout visual features (LV) and text visual features (TV) are the primary affecting factors toward Web page's visual quality. The performance of the learned visual quality classifier is close to some persons'. The learned regression function also achieves promising results.

#index 1536539
#* Clustering product features for opinion mining
#@ Zhongwu Zhai;Bing Liu;Hua Xu;Peifa Jia
#t 2011
#c 2
#% 158687
#% 311027
#% 464291
#% 465914
#% 722904
#% 747738
#% 748465
#% 769892
#% 786511
#% 805873
#% 823620
#% 828958
#% 869500
#% 891559
#% 936239
#% 939627
#% 939896
#% 956510
#% 956570
#% 1008095
#% 1055682
#% 1127964
#% 1211693
#% 1214741
#% 1251688
#% 1261588
#% 1270651
#% 1275285
#% 1292576
#% 1299639
#% 1330553
#% 1338626
#% 1470609
#% 1484383
#% 1656698
#! In sentiment analysis of product reviews, one important problem is to produce a summary of opinions based on product features/attributes (also called aspects). However, for the same feature, people can express it with many different words or phrases. To produce a useful summary, these words and phrases, which are domain synonyms, need to be grouped under the same feature group. Although several methods have been proposed to extract product features from reviews, limited work has been done on clustering or grouping of synonym features. This paper focuses on this task. Classic methods for solving this problem are based on unsupervised learning using some forms of distributional similarity. However, we found that these methods do not do well. We then model it as a semi-supervised learning problem. Lexical characteristics of the problem are exploited to automatically identify some labeled examples. Empirical evaluation shows that the proposed method outperforms existing state-of-the-art methods by a large margin.

#index 1536540
#* Materializing multi-relational databases from the web using taxonomic queries
#@ Matthew Michelson;Sofus A. Macskassy;Steven N. Minton;Lise Getoor
#t 2011
#c 2
#% 301241
#% 397369
#% 458630
#% 577319
#% 654459
#% 765409
#% 765433
#% 939540
#% 939924
#% 1016163
#% 1026925
#% 1083705
#% 1250378
#% 1250418
#% 1275182
#% 1289516
#% 1292488
#% 1305621
#% 1328133
#% 1328199
#% 1328200
#% 1338552
#! Recently, much attention has been given to extracting tables from Web data. In this problem, the column definitions and tuples (such as what "company" is headquartered in what "city,") are extracted from Web text, structured Web data such as lists, or results of querying the deep Web, creating the table of interest. In this paper, we examine the problem of extracting and discovering multiple tables in a given domain, generating a truly multi-relational database as output. Beyond discovering the relations that define single tables, our approach discovers and leverages "within column" set membership relations, and discovers relations across the extracted tables (e.g., joins). By leveraging within-column relations our method can extract table instances that are ambiguous or rare, and by discovering joins, our method generates truly multi-relational output. Further, our approach uses taxonomic queries to bootstrap the extraction, rather than the more traditional "seed instances." Creating seeds often requires more domain knowledge than taxonomic queries, and previous work has shown that extraction methods may be sensitive to which input seeds they are given. We test our approach on two real world domains: NBA basketball and cancer information. Our results demonstrate that our approach generates databases of relevant tables from disparate Web information, and discovers the relations between them. Further, we show that by leveraging the "within column" relation our approach can identify a significant number of relevant tuples that would be difficult to do so otherwise.

#index 1536541
#* What blogs tell us about websites: a demographics study
#@ Matthew Michelson;Sofus A. Macskassy
#t 2011
#c 2
#% 269281
#% 301553
#% 324924
#% 342870
#% 453320
#% 630984
#% 799896
#% 823394
#% 1040841
#! One challenge for content providers on the Web is determining who consumes their content. For instance, online newspapers want to know who is reading their articles. Previous approaches have tried to determine such audience demographics by placing cookies on users' systems, or by directly asking consumers (e.g., through surveys). The first approach may make users uncomfortable, and the second is not scalable. In this paper we focus on determining the demographics of a Website's audience by analyzing the blogs that link to the Website. We analyze both the text of the blogs and the network connectivity of the blog network to determine demographics such as whether a person "is married" or "has pets." Presumably bloggers linking to sites also consume the content of those sites. Therefore, the discovered demographics for the bloggers can be used to represent a proxy set of demographics for a subset of the Website's consumers. We demonstrate that in many cases we can infer sub-audiences for a site from these demographics. Further, this feasibility demonstrates that very specific demographics for sites can be generated as we improve the methods for determining them (e.g., finding people who play video games). In our study we analyze blogs collected from more than 590,000 bloggers collected over a six month period that link to more than 488,000 distinct, external websites.

#index 1536542
#* Cross lingual text classification by mining multilingual topics from wikipedia
#@ Xiaochuan Ni;Jian-Tao Sun;Jian Hu;Zheng Chen
#t 2011
#c 2
#% 262046
#% 722904
#% 748574
#% 757787
#% 818313
#% 823405
#% 854672
#% 879590
#% 939511
#% 939570
#% 956510
#% 1055680
#% 1055682
#% 1055769
#% 1074073
#% 1074076
#% 1269899
#% 1275012
#% 1415756
#! This paper investigates how to effectively do cross lingual text classification by leveraging a large scale and multilingual knowledge base, Wikipedia. Based on the observation that each Wikipedia concept is described by documents of different languages, we adapt existing topic modeling algorithms for mining multilingual topics from this knowledge base. The extracted topics have multiple types of representations, with each type corresponding to one language. In this work, we regard such topics extracted from Wikipedia documents as universal-topics, since each topic corresponds with same semantic information of different languages. Thus new documents of different languages can be represented in a space using a group of universal-topics. We use these universal-topics to do cross lingual text classification. Given the training data labeled for one language, we can train a text classifier to classify the documents of another language by mapping all documents of both languages into the universal-topic space. This approach does not require any additional linguistic resources, like bilingual dictionaries, machine translation tools, or labeling data for the target language. The evaluation results indicate that our topic modeling approach is effective for building cross lingual text classifier.

#index 1536543
#* Multidimensional mining of large-scale search logs: a topic-concept cube approach
#@ Dongyeop Kang;Daxin Jiang;Jian Pei;Zhen Liao;Xiaohui Sun;Ho-Jin Choi
#t 2011
#c 2
#% 310567
#% 330617
#% 458379
#% 464215
#% 466263
#% 766447
#% 853543
#% 881504
#% 963669
#% 1055707
#% 1083721
#% 1190074
#% 1190135
#% 1712595
#! In addition to search queries and the corresponding clickthrough information, search engine logs record multidimensional information about user search activities, such as search time, location, vertical, and search device. Multidimensional mining of search logs can provide novel insights and useful knowledge for both search engine users and developers. In this paper, we describe our topic-concept cube project, which addresses the business need of supporting multidimensional mining of search logs effectively and efficiently. We answer two challenges. First, search queries and click-through data are well recognized sparse, and thus have to be aggregated properly for effective analysis. Second, there is often a gap between the topic hierarchies in multidimensional aggregate analysis and queries in search logs. To address those challenges, we develop a novel topic-concept model that learns a hierarchy of concepts and topics automatically from search logs. Enabled by the topicconcept model, we construct a topic-concept cube that supports online multidimensional mining of search log data. A distinct feature of our approach is that, in addition to the standard dimensions such as time and location, our topic-concept cube has a dimension of topics and concepts, which substantially facilitates the analysis of log data. To handle a huge amount of log data, we develop distributed algorithms for learning model parameters efficiently. We also devise approaches to computing a topic-concept cube. We report an empirical study verifying the effectiveness and efficiency of our approach on a real data set of 1.96 billion queries and 2.73 billion clicks.

#index 1536544
#* Optimizing merchant revenue with rebates
#@ Rakesh Agrawal;Samuel Ieong;Raja Velu
#t 2011
#c 2
#% 325589
#% 394984
#% 757953
#% 951166
#% 1055689
#% 1171604
#% 1197813
#% 1278446
#! We study an online advertising model in which the merchant reimburses a portion of the transacted amount to the customer in a form of rebate. The customer referral and the rebate transfer might be mediated by a search engine. We investigate how the merchants can set rebate rates across different products to maximize their revenue. We consider two widely used demand models in economics---linear and log-linear---and explain how the effects of rebates can be incorporated in these models. Treating the parameters estimated as inputs to a revenue maximization problem, we develop convex optimization formulations of the problem and combinatorial algorithms for solving them. We validate our modeling assumptions using real transaction data. We conduct an extensive simulation study to evaluate the performance of our approach on maximizing revenue, and found that it generates significantly higher revenues for merchants compared to other rebate strategies. The rebate rates selected are extremely close to the optimal rates selected in hindsight.

#index 1536545
#* Searchable web sites recommendation
#@ Yang Song;Nam Nguyen;Li-wei He;Scott Imig;Robert Rounthwaite
#t 2011
#c 2
#% 46803
#% 306468
#% 345262
#% 387427
#% 607815
#% 750863
#% 879581
#% 946523
#% 956537
#% 1074093
#% 1127557
#% 1166523
#% 1173691
#% 1190101
#% 1190102
#% 1190181
#% 1227616
#% 1227617
#! In this paper, we propose a new framework for searchable web sites recommendation. Given a query, our system will recommend a list of searchable web sites ranked by relevance, which can be used to complement the web page results and ads from a search engine. We model the conditional probability of a searchable web site being relevant to a given query in term of three main components: the language model of the query, the language model of the content within the web site, and the reputation of the web site searching capability (static rank). The language models for queries and searchable sites are built using information mined from client-side browsing logs. The static rank for each searchable site leverages features extracted from these client-side logs such as number of queries that are submitted to this site, and features extracted from general search engines such as the number of web pages that indexed for this site, number of clicks per query, and the dwell-time that a user spends on the search result page and on the clicked result web pages. We also learn a weight for each kind of feature to optimize the ranking performance. In our experiment, we discover 10.5 thousand searchable sites and use 5 million unique queries, extracted from one week of log data to build and demonstrate the effectiveness of our searchable web site recommendation system.

#index 1536546
#* Inferring search behaviors using partially observable markov model with duration (POMD)
#@ Yin He;Kuansan Wang
#t 2011
#c 2
#% 394014
#% 577224
#% 731615
#% 737637
#% 752177
#% 805200
#% 823348
#% 879567
#% 946521
#% 954948
#% 954949
#% 1035578
#% 1053505
#% 1074092
#% 1093793
#% 1190055
#% 1190056
#% 1214754
#% 1214757
#% 1355037
#! This paper presents Partially Observable Markov model with Duration (POMD), a statistical method that addresses the challenge of understanding sophisticated user behaviors from the search log in which some user actions, such as reading and skipping search results, cannot be observed and recorded. POMD utilizes not only the positional but also the temporal information of the clicks in the log. In this work, we treat the user engagements with a search engine as a Markov process, and model the unobservable engagements as hidden states. POMD differs from the traditional hidden Markov model (HMM) in that not all the hidden state transitions emit observable events, and that the duration of staying in each state is explicitly factored into the core statistical model. To address the training and decoding issues emerged as the results of the variations, we propose an iterative two-stage training algorithm and a greedy segmental decoding algorithm respectively. We validate the proposed algorithm with two sets of experiments. First, we show that the search behavioral patterns inferred by POMD match well with those reported in the eye tracking experiments. Secondly, through a series of A/B comparison experiments, we demonstrate that POMD can distinguish the ranking qualities of different search engine configurations much better than the patterns inferred by the model proposed in the previous work. Both of the experimental results suggest that POMD can provide a statistical and quantitative way to understand the sophisticated search behaviors by simply mining the search logs.

#index 1536547
#* Learning website hierarchies for keyword enrichment in contextual advertising
#@ Pavan Kumar GM;Krishna P. Leela;Mehul Parsana;Sachin Garg
#t 2011
#c 2
#% 219047
#% 255137
#% 411762
#% 449588
#% 818265
#% 869484
#% 879600
#% 963669
#% 987262
#% 1019092
#% 1040857
#% 1083644
#% 1127356
#% 1190059
#% 1190099
#% 1355054
#! In Contextual advertising, textual ads relevant to the content in a webpage are embedded in the page. Content keywords are extracted offline by crawling webpages and then stored in an index for fast serving. Given a page, ad selection involves index lookup, computing similarity between the keywords of the page and those of candidate ads and returning the top-k scoring ads. In this approach, ad relevance can suffer in two scenarios. First, since page-ad similarity is computed using keywords extracted only from that particular page, a few non pertinent keywords can skew ad selection. Second, requesting page may not be present in the index but we still need to serve relevant ads. We propose a novel mechanism to mitigate these problems in the same framework. The basic idea is to enrich keywords of a particular page with keywords from other but "similar" pages. The scheme involves learning a website specific hierarchy from (page, URL) pairs of the website. Next, keywords are populated on the nodes via successive top-down and bottom-up iterations over the hierarchy. We evaluate our approach on three data sets, one small human labeled set and two large-scale sets from Yahoo's contextual advertising system. Empirical evaluation show that ads fetched by enriching keywords has 2-3% higher nDCG compared to ads fetched based on a recent semantic approach even though the index size of our approach is 7 times less than the index size of semantic approach. Evaluation over pages which are not present in the index shows that ads fetched by our method has 6-7% higher nDCG compared to ads fetched based on a recent approach which uses first N bytes of the page content. Scalability is demonstrated via map-reduce adoption of our method and training on a large data set of 220 million pages from 95,104 websites.

#index 1536548
#* Action prediction and identification from mining temporal user behaviors
#@ Dakan Wang;Gang Wang;Xiaofeng Ke;Weizhu Chen
#t 2011
#c 2
#% 757953
#% 805200
#% 818207
#% 818221
#% 818259
#% 838547
#% 869536
#% 879565
#% 879567
#% 881570
#% 955711
#% 956495
#% 1004294
#% 1019076
#% 1043044
#% 1055676
#% 1055694
#% 1166518
#% 1190081
#% 1214728
#% 1214754
#% 1227622
#! Predicting user's action provides many monetization opportunities to web service providers. If a user's future action can be predicted and identified correctly in time or in advance, we cannot only satisfy user's current need, but also facilitate and simplify user's future online activities. Traditional works on user behavior modeling such as implicit feedback or personalization mainly investigate on users' immediate, short-term or aggregate behaviors. Hence, it is difficult to understand the diversity in temporal user behavior and predict user's future action. In this paper, we consider a forecasting problem of temporal user behavior modeling. Our first objective is able to capture relevant users that will perform an action. The second objective is able to identify whether a user has finished the action, even when the action happened offline. We propose an ensemble algorithm to achieve both objectives. The experiment compares several implementation methods and demonstrates the temporal user behavior modeling using the ensemble algorithm significantly outperforms other methods.

#index 1536549
#* Collective extraction from heterogeneous web lists
#@ Ashwin Machanavajjhala;Arun Shankar Iyer;Philip Bohannon;Srujana Merugu
#t 2011
#c 2
#% 314740
#% 333943
#% 480824
#% 654469
#% 769877
#% 788090
#% 805846
#% 864416
#% 881505
#% 889107
#% 1022235
#% 1022260
#% 1023488
#% 1328133
#% 1328199
#% 1523846
#! Automatic extraction of structured records from inconsistently formatted lists on the web is challenging: different lists present disparate sets of attributes with variations in the ordering of attributes; many lists contain additional attributes and noise that can confuse the extraction process; and formatting within a list may be inconsistent due to missing attributes or manual formatting on some sites. We present a novel solution to this extraction problem that is based on i) collective extraction from multiple lists simultaneously and ii) careful exploitation of a small database of seed entities. Our approach addresses the layout homogeneity within the individual lists, content redundancy across some snippets from different sources, and the noisy attribute rendering process. We experimentally evaluate variants of this algorithm on real world data sets and show that our approach is a promising direction for extraction from noisy lists, requiring mild and thus inexpensive supervision suitable for extraction from the tail of the web.

#index 1536550
#* CMAP: effective fusion of quality and relevance for multi-criteria recommendation
#@ Xin Xin;Michael R. Lyu;Irwin King
#t 2011
#c 2
#% 266281
#% 280852
#% 314933
#% 330687
#% 414514
#% 452563
#% 528156
#% 734590
#% 734592
#% 734594
#% 803036
#% 813966
#% 818216
#% 879627
#% 963350
#% 987197
#% 987198
#% 1074061
#% 1074107
#% 1083671
#% 1181094
#% 1227602
#% 1227603
#% 1227644
#% 1292592
#% 1650569
#! The research issue of recommender systems has been treated as a classical regression problem over the decades and has obtained a great success. In the next generation of recommender systems, multi-criteria recommendation has been predicted as an important direction. Different from traditional recommender systems that aim particularly at recommending high-quality items evaluated by users' ratings, inmulti-criteria recommendation, quality only serves as one criterion, and many other criteria such as relevance, coverage, and diversity should be simultaneously optimized. Although recently there is work investigating each single criterion, there is rarely any literature that reports how each single criterion impacts each other and how to combine them in real applications. Thus in this paper, we study the relationship of two criteria, quality and relevance, as a preliminary work in multi-criteria recommendation. We first give qualitative and quantitative analysis of competitive quality-based and relevance-based algorithms in these two criteria to show that both algorithms cannot work well in the opposite criteria. Then we propose an integrated metric and finally investigate how to combine previous work together into an unified model. In the combination, we introduce a Continuous-time MArkov Process (CMAP) algorithm for ranking, which enables principled and natural integration with features derived from both quality-based and relevance-based algorithms. Through experimental verification, the combined methods can significantly outperform either single quality-based or relevance-based algorithms in the integrated metric and the CMAP model outperforms traditional combination methods by around 3%. Its linear complexity with respect to the number of users and items leads to satisfactory performance, as demonstrated by the around 7-hour computational time for over 480k users and almost 20k items.

#index 1536551
#* CoBayes: bayesian knowledge corroboration with assessors of unknown areas of expertise
#@ Gjergji Kasneci;Jurgen Van Gael;David Stern;Thore Graepel
#t 2011
#c 2
#% 44876
#% 271760
#% 272514
#% 731615
#% 829016
#% 956564
#% 1000502
#% 1190066
#% 1200291
#% 1355029
#% 1374370
#% 1409954
#% 1472273
#% 1495583
#% 1668093
#! Our work aims at building probabilistic tools for constructing and maintaining large-scale knowledge bases containing entity-relationship-entity triples (statements) extracted from the Web. In order to mitigate the uncertainty inherent in information extraction and integration we propose leveraging the "wisdom of the crowds" by aggregating truth assessments that users provide about statements. The suggested method, CoBayes, operates on a collection of statements, a set of deduction rules (e.g. transitivity), a set of users, and a set of truth assessments of users about statements. We propose a joint probabilistic model of the truth values of statements and the expertise of users for assessing statements. The truth values of statements are interconnected through derivations based on the deduction rules. The correctness of a user's assessment for a given statement is modeled by linear mappings from user descriptions and statement descriptions into a common latent knowledge space where the inner product between user and statement vectors determines the probability that the user assessment for that statement will be correct. Bayesian inference in this complex graphical model is performed using mixed variational and expectation propagation message passing. We demonstrate the viability of CoBayes in comparison to other approaches, on realworld datasets and user feedback collected from Amazon Mechanical Turk.

#index 1536552
#* Multi-dimensional search result diversification
#@ Zhicheng Dou;Sha Hu;Kun Chen;Ruihua Song;Ji-Rong Wen
#t 2011
#c 2
#% 262112
#% 296646
#% 306468
#% 340951
#% 642975
#% 766433
#% 805841
#% 818266
#% 853543
#% 872689
#% 879686
#% 905224
#% 956552
#% 960308
#% 987203
#% 1074025
#% 1074133
#% 1127472
#% 1166473
#% 1173701
#% 1206662
#% 1214650
#% 1263586
#% 1348342
#% 1400011
#% 1400021
#% 1697422
#! Most existing search result diversification algorithms diversify search results in terms of a specific dimension. In this paper, we argue that search results should be diversified in a multi-dimensional way, as queries are usually ambiguous at different levels and dimensions. We first explore mining subtopics from four types of data sources, including anchor texts, query logs, search result clusters, and web sites. Then we propose a general framework that explicitly diversifies search results based on multiple dimensions of subtopics. It balances the relevance of documents with respect to the query and the novelty of documents by measuring the coverage of subtopics. Experimental results on the TREC 2009 Web track dataset indicate that combining multiple types of subtopics do help better understand user intents. By incorporating multiple types of subtopics, our models improve the diversity of search results over the sole use of one of them, and outperform two state-of-the-art models.

#index 1536553
#* Improving social bookmark search using personalised latent variable language models
#@ Morgan Harvey;Ian Ruthven;Mark J. Carman
#t 2011
#c 2
#% 329569
#% 722904
#% 855601
#% 879587
#% 944349
#% 956552
#% 1055704
#% 1074071
#% 1083671
#% 1166510
#% 1280261
#% 1287227
#% 1667787
#% 1697448
#% 1697449
#! Social tagging systems have recently become very popular as a method of categorising information online and have been used to annotate a wide range of different resources. In such systems users are free to choose whatever keywords or "tags" they wish to annotate each resource, resulting in a highly personalised, unrestricted vocabulary. While this freedom of choice has several notable advantages, it does come at the cost of making searching of these systems more difficult as the vocabulary problem introduced is more pronounced than in a normal information retrieval setting. In this paper we propose to use hidden topic models as a principled way of reducing the dimensionality of this data to provide more accurate resource rankings with higher recall. We first describe Latent Dirichlet Allocation (LDA), a simple topic model and then introduce 2 extended models which can be used to personalise the results by including information about the user who made each annotation. We test these 3 models and compare them with 3 non-topic model baselines on a large data sample obtained from the Delicious social bookmarking site. Our evaluations show that our methods significantly outperform all of the baselines with the personalised models also improving significantly upon unpersonalised LDA.

#index 1536554
#* Strength of social influence in trust networks in product review sites
#@ Ching-man Au Yeung;Tomoharu Iwata
#t 2011
#c 2
#% 577217
#% 730089
#% 842605
#% 868480
#% 1001279
#% 1083641
#% 1083671
#% 1130901
#% 1190130
#% 1214661
#% 1214666
#% 1287243
#% 1355043
#% 1399997
#% 1400002
#% 1400031
#! Some popular product review sites such as Epinions allow users to establish a trust network among themselves, indicating who they trust in providing product reviews and ratings. While trust relations have been found to be useful in generating personalised recommendations, the relations between trust and product ratings has so far been overlooked. In this paper, we examine large datasets collected from Epinions and Ciao, two popular product review sites. We discover that in general users who trust each other tend to have smaller differences in their ratings as time passes, giving support to the theories of homophily and social influence. However, we also discover that this does not hold true across all trusted users. A trust relation does not guarantee that two users have similar preferences, implying that personalised recommendations based on trust relations do not necessarily produce more accurate predictions. We propose a method to estimate the strengths of trust relations so as to estimate the true influence among the trusted users. Our method extends the popular matrix factorisation technique for collaborative filtering, which allow us to generate more accurate rating predictions at the same time. We also show that the estimated strengths of trust relations correlate with the similarity among the users. Our work contributes to the understanding of the interplay between trust relations and product ratings, and suggests that trust networks may serve as a more general socialising venue than only an indication of similarity in user preferences.

#index 1536555
#* A combined topical/non-topical approach to identifying web sites for children
#@ Carsten Eickhoff;Pavel Serdyukov;Arjen P. de Vries
#t 2011
#c 2
#% 251571
#% 308363
#% 344923
#% 805873
#% 829975
#% 869550
#% 939396
#% 987245
#% 1014680
#% 1131829
#% 1194330
#% 1227578
#% 1260690
#% 1301004
#% 1709423
#! Today children interact more and more frequently with information services. Especially in on-line scenarios there is a great amount of content that is not suitable for their age group. Due to the growing importance and ubiquity of the Internet in today's world, denying children any unsupervised Web access is often not possible. This work presents an automatic way of distinguishing web pages for children from those for adults in order to improve child-appropriate web search engine performance. A range of 80 different features based on findings from cognitive sciences and children's psychology are discussed and evaluated. We conducted a large scale user study on the suitability of web sites and give detailed information about the insights gained. Finally a comparison to traditional web classification methods as well as human annotator performance reveals that our automatic classifier can reach a performance close to that of human agreement.

#index 1536556
#* Bid generation for advanced match in sponsored search
#@ Andrei Broder;Evgeniy Gabrilovich;Vanja Josifovski;George Mavromatis;Alex Smola
#t 2011
#c 2
#% 642985
#% 750863
#% 869501
#% 1055677
#% 1074101
#% 1127383
#% 1130910
#% 1190078
#% 1355036
#% 1355052
#! Sponsored search is a three-way interaction between advertisers, users, and the search engine. The basic ad selection in sponsored search, lets the advertiser choose the exact queries where the ad is to be shown. To increase advertising volume, many advertisers opt into advanced match, where the search engine can select additional queries that are deemed relevant for the advertiser's ad. In advanced match, the search engine is effectively bidding on the behalf of the advertisers. While advanced match has been extensively studied in the literature from the ad relevance perspective there is little work that discusses how to infer the appropriate bid value for a given advanced match. The bid value is crucial as it affects both the ad placement in revenue reordering and the amount advertisers are charged in case of a click. We propose a statistical approach to solve the bid generation problem and examine two information sources: the bidding behavior of advertisers, and the conversion data. Our key finding suggests that sophisticated advertisers' bids are driven by many factors beyond clicks and immediate measurable conversions, likely capturing the value chain of an ad display ranging from views, clicks, profit margins, etc., representing the total ROI from the advertising.

#index 1536557
#* Let web spammers expose themselves
#@ Zhicong Cheng;Bin Gao;Congkai Sun;Yanbing Jiang;Tie-Yan Liu
#t 2011
#c 2
#% 99690
#% 387427
#% 577273
#% 772018
#% 807297
#% 818223
#% 840965
#% 869471
#% 1016177
#% 1194311
#% 1194312
#% 1214746
#% 1450883
#! This paper is concerned with mining link spams (e.g., link farm and link exchange) from search engine optimization (SEO) forums. To provide quality services, it is critical for search engines to address web spam. Several techniques such as TrustRank, BadRank, and SpamRank have been proposed for this purpose. Most of these methods try to downgrade the effects of the spam websites by identifying specific link patterns of them. However, spam websites have appeared to be more and more similar to normal or even good websites in their link structures, by reforming their spam techniques. As a result, it is very challenging to automatically detect link spams from the Web graph. In this paper, we propose a different approach, which detects link spams by looking at how web spammers make link spam happen. We find that web spammers usually ally with each other, and SEO forum is one of the major means for them to form the alliance. We therefore propose mining suspicious link spams directly from the posts in the SEO forums. However, the task is non-trivial because there are also other information and even noises contained in these posts, in addition to useful clues of link spam. To tackle the challenges, we first extract all the URLs contained in the posts of the SEO forums. Second, we extract features for the URLs from their relationships with forum users (potential spammers) and from their link structure in the web graph. Third, we build a semi-supervised learning framework to calculate the spam scores for the URLs, which encodes several heuristics such as spam websites usually linking to each other, and good websites seldom linking to spam websites. We tested our approach on seven major SEO forums. A lot of spam websites were identified, a significant proportion of which cannot be detected by conventional anti-spam methods. It indicates that the proposed approach can be a good complement of existing anti-spam techniques.

#index 1536558
#* Efficient entity resolution for large heterogeneous information spaces
#@ George Papadakis;Ekaterini Ioannou;Claudia Niederée;Peter Fankhauser
#t 2011
#c 2
#% 201889
#% 310516
#% 328186
#% 480654
#% 577247
#% 587758
#% 810014
#% 830529
#% 844199
#% 853532
#% 871766
#% 874876
#% 875066
#% 913783
#% 915242
#% 1103296
#% 1217163
#% 1250576
#% 1292496
#! We have recently witnessed an enormous growth in the volume of structured and semi-structured data sets available on the Web. An important prerequisite for using and combining such data sets is the detection and merge of information that describes the same real-world entities, a task known as Entity Resolution. To make this quadratic task efficient, blocking techniques are typically employed. However, the high dynamics, loose schema binding, and heterogeneity of (semi-)structured data, impose new challenges to entity resolution. Existing blocking approaches become inapplicable because they rely on the homogeneity of the considered data and a-priory known schemata. In this paper, we introduce a novel approach for entity resolution, scaling it up for large, noisy, and heterogeneous information spaces. It combines an attribute-agnostic mechanism for building blocks with intelligent block processing techniques that boost blocks with high expected utility, propagate knowledge about identified matches, and preempt the resolution process when it gets too expensive. Our extensive evaluation on real-world, large, heterogeneous data sets verifies that the suggested approach is both effective and efficient.

#index 1536559
#* Web-scale table census and classification
#@ Eric Crestan;Patrick Pantel
#t 2011
#c 2
#% 348147
#% 658628
#% 755816
#% 956500
#% 1083721
#% 1190181
#% 1250418
#% 1279460
#% 1328199
#! We report on a census of the types of HTML tables on the Web according to a fine-grained classification taxonomy describing the semantics that they express. For each relational table type, we describe open challenges for extracting from them semantic triples, i.e., knowledge. We also present TabEx, a supervised framework for web-scale HTML table classification and apply it to the task of classifying HTML tables into our taxonomy. We show empirical evidence, through a large-scale experimental analysis over a crawl of the Web, that classification accuracy significantly outperforms several baselines. We present a detailed feature analysis and outline the most salient features for each table type.

#index 1536560
#* A probabilistic approach for learning folksonomies from structured data
#@ Anon Plangprasopchok;Kristina Lerman;Lise Getoor
#t 2011
#c 2
#% 197387
#% 310516
#% 850430
#% 855601
#% 891559
#% 906335
#% 924747
#% 937552
#% 946524
#% 960271
#% 1190133
#% 1249958
#% 1250567
#% 1270263
#% 1451235
#% 1810385
#! Learning structured representations has emerged as an important problem in many domains, including document and Web data mining, bioinformatics, and image analysis. One approach to learning complex structures is to integrate many smaller, incomplete and noisy structure fragments. In this work, we present an unsupervised probabilistic approach that extends affinity propagation [7] to combine the small ontological fragments into a collection of integrated, consistent, and larger folksonomies. This is a challenging task because the method must aggregate similar structures while avoiding structural inconsistencies and handling noise. We validate the approach on a real-world social media dataset, comprised of shallow personal hierarchies specified by many individual users, collected from the photosharing website Flickr. Our empirical results show that our proposed approach is able to construct deeper and denser structures, compared to an approach using only the standard affinity propagation algorithm. Additionally, the approach yields better overall integration quality than a state-of-the-art approach based on incremental relational clustering.

#index 1536561
#* Linking online news and social media
#@ Manos Tsagkias;Maarten de Rijke;Wouter Weerkamp
#t 2011
#c 2
#% 27049
#% 168969
#% 184496
#% 290830
#% 342710
#% 350859
#% 659265
#% 754107
#% 784148
#% 786841
#% 789959
#% 818239
#% 838508
#% 839912
#% 840066
#% 916680
#% 1019082
#% 1040837
#% 1130858
#% 1214671
#% 1224348
#% 1260628
#% 1287290
#% 1292698
#% 1330556
#% 1355045
#% 1355046
#% 1399992
#% 1537471
#% 1742093
#! Much of what is discussed in social media is inspired by events in the news and, vice versa, social media provide us with a handle on the impact of news events. We address the following linking task: given a news article, find social media utterances that implicitly reference it. We follow a three-step approach: we derive multiple query models from a given source news article, which are then used to retrieve utterances from a target social media index, resulting in multiple ranked lists that we then merge using data fusion techniques. Query models are created by exploiting the structure of the source article and by using explicitly linked social media utterances that discuss the source article. To combat query drift resulting from the large volume of text, either in the source news article itself or in social media utterances explicitly linked to it, we introduce a graph-based method for selecting discriminative terms. For our experimental evaluation, we use data from Twitter, Digg, Delicious, the New York Times Community, Wikipedia, and the blogosphere to generate query models. We show that different query models, based on different data sources, provide complementary information and manage to retrieve different social media utterances from our target index. As a consequence, data fusion methods manage to significantly boost retrieval performance over individual approaches. Our graph-based term selection method is shown to help improve both effectiveness and efficiency.

#index 1536562
#* Document assignment in multi-site search engines
#@ Ulf Brefeld;B. Barla Cambazoglu;Flavio P. Junqueira
#t 2011
#c 2
#% 194246
#% 197394
#% 232653
#% 280817
#% 280832
#% 309139
#% 316508
#% 318412
#% 344447
#% 344448
#% 420084
#% 430761
#% 458379
#% 578337
#% 643012
#% 643013
#% 818171
#% 881477
#% 983905
#% 1074360
#% 1132154
#% 1195881
#% 1227597
#% 1227628
#% 1280763
#% 1292508
#% 1373774
#% 1399951
#% 1450840
#% 1484251
#! Assigning documents accurately to sites is critical for the performance of multi-site Web search engines. In such settings, sites crawl only documents they index and forward queries to obtain best-matching documents from other sites. Inaccurate assignments may lead to inefficiencies when crawling Web pages or processing user queries. In this work, we propose a machine-learned document assignment strategy that uses the locality of document views in search results to decide upon assignments. We evaluate the performance of our strategy using various document features extracted from a large Web collection. Our experimental setup uses query logs from a number of search front-ends spread across different geographic locations and uses these logs to learn the document access patterns. We compare our technique against baselines such as region- and language-based document assignment and observe that our technique achieves substantial performance improvements with respect to recall. With our technique, we are able to obtain a small query forwarding rate (0.04) requiring roughly 45% less replication of documents compared to replicating all documents across all sites.

#index 1536563
#* Transient crowd discovery on the real-time social web
#@ Krishna Yeshwanth Kamath;James Caverlee
#t 2011
#c 2
#% 8919
#% 823395
#% 881460
#% 972357
#% 989640
#% 989663
#% 1085750
#% 1482448
#! In this paper, we study the problem of automatically discovering and tracking transient crowds in highly-dynamic social messaging systems like Twitter and Facebook. Unlike the more static and long-lived group-based membership offered on many social networks (e.g., fan of the LA Lakers), a transient crowd is a short-lived ad-hoc collection of users, representing a "hotspot" on the real-time web. Successful detection of these hotspots can positively impact related research directions in online event detection, content personalization, social information discovery, etc. Concretely, we propose to model crowd formation and dispersion through a message-based communication clustering approach over time-evolving graphs that captures the natural conversational nature of social messaging systems. Two of the salient features of the proposed approach are (i) an efficient locality- based clustering approach for identifying crowds of users in near real-time compared to more heavyweight static clustering algorithms; and (ii) a novel crowd tracking and evolution approach for linking crowds across time periods. We find that the locality-based clustering approach results in empirically high-quality clusters relative to static graph clus- tering techniques at a fraction of the computational cost. Based on a three month snapshot of Twitter consisting of 711,612 users and 61.3 million messages, we show how the proposed approach can successfully identify and track interesting crowds based on the Twitter communication structure and uncover crowd-based topics of interest.

#index 1536564
#* Adaptive bootstrapping of recommender systems using decision trees
#@ Nadav Golbandi;Yehuda Koren;Ronny Lempel
#t 2011
#c 2
#% 124010
#% 330687
#% 342767
#% 400847
#% 448194
#% 452563
#% 734590
#% 1200881
#% 1214623
#% 1260273
#% 1291600
#% 1337758
#% 1482440
#! Recommender systems perform much better on users for which they have more information. This gives rise to a problem of satisfying users new to a system. The problem is even more acute considering that some of these hard to profile new users judge the unfamiliar system by its ability to immediately provide them with satisfying recommendations, and may quickly abandon the system when disappointed. Rapid profiling of new users by a recommender system is often achieved through a bootstrapping process - a kind of an initial interview - that elicits users to provide their opinions on certain carefully chosen items or categories. The elicitation process becomes particularly effective when adapted to users' responses, making best use of users' time by dynamically modifying the questions to improve the evolving profile. In particular, we advocate a specialized version of decision trees as the most appropriate tool for this task. We detail an efficient tree learning algorithm, specifically tailored to the unique properties of the problem. Several extensions to the tree construction are also introduced, which enhance the efficiency and utility of the method. We implemented our methods within a movie recommendation service. The experimental study delivered encouraging results, with the tree-based bootstrapping process significantly outperforming previous approaches.

#index 1536565
#* Predicting future reviews: sentiment analysis models for collaborative filtering
#@ Noriaki Kawamae
#t 2011
#c 2
#% 173879
#% 220711
#% 330687
#% 769892
#% 769906
#% 823393
#% 907489
#% 939896
#% 956510
#% 961283
#% 989621
#% 1055682
#% 1176909
#% 1289476
#% 1450856
#% 1482243
#% 1536536
#% 1650569
#! This paper presents hierarchical topic models for integrating sentiment analysis with collaborative filtering. Our goal is to automatically predict future reviews to a given author from previous reviews. For this goal, we focus on differentiating author's preference, while previous sentiment analysis models process these review articles without this difference. Therefore, we propose a Latent Evaluation Topic model (LET) that infer each author's preference by introducing novel latent variables into author and his/her document layer. Because these variables distinguish the variety of words in each article by merging similar word distributions, LET incorporates the difference of writers' preferences into sentiment analysis. Consequently LET can determine the attitude of writers, and predict their reviews based on like-minded writers' reviews in the collaborative filtering approach. Experiments on review articles show that the proposed model can reduce the dimensionality of reviews to the low-dimensional set of these latent variables, and is a significant improvement over standard sentiment analysis models and collaborative filtering algorithms.

#index 1536566
#* Learning similarity function for rare queries
#@ Jingfang Xu;Gu Xu
#t 2011
#c 2
#% 169729
#% 218978
#% 249321
#% 310567
#% 330617
#% 347225
#% 722803
#% 762054
#% 869500
#% 869501
#% 869651
#% 876008
#% 939629
#% 983830
#% 987221
#% 987222
#% 1074017
#% 1130854
#% 1130855
#% 1130879
#% 1190106
#% 1211795
#% 1292484
#% 1338611
#% 1392432
#% 1400023
#! The key element of many query processing tasks can be formalized as calculation of similarities between queries. These include query suggestion, query reformulation, and query expansion. Although many methods have been proposed for query similarity calculation, they could perform poorly on rare queries. As far as we know, there was no previous work particularly about rare query similarity calculation, and this paper tries to study this problem. Specifically, we address three problems. Firstly, we define an n-gram space to represent queries with their own content and a similarity function to measure the similarities between queries. Secondly, we propose learning the similarity function by leveraging the training data derived from user behavior data. This is formalized as an optimization problem and a metric learning approach is employed to solve it efficiently. Finally, we exploit locality sensitive hashing for efficient retrieval of similar queries from a large query repository. We experimentally verified the effectiveness of the proposed approach by showing that our method can indeed enhance the accuracy of query similarity calculation for rare queries and efficiently retrieve similar queries. As an application, we also experimentally demonstrated that the similar queries found by our method can significantly improve search relevance.

#index 1536567
#* A two-view learning approach for image tag ranking
#@ Jinfeng Zhuang;Steven C.H. Hoi
#t 2011
#c 2
#% 387427
#% 442110
#% 576218
#% 642989
#% 724232
#% 755463
#% 757953
#% 760805
#% 839975
#% 881054
#% 905209
#% 905280
#% 975019
#% 1055704
#% 1074095
#% 1131836
#% 1131843
#% 1132486
#% 1132491
#% 1190090
#% 1190091
#% 1279781
#% 1338049
#% 1814726
#! Tags of social images play a central role for text-based social image retrieval and browsing tasks. However, the original tags annotated by web users could be noisy, irrelevant, and often incomplete for describing the image contents, which may severely deteriorate the performance of text-based image retrieval models. In this paper, we aim to overcome the challenge of social tag ranking for a corpus of social images with rich user-generated tags by proposing a novel two-view learning approach. It can effectively exploit both textual and visual contents of social images to discover the complicated relationship between tags and images. Unlike the conventional learning approaches that usually assume some parametric models, our method is completely data-driven and makes no assumption of the underlying models, making the proposed solution practically more effective. We formally formulate our method as an optimization task and present an efficient algorithm to solve it. To evaluate the efficacy of our method, we conducted an extensive set of experiments by applying our technique to both text-based social image retrieval and automatic image annotation tasks, in which encouraging results showed that the proposed method is more effective than the conventional approaches.

#index 1536568
#* Supervised random walks: predicting and recommending links in social networks
#@ Lars Backstrom;Jure Leskovec
#t 2011
#c 2
#% 73441
#% 348173
#% 577329
#% 593994
#% 730089
#% 748017
#% 823342
#% 881457
#% 881460
#% 881496
#% 915344
#% 983805
#% 989646
#% 1040831
#% 1083675
#% 1190058
#% 1192430
#% 1289460
#! Predicting the occurrence of links is a fundamental problem in networks. In the link prediction problem we are given a snapshot of a network and would like to infer which interactions among existing members are likely to occur in the near future or which existing interactions are we missing. Although this problem has been extensively studied, the challenge of how to effectively combine the information from the network structure with rich node and edge attribute data remains largely open. We develop an algorithm based on Supervised Random Walks that naturally combines the information from the network structure with node and edge level attributes. We achieve this by using these attributes to guide a random walk on the graph. We formulate a supervised learning task where the goal is to learn a function that assigns strengths to edges in the network such that a random walker is more likely to visit the nodes to which new links will be created in the future. We develop an efficient training algorithm to directly learn the edge strength estimation function. Our experiments on the Facebook social graph and large collaboration networks show that our approach outperforms state-of-the-art unsupervised approaches as well as approaches that are based on feature extraction.

#index 1536569
#* OOLAM: an opinion oriented link analysis model for influence persona discovery
#@ Keke Cai;Shenghua Bao;Zi Yang;Jie Tang;Rui Ma;Li Zhang;Zhong Su
#t 2011
#c 2
#% 290830
#% 342596
#% 348527
#% 402289
#% 729923
#% 754098
#% 794513
#% 805897
#% 912461
#% 941323
#% 955043
#% 989613
#% 1023420
#% 1035589
#% 1047390
#% 1127964
#% 1132846
#% 1190129
#% 1214641
#% 1214702
#% 1269378
#% 1384246
#% 1399997
#! Social influence is a complex and subtle force that governs the dynamics of social networks. In the past years, a lot of research work has been conducted to understand the spread patterns of social influence. However, most of approaches assume that influence exists between users with active social interactions, but ignore the question of what kind of influence happens between them. As such one interesting and also fundamental question is raised here: "in a social network, could the social connection reflect users'influence from both positive and negative aspects?". To this end, an Opinion Oriented Link Analysis Model (OOLAM) is proposed in this paper to characterize users' influence personae in order to exhibit their distinguishing influence ability in the social network. In particular, three types of influence personae are generalized and the problem of influence persona discovery is formally defined. Within the OOLAM model, two factors, i.e., opinion consistency and opinion creditability, are defined to capture the persona information from public opinion perspective. Extensive experimental studies have been performed to demonstrate the effectiveness of the proposed approach on influence persona analysis using real web data sets.

#index 1536570
#* eBay: an E-commerce marketplace as a complex network
#@ Zeqian Shen;Neel Sundaresan
#t 2011
#c 2
#% 309749
#% 386203
#% 832742
#% 906507
#% 907288
#% 948893
#% 956513
#% 956516
#% 1300556
#! Commerce networks involve buying and selling activities among individuals or organizations. As the growing of the Internet and e-commerce, it brings opportunities for obtaining real world online commerce networks, which are magnitude larger than before. Getting a deeper understanding of e-commerce networks, such as the eBay marketplace, in terms of what structure they have, what kind of interactions they afford, what trust and reputation measures exist, and how they evolve has tremendous value in suggesting business opportunities and building effective user applications. In this paper, we modeled the eBay network as a complex network. We analyzed the macroscopic shape of the network using degree distribution and the bow-tie model. Networks of different eBay categories are also compared. The results suggest that the categories vary from collector networks to retail networks. We also studied the local structures of the networks using motif profiling. Finally, patterns of preferential connections are visually analyzed using Auroral diagrams.

#index 1536571
#* A framework for quantitative analysis of cascades on networks
#@ Rumi Ghosh;Kristina Lerman
#t 2011
#c 2
#% 342596
#% 729923
#% 754107
#% 1080075
#% 1214658
#% 1214671
#% 1222654
#% 1300556
#% 1446954
#% 1669913
#! How does information flow in online social networks? How does the structure and size of the information cascade evolve in time? How can we efficiently mine the information contained in cascade dynamics? We approach these questions empirically and present an efficient and scalable mathematical framework for quantitative analysis of cascades on networks. We define a cascade generating function that captures the details of the microscopic dynamics of the cascades. We show that this function can also be used to compute the macroscopic properties of cascades, such as their size, spread, diameter, number of paths, and average path length. We present an algorithm to efficiently compute cascade generating function and demonstrate that while significantly compressing information within a cascade, it nevertheless allows us to accurately reconstruct its structure. We use this framework to study information dynamics on the social network of Digg. Digg allows users to post and vote on stories, and easily see the stories that friends have voted on. As a story spreads on Digg through voting, it generates cascades. We extract cascades of more than 3,500 Digg stories and calculate their macroscopic and microscopic properties. We identify several trends in cascade dynamics: spreading via chaining, branching and community. We discuss how these affect the spread of the story through the Digg social network. Our computational framework is general and offers a practical solution to quantitative analysis of the microscopic structure of even very large cascades.

#index 1536572
#* Scalable clustering of news search results
#@ Srinivas Vadrevu;Choon Hui Teo;Suju Rajan;Kunal Punera;Byron Dom;Alexander J. Smola;Yi Chang;Zhaohui Zheng
#t 2011
#c 2
#% 211044
#% 218992
#% 249321
#% 262045
#% 329562
#% 460812
#% 754124
#% 765464
#% 766433
#% 783474
#% 836155
#% 1078626
#% 1650362
#% 1742083
#! In this paper, we present a system for clustering the search results of a news search engine. The news search interface includes the relevant news articles to a given query organized in terms of related news stories. Here each cluster corresponds to a news story and the news articles are clustered into stories. We present a system that clusters the search results of a news search system in a fast and scalable manner. The clustering system is organized into three components including offline clustering, incremental clustering and realtime clustering. We propose novel techniques for clustering the search results in realtime. The experimental results with large collections of news documents reveal that our system is both scalable and also achieves good accuracy in clustering the news search results.

#index 1536573
#* Large-scale hierarchical text classification without labelled data
#@ Viet Ha-Thuc;Jean-Michel Renders
#t 2011
#c 2
#% 309141
#% 342669
#% 457933
#% 465747
#% 466501
#% 722904
#% 799753
#% 840951
#% 871759
#% 879626
#% 911852
#% 938685
#% 983883
#% 1026801
#% 1164311
#% 1249642
#% 1318756
#% 1415739
#% 1516577
#% 1650298
#! The traditional machine learning approaches for text classification often require labelled data for learning classifiers. However, when applied to large-scale classification involving thousands of categories, creating such labelled data is extremely expensive since typically the data is manually labelled by humans. Motivated by this, we propose a novel approach for large-scale hierarchical text classification which does not require any labelled data. We explore a perspective where the meaning of a category is not defined by human-labelled documents, but by its description and more importantly its relationships with other categories (e.g. its ascendants and descendants). Specifically, we take advantage of the ontological knowledge in all phases of the whole process, namely when retrieving pseudo-labelled documents, when iteratively training the category models and when categorizing test documents. Our experiments based on a taxonomy containing 1131 categories and widely adopted in the news industry as a standard for the NewsML framework demonstrate the effectiveness of our approach in these phases both qualitatively and quantitatively. In particular, we emphasize that just by taking the simple ontological knowledge defined in the category hierarchy, we could automatically build a large-scale hierarchical classifier with reasonable performance of 67% in terms of the hierarchy-based F-1 measure.

#index 1536574
#* Low-order tensor decompositions for social tagging recommendation
#@ Yuanzhe Cai;Miao Zhang;Dijun Luo;Chris Ding;Sharma Chakravarthy
#t 2011
#c 2
#% 290830
#% 330687
#% 813966
#% 1052902
#% 1127455
#% 1156304
#% 1214694
#% 1300087
#% 1327635
#% 1355024
#% 1355043
#% 1667787

#index 1536575
#* Shopping for products you don't know you need
#@ Srikanth Jagabathula;Nina Mishra;Sreenivas Gollapudi
#t 2011
#c 2
#% 39702
#% 173879
#% 222437
#% 262902
#% 330687
#% 338442
#% 452563
#% 481290
#% 765311
#% 869500
#% 946521
#% 987222
#% 989580
#% 1055677
#% 1083721
#% 1089474
#% 1130854
#% 1130868
#% 1190066
#% 1190072
#% 1190162
#% 1214684
#% 1214736
#% 1214738
#% 1222633
#% 1273828
#% 1400017
#! Recommendation engines today suggest one product to another, e.g., an accessory to a product. However, intent to buy often precedes a user's appearance in a commerce vertical: someone interested in buying a skateboard may have earlier searched for {varial heelflip}, a trick performed on a skateboard. This paper considers how a search engine can provide early warning of commercial intent. The naive algorithm of counting how often an interest precedes a commercial query is not sufficient due to the number of related ways of expressing an interest. Thus, methods are needed for finding sets of queries where all pairs are related, what we call a query community, and this is the technical contribution of the paper. We describe a random model by which we obtain relationships between search queries and then prove general conditions under which we can reconstruct query communities. We propose two complementary approaches for inferring recommendations that utilize query communities in order to magnify the recommendation signal beyond what an individual query can provide. An extensive series of experiments on real search logs shows that the query communities found by our algorithm are more interesting and unexpected than a baseline of clustering the query-click graph. Also, whereas existing query suggestion algorithms are not designed for making commercial recommendations, we show that our algorithms do succeed in forecasting commercial intent. Query communities increase both the quantity and quality of recommendations.

#index 1536576
#* On composition of a federated web search result page: using online users to provide pairwise preference for heterogeneous verticals
#@ Ashok Kumar Ponnuswami;Kumaresh Pattabiraman;Qiang Wu;Ran Gilad-Bachrach;Tapas Kanungo
#t 2011
#c 2
#% 169781
#% 448194
#% 577224
#% 818212
#% 840846
#% 946521
#% 985828
#% 987228
#% 1150174
#% 1166523
#% 1190055
#% 1227582
#% 1227616
#% 1227617
#% 1227634
#% 1250379
#% 1450915
#% 1482356
#! Modern web search engines are federated --- a user query is sent to the numerous specialized search engines called verticals like web (text documents), News, Image, Video, etc. and the results returned by these engines are then aggregated and composed into a search result page (SERP) and presented to the user. For a specific query, multiple verticals could be relevant, which makes the placement of these vertical results within blocks of textual web results challenging: how do we represent, assess, and compare the relevance of these heterogeneous entities? In this paper we present a machine-learning framework for SERP composition in the presence of multiple relevant verticals. First, instead of using the traditional label generation method of human judgment guidelines and trained judges, we use a randomized online auditioning system that allows us to evaluate triples of the form query, web block, vertical. We use a pairwise click preference to evaluate whether the web block or the vertical block had a better users' engagement. Next, we use a hinged feature vector that contains features from the web block to create a common reference frame and augment it with features representing the specific vertical judged by the user. A gradient boosted decision tree is then learned from the training data. For the final composition of the SERP, we place a vertical result at a slot if the score is higher than a computed threshold. The thresholds are algorithmically determined to guarantee specific coverage for verticals at each slot. We use correlation of clicks as our offline metric and show that click-preference target has a better correlation than human judgments based models. Furthermore, on online tests for News and Image verticals we show higher user engagement for both head and tail queries.

#index 1536577
#* Stochastic query covering
#@ Aris Anagnostopoulos;Luca Becchetti;Stefano Leonardi;Ida Mele;Piotr Sankowski
#t 2011
#c 2
#% 194299
#% 296646
#% 341672
#% 387427
#% 577302
#% 578337
#% 580676
#% 729343
#% 766447
#% 805765
#% 860861
#% 878624
#% 1089473
#% 1130403
#% 1141537
#% 1166473
#% 1190098
#% 1201766
#% 1217127
#% 1227628
#% 1348355
#% 1426267
#% 1834787
#! In this paper we introduce the problem of query covering as a means to efficiently cache query results. The general idea is to populate the cache with documents that contribute to the result pages of a large number of queries, as opposed to caching the top documents for each query. It turns out that the problem is hard and solving it requires knowledge of the structure of the queries and the results space, as well as knowledge of the input query distribution. We formulate the problem under the framework of stochastic optimization; theoretically it can be seen as a stochastic universal version of set multicover. While the problem is NP-hard to be solved exactly, we show that for any distribution it can be approximated using a simple greedy approach. Our theoretical findings are complemented by experimental activity on real datasets, showing the feasibility and potential interest of query-covering approaches in practice.

#index 1536578
#* Learning to re-rank web search results with multiple pairwise features
#@ Changsung Kang;Xuanhui Wang;Jiang Chen;Ciya Liao;Yi Chang;Belle Tseng;Zhaohui Zheng
#t 2011
#c 2
#% 564279
#% 577224
#% 818221
#% 838528
#% 840846
#% 858155
#% 879565
#% 946521
#% 983820
#% 983825
#% 987228
#% 987241
#% 1035577
#% 1035578
#% 1055712
#% 1074083
#% 1093383
#% 1190055
#% 1211826
#% 1227581
#% 1292530
#% 1355034
#! Web search ranking functions are typically learned to rank search results based on features of individual documents, i.e., pointwise features. Hence, the rich relationships among documents, which contain multiple types of useful information, are either totally ignored or just explored very limitedly. In this paper, we propose to explore multiple pairwise relationships between documents in a learning setting to rerank search results. In particular, we use a set of pairwise features to capture various kinds of pairwise relationships and design two machine learned re-ranking methods to effectively combine these features with a base ranking function: a pairwise comparison method and a pairwise function decomposition method. Furthermore, we propose several schemes to estimate the potential gains of our re-ranking methods on each query and selectively apply them to queries with high confidence. Our experiments on a large scale commercial search engine editorial data set show that considering multiple pairwise relationships is quite beneficial and our proposed methods can achieve significant gain over methods which only consider pointwise features or a single type of pairwise relationship.

#index 1536579
#* The tube over time: characterizing popularity growth of youtube videos
#@ Flavio Figueiredo;Fabrício Benevenuto;Jussara M. Almeida
#t 2011
#c 2
#% 729923
#% 956564
#% 956578
#% 1002006
#% 1055741
#% 1083624
#% 1190127
#% 1250009
#% 1309092
#% 1347756
#% 1399995
#% 1411585
#% 1423984
#% 1441135
#% 1512520
#! Understanding content popularity growth is of great importance to Internet service providers, content creators and online marketers. In this work, we characterize the growth patterns of video popularity on the currently most popular video sharing application, namely YouTube. Using newly provided data by the application, we analyze how the popularity of individual videos evolves since the video's upload time. Moreover, addressing a key aspect that has been mostly overlooked by previous work, we characterize the types of the referrers that most often attracted users to each video, aiming at shedding some light into the mechanisms (e.g., searching or external linking) that often drive users towards a video, and thus contribute to popularity growth. Our analyses are performed separately for three video datasets, namely, videos that appear in the YouTube top lists, videos removed from the system due to copyright violation, and videos selected according to random queries submitted to YouTube's search engine. Our results show that popularity growth patterns depend on the video dataset. In particular, copyright protected videos tend to get most of their views much earlier in their lifetimes, often exhibiting a popularity growth characterized by a viral epidemic-like propagation process. In contrast, videos in the top lists tend to experience sudden significant bursts of popularity. We also show that not only search but also other YouTube internal mechanisms play important roles to attract users to videos in all three datasets.

#index 1536580
#* Citation recommendation without author supervision
#@ Qi He;Daniel Kifer;Jian Pei;Prasenjit Mitra;C. Lee Giles
#t 2011
#c 2
#% 136350
#% 262096
#% 415107
#% 577220
#% 722904
#% 730089
#% 758200
#% 760853
#% 779859
#% 806594
#% 818265
#% 869484
#% 878156
#% 987262
#% 987287
#% 1019131
#% 1055685
#% 1083684
#% 1102028
#% 1106097
#% 1195999
#% 1267777
#% 1271961
#% 1399975
#! Automatic recommendation of citations for a manuscript is highly valuable for scholarly activities since it can substantially improve the efficiency and quality of literature search. The prior techniques placed a considerable burden on users, who were required to provide a representative bibliography or to mark passages where citations are needed. In this paper we present a system that considerably reduces this burden: a user simply inputs a query manuscript (without a bibliography) and our system automatically finds locations where citations are needed. We show that naïve approaches do not work well due to massive noise in the document corpus. We produce a successful approach by carefully examining the relevance between segments in a query manuscript and the representative segments extracted from a document corpus. An extensive empirical evaluation using the CiteSeerX data set shows that our approach is effective.

#index 1536581
#* Query suggestion for E-commerce sites
#@ Mohammad Al Hasan;Nish Parikh;Gyanit Singh;Neel Sundaresan
#t 2011
#c 2
#% 218978
#% 577224
#% 641976
#% 754125
#% 818226
#% 869501
#% 987193
#% 987203
#% 1023420
#% 1035578
#% 1055760
#% 1130842
#% 1130868
#% 1173699
#% 1190250
#% 1355036
#% 1392432
#% 1399983
#% 1712595
#! Query suggestion module is an integral part of every search engine. It helps search engine users narrow or broaden their searches. Published work on query suggestion methods has mainly focused on the web domain. But, the module is also popular in the domain of e-commerce for product search. In this paper, we discuss query suggestion and its methodologies in the context of e-commerce search engines. We show that dynamic inventory combined with long and sparse tail of query distribution poses unique challenges to build a query suggestion method for an e-commerce marketplace. We compare and contrast the design of a query suggestion system for web search engines and e-commerce search engines. Further, we discuss interesting measures to quantify the effectiveness of our query suggestion methodologies. We also describe the learning gained from exposing our query suggestion module to a vibrant community of millions of users.

#index 1536582
#* An algorithmic treatment of strong queries
#@ Ravi Kumar;Silvio Lattanzi;Prabhakar Raghavan
#t 2011
#c 2
#% 187983
#% 214233
#% 232752
#% 256685
#% 268114
#% 600184
#% 754117
#% 939742
#% 987208
#% 987290
#% 1035579
#% 1166508
#% 1224602
#% 1292536
#% 1305656
#% 1348548
#% 1399956
#% 1603007
#! A strong query for a target document with respect to an index is the smallest query for which the target document is returned by the index as the top result for the query. The strong query problem was first studied more than a decade ago in the context of measuring search engine overlap. Despite its simple-to-state nature and its longevity in the field, this problem has not been sufficiently addressed in a formal manner. In this paper we provide the first rigorous treatment of the strong query problem. We show an interesting connection between this problem and the set cover problem, and use it to obtain basic hardness and algorithmic results. Experiments on more than 10K documents show that our proposed algorithm performs much better than the widely-used word frequency-based heuristic. En route, our study suggests that less than four words on average can be sufficient to uniquely identify web pages.

#index 1536583
#* Enhanced email spam filtering through combining similarity graphs
#@ Anirban Dasgupta;Maxim Gurevich;Kunal Punera
#t 2011
#c 2
#% 304425
#% 766508
#% 769885
#% 840882
#% 860074
#% 936754
#% 961230
#% 987244
#% 1056065
#% 1066716
#% 1125906
#% 1128822
#% 1211829
#% 1214623
#% 1227714
#% 1260273
#% 1287222
#% 1369018
#% 1468359
#% 1531270
#! Over the last decade Email Spam has evolved from being just an irritant to users to being truly dangerous. This has led web-mail providers and academic researchers to dedicate considerable resources towards tackling this problem [9, 21, 22, 24, 26]. However, we argue that some aspects of the spam filtering problem are not handled appropriately in existing work. Principal among these are adversarial spammer efforts -- spammers routinely tune their spam emails to bypass spam-filters, and contaminate ground truth via fake HAM/SPAM votes -- and the scale and sparsity of the problem, which essentially precludes learning with a very large set of parameters. In this paper we propose an approach that learns to filter spam by striking a balance between generalizing HAM/SPAM votes across users and emails (to alleviate sparsity) and learning local models for each user (to limit effect of adversarial votes); votes are shared only amongst users and emails that are "similar" to one another. Moreover, we define user-user and email-email similarities using spam-resilient features that are extremely difficult for spammers to fake. We give a methodology that learns to combine multiple features into similarity values while directly optimizing the objective of better spam filtering. A useful side effect of this methodology is that the number of parameters that need to be estimated is very small: this helps us use off-the-shelf learning algorithms to achieve good accuracy while preventing over-training to the adversarial noise in the data. Finally, our approach gives a systematic way to incorporate existing spam-fighting technologies such as IP blacklists, keyword based classifiers, etc into one framework. Experiments on a real-world email dataset show that our approach leads to significant improvements compared to two state-of-the-art baselines.

#index 1536584
#* LambdaMerge: merging the results of query reformulations
#@ Daniel Sheldon;Milad Shokouhi;Martin Szummer;Nick Craswell
#t 2011
#c 2
#% 218978
#% 340901
#% 340959
#% 397161
#% 411762
#% 643057
#% 754125
#% 783474
#% 783475
#% 818212
#% 818267
#% 869501
#% 879585
#% 879614
#% 893735
#% 987222
#% 987265
#% 1042787
#% 1074098
#% 1130855
#% 1195859
#% 1227634
#% 1227647
#% 1355020
#% 1450900
#% 1450964
#% 1450975
#% 1450999
#% 1482284
#! Search engines can automatically reformulate user queries in a variety of ways, often leading to multiple queries that are candidates to replace the original. However, selecting a replacement can be risky: a reformulation may be more effective than the original or significantly worse, depending on the nature of the query, the source of reformulation candidates, and the corpus. In this paper, we explore methods to mitigate this risk by issuing several versions of the query (including the original) and merging their results. We focus on reformulations generated by random walks on the click graph, a method that can produce very good reformulations but is also variable and prone to topic drift. Our primary contribution is λ-Merge, a supervised merging method that is trained to directly optimize a retrieval metric (such as NDCG or MAP) using features that describe both the reformulations and the documents they return. In experiments on Bing data and GOV2, λ-Merge outperforms the original query and several unsupervised merging methods. λ-Merge also outperforms a supervised method to predict and select the best single formulation, and is competitive with an oracle that always selects the best formulation.

#index 1536585
#* Normalizing web product attributes and discovering domain ontology with minimal effort
#@ Tak-Lam Wong;Lidong Bing;Wai Lam
#t 2011
#c 2
#% 330784
#% 342703
#% 464434
#% 722904
#% 726773
#% 729978
#% 770844
#% 788107
#% 867052
#% 889107
#% 915340
#% 939377
#% 989662
#% 1074055
#% 1130927
#% 1280621
#% 1292576
#! We have developed a framework aiming at normalizing product attributes from Web pages collected from different Web sites without the need of labeled training examples. It can deal with pages composed of different layout format and content in an unsupervised manner. As a result, it can handle a variety of different domains with minimal effort. Our model is based on a generative probabilistic graphical model incorporated with Hidden Markov Models (HMM) considering both attribute names and attribute values to extract and normalize text fragments from Web pages in a unified manner. Dirichlet Process is employed to handle the unlimited number of attributes in a domain. An unsupervised inference method is proposed to predict the unobservable variables. We have also developed a method to automatically construct a domain ontology using the normalized product attributes which are the output of the inference on the graphical model. We have conducted extensive experiments and compared with existing works using prouct Web pages collected from real-world Web sites in three different domains to demonstrate the effectiveness of our framework.

#index 1536586
#* Aspect and sentiment unification model for online review analysis
#@ Yohan Jo;Alice H. Oh
#t 2011
#c 2
#% 722308
#% 722904
#% 769892
#% 876067
#% 939896
#% 956510
#% 1055682
#% 1227302
#% 1260740
#% 1261565
#% 1292503
#% 1297083
#% 1338591
#% 1400008
#% 1470684
#% 1471238
#% 1481541
#! User-generated reviews on the Web contain sentiments about detailed aspects of products and services. However, most of the reviews are plain text and thus require much effort to obtain information about relevant details. In this paper, we tackle the problem of automatically discovering what aspects are evaluated in reviews and how sentiments for different aspects are expressed. We first propose Sentence-LDA (SLDA), a probabilistic generative model that assumes all words in a single sentence are generated from one aspect. We then extend SLDA to Aspect and Sentiment Unification Model (ASUM), which incorporates aspect and sentiment together to model sentiments toward different aspects. ASUM discovers pairs of {aspect, sentiment} which we call senti-aspects. We applied SLDA and ASUM to reviews of electronic devices and restaurants. The results show that the aspects discovered by SLDA match evaluative details of the reviews, and the senti-aspects found by ASUM capture important aspects that are closely coupled with a sentiment. The results of sentiment classification show that ASUM outperforms other generative models and comes close to supervised classification methods. One important advantage of ASUM is that it does not require any sentiment labels of the reviews, which are often expensive to obtain.

#index 1536587
#* Searching patterns for relation extraction over the web: rediscovering the pattern-relation duality
#@ Yuan Fang;Kevin Chen-Chuan Chang
#t 2011
#c 2
#% 301241
#% 330616
#% 340953
#% 397160
#% 504443
#% 754068
#% 756964
#% 815868
#% 940029
#% 983614
#% 989682
#% 1250181
#% 1275182
#% 1355028
#% 1399933
#! While tuple extraction for a given relation has been an active research area, its dual problem of pattern search-- to find and rank patterns in a principled way-- has not been studied explicitly. In this paper, we propose and address the problem of pattern search, in addition to tuple extraction. As our objectives, we stress reusability for pattern search and scalability of tuple extraction, such that our approach can be applied to very large corpora like the Web. As the key foundation, we propose a conceptual model PRDualRank to capture the notion of precision and recall for both tuples and patterns in a principled way, leading to the "rediscovery" of the Pattern-Relation Duality-- the formal quantification of the reinforcement between patterns and tuples with the metrics of precision and recall. We also develop a concrete framework for PRDualRank, guided by the principles of a perfect sampling process over a complete corpus. Finally, we evaluated our framework over the real Web. Experiments show that on all three target relations our principled approach greatly outperforms the previous state-of-the-art system in both effectiveness and efficiency. In particular, we improved optimal F-score by up to 64%.

#index 1693859
#* Proceedings of the fifth ACM international conference on Web search and data mining
#@ Eytan Adar;Jaime Teevan;Eugene Agichtein;Yoelle Maarek
#t 2012
#c 2
#! We are delighted to welcome you to the Fifth ACM International Conference on Web Search and Data Mining (WSDM 2012) held on February 8--12, 2012 in Seattle, Washington. As in the previous years, WSDM has attracted an impressive number of submissions tackling the most recent technical challenges in Web search and data mining, with an ever-growing interest in their social aspects. Now in its fifth year, the conference has reached maturity and has become a leading forum for reporting the latest research developments in the field. We are happy to present here the proceedings of the conference. We received a total of 362 submissions from 38 countries and regions, out of which 75 were accepted for full paper publication in the proceedings, thus reaching an acceptance rate of 20.7% (compared to 22.3% last year). The accepted papers are from 17 countries, spanning four continents -- making this a truly international forum. Oral presentation slots were allocated to all papers. Yet, in order to maintain the single track model that most attendees prefer, we introduced this year a "spotlight" type of oral presentation. Out of the 75 accepted papers, 45 were assigned such a "spotlight" slot, while 30 were assigned a plenary slot. The type of slot was chosen by the Senior PC members and Program chairs, mostly based on whether the topic and the content of the paper were best suited for a large group presentation or for a more focused and interactive "spotlight" style of presentation.

#index 1693860
#* Nowcasting the macroeconomy with search engine data
#@ Hal R. Varian
#t 2012
#c 2
#! It is now possible to acquire real time information on economic variables of interest from various commercial sources. I illustrate how one can use Google Trends data to measure the state of the macroeconomy in various sectors, and discuss some of the ramifications for research and policy.

#index 1693861
#* Spatially-aware indexing for image object retrieval
#@ Roelof van Zwol;Lluis Garcia Pueyo
#t 2012
#c 2
#% 319464
#% 387427
#% 479973
#% 724320
#% 760805
#% 835018
#% 1058303
#% 1085939
#% 1177810
#% 1279580
#% 1482159
#% 1484413
#% 1484422
#! The success of image object retrieval systems relies on the visual bag-of-words paradigm, which allows image retrieval systems to adopt a retrieval strategy analogous to text retrieval. In this paper we propose two spatially-aware retrieval strategies for image object retrieval that replaces the vector space model. The advantage of the proposed spatially-aware indexing and retrieval strategies are threefold: (1) It allows for the deployment of small visual vocabularies, (2) the number of images evaluated at retrieval time is significantly reduced, and (3) it eliminates the need for a post-retrieval phase, which is normally used to test the spatial composition of the visual words in the retrieved images. The first spatially-aware retrieval strategy explores the direct neighbourhood of two local features for common visual words to determine the similarity of the region surrounding the local features. The second strategy embeds the spatial composition of its neighbourhood directly in the index using edge signatures. Both strategies rely on the coherence of the neighbourhood of points in different images containing similar objects. The comparison of the spatially-aware retrieval strategies against the vector space baseline shows a significant improvement in terms of early precision, and at the same time significantly reduce the number of candidates to be considered at retrieval time.

#index 1693862
#* Auralist: introducing serendipity into music recommendation
#@ Yuan Cao Zhang;Diarmuid Ó Séaghdha;Daniele Quercia;Tamas Jambor
#t 2012
#c 2
#% 301590
#% 330687
#% 452563
#% 722904
#% 734590
#% 734592
#% 783531
#% 805841
#% 860672
#% 1127472
#% 1176909
#% 1358070
#% 1358747
#% 1412455
#% 1425621
#% 1476450
#% 1613005
#% 1625357
#% 1625363
#! Recommendation systems exist to help users discover content in a large body of items. An ideal recommendation system should mimic the actions of a trusted friend or expert, producing a personalised collection of recommendations that balance between the desired goals of accuracy, diversity, novelty and serendipity. We introduce the Auralist recommendation framework, a system that - in contrast to previous work - attempts to balance and improve all four factors simultaneously. Using a collection of novel algorithms inspired by principles of "serendipitous discovery", we demonstrate a method of successfully injecting serendipity, novelty and diversity into recommendations whilst limiting the impact on accuracy. We evaluate Auralist quantitatively over a broad set of metrics and, with a user study on music recommendation, show that Auralist's emphasis on serendipity indeed improves user satisfaction.

#index 1693863
#* Efficient misbehaving user detection in online video chat services
#@ Hanqiang Cheng;Yu-Li Liang;Xinyu Xing;Xue Liu;Richard Han;Qin Lv;Shivakant Mishra
#t 2012
#c 2
#% 177826
#% 429731
#% 457672
#% 635689
#% 812418
#% 812535
#% 845226
#% 883972
#% 939122
#% 954849
#% 975161
#% 1279843
#% 1292838
#% 1560423
#% 1775427
#% 1798931
#! Online video chat services, such as Chatroulette, Omegle, and vChatter are becoming increasingly popular and have attracted millions of users. One critical problem encountered in such applications is the presence of misbehaving users ("flashers") and obscene content. Automatically filtering out obscene content from these systems in an efficient manner poses a difficult challenge. This paper presents a novel Fine-Grained Cascaded (FGC) classification solution that significantly speeds up the compute-intensive process of classifying misbehaving users by dividing image feature extraction into multiple stages and filtering out easily classified images in earlier stages, thus saving unnecessary computation costs of feature extraction in later stages. Our work is further enhanced by integrating new webcam-related contextual information (illumination and color) into the classification process, and a 2-stage soft margin SVM algorithm for combining multiple features. Evaluation results using real-world data set obtained from Chatroulette show that the proposed FGC based classification solution significantly outperforms state-of-the-art techniques.

#index 1693864
#* Beyond co-occurrence: discovering and visualizing tag relationships from geo-spatial and temporal similarities
#@ Haipeng Zhang;Mohammed Korayem;Erkang You;David J. Crandall
#t 2012
#c 2
#% 328257
#% 420078
#% 481281
#% 501066
#% 765412
#% 770857
#% 805839
#% 875980
#% 913194
#% 967244
#% 967452
#% 987205
#% 997189
#% 1055704
#% 1055707
#% 1055796
#% 1055857
#% 1127482
#% 1131835
#% 1132470
#% 1135161
#% 1190090
#% 1190131
#% 1292518
#% 1338319
#% 1484617
#% 1536522
#% 1560388
#! Studying relationships between keyword tags on social sharing websites has become a popular topic of research, both to improve tag suggestion systems and to discover connections between the concepts that the tags represent. Existing approaches have largely relied on tag co-occurrences. In this paper, we show how to find connections between tags by comparing their distributions over time and space, discovering tags with similar geographic and temporal patterns of use. Geo-spatial, temporal and geo-temporal distributions of tags are extracted and represented as vectors which can then be compared and clustered. Using a dataset of tens of millions of geo-tagged Flickr photos, we show that we can cluster Flickr photo tags based on their geographic and temporal patterns, and we evaluate the results both qualitatively and quantitatively using a panel of human judges. We also develop visualizations of temporal and geographic tag distributions, and show that they help humans recognize semantic relationships between tags. This approach to finding and visualizing similar tags is potentially useful for exploring any data having geographic and temporal annotations.

#index 1693865
#* Object matching in tweets with spatial models
#@ Nilesh Dalvi;Ravi Kumar;Bo Pang
#t 2012
#c 2
#% 869516
#% 987205
#% 1055707
#% 1190131
#% 1292482
#% 1338592
#% 1355044
#% 1399939
#% 1478118
#% 1481659
#% 1536509
#% 1544032
#% 1560379
#% 1560424
#% 1560425
#% 1592024
#! Despite their 140-character limitation, tweets embody a lot of valuable information, especially temporal and spatial. In this paper we study the geographic aspects of tweets, for a given object domain. We propose a user-level model for spatial encoding in tweets that goes beyond the explicit geo-coding or place name mentions; this model can be used to match objects to tweets. We illustrate our model and methodology using restaurants as the objects, and show a significant improvement in performance over using standard language models. En route, we obtain a method to geolocate users who tweet about geolocated objects; this may be of independent interest.

#index 1693866
#* Beyond 100 million entities: large-scale blocking-based resolution for heterogeneous data
#@ George Papadakis;Ekaterini Ioannou;Claudia Niederée;Themis Palpanas;Wolfgang Nejdl
#t 2012
#c 2
#% 201889
#% 310516
#% 480654
#% 577247
#% 810014
#% 874876
#% 913783
#% 915242
#% 931291
#% 1217163
#% 1250576
#% 1292496
#% 1292570
#% 1372726
#% 1523833
#% 1536558
#% 1538763
#% 1563874
#% 1588359
#% 1589314
#% 1890006
#! A prerequisite for leveraging the vast amount of data available on the Web is Entity Resolution, i.e., the process of identifying and linking data that describe the same real-world objects. To make this inherently quadratic process applicable to large data sets, blocking is typically employed: entities (records) are grouped into clusters - the blocks - of matching candidates and only entities of the same block are compared. However, novel blocking techniques are required for dealing with the noisy, heterogeneous, semi-structured, user-generateddata in the Web, as traditional blocking techniques are inapplicable due to their reliance on schema information. The introduction of redundancy, improves the robustness of blocking methods but comes at the price of additional computational cost. In this paper, we present methods for enhancing the efficiency of redundancy-bearing blocking methods, such as our attribute-agnostic blocking approach. We introduce novel blocking schemes that build blocks based on a variety of evidences, including entity identifiers and relationships between entities; they significantly reduce the required number of comparisons, while maintaining blocking effectiveness at very high levels. We also introduce two theoretical measures that provide a reliable estimation of the performance of a blocking method, without requiring the analytical processing of its blocks. Based on these measures, we develop two techniques for improving the performance of blocking: combining individual, complementary blocking schemes, and purging blocks until given criteria are satisfied. We test our methods through an extensive experimental evaluation, using a voluminous data set with 182 million heterogeneous entities. The outcomes of our study show the applicability and the high performance of our approach.

#index 1693867
#* Mining contrastive opinions on political texts using cross-perspective topic model
#@ Yi Fang;Luo Si;Naveen Somasundaram;Zhengtao Yu
#t 2012
#c 2
#% 642990
#% 722904
#% 727877
#% 746885
#% 769967
#% 805873
#% 854646
#% 939633
#% 939848
#% 939896
#% 943811
#% 956510
#% 1019145
#% 1035591
#% 1055682
#% 1074102
#% 1077150
#% 1127964
#% 1249457
#% 1250237
#% 1261563
#% 1261565
#% 1270702
#% 1270753
#% 1275196
#% 1292503
#% 1338675
#% 1471245
#% 1481542
#% 1536586
#% 1810980
#! This paper presents a novel opinion mining research problem, which is called Contrastive Opinion Modeling (COM). Given any query topic and a set of text collections from multiple perspectives, the task of COM is to present the opinions of the individual perspectives on the topic, and furthermore to quantify their difference. This general problem subsumes many interesting applications, including opinion summarization and forecasting, government intelligence and cross-cultural studies. We propose a novel unsupervised topic model for contrastive opinion modeling. It simulates the generative process of how opinion words occur in the documents of different collections. The ad hoc opinion search process can be efficiently accomplished based on the learned parameters in the model. The difference of perspectives can be quantified in a principled way by the Jensen-Shannon divergence among the individual topic-opinion distributions. An extensive set of experiments have been conducted to evaluate the proposed model on two datasets in the political domain: 1) statement records of U.S. senators; 2) world news reports from three representative media in U.S., China and India, respectively. The experimental results with both qualitative and quantitative analysis have shown the effectiveness of the proposed model.

#index 1693868
#* Coupled temporal scoping of relational facts
#@ Partha Pratim Talukdar;Derry Wijaya;Tom Mitchell
#t 2012
#c 2
#% 319244
#% 754068
#% 850430
#% 939595
#% 943834
#% 956564
#% 1024551
#% 1264790
#% 1267783
#% 1270341
#% 1271269
#% 1271280
#% 1271363
#% 1275182
#% 1289510
#% 1328349
#% 1372745
#% 1560399
#% 1642010
#! Recent research has made significant advances in automatically constructing knowledge bases by extracting relational facts (e.g., Bill Clinton-presidentOf-US) from large text corpora. Temporally scoping such relational facts in the knowledge base (i.e., determining that Bill Clinton-presidentOf-US is true only during the period 1993 - 2001) is an important, but relatively unexplored problem. In this paper, we propose a joint inference framework for this task, which leverages fact-specific temporal constraints, and weak supervision in the form of a few labeled examples. Our proposed framework, CoTS (Coupled Temporal Scoping), exploits temporal containment, alignment, succession, and mutual exclusion constraints among facts from within and across relations. Our contribution is multi-fold. Firstly, while most previous research has focused on micro-reading approaches for temporal scoping, we pose it in a macro-reading fashion, as a change detection in a time series of facts' features computed from a large number of documents. Secondly, to the best of our knowledge, there is no other work that has used joint inference for temporal scoping. We show that joint inference is effective compared to doing temporal scoping of individual facts independently. We conduct our experiments on large scale open-domain publicly available time-stamped datasets, such as English Gigaword Corpus and Google Books Ngrams, demonstrating CoTS's effectiveness.

#index 1693869
#* Overcoming browser cookie churn with clustering
#@ Anirban Dasgupta;Maxim Gurevich;Liang Zhang;Belle Tseng;Achint O. Thomas
#t 2012
#c 2
#% 255137
#% 273890
#% 280404
#% 283169
#% 406493
#% 430746
#% 445243
#% 464291
#% 747890
#% 750574
#% 765548
#% 818434
#% 881537
#% 896856
#% 1213625
#% 1214623
#% 1232015
#% 1489497
#% 1625344
#! Many large Internet websites are accessed by users anonymously, without requiring registration or logging-in. However, to provide personalized service these sites build anonymous, yet persistent, user models based on repeated user visits. Cookies, issued when a web browser first visits a site, are typically employed to anonymously associate a website visit with a distinct user (web browser). However, users may reset cookies, making such association short-lived and noisy. In this paper we propose a solution to the cookie churn problem: a novel algorithm for grouping similar cookies into clusters that are more persistent than individual cookies. Such clustering could potentially allow more robust estimation of the number of unique visitors of the site over a certain long time period, and also better user modeling which is key to plenty of web applications such as advertising and recommender systems. We present a novel method to cluster browser cookies into groups that are likely to belong to the same browser based on a statistical model of browser visitation patterns. We address each step of the clustering as a binary classification problem estimating the probability that two different subsets of cookies belong to the same browser. We observe that our clustering problem is a generalized interval graph coloring problem, and propose a greedy heuristic algorithm for solving it. The scalability of this method allows us to cluster hundreds of millions of browser cookies and provides significant improvements over baselines such as constrained K-means.

#index 1693870
#* mTrust: discerning multi-faceted trust in a connected world
#@ Jiliang Tang;Huiji Gao;Huan Liu
#t 2012
#c 2
#% 290830
#% 316143
#% 574205
#% 577217
#% 754098
#% 868480
#% 943767
#% 1001279
#% 1071523
#% 1083671
#% 1130901
#% 1190130
#% 1214661
#% 1247796
#% 1355042
#% 1384246
#% 1399997
#% 1400002
#% 1400031
#% 1536554
#% 1560411
#% 1603140
#! Traditionally, research about trust assumes a single type of trust between users. However, trust, as a social concept, inherently has many facets indicating multiple and heterogeneous trust relationships between users. Due to the presence of a large trust network for an online user, it is necessary to discern multi-faceted trust as there are naturally experts of different types. Our study in product review sites reveals that people place trust differently to different people. Since the widely used adjacency matrix cannot capture multi-faceted trust relationships between users, we propose a novel approach by incorporating these relationships into traditional rating prediction algorithms to reliably estimate their strengths. Our work results in interesting findings such as heterogeneous pairs of reciprocal links. Experimental results on real-world data from Epinions and Ciao show that our work of discerning multi-faceted trust can be applied to improve the performance of tasks such as rating prediction, facet-sensitive ranking, and status theory.

#index 1693871
#* Of hammers and nails: an empirical comparison of three paradigms for processing large graphs
#@ Marc Najork;Dennis Fetterly;Alan Halverson;Krishnaram Kenthapadi;Sreenivas Gollapudi
#t 2012
#c 2
#% 872
#% 268186
#% 282905
#% 309749
#% 309779
#% 401220
#% 407822
#% 410276
#% 656242
#% 656282
#% 754117
#% 824697
#% 866773
#% 954300
#% 963669
#% 983467
#% 987222
#% 1063553
#% 1166529
#% 1215445
#% 1278123
#% 1278124
#% 1299097
#% 1328095
#% 1328108
#% 1328186
#% 1355056
#% 1426513
#% 1468421
#! Many phenomena and artifacts such as road networks, social networks and the web can be modeled as large graphs and analyzed using graph algorithms. However, given the size of the underlying graphs, efficient implementation of basic operations such as connected component analysis, approximate shortest paths, and link-based ranking (e.g. PageRank) becomes challenging. This paper presents an empirical study of computations on such large graphs in three well-studied platform models, viz., a relational model, a data-parallel model, and a special-purpose in-memory model. We choose a prototypical member of each platform model and analyze the computational efficiencies and requirements for five basic graph operations used in the analysis of real-world graphs viz., PageRank, SALSA, Strongly Connected Components (SCC), Weakly Connected Components (WCC), and Approximate Shortest Paths (ASP). Further, we characterize each platform in terms of these computations using model-specific implementations of these algorithms on a large web graph. Our experiments show that there is no single platform that performs best across different classes of operations on large graphs. While relational databases are powerful and flexible tools that support a wide variety of computations, there are computations that benefit from using special-purpose storage systems and others that can exploit data-parallel platforms.

#index 1693872
#* Pairwise cross-domain factor model for heterogeneous transfer ranking
#@ Bo Long;Yi Chang;Anlei Dong;Jianzhang He
#t 2012
#c 2
#% 252011
#% 564279
#% 577224
#% 769886
#% 770804
#% 840846
#% 840898
#% 907517
#% 939332
#% 983814
#% 983820
#% 983825
#% 983828
#% 983865
#% 983899
#% 987228
#% 987241
#% 989592
#% 1074083
#% 1130817
#% 1159262
#% 1261539
#% 1272110
#% 1292566
#% 1328303
#% 1338581
#% 1826309
#! Learning to rank arises in many information retrieval applications, ranging from Web search engine, online advertising to recommendation systems. Traditional ranking mainly focuses on one type of data source, and effective modeling relies on a sufficiently large number of labeled examples, which require expensive and time-consuming labeling process. However, in many real-world applications, ranking over multiple related heterogeneous domains becomes a common situation, where in some domains we may have a relatively large amount of training data while in some other domains we can only collect very little. Theretofore, how to leverage labeled information from related heterogeneous domain to improve ranking in a target domain has become a problem of great interests. In this paper, we propose a novel probabilistic model, pairwise cross-domain factor model, to address this problem. The proposed model learns latent factors(features) for multi-domain data in partially-overlapped heterogeneous feature spaces. It is capable of learning homogeneous feature correlation, heterogeneous feature correlation, and pairwise preference correlation for cross-domain knowledge transfer. We also derive two PCDF variations to address two important special cases. Under the PCDF model, we derive a stochastic gradient based algorithm, which facilitates distributed optimization and is flexible to adopt different loss functions and regularization functions to accommodate different data distributions. The extensive experiments on real world data sets demonstrate the effectiveness of the proposed model and algorithm.

#index 1693873
#* Scalable inference in latent variable models
#@ Amr Ahmed;Moahmed Aly;Joseph Gonzalez;Shravan Narayanamurthy;Alexander J. Smola
#t 2012
#c 2
#% 39034
#% 91631
#% 232771
#% 757953
#% 1083636
#% 1385969
#% 1451206
#% 1523858
#% 1605925
#! Latent variable techniques are pivotal in tasks ranging from predicting user click patterns and targeting ads to organizing the news and managing user generated content. Latent variable techniques like topic modeling, clustering, and subspace estimation provide substantial insight into the latent structure of complex data with little or no external guidance making them ideal for reasoning about large-scale, rapidly evolving datasets. Unfortunately, due to the data dependencies and global state introduced by latent variables and the iterative nature of latent variable inference, latent-variable techniques are often prohibitively expensive to apply to large-scale, streaming datasets. In this paper we present a scalable parallel framework for efficient inference in latent variable models over streaming web-scale data. Our framework addresses three key challenges: 1) synchronizing the global state which includes global latent variables (e.g., cluster centers and dictionaries); 2) efficiently storing and retrieving the large local state which includes the data-points and their corresponding latent variables (e.g., cluster membership); and 3) sequentially incorporating streaming data (e.g., the news). We address these challenges by introducing: 1) a novel delta-based aggregation system with a bandwidth-efficient communication protocol; 2) schedule-aware out-of-core storage; and 3) approximate forward sampling to rapidly incorporate new data. We demonstrate state-of-the-art performance of our framework by easily tackling datasets two orders of magnitude larger than those addressed by the current state-of-the-art. Furthermore, we provide an optimized and easily customizable open-source implementation of the framework1.

#index 1693874
#* Learning recommender systems with adaptive regularization
#@ Steffen Rendle
#t 2012
#c 2
#% 316143
#% 425040
#% 476874
#% 734592
#% 793245
#% 840924
#% 1073982
#% 1083671
#% 1130901
#% 1214666
#% 1214694
#% 1355024
#% 1535439
#% 1558464
#! Many factorization models like matrix or tensor factorization have been proposed for the important application of recommender systems. The success of such factorization models depends largely on the choice of good values for the regularization parameters. Without a careful selection they result in poor prediction quality as they either underfit or overfit the data. Regularization values are typically determined by an expensive search that requires learning the model parameters several times: once for each tuple of candidate values for the regularization parameters. In this paper, we present a new method that adapts the regularization automatically while training the model parameters. To achieve this, we optimize simultaneously for two criteria: (1) as usual the model parameters for the regularized objective and (2) the regularization of future parameter updates for the best predictive quality on a validation set. We develop this for the generic model class of Factorization Machines which subsumes a wide variety of factorization models. We show empirically, that the advantages of our adaptive regularization method compared to expensive hyperparameter search do not come to the price of worse predictive quality. In total with our method, learning regularization parameters is as easy as learning model parameters and thus there is no need for any time-consuming search of regularization values because they are found on-the-fly. This makes our method highly attractive for practical use.

#index 1693875
#* Collaborative ranking
#@ Suhrid Balakrishnan;Sumit Chopra
#t 2012
#c 2
#% 400847
#% 411762
#% 577224
#% 722928
#% 734915
#% 956546
#% 987241
#% 1073892
#% 1074064
#% 1183200
#% 1227634
#% 1260273
#% 1287220
#% 1476448
#% 1817412
#! Typical recommender systems use the root mean squared error (RMSE) between the predicted and actual ratings as the evaluation metric. We argue that RMSE is not an optimal choice for this task, especially when we will only recommend a few (top) items to any user. Instead, we propose using a ranking metric, namely normalized discounted cumulative gain (NDCG), as a better evaluation metric for this task. Borrowing ideas from the learning to rank community for web search, we propose novel models which approximately optimize NDCG for the recommendation task. Our models are essentially variations on matrix factorization models where we also additionally learn the features associated with the users and the items for the ranking task. Experimental results on a number of standard collaborative filtering data sets validate our claims. The results also show the accuracy and efficiency of our models and the benefits of learning features for ranking.

#index 1693876
#* From chatter to headlines: harnessing the real-time web for personalized news recommendation
#@ Gianmarco De Francisci Morales;Aristides Gionis;Claudio Lucchese
#t 2012
#c 2
#% 124010
#% 280819
#% 411762
#% 577224
#% 813966
#% 879603
#% 956521
#% 1040837
#% 1206633
#% 1292502
#% 1384223
#% 1399992
#% 1432574
#% 1476495
#% 1482547
#% 1536506
#% 1536509
#% 1537471
#% 1561556
#% 1587377
#% 1607052
#! We propose a new methodology for recommending interesting news to users by exploiting the information in their twitter persona. We model relevance between users and news articles using a mix of signals drawn from the news stream and from twitter: the profile of the social neighborhood of the users, the content of their own tweet stream, and topic popularity in the news and in the whole twitter-land. We validate our approach on a real-world dataset of approximately 40k articles coming from Yahoo! News and one month of crawled twitter data. We train our model using a learning-to-rank approach and support-vector machines. The train and test set are drawn from Yahoo! toolbar log data. We heuristically identify 3214 users of twitter in the log and use their clicks on news articles to train our system. Our methodology is able to predict with good accuracy the news articles clicked by the users and rank them higher than other news articles. The results show that the combination of various signals from real-time Web and micro-blogging platforms can be a useful resource to understand user behavior.

#index 1693877
#* ETF: extended tensor factorization model for personalizing prediction of review helpfulness
#@ Samaneh Moghaddam;Mohsen Jamali;Martin Ester
#t 2012
#c 2
#% 124010
#% 316143
#% 907490
#% 956642
#% 1035590
#% 1155658
#% 1166519
#% 1190069
#% 1214666
#% 1214694
#% 1246960
#% 1246963
#% 1260273
#% 1261574
#% 1269378
#% 1287270
#% 1330520
#% 1331577
#% 1400002
#% 1400022
#% 1400122
#% 1476453
#% 1476461
#% 1482272
#% 1482445
#% 1598400
#% 1633079
#% 1642226
#! Online reviews are valuable sources of information for a variety of decision-making processes such as purchasing products. As the number of online reviews is growing rapidly, it becomes increasingly difficult for users to identify those that are helpful. This has motivated research into the problem of identifying high quality and helpful reviews automatically. The current methods assume that the helpfulness of a review is independent from the readers of that review. However, we argue that the quality of a review may not be the same for different users. For example, a professional and an amateur photographer may rate the helpfulness of a review very differently. In this paper, we introduce the problem of predicting a personalized review quality for recommendation of helpful reviews. To address this problem, we propose a series of increasingly sophisticated probabilistic graphical models, based on Matrix Factorization and Tensor Factorization. We evaluate the proposed models using a database of 1.5 million reviews and more than 13 million quality ratings obtained from Epinions.com. The experiments demonstrate that the proposed latent factor models outperform the state-of-the art approaches using textual and social features. Finally, our experiments confirm that the helpfulness of a review is indeed not the same for all users and that there are some latent factors that affect a user's evaluation of the review quality.

#index 1693878
#* Multi-relational matrix factorization using bayesian personalized ranking for social network data
#@ Artus Krohn-Grimberghe;Lucas Drumond;Christoph Freudenthaler;Lars Schmidt-Thieme
#t 2012
#c 2
#% 397155
#% 1083696
#% 1108903
#% 1130901
#% 1214703
#% 1227602
#% 1260273
#% 1292578
#% 1355024
#% 1417104
#% 1476461
#% 1535399
#% 1560408
#% 1625387
#! A key element of the social networks on the internet such as Facebook and Flickr is that they encourage users to create connections between themselves, other users and objects. One important task that has been approached in the literature that deals with such data is to use social graphs to predict user behavior (e.g. joining a group of interest). More specifically, we study the cold-start problem, where users only participate in some relations, which we will call social relations, but not in the relation on which the predictions are made, which we will refer to as target relations. We propose a formalization of the problem and a principled approach to it based on multi-relational factorization techniques. Furthermore, we derive a principled feature extraction scheme from the social data to extract predictors for a classifier on the target relation. Experiments conducted on real world datasets show that our approach outperforms current methods.

#index 1693879
#* Comment spam detection by sequence mining
#@ Ravi Kant;Srinivasan H. Sengamedu;Krishnan S. Kumar
#t 2012
#c 2
#% 217815
#% 310559
#% 329537
#% 459006
#% 463903
#% 464996
#% 477791
#% 577256
#% 629644
#% 745515
#% 956642
#% 1125904
#% 1127962
#% 1194314
#% 1224517
#% 1227714
#% 1292708
#% 1292760
#% 1339796
#% 1394505
#! Comments are supported by several web sites to increase user participation. Users can usually comment on a variety of media types - photos, videos, news articles, blogs, etc. Comment spam is one of the biggest challenges facing this feature. The traditional approach to combat spam is to train classifiers using various machine learning techniques. Since the commonly used classifiers work on the entire comment text, it is easy to mislead them by embedding spam content in good content. In this paper, we make several contributions towards comment spam detection. (1) We propose a new framework for spam detection that is immune to embed attacks. We characterize spam by a set of frequently occurring sequential patterns. (2) We introduce a variant (called min-closed) of the frequent closed sequence mining problem that succinctly captures all the frequently occurring patterns. We prove as well as experimentally show that the set of min-closed sequences is an order of magnitude smaller than the set of closed sequences and yet has exactly the same coverage. (3) We describe MCPRISM, extension of the recently published PRISM algorithm that effectively mines min-closed sequences, using prime encoding. In the process, we solve the open problem of using the prime-encoding technique to speed up traditional closed sequence mining. (4) We finally need to whittle down the set of frequent subsequences to a small set without sacrificing coverage. This problem is NP-Hard but we show that the coverage function is submodular and hence the greedy heuristic gives a fast algorithm that is close to optimal. We then describe the experiments that were carried out on a large real world comment data and the publicly available Gazelle dataset. (1) We show that nearly 80% of spam on real world data can be effectively captured by the mined sequences at very low false positive rates. (2) The sequences mined are highly discriminative. (3) On Gazelle data, the proposed algorithmic enhancements are faster by at least by a factor and by an order of magnitude on the larger comment dataset.

#index 1693880
#* Mining slang and urban opinion words and phrases from cQA services: an optimization approach
#@ Hadi Amiri;Tat-Seng Chua
#t 2012
#c 2
#% 722308
#% 746885
#% 769892
#% 815915
#% 876068
#% 939348
#% 939848
#% 939896
#% 939897
#% 995140
#% 1063632
#% 1074162
#% 1127964
#% 1133663
#% 1179994
#% 1227600
#% 1260740
#% 1261566
#% 1261670
#% 1297081
#% 1305481
#% 1310424
#% 1330520
#% 1338591
#% 1470681
#% 1471219
#% 1471256
#% 1536523
#! Current opinion lexicons contain most of the common opinion words, but they miss slang and so-called urban opinion words and phrases (e.g. delish, cozy, yummy, nerdy, and yuck). These subjectivity clues are frequently used in community questions and are useful for opinion question analysis. This paper introduces a principled approach to constructing an opinion lexicon for community-based question answering (cQA) services. We formulate the opinion lexicon induction as a semi-supervised learning task in the graph context. Our method makes use of existing opinion words to extract new opinion entities (slang and urban words/phrases) from community questions. It then models the opinion entities in a graph context to learn the polarity of the new opinion entities based on the graph connectivity information. In contrast to previous approaches, our method not only learns such polarities from the labeled data but also from the unlabeled data and is more feasible in the web context where the dictionary-based relations (such as synonym, antonym, or hyponym) between most words are not available for constructing a high quality graph. The experiments show that our approach is effective both in terms of the quality of the discovered new opinion entities as well as its ability in inferring their polarity. Furthermore, since the value of opinion lexicons lies in their usefulness in applications, we show the utility of the constructed lexicon in the sentiment classification task.

#index 1693881
#* What's in a hashtag?: content based prediction of the spread of ideas in microblogging communities
#@ Oren Tsur;Ari Rappoport
#t 2012
#c 2
#% 416733
#% 729923
#% 754107
#% 768632
#% 949164
#% 1016101
#% 1214658
#% 1214671
#% 1301004
#% 1355040
#% 1399992
#% 1432574
#% 1451243
#% 1481479
#% 1535333
#% 1536522
#% 1544032
#% 1560424
#% 1560429
#% 1606436
#% 1676017
#! Current social media research mainly focuses on temporal trends of the information flow and on the topology of the social graph that facilitates the propagation of information. In this paper we study the effect of the content of the idea on the information propagation. We present an efficient hybrid approach based on a linear regression for predicting the spread of an idea in a given time frame. We show that a combination of content features with temporal and topological features minimizes prediction error. Our algorithm is evaluated on Twitter hashtags extracted from a dataset of more than 400 million tweets. We analyze the contribution and the limitations of the various feature types to the spread of information, demonstrating that content aspects can be used as strong predictors thus should not be disregarded. We also study the dependencies between global features such as graph topology and content features.

#index 1693882
#* No search result left behind: branching behavior with browser tabs
#@ Jeff Huang;Thomas Lin;Ryen W. White
#t 2012
#c 2
#% 805898
#% 818221
#% 869476
#% 961627
#% 1022745
#% 1035578
#% 1043044
#% 1047437
#% 1074092
#% 1130878
#% 1190055
#% 1190056
#% 1348355
#% 1348356
#% 1355037
#% 1355038
#% 1384164
#% 1399957
#% 1429403
#% 1429431
#% 1450873
#% 1482222
#% 1536546
#% 1573553
#! Today's Web browsers allow users to open links in new windows or tabs. This action, which we call 'branching', is sometimes performed on search results when the user plans to eventually visit multiple results. We detect branching behavior on a large commercial search engine with a client-side script on the results page. Two-fifths of all users spawned new tabs on search results in the timeframe of our study; branching usage varied with different query types and vertical. Both branching and backtracking are viable methods for visiting multiple search results. To understand user search strategies, we treat multiple result clicks following a query as ordered events to understand user search strategies. Users branching in a query are more likely to click search results from top to bottom, while users who backtrack are less likely to do so; this is especially true for queries involving more than two clicks. These findings inform an experiment in which we take a popular click model and modify it to account for the differing user behavior when branching. By understanding that users continue examining search results before viewing a branched result, we can improve the click model for branching queries.

#index 1693883
#* Characterizing web content, user interests, and search behavior by reading level and topic
#@ Jin Young Kim;Kevyn Collins-Thompson;Paul N. Bennett;Susan T. Dumais
#t 2012
#c 2
#% 378486
#% 746870
#% 766505
#% 838442
#% 956552
#% 987272
#% 1166518
#% 1214757
#% 1338622
#% 1399944
#% 1442577
#% 1482279
#% 1536545
#% 1641961
#% 1693885
#! A user's expertise or ability to understand a document on a given topic is an important aspect of that document's relevance. However, this aspect has not been well-explored in information retrieval systems, especially those at Web scale where the great diversity of content, users, and tasks presents an especially challenging search problem. To help improve our modeling and understanding of this diversity, we apply automatic text classifiers, based on reading difficulty and topic prediction, to estimate a novel type of profile for important entities in Web search -- users, websites, and queries. These profiles capture topic and reading level distributions, which we then use in conjunction with search log data to characterize and compare different entities. We find that reading level and topic distributions provide an important new representation of Web content and user interests, and that using both together is more effective than using either one separately. In particular we find that: 1) the reading level of Web content and the diversity of visitors to a website can vary greatly by topic; 2) the degree to which a user's profile matches with a site's profile is closely correlated with the user's preference of the website in search results, and 3) site or URL profiles can be used to predict 'expertness' whether a given site or URL is oriented toward expert vs. non-expert users. Our findings provide strong evidence in favor of jointly incorporating reading level and topic distribution metadata into a variety of critical tasks in Web information systems.

#index 1693884
#* Topical clustering of search results
#@ Ugo Scaiella;Paolo Ferragina;Andrea Marino;Massimiliano Ciaramita
#t 2012
#c 2
#% 807295
#% 813043
#% 869500
#% 874693
#% 987203
#% 987328
#% 995140
#% 1074073
#% 1074118
#% 1083629
#% 1130858
#% 1196006
#% 1202162
#% 1214667
#% 1272267
#% 1292559
#% 1450850
#% 1482395
#% 1598354
#% 1711796
#! Search results clustering (SRC) is a challenging algorithmic problem that requires grouping together the results returned by one or more search engines in topically coherent clusters, and labeling the clusters with meaningful phrases describing the topics of the results included in them. In this paper we propose to solve SRC via an innovative approach that consists of modeling the problem as the labeled clustering of the nodes of a newly introduced graph of topics. The topics are Wikipedia-pages identified by means of recently proposed topic annotators [9, 11, 16, 20] applied to the search results, and the edges denote the relatedness among these topics computed by taking into account the linkage of the Wikipedia-graph. We tackle this problem by designing a novel algorithm that exploits the spectral properties and the labels of that graph of topics. We show the superiority of our approach with respect to academic state-of-the-art work [6] and well-known commercial systems (CLUSTY and LINGO3G) by performing an extensive set of experiments on standard datasets and user studies via Amazon Mechanical Turk. We test several standard measures for evaluating the performance of all systems and show a relative improvement of up to 20%.

#index 1693885
#* To each his own: personalized content selection based on text comprehensibility
#@ Chenhao Tan;Evgeniy Gabrilovich;Bo Pang
#t 2012
#c 2
#% 577224
#% 590523
#% 805877
#% 818221
#% 818259
#% 823348
#% 840924
#% 939396
#% 956552
#% 987211
#% 987221
#% 1055738
#% 1074071
#% 1166518
#% 1166525
#% 1190055
#% 1214623
#% 1227604
#% 1264737
#% 1482336
#% 1482451
#% 1536505
#% 1536512
#% 1641961
#% 1650569
#% 1693883
#! Imagine a physician and a patient doing a search on antibiotic resistance. Or a chess amateur and a grandmaster conducting a search on Alekhine's Defence. Although the topic is the same, arguably the two users in each case will satisfy their information needs with very different texts. Yet today search engines mostly adopt the one-size-fits-all solution, where personalization is restricted to topical preference. We found that users do not uniformly prefer simple texts, and that the text comprehensibility level should match the user's level of preparedness. Consequently, we propose to model the comprehensibility of texts as well as the users' reading proficiency in order to better explain how different users choose content for further exploration. We also model topic-specific reading proficiency, which allows us to better explain why a physician might choose to read sophisticated medical articles yet simple descriptions of SLR cameras. We explore different ways to build user profiles, and use collaborative filtering techniques to overcome data sparsity. We conducted experiments on large-scale datasets from a major Web search engine and a community question answering forum. Our findings confirm that explicitly modeling text comprehensibility can significantly improve content ranking (search results or answers, respectively).

#index 1693886
#* WebSets: extracting sets of entities from the web using unsupervised information extraction
#@ Bhavana Bharat Dalvi;William W. Cohen;Jamie Callan
#t 2012
#c 2
#% 754068
#% 756964
#% 815297
#% 956500
#% 1077150
#% 1127393
#% 1264744
#% 1264778
#% 1265135
#% 1270285
#% 1291356
#% 1328133
#% 1328353
#% 1338685
#% 1355026
#% 1481643
#% 1517882
#% 1523845
#% 1523913
#% 1536526
#! We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus. Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and concept-instance pairs obtained with Hearst patterns. In contrast, our method relies on a novel approach for clustering terms found in HTML tables, and then assigning concept names to these clusters using Hearst patterns. The method can be efficiently applied to a large corpus, and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs.

#index 1693887
#* Selecting actions for resource-bounded information extraction using reinforcement learning
#@ Pallika H. Kanani;Andrew K. McCallum
#t 2012
#c 2
#% 124691
#% 218098
#% 384911
#% 576214
#% 714167
#% 754068
#% 844399
#% 876046
#% 1083705
#% 1216922
#% 1290115
#% 1673023
#! Given a database with missing or uncertain content, our goal is to correct and fill the database by extracting specific information from a large corpus such as the Web, and to do so under resource limitations. We formulate the information gathering task as a series of choices among alternative, resource-consuming actions and use reinforcement learning to select the best action at each time step. We use temporal difference q-learning method to train the function that selects these actions, and compare it to an online, error-driven algorithm called SampleRank. We present a system that finds information such as email, job title and department affiliation for the faculty at our university, and show that the learning-based approach accomplishes this task efficiently under a limited action budget. Our evaluations show that we can obtain 92.4% of the final F1, by only using 14.3% of all possible actions.

#index 1693888
#* Online selection of diverse results
#@ Debmalya Panigrahi;Atish Das Sarma;Gagan Aggarwal;Andrew Tomkins
#t 2012
#c 2
#% 190611
#% 262112
#% 805841
#% 860672
#% 866683
#% 1166473
#% 1181244
#% 1190093
#% 1206662
#% 1207001
#% 1214650
#% 1312812
#% 1348342
#% 1400021
#% 1472964
#% 1482296
#% 1584774
#% 1598393
#% 1598494
#! The phenomenal growth in the volume of easily accessible information via various web-based services has made it essential for service providers to provide users with personalized representative summaries of such information. Further, online commercial services including social networking and micro-blogging websites, e-commerce portals, leisure and entertainment websites, etc. recommend interesting content to users that is simultaneously diverse on many different axes such as topic, geographic specificity, etc. The key algorithmic question in all these applications is the generation of a succinct, representative, and relevant summary from a large stream of data coming from a variety of sources. In this paper, we formally model this optimization problem, identify its key structural characteristics, and use these observations to design an extremely scalable and efficient algorithm. We analyze the algorithm using theoretical techniques to show that it always produces a nearly optimal solution. In addition, we perform large-scale experiments on both real-world and synthetically generated datasets, which confirm that our algorithm performs even better than its analytical guarantees in practice, and also outperforms other candidate algorithms for the problem by a wide margin.

#index 1693889
#* Overlapping clusters for distributed computation
#@ Reid Andersen;David F. Gleich;Vahab Mirrokni
#t 2012
#c 2
#% 121466
#% 274612
#% 280059
#% 296756
#% 322846
#% 492615
#% 656714
#% 720319
#% 785089
#% 805897
#% 840965
#% 841688
#% 898311
#% 1013696
#% 1036079
#% 1039687
#% 1055741
#% 1169663
#% 1211811
#% 1299149
#% 1404181
#% 1484141
#! Most graph decomposition procedures seek to partition a graph into disjoint sets of vertices. Motivated by applications of clustering in distributed computation, we describe a graph decomposition algorithm for the paradigm where the partitions intersect. This algorithm covers the vertex set with a collection of overlapping clusters. Each vertex in the graph is well-contained within some cluster in the collection. We then describe a framework for distributed computation across a collection of overlapping clusters and describe how this framework can be used in various algorithms based on the graph diffusion process. In particular, we focus on two illustrative examples: (i) the simulation of a randomly walking particle and (ii) the solution of a linear system, e.g. PageRank. Our simulation results for these two cases show a significant reduction in swapping between clusters in a random walk, a significant decrease in communication volume during a linear system solve in a geometric mesh, and some ability to reduce the communication volume during a linear system solve in an information network.

#index 1693890
#* Sponsored search auctions with conflict constraints
#@ Panagiotis Papadimitriou;Hector Garcia-Molina
#t 2012
#c 2
#% 818584
#% 1000451
#% 1171607
#% 1171610
#% 1190079
#% 1206663
#% 1253021
#% 1289299
#% 1399970
#% 1426676
#% 1560367
#! In sponsored search auctions advertisers compete for ad slots in the search engine results page, by bidding on keywords of interest. To improve advertiser expressiveness, we augment the bidding process with conflict constraints. With such constraints, advertisers can condition their bids on the non-appearance of certain undesired ads on the results page. We study the complexity of the allocation problem in these augmented SSA and we introduce an algorithm that can efficiently allocate the ad slots to advertisers. We evaluate the algorithm run time in simulated conflict scenarios and we study the implications of the conflict constraints on search engine revenue. Our results show that the allocation problem can be solved within few tens of milliseconds and that the adoption of conflict constraints can potentially increase search engine revenue.

#index 1693891
#* Post-click conversion modeling and analysis for non-guaranteed delivery display advertising
#@ Rómer Rosales;Haibin Cheng;Eren Manavoglu
#t 2012
#c 2
#% 115608
#% 757953
#% 818221
#% 956546
#% 1040854
#% 1040857
#% 1055694
#% 1055713
#% 1280759
#% 1292471
#% 1450842
#% 1450847
#% 1451160
#% 1587856
#! In on-line search and display advertising, the click-trough rate (CTR) has been traditionally a key measure of ad/campaign effectiveness. More recently, the market has gained interest in more direct measures of profitability, one early alternative is the conversion rate (CVR). CVRs measure the proportion of certain users who take a predefined, desirable action, such as a purchase, registration, download, etc.; as compared to simply page browsing. We provide a detailed analysis of conversion rates in the context of non-guaranteed delivery targeted advertising. In particular we focus on the post-click conversion (PCC) problem or the analysis of conversions after a user click on a referring ad. The key elements we study are the probability of a conversion given a click in a user/page context, P(conversion | click, context). We provide various fundamental properties of this process based on contextual information, formalize the problem of predicting PCC, and propose an approach for measuring attribute relevance when the underlying attribute distribution is non-stationary. We provide experimental analyses based on logged events from a large-scale advertising platform.

#index 1693892
#* Incorporating revisiting behaviors into click models
#@ Danqing Xu;Yiqun Liu;Min Zhang;Shaoping Ma;Liyun Ru
#t 2012
#c 2
#% 411762
#% 734915
#% 752177
#% 757953
#% 766472
#% 818221
#% 823348
#% 851306
#% 874715
#% 879565
#% 879567
#% 956546
#% 1035578
#% 1055713
#% 1074092
#% 1166517
#% 1190055
#% 1190056
#% 1214675
#% 1384755
#! Click-through behaviors are treated as invaluable sources of user feedback and they have been leveraged in several commercial search engines in recent years. However, estimating unbiased relevance is always a challenging task because of position bias. To solve this problem, many researchers have proposed a variety of assumptions to model click-through behaviors. Most of these models share a common examination hypothesis, which is that users examine search results from the top to the bottom. Nevertheless, this model cannot draw a complete picture of information-seeking behaviors. Many eye-tracking studies find that user interactions are not sequential but contain revisiting patterns. If a user clicks on a higher ranked document after having clicked on a lower-ranked one, we call this scenario a revisiting pattern, and we believe that the revisiting patterns are important signals regarding a user's click preferences. This paper incorporates revisiting behaviors into click models and introduces a novel click model named Temporal Hidden Click Model (THCM). This model dynamically models users' click behaviors with a temporal order. In our experiment, we collect over 115 million query sessions from a widely-used commercial search engine and then conduct a comparative analysis between our model and several state-of-the-art click models. The experimental results show that the THCM model achieves a significant improvement in the Normalized Discounted Cumulative Gain (NDCG), the click perplexity and click distributions metrics.

#index 1693893
#* A noise-aware click model for web search
#@ Weizhu Chen;Dong Wang;Yuchen Zhang;Zheng Chen;Adish Singla;Qiang Yang
#t 2012
#c 2
#% 411762
#% 720198
#% 766472
#% 879567
#% 956546
#% 1035578
#% 1166517
#% 1190055
#% 1190056
#% 1214675
#% 1227640
#% 1269878
#% 1355034
#% 1355048
#% 1399962
#% 1450842
#% 1450873
#% 1450885
#% 1451161
#% 1482222
#% 1482279
#% 1560356
#% 1560365
#% 1598363
#% 1606083
#% 1810385
#! Recent advances in click model have established it as an attractive approach to infer document relevance. Most of these advances consider the user click/skip behavior as binary events but neglect the context in which a click happens. We show that real click behavior in industrial search engines is often noisy and not always a good indication of relevance. For a considerable percentage of clicks, users select what turn out to be irrelevant documents and these clicks should not be directly used as evidence for relevance inference. Thus in this paper, we put forward an observation that the relevance indication degree of a click is not a constant, but can be differentiated by user preferences and the context in which the user makes her click decision. In particular, to interpret the click behavior discriminatingly, we propose a Noise-aware Click Model (NCM) by characterizing the noise degree of a click, which indicates the quality of the click for inferring relevance. Specifically, the lower the click noise is, the more important the click is in its role for relevance inference. To verify the necessity of explicitly accounting for the uninformative noise in a user click, we conducted experiments on a billion-scale dataset. Extensive experimental results demonstrate that as compared with two state-of-the-art click models in Web Search, NCM can better interpret user click behavior and achieve significant improvements in terms of both perplexity and NDCG.

#index 1693894
#* Personalized click model through collaborative filtering
#@ Si Shen;Botao Hu;Weizhu Chen;Qiang Yang
#t 2012
#c 2
#% 577224
#% 766472
#% 818221
#% 879567
#% 956546
#% 1035578
#% 1074092
#% 1166517
#% 1190055
#% 1190056
#% 1214675
#% 1260273
#% 1355024
#% 1355034
#% 1355048
#% 1450842
#% 1451160
#% 1482222
#% 1560356
#% 1606083
#! Click modeling aims to interpret the users' search click data in order to predict their clicking behavior. Existing models can well characterize the position bias of documents and snippets in relation to users' mainstream click behavior. Yet, current advances depict users' search actions only in a general setting by implicitly assuming that all users act in the same way, regardless of the fact that anyone, motivated with some individual interest, is more likely to click on a link than others. It is in light of this that we put forward a novel personalized click model to describe the user-oriented click preferences, which applies and extends matrix / tensor factorization from the view of collaborative filtering to connect users, queries and documents together. Our model serves as a generalized personalization framework that can be incorporated to the previously proposed click models and, in many cases, to their future extensions. Despite the sparsity of search click data, our personalized model demonstrates its advantage over the best click models previously discussed in the Web-search literature, supported by our large-scale experiments on a real dataset. A delightful bonus is the model's ability to gain insights into queries and documents through latent feature vectors, and hence to handle rare and even new query-document pairs much better than previous click models.

#index 1693895
#* Fair and balanced: learning to present news stories
#@ Amr Ahmed;Choon Hui Teo;S.V.N. Vishwanathan;Alex Smola
#t 2012
#c 2
#% 55490
#% 227736
#% 869534
#% 989613
#% 1035578
#% 1166473
#% 1190055
#% 1214650
#% 1260273
#% 1355034
#% 1385966
#% 1470696
#% 1598370
#! Relevance, diversity and personalization are key issues when presenting content which is apt to pique a user's interest. This is particularly true when presenting an engaging set of news stories. In this paper we propose an efficient algorithm for selecting a small subset of relevant articles from a streaming news corpus. It offers three key pieces of improvement over past work: 1) It is based on a detailed model of a user's viewing behavior which does not require explicit feedback. 2) We use the notion of submodularity to estimate the propensity of interacting with content. This improves over the classical context independent relevance ranking algorithms. Unlike existing methods, we learn the submodular function from the data. 3) We present an efficient online algorithm which can be adapted for personalization, story adaptation, and factorization models. Experiments show that our system yields a significant improvement over a retrieval system deployed in production.

#index 1693896
#* Extracting search-focused key n-grams for relevance ranking in web search
#@ Chen Wang;Keping Bi;Yunhua Hu;Hang Li;Guihong Cao
#t 2012
#c 2
#% 169781
#% 262096
#% 280851
#% 281480
#% 309095
#% 321635
#% 340899
#% 420487
#% 577224
#% 818233
#% 818262
#% 823348
#% 855293
#% 857180
#% 879567
#% 881477
#% 978382
#% 1206941
#% 1227621
#% 1227720
#% 1268491
#% 1273825
#% 1292502
#% 1355036
#% 1450887
#% 1598394
#% 1604467
#! In web search, relevance ranking of popular pages is relatively easy, because of the inclusion of strong signals such as anchor text and search log data. In contrast, with less popular pages, relevance ranking becomes very challenging due to a lack of information. In this paper the former is referred to as head pages, and the latter tail pages. We address the challenge by learning a model that can extract search-focused key n-grams from web pages, and using the key n-grams for searches of the pages, particularly, the tail pages. To the best of our knowledge, this problem has not been previously studied. Our approach has four characteristics. First, key n-grams are search-focused in the sense that they are defined as those which can compose "good queries" for searching the page. Second, key n-grams are learned in a relative sense using learning to rank techniques. Third, key n-grams are learned using search log data, such that the characteristics of key n-grams in the search log data, particularly in the heads; can be applied to the other data, particularly to the tails. Fourth, the extracted key n-grams are used as features of the relevance ranking model also trained with learning to rank techniques. Experiments validate the effectiveness of the proposed approach with large-scale web search datasets. The results show that our approach can significantly improve relevance ranking performance on both heads and tails; and particularly tails, compared with baseline approaches. Characteristics of our approach have also been fully investigated through comprehensive experiments.

#index 1693897
#* Query suggestion by constructing term-transition graphs
#@ Yang Song;Dengyong Zhou;Li-wei He
#t 2012
#c 2
#% 262084
#% 342707
#% 348173
#% 823348
#% 838532
#% 869501
#% 987222
#% 1074052
#% 1083721
#% 1130854
#% 1130855
#% 1130868
#% 1227578
#% 1227619
#% 1292530
#% 1355020
#% 1355032
#% 1400023
#% 1417245
#! Query suggestion is an interactive approach for search engines to better understand users information need. In this paper, we propose a novel query suggestion framework which leverages user re-query feedbacks from search engine logs. Specifically, we mined user query reformulation activities where the user only modifies part of the query by (1) adding terms after the query, (2) deleting terms within the query, or (3) modifying terms to new terms. We build a term-transition graph based on the mined data. Two models are proposed which address topic-level and term-level query suggestions, respectively. In the first topic-based unsupervised Pagerank model, we perform random walk on each of the topic-based term-transition graph and calculate the Pagerank for each term within a topic. Given a new query, we suggest relevant queries based on its topic distribution and term-transition probability within each topic. Our second model resembles the supervised learning-to-rank (LTR) framework, in which term modifications are treated as documents so that each query reformulation is treated as a training instance. A rich set of features are constructed for each (query, document) pair from Pagerank, Wikipedia, N-gram, ODP and so on. This supervised model is capable of suggesting new queries on a term level which addresses the limitation of previous methods. Experiments are conducted on a large data set from a commercial search engine. By comparing the with state-of-the-art query suggestion methods [4, 2], our proposals exhibit significant performance increase for all categories of queries.

#index 1693898
#* Language models for keyword search over data graphs
#@ Yosi Mass;Yehoshua Sagiv
#t 2012
#c 2
#% 660011
#% 750863
#% 824693
#% 875017
#% 875061
#% 960243
#% 960259
#% 993987
#% 1015325
#% 1063539
#% 1292565
#% 1426537
#% 1426613
#% 1479590
#% 1482251
#! In keyword search over data graphs, an answer is a non-redundant subtree that includes the given keywords. This paper focuses on improving the effectiveness of that type of search. A novel approach that combines language models with structural relevance is described. The proposed approach consists of three steps. First, language models are used to assign dynamic, query-dependent weights to the graph. Those weights complement static weights that are pre-assigned to the graph. Second, an existing algorithm returns candidate answers based on their weights. Third, the candidate answers are re-ranked by creating a language model for each one. The effectiveness of the proposed approach is verified on a benchmark of three datasets: IMDB, Wikipedia and Mondial. The proposed approach outperforms all existing systems on the three datasets, which is a testament to its robustness. It is also shown that the effectiveness can be further improved by augmenting keyword queries with very basic knowledge about the structure.

#index 1693899
#* Large-scale analysis of individual and task differences in search result page examination strategies
#@ Georg Buscher;Ryen W. White;Susan Dumais;Jeff Huang
#t 2012
#c 2
#% 187999
#% 319106
#% 803833
#% 805200
#% 818221
#% 860570
#% 874715
#% 954948
#% 954949
#% 955711
#% 956495
#% 956552
#% 987224
#% 1043044
#% 1048694
#% 1074071
#% 1074148
#% 1130852
#% 1154062
#% 1166518
#% 1186546
#% 1190055
#% 1227582
#% 1384641
#% 1434128
#% 1450834
#% 1455264
#% 1536511
#% 1573487
#% 1598370
#% 1678005
#! Understanding the impact of individual and task differences on search result page examination strategies is important in developing improved search engines. Characterizing these effects using query and click data alone is common but insufficient since they provide an incomplete picture of result examination behavior. Cursor- or gaze-tracking studies reveal richer interaction patterns but are often done in small-scale laboratory settings. In this paper we leverage large-scale rich behavioral log data in a naturalistic setting. We examine queries, clicks, cursor movements, scrolling, and text highlighting for millions of queries on the Bing commercial search engine to better understand the impact of user, task, and user-task interactions on user behavior on search result pages (SERPs). By clustering users based on cursor features, we identify individual, task, and user-task differences in how users examine results which are similar to those observed in small-scale studies. Our findings have implications for developing search support for behaviorally-similar searcher cohorts, modeling search behavior, and designing search systems that leverage implicit feedback.

#index 1693900
#* Sequence clustering and labeling for unsupervised query intent discovery
#@ Jackie Chi Kit Cheung;Xiao Li
#t 2012
#c 2
#% 137711
#% 296738
#% 310567
#% 330617
#% 451355
#% 466668
#% 756964
#% 815297
#% 939600
#% 939601
#% 999292
#% 1077150
#% 1227610
#% 1227648
#% 1250378
#% 1338552
#% 1399933
#% 1400017
#% 1426566
#% 1471314
#% 1476276
#% 1560359
#! One popular form of semantic search observed in several modern search engines is to recognize query patterns that trigger instant answers or domain-specific search, producing semantically enriched search results. This often requires understanding the query intent in addition to the meaning of the query terms in order to access structured data sources. A major challenge in intent understanding is to construct a domain-dependent schema and to annotate search queries based on such a schema, a process that to date has required much manual annotation effort. We present an unsupervised method for clustering queries with similar intent and for producing a pattern consisting of a sequence of semantic concepts and/or lexical items for each intent. Furthermore, we leverage the discovered intent patterns to automatically annotate a large number of queries beyond those used in clustering. We evaluated our method on 10 selected domains, discovering over 1400 intent patterns and automatically annotating 125K (and potentially many more) queries. We found that over 90% of patterns and 80% of instance annotations tested are judged to be correct by a majority of annotators.

#index 1693901
#* IR system evaluation using nugget-based test collections
#@ Virgil Pavlu;Shahzad Rajput;Peter B. Golbus;Javed A. Aslam
#t 2012
#c 2
#% 255137
#% 262102
#% 312689
#% 340890
#% 342727
#% 544011
#% 766407
#% 879598
#% 879641
#% 939970
#% 940038
#% 940039
#% 948378
#% 987196
#% 1074132
#% 1074133
#% 1074134
#% 1085172
#% 1107641
#% 1130811
#% 1263584
#% 1560395
#% 1622375
#% 1642150
#! The development of information retrieval systems such as search engines relies on good test collections, including assessments of retrieved content. The widely employed Cranfield paradigm dictates that the information relevant to a topic be encoded at the level of documents, therefore requiring effectively complete document relevance assessments. As this is no longer practical for modern corpora, numerous problems arise, including scalability, reusability, and applicability. We propose a new method for relevance assessment based on relevant information, not relevant documents. Once the relevant 'nuggets' are collected, our matching method can assess any document for relevance with high accuracy, and so any retrieved list of documents can be assessed for performance. In this paper we analyze the performance of the matching function by looking at specific cases and by comparing with other methods. We then show how these inferred relevance assessments can be used to perform IR system evaluation, and we discuss in particular reusability and scalability. Our main contribution is a methodology for producing test collections that are highly accurate, more complete, scalable, reusable, and can be generated with similar amounts of effort as existing methods, with great potential for future applications.

#index 1693902
#* Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries
#@ Alexander Kotov;ChengXiang Zhai
#t 2012
#c 2
#% 25942
#% 54413
#% 169729
#% 218978
#% 240212
#% 252328
#% 280847
#% 298183
#% 340899
#% 342707
#% 375017
#% 413579
#% 533966
#% 766406
#% 766440
#% 783633
#% 818240
#% 838530
#% 838532
#% 879584
#% 879666
#% 983820
#% 987333
#% 1195859
#% 1275012
#% 1399987
#% 1410894
#% 1641932
#% 1682057
#! Query expansion is an important and commonly used technique for improving Web search results. Existing methods for query expansion have mostly relied on global or local analysis of document collection, click-through data, or simple ontologies such as WordNet. In this paper, we present the results of a systematic study of the methods leveraging the ConceptNet knowledge base, an emerging new Web resource, for query expansion. Specifically, we focus on the methods leveraging ConceptNet to improve the search results for poorly performing (or difficult) queries. Unlike other lexico-semantic resources, such as WordNet and Wikipedia, which have been extensively studied in the past, ConceptNet features a graph-based representation model of commonsense knowledge, in which the terms are conceptually related through rich relational ontology. Such representation structure enables complex, multi-step inferences between the concepts, which can be applied to query expansion. We first demonstrate through simulation experiments that expanding queries with the related concepts from ConceptNet has great potential for improving the search results for difficult queries. We then propose and study several supervised and unsupervised methods for selecting the concepts from ConceptNet for automatic query expansion. The experimental results on multiple data sets indicate that the proposed methods can effectively leverage ConceptNet to improve the retrieval performance of difficult queries both when used in isolation as well as in combination with pseudo-relevance feedback.

#index 1693903
#* Domain bias in web search
#@ Samuel Ieong;Nina Mishra;Eldar Sadikov;Li Zhang
#t 2012
#c 2
#% 577224
#% 754058
#% 754060
#% 810054
#% 818221
#% 824716
#% 879565
#% 946521
#% 954949
#% 956546
#% 1016177
#% 1035574
#% 1035578
#% 1074092
#% 1166522
#% 1190055
#% 1190056
#% 1399989
#% 1400034
#% 1451161
#! This paper uncovers a new phenomenon in web search that we call domain bias --- a user's propensity to believe that a page is more relevant just because it comes from a particular domain. We provide evidence of the existence of domain bias in click activity as well as in human judgments via a comprehensive collection of experiments. We begin by studying the difference between domains that a search engine surfaces and that users click. Surprisingly, we find that despite changes in the overall distribution of surfaced domains, there has not been a comparable shift in the distribution of clicked domains. Users seem to have learned the landscape of the internet and their click behavior has thus become more predictable over time. Next, we run a blind domain test, akin to a Pepsi/Coke taste test, to determine whether domains can shift a user's opinion of which page is more relevant. We find that domains can actually flip a user's preference about 25% of the time. Finally, we demonstrate the existence of systematic domain preferences, even after factoring out confounding issues such as position bias and relevance, two factors that have been used extensively in past work to explain user behavior. The existence of domain bias has numerous consequences including, for example, the importance of discounting click activity from reputable domains.

#index 1693904
#* Optimized top-k processing with global page scores on block-max indexes
#@ Dongdong Shan;Shuai Ding;Jing He;Hongfei Yan;Xiaoming Li
#t 2012
#c 2
#% 198335
#% 212665
#% 268079
#% 340886
#% 340887
#% 570319
#% 643566
#% 730065
#% 818229
#% 818255
#% 867054
#% 879611
#% 879651
#% 907504
#% 987214
#% 987240
#% 1015265
#% 1019139
#% 1055710
#% 1075132
#% 1130876
#% 1190095
#% 1355053
#% 1355057
#% 1392439
#% 1482301
#% 1587386
#% 1594565
#% 1598344
#% 1598433
#% 1598490
#% 1682445
#! Large web search engines are facing formidable performance challenges because they have to process thousands of queries per second on tens of billions of documents, within interactive response time. Among many others, Top-k query processing (also called early termination or dynamic pruning) is an important class of optimization techniques that can improve the search efficiency and achieve faster query processing by avoiding the scoring of documents that are unlikely to be in the top results. One recent technique is using Block-Max index. In the Block-Max index, the posting lists are organized as blocks and the maximum score for each block is stored to improve the query efficiency. Although query processing speedup is achieved with Block-Max index, the ranking function for the Top-k results is the term-based approach. It is well known that documents' static scores are also important for a good ranking function. In this paper, we show that the performance of the state-of-the-art algorithms with the Block-Max index is degraded when the static score is added in the ranking function. Then we study efficient techniques for Top-k query processing in the case where a page's static score is given, such as PageRank, in addition to the term-based approach. In particular, we propose a set of new algorithms based on the WAND and MaxScore with Block-Max index using local score, which outperform the existing ones. Then we propose new techniques to estimate a better score upper bound for each block. We also study the search efficiency on different index structures where the document identifiers are assigned by URL sorting or by static document scores. Experiments on TREC GOV2 and ClueWeb09B show that considerable performance gains are achieved.

#index 1693905
#* Probabilistic models for personalizing web search
#@ David Sontag;Kevyn Collins-Thompson;Paul N. Bennett;Ryen W. White;Susan Dumais;Bodo Billerbeck
#t 2012
#c 2
#% 399057
#% 642975
#% 729625
#% 771571
#% 818207
#% 818224
#% 818259
#% 832349
#% 881540
#% 956495
#% 956552
#% 987222
#% 1074071
#% 1083721
#% 1214757
#% 1227577
#% 1227621
#% 1357833
#% 1399944
#% 1482279
#% 1536505
#! We present a new approach for personalizing Web search results to a specific user. Ranking functions for Web search engines are typically trained by machine learning algorithms using either direct human relevance judgments or indirect judgments obtained from click-through data from millions of users. The rankings are thus optimized to this generic population of users, not to any specific user. We propose a generative model of relevance which can be used to infer the relevance of a document to a specific user for a search query. The user-specific parameters of this generative model constitute a compact user profile. We show how to learn these profiles from a user's long-term search history. Our algorithm for computing the personalized ranking is simple and has little computational overhead. We evaluate our personalization approach using historical search data from thousands of users of a major Web search engine. Our findings demonstrate gains in retrieval performance for queries with high ambiguity, with particularly large improvements for acronym queries.

#index 1693906
#* Effective query formulation with multiple information sources
#@ Michael Bendersky;Donald Metzler;W. Bruce Croft
#t 2012
#c 2
#% 94368
#% 144034
#% 169781
#% 218978
#% 262096
#% 397127
#% 411760
#% 750863
#% 783474
#% 818262
#% 879584
#% 976952
#% 987231
#% 987232
#% 987272
#% 987333
#% 1019124
#% 1074081
#% 1074098
#% 1074112
#% 1074133
#% 1166473
#% 1195837
#% 1227584
#% 1227614
#% 1227636
#% 1355019
#% 1400021
#% 1415737
#% 1450848
#% 1450900
#% 1450901
#% 1482186
#% 1482203
#% 1526569
#% 1598374
#% 1598393
#% 1598394
#% 1604467
#% 1715627
#! Most standard information retrieval models use a single source of information (e.g., the retrieval corpus) for query formulation tasks such as term and phrase weighting and query expansion. In contrast, in this paper, we present a unified framework that automatically optimizes the combination of information sources used for effective query formulation. The proposed framework produces fully weighted and expanded queries that are both more effective and more compact than those produced by the current state-of-the-art query expansion and weighting methods. We conduct an empirical evaluation of our framework for both newswire and web corpora. In all cases, our combination of multiple information sources for query formulation is found to be more effective than using any single source. The proposed query formulations are especially advantageous for large scale web corpora, where they also reduce the number of terms required for effective query expansion, and improve the diversity of the retrieved results.

#index 1693907
#* Learning to rank with multi-aspect relevance for vertical search
#@ Changsung Kang;Xuanhui Wang;Yi Chang;Belle Tseng
#t 2012
#c 2
#% 169781
#% 262096
#% 321635
#% 330769
#% 340948
#% 411762
#% 452641
#% 577224
#% 757953
#% 818221
#% 838412
#% 840846
#% 946521
#% 956542
#% 1074094
#% 1190103
#% 1227616
#% 1338581
#% 1355018
#% 1450874
#% 1450915
#% 1450953
#% 1550750
#% 1560391
#! Many vertical search tasks such as local search focus on specific domains. The meaning of relevance in these verticals is domain-specific and usually consists of multiple well-defined aspects (e.g., text matching and distance in local search). Thus the overall relevance between a query and a document is a tradeoff between multiple relevance aspects. Such a tradeoff can vary for different types of queries or in different contexts. In this paper, we explore these vertical-specific aspects in the learning to rank setting. We propose a novel formulation in which the relevance between a query and a document is assessed with respect to each aspect, forming the multi-aspect relevance. In order to compute a ranking function, we study two types of learning-based approaches to estimate the tradeoff between these relevance aspects: a label aggregation method and a model aggregation method. Since there are only a few aspects, a minimal amount of training data is needed to learn the tradeoff. We conduct both offline and online test experiments on a local search engine and the experimental results show that our proposed multi-aspect relevance formulation is very promising. The two types of aggregation methods perform more effectively than a set of baseline methods including a conventional learning to rank method.

#index 1693908
#* Beyond ten blue links: enabling user click modeling in federated web search
#@ Danqi Chen;Weizhu Chen;Haixun Wang;Zheng Chen;Qiang Yang
#t 2012
#c 2
#% 766472
#% 956546
#% 1035578
#% 1074092
#% 1074093
#% 1150174
#% 1166517
#% 1166523
#% 1190055
#% 1190056
#% 1214675
#% 1227616
#% 1227617
#% 1355034
#% 1355048
#% 1450842
#% 1450873
#% 1450915
#% 1451161
#% 1482230
#% 1536576
#% 1560356
#% 1587348
#% 1606083
#% 1641937
#! Click models have been positioned as an effective approach to interpret user click behavior in search engines. Existing click models mostly focus on traditional Web search that considers only ten homogeneous Web HTML documents that appear on the first search-result page. However, in modern commercial search engines, more and more Web search results are federated from multiple sources and contain non-HTML results returned by other heterogeneous vertical engines, such as video or image search engines. In this paper, we study user click behavior in federated search. We observed that user click behavior in federated search is highly different from that in traditional Web search, making it difficult to interpret using existing click models. In response, we propose a novel federated click model (FCM) to interpret user click behavior in federated search. In particular, we take into considerations two new biases in FCM. The first comes from the observation that users tend to be attracted by vertical results and their visual attention on them may increase the examination probability of other nearby web results. The other illustrates that user click behavior on vertical results may lead to more clues of search relevance due to their presentation style in federated search. With these biases and an effective model to correct them, FCM is more accurate in characterizing user click behavior in federated search. Our extensive experimental results show that FCM can outperform other click models in interpreting user click behavior in federated search and achieve significant improvements in terms of both perplexity and log-likelihood.

#index 1693909
#* Finding the right consumer: optimizing for conversion in display advertising campaigns
#@ Yandong Liu;Sandeep Pandey;Deepak Agarwal;Vanja Josifovski
#t 2012
#c 2
#% 283169
#% 397155
#% 754126
#% 881544
#% 1114743
#% 1190081
#% 1214623
#% 1214642
#% 1214692
#% 1356185
#% 1399936
#% 1400100
#% 1405661
#% 1431624
#% 1450847
#% 1482282
#% 1535253
#! The ultimate goal of advertisers are conversions representing desired user actions on the advertisers' websites in the form of purchases and product information request. In this paper we address the problem of finding the right audience for display campaigns by finding the users that are most likely to convert. This challenging problem is at the heart of display campaign optimization and has to deal with several issues such as very small percentage of converters in the general population, high-dimensional representation of the user profiles, large churning rate of users and advertisers. To overcome these difficulties, in our approach we use two sources of information: a seed set of users that have converted for a campaign in the past; and a description of the campaign based on the advertiser's website. We explore the importance of the information provided by each of these two sources in a principled manner and then combine them to propose models for predicting converters. In particular, we show how seed set can be used to capture the campaign-specific targeting constraints, while the campaign metadata allows to share targeting knowledge across campaigns. We give methods for learning these models and perform experiments on real-world advertising campaigns. Our findings show that the seed set and the campaign metadata are complimentary to each other and both sources provide valuable information for conversion optimization.

#index 1693910
#* Fast top-k retrieval for model based recommendation
#@ Deepak Agarwal;Maxim Gurevich
#t 2012
#c 2
#% 198335
#% 643566
#% 730065
#% 956521
#% 1117691
#% 1127449
#% 1190099
#% 1190124
#% 1211829
#% 1355057
#% 1476448
#! A crucial task in many recommender problems like computational advertising, content optimization, and others is to retrieve a small set of items by scoring a large item inventory through some elaborate statistical/machine-learned model. This is challenging since the retrieval has to be fast (few milliseconds) to load the page quickly. Fast retrieval is well studied in the information retrieval (IR) literature, especially in the context of document retrieval for queries. When queries and documents have sparse representation and relevance is measured through cosine similarity (or some variant thereof), one could build highly efficient retrieval algorithms that scale gracefully to increasing item inventory. The key components exploited by such algorithms is sparse query-document representation and the special form of the relevance function. Many machine-learned models used in modern recommender problems do not satisfy these properties and since brute force evaluation is not an option with large item inventory, heuristics that filter out some items are often employed to reduce model computations at runtime. In this paper, we take a two-stage approach where the first stage retrieves top-K items using our approximate procedures and the second stage selects the desired top-k using brute force model evaluation on the K retrieved items. The main idea of our approach is to reduce the first stage to a standard IR problem, where each item is represented by a sparse feature vector (a.k.a. the vector-space representation) and the query-item relevance score is given by vector dot product. The sparse item representation is learnt to closely approximate the original machine-learned score by using retrospective data. Such a reduction allows leveraging extensive work in IR that resulted in highly efficient retrieval systems. Our approach is model-agnostic, relying only on data generated from the machine-learned model. We obtain significant improvements in the computational cost vs. accuracy tradeoff compared to several baselines in our empirical evaluation on both synthetic models and on a click-through (CTR) model used in online advertising.

#index 1693911
#* Relational click prediction for sponsored search
#@ Chenyan Xiong;Taifeng Wang;Wenkui Ding;Yidong Shen;Tie-Yan Liu
#t 2012
#c 2
#% 642975
#% 956546
#% 1055687
#% 1055694
#% 1166473
#% 1171607
#% 1190057
#% 1355048
#% 1355051
#% 1399962
#% 1399970
#% 1400011
#% 1450834
#% 1450842
#% 1536552
#% 1587856
#! This paper is concerned with the prediction of clicking an ad in sponsored search. The accurate prediction of user's click on an ad plays an important role in sponsored search, because it is widely used in both ranking and pricing of the ads. Previous work on click prediction usually takes a single ad as input, and ignores its relationship to the other ads shown in the same page. This independence assumption here, however, might not be valid in the real scenario. In this paper, we first perform an analysis on this issue by looking at the click-through rates (CTR) of the same ad, in the same position and for the same query, but surrounded by different ads. We found that in most cases the CTR varies largely, which suggests that the relationship between ads is really an important factor in predicting click probability. Furthermore, our investigation shows that the more similar the surrounding ads are to an ad, the lower the CTR of the ad is. Based on this observation, we design a continuous conditional random fields (CRF) based model for click prediction, which considers both the features of an ad and its similarity to the surrounding ads. We show that the model can be effectively learned using maximum likelihood estimation, and can also be efficiently inferred due to its closed form solution. Our experimental results on the click-through log from a commercial search engine show that the proposed model can predict clicks more accurately than previous independent models. To our best knowledge this is the first work that predicts ad clicks by considering the relationship between ads.

#index 1693912
#* "I loan because...": understanding motivations for pro-social lending
#@ Yang Liu;Roy Chen;Yan Chen;Qiaozhu Mei;Suzy Salib
#t 2012
#c 2
#% 144034
#% 197394
#% 288211
#% 466263
#% 590523
#% 592108
#% 765519
#% 854646
#% 998622
#% 1035587
#% 1043040
#% 1074093
#% 1127964
#% 1130878
#% 1264955
#% 1299754
#% 1301383
#% 1450992
#% 1558464

#index 1693913
#* Correlating financial time series with micro-blogging activity
#@ Eduardo J. Ruiz;Vagelis Hristidis;Carlos Castillo;Aristides Gionis;Alejandro Jaimes
#t 2012
#c 2
#% 316548
#% 577620
#% 786841
#% 835345
#% 987335
#% 989640
#% 1016176
#% 1040837
#% 1065403
#% 1080077
#% 1083700
#% 1134178
#% 1355042
#% 1394202
#% 1399992
#% 1478118
#% 1560422
#% 1583594
#! We study the problem of correlating micro-blogging activity with stock-market events, defined as changes in the price and traded volume of stocks. Specifically, we collect messages related to a number of companies, and we search for correlations between stock-market events for those companies and features extracted from the micro-blogging messages. The features we extract can be categorized in two groups. Features in the first group measure the overall activity in the micro-blogging platform, such as number of posts, number of re-posts, and so on. Features in the second group measure properties of an induced interaction graph, for instance, the number of connected components, statistics on the degree distribution, and other graph-based properties. We present detailed experimental results measuring the correlation of the stock market events with these features, using Twitter as a data source. Our results show that the most correlated features are the number of connected components and the number of nodes of the interaction graph. The correlation is stronger with the traded volume than with the price of the stock. However, by using a simulator we show that even relatively small correlations between price and micro-blogging features can be exploited to drive a stock trading strategy that outperforms other baseline strategies.

#index 1693914
#* Harmony and dissonance: organizing the people's voices on political controversies
#@ Rawia Awadallah;Maya Ramanath;Gerhard Weikum
#t 2012
#c 2
#% 274612
#% 301241
#% 504443
#% 817472
#% 956564
#% 1127964
#% 1166534
#% 1166537
#% 1227758
#% 1261563
#% 1450945
#% 1484323
#! The wikileaks documents about the death of Osama Bin Laden and the debates about the economic crisis in Greece and other European countries are some of the controversial topics being played on the news everyday. Each of these topics has many different aspects, and there is no absolute, simple truth in answering questions such as: should the EU guarantee the financial stability of each member country, or should the countries themselves be solely responsible? To understand the landscape of opinions, it would be helpful to know which politician or other stakeholder takes which position - support or opposition - on these aspects of controversial topics.

#index 1693915
#* Identifying content for planned events across social media sites
#@ Hila Becker;Dan Iter;Mor Naaman;Luis Gravano
#t 2012
#c 2
#% 309131
#% 350859
#% 838546
#% 987219
#% 1035587
#% 1055773
#% 1077150
#% 1181094
#% 1206746
#% 1209065
#% 1298864
#% 1332406
#% 1355045
#% 1399992
#% 1400018
#% 1470583
#% 1563712
#% 1591968
#! User-contributed Web data contains rich and diverse information about a variety of events in the physical world, such as shows, festivals, conferences and more. This information ranges from known event features (e.g., title, time, location) posted on event aggregation platforms (e.g., Last.fm events, EventBrite, Facebook events) to discussions and reactions related to events shared on different social media sites (e.g., Twitter, YouTube, Flickr). In this paper, we focus on the challenge of automatically identifying user-contributed content for events that are planned and, therefore, known in advance, across different social media sites. We mine event aggregation platforms to extract event features, which are often noisy or missing. We use these features to develop query formulation strategies for retrieving content associated with an event on different social media sites. Further, we explore ways in which event content identified on one social media site can be used to retrieve additional relevant event content on other social media sites. We apply our strategies to a large set of user-contributed events, and analyze their effectiveness in retrieving relevant event content from Twitter, YouTube, and Flickr.

#index 1693916
#* Daily deals: prediction, social diffusion, and reputational ramifications
#@ John W. Byers;Michael Mitzenmacher;Georgios Zervas
#t 2012
#c 2
#% 729923
#% 739687
#% 937549
#% 995868
#% 1126251
#% 1183359
#% 1246431
#% 1399992
#% 1669913
#! Daily deal sites have become the latest Internet sensation, providing discounted offers to customers for restaurants, ticketed events, services, and other items. We begin by undertaking a study of the economics of daily deals on the web, based on a dataset we compiled by monitoring Groupon and LivingSocial sales in 20 large cities over several months. We use this dataset to characterize deal purchases; glean insights about operational strategies of these firms; and evaluate customers' sensitivity to factors such as price, deal scheduling, and limited inventory. We then marry our daily deals dataset with additional datasets we compiled from Facebook and Yelp users to study the interplay between social networks and daily deal sites. First, by studying user activity on Facebook while a deal is running, we provide evidence that daily deal sites benefit from significant word-of-mouth effects during sales events, consistent with results predicted by cascade models. Second, we consider the effects of daily deals on the longer-term reputation of merchants, based on their Yelp reviews before and after they run a daily deal. Our analysis shows that while the number of reviews increases significantly due to daily deals, average rating scores from reviewers who mention daily deals are 10% lower than scores of their peers on average.

#index 1693917
#* On clustering heterogeneous social media objects with outlier links
#@ Guo-Jun Qi;Charu C. Aggarwal;Thomas S. Huang
#t 2012
#c 2
#% 36672
#% 118771
#% 232651
#% 722904
#% 775746
#% 780874
#% 824711
#% 839851
#% 881514
#% 987253
#% 989586
#% 1055703
#% 1190241
#% 1214695
#% 1214714
#% 1328169
#% 1855804
#% 1855967
#! The clustering of social media objects provides intrinsic understanding of the similarity relationships between documents, images, and their contextual sources. Both content and link structure provide important cues for an effective clustering algorithm of the underlying objects. While link information provides useful hints for improving the clustering process, it also contains a significant amount of noisy information. Therefore, a robust clustering algorithm is required to reduce the impact of noisy links. In order to address the aforementioned problems, we propose heterogeneous random fields to model the structure and content of social media networks. We design a probability measure on the social media networks which output a configuration of clusters that are consistent with both content and link structure. Furthermore, noisy links can also be detected, and their impact on the clustering algorithm can be significantly reduced. We conduct experiments on a real social media network and show the advantage of the method over other state-of-the-art algorithms.

#index 1693918
#* Adding semantics to microblog posts
#@ Edgar Meij;Wouter Weerkamp;Maarten de Rijke
#t 2012
#c 2
#% 400847
#% 577318
#% 987260
#% 1019082
#% 1077150
#% 1130858
#% 1333455
#% 1379671
#% 1399992
#% 1429423
#% 1433131
#% 1482395
#% 1517986
#% 1536537
#% 1536561
#% 1540321
#% 1587369
#% 1591965
#% 1591968
#% 1603813
#% 1624266
#% 1642135
#% 1653172
#% 1697479
#% 1775638
#! Microblogs have become an important source of information for the purpose of marketing, intelligence, and reputation management. Streams of microblogs are of great value because of their direct and real-time nature. Determining what an individual microblog post is about, however, can be non-trivial because of creative language usage, the highly contextualized and informal nature of microblog posts, and the limited length of this form of communication. We propose a solution to the problem of determining what a microblog post is about through semantic linking: we add semantics to posts by automatically identifying concepts that are semantically related to it and generating links to the corresponding Wikipedia articles. The identified concepts can subsequently be used for, e.g., social media mining, thereby reducing the need for manual inspection and selection. Using a purpose-built test collection of tweets, we show that recently proposed approaches for semantic linking do not perform well, mainly due to the idiosyncratic nature of microblog posts. We propose a novel method based on machine learning with a set of innovative features and show that it is able to achieve significant improvements over all other methods, especially in terms of precision.

#index 1693919
#* Exploring social influence via posterior effect of word-of-mouth recommendations
#@ Junming Huang;Xue-Qi Cheng;Hua-Wei Shen;Tao Zhou;Xiaolong Jin
#t 2012
#c 2
#% 342596
#% 577217
#% 729923
#% 739578
#% 803395
#% 879628
#% 949164
#% 989613
#% 1035589
#% 1083624
#% 1083641
#% 1083665
#% 1214702
#% 1355040
#% 1473316
#% 1536509
#% 1536554
#% 1560424
#% 1598352
#% 1603196
#! Word-of-mouth has proven an effective strategy for promoting products through social relations. Particularly, existing studies have convincingly demonstrated that word-of-mouth recommendations can boost users' prior expectation and hence encourage them to adopt a certain innovation, such as buying a book or watching a movie. However, less attention has been paid to studying the posterior effect of word-of-mouth recommendations, i.e., whether or not word-of-mouth recommendations can influence users' posterior evaluation on the products or services recommended to them, the answer to which is critical to estimating user satisfaction when proposing a word-of-mouth marketing strategy. In order to fill this gap, in this paper we empirically study the above issue and verify that word-of-mouth recommendations are strongly associated with users' posterior evaluation. Through elaborately designed statistical hypothesis tests we prove the causality that word-of-mouth recommendations directly prompt the posterior evaluation of receivers. Finally, we propose a method for investigating users' social influence, namely, their ability to affect followers' posterior evaluation via word-of-mouth recommendations, by examining the number of their followers and their sensitivity of discovering good items. The experimental results on real datasets show that our method can successfully identify 78% influential friends with strong social influence.

#index 1693920
#* Find me opinion sources in blogosphere: a unified framework for opinionated blog feed retrieval
#@ Xueke Xu;Songbo Tan;Yue Liu;Xueqi Cheng;Zheng Lin;Jiafeng Guo
#t 2012
#c 2
#% 342707
#% 750863
#% 769967
#% 838521
#% 956510
#% 1019145
#% 1074086
#% 1074094
#% 1074102
#% 1074171
#% 1130913
#% 1130915
#% 1166507
#% 1195856
#% 1195902
#% 1280262
#% 1292562
#% 1344777
#% 1450879
#% 1457110
#% 1560223
#% 1710578
#! This paper aims to find blog feeds having a principal inclination towards making opinionated comments on the given topic, so that we can subscribe to them to track influential and interesting opinions in the blogosphere. One major challenge is assigning topic-related opinion scores to blog feeds, which is embodied in two aspects. Firstly, we should identify whether the blog feed has a principal on-topic opinionated inclination. This inclination should be collectively revealed by all posts of the feed. We should fully consider evidences from all the posts of the feed to identify salient information among many posts of the feed. Secondly, we should capture topic-related opinions in the blog feed while ignoring irrelevant opinions. In this paper, we propose a unified framework for opinionated blog feed retrieval, which combines topic relevance and opinion scores with a generative model. Furthermore, we propose a language modeling approach to estimating opinion scores that is seamlessly integrated into the framework, where two language models, Topic-specific Opinion Model (TOM) and Topic-biased Feed Model (TFM), work collectively to reflect whether the blog feed shows a principal on-topic opinionated inclination. To estimate TFM, we propose a topic-biased random walk to exploit both content and structural information to capture topic-biased salient information in the feed. As for TOM estimation, we propose to use a generative mixture model with prior guidance to effectively capture topic-specific opinion expressing language usage. The conducted experiments in the context of the TREC 2009-2010 Blog Track show the effectiveness of our proposed approaches.

#index 1693921
#* Understanding cyclic trends in social choices
#@ Anish Das Sarma;Sreenivas Gollapudi;Rina Panigrahy;Li Zhang
#t 2012
#c 2
#% 574104
#% 963337
#% 1214671
#% 1425621
#% 1669913
#! Motivated by trends in popularity of products, we present a formal model for studying trends in our choice of products in terms of three parameters: (1) their innate utility; (2) individual boredom associated with repeated usage of an item; and (3) social influences associated with the preferences from other people. Different from previous work, in this paper we introduce boredom to explain the cyclic pattern in individual and social choices. We formally model boredom and show that a rational individual would make cyclic choices when considering the boredom factor. Furthermore, we extend the model to social choices by showing that a society that votes for a particular style or product can be viewed as a single individual cycling through different choices. We adopt a natural model of utility an individual derives from using an item, i.e., the utility of an item gets discounted by its repeated use and increases when the item is not used. We address the problem of optimally choosing items for usage, so as to maximize overall user satisfaction over a period of time. First we show that the simple greedy heuristic of always choosing the item with the maximum current composite utility can be arbitrarily worse than the optimal. Second, we prove that even with just a single individual, determining the optimal strategy for choosing items is NP-hard. Third, we show that a simple modification to the greedy algorithm is a provably close approximation to the optimal strategy. Finally, we present an experimental study over real-world data collected from query logs to compare our algorithms.

#index 1693922
#* Maximizing product adoption in social networks
#@ Smriti Bhagat;Amit Goyal;Laks V.S. Lakshmanan
#t 2012
#c 2
#% 342596
#% 729923
#% 989613
#% 1055690
#% 1083641
#% 1214702
#% 1355040
#% 1399993
#% 1476461
#% 1535380
#% 1560202
#% 1628176
#% 1688456
#! One of the key objectives of viral marketing is to identify a small set of users in a social network, who when convinced to adopt a product will influence others in the network leading to a large number of adoptions in an expected sense. The seminal work of Kempe et al. [13] approaches this as the problem of influence maximization. This and other previous papers tacitly assume that a user who is influenced (or, informed) about a product necessarily adopts the product and encourages her friends to adopt it. However, an influenced user may not adopt the product herself, and yet form an opinion based on the experiences of her friends, and share this opinion with others. Furthermore, a user who adopts the product may not like the product and hence not encourage her friends to adopt it to the same extent as another user who adopted and liked the product. This is independent of the extent to which those friends are influenced by her. Previous works do not account for these phenomena. We argue that it is important to distinguish product adoption from influence. We propose a model that factors in a user's experience (or projected experience) with a product. We adapt the classical Linear Threshold (LT) propagation model by defining an objective function that explicitly captures product adoption, as opposed to influence. We show that under our model, adoption maximization is NP-hard and the objective function is monotone and submodular, thus admitting an approximation algorithm. We perform experiments on three real popular social networks and show that our model is able to distinguish between influence and adoption, and predict product adoption much more accurately than approaches based on the classical LT model.

#index 1693923
#* Answers, not links: extracting tips from yahoo! answers to address how-to web queries
#@ Ingmar Weber;Antti Ukkonen;Aris Gionis
#t 2012
#c 2
#% 342727
#% 590523
#% 642979
#% 754059
#% 787639
#% 805878
#% 818253
#% 869501
#% 1035587
#% 1043040
#% 1055718
#% 1055738
#% 1083720
#% 1092530
#% 1127383
#% 1183152
#% 1190007
#% 1400099
#% 1482231
#% 1497569
#% 1664745
#! We investigate the problem of mining "tips" from Yahoo! Answers and displaying those tips in response to related web queries. Here, a "tip" is a short, concrete and self-contained bit of non-obvious advice such as "To zest a lime if you don't have a zester : use a cheese grater." First, we estimate the volume of web queries with "how-to" intent, which could be potentially addressed by a tip. Second, we analyze how to detect such queries automatically without solely relying on literal "how to *" patterns. Third, we describe how to derive potential tips automatically from Yahoo! Answers, and we develop machine-learning techniques to remove low-quality tips. Finally, we discuss how to match web queries with "how-to" intent to tips. We evaluate both the quality of these direct displays as well as the size of the query volume that can be addressed by serving tips.

#index 1693924
#* A straw shows which way the wind blows: ranking potentially popular items from early votes
#@ Peifeng Yin;Ping Luo;Min Wang;Wang-Chien Lee
#t 2012
#c 2
#% 280852
#% 823332
#% 949164
#% 1071522
#% 1083671
#% 1195868
#% 1309092
#% 1399995
#% 1411585
#% 1517899
#% 1536579
#% 1561458
#% 1581403
#% 1697430
#! Prediction of popular items in online content sharing systems has recently attracted a lot of attention due to the tremendous need of users and its commercial values. Different from previous works that make prediction by fitting a popularity growth model, we tackle this problem by exploiting the latent conforming and maverick personalities of those who vote to assess the quality of on-line items. We argue that the former personality prompts a user to cast her vote conforming to the majority of the service community while on the contrary the later personality makes her vote different from the community. We thus propose a Conformer-Maverick (CM) model to simulate the voting process and use it to rank top-k potentially popular items based on the early votes they received. Through an extensive experimental evaluation, we validate our ideas and find that our proposed CM model achieves better performance than baseline solutions, especially for smaller k.

#index 1693925
#* A large-scale sentiment analysis for Yahoo! answers
#@ Onur Kucuktunc;B. Barla Cambazoglu;Ingmar Weber;Hakan Ferhatosmanoglu
#t 2012
#c 2
#% 577355
#% 727877
#% 815915
#% 854646
#% 1019145
#% 1051058
#% 1127964
#% 1190865
#% 1195855
#% 1260722
#% 1261563
#% 1292771
#% 1299642
#% 1314910
#% 1450879
#% 1450894
#% 1497569
#% 1531994
#% 1536515
#% 1550734
#! Sentiment extraction from online web documents has recently been an active research topic due to its potential use in commercial applications. By sentiment analysis, we refer to the problem of assigning a quantitative positive/negative mood to a short bit of text. Most studies in this area are limited to the identification of sentiments and do not investigate the interplay between sentiments and other factors. In this work, we use a sentiment extraction tool to investigate the influence of factors such as gender, age, education level, the topic at hand, or even the time of the day on sentiments in the context of a large online question answering site. We start our analysis by looking at direct correlations, e.g., we observe more positive sentiments on weekends, very neutral ones in the Science & Mathematics topic, a trend for younger people to express stronger sentiments, or people in military bases to ask the most neutral questions. We then extend this basic analysis by investigating how properties of the (asker, answerer) pair affect the sentiment present in the answer. Among other things, we observe a dependence on the pairing of some inferred attributes estimated by a user's ZIP code. We also show that the best answers differ in their sentiments from other answers, e.g., in the Business & Finance topic, best answers tend to have a more neutral sentiment than other answers. Finally, we report results for the task of predicting the attitude that a question will provoke in answers. We believe that understanding factors influencing the mood of users is not only interesting from a sociological point of view, but also has applications in advertising, recommendation, and search.

#index 1693926
#* Tips, dones and todos: uncovering user profiles in foursquare
#@ Marisa Affonso Vasconcelos;Saulo Ricci;Jussara Almeida;Fabrício Benevenuto;Virgílio Almeida
#t 2012
#c 2
#% 1227655
#% 1250009
#% 1281823
#% 1400018
#% 1475162
#% 1477791
#% 1487804
#% 1582104
#% 1595680
#% 1606045
#% 1606049
#! Online Location Based Social Networks (LBSNs), which combine social network features with geographic information sharing, are becoming increasingly popular. One such application is Foursquare, which doubled its user population in less than six months. Among other features, Foursquare allows users to leave tips (i.e., reviews or recommendations) at specific venues as well as to give feedback on previously posted tips by adding them to their to-do lists or marking them as done. In this paper, we analyze how Foursquare users exploit these three features - tips, dones and to-dos - uncovering different behavior profiles. Our study reveals the existence of very active and influential users, some of which are famous businesses and brands, that seem engaged in posting tips at a large variety of venues while also receiving a great amount of user feedback on them. We also provide evidence of spamming, showing the existence of users that post tips whose contents are unrelated to the nature or domain of the venue where the tips were left.

#index 1693927
#* When will it happen?: relationship prediction in heterogeneous information networks
#@ Yizhou Sun;Jiawei Han;Charu C. Aggarwal;Nitesh V. Chawla
#t 2012
#c 2
#% 730089
#% 853532
#% 1083734
#% 1117026
#% 1199830
#% 1366213
#% 1378224
#% 1451163
#% 1451178
#% 1482199
#% 1635098
#! Link prediction, i.e., predicting links or interactions between objects in a network, is an important task in network analysis. Although the problem has attracted much attention recently, there are several challenges that have not been addressed so far. First, most existing studies focus only on link prediction in homogeneous networks, where all objects and links belong to the same type. However, in the real world, heterogeneous networks that consist of multi-typed objects and relationships are ubiquitous. Second, most current studies only concern the problem of whether a link will appear in the future but seldom pay attention to the problem of when it will happen. In this paper, we address both issues and study the problem of predicting when a certain relationship will happen in the scenario of heterogeneous networks. First, we extend the link prediction problem to the relationship prediction problem, by systematically defining both the target relation and the topological features, using a meta path-based approach. Then, we directly model the distribution of relationship building time with the use of the extracted topological features. The experiments on citation relationship prediction between authors on the DBLP network demonstrate the effectiveness of our methodology.

#index 1693928
#* The life and death of online groups: predicting group growth and longevity
#@ Sanjay Ram Kairam;Dan J. Wang;Jure Leskovec
#t 2012
#c 2
#% 232816
#% 573896
#% 770174
#% 881460
#% 954993
#% 1002007
#% 1055741
#% 1214722
#% 1451242
#! We pose a fundamental question in understanding how to identify and design successful communities: What factors predict whether a community will grow and survive in the long term? Social scientists have addressed this question extensively by analyzing offline groups which endeavor to attract new members, such as social movements, finding that new individuals are influenced strongly by their ties to members of the group. As a result, prior work on the growth of communities has treated growth primarily as a diffusion processes, leading to findings about group evolution which can be difficult to explain. The proliferation of online social networks and communities, however, has created new opportunities to study, at a large scale and with very fine resolution, the mechanisms which lead to the formation, growth, and demise of online groups. In this paper, we analyze data from several thousand online social networks built on the Ning platform with the goal of understanding the factors contributing to the growth and longevity of groups within these networks. Specifically, we investigate the role that two types of growth (growth through diffusion and growth by other means) play during a group's formative stages from the perspectives of both the individual member and the group. Applying these insights to a population of groups of different ages and sizes, we build a model to classify groups which will grow rapidly over the short-term and long-term. Our model achieves over 79% accuracy in predicting group growth over the following two months and over 78% accuracy in predictions over the following two years. We utilize a similar approach to predict which groups will die within a year. The results of our combined analysis provide insight into how both early non-diffusion growth and a complex set of network constraints appear to contribute to the initial and continued growth and success of groups within social networks. Finally we discuss implications of this work for the design, maintenance, and analysis of online communities.

#index 1693929
#* Evaluating search in personal social media collections
#@ Chia-Jung Lee;W. Bruce Croft;Jin Young Kim
#t 2012
#c 2
#% 642983
#% 642992
#% 783474
#% 818262
#% 860036
#% 879678
#% 987195
#% 1074112
#% 1292493
#% 1292597
#% 1384210
#% 1400018
#% 1415742
#% 1450835
#% 1536506
#% 1564816
#% 1581900
#% 1587369
#! The prevalence of social media applications is generating potentially large personal archives of posts, tweets, and other communications. The existence of these archives creates a need for search tools, which can be seen as an extension of current desktop search services. Little is currently known about the best search techniques for personal archives of social data, because of the difficulty of creating test collections. In this paper, we describe how test collections for personal social data can be created by using games to collect queries. We then compare a range of retrieval models that exploit the semi-structured nature of social data. Our results show that a mixture of language models with field distribution estimation can be effective for this type of data, with certain fields, such as the name of the poster, being particularly important. We also analyze the properties of the queries that were generated by users with two versions of the games.

#index 1693930
#* Learning evolving and emerging topics in social media: a dynamic nmf approach with temporal regularization
#@ Ankan Saha;Vikas Sindhwani
#t 2012
#c 2
#% 67565
#% 71108
#% 262042
#% 350859
#% 643008
#% 643056
#% 722904
#% 875959
#% 995168
#% 1035128
#% 1050550
#% 1176853
#% 1386100
#% 1440394
#% 1501267
#! As massive repositories of real-time human commentary, social media platforms have arguably evolved far beyond passive facilitation of online social interactions. Rapid analysis of information content in online social media streams (news articles, blogs,tweets etc.) is the need of the hour as it allows business and government bodies to understand public opinion about products and policies. In most of these settings, data points appear as a stream of high dimensional feature vectors. Guided by real-world industrial deployment scenarios, we revisit the problem of online learning of topics from streaming social media content. On one hand, the topics need to be dynamically adapted to the statistics of incoming datapoints, and on the other hand, early detection of rising new trends is important in many applications. We propose an online nonnegative matrix factorizations framework to capture the evolution and emergence of themes in unstructured text under a novel temporal regularization framework. We develop scalable optimization algorithms for our framework, propose a new set of evaluation metrics, and report promising empirical results on traditional TDT tasks as well as streaming Twitter data. Our system is able to rapidly capture emerging themes, track existing topics over time while maintaining temporal consistency and continuity in user views, and can be explicitly configured to bound the amount of information being presented to the user.

#index 1693931
#* Effects of user similarity in social media
#@ Ashton Anderson;Daniel Huttenlocher;Jon Kleinberg;Jure Leskovec
#t 2012
#c 2
#% 202009
#% 202011
#% 220711
#% 452563
#% 577367
#% 754098
#% 801371
#% 1047390
#% 1055738
#% 1127964
#% 1132846
#% 1190069
#% 1190129
#% 1291600
#% 1384246
#% 1399997
#! There are many settings in which users of a social media application provide evaluations of one another. In a variety of domains, mechanisms for evaluation allow one user to say whether he or she trusts another user, or likes the content they produced, or wants to confer special levels of authority or responsibility on them. Earlier work has studied how the relative status between two users - that is, their comparative levels of status in the group - affects the types of evaluations that one user gives to another. Here we study how similarity in the characteristics of two users can affect the evaluation one user provides of another. We analyze this issue under a range of natural similarity measures, showing how the interaction of similarity and status can produce strong effects. Among other consequences, we find that evaluations are less status-driven when users are more similar to each other; and we use effects based on similarity to provide a plausible mechanism for a complex phenomenon observed in studies of user evaluation, that evaluations are particularly low among users of roughly equal status. Our work has natural applications to the prediction of evaluation outcomes based on user characteristics, and the use of similarity information makes possible a novel application that we introduce here - to estimate the chance of a favorable overall evaluation from a group knowing only the attributes of the group's members, but not their expressed opinions.

#index 1693932
#* How user behavior is related to social affinity
#@ Rina Panigrahy;Marc Najork;Yinglian Xie
#t 2012
#c 2
#% 46803
#% 214077
#% 255137
#% 342596
#% 544011
#% 729923
#% 765261
#% 793252
#% 1061639
#% 1083641
#% 1083672
#% 1198214
#% 1355056
#% 1521673
#% 1584827
#% 1669913
#! Previous research has suggested that people who are in the same social circle exhibit similar behaviors and tastes. The rise of social networks gives us insights into the social circles of web users, and recommendation services (including search engines, advertisement engines, and collaborative filtering engines) provide a motivation to adapt recommendations to the interests of the audience. An important primitive for supporting these applications is the ability to quantify how connected two users are in a social network. The shortest-path distance between a pair of users is an obvious candidate measure. This paper introduces a new measure of "affinity" in social networks that takes into account not only the distance between two users, but also the number of edge-disjoint paths between them, i.e. the "robustness" of their connection. Our measure is based on a sketch-based approach, and affinity queries can be answered extremely efficiently (at the expense of a one-time offline sketch computation). We compare this affinity measure against the "approximate shortest-path distance", a sketch-based distance measure with similar efficiency characteristics. Our empirical study is based on a Hotmail email exchange graph combined with demographic information and Bing query history, and a Twitter mention-graph together with the text of the underlying tweets. We found that users who are close to each other - either in terms of distance or affinity - have a higher similarity in terms of demographics, queries, and tweets.

#index 1693933
#* Finding your friends and following them to where you are
#@ Adam Sadilek;Henry Kautz;Jeffrey P. Bigham
#t 2012
#c 2
#% 44876
#% 261550
#% 277396
#% 541077
#% 716892
#% 762054
#% 858102
#% 955712
#% 989580
#% 1040837
#% 1124987
#% 1264733
#% 1399939
#% 1399992
#% 1425621
#% 1476154
#% 1536568
#% 1606049
#% 1650318
#! Location plays an essential role in our lives, bridging our online and offline worlds. This paper explores the interplay between people's location, interactions, and their social ties within a large real-world dataset. We present and evaluate Flap, a system that solves two intimately related tasks: link and location prediction in online social networks. For link prediction, Flap infers social ties by considering patterns in friendship formation, the content of people's messages, and user location. We show that while each component is a weak predictor of friendship alone, combining them results in a strong model, accurately identifying the majority of friendships. For location prediction, Flap implements a scalable probabilistic model of human mobility, where we treat users with known GPS positions as noisy sensors of the location of their friends. We explore supervised and unsupervised learning scenarios, and focus on the efficiency of both learning and inference. We evaluate Flap on a large sample of highly active users from two distinct geographical areas and show that it (1) reconstructs the entire friendship graph with high accuracy even when no edges are given; and (2) infers people's fine-grained location, even when they keep their data private and we can only access the location of their friends. Our models significantly outperform current comparable approaches to either task.

#index 1693934
#* How to win friends and influence people, truthfully: influence maximization mechanisms for social networks
#@ Yaron Singer
#t 2012
#c 2
#% 297675
#% 342596
#% 577217
#% 729923
#% 769887
#% 836506
#% 836550
#% 868469
#% 963247
#% 989613
#% 1039690
#% 1055690
#% 1336439
#% 1355040
#% 1400056
#% 1449326
#% 1451242
#% 1521647
#% 1523718
#% 1536509
#% 1584771
#% 1584802
#% 1605972
#! Throughout the past decade there has been extensive research on algorithmic and data mining techniques for solving the problem of influence maximization in social networks: if one can incentivize a subset of individuals to become early adopters of a new technology, which subset should be selected so that the word-of-mouth effect in the social network is maximized? Despite the progress in modeling and techniques, the incomplete information aspect of the problem has been largely overlooked. While data can often provide the network structure and influence patterns may be observable, the inherent cost individuals have to become early adopters is difficult to extract. In this paper we introduce mechanisms that elicit individuals' costs while providing desirable approximation guarantees in some of the most well-studied models of social network influence. We follow the mechanism design framework which advocates for allocation and payment schemes that incentivize individuals to report their true information. We also performed experiments using the Mechanical Turk platform and social network data to provide evidence of the framework's effectiveness in practice.

#index 1693935
#* Inferring social ties across heterogenous networks
#@ Jie Tang;Tiancheng Lou;Jon Kleinberg
#t 2012
#c 2
#% 464434
#% 754098
#% 955712
#% 1083734
#% 1194122
#% 1214702
#% 1269756
#% 1384246
#% 1399997
#% 1425621
#% 1451159
#% 1451162
#% 1451163
#% 1536568
#% 1560425
#% 1617365
#% 1642046
#% 1650318
#! It is well known that different types of social ties have essentially different influence on people. However, users in online social networks rarely categorize their contacts into "family", "colleagues", or "classmates". While a bulk of research has focused on inferring particular types of relationships in a specific social network, few publications systematically study the generalization of the problem of inferring social ties over multiple heterogeneous networks. In this work, we develop a framework for classifying the type of social relationships by learning across heterogeneous networks. The framework incorporates social theories into a factor graph model, which effectively improves the accuracy of inferring the type of social relationships in a target network by borrowing knowledge from a different source network. Our empirical study on five different genres of networks validates the effectiveness of the proposed framework. For example, by leveraging information from a coauthor network with labeled advisor-advisee relationships, the proposed framework is able to obtain an F1-score of 90% (8-28% improvements over alternative methods) for inferring manager-subordinate relationships in an enterprise email network.

#index 1693936
#* The secret life of social links
#@ Hilary Mason
#t 2012
#c 2
#! The social web is a messy place! At bitly, we see hundreds of millions of shares and clicks per day--clicks that contain all sorts of wonderful content from lolcats to spacecraft launches. I'll discuss our philosophy, tools, and techniques for looking at the data, and new research opportunities that weren't possible before.

#index 1693937
#* Exploration and discovery of user-generated content in large information spaces
#@ Luca Chiarandini
#t 2012
#c 2
#% 5632
#% 15452
#% 304235
#% 402090
#% 734961
#% 1573486
#! The accumulation of large collections of social media data poses new challenges for the design of exploratory experiences, such as when a user browses through a collection to discover content (e.g. exploring photo collections, network of friends, etc). Cardinality and characteristics of the set, together with volatility of the information, resulting from fast and continuous creation, deletion and updating of entries, trigger novel research questions. In this context, we plan to investigate and contribute to the data analysis, and user interface design of exploratory experiences. The proposed approach is an iterative process where analysis and design phases are performed in cycles. The long-term vision is to understand the underlying reasoning in order to be able to automatically replicate it.

#index 1693938
#* Computational advertising: leveraging user interaction & contextual factors for improved ad relevance & targeting
#@ Kushal S. Dave
#t 2012
#c 2
#% 1451020
#% 1482460
#! Computational advertising refers to finding the most relevant ads matching a particular context on the web. The core problem attacked in computational advertising CA is of the match making between the ads and the context. My research work aims at leveraging various user interaction, ad and advertiser related information and contextual information for improving the relevance, ranking and targeting of ads. The research work focuses on the identification of various factors that contribute in retrieving and ranking the most relevant set of ads that match best with the context. Specifically, information associated with the user, publisher and advertiser is leveraged for this purpose.

#index 1693939
#* The early bird gets the buzz: detecting anomalies and emerging trends in information networks
#@ Brian Thompson
#t 2012
#c 2
#% 288952
#% 1202160
#! In this work we propose a novel approach to anomaly detection in streaming communication data. We first build a stochastic model for the system based on temporal communication patterns across each edge, which we call the REWARDS (REneWal theory Approach for Real-time Data Streams) model. We then define a measure of anomaly for an arbitrary subgraph based on the likelihood of its recent activity given past behavior. Finally, we develop an algorithm to efficiently identify subgraphs with the most anomalous activity. Although our work has until now focused on the cybersecurity domain, the model we present is more broadly applicable to information retrieval in data streams and information networks.

#index 1693940
#* Characterizing and harnessing peer-production of information in social tagging systems
#@ Elizeu Santos-Neto
#t 2012
#c 2
#% 888213
#% 935278
#% 1169814
#% 1409193
#% 1512509
#% 1536516
#% 1641939
#! Assessing the value of individual users' contributions in peer-production systems is paramount to the design of mechanisms that support collaboration and improve users' experience. For instance, to incentivize contributions, file sharing systems based on the BitTorrent protocol equate value with volume of contributed content and use a prioritization mechanism to reward users who contribute more. This approach and similar techniques used in resource sharing systems rely on the fact that the physical resources shared among users are easily quantifiable. In contrast, information-sharing systems, like social tagging systems, lack the notion of a physical resource unit (e.g., content size, bandwidth) that facilitates the task of evaluating user contributions. For this reason, the issue of estimating the value of user contributions in information sharing systems remains largely unexplored. This paper outlines a research project to tackle the problem of assessing the value of contributions in social tagging systems.

#index 1693941
#* Mining, searching and exploiting collaboratively generated content on the web
#@ Eugene Agichtein;Evgeniy Gabrilovich
#t 2012
#c 2
#! Proliferation of ubiquitous access to the Internet enables millions of Web users to collaborate online on a variety of activities. Many of these activities result in the construction of large repositories of knowledge, either as their primary aim (e.g., Wikipedia) or as a by-product (e.g., Yahoo! Answers). In this tutorial, we will discuss organizing and exploiting Collaboratively Generated Content (CGC) for information organization and retrieval. Specifically, we intend to cover two complementary areas of the problem: (1) using such content as a powerful enabling resource for knowledge-enriched, intelligent representations and new information retrieval algorithms, and (2) development of supporting technologies for extracting, filtering, and organizing collaboratively created content. The unprecedented amounts of information in CGC enable new, knowledge-rich approaches to information access, which are significantly more powerful than the conventional word-based methods. Considerable progress has been made in this direction over the last few years. Examples include explicit manipulation of human-defined concepts and their use to augment the bag of words (cf. Explicit Semantic Analysis), using large-scale taxonomies of topics from Wikipedia or the Open Directory Project to construct additional class-based features, or using Wikipedia for better word sense disambiguation. However, the quality and comprehensiveness of collaboratively created content vary significantly, and in order for this resource to be useful, a significant amount of preprocessing, filtering, and organization is necessary. Consequently, new methods for analyzing CGC and corresponding user interactions are required to effectively harness the resulting knowledge. Thus, not only the content repositories can be used to improve IR methods, but the reverse pollination is also possible, as better information extraction methods can be used for automatically collecting more knowledge, or verifying the contributed content. This natural connection between modeling the generation process of CGC and effectively using the accumulated knowledge suggests covering both areas together in a single tutorial. The intended audience of the tutorial includes IR researchers and graduate students, who would like to learn about the recent advances and research opportunities in working with collaboratively generated content. The emphasis of the tutorial is on comparing the existing approaches and presenting practical techniques that IR practitioners can use in their research. We also cover open research challenges, as well as survey available resources (software tools and data) for getting started in this research field.

#index 1693942
#* Collaborative information seeking: understanding users, systems, and content
#@ Chirag Shah
#t 2012
#c 2
#% 157698
#% 240162
#% 246877
#% 399447
#% 998795
#! The course will introduce the student to theories, methodologies, and tools that focus on information retrieval/seeking in collaboration. The student will have an opportunity to learn about the social aspect of IR with a focus on collaborative information seeking (CIS) situations, systems, and evaluation techniques. Traditionally, IR is considered an individual pursuit, and not surprisingly, the majority of tools, techniques, and models developed for addressing information need, retrieval, and usage have focused on single users. The assumption of information seekers being independent and IR problem being individual has been challenged often in the recent past. This course will introduce such works to the students, with an emphasis on understanding models and systems that support collaborative search or browsing. In addition, the course will provide samples of data collected through several experiments to demonstrate various mining and analysis techniques. Specifically, the course will (1) outline the research and latest developments in the field of collaborative IR, (2) list the challenges for designing and evaluating collaborative IR systems, and (3) show how traditional single user IR models and systems could be mapped to those for CIS. This will be achieved through introduction to appropriate literature, algorithms and interfaces that facilitate CIS, and methodologies for studying and evaluating them. Thus, the course will offer a balance between theoretical and practical elements of CIS.

#index 1693943
#* Machine learning for query-document matching in search
#@ Hang Li;Jun Xu
#t 2012
#c 2
#% 817577
#% 1190106
#% 1227610
#% 1399944
#% 1536566
#% 1536584
#% 1560222
#% 1591934
#% 1598394
#% 1598399
#% 1598401
#% 1598402
#% 1606370
#! In web search, relevance is one of the most important factors to meet users' satisfaction, and the success of a web search engine heavily depends on its performance on relevance. It has been observed that many hard cases in search relevance are due to term mismatch between query and documnt (e.g., query 'ny times' does not match well with document only containing 'new york times'), and thus it is not exaggerated to say that dealing with mismatch between query and document is one of the most critical research problems in web search. Recently researchers have spent significant effort to address the grand challenge. The major approach is to conduct more query and document understanding, and perform better matching between enriched query and document representations. With the availability of large amount of log data and advanced machine learning techniques, this becomes more feasible and significant progress has been made recently. In this tutorial, we will give a systematic and detailed presentation on newly developed machine learning technologies for query document matching in search. We will focus on the fundamental problems, as well as the novel solutions for query document matching at word form level, word sense level, topic level, and structure level. We will talk about novel technologies about query spelling error correction [3, 13], query rewriting [1, 4, 6, 7], query classification [2], topic modeling of documents [5, 9], query document matching [8, 10, 11, 12], and query document-title translation. The ideas and solutions introduced in this tutorial may motivate industrial practitioners to turn the research fruits into product reality. The summary of the state-of-the-art methods and the discussions on the technical issues in this tutorial may stimulate academic researchers to find new research directions and solutions. Matching between query and document is not limited to search, and similar problems can be observed at online advertisement, recommendation system, and other applications, as matching between objects from two spaces. The technologies we introduce can be generalized into more general machine learning techniques, which we call learning to match.

#index 1693944
#* 2nd international workshop on diversity in document retrieval (DDR 2012)
#@ Craig Macdonald;Jun Wang;Charles Clarke
#t 2012
#c 2
#% 262112
#% 1074133
#% 1166473
#% 1227591
#% 1400021
#% 1598438
#% 1697422
#! When an ambiguous query is received, a sensible approach is for the information retrieval (IR) system to diversify the results retrieved for this query, in the hope that at least one of the interpretations of the query intent will satisfy the user. Diversity is an increasingly important topic, of interest to both academic researchers (such as participants in the TREC Web and Blog track diversity tasks, or the NTCIR INTENT task), as well as to search engines professionals. In the 2nd edition of the Diversity in Document Retrieval workshop (DDR 2012), we solicited submissions both on approaches and models for diversity, the evaluation of diverse search results, and on applications of diverse search results. This workshop builds upon a successful 1st edition of DDR which was held at ECIR 2011 in Dublin, Ireland.

#index 1693945
#* WSCD 2012: workshop on web search click data 2012
#@ Pavel Serdyukov;Nick Craswell;Georges Dupret
#t 2012
#c 2
#! WSCD2012 is the second workshop on Web Search Click Data, following WSCD2009. It is a forum for new research relating to Web search usage logs and for discussing desirable properties of publicly released search log datasets. Research relating to search logs has been hampered by the limited availability of click datasets. This workshop comes with a new click dataset based on click logs and an accompanying challenge to predict the relevance of documents based on clicks.

#index 1948121
#* Proceedings of the sixth ACM international conference on Web search and data mining
#@ Stefano Leonardi;Alessandro Panconesi;Paolo Ferragina;Aristides Gionis
#t 2013
#c 2
#! We are delighted to welcome you to the sixth ACM International Conference on Web Search and Data Mining (WSDM 2013) held on February 4-8, 2013, in Rome, Italy. As in the previous years, WSDM has attracted an impressive number of submissions tackling the most recent technical challenges in Web search and data mining, with an ever-growing interest in their social aspects. Now in its sixth year, the conference has reached maturity and has become a leading forum for disseminating the latest research developments in the field. We are happy to present here the proceedings of the conference. We received a total of 387 submissions (compared to 362 of the last year) from 36 countries and regions, out of which 73 were accepted for full paper publication in the proceedings, thus reaching an acceptance rate of 18.9% (compared to 20.7% of last year and 22.3% of two years ago). The authors of the accepted papers are from 20 countries, spanning four continents - making this a truly international forum. Oral presentation slots were allocated to all papers. Yet, in order to maintain the single-track model that most attendees prefer, we preserved this year the format with "spotlight" type and "plenary" type presentations. Out of the 73 accepted papers, 40 were assigned a "spotlight" slot, while 33 were assigned a plenary slot. The type of slot was chosen by the Program Chairs, mostly based on whether the topic and the content of the paper were best suited for a large group presentation or for a more focused and interactive "spotlight" style of presentation.

#index 1948122
#* The virtual lab
#@ Duncan J. Watts
#t 2013
#c 2
#! The Internet and the Web have transformed society, spawning new industries, altering social and cultural practices, and challenging long-accepted notions of individual privacy, intellectual property, and national security. In this talk, I argue that social science is also being transformed. In particular, I describe how crowd sourcing sites like Amazon's Mechanical Turk are increasingly being used by researchers to create "virtual labs" in which they can conduct behavioral experiments on a scale and speed that would have been hard to imagine just a decade ago. To illustrate the point, I describe some recent experiments that showcase the advantages of virtual over traditional physical labs, as well as some of the limitations. I then discuss how this relatively new experimental capability may unfold in the near future, along with some implications for social and behavioral science.

#index 1948123
#* Information-theoretic measures of influence based on content dynamics
#@ Greg Ver Steeg;Aram Galstyan
#t 2013
#c 2
#% 577329
#% 1183091
#% 1214482
#% 1214680
#% 1269756
#% 1311981
#% 1355042
#% 1399992
#% 1536509
#% 1560429
#% 1561559
#% 1587367
#% 1592006
#% 1617342
#% 1746849
#% 1746868
#! The fundamental building block of social influence is for one person to elicit a response in another. Researchers measuring a "response" in social media typically depend either on detailed models of human behavior or on platform-specific cues such as re-tweets, hash tags, URLs, or mentions. Most content on social networks is difficult to model because the modes and motivation of human expression are diverse and incompletely understood. We introduce content transfer, an information-theoretic measure with a predictive interpretation that directly quantifies the strength of the effect of one user's content on another's in a model-free way. Estimating this measure is made possible by combining recent advances in non-parametric entropy estimation with increasingly sophisticated tools for content representation. We demonstrate on Twitter data collected for thousands of users that content transfer is able to capture non-trivial, predictive relationships even for pairs of users not linked in the follower or mention graph. We suggest that this measure makes large quantities of previously under-utilized social media content accessible to rigorous statistical causal analysis.

#index 1948124
#* Characterizing and curating conversation threads: expansion, focus, volume, re-entry
#@ Lars Backstrom;Jon Kleinberg;Lillian Lee;Cristian Danescu-Niculescu-Mizil
#t 2013
#c 2
#% 1055736
#% 1190088
#% 1292698
#% 1399992
#% 1400022
#% 1451195
#% 1536509
#% 1556450
#% 1560424
#% 1746868
#% 1872259
#% 1906975
#% 1913337
#! Discussion threads form a central part of the experience on many Web sites, including social networking sites such as Facebook and Google Plus and knowledge creation sites such as Wikipedia. To help users manage the challenge of allocating their attention among the discussions that are relevant to them, there has been a growing need for the algorithmic curation of on-line conversations --- the development of automated methods to select a subset of discussions to present to a user. Here we consider two key sub-problems inherent in conversational curation: length prediction --- predicting the number of comments a discussion thread will receive --- and the novel task of re-entry prediction --- predicting whether a user who has participated in a thread will later contribute another comment to it. The first of these sub-problems arises in estimating how interesting a thread is, in the sense of generating a lot of conversation; the second can help determine whether users should be kept notified of the progress of a thread to which they have already contributed. We develop and evaluate a range of approaches for these tasks, based on an analysis of the network structure and arrival pattern among the participants, as well as a novel dichotomy in the structure of long threads. We find that for both tasks, learning-based approaches using these sources of information.

#index 1948125
#* Structure and dynamics of information pathways in online media
#@ Manuel Gomez Rodriguez;Jure Leskovec;Bernhard Schölkopf
#t 2013
#c 2
#% 729923
#% 868469
#% 983424
#% 1214671
#% 1299294
#% 1386131
#% 1451242
#% 1535333
#% 1536522
#% 1560424
#% 1605965
#% 1746901
#% 1872232
#% 1978756
#! Diffusion of information, spread of rumors and infectious diseases are all instances of stochastic processes that occur over the edges of an underlying network. Many times networks over which contagions spread are unobserved, and such networks are often dynamic and change over time. In this paper, we investigate the problem of inferring dynamic networks based on information diffusion data. We assume there is an unobserved dynamic network that changes over time, while we observe the results of a dynamic process spreading over the edges of the network. The task then is to infer the edges and the dynamics of the underlying network. We develop an on-line algorithm that relies on stochastic convex optimization to efficiently solve the dynamic network inference problem. We apply our algorithm to information diffusion among 3.3 million mainstream media and blog sites and experiment with more than 179 million different pieces of information spreading over the network in a one year period. We study the evolution of information pathways in the online media space and find interesting insights. Information pathways for general recurrent topics are more stable across time than for on-going news events. Clusters of news media sites and blogs often emerge and vanish in matter of days for on-going news events. Major social movements and events involving civil population, such as the Libyan'{}s civil war or Syria'{}s uprise, lead to an increased amount of information pathways among blogs as well as in the overall increase in the network centrality of blogs and social media sites.

#index 1948126
#* Cascade-based community detection
#@ Nicola Barbieri;Francesco Bonchi;Giuseppe Manco
#t 2013
#c 2
#% 277483
#% 342596
#% 577217
#% 729923
#% 949164
#% 1063629
#% 1083624
#% 1083641
#% 1107420
#% 1117695
#% 1198232
#% 1214702
#% 1328169
#% 1355040
#% 1355042
#% 1399993
#% 1400031
#% 1425621
#% 1451244
#% 1456839
#% 1536509
#% 1560424
#% 1598013
#% 1617267
#% 1669913
#% 1689531
#% 1707462
#% 1746831
#% 1872388
#% 1874860
#! Given a directed social graph and a set of past informa- tion cascades observed over the graph, we study the novel problem of detecting modules of the graph (communities of nodes), that also explain the cascades. Our key observation is that both information propagation and social ties forma- tion in a social network can be explained according to the same latent factor, which ultimately guide a user behavior within the network. Based on this observation, we propose the Community-Cascade Network (CCN) model, a stochas- tic mixture membership generative model that can fit, at the same time, the social graph and the observed set of cas- cades. Our model produces overlapping communities and for each node, its level of authority and passive interest in each community it belongs. For learning the parameters of the CCN model, we devise a Generalized Expectation Maximization procedure. We then apply our model to real-world social networks and in- formation cascades: the results witness the validity of the proposed CCN model, providing useful insights on its signif- icance for analyzing social behavior.

#index 1948127
#* Patent partner recommendation in enterprise social networks
#@ Sen Wu;Jimeng Sun;Jie Tang
#t 2013
#c 2
#% 220708
#% 387427
#% 955502
#% 955712
#% 956521
#% 989621
#% 1183091
#% 1214668
#% 1399997
#% 1451162
#% 1451238
#% 1536568
#% 1538534
#% 1598567
#% 1606056
#% 1625374
#% 1642046
#% 1642411
#% 1688485
#% 1693935
#% 1810385
#% 1872384
#% 1872393
#! It is often challenging to incorporate users' interactions into a recommendation framework in an online model. In this paper, we propose a novel interactive learning framework to formulate the problem of recommending patent partners into a factor graph model. The framework involves three phases: 1) candidate generation, where we identify the potential set of collaborators; 2) candidate refinement, where a factor graph model is used to adjust the candidate rankings; 3) interactive learning method to efficiently update the existing recommendation model based on inventors' feedback. We evaluate our proposed model on large enterprise patent networks. Experimental results demonstrate that the recommendation accuracy of the proposed model significantly outperforms several baselines methods using content similarity, collaborative filtering and SVM-Rank. We also demonstrate the effectiveness and efficiency of the interactive learning, which performs almost as well as offline re-training, but with only 1 percent of the running time.

#index 1948128
#* Exploiting homophily effect for trust prediction
#@ Jiliang Tang;Huiji Gao;Xia Hu;Huan Liu
#t 2013
#c 2
#% 577217
#% 754098
#% 943767
#% 955712
#% 987253
#% 1071523
#% 1083671
#% 1176889
#% 1233312
#% 1247796
#% 1269378
#% 1296933
#% 1318628
#% 1326699
#% 1355042
#% 1400002
#% 1524266
#% 1536533
#% 1606051
#% 1617325
#% 1668087
#% 1688533
#% 1693870
#% 1872260
#% 1932103
#! Trust plays a crucial role for online users who seek reliable information. However, in reality, user-specified trust relations are very sparse, i.e., a tiny number of pairs of users with trust relations are buried in a disproportionately large number of pairs without trust relations, making trust prediction a daunting task. As an important social concept, however, trust has received growing attention and interest. Social theories are developed for understanding trust. Homophily is one of the most important theories that explain why trust relations are established. Exploiting the homophily effect for trust prediction provides challenges and opportunities. In this paper, we embark on the challenges to investigate the trust prediction problem with the homophily effect. First, we delineate how it differs from existing approaches to trust prediction in an unsupervised setting. Next, we formulate the new trust prediction problem into an optimization problem integrated with homophily, empirically evaluate our approach on two datasets from real-world product review sites, and compare with representative algorithms to gain a deep understanding of the role of homophily in trust prediction.

#index 1948129
#* Efficient and effective retrieval using selective pruning
#@ Nicola Tonellotto;Craig Macdonald;Iadh Ounis
#t 2013
#c 2
#% 198335
#% 213786
#% 643566
#% 730065
#% 766463
#% 783506
#% 818262
#% 879611
#% 987356
#% 1019084
#% 1166469
#% 1173690
#% 1268491
#% 1355057
#% 1450846
#% 1467729
#% 1536512
#% 1598342
#% 1598344
#% 1598393
#% 1620189
#% 1621236
#% 1674994
#% 1879054
#% 1879137
#! Retrieval can be made more efficient by deploying dynamic pruning strategies such as WAND, which do not degrade effectiveness up to a given rank. It is possible to increase the efficiency of such techniques by pruning more 'aggressively'. However, this may reduce effectiveness. In this work, we propose a novel selective framework that determines the appropriate amount of pruning aggressiveness on a per-query basis, thereby increasing overall efficiency without significantly reducing overall effectiveness. We postulate two hypotheses about the queries that should be pruned more aggressively, which generate two approaches within our framework, based on query performance predictors and query efficiency predictors, respectively. We thoroughly experiment to ascertain the efficiency and effectiveness impacts of the proposed approaches, as part of a search engine deploying state-of-the-art learning to rank techniques. Our results on 50 million documents of the TREC ClueWeb09 collection show that by using query efficiency predictors to target inefficient queries, we observe that a 36% reduction in mean response time and a 50% reduction of the response times experienced by the slowest 10% of queries can be achieved while still ensuring effectiveness.

#index 1948130
#* Document selection for tiered indexing in commerce search
#@ Debmalya Panigrahi;Sreenivas Gollapudi
#t 2013
#c 2
#% 190611
#% 194299
#% 201876
#% 248797
#% 256685
#% 297675
#% 397401
#% 654504
#% 728102
#% 805864
#% 907547
#% 958201
#% 987215
#% 987216
#% 1074067
#% 1127394
#% 1190098
#% 1227597
#% 1426566
#% 1536531
#% 1536577
#! A search engine aims to return a set of relevant documents in response to a query, while minimizing the response time. This has led to the use of a tiered index, where the search engine maintains a small cache of documents that can serve a large fraction of queries. We give a novel algorithm for the selection of documents in a tiered index for commerce search (i.e. users searching for products on the web) that effectively exploits the superior structural characteristics of commerce search queries. This is in sharp contrast to previous approaches to tiered indexing that were aimed at general web search where queries are typically unstructured. We theoretically analyze our algorithms and give performance guarantees even in worst-case scenarios. We then complement and strengthen our theoretical claims by performing exhaustive experiments on real-world commerce search data, and show that our algorithm outperforms state-of-the-art tiered indexing techniques that were developed for general web search.

#index 1948131
#* Quasi-succinct indices
#@ Sebastiano Vigna
#t 2013
#c 2
#% 213786
#% 259235
#% 324129
#% 587845
#% 656274
#% 737340
#% 786632
#% 864446
#% 866705
#% 867054
#% 879326
#% 879611
#% 996633
#% 1019138
#% 1077150
#% 1096070
#% 1227595
#% 1412873
#! Compressed inverted indices in use today are based on the idea of gap compression: documents pointers are stored in increasing order, and the gaps between successive document pointers are stored using suitable codes which represent smaller gaps using less bits. Additional data such as counts and positions is stored using similar techniques. A large body of research has been built in the last 30 years around gap compression, including theoretical modeling of the gap distribution, specialized instantaneous codes suitable for gap encoding, and ad hoc document reorderings which increase the efficiency of instantaneous codes. This paper proposes to represent an index using a different architecture based on quasi-succinct representation of monotone sequences. We show that, besides being theoretically elegant and simple, the new index provides expected constant-time operations, space savings, and, in practice, significant performance improvements on conjunctive, phrasal and proximity queries.

#index 1948132
#* Identifying users' topical tasks in web search
#@ Wen Hua;Yangqiu Song;Haixun Wang;Xiaofang Zhou
#t 2013
#c 2
#% 186340
#% 754068
#% 1063570
#% 1083721
#% 1130868
#% 1130878
#% 1275182
#% 1292473
#% 1450885
#% 1482279
#% 1536532
#% 1641928
#% 1746847
#% 1770359
#% 1826433
#! A search task represents an atomic information need of a user in web search. Tasks consist of queries and their reformulations, and identifying tasks is important for search engines since they provide valuable information for determining user satisfaction with search results, predicting user search intent, and suggesting queries to the user. Traditional approaches to identifying tasks exploit either temporal or lexical features of queries. However, many query refinements are topical, which means that a query and its refinements may not be similar on the lexical level. Furthermore, multiple tasks in the same search session may interleave, which means we cannot simply order the searches by their timestamps and divide the session into multiple tasks. Thus, in order to identify tasks correctly, we need to be able to compare two queries at the semantic level. In this paper, we use a knowledgebase known as Probase to infer the conceptual meanings of queries, and automatically identify the topical query refinements in the tasks. Experimental results on real search log data demonstrate that Probase can indeed help estimate the topical affinity between queries, and thus enable us to merge queries that are topically related but dissimilar at the lexical level.

#index 1948133
#* Differences in search engine evaluations between query owners and non-owners
#@ Alexandra Chouldechova;David Mease
#t 2013
#c 2
#% 309767
#% 312689
#% 751596
#% 818259
#% 967660
#% 987263
#% 1047297
#% 1074134
#% 1074136
#% 1130811
#% 1130866
#% 1292528
#% 1450884
#% 1536505
#% 1598464
#! The query-document relevance judgments used in web search engine evaluation are traditionally provided by human assessors who have no particular association with the specific queries selected for the evaluation. Most commonly, queries are randomly sampled from search logs and in turn randomly assigned to the human assessors. In this paper, we consider a very different approach in which we instead ask the human assessors to provide their own queries from their recent search experiences. Using these queries as our sample, we compare the relevance judgments from the "owners" of the queries to the relevance judgments of the non-owners. We conduct experiments which reveal that query ownership has a substantial and beneficial impact on the accuracy of relevance judgments. In particular, we observe that owners are more consistently able to distinguish a higher quality set of search results from a lower quality set in a blind comparison. The implication for web search evaluation is that query owners provide more valuable relevance judgments than non-owners, presumably due to the background knowledge associated with their queries. We quantify the benefit of using owner assessments versus non-owner assessments in terms of sample size reduction. We also touch on some of the practical challenges associated with using query owners as assessors.

#index 1948134
#* Optimizing top-k document retrieval strategies for block-max indexes
#@ Constantinos Dimopoulos;Sergey Nepomnyachiy;Torsten Suel
#t 2013
#c 2
#% 198335
#% 212665
#% 290703
#% 340886
#% 340887
#% 387427
#% 397608
#% 728102
#% 730065
#% 818229
#% 864446
#% 867054
#% 879611
#% 893128
#% 987214
#% 1015265
#% 1055710
#% 1166469
#% 1190095
#% 1216713
#% 1302865
#% 1392439
#% 1482300
#% 1587386
#% 1598344
#% 1598433
#% 1693904
#! Large web search engines use significant hardware and energy resources to process hundreds of millions of queries each day, and a lot of research has focused on how to improve query processing efficiency. One general class of optimizations called early termination techniques is used in all major engines, and essentially involves computing top results without an exhaustive traversal and scoring of all potentially relevant index entries. Recent work in [9,7] proposed several early termination algorithms for disjunctive top-k query processing, based on a new augmented index structure called Block-Max Index that enables aggressive skipping in the index. In this paper, we build on this work by studying new algorithms and optimizations for Block-Max indexes that achieve significant performance gains over the work in [9,7]. We start by implementing and comparing Block-Max oriented algorithms based on the well-known Maxscore and WAND approaches. Then we study how to build better Block-Max index structures and design better index-traversal strategies, resulting in new algorithms that achieve a factor of 2 speed-up over the best results in [9] with acceptable space overheads. We also describe and evaluate a hierarchical algorithm for a new recursive Block-Max index structure.

#index 1948135
#* Improving the sensitivity of online controlled experiments by utilizing pre-experiment data
#@ Alex Deng;Ya Xu;Ron Kohavi;Toby Walker
#t 2013
#c 2
#% 1154062
#% 1451140
#% 1562244
#% 1667281
#% 1872323
#! Online controlled experiments are at the heart of making data-driven decisions at a diverse set of companies, including Amazon, eBay, Facebook, Google, Microsoft, Yahoo, and Zynga. Small differences in key metrics, on the order of fractions of a percent, may have very significant business implications. At Bing it is not uncommon to see experiments that impact annual revenue by millions of dollars, even tens of millions of dollars, either positively or negatively. With thousands of experiments being run annually, improving the sensitivity of experiments allows for more precise assessment of value, or equivalently running the experiments on smaller populations (supporting more experiments) or for shorter durations (improving the feedback cycle and agility). We propose an approach (CUPED) that utilizes data from the pre-experiment period to reduce metric variability and hence achieve better sensitivity. This technique is applicable to a wide variety of key business metrics, and it is practical and easy to implement. The results on Bing's experimentation system are very successful: we can reduce variance by about 50%, effectively achieving the same statistical power with only half of the users, or half the duration.

#index 1948136
#* Playing by the rules: mining query associations to predict search performance
#@ Youngho Kim;Ahmed Hassan;Ryen W. White;Yi-Min Wang
#t 2013
#c 2
#% 300120
#% 397161
#% 729417
#% 805200
#% 879613
#% 907544
#% 939376
#% 944349
#% 956541
#% 983808
#% 983905
#% 987260
#% 987263
#% 987321
#% 1130878
#% 1275193
#% 1301004
#% 1355038
#% 1384094
#% 1399944
#% 1415713
#% 1450833
#% 1468142
#% 1537504
#% 1598367
#% 1598368
#% 1641927
#% 1879020
#! Understanding the characteristics of queries where a search engine is failing is important for improving engine performance. Previous work largely relies on user-interaction features (e.g., clickthrough statistics) to identify such underperforming queries. However, relying on interaction behavior means that searchers need to become dissatisfied and need to exhibit that in their search behavior, by which point it may be too late to help them. In this paper, we propose a method to generate underperforming query identification rules instantly using topical and lexical attributes. The method first generates query attributes using sources such as topics, concepts (entities), and keywords in queries. Then, association rules are learned by exploiting the FP-growth algorithm and decision trees using underperforming query examples. We develop a query classification model capable of accurately estimating dissatisfaction using the generated rules, and demonstrate significant performance gains over state-of-the-art query performance prediction models.

#index 1948137
#* NCDawareRank: a novel ranking method that exploits the decomposable structure of the web
#@ Athanasios N. Nikolakopoulos;John D. Garofalakis
#t 2013
#c 2
#% 214028
#% 283833
#% 510723
#% 565488
#% 754060
#% 754088
#% 769460
#% 794132
#% 807318
#% 818225
#% 838516
#% 853937
#% 869471
#% 878224
#% 879603
#% 1278065
#% 1279489
#% 1335739
#% 1399971
#% 1404178
#% 1573080
#% 1614689
#! Research about the topological characteristics of the hyperlink graph has shown that Web possesses a nested block structure, indicative of its innate hierarchical organization. This crucial observation opens the way for new approaches that can usefully regard Web as a Nearly Completely Decomposable(NCD) system; In recent years, such approaches gave birth to various efficient methods and algorithms that exploit NCD from a computational point of view and manage to considerably accelerate the extraction of the PageRank vector. However, very little have been done towards the qualitative exploitation of NCD. In this paper we propose NCDawareRank, a novel ranking method that uses the intuition behind NCD to generalize and refine PageRank. NCDawareRank considers both the link structure and the hierarchical nature of the Web in a way that preserves the mathematically attractive characteristics of PageRank and at the same time manages to successfully resolve many of its known problems, including Web Spamming Susceptibility and Biased Ranking of Newly Emerging Pages. Experimental results show that NCDawareRank is more resistant to direct manipulation, alleviates the problems caused by the sparseness of the link graph and assigns more reasonable ranking scores to newly added pages, while maintaining the ability to be easily implemented on a large-scale and in a computationally efficient manner.

#index 1948138
#* Rank quantization
#@ Ravi Kumar;Ronny Lempel;Roy Schwartz;Sergei Vassilvitskii
#t 2013
#c 2
#% 268079
#% 310515
#% 330769
#% 578337
#% 818255
#% 823353
#% 838462
#% 867054
#% 881515
#% 952823
#% 963243
#% 1063473
#% 1077150
#% 1091267
#% 1169990
#% 1419423
#! We study the problem of aggregating and summarizing partial orders, on a large scale. Our motivation is two-fold: to discover elements at similar preference levels and to reduce the number of bits needed to store an element's position in a full ranking.We proceed in two steps: first, we find a total order by linearizing the rankings induced by the multiple partial orders and removing potentially inconsistent pairwise preferences. Next, given a total order, we introduce and formalize the rank quantization problem, which intuitively aims to bucketize the total order in a manner that mostly preserves the relations appearing in the partial orders. We show an exact quadratic-time quantization algorithm, as well as a greedy 2/3-approximation algorithm whose running is substantially faster on sparse instances. As an application, we aggregate rankings of top-10 search results over millions of search engine queries, approximately reproducing and then efficiently encoding the underlying static ranks used by the engine. We evaluate the performance of our algorithms on a web dataset of 12 million(2^{23.5}) unique pages and show that we can quantize the pages' static ranks using as few as eight bits, with only a minor degradation in search quality.

#index 1948139
#* Time-sensitive web image ranking and retrieval via dynamic multi-task regression
#@ Gunhee Kim;Eric P. Xing
#t 2013
#c 2
#% 229931
#% 425052
#% 571769
#% 577224
#% 788094
#% 915344
#% 956521
#% 997240
#% 1119135
#% 1119142
#% 1130999
#% 1131921
#% 1211771
#% 1464097
#% 1484416
#% 1484459
#% 1484617
#% 1495442
#% 1536521
#% 1649067
#% 1746858
#% 1872359
#! In this paper, we investigate a time-sensitive image retrieval problem, in which given a query keyword, a query time point, and optionally user information, we retrieve the most relevant and temporally suitable images from the database. Inspired by recently emerging interests on query dynamics in information retrieval research, our time-sensitive image retrieval algorithm can infer users' implicit search intent better and provide more engaging and diverse search results according to temporal trends of Web user photos. We model observed image streams as instances of multivariate point processes represented by several different descriptors, and develop a regularized multi-task regression framework that automatically selects and learns stochastic parametric models to solve the relations between image occurrence probabilities and various temporal factors that influence them. Using Flickr datasets of more than seven million images of 30 topics, our experimental results show that the proposed algorithm is more successful in time-sensitive image retrieval than other candidate methods, including ranking SVM, a PageRank-based image ranking, and a generative temporal topic model.

#index 1948140
#* Absence time and user engagement: evaluating ranking functions
#@ Georges Dupret;Mounia Lalmas
#t 2013
#c 2
#% 411762
#% 879567
#% 1055676
#% 1074107
#% 1130811
#% 1130868
#% 1130878
#% 1190055
#% 1355038
#% 1450895
#% 1536534
#% 1573487
#% 1667281
#% 1879012
#% 1888037
#! In the online industry, user engagement is measured with various engagement metrics used to assess users' depth of engagement with a website. Widely-used metrics include clickthrough rates, page views and dwell time. Relying solely on these metrics can lead to contradictory if not erroneous conclusions regarding user engagement. In this paper, we propose the time between two user visits, or the absence time, to measure user engagement. Our assumption is that if users find a website interesting, engaging or useful, they will return to it sooner -a reflection of their engagement with the site -than if this is not the case. This assumption has the advantage of being simple and intuitive and applicable to a large number of settings. As a case study, we use a community Q&A website, and compare the behaviour of users exposed to six functions used to rank past answers, both in terms of traditional metrics and absence time. We use Survival Analysis to show the relation between absence time and other engagement metrics. We demonstrate that the absence time leads to coherent, interpretable results and helps to better understand other metrics commonly used to evaluate user engagement in search.

#index 1948141
#* Reusing historical interaction data for faster online learning to rank for IR
#@ Katja Hofmann;Anne Schuth;Shimon Whiteson;Maarten de Rijke
#t 2013
#c 2
#% 384911
#% 411762
#% 425053
#% 466751
#% 577224
#% 840846
#% 876056
#% 1035578
#% 1073938
#% 1130811
#% 1166517
#% 1211840
#% 1268491
#% 1399999
#% 1450912
#% 1536534
#% 1641943
#% 1641945
#% 1667281
#% 1693892
#% 1792878
#% 1919816
#% 1949159
#! Online learning to rank for information retrieval (IR) holds promise for allowing the development of "self-learning" search engines that can automatically adjust to their users. With the large amount of e.g., click data that can be collected in web search settings, such techniques could enable highly scalable ranking optimization. However, feedback obtained from user interactions is noisy, and developing approaches that can learn from this feedback quickly and reliably is a major challenge. In this paper we investigate whether and how previously collected (historical) interaction data can be used to speed up learning in online learning to rank for IR. We devise the first two methods that can utilize historical data (1) to make feedback available during learning more reliable and (2) to preselect candidate ranking functions to be evaluated in interactions with users of the retrieval system. We evaluate both approaches on 9 learning to rank data sets and find that historical data can speed up learning, leading to substantially and significantly higher online performance. In particular, our pre-selection method proves highly effective at compensating for noise in user feedback. Our results show that historical data can be used to make online learning to rank for IR much more effective than previously possible, especially when feedback is noisy.

#index 1948142
#* Pairwise ranking aggregation in a crowdsourced setting
#@ Xi Chen;Paul N. Bennett;Kevyn Collins-Thompson;Eric Horvitz
#t 2013
#c 2
#% 416797
#% 961137
#% 1268491
#% 1415710
#% 1472273
#% 1551226
#% 1605919
#% 1746846
#% 1875704
#! Inferring rankings over elements of a set of objects, such as documents or images, is a key learning problem for such important applications as Web search and recommender systems. Crowdsourcing services provide an inexpensive and efficient means to acquire preferences over objects via labeling by sets of annotators. We propose a new model to predict a gold-standard ranking that hinges on combining pairwise comparisons via crowdsourcing. In contrast to traditional ranking aggregation methods, the approach learns about and folds into consideration the quality of contributions of each annotator. In addition, we minimize the cost of assessment by introducing a generalization of the traditional active learning scenario to jointly select the annotator and pair to assess while taking into account the annotator quality, the uncertainty over ordering of the pair, and the current model uncertainty. We formalize this as an active learning strategy that incorporates an exploration-exploitation tradeoff and implement it using an efficient online Bayesian updating scheme. Using simulated and real-world data, we demonstrate that the active learning strategy achieves significant reductions in labeling cost while maintaining accuracy.

#index 1948143
#* Optimizing parallel algorithms for all pairs similarity search
#@ Maha Ahmed Alabduljalil;Xun Tang;Tao Yang
#t 2013
#c 2
#% 204673
#% 345087
#% 387427
#% 479973
#% 769944
#% 813966
#% 867054
#% 869500
#% 893164
#% 956506
#% 956518
#% 963669
#% 1035590
#% 1055684
#% 1227596
#% 1280780
#% 1426543
#% 1450881
#% 1535356
#% 1746916
#! All pairs similarity search is used in many web search and data mining applications. Previous work has used comparison filtering, inverted indexing, and parallel accumulation of partial intermediate results to expedite its execution. However, shuffling intermediate results can incur significant communication overhead as data scales up. This paper studies a scalable two-step approach called Partition-based Similarity Search (PSS) which incorporates several optimization techniques. First, PSS uses a static partitioning algorithm that places dissimilar vectors into different groups and balance the comparison workload with a circular assignment. Second, PSS executes comparison tasks in parallel, each using a hybrid data structure that combines the advantages of forward and inverted indexing. Our evaluation results show that the proposed approach leads to an early elimination of unnecessary I/O and data communication while sustaining parallel efficiency. As a result, it improves performance by an order of magnitude when dealing with large datasets.

#index 1948144
#* Bursty subgraphs in social networks
#@ Milad Eftekhar;Nick Koudas;Yashar Ganjali
#t 2013
#c 2
#% 342596
#% 577220
#% 729923
#% 729943
#% 1355042
#% 1451233
#% 1523892
#% 1536507
#% 1536528
#% 1693876
#% 1693913
#! Data available through social media and content sharing platforms present opportunities for analysis and mining. In the context of social networks, it is interesting to formalize and locate bursts of activities amongst users, related to a particular event and to report sets of socially connected users participating in such bursts. Such collections present new opportunities for understanding social events, and render new ways of online marketing. In this paper, we model social information using two conceptualized graph models. The first one (the action graph) provides a detailed model of all activities of all users while the second one (the holistic graph) provides an aggregate view on each user in the social media. We also propose two models to define the notion of "burst". The first model (intrinsic burst model) takes the intrinsic characteristics of each user into account to recognize the bursty behaviors; while the second model (social burst model) considers neighbors' influences when identifying bursts. We provide two linear algorithms to detect bursts based on the proposed models. These algorithms have been extensively evaluated on a month of full Twitter dataset certifying the practicality of our approach. A detailed qualitative study of our techniques is also presented.

#index 1948145
#* Sharding social networks
#@ Quang Duong;Sharad Goel;Jake Hofman;Sergei Vassilvitskii
#t 2013
#c 2
#% 346696
#% 823395
#% 891559
#% 1055741
#% 1399992
#% 1426483
#% 1464649
#% 1468392
#% 1581906
#% 1948175
#! Online social networking platforms regularly support hundreds of millions of users, who in aggregate generate substantially more data than can be stored on any single physical server. As such, user data are distributed, or sharded, across many machines. A key requirement in this setting is rapid retrieval not only of a given user's information, but also of all data associated with his or her social contacts, suggesting that one should consider the topology of the social network in selecting a sharding policy. In this paper we formalize the problem of efficiently sharding large social network databases, and evaluate several sharding strategies, both analytically and empirically. We find that random sharding---the de facto standard---results in provably poor performance even when frequently accessed nodes are replicated to many shards. By contrast, we demonstrate that one can substantially reduce querying costs by identifying and assigning tightly knit communities to shards. In particular, our theoretical analysis motivates a novel, scalable sharding algorithm that outperforms both random and location-based sharding schemes.

#index 1948146
#* Arrival and departure dynamics in social networks
#@ Shaomei Wu;Atish Das Sarma;Alex Fabrikant;Silvio Lattanzi;Andrew Tomkins
#t 2013
#c 2
#% 54200
#% 511151
#% 823342
#% 880397
#% 881054
#% 881460
#% 881523
#% 1044492
#% 1083624
#% 1083641
#% 1083675
#% 1198232
#% 1536509
#% 1560424
#% 1888895
#! In this paper, we consider the natural arrival and departure of users in a social network, and ask whether the dynamics of arrival, which have been studied in some depth, also explain the dynamics of departure, which are not as well studied. Through study of the DBLP co-authorship network and a large online social network, we show that the dynamics of departure behave differently from the dynamics of formation. In particular, the probability of departure of a user with few friends may be understood most accurately as a function of the raw number of friends who are active. For users with more friends, however, the probability of departure is best predicted by the overall fraction of the user's neighborhood that is active, independent of size. We then study global properties of the sub-graphs induced by active and inactive users, and show that active users tend to belong to a core that is densifying and is significantly denser than the inactive users. Further, the inactive set of users exhibit a higher density and lower conductance than the degree distribution alone can explain. These two aspects suggest that nodes at the fringe are more likely to depart and subsequent departure are correlated among neighboring nodes in tightly-knit communities.

#index 1948147
#* Three findings regarding privacy online
#@ Catherine Tucker
#t 2013
#c 2
#% 1531699
#! The Internet now enables firms to collect detailed and potentially intrusive data about their customers both easily and cheaply. I discuss three empirical results related to customer privacy-protection that is enacted in response to this change. 1) Privacy protection that focuses on obtaining consent appears to lead to less effective advertising. This is based on results from extensive empirical analysis into how effectiveness of online advertising changed in response to the implementation of the E-Privacy Directive in Europe. 2) Privacy protection which gives direct control over customers' privacy appears to enhance economic outcomes. This is based on a detailed case study of customer responsiveness to different forms of advertising in the wake of a change in Facebook privacy policies. 3) Restricting the length of time that potentially private data is stored appears to have little economic impact. This is based on empirical analysis of aggregate search behavior following a change in the length of time search engines were told they could store search engine query data in Europe.

#index 1948148
#* Optimized interleaving for online retrieval evaluation
#@ Filip Radlinski;Nick Craswell
#t 2013
#c 2
#% 577224
#% 642975
#% 731615
#% 1077150
#% 1130811
#% 1214757
#% 1292763
#% 1450892
#% 1450912
#% 1536534
#% 1573487
#% 1641943
#% 1667281
#% 1918346
#! Interleaving is an online evaluation technique for comparing the relative quality of information retrieval functions by combining their result lists and tracking clicks. A sequence of such algorithms have been proposed, each being shown to address problems in earlier algorithms. In this paper, we formalize and generalize this process, while introducing a formal model: We identify a set of desirable properties for interleaving, then show that an interleaving algorithm can be obtained as the solution to an optimization problem within those constraints. Our approach makes explicit the parameters of the algorithm, as well as assumptions about user behavior. Further, we show that our approach leads to an unbiased and more efficient interleaving algorithm than any previous approach, using a novel log-based analysis of user search behavior.

#index 1948149
#* Mining the web to predict future events
#@ Kira Radinsky;Eric Horvitz
#t 2013
#c 2
#% 350859
#% 577220
#% 939376
#% 956509
#% 956564
#% 1155729
#% 1269526
#% 1470600
#% 1560381
#% 1642059
#% 1642159
#% 1746888
#! We describe and evaluate methods for learning to forecast forthcoming events of interest from a corpus containing 22 years of news stories. We consider the examples of identifying significant increases in the likelihood of disease outbreaks, deaths, and riots in advance of the occurrence of these events in the world. We provide details of methods and studies, including the automated extraction and generalization of sequences of events from news corpora and multiple web resources. We evaluate the predictive power of the approach on real-world events withheld from the system.

#index 1948150
#* Studying inter-national mobility through IP geolocation
#@ Bogdan State;Ingmar Weber;Emilio Zagheni
#t 2013
#c 2
#% 1147434
#% 1147435
#% 1278589
#% 1391820
#% 1400055
#% 1429406
#% 1450019
#% 1479668
#% 1566955
#% 1621337
#% 1641651
#% 1905956
#! The increasing ubiquity of Internet use has opened up new avenues in the study of human mobility. Easily-obtainable geolocation data resulting from repeated logins to the same website offer the possibility of observing long-term patterns of mobility for a large number of individuals. We use data on the geographic locations from where over 100 million anonymized users log into Yahoo!~services to generate the first global map of short- and medium-term mobility flows. We develop a protocol to identify anonymized users who, over a one-year period, had spent more than 3 months in a different country from their stated country of residence ("migrants"), and users who spent less than a month in another country ("tourists"). We compute aggregate estimates of migration propensities between countries, as inferred from a user's location over the observed period. Geolocation data allow us to characterize also the pendularity of migration flows -- i.e., the extent to which migrants travel back and forth between their countries of origin and destination. We use data regarding visa regimes, colonial ties, geographic location and economic development to predict migration and tourism flows. Our analysis shows the persistence of traditional migration patterns as well as the emergence of new routes. Migrations tend to be more pendular between countries that are close to each other. We observe particularly high levels of pendularity within the European Economic Area, even after we control for distance and visa regimes. The dataset, methodology and results presented have important implications for the travel industry, as well as for several disciplines in social sciences, including geography, demography and the sociology of networks.

#index 1948151
#* From machu_picchu to "rafting the urubamba river": anticipating information needs via the entity-query graph
#@ Ilaria Bordino;Gianmarco De Francisci Morales;Ingmar Weber;Francesco Bonchi
#t 2013
#c 2
#% 248218
#% 577302
#% 818207
#% 838531
#% 860672
#% 869501
#% 879567
#% 987211
#% 987212
#% 987372
#% 989578
#% 1019082
#% 1130854
#% 1130858
#% 1130868
#% 1173699
#% 1190074
#% 1214667
#% 1214725
#% 1280748
#% 1287706
#% 1292502
#% 1333455
#% 1355035
#% 1399955
#% 1426513
#% 1450885
#% 1450977
#% 1484390
#% 1560355
#% 1560359
#% 1609172
#% 1632459
#% 1641928
#% 1711796
#% 1712595
#% 1879027
#! We study the problem of anticipating user search needs, based on their browsing activity. Given the current web page p that a user is visiting we want to recommend a small and diverse set of search queries that are relevant to the content of p, but also non-obvious and serendipitous. We introduce a novel method that is based on the content of the page visited, rather than on past browsing patterns as in previous literature. Our content-based approach can be used even for previously unseen pages. We represent the topics of a page by the set of Wikipedia entities extracted from it. To obtain useful query suggestions for these entities, we exploit a novel graph model that we call EQGraph (Entity-Query Graph), containing entities, queries, and transitions between entities, between queries, as well as from entities to queries. We perform Personalized PageRank computation on such a graph to expand the set of entities extracted from a page into a richer set of entities, and to associate these entities with relevant query suggestions. We develop an efficient implementation to deal with large graph instances and suggest queries from a large and diverse pool. We perform a user study that shows that our method produces relevant and interesting recommendations, and outperforms an alternative method based on reverse IR.

#index 1948152
#* Personalizing atypical web search sessions
#@ Carsten Eickhoff;Kevyn Collins-Thompson;Paul N. Bennett;Susan Dumais
#t 2013
#c 2
#% 201991
#% 309767
#% 399057
#% 641979
#% 754126
#% 771571
#% 805200
#% 818259
#% 832349
#% 881540
#% 919706
#% 987224
#% 1043044
#% 1130878
#% 1166518
#% 1173692
#% 1227621
#% 1227622
#% 1301004
#% 1355036
#% 1357833
#% 1384094
#% 1397425
#% 1536505
#% 1587349
#% 1587350
#% 1598334
#% 1598368
#% 1641961
#% 1693883
#% 1693885
#% 1879011
#% 1967798
#! Most research in Web search personalization models users as static or slowly evolving entities with a given set of preferences defined by their past behavior. However, recent publications as well as empirical evidence suggest that for a significant number of search sessions, users diverge from their regular search profiles in order to satisfy atypical, limited-duration information needs. In this work, we conduct a large-scale inspection of real-life search sessions to further understand this scenario. Subsequently, we design an automatic means of detecting and supporting such atypical sessions. We demonstrate significant improvements over state-of-the-art Web search personalization techniques by accounting for the typicality of search sessions. The proposed method is evaluated based on Web-scale search session data spanning several months of user activity.

#index 1948153
#* Expediting search trend detection via prediction of query counts
#@ Nadav Golbandi Golbandi;Liran Katzir Katzir;Yehuda Koren Koren;Ronny Lempel Lempel
#t 2013
#c 2
#% 279755
#% 765412
#% 1001364
#% 1023420
#% 1355017
#% 1426611
#% 1450878
#% 1467831
#% 1481645
#% 1536521
#% 1598486
#! The massive volume of queries submitted to major Web search engines reflects human interest at a global scale. While the popularity of many search queries is stable over time or fluctuates with periodic regularity, some queries experience a sudden and ephemeral rise in popularity that is unexplained by their past volumes. Typically the popularity surge is precipitated by some real-life event in the news cycle. Such queries form what are known as search trends. All major search engines, using query log analysis and other signals, invest in detecting such trends. The goal is to surface trends accurately, with low latency relative to the actual event that sparked the trend. This work formally defines precision, recall and latency metrics related to top-k search trend detection. Then, observing that many trend detection algorithms rely on query counts, we develop a linear auto-regression model to predict future query counts. Subsequently, we tap the predicted counts to expedite search trend detection by plugging them into an existing trend detection scheme. Experimenting with query logs from a major Web search engine, we report both the stand-alone accuracy of our query count predictions, as well as the task-oriented effects of the prediction on the emitted trends. We show an average reduction in trend detection latency of roughly twenty minutes, with a negligible impact on the precision and recall metrics.

#index 1948154
#* News recommendation via hypergraph learning: encapsulation of user behavior and news content
#@ Lei Li;Tao Li
#t 2013
#c 2
#% 330687
#% 397155
#% 734592
#% 754106
#% 875947
#% 956521
#% 983922
#% 1083698
#% 1127465
#% 1159803
#% 1176912
#% 1190124
#% 1214623
#% 1279654
#% 1309886
#% 1355025
#% 1356185
#% 1399999
#% 1476453
#% 1484439
#% 1486729
#% 1545597
#% 1560184
#% 1598346
#% 1625390
#% 1650470
#% 1710959
#% 1869463
#! Personalized news recommender systems have gained increasing attention in recent years. Within a news reading community, the implicit correlations among news readers, news articles, topics and named entities, e.g., what types of named entities in articles are preferred by users, and why users like the articles, could be valuable for building an effective news recommender. In this paper, we propose a novel news personalization framework by mining such correlations. We use hypergraph to model various high-order relations among different objects in news data, and formulate news recommendation as a ranking problem on fine-grained hypergraphs. In addition, by transductive inference, our proposed algorithm is capable of effectively handling the so-called cold-start problem. Extensive experiments on a data set collected from various news websites have demonstrated the effectiveness of our proposed algorithm.

#index 1948155
#* Group sparse topical coding: from code to topic
#@ Lu Bai;Jiafeng Guo;Yanyan Lan;Xueqi Cheng
#t 2013
#c 2
#% 329569
#% 722904
#% 879587
#% 961184
#% 1034713
#% 1073906
#% 1117695
#% 1130899
#% 1211744
#% 1211800
#% 1305467
#% 1598402
#% 1605963
#% 1742154
#! Learning low dimensional representations of text corpora is critical in many content analysis and data mining applications. It is even more desired and challenging to learn a sparse representation in practice for large scale text modeling. However, traditional probabilistic topic models (PTM) lack a mechanism to directly control the posterior sparsity of the inferred representations; While the emerged non-probabilistic models (NPM) can explicitly control sparsity using sparse constraint like l_1 norm, they convey different limitations in latent representations. To address the existing problems, we propose a novel non-probabilistic topic model for discovering sparse latent representations of large text corpora, referred as group sparse topical coding (GSTC). Our model enjoys both the merits of the PTMs and NPMs. On one hand, GSTC can naturally derive document-level admixture proportions in topic simplex like PTMs, which is useful for semantic analysis, classification or retrieval. On the other hand, GSTC can directly control the sparsity of the inferred representations with group lasso by relaxing the normalization constraint. Moreover, the relaxed non-probabilistic GSTC can be effectively learned using coordinate descent method. Experimental results on benchmark datasets show that GSTC can discover meaningful compact latent representations of documents, and improve the document classification accuracy and time efficiency.

#index 1948156
#* TYPiMatch: type-specific unsupervised learning of keys and key values for heterogeneous web data integration
#@ Yongtao Ma;Thanh Tran
#t 2013
#c 2
#% 248801
#% 350103
#% 572256
#% 577247
#% 729913
#% 765463
#% 788090
#% 864392
#% 893164
#% 915242
#% 915340
#% 1022229
#% 1217163
#% 1250576
#% 1523838
#% 1536558
#% 1606344
#% 1641518
#% 1654048
#! Instance matching and blocking, a preprocessing step used for selecting candidate matches, require determining the most representative attributes of instances called keys, based on which similarities between instances are computed. We show that for the problem of learning blocking keys and key values, both generic techniques that do not exploit type information and supervised learning techniques optimized for one single predefined type of instances do not perform well on heterogeneous Web data capturing instances for which the predefined type is too general. That is, they actually belong to some subtypes that are not explicitly specified in the data. We propose an unsupervised approach for learning these subtypes and the subtype-specific blocking keys and key values. Compared to state-of-the-art supervised and unsupervised learning approaches that are optimized for one single type, our approach improves efficiency as well as result quality. In particular, we show that the proposed strategy of learning subtype-specific blocking keys and key values improves both blocking and instance matching results.

#index 1948157
#* Robust query rewriting using anchor data
#@ Nick Craswell;Bodo Billerbeck;Dennis Fetterly;Marc Najork
#t 2013
#c 2
#% 340901
#% 348155
#% 577301
#% 578337
#% 754125
#% 766497
#% 810906
#% 817577
#% 824588
#% 869501
#% 869548
#% 907493
#% 987222
#% 1074094
#% 1074098
#% 1227584
#% 1292550
#% 1355020
#% 1399978
#% 1442574
#% 1482362
#% 1582081
#% 1598394
#% 1598414
#% 1693906
#! Query rewriting algorithms can be used as a form of query expansion, by combining the user's original query with automatically generated rewrites. Rewriting algorithms bring linguistic datasets to bear without the need for iterative relevance feedback, but most studies of rewriting have used proprietary datasets such as large-scale search logs. By contrast this paper uses readily available data, particularly ClueWeb09 link text with over 1.2 billion anchor phrases, to generate rewrites. To avoid overfitting, our initial analysis is performed using Million Query Track queries, leading us to identify three algorithms which perform well. We then test the algorithms on Web and newswire data. Results show good properties in terms of robustness and early precision.

#index 1948158
#* Wiki3C: exploiting wikipedia for context-aware concept categorization
#@ Peng Jiang;Huiman Hou;Lijiang Chen;Shimin Chen;Conglei Yao;Chengkai Li;Min Wang
#t 2013
#c 2
#% 766409
#% 869521
#% 987328
#% 987333
#% 1019082
#% 1019105
#% 1055680
#% 1130858
#% 1166528
#% 1214660
#% 1227594
#% 1250362
#% 1250381
#% 1269895
#% 1269899
#% 1275012
#% 1330552
#% 1338627
#% 1471191
#% 1475763
#% 1483550
#% 1489451
#% 1625382
#! Wikipedia is an important human generated knowledge base containing over 21 million articles organized by millions of categories. In this paper, we exploit Wikipedia for a new task of text mining: Context-aware Concept Categorization. In the task, we focus on categorizing concepts according to their context. We exploit article link feature and category structure in Wikipedia, followed by introducing Wiki3C, an unsupervised and domain independent concept categorization approach based on context. In the approach, we investigate two strategies to select and filter Wikipedia articles for the category representation. Besides, a probabilistic model is employed to compute the semantic relatedness between two concepts in Wikipedia. Experimental evaluation using manually labeled ground truth shows that our proposed Wiki3C can achieve a noticeable improvement over the baselines without considering contextual information.

#index 1948159
#* Crawling deep web entity pages
#@ Yeye He;Dong Xin;Venkatesh Ganti;Sriram Rajaraman;Nirav Shah
#t 2013
#c 2
#% 255137
#% 809418
#% 835018
#% 864434
#% 955762
#% 956504
#% 956507
#% 956538
#% 1019130
#% 1063570
#% 1083644
#% 1127557
#% 1227610
#% 1266249
#% 1355054
#% 1484298
#! Deep-web crawl is concerned with the problem of surfacing hidden content behind search interfaces on the Web. While many deep-web sites maintain document-oriented textual content (e.g., Wikipedia, PubMed, Twitter, etc.), which has traditionally been the focus of the deep-web literature, we observe that a significant portion of deep-web sites, including almost all online shopping sites, curate structured entities as opposed to text documents. Although crawling such entity-oriented content is clearly useful for a variety of purposes, existing crawling techniques optimized for document oriented content are not best suited for entity-oriented sites. In this work, we describe a prototype system we have built that specializes in crawling entity-oriented deep-web sites. We propose techniques tailored to tackle important subproblems including query generation, empty page filtering and URL deduplication in the specific context of entity oriented deep-web sites. These techniques are experimentally evaluated and shown to be effective.

#index 1948160
#* Using early view patterns to predict the popularity of youtube videos
#@ Henrique Pinto;Jussara M. Almeida;Marcos A. Gonçalves
#t 2013
#c 2
#% 757953
#% 956564
#% 1002005
#% 1309092
#% 1399995
#% 1411585
#% 1423984
#% 1517940
#% 1536579
#% 1693924
#! Predicting Web content popularity is an important task for supporting the design and evaluation of a wide range of systems, from targeted advertising to effective search and recommendation services. We here present two simple models for predicting the future popularity of Web content based on historical information given by early popularity measures. Our approach is validated on datasets consisting of videos from the widely used YouTube video-sharing portal. Our experimental results show that, compared to a state-of-the-art baseline model, our proposed models lead to significant decreases in relative squared errors, reaching up to 20% reduction on average, and larger reductions (of up to 71%) for videos that experience a high peak in popularity in their early days followed by a sharp decrease in popularity.

#index 1948161
#* Geo topic model: joint modeling of user's activity area and interests for location recommendation
#@ Takeshi Kurashima;Tomoharu Iwata;Takahide Hoshide;Noriko Takaya;Ko Fujimura
#t 2013
#c 2
#% 643007
#% 722904
#% 723186
#% 734590
#% 769895
#% 867057
#% 956521
#% 982675
#% 1047432
#% 1190131
#% 1190134
#% 1214685
#% 1305518
#% 1400036
#% 1429406
#% 1482236
#% 1484411
#% 1524237
#% 1598364
#% 1650298
#% 1872249
#! This paper proposes a method that analyzes the location log data of multiple users to recommend locations to be visited. The method uses our new topic model, called Geo Topic Model, that can jointly estimate both the user's interests and activity area hosting the user's home, office and other personal places. By explicitly modeling geographical features of locations and users, the user's interests in other features of locations, which we call latent topics, can be inferred effectively. The topic interests estimated by our model 1) lead to high accuracy in predicting visit behavior as driven by personal interests, 2) make possible the generation of recommendations when the user is in an unfamiliar area (e.g. sightseeing), and 3) enable the recommender system to suggest an interpretable representation of the user profile that can be customized by the user. Experiments are conducted using real location logs of landmark and restaurant visits to evaluate the recommendation performance of the proposed method in terms of the accuracy of predicting visit selections. We also show that our model can estimate latent features of locations such as art, nature and atmosphere as latent topics, and describe each user's preference based on them.

#index 1948162
#* Latent factor models with additive and hierarchically-smoothed user preferences
#@ Amr Ahmed;Bhargav Kanagal;Sandeep Pandey;Vanja Josifovski;Lluis Garcia Pueyo;Jeff Yuan
#t 2013
#c 2
#% 414514
#% 1083671
#% 1127452
#% 1211838
#% 1214623
#% 1214666
#% 1260273
#% 1287234
#% 1355024
#% 1417104
#% 1605925
#% 1625364
#% 1693873
#! Items in recommender systems are usually associated with annotated attributes: for e.g., brand and price for products; agency for news articles, etc. Such attributes are highly informative and must be exploited for accurate recommendation. While learning a user preference model over these attributes can result in an interpretable recommender system and can hands the cold start problem, it suffers from two major drawbacks: data sparsity and the inability to model random effects. On the other hand, latent-factor collaborative filtering models have shown great promise in recommender systems; however, its performance on rare items is poor. In this paper we propose a novel model LFUM, which provides the advantages of both of the above models. We learn user preferences (over the attributes) using a personalized Bayesian hierarchical model that uses a combination(additive model) of a globally learned preference model along with user-specific preferences. To combat data-sparsity, we smooth these preferences over the item-taxonomy using an efficient forward-filtering and backward-smoothing inference algorithm. Our inference algorithms can handle both discrete attributes (e.g., item brands) and continuous attributes (e.g., item prices). We combine the user preferences with the latent-factor models and train the resulting collaborative filtering system end-to-end using the successful BPR ranking algorithm. In our extensive experimental analysis, we show that our proposed model outperforms several commonly used baselines and we carry out an ablation study showing the benefits of each component of our model.

#index 1948163
#* App recommendation: a contest between satisfaction and temptation
#@ Peifeng Yin;Ping Luo;Wang-Chien Lee;Min Wang
#t 2013
#c 2
#% 190581
#% 280852
#% 301259
#% 330687
#% 342685
#% 458379
#% 581878
#% 722904
#% 765519
#% 765520
#% 840924
#% 1023369
#% 1073982
#% 1260273
#% 1355025
#% 1396094
#% 1476448
#% 1480830
#% 1535449
#% 1536533
#% 1558464
#% 1558820
#% 1589885
#% 1598363
#% 1598366
#% 1598434
#% 1617267
#% 1646507
#% 1673052
#% 1741311
#! Due to the huge and still rapidly growing number of mobile applications (apps), it becomes necessary to provide users an app recommendation service. Different from conventional item recommendation where the user interest is the primary factor, app recommendation also needs to consider factors that invoke a user to replace an old app (if she already has one) with a new app. In this work we propose an Actual- Tempting model that captures such factors in the decision process of mobile app adoption. The model assumes that each owned app has an actual satisfactory value and a new app under consideration has a tempting value. The former stands for the real satisfactory value the owned app brings to the user while the latter represents the estimated value the new app may seemingly have. We argue that the process of app adoption therefore is a contest between the owned apps' actual values and the candidate app's tempting value. Via the extensive experiments we show that the AT model performs significantly better than the conventional recommendation techniques such as collaborative filtering and content-based recommendation. Furthermore, the best recommendation performance is achieved when the AT model is combined with them.

#index 1948164
#* Threading machine generated email
#@ Nir Ailon;Zohar S. Karnin;Edo Liberty;Yoelle Maarek
#t 2013
#c 2
#% 230532
#% 320944
#% 466240
#% 844308
#% 879568
#% 1606030
#! Viewing email messages as parts of a sequence or a thread is a convenient way to quickly understand their context. Current threading techniques rely on purely syntactic methods, matching sender information, subject line, and reply/forward prefixes. As such, they are mostly limited to personal conversations. In contrast, machine-generated email, which amount, as per our experiments, to more than 60% of the overall email traffic, requires a different kind of threading that should reflect how a sequence of emails is caused by a few related user actions. For example, purchasing goods from an online store will result in a receipt or a confirmation message, which may be followed, possibly after a few days, by a shipment notification message from an express shipping service. In today's mail systems, they will not be a part of the same thread, while we believe they should. In this paper, we focus on this type of threading that we coin "causal threading". We demonstrate that, by analyzing recurring patterns over hundreds of millions of mail users, we can infer a causality relation between these two individual messages. In addition, by observing multiple causal relations over common messages, we can generate "causal threads" over a sequence of messages. The four key stages of our approach consist of: (1) identifying messages that are instances of the same email type or "template" (generated by the same machine process on the sender side) (2) building a causal graph, in which nodes correspond to email templates and edges indicate potential causal relations (3) learning a causal relation prediction function, and (4) automatically "threading" the incoming email stream. We present detailed experimental results obtained by analyzing the inboxes of 12.5 million Yahoo! Mail users, who voluntarily opted-in for such research. Supervised editorial judgments show that we can identify more than 70% (recall rate) of all "causal threads" at a precision level of 90%. In addition, for a search scenario we show that we achieve a precision close to 80% at 90% recall. We believe that supporting causal threads in email clients opens new grounds for improving both email search and browsing experiences.

#index 1948165
#* Predicting content change on the web
#@ Kira Radinsky;Paul N. Bennett
#t 2013
#c 2
#% 209021
#% 310545
#% 330604
#% 348137
#% 390723
#% 480136
#% 577370
#% 640706
#% 731406
#% 734915
#% 805879
#% 836154
#% 956541
#% 956546
#% 993974
#% 1166533
#% 1369421
#% 1465382
#% 1536501
#% 1560388
#! Accurate prediction of changing web page content improves a variety of retrieval and web related components. For example, given such a prediction algorithm one can both design a better crawling strategy that only recrawls pages when necessary as well as a proactive mechanism for personalization that pushes content associated with user revisitation directly to the user. While many techniques for modeling change have focused simply on past change frequency, our work goes beyond that by additionally studying the usefulness in page change prediction of: the page's content; the degree and relationship among the prediction page's observed changes; the relatedness to other pages and the similarity in the types of changes they undergo. We present an expert prediction framework that incorporates the information from these other signals more effectively than standard ensemble or basic relational learning techniques. In an empirical analysis, we find that using page content as well as related pages significantly improves prediction accuracy and compare it to common approaches. We present numerous similarity metrics to identify related pages and focus specifically on measures of temporal content similarity. We observe that the different metrics yield related pages that are qualitatively different in nature and have different effects on the prediction performance.

#index 1948166
#* Triplex transfer learning: exploiting both shared and distinct concepts for text classification
#@ Fuzhen Zhuang;Ping Luo;Changying Du;Qing He;Zhongzhi Shi
#t 2013
#c 2
#% 329569
#% 466263
#% 881468
#% 983828
#% 989592
#% 1019099
#% 1074074
#% 1074129
#% 1083655
#% 1130817
#% 1211714
#% 1214655
#% 1227700
#% 1270196
#% 1318623
#% 1377374
#% 1451257
#% 1464068
#% 1482214
#% 1598427
#% 1606063
#% 1617368
#% 1826308
#% 1862326
#! Transfer learning focuses on the learning scenarios when the test data from target domains and the training data from source domains are drawn from similar but different data distribution with respect to the raw features. Some recent studies argued that the high-level concepts (e.g. word clusters) can help model the data distribution difference, and thus are more appropriate for classification. Specifically, these methods assume that all the data domains have the same set of shared concepts, which are used as the bridge for knowledge transfer. However, besides these shared concepts each domain may have its own distinct concepts. To address this point, we propose a general transfer learning framework based on non-negative matrix tri-factorization which allows to explore both shared and distinct concepts among all the domains simultaneously. Since this model provides more flexibility in fitting the data it may lead to better classification accuracy. To solve the proposed optimization problem we develop an iterative algorithm and also theoretically analyze its convergence. Finally, extensive experiments show the significant superiority of our model over the baseline methods. In particular, we show that our method works much better in the more challenging tasks when distinct concepts may exist.

#index 1948167
#* Have you done anything like that?: predicting performance using inter-category reputation
#@ Marios Kokkodis;Panagiotis G. Ipeirotis
#t 2013
#c 2
#% 739578
#% 879593
#% 1035587
#% 1055738
#% 1074111
#% 1166519
#% 1176947
#% 1183174
#% 1190060
#% 1190069
#% 1219797
#% 1261574
#% 1400002
#% 1450880
#% 1495595
#% 1537498
#% 1605931
#% 1633079
#! Online labor markets such as oDesk and Amazon Mechanical Turk have been growing in importance over the last few years. In these markets, employers post tasks on which remote contractors work and deliver the product of their work. As in most online marketplaces, reputation mechanisms play a very important role in facilitating transactions, since they instill trust and are often predictive of the future satisfaction of the employer. However, labor markets are usually highly heterogeneous in terms of available task categories; in such scenarios, past performance may not be a representative signal of future performance. To account for this heterogeneity, in our work, we build models that predict the performance of a worker based on prior, category-specific feedback. Our models assume that each worker has a category-specific quality, which is latent and not directly observable; what is observable, though, is the set of feedback ratings of the worker and of other contractors with similar work histories. Based on this information, we build a multi-level, hierarchical scheme that deals effectively with the data sparseness, which is inherent in many cases of interest (i.e., contractors with relatively brief work histories). We evaluate our models on a large corpus of real transactional data from oDesk, an online labor market with hundreds of millions of dollars in transaction volume. Our results show an improved accuracy of up to 47% compared to the existing baseline.

#index 1948168
#* Learning multiple-question decision trees for cold-start recommendation
#@ Mingxuan Sun;Fuxin Li;Joonseok Lee;Ke Zhou;Guy Lebanon;Hongyuan Zha
#t 2013
#c 2
#% 280852
#% 330687
#% 342767
#% 397155
#% 734592
#% 788069
#% 840924
#% 1074062
#% 1127452
#% 1200881
#% 1211765
#% 1214623
#% 1227603
#% 1291600
#% 1482440
#% 1536564
#% 1598365
#% 1650470
#% 1650569
#% 1672989
#! For cold-start recommendation, it is important to rapidly profile new users and generate a good initial set of recommendations through an interview process --- users should be queried adaptively in a sequential fashion, and multiple items should be offered for opinion solicitation at each trial. In this work, we propose a novel algorithm that learns to conduct the interview process guided by a decision tree with multiple questions at each split. The splits, represented as sparse weight vectors, are learned through an L_1-constrained optimization framework. The users are directed to child nodes according to the inner product of their responses and the corresponding weight vector. More importantly, to account for the variety of responses coming to a node, a linear regressor is learned within each node using all the previously obtained answers as input to predict item ratings. A user study, preliminary but first in its kind in cold-start recommendation, is conducted to explore the efficient number and format of questions being asked in a recommendation survey to minimize user cognitive efforts. Quantitative experimental validations also show that the proposed algorithm outperforms state-of-the-art approaches in terms of both the prediction accuracy and user cognitive efforts.

#index 1948169
#* Online multi-modal distance learning for scalable multimedia retrieval
#@ Hao Xia;Pengcheng Wu;Steven C.H. Hoi
#t 2013
#c 2
#% 318785
#% 342617
#% 563100
#% 763697
#% 791406
#% 829025
#% 840938
#% 860956
#% 863388
#% 884027
#% 945539
#% 961152
#% 961190
#% 983849
#% 983953
#% 1069003
#% 1073922
#% 1077150
#% 1119135
#% 1119137
#% 1211823
#% 1279781
#% 1327692
#% 1386134
#% 1441138
#% 1474963
#% 1499803
#% 1536524
#% 1536567
#% 1551232
#% 1606366
#% 1747274
#% 1775748
#% 1860977
#% 1861871
#% 1872319
#! In many real-word scenarios, e.g., multimedia applications, data often originates from multiple heterogeneous sources or are represented by diverse types of representation, which is often referred to as "multi-modal data". The definition of distance between any two objects/items on multi-modal data is a key challenge encountered by many real-world applications, including multimedia retrieval. In this paper, we present a novel online learning framework for learning distance functions on multi-modal data through the combination of multiple kernels. In order to attack large-scale multimedia applications, we propose Online Multi-modal Distance Learning (OMDL) algorithms, which are significantly more efficient and scalable than the state-of-the-art techniques. We conducted an extensive set of experiments on multi-modal image retrieval applications, in which encouraging results validate the efficacy of the proposed technique.

#index 1948170
#* Unsupervised graph-based topic labelling using dbpedia
#@ Ioana Hulpus;Conor Hayes;Marcel Karnstedt;Derek Greene
#t 2013
#c 2
#% 722904
#% 875992
#% 876017
#% 878454
#% 989620
#% 1227594
#% 1249548
#% 1288161
#% 1323439
#% 1428692
#% 1450851
#% 1592082
#% 1642249
#% 1650298
#% 1711748
#! Automated topic labelling brings benefits for users aiming at analysing and understanding document collections, as well as for search engines targetting at the linkage between groups of words and their inherent topics. Current approaches to achieve this suffer in quality, but we argue their performances might be improved by setting the focus on the structure in the data. Building upon research for concept disambiguation and linking to DBpedia, we are taking a novel approach to topic labelling by making use of structured data exposed by DBpedia. We start from the hypothesis that words co-occuring in text likely refer to concepts that belong closely together in the DBpedia graph. Using graph centrality measures, we show that we are able to identify the concepts that best represent the topics. We comparatively evaluate our graph-based approach and the standard text-based approach, on topics extracted from three corpora, based on results gathered in a crowd-sourcing experiment. Our research shows that graph-based analysis of DBpedia can achieve better results for topic labelling in terms of both precision and topic coverage.

#index 1948171
#* Estimating content concreteness for finding comprehensible documents
#@ Shinya Tanaka;Adam Jatowt;Makoto P. Kato;Katsumi Tanaka
#t 2013
#c 2
#% 190581
#% 269217
#% 1107148
#% 1292770
#% 1536555
#% 1632527
#% 1641961
#% 1649041
#! Document comprehensibility is one of key factors determining document quality and, in result, user's satisfaction. Relevant web pages are of little utility if they are incomprehensible or impose too much cognitive burden on readers. Traditional measures of text difficulty focus often on syntactic factors of text such as sentence length, word length, syllable count, or they utilize fixed list of common terms. However, document comprehensibility depends on many factors, of which concreteness and the ease of concept visualization are crucial ones. In this paper, we first propose a method for predicting the concreteness of terms using SVM regression. We then extend it to calculating document concreteness level. The experimental results indicate satisfactory accuracy in estimating both term and document concreteness as well as demonstrate positive correlation between the document concreteness and comprehensibility. Our ultimate goal is to enable comprehension-driven search, which will return both relevant and comprehensible results.

#index 1948172
#* Translating related words to videos and back through latent topics
#@ Pradipto Das;Rohini K. Srihari;Jason J. Corso
#t 2013
#c 2
#% 194009
#% 424085
#% 642990
#% 722904
#% 816173
#% 817472
#% 1148273
#% 1166510
#% 1187347
#% 1214625
#% 1260412
#% 1292851
#% 1375807
#% 1482343
#% 1493670
#% 1495421
#% 1502531
#% 1558464
#% 1626633
#% 1711765
#% 1781724
#! Documents containing video and text are becoming more and more widespread and yet content analysis of those documents depends primarily on the text. Although automated discovery of semantically related words from text improves free text query understanding, translating videos into text summaries facilitates better video search particularly in the absence of accompanying text. In this paper, we propose a multimedia topic modeling framework suitable for providing a basis for automatically discovering and translating semantically related words obtained from textual metadata of multimedia documents to semantically related videos or frames from videos. The framework jointly models video and text and is flexible enough to handle different types of document features in their constituent domains such as discrete and real valued features from videos representing actions, objects, colors and scenes as well as discrete features from text. Our proposed models show much better fit to the multimedia data in terms of held-out data log likelihoods. For a given query video, our models translate low level vision features into bag of keyword summaries which can be further translated using simple natural language generation techniques into human readable paragraphs. We quantitatively compare the results of video to bag of words translation against a state-of-the-art baseline object recognition model from computer vision. We show that text translations from multimodal topic models vastly outperform the baseline on a multimedia dataset downloaded from the Internet.

#index 1948173
#* What's in a name?: an unsupervised approach to link users across communities
#@ Jing Liu;Fan Zhang;Xinying Song;Young-In Song;Chin-Yew Lin;Hsiao-Wuen Hon
#t 2013
#c 2
#% 392851
#% 740995
#% 754061
#% 879635
#% 913783
#% 937552
#% 956511
#% 963718
#% 1119132
#% 1264748
#% 1318653
#% 1355042
#% 1375909
#% 1468142
#% 1484257
#% 1558464
#% 1560390
#% 1598376
#% 1642060
#% 1715211
#% 1796700
#% 1919945
#% 1966622
#! In this paper, we consider the problem of linking users across multiple online communities. Specifically, we focus on the alias-disambiguation step of this user linking task, which is meant to differentiate users with the same usernames. We start quantitatively analyzing the importance of the alias-disambiguation step by conducting a survey on 153 volunteers and an experimental analysis on a large dataset of About.me (75,472 users). The analysis shows that the alias-disambiguation solution can address a major part of the user linking problem in terms of the coverage of true pairwise decisions (46.8%). To the best of our knowledge, this is the first study on human behaviors with regards to the usages of online usernames. We then cast the alias-disambiguation step as a pairwise classification problem and propose a novel unsupervised approach. The key idea of our approach is to automatically label training instances based on two observations: (a) rare usernames are likely owned by a single natural person, e.g. pennystar88 as a positive instance; (b) common usernames are likely owned by different natural persons, e.g. tank as a negative instance. We propose using the n-gram probabilities of usernames to estimate the rareness or commonness of usernames. Moreover, these two observations are verified by using the dataset of Yahoo! Answers. The empirical evaluations on 53 forums verify: (a) the effectiveness of the classifiers with the automatically generated training data and (b) that the rareness and commonness of usernames can help user linking. We also analyze the cases where the classifiers fail.

#index 1948174
#* Big data, lifelong machine learning and transfer learning
#@ Qiang Yang
#t 2013
#c 2
#! A major challenge in today's world is the Big Data problem, which manifests itself in Web and Mobile domains as rapidly changing and heterogeneous data streams. A data-mining system must be able to cope with the influx of changing data in a continual manner. This calls for Lifelong Machine Learning, which in contrast to the traditional one-shot learning, should be able to identify the learning tasks at hand and adapt to the learning problems in a sustainable manner. A foundation for lifelong machine learning is transfer learning, whereby knowledge gained in a related but different domain may be transferred to benefit learning for a current task. To make effective transfer learning, it is important to maintain a continual and sustainable channel in the life time of a user in which the data are annotated. In this talk, I outline the lifelong machine learning situations, give several examples of transfer learning and applications for lifelong machine learning, and discuss cases of successful extraction of data annotations to meet the Big Data challenge.

#index 1948175
#* Balanced label propagation for partitioning massive graphs
#@ Johan Ugander;Lars Backstrom
#t 2013
#c 2
#% 258598
#% 594021
#% 757953
#% 881460
#% 1399939
#% 1872377
#! Partitioning graphs at scale is a key challenge for any application that involves distributing a graph across disks, machines, or data centers. Graph partitioning is a very well studied problem with a rich literature, but existing algorithms typically can not scale to billions of edges, or can not provide guarantees about partition sizes. In this work we introduce an efficient algorithm, balanced label propagation, for precisely partitioning massive graphs while greedily maximizing edge locality, the number of edges that are assigned to the same shard of a partition. By combining the computational efficiency of label propagation --- where nodes are iteratively relabeled to the same 'label' as the plurality of their graph neighbors --- with the guarantees of constrained optimization --- guiding the propagation by a linear program constraining the partition sizes --- our algorithm makes it practically possible to partition graphs with billions of edges. Our algorithm is motivated by the challenge of performing graph predictions in a distributed system. Because this requires assigning each node in a graph to a physical machine with memory limitations, it is critically necessary to ensure the resulting partition shards do not overload any single machine. We evaluate our algorithm for its partitioning performance on the Facebook social graph, and also study its performance when partitioning Facebook's 'People You May Know' service (PYMK), the distributed system responsible for the feature extraction and ranking of the friends-of-friends of all active Facebook users. In a live deployment, we observed average query times and average network traffic levels that were 50.5% and 37.1% (respectively) when compared to the previous naive random sharding.

#index 1948176
#* GOP primary season on twitter: "popular" political sentiment in social media
#@ Yelena Mejova;Padmini Srinivasan;Bob Boynton
#t 2013
#c 2
#% 854646
#% 1249457
#% 1384224
#% 1472908
#! As mainstream news media and political campaigns start to pay attention to the political discourse online, a systematic analysis of political speech in social media becomes more critical. What exactly do people say on these sites, and how useful is this data in estimating political popularity? In this study we examine Twitter discussions surrounding seven US Republican politicians who were running for the US Presidential nomination in 2011. We show this largely negative rhetoric to be laced with sarcasm and humor and dominated by a small portion of users. Furthermore, we show that using out-of-the-box classification tools results in a poor performance, and instead develop a highly optimized multi-stage approach designed for general-purpose political sentiment classification. Finally, we compare the change in sentiment detected in our dataset before and after 19 Republican debates, concluding that, at least in this case, the Twitter political chatter is not indicative of national political polls.

#index 1948177
#* Towards Twitter context summarization with user influence models
#@ Yi Chang;Xuanhui Wang;Qiaozhu Mei;Yan Liu
#t 2013
#c 2
#% 262112
#% 268079
#% 420077
#% 722904
#% 731630
#% 787502
#% 815920
#% 818226
#% 989577
#% 992302
#% 1272053
#% 1355042
#% 1399992
#% 1470582
#% 1470583
#% 1470662
#% 1482254
#% 1536509
#% 1536537
#% 1560425
#% 1642046
#% 1688557
#% 1711764
#! Twitter has become one of the most popular platforms for users to share information in real time. However, as an individual tweet is short and lacks sufficient contextual information, users cannot effectively understand or consume information on Twitter, which can either make users less engaged or even detached from using Twitter. In order to provide informative context to a Twitter user, we propose the task of Twitter context summarization, which generates a succinct summary from a large but noisy Twitter context tree. Traditional summarization techniques only consider text information, which is insufficient for Twitter context summarization task, since text information on Twitter is very sparse. Given that there are rich user interactions in Twitter, we thus study how to improve summarization methods by leveraging such signals. In particular, we study how user influence models, which project user interaction information onto a Twitter context tree, can help Twitter context summarization within a supervised learning framework. To evaluate our methods, we construct a data set by asking human editors to manually select the most informative tweets as a summary. Our experimental results based on this editorial data set show that Twitter context summarization is a promising research topic and pairwise user influence signals can significantly improve the task performance.

#index 1948178
#* Exploiting social relations for sentiment analysis in microblogging
#@ Xia Hu;Lei Tang;Jiliang Tang;Huan Liu
#t 2013
#c 2
#% 769892
#% 854646
#% 938687
#% 939897
#% 987340
#% 1127964
#% 1211747
#% 1279645
#% 1292559
#% 1399992
#% 1417091
#% 1536515
#% 1544009
#% 1606084
#% 1642051
#% 1642280
#% 1648839
#% 1694627
#% 1707483
#! Microblogging, like Twitter and Sina Weibo, has become a popular platform of human expressions, through which users can easily produce content on breaking news, public events, or products. The massive amount of microblogging data is a useful and timely source that carries mass sentiment and opinions on various topics. Existing sentiment analysis approaches often assume that texts are independent and identically distributed (i.i.d.), usually focusing on building a sophisticated feature space to handle noisy and short texts, without taking advantage of the fact that the microblogs are networked data. Inspired by the social sciences findings that sentiment consistency and emotional contagion are observed in social networks, we investigate whether social relations can help sentiment analysis by proposing a Sociological Approach to handling Noisy and short Texts (SANT) for sentiment classification. In particular, we present a mathematical optimization formulation that incorporates the sentiment consistency and emotional contagion theories into the supervised learning process; and utilize sparse learning to tackle noisy texts in microblogging. An empirical study of two real-world Twitter datasets shows the superior performance of our framework in handling noisy and short tweets.

#index 1948179
#* Connecting comments and tags: improved modeling of social tagging systems
#@ Dawei Yin;Shengbo Guo;Boris Chidlovskii;Brian D. Davison;Cedric Archambeau;Guillaume Bouchard
#t 2013
#c 2
#% 722904
#% 769892
#% 805873
#% 956544
#% 987253
#% 1035591
#% 1073982
#% 1074070
#% 1074117
#% 1083696
#% 1100174
#% 1127455
#% 1130816
#% 1133994
#% 1166510
#% 1214623
#% 1214666
#% 1214694
#% 1214717
#% 1227644
#% 1355024
#% 1451236
#% 1451257
#% 1536533
#% 1617375
#% 1650298
#% 1667787
#% 1711777
#! Collaborative tagging systems are now deployed extensively to help users share and organize resources. Tag prediction and recommendation can simplify and streamline the user experience, and by modeling user preferences, predictive accuracy can be significantly improved. However, previous methods typically model user behavior based only on a log of prior tags, neglecting other behaviors and information in social tagging systems, e.g., commenting on items and connecting with other users. On the other hand, little is known about the connection and correlations among these behaviors and contexts in social tagging systems. In this paper, we investigate improved modeling for predictive social tagging systems. Our explanatory analyses demonstrate three significant challenges: coupled high order interaction, data sparsity and cold start on items. We tackle these problems by using a generalized latent factor model and fully Bayesian treatment. To evaluate performance, we test on two real-world data sets from Flickr and Bibsonomy. Our experiments on these data sets show that to achieve best predictive performance, it is necessary to employ a fully Bayesian treatment in modeling high order relations in social tagging system. Our methods noticeably outperform state-of-the-art approaches.

#index 1948180
#* Co-factorization machines: modeling user interests and predicting individual decisions in Twitter
#@ Liangjie Hong;Aziz S. Doumith;Brian D. Davison
#t 2013
#c 2
#% 577224
#% 1073906
#% 1127484
#% 1211822
#% 1214623
#% 1291600
#% 1305467
#% 1355024
#% 1355025
#% 1417104
#% 1457039
#% 1476448
#% 1482397
#% 1484274
#% 1535399
#% 1535449
#% 1560174
#% 1560408
#% 1598363
#% 1605963
#% 1605981
#% 1625344
#% 1625358
#% 1642229
#% 1688496
#% 1688506
#% 1689528
#% 1693875
#% 1730808
#% 1879057
#% 1879058
#! Users of popular services like Twitter and Facebook are often simultaneously overwhelmed with the amount of information delivered via their social connections and miss out on much content that they might have liked to see, even though it was distributed outside of their social circle. Both issues serve as difficulties to the users and drawbacks to the services. Social media service providers can benefit from understanding user interests and how they interact with the service, potentially predicting their behaviors in the future. In this paper, we address the problem of simultaneously predicting user decisions and modeling users' interests in social media by analyzing rich information gathered from Twitter. The task differs from conventional recommender systems as the cold-start problem is ubiquitous, and rich features, including textual content, need to be considered. We build predictive models for user decisions in Twitter by proposing Co-Factorization Machines (CoFM), an extension of a state-of-the-art recommendation model, to handle multiple aspects of the dataset at the same time. Additionally, we discuss and compare ranking-based loss functions in the context of recommender systems, providing the first view of how they vary from each other and perform in real tasks. We explore an extensive set of features and conduct experiments on a real-world dataset, concluding that CoFM with ranking-based loss functions is superior to state-of-the-art methods and yields interpretable latent factors.

#index 1948181
#* Wikipedia entity expansion and attribute extraction from the web using semi-supervised learning
#@ Lidong Bing;Wai Lam;Tak-Lam Wong
#t 2013
#c 2
#% 729978
#% 754068
#% 805846
#% 881505
#% 939527
#% 939924
#% 956503
#% 1019061
#% 1019130
#% 1063570
#% 1092530
#% 1117028
#% 1190118
#% 1275182
#% 1328133
#% 1328199
#% 1338552
#% 1338626
#% 1338685
#% 1355026
#% 1380965
#% 1409954
#% 1471191
#% 1471208
#% 1471405
#% 1481552
#% 1523913
#% 1536559
#% 1592311
#% 1642064
#% 1826065
#! We develop a new framework to achieve the goal of Wikipedia entity expansion and attribute extraction from the Web. Our framework takes a few existing entities that are automatically collected from a particular Wikipedia category as seed input and explores their attribute infoboxes to obtain clues for the discovery of more entities for this category and the attribute content of the newly discovered entities. One characteristic of our framework is to conduct discovery and extraction from desirable semi-structured data record sets which are automatically collected from the Web. A semi-supervised learning model with Conditional Random Fields is developed to deal with the issues of extraction learning and limited number of labeled examples derived from the seed entities. We make use of a proximate record graph to guide the semi-supervised learning process. The graph captures alignment similarity among data records. Then the semi-supervised learning process can leverage the unlabeled data in the record set by controlling the label regularization under the guidance of the proximate record graph. Extensive experiments on different domains have been conducted to demonstrate its superiority for discovering new entities and extracting attribute content.

#index 1948182
#* Retweet or not?: personalized tweet re-ranking
#@ Wei Feng;Jianyong Wang
#t 2013
#c 2
#% 1083671
#% 1214623
#% 1379671
#% 1512437
#% 1536533
#% 1560174
#% 1617342
#% 1642229
#% 1693874
#% 1730808
#! With Twitter being widely used around the world, users are facing enormous new tweets every day. Tweets are ranked in chronological order regardless of their potential interestedness. Users have to scan through pages of tweets to find useful information. Thus more personalized ranking scheme is needed to filter the overwhelmed information. Since retweet history reveals users' personal preference for tweets, we study how to learn a predictive model to rank the tweets according to their probability of being retweeted. In this way, users can find interesting tweets in a short time. To model the retweet behavior, we build a graph made up of three types of nodes: users, publishers and tweets. To incorporate all sources of information like users' profile, tweet quality, interaction history, etc, nodes and edges are represented by feature vectors. All these feature vectors are mapped to node weights and edge weights. Based on the graph, we propose a feature-aware factorization model to re-rank the tweets, which unifies the linear discriminative model and the low-rank factorization model seamlessly. Finally, we conducted extensive experiments on a real dataset crawled from Twitter. Experimental results show the effectiveness of our model.

#index 1948183
#* Overlapping community detection at scale: a nonnegative matrix factorization approach
#@ Jaewon Yang;Jure Leskovec
#t 2013
#c 2
#% 258598
#% 757953
#% 881460
#% 995168
#% 1013696
#% 1198232
#% 1214722
#% 1606047
#% 1835483
#% 1872290
#% 1978762
#% 1978787
#! Network communities represent basic structures for understanding the organization of real-world networks. A community (also referred to as a module or a cluster) is typically thought of as a group of nodes with more connections amongst its members than between its members and the remainder of the network. Communities in networks also overlap as nodes belong to multiple clusters at once. Due to the difficulties in evaluating the detected communities and the lack of scalable algorithms, the task of overlapping community detection in large networks largely remains an open problem. In this paper we present BIGCLAM (Cluster Affiliation Model for Big Networks), an overlapping community detection method that scales to large networks of millions of nodes and edges. We build on a novel observation that overlaps between communities are densely connected. This is in sharp contrast with present community detection methods which implicitly assume that overlaps between communities are sparsely connected and thus cannot properly extract overlapping communities in networks. In this paper, we develop a model-based community detection algorithm that can detect densely overlapping, hierarchically nested as well as non-overlapping communities in massive networks. We evaluate our algorithm on 6 large social, collaboration and information networks with ground-truth community information. Experiments show state of the art performance both in terms of the quality of detected communities as well as in speed and scalability of our algorithm.

#index 1948184
#* LaFT-tree: perceiving the expansion trace of one's circle of friends in online social networks
#@ Jun Zhang;Chaokun Wang;Jianmin Wang;Philip S. Yu
#t 2013
#c 2
#% 464996
#% 593994
#% 722904
#% 730089
#% 881523
#% 937549
#% 1083675
#% 1268040
#% 1396209
#% 1482199
#% 1512414
#% 1642046
#% 1642049
#! Many patterns have been discovered to explain and analyze how people make friends. Among them is the triadic closure, supported by the principle of the transitivity of friendship, which means for an individual the friends of her friend are more likely to become her new friends. However, people's motivations under this principle haven't been well studied, and it's still unknown that how this principle works in diverse situations. In this paper, we try to study this principle deeply based on the behavior modeling. We study how one expands her egocentric network via her friends, also called intermediaries, based on the transitivity of friendship. We propose LaFT-Tree, a tree-based representation of friendship formation inspired from triadic closure. LaFT-Tree provides a hierarchical view of the flat structure of one's egocentric network by visualizing the expansion trace of one's egocentric network. We model people's friend-making behaviors using LaFT-LDA, a generative model for LaFT-Tree learning. The proposed model is evaluated on both synthetic and real-world social networks and experimental results demonstrate the effectiveness of LaFT-LDA for LaFT-Tree inference. We also present some interesting applications of the LaFT-Tree, showing that our model can be generalized and benefit other social network analysis tasks.

#index 1948185
#* A peek into the future: predicting the evolution of popularity in user generated content
#@ Mohamed Ahmed;Stella Spagna;Felipe Huici;Saverio Niccolini
#t 2013
#c 2
#% 795273
#% 956509
#% 989613
#% 1002005
#% 1035589
#% 1190127
#% 1327475
#% 1399992
#% 1411585
#% 1517940
#% 1524042
#% 1536522
#% 1536579
#% 1540321
#% 1560174
#% 1581403
#% 1621790
#% 1635246
#% 1642203
#% 1642417
#% 1697430
#! Content popularity prediction finds application in many areas, including media advertising, content caching, movie revenue estimation, traffic management and macro-economic trends forecasting, to name a few. However, predicting this popularity is difficult due to, among others, the effects of external phenomena, the influence of context such as locality and relevance to users,and the difficulty of forecasting information cascades. In this paper we identify patterns of temporal evolution that are generalisable to distinct types of data, and show that we can (1) accurately classify content based on the evolution of its popularity over time and (2) predict the value of the content's future popularity. We verify the generality of our method by testing it on YouTube, Digg and Vimeo data sets and find our results to outperform the K-Means baseline when classifying the behaviour of content and the linear regression baseline when predicting its popularity.

#index 1948186
#* Online community detection in social sensing
#@ Guo-Jun Qi;Charu C. Aggarwal;Thomas S. Huang
#t 2013
#c 2
#% 281214
#% 881460
#% 881514
#% 907380
#% 960283
#% 989586
#% 1055741
#% 1127436
#% 1190134
#% 1190241
#% 1214714
#% 1328169
#% 1372657
#% 1523860
#% 1538537
#% 1567510
#% 1606049
#% 1606051
#% 1720762
#% 1846760
#% 1953293
#! The proliferation of location and GPS data streams which are collected in a wide variety of participatory sensing applications has created numerous possibilities for analysis of the underlying patterns of activity. Typically, the spatio-temporal patterns arising from such activity can be analyzed in order to determine the latent community structure in the underlying data. In this paper, we will examine the problem of online community detection from the location data collected from such social sensing applications in real time. Such data brings numerous challenges associated with it, in that they can be of a relatively large scale, and can be extremely noisy from the perspective of both data representation and analysis. Furthermore, the community structure in the underlying data cannot be directly inferred from the shape of the underlying trajectories, since a considerable amount of variation may exist in terms of trajectories of individuals belonging to the same community. In this paper, we will design online algorithms for community detection in social sensing applications. Our algorithm uses a robust and efficiently updateable model with the use of Gibbs sampling, and we will show its effectiveness and efficiency for social sensing applications.

#index 1948187
#* Distinguishing topical and social groups based on common identity and bond theory
#@ Przemyslaw A. Grabowicz;Luca Maria Aiello;Victor M. Eguiluz;Alejandro Jaimes
#t 2013
#c 2
#% 350002
#% 751857
#% 881460
#% 885038
#% 1002007
#% 1035581
#% 1071127
#% 1279896
#% 1550723
#% 1619131
#% 1693928
#% 1740995
#% 1880400
#! Social groups play a crucial role in social media platforms because they form the basis for user participation and engagement. Groups are created explicitly by members of the community, but also form organically as members interact. Due to their importance, they have been studied widely (e.g., community detection, evolution, activity, etc.). One of the key questions for understanding how such groups evolve is whether there are different types of groups and how they differ. In Sociology, theories have been proposed to help explain how such groups form. In particular, the common identity and common bond theory states that people join groups based on identity (i.e., interest in the topics discussed) or bond attachment (i.e., social relationships). The theory has been applied qualitatively to small groups to classify them as either topical or social. We use the identity and bond theory to define a set of features to classify groups into those two categories. Using a dataset from Flickr, we extract user-defined groups and automatically-detected groups, obtained from a community detection algorithm. We discuss the process of manual labeling of groups into social or topical and present results of predicting the group label based on the defined features. We directly validate the predictions of the theory showing that the metrics are able to forecast the group type with high accuracy. In addition, we present a comparison between declared and detected groups along topicality and sociality dimensions.

#index 1948188
#* Modeling the impact of lifestyle on health at scale
#@ Adam Sadilek;Henry Kautz
#t 2013
#c 2
#% 197394
#% 840882
#% 983905
#% 1425621
#% 1426661
#% 1536568
#% 1561563
#% 1606049
#% 1693933
#! Research in computational epidemiology to date has concentrated on estimating summary statistics of populations and simulated scenarios of disease outbreaks. Detailed studies have been limited to small domains, as scaling the methods involved poses considerable challenges. By contrast, we model the associations of a large collection of social and environmental factors with the health of particular individuals. Instead of relying on surveys, we apply scalable machine learning techniques to noisy data mined from online social media and infer the health state of any given person in an automated way. We show that the learned patterns can be subsequently leveraged in descriptive as well as predictive fine-grained models of human health. Using a unified statistical model, we quantify the impact of social status, exposure to pollution, interpersonal interactions, and other important lifestyle factors on one's health. Our model explains more than 54% of the variance in people's health (as estimated from their online communication), and predicts the future health status of individuals with 91% accuracy. Our methods complement traditional studies in life sciences, as they enable us to perform large-scale and timely measurement, inference, and prediction of previously elusive factors that affect our everyday lives.

#index 1948189
#* Collective inference for network data with copula latent markov networks
#@ Rongjing Xiang;Jennifer Neville
#t 2013
#c 2
#% 840852
#% 850430
#% 856762
#% 915344
#% 916784
#% 925382
#% 943324
#% 949164
#% 961206
#% 961268
#% 961278
#% 983860
#% 1117695
#% 1214703
#% 1385987
#% 1506206
#% 1562580
#% 1650403
#% 1650536
#% 1688440
#% 1762741
#! The popularity of online social networks and social media has increased the amount of linked data available in Web domains. Relational and Gaussian Markov networks have both been applied successfully for classification in these relational settings. However, since Gaussian Markov networks model joint distributions over continuous label space, it is difficult to use them to reason about uncertainty in discrete labels. On the other hand, relational Markov networks model probability distributions over discrete label space, but since they condition on the graph structure, the marginal probability for an instance will vary based on the structure of the subnetwork observed around the instance. This implies that the marginals will not be identical across instances and can sometimes result in poor prediction performance. In this work, we propose a novel latent relational model based on copulas which allows use to make predictions in a discrete label space while ensuring identical marginals and at the same time incorporating some desirable properties of modeling relational dependencies in a continuous space. While copulas have recently been used for descriptive modeling, they have not been used for collective classification in large scale network data and the associated conditional inference problem has not been considered before. We develop an approximate inference algorithm, and demonstrate empirically that our proposed Copula Latent Markov Network models based on approximate inference outperform a number of competing relational classification models over a range of real-world relational classification tasks.

#index 1948190
#* Influence diffusion dynamics and influence maximization in social networks with friend and foe relationships
#@ Yanhua Li;Wei Chen;Yajun Wang;Zhi-Li Zhang
#t 2013
#c 2
#% 729923
#% 989613
#% 1130830
#% 1214641
#% 1274948
#% 1384246
#% 1399997
#% 1407355
#% 1407359
#% 1425621
#% 1449357
#% 1451243
#% 1487838
#% 1535380
#% 1535434
#% 1540209
#% 1540249
#% 1560421
#% 1688456
#% 1932885
#! Influence diffusion and influence maximization in large-scale online social networks (OSNs) have been extensively studied because of their impacts on enabling effective online viral marketing. Existing studies focus on social networks with only friendship relations, whereas the foe or enemy relations that commonly exist in many OSNs, e.g., Epinions and Slashdot, are completely ignored. In this paper, we make the first attempt to investigate the influence diffusion and influence maximization in OSNs with both friend and foe relations, which are modeled using positive and negative edges on signed networks. In particular, we extend the classic voter model to signed networks and analyze the dynamics of influence diffusion of two opposite opinions. We first provide systematic characterization of both short-term and long-term dynamics of influence diffusion in this model, and illustrate that the steady state behaviors of the dynamics depend on three types of graph structures, which we refer to as balanced graphs, anti-balanced graphs, and strictly unbalanced graphs. We then apply our results to solve the influence maximization problem and develop efficient algorithms to select initial seeds of one opinion that maximize either its short-term influence coverage or long-term steady state influence coverage. Extensive simulation results on both synthetic and real-world networks, such as Epinions and Slashdot, confirm our theoretical analysis on influence diffusion dynamics, and demonstrate that our influence maximization algorithms perform consistently better than other heuristic algorithms.

#index 1948191
#* Modeling dynamic behavior in large evolving graphs
#@ Ryan A. Rossi;Brian Gallagher;Jennifer Neville;Keith Henderson
#t 2013
#c 2
#% 349550
#% 823342
#% 824709
#% 848512
#% 853534
#% 881460
#% 881514
#% 949164
#% 989640
#% 989663
#% 1083699
#% 1126263
#% 1179992
#% 1190127
#% 1211731
#% 1491561
#% 1506205
#% 1524059
#% 1524266
#% 1535361
#% 1536522
#% 1605987
#% 1874807
#% 1895298
#% 1956742
#! Given a large time-evolving graph, how can we model and characterize the temporal behaviors of individual nodes (and network states)? How can we model the behavioral transition patterns of nodes? We propose a temporal behavior model that captures the "roles" of nodes in the graph and how they evolve over time. The proposed dynamic behavioral mixed-membership model (DBMM) is scalable, fully automatic (no user-defined parameters), non-parametric/data-driven (no specific functional form or parameterization), interpretable (identifies explainable patterns), and flexible (applicable to dynamic and streaming networks). Moreover, the interpretable behavioral roles are generalizable and computationally efficient. We applied our model for (a) identifying patterns and trends of nodes and network states based on the temporal behavior, (b) predicting future structural changes, and (c) detecting unusual temporal behavior transitions. The experiments demonstrate the scalability, flexibility, and effectiveness of our model for identifying interesting patterns, detecting unusual structural transitions, and predicting the future structural changes of the network and individual nodes.

#index 1948192
#* On the streaming complexity of computing local clustering coefficients
#@ Konstantin Kutzkov;Rasmus Pagh
#t 2013
#c 2
#% 249238
#% 281214
#% 379443
#% 569754
#% 749449
#% 874902
#% 1176970
#% 1176985
#% 1214705
#% 1404840
#% 1454142
#% 1549697
#% 1581819
#% 1682599
#% 1701869
#% 1770116
#% 1770492
#% 1916635
#! Due to a large number of applications, the problem of estimating the number of triangles in graphs revealed as a stream of edges, and the closely related problem of estimating the graph's clustering coefficient, have received considerable attention in the last decade. Both efficient algorithms and impossibility results have shed light on the computational complexity of the problem. Motivated by applications in Web mining, Becchetti et al.~presented new algorithms for the estimation of the local number of triangles, i.e., the number of triangles incident to individual vertices. The algorithms are shown, both theoretically and experimentally, to efficiently handle the problem. However, at least two passes over the data are needed and thus the algorithms are not suitable for real streaming scenarios. In the present work, we consider the problem of estimating the clustering coefficient of individual vertices in a graph over n vertices revealed as a stream of m edges. As a first result we show that any one pass randomized streaming algorithm that can distinguish a graph with no triangles from a graph having a vertex of degree d with clustering coefficient 1/2 must use Ω(m/d) bits of space in expectation. Our second result is a new randomized one pass algorithm estimating the local clustering coefficient of each vertex with degree at least d. The space requirement of our algorithm is within a logarithmic factor of the lower bound, thus our approach is close to optimal. We also extend the algorithm to local triangle counting and report experimental results on its performance on real-life graphs.

#index 1948193
#* Learning query and document similarities from click-through bipartite graph with metadata
#@ Wei Wu;Hang Li;Jun Xu
#t 2013
#c 2
#% 262096
#% 309095
#% 310567
#% 342961
#% 577273
#% 643023
#% 750863
#% 818218
#% 855563
#% 869651
#% 879594
#% 987222
#% 989578
#% 1083688
#% 1127383
#% 1130879
#% 1190106
#% 1214645
#% 1268491
#% 1536566
#% 1598399
#% 1606370
#% 1742155
#% 1762834
#! We consider learning query and document similarities from a click-through bipartite graph with metadata on the nodes. The metadata contains multiple types of features of queries and documents. We aim to leverage both the click-through bipartite graph and the features to learn query-document, document-document, and query-query similarities. The challenges include how to model and learn the similarity functions based on the graph data. We propose solving the problems in a principled way. Specifically, we use two different linear mappings to project the queries and documents in two different feature spaces into the same latent space, and take the dot product in the latent space as their similarity. Query-query and document-document similarities can also be naturally defined as dot products in the latent space. We formalize the learning of similarity functions as learning of the mappings that maximize the similarities of the observed query-document pairs on the enriched click-through bipartite graph. When queries and documents have multiple types of features, the similarity function is defined as a linear combination of multiple similarity functions, each based on one type of features. We further solve the learning problem by using a new technique called Multi-view Partial Least Squares (M-PLS). The advantages include the global optimum which can be obtained through Singular Value Decomposition (SVD) and the capability of finding high quality similar queries. We conducted large scale experiments on enterprise search data and web search data. The experimental results on relevance ranking and similar query finding demonstrate that the proposed method works significantly better than the baseline methods.

#index 1948194
#* Optimizing budget constrained spend in search advertising
#@ Chinmay Karande;Aranyak Mehta;Ramakrishnan Srikant
#t 2013
#c 2
#% 723279
#% 808362
#% 868445
#% 868473
#% 954300
#% 956547
#% 963332
#% 963358
#% 963361
#% 991785
#% 1039684
#% 1071488
#% 1222625
#% 1336462
#% 1404809
#% 1426653
#% 1426655
#% 1496079
#% 1606074
#! Search engine ad auctions typically have a significant fraction of advertisers who are budget constrained, i.e., if allowed to participate in every auction that they bid on, they would spend more than their budget. This yields an important problem: selecting the ad auctions which these advertisers participate, in order to optimize different system objectives such as the return on investment for advertisers, and the quality of ads shown to users. We present a system and algorithms for optimizing budget constrained spend. The system is designed be deployed in a large search engine, with hundreds of thousands of advertisers, millions of searches per hour, and with the query stream being only partially predictable. We have validated the system design by implementing it in the Google ads serving system and running experiments on live traffic. We have also compared our algorithm to previous work that casts this problem as a large linear programming problem limited to popular queries, and show that our algorithms yield substantially better results.

#index 1948195
#* Characterizing and supporting cross-device search tasks
#@ Yu Wang;Xiao Huang;Ryen W. White
#t 2013
#c 2
#% 339188
#% 438557
#% 751830
#% 805898
#% 860086
#% 879567
#% 955711
#% 956495
#% 987211
#% 1047385
#% 1047436
#% 1047437
#% 1055697
#% 1083721
#% 1126944
#% 1130878
#% 1169589
#% 1190135
#% 1217296
#% 1227582
#% 1266459
#% 1292754
#% 1384320
#% 1399965
#% 1450832
#% 1450884
#% 1482279
#% 1598334
#% 1619985
#% 1642076
#% 1765973
#% 1879011
#% 1879024
#! Web searchers frequently transition from desktop computers and laptops to mobile devices, and vice versa. Little is known about the nature of cross-device search tasks, yet they represent an important opportunity for search engines to help their users, especially those on the target (post-switch) device. For example, the search engine could save the current session and re-instate it post switch, or it could capitalize on down-time between devices to proactively re-trieve content on behalf of the searcher. In this paper, we present a log-based study to define and characterize cross-device search be-havior and predict the resumption of cross-device tasks. Using data from a large commercial search engine, we show that there are dis-cernible and noteworthy patterns of search behavior associated with device transitions. We also develop learned models for predicting task resumption on the target device using behavioral, topical, geo-spatial, and temporal features. Our findings show that our models can attain strong prediction accuracy and have direct implications for the development of tools to help people search more effectively in a multi-device world.

#index 1948196
#* Learning to rank for spatiotemporal search
#@ Blake Shaw;Jon Shea;Siddhartha Sinha;Andrew Hogue
#t 2013
#c 2
#% 309095
#% 345364
#% 549134
#% 661923
#% 723186
#% 766441
#% 818221
#% 869534
#% 946521
#% 946811
#% 976952
#% 1055697
#% 1063553
#% 1065183
#% 1227637
#% 1250379
#% 1399939
#% 1442577
#% 1476142
#% 1482254
#% 1550750
#% 1667202
#% 1693926
#% 1693933
#! In this article we consider the problem of mapping a noisy estimate of a user's current location to a semantically meaningful point of interest, such as a home, restaurant, or store. Despite the poor accuracy of GPS on current mobile devices and the relatively high density of places in urban areas, it is possible to predict a user's location with considerable precision by explicitly modeling both places and users and by combining a variety of signals about a user's current context. Places are often simply modeled as a single latitude and longitude when in fact they are complex entities existing in both space and time and shaped by the millions of people that interact with them. Similarly, models of users reveal complex but predictable patterns of mobility that can be exploited for this task. We propose a novel spatial search algorithm that infers a user's location by combining aggregate signals mined from billions of foursquare check-ins with real-time contextual information. We evaluate a variety of techniques and demonstrate that machine learning algorithms for ranking and spatiotemporal models of places and users offer significant improvement over common methods for location search based on distance and popularity.

#index 1948197
#* Maguro, a system for indexing and searching over very large text collections
#@ Knut Magne Risvik;Trishul Chilimbi;Henry Tan;Karthik Kalyanaraman;Chris Anderson
#t 2013
#c 2
#% 221973
#% 249153
#% 342397
#% 387427
#% 411762
#% 578337
#% 730065
#% 879608
#% 938073
#% 957218
#% 976948
#% 983467
#% 1077150
#% 1080070
#% 1106874
#% 1127559
#% 1605019
#! Maguro is a system for efficiently searching very large collections of text content of up to 1 trillion documents at low cost. Search engines span across content that is very dynamic and highly augmented with metadata to the tail content of the web. A long tail distribution of content calls for different trade-offs in the design space for good efficiency across the entire index range. Maguro is designed for the long tail of content with less dynamics and less metadata, but very good cost efficiency. Maguro is part of the serving stack in Bing and allows us to scale the index significantly better.

#index 1948198
#* Sponsored search auctions
#@ Riccardo Colini-Baldeschi
#t 2013
#c 2
#% 813836
#% 847055
#% 1141529
#% 1190079
#% 1231032
#% 1336476
#% 1584797
#% 1747550
#% 1747559
#% 1747589
#% 1770440
#% 1783959
#% 1888816
#! Sponsored search auctions are used to allocate ad slots to advertisers. The standard mechanism for sponsored search auctions is the Generalized-Second-Price (GSP) auction. Even if GSP seems to be established, a lot of open problems remain in the area and many significant researches have been done in the recent years. My research proposal is focusing in some specific aspects of the sponsored search auctions like revenue maximization and the design of mechanisms that obtain some form of social efficiency. In this paper we start from a brief history of the sponsored search auctions and then we present the formal model. In the last two sections I will introduce the obtained results and some open problems.

#index 1948199
#* On the prediction of popularity of trends and hits for user generated videos
#@ Flavio Figueiredo
#t 2013
#c 2
#% 866298
#% 879633
#% 1002006
#% 1309092
#% 1399995
#% 1411585
#% 1536522
#% 1536579
#% 1598417
#% 1621790
#% 1693924
#% 1872373
#% 1948160
#! User generated content (UGC) has emerged as the predominant form of media publishing on the Web 2.0. Motivated by the large adoption of video sharing on the Web 2.0, the objective of our work is to understand and predict popularity trends (e.g, will a video be viral?) and hits (e.g, how may views will a video receive?) of user generated videos. Such knowledge is paramount to the effective design of various services including content distribution and advertising. Thus, in this paper we formalize the problem of predicting trends and hits in user generated videos. Also, we describe our research methodology on approaching this problem. To the best of knowledge, our work is novel in focusing on the problem of predicting popularity trends complementary to hits. Moreover, we intend on evaluating efficacy of our results not only based on common statistical error metrics, but also on the possible online advertising revenues our predictions can generate. After describing our proposal, we here summarize our latest findings regarding (1) uncovering common popularity trends; (2) measuring associations between UGC features and popularity trends; and (3) assessing the effectiveness of models for predicting popularity trends.

#index 1948200
#* On the quest of discovering cultural trails in social media
#@ Ruth Olimpia Garcia Gavilanes
#t 2013
#c 2
#% 1173417
#% 1558460
#% 1573470
#% 1642033
#% 1642690
#! With the constant increasing reach of the Web and in particular of Social Media, people create and share content that harbors information about habits, norms, preferences and values. Consequently, studying how culture influences users in online social media has increased the interest of several sectors such as the advertising industry, search engines and corporations. As a consequence, anthropological and computational models need to interact and complement each other to better target these new demands. Recently, several studies have analyzed culture from large-scale data but not many took into consideration the cultural models proposed by anthropological theory. By carrying out several experiments on large-scale data from the Web, we propose to combine theoretical concepts of culture with information technology techniques to process, analyze, model and interpret data from the Web. We plan to discover synergies between traditional social studies of culture and those derived from our experiments.

#index 1948201
#* Towards web-scale structured web data extraction
#@ Tomas Grigalis
#t 2013
#c 2
#% 275915
#% 330784
#% 480648
#% 480824
#% 769437
#% 805845
#% 805846
#% 956564
#% 1023488
#% 1089602
#% 1127393
#% 1150163
#% 1190153
#% 1200332
#% 1217114
#% 1217172
#% 1292470
#% 1327636
#% 1328199
#% 1364949
#% 1426449
#% 1503758
#% 1538764
#% 1560398
#% 1594640
#% 1741034
#% 1770375
#% 1826065
#% 1892367
#! In this paper we present an ongoing PhD research on unsupervised and domain-independent structured data extraction from the Web. We propose a novel method to extract structured data records from template-generated Web pages. The method is based on clustering visually similar Web page elements by exploiting their visual formatting and HTML structural features. Tag paths of clustered Web page elements are then employed to derive extraction rules. These rules, called wrappers, can be later reused on thousands of same template-generated Web pages. This opens the possibility for the proposed method to be deployed in Web-Scale structured data extraction systems.

#index 1948202
#* Building user profiles to improve user experience in recommender systems
#@ Anisio Lacerda;Nivio Ziviani
#t 2013
#c 2
#% 428272
#% 860672
#% 1127472
#% 1176909
#% 1241486
#% 1259245
#% 1418196
#% 1476448
#% 1625357
#% 1777103
#% 1893811
#! Recommender systems are quickly becoming ubiquitous in many Web applications, including e-commerce, social media channels, content providers, among others. These systems act as an enabling mechanism designed to overcome the information overload problem by improving browsing and consumption experience. Crucial to the performance of a recommender system is the accuracy of the user profiles used to represent the interests of the users. In this proposal, we analyze three different aspects of user profiling: (i) selecting the most informative events from the interaction between users and the system, (ii) combining different recommendation algorithms to (iii) including trust-aware information in user profiles to improve the accuracy of recommender systems.

#index 1948203
#* Web usage mining for enhancing search-result delivery and helping users to find interesting web content
#@ Ida Mele
#t 2013
#c 2
#% 194299
#% 262112
#% 296646
#% 323131
#% 630984
#% 755396
#% 860861
#% 956521
#% 987212
#% 999292
#% 1074133
#% 1130868
#% 1141537
#% 1166473
#% 1355040
#% 1404875
#% 1536577
#% 1581410
#% 1834787
#% 1919799
#! Web usage mining is the application of data mining techniques to the data generated by the interactions of users with web servers. This kind of data, stored in server logs, represents a valuable source of information, which can be exploited to optimize the document-retrieval task, or to better understand, and thus, satisfy user needs. Our research focuses on two important issues: improving search-engine performance through static caching of search results, and helping users to find interesting web pages by recommending news articles and blog posts. Concerning the static caching of search results, we present the query covering approach. The general idea is to populate the cache with those documents that contribute to the result pages of a large number of queries, as opposed to caching the top documents of most frequent queries. For the recommendation of web pages, we present a graph-based approach, which leverages the user-browsing logs to identify early adopters. These users discover interesting content before others, and monitoring their activity we can find web pages to recommend.

#index 1948204
#* Advanced graph mining for community evaluation in social networks and the web
#@ Christos Giatsidis;Fragkiskos D. Malliaros;Michalis Vazirgiannis
#t 2013
#c 2
#% 867050
#% 995140
#% 1549865
#% 1628678
#% 1635210
#% 1688455
#% 1835483
#! Graphs constitute a dominant data structure and appear essentially in all forms of information. Examples are the Web graph, numerous social networks, protein interaction networks, terms dependency graphs and network topologies. The main features of these graphs are their huge volume and rate of change. Presumably, there is important hidden knowledge in the macroscopic topology and features of these graphs. A cornerstone issue here is the detection and evaluation of communities -- bearing multiple and diverse semantics. The tutorial reports the basic models of graph structures for undirected, directed and signed graphs and their properties. Next we offer a thorough review of fundamental methods for graph clustering and community detection, on both undirected and directed graphs. Then we survey community evaluation measures, including both the individual node based ones as well as those that take into account aggregate properties of communities. A special mention is made on approaches that capitalize on the concept of degeneracy (k-cores and extensions), as a novel means of community detection and evaluation. We justify the above foundational framework with applications on citation graphs, trust networks and protein graphs.

#index 1948205
#* Anomaly, event, and fraud detection in large network datasets
#@ Leman Akoglu;Christos Faloutsos
#t 2013
#c 2
#% 729983
#% 844334
#% 956513
#% 989640
#% 1016177
#% 1056094
#% 1214748
#% 1650403
#% 1710593
#! Detecting anomalies and events in data is a vital task, with numerous applications in security, finance, health care, law enforcement, and many others. While many techniques have been developed in past years for spotting outliers and anomalies in unstructured collections of multi-dimensional points, with graph data becoming ubiquitous, techniques for structured graph data have been of focus recently. As objects in graphs have long-range correlations, novel technology has been developed for abnormality detection in graph data. The goal of this tutorial is to provide a general, comprehensive overview of the state-of-the-art methods for anomaly, event, and fraud detection in data represented as graphs. As a key contribution, we provide a thorough exploration of both data mining and machine learning algorithms for these detection tasks. We give a general framework for the algorithms, categorized under various settings: unsupervised vs.(semi-)supervised, for static vs. dynamic data. We focus on the scalability and effectiveness aspects of the methods, and highlight results on crucial real-world applications, including accounting fraud and opinion spam detection.

#index 1948206
#* Models and algorithms for social influence analysis
#@ Jimeng Sun;Jie Tang
#t 2013
#c 2
#% 729923
#% 1035589
#% 1083624
#% 1083641
#% 1214641
#% 1214702
#% 1222654
#% 1355040
#% 1399993
#% 1451242
#% 1451245
#% 1482198
#% 1606084
#! Social influence is the behavioral change of a person because of the perceived relationship with other people, organizations and society in general. Social influence has been a widely accepted phenomenon in social networks for decades. Many applications have been built based around the implicit notation of social influence between people, such as marketing, advertisement and recommendations. With the exponential growth of online social network services such as Facebook and Twitter, social influence can for the first time be measured over a large population. In this tutorial, we survey the research on social influence analysis with a focus on the computational aspects. First, we introduce how to verify the existence of social influence in various social networks. Second, we present computational models for quantifying social influence. Third, we describe how social influence can help real applications. In particular, we will focus on opinion leader finding and influence maximization for viral marketing. Finally, we apply the selected algorithms of social influence analysis on different social network data, such as twitter, arnetminer data, weibo, and slashdot forum.

#index 1948207
#* Data-driven political science
#@ Ingmar Weber;Ana-Maria Popescu;Marco Pennacchiotti
#t 2013
#c 2
#% 783529
#% 1484617
#% 1605961
#% 1879112
#% 1905953
#% 1920051
#% 1967836
#! The tutorial will summarize the state-of-the art in the growing area of computational political science. Like many others, this research domain is being revolutionized by the availability of open, big data and the increasing reach and importance of social media. The surging interest on the part of the academic community is matched by intense efforts on the part of political campaigns to use online data in order to learn how to best disseminate information and reach the right potential donors or voters. In this context, a tutorial can summarize existing methods in a fascinating, high-interest area and allow participants with diverse backgrounds to get inspiration from the methods and problems studied. The tutorial will feature seminal research concerning (i) political polarization, (ii) election prediction and polling, and (iii) political campaigning and influence propagation. The goal is not only to familiarize attendees with ideas from related conferences such as WWW, ICWSM or CIKM, but also to present ideas and quantitative methods closer to political science such as Poole's and Rosenthal's NOMINATE score for a politician's political orientation.

#index 1948208
#* Exploring structure and content on the web: extraction and integration of the semi-structured web
#@ Tim Weninger;Jiawei Han
#t 2013
#c 2
#% 480824
#% 729978
#% 864416
#% 902460
#% 1044500
#% 1127393
#% 1328133
#% 1400030
#% 1560163
#% 1565404
#% 1846461
#% 1918398
#! In this tutorial we view the World Wide Web as a type of massive, decentralized database. At present, this "Web database" is presented in a manner largely devoid of any consistent meaning or schema. That is not to say that Web-data lacks an underlying organization; in fact, most Web content is generated from an underlying schema-bound, or otherwise structured database. Information extraction is generally concerned with the reconciliation of unstructured or semi-structured Web content with the neatly structured database paradigm. With this Web-database in hand, researchers and practitioners have recently begun developing mechanisms which return structured results in response to an unstructured query. These new developments are a product of (1) record, list and table extraction from large numbers of semi-structured Web pages, (2) integration of these disparate extraction results into a consistent form, and (3) analysis of the newly extracted and integrated Web data. Among the many fruits of this line of work is the ability for semi-structured Web data to enhance the search capabilities of a schema-bound database. Alternatively, structured database records have also been used to augment Web page collections typically used by Web search engines. We will cover several key technologies, and principles explored so far in the area of Web information extraction, search and exploration.

#index 1948209
#* Temporal web dynamics and its application to information retrieval
#@ Kira Radinsky;Fernando Diaz;Susan Dumais;Milad Shokouhi;Anlei Dong;Yi Chang
#t 2013
#c 2
#% 575568
#% 640706
#% 643520
#% 805200
#% 823342
#% 875959
#% 1155729
#% 1166523
#% 1166533
#% 1214671
#% 1227620
#% 1355017
#% 1399966
#% 1482241
#% 1560388
#% 1598372
#% 1598453
#% 1641927
#% 1642196
#% 1746858
#% 1746888
#% 1879052
#! The World Wide Web is highly dynamic and is constantly evolving to cover the latest information about the physical and social updates in the world. At the same time, the changes in web contents are entangled with new information needs and time-sensitive user interactions with information sources. To address these temporal information needs effectively, it is essential for the search engines to model web dynamics and understand the changes in user behavior over time that are caused by them. In this full-day tutorial, we focus on modeling time-sensitive content on the web, and discuss the state-of-the-art approaches for integrating temporal signals in web search. We address many of the related research topics including those associated with searching dynamic collections, defining time-sensitive relevance, understanding user query behavior over time, and investigating the mains reasons behind content changes. We also cover algorithms, architectures, evaluation methodologies and metrics for time-aware search, and discuss the latest breakthroughs and open challenges, both from the algorithmic and the architectural perspectives.

#index 1948210
#* (big) usage data in web search
#@ Ricardo Baeza-Yates;Yoelle Maarek
#t 2013
#c 2
#! Web Search, which takes its root in the mature field of information retrieval, evolved tremendously over the last 15 years. The field encountered its first revolution when it started to deal with huge amounts of Web pages. Then, a major step was accomplished when engines started to consider the structure of the Web graph and leveraged link analysis in both crawling and ranking. Finally, a more discrete, but no less critical step, was made when search engines started to monitor and exploit the numerous (mostly implicit) signals provided by users while interacting with the search engine. In this tutorial we focus on this "revolution" of large scale usage data. In the first part of this tutorial, we focus on usage data, which typically refers to any type of information provided by the user while interacting with the search engine. It comes first under its raw form as a set of individual signals, but is typically mined after multiple signals have been aggregated and linked to the same interaction event. The two major types of such data are (1) query streams, which include the query string that the user issued, together with the time-stamp of the query, a user identifier, possibly the IP of the machine on which the browser runs, and (2) click data, which include the reference to the element the user clicked on the page together with the timestamp, user identifier, possibly IP, the rank of the link if it is a result, etc. Exploiting usage data under its multiple forms brought an unprecedented wealth of implicit information to Web Search. We discuss in the second part of this tutorial some of the key Web search applications that it made possible. One such example is the query spelling correction feature embodied now in all search engines. In fact, after years of very sophisticated spell checking research, simply counting similar queries at a small edit distance would in most cases surface the most popular spelling as the correct one, a beautiful and simple demonstration of the wisdom of crowds principle.

#index 1948211
#* Econometric analysis and digital marketing: how tomeasure the effectiveness of an ad
#@ Ayman Farahat;James Shanahan
#t 2013
#c 2
#! Over the past 18 years online advertising has grown to a $70 billion industry worldwide annually. Despite this impressive growth, online advertising faces many (and some would say traditional) challenges including how to measure the efficiency or the potential loss of sales caused by the inefficient use of advertising dollars. Consequently, it is vital to measure, maximize, and benchmark the efficiency of advertising media expenditures. This tutorial introduces the field of econometrics as a means of measuring the effectiveness of digital marketing. Econometrics is a field that extends and applies statistical methods to the analysis of economic phenomena. In that vein, econometrics goes beyond traditional statistics and explicitly recognizes the complexities of human behavior. Consider for example the impact of deep discounts on survival of restaurants. Struggling businesses are more likely to offer these deep discounts and eventually fail. A naïve application of statistical techniques will overestimate the impact of deep discounts on business survival. In this case, the discounts are an endogenous variable as compared to an exogenous variable. This type of specification error highlights why we need a deeper look at the variables that go into statistical models. Econometrics addresses these and other issues in a formal and rigorous manner.

#index 1948212
#* WSCD2013: workshop on web search click data 2013
#@ Pavel Serdyukov;Georges Dupret;Nick Craswell
#t 2013
#c 2
#! WSCD 2013 is the third workshop on Web Search Click Data, following WSCD 2009 and WSCD 2012. It is a forum for new research relating to Web search usage logs and for discussing desirable properties of publicly released search log datasets. Research relating to search logs has been hampered by the limited availability of click datasets. This workshop comes with a new dataset based on logged user search behaviour and an accompanying challenge to predict switches between search engines within a given search session.

#index 1948213
#* 3rd workshop on context-awareness in retrieval and recommendation
#@ Matthias Böhmer;Ernesto W. De Luca;Alan Said;Jaime Teevan
#t 2013
#c 2
#% 1543112
#% 1729448
#! Context-aware information is widely available in various ways and is becoming more and more important for enhancing retrieval performance and recommendation results. The current main issue to cope with is not only recommending or retrieving the most relevant items and content, but defining them ad hoc. Other relevant issues include personalizing and adapting the information and the way it is displayed to the user's current situation and interests. Ubiquitous computing further provides new means for capturing user feedback on items and providing information.

#index 1948214
#* Workshop on large-scale and distributed systems for information retrieval (LSDS-IR 2013)
#@ Nicola Tonellotto;Craig Macdonald;Ismail Sengor Altingovde
#t 2013
#c 2
#! The LSDS-IR'13 workshop aims to bring together both information retrieval practitioners from industry, as well as academic researchers concerned with efficient and distributed IR systems. The workshop also welcomes contributions that propose different ways of leveraging diversity and multiplicity of resources available in distributed systems. The main goal of the workshop is to attract people from industry and academia to present and discuss ideas, problems and results in efficiency of large scale and distributed information retrieval systems, and to foster their participation to the WSDM conference.

#index 1948215
#* Workshop on semantic personalized information management (SPIM'13)
#@ Till Plumbaum;Ernesto William De Luca;Aldo Gangemi;Michael Hausenblas
#t 2013
#c 2
#! The SPIM workshop focuses especially on people that are working on the social or semantic Web, machine learning, user modeling, recommender systems, information retrieval, semantic interaction, or their combination. The goal is to bring together researchers and practitioners to initiating discussions on the different requirements and challenges coming with the social and semantic Web for personalized information retrieval systems. The workshop aims at improving the exchange of ideas between the different research communities and practitioners involved in the research on semantic personalized information management.

#index 1948216
#* Search and exploration of X-Rated information (SEXI 2013)
#@ Vanessa Murdock;Charles L.A. Clarke;Jaap Kamps;Jussi Karlgren
#t 2013
#c 2
#% 811256
#! Adult content is pervasive on the Web, has been a driving factor in the adoption of the Internet medium. It is responsible for a significant fraction of traffic and revenues, yet rarely attracts attention in research. We propose that the research questions surrounding adult content access behaviors are unique, and we believe interesting and valuable research in this area can be done ethically. The workshop on Search and Exploration of X-Rated Information (SEXI) addresses these issues for information access tasks related to adult content.

#index 2045845
#* Proceedings of the 7th ACM international conference on Web search and data mining
#@ Ben Carterette;Fernando Diaz;Carlos Castillo;Donald Metzler
#t 2014
#c 2
#! It is our great pleasure to welcome you to the Seventh ACM International Conference on Web Search and Data Mining (WSDM 2014) held on February 24--28, 2014 in New York City, New York, USA. As with previous installments, WSDM attracted many high quality submissions covering a broad spectrum of Web search and data mining topics. WSDM continues be a leading forum for reporting the latest research developments in the field. We are delighted to present here the proceedings of the conference. We received a total of 355 submissions from a diverse group of 44 countries and regions, of which 64 were accepted for full paper publication in the proceedings, thus achieving an acceptance rate of 18%. The accepted papers are from 20 different countries and represent a nice mix of academic and industrial research, making this a truly international and diverse forum. Some of the most popular research topics this year include Web search, computational advertising, recommender systems, and social networks. As in the past, WSDM 2014 continues to be a single track conference. To accommodate this, 19 papers were chosen to be presented as long presentations, while the remaining 45 will be presented as short presentations. As in the past, all authors of accepted papers were afforded the opportunity to present a poster during the poster session. There were many remarkable papers submitted to the conference. We chose 10 of the most exceptional papers as Best Paper Award candidates.

#index 2045846
#* Behavioral data mining and network analysis in massive online games
#@ Muhammad Aurangzeb Ahmad;Jaideep Srivastava
#t 2014
#c 2
#! The last decade has been characterized by an explosion of social media in a variety of forms. Since the data is captured in digital form it has become possible for the first time study human behavior at a massive scale. Not only is it possible to address traditional questions in the social sciences regarding collective dynamics of human behaviors but it is also possible to study new types of human behaviors which have arisen as a result of usage of new mediums like twitter, YouTube, Facebook, one games etc. Each of these mediums has its respective limitations and affordances. Out of all these mediums the most complex and data rich medium is that of Massive Online Games (MOGs). MOGs refer to massive online persistent environments (World of Warcraft, EVE Online, EverQuest etc) shared by millions of people . In general these environments are characterized by a rich array of activities and social interactions with a wide array of behaviors e.g., cooperation, trade, quest, deceit, mentoring etc. Such environments allow one to study human behavior at a level of granularity where it was not possible to do so previously. Given the challenges associated with analyzing this type of data traditional techniques in data mining and social network analysis have to be extended with insights from the social sciences. The tutorial will cover predictive and generative models in the study of MOGs. Additionally we will cover some SNA techniques which are more appropriate for MOGs given the multi-dimensionality of the data (P*/ERGM Models, IR Based Network Analysis, Hypergrah based Techniques, Coextensive Social Networks etc). We also describe the various ways in which MOGs exhibit similarities to the real world e.g., economic behaviors, clandestine behaviors, mentoring etc).

#index 2045847
#* Exploration and mining of web repositories
#@ Nan Zhang;Gautam Das
#t 2014
#c 2
#! With the proliferation of very large data repositories hidden behind web interfaces, e.g., keyword search, form-like search and hierarchical/graph-based browsing interfaces for Amazon.com, eBay.com, etc., efficient ways of searching, exploring and/or mining such web data are of increasing importance. There are two key challenges facing these tasks: how to properly understand web interfaces, and how to bypass the interface restrictions. In this tutorial, we start with a general overview of web search and data mining, including various exciting applications enabled by the effective search, exploration, and mining of web repositories. Then, we focus on the fundamental developments in the field, including web interface understanding, crawling, sampling, and data analytics over web repositories with various types of interfaces. We also discuss the potential changes required for query processing, data mining and machine learning algorithms to be applied to web data. Our goal is two-fold: one is to promote the awareness of existing web data search/explora-tion/mining techniques among all web researchers who are interested in leveraging web data, and the other is to encourage researchers, especially those who have not previously worked in web search and mining before, to initiate their own research in these exciting areas.

#index 2045848
#* Big graph mining for the web and social media: algorithms, anomaly detection, and applications
#@ U. Kang;Leman Akoglu;Duen Horng Chau
#t 2014
#c 2
#! Graphs are everywhere: social networks, computer net- works, mobile call networks, the World Wide Web, protein interaction networks, and many more. The lower cost of disk storage, the success of social networking websites and Web 2.0 applications, and the high availability of data sources lead to graphs being generated at unprecedented size. They are now measured in terabytes or even petabytes, with more than billions of nodes and edges. Finding patterns on large graphs have a lot of applica- tions including cyber security on the Web, social media min- ing (Facebook, Twitter), and fraud detection, among others. This tutorial will cover topics related to finding patterns and anomalies and sensemaking in large-scale graphs with appli- cations to real-world problems in social media and the Web. Specifically, we aim to answer the following questions: How can we scale up graph mining algorithms for massive graphs with billions of edges? How can we find anomalies in such large-scale graphs? How can we make sense of disk-resident large graphs, what and how can we do visual analytics? How can we use the algorithms and anomaly detection techniques to solve challenging real-world problems that play key role in social media and the Web? Our tutorial consists of three main parts. We start with scalable graph mining algorithms for billion-scale graphs, in- cluding structure analysis, eigensolvers, storage and index- ing, and graph layout and graph compression. Next we de- scribe anomaly detection techniques for large scale graphs with applications on social media. Finally, we discuss vi- sual analytics techniques which leverage these algorithms and anomaly detection techniques in the previous parts.

#index 2045849
#* Diversity and novelty in web search, recommender systems and data streams
#@ Rodrygo L.T. Santos;Pablo Castells;Ismail Sengor Altingovde;Fazli Can
#t 2014
#c 2
#! This tutorial aims to provide a unifying account of current research on diversity and novelty in the domains of web search, recommender systems, and data stream processing.

#index 2045850
#* Multilingual probabilistic topic modeling and its applications in web mining and search
#@ Marie-Francine Moens;Ivan Vulié
#t 2014
#c 2
#% 722904
#% 1536542
#% 1967743
#% 2005711
#% 2012170
#! Multilingual topic models are a fairly novel group of unsupervised, language-independent and generative machine learning models. This tutorial covers all key aspects of their probabilistic framework and demonstrates how to easily integrate these models into frameworks for cross-lingual and multilingual Web mining and search.

#index 2045851
#* Entity linking and retrieval for semantic search
#@ Edgar Meij;Krisztian Balog;Daan Odijk
#t 2014
#c 2
#! More and more search engine users are expecting direct answers to their information needs, rather than links to documents. Semantic search and its recent applications enabled search engines to organize their wealth of information around entities. Entity linking and retrieval provide the building stones for organizing the web of entities. This tutorial aims to cover all facets of semantic search from a unified point of view and connect real-world applications with results from scientific publications. We provide a comprehensive overview of entity linking and retrieval in the context of semantic search and thoroughly explore techniques for query understanding, entity-based retrieval and ranking on unstructured text, structured knowledge repositories, and a mixture of these. We point out the connections between published approaches and applications, and provide hands-on examples on real-world use cases and datasets.

#index 2045852
#* Integration of large scale knowledge bases using probabilistic graphical models
#@ Arnab Kumar Dutta
#t 2014
#c 2
#% 850430
#% 956564
#% 1100143
#% 1250362
#% 1275012
#% 1275182
#% 1288161
#% 1409954
#% 1471191
#% 1471208
#% 1536527
#% 1652065
#% 1654048
#% 1711865
#% 1942749
#% 2025262
#! Over the recent past, information extraction (IE) systems such as Nell and ReVerb have attained much success in creating large knowledge resources with minimal supervision. But, these resources in general, lack schema information and contain facts with high degree of ambiguity which are often difficult to interpret. Whereas, Wikipedia-based IE projects like DBpedia and Yago are structured, have disambiguated facts with unique identifiers and maintain a well-defined schema. In this work, we propose a probabilistic method to integrate these two types of IE projects where the structured knowledge bases benefit from the wide coverage of the semi-supervised IE projects and the latter benefits from the schema information of the former.

#index 2045853
#* Strategy in action: analyzing online search behavior bymining search strategies
#@ Chathra Hendahewa
#t 2014
#c 2
#% 148007
#% 399057
#% 793374
#% 805898
#% 956509
#% 1024551
#% 1355038
#% 1537504
#% 1641927
#% 1746847
#% 1746858
#% 1879024
#% 1919710
#% 1948153
#% 1978793
#! Analyzing people's Web search behavior has been a significant topic of interest in the Information Retrieval domain and search engine industry over the past decade. Research in this area has focused on improving search and retrieval capabilities leading to high demands and expectations of Web search users. Understanding and analyzing the Web search process when users are performing Web search tasks is a challenging problem due to many reasons such as subjectivity, dynamic nature, difficulty in measurement of success and difficulty in evaluation. I propose to analyze the users' Web search behavior in order to identify the strategies and tactics they use in fulfilling their task. In order to achieve this, I intend to use data mining and machine learning methods with an emphasis on time series analysis given that the user search process can be considered as a sequence of time related events.

#index 2045854
#* On discovering non-obvious recommendations: using unexpectedness and neighborhood selection methods in collaborative filtering systems
#@ Panagiotis Adamopoulos
#t 2014
#c 2
#% 220709
#% 220711
#% 253310
#% 424021
#% 739691
#% 805841
#% 813966
#% 860672
#% 955930
#% 1200360
#% 1260273
#% 1287265
#% 1312141
#% 1541748
#% 1623931
#% 1625354
#% 1755308
#% 1756046
#% 1906108
#% 2007482
#% 2008118
#! This paper proposes a number of studies in order to move the field of recommender systems beyond the traditional paradigm and the classical perspective of rating prediction accuracy. We contribute to existing helpful but less explored recommendation strategies and propose new approaches targeting to more useful recommendations for both users and businesses. Working toward this direction, we discuss the studies we have conducted so far and present our future research plans. The overall goal of this research program is to expand our focus from even more accurate rating predictions toward a more holistic experience for the users, by providing them with non-obvious but high quality recommendations and avoiding the over-specialization and concentration bias problems. In particular, we propose a new probabilistic neighborhood-based method as an improvement of the standard $k$-nearest neighbors approach, alleviating some of the most common problems of collaborative filtering recommender systems, based on classical metrics of dispersion and diversity as well as some newly proposed metrics. Furthermore, we propose a concept of unexpectedness in recommender systems and operationalize it by suggesting various mechanisms for specifying the expectations of the users and proposing a recommendation method for providing the users with unexpected but high quality personalized recommendations that fairly match their interests. Besides, in order to generate utility-based recommendations for Massive Open Online Courses (MOOCs) that better serve the educational needs of students, we study the satisfaction of users with online courses vis-a-vis student retention. Finally, we summarize the conclusions of the conducted studies, discuss the limitations of our work and also outline the managerial implications of the proposed stream of research.

#index 2045855
#* Exploratory search with semantic transformations using collaborative knowledge bases
#@ Yegin Genc
#t 2014
#c 2
#% 262112
#% 722904
#% 766525
#% 835027
#% 840419
#% 857477
#% 857478
#% 955306
#% 1019082
#% 1047297
#% 1130858
#% 1269107
#% 1270267
#% 1272267
#% 1275012
#% 1281972
#% 1384134
#% 1451241
#% 1484272
#% 1558472
#% 1607030
#% 1693918
#% 1913791
#% 1992451
#! Sometimes we search for simple facts. Other times we search for relationships between concepts. While existing information retrieval systems work well for simple searches, they are less satisfying for complex inquiries because of the ill-structured nature of many searches and the cognitive load involved in the search process. Search can be improved by leveraging the network of concepts that are maintained by collaborative knowledge bases such as Wikipedia. By treating exploratory search inquires as networks of concepts -- and then mapping documents to these concepts, exploratory search performance can be improved. This method is applied to an exploratory search task: given a journal abstract, abstracts are ranked based their relevancy to the seed abstract. The results show comparable relevancy scores to state of the art techniques while at the same time providing better diversity.

#index 2045856
#* Search by multiple examples
#@ Mingzhu Zhu;Yi-Fang Brook Wu
#t 2014
#c 2
#% 248225
#% 266215
#% 266292
#% 445243
#% 465895
#% 466263
#% 727883
#% 729621
#% 879567
#% 1074082
#% 1077150
#% 1083647
#% 1154321
#% 1279298
#% 1440454
#% 1605962
#% 1974557
#! It is often difficult for users to adopt keywords to express their information needs. Search-By-Multiple-Examples (SBME), a promising method for overcoming this problem, allows users to specify their information needs as a set of relevant documents rather than as a set of keywords. Most of the studies on SBME adopt the Positive Unlabeled learning (PU learning) techniques by treating the users' provided examples (denote as query examples) as positive set and the entire data collection as unlabeled set. However, it is inefficient to treat the entire data collection as unlabeled set, as its size can be huge. In addition, the query examples are treated as being relevant to a single topic, but it is often the case that they can be relevant to multiple topics. As the query examples are much fewer than the unlabeled data, the system performance may downgrade dramatically because of the class imbalance problem. What's more, the experiments conducted in these studies have not taken into account the settings in online search, which are very different from the controlled experiments scenario. This proposed research seeks to explore how to improve SBME by exploring: (1) how to predict user' information needs by modeling the content of the documents using probabilistic topic models; (2) how to deal with the class imbalance problem by reducing the size of the unlabeled data and adopting machine learning techniques. We will also conduct extensive experiments to better evaluate SBME using different sizes of query examples to simulate users' information needs.

#index 2045857
#* Log-based personalization: the 4th web search click data (WSCD) workshop
#@ Pavel Serdyukov;Georges Dupret;Nick Craswell
#t 2014
#c 2
#! WSCD 2014 is the fourth workshop on Web Search Click Data, following WSCD 2009, WSCD 2011 and WSCD 2012. It is a forum for new research relating to Web search usage logs and for discussing desirable properties of publicly released search log datasets. Research relating to search logs has been hampered by the limited availability of click datasets. This series of workshops comes with new datasets based on logged user search behaviour and accompanying data mining challenges. This year the challenge and the workshop are focused on the tasks of personalization using logs.

#index 2045858
#* Web-scale classification: web classification in the big data era
#@ Ioannis Partalas;Massih-Reza Amini;Ion Androutsopoulos;Thierry Artieres;Patrick Gallinari;Eric Gaussier;Georgios Paliouras
#t 2014
#c 2
#% 1457107
#! This paper provides an overview of the workshop Web-Scale Classification: Web Classification in the Big Data Era which was held in New York City, on February 28th as a workshop of the seventh International Conference on Web Search and Data Mining. The goal of the workshop was to discuss and assess recent research focusing on classification and mining in Web-scale category systems. The workshop brought together members of several communities such web mining, machine learning, text classification and social media mining.

#index 2045859
#* 1st workshop on diffusion networks and cascade analytics
#@ Peng Cui;Fei Wang;Hanghang Tong;Manuel Gomez Rodriguez
#t 2014
#c 2
#% 1991839
#! Diffusion and cascades have been studied for many years in sociology, and different theoretical models have been developed. However, experimental validation has been always carried out in relatively small datasets. In recent years, with the availability of large-scale network and cascade data, research on cascading and diffusion phenomena has aroused considerable interests from various fields in computer science. One of the main goals is to discover different propagation patterns from historical cascade data. In this context, understanding the mechanisms underlying diffusion in both micro- and macro-scale levels and further develop predictive model of diffusion are fundamental problems of crucial importance.

#index 2045860
#* Workshop on large-scale and distributed systems for information retrieval (LSDS-IR 2014)
#@ Ismail Sengor Altingovde;B. Barla Cambazoglu;Craig Macdonald;Nicola Tonellotto
#t 2014
#c 2
#! The LSDS-IR'14 workshop aims to bring together information retrieval practitioners from industry and academic researchers concerned with efficient and distributed IR systems. The workshop also welcomes contributions that propose different ways of leveraging diversity and multiplicity of resources available in distributed systems. The main goal of the workshop is to attract people from industry and academia to present and discuss ideas, problems, and results related to the efficiency of large scale and distributed information retrieval systems.

#index 2045861
#* Data design for personalization: current challenges and emerging opportunities
#@ Elizabeth F. Churchill;Atish Das Sarma
#t 2014
#c 2
#! There are several definitions of personalization but one that relates specifically to internet technologies is the following: Personalization technology enables the dynamic insertion, customization or suggestion of content in any format that is relevant to the individual user, based on the user's implicit behavior and preferences, and explicitly given details. Personalization is central to most Internet experiences. Personalization is a data-driven process, whether the data are explicitly gathered (e.g., by asking people to fill out forms) or implicitly (e.g. through analysis of behavioral data). It is clear that designing for effective personalization poses interesting engineering and computer science challenges. However, personalization is also a user experience issue. We believe that encouraging dialogue and collaboration between data mining experts, content providers, and user-focused researchers will offer gains in the area of personalization for search and for other domains. This is increasingly the case as devices enable more forms of data to be gathered, are always on/connected and are always with users. This workshop brings researchers interested in the area of personalization to share their research, explore possibilities for collaboration, and work on defining an agenda for Data Design for Personalization.

#index 2045862
#* Visualizing brand associations from web community photos
#@ Gunhee Kim;Eric P. Xing
#t 2014
#c 2
#% 593047
#% 844334
#% 883972
#% 884059
#% 902495
#% 997090
#% 1055702
#% 1338043
#% 1883970
#% 1884422
#% 1923623
#% 2014226
#! Brand Associations, one of central concepts in marketing, describe customers' top-of-mind attitudes or feelings toward a brand. Thus, this consumer-driven brand equity often attains the grounds for purchasing products or services of the brand. Traditionally, brand associations are measured by analyzing the text data from consumers' responses to the survey or their online conversation logs. In this paper, we propose to go beyond text data and leverage large-scale online photo collections contributed by the general public, which have not been explored so far. As a first technical step toward the study of photo-based brand associations, we aim to jointly achieve the following two visualization tasks in a mutually-rewarding way: (i) detecting and visualizing core visual concepts associated with brands, and (ii) localizing the regions of brand in the images. With experiments on about five millions of images of 48 brands crawled from five popular online photo sharing sites, we demonstrate that our approach can discover complementary views on the brand associations that are hardly mined from the text data. We also quantitatively show that our approach outperforms other candidate methods on the both visualization tasks.

#index 2045863
#* FENNEL: streaming graph partitioning for massive scale graphs
#@ Charalampos Tsourakakis;Christos Gkantsidis;Bozidar Radunovic;Milan Vojnovic
#t 2014
#c 2
#% 274612
#% 414944
#% 469887
#% 519693
#% 600034
#% 765247
#% 1023420
#% 1164919
#% 1318636
#% 1399992
#% 1426483
#% 1426513
#% 1464649
#% 1583710
#% 1730736
#% 1872377
#% 1875014
#% 1911310
#% 1948175
#% 1991845
#% 1991895
#! Balanced graph partitioning in the streaming setting is a key problem to enable scalable and efficient computations on massive graph data such as web graphs, knowledge graphs, and graphs arising in the context of online social networks. Two families of heuristics for graph partitioning in the streaming setting are in wide use: place the newly arrived vertex in the cluster with the largest number of neighbors or in the cluster with the least number of non-neighbors. In this work, we introduce a framework which unifies the two seemingly orthogonal heuristics and allows us to quantify the interpolation between them. More generally, the framework enables a well principled design of scalable, streaming graph partitioning algorithms that are amenable to distributed implementations. We derive a novel one-pass, streaming graph partitioning algorithm and show that it yields significant performance improvements over previous approaches using an extensive set of real-world and synthetic graphs. Surprisingly, despite the fact that our algorithm is a one-pass streaming algorithm, we found its performance to be in many cases comparable to the de-facto standard offline software METIS and in some cases even superiror. For instance, for the Twitter graph with more than 1.4 billion of edges, our method partitions the graph in about 40 minutes achieving a balanced partition that cuts as few as 6.8% of edges, whereas it took more than 81/2 hours by METIS to produce a balanced partition that cuts 11.98% of edges. We also demonstrate the performance gains by using our graph partitioner while solving standard PageRank computation in a graph processing platform with respect to the communication cost and runtime.

#index 2045864
#* Search engine click spam detection based on bipartite graph propagation
#@ Xin Li;Min Zhang;Yiqun Liu;Shaoping Ma;Yijiang Jin;Liyun Ru
#t 2014
#c 2
#% 255170
#% 310559
#% 411762
#% 463903
#% 464996
#% 577224
#% 778732
#% 824665
#% 879567
#% 1016177
#% 1020406
#% 1022521
#% 1035578
#% 1055760
#% 1112365
#% 1125900
#% 1164783
#% 1190055
#% 1190056
#% 1399983
#! Using search engines to retrieve information has become an important part of people's daily lives. For most search engines, click information is an important factor in document ranking. As a result, some websites cheat to obtain a higher rank by fraudulently increasing clicks to their pages, which is referred to as "Click Spam". Based on an analysis of the features of fraudulent clicks, a novel automatic click spam detection approach is proposed in this paper, which consists of 1. modeling user sessions with a triple sequence, which, to the best of our knowledge, takes into account not only the user action but also the action objective and the time interval between actions for the first time; 2. using the user-session bipartite graph propagation algorithm to take advantage of cheating users to find more cheating sessions; and 3. using the pattern-session bipartite graph propagation algorithm to obtain cheating session patterns to achieve higher precision and recall of click spam detection. Experimental results based on a Chinese commercial search engine using real-world log data containing approximately 80 million user clicks per day show that 2.6% of all clicks were detected as spam with a precision of up to 97%.

#index 2045865
#* Latent dirichlet allocation based diversified retrieval for e-commerce search
#@ Jun Yu;Sunil Mohan;Duangmanee (Pew) Putthividhya;Weng-Keen Wong
#t 2014
#c 2
#% 248214
#% 262112
#% 397133
#% 642975
#% 722904
#% 766448
#% 805841
#% 805863
#% 818266
#% 879566
#% 879618
#% 879686
#% 987313
#% 1073970
#% 1074025
#% 1074133
#% 1166473
#% 1190062
#% 1190093
#% 1206662
#% 1229386
#% 1348342
#% 1450988
#% 1451241
#% 1560378
#% 1605921
#! Diversified retrieval is a very important problem on many e-commerce sites, e.g. eBay and Amazon. Using IR approaches without optimizing for diversity results in a clutter of redundant items that belong to the same products. Most existing product taxonomies are often too noisy, with overlapping structures and non-uniform granularity, to be used directly in diversified retrieval. To address this problem, we propose a Latent Dirichlet Allocation (LDA) based diversified retrieval approach that selects diverse items based on the hidden user intents. Our approach first discovers the hidden user intents of a query using the LDA model, and then ranks the user intents by making trade-offs between their relevance and information novelty. Finally, it chooses the most representative item for each user intent to display. To evaluate the diversity in the search results on e-commerce sites, we propose a new metric, average satisfaction, measuring user satisfaction with the search results. Through our empirical study on eBay, we show that the LDA model discovers meaningful user intents and the LDA-based approach provides significantly higher user satisfaction than the eBay production ranker and three other diversified retrieval approaches.

#index 2045866
#* Learning social network embeddings for predicting information diffusion
#@ Simon Bourigault;Cedric Lagnier;Sylvain Lamprier;Ludovic Denoyer;Patrick Gallinari
#t 2014
#c 2
#% 464615
#% 729923
#% 770767
#% 987246
#% 1107420
#% 1130830
#% 1214671
#% 1275188
#% 1411585
#% 1451242
#% 1535333
#% 1560424
#% 1613641
#% 1747165
#% 1747172
#% 1925630
#% 1948123
#% 1948182
#% 1967741
#% 2011677
#! Analyzing and modeling the temporal diffusion of information on social media has mainly been treated as a diffusion process on known graphs or proximity structures. The underlying phenomenon results however from the interactions of several actors and media and is more complex than what these models can account for and cannot be explained using such limiting assumptions. We introduce here a new approach to this problem whose goal is to learn a mapping of the observed temporal dynamic onto a continuous space. Nodes participating to diffusion cascades are projected in a latent representation space in such a way that information diffusion can be modeled efficiently using a heat diffusion process. This amounts to learning a diffusion kernel for which the proximity of nodes in the projection space reflects the proximity of their infection time in cascades. The proposed approach possesses several unique characteristics compared to existing ones. Since its parameters are directly learned from cascade samples without requiring any additional information, it does not rely on any pre-existing diffusion structure. Because the solution to the diffusion equation can be expressed in a closed form in the projection space, the inference time for predicting the diffusion of a new piece of information is greatly reduced compared to discrete models. Experiments and comparisons with baselines and alternative models have been performed on both synthetic networks and real datasets. They show the effectiveness of the proposed method both in terms of prediction quality and of inference speed.

#index 2045867
#* Lessons from the journey: a query log analysis of within-session learning
#@ Carsten Eickhoff;Jaime Teevan;Ryen White;Susan Dumais
#t 2014
#c 2
#% 399057
#% 590523
#% 751596
#% 754059
#% 818221
#% 818259
#% 832349
#% 855200
#% 857478
#% 869527
#% 968433
#% 1130878
#% 1166518
#% 1195879
#% 1213448
#% 1297195
#% 1357833
#% 1397425
#% 1450902
#% 1457107
#% 1598513
#% 1641961
#% 1682429
#% 1693883
#% 1693885
#% 1765140
#% 1879011
#% 1891125
#% 1948152
#% 2004721
#! The Internet is the largest source of information in the world. Search engines help people navigate the huge space of available data in order to acquire new skills and knowledge. In this paper, we present an in-depth analysis of sessions in which people explicitly search for new knowledge on the Web based on the log files of a popular search engine. We investigate within-session and cross-session developments of expertise, focusing on how the language and search behavior of a user on a topic evolves over time. In this way, we identify those sessions and page visits that appear to significantly boost the learning process. Our experiments demonstrate a strong connection between clicks and several metrics related to expertise. Based on models of the user and their specific context, we present a method capable of automatically predicting, with good accuracy, which clicks will lead to enhanced learning. Our findings provide insight into how search engines might better help users learn as they search.

#index 2045868
#* Detecting non-gaussian geographical topics in tagged photo collections
#@ Christoph Carl Kling;Jérôme Kunegis;Sergej Sizov;Steffen Staab
#t 2014
#c 2
#% 103743
#% 722904
#% 829039
#% 869516
#% 1016371
#% 1355044
#% 1385969
#% 1560379
#% 1650298
#% 1746875
#% 1992392
#! Nowadays, large collections of photos are tagged with GPS coordinates. The modelling of such large geo-tagged corpora is an important problem in data mining and information retrieval, and involves the use of geographical information to detect topics with a spatial component. In this paper, we propose a novel geographical topic model which captures dependencies between geographical regions to support the detection of topics with complex, non-Gaussian distributed spatial structures. The model is based on a multi-Dirichlet process (MDP), a novel generalisation of the hierarchical Dirichlet process extended to support multiple base distributions. Our method thus is called the MDP-based geographical topic model (MGTM). We show how to use a MDP to dynamically smooth topic distributions between groups of spatially adjacent documents. In systematic quantitative and qualitative evaluations using independent datasets from prior related work, we show that such a model can exploit the adjacency of regions and leads to a significant improvement in the quality of topics compared to the state of the art in geographical topic modelling.

#index 2045869
#* Spatial compactness meets topical consistency: jointly modeling links and content for community detection
#@ Mrinmaya Sachan;Avinava Dubey;Shashank Srivastava;Eric P. Xing;Eduard Hovy
#t 2014
#c 2
#% 313959
#% 438553
#% 722904
#% 788094
#% 869480
#% 1083684
#% 1117695
#% 1211773
#% 1211828
#% 1289476
#% 1385969
#% 1605987
#% 1746831
#% 1747070
#% 1807663
#! In this paper, we address the problem of discovering topically meaningful, yet compact (densely connected) communities in a social network. Assuming the social network to be an integer-weighted graph (where the weights can be intuitively defined as the number of common friends, followers, documents exchanged, etc.), we transform the social network to a more efficient representation. In this new representation, each user is a bag of her one-hop neighbors. We propose a mixed-membership model to identify compact communities using this transformation. Next, we augment the representation and the model to incorporate user-content information imposing topical consistency in the communities. In our model a user can belong to multiple communities and a community can participate in multiple topics. This allows us to discover community memberships as well as community and user interests. Our method outperforms other well known baselines on two real-world social networks. Finally, we also provide a fast, parallel approximation of the same.

#index 2045870
#* Modeling dwell time to predict click-level satisfaction
#@ Youngho Kim;Ahmed Hassan;Ryen W. White;Imed Zitouni
#t 2014
#c 2
#% 136350
#% 320432
#% 340974
#% 397161
#% 577224
#% 642982
#% 731615
#% 766454
#% 805200
#% 879565
#% 879567
#% 907516
#% 907544
#% 944349
#% 956541
#% 987260
#% 987263
#% 1074128
#% 1130811
#% 1130868
#% 1227585
#% 1301004
#% 1338622
#% 1355038
#% 1399944
#% 1450833
#% 1450876
#% 1450902
#% 1468142
#% 1537504
#% 1558464
#% 1598368
#% 1641927
#% 1746855
#% 1813854
#% 1879020
#% 1948136
#% 1991863
#% 2006183
#! Clicks on search results are the most widely used behavioral signals for predicting search satisfaction. Even though clicks are correlated with satisfaction, they can also be noisy. Previous work has shown that clicks are affected by position bias, caption bias, and other factors. A popular heuristic for reducing this noise is to only consider clicks with long dwell time, usually equaling or exceeding 30 seconds. The rationale is that the more time a searcher spends on a page, the more likely they are to be satisfied with its contents. However, having a single threshold value assumes that users need a fixed amount of time to be satisfied with any result click, irrespective of the page chosen. In reality, clicked pages can differ significantly. Pages have different topics, readability levels, content lengths, etc. All of these factors may affect the amount of time spent by the user on the page. In this paper, we study the effect of different page characteristics on the time needed to achieve search satisfaction. We show that the topic of the page, its length and its readability level are critical in determining the amount of dwell time needed to predict whether any click is associated with satisfaction. We propose a method to model and provide a better understanding of click dwell time. We estimate click dwell time distributions for SAT (satisfied) or DSAT (dissatisfied) clicks for different click segments and use them to derive features to train a click-level satisfaction model. We compare the proposed model to baseline methods that use dwell time and other search performance predictors as features, and demonstrate that the proposed model achieves significant improvements.

#index 2045871
#* Struggling or exploring?: disambiguating long search sessions
#@ Ahmed Hassan;Ryen W. White;Susan T. Dumais;Yi-Min Wang
#t 2014
#c 2
#% 577224
#% 748600
#% 805200
#% 807420
#% 818221
#% 823348
#% 857478
#% 860649
#% 987209
#% 987263
#% 1047436
#% 1130852
#% 1130878
#% 1185582
#% 1292474
#% 1355038
#% 1384094
#% 1400034
#% 1450833
#% 1450902
#% 1450977
#% 1450994
#% 1598368
#% 1598439
#% 1693883
#% 1765695
#% 1879020
#% 1879035
#% 1919837
#% 1988849
#! Web searchers often exhibit directed search behaviors such as navigating to a particular Website. However, in many circumstances they exhibit different behaviors that involve issuing many queries and visiting many results. In such cases, it is not clear whether the user's rationale is to intentionally explore the results or whether they are struggling to find the information they seek. Being able to disambiguate between these types of long search sessions is important for search engines both in performing retrospective analysis to understand search success, and in developing real-time support to assist searchers. The difficulty of this challenge is amplified since many of the characteristics of exploration (e.g., multiple queries, long duration) are also observed in sessions where people are struggling. In this paper, we analyze struggling and exploring behavior in Web search using log data from a commercial search engine. We first compare and contrast search behaviors along a number dimensions, including query dynamics during the session. We then build classifiers that can accurately distinguish between exploring and struggling sessions using behavioral and topical features. Finally, we show that by considering the struggling/exploring prediction we can more accurately predict search satisfaction.

#index 2045872
#* Heterogeneous graph-based intent learning with queries, web pages and Wikipedia concepts
#@ Xiang Ren;Yujing Wang;Xiao Yu;Jun Yan;Zheng Chen;Jiawei Han
#t 2014
#c 2
#% 342621
#% 430746
#% 766433
#% 987222
#% 1074093
#% 1190102
#% 1214657
#% 1214701
#% 1214708
#% 1227577
#% 1275012
#% 1399972
#% 1400017
#% 1400033
#% 1400099
#% 1598339
#% 1642076
#% 1693900
#% 1746800
#% 1879023
#% 1918385
#% 1919823
#% 1948151
#% 1948193
#! The problem of learning user search intents has attracted intensive attention from both industry and academia. However, state-of-the-art intent learning algorithms suffer from different drawbacks when only using a single type of data source. For example, query text has difficulty in distinguishing ambiguous queries; search log is bias to the order of search results and users' noisy click behaviors. In this work, we for the first time leverage three types of objects, namely queries, web pages and Wikipedia concepts collaboratively for learning generic search intents and construct a heterogeneous graph to represent multiple types of relationships between them. A novel unsupervised method called heterogeneous graph-based soft-clustering is developed to derive an intent indicator for each object based on the constructed heterogeneous graph. With the proposed co-clustering method, one can enhance the quality of intent understanding by taking advantage of different types of data, which complement each other, and make the implicit intents easier to interpret with explicit knowledge from Wikipedia concepts. Experiments on two real-world datasets demonstrate the power of the proposed method where it achieves a 9.25% improvement in terms of NDCG on search ranking task and a 4.67% enhancement in terms of Rand index on object co-clustering task compared to the best state-of-the-art method.

#index 2045873
#* Active learning for networked data based on non-progressive diffusion model
#@ Zhilin Yang;Jie Tang;Bin Xu;Chunxiao Xing
#t 2014
#c 2
#% 342596
#% 464268
#% 549447
#% 577217
#% 729923
#% 869526
#% 1083734
#% 1166535
#% 1201481
#% 1214702
#% 1264829
#% 1272282
#% 1350272
#% 1355040
#% 1426567
#% 1451243
#% 1617365
#% 1650318
#% 1662950
#% 1798204
#% 1878115
#% 1991890
#! We study the problem of active learning for networked data, where samples are connected with links and their labels are correlated with each other. We particularly focus on the setting of using the probabilistic graphical model to model the networked data, due to its effectiveness in capturing the dependency between labels of linked samples. We propose a novel idea of connecting the graphical model to the information diffusion process, and precisely define the active learning problem based on the non-progressive diffusion model. We show the NP-hardness of the problem and propose a method called MaxCo to solve it. We derive the lower bound for the optimal solution for the active learning setting, and develop an iterative greedy algorithm with provable approximation guarantees. We also theoretically prove the convergence and correctness of MaxCo. We evaluate MaxCo on four different genres of datasets: Coauthor, Slashdot, Mobile, and Enron. Our experiments show a consistent improvement over other competing approaches.

#index 2045874
#* Fast approximation of betweenness centrality through sampling
#@ Matteo Riondato;Evgenios M. Kornaropoulos
#t 2014
#c 2
#% 235295
#% 280394
#% 335411
#% 685134
#% 818434
#% 1272322
#% 1369005
#% 1404186
#% 1428692
#% 1563255
#% 1611331
#% 1667817
#% 1737767
#% 1747152
#% 1898034
#! Betweenness centrality is a fundamental measure in social network analysis, expressing the importance or influence of individual vertices in a network in terms of the fraction of shortest paths that pass through them. Exact computation in large networks is prohibitively expensive and fast approximation algorithms are required in these cases. We present two efficient randomized algorithms for betweenness estimation. The algorithms are based on random sampling of shortest paths and offer probabilistic guarantees on the quality of the approximation. The first algorithm estimates the betweenness of all vertices: all approximated values are within an additive factor ɛ from the real values, with probability at least 1-δ. The second algorithm focuses on the top-K vertices with highest betweenness and approximate their betweenness within a multiplicative factor ɛ, with probability at least $1-δ. This is the first algorithm that can compute such approximation for the top-K vertices. We use results from the VC-dimension theory to develop bounds to the sample size needed to achieve the desired approximations. By proving upper and lower bounds to the VC-dimension of a range set associated with the problem at hand, we obtain a sample size that is independent from the number of vertices in the network and only depends on a characteristic quantity that we call the vertex-diameter, that is the maximum number of vertices in a shortest path. In some cases, the sample size is completely independent from any property of the graph. The extensive experimental evaluation that we performed using real and artificial networks shows that our algorithms are significantly faster and much more scalable as the number of vertices in the network grows than previously presented algorithms with similar approximation guarantees.

#index 2045875
#* Learning latent representations of nodes for classifying in heterogeneous social networks
#@ Yann Jacob;Ludovic Denoyer;Patrick Gallinari
#t 2014
#c 2
#% 840965
#% 961218
#% 1214701
#% 1267758
#% 1288794
#% 1482198
#% 1495579
#% 1598399
#% 1606073
#% 1635130
#% 1642057
#% 1707456
#% 1722664
#% 1763764
#% 1919776
#% 1919865
#% 1991777
#! Social networks are heterogeneous systems composed of different types of nodes (e.g. users, content, groups, etc.) and relations (e.g. social or similarity relations). While learning and performing inference on homogeneous networks have motivated a large amount of research, few work exists on heterogeneous networks and there are open and challenging issues for existing methods that were previously developed for homogeneous networks. We address here the specific problem of nodes classification and tagging in heterogeneous social networks, where different types of nodes are considered, each type with its own label or tag set. We propose a new method for learning node representations onto a latent space, common to all the different node types. Inference is then performed in this latent space. In this framework, two nodes connected in the network will tend to share similar representations regardless of their types. This allows bypassing limitations of the methods based on direct extensions of homogenous frameworks and exploiting the dependencies and correlations between the different node types. The proposed method is tested on two representative datasets and compared to state-of-the-art methods and to baselines.

#index 2045876
#* Is a picture really worth a thousand words?: - on the role of images in e-commerce
#@ Wei Di;Neel Sundaresan;Robinson Piramuthu;Anurag Bhardwaj
#t 2014
#c 2
#% 250678
#% 314988
#% 316798
#% 341929
#% 608010
#% 994112
#% 1019975
#% 1560168
#% 1875829
#! In online peer-to-peer commerce places where physical examination of the goods is infeasible, textual descriptions, images of the products, reputation of the participants, play key roles. Visual image is a powerful channel to convey crucial information towards e-shoppers and influence their choice. In this paper, we investigate a well-known online marketplace where over millions of products change hands and most are described with the help of one or more images. We present a systematic data mining and knowledge discovery approach that aims to quantitatively dissect the role of images in e-commerce in great detail. Our goal is two-fold. First, we aim to get a thorough understanding of impact of images across various dimensions: product categories, user segments, conversion rate. We present quantitative evaluation of the influence of images and show how to leverage different image aspects, such as quantity and quality, to effectively raise sale. Second, we study interaction of image data with other selling dimensions by jointly modeling them with user behavior data. Results suggest that "watch" behavior encodes complex signals combining both attention and hesitation from buyer, in which image still holds an important role when compared to other selling variables, especially for products for which appearance is important. We conclude on how these findings can benefit sellers in a high competitive online e-commerce market.

#index 2045877
#* Trust, but verify: predicting contribution quality for knowledge base construction and curation
#@ Chun How Tan;Eugene Agichtein;Panos Ipeirotis;Evgeniy Gabrilovich
#t 2014
#c 2
#% 302390
#% 309141
#% 420528
#% 465747
#% 722904
#% 739578
#% 751850
#% 879593
#% 1035587
#% 1055738
#% 1074111
#% 1083692
#% 1166519
#% 1190060
#% 1211727
#% 1264744
#% 1288161
#% 1288485
#% 1355029
#% 1439707
#% 1450880
#% 1452843
#% 1452857
#% 1472273
#% 1628024
#% 1711594
#% 1913780
#% 1925702
#% 1948167
#% 1972770
#% 1992398
#! The largest publicly available knowledge repositories, such as Wikipedia and Freebase, owe their existence and growth to volunteer contributors around the globe. While the majority of contributions are correct, errors can still creep in, due to editors' carelessness, misunderstanding of the schema, malice, or even lack of accepted ground truth. If left undetected, inaccuracies often degrade the experience of users and the performance of applications that rely on these knowledge repositories. We present a new method, CQUAL, for automatically predicting the quality of contributions submitted to a knowledge base. Significantly expanding upon previous work, our method holistically exploits a variety of signals, including the user's domains of expertise as reflected in her prior contribution history, and the historical accuracy rates of different types of facts. In a large-scale human evaluation, our method exhibits precision of 91% at 80% recall. Our model verifies whether a contribution is correct immediately after it is submitted, significantly alleviating the need for post-submission human reviewing.

#index 2045878
#* Chinese-English mixed text normalization
#@ Qi Zhang;Huan Chen;Xuanjing Huang
#t 2014
#c 2
#% 311037
#% 579944
#% 939380
#% 939909
#% 983525
#% 1073892
#% 1074093
#% 1251641
#% 1264825
#% 1470657
#% 1470681
#% 1471257
#% 1484342
#% 1497569
#% 1591966
#% 1591989
#% 1711864
#% 1783840
#% 1913335
#% 1913352
#% 1913572
#% 1913608
#% 1913695
#% 1932851
#! Along with the expansion of globalization, multilingualism has become a popular social phenomenon. More than one language may occur in the context of a single conversation. This phenomenon is also prevalent in China. A huge variety of informal Chinese texts contain English words, especially in emails, social media, and other user generated informal contents. Since most of the existing natural language processing algorithms were designed for processing monolingual information, mixed multilingual texts cannot be well analyzed by them. Hence, it is of critical importance to preprocess the mixed texts before applying other tasks. In this paper, we firstly analyze the phenomena of mixed usage of Chinese and English in Chinese microblogs. Then, we detail the proposed two-stage method for normalizing mixed texts. We propose to use a noisy channel approach to translate in-vocabulary words into Chinese. For better incorporating the historical information of users, we introduce a novel user aware neural network language model. For the out-of-vocabulary words (such as pronunciations, informal expressions and et al.), we propose to use a graph-based unsupervised method to categorize them. Experimental results on a manually annotated microblog dataset demonstrate the effectiveness of the proposed method. We also evaluate three natural language parsers with and without using the proposed method as the preprocessing step. From the results, we can see that the proposed method can significantly benefit other NLP tasks in processing mixed text.

#index 2045879
#* Scalable topic-specific influence analysis on microblogs
#@ Bin Bi;Yuanyuan Tian;Yannis Sismanis;Andrey Balmin;Junghoo Cho
#t 2014
#c 2
#% 268079
#% 348173
#% 643056
#% 722904
#% 729923
#% 867050
#% 989613
#% 1077150
#% 1191550
#% 1214641
#% 1214702
#% 1355042
#% 1385969
#% 1426486
#% 1426513
#% 1523820
#% 1523858
#% 1536507
#% 1592152
#% 1594630
#% 1693873
#% 1783374
#% 1879050
#% 1978791
#! Social influence analysis on microblog networks, such as Twitter, has been playing a crucial role in online advertising and brand management. While most previous influence analysis schemes rely only on the links between users to find key influencers, they omit the important text content created by the users. As a result, there is no way to differentiate the social influence in different aspects of life (topics). Although a few prior works do support topic-specific influence analysis, they either separate the analysis of content from the analysis of network structure, or assume that content is the only cause of links, which is clearly an inappropriate assumption for microblog networks. To address the limitations of the previous approaches, we propose a novel Followship-LDA (FLDA) model, which integrates both content topic discovery and social influence analysis in the same generative process. This model properly captures the content-related and content-independent reasons why a user follows another in a microblog network. We demonstrate that FLDA produces results with significantly better precision than existing approaches. Furthermore, we propose a distributed Gibbs sampling algorithm for FLDA, and demonstrate that it provides excellent scalability on large clusters. Finally, we incorporate the FLDA model in a general search framework for topic-specific influencers. A user freely expresses his/her interest by typing a few keywords, the search framework will return a ranked list of key influencers that satisfy the user's interest.

#index 2045880
#* Entity linking at the tail: sparse signals, unknown entities, and phrase models
#@ Yuzhe Jin;Emre Kıcıman;Kuansan Wang;Ricky Loynd
#t 2014
#c 2
#% 262096
#% 280819
#% 722904
#% 722929
#% 747890
#% 770857
#% 868096
#% 1019082
#% 1063570
#% 1077150
#% 1130858
#% 1130876
#% 1214667
#% 1250185
#% 1330553
#% 1482205
#% 1484272
#% 1484390
#% 1592023
#% 1598410
#% 1642000
#% 1711796
#% 1711864
#% 1815965
#% 1991885
#% 2030490
#! Web search is seeing a paradigm shift from keyword based search to an entity-centric organization of web data. To support web search with this deeper level of understanding, a web-scale entity linking system must have 3 key properties: First, its feature extraction must be robust to the diversity of web documents and their varied writing styles and content structures. Second, it must maintain high-precision linking for "tail" (unpopular) entities that is robust to the existence of confounding entities outside of the knowledge base and entity profiles with minimal information. Finally, the system must represent large-scale knowledge bases with a scalable and powerful feature representation. We have built and deployed a web-scale unsupervised entity linking system for a commercial search engine that addresses these requirements by combining new developments in sparse signal recovery to identify the most discriminative features from noisy, free-text web documents; explicit modeling of out-of-knowledge-base entities to improve precision at the tail; and the development of a new phrase-unigram language model to efficiently capture high-order dependencies in lexical features. Using a knowledge base of 100M unique people from a popular social networking site, we present experimental results in the challenging domain of people-linking at the tail, where most entities have limited web presence. Our experimental results show that this system substantially improves on the precision-recall tradeoff over baseline methods, achieving precision over 95% with recall over 60%.

#index 2045881
#* Going beyond Corr-LDA for detecting specific comments on news & blogs
#@ Mrinal Kanti Das;Trapit Bansal;Chiranjib Bhattacharyya
#t 2014
#c 2
#% 642990
#% 722904
#% 1055682
#% 1270702
#% 1642195
#% 1693879
#% 1711748
#% 1918361
#% 1919818
#! Understanding user generated comments in response to news and blog posts is an important area of research. After ignoring irrelevant comments, one finds that a large fraction, approximately 50%, of the comments are very specific and can be further related to certain parts of the article instead of the entire story. For example, in a recent product review of Google Nexus 7 in ArsTechnica (a popular blog), the reviewer talks about the prospect of "Retina equipped iPad mini" in a few sentences. It is interesting that although the article is on Nexus 7, but a significant number of comments are focused on this specific point regarding "iPad". We pose the problem of detecting such comments as specific comments location (SCL) problem. SCL is an important open problem with no prior work. SCL can be posed as a correspondence problem between comments and the parts of the relevant article, and one could potentially use Corr-LDA type models. Unfortunately, such models do not give satisfactory performance as they are restricted to using a single topic vector per article-comments pair. In this paper we propose to go beyond the single topic vector assumption and propose a novel correspondence topic model, namely SCTM, which admits multiple topic vectors (MTV) per article-comments pair. The resulting inference problem is quite complicated because of MTV and has no off-the-shelf solution. One of the major contributions of this paper is to show that using stick-breaking process as a prior over MTV, one can derive a collapsed Gibbs sampling procedure, which empirically works well for SCL. SCTM is rigorously evaluated on three datasets, crawled from Yahoo! News (138,000 comments) and two blogs, ArsTechnica (AT) Science (90,000 comments) and AT-Gadget (160,000 comments). We observe that SCTM performs better than Corr-LDA, not only in terms of metrics like perplexity and topic coherence but also discovers more unique topics. We see that this immediately leads to an order of magnitude improvement in F1 score over Corr-LDA for SCL.

#index 2045882
#* The last click: why users give up information network navigation
#@ Aju Thalappillil Scaria;Rose Marie Philip;Robert West;Jure Leskovec
#t 2014
#c 2
#% 268079
#% 300078
#% 325198
#% 717133
#% 751830
#% 987224
#% 1083643
#% 1130852
#% 1227582
#% 1305546
#% 1450902
#% 1560219
#% 1598368
#% 1746860
#% 1800593
#% 1890915
#% 1919710
#! An important part of finding information online involves clicking from page to page until an information need is fully satisfied. This is a complex task that can easily be frustrating and force users to give up prematurely. An empirical analysis of what makes users abandon click-based navigation tasks is hard, since most passively collected browsing logs do not specify the exact target page that a user was trying to reach. We propose to overcome this problem by using data collected via Wikispeedia, a Wikipedia-based human-computation game, in which users are asked to navigate from a start page to an explicitly given target page (both Wikipedia articles) by only tracing hyperlinks between Wikipedia articles. Our contributions are two-fold. First, by analyzing the differences between successful and abandoned navigation paths, we aim to understand what types of behavior are indicative of users giving up their navigation task. We also investigate how users make use of back clicks during their navigation. We find that users prefer backtracking to high-degree nodes that serve as landmarks and hubs for exploring the network of pages. Second, based on our analysis, we build statistical models for predicting whether a user will finish or abandon a navigation task, and if the next action will be a back click. Being able to predict these events is important as it can potentially help us design more human-friendly browsing interfaces and retain users who would otherwise have given up navigating a website.

#index 2045883
#* On building entity recommender systems using user click log and freebase knowledge
#@ Xiao Yu;Hao Ma;Bo-June (Paul) Hsu;Jiawei Han
#t 2014
#c 2
#% 304425
#% 330687
#% 452563
#% 577273
#% 724539
#% 734591
#% 840924
#% 956551
#% 1063570
#% 1083671
#% 1083721
#% 1176909
#% 1214661
#% 1214666
#% 1227622
#% 1260273
#% 1292601
#% 1330550
#% 1457044
#% 1536533
#% 1598360
#% 1606073
#% 1650569
#% 1746857
#% 1872391
#% 2007483
#! Due to their commercial value, search engines and recommender systems have become two popular research topics in both industry and academia over the past decade. Although these two fields have been actively and extensively studied separately, researchers are beginning to realize the importance of the scenarios at their intersection: providing an integrated search and information discovery user experience. In this paper, we study a novel application, i.e., personalized entity recommendation for search engine users, by utilizing user click log and the knowledge extracted from Freebase. To better bridge the gap between search engines and recommender systems, we first discuss important heuristics and features of the datasets. We then propose a generic, robust, and time-aware personalized recommendation framework to utilize these heuristics and features at different granularity levels. Using movie recommendation as a case study, with user click log dataset collected from a widely used commercial search engine, we demonstrate the effectiveness of our proposed framework over other popular and state-of-the-art recommendation techniques.

#index 2045884
#* Adapting deep RankNet for personalized search
#@ Yang Song;Hongning Wang;Xiaodong He
#t 2014
#c 2
#% 387427
#% 818207
#% 818221
#% 818259
#% 840846
#% 956552
#% 1073905
#% 1074071
#% 1227622
#% 1232034
#% 1292485
#% 1338581
#% 1442578
#% 1482279
#% 1598347
#% 1609856
#% 1693905
#% 1988828
#% 1992511
#% 2006167
#! RankNet is one of the widely adopted ranking models for web search tasks. However, adapting a generic RankNet for personalized search is little studied. In this paper, we first continue-trained a variety of RankNets with different number of hidden layers and network structures over a previously trained global RankNet model, and observed that a deep neural network with five hidden layers gives the best performance. To further improve the performance of adaptation, we propose a set of novel methods categorized into two groups. In the first group, three methods are proposed to properly assess the usefulness of each adaptation instance and only leverage the most informative instances to adapt a user-specific RankNet model. These assessments are based on KL-divergence, click entropy or a heuristic to ignore top clicks in adaptation queries. In the second group, two methods are proposed to regularize the training of the neural network in RankNet: one of these methods regularize the error back-propagation via a truncated gradient approach, while the other method limits the depth of the back propagation when adapting the neural network. We empirically evaluate our approaches using a large-scale real-world data set. Experimental results exhibit that our methods all give significant improvements over a strong baseline ranking system, and the truncated gradient approach gives the best performance, significantly better than all others.

#index 2045885
#* An efficient framework for online advertising effectiveness measurement and comparison
#@ Pengyuan Wang;Yechao Liu;Marsha Meytlis;Han-Yun Tsao;Jian Yang;Pei Huang
#t 2014
#c 2
#% 156186
#% 448194
#% 1451139
#% 1560370
#% 1881295
#% 1881297
#% 1891168
#! In online advertising market it is crucial to provide advertisers with a reliable measurement of advertising effectiveness to make better marketing campaign planning. The basic idea for ad effectiveness measurement is to compare the performance (e.g., success rate) among users who were and who were not exposed to a certain treatment of ads. When a randomized experiment is not available, a naive comparison can be biased because exposed and unexposed populations typically have different features. One solid methodology for a fair comparison is to apply inverse propensity weighting with doubly robust estimation to the observational data. However the existing methods were not designed for the online advertising campaign, which usually suffers from huge volume of users, high dimensionality, high sparsity and imbalance. We propose an efficient framework to address these challenges in a real campaign circumstance. We utilize gradient boosting stumps for feature selection and gradient boosting trees for model fitting, and propose a subsampling-and-backscaling procedure that enables analysis on extremely sparse conversion data. The choice of features, models and feature selection scheme are validated with irrelevant conversion test. We further propose a parallel computing strategy, combined with the subsampling-and-backscaling procedure to reach computational efficiency. Our framework is applied to an online campaign involving millions of unique users, which shows substantially better model fitting and efficiency. Our framework can be further generalized to comparison of multiple treatments and more general treatment regimes, as sketched in the paper. Our framework is not limited to online advertising, but also applicable to other circumstances (e.g., social science) where a 'fair' comparison is needed with observational data.

#index 2045886
#* Taxonomy discovery for personalized recommendation
#@ Yuchen Zhang;Amr Ahmed;Vanja Josifovski;Alexander Smola
#t 2014
#c 2
#% 783531
#% 1083671
#% 1145214
#% 1214623
#% 1260273
#% 1310058
#% 1355024
#% 1357698
#% 1417104
#% 1605928
#% 1606391
#% 1869831
#! Personalized recommender systems based on latent factor models are widely used to increase sales in e-commerce. Such systems use the past behavior of users to recommend new items that are likely to be of interest to them. However, latent factor model suffer from sparse user-item interaction in online shopping data: for a large portion of items that do not have sufficient purchase records, their latent factors cannot be estimated accurately. In this paper, we propose a novel approach that automatically discovers the taxonomies from online shopping data and jointly learns a taxonomy-based recommendation system. Out model is non-parametric and can learn the taxonomy structure automatically from the data. Since the taxonomy allows purchase data to be shared between items, it effectively improves the accuracy of recommending tail items by sharing strength with the more frequent items. Experiments on a large-scale online shopping dataset confirm that our proposed model improves significantly over state-of-the-art latent factor models. Moreover, our model generates high-quality and human readable taxonomies. Finally, using the algorithm-generated taxonomy, our model even outperforms latent factor models based on the human-induced taxonomy, thus alleviating the need for costly manual taxonomy generation.

#index 2045887
#* Exploiting contextual factors for click modeling in sponsored search
#@ Dawei Yin;Shike Mei;Bin Cao;Jian-Tao Sun;Brian D. Davison
#t 2014
#c 2
#% 818221
#% 956546
#% 989572
#% 1035578
#% 1074092
#% 1166517
#% 1190055
#% 1190056
#% 1214675
#% 1260273
#% 1291600
#% 1292472
#% 1355048
#% 1355051
#% 1399962
#% 1450842
#% 1451020
#% 1451161
#% 1455698
#% 1470525
#% 1605928
#% 1606083
#% 1693894
#% 1693911
#% 1872324
#% 1919863
#% 1948179
#! Sponsored search is the primary business for today's commercial search engines. Accurate prediction of the Click-Through Rate (CTR) for ads is key to displaying relevant ads to users. In this paper, we systematically study the two kinds of contextual factors influencing the CTR: 1) In micro factors, we focus on the factors for mainline ads, including ad depth, query diversity, ad interaction. 2) In macro factors, we try to understand the correlations of clicks between organic search and sponsored search. Based on this data analysis, we propose novel click models which harvest these new explored factors. To the best of our knowledge, this is the first paper to examine and model the effects of the above contextual factors in sponsored search. Extensive experiments on large-scale real-world datasets show that by incorporating these contextual factors, our novel click models can outperform state-of-the-art methods.

#index 2045888
#* Nonparametric bayesian upstream supervised multi-modal topic models
#@ Renjie Liao;Jun Zhu;Zengchang Qin
#t 2014
#c 2
#% 303620
#% 457912
#% 642990
#% 722904
#% 760805
#% 812535
#% 891549
#% 975105
#% 1013668
#% 1126940
#% 1148273
#% 1181094
#% 1484424
#% 1502531
#% 1536524
#% 1677726
#% 1872343
#% 1884410
#% 1931621
#% 1948169
#% 2004118
#! Learning with multi-modal data is at the core of many multimedia applications, such as cross-modal retrieval and image annotation. In this paper, we present a nonparametric Bayesian approach to learning upstream supervised topic models for analyzing multi-modal data. Our model develops a compound nonparametric Bayesian multi-modal prior to describe the correlation structure of data both within each individual modality and between different modalities. It extends the hierarchical Dirichlet process (HDP) through incorporating upstream supervised response variables and values of latent functions under Gaussian process (GP). Upstream responses shared by data from multiple modalities are beneficial for discriminatively training and GP allows flexible structure learning of correlations. Hence, our model inherits the automatic determination of the number of topics from HDP, structure learning from GP and enhanced predictive capacity from upstream supervision. We also provide efficient variational inference and prediction algorithms. Empirical studies demonstrate superior performances on several benchmark datasets compared with previous competitors.

#index 2045889
#* Improving search relevance for short queries in community question answering
#@ Haocheng Wu;Wei Wu;Ming Zhou;Enhong Chen;Lei Duan;Heung-Yeung Shum
#t 2014
#c 2
#% 169781
#% 262096
#% 309095
#% 309126
#% 340899
#% 818262
#% 835732
#% 838398
#% 1074110
#% 1227600
#% 1292492
#% 1355019
#% 1479039
#% 1591994
#% 1592071
#% 1598375
#% 1598394
#% 1879023
#% 1988889
#% 1992423
#% 1992506
#% 2008821
#! Relevant question retrieval and ranking is a typical task in community question answering (CQA). Existing methods mainly focus on long and syntactically structured queries. However, when an input query is short, the task becomes challenging, due to a lack information regarding user intent. In this paper, we mine different types of user intent from various sources for short queries. With these intent signals, we propose a new intent-based language model. The model takes advantage of both state-of-the-art relevance models and the extra intent information mined from multiple sources. We further employ a state-of-the-art learning-to-rank approach to estimate parameters in the model from training data. Experiments show that by leveraging user intent prediction, our model significantly outperforms the state-of-the-art relevance models in question search.

#index 2045890
#* Predicting response in mobile advertising with hierarchical importance-aware factorization machine
#@ Richard J. Oentaryo;Ee-Peng Lim;Jia-Wei Low;David Lo;Michael Finegold
#t 2014
#c 2
#% 207195
#% 868445
#% 956546
#% 1190057
#% 1211806
#% 1214666
#% 1287221
#% 1451160
#% 1451238
#% 1535439
#% 1605928
#% 1693894
#% 1730808
#% 1992375
#! Mobile advertising has recently seen dramatic growth, fueled by the global proliferation of mobile phones and devices. The task of predicting ad response is thus crucial for maximizing business revenue. However, ad response data change dynamically over time, and are subject to cold-start situations in which limited history hinders reliable prediction. There is also a need for a robust regression estimation for high prediction accuracy, and good ranking to distinguish the impacts of different ads. To this end, we develop a Hierarchical Importance-aware Factorization Machine (HIFM), which provides an effective generic latent factor framework that incorporates importance weights and hierarchical learning. Comprehensive empirical studies on a real-world mobile advertising dataset show that HIFM outperforms the contemporary temporal latent factor models. The results also demonstrate the efficacy of the HIFM's importance-aware and hierarchical learning in improving the overall prediction and prediction in cold-start scenarios, respectively.

#index 2045891
#* A few good predictions: selective node labeling in a social network
#@ Gaurish Chaudhari;Vashist Avadhanula;Sunita Sarawagi
#t 2014
#c 2
#% 248810
#% 961268
#% 961278
#% 983810
#% 1083628
#% 1176950
#% 1190108
#% 1264778
#% 1289267
#% 1355041
#% 1386003
#% 1417383
#% 1481659
#% 1482254
#% 1617313
#% 1693933
#% 1746875
#% 1893833
#% 1948189
#! Many social network applications face the following problem: given a network G=(V,E) with labels on a small subset O \subset V of nodes and an optional set of features on nodes and edges, predict the labels of the remaining nodes. Much research has gone into designing learning models and inference algorithms for accurate predictions in this setting. However, a core hurdle to any prediction effort is that for many nodes there is insufficient evidence for inferring a label. We propose that instead of focusing on the impossible task of providing high accuracy over all nodes, we should focus on selectively making the few node predictions which will be correct with high probability. Any selective prediction strategy will require that the scores attached to node predictions be well-calibrated. Our evaluations show that existing prediction algorithms are poorly calibrated. We propose a new method of training a graphical model using a conditional likelihood objective that provides better calibration than the existing joint likelihood objective. We augment it with a decoupled confidence model created using a novel unbiased training process. Empirical evaluation on two large social networks show that we are able to select a large number of predictions with accuracy as high as 95%, even when the best overall accuracy is only 40%.

#index 2045892
#* Sampling dilemma: towards effective data sampling for click prediction in sponsored search
#@ Jun Feng;Jiang Bian;Taifeng Wang;Wei Chen;Xiaoyan Zhu;Tie-Yan Liu
#t 2014
#c 2
#% 211044
#% 956546
#% 987209
#% 987361
#% 990182
#% 1074101
#% 1214728
#% 1246499
#% 1355051
#% 1450842
#% 1693911
#! Precise prediction of the probability that users click on ads plays a key role in sponsored search. State-of-the-art sponsored search systems typically employ a machine learning approach to conduct click prediction. While paying much attention to extracting useful features and building effective models, previous studies have overshadowed seemingly less obvious but essentially important challenges in terms of data sampling. To fulfill the learning objective of click prediction, it is not only necessary to ensure that the sampled training data implies the similar input distribution compared with the real world one, but also to guarantee that the sampled training data yield the consistent conditional output distribution, i.e. click-through rate (CTR), with the real world data. However, due to the sparseness of clicks in sponsored search, it is a bit contradictory to address these two challenges simultaneously. In this paper, we first take a theoretical analysis to reveal this sampling dilemma, followed by a thorough data analysis which demonstrates that the straightforward random sampling method may not be effective to balance these two kinds of consistency in sampling dilemma simultaneously. To address this problem, we propose a new sampling algorithm which can succeed in retaining the consistency between the sampled data and real world in terms of both input distribution and conditional output distribution. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that this new sampling algorithm can effectively address the sampling dilemma. Further experiments illustrate that, by using the training data obtained by our new sampling algorithm, we can learn the model with much higher accuracy in click prediction.

#index 2045893
#* Detecting cohesive and 2-mode communities indirected and undirected networks
#@ Jaewon Yang;Julian McAuley;Jure Leskovec
#t 2014
#c 2
#% 258598
#% 281214
#% 823342
#% 833003
#% 868089
#% 869485
#% 995168
#% 1002007
#% 1013696
#% 1117695
#% 1198232
#% 1214695
#% 1214722
#% 1272187
#% 1399992
#% 1606047
#% 1756065
#% 1872304
#% 1872378
#% 1948183
#% 1978762
#% 1978787
#% 2002860
#! Networks are a general language for representing relational information among objects. An effective way to model, reason about, and summarize networks, is to discover sets of nodes with common connectivity patterns. Such sets are commonly referred to as network communities. Research on network community detection has predominantly focused on identifying communities of densely connected nodes in undirected networks. In this paper we develop a novel overlapping community detection method that scales to networks of millions of nodes and edges and advances research along two dimensions: the connectivity structure of communities, and the use of edge directedness for community detection. First, we extend traditional definitions of network communities by building on the observation that nodes can be densely interlinked in two different ways: In cohesive communities nodes link to each other, while in 2-mode communities nodes link in a bipartite fashion, where links predominate between the two partitions rather than inside them. Our method successfully detects both 2-mode as well as cohesive communities, that may also overlap or be hierarchically nested. Second, while most existing community detection methods treat directed edges as though they were undirected, our method accounts for edge directions and is able to identify novel and meaningful community structures in both directed and undirected networks, using data from social, biological, and ecological domains.

#index 2045894
#* Modelling growth of urban crowd-sourced information
#@ Giovanni Quattrone;Afra Mashhadi;Daniele Quercia;Chris Smith-Clarke;Licia Capra
#t 2014
#c 2
#% 300078
#% 881460
#% 1001090
#% 1082200
#% 1083624
#% 1214671
#% 1355280
#% 1384309
#% 1560424
#% 1620919
#% 1693934
#% 1746822
#% 1872232
#% 1894912
#% 1954396
#% 1971033
#! Urban crowd-sourcing has become a popular paradigm to harvest spatial information about our evolving cities directly from citizens. OpenStreetMap is a successful example of such paradigm, with an accuracy of its user-generated content comparable to that of curated databases (e.g., Ordnance Survey). Coverage is however low and most importantly non-uniformly distributed across the city. Being able to model the spontaneous growth of digital information in these domains is required, so to be able to plan interventions aimed at gathering content about areas that would otherwise be neglected. Inspired by models of physical urban growth developed by urban planners, we build a model of digital growth of crowd-sourced spatial information that is both easy to interpret and dynamic, so to be able to determine what factors impact growth and how these change over time. We build and test the model against five years of OpenStreetMap data for the city of London, UK. We then run the model against two other cities, chosen for their different physical and digital growth's characteristics, so to stress-test the model. We conclude with a discussion of the implications of this work on both developers and users of urban crowd-sourcing applications.

#index 2045895
#* WebChild: harvesting and organizing commonsense knowledge from the web
#@ Niket Tandon;Gerard de Melo;Fabian Suchanek;Gerhard Weikum
#t 2014
#c 2
#% 198055
#% 286069
#% 532186
#% 783633
#% 860015
#% 956564
#% 1063570
#% 1117028
#% 1267783
#% 1300591
#% 1409954
#% 1481650
#% 1484289
#% 1484487
#% 1693886
#% 1711774
#% 1711865
#% 1870547
#% 1992420
#% 2006061
#! This paper presents a method for automatically constructing a large commonsense knowledge base, called WebChild, from Web contents. WebChild contains triples that connect nouns with adjectives via fine-grained relations like hasShape, hasTaste, evokesEmotion, etc. The arguments of these assertions, nouns and adjectives, are disambiguated by mapping them onto their proper WordNet senses. Our method is based on semi-supervised Label Propagation over graphs of noisy candidate assertions. We automatically derive seeds from WordNet and by pattern matching from Web text collections. The Label Propagation algorithm provides us with domain sets and range sets for 19 different relations, and with confidence-ranked assertions between WordNet senses. Large-scale experiments demonstrate the high accuracy (more than 80 percent) and coverage (more than four million fine grained disambiguated assertions) of WebChild.

#index 2045896
#* A self-adapting latency/power tradeoff model for replicated search engines
#@ Ana Freire;Craig Macdonald;Nicola Tonellotto;Iadh Ounis;Fidel Cacheda
#t 2014
#c 2
#% 352664
#% 387427
#% 430941
#% 578337
#% 743083
#% 985830
#% 1046346
#% 1239031
#% 1290542
#% 1417245
#% 1450846
#% 1456972
#% 1480887
#% 1598432
#% 1746858
#% 1879054
#% 1879055
#% 1893301
#% 1967771
#% 1988942
#! For many search settings, distributed/replicated search engines deploy a large number of machines to ensure efficient retrieval. This paper investigates how the power consumption of a replicated search engine can be automatically reduced when the system has low contention, without compromising its efficiency. We propose a novel self-adapting model to analyse the trade-off between latency and power consumption for distributed search engines. When query volumes are high and there is contention for the resources, the model automatically increases the necessary number of active machines in the system to maintain acceptable query response times. On the other hand, when the load of the system is low and the queries can be served easily, the model is able to reduce the number of active machines, leading to power savings. The model bases its decisions on examining the current and historical query loads of the search engine. Our proposal is formulated as a general dynamic decision problem, which can be quickly solved by dynamic programming in response to changing query loads. Thorough experiments are conducted to validate the usefulness of the proposed adaptive model using historical Web search traffic submitted to a commercial search engine. Our results show that our proposed self-adapting model can achieve an energy saving of 33% while only degrading mean query completion time by 10 ms compared to a baseline that provisions replicas based on a previous day's traffic.

#index 2045897
#* Prediction in a microblog hybrid network using bonacich potential
#@ Shanchan Wu;Louiqa Raschid
#t 2014
#c 2
#% 169777
#% 577224
#% 729923
#% 730089
#% 823388
#% 881526
#% 1077150
#% 1117026
#% 1211760
#% 1214702
#% 1260273
#% 1449326
#% 1451243
#% 1482397
#% 1536509
#% 1661308
#% 1689528
#% 1948180
#! Microblogs such as Twitter support a rich variety of user interactions using hashtags, urls, retweets and mentions. Microblogs are an exemplar of a hybrid network; there is an explicit network of followers, as well as an implicit network of users who retweet other users, and users who mention other users. These networks are important proxies for influence. In this paper, we develop a comprehensive behavioral model of an individual user and her interactions in the hybrid network. We choose a focal user and predict those users who will be influenced by her, and will retweet and/or mention the focal user, in the near future. We define a potential function, based on a hybrid network, which reflects the likelihood of a candidate user being influenced by, and having a specific type of link to, a focal user, in the future. We show that the potential function based prediction model converges to the Bonacich centrality metric. We develop a fast unsupervised solution which approximates the future hybrid network and the future Bonacich potential. We perform an extensive evaluation over a microblog network and a stream of tweets from Twitter. Our solution outperforms several baseline methods including ones based on singular value decomposition (SVD) and a supervised Ranking SVM.

#index 2045898
#* Improving pairwise learning for item recommendation from implicit feedback
#@ Steffen Rendle;Christoph Freudenthaler
#t 2014
#c 2
#% 840924
#% 1083671
#% 1176909
#% 1176959
#% 1190066
#% 1214666
#% 1355024
#% 1417104
#% 1476453
#% 1650569
#% 1730808
#% 1826503
#% 1869831
#% 1879057
#% 1893826
#% 1948162
#% 1948180
#% 2010321
#! Pairwise algorithms are popular for learning recommender systems from implicit feedback. For each user, or more generally context, they try to discriminate between a small set of selected items and the large set of remaining (irrelevant) items. Learning is typically based on stochastic gradient descent (SGD) with uniformly drawn pairs. In this work, we show that convergence of such SGD learning algorithms slows down considerably if the item popularity has a tailed distribution. We propose a non-uniform item sampler to overcome this problem. The proposed sampler is context-dependent and oversamples informative pairs to speed up convergence. An efficient implementation with constant amortized runtime costs is developed. Furthermore, it is shown how the proposed learning algorithm can be applied to a large class of recommender models. The properties of the new learning algorithm are studied empirically on two real-world recommender system problems. The experiments indicate that the proposed adaptive sampler improves the state-of-the art learning algorithm largely in convergence without negative effects on prediction quality or iteration runtime.

#index 2045899
#* Improving the efficiency of multi-site web search engines
#@ Guillem Francès;Xiao Bai;B. Barla Cambazoglu;Ricardo Baeza-Yates
#t 2014
#c 2
#% 209021
#% 410276
#% 411762
#% 465754
#% 860861
#% 987215
#% 1132154
#% 1190098
#% 1227597
#% 1292508
#% 1301004
#% 1399951
#% 1450839
#% 1450840
#% 1536562
#% 1558842
#% 1598431
#% 1641920
#% 1834787
#% 1879055
#% 1879056
#% 1921840
#! A multi-site web search engine is composed of a number of search sites geographically distributed around the world. Each search site is typically responsible for crawling and indexing the web pages that are in its geographical neighborhood. A query is selectively processed on a subset of search sites that are predicted to return the best-matching results. The scalability and efficiency of multi-site web search engines have attracted a lot of research attention in recent years. In particular, research has focused on replicating important web pages across sites, forwarding queries to relevant sites, and caching results of previous queries. Yet, these problems have only been studied in isolation, but no prior work has properly investigated the interplay between them. In this paper, we take this challenge up and conduct what we believe is the first comprehensive analysis of a full stack of techniques for efficient multi-site web search. Specifically, we propose a document replication technique that improves the query locality of the state-of-the-art approaches with various replication budget distribution strategies. We devise a machine learning approach to decide the query forwarding patterns, achieving a significantly lower false positive ratio than a state-of-the-art thresholding approach with little negative impact on search result quality. We propose three result caching strategies that reduce the number of forwarded queries and analyze the trade-off they introduce in terms of storage and network overheads. Finally, we show that the combination of the best-of-the-class techniques yields very promising search efficiency, rendering multi-site, geographically distributed web search engines an attractive alternative to centralized web search engines.

#index 2045900
#* Knowledge-based graph document modeling
#@ Michael Schuhmacher;Simone Paolo Ponzetto
#t 2014
#c 2
#% 1083703
#% 1196948
#% 1264797
#% 1272185
#% 1275012
#% 1275182
#% 1288161
#% 1321298
#% 1351373
#% 1414358
#% 1473935
#% 1489451
#% 1558472
#% 1585243
#% 1592043
#% 1641869
#% 1642243
#% 1692161
#% 1693884
#% 1711796
#% 1746843
#% 1906035
#% 1911157
#% 1911158
#% 1913604
#% 1918090
#% 1918389
#% 1919042
#% 1925700
#% 1925702
#% 1932476
#% 1948170
#% 1992411
#! We propose a graph-based semantic model for representing document content. Our method relies on the use of a semantic network, namely the DBpedia knowledge base, for acquiring fine-grained information about entities and their semantic relations, thus resulting in a knowledge-rich document model. We demonstrate the benefits of these semantic representations in two tasks: entity ranking and computing document semantic similarity. To this end, we couple DBpedia's structure with an information-theoretic measure of concept association, based on its explicit semantic relations, and compute semantic similarity using a Graph Edit Distance based measure, which finds the optimal matching between the documents' entities using the Hungarian method. Experimental results show that our general model outperforms baselines built on top of traditional methods, and achieves a performance close to that of highly specialized methods that have been tuned to these specific tasks.

#index 2045901
#* Inferring the impacts of social media on crowdfunding
#@ Chun-Ta Lu;Sihong Xie;Xiangnan Kong;Philip S. Yu
#t 2014
#c 2
#% 757953
#% 1301020
#% 1399995
#% 1411585
#% 1535333
#% 1693913
#% 1747045
#% 1872229
#% 1872232
#% 1948160
#% 1948185
#% 1976993
#% 1991797
#! Crowdfunding -- in which people can raise funds through collaborative contributions of general public (i.e., crowd) -- has emerged as a billion dollars business for supporting more than one million ventures. However, very few research works have examined the process of crowdfunding. In particular, none has studied how social networks help crowdfunding projects to succeed. To gain insights into the effects of social networks in crowdfunding, we analyze the hidden connections between the fundraising results of projects on crowdfunding websites and the corresponding promotion campaigns in social media. Our analysis considers the dynamics of crowdfunding from two aspects: how fundraising activities and promotional activities on social media simultaneously evolve over time, and how the promotion campaigns influence the final outcomes. From our investigation, we identify a number of important principles that provide a useful guide for devising effective campaigns. For example, we observe temporal distribution of customer interest, strong correlations between a crowdfunding project's early promotional activities and the final outcomes, and the importance of concurrent promotion from multiple sources. We then show that these discoveries can help predict several important quantities, including overall popularity and the success rate of the project. Finally, we show how to use these discoveries to help design crowdfunding sites.

#index 2045902
#* LASER: a scalable response prediction platform for online advertising
#@ Deepak Agarwal;Bo Long;Jonathan Traupman;Doris Xin;Liang Zhang
#t 2014
#c 2
#% 63342
#% 251365
#% 325838
#% 425053
#% 956546
#% 1039685
#% 1074360
#% 1211829
#% 1355052
#% 1399999
#% 1451160
#% 1475077
#% 1543893
#% 1693910
#% 1745124
#% 1992304
#! We describe LASER, a scalable response prediction platform currently used as part of a social network advertising system. LASER enables the familiar logistic regression model to be applied to very large scale response prediction problems, including ones beyond advertising. Though the underlying model is well understood, we apply a whole-system approach to address model accuracy, scalability, explore-exploit, and real-time inference. To facilitate training with both large numbers of training examples and high dimensional features on commodity clustered hardware, we employ the Alternating Direction Method of Multipliers (ADMM). Because online advertising applications are much less static than classical presentations of response prediction, LASER employs a number of techniques that allows it to adapt in real time. LASER models can be divided into components with different re-training frequencies, allowing us to learn from changes in ad campaign performance frequently without incurring the cost of retraining larger, more stable sections of the model. Thompson sampling during online inference further helps by efficiently balancing exploration of new ads with exploitation of long running ones. To enable predictions made with the most recent feature data, we employ a range of techniques, including extensive caching and lazy evaluation, to permit real time, low latency scoring. LASER models are defined using a configuration language that ties together the training, validation, and inference pieces and permits even non-programming analysts to experiment with different model structures without modifications to code or interruptions to running servers. Finally, we show via extensive offline experiments and online A/B tests that this system provides significant benefits to prediction accuracy, gains in revenue and CTR, and reductions in system latency.

#index 2045903
#* Understanding and promoting micro-finance activities in Kiva.org
#@ Jaegul Choo;Changhyun Lee;Daniel Lee;Hongyuan Zha;Haesun Park
#t 2014
#c 2
#% 124004
#% 314933
#% 387427
#% 643008
#% 813966
#% 902496
#% 989585
#% 1038899
#% 1055685
#% 1074012
#% 1077150
#% 1083696
#% 1214688
#% 1358747
#% 1450837
#% 1598363
#% 1693912
#! Non-profit Micro-finance organizations provide loaning opportunities to eradicate poverty by financially equipping impoverished, yet skilled entrepreneurs who are in desperate need of an institution that lends to those who have little. Kiva.org, a widely-used crowd-funded micro-financial service, provides researchers with an extensive amount of publicly available data containing a rich set of heterogeneous information regarding micro-financial transactions. Our objective in this paper is to identify the key factors that encourage people to make micro-financing donations, and ultimately, to keep them actively involved. In our contribution to further promote a healthy micro-finance ecosystem, we detail our personalized loan recommendation system which we formulate as a supervised learning problem where we try to predict how likely a given lender will fund a new loan. We construct the features for each data item by utilizing the available connectivity relationships in order to integrate all the available Kiva data sources. For those lenders with no such relationships, e.g., first-time lenders, we propose a novel method of feature construction by computing joint nonnegative matrix factorizations. Utilizing gradient boosting tree methods, a state-of-the-art prediction model, we are able to achieve up to 0.92 AUC (area under the curve) value, which shows the potential of our methods for practical deployment. Finally, we point out several interesting phenomena on lenders' social behaviors in micro-finance activities.

#index 2045904
#* Ranking in heterogeneous social media
#@ Min-Hsuan Tsai;Charu Aggarwal;Thomas Huang
#t 2014
#c 2
#% 213673
#% 268079
#% 282202
#% 318785
#% 348173
#% 577273
#% 577329
#% 881477
#% 899292
#% 956551
#% 960259
#% 975019
#% 1119135
#% 1190090
#% 1264995
#% 1372657
#% 1400049
#% 1560384
#% 1567510
#! The problem of image search has been studied extensively in recent years because of the large and increasing repositories of images on the web, social media, and other linked networks. Most of the available techniques for keyword-based image search on the web use the text in the surrounding or linked text in order to retrieve related images. Many image repositories on the web are built upon social media platforms such as Flickr. Such platforms provide a rich level of information in terms of the user linkage information to images, tags or other comments which are contributed by the users. It is reasonable to assume that the content of the images, users and other social cues such as tags and comments are often related to one another. Therefore, such cues can be useful for improving the effectiveness of search and ranking algorithms. In this paper, we propose SocialRank, which is a technique for using social hints in order to improve the image search and ranking process. Furthermore, we propose a holistic framework to combine social tags, social network text, linkage between actors and images, as well as the actual image features in order to create a ranking technique for image search. We design a PageRank-like method which can combine these different methods in order to provide an effective method for image search and ranking in social networks.

#index 2045905
#* Social collaborative retrieval
#@ Ko-Jen Hsiao;Alex Kulesza;Alfred Hero
#t 2014
#c 2
#% 220708
#% 330687
#% 452563
#% 722904
#% 790459
#% 1001279
#% 1127455
#% 1130901
#% 1227601
#% 1227602
#% 1275183
#% 1292590
#% 1358747
#% 1457039
#% 1476453
#% 1536533
#% 1625411
#% 1650569
#! Socially-based recommendation systems have recently attracted significant interest, and a number of studies have shown that social information can dramatically improve a system's predictions of user interests. Meanwhile, there are now many potential applications that involve aspects of both recommendation and information retrieval, and the task of collaborative retrieval---a combination of these two traditional problems---has recently been introduced. Successful collaborative retrieval requires overcoming severe data sparsity, making additional sources of information, such as social graphs, particularly valuable. In this paper we propose a new model for collaborative retrieval, and show that our algorithm outperforms current state-of-the-art approaches by incorporating information from social networks. We also provide empirical analyses of the ways in which cultural interests propagate along a social graph using a real-world music dataset.

#index 2045906
#* Relative confidence sampling for efficient on-line ranker evaluation
#@ Masrour Zoghi;Shimon A. Whiteson;Maarten de Rijke;Remi Munos
#t 2014
#c 2
#% 425053
#% 577224
#% 857180
#% 943049
#% 1035578
#% 1130811
#% 1166517
#% 1173703
#% 1173704
#% 1211840
#% 1227581
#% 1292763
#% 1415711
#% 1450912
#% 1606376
#% 1641943
#% 1667281
#% 1792878
#% 1932333
#% 1948148
#% 1949159
#% 2006199
#% 2030783
#! A key challenge in information retrieval is that of on-line ranker evaluation: determining which one of a finite set of rankers performs the best in expectation on the basis of user clicks on presented document lists. When the presented lists are constructed using interleaved comparison methods, which interleave lists proposed by two different candidate rankers, then the problem of minimizing the total regret accumulated while evaluating the rankers can be formalized as a K-armed dueling bandits problem. In this paper, we propose a new method called relative confidence sampling (RCS) that aims to reduce cumulative regret by being less conservative than existing methods in eliminating rankers from contention. In addition, we present an empirical comparison between RCS and two state-of-the-art methods, relative upper confidence bound and SAVAGE. The results demonstrate that RCS can substantially outperform these alternatives on several large learning to rank datasets.

#index 2045907
#* Estimating ad group performance in sponsored search
#@ Dawei Yin;Bin Cao;Jian-Tao Sun;Brian D. Davison
#t 2014
#c 2
#% 329569
#% 722904
#% 818221
#% 956546
#% 1035578
#% 1073906
#% 1074092
#% 1166517
#% 1190055
#% 1190056
#% 1211822
#% 1211848
#% 1214675
#% 1260273
#% 1268491
#% 1291600
#% 1355031
#% 1355048
#% 1355051
#% 1355052
#% 1450842
#% 1451161
#% 1457039
#% 1470525
#% 1605928
#% 1605963
#% 1606083
#% 1693894
#% 1693908
#% 1693911
#% 1730808
#% 1872324
#% 1948179
#! In modern commercial search engines, the pay-per-click (PPC) advertising model is widely used in sponsored search. The search engines try to deliver ads which can produce greater click yields (the total number of clicks for the list of ads per impression). Therefore, predicting user clicks plays a critical role in sponsored search. The current ad-delivery strategy is a two-step approach which first predicts individual ad CTR for the given query and then selects the ads with higher predicted CTR. However, this strategy is naturally suboptimal and correlation between ads is often ignored under this strategy. The learning problem is focused on predicting individual performance rather than group performance which is the more important measurement. In this paper, we study click yield measurement in sponsored search and focus on the problem---predicting group performance (click yields) in sponsored search. To tackle all challenges in this problem---depth effects, interactive influence, cold start and sparseness of ad textual information---we first investigate several effects and propose a novel framework that could directly predict group performance for lists of ads. Our extensive experiments on a large-scale real-world dataset from a commercial search engine show that we achieve significant improvement by solving the sponsored search problem from the new perspective. Our methods noticeably outperform existing state-of-the-art approaches.

#index 2045908
#* Partner tiering in display advertising
#@ Anand Bhalgat;Nitish Korula;Hannadiy Leontyev;Max Lin;Vahab Mirrokni
#t 2014
#c 2
#% 836518
#% 1222625
#% 1336448
#% 1336462
#% 1404809
#% 1426653
#% 1496079
#% 1536535
#% 1584773
#% 1872374
#% 1872376
#% 1881599
#! Display ads on the Internet are often sold by publishers to advertisers in bundles of thousands or millions of impressions over a particular time period. The ad delivery systems assign ads to pages on behalf of publishers to satisfy these contracts, and at the same time, try to maximize the overall quality of assignment. This is usually modeled in the literature as an online allocation problem, where contracts are represented by overall delivery constraints. However an important aspect of these contracts is missed by the classical formulation: a majority of these contracts are not between advertisers and publishers; a set of publishers is typically represented by a middle-man and advertisers buy inventory from the middle man. As publishers vary in quality and importance, advertisers prefer these publishers differently. Similarly, as the inventory of ads is limited, ad-delivery engine needs to prefer a high-quality publisher over a low quality publisher for supplying ads. We formulate this problem as a hierarchical online matching problem where each incoming impression has a level indicating its importance, and study its theoretical properties. We also design practical solutions to this problem and study their performance on real data sets.

#index 2045909
#* Personalized entity recommendation: a heterogeneous information network approach
#@ Xiao Yu;Xiang Ren;Yizhou Sun;Quanquan Gu;Bradley Sturt;Urvashi Khandelwal;Brandon Norick;Jiawei Han
#t 2014
#c 2
#% 190265
#% 330687
#% 577273
#% 578684
#% 643007
#% 770816
#% 840924
#% 881477
#% 956551
#% 1083671
#% 1130901
#% 1176909
#% 1214701
#% 1227602
#% 1260273
#% 1287226
#% 1327693
#% 1417104
#% 1457044
#% 1606073
#% 1625374
#% 1872391
#% 1991814
#% 2007483
#! Among different hybrid recommendation techniques, network-based entity recommendation methods, which utilize user or item relationship information, are beginning to attract increasing attention recently. Most of the previous studies in this category only consider a single relationship type, such as friendships in a social network. In many scenarios, the entity recommendation problem exists in a heterogeneous information network environment. Different types of relationships can be potentially used to improve the recommendation quality. In this paper, we study the entity recommendation problem in heterogeneous information networks. Specifically, we propose to combine heterogeneous relationship information for each user differently and aim to provide high-quality personalized recommendation results using user implicit feedback data and personalized recommendation models. In order to take full advantage of the relationship heterogeneity in information networks, we first introduce meta-path-based latent features to represent the connectivity between users and items along different types of paths. We then define recommendation models at both global and personalized levels and use Bayesian ranking optimization techniques to estimate the proposed models. Empirical studies show that our approaches outperform several widely employed or the state-of-the-art entity recommendation techniques.

#index 2045910
#* Scalable K-Means by ranked retrieval
#@ Andrei Broder;Lluis Garcia-Pueyo;Vanja Josifovski;Sergei Vassilvitskii;Srihari Venkatesan
#t 2014
#c 2
#% 198335
#% 213786
#% 280463
#% 387427
#% 443984
#% 466425
#% 580668
#% 594012
#% 656775
#% 730065
#% 765518
#% 771247
#% 867054
#% 871356
#% 879405
#% 956506
#% 987221
#% 991230
#% 1023422
#% 1400102
#% 1567038
#% 1611340
#% 1683906
#% 1701376
#% 1741029
#% 1813854
#! The k-means clustering algorithm has a long history and a proven practical performance, however it does not scale to clustering millions of data points into thousands of clusters in high dimensional spaces. The main computational bottleneck is the need to recompute the nearest centroid for every data point at every iteration, aprohibitive cost when the number of clusters is large. In this paper we show how to reduce the cost of the k-means algorithm by large factors by adapting ranked retrieval techniques. Using a combination of heuristics, on two real life data sets the wall clock time per iteration is reduced from 445 minutes to less than 4, and from 705 minutes to 1.4, while the clustering quality remains within 0.5% of the k-means quality. The key insight is to invert the process of point-to-centroid assignment by creating an inverted index over all the points and then using the current centroids as queries to this index to decide on cluster membership. In other words, rather than each iteration consisting of "points picking centroids", each iteration now consists of "centroids picking points". This is much more efficient, but comes at the cost of leaving some points unassigned to any centroid. We show experimentally that the number of such points is low and thus they can be separately assigned once the final centroids are decided. To speed up the computation we sparsify the centroids by pruning low weight features. Finally, to further reduce the running time and the number of unassigned points, we propose a variant of the WAND algorithm that uses the results of the intermediate results of nearest neighbor computations to improve performance.

#index 2045911
#* Sentiment analysis on evolving social streams: how self-report imbalances can help
#@ Pedro Calais Guerra;Wagner Meira, Jr.;Claire Cardie
#t 2014
#c 2
#% 204531
#% 465754
#% 815915
#% 840920
#% 868089
#% 891559
#% 1083692
#% 1127964
#% 1190942
#% 1214654
#% 1219797
#% 1289531
#% 1301020
#% 1400018
#% 1561563
#% 1596326
#% 1598381
#% 1605929
#% 1671152
#% 1688527
#% 1711776
#% 1948178
#% 1954374
#% 1992453
#! Real-time sentiment analysis is a challenging machine learning task, due to scarcity of labeled data and sudden changes in sentiment caused by real-world events that need to be instantly interpreted. In this paper we propose solutions to acquire labels and cope with concept drift in this setting, by using findings from social psychology on how humans prefer to disclose some types of emotions. In particular, we use findings that humans are more motivated to report positive feelings rather than negative feelings and also prefer to report extreme feelings rather than average feelings. We map each of these self-report imbalances on two machine learning sub-tasks. The preference on the disclosure of positive feelings can be explored to generate labeled data on polarizing topics, where a positive event for one group usually induces negative feelings from the opposing group, generating an imbalance on user activity that unveils the current dominant sentiment. Based on the knowledge that extreme experiences are more reported than average experiences, we propose a feature representation strategy that focus on terms which appear at spikes in the social stream. When comparing to a static text representation (TF-IDF), we found that our feature representation is more capable of detecting new informative features that capture the sudden changes on sentiment stream caused by real-world events. We show that our social psychology-inspired framework produces accuracies up to 84% while analyzing live reactions in the debate of two popular sports on Twitter - soccer and football - despite requiring no human effort in generating supervisory labels.

#index 2045912
#* User modeling in search logs via a nonparametric bayesian approach
#@ Hongning Wang;ChengXiang Zhai;Feng Liang;Anlei Dong;Yi Chang
#t 2014
#c 2
#% 232684
#% 306468
#% 330687
#% 387427
#% 413615
#% 577224
#% 754059
#% 818221
#% 879565
#% 956495
#% 1074071
#% 1074092
#% 1190055
#% 1227620
#% 1399946
#% 1417245
#% 1442577
#% 1536521
#% 1606083
#% 1641936
#% 1693905
#! Searchers' information needs are diverse and cover a broad range of topics; hence, it is important for search engines to accurately understand each individual user's search intents in order to provide optimal search results. Search log data, which records users' search behaviors when interacting with search engines, provides a valuable source of information about users' search intents. Therefore, properly characterizing the heterogeneity among the users' observed search behaviors is the key to accurately understanding their search intents and to further predicting their behaviors. In this work, we study the problem of user modeling in the search log data and propose a generative model, dpRank, within a non-parametric Bayesian framework. By postulating generative assumptions about a user's search behaviors, dpRank identifies each individual user's latent search interests and his/her distinct result preferences in a joint manner. Experimental results on a large-scale news search log data set validate the effectiveness of the proposed approach, which not only provides in-depth understanding of a user's search intents but also benefits a variety of personalized applications.

#index 2045913
#* Effective co-betweenness centrality computation
#@ Mostafa Haghir Chehreghani
#t 2014
#c 2
#% 290830
#% 503213
#% 823342
#% 866351
#% 910872
#% 937549
#% 1053980
#% 1360062
#% 1369005
#% 1409360
#% 1764779
#! Betweenness centrality of vertices is essential in the analysis of social and information networks, and co-betweenness centrality is one of two natural ways to extend it to sets of vertices. Existing algorithms for co-betweenness centrality computation suffer from at least one of the following problems: i) their applicability is limited to special cases like sequences, sets of size two, and ii) they are not efficient in terms of time complexity. In this paper, we present efficient algorithms for co-betweenness centrality computation of any set or sequence of vertices in weighted and unweighted networks. We also develop effective methods for co-betweenness centrality computation of sets and sequences of edges. These results provide a clear and extensive view about the complexity of co-betweenness centrality computation for vertices and edges in weighted and un-weighted networks. Finally, we perform extensive experiments on real-world networks from different domains including social, information and communication networks, to show the empirical efficiency of the proposed methods.

#index 2045914
#* Scalable hierarchical multitask learning algorithms for conversion optimization in display advertising
#@ Amr Ahmed;Abhimanyu Das;Alexander J. Smola
#t 2014
#c 2
#% 232771
#% 236497
#% 466904
#% 840962
#% 961190
#% 989644
#% 1128929
#% 1211823
#% 1211829
#% 1292771
#% 1302843
#% 1302853
#% 1523858
#% 1606397
#% 1693873
#% 1705537
#% 1745125
#% 1746906
#% 1919810
#! Many estimation tasks come in groups and hierarchies of related problems. In this paper we propose a hierarchical model and a scalable algorithm to perform inference for multitask learning. It infers task correlation and subtask structure in a joint sparse setting. Implementation is achieved by a distributed subgradient oracle and the successive application of prox-operators pertaining to groups and subgroups of variables. We apply this algorithm to conversion optimization in display advertising. Experimental results on over 1TB data for up to 1 billion observations and 1 million attributes show that the algorithm provides significantly better prediction accuracy while simultaneously beingefficiently scalable by distributed parameter synchronization.

#index 2045915
#* Discovering common motifs in cursor movement data for improving web search
#@ Dmitry Lagun;Mikhail Ageev;Qi Guo;Eugene Agichtein
#t 2014
#c 2
#% 86950
#% 292179
#% 427199
#% 462231
#% 480133
#% 564263
#% 766472
#% 770782
#% 795273
#% 812372
#% 860570
#% 874272
#% 954948
#% 1048694
#% 1074148
#% 1083693
#% 1183067
#% 1202948
#% 1213471
#% 1214716
#% 1355045
#% 1384641
#% 1450833
#% 1450845
#% 1550142
#% 1573487
#% 1693899
#% 1746855
#% 1765932
#% 1766010
#% 1779112
#% 1872261
#% 1879012
#% 1879187
#% 1905282
#% 1919870
#% 1978793
#% 1988853
#% 1988860
#! Web search behavior and interaction data, such as mouse cursor movements, can provide valuable information on how searchers examine and engage with the web search results. This interaction data is far richer than traditional search click data, and can be used to improve search ranking, evaluation, and presentation. Unfortunately, the diversity and complexity inherent in this interaction data make it more difficult to capture salient behavior characteristics through traditional feature engineering. To address this problem, we introduce a novel approach of automatically discovering frequent subsequences, or motifs, in mouse cursor movement data. In order to scale our approach to realistic datasets, we introduce novel optimizations for motif discovery, specifically designed for mining cursor movement data. As a practical application, we show that by encoding the motifs discovered from thousands of real web search sessions as features, enables significant improvements on result relevance estimation and re-ranking tasks, compared to a state-of-the-art baseline that relies on extensive feature engineering. These results, complemented with visualization and qualitative analysis, demonstrate that our approach is able to automatically capture key characteristics of mouse cursor movement behavior, providing a valuable new tool for search behavior analysis.

#index 2045916
#* Using linked data to mine RDF from wikipedia's tables
#@ Emir Muñoz;Aidan Hogan;Alessandra Mileo
#t 2014
#c 2
#% 348147
#% 943552
#% 1063570
#% 1127393
#% 1288161
#% 1440454
#% 1482339
#% 1523913
#% 1536559
#% 1567948
#% 1592311
#% 1641869
#% 1925702
#! The tables embedded in Wikipedia articles contain rich, semi-structured encyclopaedic content. However, the cumulative content of these tables cannot be queried against. We thus propose methods to recover the semantics of Wikipedia tables and, in particular, to extract facts from them in the form of RDF triples. Our core method uses an existing Linked Data knowledge-base to find pre-existing relations between entities in Wikipedia tables, suggesting the same relations as holding for other entities in analogous columns on different rows. We find that such an approach extracts RDF triples from Wikipedia's tables at a raw precision of 40%. To improve the raw precision, we define a set of features for extracted triples that are tracked during the extraction phase. Using a manually labelled gold standard, we then test a variety of machine learning methods for classifying correct/incorrect triples. One such method extracts 7.9 million unique and novel RDF triples from over one million Wikipedia tables at an estimated precision of 81.5%.

#index 2045917
#* Democracy is good for ranking: towards multi-view rank learning and adaptation in web search
#@ Wei Gao;Pei Yang
#t 2014
#c 2
#% 252011
#% 309095
#% 316509
#% 748550
#% 770772
#% 770847
#% 815908
#% 983820
#% 987226
#% 1035577
#% 1039843
#% 1074021
#% 1074063
#% 1074082
#% 1130923
#% 1134122
#% 1261539
#% 1268491
#% 1272110
#% 1292528
#% 1292566
#% 1338581
#% 1442578
#% 1450849
#% 1456843
#% 1598345
#% 1606063
#% 1617369
#% 1755385
#! Web search ranking models are learned from features originated from different views or perspectives of document relevancy, such as query dependent or independent features. This seems intuitively conformant to the principle of multi-view approach that leverages distinct complementary views to improve model learning. In this paper, we aim to obtain optimal separation of ranking features into non-overlapping subsets (i.e., views), and use such different views for rank learning and adaptation. We present a novel semi-supervised multi-view ranking model, which is then extended into an adaptive ranker for search domains where no training data exists. The core idea is to proactively strengthen view consistency (i.e., the consistency between different rankings each predicted by a distinct view-based ranker) especially when training and test data follow divergent distributions. For this purpose, we propose a unified framework based on listwise ranking scheme to mutually reinforce the view consistency of target queries and the appropriate weighting of source queries that act as prior knowledge. Based on LETOR and Yahoo Learning to Rank datasets, our method significantly outperforms some strong baselines including single-view ranking models commonly used and multi-view ranking models that do not impose view consistency on target data.

#index 2045918
#* Exploiting user disagreement for web search evaluation: an experimental approach
#@ Thomas Demeester;Robin Aly;Djoerd Hiemstra;Dong Nguyen;Dolf Trieschnigg;Chris Develder
#t 2014
#c 2
#% 129694
#% 312689
#% 411762
#% 575729
#% 642975
#% 1074124
#% 1074134
#% 1095876
#% 1166473
#% 1227640
#% 1263586
#% 1292473
#% 1292527
#% 1450896
#% 1450904
#% 1482378
#% 1598440
#% 1806006
#% 1913790
#% 1919835
#! To express a more nuanced notion of relevance as compared to binary judgments, graded relevance levels can be used for the evaluation of search results. Especially in Web search, users strongly prefer top results over less relevant results, and yet they often disagree on which are the top results for a given information need. Whereas previous works have generally considered disagreement as a negative effect, this paper proposes a method to exploit this user disagreement by integrating it into the evaluation procedure. First, we present experiments that investigate the user disagreement. We argue that, with a high disagreement, lower relevance levels might need to be promoted more than in the case where there is global consensus on the top results. This is formalized by introducing the User Disagreement Model, resulting in a weighting of the relevance levels with a probabilistic interpretation. A validity analysis is given, and we explain how to integrate the model with well-established evaluation metrics. Finally, we discuss a specific application of the model, in the estimation of suitable weights for the combined relevance of Web search snippets and pages.

#index 2048832
#* Customized tour recommendations in urban areas
#@ Aristides Gionis;Theodoros Lappas;Konstantinos Pelechrinis;Evimaria Terzi
#t 2014
#c 2
#% 146197
#% 408396
#% 816393
#% 836516
#% 1061913
#% 1128528
#% 1184570
#% 1426523
#% 1429406
#% 1480830
#% 1482236
#% 1582104
#% 1594690
#% 1606038
#% 1871882
#% 1872250
#% 1930552
#% 1941002
#! The ever-increasing urbanization coupled with the unprecedented capacity to collect and process large amounts of data have helped to create the vision of intelligent urban environments. One key aspect of such environments is that they allow people to effectively navigate through their city. While GPS technology and route-planning services have undoubtedly helped towards this direction, there is room for improvement in intelligent urban navigation. This vision can be fostered by the proliferation of location-based social networks, such as Foursquare or Path, which record the physical presence of users in different venues through check-ins. This information can then be used to enhance intelligent urban navigation, by generating customized path recommendations for users. In this paper, we focus on the problem of recommending customized tours in urban settings. These tours are generated so that they consider (a) the different types of venues that the user wants to visit, as well as the order in which the user wants to visit them, (b) limitations on the time to be spent or distance to be covered, and (c) the merit of visiting the included venues. We capture these requirements in a generic definition that we refer to as the TourRec problem. We then introduce two instances of the TourRec problem, study their complexity, and propose efficient algorithmic solutions. Our experiments on real data collected from Foursquare demonstrate the efficacy of our algorithms and the practical utility of the reported recommendations.

#index 2048833
#* Transferring heterogeneous links across location-based social networks
#@ Jiawei Zhang;Xiangnan Kong;Philip S. Yu
#t 2014
#c 2
#% 730089
#% 975021
#% 1030874
#% 1190134
#% 1227601
#% 1399992
#% 1480830
#% 1535418
#% 1605971
#% 1606045
#% 1606049
#% 1606051
#% 1635098
#% 1693935
#% 1978832
#% 1985019
#% 2006033
#% 2010449
#! ocation-based social networks (LBSNs) are one kind of online social networks offering geographic services and have been attracting much attention in recent years. LBSNs usually have complex structures, involving heterogeneous nodes and links. Many recommendation services in LBSNs (e.g., friend and location recommendation) can be cast as link prediction problems (e.g., social link and location link prediction). Traditional link prediction researches on LBSNs mostly focus on predicting either social links or location links, assuming the prediction tasks of different types of links to be independent. However, in many real-world LBSNs, the prediction tasks for social links and location links are strongly correlated and mutually influential. Another key challenge in link prediction on LBSNs is the data sparsity problem (i.e., "new network" problem), which can be encountered when LBSNs branch into new geographic areas or social groups. Actually, nowadays, many users are involved in multiple networks simultaneously and users who just join one LBSN may have been using other LBSNs for a long time. In this paper, we study the problem of predicting multiple types of links simultaneously for a new LBSN across partially aligned LBSNs and propose a novel method TRAIL (TRAnsfer heterogeneous lInks across LBSNs). TRAIL can accumulate information for locations from online posts and extract heterogeneous features for both social links and location links. TRAIL can predict multiple types of links simultaneously. In addition, TRAIL can transfer information from other aligned networks to the new network to solve the problem of lacking information. Extensive experiments conducted on two real-world aligned LBSNs show that TRAIL can achieve very good performance and substantially outperform the baseline methods.

#index 2048834
#* Supervised N-gram topic model
#@ Noriaki Kawamae
#t 2014
#c 2
#% 529193
#% 769892
#% 876067
#% 939624
#% 939896
#% 1035590
#% 1055682
#% 1117083
#% 1338553
#% 1385969
#% 1451218
#% 1470684
#% 1481541
#% 1482272
#% 1536536
#% 1536565
#% 1606398
#% 1747037
#% 1913589
#! We propose a Bayesian nonparametric topic model that rep- resents relationships between given labels and the corre- sponding words/phrases, from supervised articles. Unlike existing supervised topic models, our proposal, supervised N-gram topic model (SNT), focuses on both a number of topics and power-law distribution in the word frequencies to extract topic specific N-grams. To achieve this goal, SNT takes a Bayesian nonparametric approach to topic sampling, which generates word distribution jointly with the given variable in textual order, and then form each N-gram word as a hierarchy of Pitman-Yor process priors. Experiments on labeled text data show that SNT is useful as a generative model for discovering more phrases that complement human experts and domain specific knowledge than the existing al- ternatives. The results show that SNT can be applied to various tasks such as automatic annotation.

#index 2048835
#* Modeling opinion dynamics in social networks
#@ Abhimanyu Das;Sreenivas Gollapudi;Kamesh Munagala
#t 2014
#c 2
#% 729923
#% 1227295
#% 1384440
#% 1451242
#% 1783965
#% 1872232
#% 1905898
#% 1987785
#! Our opinions and judgments are increasingly shaped by what we read on social media -- whether they be tweets and posts in social networks, blog posts, or review boards. These opinions could be about topics such as consumer products, politics, life style, or celebrities. Understanding how users in a network update opinions based on their neighbor's opinions, as well as what global opinion structure is implied when users iteratively update opinions, is important in the context of viral marketing and information dissemination, as well as targeting messages to users in the network. In this paper, we consider the problem of modeling how users update opinions based on their neighbors' opinions. We perform a set of online user studies based on the celebrated conformity experiments of Asch [1]. Our experiments are carefully crafted to derive quantitative insights into developing a model for opinion updates (as opposed to deriving psychological insights). We show that existing and widely studied theoretical models do not explain the entire gamut of experimental observations we make. This leads us to posit a new, nuanced model that we term the BVM. We present preliminary theoretical and simulation results on the convergence and structure of opinions in the entire network when users iteratively update their respective opinions according to the BVM. We show that consensus and polarization of opinions arise naturally in this model under easy to interpret initial conditions on the network.

#index 2048836
#* Data that matter: opportunities in crisis informatics research
#@ Leysia Palen
#t 2014
#c 2
#% 1429836
#! In an increasingly global society and on a planet experiencing effects of climate change, large-scale emergencies both instigated by humans and arising from nature can devastate human life and our tightly- woven social fabric. With a promise of improved warning and coordination, a prevailing hope is that information and communication technology (ICT) can help reduce the impacts of large-scale disruptions, including political crises, natural disasters, pandemics, and terrorist threats. Much of the focus of development has been on the formal emergency response effort. However, social computing is changing the way we understand information distribution. By viewing the citizenry as a powerful, self-organizing, and collectively intelligent force, ICT is now playing a remarkable and transformational role in the way society responds to mass emergencies and disasters. Furthermore, this view of a civil society that can be augmented by ICT is based on social and behavioral knowledge about how people truly respond in disaster, rather than on simplified and mythical portrayals of people unable to help themselves [2]. Indeed, long before the advent of widely available social computing platforms, research has shown that disaster victims themselves are the true first responders, frequently acting on the basis of knowledge not available to officials [1, 3, 6]. We argue that this transformative view is critical to our global future: When large-scale emergencies happen, there is often no way to survive it in practical terms unless we rely on each other for help. The urgency and scale of many disaster events are such that no one, not even the most experienced and best technology- equipped responders' can rescue all victims or direct all people over the span of the event as to what the best course of action might be. Climate change and population migration to geographically vulnerable areas mean that naturally occurring hazards will exert increasingly extensive damage. Man-made and terrorist threats can also have greater potential to cause lasting damage to the social and built environment. It is instead necessary, through innovative ICT, to leverage the power of the collective intelligence of the citizenry to support natural instincts, which are to search for reliable information using any means possible to optimize for local conditions [5].

#index 2054288
#* Who watches (and shares) what on youtube? and when?: using twitter to understand youtube viewership
#@ Adiya Abisheva;Venkata Rama Kiran Garimella;David Garcia;Ingmar Weber
#t 2014
#c 2
#% 917962
#% 1411585
#% 1536504
#% 1536506
#% 1536579
#% 1554323
#% 1605961
#% 1746823
#% 1765772
#% 1879112
#% 1893813
#% 1912617
#% 1948160
#% 1967836
#% 1991982
#% 2011668
#! By combining multiple social media datasets, it is possible to gain insight into each dataset that goes beyond what could be obtained with either individually. In this paper we combine user-centric data from Twitter with video-centric data from YouTube to build a rich picture of who watches and shares what on YouTube. We study 87K Twitter users, 5.6 million YouTube videos and 15 million video sharing events from user-, video- and sharing-event-centric perspectives. We show that features of Twitter users correlate with YouTube features and sharing-related features. For example, urban users are quicker to share than rural users. We find a superlinear relationship between initial Twitter shares and the final amounts of views. We discover that Twitter activity metrics play more role in video popularity than mere amount of followers. We also reveal the existence of correlated behavior concerning the time between video creation and sharing within certain timescales, showing the time onset for a coherent response, and the time limit after which collective responses are extremely unlikely. Response times depend on the category of the video, suggesting Twitter video sharing is highly dependent on the video content. To the best of our knowledge, this is the first large-scale study combining YouTube and Twitter data, and it reveals novel, detailed insights into who watches (and shares) what on YouTube, and when.

#index 2054289
#* Who likes it more?: mining worth-recommending items from long tails by modeling relative preference
#@ Yu-Chieh Ho;Yi-Ting Chiang;Jane Yung-Jen Hsu
#t 2014
#c 2
#% 220706
#% 280852
#% 734590
#% 860672
#% 918842
#% 1083671
#% 1127451
#% 1127465
#% 1200360
#% 1260273
#% 1287293
#% 1526175
#% 1605855
#% 1730808
#% 1755308
#% 1766223
#% 1848116
#% 1872522
#% 1948202
#% 2000626
#% 2047565
#! Recommender systems are useful tools that help people to filter and explore massive information. While the accuracy of recommender systems is important, many recent research indicated that focusing merely on accuracy not only is insufficient to meet user needs, but also may be harmful. Other characteristics such as novelty, unexpectedness and diversity should also be taken into consideration. Previous work has shown that more the sales of long-tail items could be more beneficial to both customers and some business models. However, the majority of collaborative filtering approaches tends to recommend popular selling items. In this work, we focus on long-tail item promotion and aggregate diversity enhancement, and propose a novel approach which diversifies the results of recommender systems by considering ``recommendations" as resources to be allocated to the items. Our approach increases the quantity and quality of long-tail item recommendations by adding more variation into the recommendation and maintains a certain level of accuracy simultaneously. The experimental results show that this approach can discover more worth-recommending items from Long Tails and improves user experience.

