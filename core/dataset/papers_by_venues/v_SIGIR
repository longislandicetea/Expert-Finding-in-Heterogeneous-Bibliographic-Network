#index 218977
#* The network computer (abstract)
#@ Andy Hopper
#t 1996
#c 13

#index 218978
#* Query expansion using local and global document analysis
#@ Jinxi Xu;W. Bruce Croft
#t 1996
#c 13
#% 118738
#% 144029
#% 169729
#% 169779
#% 169809
#% 184489
#% 289079

#index 218979
#* The design of a high performance information filtering system
#@ Timothy A. H. Bell;Alistair Moffat
#t 1996
#c 13
#% 36399
#% 67565
#% 90848
#% 115465
#% 115473
#% 124004
#% 169712
#% 169805
#% 169806
#% 169814
#% 169817
#% 185255
#% 188587
#% 194247
#% 213786
#% 290703
#% 319273
#% 463734
#! A high performance information filtering system has three mainrequirements: it must be effective in supplying users with usefulinformation, it must do so in a timely fashion, and it must be ableto handle a large throughput of information and a large number ofuser profiles efficiently. These three requirements pose adifficult problem, and to our knowledge no existing system iscapable of meeting all three. In this paper we describe a systemwhich combines a number of techniques from other informationretrieval and filtering systems, and is capable of providing highperformance on a typical workstation platform. We provide estimatesof computing resource usage, and show that our system is alsoscalable.

#index 218982
#* Pivoted document length normalization
#@ Amit Singhal;Chris Buckley;Mandar Mitra
#t 1996
#c 13
#% 46803
#% 67565
#% 111456
#% 169781
#% 321635
#% 406493
#% 648778
#% 648799
#% 817970

#index 218984
#* Retrieving spoken documents by combining multiple index sources
#@ G. J. F. Jones;J. T. Foote;K. Spärck Jones;S. J. Young
#t 1996
#c 13
#% 100958
#% 169781
#% 184496
#% 194209
#% 206399
#% 375017
#% 818073
#% 968476

#index 218985
#* Viewing stemming as recall enhancement
#@ Wessel Kraaij;Renée Pohlmann
#t 1996
#c 13
#% 67565
#% 144034
#% 144074
#% 184486
#% 208934
#% 406493

#index 218988
#* Querying across languages: a dictionary-based approach to multilingual information retrieval
#@ David A. Hull;Gregory Grefenstette
#t 1996
#c 13
#% 144074
#% 169768
#% 194275
#% 208934
#% 217207
#% 648114
#% 740900
#% 740915
#% 742247
#% 742250
#% 744272
#% 748337
#% 748354

#index 218989
#* Experiments in multilingual information retrieval using the SPIDER system
#@ Páraic Sheridan;Jean Paul Ballerini
#t 1996
#c 13
#% 35938
#% 144029
#% 144072
#% 194267
#% 208934
#% 649261
#% 677232
#% 677703

#index 218990
#* Visualizing search results: some alternatives to query-document similarity
#@ Lucy Terry Nowell;Robert K. France;Deborah Hix;Lenwood S. Heath;Edward A. Fox
#t 1996
#c 13
#% 68659
#% 72935
#% 118771
#% 118772
#% 137473
#% 144023
#% 162376
#% 162452
#% 169783
#% 172811
#% 172812
#% 185254
#% 201992
#% 201993
#% 334582
#% 726144
#% 726258

#index 218992
#* Reexamining the cluster hypothesis: scatter/gather on retrieval results
#@ Marti A. Hearst;Jan O. Pedersen
#t 1996
#c 13
#% 46809
#% 60987
#% 67565
#% 109199
#% 109200
#% 118771
#% 118772
#% 144023
#% 152957
#% 201992
#% 214711
#% 228105
#% 375017

#index 218994
#* Evaluation of a tool for visualization of information retrieval results
#@ Aravindan Veerasamy;Nicholas J. Belkin
#t 1996
#c 13
#% 96288
#% 109199
#% 142621
#% 142622
#% 169783
#% 201992
#% 202027
#% 233948
#% 238802

#index 219011
#* An architecture for implementing extensible information-seeking environments
#@ David G. Hendry;David J. Harper
#t 1996
#c 13
#% 3582
#% 85443
#% 116575
#% 120106
#% 127449
#% 127612
#% 143530
#% 151549
#% 181414
#% 181610
#% 185244
#% 185254
#% 186518
#% 194249
#% 238283

#index 219012
#* Document retrieval facilities for repository-based system development environments
#@ Andreas Henrich
#t 1996
#c 13
#% 46803
#% 67565
#% 68354
#% 87464
#% 109212
#% 112296
#% 116091
#% 118760
#% 144070
#% 161102
#% 163433
#% 169811
#% 169849
#% 170899
#% 172899
#% 172927
#% 199772
#% 435143
#% 458517
#% 458520
#% 481442
#% 481602
#% 527027
#% 616343
#% 616358
#% 616359

#index 219013
#* Performance evaluation of a distributed architecture for information retrieval
#@ Brendon Cahoon;Kathryn S. McKinley
#t 1996
#c 13
#% 35207
#% 143983
#% 173905
#% 183334
#% 184489
#% 194246
#% 194275
#% 286382
#% 339621
#% 677246

#index 219014
#* Elicitations during information retrieval: implications for IR system design
#@ Amanda Spink;Abby Goodrum;David Robins;Mei Mei Wu
#t 1996
#c 13
#% 42988
#% 130584
#% 169732
#% 187997
#% 235292

#index 219024
#* Evaluating user interfaces to information retrieval systems: a case study on user support
#@ Giorgio Brajnik;Stefano Mizzaro;Carlo Tasso
#t 1996
#c 13
#% 27049
#% 42988
#% 50506
#% 54412
#% 62198
#% 85443
#% 87101
#% 116374
#% 133891
#% 133892

#index 219025
#* Efficient processing of one and two dimensional proximity queries in associative memory
#@ K. L. Liu;G. J. Lipovski;C. Yu;Naphtali Rishe
#t 1996
#c 13
#% 2115
#% 5183
#% 65954
#% 93113
#% 123245
#% 144011
#% 172922
#% 172949
#% 317950
#% 406493
#% 437320
#% 463908

#index 219028
#* Efficient transaction support for dynamic information retrieval systems
#@ Mohan Kamath;Krithi Ramamritham
#t 1996
#c 13
#% 9241
#% 83183
#% 102808
#% 116082
#% 116083
#% 116085
#% 116086
#% 116087
#% 144070
#% 169849
#% 172922
#% 191171
#% 194247
#% 194253
#% 318425
#% 403195
#% 480286
#% 481439

#index 219031
#* Image organization and retrieval with automatically constructed feature vectors
#@ Kyung-Ah Han;Sung-Hyun Myaeng
#t 1996
#c 13
#% 60576
#% 83962
#% 102772
#% 157132
#% 181409
#% 227780
#% 406493
#% 463414

#index 219033
#* Phonetic string matching: lessons from information retrieval
#@ Justin Zobel;Philip Dart
#t 1996
#c 13
#% 45048
#% 57485
#% 67565
#% 85590
#% 120649
#% 121278
#% 189867
#% 194276
#% 204294
#% 317975

#index 219036
#* Experiments on using semantic distances between words in image caption retrieval
#@ Alan F. Smeaton;Ian Quigley
#t 1996
#c 13
#% 120109
#% 144031
#% 153026
#% 174315
#% 184496
#% 193105
#% 661712
#% 817957

#index 219040
#* Automatic linking of thesauri
#@ S. Amba;N. Narasimhamurthi;Kevin C. O'Kane;Philip M. Turner
#t 1996
#c 13
#% 367862
#% 406493

#index 219041
#* A new method of weighting query terms for ad-hoc retrieval
#@ K. L. Kwok
#t 1996
#c 13
#% 46803
#% 90640
#% 183255

#index 219042
#* A relevance terminological logic for information retrieval
#@ Carlo Meghini;Umberto Straccia
#t 1996
#c 13
#% 55925
#% 65953
#% 144069
#% 169764
#% 194290

#index 219043
#* Retrieval of complex objects using a four-valued logic
#@ Thomas Rölleke;Norbert Fuhr
#t 1996
#c 13
#% 65953
#% 71755
#% 116625
#% 157172
#% 167245
#% 172927
#% 176530
#% 186336
#% 189739
#% 194290
#% 194293
#% 215225

#index 219044
#* Using n-grams for Korean text retrieval
#@ Joo Ho Lee;Jeong Soo Ahn
#t 1996
#c 13
#% 46803
#% 67565
#% 194276
#% 321635
#% 406493

#index 219045
#* On Chinese text retrieval
#@ Jian-Yun Nie;Martin Brisebois;Xiaobo Ren
#t 1996
#c 13
#% 144042
#% 144044
#% 162463
#% 187759
#% 194263
#% 194265
#% 220132
#% 648114
#% 740900
#% 748591
#% 756901

#index 219046
#* An application of plausible reasoning to information retrieval
#@ Farhad Oroumchian;Robert N. Oddy
#t 1996
#c 13
#% 25942
#% 25944
#% 65953
#% 73028
#% 215128
#% 406493

#index 219047
#* A belief network model for IR
#@ Berthier A. N. Ribeiro;Richard Muntz
#t 1996
#c 13
#% 44876
#% 46803
#% 90640
#% 104923
#% 111303
#% 176530
#% 187773
#% 198980

#index 219048
#* Document filtering with inference networks
#@ Jamie Callan
#t 1996
#c 13
#% 65956
#% 111303
#% 124004
#% 124009
#% 184489
#% 228097
#% 406493
#% 978507

#index 219049
#* Incremental relevance feedback for information filtering
#@ James Allan
#t 1996
#c 13
#% 54465
#% 111456
#% 118728
#% 169717
#% 169806
#% 194301
#% 219048
#% 840583
#% 978507

#index 219050
#* Method combination for document filtering
#@ David A. Hull;Jan O. Pedersen;Hinrich Schütze
#t 1996
#c 13
#% 44876
#% 57484
#% 111303
#% 132938
#% 144074
#% 144076
#% 157135
#% 169774
#% 169805
#% 183255
#% 184486
#% 184496
#% 191854
#% 194276
#% 194283
#% 194284
#% 319273
#% 476546
#% 682454
#% 682681
#% 748583

#index 219051
#* Combining classifiers in text categorization
#@ Leah S. Larkey;W. Bruce Croft
#t 1996
#c 13
#% 5182
#% 67565
#% 73028
#% 73046
#% 109192
#% 111303
#% 118731
#% 118736
#% 127850
#% 144076
#% 165110
#% 187773
#% 194284
#% 194301
#% 288211
#% 375017
#% 1290067

#index 219052
#* Training algorithms for linear text classifiers
#@ David D. Lewis;Robert E. Schapire;James P. Callan;Ron Papka
#t 1996
#c 13
#% 1527
#% 46803
#% 115473
#% 115476
#% 151296
#% 165111
#% 169717
#% 169777
#% 169806
#% 184489
#% 194284
#% 194301
#% 197922
#% 218982
#% 219053
#% 375017
#% 406493
#% 682442

#index 219053
#* Context-sensitive learning methods for text categorization
#@ William W. Cohen;Yoram Singer
#t 1996
#c 13
#% 81507
#% 88326
#% 127850
#% 165110
#% 165663
#% 169717
#% 169806
#% 203129
#% 219052
#% 232319
#% 375017
#% 520224

#index 219054
#* Detection of shifts in user interests for personalized information filtering
#@ W. Lam;S. Mukhopadhyay;J. Mostafa;M. Palakal
#t 1996
#c 13
#% 18566
#% 56461
#% 96269
#% 124004
#% 406493
#% 978507

#index 219055
#* Interactive information retrieval systems: from user centered interface design to software design
#@ P. Mulhem;L. Nigay
#t 1996
#c 13
#% 25945
#% 86371
#% 96285
#% 127237
#% 127586
#% 152957
#% 167200
#% 169739
#% 169783
#% 172722
#% 172811
#% 194276
#% 201997
#% 231567
#% 234405
#% 394790
#% 525682
#% 648489

#index 219056
#* Panel: building and using test collections
#@ Donna Harman
#t 1996
#c 13
#% 3965
#% 46803
#% 108999
#% 169777
#% 169779
#% 228105

#index 219057
#* System demonstrations: abstracts
#@ Marc Rittberger
#t 1996
#c 13

#index 219058
#* IR application development with FireWorks
#@ David J. Harper;David G. Hendry;Jan-Jaap IJdens;Joemon Jose
#t 1996
#c 13

#index 219059
#* A novel client-server protocol for the demanding Opac user
#@ E. J. Yannakoudakis
#t 1996
#c 13

#index 219061
#* WING: a multiple-view smooth information retrieval system
#@ Toshiyuki Masui;Mitsuru Minakuchi;George R. Borden;Kouichi Kashiwagi
#t 1996
#c 13

#index 219062
#* Visualizing search results with Envision
#@ Lucy Terry Nowell;Robert K. France;Edward A. Fox
#t 1996
#c 13

#index 219063
#* Ariadne: electronic information for computer scientists
#@ Markus Dreger;Stefan Lohrum;Kai Grossjohann;Claus Dieter Ziegler
#t 1996
#c 13

#index 219064
#* WebCompass: an agent-based metasearch and metadata discovery tool for the Web
#@ Brad Allen;John Jensen;Jay Nelson;Brian Ulicny;Kristina Lerman;Linda Rudell-Betts
#t 1996
#c 13

#index 219066
#* Querying hierarchically structured texts with generalized context-free grammars
#@ Yves Marcoux;Martin Sevigny
#t 1996
#c 13

#index 219067
#* The CD-ROM of Crete: a multimedia tourism application, based on geographic interaction and information retrieval techniques
#@ N. Moumoutzis;M. Frangonikolakis
#t 1996
#c 13

#index 219068
#* An efficient retrieval algorithm of two-trie structures
#@ Takako Tsuji;Syouji Mizobuchi;Masami Shishibori;Jun-ichi Aoe
#t 1996
#c 13

#index 219069
#* An efficient retrieval algorithm of binary digital search-trees using hierarchical structures
#@ Masami Shishibori;Yoshitaka Hayashi;Kazuhiro Morita;Jun-ichi Aoe
#t 1996
#c 13

#index 219070
#* Assessed relevance and stylistic variation
#@ Jussi Karlgren
#t 1996
#c 13

#index 219071
#* A spatial feature based photograph retrieval system
#@ Joemon M. Jose;David John Harper;David G. Hendry
#t 1996
#c 13

#index 219072
#* On the potential utility of negative relevance feedback in interactive information retrieval
#@ Colleen Cool;Nicholas J. Belkin;Jürgen Koenemann
#t 1996
#c 13

#index 219073
#* Retrieval of paintings by specifying impression words
#@ Kozaburo Hachimura
#t 1996
#c 13

#index 219074
#* Extraction of a word list from an existing dictionary to be used in a communication-aid software
#@ Brigitte Le Pévédic
#t 1996
#c 13

#index 219075
#* Merging hypertext and information retrieval in the interface
#@ Gene Golovchinsky;Mark Chignell
#t 1996
#c 13

#index 219076
#* Fast full text search with free word using TS-file
#@ Takashi Sato
#t 1996
#c 13

#index 219077
#* OLISTICO: an evaluation environment for interactive IR applications
#@ M. Agosti;R. Bandiera;F. Bazo;R. Colotti;S. Gabrielli
#t 1996
#c 13

#index 219079
#* Foundations of advanced information visualization for information retrieval systems
#@ Mark E. Rorvig;Matthias Hemmje
#t 1996
#c 13

#index 219094
#* Courseware, training and curriculum in information retrieval
#@ Edward A. Fox
#t 1996
#c 13

#index 219095
#* Research in information retrieval and the practical needs of research and cultural libraries
#@ Encarnacion Rancitelli
#t 1996
#c 13

#index 219096
#* Cross-linguistic information retrieval workshop
#@ Gregory Grefenstette
#t 1996
#c 13

#index 219098
#* Networked information retrieval
#@ Norbert Fuhr
#t 1996
#c 13

#index 223368
#* A deductive data model for query expansion
#@ Kalervo Järvelin;Jaana Kristensen;Timo Niemi;Eero Sormunen;Hiekki Keskustalo
#t 1996
#c 13
#% 16774
#% 86528
#% 104438
#% 134879
#% 134884
#% 139924
#% 142259
#% 144074
#% 157884
#% 186082
#% 187756
#% 227805
#% 368248
#% 461867

#index 225843
#* Posters: abstracts
#@ Elizabeth D. Liddy
#t 1996
#c 13

#index 232642
#* Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval
#@ Nicholas J. Belkin;A. Desai Narasimhalu;Peter Willett;William Hersh;Fazli Can;Ellen Voorhees
#t 1997
#c 13

#index 232643
#* Users lost (summary): reflections on the past, future, and limits of information science
#@ Tafko Saracevic
#t 1997
#c 13

#index 232644
#* Fast and effective query refinement
#@ Bienvenido Vélez;Ron Weiss;Mark A. Sheldon;David K. Gifford
#t 1997
#c 13
#% 3621
#% 4489
#% 54435
#% 66172
#% 115462
#% 115476
#% 118726
#% 118728
#% 118771
#% 144027
#% 144029
#% 169816
#% 186326
#% 211526
#% 215227
#% 218978
#% 290477
#% 649572
#% 669050

#index 232645
#* On relevance weights with little relevance information
#@ S. E. Robertson;S. Walker
#t 1997
#c 13
#% 169781

#index 232646
#* Learning routing queries in a query zone
#@ Amit Singhal;Mandar Mitra;Chris Buckley
#t 1997
#c 13
#% 46803
#% 116165
#% 124004
#% 169717
#% 169805
#% 169806
#% 194283
#% 194301
#% 218982
#% 219048
#% 219049
#% 321635
#% 406493

#index 232647
#* Comparing representations in Chinese information retrieval
#@ K. L. Kwok
#t 1997
#c 13
#% 90640
#% 169806
#% 183255
#% 187759
#% 194263
#% 194265
#% 197863
#% 216017
#% 219045
#% 742413
#% 756901
#% 854980

#index 232648
#* Chinese text retrieval without using a dictionary
#@ Aitao Chen;Jianzhang He;Liangjie Xu;Fredric C. Gey;Jason Meggs
#t 1997
#c 13
#% 162463
#% 187759
#% 216017
#% 219045
#% 230580
#% 742413
#% 748499
#% 748716
#% 756901

#index 232650
#* PAT-tree-based keyword extraction for Chinese information retrieval
#@ Lee-Feng Chien
#t 1997
#c 13
#% 1921
#% 93113
#% 115467
#% 124004
#% 162463
#% 194263
#% 194265
#% 197863
#% 210986
#% 219045
#% 288578
#% 742367
#% 756901

#index 232651
#* Almost-constant-time clustering of arbitrary corpus subsets4
#@ Craig Silverstein;Jan O. Pedersen
#t 1997
#c 13
#% 36672
#% 118771
#% 144023
#% 218992

#index 232653
#* Feature selection, perceptron learning, and a usability case study for text categorization
#@ Hwee Tou Ng;Wei Boon Goh;Kok Leong Low
#t 1997
#c 13
#% 85272
#% 118736
#% 127850
#% 165110
#% 169805
#% 194283
#% 219052
#% 219053

#index 232655
#* Projections for efficient document clustering
#@ Hinrich Schütze;Craig Silverstein
#t 1997
#c 13
#% 85447
#% 118738
#% 118771
#% 144023
#% 218992
#% 219050
#% 228105
#% 229348
#% 232651
#% 282323
#% 375017

#index 232656
#* Phrasal translation and query expansion techniques for cross-language information retrieval
#@ Lisa Ballesteros;W. Bruce Croft
#t 1997
#c 13
#% 54453
#% 86371
#% 184489
#% 211043
#% 218978
#% 218988
#% 218989
#% 289079
#% 562054
#% 649261
#% 748444

#index 232658
#* QUILT: implementing a large-scale cross-language text retrieval system
#@ Mark W. Davis;William C. Ogden
#t 1997
#c 13
#% 46803
#% 744491

#index 232660
#* Cross-language speech retrieval: establishing a baseline performance
#@ Páraic Sheridan;Martin Wechsler;Peter Schäuble
#t 1997
#c 13
#% 118751
#% 144072
#% 171744
#% 194297
#% 218984
#% 218988
#% 218989
#% 219044
#% 219096
#% 219917
#% 437509
#% 562054
#% 649261
#% 704026
#% 968469
#% 1783212

#index 232662
#* Dempster-Shafer's theory of evidence applied to structured documents: modelling uncertainty
#@ Mounia Lalmas
#t 1997
#c 13
#% 97127
#% 120103
#% 120104
#% 164547
#% 203773
#% 219042
#% 219043
#% 360717

#index 232663
#* Computationally tractable probabilistic modeling of Boolean operators
#@ Warren R. Greiff;W. Bruce Croft;Howard Turtle
#t 1997
#c 13
#% 44876
#% 67565
#% 86371
#% 109042
#% 115477
#% 144076
#% 169779
#% 169782
#% 184496
#% 187773
#% 319273
#% 648773

#index 232667
#* A method for monolingual thesauri merging
#@ Marios Sintichakis;Panos Constantopoulos
#t 1997
#c 13
#% 1428
#% 22948
#% 36157
#% 44316
#% 49594
#% 55490
#% 58317
#% 90639
#% 100009
#% 104438
#% 137471
#% 157884
#% 157893
#% 157894
#% 209427
#% 223368
#% 435142
#% 442787
#% 444875

#index 232668
#* Textual context analysis for information retrieval
#@ Mark A. Stairmand
#t 1997
#c 13
#% 9197
#% 27049
#% 46803
#% 78171
#% 131434
#% 144012
#% 158687
#% 169729
#% 169768
#% 169809
#% 169811
#% 201216
#% 648114
#% 740329
#% 748583
#% 748631
#% 756179

#index 232670
#* Effective use of natural language processing techniques for automatic conflation of multi-word terms: the role of derivational morphology, part of speech tagging, and shallow parsing
#@ Evelyne Tzoukermann;Judith L. Klavans;Christian Jacquemin
#t 1997
#c 13

#index 232673
#* Guessing morphology from terms and corpora
#@ Christian Jacquemin
#t 1997
#c 13
#% 144034
#% 169766
#% 206404
#% 208020
#% 208934
#% 317975
#% 375017
#% 746866

#index 232676
#* Optimal demand-oriented topology for hypertext systems
#@ Scott Aaronson
#t 1997
#c 13
#% 89355
#% 103311
#% 103312
#% 103327
#% 134526
#% 148202
#% 361047
#% 369236
#% 381322
#% 656701

#index 232677
#* Passage retrieval revisited
#@ Marcin Kaszkiel;Justin Zobel
#t 1997
#c 13
#% 67565
#% 144011
#% 144012
#% 169809
#% 169811
#% 169813
#% 184491
#% 185255
#% 212665
#% 213786
#% 218982
#% 290703
#% 459007
#! Ranking based on passages addresses some of the shortcomings ofwhole-document ranking. It provides convenient units of text toreturn to the user, avoids the difficulties of comparing documentsof different length, and enables identification of short blocks ofrelevant material amongst otherwise irrelevant text. In this paperwe explore the potential of passage retrieval, based on anexperimental evaluation of the ability of passages to identifyrelevant documents. We compare our scheme of arbitrary passageretrieval to several other document retrieval and passage retrievalmethods; we show experimentally that, compared to these methods,ranking via fixed-length passages is robust and effective. Ourexperiments also show that, compared to whole-document ranking,ranking via fixed-length arbitrary passages significantly improvesretrieval effectiveness, by 8% for TREC disks 2 and 4 and by18%-37% for the Federal Register collection.

#index 232680
#* Exploration of text collections with hierarchical feature maps
#@ Dieter Merkl
#t 1997
#c 13
#% 35674
#% 36672
#% 41096
#% 46809
#% 64716
#% 67565
#% 72418
#% 92144
#% 109213
#% 120110
#% 136369
#% 157549
#% 166644
#% 176705
#% 200397
#% 234978
#% 380342
#% 477498
#% 493076
#% 493559
#% 493560
#% 497501
#% 564088

#index 232684
#* Users' perception of the performance of a filtering system
#@ Raya Fidel;Michael Crandall
#t 1997
#c 13
#% 109202
#% 124004
#% 124007
#% 159108
#% 162461
#% 167557
#% 341163
#% 463734

#index 232685
#* Time, relevance and interaction modelling for information retrieval
#@ M. D. Dunlop
#t 1997
#c 13
#% 8964
#% 142564
#% 142615
#% 172790
#% 184490
#% 206451
#% 319640
#% 360717
#% 375017
#% 394790
#% 406493
#% 407891

#index 232688
#* How to read less and know more: approximate OCR for Thai
#@ Doug Cooper
#t 1997
#c 13
#% 169778
#% 625311
#% 756827
#% 757827

#index 232692
#* Overlapping statistical word indexing: a new indexing method for Japanese text
#@ Yasushi Ogawa;Toru Matsuda
#t 1997
#c 13
#% 66085
#% 115462
#% 144042
#% 144044
#% 169781
#% 194263
#% 194265
#% 219044
#% 219045
#% 406493
#% 481774
#% 677307
#% 748591

#index 232696
#* Effectiveness of a graphical display of retrieval results
#@ Aravindan Veerasamy;Russell Heikes
#t 1997
#c 13
#% 96288
#% 109199
#% 142621
#% 142622
#% 169783
#% 201992
#% 202027
#% 218994
#% 233948
#% 242385

#index 232698
#* Cat-a-Cone: an interactive interface for specifying searches and viewing retrieval results using a large category hierarchy
#@ Marti A. Hearst;Chandu Karadi
#t 1997
#c 13
#% 56449
#% 60987
#% 67565
#% 85447
#% 109199
#% 109200
#% 109213
#% 118771
#% 118772
#% 127574
#% 129664
#% 137610
#% 142618
#% 144053
#% 151477
#% 152957
#% 162452
#% 169718
#% 169783
#% 173424
#% 201992
#% 206402
#% 207836
#% 214669
#% 217251
#% 221978
#% 227782
#% 229436
#% 232679
#% 275773
#% 363038
#% 565900
#% 619859
#% 641060

#index 232701
#* A probabilistic model for distributed information retrieval
#@ Christoph Baumgarten
#t 1997
#c 13

#index 232703
#* Analyses of multiple evidence combination
#@ Joon Ho Lee
#t 1997
#c 13
#% 111303
#% 144010
#% 144076
#% 169774
#% 184496
#% 194276
#% 648773

#index 232705
#* Image retrieval by appearance
#@ S. Ravela;R. Manmatha
#t 1997
#c 13
#% 21189
#% 71174
#% 120270
#% 180669
#% 192795
#% 208708
#% 212689
#% 362799
#% 363808
#% 437404
#% 437405
#% 437409
#% 457641
#% 592191
#% 625132
#% 627093
#% 627095

#index 232707
#* Using semantic contents and WordNet in image retrieval
#@ Y. Alp Aslandogan;Chuck Thier;Clement T. Yu;Jon Zou;Naphtali Rishe
#t 1997
#c 13
#% 55490
#% 86376
#% 124466
#% 144031
#% 165458
#% 169940
#% 183355
#% 183432
#% 211510
#% 218982
#% 219036
#% 219840
#% 219847
#% 220106
#% 220107
#% 220110
#% 437407
#% 437408
#% 437509
#% 452795
#% 464046
#% 481283

#index 232710
#* Image retrieval by hypertext links
#@ V. Harmandas;M. Sanderson;M. D. Dunlop
#t 1997
#c 13
#% 41669
#% 142615
#% 142617
#% 151407
#% 197843
#% 210684
#% 360717
#% 375017
#% 457672
#% 676177

#index 232713
#* Automatic feedback using past queries: social searching?
#@ Larry Fitzpatrick;Mei Dent
#t 1997
#c 13
#% 159108
#% 169803
#% 169806
#% 194299

#index 232717
#* Exploiting clustering and phrases for context-based information retrieval
#@ Peter G. Anick;Shivakumar Vaithyanathan
#t 1997
#c 13
#% 25938
#% 36672
#% 46809
#% 56828
#% 65964
#% 67565
#% 86531
#% 87101
#% 118728
#% 118771
#% 144010
#% 144027
#% 169729
#% 169739
#% 169766
#% 169819
#% 196896
#% 211526
#% 218992
#% 375017
#% 676640
#% 748689

#index 232719
#* The potential and actual effectiveness of interactive query expansion
#@ Mark Magennis;Cornelis J. van Rijsbergen
#t 1997
#c 13
#% 35938
#% 45311
#% 45314
#% 54435
#% 87766
#% 118726
#% 133893
#% 157884
#% 169806
#% 214709
#% 448725

#index 249841
#* Real life information retrieval (panel): commercial search engines
#@ Michael Lesk;Doug Cutting;Jan Pedersen;Terry Noreault;Matt Koll
#t 1997
#c 13

#index 249842
#* ACHIRA (abstracts): automatic construction of hypertexts for information retrieval applications
#@ M. Agosti;L. Benfante;M. Melucci
#t 1997
#c 13

#index 249843
#* Semantic search and semantic categorization (abstracts)
#@ Hsinchun Chen;Andrea L. Houston;Robin R. Sewell;Bruce R. Schatz
#t 1997
#c 13

#index 249844
#* Visual SOM (abstract)
#@ Hsinchun Chen;Marshall Ramsey;Terry R. Smith
#t 1997
#c 13

#index 262036
#* Advantages of query biased summaries in information retrieval
#@ Anastasios Tombros;Mark Sanderson
#t 1998
#c 13
#% 71752
#% 81589
#% 169770
#% 169809
#% 194251
#% 194252
#% 198294
#% 230530
#% 288614
#% 324101
#% 1478826

#index 262037
#* A theory of term weighting based on exploratory data analysis
#@ Warren R. Greiff
#t 1998
#c 13
#% 54417
#% 57484
#% 111304
#% 118756
#% 169780
#% 232645
#% 289340
#% 375017
#% 406493

#index 262039
#* New techniques for open-vocabulary spoken document retrieval
#@ Martin Wechsler;Eugen Munteanu;Peter Schäuble
#t 1998
#c 13
#% 118751
#% 137711
#% 184488
#% 194297
#% 218982
#% 218984
#% 219917
#% 232660
#% 358722
#% 405882
#% 627051
#% 968469
#% 1783201
#% 1783212

#index 262042
#* A study of retrospective and on-line event detection
#@ Yiming Yang;Tom Pierce;Jaime Carbonell
#t 1998
#c 13
#% 18713
#% 46809
#% 54221
#% 60987
#% 67565
#% 118771
#% 219048
#% 375017

#index 262043
#* On-line new event detection and tracking
#@ James Allan;Ron Papka;Victor Lavrenko
#t 1998
#c 13
#% 46809
#% 55490
#% 118730
#% 118736
#% 184489
#% 188076
#% 194301
#% 219048
#% 219049
#% 219052
#% 219054
#% 232646
#% 237788
#% 262042
#% 375017
#% 742145
#% 1290045

#index 262045
#* Web document clustering: a feasibility demonstration
#@ Oren Zamir;Oren Etzioni
#t 1998
#c 13
#% 18713
#% 46809
#% 115478
#% 118771
#% 144023
#% 151477
#% 218992
#% 232651
#% 232655
#% 235941
#% 375017

#index 262046
#* The effects of query structure and dictionary setups in dictionary-based cross-language information retrieval
#@ Ari Pirkola
#t 1998
#c 13
#% 217207
#% 218988
#% 218989
#% 219096
#% 232656
#% 504888
#% 562054
#% 855000

#index 262047
#* Resolving ambiguity for cross-language retrieval
#@ Lisa Ballesteros;W. Bruce Croft
#t 1998
#c 13
#% 86371
#% 184489
#% 211043
#% 218978
#% 218988
#% 218989
#% 232656
#% 232658
#% 241238
#% 289079
#% 504888
#% 562054
#% 748337
#% 748444

#index 262048
#* Cross-language information retrieval with the UMLS metathesaurus
#@ David Eichmann;Miguel E. Ruiz;Padmini Srinivasan
#t 1998
#c 13
#% 169777
#% 217207
#% 218988
#% 218989
#% 232656
#% 232660
#% 562054
#% 840583

#index 262050
#* Using a generalized instance set for automatic text categorization
#@ Wai Lam;Chao Yang Ho
#t 1998
#c 13
#% 1527
#% 118736
#% 165110
#% 169718
#% 219052
#% 219053
#% 840583

#index 262054
#* Automatic essay grading using text categorization techniques
#@ Leah S. Larkey
#t 1998
#c 13
#% 5182
#% 73046
#% 109192
#% 118731
#% 118736
#% 127850
#% 144034
#% 184489
#% 219051
#% 219052
#% 288211
#% 375017

#index 262056
#* The future of Internet search (keynote address)
#@ Steve Kirsch
#t 1998
#c 13

#index 262059
#* Distributional clustering of words for text classification
#@ L. Douglas Baker;Andrew Kachites McCallum
#t 1998
#c 13
#% 115608
#% 158687
#% 194285
#% 230532
#% 241769
#% 246831
#% 266215
#% 420054
#% 465754
#% 465895
#% 618434
#% 748465
#% 748619

#index 262061
#* Improved algorithms for topic distillation in a hyperlinked environment
#@ Krishna Bharat;Monika R. Henzinger
#t 1998
#c 13
#% 46803
#% 54435
#% 118771
#% 144027
#% 169819
#% 214673
#% 232698
#% 232717
#% 232719
#% 268073
#% 268186
#% 282905
#% 676640

#index 262063
#* Effective retrieval with distributed collections
#@ Jinxi Xu;Jamie Callan
#t 1998
#c 13
#% 111456
#% 144074
#% 172898
#% 184486
#% 184489
#% 194244
#% 194245
#% 194246
#% 218978
#% 232701
#% 481748
#% 672628
#% 677173

#index 262065
#* Evaluating database selection techniques: a testbed and experiment
#@ James C. French;Allison L. Powell;Charles L. Viles;Travis Emmitt;Kevin J. Prey
#t 1998
#c 13
#% 172898
#% 184496
#% 194245
#% 194246
#% 194275
#% 199772
#% 481748
#% 854961

#index 262067
#* The impact of query structure and query expansion on retrieval performance
#@ Jaana Kekäläinen;Kalervo Järvelin
#t 1998
#c 13
#% 3004
#% 86528
#% 111456
#% 118726
#% 144074
#% 144076
#% 157884
#% 169729
#% 169806
#% 184487
#% 184496
#% 187756
#% 187773
#% 208934
#% 208935
#% 218978
#% 223368
#% 262046
#% 319273

#index 262069
#* A flexible model for retrieval of SGML documents
#@ Sung Hyon Myaeng;Don-Hyun Jang;Mun-Seok Kim;Zong-Cheol Zhoo
#t 1998
#c 13
#% 56449
#% 90844
#% 111303
#% 169811
#% 194254
#% 204662
#% 219047
#% 232662
#% 232677
#% 319273
#% 464230

#index 262071
#* Discovering typical structures of documents: a road map approach
#@ Ke Wang;Huiqing Liu
#t 1998
#c 13
#% 152934
#% 210214
#% 340295
#% 462062
#% 463919
#% 464720
#% 481290

#index 262073
#* A cognitive model for searching for III-defined targets on the Web: the relationship between search strategies and user satisfaction
#@ Mari Saito;Kazunori Ohmura
#t 1998
#c 13
#% 1260
#% 43017
#% 201993
#% 218322
#% 229436

#index 262075
#* Comparing interactive information retrieval systems across sites: the TREC-6 interactive track matrix experiment
#@ Eric Lagergren;Paul Over
#t 1998
#c 13
#% 83853
#% 133892
#% 144074
#% 262076

#index 262076
#* Aspect windows, 3-D visualizations, and indirect comparisons of information retrieval systems
#@ Russell C. Swan;James Allan
#t 1998
#c 13
#% 55700
#% 118772
#% 122797
#% 194270
#% 201992
#% 218992
#% 218994
#% 219024
#% 238081
#% 245840
#% 262075
#% 262130
#% 375017

#index 262078
#* Modeling and combining evidence provided by document relationships using probabilistic argumentation systems
#@ Justin Picard
#t 1998
#c 13
#% 3460
#% 25942
#% 44876
#% 64897
#% 65953
#% 111303
#% 118756
#% 120103
#% 142617
#% 169780
#% 184496
#% 194276
#% 224692
#% 232710
#% 287307
#% 735177

#index 262080
#* Predicting the performance of linearly combined IR systems
#@ Christopher C. Vogt;Garrison W. Cottrell
#t 1998
#c 13
#% 132779
#% 169774
#% 174664
#% 184486
#% 184496
#% 232703

#index 262081
#* Experiments in Japanese text retrieval and routing using the NEAT system
#@ Gareth J. F. Jones;Tetsuya Sakai;Masahiro Kajiura;Kazuo Sumita
#t 1998
#c 13
#% 92696
#% 144044
#% 169781
#% 169803
#% 184496
#% 194263
#% 194265
#% 218984
#% 219044
#% 219045
#% 219049
#% 232692
#% 481774
#% 595892
#% 677232

#index 262084
#* Improving automatic query expansion
#@ Mandar Mitra;Amit Singhal;Chris Buckley
#t 1998
#c 13
#% 218978
#% 218982
#% 319273

#index 262085
#* Boosting and Rocchio applied to text filtering
#@ Robert E. Schapire;Yoram Singer;Amit Singhal
#t 1998
#c 13
#% 67565
#% 118731
#% 169717
#% 169718
#% 169719
#% 169781
#% 194284
#% 194285
#% 194301
#% 218982
#% 219048
#% 219049
#% 219050
#% 219052
#% 219053
#% 232646
#% 232653
#% 232728
#% 235377
#% 321635
#% 375017
#% 406493
#% 458379
#% 465746
#% 565531
#% 817970
#% 1499573

#index 262087
#* Learning while filtering documents
#@ Jamie Callan
#t 1998
#c 13
#% 65956
#% 124009
#% 169781
#% 194301
#% 218979
#% 219048
#% 219049
#% 219052
#% 682442
#% 978507

#index 262089
#* Spatial querying for image retrieval: a user-oriented evaluation
#@ Joemon M. Jose;Jonathan Furner;David J. Harper
#t 1998
#c 13
#% 133888
#% 133892
#% 167563
#% 194269
#% 206449
#% 208927
#% 208932
#% 208935
#% 218994
#% 219024
#% 228351
#% 236048
#% 374663
#% 437404
#% 562344

#index 262090
#* Extracting classification knowledge of Internet documents with mining term associations: a semantic approach
#@ Shian-Hua Lin;Chi-Sheng Shih;Meng Chang Chen;Jan-Ming Ho;Ming-Tat Ko;Yueh-Ming Huang
#t 1998
#c 13
#% 55490
#% 84654
#% 86532
#% 115462
#% 152934
#% 165110
#% 210160
#% 232650
#% 237052
#% 415986
#% 481290
#% 677703

#index 262092
#* Improving two-stage ad-hoc retrieval for short queries
#@ K. L. Kwok;M. Chan
#t 1998
#c 13
#% 169729
#% 183255
#% 218978
#% 219041
#% 232646
#% 375017

#index 262094
#* DOLORES: a system for logic-based retrieval of multimedia objects
#@ Norbert Fuhr;Norbert Gövert;Thomas Rölleke
#t 1998
#c 13
#% 65953
#% 83336
#% 111871
#% 118759
#% 120106
#% 176471
#% 194249
#% 194277
#% 214702
#% 219043
#% 232662
#% 243161
#% 437405
#% 458744
#% 648114

#index 262095
#* RELIEF: combining expressiveness and rapidity into a single system
#@ Iadh Ounis;Marius Paşca
#t 1998
#c 13
#% 2298
#% 120103
#% 144069
#% 194247
#% 227802
#% 406493
#% 465640
#% 586815
#% 637752
#% 1783116
#% 1783131

#index 262096
#* A language modeling approach to information retrieval
#@ Jay M. Ponte;W. Bruce Croft
#t 1998
#c 13
#% 55490
#% 73045
#% 73046
#% 142400
#% 169781
#% 219041
#% 504890

#index 262097
#* Efficient construction of large test collections
#@ Gordon V. Cormack;Christopher R. Palmer;Charles L. A. Clarke
#t 1998
#c 13
#% 144010
#% 184486
#% 219096
#% 262102
#% 262105

#index 262099
#* Compressed inverted files with reduced decoding overheads
#@ Anh Ngoc Vo;Alistair Moffat
#t 1998
#c 13
#% 40395
#% 67565
#% 70370
#% 115462
#% 184486
#% 198335
#% 212665
#% 213786
#% 228097
#% 290703
#% 406493
#% 587845

#index 262101
#* Fast searching on compressed text allowing errors
#@ Edleno Silva de Moura;Gonzalo Navarro;Nivio Ziviani;Ricardo Baeza-Yates
#t 1998
#c 13
#% 3244
#% 57849
#% 68073
#% 69506
#% 91245
#% 120648
#% 120649
#% 121231
#% 162462
#% 197695
#% 203282
#% 207561
#% 223799
#% 240024
#% 324012
#% 375076
#% 481492
#% 546270

#index 262102
#* How reliable are the results of large-scale information retrieval experiments?
#@ Justin Zobel
#t 1998
#c 13
#% 57485
#% 133889
#% 133892
#% 184486
#% 184491
#% 204294
#% 208929
#% 208931
#% 236052
#! Two stages in measurement of techniques for informationretrieval are gathering of documents for relevance assessment anduse of the assessments to numerically evaluate effectiveness. Weconsider both of these stages in the context of the TRECexperiments, to determine whether they lead to measurements thatare trustworthy and fair. Our detailed empirical investigation ofthe TREC results shows that the measured relative performance ofsystems appears to be reliable, but that recall is overestimated:it is likely that many relevant documents have not been found. Wepropose a new pooling strategy that can significantly in- creasethe number of relevant documents found for given effort, withoutcompromising fairness.

#index 262105
#* Variations in relevance judgments and the measurement of retrieval effectiveness
#@ Ellen M. Voorhees
#t 1998
#c 13
#% 129694
#% 153019
#% 208931
#% 374663
#% 840583

#index 262107
#* Measures of relative relevance and ranked half-life: performance indicators for interactive IR
#@ Pia Borlund;Peter Ingwersen
#t 1998
#c 13
#% 133891
#% 133893
#% 137475
#% 144010
#% 170465
#% 184486
#% 194269
#% 204294
#% 219024
#% 248073
#% 248078
#% 375017

#index 262109
#* Tools for searching the Web (panel)
#@ Donna Harman;Paul Over
#t 1998
#c 13

#index 262111
#* Modern classical document indexing: a linguistic contribution to knowledge-based IR
#@ Bas van Bakel
#t 1998
#c 13

#index 262112
#* The use of MMR, diversity-based reranking for reordering documents and producing summaries
#@ Jaime Carbonell;Jade Goldstein
#t 1998
#c 13
#% 67565
#% 194251
#% 648114

#index 262114
#* A method for scoring correlated features in query expansion
#@ Martin Franz;Salim Roukos
#t 1998
#c 13
#% 218978
#% 289079
#% 406493

#index 262115
#* Using maps as a user interface to a digital library
#@ Mountaz Hascoët;Xavier Soinard
#t 1998
#c 13
#% 201214

#index 262117
#* Comparison between proximity operation and dependency operation in Japanese full-text retrieval
#@ Yasuaki Hyoudo;Kazuhiko Niimi;Takashi Ikeda
#t 1998
#c 13

#index 262118
#* Term-ordered query evaluation versus document-ordered query evaluation for large document databases
#@ Marcin Kaszkiel;Justin Zobel
#t 1998
#c 13
#% 67565
#% 184486
#% 198335
#% 212665
#% 213786
#% 249989
#% 290703

#index 262120
#* Lessons from BMIR-J2: a test collection for Japanese IR systems
#@ Tsuyoshi Kitani;Yasushi Ogawa;Tetsuya Ishikawa;Haruo Kimoto;Ikuo Keshi;Jun Toyoura;Toshikazu Fukushima;Kunio Matsui;Yoshihiro Ueda;Tetsuya Sakai;Takenobu Tokunaga;Hiroshi Tsuruoka;Hidekazu Nakawatase;Teru Agata
#t 1998
#c 13
#% 219056

#index 262124
#* Automatically locating, extracting and analyzing tabular data
#@ William Kornfeld;John Wattecamps
#t 1998
#c 13
#% 3888

#index 262125
#* Using global colour features for general photographic image indexing and retrieval
#@ Ting-Sheng Lai;John Tait
#t 1998
#c 13
#% 1346975

#index 262128
#* Automatic acquisition of phrasal knowledge for English-Chinese bilingual information retrieval
#@ Ming-Jer Lee;Lee-Feng Chien
#t 1998
#c 13
#% 194263
#% 232650
#% 232656
#% 624474

#index 262130
#* Visual interactions with a multidimensional ranked list
#@ Anton Leouski;James Allan
#t 1998
#c 13
#% 118772
#% 122797
#% 218992
#% 375017

#index 262132
#* Predicting query times
#@ Rodger McNab;Yong Wang;Ian H. Witten;Carl Gutwin
#t 1998
#c 13
#% 115181

#index 262134
#* The WebCluster project. Using clustering for mediating access to the World Wide Web
#@ Mourad Mechkour;David J. Harper;Gheorghe Muresan
#t 1998
#c 13
#% 36672
#% 115478
#% 218992
#% 230024
#% 246166
#% 375017

#index 262136
#* Automatic abstracting of magazine articles: the creation of 'Highlight' abstracts
#@ Marie-Francine Moens;Jos Dumortier
#t 1998
#c 13
#% 246874

#index 262139
#* Optimizing recall/precision scores in IR over the WWW
#@ Matthew Montebello
#t 1998
#c 13
#% 406493
#% 511733
#% 1499473

#index 262141
#* Interactive multidimensional document visualization
#@ Josiane Mothe;Taoufiq Dkaki
#t 1998
#c 13
#% 137473
#% 218990
#% 232102

#index 262142
#* Speech retrieval using phonemes with error correction
#@ Corinna Ng;Justin Zobel
#t 1998
#c 13
#% 219033
#% 290703
#% 1783212

#index 262144
#* Optimizing query evaluation in n-gram indexing
#@ Yasushi Ogawa;Toru Matsuda
#t 1998
#c 13
#% 194247
#% 198335

#index 262146
#* Four text classification algorithms compared on a Dutch corpus
#@ Hein Ragas;Cornelis H. A. Koster
#t 1998
#c 13
#% 219053

#index 262147
#* Automatic acquisition of terminological relations from a corpus for query expansion
#@ Jean-David Sta
#t 1998
#c 13
#% 1449
#% 54435
#% 132648
#% 756228

#index 262149
#* Keyword extraction of radio news using term weighting with an encyclopedia and newspaper articles
#@ Fumiyo Fukumoto;Yoshihiro Sekiguchi;Yoshimi Suzuki
#t 1998
#c 13
#% 624545

#index 262151
#* Efficient search server assignment in a disproportionate system environment
#@ Toru Takaki;Tsuyoshi Kitani
#t 1998
#c 13
#% 219013
#% 677062

#index 262153
#* Multilingual keyword extraction for term suggestion
#@ Yuen-Hsien Tseng
#t 1998
#c 13
#% 115462
#% 206402

#index 262155
#* Experiments of collecting WWW information using distributed WWW robots
#@ Hayato Yamana;Kent Tamura;Hiroyuki Kawano;Satoshi Kamei;Masanori Harada;Hideki Nishimura;Isao Asai;Hiroyuki Kusumoto;Yoichi Shinoda;Yoichi Muraoka
#t 1998
#c 13

#index 262157
#* Presenting Web site search results in context: a demonstration
#@ Michael Chen;Marti A. Hearst
#t 1998
#c 13
#% 58866
#% 856157

#index 262158
#* Towards a fast precision-oriented image retrieval system
#@ Yves Chiaramella;Philippe Mulhem;Mourad Mechkour;Iadh Ounis;Marius Paşca
#t 1998
#c 13
#% 2298
#% 262095
#% 563749

#index 262159
#* Cheshire II: combining probabilistic and Boolean retrieval
#@ Ray R. Larson
#t 1998
#c 13
#% 206512

#index 262160
#* Teraphim: an engine for distributed information retrieval
#@ Owen de Kretser;Alistair Moffat;Justin Zobel
#t 1998
#c 13
#% 162462
#% 179161
#% 184491
#% 188587
#% 197695
#% 212665
#% 213786
#% 253191
#% 443121
#% 635851

#index 262161
#* Personal browser
#@ Yi-Shiou Chen;Schy Chiou;Yuan-Kai Wang;Wen-Lian Hsu
#t 1998
#c 13
#% 165110
#% 194244
#% 201073
#% 218979

#index 262162
#* A research prototype image retrieval system
#@ S. Nepal;M. V. Ramakrishna;J. A. Thom
#t 1998
#c 13
#% 213673
#% 437405
#% 443053
#% 452797

#index 262163
#* The structured information manager (SIM)
#@ Ron Sacks-Davis;Alan Kent
#t 1998
#c 13

#index 262164
#* PWA: an extended probabilistic Web algebra
#@ Dan Smith;Rattasit Sukhahuta
#t 1998
#c 13
#% 209725
#% 442830

#index 262165
#* Cafe: an indexed approach to searching genomic databases
#@ Hugh E. Williams
#t 1998
#c 13
#% 459007

#index 262166
#* Fast speculative search engine on the highly parallel computer EM-X
#@ Hayato Yamana;Hanpei Koike;Yuetsu Kodama;Hirofumi Sakane;Yoshinori Yamaguchi
#t 1998
#c 13
#% 202065
#% 202629
#% 219617

#index 280793
#* “User revealment”—a comparison of initial queries and ensuing question development in online searching and in human reference interactions
#@ Ragnar Nordlie
#t 1999
#c 13
#% 36402
#% 115181
#% 116374
#% 144918
#% 186518
#% 206505
#% 206512

#index 280797
#* Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval
#@ Fredric Gey;Marti Hearst;Richard Tong
#t 1999
#c 13

#index 280806
#* Visualization of search results: a comparative evaluation of text, 2D, and 3D interfaces
#@ Marc M. Sebrechts;John V. Cugini;Sharon J. Laskowski;Joanna Vasilakis;Michael S. Miller
#t 1999
#c 13
#% 151477
#% 173424
#% 202036
#% 218990
#% 218994
#% 232683
#% 232696
#% 232698
#% 262076
#% 287606

#index 280809
#* From reading to retrieval: freeform ink annotations as queries
#@ Gene Golovchinsky;Morgan N. Price;Bill N. Schilit
#t 1999
#c 13
#% 37743
#% 86371
#% 118726
#% 133893
#% 144011
#% 144074
#% 148007
#% 169809
#% 184486
#% 223789
#% 232895
#% 237318
#% 238195
#% 240738
#% 240744
#% 247296
#% 247297
#% 249089
#% 249158
#% 366631

#index 280811
#* SCAN: designing and evaluating user interfaces to support retrieval from speech archives
#@ Steve Whittaker;Julia Hirschberg;John Choi;Don Hindle;Fernando Pereira;Amit Singhal
#t 1999
#c 13
#% 172764
#% 172800
#% 194223
#% 201992
#% 218984
#% 231483
#% 232814
#% 239594
#% 258236
#% 258237
#% 318462
#% 434737
#% 648114
#% 700586
#% 840583
#% 855048

#index 280815
#* Document expansion for speech retrieval
#@ Amit Singhal;Fernando Pereira
#t 1999
#c 13
#% 11646
#% 25942
#% 46809
#% 54431
#% 73028
#% 100958
#% 137711
#% 144029
#% 162362
#% 169781
#% 218984
#% 237291
#% 240038
#% 262039
#% 288306
#% 288541
#% 855048

#index 280817
#* A re-examination of text categorization methods
#@ Yiming Yang;Xin Liu
#t 1999
#c 13
#% 118736
#% 144009
#% 165111
#% 169718
#% 169719
#% 190581
#% 194289
#% 197394
#% 219052
#% 219053
#% 232653
#% 262050
#% 262059
#% 318412
#% 375017
#% 376266
#% 458379
#% 461692
#% 465747
#% 465754
#% 669214

#index 280819
#* Probabilistic latent semantic indexing
#@ Thomas Hofmann
#t 1999
#c 13
#% 78792
#% 262080
#% 277483
#% 304908
#% 406493
#% 495929
#% 748465
#% 1650298

#index 280822
#* A similarity-based probability model for latent semantic indexing
#@ Chris H. Q. Ding
#t 1999
#c 13
#% 46803
#% 120104
#% 132648
#% 187772
#% 194285
#% 200694
#% 204298
#% 248027
#% 375017
#% 406493
#% 485556

#index 280824
#* Using a belief revision operator for document ranking in extended Boolean models
#@ David E. Losada;Alvaro Barreiro
#t 1999
#c 13
#% 109945
#% 144069
#% 169776
#% 194290
#% 194291
#% 203773
#% 231750
#% 259459
#% 259460
#% 319273
#% 384634
#% 480008

#index 280826
#* Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web
#@ Jian-Yun Nie;Michel Simard;Pierre Isabelle;Richard Durand
#t 1999
#c 13
#% 232658
#% 262047
#% 262870
#% 384972
#% 406493
#% 419841
#% 648114
#% 740901
#% 740915

#index 280829
#* A new statistical formula for Chinese text segmentation incorporating contextual information
#@ Yubin Dai;Teck Ee Loh;Christopher S. G. Khoo
#t 1999
#c 13
#% 153023
#% 187759
#% 216017
#% 232692
#% 748499

#index 280831
#* Information retrieval based on context distance and morphology
#@ Hongyan Jing;Evelyne Tzoukermann
#t 1999
#c 13
#% 131320
#% 144034
#% 194294
#% 208934
#% 218978
#% 741083
#% 741875
#% 748703
#% 817846

#index 280832
#* Partial replica selection based on relevance for information retrieval
#@ Zhihong Lu;Kathryn S. McKinley
#t 1999
#c 13
#% 59265
#% 109209
#% 123074
#% 172898
#% 194244
#% 194246
#% 194275
#% 209654
#% 318016
#% 481748
#% 610649
#% 616995
#% 628061
#% 646351
#% 708700

#index 280833
#* Efficient distributed algorithms to build inverted files
#@ Berthier Ribeiro-Neto;Edleno S. Moura;Marden S. Neubert;Nivio Ziviani
#t 1999
#c 13
#% 129023
#% 169817
#% 188587
#% 212665
#% 216513
#% 249153
#% 290703
#% 375076
#% 439903
#% 616169

#index 280834
#* Effective document presentation with a locality-based similarity heuristic
#@ Owen de Kretser;Alistair Moffat
#t 1999
#c 13
#% 115462
#% 184486
#% 184491
#% 212665
#% 213786
#% 218976
#% 218982
#% 218990
#% 218992
#% 218994
#% 228097
#% 232642
#% 232677
#% 232696
#% 234793
#% 253191
#% 262099
#% 290703
#% 375017
#% 406493

#index 280835
#* Summarizing text documents: sentence selection and evaluation metrics
#@ Jade Goldstein;Mark Kantrowitz;Vibhu Mittal;Jaime Carbonell
#t 1999
#c 13
#% 71752
#% 198296
#% 218978
#% 262112
#% 283177
#% 381263
#% 406493
#% 648114
#% 741106
#% 748577
#% 855043

#index 280836
#* The decomposition of human-written summary sentences
#@ Hongyan Jing;Kathleen R. McKeown
#t 1999
#c 13
#% 280838
#% 748342
#% 748343

#index 280838
#* The automatic construction of large-scale corpora for summarization research
#@ Daniel Marcu
#t 1999
#c 13
#% 194251
#% 266370
#% 288614
#% 708199
#% 708427
#% 741058
#% 746875

#index 280839
#* Phrase recognition and expansion for short, precision-biased queries based on a query log
#@ Erika F. de Lima;Jan O. Pedersen
#t 1999
#c 13
#% 67565
#% 253188
#% 740916
#% 742368
#% 746866
#% 748561
#% 748722
#% 854961
#% 855048
#% 1478822

#index 280840
#* The paraphrase search assistant: terminological feedback for iterative information seeking
#@ Peter G. Anick;Suresh Tipirneni
#t 1999
#c 13
#% 25938
#% 54435
#% 56828
#% 67565
#% 78171
#% 86380
#% 118739
#% 169766
#% 185254
#% 206402
#% 214709
#% 231567
#% 232717
#% 237340
#% 262153
#% 747670
#% 756842

#index 280841
#* Phrasier: a system for interactive document retrieval using keyphrases
#@ Steve Jones;Mark S. Staveley
#t 1999
#c 13
#% 67565
#% 109190
#% 115446
#% 230519
#% 230521
#% 230530
#% 230538
#% 232717
#% 240738
#% 252750
#% 253188
#% 290703
#% 438055
#% 495937
#% 635851
#% 1783134

#index 280843
#* Content-based retrieval using heuristic search
#@ Dimitris Papadias;Marios Mantzourogiannis;Panos Kalnis;Nikos Mamoulis;Ishfaq Ahmad
#t 1999
#c 13
#% 10658
#% 75299
#% 86949
#% 114994
#% 116335
#% 181409
#% 219847
#% 228351
#% 260019
#% 261903
#% 285421
#% 319244
#% 369236
#% 443054
#% 478645
#% 479620
#% 534159

#index 280845
#* Content-based retrieval for music collections
#@ Yuen-Hsien Tseng
#t 1999
#c 13
#% 90659
#% 115462
#% 120649
#% 185287
#% 194192
#% 204646
#% 219841
#% 260018
#% 261882
#% 262153
#% 434753
#% 504885

#index 280846
#* Relevance feedback retrieval of time series data
#@ Eamonn J. Keogh;Michael J. Pazzani
#t 1999
#c 13
#% 172949
#% 211487
#% 460862
#% 461885
#% 481609
#% 661026

#index 280847
#* Combining multiple evidence from different types of thesaurus for query expansion
#@ Rila Mandala;Takenobu Tokunaga;Hozumi Tanaka
#t 1999
#c 13
#% 85447
#% 118739
#% 129655
#% 134884
#% 144012
#% 144029
#% 144031
#% 169729
#% 169806
#% 185288
#% 229348
#% 363038
#% 406493
#% 741058
#% 748583
#% 748691
#% 834876
#% 840583
#% 855048
#% 1275285

#index 280848
#* Context-sensitive vocabulary mapping with a spreading activation network
#@ Jonghoon Lee;David Dubin
#t 1999
#c 13
#% 1484
#% 1664
#% 25942
#% 65946
#% 65947
#% 92135
#% 109206
#% 137471
#% 144020
#% 167237
#% 219040
#% 232667
#% 234793
#% 252328
#% 348319
#% 689963

#index 280849
#* Deriving concept hierarchies from text
#@ Mark Sanderson;Bruce Croft
#t 1999
#c 13
#% 4143
#% 60987
#% 198058
#% 214711
#% 218978
#% 218992
#% 288221
#% 363038
#% 375017
#% 478262
#% 742724
#% 748550
#% 748703

#index 280850
#* A hidden Markov model information retrieval system
#@ David R. H. Miller;Tim Leek;Richard M. Schwartz
#t 1999
#c 13
#% 219053
#% 262085
#% 262096
#% 288166
#% 742424
#% 854961
#% 855048

#index 280851
#* Information retrieval as statistical translation
#@ Adam Berger;John Lafferty
#t 1999
#c 13
#% 46803
#% 81669
#% 99650
#% 262096
#% 300542
#% 593898
#% 740915
#% 817944

#index 280852
#* An algorithmic framework for performing collaborative filtering
#@ Jonathan L. Herlocker;Joseph A. Konstan;Al Borchers;John Riedl
#t 1999
#c 13
#% 4178
#% 46803
#% 124010
#% 159108
#% 169803
#% 173879
#% 202009
#% 202010
#% 202011
#% 220707
#% 220708
#% 220709
#% 220711
#% 260778
#% 266281
#% 375017
#% 465928
#% 641060
#% 1650569

#index 280853
#* Comparing the performance of database selection algorithms
#@ James C. French;Allison L. Powell;Jamie Callan;Charles L. Viles;Travis Emmitt;Kevin J. Prey;Yun Mou
#t 1999
#c 13
#% 172898
#% 184488
#% 184489
#% 184496
#% 194245
#% 194246
#% 194275
#% 218982
#% 236256
#% 262063
#% 262065
#% 282422
#% 287463
#% 481748
#% 567255
#% 854961

#index 280854
#* A probabilistic solution to the selection and fusion problem in distributed information retrieval
#@ Christoph Baumgarten
#t 1999
#c 13
#% 118759
#% 144010
#% 194246
#% 232701
#% 262065
#% 358722
#% 406493
#% 479642
#% 481748

#index 280856
#* Cluster-based language models for distributed retrieval
#@ Jinxi Xu;W. Bruce Croft
#t 1999
#c 13
#% 36672
#% 109209
#% 118771
#% 144074
#% 172898
#% 194244
#% 194246
#% 211526
#% 218992
#% 232651
#% 262063
#% 262065
#% 262096
#% 267454
#% 375017
#% 682332
#% 1830073

#index 280857
#* Adaptive cluster-based browsing using incrementally expanded queries and its effects (poster abstract)
#@ Koji Eguchi
#t 1999
#c 13
#% 218992
#% 511647

#index 280858
#* A comparison of query translation methods for English-Japanese cross-language information retrieval (poster abstract)
#@ Gareth Jones;Tetsuya Sakai;Nigel Collier;Akira Kumano;Kazuo Sumita
#t 1999
#c 13
#% 218988
#% 262047
#% 262081
#% 262120

#index 280859
#* Estimating precision by random sampling (poster abstract)
#@ Gordon V. Cormack;Ondrej Lhotak;Christopher R. Palmer
#t 1999
#c 13
#% 262097

#index 280860
#* Evaluating a visualisation of image similarity (poster abstract)
#@ Kerry Rodden;Wojciech Basalaj;David Sinclair;Kenneth Wood
#t 1999
#c 13
#% 262089
#% 641117
#% 718437

#index 280862
#* Fundamental properties of aboutness (poster abstract)
#@ Peter Bruza;Dawei Song;Kam-Fai Wong
#t 1999
#c 13
#% 109187
#% 169741
#% 187761
#% 189699
#% 220131
#% 637553

#index 280864
#* A general language model for information retrieval (poster abstract)
#@ Fei Song;W. Bruce Croft
#t 1999
#c 13
#% 262096
#% 279755
#% 280850
#% 363592

#index 280866
#* Hierarchical neural networks for text categorization (poster abstract)
#@ Miguel E. Ruiz;Padmini Srinivasan
#t 1999
#c 13
#% 169777
#% 219052
#% 232646
#% 232653
#% 669156

#index 280867
#* Improving retrieval on imperfect speech transcriptions (poster abstract)
#@ P. Jourlin;S. E. Johnson;K. Spärck Jones;P. C. Woodland
#t 1999
#c 13
#% 969345

#index 280868
#* Information seeking at different stages of the R&D research process (poster abstract)
#@ Sandra G. Hirsh
#t 1999
#c 13
#% 267749

#index 280876
#* An intelligent adaptive filtering agent based on an on-line learning model (poster abstract)
#@ Wai Lam;Kwok Leung Yu
#t 1999
#c 13
#% 129993
#% 262087
#% 682442

#index 280879
#* Interactive Internet search through automatic clustering (poster abstract): an empirical study
#@ Dmitri Roussinov;Kristine Tolle;Marshall Ramsey;Hsinchun Chen
#t 1999
#c 13
#% 214711
#% 234978
#% 249843
#% 262130

#index 280883
#* Jester 2.0 (poster abstract): evaluation of an new linear time collaborative filtering algorithm
#@ Dhruv Gupta;Mark Digiovanni;Hiro Narita;Ken Goldberg
#t 1999
#c 13
#% 124010
#% 220706
#% 220711

#index 280886
#* A knowledge management tool for speech interfaces (poster abstract)
#@ Niels Bouwmeester
#t 1999
#c 13

#index 280888
#* Machine translation and monolingual information retrieval (poster abstract)
#@ Martin Franz;J. Scott McCarley
#t 1999
#c 13
#% 218978
#% 740915
#% 748342
#% 786534
#% 855048

#index 280890
#* Music retrieval as text retrieval (poster abstract): simple yet effective
#@ J. Stephen Downie
#t 1999
#c 13
#% 204646

#index 280892
#* NACSIS test collection workshop (NTCIR-1) (poster abstract)
#@ Noriko Kando;Kazuko Kuriyama;Toshihiko Nozue
#t 1999
#c 13

#index 280894
#* A new approach for image classification and retrieval (poster abstract)
#@ Cheong Yiu Fung;Kai Fock Loe
#t 1999
#c 13
#% 261880
#% 592074

#index 280895
#* Query expansion method based on word contribution (poster abstract)
#@ Keiichiro Hoashi;Kazunori Matsumoto;Naomi Inoue;Kazuo Hashimoto
#t 1999
#c 13
#% 194301
#% 855048

#index 280899
#* Searching on the Web (poster abstract): two types of expertise
#@ Christoph Hoelscher;Gerhard Strube
#t 1999
#c 13
#% 245840
#% 253188

#index 280901
#* Searching program source code with a structured text retrieval system (poster abstract)
#@ Charles Clarke;Anthony Cox;Susan Sim
#t 1999
#c 13
#% 68701
#% 157877
#% 272455

#index 280903
#* Statistical phrases for vector-space information retrieval (poster abstract)
#@ Andrew Turpin;Alistair Moffat
#t 1999
#c 13
#% 56830
#% 218982
#% 253191
#% 280834
#% 1783134

#index 280905
#* Supporting content retrieval from WWW via “basic level categories” (poster abstract)
#@ Eduard Hoenkamp;Onno Stegeman;Lambert Schomaker
#t 1999
#c 13
#% 228351

#index 280907
#* 30,000 hits may be better than 300 (poster abstract): precision anomalies in Internet searches
#@ Caroline M. Eastman
#t 1999
#c 13
#% 253188
#% 259526

#index 280909
#* Ultra-summarization (poster abstract): a statistical approach to generating highly condensed non-extractive summaries
#@ Michael J. Witbrock;Vibhu O. Mittal
#t 1999
#c 13
#% 280835

#index 280910
#* Visual MeSH
#@ Xia Lin
#t 1999
#c 13
#% 357597

#index 280911
#* Web searching behavior of aerospace engineers (poster abstract)
#@ Raya Fidel;Efthimis N. Efthimiadis
#t 1999
#c 13
#% 161327

#index 280912
#* Advanced search technologies for unfamiliar metadata (demonstration abstract)
#@ Barbara Norgard;Youngin Kim;Michael Buckland;Aitao Chen;Ray Larson;Fred Gey
#t 1999
#c 13
#% 255434
#% 740900

#index 280915
#* Ant World (demonstration abstract)
#@ Paul Kantor;Endre Boros;Ben Melamed;Dave Neu;Vladimir Menkov;Qin Shi;Myung-Ho Kim
#t 1999
#c 13

#index 280916
#* CHROMA (demonstration abstract): a content-based image retrieval system
#@ Ting-Sheng Lai;John Tait
#t 1999
#c 13
#% 262125
#% 437405
#% 1783092

#index 280918
#* Crystal (demonstration abstract): a content-based music retrieval system
#@ Yuen-Hsien Tseng
#t 1999
#c 13
#% 194192
#% 204646
#% 280845

#index 280919
#* CueVideo (demonstration abstract): automated video/audio indexing and browsing
#@ Arnon Amir;Savitha Srinivasan;Dulce Ponceleon;Dragutin Petkovic
#t 1999
#c 13

#index 280923
#* A demonstration of WHIRL (demonstration abstract)
#@ William W. Cohen
#t 1999
#c 13
#% 55490
#% 248801
#% 252834

#index 280924
#* “Drag-and-drop” technique for MEDLINE searching (demonstration abstract)
#@ Xia Lin
#t 1999
#c 13
#% 237319
#% 357597

#index 280925
#* Extraction/gathering with the Taylor system (demonstration abstract)
#@ François Paradis;Jon Dell'Oro;Ross Wilkinson
#t 1999
#c 13
#% 228328
#% 290703
#% 534049

#index 280927
#* Information access across the language barrier (demonstration abstract): the MuST system
#@ Chin-Yew Lin
#t 1999
#c 13
#% 262036
#% 562054

#index 280928
#* Information retrieval library (IRLIB) (demonstration abstract)
#@ Carolyn Schmidt;Donna Harman
#t 1999
#c 13

#index 280929
#* An Internet-based newspaper filtering and personalization system (demonstration abstract)
#@ Aleksander Kołcz;Joshua Alspector
#t 1999
#c 13
#% 194301
#% 230395
#% 232646
#% 262085

#index 280930
#* Jester 2.0 (demonstration abstract): collaborative filtering to retrieve jokes
#@ Dhruv Gupta;Mark Digiovanni;Hiro Narita;Ken Goldberg
#t 1999
#c 13

#index 280931
#* The MultiText retrieval system (demonstration abstract)
#@ Gordon V. Cormack;Charles L. A. Clarke;Christopher R. Palmer;Robert C. Good
#t 1999
#c 13
#% 298182

#index 280932
#* Text and image retrieval in Cheshire II (demonstration abstract)
#@ Ray R. Larson
#t 1999
#c 13
#% 206512

#index 280933
#* Visualizing Internet search results with adaptive self-organizing maps (demonstration abstract)
#@ Dmitri Roussinov;Kristine Tolle;Marshall Ramsey;Michael McQuaid;Hsinchun Chen
#t 1999
#c 13

#index 280934
#* WebCluster, a tool for mediated information access (demonstration abstract)
#@ Gheorghe Muresan;David J. Harper;Mourad Mechkour
#t 1999
#c 13
#% 260012
#% 1783145

#index 281500
#* ATTICS (poster abstract): a software platform for online text classification
#@ David D. Lewis;Daniel L. Stern;Amit Singhal
#t 1999
#c 13
#% 120106
#% 127850
#% 136350
#% 194294
#% 212997
#% 219048
#% 648114
#% 1499571

#index 281503
#* Discovering Chinese words from unsegmented text (poster abstract)
#@ Xianping Ge;Wanda Pratt;Padhraic Smyth
#t 1999
#c 13
#% 232648

#index 285010
#* Applying user research directly to information system design (panel session)
#@ Raya Fidel;Efthimis Efthimiadis;Annelise Mark Pejtersen;Marcia J. Bates
#t 1999
#c 13

#index 285255
#* The economics of search
#@ Hal R. Varian
#t 1999
#c 13

#index 308387
#* Salton Award lecture: on theoretical argument in information retrieval (summary only): on theoretical argument in information retrieval
#@ Stephen Robertson
#t 2000
#c 13
#! The last winner of the Salton Award, Tefko Saracevic, gave an acceptance address at SIGIR in Philadelphia in 1997. Previous winners were William Cooper (1994), Cyril Cleverdon (1991), Karen Sparck Jones (1988) and Gerard Salton himself (1985).In this talk, I plan to follow the tradition of acceptance addresses, and present a personal view of and retrospective on some of the areas in which I work. However, I will not be saying much about what are perhaps the two most obvious parts of my work: the probabilistic approach to retrieval and evaluation of retrieval systems. Rather I will attempt to get under the skin of my take on IR, by discussing the nature of theoretical argument in the field, partly through examples. This talk is about the place of theory in the study of information retrieval (in some sense following Bill Cooper's 1994 topic), but not so much Theory with a capital T — rather what might be described as small-t theory.The field has a very strong pragmatic orientation, reflected both in the attitudes of the commercial participants and in the emphasis on formal evaluation in the academic environment. Nevertheless, there are many theoretical ideas buried in, or implied by, the ways we talk about the field — the language we use to discuss it. I will be discussing two areas to illustrate these low-level theoretical ideas: precision devices, and the apparent symmetry between retrieval and filtering.The phrase `precision device' used to have a rather clear meaning in IR, in the days of set-based retrieval systems. In that context, a precision device was a device to enable the restriction of the retrieved set to those most likely (out of the documents originally included) to be useful. These days, with the ubiquitous scoring and ranking methods largely replacing set-based retrieval, the idea has lost its meaning It is worth exploring the formal relationships involved to understand the change a little better.My second area is to do with the relation between filtering and the more traditional type of adhoc information retrieval. There is a tendency and a temptation to see these as the same kind of thing, sometimes with a more specific assumption of duality, based on the inversion of the roles of documents and queries. It is important to see how far this parallel extends, and where it breaks down. I explore the nature of the duality and the kinds of reasons why it does break down.These examples reflect my interest in the basic logical structure of information retrieval systems and the situations in which such systems may be found. I argue for a certain level of logical argument in information retrieval, which might be taken as small-t theory, though not as capital-T Theory. I believe there are reasons to think a Grand Theory of IR to be an unattainable goal — such a theory would have to encompass so many different aspects of retrieval, having to do for example with human cognition and behaviour and the structure of knowledge, as well as with the statistical concepts that inform the probabilistic approach.However, accepting the unattainability of a Grand Theory does not preclude the development of further and more useful models based on particular aspects and lower-level logic The low-level logic is important not only in its own right, but as the basis for linking together more sophisticated theories concerned with more restricted domains. The most elaborate and complete theory of (say) user behaviour is of no use at all without a strong linkage between the parts of that theory and the entities relevant to IR that fall outside its scope. The glue that provides that linkage has to be low-level logic.

#index 309062
#* Relevance and contributing information types of searched documents in task performance
#@ Pertti Vakkari
#t 2000
#c 13
#% 83855
#% 167557
#% 167561
#% 186622
#% 246770
#% 250785
#% 267587
#% 267654
#% 270279
#% 303510
#! End-users base the relevance judgements of the searched documents on the expected contribution to their task of the information contained in the documents. There is a shortage of studies analyzing the relationships between the experienced contribution, relevance assessments and type of information initially sought. This study categorizes the types of information in documents being used in writing a research proposal for a master's thesis by eleven students throughout the various stages of the proposal writing process. The role of the specificity of the searched information in influencing its contribution is analyzed. The results demonstrate that different types of information are sought at different stages of the writing process and thus the contribution of the information also differs at the different stages. The categories of the contributing information can be understood of topicality.

#index 309088
#* Relevance feedback with a small number of relevance judgements: incremental relevance feedback vs. document clustering
#@ Makoto Iwayama
#t 2000
#c 13
#% 118728
#% 118771
#% 169717
#% 169806
#% 194289
#% 194301
#% 218982
#% 218992
#% 219049
#% 232646
#% 232655
#% 262036
#% 262085
#% 375017
#! The use of incremental relevance feedback and document clustering were investigated in an relevance feedback environment in which the number of relevance judgements was quite small. Through experiments on the TREC collection, the incremental relevance feedback approach was found not to improve the overall search effectiveness. The clustering approach was found to be promising, although it sometimes over-focuses on a particular topic in a query and ignores the others. To overcome this problem, a query-biased clustering algorithm was developed and shown to be effective.

#index 309089
#* Do batch and user evaluations give the same results?
#@ William Hersh;Andrew Turpin;Susan Price;Benjamin Chan;Dale Kramer;Lynetta Sacherek;Daniel Olson
#t 2000
#c 13
#% 49490
#% 144010
#% 167563
#% 169781
#% 218982
#% 253191
#% 262075
#% 290703
#% 374663
#! Do improvements in system performance demonstrated by batch evaluations confer the same benefit for real users? We carried out experiments designed to investigate this question. After identifying a weighting scheme that gave maximum improvement over the baseline in a non-interactive evaluation, we used it with real users searching on an instance recall task. Our results showed the weighting scheme giving beneficial results in batch studies did not do so with real users. Further analysis did identify other factors predictive of instance recall, including number of documents saved by the user, document recall, and number of documents seen by the user.

#index 309091
#* A novel method for the evaluation of Boolean query effectiveness across a wide operational range
#@ Eero Sormunen
#t 2000
#c 13
#% 1358
#% 3621
#% 36399
#% 86465
#% 133892
#% 169779
#% 187998
#% 188581
#% 194269
#% 223368
#% 260242
#% 264186
#% 374663
#% 395687
#% 406493
#% 412887
#! Traditional methods for the system-oriented evaluation of Boolean IR system suffer from validity and reliability problems. Laboratory-based research neglects the searcher and studies suboptimal queries. Research on operational systems fails to make a distinction between searcher performance and system performance. This approach is neither capable of measuring performance at standard points of operation (e.g. across R0.0-R1.0).A new laboratory-based evaluation method for Boolean IR systems is proposed. It is based on a controlled formulation of inclusive query plans, on an automatic conversion of query plans into elementary queries, and on combining elementary queries into optimal queries at standard points of operation. Major results of a large case experiment are reported. The validity, reliability, and efficiency of the method are considered in the light of empirical and analytical test data.

#index 309093
#* Evaluating evaluation measure stability
#@ Chris Buckley;Ellen M. Voorhees
#t 2000
#c 13
#% 133889
#% 133892
#% 133893
#% 144074
#% 194284
#% 248074
#% 262034
#% 262097
#% 262102
#% 262105
#% 306497
#% 375017
#! This paper presents a novel way of examining the accuracy of the evaluation measures commonly used in information retrieval experiments. It validates several of the rules-of-thumb experimenters use, such as the number of queries needed for a good experiment is at least 25 and 50 is better, while challenging other beliefs, such as the common evaluation measures are equally reliable. As an example, we show that Precision at 30 documents has about twice the average error rate as Average Precision has. These results can help information retrieval researchers design experiments that provide a desired level of confidence in their results. In particular, we suggest researchers using Web measures such as Precision at 10 documents will need to use many more than 50 queries or will have to require two methods to have a very large difference in evaluation scores before concluding that the two methods are actually different.

#index 309095
#* IR evaluation methods for retrieving highly relevant documents
#@ Kalervo Järvelin;Jaana Kekäläinen
#t 2000
#c 13
#% 1358
#% 106122
#% 111456
#% 157904
#% 187773
#% 188581
#% 262067
#% 262107
#% 262276
#% 420479
#! This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modern large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In-Query1) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods.

#index 309096
#* Automatic generation of overview timelines
#@ Russell Swan;James Allan
#t 2000
#c 13
#% 60987
#% 109213
#% 169784
#% 173424
#% 214715
#% 231605
#% 237267
#% 249147
#% 262042
#% 262043
#% 270734
#% 287196
#% 309219
#% 677173
#% 742424
#! We present a statistical model of feature occurrence over time, and develop tests based on classical hypothesis testing for significance of term appearance on a given date. Using additional classical hypothesis testing we are able to combine these terms to generate “topics” as defined by the Topic Detection and Tracking study. The groupings of terms obtained can be used to automatically generate an interactive timeline displaying the major events and topics covered by the corpus. To test the validity of our technique we extracted a large number of these topics from a test corpus and had human evaluators judge how well the selected features captured the gist of the topics, and how they overlapped with a set of known topics from the corpus. The resulting topics were highly rated by evaluators who compared them to known topics.

#index 309098
#* Event tracking based on domain dependency
#@ Fumiyo Fukumoto;Yoshimi Suzuki
#t 2000
#c 13
#% 118731
#% 118771
#% 169718
#% 194251
#% 219052
#% 219054
#% 262042
#% 262043
#% 262059
#% 311034
#% 406493
#% 465747
#% 742368
#% 744551
#% 1275285
#! This paper proposes a method for event tracking on broadcast news stories based on distinction between a topic and an event. A topic and an event are identified using a simple criterion called domain dependency of words: how greatly a word features a given set of data. The method was tested on the TDT corpus which has been developed by the TDT Pilot Study and the result can be regarded as promising the usefulness of the method.

#index 309100
#* Improving text categorization methods for event tracking
#@ Yiming Yang;Tom Ault;Thomas Pierce;Charles W. Lattimer
#t 2000
#c 13
#% 169718
#% 194276
#% 219051
#% 219052
#% 219053
#% 262042
#% 262043
#% 262050
#% 262085
#% 280817
#% 318412
#% 445316
#% 445319
#! Automated tracking of events from chronologically ordered document streams is a new challenge for statistical text classification. Existing learning techniques must be adapted or improved in order to effectively handle difficult situations where the number of positive training instances per event is extremely small, the majority of training documents are unlabelled, and most of the events have a short duration in time. We adapted several supervised text categorization methods, specifically several new variants of the k-Nearest Neighbor (kNN) algorithm and a Rocchio approach, to track events. All of these methods showed significant improvement (up to 71% reduction in weighted error rates) over the performance of the original kNN algorithm on TDT benchmark collections, making kNN among the top-performing systems in the recent TDT3 official evaluation. Furthermore, by combining these methods, we significantly reduced the variance in performance of our event tracking system over different data collections, suggesting a robust solution for parameter optimization.

#index 309101
#* Evaluation of a simple and effective music information retrieval method
#@ Stephen Downie;Michael Nelson
#t 2000
#c 13
#% 36400
#% 115476
#% 133892
#% 204646
#% 280890
#% 406493
#! We developed, and then evaluated, a music information retrieval (MIR) system based upon the intervals found within the melodies of a collection of 9354 folksongs. The songs were converted to an interval-only representation of monophonic melodies and then fragmented t into length-n subsections called n-grams. The length of these n-grams and the degree to which we precisely represent the intervals are variables analyzed in this paper. We constructed a collection of “musical word” databases using the text-based, SMART information retrieval system. A group of simulated queries, some of which contained simulated errors, was run against these databases. The results were evaluated using the normalized precision and normalized recall measures. Our concept of “musical words” shows great merit thus implying that useful MIR systems can be constructed simply and efficiently using pre-existing text-based information retrieval software. Second, this study is a formal and comprehensive evaluation of a MIR system using rigorous statistical analyses to determine retrieval effectiveness.

#index 309102
#* Phonetic confusion matrix based spoken document retrieval
#@ Savitha Srinivasan;Dragutin Petkovic
#t 2000
#c 13
#% 183496
#% 218984
#% 237291
#% 253188
#% 262039
#% 280919
#% 288166
#% 589958
#! Combined word-based index and phonetic indexes have been used to improve the performance of spoken document retrieval systems primarily by addressing the out-of-vocabulary retrieval problem. However, a known problem with phonetic recognition is its limited accuracy in comparison with word level recognition. We propose a novel method for phonetic retrieval in the CueVideo system based on the probabilistic formulation of term weighting using phone confusion data in a Bayesian framework. We evaluate this method of spoken document retrieval against word-based retrieval for the search levels identified in a realistic video-based distributed learning setting. Using our test data, we achieved an average recall of 0.88 with an average precision of 0.69 for retrieval of out-of-vocabulary words on phonetic transcripts with 35% word error rate. For in-vocabulary words, we achieved a 17% improvement in recall over word-based retrieval with a 17% loss in precision for word error rites ranging from 35 to 65%.

#index 309103
#* Multiple evidence combination in image retrieval: Diogenes searches for people on the Web
#@ Y. Alp Aslandogan;Clement T. Yu
#t 2000
#c 13
#! In this work, we examine evidence combination mechanisms for classifying multimedia information. In particular, we examine linear and Dempster-Shafer methods of evidence combination in the context of identifying personal images on the World Wide Web. An automatic web search engine named Diogenes1 searches the web for personal images and combines different pieces of evidence for identification. The sources of evidence consist of input from face detection/recognition and text/HTML analysis modules. A degree of uncertainty is involved with both of these sources. Diogenes automatically determines the uncertainty locally for each retrieval and uses this information to set a relative significance for each evidence. To our knowledge, Diogenes is the first image search engine using Dempster-Shafer evidence combination based on automatic object recognition and dynamic local uncertainty assessment. In our experiments Diogenes comfortably outperformed some well known commercial and research prototype image search engines for celebrity image queries.

#index 309104
#* Link-based and content-based evidential information in a belief network model
#@ Ilmério Silva;Berthier Ribeiro-Neto;Pável Calado;Edleno Moura;Nívio Ziviani
#t 2000
#c 13
#% 44876
#% 104923
#% 111303
#% 115462
#% 219047
#% 253188
#% 262061
#% 268073
#% 268079
#% 281174
#% 282905
#% 290703
#% 387427
#% 406493
#% 617186
#% 836019
#! This work presents an information retrieval model developed to deal with hyperlinked environments. The model is based on belief networks and provides a framework for combining information extracted from the content of the documents with information derived from cross-references among the documents. The information extracted from the content of the documents is based on statistics regarding the keywords in the collection and is one of the basis for traditional information retrieval (IR) ranking algorithms. The information derived from cross-references among the documents is based on link references in a hyperlinked environment and has received increased attention lately due to the success of the Web. We discuss a set of strategies for combining these two types of sources of evidential information and experiment with them using a reference collection extracted from the Web. The results show that this type of combination can improve the retrieval performance without requiring any extra information from the users at query time. In our experiments, the improvements reach up to 59% in terms of average precision figures.

#index 309106
#* The feature quantity: an information theoretic perspective of Tfidf-like measures
#@ Akiko Aizawa
#t 2000
#c 13
#% 169806
#% 262037
#% 280817
#% 309165
#% 465747
#% 465754
#% 465895
#! The feature quantity, a quantitative representation of specificity introduced in this paper, is based on an information theoretic perspective of co-occurrence events between terms and documents. Mathematically, the feature quantity is defined as a product of probability and information, and maintains a good correspondence with the tfidf-like measures popularly used in today's IR systems. In this paper, we present a formal description of the feature quantity, as well as some illustrative examples of applying such a quantity to different types of information retrieval tasks: representative term selection and text categorization.

#index 309109
#* INSYDER — an information assistant for business intelligence
#@ Harald Reiterer;Gabriela Mußler;Thomas M. Mann;Siegfried Handschuh
#t 2000
#c 13
#% 169729
#% 201992
#% 214669
#% 214709
#% 231567
#% 240805
#% 262045
#% 268722
#% 270633
#% 379921
#% 431492
#% 504755
#% 588673
#% 641057
#% 1499473
#! The WWW is the most important resource for external business information. This paper presents a tool called INSYDER, an information assistant for finding and analysis business information from the WWW. INSYDER is a system using different agents for crawling the Web, evaluating and visualising the results. These agents, the used visualisations, and a first summary of user studies are presented.

#index 309112
#* Structured translation for cross-language information retrieval
#@ Ruth Sperer;Douglas W. Oard
#t 2000
#c 13
#% 131320
#% 218989
#% 262046
#% 262067
#% 282944
#% 465914
#% 741080
#% 756821
#% 786534
#% 786535
#! The paper introduces a query translation model that reflects the structure of the cross-language information retrieval task. The model is based on a structured bilingual dictionary in which the translations of each term are clustered into groups with distinct meanings. Query translation is modeled as a two-stage process, with the system first determining the intended meaning of a query term and then selecting translations appropriate to that meaning that might appear in the document collection. An implementation of structured translation based on automatic dictionary clustering is described and evaluated by using Chinese queries to retrieve English documents. Structured translation achieved an average precision that was statistically indistinguishable from Pirkola's technique for very short queries, but Pirkola's technique outperformed structured translation on long queries. The paper concludes with some observations on future work to improve retrieval effectiveness and on other potential uses of structured translation in interactive cross-language retrieval applications.

#index 309113
#* Automatic adaptation of proper noun dictionaries through cooperation of machine learning and probabilistic methods
#@ Georgios Petasis;Alessandro Cucchiarelli;Paola Velardi;Georgios Paliouras;Vangelis Karkaletsis;Constantine D. Spyropoulos
#t 2000
#c 13
#% 136350
#% 196896
#% 198058
#% 742023
#% 742424
#% 756177
#% 756816
#% 756952
#% 757318
#% 815337
#% 817846
#! The recognition of Proper Nouns (PNs) is considered an important task in the area of Information Retrieval and Extraction. However the high performance of most existing PN classifiers heavily depends upon the availability of large dictionaries of domain-specific Proper Nouns, and a certain amount of manual work for rule writing or manual tagging. Though it is not a heavy requirement to rely on some existing PN dictionary (often these resources are available on the web), its coverage of a domain corpus may be rather low, in absence of manual updating. In this paper we propose a technique for the automatic updating of an PN Dictionary through the cooperation of an inductive and a probabilistic classifier. In our experiments we show that, whenever an existing PN Dictionary allows the identification of 50% of the proper nouns within a corpus, our technique allows, without additional manual effort, the successful recognition of about 90% of the remaining 50%.

#index 309114
#* Document centered approach to text normalization
#@ Andrei Mikheev
#t 2000
#c 13
#% 71063
#% 194294
#% 239316
#% 624428
#% 740916
#% 741065
#% 741072
#% 786528
#% 815336
#% 817779
#% 817846
#! In this paper we present an approach to tackle three important problems of text normalization: sentence boundary disambiguation, disambiguation of capitalized words when they are used in positions where capitalization is expected, and identification of abbreviations. The main feature of our approach is that it uses a minimum of pre-built resources, instead dynamically inferring disambiguation clues from the entire document itself. This makes it domain independent, closely targeted to each individual document and portable to other languages. We thoroughly evaluated this approach on several corpora and it showed high accuracy.

#index 309115
#* OCELOT: a system for summarizing Web pages
#@ Adam L. Berger;Vibhu O. Mittal
#t 2000
#c 13
#% 252472
#% 262096
#% 280835
#% 280909
#% 324101
#% 740915
#% 786575
#% 818045
#! We introduce OCELOT, a prototype system for automatically generating the “gist” of a web page by summarizing it. Although most text summarization research to date has focused on the task of news articles, web pages are quite different in both structure and content. Instead of coherent text with a well-defined discourse structure, they are more often likely to be a chaotic jumble of phrases, links, graphics and formatting commands. Such text provides little foothold for extractive summarization techniques, which attempt to generate a summary of a document by excerpting a contiguous, coherent span of text from it. This paper builds upon recent work in non-extractive summarization, producing the gist of a web page by “translating” it into a more concise representation rather than attempting to extract a text span verbatim. OCELOT uses probabilistic models to guide it in selecting and ordering words into a gist. This paper describes a technique for learning these models automatically from a collection of human-summarized web pages.

#index 309116
#* Extracting sentence segments for text summarization: a machine learning approach
#@ Wesley T. Chuang;Jihoon Yang
#t 2000
#c 13
#% 136350
#% 144580
#% 194251
#% 288614
#% 376266
#% 449588
#% 708199
#% 708427
#! With the proliferation of the Internet and the huge amount of data it transfers, text summarization is becoming more important. We present an approach to the design of an automatic text summarizer that generates a summary by extracting sentence segments. First, sentences are broken into segments by special cue markers. Each segment is represented by a set of predefined features (e.g. location of the segment, average term frequencies of the words occurring in the segment, number of title words in the segment, and the like). Then a supervised learning algorithm is used to train the summarizer to extract important sentence segments, based on the feature vector. Results of experiments on U.S. patents indicate that the performance of the proposed approach compares very favorably with other approaches (including Microsoft Word summarizer) in terms of precision, recall, and classification accuracy.

#index 309119
#* An experimental comparison of naive Bayesian and keyword-based anti-spam filtering with personal e-mail messages
#@ Ion Androutsopoulos;John Koutsias;Konstantinos V. Chandrinos;Constantine D. Spyropoulos
#t 2000
#c 13
#% 165110
#% 165115
#% 219052
#% 230532
#% 245184
#% 246832
#% 252349
#% 271060
#% 376266
#% 406493
#% 457672
#% 746868
#% 757877
#% 817843
#% 1478939
#! The growing problem of unsolicited bulk e-mail, also known as “spam”, has generated a need for reliable anti-spam e-mail filters. Filters of this type have so far been based mostly on manually constructed keyword patterns. An alternative approach has recently been proposed, whereby a Naive Bayesian classifier is trained automatically to detect spam messages. We test this approach on a large collection of personal e-mail messages, which we make publicly available in “encrypted” form contributing towards standard benchmarks. We introduce appropriate cost-sensitive measures, investigating at the same time the effect of attribute-set size, training-corpus size, lemmatization, and stop lists, issues that have not been explored in previous experiments. Finally, the Naive Bayesian filter is compared, in terms of performance, to a filter that uses keyword patterns, and which is part of a widely used e-mail reader.

#index 309122
#* Text filtering by boosting naive Bayes classifiers
#@ Yu-Hwan Kim;Shang-Yoon Hahn;Byoung-Tak Zhang
#t 2000
#c 13
#% 124004
#% 194284
#% 194301
#% 209021
#% 218982
#% 219052
#% 262085
#% 280817
#% 302391
#% 465895
#% 565531
#% 1499573
#! Several machine learning algorithms have recently been used for text categorization and filtering. In particular, boosting methods such as AdaBoost have shown good performance applied to real text data. However, most of existing boosting algorithms are based on classifiers that use binary-valued features. Thus, they do not fully make use of the weight information provided by standard term weighting methods. In this paper, we present a boosting-based learning method for text filtering that uses naive Bayes classifiers as a weak learner. The use of naive Bayes allows the boosting algorithm to utilize term frequency information while maintaining probabilistically accurate confidence ratio. Applied to TREC-7 and TREC-8 filtering track documents, the proposed method obtained a significant improvement in LF1, LF2, F1 and F3 measures compared to the best results submitted by other TREC entries.

#index 309123
#* Document filtering method using non-relevant information profile
#@ Keiichiro Hoashi;Kazunori Matsumoto;Naomi Inoue;Kazuo Hashimoto
#t 2000
#c 13
#% 280895
#! Document filtering is a task to retrieve documents relevant to a user's profile from a flow of documents. Generally, filtering systems calculate the similarity between the profile and each incoming document, and retrieve documents with similarity higher than a threshold. However, many systems set a relatively high threshold to reduce retrieval of non-relevant documents, which results in the ignorance of many relevant documents. In this paper, we propose the use of a non-relevant information profile to reduce the mistaken retrieval of non-relevant documents. Results from experiments show that this filter has successfully rejected a sufficient number of non-relevant documents, resulting in an improvement of filtering performance.

#index 309124
#* Question-answering by predictive annotation
#@ John Prager;Eric Brown;Anni Coden;Dragomir Radev
#t 2000
#c 13
#% 1786
#% 144033
#% 169729
#% 198058
#% 266219
#% 290703
#% 384022
#% 406493
#% 742082
#% 742425
#! We present a new technique for question answering called Predictive Annotation. Predictive Annotation identifies potential answers to questions in text, annotates them accordingly and indexes them. This technique, along with a complementary analysis of questions, passage-level ranking and answer selection, produces a system effective at answering natural-language fact-seeking questions posed against large document collections. Experimental results show the effects of different parameter settings and lead to a number of general observations about the question-answering problem.

#index 309126
#* Bridging the lexical chasm: statistical approaches to answer-finding
#@ Adam Berger;Rich Caruana;David Cohn;Dayne Freitag;Vibhu Mittal
#t 2000
#c 13
#% 46803
#% 81669
#% 218978
#% 280819
#% 280851
#% 676170
#! This paper investigates whether a machine can automatically learn the task of finding, within a large collection of candidate responses, the answers to questions. The learning process consists of inspecting a collection of answered questions and characterizing the relation between question and answer with a statistical model. For the purpose of learning this relation, we propose two sources of data: Usenet FAQ documents and customer service call-center dialogues from a large retail company. We will show that the task of “answer-finding” differs from both document retrieval and tradition question-answering, presenting challenges different from those found in these problems. The central aim of this work is to discover, through theoretical and empirical investigation, those statistical techniques best suited to the answer-finding problem.

#index 309127
#* Building a question answering test collection
#@ Ellen M. Voorhees;Dawn M. Tice
#t 2000
#c 13
#% 144033
#% 262105
#% 298854
#% 306497
#% 676170
#% 815330
#% 815343
#! The TREC-8 Question Answering (QA) Track was the first large-scale evaluation of domain-independent question answering systems. In addition to fostering research on the QA task, the track was used to investigate whether the evaluation methodology used for document retrieval is appropriate for a different natural language processing task. As with document relevance judging, assessors had legitimate differences of opinions as to whether a response actually answers a question, but comparative evaluation of QA systems was stable despite these differences. Creating a reusable QA test collection is fundamentally more difficult than creating a document retrieval test collection since the QA task has no equivalent to document identifiers.

#index 309128
#* Document clustering using word clusters via the information bottleneck method
#@ Noam Slonim;Naftali Tishby
#t 2000
#c 13
#% 46809
#% 115608
#% 118771
#% 144023
#% 158687
#% 194289
#% 218992
#% 232651
#% 232655
#% 262045
#% 262059
#% 262134
#% 272512
#% 280819
#% 280856
#% 280857
#% 280879
#% 280934
#% 375017
#% 748465
#! We present a novel implementation of the recently introduced information bottleneck method for unsupervised document clustering. Given a joint empirical distribution of words and documents, p(x, y), we first cluster the words, Y, so that the obtained word clusters, Ytilde;, maximally preserve the information on the documents. The resulting joint distribution. p(X, Ytilde;), contains most of the original information about the documents, I(X; Ytilde;) &ap; I(X; Y), but it is much less sparse and noisy. Using the same procedure we then cluster the documents, X, so that the information about the word-clusters is preserved. Thus, we first find word-clusters that capture most of the mutual information about to set of documents, and then find document clusters, that preserve the information about the word clusters. We tested this procedure over several document collections based on subsets taken from the standard 20Newsgroups corpus. The results were assessed by calculating the correlation between the document clusters and the correct labels for these documents. Finding from our experiments show that this double clustering procedure, which uses the information bottleneck method, yields significantly superior performance compared to other common document distributional clustering algorithms. Moreover, the double clustering procedure improves all the distributional clustering methods examined here.

#index 309129
#* Latent semantic space: iterative scaling improves precision of inter-document similarity measurement
#@ Rie Kubota Ando
#t 2000
#c 13
#% 118749
#% 118762
#% 124009
#% 204298
#% 248027
#% 262217
#% 280822
#% 318407
#% 375636
#% 406493
#% 485545
#% 485556
#% 607945
#% 853653
#! We present a novel algorithm that creates document vectors with reduced dimensionality. This work was motivated by an application characterizing relationships among documents in a collection. Our algorithm yielded inter-document similarities with an average precision up to 17.8% higher than that of singular value decomposition (SVD) used for Latent Semantic Indexing. The best performance was achieved with dimensional reduction rates that were 43% higher than SVD on average. Our algorithm creates basis vectors for a reduced space by iteratively “scaling” vectors and computing eigenvectors. Unlike SVD, it breaks the symmetry of documents and terms to capture information more evenly across documents. We also discuss correlation with a probabilistic model and evaluate a method for selecting the dimensionality using log-likelihood estimation.

#index 309131
#* An investigation of linguistic features and clustering algorithms for topical document clustering
#@ Vasileios Hatzivassiloglou;Luis Gravano;Ankineedu Maganti
#t 2000
#c 13
#% 46803
#% 66085
#% 71743
#% 115462
#% 120109
#% 218992
#% 248801
#% 262042
#% 280817
#% 283171
#% 742425
#% 815336
#! We investigate four hierarchical clustering methods (single-link, complete-link, groupwise-average, and single-pass) and two linguistically motivated text features (noun phrase heads and proper names) in the context of document clustering. A statistical model for combining similarity information from multiple sources is described and applied to DARPA's Topic Detection and Tracking phase 2 (TDT2) data. This model, based on log-linear regression, alleviates the need for extensive search in order to determine optimal weights for combining input features. Through an extensive series of experiments with more than 40,000 documents from multiple news sources and modalities, we establish that both the choice of clustering algorithm and the introduction of the additional features have an impact on clustering performance. We apply our optimal combination of features to the TDT2 test data, obtaining partitions of the documents that compare favorably with the results obtained by participants in the official TDT2 competition.

#index 309133
#* The impact of database selection on distributed searching
#@ Allison L. Powell;James C. French;Jamie Callan;Margaret Connell;Charles L. Viles
#t 2000
#c 13
#% 144074
#% 172898
#% 184496
#% 194245
#% 194246
#% 194275
#% 262063
#% 262065
#% 263704
#% 267454
#% 280853
#% 280856
#% 282422
#% 287463
#% 481748
#% 567255
#! The proliferation of online information resources increases the importance of effective and efficient distributed searching. Distributed searching is cast in three parts — database selection, query processing, and results merging. In this paper we examine the effect of database selection on retrieval performance. We look at retrieval performance in three different distributed retrieval testbeds and distill some general results. First we find that good database selection can result in better retrieval effectiveness than can be achieved in a centralized database. Second we find that good performance can be achieved when only a few sites are selected and that the performance generally increases as more sites are selected. Finally we find that when database selection is employed, it is not necessary to maintain collection wide information (CWI), e.g. global idf. Local information can be used to achieve superior performance. This means that distributed systems can be engineered with more autonomy and less cooperation. This work suggests that improvements in database selection can lead to broader improvements in retrieval performance, even in centralized (i.e. single database) systems. Given a centralized database and a good selection mechanism, retrieval performance can be improved by decomposing that database conceptually and employing a selection step.

#index 309135
#* Hill climbing algorithms for content-based retrieval of similar configurations
#@ Dimitris Papadias
#t 2000
#c 13
#% 23998
#% 124680
#% 126390
#% 181409
#% 227932
#% 248796
#% 260019
#% 273886
#% 280843
#% 319244
#% 443054
#% 443133
#% 443529
#% 479462
#% 479620
#% 481429
#% 593030
#% 627211
#% 1499502
#! The retrieval of stored images matching an input configuration is an important form of content-based retrieval. Exhaustive processing (i.e., retrieval of the best solutions) of configuration similarity queries is, in general, exponential and fast search for sub-optimal solutions is the only way to deal with the vast (and ever increasing) amounts of multimedia information in several real-time applications. In this paper we discuss the utilization of hill climbing heuristics that can provide very good results within limited processing time. We propose several heuristics, which differ on the way that they search through the solution space, and identify the best ones depending on the query and image characteristics. Finally we develop new algorithms that take advantage of the specific structure of the problem to improve performance.

#index 309139
#* Partial collection replication versus caching for information retrieval systems
#@ Zhihong Lu;Kathryn S. McKinley
#t 2000
#c 13
#% 35207
#% 35959
#% 59265
#% 71758
#% 100817
#% 104439
#% 123074
#% 148659
#% 167607
#% 173905
#% 209654
#% 219013
#% 280832
#% 286382
#% 298181
#% 344720
#% 463579
#% 610649
#% 616995
#% 646494
#% 708700
#! The explosion of content in distributed information retrieval (IR) systems requires new mechanisms to attain timely and accurate retrieval of unstructured text. In this paper, we compare two mechanisms to improve IR system performance: partial collection replication and caching. When queries have locality, both mechanisms return results more quickly than sending queries to the original collection(s). Caches return results when queries exactly match a previous one. Partial replicas are a form of caching that return results when the IR technology determines the query is a good match. Caches are simpler and faster, but replicas can increase locality by detecting similarity between queries that are not exactly the same. We use real traces from THOMAS and Excite to measure query locality and similarity. With a very restrictive definition of query similarity, similarity improves query locality up to 15% over exact match. We use a validated simulator to compare their performance, and find that even if the partial replica hit rate increases only 3 to 6%, it will outperform simple caching under a variety of configurations. A combined approach will probably yield the best performance.

#index 309141
#* Hierarchical classification of Web content
#@ Susan Dumais;Hao Chen
#t 2000
#c 13
#% 115608
#% 165110
#% 169718
#% 190581
#% 194283
#% 218992
#% 219053
#% 232653
#% 260001
#% 262045
#% 269218
#% 280817
#% 280866
#% 297550
#% 420466
#% 458379
#% 461692
#% 465747
#% 465754
#% 466078
#% 571073
#% 856161
#! This paper explores the use of hierarchical structure for classifying a large, heterogeneous collection of web content. The hierarchical structure is initially used to train different second-level classifiers. In the hierarchical case, a model is learned to distinguish a second-level category from other categories within the same top level. In the flat non-hierarchical case, a model distinguishes a second-level category from all other second-level categories. Scoring rules can further take advantage of the hierarchy by considering only second-level categories that exceed a threshold at the top level.We use support vector machine (SVM) classifiers, which have been shown to be efficient and effective for classification, but not previously explored in the context of hierarchical classification. We found small advantages in accuracy for hierarchical models over flat models. For the hierarchical approach, we found the same accuracy using a sequential Boolean decision rule and a multiplicative decision rule. Since the sequential approach is much more efficient, requiring only 14%-16% of the comparisons used in the other approaches, we find it to be a good choice for classifying text into large hierarchical structures.

#index 309142
#* A practical hypertext catergorization method using links and incrementally available class information
#@ Hyo-Jung Oh;Sung Hyon Myaeng;Mann-Ho Lee
#t 2000
#c 13
#% 85272
#% 142617
#% 169719
#% 197843
#% 248810
#% 262061
#% 266215
#% 280817
#% 282905
#% 438136
#% 445243
#! As WWW grows at an increasing speed, a classifier targeted at hypertext has become in high demand. While document categorization is quite a mature, the issue of utilizing hypertext structure and hyperlinks has been relatively unexplored. In this paper, we propose a practical method for enhancing both the speed and the quality of hypertext categorization using hyperlinks. In comparison against a recently proposed technique that appears to be the only one of the kind, we obtained up to 18.5% of improvement in effectiveness while reducing the processing time dramatically. We attempt to explain through experiments what factors contribute to the improvement.

#index 309145
#* Topical locality in the Web
#@ Brian D. Davison
#t 2000
#c 13
#% 220709
#% 232708
#% 232912
#% 248218
#% 248810
#% 249110
#% 262061
#% 268073
#% 268078
#% 268079
#% 268087
#% 268114
#% 281186
#% 281209
#% 281251
#% 281253
#% 282905
#% 311040
#! Most web pages are linked to others with related content. This idea, combined with another that says that text in, and possibly around, HTML anchors describe the pages to which they point, is the foundation for a usable World-Wide Web. In this paper, we examine to what extent these ideas hold by empirically testing whether topical locality mirrors spatial locality of pages on the Web. In particular, we find that the likelihood of linked pages having similar textual content to be high; the similarity of sibling pages increases when the links from the parent are close together; titles, descriptions, and anchor text represent at least part of the target page; and that anchor text may be a useful discriminator among unseen child pages. These results show the foundations necessary for the success of many web systems, including search engines, focused crawlers, linkage analyzers, and intelligent web agents.

#index 309146
#* Interactive Internet search: keyword, directory and query reformulation mechanisms compared
#@ Peter Bruza;Robert McArthur;Simon Dennis
#t 2000
#c 13
#% 169732
#% 253188
#% 280840
#% 296646
#% 433821
#! This article compares search effectiveness when using query-based Internet search (via the Google search engine), directory-based search (via Yahoo) and phrase-based query reformulation assisted search (via the Hyperindex browser) by means of a controlled, user-based experimental study. The focus was to evaluate aspects of the search process. Cognitive load was measured using a secondary digit-monitoring task to quantify the effort of the user in various search states; independent relevance judgements were employed to gauge the quality of the documents accessed during the search process. Time was monitored in various search states. Results indicated the directory-based search does not offer increased relevance over the query-based search (with or without query formulation assistance), and also takes longer. Query reformulation does significantly improve the relevance of the documents through which the user must trawl versus standard query-based internet search. However, the improvement in document relevance comes at the cost of increased search time and increased cognitive load.

#index 309150
#* Incorporating quality metrics in centralized/distributed information retrieval on the World Wide Web
#@ Xiaolan Zhu;Susan Gauch
#t 2000
#c 13
#% 212361
#% 282424
#% 294995
#% 635851
#! Most information retrieval systems on the Internet rely primarily on similarity ranking algorithms based solely on term frequency statistics. Information quality is usually ignored. This leads to the problem that documents are retrieved without regard to their quality. We present an approach that combines similarity-based similarity ranking with quality ranking in centralized and distributed search environments. Six quality metrics, including the currency, availability, information-to-noise ratio, authority, popularity, and cohesiveness, were investigated. Search effectiveness was significantly improved when the currency, availability, information-to-noise ratio and page cohesiveness metrics were incorporated in centralized search. The improvement seen when the availability, information-to- noise ratio, popularity, and cohesiveness metrics were incorporated in site selection was also significant. Finally, incorporating the popularity metric in information fusion resulted in a significant improvement. In summary, the results show that incorporating quality metrics can generally improve search effectiveness in both centralized and distributed search environments.

#index 309151
#* Does “authority” mean quality? predicting expert quality ratings of Web documents
#@ Brian Amento;Loren Terveen;Will Hill
#t 2000
#c 13
#% 214669
#% 214673
#% 214711
#% 232679
#% 232912
#% 262061
#% 268073
#% 272915
#% 279109
#% 282905
#% 648114
#% 761959
#! For many topics, the World Wide Web contains hundreds or thousands of relevant documents of widely varying quality. Users face a daunting challenge in identifying a small subset of documents worthy of their attention.Link analysis algorithms have received much interest recently, in large part for their potential to identify high quality items. We report here on an experimental evaluation of this potential.We evaluated a number of link and content-based algorithms using a dataset of web documents rated for quality by human topic experts. Link-based metrics did a good job of picking out high-quality items. Precision at 5 is about 0.75, and precision at 10 is about 0.55; this is in a dataset where 0.32 of all documents were of high quality. Surprisingly, a simple content-based metric performed nearly as well; ranking documents by the total number of pages on their containing site.

#index 309155
#* Document classification on neural networks using only positive examples (poster session)
#@ Larry M. Manevitz;Malik Yousef
#t 2000
#c 13
#% 375017
#% 406493
#! In this paper, we show how a simple feed-forward neural network can be trained to filter documents when only positive information is available, and that this method seems to be superior to more standard methods, such as tf-idf retrieval based on an “average vector”. A novel experimental finding that retrieval is enhanced substantially in this context by carrying out a certain kind of uniform transformation (“Hadamard”) of the information prior to the training of the network.

#index 309157
#* New paradigms in information visualization (poster session)
#@ Peter Au;Matthew Carey;Shalini Sewraz;Yike Guo;Stefan M. Rüger
#t 2000
#c 13
#% 102726
#% 109199
#% 118772
#% 152957
#% 169783
#% 232679
#% 240338
#% 296984
#! We present three new visualization front-ends that aid navigation through the set of documents returned by a search engine (hit documents). We cluster the hit documents to visually group these documents and label the groups with related words. The different front-ends cater for different user needs, but all can browse cluster information as well as drilling up or down in one or more clusters and refining the search using one or more of the suggested related keywords.

#index 309159
#* Latent semantic indexing model for Boolean query formulation (poster session)
#@ Dae-Ho Baek;HeuiSeok Lim;Hae-Chang Rim
#t 2000
#c 13
#% 88950
#% 200694
#% 257145
#% 319273
#! A new model named Boolean Latent Semantic Indexing model based on the Singular Value Decomposition and Boolean query formulation is introduced. While the Singular Value Decomposition alleviates the problems of lexical matching in the traditional information retrieval model, Boolean query formulation can help users to make precise representation of their information search needs. Retrieval experiments on a number of test collections seem to show that the proposed model achieves substantial performance gains over the Latent Semantic Indexing model.

#index 309162
#* Generation of user profiles for information filtering — research agenda (poster session)
#@ Tsvi Kuflik;Peretz Shoval
#t 2000
#c 13
#% 124004
#% 159108
#% 194283
#% 220709
#% 227031
#% 229344
#% 232708
#% 276338
#% 303389
#% 406493
#% 446038
#% 465928
#% 1275346
#! In information filtering (IF) systems, user long-term needs we expressed as user profiles. The quality of a user profile has a major impact on the performance of IF systems. The focus of the proposed research is on the study of user profile generation and update. The paper introduces methods for user profile generation, and proposes a research agenda for their comparison and evaluation.

#index 309165
#* Variance based classifier comparison in text catergorization (poster session)
#@ Atsuhiro Takasu;Kenro Aihara
#t 2000
#c 13
#% 280817
#% 458379
#% 465747
#! Text categorization is one of the key functions for utilizing vast amount of documents. It can be seen as a classification problem, which has been studied in pattern recognition and machine learning fields for a long time and several classification methods have been developed such as statistical classification, decision tree, support vector machines and so on. Many researchers applied those classification methods to text categorization and reported their performance (e.g., decision tree[3], Bayes classifier[2], support vector machine[l]). Yang conducted comprehensive study of comparison or text categorization and reported that k nearest neighbor and support vector machines works well for text categorization[4].In the previous studies, classification methods were usually compared using single pair of training and test data However, classification method with more complex family of classifiers requires more training data and small training data may result in deriving unreliable classifier, that is, the performance of the derived classifier varies much depending on training data. Therefore, we need to take the size of training data into account when comparing and selecting a classification method. In this paper, we discuss how to select a classifier from those derived by various classification methods and how the size of training data affects the performance of the derived classifier.In order to evaluate the reliability of classification method, we consider the variance of accuracy of derived classifier. We first construct a statistical model. In the text categorization, each document is usually represented with a feature vector that consists of weighted frequencies of terms. In the vector space model, document is a point in high dimensional feature space and a classifier separates the feature space into subspaces each of which is labeled with a category.

#index 309168
#* The use of phrases from query texts in information retrieval (poster session)
#@ Masumi Narita;Yasushi Ogawa
#t 2000
#c 13
#% 184494
#% 336784

#index 309169
#* Pseudo-frequency method (poster session): an efficient document ranking retrieval method for n-gram indexing
#@ Ogawa Yasushi
#t 2000
#c 13
#! Although n-gram (n successive characters) indexing is widely used in retrieval systems for documents in Japanese and other Asian languages, it is difficult to process ranking retrieval efficiently using n-gram indexing. This is because frequency information for query words needs to be computed using indexed data since this information is not directly available from the n-gram index. To reduce processing costs, this paper proposes a pseudo-frequency method, which uses a word's estimated frequencies instead of precise ones. The results of experiments on NTCIR, a Japanese IR test collection, showed that the proposed method speeded up retrieval without degrading retrieval effectiveness.

#index 309170
#* Lexical semantic relatedness and online new event detection (poster session)
#@ Nicola Stokes;Paula Hatch;Joe Carthy
#t 2000
#c 13
#% 740329

#index 309171
#* Modeling question-response patterns by scaling and visualization (poster session)
#@ Mark Rorvig
#t 2000
#c 13
#% 318786
#! The evaluation of question difficulty is usually considered the domain of Latent Trait Theory. However, these methods require standardized question sets normalized by large populations, rendering them inefficient for use in the numerous areas where questions must be evaluated. A new technique is illustrated that models the question-response cycle well, but without the procedural difficulty of the traditional methods.

#index 309172
#* The effect of query type on subject searching behavior of image databases (poster session): an exploratory study
#@ Efthimis N. Efthimiadis;Raya Fidel
#t 2000
#c 13
#% 237327
#% 238912
#% 239575
#% 702871

#index 309174
#* The role of a judge in a user based retrieval experiment (poster session)
#@ Mingfang Wu;Michael Fuller;Ross Wilkinson
#t 2000
#c 13
#% 218992
#% 262075
#% 262076
#% 262097
#% 262105
#% 290703

#index 309176
#* Auto-construction of a live thesaurus from search term logs for interactive Web search (poster session)
#@ Shui-Lung Chuang;Hsiao-Tieh Pu;Wen-Hsiang Lu;Lee-Feng Chien
#t 2000
#c 13
#! The purpose of this paper is to present an on-going research that is intended to construct a live thesaurus directly from search term logs of real-world search engines. Such a thesaurus designed can contain representative search terms, their frequency in use, the corresponding subject categories, the associated and relevant terms, and the hot visiting Web sites/pages the search terms may reach.

#index 309178
#* Cognitive approach for building user model in an information retrieval context (poster session)
#@ Amina Sayeb Belhassen;Nabil Ben Abdallah;Henda Hadjami Ben Ghezala
#t 2000
#c 13
#% 16955
#% 116374
#% 169803
#% 259543
#% 1499473
#! The recent development of communication networks and multimedia system provides users with the availability of a huge amount of information making worse the problem of information overload [9]. The evolution of system design is necessary becoming more user centred, and more personally involving. A review of survey studies on Internet users since 1993 confirms that a greater percentage of people are becoming online citizens, and professionals are integrating more online components into their work process. A review of the experimental literature on Internet user's reveals that there is intense interest in humanising the online environment by integrating affective and cognitive components [8].We are specially in concern with the effects on the evolution on information retrieval. We can notice significant changes in the information retrieval world over the past five or so years due to the emergence of Internet and one of its most important and widely used services, the world wide Web (WWW) or simply the Web.While reviewing the progress of research in information retrieval and user modelling, we can observe that many systems and prtotypes are created [5], [10], [11], but all of them, share some basic limitations: the techniques used to represent knowledge in the user model is based on simple list of keyword, the type of the considered knowledge is very limited, usually restricted to single word, or to (some) structural characteristic: the learning capability are very poor.We aim to propose a cognitive approach for building user model in an information retrieval context. In fact cognitive approach is based on identifying how users process information and what constitutes an appropriate model to represent this process, and because IR, under the cognitive paradigm takes the user into account in a high-priority way [1]. However, within the cognitive paradigm, there is no general model valid for our documentary approach that satisfactorily how user knowledge is represented for the purpose of processing information. The lack of such a model does not allow one to identify a user's cognitive state with regard to his or her information needs and requirements.Methodology adopting the cognitive viewpoint in IR are Synthesised by Daniel [4] in three groups, which comprise the representation of:users and their problem, which stems from the hypothesis proposed by Belkin on the `anomalous states of knowledge' (ASK), according to which the user searches for informationsearch strategy, which compile the different ways search strategies and processes are carried out, depending on the variable involved - user, intermediary, IR systems [6],[7]document and information, which is considered a major goal of current IR research, since it embraces the whole corpus of studies about user models intended to eliminate the intermediary's role in retrieval system. The aim of this approach is to allow users direct access to the system by means of the representation of documents and intelligent interfaces.User-centered paradigm now dominates in studies of information needs and information retrieval. We have the goal of developing new approaches to information retrieval which are based on user modelling techniques for building and managing the representation of the user preferances. In this paper, we describe two complementary approaches which are necessary for building user model and its integration in an information retrieval system:a conceptual one based on the description of knowledge needed by the user in an information retrieval context,a functional approach which deals with dynamic aspects of the model. Within this approach we aim to determine the role played by the model in an information retrieval context.In many studies of IR interesting in user modelling we find different kind of knowledge trying to describe user's need. So our conceptual approach has consisted in enumerating these knowledge and integrating them in their adequate components in information retrieval architecture. Almost of these studies that identified cognitive characteristics have used quantitative methods to measure them. What is needed is a qualitative study and appropriate method to ascertain these cognitive characteristic [12].Our main objective is the development of techniques for modelling the user as an interactive part of IR, so we propose our functional approach which deals with identifying cognitive characteristic within the role played by the user model in an information retrieval architecture. So we began by presenting the conceptual approach and so the functional one.

#index 309182
#* Multimedia information retrieval from recorded presentations (poster session)
#@ Wolfgang Hürst;Rainer Müller;Christoph Mayer
#t 2000
#c 13
#% 220382
#% 286906
#% 286956
#% 438054
#% 582012
#! In presentation recording special effort is usually put into the automation of the production process, that is in automatically creating high quality data files without much or any need for manual recording and post-editing [5]. With the advent of such systems and their usage in classroom teaching, at conferences, etc., there is an increasing need for techniques and abilities which enable users to search in those documents and to localize some specific information. In this paper we describe how we integrated information retrieval techniques into the Authoring on the Fly (AOF) system, an approach for automatic presentation recording. We have chosen the AOF system for two reasons. On the one hand, it is a well-established way for presentation recording, used by various universities and institutions1. On the other hand it is general enough to illustrate typical problems and challenges a developer is facing when designing a system for information retrieval from multimedia data streams which occur in the presentation recording scenario.

#index 309184
#* Influence of speech recognition errors on topic detection (poster session)
#@ J. Scott McCarley;Martin Franz
#t 2000
#c 13
#% 280815
#! We investigate the effect of speech-recognition errors on a system for the unsupervised, nearly synchronous clustering of broadcast news stories, using the TDT (Topic Detection and Tracking) Corpora. Two questions are addressed: (1) Are speech recognition errors detrimental to the performance of the system? (2) Can a background collection of contemporaneous clean text improve performance? We investigate both the large-cluster and small-cluster limits.

#index 309187
#* Word document density and relevance scoring (poster session)
#@ Martin Franz;J. Scott McCarley
#t 2000
#c 13
#% 218978
#% 742513
#% 746910
#! Previous work addressing the issue of word distribution in documents has shown the importance of Word repetitiveness as an indicator of the word content-bearing characteristics. In this paper we propose a simple method using a measure of the tendency of words to repeat within a document to separate the words with similar document frequencies, but different topic discriminating characteristics. We describe the application of the new measure in query-document relevance scoring. Experiments on the TREC Ad Hoc and Spoken Document Retrieval tasks [7] show useful performance improvements.

#index 309189
#* Ranking digital images using combination of evidences (poster session)
#@ Iadh Ounis
#t 2000
#c 13
#% 2298
#% 194276
#% 213673
#% 262095
#% 437405
#% 1783152

#index 309192
#* Collaborative filtering and the generalized vector space model (poster session)
#@ Ian Soboroff;Charles Nicholas
#t 2000
#c 13
#% 202011
#% 220711
#% 1650569
#! Collaborative filtering is a technique for recommending documents to users based on how similar their tastes are to other users. If two users tend to agree on what they like, the system will recommend the same documents to them. The generalized vector space model of information retrieval represents a document by a vector of its similarities to all other documents. The process of collaborative filtering is nearly identical to the process of retrieval using GVSM in a matrix of user ratings. Using this observation, a model for filtering collaboratively using document content is possible.

#index 309194
#* Theme-based retrieval of Web news (poster session)
#@ Nuno Maria;Mário J. Silva
#t 2000
#c 13
#% 262043
#% 269217
#% 280817
#% 445316
#! We present our framework for classification of Web news, based on support vector machines, and some of the initial measurements of its accuracy.

#index 309196
#* Stemming and its effects on TFIDF ranking (poster session)
#@ Mark Kantrowitz;Behrang Mohit;Vibhu Mittal
#t 2000
#c 13
#% 144034
#% 208020
#% 208934
#% 218985
#% 324015

#index 309198
#* Exploration of a heuristic approach to threshold learning in adaptive filtering (poster session)
#@ Chengxiang Zhai;Peter Jansen;David A. Evans
#t 2000
#c 13
#! In this paper we examine the learning behavior of a heuristic threshold setting approach to information filtering. In particular, we study how different initial threshold settings and different updating parameter settings affect threshold learning. The results on one of the TREC news databases indicate that (1) learning allows recovery from the inevitable non-optimality of the initial conditions, and (2) a greater “willingness to learn” (expressed by a deliberate lowering of the score threshold in the learning stage) does eventually lead to a higher performance in spite of the expected initial performance penalty.

#index 309202
#* On the design and evaluation of a multi-dimensional approach to information retrieval (poster session)
#@ M. Catherine McCabe;Jinho Lee;Abdur Chowdhury;David Grossman;Ophir Frieder
#t 2000
#c 13
#% 218982
#% 224702
#% 321635
#% 385946
#% 480286
#% 482093
#% 640190
#! We present a method of searching text collections that takes advantage of hierarchrical information within documents and integrates searches of structured and unstructured data. We show that Multidimensional databases (MDB), designed for accessing data along hierarchical dimensions, are effective for information retrieval. We demonstrate a method of using On-Line Analytic Processing (OLAP) techniques on a text collection. This combines traditional information retrieval and the slicing, dicing, drill-down, and roll-up of OLAP. We demonstrate use of a prototype for searching documents from the TREC collection.

#index 309204
#* SWAMI (poster session): a framework for collaborative filtering algorithm development and evaluation
#@ Danyel Fisher;Kris Hildrum;Jason Hong;Mark Newman;Megan Thomas;Rich Vuduc
#t 2000
#c 13
#% 173879
#% 190581
#% 202011
#% 280852
#% 564279
#! We present a Java-based framework, SWAMI (Shared Wisdom through the Amalgamation of Many Interpretations) for building and studying collaborative filtering systems. SWAMI consists of three components: a prediction engine, an evaluation system, and a visualization component. The prediction engine provides a common interface for implementing different prediction algorithms. The evaluation system provides a standardized testing methodology and metrics for analyzing the accuracy and run-time performance of prediction algorithms. The visualization component suggests how graphical representations can inform the development and analysis of prediction algorithms. We demonstrate SWAMI on the Each Movie data set by comparing three prediction algorithms: a traditional Pearson correlation-based method, support vector machines, and a new accurate and scalable correlation-based method based on clustering techniques.

#index 309206
#* Learning probabilistic models of the Web (poster session)
#@ Thomas Hofmann
#t 2000
#c 13
#% 262061
#% 262096
#% 280819
#% 280851
#% 282905
#% 466900
#! In the World Wide Web, myriads of hyperlinks connect documents and pages to create an unprecedented, highly complex graph structure - the Web graph. This paper presents a novel approach to learning probabilistic models of the Web, which can be used to make reliable predictions about connectivity and information content of Web documents. The proposed method is a probabilistic dimension reduction technique which recasts and unites Latent Semantic Analysis and Kleinberg's Hubs-and-Authorities algorithm in a statistical setting.This meant to be a first step towards the development of a statistical foundation for Web—related information technologies. Although this paper does not focus on a particular application, a variety of algorithms operating in the Web/Internet environment can take advantage of the presented techniques, including search engines, Web crawlers, and information agent systems.

#index 309207
#* Effects of out of vocabulary words in spoken document retrieval (poster session)
#@ P. C. Woodland;S. E. Johnson;P. Jourlin;K. Spärck Jones
#t 2000
#c 13
#% 280815
#! The effects of out-of-vocabulary (OOV) items in spoken document retrieval (SDR) are investigated. Several sets of transcriptions were created for the TREC-8 SDR task using a speech recognition system varying the vocabulary sizes and OOV rates, and the relative retrieval performance measured. The effects of OOV terms on a simple baseline IR system and on more sophisticated retrieval systems are described. The use of a parallel corpus for query and document expansion is found to be especially beneficial, and with this data set, good retrieval performance can be achieved even for fairly high OOV rates.

#index 309209
#* Towards an adaptive and task-specific ranking mechanism in Web searching (poster session)
#@ Chen Ding;Chi-Hung Chi
#t 2000
#c 13
#% 268079
#% 282905

#index 309210
#* Beyond the traditional query operators (poster session)
#@ Chen Ding;Chi-Hung Chi
#t 2000
#c 13
#% 214673
#% 268079
#% 282905

#index 309211
#* Bayes optimal metasearch: a probabilistic model for combining the results of multiple retrieval systems (poster session)
#@ Javed A. Aslam;Mark Montague
#t 2000
#c 13
#% 232703
#% 420464
#! We introduce a new, probabilistic model for combining the outputs of an arbitrary number of query retrieval systems. By gathering simple statistics on the average performance of a given set of query retrieval systems, we construct a Bayes optimal mechanism for combining the outputs of these systems. Our construction yields a metasearch strategy whose empirical performance nearly always exceeds the performance of any of the constituent systems. Our construction is also robust in the sense that if “good” and “bad” systems are combined, the Performance of the composite is still on par with, or exceeds, that of the best constituent system. Finally, our model and theory provide theoretical and empirical avenues for the improvement of this metasearch strategy.

#index 309212
#* Information access for context-aware appliances (poster session)
#@ Gareth J. F. Jones;Peter J. Brown
#t 2000
#c 13
#% 124004
#% 388526
#! The emergence of networked context-aware mobile computing appliances potentially offers opportunities for remote access to huge online information resources. Information access in context-aware information appliances can utilize existing techniques developed for effective information retrieval and information filtering; however, practical physical and operational features of these devices and the availability of context information itself suggest that the document selection process should make use of this contextual data.

#index 309213
#* Finding relevant passages using noun-noun compounds (poster session): coherence vs. proximity
#@ Eduard Hoenkamp;Rob de Groot
#t 2000
#c 13
#% 280905
#% 433821
#% 445025
#! Intuitively, words forming phrases are a more precise description of content than words as a sequence of keywords. Yet, evidence that phrases would be more effective for information retrieval is inconclusive. This paper isolates a neglected class of phrases, that is abundant in communication, has an established theoretical foundation, and shows promise for an effective expression of the user's information need: the noun-noun compound (NNC). In an experiment, a variety of meaningful NNCs were used to isolate relevant passages in a large and varied corpus. In a first pass, passages were retrieved based on textual proximity of the words or their semantic peers. A second pass retained only passages containing a syntactically coherent structure equivalent to the original NNC. This second pass showed a dramatic increase in precision. Preliminary results show the validity of our intuition about phrases in the special but very productive case of NNCs.

#index 309214
#* Semantic Explorer — navigation in documents collections; Proxima Daily — learning personal newspaper (demonstration session)
#@ Vadim Asadov;Serge Shumsky
#t 2000
#c 13

#index 309215
#* Integrated search tools for newspaper digital libraries (demonstration session)
#@ S. L. Mantzaris;B. Gatos;N. Gouraros;P. Tzavelis
#t 2000
#c 13
#% 240038
#% 248010
#% 625504

#index 309216
#* Managing photos with AT&T Shoebox (demonstration session)
#@ Timothy J. Mills;David Pye;David Sinclair;Kenneth R. Wood
#t 2000
#c 13
#% 280849

#index 309217
#* ClusterBook, a tool for dual information access (demonstration session)
#@ Gheorghe Mureşan;David J. Harper;Ayşe Göker;Peter Lowit
#t 2000
#c 13

#index 309218
#* Uexküll (demonstration session): an interactive visual user interface for document retrieval in vector space
#@ Michael Preminger;Sandor Daranyi
#t 2000
#c 13

#index 309219
#* TimeMine (demonstration session): visualizing automatically constructed timelines
#@ Russell Swan;James Allan
#t 2000
#c 13
#% 237267
#% 287196
#% 309096

#index 309220
#* The Cambridge University Multimedia Document Retrieval demo system (demonstration session)
#@ A. Tuerk;S. E. Johnson;P. Jourlin;K. Spärck Jones;P. C. Woodland
#t 2000
#c 13

#index 340882
#* Applying summarization techniques for term selection in relevance feedback
#@ Adenike M. Lam-Adesina;Gareth J. F. Jones
#t 2001
#c 13
#% 71752
#% 92696
#% 115473
#% 169768
#% 169781
#% 169809
#% 194298
#% 218978
#% 262036
#% 262084
#% 288614
#% 375017
#% 387791
#% 747816
#! Query-expansion is an effective Relevance Feedback technique for improving performance in Information Retrieval. In general query-expansion methods select terms from the complete contents of relevant documents. One problem with this approach is that expansion terms unrelated to document relevance can be introduced into the modified query due to their presence in the relevant documents and distribution in the document collection. Motivated by the hypothesis that query-expansion terms should only be sought from the most relevant areas of a document, this investigation explores the use of document summaries in query-expansion. The investigation explores the use of both context-independent standard summaries and query-biased summaries. Experimental results using the Okapi BM25 probabilistic retrieval model with the TREC-8 ad hoc retrieval task show that query-expansion using document summaries can be considerably more effective than using full-document expansion. The paper also presents a novel approach to term-selection that separates the choice of relevant documents from the selection of a pool of potential expansion terms. Again, this technique is shown to be more effective that standard methods.

#index 340883
#* Temporal summaries of new topics
#@ James Allan;Rahul Gupta;Vikas Khandelwal
#t 2001
#c 13
#% 194251
#% 198294
#% 198297
#% 230530
#% 262096
#% 262112
#% 279755
#% 280835
#% 280909
#% 288614
#% 309096
#% 309115
#% 318409
#% 387791
#% 817550
#% 853645
#% 853647
#% 853648
#% 853649
#% 853650
#% 853651
#% 853652
#% 1306081
#! We discuss technology to help a person monitor changes in news coverage over time. We define temporal summaries of news stories as extracting a single sentence from each event within a news topic, where the stories are presented one at a time and sentences from a story must be ranked before the next story can be considered. We explain a method for evaluation, and describe an evaluation corpus that we have built. We also propose several methods for constructing temporal summaries and evaluate their effectiveness in comparison to degenerate cases. We show that simple approaches are effective, but that the problem is far from solved.

#index 340884
#* Generic text summarization using relevance measure and latent semantic analysis
#@ Yihong Gong;Xin Liu
#t 2001
#c 13
#% 132779
#% 259990
#% 280835
#% 678757
#! In this paper, we propose two generic text summarization methods that create text summaries by ranking and extracting sentences from the original documents. The first method uses standard IR methods to rank sentence relevances, while the second method uses the latent semantic analysis technique to identify semantically important sentences, for summary creations. Both methods strive to select sentences that are highly ranked and different from each other. This is an attempt to create a summary with a wider coverage of the document's main content and less redundancy. Performance evaluations on the two summarization methods are conducted by comparing their summarization outputs with the manual summaries generated by three independent human evaluators. The evaluations also study the influence of different VSM weighting schemes on the text summarization performances. Finally, the causes of the large disparities in the evaluators' manual summarization results are investigated, and discussions on human text summarization patterns are presented.

#index 340885
#* A new approach to unsupervised text summarization
#@ Tadashi Nomoto;Yuji Matsumoto
#t 2001
#c 13
#% 194251
#% 235374
#% 262112
#% 279755
#% 280838
#% 288614
#% 466083
#% 466425
#% 729437
#% 742513
#% 746875
#% 757855
#% 817578
#! The paper presents a novel approach to unsupervised text summarization. The novelty lies in exploiting the diversity of concepts in text for summarization, which has not received much attention in the summarization literature. A diversity-based approach here is a principled generalization of Maximal Marginal Relevance criterion by Carbonell and Goldstein \cite{carbonell-goldstein98}.We propose, in addition, aninformation-centricapproach to evaluation, where the quality of summaries is judged not in terms of how well they match human-created summaries but in terms of how well they represent their source documents in IR tasks such document retrieval and text categorization.To find the effectiveness of our approach under the proposed evaluation scheme, we set out to examine how a system with the diversity functionality performs against one without, using the BMIR-J2 corpus, a test data developed by a Japanese research consortium. The results demonstrate a clear superiority of a diversity based approach to a non-diversity based approach.

#index 340886
#* Vector-space ranking with effective early termination
#@ Vo Ngoc Anh;Owen de Kretser;Alistair Moffat
#t 2001
#c 13
#% 40395
#% 157880
#% 169817
#% 179161
#% 184486
#% 184490
#% 194247
#% 198335
#% 212665
#% 213786
#% 218982
#% 228097
#% 253191
#% 262034
#% 262097
#% 262099
#% 262102
#% 290703
#! Considerable research effort has been invested in improving the effectiveness of information retrieval systems. Techniques such as relevance feedback, thesaural expansion, and pivoting all provide better quality responses to queries when tested in standard evaluation frameworks. But such enhancements can add to the cost of evaluating queries. In this paper we consider the pragmatic issue of how to improve the cost-effectiveness of searching. We describe a new inverted file structure using quantized weights that provides superior retrieval effectiveness compared to conventional inverted file structures when early termination heuristics are employed. That is, we are able to reach similar effectiveness levels with less computational cost, and so provide a better cost/performance compromise than previous inverted file organisations.

#index 340887
#* Static index pruning for information retrieval systems
#@ David Carmel;Doron Cohen;Ronald Fagin;Eitan Farchi;Michael Herscovici;Yoelle S. Maarek;Aya Soffer
#t 2001
#c 13
#% 65964
#% 169817
#% 198335
#% 212665
#% 228097
#% 262099
#% 406493
#% 463737
#! We introduce static index pruning methods that significantly reduce the index size in information retrieval systems.We investigate uniform and term-based methods that each remove selected entries from the index and yet have only a minor effect on retrieval results. In uniform pruning, there is a fixed cutoff threshold, and all index entries whose contribution to relevance scores is bounded above by a given threshold are removed from the index. In term-based pruning, the cutoff threshold is determined for each term, and thus may vary from term to term. We give experimental evidence that for each level of compression, term-based pruning outperforms uniform pruning, under various measures of precision. We present theoretical and experimental evidence that under our term-based pruning scheme, it is possible to prune the index greatly and still get retrieval results that are almost as good as those based on the full index.

#index 340888
#* Rank-preserving two-level caching for scalable search engines
#@ Paricia Correia Saraiva;Edleno Silva de Moura;Novio Ziviani;Wagner Meira;Rodrigo Fonseca;Berthier Riberio-Neto
#t 2001
#c 13
#% 35959
#% 67565
#% 152925
#% 169849
#% 212665
#% 248794
#% 249153
#% 253188
#% 268079
#% 281184
#% 296646
#% 309104
#% 309139
#% 340296
#% 387427
#% 420492
#% 438325
#% 481916
#% 611156
#% 708700
#% 836019

#index 340889
#* Using event segmentation to improve indexing of consumer photographs
#@ Amanda Stent;Alexander Loui
#t 2001
#c 13
#% 287163
#% 298998
#% 742425
#% 747944
#% 1783152
#! Automatic albuming --- the automatic organization of photographs, either as an end in itself or for use in other applications -- is an application that promises to be of great assistance to photographers. Relatively sophisticated image content analysis techniques have been used for image indexing, organization and retrieval. In this paper, we describe a method of organizing photographs into events using spoken photograph captions. The results of this process can be used to improve image indexing and retrieval.

#index 340890
#* Ranking retrieval systems without relevance judgments
#@ Ian Soboroff;Charles Nicholas;Patrick Cahan
#t 2001
#c 13
#% 169774
#% 194269
#% 208931
#% 262034
#% 262097
#% 262102
#% 262105
#% 290793
#% 312689
#% 320355
#! The most prevalent experimental methodology for comparing the effectiveness of information retrieval systems requires a test collection, composed of a set of documents, a set of query topics, and a set of relevance judgments indicating which documents are relevant to which topics. It is well known that relevance judgments are not infallible, but recent retrospective investigation into results from the Text REtrieval Conference (TREC) has shown that differences in human judgments of relevance do not affect the relative measured performance of retrieval systems. Based on this result, we propose and describe the initial results of a new evaluation methodology which replaces human relevance judgments with a randomly selected mapping of documents to topics which we refer to aspseudo-relevance judgments.Rankings of systems with our methodology correlate positively with official TREC rankings, although the performance of the top systems is not predicted well. The correlations are stable over a variety of pool depths and sampling techniques. With improvements, such a methodology could be useful in evaluating systems such as World-Wide Web search engines, where the set of documents changes too often to make traditional collection construction techniques practical.

#index 340892
#* Evaluation by highly relevant documents
#@ Ellen M. Voorhees
#t 2001
#c 13
#% 262107
#% 268079
#% 280041
#% 281174
#% 306497
#% 309093
#% 309095
#% 312689
#! Given the size of the web, the search engine industry has argued that engines should be evaluated by their ability to retrieve highly relevant pages rather than all possible relevant pages. To explore the role highly relevant documents play in retrieval system evaluation, assessors for the \mbox{TREC-9} web track used a three-point relevance scale and also selected best pages for each topic. The relative effectiveness of runs evaluated by different relevant document sets differed, confirming the hypothesis that different retrieval techniques work better for retrieving highly relevant documents. Yet evaluating by highly relevant documents can be unstable since there are relatively few highly relevant documents. TREC assessors frequently disagreed in their selection of the best page, and subsequent evaluation by best page across different assessors varied widely. The discounted cumulative gain measure introduced by J\"{a}rvelin and Kek\"{a}l\"{a}inen increases evaluation stability by incorporating all relevance judgments while still giving precedence to highly relevant documents.

#index 340893
#* Meta-scoring: automatically evaluating term weighting schemes in IR without precision-recall
#@ Rong Jin;Christos Falusos;Alex G. Hauptmann
#t 2001
#c 13
#% 46803
#% 115608
#% 132779
#% 248058
#! In this paper, we present a method that can automatically evaluate performance of different term weighting schemes in information retrieval without resorting to precision-recall based on human relevance judgments. Specifically, the problem is: given two document-term matrixes generated from two different term weighting schemes, can we tell which term weighting scheme will performance better than the other? We propose a meta-scoring function, which takes as input the document-term matrix generated by some term weighting scheme and computes a goodness score from the document-term matrix. In our experiments, we found out that this score is highly correlated with the precision-recall measurement for all the collections and term weighting schema we tried. Thus, we conclude that our meta-scoring function can be a substitute for the precision-recall measurement that needs relevance judgments of human subject. Furthermore, this meta-scoring function is not limited only to text information retrieval can be applied to fields such as image and DNA retrieval.

#index 340894
#* Improving cross language retrieval with triangulated translation
#@ Tim Gollins;Mark Sanderson
#t 2001
#c 13
#% 169774
#% 218989
#% 262047
#% 384972
#% 466092
#% 562054
#% 786534
#! Most approaches to cross language information retrieval assume that resources providing a direct translation between the query and document languages exist. This paper presents research examining the situation where such an assumption is false. Here, an intermediate (or pivot) language provides a means of transitive translation of the query language to that of the document via the pivot, at the cost, however, of introducing much error. The paper reports the novel approach of translating in parallel across multiple intermediate languages and fusing the results. Such a technique removes the error, raising the effectiveness of the tested retrieval system, up to and possibly above the level expected, had a direct translation route existed. Across a number of retrieval situations and combinations of languages, the approach proves to be highly effective.

#index 340895
#* Improving query translation for cross-language information retrieval using statistical models
#@ Jianfeng Gao;Jian-Yun Nie;Endong Xun;Jian Zhang;Ming Zhou;Changning Huang
#t 2001
#c 13
#% 144074
#% 218988
#% 232656
#% 262047
#% 280826
#% 316902
#% 375017
#% 420472
#% 648114
#% 740915
#% 740916
#% 742162
#% 817555
#% 853854
#! Dictionaries have often been used for query translation in cross-language information retrieval (CLIR). However, we are faced with the problem of translation ambiguity, i.e. multiple translations are stored in a dictionary for a word. In addition, a word-by-word query translation is not precise enough. In this paper, we explore several methods to improve the previous dictionary-based query translation. First, as many as possible, noun phrases are recognized and translated as a whole by using statistical models and phrase translation patterns. Second, the best word translations are selected based on the cohesion of the translation words. Our experimental results on TREC English-Chinese CLIR collection show that these techniques result in significant improvements over the simple dictionary approaches, and achieve even better performance than a high-quality machine translation system.

#index 340897
#* Evaluating a probabilistic model for cross-lingual information retrieval
#@ Jinxi Xu;Ralph Weischedel;Chanh Nguyen
#t 2001
#c 13
#% 144074
#% 218982
#% 232647
#% 262046
#% 262047
#% 262096
#% 280850
#% 280851
#% 298183
#% 309112
#% 740915
#% 786534
#! This work proposes and evaluates a probabilistic cross-lingual retrieval system. The system uses a generative model to estimate the probability that a document in one language is relevant, given a query in another language. An important component of the model is translation probabilities from terms in documents to terms in a query. Our approach is evaluated when 1) the only resource is a manually generated bilingual word list, 2) the only resource is a parallel corpus, and 3) both resources are combined in a mixture model. The combined resources produce about 90% of monolingual performance in retrieving Chinese documents. For Spanish the system achieves 85% of monolingual performance using only a pseudo-parallel Spanish-English corpus. Retrieval results are comparable with those of the structural query translation technique (Pirkola, 1998) when bilingual lexicons are used for query translation. When parallel texts in addition to conventional lexicons are used, it achieves better retrieval results but requires more computation than the structural query translation technique. It also produces slightly better results than using a machine translation system for CLIR, but the improvement over the MT system is not significant.

#index 340899
#* Document language models, query models, and risk minimization for information retrieval
#@ John Lafferty;Chengxiang Zhai
#t 2001
#c 13
#% 73045
#% 120104
#% 262096
#% 268079
#% 280850
#% 280851
#% 289109
#% 290703
#% 290830
#! We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval. A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. The query language model can be exploited to model user preferences, the context of a query, synonomy and word senses. While recent work has incorporated word translation models for this purpose, we introduce a new method using Markov chains defined on a set of documents to estimate the query models. The Markov chain method has connections to algorithms from link analysis and social networks. The new approach is evaluated on TREC collections and compared to the basic language modeling approach and vector space models together with query expansion using Rocchio. Significant improvements are obtained over standard query expansion methods for strong baseline TF-IDF systems, with the greatest improvements attained for short queries on Web data.

#index 340901
#* Relevance based language models
#@ Victor Lavrenko;W. Bruce Croft
#t 2001
#c 13
#% 169781
#% 248214
#% 262043
#% 262096
#% 278106
#% 280850
#% 280851
#% 280864
#% 298183
#% 300542
#% 309115
#% 309126
#% 740915
#% 748738
#! We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate arelevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accurate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data.

#index 340903
#* A statistical learning learning model of text classification for support vector machines
#@ Thorsten Joachims
#t 2001
#c 13
#% 1295
#% 109192
#% 190581
#% 248027
#% 260001
#% 269217
#% 287284
#% 420077
#% 458379
#% 466759
#% 708398
#! This paper develops a theoretical learning model of text classification for Support Vector Machines (SVMs). It connects the statistical properties of text-classification tasks with the generalization performance of a SVM in a quantitative way. Unlike conventional approaches to learning text classifiers, which rely primarily on empirical evidence, this model explains why and when SVMs perform well for text classification. In particular, it addresses the following questions: Why can support vector machines handle the large feature spaces in text classification effectively? How is this related to the statistical properties of text? What are sufficient conditions for applying SVMs to text-classification problems successfully?

#index 340904
#* A study of thresholding strategies for text categorization
#@ Yiming Yang
#t 2001
#c 13
#% 118731
#% 169718
#% 169777
#% 219052
#% 219053
#% 262085
#% 280817
#% 287284
#% 309100
#% 318412
#% 375017
#% 430761
#% 458379
#% 466922
#! Thresholding strategies in automated text categorization are an underexplored area of research. This paper presents an examination of the effect of thresholding strategies on the performance of a classifier under various conditions. Using k-Nearest Neighbor (kNN) as the classifier and five evaluation benchmark collections as the testbets, three common thresholding methods were investigated, including rank-based thresholding (RCut), proportion-based assignments (PCut) and score-based local optimization (SCut); in addition, new variants of these methods are proposed to overcome significant problems in the existing approaches. Experimental results show that the choice of thresholding strategy can significantly influence the performance of kNN, and that the ``optimal'' strategy may vary by application. SCut is potentially better for fine-tuning but risks overfitting. PCut copes better with rare categories and exhibits a smoother trade-off in recall versus precision, but is not suitable for online decision making. RCut is most natural for online response but is too coarse-grained for global or local optimization. RTCut, a new method combining the strength of category ranking and scoring, outperforms both PCut and RCut significantly.

#index 340905
#* On feature distributional clustering for text categorization
#@ Ron Bekkerman;Ran El-Yaniv;Naftali Tishby;Yoad Winter
#t 2001
#c 13
#% 115608
#% 190581
#% 197394
#% 260001
#% 262059
#% 309208
#% 311034
#% 406493
#% 458379
#% 465747
#% 465754
#% 465895
#% 562950
#% 729437
#% 742372
#% 748465
#! We describe a text categorization approach that is based on a combination of feature distributional clusters with a support vector machine (SVM) classifier. Our feature selection approach employs distributional clustering of words via the recently introducedinformation bottleneck method, which generates a more efficientword-clusterrepresentation of documents. Combined with the classification power of an SVM, this method yields high performance text categorization that can outperform other recent methods in terms of categorization accuracy and representation efficiency. Comparing the accuracy of our method with other techniques, we observe significant dependency of the results on the data set. We discuss the potential reasons for this dependency.

#index 340910
#* Iterative residual rescaling
#@ Rie Kubota Ando;Lillian Lee
#t 2001
#c 13
#% 118749
#% 118762
#% 124009
#% 200694
#% 204298
#% 207677
#% 280822
#% 309128
#% 309129
#% 321053
#% 338443
#% 485556
#% 607945
#% 714005
#! We consider the problem of creating document representations in which inter-document similarity measurements correspond to semantic similarity. We first present a novelsubspace-basedframework for formalizing this task. Using this framework, we derive a new analysis ofLatent Semantic Indexing(LSI), showing a precise relationship between its performance and theuniformityof the underlying distribution of documents over topics. This analysis helps explain the improvements gained by Ando's (2000)Iterative Residual Rescaling(\ours) algorithm: \ours\ can compensate for distributional non-uniformity. A further benefit of our framework is that it provides a well-motivated, effective method for automatically determining the rescaling factor \ours\ depends on, leading to further improvements. A series of experiments over various settings and with several evaluation metrics validates our claims.

#index 340911
#* Expressive retrieval from XML documents
#@ Taurai Tapiwa Chinenyanga;Nicholas Kushmerick
#t 2001
#c 13
#% 55490
#% 116303
#% 201889
#% 232662
#% 234797
#% 248801
#% 273922
#% 281149
#% 309726
#% 309851
#% 312870
#% 479803
#% 479956
#% 504581
#! The emergence of XML as a standard interchange format for structured documents/data has given rise to many XML query language proposals. However, some of these languages do not support information retrieval-style ranked queries based on textual similarity. There have been several extensions to these query languages to support keyword search, but the resulting query languages cannot express queries such as``find books and CDs with similar titles''. Either these extensions use keywords as mere boolean filters, or similarities can be calculated only between data values and constants rather than two data values. We propose ELIXIR, an \textbf{\underline{e}}xpressive and \textbf{\underline{e}}fficient\textbf{\underline{l}}anguage for \textbf{\underline{X}}ML \textbf{\underline{i}}nformation \textbf{\underline{r}}etrieval that extends the query language XML-QL \cite{deutsch-www8,deutsch-deb99} with a textual similarity operator. ELIXIR is a general-purpose XML information retrieval language, sufficiently expressive to handle the above query. Our algorithm for answering ELIXIR queries rewrites the original ELIXIR query into a series of XML-QL queries that generate intermediate relational data, and uses relational database techniques to efficiently evaluate the similarity operators on this intermediate data, yielding an XML document with nodes ranked by similarity. Our experiments demonstrate that our prototype scales well with the size of the XML data and complexity of the query.

#index 340914
#* XIRQL: a query language for information retrieval in XML documents
#@ Norbert Fuhr;Kai Großjohann
#t 2001
#c 13
#% 144012
#% 215225
#% 232662
#% 237053
#% 262034
#% 262069
#% 262094
#% 280038
#% 318049
#% 504578
#% 504581
#! Based on the document-centric view of XML, we present the query language XIRQL. Current proposals for XML query languages lack most IR-related features, which are weighting and ranking, relevance-oriented search, datatypes with vague predicates, and semantic relativism. XIRQL integrates these features by using ideas from logic-based probabilistic IR models, in combination with concepts from the database area. For processing XIRQL queries, a path algebra is presented, that also serves as a starting point for query optimization.

#index 340915
#* Empirical investigations on query modification using abductive explanations
#@ Ian Rithven;Mounia Lalmas;Keith van Rijsbergen
#t 2001
#c 13
#% 45311
#% 103810
#% 115473
#% 118726
#% 144007
#% 169806
#% 232719
#% 309150
#% 345739
#! In this paper we report on a series of experiments designed to investigate query modification techniques motivated by the area of abductive reasoning. In particular we use the notion of abductive explanation, explanations being a description of data that highlight important features of the data. We describe several methods of creating abductive explanations, exploring term reweighting and query reformulation techniques and demonstrate their suitability for relevance feedback.

#index 340916
#* Generic summaries for indexing in information retrieval
#@ Tetsuya Sakai;Karen Sparck-Jones
#t 2001
#c 13
#% 198294
#% 218978
#% 262036
#% 262105
#% 280835
#% 296647
#% 306498
#% 309093
#% 309095
#% 316903
#% 340964
#% 387791
#% 747816
#% 757855
#! This paper examines the use of generic summaries for indexing in information retrieval. Our main observations are that: (1) With or without pseudo-relevance feedback, a summary index may be as effective as the corresponding fulltext index forprecision-oriented search of highly relevant documents. %43 But a reasonably sophisticated summarizer, using a compression ratio of 10-30%, is desirable for this purpose. (2) In pseudo-relevance feedback, using a summary index at initial search and a fulltext index at final search is possibly effective for precision-oriented search, regardless of relevance levels. This strategy is significantly more effective than the one using the summary index only and probably more effective than using summaries as mere term selection filters. %the use of summaries as mere term selection filters. %The summary quality is probably not a critical factor for this strategy, For this strategy, the summary quality is probably not a critical factor, and a compression ratio of 5-10% appears best.

#index 340918
#* Automatic generation of concise summaries of spoken dialogues in unrestricted domains
#@ Klaus Zechner
#t 2001
#c 13
#% 136350
#% 179800
#% 194251
#% 262112
#% 280811
#% 387791
#% 539186
#% 740409
#% 741058
#% 742225
#% 742398
#% 746895
#% 757426
#% 817580
#! Automatic summarization of open domain spoken dialogues is a new research area. This paper introduces the task, the challenges involved, and presents an approach to obtain automatic extract summaries for multi-party dialogues of four different genres, without any restriction on domain. We address the following issues which are intrinsic to spoken dialogue summarization and typically can be ignored when summarizing written text such as newswire data: (i) detection and removal of speech disfluencies; (ii) detection and insertion of sentence boundaries; (iii) detection and linking of cross-speaker information units (question-answer pairs). A global system evaluation using a corpus of 23 relevance annotated dialogues containing 80 topical segments shows that for the two more informal genres, our summarization system using dialogue specific components significantly outperforms a baseline using TFIDF term weighting with maximum marginal relevance ranking (MMR).

#index 340919
#* Enhanced topic distillation using text, markup tags, and hyperlinks
#@ Soumen Chakrabarti;Mukul Joshi;Vivek Tawde
#t 2001
#c 13
#% 115608
#% 197843
#% 262061
#% 268079
#% 278106
#% 282905
#% 330676
#% 330707
#% 330787
#% 375017
#% 406493
#% 438136
#% 479659
#% 504890
#% 706148
#% 748583
#! Topic distillation is the analysis of hyperlink graph structure to identify mutually reinforcing authorities (popular pages) and hubs (comprehensive lists of links to authorities). Topic distillation is becoming common in Web search engines, but the best-known algorithms model the Web graph at a coarse grain, with whole pages as single nodes. Such models may lose vital details in the markup tag structure of the pages, and thus lead to a tightly linked irrelevant subgraph winning over a relatively sparse relevant subgraph, a phenomenon called topic drift or contamination. The problem gets especially severe in the face of increasingly complex pages with navigation panels and advertisement links. We present an enhanced topic distillation algorithm which analyzes text, the markup tag trees that constitute HTML pages, and hyperlinks between pages. It thereby identifies subtrees which have high text- and hyperlink-based coherence w.r.t. the query. These subtrees get preferential treatment in the mutual reinforcement process. Using over 50 queries, 28 from earlier topic distillation work, we analyzed over 700,000 pages and obtained quantitative and anecdotal evidence that the new algorithm reduces topic drift.

#index 340920
#* Transparent Queries: investigation users' mental models of search engines
#@ Jack Muramatsu;Wanda Pratt
#t 2001
#c 13
#% 86380
#% 115181
#% 206505
#% 214709
#% 228114
#% 309767
#% 406493
#% 441300
#! Typically, commercial Web search engines provide very little feedback to the user concerning how a particular query is processed and interpreted. Specifically, they apply key query transformations without the users knowledge. Although these transformations have a pronounced effect on query results, users have very few resources for recognizing their existence and understanding their practical importance. We conducted a user study to gain a better understanding of users knowledge of and reactions to the operation of several query transformations that web search engines automatically employ. Additionally, we developed and evaluated Transparent Queries, a software system designed to provide users with lightweight feedback about opaque query transformations. The results of the study suggest that users do indeed have difficulties understanding the operation of query transformations without additional assistance. Finally, although transparency is helpful and valuable, interfaces that allow direct control of query transformations might ultimately be more helpful for end-users.

#index 340921
#* Why batch and user evaluations do not give the same results
#@ Andrew H. Turpin;William Hersh
#t 2001
#c 13
#% 46803
#% 144010
#% 167563
#% 169781
#% 218982
#% 290703
#% 309089
#% 309093
#! Much system-oriented evaluation of information retrieval systems has used the Cranfield approach based upon queries run against test collections in a batch mode. Some researchers have questioned whether this approach can be applied to the real world, but little data exists for or against that assertion. We have studied this question in the context of the TREC Interactive Track. Previous results demonstrated that improved performance as measured by relevance-based metrics in batch studies did not correspond with the results of outcomes based on real user searching tasks. The experiments in this paper analyzed those results to determine why this occurred. Our assessment showed that while the queries entered by real users into systems yielding better results in batch studies gave comparable gains in ranking of relevant documents for those users, they did not translate into better performance on specific tasks. This was most likely due to users being able to adequately find and utilize relevant documents ranked further down the output list.

#index 340922
#* Evaluating a content based image retrieval system
#@ Sharon McDonald;Ting-Sheng Lai;John Tait
#t 2001
#c 13
#% 232907
#% 262089
#% 280793
#% 324983
#% 334924
#% 434970
#% 741906
#% 1346975
#% 1783092
#! Content Based Image Retrieval (CBIR) presents special challenges in terms of how image data is indexed, accessed, and how end systems are evaluated. This paper discusses the design of a CBIR system that uses global colour as the primary indexing key, and a user centered evaluation of the systems visual search tools. The results indicate that users are able to make use of a range of visual search tools, and that different tools are used at different points in the search process. The results also show that the provision of a structured navigation and browsing tool can support image retrieval, particularly in situations in which the user does not have a target image in mind. The results are discussed in terms of their implications for the design of visual search tools, and their implications for the use of user-centered evaluation for CBIR systems.

#index 340924
#* Evaluating topic-driven web crawlers
#@ Filippo Menczer;Gautam Pant;Padmini Srinivasan;Miguel E. Ruiz
#t 2001
#c 13
#% 1527
#% 92148
#% 169805
#% 176502
#% 219052
#% 232653
#% 262061
#% 268073
#% 268079
#% 268087
#% 281166
#% 281251
#% 281253
#% 282905
#% 290703
#% 309104
#% 309151
#% 311040
#% 318412
#% 375017
#% 682442
#% 840583
#! Due to limited bandwidth, storage, and computational resources, and to the dynamic nature of the Web, search engines cannot index every Web page, and even the covered portion of the Web cannot be monitored continuously for changes. Therefore it is essential to develop effective crawling strategies to prioritize the pages to be indexed. The issue is even more important for topic-specific search engines, where crawlers must make additional decisions based on the relevance of visited pages. However, it is difficult to evaluate alternative crawling strategies because relevant sets are unknown and the search space is changing. We propose three different methods to evaluate crawling strategies. We apply the proposed metrics to compare three topic-driven crawling algorithms based on similarity ranking, link analysis, and adaptive agents.

#index 340928
#* Effective site finding using link anchor information
#@ Nick Craswell;David Hawking;Stephen Robertson
#t 2001
#c 13
#% 255179
#% 262061
#% 268079
#% 281174
#% 290830
#% 309145
#% 309150
#% 309151
#! Link-based ranking methods have been described in the literature and applied in commercial Web search engines. However, according to recent TREC experiments, they are no better than traditional content-based methods. We conduct a different type of experiment, in which the task is to find the main entry point of a specific Web site. In our experiments, ranking based on link anchor text is twice as effective as ranking based on document content, even though both methods used the same BM25 formula. We obtained these results using two sets of 100 queries on a 18.5 million document set and another set of 100 on a 0.4 million document set. This site finding effectiveness begins to explain why many search engines have adopted link methods. It also opens a rich new area for effectiveness improvement, where traditional methods fail.

#index 340932
#* Stable algorithms for link analysis
#@ Andrew Y. Ng;Alice X. Zheng;Michael I. Jordan
#t 2001
#c 13
#% 248027
#% 262061
#% 268079
#% 282905
#% 309151
#% 309868
#% 420495
#% 466574
#% 1289272
#! The Kleinberg HITS and the Google PageRank algorithms are eigenvector methods for identifying ``authoritative'' or ``influential'' articles, given hyperlink or citation information. That such algorithms should give reliable or consistent answers is surely a desideratum, and in~\cite{ijcaiPaper}, we analyzed when they can be expected to give stable rankings under small perturbations to the linkage patterns. In this paper, we extend the analysis and show how it gives insight into ways of designing stable link analysis methods. This in turn motivates two new algorithms, whose performance we study empirically using citation data and web hyperlink data.

#index 340934
#* Modeling score distributions for combining the outputs of search engines
#@ R. Manmatha;T. Rath;F. Feng
#t 2001
#c 13
#% 169781
#% 194246
#% 194275
#% 194276
#% 211820
#% 232703
#% 248010
#% 262080
#% 309211
#% 375017
#% 437405
#! In this paper the score distributions of a number of text search engines are modeled. It is shown empirically that the score distributions on a per query basis may be fitted using an exponential distribution for the set of non-relevant documents and a normal distribution for the set of relevant documents. Experiments show that this model fits TREC-3 and TREC-4 data for not only probabilistic search engines like INQUERY but also vector space search engines like SMART for English. We have also used this model to fit the output of other search engines like LSI search engines and search engines indexing other languages like Chinese.It is then shown that given a query for which relevance information is not available, a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution. These distributions can be used to map the scores of a search engine to probabilities. We also discuss how the shape of the score distributions arise given certain assumptions about word distributions in documents. We hypothesize that all 'good' text search engines operating on any language have similar characteristics.This model has many possible applications. For example, the outputs of different search engines can be combined by averaging the probabilities (optimal if the search engines are independent) or by using the probabilities to select the best engine for each query. Results show that the technique performs as well as the best current combination techniques.

#index 340936
#* Models for metasearch
#@ Javed A. Aslam;Mark Montague
#t 2001
#c 13
#% 71772
#% 71774
#% 109192
#% 174664
#% 219050
#% 232703
#% 273033
#% 340934
#% 340959
#% 420464
#% 708847
#% 709230
#! Given the ranked lists of documents returned by multiple search engines in response to a given query, the problem ofmetasearchis to combine these lists in a way which optimizes the performance of the combination. This paper makes three contributions to the problem of metasearch: (1) We describe and investigate a metasearch model based on an optimal democratic voting procedure, the Borda Count; (2) we describe and investigate a metasearch model based on Bayesian inference; and (3) we describe and investigate a model for obtaining upper bounds on the performance of metasearch algorithms. Our experimental results show that metasearch algorithms based on the Borda and Bayesian models usually outperform the best input system and are competitive with, and often outperform, existing metasearch strategies. Finally, our initial upper bounds demonstrate that there is much to learn about the limits of the performance of metasearch.

#index 340938
#* The score-distributional threshold optimization for adaptive binary classification tasks
#@ Avi Arampatzis;André van Hameran
#t 2001
#c 13
#% 169806
#% 194284
#% 262085
#% 262087
#% 280854
#! The thresholding of document scores has proved critical for the effectiveness of classification tasks. We review the most important approaches to thresholding, and introduce thescore-distributional (S-D) threshold optimizationmethod. The method is based on score distributions and is capable of optimizing any effectiveness measure defined in terms of the traditional contingency table.As a byproduct, we provide a model forscore distributions, and demonstrate its high accuracy in describing empirical data. The estimation method can be performed incrementally, a highly desirable feature for adaptive environments. Our work in modeling score distributions is useful beyond threshold optimization problems. It directly applies to other retrieval environments that make use of score distributions,e.g., distributed retrieval, or topic detection and tracking.The most accurate version of S-D thresholding --- although incremental --- can be computationally heavy. Therefore, we also investigate more practical solutions. We suggest practical approximations and discuss adaptivity, threshold initialization, and incrementality issues. The practical version of S-D thresholding has been tested in the context of the TREC-9 Filtering Track and found to be very effective [2].

#index 340940
#* A meta-learning approach for text categorization
#@ Wai Lam;Kwok-Yin Lai
#t 2001
#c 13
#% 165110
#% 165111
#% 169718
#% 190581
#% 219050
#% 219051
#% 219052
#% 219053
#% 260001
#% 262050
#% 280817
#% 309100
#% 311034
#% 316500
#% 316508
#% 458379
#% 465754
#% 466572
#% 1271840
#! We investigate a meta-model approach, called Meta-learning Using Document Feature characteristics (MUDOF), for the task of automatic textual document categorization. It employs a meta-learning phase using document feature characteristics. Document feature characteristics, derived from the training document set, capture some inherent category-specific properties of a particular category. Different from existing categorization methods, MUDOF can automatically recommend a suitable algorithm for each category based on the category-specific statistical characteristics. Hence, different algorithms may be employed for different categories. Experiments have been conducted on a real-world document collection demonstrating the effectiveness of our approach. The results confirm that our meta-model approach can exploit the advantage of its component algorithms, and demonstrate a better performance than existing algorithms.

#index 340941
#* Maximum likelihood estimation for filtering thresholds
#@ Yi Zhang;Jamie Callan
#t 2001
#c 13
#% 132779
#% 169777
#% 219048
#% 219049
#% 262085
#% 262087
#% 309122
#% 316500
#% 340934
#! Information filtering systems based on statistical retrieval models usually compute a numeric score indicating how well each document matches each profile. Documents with scores above profile-specificdissemination thresholdsare delivered.An optimal dissemination threshold is one that maximizes a given utility function based on the distributions of the scores of relevant and non-relevant documents. The parameters of the distribution can be estimated using relevance information, but relevance information obtained while filtering isbiased. This paper presents a new method of adjusting dissemination thresholds that explicitly models and compensates for this bias. The new algorithm, which is based on the Maximum Likelihood principle, jointly estimates the parameters of the density distributions for relevant and non-relevant documents and the ratio of the relevant document in the corpus. Experiments with TREC-8 and TREC-9 Filtering Track data demonstrate the effectiveness of the algorithm.

#index 340942
#* Unsupervised and supervised clustering for topic tracking
#@ Martin Franz;Todd Ward;J. Scott McCarley;Wei-Jing Zhu
#t 2001
#c 13
#% 262042
#% 262043
#% 309100
#% 309131
#! We investigate important differences between two styles of document clustering in the context of Topic Detection and Tracking. Converting a Topic Detection system into a Topic Tracking system exposes fundamental differences between these two tasks that are important to consider in both the design and the evaluation of TDT systems. We also identify features that can be used in systems for both tasks.

#index 340944
#* Intelligent information triage
#@ Sofus A. Macskassy;Foster Provost
#t 2001
#c 13
#% 124009
#% 240744
#% 262042
#% 262043
#% 262085
#% 280413
#% 309100
#% 316546
#% 316548
#% 318783
#% 331909
#% 406493
#% 445316
#% 465755
#% 465895
#% 969387
#% 1499571
#! In many applications, large volumes of time-sensitive textual information require triage: rapid, approximate prioritization for subsequent action. In this paper, we explore the use of prospective indications of the importance of a time-sensitive document, for the purpose of producing better document filtering or ranking. By prospective, we mean importance that could be assessed by actions that occur in the future. For example, a news story may be assessed (retrospectively) as being important, based on events that occurred after the story appeared, such as a stock price plummeting or the issuance of many follow-up stories. If a system could anticipate (prospectively) such occurrences, it could provide a timely indication of importance. Clearly, perfect prescience is impossible. However, sometimes there is sufficient correlation between the content of an information item and the events that occur subsequently. We describe a process for creating and evaluating approximate information-triage procedures that are based on prospective indications. Unlike many information-retrieval applications for which document labeling is a laborious, manual process, for many prospective criteria it is possible to build very large, labeled, training corpora automatically. Such corpora can be used to train text classification procedures that will predict the (prospective) importance of each document. This paper illustrates the process with two case studies, demonstrating the ability to predict whether a news story will be followed by many, very similar news stories, and also whether the stock price of one or more companies associated with a news story will move significantly following the appearance of that story. We conclude by discussing how the comprehensibility of the learned classifiers can be critical to success.}

#index 340946
#* Discovering information flow suing high dimensional conceptual space
#@ Dawei Song;Peter Bruza
#t 2001
#c 13
#% 65953
#% 237519
#% 319874
#% 353872
#! This paper presents an informational inference mechanism realized via the use of a high dimensional conceptual space. More specifically, we claim to have operationalized important aspects of G聞rdenforss recent three-level cognitive model. The connectionist level is primed with the Hyperspace Analogue to Language (HAL) algorithm which produces vector representations for use at the conceptual level. We show how inference at the symbolic level can be implemented by employing Barwise and Seligmans theory of information flow. This article also features heuristics for enhancing HAL-based representations via the use of quality properties, determining concept inclusion and computing concept composition. The worth of these heuristics in underpinning informational inference are demonstrated via a series of experiments. These experiments, though small in scale, show that informational inference proposed in this article has a very different character to the semantic associations produced by the Minkowski distance metric and concept similarity computed via the cosine coefficient. In short, informational inference generally uncovers concepts that are carried, or, in some cases, implied by another concept, (or combination of concepts).

#index 340948
#* A study of smoothing methods for language models applied to Ad Hoc information retrieval
#@ Chengxiang Zhai;John Lafferty
#t 2001
#c 13
#% 46803
#% 120104
#% 176530
#% 218982
#% 248058
#% 262096
#% 280850
#% 280851
#% 280864
#% 300542
#% 448725
#! Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections.

#index 340950
#* Topic segmentation with an aspect hidden Markov model
#@ David M. Blei;Pedro J. Moreno
#t 2001
#c 13
#% 199760
#% 278106
#% 280819
#! We present a novel probabilistic method for topic segmentation on unstructured text. One previous approach to this problem utilizes the hidden Markov model (HMM) method for probabilistically modeling sequence data [7]. The HMM treats a document as mutually independent sets of words generated by a latent topic variable in a time series. We extend this idea by embedding Hofmann's aspect model for text [5] into the segmenting HMM to form an aspect HMM (AHMM). In doing so, we provide an intuitive topical dependency between words and a cohesive segmentation model. We apply this method to segment unbroken streams of New York Times articles as well as noisy transcripts of radio programs on SpeechBot, an online audio archive indexed by an automatic speech recognition engine. We provide experimental comparisons which show that the AHMM outperforms the HMM for this task.

#index 340951
#* Finding topic words for hierarchical summarization
#@ Dawn Lawrie;W. Bruce Croft;Arnold Rosenberg
#t 2001
#c 13
#% 218978
#% 262112
#% 280840
#% 280849
#% 281480
#% 283171
#% 309115
#% 340901
#% 375017
#% 406493
#% 408396
#% 853647
#! Hierarchies have long been used for organization, summarization, and access to information. In this paper we define summarization in terms of a probabilistic language model and use the definition to explore a new technique for automatically generating topic hierarchies by applying a graph-theoretic algorithm, which is an approximation of the Dominating Set Problem. The algorithm efficiently chooses terms according to a language model. We compare the new technique to previous methods proposed for constructing topic hierarchies including subsumption and lexical hierarchies, as well as the top TF.IDF terms. Our results show that the new technique consistently performs as well as or better than these other techniques. They also show the usefulness of hierarchies compared with a list of terms.

#index 340953
#* Exploiting redundancy in question answering
#@ Charles L. A. Clarke;Gordon V. Cormack;Thomas R. Lynam
#t 2001
#c 13
#% 144033
#% 268079
#% 281174
#% 306494
#% 309124
#% 309127
#% 309150
#% 478263
#% 742082
#% 742086
#% 815325
#! Our goal is to automatically answer brief factual questions of the form ``When was the Battle of Hastings?'' or ``Who wrote The Wind in the Willows?''. Since the answer to nearly any such question can now be found somewhere on the Web, the problem reduces to finding potential answers in large volumes of data and validating their accuracy. We apply a method for arbitrary passage retrieval to the first half of the problem and demonstrate that answer redundancy can be used to address the second half. The success of our approach depends on the idea that the volume of available Web data is large enough to supply the answer to most factual questions multiple times and in multiple contexts. A query is generated from a question and this query is used to select short passages that may contain the answer from a large collection of Web data. These passages are analyzed to identify candidate answers. The frequency of these candidates within the passages is used to ``vote'' for the most likely answer. The approach is experimentally tested on questions taken from the TREC-9 question-answering test collection. As an additional demonstration, the approach is extended to answer multiple choice trivia questions of the form typically asked in trivia quizzes and television game shows.

#index 340954
#* High performance question/answering
#@ Marius A. Pasca;Sandra M. Harabagiu
#t 2001
#c 13
#% 198058
#% 742102
#% 748722
#% 815828
#! In this paper we present the features of a Question/Answering (Q/A) system that had unparalleled performance in the TREC-9 evaluations. We explain the accuracy of our system through the unique characteristics of its architecture: (1) usage of a wide-coverage answer type taxonomy; (2) repeated passage retrieval; (3) lexico-semantic feedback loops; (4) extraction of the answers based on machine learning techniques; and (5) answer caching. Experimental results show the effects of each feature on the overall performance of the Q/A system and lead to general conclusions about Q/A from large text collections.

#index 340957
#* Searcher performance in question answering
#@ Mingfang Wu;Michael Fuller;Ross Wilkinson
#t 2001
#c 13
#% 194251
#% 218992
#% 230530
#% 262075
#% 309089
#% 309115
#% 329092
#% 394709
#% 729027
#! There are many tasks that require information finding. Some can be largely automated, and others greatly benefit from successful interaction between system and searcher. We are interested in the task of answering questions where some synthesis of information is required-the answer would not generally be given from a single passage of a single document. We investigate whether variation in the way a list of documents is delivered affected searcher performance in the question answering task. We will show that there is a significant difference in performance using a list customized to the task type, compared with a standard web-engine list. This indicates that paying attention to the task and the searcher interaction may provide substantial improvement in task performance.

#index 340958
#* Toward an improved concept-based information retrieval system
#@ Peter V. Henstock;Daniel J. Pack;Young-Suk Lee;Clifford J. Weinstein
#t 2001
#c 13
#% 131325
#% 406493
#% 742368
#! This paper presents a novel information retrieval system that includes 1) the addition of concepts to facilitate the identification of the correct word sense, 2) a natural language query interface, 3) the inclusion of weights and penalties for proper nouns that build upon the Okapi weighting scheme, and 4) a term clustering technique that exploits the spatial proximity of search terms in a document to further improve the performance. The effectiveness of the system is validated by experimental results.

#index 340959
#* Metasearch consistency
#@ Mark Montague;Javed A. Aslam
#t 2001
#c 13
#% 174664
#% 232703
#% 340936
#% 709230
#! We investigate the performance of metasearch algorithms in terms of how much they improve consistency. We find that three different metasearch algorithms, each over three datasets, usually improve the consistency of search results; sometimes the improvement is dramatic. Furthermore, consistency tends to improve when performance improves.

#index 340960
#* Anchor text mining for translation extraction of query terms
#@ Wen-Hsiang Lu;Lee-Feng Chein;Hsi-Jian Lee
#t 2001
#c 13
#% 176530
#% 211043
#% 262047
#% 280826
#! This paper presents an approach to automatically extracting the bilingual translations of many Web query terms through mining the Web anchor texts. Some preliminary experiments are conducted on using 109,416 Web pages containing both Chinese and English anchor texts in their in-links to extract Chinese translations of 200 English queries selected from popular query terms in Taiwan. It is found that the effective translations of 75% of the popular query terms can be extracted, in which 87.2% cannot be obtained in common translation dictionaries.

#index 340961
#* Selecting expansion terms in automatic query expansion
#@ Hiroko Mano;Yasushi Ogawa
#t 2001
#c 13
#% 232645

#index 340962
#* An experimental framework for email categorization and management
#@ Kenricj Mock
#t 2001
#c 13
#% 214751
#% 466564
#% 637575
#! Many problems are difficult to adequately explore until a prototype exists in order to elicit user feedback. One such problem is a system that automatically categorizes and manages email. Due to a myriad of user interface issues, a prototype is necessary to determine what techniques and technologies are effective in the email domain. This paper describes the implementation of an add-in for Microsoft Outlook 2000 TM that intends to address two problems with email: 1) help manage the inbox by automatically classifying email based on user folders, and 2) to aid in search and retrieval by providing a list of email relevant to the selected item. This add-in represents a first step in an experimental system for the study of other issues related to information management. The system has been set up to allow experimentation with other classification algorithms and the source code is available online in an effort to promote further experimentation.

#index 340963
#* Analyses of multiple-evidence combinations for retrieval strategies
#@ Abdur Chowdhury;Ophir Frieder;David Grossman;Catherine McCabe
#t 2001
#c 13
#% 184496
#% 218982
#% 232703
#% 280903

#index 340964
#* Flexible pseudo-relevance feedback using optimization tables
#@ Tetsuya Sakai;Stephen E. Robertson
#t 2001
#c 13
#% 316891

#index 340966
#* Quantifying the utility of parallel corpora
#@ Martin Franz;J. Scott McCarley;Todd Ward;Wei-Jing Zhu
#t 2001
#c 13
#% 740915
#% 786534
#! Our English-Chinese cross-language IR system is trained from parallel corpora; we investigate its performance as a function of training corpus size for three different training corpora. We find that the performance of the system as trained on the three parallel corpora can be related by a simple measure, namely the out-of-vocabulary rate of query words.

#index 340967
#* Unitary operators for fast latent semantic indexing (FLSI)
#@ Eduard Hoenkamp
#t 2001
#c 13
#% 116390
#% 200694
#% 470400
#! Latent Semantic Indexing (LSI) dramatically reduces the dimension of the document space by mapping it into a space spanned by conceptual indices. Empirically, the number of concepts that can represent the documents are far fewer than the great variety of words in the textual representation. Although this almost obviates the problem of lexical matching, the mapping incurs a high computational cost compared to document parsing, indexing, query matching, and updating. This paper shows how LSI is based on a unitary transformation, for which there are computationally more attractive alternatives. This is exemplified by the Haar transform, which is memory efficient, and can be computed in linear to sublinear time. The principle advantages of LSI are thus preserved while the computational costs are drastically reduced.

#index 340968
#* Probabilistic combination of content and links
#@ Rong Jin;Susan Dumais
#t 2001
#c 13
#% 262061
#% 262078
#% 268079
#% 282905
#% 309104
#% 309749
#! Previous research has shown that citations and hypertext links can be usefully combined with document content to improve retrieval. Links can be used in many ways, e.g., link topology can be used to identify important pages, anchor text can be used to augment the text of cited pages, and activation can be spread to linked pages. This paper introduces a probabilistic model that integrates content matching and these three uses of link information in a single unified framework. Experiments with a web collection show benefits for link information especially for general queries.

#index 340970
#* Structure and content-based segmentation of speech transcripts
#@ Dulce Ponceleon;Savitha Srinivasan
#t 2001
#c 13
#% 309102
#! algorithm for the segmentation of an audio/video source into topically cohesive segments based on automatic speech recognition (ASR) transcriptions is presented. A novel two-pass algorithm is described that combines a boundary-based method with a content-based method. In the first pass, the temporal proximity and the rate of arrival of ngram features is analyzed in order to compute an initial segmentation. In the content- based second pass, changes in content-bearing words are detected by using the ngram features as queries in an information-retrieval system. The second pass validates the initial segments and merges them as needed. Feasibility of the segmentation task can vary enormously depending on the structure of the audio content, and the accuracy of ASR. For real-world corporate training data our method identifies, at worst, a single salient segment of the audio and, at best, a high-level table-of-contents. We illustrate the algorithm in detail with some examples and validate the results with segmentation boundaries generated manually.

#index 340971
#* Text summarization via hidden Markov models
#@ John M. Conroy;Dianne P. O'leary
#t 2001
#c 13
#% 194251
#% 280836
#% 438324
#! A sentence extract summary of a document is a subset of the document's sentences that contains the main ideas in the document. We present an approach to generating such summaries, a hidden Markov model that judges the likelihood that each sentence should be contained in the summary. We compare the results of this method with summaries generated by humans, showing that we obtain significantly higher agreement than do earlier methods.

#index 340974
#* Reading time, scrolling and interaction: exploring implicit sources of user preferences for relevance feedback
#@ Diane Kelly;Nicholas J. Belkin
#t 2001
#c 13
#% 127574
#% 169803
#% 220711
#% 230395
#% 302085

#index 340988
#* Interactive phrase browsing within compressed text
#@ Raymond Wan;Alistair Moffat
#t 2001
#c 13
#% 223799
#% 237338
#% 262101
#% 327256

#index 340989
#* Query-biased web page summarisation: a task-oriented evaluation
#@ Ryen White;Joemon M. Jose;Ian Ruthven
#t 2001
#c 13
#% 262036
#! We present a system that offers a new way of assessing web document relevance and new approach to the web-based evaluation of such a system. Provisionally named WebDocSum, the system is a query-biased web page summariser that aims to provide an alternative to the short, irrelevant abstracts typical of many web search result lists. Based on an initial evaluation the system appears to be more useful in helping users gauge document relevance than the traditional ranked titles/abstracts approach.

#index 340990
#* Query expansion based on predictive algorithms for collaborative filtering
#@ Keiichiro Hoashi;Kazunori Matsumoto;Naomi Inoue;Kazuo Hashimoto
#t 2001
#c 13
#% 1650569

#index 340991
#* Query optimization for vector space problems
#@ K. Goda;T. Tamura;M. Kitsuregawa;A. Chowdhury;O. Frieder
#t 2001
#c 13
#% 218982
#% 224702
#% 347040
#! We present performance measurement results for a parallel SQL based information retrieval system implemented on a PC cluster system. We used the Web-TREC dataset under a left-deep query execution plan. We achieved satisfactory speed up.

#index 340992
#* Generic topic segmentation of document texts
#@ Marie-Francine Moens;Rik De Busser
#t 2001
#c 13
#% 267664
#% 740329
#% 741058
#! Topic segmentation is an important initial step in many text-based tasks. A hierarchical representation of a texts topics is useful in retrieval and allows judging relevancy at different levels of detail. This short paper describes research on generic algorithms for topic detection and segmentation that are applicable on texts of heterogeneous types and domains.

#index 340993
#* Towards the use of prosodic information for spoken document retrieval
#@ Fabio Crestani
#t 2001
#c 13
#% 617227

#index 340994
#* A homogeneous framework to model relevance feedback
#@ David E. Losada;Alvaro Barreiro
#t 2001
#c 13
#% 54435
#% 280824
#% 309123
#% 566401
#! Relevance feedback is an appreciated process to produce increasingly better retrieval. Usually, positive feedback plays a fundamental role in the feedback process whereas the role of negative feedback is limited. We think that negative feedback is a promising precision oriented mechanism and we propose a logical framework in which positive and negative feedback are homogeneously modeled. Evaluation results against small test collections are provided.

#index 340995
#* Combining semantic and syntactic document classifiers to improve first story detection
#@ Nicola Stokes;Joe Carthy
#t 2001
#c 13
#% 389801
#% 740329
#! In this paper we describe a type of data fusion involving the combination of evidence derived from multiple document representations. Our aim is to investigate if a composite representation can improve the online detection of novel events in a stream of broadcast news stories. This classification process otherwise known as first story detection FSD (or in the Topic Detection and Tracking pilot study as online new event detection [1]), is one of three main classification tasks defined by the TDT initiative. Our composite document representation consists of a semantic representation (based on the lexical chains derived from a text) and a syntactic representation (using proper nouns). Using the TDT1 evaluation methodology, we evaluate a number of document representation combinations using these document classifiers.

#index 340996
#* Browsing in a digital library collecting linearly arranged documents
#@ Yanhua Qu;Keizo Sato;Makoto Nakashima;Tetsuro Ito
#t 2001
#c 13
#% 92533
#% 234992
#% 282956
#% 287228
#% 309088
#! A method of assisting a user in finding the required documents effectively is proposed. A user being informed which documents are worth examining can browse in a digital library (DL) in a linear fashion. Computational evaluations were carried out, and a DL and its navigator are designed and constructed.

#index 340997
#* Feature selection for polyphonic music retrieval
#@ Jeremy Pickens
#t 2001
#c 13
#% 260018
#% 261882
#% 261908
#% 281389
#% 449284

#index 340998
#* Automatic information extraction from web pages
#@ Budi Rahardjo;Roland H. C. Yap
#t 2001
#c 13
#% 271065
#% 511733
#% 979358
#! Many web pages have implicit structure. In this paper, we show the feasibility of automatically extracting data from web pages by using approximate matching techniques. This can be applied to generate automatic wrappers or to notify/display web page differences, web page change monitoring, etc.

#index 340999
#* Automatic web search query generation to create minority language corpora
#@ Rayid Ghani;Rosie Jones;Dunja Mladenic
#t 2001
#c 13
#% 144007
#% 316522
#% 466266
#! The Web is a valuable source of language specific resources but collecting, organizing and utilizing this information is difficult. We describe CorpusBuilder, an approach for automatically generating Web-search queries to collect documents in a minority language. It differs from pseudo-relevance feedback in that retrieved documents are labeled by an automatic language classifier as relevant or irrelevant and a subset of documents is used to generate new queries. We experiment with various query-generation methods and query-lengths to find inclusion/exclusion terms that are helpful for finding documents in the target language and find that using odds-ratio scores calculated over the documents acquired so far was one of the most consistently accurate query-generation methods. We also describe experiments using a handful of words elicited from a user instead of initial documents and show that the methods perform similarly. Applying the same approach to multiple languages show that our system generalizes to a variety of languages.

#index 341000
#* Perpetual consistency improves image retrieval performance
#@ Huizhong Long;Wee Kheng Leow
#t 2001
#c 13
#% 234793
#% 284557
#% 592183
#% 592279
#% 626946
#% 1289337
#! An ideal retrieval system should retrieve images that satisfy the user's need, and should, therefore, measure image similarity in a manner consistent with human's perception. However, existing computational similarity measures are not perceptually consistent. This paper proposes an approach of improving retrieval performance by improving the perceptual consistency of computational similarity measures for textures based on relevance feedback judgments.

#index 341001
#* Intelligent object-based image retrieval suing cluster-driven personal preference learning
#@ Kyong-Mi Lee;W. Nick Street
#t 2001
#c 13
#! This paper introduces a personalization method for image retrieval based on the learning of personal preferences. The proposed system indexes objects based on shape and groups them into a set of clusters, or prototypes. Our personalization method refines corresponding prototypes from objects provided by the user in the foreground, and simultaneously adapts the database index in the background.

#index 341002
#* Construction of a hierarchical classifier schema using a combination of text-based and image-based approaches
#@ Cheng Lu;Mark S. Drew
#t 2001
#c 13
#% 44876
#% 482113
#% 718454
#! Web document hierarchical classification approaches often rely on textual features alone even though web pages include multimedia data. We propose a new hierarchical integrated web classification approach that combines image-based and text-based approaches. Instead of using a flat classifier to combine text and image classification, we perform classification on a hierarchy differently on different levels of the tree, using text for branches and images only at leaves. The results of our experiments show that the use of the hierarchical structure improved web document classification performance significantly.

#index 341003
#* A method based on the chi-square test for document classification
#@ Michael Oakes;Robert Gaaizauskas;Helene Fowkes;Anna Jonsson;Vincent Wan;Micheline Beaulieu
#t 2001
#c 13
#% 260001
#! We introduce a method for document classification based on using the chi-square test to identify characteristic vocabulary of document classes.

#index 341006
#* Query clustering using content words and user feedback
#@ Ji-Rong Wen;Jian-Yun Nie;Hong-Jiang Zhang
#t 2001
#c 13
#% 310567
#% 375017
#! Query clustering is crucial for automatically discovering frequently asked queries (FAQs) or most popular topics on a question-answering search engine. Due to the short length of queries, the traditional approaches based on keywords are not suitable for query clustering. This paper describes our attempt to cluster similar queries according to their contents as well as the document click information in the user logs.

#index 341009
#* Modifications of Kleinberg's HITS algorithm using matrix exponentiation and web log records
#@ Joel C. Miller;Gregory Rae;Fred Schaefer;Lesley A. Ward;Thomas LoFaro;Ayman Farahat
#t 2001
#c 13
#% 249110
#% 290830

#index 341011
#* Cite me, cite my references?: (Scholarly use of the ACM SIGIR proceedings based on two citation indexes)
#@ Elana Broch
#t 2001
#c 13
#% 287209
#% 433674
#! A three-part study was designed to document Internet use in scholarly research, using the Annual SIGIR Conference Proceedings from 1997 through 1999. The results suggest an increasing trend toward electronic self-publishing. Furthermore, while electronic availability did not insure that one would be cited, the most highly cited articles were available on the "free" web. The study also found that electronic availability has not, in most cases, decreased the length of time between publication and citation.

#index 341012
#* iFind: a web image search engine
#@ Zheng Chen;Liu Wenyin;Chunhui Hu;Mingjing Li;Hong-Jiang Zhang
#t 2001
#c 13
#% 316148

#index 341013
#* Building interoperable digital library services: MARIAN, open archives, and the NDLTD
#@ Edward A. Fox;Robert France;Marcos Andre Goncalves;Hussein Suleman
#t 2001
#c 13
#! In this demonstration, we present interoperable and personalized search services for the Networked Digital Library of Theses and Dissertations (NDLTD). Using standard protocols and software, including those specified by the Open Archives Initiative (OAI), distributed sites can share metadata easily. On top of these harvesting protocols, we implement a union collection of theses managed by the MARIAN digital library system. Our demonstration covers aspects of NDLTD, OAI, and MARIAN.

#index 341014
#* AUTINDEX: an automatic multilingual indexing system
#@ Bärbel Ripplinger;Paul Schmidt
#t 2001
#c 13

#index 341015
#* Does visualization improve our ability to find and learn from internet based information?
#@ Daniel A. Kauwell;Jim Levin;Hwan Jo Yu;Young Jin Lee;Jeff Ellen;Arun Bahalla
#t 2001
#c 13
#% 249193

#index 341016
#* The HySpirit retrieval platform
#@ Thomas Rölleke;Ralf Lübeck;Gabriella Kazai
#t 2001
#c 13
#% 215225
#% 458744

#index 341017
#* Distributed resource discovery and structured data searching with Chesire II
#@ Ray R. Larson
#t 2001
#c 13
#% 262063
#% 262065
#! This demonstration will show describe the construction and application of Cross-Domain Information Servers using features of the standard Z39.50 information retrieval protocol[Z39.50]. The system is currently being used to build and search distributed indexes for databases with disparate structured data (SGML and XML). We use the Z39.50 Explain Database to determine the databases and indexes of a given server, then use the Z39.50 SCAN facility to extract the contents of the indexes. This information is used to build collection documents that can be retrieved using probabilistic retrieval algorithms.

#index 341018
#* Searching the deep web: distributed explorit directed query applications
#@ Valerie S. Allen;Abe Lederman
#t 2001
#c 13
#! In 1999 a directed query distributed search engine was integrated into a new Department of Energy Virtual Library of Energy Science and Technology. Millions of pages of government information across multiple agencies were made immediately searchable via one query, setting the stage for the development of a variety of interagency initiatives and applications.

#index 341019
#* CROWSE: a system for organizing repositories and web search results
#@  Kinshuman;Sudeshna Sarkar
#t 2001
#c 13

#index 341020
#* MS read: user modeling in the web environment
#@ Natasa Milió-Frayling;Ralph Sommerer
#t 2001
#c 13
#% 403090
#! MS Read is a prototype application implemented as an extension of the Web Browser that creates an evolving model of the users topic of interest. It uses that model to analyze documents that are accessed while searching and browsing the Web. In the presented version of MS Read the model is used to highlight topic related terminology in the documents. MS Read model of the user need is created by applying natural language processing to search queries captured within the Browser and to topic descriptions explicitly provided by the user while browsing and reading documents. It is semantically enhanced using linguistic and custom knowledge resources.

#index 397121
#* Landmarks in information retrieval: the message out of the bottle
#@ Keith Van Rijsbergen
#t 2002
#c 13
#! For many years I have wanted to give a talk like this: look backon our subject, identify the high (and perhaps low) points,consider what worked, what did not work, and speculate a littleabout the future. Now that I at last have the opportunity to givesuch a talk the realisation has dawned just how difficult it is todo justice to the topic. The only way out of this difficulty for meis to emphasise that this is a personal account, based on myinvolvement with the field since 1968, and that errors of omissionand commission are not deliberate but simply due to lack ofknowledge and time on my part.To talk of landmarks is easy but to say what they are in IR isnot. They come in various shapes and sizes: events, publications,experiment, ideas, etc. In the course of this presentation I shallbe judiciously mixing all of these. However, the emphasis will beon ideas and their subsequent modelling and testing throughexperimentation. The interaction between theory and experiment willbe a recurring theme. I will try and associate these developmentswith key individuals, thereby running the risk of ignoring some; Iapologise for this in advance.The pre-history of our subject can be traced back to the work inthe 19th century, perhaps even further, but I will pick it up atthe middle of the last century (20th) starting with the work ofRobert Fairthorne and Vannevar Bush. This early work emphasised thepossibility of using mechanical devices to store and retrieveinformation. Of course the foundations of modern informationretrieval were properly laid after 1945 with the pioneering work ofCleverdon, Salton, Sparck Jones, and others. This work gave rise toa strong experimental methodology for the evaluation of theoreticalideas, which has been sustained to this day. It has been a hallmarkof IR research that theory is developed in the context ofexperimentation. There is no doubt that many disciplines arejealous of the success of TREC.IR research has thrown up a number of successful models. Thesemodels have been based on some, often unstated, assumptions (orhypotheses). I will attempt to identify some of the underlyingideas, giving credit where is due, that led to the fruitfulexploration of retrieval models. This will include system-orientedas well as user-oriented ideas, especially those concerned with themeasurement of retrieval performance.IR has been fortunate in that the subject has grown through theactive collaboration between computer scientists and informationscientists. This has meant that traditional approaches to thestorage and retrieval of information emanating from the libraryworld, for example, have always strongly influenced newdevelopments. This tension between manual (human) processes andautomatic computer-based processes in IR has always been fruitful.Even now with the evolution of ideas about meta-data and ontologiesneeded to enhance web retrieval, the debate about controlledvocabularies versus automatic indexing is relevant. Issues ofscalability are particularly important here.One of the strengths that have emerged in our subject is thatmany of our models can be deployed independently of medium ormodality. For example, retrieving images or audio sequences can behandled in ways similar to those used to retrieve text data. Thishas proved to be great boon to IR. The development of web retrievalthrough the deployment of various kinds of search engines has beenbased on the considerable early work in IR although detailing thespecific influences is not easy. It is clear that the underlyingmathematical and statistical models in IR have been ubiquitous inapplication. The extreme difficulty encountered in making NLP workfor IR forced researchers to develop powerful statistical,probabilistic, geometrical, and logical techniques to complementlinguistic ones. This is now paying off because of the similardifficulties encountered in other media.Having given some account of how we got here I will spend alittle time talking about where we go from here, how do we extractthe message from the bottle?

#index 397122
#* Is natural language an inconvenience or an opportunity for IR?
#@ Kimmo Koskenniemi
#t 2002
#c 13
#! Natural language (NL) has evolved to facilitate human communication. It enables the speaker to make the listener's mind wander among her experiences and mental associations roughly according to the intentions of the speaker. The speaker and the listener usually share experiences and expectations, and they use mostly the same units and rules of a shared NL. Written language functions similarly, but in a less interactive way, with fewer possibilities for feedback.Both the symbols of NL (i.e. words or morphemes), and their arrangements are meaningful. Not with universal and precise meanings, but similar enough among different speakers and accurate enough for the communication mostly to succeed.NLs are mostly very large systems. Hundreds of thousands of words and infinitely many possible utterances. Even inflection alone might produce huge numbers of forms, e.g. more than ten thousand distinct forms out of every Finnish verb entry.NL processing (for IR or any other purpose) must cope with phenomena like (1) inflection and compounding, (2) synonymy, (3) polysemy, (4) ambiguity, (5) anaphora and (6) head-modifier relations among words and phrases.Language technology can neutralize much of the effect of these 'inconveniences' inherent with NL, but what kinds of advantages could NL have? Redundant use of synonymous expressions can effectively identify new concepts. Multilingual parallel documents may help in identifying their exact content. NLs typically carry connotations, i.e. what is implied but not explicitly said (e.g. attitudes, politeness). Vague associations are easy to express in NL, but not always in formal systems (e.g. "a few years ago there was an article about the rival of Yeltsin - I don't remember his name but - he then went over to some region in Siberia - but what did the guy promise?") Jokes and humor belong to NLs, not to formal systems. .Are there any alternatives for NL? Not really, because any artificial and more precise formalisms fail to adapt to new concepts and they do not easily allow restructuring of previous ideas.One challenge for language technology is to find better solutions for the above 'inconveniences' in order to provide various IR, document classification, indexing and summarizing methods with more accurate and adequate input data. With more accurate input some of the more demanding tasks of IR can perhaps be solved.

#index 397123
#* Impact transformation: effective and efficient web retrieval
#@ Vo Ngoc Anh;Alistair Moffat
#t 2002
#c 13
#% 46803
#% 67565
#% 157880
#% 198335
#% 212665
#% 213786
#% 218982
#% 228097
#% 290703
#% 309093
#% 340886
#% 340887
#% 396728
#! We extend the applicability of impact transformation, which is a technique for adjusting the term weights assigned to documents so as to boost the effectiveness of retrieval when short queries are applied to large document collections. In conjunction with techniques called quantization and thresholding, impact transformation allows improved query execution rates compared to traditional vector-space similarity computations, as the number of arithmetic operations can be reduced. The transformation also facilitates a new dynamic query pruning heuristic. We give results based upon the trec web data that show the combination of these various techniques to yield highly competitive retrieval, in terms of both effectiveness and efficiency, for both short and long queries.

#index 397124
#* Analysis of lexical signatures for finding lost or related documents
#@ Seung-Taek Park;David M. Pennock;C. Lee Giles;Robert Krovetz
#t 2002
#c 13
#% 176503
#% 197528
#% 209685
#% 268197
#% 309095
#% 340892
#% 438103
#% 438365
#% 584893
#! A lexical signature of a web page is often sufficient for finding the page, even if its URL has changed. We conduct a large-scale empirical study of eight methods for generating lexical signatures, including Phelps and Wilensky's [14] original proposal (PW) and seven of our own variations. We examine their performance on the web and on a TREC data set, evaluating their ability both to uniquely identify the original document and to locate other relevant documents if the original is lost. Lexical signatures chosen to minimize document frequency (DF) are good at unique identification but poor at finding relevant documents. PW works well on the relatively small TREC data set, but acts almost identically to DF on the web, which contains billions of documents. Term-frequency-based lexical signatures (TF) are very easy to compute and often perform well, but are highly dependent on the ranking system of the search engine used. In general, TFIDF-based method and hybrid methods (which combine DF with TF or TFIDF) seem to be the most promising candidates for generating effective lexical signatures.

#index 397125
#* Using sampled data and regression to merge search engine results
#@ Luo Si;Jamie Callan
#t 2002
#c 13
#% 184489
#% 194245
#% 194275
#% 227891
#% 232703
#% 262063
#% 280853
#% 280856
#% 282422
#% 301225
#% 309133
#% 309253
#% 316534
#% 340146
#% 340934
#% 340936
#% 481748
#% 567255
#! This paper addresses the problem of merging results obtained from different databases and search engines in a distributed information retrieval environment. The prior research on this problem either assumed the exchange of statistics necessary for normalizing scores (cooperative solutions) or is heuristic. Both approaches have disadvantages. We show that the problem in uncooperative environments is simpler when viewed as a component of a distributed IR system that uses query-based sampling to create resource descriptions. Documents sampled for creating resource descriptions can also be used to create a sample centralized index, and this index is a source of training data for adaptive results merging algorithms. A variety of experiments demonstrate that this new approach is more effective than a well-known alternative, and that it allows query-by-query tuning of the results merging function.

#index 397126
#* The Importance of Prior Probabilities for Entry Page Search
#@ Wessel Kraaij;Thijs Westerveld;Djoerd Hiemstra
#t 2002
#c 13
#% 46803
#% 169781
#% 218982
#% 253191
#% 262034
#% 262061
#% 262096
#% 268079
#% 290830
#% 309145
#% 309151
#% 340928
#% 340932
#% 376266
#% 458369
#% 466574
#% 729027
#! An important class of searches on the world-wide-web has the goal to find an entry page (homepage) of an organisation. Entry page search is quite different from Ad Hoc search. Indeed a plain Ad Hoc system performs disappointingly. We explored three non-content features of web pages: page length, number of incoming links and URL form. Especially the URL form proved to be a good predictor. Using URL form priors we found over 70% of all entry pages at rank 1, and up to 89% in the top 10. Non-content features can easily be embedded in a language model framework as a prior probability.

#index 397127
#* Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term
#@ Djoerd Hiemstra
#t 2002
#c 13
#% 46803
#% 169741
#% 169781
#% 169817
#% 228097
#% 252472
#% 262096
#% 277396
#% 280850
#% 280851
#% 287253
#% 340899
#% 340920
#% 340948
#% 342682
#% 375017
#% 397126
#! This paper follows a formal approach to information retrieval based on statistical language models. By introducing some simple reformulations of the basic language modeling approach we introduce the notion of importance of a query term. The importance of a query term is an unknown parameter that explicitly models which of the query terms are generated from the relevant documents (the important terms), and which are not (the unimportant terms). The new language modeling approach is shown to explain a number of practical facts of today's information retrieval systems that are not very well explained by the current state of information retrieval theory, including stop words, mandatory terms, coordination level ranking and retrieval using phrases.

#index 397128
#* Title language model for information retrieval
#@ Rong Jin;Alex G. Hauptmann;Cheng Xiang Zhai
#t 2002
#c 13
#% 262096
#% 280850
#% 280851
#% 340882
#% 340899
#% 340901
#% 340948
#% 740915
#! In this paper, we propose a new language model, namely, a title language model, for information retrieval. Different from the traditional language model used for retrieval, we define the conditional probability P(Q|D) as the probability of using query Q as the title for document D. We adopted the statistical translation model learned from the title and document pairs in the collection to compute the probability P(Q|D). To avoid the sparse data problem, we propose two new smoothing methods. In the experiments with four different TREC document collections, the title language model for information retrieval with the new smoothing method outperforms both the traditional language model and the vector space model for IR significantly.

#index 397129
#* Two-stage language models for information retrieval
#@ ChengXiang Zhai;John Lafferty
#t 2002
#c 13
#% 46803
#% 218982
#% 262092
#% 262096
#% 280850
#% 280851
#% 321635
#% 340899
#% 340901
#% 340948
#% 443940
#! The optimal settings of retrieval parameters often depend on both the document collection and the query, and are usually found through empirical tuning. In this paper, we propose a family of two-stage language models for information retrieval that explicitly captures the different influences of the query and document collection on the optimal settings of retrieval parameters. As a special case, we present a two-stage smoothing method that allows us to estimate the smoothing parameters completely automatically. In the first stage, the document language model is smoothed using a Dirichlet prior with the collection language model as the reference model. In the second stage, the smoothed document language model is further interpolated with a query background language model. We propose a leave-one-out method for estimating the Dirichlet parameter of the first stage, and the use of document mixture models for estimating the interpolation parameter of the second stage. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to---or better than---the best results achieved using a single smoothing method and exhaustive parameter search on the test data.

#index 397130
#* Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes
#@ Ryen W. White;Ian Ruthven;Joemon M. Jose
#t 2002
#c 13
#% 92696
#% 127574
#% 232719
#% 306468
#% 309115
#% 565237
#% 717120
#! In this paper we present an evaluation of techniques that are designed to encourage web searchers to interact more with the results of a web search. Two specific techniques are examined: the presentation of sentences that highly match the searcher's query and the use of implicit evidence. Implicit evidence is evidence captured from the searcher's interaction with the retrieval results and is used to automatically update the display. Our evaluation concentrates on the effectiveness and subject perception of these techniques. The results show, with statistical significance, that the techniques are effective and efficient for information seeking.

#index 397131
#* Predicting category accesses for a user in a structured information space
#@ Mao Chen;Andrea S. LaPaugh;Jaswinder Pal Singh
#t 2002
#c 13
#% 118771
#% 209662
#% 214673
#% 220711
#% 232912
#% 284823
#% 316139
#% 325198
#% 327254
#% 443194
#% 630984
#% 963898
#% 1275346
#! In a categorized information space, predicting users' information needs at the category level can facilitate personalization, caching and other topic-oriented services. This paper presents a two-phase model to predict the category of a user's next access based on previous accesses. Phase 1 generates a snapshot of a user's preferences among categories based on a temporal and frequency analysis of the user's access history. Phase 2 uses the computed preferences to make predictions at different category granularities. Several alternatives for each phase are evaluated, using the rating behaviors of on-line raters as the form of access considered. The results show that a method based on re-access pattern and frequency analysis of a user's whole history has the best prediction quality, even over a path-based method (Markov model) that uses the combined history of all users.

#index 397132
#* Detecting and Browsing Events in Unstructured text
#@ David A. Smith
#t 2002
#c 13
#% 78171
#% 144012
#% 218992
#% 230530
#% 262042
#% 287196
#% 295522
#% 309096
#% 330677
#% 332733
#% 337507
#% 342660
#% 740900
#% 815099
#! Previews and overviews of large, heterogeneous information resources help users comprehend the scope of collections and focus on particular subsets of interest. For narrative documents, questions of "what happened? where? and when?" are natural points of entry. Building on our earlier work at the Perseus Project with detecting terms, place names, and dates, we have exploited co-occurrences of dates and place names to detect and describe likely events in document collections. We compare statistical measures for determining the relative significance of various events. We have built interfaces that help users preview likely regions of interest for a given range of space and time by plotting the distribution and relevance of various collocations. Users can also control the amount of collocation information in each view. Once particular collocations are selected, the system can identify key phrases associated with each possible event to organize browsing of the documents themselves.

#index 397133
#* Novelty and redundancy detection in adaptive filtering
#@ Yi Zhang;Jamie Callan;Thomas Minka
#t 2002
#c 13
#% 262043
#% 280850
#% 316546
#% 340941
#% 340948
#% 340995
#% 342707
#% 466078
#% 786511
#! This paper addresses the problem of extending an adaptive information filtering system to make decisions about the novelty and redundancy of relevant documents. It argues that relevance and redundance should each be modelled explicitly and separately. A set of five redundancy measures are proposed and evaluated in experiments with and without redundancy thresholds. The experimental results demonstrate that the cosine similarity metric and a redundancy measure based on a mixture of language models are both effective for identifying redundant documents.

#index 397134
#* Improving realism of topic tracking evaluation
#@ Anton Leuski;James Allan
#t 2002
#c 13
#% 55490
#% 169717
#% 194298
#% 350859
#% 575571
#% 575572
#% 714020
#% 744549
#! Topic tracking and information filtering are models of interactive tasks, but their evaluations are generally done in a way that does not reflect likely usage. The models either force frequent judgments or disallow any at all, assume the user is always available to make a judgment, and do not allow for user fatigue. In this study we extend the evaluation framework for topic tracking to incorporate those more realistic issues. We demonstrate that tracking can be done in a realistic interactive setting with minimal impact on tracking cost and with substantial reduction in required interaction.

#index 397135
#* Bayesian online classifiers for text classification and filtering
#@ Kian Ming Adam Chai;Hai Leong Chieu;Hwee Tou Ng
#t 2002
#c 13
#% 46803
#% 127850
#% 132676
#% 132697
#% 169777
#% 190581
#% 194284
#% 219052
#% 232653
#% 269217
#% 274216
#% 274217
#% 280817
#% 340904
#% 375017
#% 458379
#% 464284
#% 740900
#! This paper explores the use of Bayesian online classifiers to classify text documents. Empirical results indicate that these classifiers are comparable with the best text classification systems. Furthermore, the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task.

#index 397136
#* The use of unlabeled data to improve supervised learning for text summarization
#@ Massih-Reza Amini;Patrick Gallinari
#t 2002
#c 13
#% 131258
#% 194251
#% 252011
#% 262112
#% 266292
#% 266370
#% 280835
#% 283177
#% 309115
#% 309116
#% 340885
#% 741106
#% 757855
#! With the huge amount of information available electronically, there is an increasing demand for automatic text summarization systems. The use of machine learning techniques for this task allows one to adapt summaries to the user needs and to the corpus characteristics. These desirable properties have motivated an increasing amount of work in this field over the last few years. Most approaches attempt to generate summaries by extracting sentence segments and adopt the supervised learning paradigm which requires to label documents at the text span level. This is a costly process, which puts strong limitations on the applicability of these methods. We investigate here the use of semi-supervised algorithms for summarization. These techniques make use of few labeled data together with a larger amount of unlabeled data. We propose new semi-supervised algorithms for training classification models for text summarization. We analyze their performances on two data sets - the Reuters news-wire corpus and the Computation and Language (cmp_lg) collection of TIPSTER SUMMAC. We perform comparisons with a baseline - non learning - system, and a reference trainable summarizer system.

#index 397137
#* Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering
#@ Hongyuan Zha
#t 2002
#c 13
#% 3084
#% 194251
#% 282905
#% 305242
#% 309115
#% 309116
#% 340884
#% 340885
#% 387791
#% 397214
#% 466083
#! A novel method for simultaneous keyphrase extraction and generic text summarization is proposed by modeling text documents as weighted undirected and weighted bipartite graphs. Spectral graph clustering algorithms are useed for partitioning sentences of the documents into topical groups with sentence link priors being exploited to enhance clustering quality. Within each topical group, saliency scores for keyphrases and sentences are generated based on a mutual reinforcement principle. The keyphrases and sentences are then ranked according to their saliency scores and selected for inclusion in the top keyphrase list and summaries of the document. The idea of building a hierarchy of summaries for documents capturing different levels of granularity is also briefly discussed. Our method is illustrated using several examples from news articles, news broadcast transcripts and web documents.

#index 397138
#* Cross-document summarization by concept classification
#@ Hilda Hardy;Nobuyuki Shimizu;Tomek Strzalkowski;Liu Ting;Xinyang Zhang;G. Bowden Wise
#t 2002
#c 13
#% 46809
#% 194252
#% 198058
#% 218982
#% 262112
#% 748583
#! In this paper we describe a Cross Document Summarizer XDoX designed specifically to summarize large document sets (50-500 documents and more). Such sets of documents are typically obtained from routing or filtering systems run against a continuous stream of data, such as a newswire. XDoX works by identifying the most salient themes within the set (at the granularity level that is regulated by the user) and composing an extraction summary, which reflects these main themes. In the current version, XDoX is not optimized to produce a summary based on a few unrelated documents; indeed, such summaries are best obtained simply by concatenating summaries of individual documents. We show examples of summaries obtained in our tests as well as from our participation in the first Document Understanding Conference (DUC).

#index 397139
#* Unsupervised document classification using sequential information maximization
#@ Noam Slonim;Nir Friedman;Naftali Tishby
#t 2002
#c 13
#% 115608
#% 218992
#% 262045
#% 277483
#% 280857
#% 309128
#% 340905
#% 375017
#% 528174
#% 748465
#! We present a novel sequential clustering algorithm which is motivated by the Information Bottleneck (IB) method. In contrast to the agglomerative IB algorithm, the new sequential (sIB) approach is guaranteed to converge to a local maximum of the information with time and space complexity typically linear in the data size. information, as required by the original IB principle. Moreover, the time and space complexity are significantly improved. We apply this algorithm to unsupervised document classification. In our evaluation, on small and medium size corpora, the sIB is found to be consistently superior to all the other clustering methods we examine, typically by a significant margin. Moreover, the sIB results are comparable to those obtained by a supervised Naive Bayes classifier. Finally, we propose a simple procedure for trading cluster's recall to gain higher precision, and show how this approach can extract clusters which match the existing topics of the corpus almost perfectly.

#index 397140
#* Topic difference factor extraction between two document sets and its application to text categorization
#@ Takahiko Kawatani
#t 2002
#c 13
#% 80995
#% 118736
#% 165111
#% 169718
#% 280817
#% 311034
#% 458379
#% 465754
#! To improve performance in text categorization, it is important to extract distinctive features for each class. This paper proposes topic difference factor analysis (TDFA) as a method to extract projection axes that reflect topic differences between two document sets. Suppose all sentence vectors that compose each document are projected onto projection axes. TDFA obtains the axes that maximize the ratio between the document sets as to the sum of squared projections by solving a generalized eigenvalue problem. The axes are called topic difference factors (TDF's). By applying TDFA to the document set that belongs to a given class and a set of documents that is misclassified as belonging to that class by an existent classifier, we can obtain features that take large values in the given class but small ones in other classes, as well as features that take large values in other classes but small ones in the given class. A classifier was constructed applying the above features to complement the kNN classifier. As the results, the micro averaged F1 measure for Reuters-21578 improved from 83.69 to 87.27%.

#index 397141
#* Text genre classification with genre-revealing and subject-revealing features
#@ Yong-Bae Lee;Sung Hyon Myaeng
#t 2002
#c 13
#% 280817
#% 292563
#% 309142
#% 746867
#% 756232
#% 757403
#! Subject or prepositional content has been the focus of most classification research. Genre or style, on the other hand, is a different and important property of text, and automatic text genre classification is becoming important for classification and retrieval purposes as well as for some natural language processing research. In this paper, we present a method for automatic genre classification that is based on statistically selected features obtained from both subject-classified and genre classified training data. The experimental results show that the proposed method outperforms a direct application of a statistical learner often used for subject classification. We also observe that the deviation formula and discrimination formula using document frequency ratios also work as expected. We conjecture that this dual feature set approach can be generalized to improve the performance of subject classification as well.

#index 397142
#* A new family of online algorithms for category ranking
#@ Koby Crammer;Yoram Singer
#t 2002
#c 13
#% 187104
#% 218982
#% 232653
#% 252034
#% 262085
#% 302391
#% 309208
#% 375017
#! We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stems from recent advances in online learning algorithms. The algorithms we present are simple to implement and are time and memory efficient. We evaluate the algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora the algorithms we present outperform adaptations to topic-ranking of Rocchio's algorithm and the Perceptron algorithm. We also outline the formal analysis of the algorithm in the mistake bound model. To our knowledge, this work is the first to report performance results with the entire new Reuters corpus.

#index 397143
#* Comparing cross-language query expansion techniques by degrading translation resources
#@ Paul McNamee;James Mayfield
#t 2002
#c 13
#% 118726
#% 144029
#% 218978
#% 232656
#% 262047
#% 262092
#% 262096
#% 280851
#% 340966
#% 397195
#% 420520
#% 561155
#% 561307
#% 561334
#% 748442
#% 786575
#% 853854
#! The quality of translation resources is arguably the most important factor affecting the performance of a cross-language information retrieval system. While many investigations have explored the use of query expansion techniques to combat errors induced by translation, no study has yet examined the effectiveness of these techniques across resources of varying quality. This paper presents results using parallel corpora and bilingual wordlists that have been deliberately degraded prior to query translation. Across different languages, translingual resources, and degrees of resource degradation, pre-translation query expansion is tremendously effective. In several instances, pre-translation expansion results in better performance when no translations are available, than when an uncompromised resource is used without pre-translation expansion. We also demonstrate that post-translation expansion using relevance feedback can confer modest performance gains. Measuring the efficacy of these techniques with resources of different quality suggests an explanation for the conflicting reports that have appeared in the literature.

#index 397144
#* Statistical cross-language information retrieval using n-best query translations
#@ Marcello Federico;Nicola Bertoldi
#t 2002
#c 13
#% 25470
#% 95730
#% 115462
#% 144074
#% 280851
#% 340897
#% 340948
#% 456926
#% 561165
#% 740915
#! This paper presents a novel statistical model for cross-language information retrieval. Given a written query in the source language, documents in the target language are ranked by integrating probabilities computed by two statistical models: a query-translation model, which generates most probable term-by-term translations of the query, and a query-document model, which evaluates the likelihood of each document and translation. Integration of the two scores is performed over the set of N most probable translations of the query. Experimental results with values N=1, 5, 10 are presented on the Italian-English bilingual track data used in the CLEF 2000 and 2001 evaluation campaigns.

#index 397145
#* Cross-lingual relevance models
#@ Victor Lavrenko;Martin Choquette;W. Bruce Croft
#t 2002
#c 13
#% 81669
#% 218989
#% 232656
#% 248058
#% 262047
#% 280851
#% 340894
#% 340897
#% 340899
#% 340901
#! We propose a formal model of Cross-Language Information Retrieval that does not rely on either query translation or document translation. Our approach leverages recent advances in language modeling to directly estimate an accurate topic model in the target language, starting with a query in the source language. The model integrates popular techniques of disambiguation and query expansion in a unified formal framework. We describe how the topic model can be estimated with either a parallel corpus or a dictionary. We test the framework by constructing Chinese topic models from English queries and using them in the CLIR task of TREC9. The model achieves performance around 95% of the strong mono-lingual baseline in terms of average precision. In initial precision, our model outperforms the mono-lingual baseline by 20%. The main contribution of this work is the unified formal model which integrates techniques that are essential for effective Cross-Language Retrieval.

#index 397146
#* Resolving query translation ambiguity using a decaying co-occurrence model and syntactic dependence relations
#@ Jianfeng Gao;Ming Zhou;Jian-Yun Nie;Hongzhao He;Weijun Chen
#t 2002
#c 13
#% 218988
#% 232656
#% 262047
#% 280847
#% 340895
#% 624428
#% 740915
#% 746871
#% 747738
#% 786536
#% 786550
#! Bilingual dictionaries have been commonly used for query translation in cross-language information retrieval (CLIR). However, we are faced with the problem of translation selection. Several recent studies suggested the utilization of term co-occurrences in this selection. This paper presents two extensions to improve them. First, we extend the basic co-occurrence model by adding a decaying factor that decreases the mutual information when the distance between the terms increases. Second, we incorporate a triple translation model, in which syntactic dependence relations (represented as triples) are integrated. Our evaluation on translation accuracy shows that translating triples as units is more precise than a word-by-word translation. Our CLIR experiments show that the addition of the decaying factor leads to substantial improvements of the basic co-occurrence model; and the triple translation model brings further improvements.

#index 397147
#* Document clustering with cluster refinement and model selection capabilities
#@ Xin Liu;Yihong Gong;Wei Xu;Shenghuo Zhu
#t 2002
#c 13
#% 46809
#% 99650
#% 118771
#% 262042
#% 262043
#% 262059
#% 466425
#% 495795
#% 729437
#% 748465
#! In this paper, we propose a document clustering method that strives to achieve: (1) a high accuracy of document clustering, and (2) the capability of estimating the number of clusters in the document corpus (i.e. the model selection capability). To accurately cluster the given document corpus, we employ a richer feature set to represent each document, and use the Gaussian Mixture Model (GMM) together with the Expectation-Maximization (EM) algorithm to conduct an initial document clustering. From this initial result, we identify a set of discriminative featuresfor each cluster, and refine the initially obtained document clusters by voting on the cluster label of each document using this discriminative feature set. This self-refinement process of discriminative feature identification and cluster label voting is iteratively applied until the convergence of document clusters. On the other hand, the model selection capability is achieved by introducing randomness in the cluster initialization stage, and then discovering a value C for the number of clusters N by which running the document clustering process for a fixed number of times yields sufficiently similar results. Performance evaluations exhibit clear superiority of the proposed method with its improved document clustering and model selection accuracies. The evaluations also demonstrate how each feature as well as the cluster refinement process contribute to the document clustering accuracy.

#index 397148
#* Document clustering with committees
#@ Patrick Pantel;Dekang Lin
#t 2002
#c 13
#% 118771
#% 218992
#% 228097
#% 296738
#% 316709
#% 375017
#% 406493
#% 438137
#% 465747
#% 466890
#% 631985
#% 748499
#! Document clustering is useful in many information retrieval tasks: document browsing, organization and viewing of retrieval results, generation of Yahoo-like hierarchies of documents, etc. The general goal of clustering is to group data elements such that the intra-group similarities are high and the inter-group similarities are low. We present a clustering algorithm called CBC (Clustering By Committee) that is shown to produce higher quality clusters in document clustering tasks as compared to several well known clustering algorithms. It initially discovers a set of tight clusters (high intra-group similarity), called committees, that are well scattered in the similarity space (low inter-group similarity). The union of the committees is but a subset of all elements. The algorithm proceeds by assigning elements to their most similar committee. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and manually constructed classes (the answer key). This evaluation measure is more intuitive and easier to interpret than previous evaluation measures.

#index 397149
#* Probabilistic combination of text classifiers using reliability indicators: models and results
#@ Paul N. Bennett;Susan T. Dumais;Eric Horvitz
#t 2002
#c 13
#% 132938
#% 144076
#% 169717
#% 169774
#% 187773
#% 197922
#% 219050
#% 219051
#% 219052
#% 260001
#% 280817
#% 309141
#% 311034
#% 331909
#% 340940
#% 342668
#% 375017
#% 445319
#% 458379
#% 466572
#% 722754
#% 1272397
#% 1650705
#! The intuition that different text classifiers behave in qualitatively different ways has long motivated attempts to build a better metaclassifier via some combination of classifiers. We introduce a probabilistic method for combining classifiers that considers the context-sensitive reliabilities of contributing classifiers. The method harnesses reliability indicators---variables that provide a valuable signal about the performance of classifiers in different situations. We provide background, present procedures for building metaclassifiers that take into consideration both reliability indicators and classifier outputs, and review a set of comparative studies undertaken to evaluate the methodology.

#index 397150
#* Efficient phrase querying with an auxiliary index
#@ Dirk Bahle;Hugh E. Williams;Justin Zobel
#t 2002
#c 13
#% 86531
#% 109190
#% 212665
#% 213786
#% 253191
#% 280839
#% 281174
#% 290703
#% 301263
#% 303395
#% 309146
#% 323131
#% 340886
#% 379516
#! Search engines need to evaluate queries extremely fast, a challenging task given the vast quantities of data being indexed. A significant proportion of the queries posed to search engines involve phrases. In this paper we consider how phrase queries can be efficiently supported with low disk overheads. Previous research has shown that phrase queries can be rapidly evaluated using nextword indexes, but these indexes are twice as large as conventional inverted files. We propose a combination of nextword indexes with inverted files as a solution to this problem. Our experiments show that combined use of an auxiliary nextword index and a conventional inverted file allow evaluation of phrase queries in half the time required to evaluate such queries with an inverted file alone, and the space overhead is only 10% of the size of the inverted file. Further time savings are available with only slight increases in disk requirements.

#index 397151
#* Compression of inverted indexes For fast query evaluation
#@ Falk Scholer;Hugh E. Williams;John Yiannis;Justin Zobel
#t 2002
#c 13
#% 169817
#% 179161
#% 212665
#% 213786
#% 262099
#% 290703
#% 311799
#% 323131
#% 340886
#% 420492
#% 438325
#! Compression reduces both the size of indexes and the time needed to evaluate queries. In this paper, we revisit the compression of inverted lists of document postings that store the position and frequency of indexed terms, considering two approaches to improving retrieval efficiency: better implementation and better choice of integer compression schemes. First, we propose several simple optimisations to well-known integer compression schemes, and show experimentally that these lead to significant reductions in time. Second, we explore the impact of choice of compression scheme on retrieval efficiency.In experiments on large collections of data, we show two surprising results: use of simple byte-aligned codes halves the query evaluation time compared to the most compact Golomb-Rice bitwise compression schemes; and, even when an index fits entirely in memory, byte-aligned codes result in faster query evaluation than does an uncompressed index, emphasising that the cost of transferring data from memory to the CPU cache is less for an appropriately compressed index than for an uncompressed index. Moreover, byte-aligned schemes have only a modest space overhead: the most compact schemes result in indexes that are around 10% of the size of the collection, while a byte-aligned scheme is around 13%. We conclude that fast byte-aligned codes should be used to store integers in inverted lists.

#index 397152
#* Set-based model: a new approach for information retrieval
#@ Bruno Pôssas;Nivio Ziviani;Wagner Meira, Jr.;Berthier Ribeiro-Neto
#t 2002
#c 13
#% 18616
#% 46803
#% 152934
#% 184486
#% 212665
#% 228088
#% 263702
#% 287307
#% 288541
#% 288995
#% 290703
#% 307373
#% 310494
#% 387427
#% 448776
#% 481290
#% 840583
#! The objective of this paper is to present a new technique for computing term weights for index terms, which leads to a new ranking mechanism, referred to as set-based model. The components in our model are no longer terms, but termsets. The novelty is that we compute term weights using a data mining technique called association rules, which is time efficient and yet yields nice improvements in retrieval effectiveness. The set-based model function for computing the similarity between a document and a query considers the termset frequency in the document and its scarcity in the document collection. Experimental results show that our model improves the average precision of the answer set for all three collections evaluated. For the TReC-3 collection, our set-based model led to a gain, relative to the standard vector space model, of 37% in average precision curves and of 57% in average precision for the top 10 documents. Like the vector space model, the set-based model has time complexity that is linear in the number of documents in the collection.

#index 397153
#* Collaborative filtering with privacy via factor analysis
#@ John Canny
#t 2002
#c 13
#% 266281
#% 280852
#% 280883
#% 283169
#% 337046
#% 528182
#% 616944
#% 668895
#! Collaborative filtering (CF) is valuable in e-commerce, and for direct recommendations for music, movies, news etc. But today's systems have several disadvantages, including privacy risks. As we move toward ubiquitous computing, there is a great potential for individuals to share all kinds of information about places and things to do, see and buy, but the privacy risks are severe. In this paper we describe a new method for collaborative filtering which protects the privacy of individual data. The method is based on a probabilistic factor analysis model. Privacy protection is provided by a peer-to-peer protocol which is described elsewhere, but outlined in this paper. The factor analysis approach handles missing data without requiring default values for them. We give several experiments that suggest that this is most accurate method for CF to date. The new algorithm has other advantages in speed and storage over previous algorithms. Finally, we suggest applications of the approach to other kinds of statistical analyses of survey or questionaire data.

#index 397154
#* Inverted file search algorithms for collaborative filtering
#@ Rickard Cöster;Martin Svensson
#t 2002
#c 13
#% 202011
#% 220711
#% 228097
#% 260778
#% 280852
#% 287715
#% 290703
#% 384345
#% 406493
#% 465928
#% 466913
#% 1650569
#! This paper explores the possibility of using a disk based inverted file structure for collaborative filtering. Our hypothesis is that this allows for faster calculation of predictions and also that early termination heuristics may be used to further speed up the filtering process and perhaps even improve the quality of the predictions. In an experiment on the EachMovie dataset this was tested. Our results indicate that searching the inverted file structure is many times faster than general in-memory vector search, even for very large profiles. The Continue termination heuristics produces the best ranked predictions in our experiments, and Quit is the top performer in terms of speed.

#index 397155
#* Methods and metrics for cold-start recommendations
#@ Andrew I. Schein;Alexandrin Popescul;Lyle H. Ungar;David M. Pennock
#t 2002
#c 13
#% 173879
#% 202009
#% 202011
#% 220706
#% 220711
#% 266281
#% 280819
#% 280852
#% 283169
#% 301259
#% 314933
#% 330687
#% 406493
#% 465906
#% 465928
#% 495929
#% 528152
#% 528156
#% 528182
#% 529806
#% 564279
#% 1272396
#% 1650569
#! We have developed a method for recommending items that combines content and collaborative data under a single probabilistic framework. We benchmark our algorithm against a naïve Bayes classifier on the cold-start problem, where we wish to recommend items that no one in the community has yet rated. We systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real-world applications. We advocate heuristic recommenders when benchmarking to give competent baseline performance. We introduce a new performance metric, the CROC curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. Though the emphasis of our testing is on cold-start recommending, our methods for recommending and evaluation are general.

#index 397156
#* Term selection for searching printed Arabic
#@ Kareem Darwish;Douglas W. Oard
#t 2002
#c 13
#% 177564
#% 214354
#% 237945
#% 504885
#% 757288
#% 854585
#! Since many Arabic documents are available only in print, automating retrieval from collections of scanned Arabic document images using Optical Character Recognition (OCR) is an interesting problem. Arabic combines rich morphology with a writing system that presents unique challenges to OCR systems. These factors must be considered when selecting terms for automatic indexing. In this paper, alternative choices of indexing terms are explored using both an existing electronic text collection and a newly developed collection built from images of actual printed Arabic documents. Character n-grams or lightly stemmed words were found to typically yield near-optimal retrieval effectiveness, and combining both types of terms resulted in robust performance across a broad range of conditions.

#index 397157
#* Empirical studies in strategies for Arabic retrieval
#@ Jinxi Xu;Alexander Fraser;Ralph Weischedel
#t 2002
#c 13
#% 144074
#% 218989
#% 262096
#% 270945
#% 280826
#% 280850
#% 280851
#% 340897
#% 740915
#% 757288
#% 786534
#% 817596
#! This work evaluates a few search strategies for Arabic monolingual and cross-lingual retrieval, using the TREC Arabic corpus as the test-bed. The release by NIST in 2001 of an Arabic corpus of nearly 400k documents with both monolingual and cross-lingual queries and relevance judgments has been a new enabler for empirical studies. Experimental results show that spelling normalization and stemming can significantly improve Arabic monolingual retrieval. Character tri-grams from stems improved retrieval modestly on the test corpus, but the improvement is not statistically significant. To further improve retrieval, we propose a novel thesaurus-based technique. Different from existing approaches to thesaurus-based retrieval, ours formulates word synonyms as probabilistic term translations that can be automatically derived from a parallel corpus. Retrieval results show that the thesaurus can significantly improve Arabic monolingual retrieval. For cross-lingual retrieval (CLIR), we found that spelling normalization and stemming have little impact.

#index 397158
#* Improving stemming for Arabic information retrieval: light stemming and co-occurrence analysis
#@ Leah S. Larkey;Lisa Ballesteros;Margaret E. Connell
#t 2002
#c 13
#% 115470
#% 144034
#% 177564
#% 184489
#% 208934
#% 218985
#% 241238
#% 270945
#% 316910
#% 375017
#% 397157
#% 561152
#% 561154
#% 561167
#% 561168
#% 561328
#% 704106
#% 704571
#% 741043
#% 757288
#% 817566
#! Arabic, a highly inflected language, requires good stemming for effective information retrieval, yet no standard approach to stem驴ming has emerged. We developed several light stemmers based on heuristics and a statistical stemmer based on co-occurrence for Arabic retrieval. We compared the retrieval effectiveness of our stemmers and of a morphological analyzer on the TREC-2001 data. The best light stemmer was more effective for cross-lan驴guage retrieval than a morphological stemmer which tried to find the root for each word. A repartitioning process consisting of vowel removal followed by clustering using co-occurrence analy驴sis pro驴duced stem classes which were better than no stemming or very light stemming, but still inferior to good light stemming or mor驴phological analysis.

#index 397159
#* Automatic query wefinement using lexical affinities with maximal information gain
#@ David Carmel;Eitan Farchi;Yael Petruschka;Aya Soffer
#t 2002
#c 13
#% 65964
#% 118738
#% 169729
#% 218978
#% 229346
#% 262067
#% 262084
#% 282424
#% 326522
#% 340934
#% 340941
#% 840577
#! This work describes an automatic query refinement technique, which focuses on improving precision of the top ranked documents. The terms used for refinement are lexical affinities (LAs), pairs of closely related words which contain exactly one of the original query terms. Adding these terms to the query is equivalent to re-ranking search results, thus, precision is improved while recall is preserved. We describe a novel method that selects the most "informative" LAs for refinement, namely, those LAs that best separate relevant documents from irrelevant documents in the set of results. The information gain of candidate LAs is determined using unsupervised estimation that is based on the scoring function of the search engine. This method is thus fully automatic and its quality depends on the quality of the scoring function. Experiments we conducted with TREC data clearly show a significant improvement in the precision of the top ranked documents.

#index 397161
#* Predicting query performance
#@ Steve Cronen-Townsend;Yun Zhou;W. Bruce Croft
#t 2002
#c 13
#% 4430
#% 115608
#% 144034
#% 219041
#% 252472
#% 262096
#% 279755
#% 280864
#% 326522
#% 333336
#% 337428
#% 340901
#% 995516
#! We develop a method for predicting query performance by computing the relative entropy between a query language model and the corresponding collection language model. The resulting clarity score measures the coherence of the language usage in documents whose models are likely to generate the query. We suggest that clarity scores measure the ambiguity of a query with respect to a collection of documents and show that they correlate positively with average precision in a variety of TREC test sets. Thus, the clarity score may be used to identify ineffective queries, on average, without relevance information. We develop an algorithm for automatically setting the clarity score threshold between predicted poorly-performing queries and acceptable queries and validate it using TREC data. In particular, we compare the automatic thresholds to optimum thresholds and also check how frequently results as good are achieved in sampling experiments that randomly assign queries to the two classes.

#index 397162
#* Using part-of-speech patterns to reduce query ambiguity
#@ James Allan;Hema Raghavan
#t 2002
#c 13
#% 169768
#% 216150
#% 218992
#% 262076
#% 280840
#% 280849
#% 309146
#% 329091
#% 340951
#% 342660
#% 397161
#% 641139
#% 677173
#% 714020
#% 995516
#! Query ambiguity is a generally recognized problem, particularly in Web environments where queries are commonly only one or two words in length. In this study, we explore one technique that finds commonly occurring patterns of parts of speech near a one-word query and allows them to be transformed into clarification questions. We use a technique derived from statistical language modeling to show that the clarification queries will reduce ambiguity much of the time, and often quite substantially.

#index 397163
#* The effect of topic set size on retrieval experiment error
#@ Ellen M. Voorhees;Chris Buckley
#t 2002
#c 13
#% 133889
#% 133893
#% 306497
#% 309093
#% 312689
#% 318407
#! Retrieval mechanisms are frequently compared by computing the respective average scores for some effectiveness metric across a common set of information needs or topics, with researchers concluding one method is superior based on those averages. Since comparative retrieval system behavior is known to be highly variable across topics, good experimental design requires that a "sufficient" number of topics be used in the test. This paper uses TREC results to empirically derive error rates based on the number of topics used in a test and the observed difference in the average scores. The error rates quantify the likelihood that a different set of topics of the same size would lead to a different conclusion. We directly compute error rates for topic sets up to size 25, and extrapolate those rates for larger topic set sizes. The error rates found are larger than anticipated, indicating researchers need to take care when concluding one method is better than another, especially if few topics are used.

#index 397164
#* Liberal relevance criteria of TREC -: counting on negligible documents?
#@ Eero Sormunen
#t 2002
#c 13
#% 1358
#% 129694
#% 262102
#% 262105
#% 280041
#% 309095
#% 312689
#% 340892
#! Most test collections (like TREC and CLEF) for experimental research in information retrieval apply binary relevance assessments. This paper introduces a four-point relevance scale and reports the findings of a project in which TREC-7 and TREC-8 document pools on 38 topics were reassessed. The goal of the reassessment was to build a subcollection of TREC for experiments on highly relevant documents and to learn about the assessment process as well as the characteristics of a multigraded relevance corpus.Relevance criteria were defined so that a distinction was made between documents rich in topical information (relevant and highly relevant documents) and poor in topical information (marginally relevant documents). It turned out that about 50% of documents assessed as relevant were regarded as marginal. The characteristics of the relevance corpus and lessons learned from the reassessment project are discussed. The need to develop more elaborated relevance assessment schemes is emphasized.

#index 397165
#* Robust temporal and spectral modeling for query By melody
#@ Shai Shalev-Shwartz;Shlomo Dubnov;Nir Friedman;Yoram Singer
#t 2002
#c 13
#% 75936
#% 194192
#% 267568
#% 270994
#! Query by melody is the problem of retrieving musical performances from melodies. Retrieval of real performances is complicated due to the large number of variations in performing a melody and the presence of colored accompaniment noise. We describe a simple yet effective probabilistic model for this task. We describe a generative model that is rich enough to capture the spectral and temporal variations of musical performances and allows for tractable melody retrieval. While most of previous studies on music retrieval from melodies were performed with either symbolic (e.g. MIDI) data or with monophonic (single instrument) performances, we performed experiments in retrieving live and studio recordings of operas that contain a leading vocalist and rich instrumental accompaniment. Our results show that the probabilistic approach we propose is effective and can be scaled to massive datasets.

#index 397166
#* Video retrieval using an MPEG-7 based inference network
#@ Andrew Graves;Mounia Lalmas
#t 2002
#c 13
#% 111456
#% 262069
#% 302431
#% 340914
#% 387427
#% 398890
#% 434908
#% 447811
#% 458404
#% 1775110
#! This work proposes a model for video retrieval based upon the inference network model. The document network is constructed using video metadata encoded using MPEG-7 and captures information pertaining to the structural aspects (video breakdown into shots and scenes), conceptual aspects (video, scene and shot content) and contextual aspects (context information about the position of conceptual content within the document). The retrieval process a) exploits the distribution of evidence among the shots to perform ranking of different levels of granularity, b) addresses the idea that evidence may be inherited during evaluation, and c) exploits the contextual information to perform constrained queries.

#index 397167
#* Using self-supervised word segmentation in Chinese information retrieval
#@ Fuchun Peng;Xiangji Huang;Dale Schuurmans;Nick Cercone;Stephen E. Robertson
#t 2002
#c 13
#% 549575
#! We propose a self-supervised word-segmentation technique for Chinese information retrieval. This method combines the advantages of traditional dictionary based approaches with character based approaches, while overcoming many of their shortcomings. Experiments on TREC data show comparable performance to both the dictionary based and the character based approaches. However, our method is language independent and unsupervised, which provides a promising avenue for constructing accurate multilingual information retrieval systems that are flexible and adaptive.

#index 397168
#* Automatic classification in product catalogs
#@ Ben Wolin
#t 2002
#c 13
#% 115462
#% 120634
#% 184488
#% 321635
#% 361100
#% 648320
#! In this paper, we present the AutoCat system for product classification. AutoCat uses a vector space model, modified to consider product attributes unavailable in traditional document classification. We present key features of our user interface, developed to assist users with evaluating and editing the output of the classification algorithm. Finally, we present observations about the use of this technology in the field.

#index 397169
#* PageRank, HITS and a unified framework for link analysis
#@ Chris Ding;Xiaofeng He;Parry Husbands;Hongyuan Zha;Horst D. Simon
#t 2002
#c 13
#% 268079
#% 290830
#! Two popular link-based webpage ranking algorithms are (i) PageRank[1] and (ii) HITS (Hypertext Induced Topic Selection)[3]. HITS makes the crucial distinction of hubs and authorities and computes them in a mutually reinforcing way. PageRank considers the hyperlink weight normalization and the equilibrium distribution of random surfers as the citation score. We generalize and combine these key concepts into a unified framework, in which we prove that rankings produced by PageRank and HITS are both highly correlated with the ranking by in-degree and out-degree.

#index 397170
#* Task orientation in question answering
#@ Vanessa Murdock;W. Bruce Croft
#t 2002
#c 13
#% 218978
#% 309126
#% 341964

#index 397171
#* Experiments in high-dimensional text categorization
#@ Fred J. Damerau;Tong Zhang;Sholom M. Weiss;Nitin Indurkhya
#t 2002
#c 13
#! We present results for automated text categorization of the Reuters-810000 collection of news stories. Our experiments use the entire one-year collection of 810,000 stories and the entire subject index. We divide the data into monthly groups and provide an initial benchmark of text categorization performance on the complete collection. Experimental results show that efficient sparse-feature implementations of linear methods and decision trees, using a global unstemmed dictionary, can readily handle applications of this size. Predictive performance is approximately as strong as the best results for the much smaller older Reuters collections. Detailed results are provided over time periods. It is shown that a smaller time horizon does not diminish predictive quality, implying reduced demands for retraining when sample size is large.

#index 397172
#* The relationship between ASK and relevance criteria
#@ Xiao-Jun Yuan;Nicholas J. Belkin;Ja-Young Kim
#t 2002
#c 13
#% 83855
#% 227769
#% 235292
#% 260244

#index 397173
#* ICA and SOM in text document analysis
#@ Ella Bingham;Jukka Kuusisto;Krista Lagus
#t 2002
#c 13
#% 176172
#% 234978
#% 304915
#% 406493
#% 429567
#% 1860500
#% 1860651
#! In this study we show experimental results on using Independent Component Analysis (ICA) and the Self-Organizing Map (SOM) in document analysis. Our documents are segments of spoken dialogues carried out over the telephone in a customer service, transcribed into text. The task is to analyze the topics of the discussions, and to group the discussions into meaningful subsets. The quality of the grouping is studied by comparing to a manual topical classification of the documents.

#index 397174
#* Improving hierarchical text classification using unlabeled data
#@ Vijay Boyapati
#t 2002
#c 13
#% 266292
#% 280866
#% 458369
#% 465747

#index 397175
#* Do thumbnail previews help users make better relevance decisions about web search results?
#@ Susan Dziadosz;Raman Chandrasekar
#t 2002
#c 13
#% 324984
#% 447042
#! We describe an empirical evaluation of the utility of thumbnail previews in web search results. Results pages were constructed to show text-only summaries, thumbnail previews only, or the combination of text summaries and thumbnail previews. We found that in the combination case, users were able to make more accurate decisions about the potential relevance of results than in either of the other versions, with hardly any increase in speed of processing the page as a whole.

#index 397176
#* Amilcare: adaptive information extraction for document annotation
#@ Fabio Ciravegna;Alexiei Dingli;Yorick Wilks;Daniela Petrelli
#t 2002
#c 13
#% 742446
#% 854224
#% 1289318

#index 397177
#* The impact of corpus size on question answering performance
#@ C. L. A. Clarke;G. V. Cormack;M. Laszlo;T. R. Lynam;E. L. Terra
#t 2002
#c 13
#% 280859
#% 309127
#% 330609
#% 330616
#% 340953
#! Using our question answering system, questions from the TREC 2001 evaluation were executed over a series of Web data collections, with the sizes of the collections increasing from 25 gigabytes up to nearly a terabyte.

#index 397178
#* Effective collection metasearch in a hierarchical environment: global vs. localized retrieval performance
#@ Jack G. Conrad;Changwen Yang;Joanne S. Claussen
#t 2002
#c 13
#% 296098
#% 665561
#! We compare standard global IR searching with user-centric localized techniques to address the database selection problem. We conduct a series of experiments to compare the retrieval effectiveness of three separate search modes applied to a hierarchically structured data environment of textual database representations. The data environment is represented as a tree-like directory containing over 15,000 unique databases and over 100,000 total leaf nodes. Our search modes consist of varying degrees of browse and search, from a global search at the root node to a refined search at a sub-node using dynamically-calculated inverse document frequencies (idfs) to score candidate databases for probable relevance. Our findings indicate that a browse and search approach that relies upon localized searching from sub-nodes is capable of producing the most effective results.

#index 397179
#* Experimenting with graphical user interfaces for structured document retrieval
#@ Fabio Crestani;Pablo de la Fuente;Jesús Vegas
#t 2002
#c 13
#% 458406

#index 397180
#* The web retrieval task and its evaluation in the third NTCIR workshop
#@ Koji Eguchi;Keizo Oyama;Emi Ishida;Kazuko Kuriyama;Noriko Kando
#t 2002
#c 13
#! This paper gives an overview of the evaluation method used for the Web Retrieval Task in the Third NTCIR Workshop, which is currently in progress. In the Web Retrieval Task, we try to assess the retrieval effectiveness of each Web search engine system using a common data set, and attempt to build a re-usable test collection suitable for evaluating Web search engine systems. With these objectives, we have built 100-gigabyte and 10-gigabyte document sets, mainly gathered from the '.jp' domain. Relevance judgment is performed on the retrieved documents, which are written in Japanese or English.

#index 397181
#* How Many Bits are Needed to Store Term Frequencies?
#@ Martin Franz;J. Scott McCarley
#t 2002
#c 13
#% 218978
#% 387427
#% 394709
#! Search algorithms in most current text retrieval systems use index data structures extracted from the original text documents. In this paper we focus on reducing the size of the indices by reducing the amount of space dedicated to store term frequencies. In experiments using TREC Ad Hoc [2, 3] corpora and query sets, we show that it is possible to store the term frequency in only two bits without decreasing retrieval performance.

#index 397182
#* Non-linear reading for a structured web indexation
#@ Mathias Géry
#t 2002
#c 13
#% 268079
#% 290830
#% 309749
#% 330676
#% 406493
#% 420508
#! The growth of the Web has posed new challenges for Information Retrieval (IR). Most of the current systems are based on traditional models, which have been developed for atomic and independents documents and are not adapted to the Web. A promising research orientation consists of studying the impact of the Web structure on indexing. The HyperDocument model presented in this article is based on essential aspects of information comprehension: content, composition and linear/non-linear reading.

#index 397183
#* Document normalization revisited
#@ Abdur Chowdhury;M. Catherine McCabe;David Grossman;Ophir Frieder
#t 2002
#c 13
#% 218982
#! Cosine Pivoted Document Length Normalization has reached a point of stability where many researchers indiscriminately apply a specific value of 0.2 regardless of the collection. Our efforts, however, demonstrate that applying this specific value without tuning for the document collection degrades average precision by as much as 20%.

#index 397184
#* User-centered interface design for cross-language information retrieval
#@ Preben Hansen;Daniela Petrelli;Jussi Karlgren;Micheline Beaulieu;Mark Sanderson
#t 2002
#c 13
#% 245740
#% 262046
#% 607985
#! This paper reports on the user-centered design methodology and techniques used for the elicitation of user requirements and how these requirements informed the first phase of the user interface design for a Cross-Language Information Retrieval System. We describe a set of factors involved in analysis of the data collected and, finally discuss the implications for user interface design based on the findings.

#index 397185
#* Implementation of relevance feedback for content-based music retrieval based on user prefences
#@ Keiichiro Hoashi;Erik Zeitler;Naomi Inoue
#t 2002
#c 13

#index 397186
#* Spatial information retrieval and geographical ontologies an overview of the SPIRIT project
#@ Christopher B. Jones;R. Purves;A. Ruas;M. Sanderson;M. Sester;M. van Kreveld;R. Weibel
#t 2002
#c 13
#% 169729
#% 218978
#% 232667
#% 329657
#% 330677
#% 487215

#index 397187
#* A visualisation tool for topic tracking analysis and development
#@ Gareth J. F. Jones;Steven M. Gabb
#t 2002
#c 13
#% 201992
#% 309100
#! Topic Detection and Tracking (TDT) research explores the development of algorithms to detect novel events and track their development over time for online reports. Development of these methods requires careful evaluation and analysis. Traditional reductive methods of evaluation only represent some of the available information of algorithm behaviour. We describe a visualisation tool for topic tracking which makes it easy to analysis and compare the temporal behaviour of tracking algorithms.

#index 397188
#* A new method of parameter estimation for multinomial naive bayes text classifiers
#@ Sang-Bum Kim;Hae-Chang Rim;Heui-Seok Lim
#t 2002
#c 13
#% 262096
#% 280817
#! Multinomial naive Bayes classifiers have been widely used for the probabilistic text classification. However, their parameter estimation method sometimes generates inappropriate probabilities. In this paper, we propose a topic document model approach for naive Bayes text classification, where their parameters are estimated with an expectation from the training documents. Experiments are conducted on Reuters 21578 and 20 Newsgroup collection, and our proposed approach obtained a significant improvement in performace over the conventional approach.

#index 397189
#* Study of category score algorithms for k-NN classifier
#@ Huaizhong KOU;Georges Gardarin
#t 2002
#c 13
#% 280817
#% 340904
#% 458379
#! We analyzes category score algorithms for k-NN classifier found in the literature, including majority voting algorithm (MVA), simple sum algorithm (SSA). MVA and SSA are two mainly used algorithms to estimate score for candidate categories in k-NN classifier systems. Based on the hypothesis that utilization of internal relation between documents and categories could improve system performance, two new weighting score models: concept-based weighting (CBW) score model and term independence-based weighting (IBW) score model are proposed. Our experimental results confirm our hypothesis and show that in the term of precision average IBW and CBW are better than the other score models, while SSA is higher than MVA. According to macro-average F1 CBW performs best. Rocchio-based algorithm (RBA) always performs worst.

#index 397190
#* Higher precision for two-word queries
#@ K. L. Kwok
#t 2002
#c 13
#% 56830
#% 398019
#% 420504
#! Queries have specific properties, and may need individualized methods and parameters to optimize retrieval. Length is one property. We look at how two-word queries may attain higher precision by re-ranking using word co-occurrence evidence in retrieved documents. Co-occurrence within document context is not sufficient, but window context including sentence context evidence can provide precision improvements at low recall region of 4 to 10% using initial retrieval results, and positively affects pseudo-relevance feedback.

#index 397191
#* The boomerang effect: retrieving scientific documents via the network of references and citations
#@ Birger Larsen;Peter Ingwersen
#t 2002
#c 13
#% 137475
#% 169739
#% 249143
#% 398897

#index 397192
#* A logistic regression approach to distributed IR
#@ Ray R. Larson
#t 2002
#c 13
#% 118756
#% 262065
#% 280853
#% 287463
#% 337234
#% 712909
#! This poster session examines a probabilistic approach to distributed information retrieval using a Logistic Regression algorithm for estimation of collection relevance. The algorithm is compared to other methods for distributed search using test collections developed for distributed search evaluation.

#index 397193
#* Automatic metadata generation & evaluation
#@ Elizabeth D. Liddy;Eileen Allen;Sarah Harwell;Susan Corieri;Ozgur Yilmazel;N. Ercan Ozgencil;Anne Diekema;Nancy McCracken;Joanne Silverstein;Stuart Sutton
#t 2002
#c 13
#! The poster reports on a project in which we are investigating methods for breaking the human metadata-generation bottleneck that plagues Digital Libraries. The research question is whether metadata elements and values can be automatically generated from the content of educational resources, and correctly assigned to mathematics and science educational materials. Natural Language Processing and Machine Learning techniques were implemented to automatically assign values of the GEMgenerate metadata element set tofor learning resources provided by the Gateway for Education (GEM), a service that offers web access to a wide range of educational materials. In a user study, education professionals evaluated the metadata assigned to learning resources by either automatic tagging or manual assignment. Results show minimal difference in the eyes of the evaluators between automatically generated metadata and manually assigned metadata.

#index 397194
#* A critical examination of TDT's cost function
#@ R. Manmatha;Ao Feng;James Allan
#t 2002
#c 13
#% 340934
#% 340938
#% 575571
#! Topic Detection and Tracking (TDT) tasks are evaluated using a cost function. The standard TDT cost function assumes a constant probability of relevance P(rel) across all topics. In practice, P(rel) varies widely across topics. We argue using both theoretical and experimental evidence that the cost function should be modified to account for the varying P(rel).

#index 397195
#* Converting on-line bilingual dictionaries from human-readable to machine-readable form
#@ James Mayfield;Paul McNamee
#t 2002
#c 13
#% 531459
#! We describe a language called ABET that allows rapid conversion of on-line human-readable bilingual dictionaries to machine-readable form.

#index 397196
#* Modeling (in)variability of human judgments for text summarization
#@ Tadashi Nomoto;Yuji Matsumoto
#t 2002
#c 13
#% 466352
#% 466479
#! The paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human biases in summarization. In particular, we explore the use of probabilistic decision tree within the clustering framework to account for the variation as well as regularity in human created summaries.

#index 397197
#* Content-based music indexing and organization
#@ Andreas Rauber;Elias Pampalk;Dieter Merkl
#t 2002
#c 13
#% 194192
#% 234978
#% 316259
#% 493275
#% 930290
#! While electronic music archives are gaining popularity, access to and navigation within these archives is usually limited to text-based queries or manually predefined genre category browsing. We present a system that automatically organizes a music collection according to the perceived sound similarity resembling genres or styles of music. Audio signals are processed according to psychoacoustic models to obtain a time-invariant representation of its characteristics. Subsequent clustering provides an intuitive interface where similar pieces of music are grouped together on a map display.

#index 397198
#* Relative and absolute term selection criteria: a comparative study for English and Japanese IR
#@ Tetsuya Sakai;Stephen E. Robertson
#t 2002
#c 13
#% 92696
#% 276152
#% 340964

#index 397199
#* Experiments on data fusion using headline information
#@ Xiao Mang Shou;Mark Sanderson
#t 2002
#c 13
#% 232703
#% 340934
#% 340936
#% 342672
#% 433674
#! This poster describes initial work exploring a relatively unexamined area of data fusion: fusing the results of retrieval systems whose collections have no overlap between them. Many of the effective meta-search/data fusion strategies gain much of their success from exploiting document overlap across the source systems being merged. When the intersection of the collections is the empty set, the strategies generally degrade to a simpler form. In order to address such situations, two strategies were examined: re-ranking of merged results using a locally run search on the text fragments returned by the source search engines; and re-ranking based on cross document similarity, again using text fragments presented in the retrieved list. Results, from experiments, which go beyond previous work, indicate that both strategies improve fusion effectiveness.

#index 397200
#* Building thematic lexical resources by term categorization
#@ Alberto Lavelli;Bernardo Magnini;Fabrizio Sebastiani
#t 2002
#c 13
#% 212683
#% 311034
#% 316508
#% 344447
#! We discuss the automatic generation of thematic lexicons by means of term categorization, a novel task employing techniques from information retrieval (IR) and machine learning (ML). Specifically, we view the generation of such lexicons as an iterative process of learning previously unknown associations between terms and themes (i.e. disciplines, or fields of activity). The process is iterative, in that it generates, for each ci in a set C = {c1,...,cm} of themes, a sequence Li0⊆ Li1⊆ ... ⊆ Lin of lexicons, bootstrapping from an initial lexicon Li0 and a set of text corpora &THgr; = {&thgr;0,...,&thgr;n-1} given as input. The method is inspired by text categorization, the discipline concerned with labelling natural language texts with labels from a predefined set of themes, or categories. However, while text categorization deals with documents represented as vectors in a space of terms, term categorization deals (dually) with terms represented as vectors in a space of documents, and labels terms (instead of documents) with themes. As a learning device we adopt boosting, since (a) it has demonstrated state-of-the-art effectiveness in a variety of text categorization applications, and (b) it naturally allows for a form of "data cleaning", thereby making the process of generating a thematic lexicon an iteration of generate-and-test steps.

#index 397201
#* Topic structure modeling
#@ David A. Evans;James G. Shanahan;Victor Sheftel
#t 2002
#c 13
#% 995516
#! In this paper, we present a method based on document probes to quantify and diagnose topic structure, distinguishing topics as monolithic, structured, or diffuse. The method also yields a structure analysis that can be used directly to optimize filter (classifier) creation. Preliminary results illustrate the predictive value of the approach on TREC/Reuters-96 topics.

#index 397202
#* Language model for IR using collection information
#@ Rong Jin;Luo Si;Alex G. Hauptmann;Jamie Callan
#t 2002
#c 13
#% 187773
#% 280856
#% 340899
#! Information retrieval using meta data can be traced back to the early age of IR where documents are represented by the controlled vocabulary. In this paper, we explore the usage of meta-data information under the framework of language model. We present a new language model that is able to take advantage of the category information for documents to improve the retrieval accuracy. We compare the new language model with the traditional language model over the TREC4 dataset where the collection information for documents is obtained using the k-means clustering method. The new language model outperforms the traditional language model, which verifies our statement.

#index 397203
#* Automatic evaluation of world wide web search services
#@ Abdur Chowdhury;Ian Soboroff
#t 2002
#c 13
#% 280041
#% 281174
#% 286304
#% 330787
#% 438557
#! Users of the World-Wide Web are not only confronted by an immense overabundance of information, but also by a plethora of tools for searching for the web pages that suit their information needs. Web search engines differ widely in interface, features, coverage of the web, ranking methods, delivery of advertising, and more. In this paper, we present a method for comparing search engines automatically based on how they rank known item search results. Because the engines perform their search on overlapping (but different) subsets of the web collected at different points in time, evaluation of search engines poses significant challenges to the traditional information retrieval methodology. Our method uses known item searching; comparing the relative ranks of the items in the search engines' rankings. Our approach automatically constructs known item queries using query log analysis and automatically constructs the result via analysis of editor comments from the ODP (Open Directory Project). Additionally, we present our comparison on five (Lycos, Netscape, Fast, Google, HotBot) well-known search services and find that some services perform known item searches better than others, but the majority are statistically equivalent.

#index 397204
#* Does WT10g look like the web?
#@ Ian Soboroff
#t 2002
#c 13
#% 309749
#% 330787
#% 729027
#! We measure the WT10g test collection, used in the TREC-9 and TREC 2001 Web Tracks, with common measures used in the web topology community, in order to see if WT10g "looks like" the web. This is not an idle question; characteristics of the web, such as power law relationships, diameter, and connected components have all been observed within the scope of general web crawls, constructed by blindly following links. In contrast, WT10g was carved out from a larger crawl specifically to be a web search test collection within the reach of university researchers. Does such a collection retain the properties of the larger web? In the case of WT10g, yes.

#index 397205
#* Biterm language models for document retrieval
#@ Munirathnam Srikanth;Rohini Srihari
#t 2002
#c 13
#% 81669
#% 252472
#% 262096
#% 280850
#% 280851
#% 280864
#% 340899
#% 650846

#index 397206
#* Selecting indexing strings using adaptation
#@ Yoshiyuki Takeda;Kyoji Umemura
#t 2002
#c 13
#% 853857
#! It is not easy to tokenize agglutinative languages like Japanese and Chinese into words. Many IR systems start with a dictionary-based morphology program like ChaSen [4]. Unfortunately, dictionaries cannot cover all possible words; unknown words such as proper nouns are important for IR. This paper proposes a statistical dictionary-free method for selecting index strings based on recent work on adaptive language modeling.

#index 397207
#* Error correction in a Chinese OCR test collection
#@ Yuen-Hsien Tseng
#t 2002
#c 13
#% 162362
#% 280845
#% 327246
#% 575730
#! This article proposes a technique for correcting Chinese OCR errors to support retrieval of scanned documents. The technique uses a completely automatic technique (no manually constructed lexicons or confusion resources) to identify both keywords and confusable terms. Improved retrieval effectiveness on a single term query experiment is demonstrated.

#index 397208
#* User interface effects in past batch versus user experiments
#@ Andrew Turpin;William Hersh
#t 2002
#c 13
#% 184486
#% 309089
#% 340921

#index 397209
#* K-tree/forest: efficient indexes for boolean queries
#@ Rakesh M. Verma;Sanjiv Behl
#t 2002
#c 13
#% 387427
#% 396639
#! In Information Retrieval it is well-known that the complexity of processing boolean queries depends on the size of the intermediate results, which could be huge (and are typically on disk) even though the size of the final result may be quite small. In the case of inverted files the most time consuming operation is the merging or intersection of the list of occurrences [1]. We propose, the Keyword tree (K-tree) and forest, efficient structures to handle boolean queries in keyword-based information retrieval. Extensive simulations show that K-tree is orders-of-magnitude faster (i.e., far fewer I/O's) for boolean queries than the usual approach of merging the lists of occurrences and incurs only a small overhead for single keyword queries. The K-tree can be efficiently parallelized as well. The construction cost of K-tree is comparable to the cost of building inverted files.

#index 397210
#* Example-based phrase translation in Chinese-English CLIR
#@ Bin Wang;Xueqi Cheng;Shuo Bai
#t 2002
#c 13
#% 262870
#! This paper proposes an example-based phrase translation method in a Chinese to English cross-language information retrieval (CLIR) system. The method can generate much more accurate query translations than dictionary-based and common MT-based methods, and then improves the retrieval performance of our CLIR system.

#index 397211
#* Probabilistic multimedia retrieval
#@ Thijs Westerveld
#t 2002
#c 13
#% 340899
#% 713059
#! We present a framework in which probabilistic models for textual and visual information retrieval can be integrated seamlessly. The framework facilitates searching for imagery using textual descriptions and visual examples simultaneously. The underlying Language Models for text and Gaussian Mixture Models for images have proven successful in various retrieval tasks.

#index 397212
#* Chinese keyword extraction based on max-duplicated strings of the documents
#@ Wenfeng Yang
#t 2002
#c 13
#% 115467
#% 232650
#! The corpus analysis methods in Chinese keyword extraction look on the corpus as a single sample of language stochastic process. But the distributions of keywords in the whole corpus and in each document are very different from each other. The extraction based on global statistical information only can get significant keywords in the whole corpus. Max-duplicated strings contain the local significant keywords in each document. In this paper, we designed an efficient algorithm to extract the max-duplicated strings by building PAT-tree for the document, so that the keywords can be picked out from the max-duplicated strings by their SIG values in the corpus.

#index 397213
#* A hierarchical approach: query large music database by acoustic input
#@ Yazhong Feng;Yueting Zhuang;Yunhe Pan
#t 2002
#c 13
#% 204646
#% 280890
#% 316259
#% 555445

#index 397214
#* Correlating multilingual documents via bipartite graph modeling
#@ Hongyuan Zha;Xiang Ji
#t 2002
#c 13
#% 342659
#! There is enormous amount of multilingual documents from various sources and possibly from different countries describing a single event or a set of related events. It is desirable to construct text mining methods that can compare and highlight similarities and differences of those multilingual documents. We discuss our ongoing research that seeks to model a pair of multilingual documents as a weighted bipartite graph with the edge weights computed by means of machine translation. We use spectral method to identify dense subgraphs of the weighted bipartite graph which can be considered as corresponding to sentences that correlate well in textual contents. We illustrate our approach using English and German texts.

#index 397215
#* A system using implicit feedback and top ranking sentences to help users find relevant web documents
#@ Ryen W. White;Joemon M. Jose;Ian Ruthven
#t 2002
#c 13
#% 717120
#! We present a web search interface designed to encourage users to interact more fully with the results of a web search. Wrapping around a major commercial search engine, the system combines three main features; real-time query-biased web document summarisation, the presentation of sentences highly relevant to the searcher's query, and evidence captured from searcher interaction with the retrieval results.

#index 397216
#* Indexing, searching, and retrieving of recorded live presentations with the AOF (authoring on the fly) search engine
#@ Wolfgang Hürst
#t 2002
#c 13
#% 582012
#! The tremendous amount of data resulting from the regular usage of tools for automatic presentation recording demand for elaborate search functionality. A detailed analysis of the according multimedia documents is required to allow search at a very detailed level. Unfortunately, the produced data differs significantly from traditional documents. In this demo, we discuss the problems appearing in the presentation retrieval scenario and introduce aofSE, a search engine to study and illustrate these problems as well as to develop and present according solutions and new approaches for this task.

#index 397217
#* UTACLIR -: general query translation framework for several language pairs
#@ Heikki Keskustalo;Turid Hedlund;Eija Airio
#t 2002
#c 13
#% 340894
#% 420520
#% 561144
#% 561170

#index 397218
#* HyREX: hyper-media retrieval engine for XML
#@ Norbert Fuhr;Norbert Gövert;Kai Großjohann
#t 2002
#c 13
#% 340914

#index 397219
#* Query performance analyser -: a web-based tool for IR research and instruction
#@ Eero Sormunen;Sakari Hokkanen;Petteri Kangaslampi;Petri Pyy;Bemmu Sepponen
#t 2002
#c 13
#% 565236
#! The Interactive Query Performance Analyser (QPA) for information retrieval systems is a Web-based tool for analysing and comparing the performance of individual queries. On top of a standard test collection, it gives an instant visualisation of the performance achieved in a given search topic by any user-generated query. In addition to experimental IR research, QPA can be used in user training to demonstrate the characteristics of and compare differences between IR systems and searching strategies. The first prototype (versions 3.0 and 3.5) of the Query Performance Analyser was developed at the Department of Information Studies, University of Tampere, to serve as a tool for rapid query performance analysis, comparison and visualisation [4,5]. Later, it has been applied to interactive optimisation of queries [2,3]. The analyser has served also in learning environments for IR [1].The demonstration is based on the newest version of the Query Performance Analyser (v. 5.1). It is interfaced to a traditional Boolean IR system (TRIP) and a probabilistic IR system (Inquery) providing access to the TREC collection and two Finnish test collections. Version 5.1 supports multigraded relevance scales, new types of performance visualisations, and query conversions based on mono- and multi-lingual dictionaries. The motivation in developing the analyser is to emphasise the necessity of analysing the behaviour of individual queries. Information retrieval experiments usually measure the average effectiveness of IR methods developed. The analysis of individual queries is neglected although test results may contain individual test topics where general findings do not hold. For the real user of an IR system, the study of variation in results is even more important than averages.

#index 397220
#* Adaptive information extraction for document annotation in amilcare
#@ Fabio Ciravegna;Alexiei Dingli;Yorick Wilks;Daniela Petrelli
#t 2002
#c 13
#% 742446
#% 1289318
#! Amilcare is a tool for Adaptive Information Extraction (IE) designed for supporting active annotation of documents for the Semantic Web (SW). It can be used either for unsupervised document annotation or as a support for human annotation. Amilcare is portable to new applications/domains without any knowledge of IE, as it just requires users to annotate a small training corpus with the information to be extracted. It is based on (LP)2, a supervised learning strategy for IE able to cope with different texts types, from newspaper-like texts, to rigidly formatted Web pages and even a mixture of them[1][5].Adaptation starts with the definition of a tag set for annotation, possibly organized as an ontology. Then users have to manually annotate a small training corpus. Amilcare provides a default mouse-based interface called Melita, where annotations are inserted by first selecting a tag from the ontology and then identifying the text area to annotate with the mouse. Differently from similar annotation tools [4, 5], Melita actively supports training corpus annotation. While users annotate texts, Amilcare runs in the background learning how to reproduce the inserted annotation. Induced rules are silently applied to new texts and their results are compared with the user annotation. When its rules reach a (user-defined) level of accuracy, Melita presents new texts with a preliminary annotation derived by the rule application. In this case users have just to correct mistakes and add missing annotations. User corrections are inputted back to the learner for retraining. This technique focuses the slow and expensive user activity on uncovered cases, avoiding requiring annotating cases where a satisfying effectiveness is already reached. Moreover validating extracted information is a much simpler task than tagging bare texts (and also less error prone), speeding up the process considerably. At the end of the corpus annotation process, the system is trained and the application can be delivered. MnM [6] and Ontomat annotizer [7] are two annotation tools adopting Amilcare's learner.In this demo we simulate the annotation of a small corpus and we show how and when Amilcare is able to support users in the annotation process, focusing on the way the user can control the tool's proactivity and intrusivity. We will also quantify such support with data derived from a number of experiments on corpora. We will focus on training corpus size and correctness of suggestions when the corpus is increased.

#index 397221
#* ExWrap: semi-automatic wrapper generation by example
#@ Bethina Schmitt;Michael Christoffel;Jürgen Schneider
#t 2002
#c 13
#% 338206

#index 397222
#* Souvenir: flexible note-taking tool to pinpoint and share media highlights
#@ Anselm Spoerri
#t 2002
#c 13
#! Digital media audio/video can be difficult to search and share in a personal way. Souvenir is a software system that offers users a flexible and comprehensive way to use their handwritten or text notes to retrieve and share specific media moments. Users can take notes on a variety of devices, such as the paper-based CrossPad, the Palm Pilot and standard keyboard devices. Souvenir segments handwritten notes into an effective media index without the need for handwriting recognition. Users can use their notes to create hyperlinks to random-access media stored in a digital library. Souvenir also has web publishing and email capabilities to enable anyone to access or email media moments directly from a web page. Souvenir annotations capture information that can not be easily inferred by automatic media indexing tools.

#index 397223
#* Hierarchical approach to term suggestion device
#@ Hideo Joho;Mark Sanderson;Micheline Beaulieu
#t 2002
#c 13
#% 280849
#% 346553
#! Our demonstration shows the hierarchy system working on a locally run search engine. Hierarchies are dynamically generated from the retrieved documents, and visualised on the menus. When a user selects a term from the hierarchy, the documents linked to the term are listed, and the term is then added to the initial query to rerun a search. Through the demonstration we illustrate how hierarchical presentation of expansion terms is achieved, and how our approach supports users to articulate their information needs using the hierarchy.

#index 397224
#* Translingual vocabulary mappings for multilingual information access
#@ Fredric C. Gey;Aitao Chen;Michael Buckland;Ray Larson
#t 2002
#c 13
#% 815097

#index 397225
#* GS textplorer -: adaptive framework for information retrieval
#@ Jukka Honkela;Ville H. Tuulos
#t 2002
#c 13
#% 234978
#% 1860651

#index 397226
#* CuTeX: a system for extracting data from text tables
#@ Hasan Davulcu;Saikat Mukherjee;Arvind Seth;I. V. Ramakrishnan
#t 2002
#c 13
#% 237328
#% 248808
#! A wealth of information relevant for e-commerce often appears intext form. This includes specification and performance data sheetsof products, financial statements, product offerings etc. Typicallythese types of product and financial data are published in tabularform. The only separators between items in the table are whitespaces and line separators. We will refer to such tables as texttables. Due to the lack of structure in such tables, theinformation present is not readily queriable using traditionaldatabase query languages like SQL. One way to make it amenable tostandard database querying techniques is to extract the data itemsin the tables and create a database out of the extracted data. Butextraction from text tables poses difficulties due to theirregularity of the data in the column.Existing techniques like [1] and [3] are based on finding fixedseparators between successive columns. However, it is not alwayspossible to find fixed separators. Even if fixed separators existthey may not unambiguously separate columns that have multiworditems. Another set of techniques are based on regular expressions.The problems here are: (i) they are difficult to construct and (ii)they depend on lexical similarity between column items.Note that, by visual inspection a casual observer can correctlyassociate every item in a text table to its corresponding column.This is because all the items belonging to a column appear"clustered" more closely to each other than to items in differentcolumns. Whereas such clusters can be clearly discerned by a humanobserver, making them machine recognizable is the key to robustautomated extraction of data items from text-based tables.Clustering enables us to make associations between items in acolumn based not merely on examining items in adjacent rows butacross all the rows in the table.We have designed and implemented the CuteX system forextracting data from irregular text tables. The input is a filecontaining only text tables. The output produced by CuteX isan association between every items in a column. Note thatCuteX does not do table detection in text. The innovativeaspect of CuteX is its clustering-based algorithm thatdrives the extraction process. In CuteX each line is brokendown into a set of tokens. Each token is a contiguous sequence ofnon white-space characters. The center of any token in a cluster iscloser to the center of some other token in the same cluster.Inter-cluster gaps are gaps between the extremal tokens in theclusters. Starting with an initial set of clusters, adjacentclusters are merged into bigger clusters based on the inter-clustergaps. The algorithm terminates when no more clusters can be merged.We have formalized the notion of a correct extraction and developeda syntactic characterization of tables on which this algorithm willalways produce a correct extraction. Details appear in [2]. Anunique aspect of the algorithm is its robustness in the presence ofmisalignments.Precision of extraction can be improved by supplying the minimumseparation between columns as a parameter. Such a separator isestimated by sampling a few input tables. The clustering algorithmdoes not merge adjacent clusters if the gap between them is largerthan this parameter value. Note though that the minimum column gapcannot be used as a fixed separator since doing so amounts to doinglocalized determination, making it brittle to misalignments.CuteX is implemented in Java and is approximately about3000 lines of code. The system automatically partitions the set ofinput text tables into directories containing correct and incorrectextractions. At the end of an extraction, the user can examine thedirectory containing incorrectly extracted tables, sample a few ofthem, identify if it was caused by an erroneous estimate of theminimum column gap, re-adjust the configuration parameter and starta new extraction on all these tables. Successive iterations cangenerate a higher extraction yield.The primary focus of the demonstration will be on illustratingthe robustness and the iterative process of improving theextraction yield of the clustering algorithm.

#index 397227
#* YellowPager: a tool for ontology-based mining of service directories from web sources
#@ Prashant Choudhari;Hasan Davulcu;Abhishek Joglekar;Akshay More;Saikat Mukherjee;Supriya Patil;I. V. Ramakrishnan
#t 2002
#c 13
#! The web has established itself as the dominant medium for doing electronic commerce. Realizing that its global reach provides significant market and business opportunities, service providers, both large and small are advertising their services on the web. A number of them operate their own web sites promoting their services at length while others are merely listed in a referral site. Aggregating all of the providers into a queriable service directory makes it easy for customers to locate the one most suited for his/her needs.YellowPager is a tool for creating service directories by mining web sources. Service directories created by YellowPager have several merits compared to those generated by existing practices, which typically require participation by service providers (e.g. Verizon's SuperYellowPages.com). Firstly, the information content will be rich. Secondly since the process is automated and repeatable the content can always be kept current. Finally the same process can be readily adapted to different domains.YellowPager builds service directories by mining the web through a combination of keyword-based search engines,web agents, text classifiers and novel extraction algorithms.The extraction is driven by a services ontology consisting of a taxonomy of service concepts and their associated attributes (such as names and addresses) and type descriptions for the attributes. In addition the ontology also associates an extractor function with each attribute. Applying the function to a web page will identify all the occurrences of the attribute in that page.YellowPager's mining algorithm consists of a training step followed by classification and extraction steps. In the training step a classifier is trained to identify web pages relevant to the service of interest. The classification step proceeds by doing a search for the particular service of interest using a keyword based web search engine and retrieves all the matching web pages. From these pages the relevant ones are identified using the classifier. The final step is extraction of attribute values, associated with the service, from these pages. Each web page is parsed into a DOM tree and the extractor functions are applied. All of the attributes corresponding to a service provider are then correctly aggregated. This can pose difficulties especially in the presence of multiple service providers in a page. Using a novel concept of scoring and conflict resolution to prevent erroneous associations of attributes with service provider entities in the page, the algorithm aggregates all the attribute occurrences correctly. The extractor function may not be complete in the sense that it cannot always identify all the attributes in a page. By exploiting the regularity of the sequence in which attributes occurr in referral pages, the mining algorithm automatically learns generalized patterns to locate attributes that the extractor function misses. The distinguishing aspects of YellowPager's extraction algorithm are: (i) it is unsupervised, and (ii) the attribute values in the pages are extracted independent of any page-specific relationships that may exist among the markup tags.YellowPager has been used by a large pet food producer to build a directory of veterinarian service providers in the United States. The resulting database was found to be much larger and richer than that found in Vetquest, Vetworld, and the Super Yellow pages.YellowPager is implemented in JAVA and is interfaced to Rainbow, a library utility in C that is used for classification. The tool will demonstrate the creation of a service directory for any service domain by mining web sources.

#index 455158
#* Information Retrieval Techniques for Speech Applications [this book is based on the workshop “Information Retrieval Techniques for Speech Applications”, held as part of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in New Orleans, USA, in September 2001].
#@ Anni Coden;Eric W. Brown;Savitha Srinivasan
#t 2001
#c 13

#index 501794
#* Segmenting Conversations by Topic, Initiative, and Style
#@ Klaus Ries
#t 2001
#c 13
#% 9197
#% 71063
#% 169813
#% 172800
#% 232814
#% 278106
#% 280815
#% 319382
#% 504890
#% 706148
#% 708199
#% 708427
#% 741057
#% 741058
#% 741060
#% 742204
#% 748667
#% 757426
#% 815061
#% 969375
#! Topical segmentation is a basic tool for information access to audio records of meetings and other types of speech documents which may be fairly long and contain multiple topics. Standard segmentation algorithms are typically based on keywords, pitch contours or pauses. This work demonstrates that speaker initiative and style may be used as segmentation criteria as well. A probabilistic segmentation procedure is presented which allows the integration and modeling of these features in a clean framework with good results.Keyword based segmentation methods degrade significantly on our meeting database when speech recognizer transcripts are used instead of manual transcripts. Speaker initiative is an interesting feature since it delivers good segmentations and should be easy to obtain from the audio. Speech style variation at the beginning, middle and end of topics may also be exploited for topical segmentation and would not require the detection of rare keywords.

#index 501795
#* WASABI: Framework for Real-Time Speech Analysis Applications (Demo)
#@ Eric W. Brown;Anni Coden
#t 2001
#c 13
#% 309124
#% 608367
#% 771356
#! Speech is a tantalizing mode of human communication. On the one hand, humans understand speech with ease and use speech to express complex ideas, information, and knowledge. On the other hand, automatic speech recognition with computers is still very hard, and extracting knowledge from speech is even harder. We have developed a framework for analyzing speech that enables the incorporation of speech into a variety of information and knowledge management applications. We briefly describe the framework and demonstrate two applications that use the framework.

#index 501796
#* Extracting Caller Information from Voicemail
#@ Jing Huang;Geoffrey Zweig;Mukund Padmanabhan
#t 2001
#c 13
#% 742105
#% 818061
#! In this paper we address the problem of extracting caller information from voicemail messages, such as the identity and phone number of the caller. Previous work in information extraction from speech includes spoken document retrieval and named entity detection. This task differs from the named entity task in that the information we are interested in is a subset of the named entities in the message, and consequently, the need to pick the correct subset makes the problem more difficult. Also, the caller's identity may include information that is not typically associated with a named entity. In this work, we present two information extraction methods, one based on hand-crafted rules, one based on statistically trained maximum entropy model.We evaluate their performance on both manually transcribed messages and on the output of a speech recognition system.

#index 501797
#* Speech-Driven Text Retrieval: Using Target IR Collections for Statistical Language Model Adaptation in Speech Recognition
#@ Atsushi Fujii;Katunobu Itou;Tetsuya Ishikawa
#t 2001
#c 13
#% 169781
#% 218984
#% 232660
#% 262039
#% 262092
#% 262105
#% 280811
#% 280815
#% 309102
#% 624426
#% 969345
#! Speech recognition has of late become a practical technology for real world applications. Aiming at speech-driven text retrieval, which facilitates retrieving information with spoken queries, we propose a method to integrate speech recognition and retrieval methods. Since users speak contents related to a target collection, we adapt statistical language models used for speech recognition based on the target collection, so as to improve both the recognition and retrieval accuracy. Experiments using existing test collections combined with dictated queries showed the effectiveness of our method.

#index 501798
#* Clustering of Imperfect Transcripts Using a Novel Similarity Measure
#@ Oktay Ibrahimov;Ishwar K. Sethi;Nevenka Dimitrova
#t 2001
#c 13
#% 46809
#% 239594
#% 293975
#% 334756
#% 434753
#% 445316
#% 582007
#% 969345
#% 1180270
#% 1857416
#% 1857658
#% 1857673
#! There has been a surge of interest in the last several years in methods for automatic generation of content indices for multimedia documents, particularly with respect to video and audio documents. As a result, there is much interest in methods for analyzing transcribed documents from audio and video broadcasts and telephone conversations and messages. The present paper deals with such an analysis by presenting a clustering technique to partition a set of transcribed documents into different meaningful topics. Our method determines the intersection between matching transcripts, evaluates the information contribution by each transcript, assesses the information closeness of overlapping words and calculates similarity based on Chi-square method. The main novelty of our method lies in the proposed similarity measure that is designed to withstand the imperfections of transcribed documents. Experimental results using documents of varying quality of transcription are presented to demonstrate the efficacy of the proposed methodology.

#index 501799
#* Speech and Hand Transcribed Retrieval
#@ Mark Sanderson;Xiao Mang Shou
#t 2001
#c 13
#% 280815
#% 505055
#! This paper describes the issues and preliminary work involved in the creation of an information retrieval system that will manage the retrieval from collections composed of both speech recognised and ordinary text documents. In previous work, it has been shown that because of recognition errors, ordinary documents are generally retrieved in preference to recognised ones. Means of correcting or eliminating the observed bias is the subject of this paper. Initial ideas and some preliminary results are presented.

#index 501923
#* Extracting Keyphrases from Spoken Audio Documents
#@ Alain Désilets;Berry de Bruijn;Joel Martin
#t 2001
#c 13
#% 27049
#% 151565
#% 309102
#% 337415
#% 341351
#% 420487
#% 434737
#% 608367
#% 742225
#% 970588
#! Spoken audio documents are becoming more and more common on the World Wide Web, and this is likely to be accelerated by the widespread deployment of broadband technologies. Unfortunately, speech documents are inherently hard to browse because of their transient nature. One approach to this problem is to label segments of a spoken document with keyphrases that summarise them. In this paper, we investigate an approach for automatically extracting keyphrases from spoken audio documents. We use a keyphrase extraction system (Extractor) originally developed for text, and apply it to errorful Speech Recognition transcripts, which may contain multiple hypotheses for each of the utterances. We show that keyphrase extraction is an "easier" task than full text transcription and that keyphrases can be extracted with reasonable precision from transcripts with Word Error Rates (WER) as high as 62%. This robustness to noise can be attributed to the fact that keyphrase words have a lower WER than non-keyphrase words and that they tend to have more redundancy in the audio. From this we conclude that keyphrase extraction is feasible for a wide range of spoken documents, including less-than-broadcast casual speech. We also show that including multiple utterance hypotheses does not improve the precision of the extracted keyphrases.

#index 501924
#* Perspectives on Information Retrieval and Speech
#@ James Allan
#t 2001
#c 13
#% 216150
#% 350859
#! Several years of research have suggested that the accuracy of spoken document retrieval systems is not adversely affected by speech recognition errors. Even with error rates of around 40%, the effectiveness of an IR system falls less than 10%. The paper hypothesizes that this robust behavior is the result of repetition of important words in the text--meaning that losing one or two occurrences is not crippling-- and the result of additional related words providing a greater context-- meaning that those words will match even if the seemingly critical word is misrecognized. This hypothesis is supported by examples from TREC's SDR track, the TDT evaluation, and some work showing the impact of recognition errors on spoken queries.

#index 501925
#* The Use of Speech Retrieval Systems: A Study Design
#@ Jinmook Kim;Douglas W. Oard
#t 2001
#c 13
#% 83855
#% 167557
#% 246770
#% 249205
#% 258236
#% 308569
#! What criteria for relevance do users apply when selecting a speech recording? What attributes of the recording do they rely on for each criterion? This paper proposes a qualitative research study design to explore those questions. A conceptual framework is presented, research questions are introduced, and the study design is described. The paper concludes with some observations on how the results of the study might inform the design of future systems.

#index 566935
#* Capitalization Recovery for Text
#@ Eric W. Brown;Anni Coden
#t 2001
#c 13
#% 194223
#% 279755
#% 293977
#% 309114
#% 387427
#% 742351
#% 742399
#% 742424
#% 742425
#! Proper capitalization in text is a useful, often mandatory characteristic. Many text processing techniques rely on proper capitalization, and people can more easily read mixed case text. Proper capitalization, however, is often absent in a number of text sources, including automatic speech recognition output and closed caption text. The value of these text sources can be greatly enhanced with proper capitalization. We describe and evaluate a series of techniques that can recover proper capitalization. Our final system is able to recover more than 88% of the capitalized words with better than 90% accuracy.

#index 766404
#* Challenges in using lifetime personal information stores
#@ Gordon Bell;Jim Gemmell;Roger Lueder
#t 2004
#c 13
#% 290150
#% 318522
#% 339375
#% 451595
#% 642983
#! Within five years, our personal computers with terabyte disk drives will be able to store everything we read, write, hear, and many of the images we see including video. Vannevar Bush outlined such a system in his famous 1945 Memex article [1]. For the last four years we have worked on MyLifeBits www.MyLifeBits.com http://www.MyLifeBits.com, a system to digitally store everything from one's life, including books, articles, personal financial records, memorabilia, email, written correspondence, photos(time, location taken), telephone calls, video, television programs, and web pages visited. We recently added content from personal devices that automatically record photos and audio.The project started with the capture of Bell's content [2], followed by an effort to explore the use of the SQL database for storage and retrieval. Work has continued along these lines to extend content capture from every useful source e.g. a meeting capture system. The second phase of the project includes the design of tools and links for annotation, collections, cluster analysis, facets for characterizing the content, creation of timelines and stories, and other inherent database related capabilities, e.g. the ability to pivot on an event or photo or person to retrieve linked information [3]. Ideally we would like to have a system that would read every document, extract meta-data(e.g. Dublin Core) and classify it using multiple ontologies, faceted classifications, or the relevant.While such a system has implications for future computing devices and their users, these systems will only exist if we can effectively utilize the vast personal stores. Although our system is exploratory, the Stuff I've Seen system [4] demonstrates the utility and necessity of easy search and access to one's own data. Other research efforts with similar goals relating to personal information include Haystack [5], LifeStreams [6], and the UK "Memories for Life" Grand Challenge.There are serious research issues beyond the problem of making the information useful through rapid and easy retrieval.The "Dear Appy" problem ("Dear Appy, My application, or platform, or media left me unreadable. Signed, Lost Data") is unsettling to archivists and computer professionals -and must be solved.Just navigating the stored life of individual would at first glance appear to take almost a lifetime to sift through. While we are making progress in the capture of less traditionally archived content(e.g. meetings, phone calls & video), automatic interpretation and index of voice are illusive. MyLifeBits is currently focused on retrieval including the hopefully automatic, addition of meta-data e.g. document type identification, high level knowledge. While such data is essential for the archivist, it is unclear how useful such meta-data is to a one's own information; without such higher level knowledge and concepts, the vast amount of raw bits may be completely unusable.The most cited problem of personal archives is the control of the content including personal security, together with joint ownership of content by other individuals and organizations. In many corporations, periodic expunging of documents is the standard. Similarly, the aspects of a person's life not available in public documents is owned by the organization and all documents may have to be tagged in such a way that it can be expunged, if necessary, when an individual is no longer part of the organization. The HPPA law in the US and even more stringent privacy laws in other counties have major implications for personal stores.

#index 766405
#* Chemoinformatics: an application domain for information retrieval techniques
#@ Peter Willett
#t 2004
#c 13
#% 370823
#% 1213000
#! Chemoinformatics is the generic name for the techniques used to represent, store and process information about the two-dimensional (2D) and three-dimensional (3D) structures of chemical molecules [1, 2]. Chemoinformatics has attracted much recent prominence as a result of developments in the methods that are used to synthesize new molecules and then to test them for biological activity. These developments have resulted in a massive increase in the amounts of structural and biological information that is available to support discovery programmes in the pharmaceutical and agrochemical industries.Chemoinformatics may appear to be far removed from information retrieval (IR), and there are indeed many significant differences, most notably in the use of graph representations to encode chemical molecules, rather than the strings that are used to encode text; however, there are also many similarities between the two fields, and this paper will exemplify some of these relationships. The most obvious area of similarity is in the principal types of database search that are carried out, with both application domains making extensive use of exact match, partial match and best match searching procedures: in the IR context these are known-item searching, Boolean searching and ranked-output searching; in the chemical context, these are structure searching, substructure searching and similarity searching. In IR, there is a natural distinction between an initial ranked-output search and one in which relevance feedback can be employed, where the keywords in the query statement are assigned weights based on their differential occurrences in known-relevant and known-nonrelevant documents. In the chemoinformatics technique called substructural analysis, substructural fragments are assigned weights based on their occurrence in molecules that do possess, and molecules that do not possess, some desired biological activity [3]. The analogy between relevance and biological activity has also resulted in the development of measures to quantify the effectiveness of chemical searching procedures that are based on the standard IR concepts of recall and precision [4].Analogies such as these have provided the basis for some of the chemoinformatics research carried out in Sheffield. The starting point was the recognition that techniques applicable to documents represented by keywords might also be applicable to molecules represented by substructural fragments. This led directly to the introduction of similarity searching, something that is now a standard tool in chemoinformatics software systems; in particular, its use for virtual screening, i.e., the ranking of a database in order of decreasing probability of activity so as to maximize the cost-effectiveness of biological testing [5]. Measures of inter-molecular structural similarity also lie at the heart of systems for clustering chemical databases: just as IR has the Cluster Hypothesis (similar documents tend to be relevant to the same requests) as a basis for document clustering, so the Similar Property Principle (similar molecules tend to have similar properties) has led to clustering becoming a well-established tool for the organization of large chemical databases [6]. More recently, we have applied another IR technique, the use of data fusion to combine different rankings of a database, to chemoinformatics and again found that it is equally applicable in this new domain [7].The many similarities between IR and chemoinformatics that have already been identified suggest that chemoinformatics is a domain of which IR researchers should be aware when considering the applicability of new techniques that they have developed.

#index 766406
#* Evaluating high accuracy retrieval techniques
#@ Chirag Shah;W. Bruce Croft
#t 2004
#c 13
#% 67565
#% 144031
#% 169729
#% 169768
#% 203770
#% 208934
#% 218978
#% 262096
#% 280851
#% 340901
#% 397161
#% 478258
#% 642977
#% 756766
#% 789959
#% 803037
#% 815868
#% 995516
#% 1830411
#! Although information retrieval research has always been concerned with improving the effectiveness of search, in some applications, such as information analysis, a more specific requirement exists for high accuracy retrieval. This means that achieving high precision in the top document ranks is paramount. In this paper we present work aimed at achieving high accuracy in ad-hoc document retrieval by incorporating approaches from question answering(QA). We focus on getting the first relevant result as high as possible in the ranked list and argue that traditional precision and recall are not appropriate measures for evaluatin this task. We instead use the mean reciprocal rank(MRR) of the first relevant result. We evaluate three different methods for modifying queries to achieve high accuracy. The experiments done on TREC data provide support for the approach of using MRR and incorporating QA techniques for getting high accuracy in ad-hoc retrieval task.

#index 766407
#* Scaling IR-system evaluation using term relevance sets
#@ Einat Amitay;David Carmel;Ronny Lempel;Aya Soffer
#t 2004
#c 13
#% 65964
#% 109187
#% 255137
#% 262102
#% 268087
#% 281174
#% 288408
#% 309093
#% 330609
#% 340890
#% 453464
#% 561315
#% 577370
#% 643029
#% 728355
#% 730009
#% 731621
#! This paper describes an evaluation method based on Term Relevance Sets Trels that measures an IR system's quality by examining the content of the retrieved results rather than by looking for pre-specified relevant pages. Trels consist of a list of terms believed to be relevant for a particular query as well as a list of irrelevant terms. The proposed method does not involve any document relevance judgments, and as such is not adversely affected by changes to the underlying collection. Therefore, it can better scale to very large, dynamic collections such as the Web. Moreover, this method can evaluate a system's effectiveness on an updatable "live" collection, or on collections derived from different data sources. Our experiments show that the proposed method is very highly correlated with official TREC measures.

#index 766408
#* Using temporal profiles of queries for precision prediction
#@ Fernando Diaz;Rosie Jones
#t 2004
#c 13
#% 144034
#% 290482
#% 340901
#% 397161
#% 577220
#% 719598
#% 730070
#! A key missing component in information retrieval systems is self-diagnostic tests to establish whether the system can provide reasonable results for a given query on a document collection. If we can measure properties of a retrieved set of documents which allow us to predict average precision, we can automate the decision of whether to elicit relevance feedback, or modify the retrieval system in other ways. We use meta-data attached to documents in the form of time stamps to measure the distribution of documents retrieved in response to a query, over the time domain, to create a temporal profile for a query. We define some useful features over this temporal profile. We find that using these temporal features, together with the content of the documents retrieved, we can improve the prediction of average precision for a query.

#index 766409
#* Retrieval evaluation with incomplete information
#@ Chris Buckley;Ellen M. Voorhees
#t 2004
#c 13
#% 43863
#% 100008
#% 109187
#% 187763
#% 262034
#% 262097
#% 262102
#% 309093
#% 312689
#% 340890
#% 340892
#% 397163
#% 561315
#% 643029
#! This paper examines whether the Cranfield evaluation methodology is robust to gross violations of the completeness assumption (i.e., the assumption that all relevant documents within a test collection have been identified and are present in the collection). We show that current evaluation measures are not robust to substantially incomplete relevance judgments. A new measure is introduced that is both highly correlated with existing measures when complete judgments are available and more robust to incomplete judgment sets. This finding suggests that substantially larger or dynamic test collections built using current pooling practices should be viable laboratory tools, despite the fact that the relevance information will be incomplete and imperfect.

#index 766410
#* Forming test collections with no system pooling
#@ Mark Sanderson;Hideo Joho
#t 2004
#c 13
#% 118731
#% 219056
#% 232660
#% 232710
#% 262097
#% 262102
#% 262105
#% 319273
#% 340890
#% 340892
#% 340934
#% 420526
#% 575572
#% 643005
#% 766409
#! Forming test collection relevance judgments from the pooled output of multiple retrieval systems has become the standard process for creating resources such as the TREC, CLEF, and NTCIR test collections. This paper presents a series of experiments examining three different ways of building test collections where no system pooling is used. First, a collection formation technique combining manual feedback and multiple systems is adapted to work with a single retrieval system. Second, an existing method based on pooling the output of multiple manual searches is re-examined: testing a wider range of searchers and retrieval systems than has been examined before. Third, a new approach is explored where the ranked output of a single automatic search on a single retrieval system is assessed for relevance: no pooling whatsoever. Using established techniques for evaluating the quality of relevance judgments, in all three cases, test collections are formed that are as good as TREC.

#index 766411
#* Building an information retrieval test collection for spontaneous conversational speech
#@ Douglas W. Oard;Dagobert Soergel;David Doermann;Xiaoli Huang;G. Craig Murray;Jianqiang Wang;Bhuvana Ramabhadran;Martin Franz;Samuel Gustman;James Mayfield;Liliya Kharevych;Stephanie Strassel
#t 2004
#c 13
#% 207677
#% 258236
#% 262097
#% 267753
#% 280817
#% 309093
#% 312689
#% 378480
#% 575572
#! Test collections model use cases in ways that facilitate evaluation of information retrieval systems. This paper describes the use of search-guided relevance assessment to create a test collection for retrieval of spontaneous conversational speech. Approximately 10,000 thematically coherent segments were manually identified in 625 hours of oral history interviews with 246 individuals. Automatic speech recognition results, manually prepared summaries, controlled vocabulary indexing, and name authority control are available for every segment. Those features were leveraged by a team of four relevance assessors to identify topically relevant segments for 28 topics developed from actual user requests. Search-guided assessment yielded sufficient inter-annotator agreement to support formative evaluation during system development. Baseline results for ranked retrieval are presented to illustrate use of the collection.

#index 766412
#* A formal study of information retrieval heuristics
#@ Hui Fang;Tao Tao;ChengXiang Zhai
#t 2004
#c 13
#% 46803
#% 67565
#% 111303
#% 120104
#% 169781
#% 176530
#% 232645
#% 253191
#% 262096
#% 340948
#% 406493
#% 411760
#! Empirical studies of information retrieval methods show that good retrieval performance is closely related to the use of various retrieval heuristics, such as TF-IDF weighting. One basic research question is thus what exactly are these "necessary" heuristics that seem to cause good retrieval performance. In this paper, we present a formal study of retrieval heuristics. We formally define a set of basic desirable constraints that any reasonable retrieval function should satisfy, and check these constraints on a variety of representative retrieval functions. We find that none of these retrieval functions satisfies all the constraints unconditionally. Empirical results show that when a constraint is not satisfied, it often indicates non-optimality of the method, and when a constraint is satisfied only for a certain range of parameter values, its performance tends to be poor when the parameter is out of the range. In general, we find that the empirical performance of a retrieval formula is tightly related to how well it satisfies these constraints. Thus the proposed constraints provide a good explanation of many empirical observations and make it possible to evaluate any existing or new retrieval formula analytically.

#index 766413
#* Probabilistic model for contextual retrieval
#@ Ji-Rong Wen;Ni Lao;Wei-Ying Ma
#t 2004
#c 13
#% 262084
#% 262096
#% 280851
#% 298183
#% 310567
#% 330705
#% 340899
#% 342961
#% 397128
#% 641976
#% 740915
#% 799013
#! Contextual retrieval is a critical technique for facilitating many important applications such as mobile search, personalized search, PC troubleshooting, etc. Despite of its importance, there is no comprehensive retrieval model to describe the contextual retrieval process. We observed that incompatible context, noisy context and incomplete query are several important issues commonly existing in contextual retrieval applications. However, these issues have not been previously explored and discussed. In this paper, we propose probabilistic models to address these problems. Our study clearly shows that query log is the key to build effective contextual retrieval models. We also conduct a case study in the PC troubleshooting domain to testify the performance of the proposed models and experimental results show that the models can achieve very good retrieval precision.

#index 766414
#* Discriminative models for information retrieval
#@ Ramesh Nallapati
#t 2004
#c 13
#% 118756
#% 169780
#% 190581
#% 211044
#% 227819
#% 262096
#% 269217
#% 274731
#% 314739
#% 340948
#% 397126
#% 413593
#% 420077
#% 458379
#% 642976
#% 642992
#% 840583
#! Discriminative models have been preferred over generative models in many machine learning problems in the recent past owing to some of their attractive theoretical properties. In this paper, we explore the applicability of discriminative classifiers for IR. We have compared the performance of two popular discriminative models, namely the maximum entropy model and support vector machines with that of language modeling, the state-of-the-art generative model for IR. Our experiments on ad-hoc retrieval indicate that although maximum entropy is significantly worse than language models, support vector machines are on par with language models. We argue that the main reason to prefer SVMs over language models is their ability to learn arbitrary features automatically as demonstrated by our experiments on the home-page finding task of TREC-10.

#index 766415
#* The overlap problem in content-oriented XML retrieval evaluation
#@ Gabriella Kazai;Mounia Lalmas;Arjen P. de Vries
#t 2004
#c 13
#% 57485
#% 176530
#% 411762
#% 575729
#% 757550
#! Within the INitiative for the Evaluation of XML Retrieval(INEX) a number of metrics to evaluate the effectiveness of content-oriented XML retrieval approaches were developed. Although these metrics provide a solution towards addressing the problem of overlapping result elements, they do not consider the problem of overlapping reference components within the recall-base, thus leading to skewed effectiveness scores. We propose alternative metrics that aim to provide a solution to both overlap issues.

#index 766416
#* Length normalization in XML retrieval
#@ Jaap Kamps;Maarten de Rijke;Börkur Sigurbjörnsson
#t 2004
#c 13
#% 165307
#% 169811
#% 217268
#% 236052
#% 280850
#% 280851
#% 340948
#% 397126
#% 406493
#% 411760
#% 642993
#% 643044
#! XML retrieval is a departure from standard document retrieval in which each individual XML element, ranging from italicized words or phrases to full blown articles, is a potentially retrievable unit. The distribution of XML element lengths is unlike what we usually observe in standard document collections, prompting us to revisit the issue of document length normalization. We perform a comparative analysis of arbitrary elements versus relevant elements, and show the importance of length as a parameter for XML retrieval. Within the language modeling framework, we investigate a range of techniques that deal with length either directly or indirectly. We observe a length bias introduced by the amount of smoothing, and show the importance of extreme length priors for XML retrieval. We also show that simply removing shorter elements from the index (by introducing a cut-off value) does not create an appropriate document length normalization. Even after increasing the minimal size of XML elements occurring in the index, the importance of an extreme length bias remains.

#index 766417
#* Configurable indexing and ranking for XML information retrieval
#@ Shaorong Liu;Qinghua Zou;Wesley W. Chu
#t 2004
#c 13
#% 340914
#% 345712
#% 406493
#% 479465
#% 504581
#% 590529
#% 642993
#% 1015269
#! Indexing and ranking are two key factors for efficient and effective XML information retrieval. Inappropriate indexing may result in false negatives and false positives, and improper ranking may lead to low precisions. In this paper, we propose a configurable XML information retrieval system, in which users can configure appropriate index types for XML tags and text contents. Based on users' index configurations, the system transforms XML structures into a compact tree representation, Ctree, and indexes XML text contents. To support XML ranking, we propose the concepts of "weighted term frequency" and "inverted element frequency," where the weight of a term depends on its frequency and location within an XML element as well as its popularity among similar elements in an XML dataset. We evaluate the effectiveness of our system through extensive experiments on the INEX 03 dataset and 30 content and structure (CAS) topics. The experimental results reveal that our system has significantly high precision at low recall regions and achieves the highest average precision (0.3309) as compared with 38 official INEX 03 submissions using the strict evaluation metric.

#index 766418
#* Locality preserving indexing for document representation
#@ Xiaofei He;Deng Cai;Haifeng Liu;Wei-Ying Ma
#t 2004
#c 13
#% 118749
#% 118762
#% 124009
#% 248027
#% 262217
#% 280819
#% 280822
#% 304915
#% 309129
#% 340910
#% 342617
#% 406493
#% 643008
#% 729437
#! Document representation and indexing is a key problem for document analysis and processing, such as clustering, classification and retrieval. Conventionally, Latent Semantic Indexing (LSI) is considered effective in deriving such an indexing. LSI essentially detects the most representative features for document representation rather than the most discriminative features. Therefore, LSI might not be optimal in discriminating documents with different semantics. In this paper, a novel algorithm called Locality Preserving Indexing (LPI) is proposed for document indexing. Each document is represented by a vector with low dimensionality. In contrast to LSI which discovers the global structure of the document space, LPI discovers the local structure and obtains a compact document representation subspace that best detects the essential semantic structure. We compare the proposed LPI approach with LSI on two standard databases. Experimental results show that LPI provides better representation in the sense of semantic structure.

#index 766419
#* Polynomial filtering in latent semantic indexing for information retrieval
#@ E. Kokiopoulou;Y. Saad
#t 2004
#c 13
#% 115470
#% 200694
#% 224113
#% 262217
#% 616105
#! Latent Semantic Indexing (LSI) is a well established and effective framework for conceptual information retrieval. In traditional implementations of LSI the semantic structure of the collection is projected into the k-dimensional space derived from a rank-k approximation of the original term-by-document matrix. This paper discusses a new way to implement the LSI methodology, based on polynomial filtering. The new framework does not rely on any matrix decomposition and therefore its computational cost and storage requirements are low relative to traditional implementations of LSI. Additionally, it can be used as an effective information filtering technique when updating LSI models based on user feedback.

#index 766420
#* On scaling latent semantic indexing for large peer-to-peer systems
#@ Chunqiang Tang;Sandhya Dwarkadas;Zhichen Xu
#t 2004
#c 13
#% 41374
#% 200694
#% 218982
#% 229247
#% 248027
#% 262217
#% 276754
#% 280856
#% 285932
#% 287463
#% 291942
#% 316478
#% 316534
#% 321635
#% 340176
#% 342617
#% 425010
#% 479649
#% 643013
#% 646221
#% 648114
#! The exponential growth of data demands scalable infrastructures capable of indexing and searching rich content such as text, music, and images. A promising direction is to combine information re-trieval with peer-to-peer technology for scalability, fault-tolerance, and low administration cost. One pioneering work along this di-rection is pSearch [32, 33]. pSearch places documents onto a peer-to- peer overlay network according to semantic vectors produced using Latent Semantic Indexing (LSI). The search cost for a query is reduced since documents related to the query are likely to be co-located on a small number of nodes. Unfortunately, because of its reliance on LSI, pSearch also inherits the limitations of LSI. (1) When the corpus is large and heterogeneous, LSI's retrieval quality is inferior to methods such as Okapi. (2) The Singular Value Decomposition (SVD) used in LSI is unscalable in terms of both memory consumption and computation time.This paper addresses the above limitations of LSI and makes the following contributions. (1) To reduce the cost of SVD, we reduce the size of its input matrix through document clustering and term selection. Our method retains the retrieval quality of LSI but is several orders of magnitude more efficient. (2) Through extensive experimentation, we found that proper normalization of semantic vectors for terms and documents improves recall by 76%. (3) To further improve retrieval quality, we use low-dimensional subvectors of semantic vectors to cluster documents in the overlay and then use Okapi to guide the search and document selection.

#index 766422
#* GaP: a factor model for discrete data
#@ John Canny
#t 2004
#c 13
#% 272325
#% 280819
#% 313975
#% 340948
#% 643008
#% 722904
#% 1860500
#! We present a probabilistic model for a document corpus that combines many of the desirable features of previous models. The model is called "GaP" for Gamma-Poisson, the distributions of the first and last random variable. GaP is a factor model, that is it gives an approximate factorization of the document-term matrix into a product of matrices Λ and X. These factors have strictly non-negative terms. GaP is a generative probabilistic model that assigns finite probabilities to documents in a corpus. It can be computed with an efficient and simple EM recurrence. For a suitable choice of parameters, the GaP factorization maximizes independence between the factors. So it can be used as an independent-component algorithm adapted to document data. The form of the GaP model is empirically as well as analytically motivated. It gives very accurate results as a probabilistic model (measured via perplexity) and as a retrieval model. The GaP model projects documents and terms into a low-dimensional space of "themes," and models texts as "passages" of terms on the same theme.

#index 766423
#* Belief revision for adaptive information retrieval
#@ Raymond Y.K. Lau;Peter D. Bruza;Dawei Song
#t 2004
#c 13
#% 54459
#% 111942
#% 160378
#% 169738
#% 169741
#% 219053
#% 280824
#% 319874
#% 342819
#% 406493
#% 407891
#% 465747
#% 480008
#% 481290
#% 572500
#% 1273685
#% 1290097
#! Applying Belief Revision logic to model adaptive information retrieval is appealing since it provides a rigorous theoretical foundation to model partiality and uncertainty inherent in any information retrieval (IR) processes. In particular, a retrieval context can be formalised as a belief set and the formalised context is used to disambiguate vague user queries. Belief revision logic also provides a robust computational mechanism to revise an IR system's beliefs about the users' changing information needs. In addition, information flow is proposed as a text mining method to automatically acquire the initial IR contexts. The advantage of a belief-based IRsystem is that its IR behaviour is more predictable and explanatory. However, computational efficiency is often a concern when the belief revision formalisms are applied to large real-life applications. This paper describes our belief-based adaptive IR system which is underpinned by an efficient belief revision mechanism. Our initial experiments show that the belief-based symbolic IR model is more effective than a classical quantitative IR model. To our best knowledge, this is the first successful empirical evaluation of a logic-based IR model based on large IR benchmark collections.

#index 766424
#* Tuning before feedback: combining ranking discovery and blind feedback for robust retrieval
#@ Weiguo Fan;Ming Luo;Li Wang;Wensi Xi;Edward A. Fox
#t 2004
#c 13
#% 46803
#% 54970
#% 92696
#% 111304
#% 114994
#% 118726
#% 124073
#% 157135
#% 169774
#% 169780
#% 232703
#% 240146
#% 252473
#% 253191
#% 326522
#% 345139
#% 399890
#% 420464
#% 733838
#% 738746
#% 740768
#% 773034
#% 787501
#% 804914
#% 873823
#! Both ranking functions and user queries are very important factors affecting a search engine's performance. Prior research has looked at how to improve ad-hoc retrieval performance for existing queries while tuning the ranking function, or modify and expand user queries using a fixed ranking scheme using blind feedback. However, almost no research has looked at how to combine ranking function tuning and blind feedback together to improve ad-hoc retrieval performance. In this paper, we look at the performance improvement for ad-hoc retrieval from a more integrated point of view by combining the merits of both techniques. In particular, we argue that the ranking function should be tuned first, using user-provided queries, before applying the blind feedback technique. The intuition is that highly-tuned ranking offers more high quality documents at the top of the hit list, thus offers a stronger baseline for blind feedback. We verify this integrated model in a large scale heterogeneous collection and the experimental results show that combining ranking function tuning and blind feedback can improve search performance by almost 30% over the baseline Okapi system.

#index 766425
#* Translating unknown queries with web corpora for cross-language information retrieval
#@ Pu-Jen Cheng;Jei-Wen Teng;Ruei-Cheng Chen;Jenq-Haur Wang;Wen-Hsiang Lu;Lee-Feng Chien
#t 2004
#c 13
#% 99650
#% 211043
#% 218988
#% 232650
#% 280826
#% 340897
#% 397145
#% 401405
#% 655486
#% 735133
#% 740395
#% 747947
#% 748444
#% 750865
#% 760835
#% 786574
#% 786575
#! It is crucial for cross-language information retrieval (CLIR) systems to deal with the translation of unknown queries due to that real queries might be short. The purpose of this paper is to investigate the feasibility of exploiting the Web as the corpus source to translate unknown queries for CLIR. We propose an online translation approach to determine effective translations for unknown query terms via mining of bilingual search-result pages obtained from Web search engines. This approach can alleviate the problem of the lack of large bilingual corpora, translate many unknown query terms, provide flexible query specifications, and extract semantically-close translations to benefit CLIR tasks -- especially for cross-language Web search.

#index 766426
#* Resource selection for domain-specific cross-lingual IR
#@ Monica Rogati;Yiming Yang
#t 2004
#c 13
#% 286307
#% 316880
#% 561160
#% 740915
#% 786575
#% 817596
#% 969401
#! An under-explored question in cross-language information retrieval (CLIR) is to what degree the performance of CLIR methods depends on the availability of high-quality translation resources for particular domains. To address this issue, we evaluate several competitive CLIR methods - with different training corpora - on test documents in the medical domain. Our results show severe performance degradation when using a general-purpose training corpus or a commercial machine translation system (SYSTRAN), versus a domain-specific training corpus. A related unexplored question is whether we can improve CLIR performance by systematically analyzing training resources and optimally matching them to target collections. We start exploring this problem by suggesting a simple criterion for automatically matching training resources to target corpora. By using cosine similarity between training and target corpora as resource weights we obtained an average of 5.6% improvement over using all resources with no weights. The same metric yields 99.4% of the performance obtained when an oracle chooses the optimal resource every time.

#index 766427
#* Using the web for automated translation extraction in cross-language information retrieval
#@ Ying Zhang;Phil Vines
#t 2004
#c 13
#% 262047
#% 290703
#% 316880
#% 316902
#% 397144
#% 397146
#% 420472
#% 561307
#% 562054
#% 730025
#! There have been significant advances in Cross-Language Information Retrieval (CLIR) in recent years. One of the major remaining reasons that CLIR does not perform as well as monolingual retrieval is the presence of out of vocabulary (OOV) terms. Previous work has either relied on manual intervention or has only been partially successful in solving this problem. We use a method that extends earlier work in this area by augmenting this with statistical analysis, and corpus-based translation disambiguation to dynamically discover translations of OOV terms. The method can be applied to both Chinese-English and English-Chinese CLIR, correctly extracting translations of OOV terms from the Web automatically, and thus is a significant improvement on earlier work.

#index 766428
#* Dependence language model for information retrieval
#@ Jianfeng Gao;Jian-Yun Nie;Guangyuan Wu;Guihong Cao
#t 2004
#c 13
#% 109192
#% 120104
#% 157910
#% 169781
#% 184488
#% 252472
#% 262096
#% 280850
#% 287253
#% 298183
#% 340899
#% 397129
#% 397146
#% 397205
#% 413593
#% 458369
#% 647106
#% 707797
#% 748722
#% 815808
#% 817484
#! This paper presents a new dependence language modeling approach to information retrieval. The approach extends the basic language modeling approach based on unigram by relaxing the independence assumption. We integrate the linkage of a query as a hidden variable, which expresses the term dependencies within the query as an acyclic, planar, undirected graph. We then assume that a query is generated from a document in two stages: the linkage is generated first, and then each term is generated in turn depending on other related terms according to the linkage. We also present a smoothing method for model parameter estimation and an approach to learning the linkage of a sentence in an unsupervised manner. The new approach is compared to the classical probabilistic retrieval model and the previously proposed language models with and without taking into account term dependencies. Results show that our model achieves substantial and significant improvements on TREC collections.

#index 766429
#* Parsimonious language models for information retrieval
#@ Djoerd Hiemstra;Stephen Robertson;Hugo Zaragoza
#t 2004
#c 13
#% 235918
#% 262096
#% 279755
#% 280819
#% 280850
#% 280851
#% 287253
#% 340897
#% 340899
#% 340901
#% 342707
#% 375017
#% 379604
#% 397126
#% 397133
#% 397144
#% 713059
#! We systematically investigate a new approach to estimating the parameters of language models for information retrieval, called parsimonious language models. Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing. As such, they need fewer (non-zero) parameters to describe the data. We apply parsimonious models at three stages of the retrieval process: 1) at indexing time; 2) at search time; 3) at feedback time. Experimental results show that we are able to build models that are significantly smaller than standard models, but that still perform at least as well as the standard approaches.

#index 766430
#* Cluster-based retrieval using language models
#@ Xiaoyong Liu;W. Bruce Croft
#t 2004
#c 13
#% 11646
#% 46809
#% 57990
#% 218992
#% 228105
#% 262096
#% 280850
#% 280856
#% 324129
#% 324192
#% 340901
#% 342660
#% 375017
#% 397129
#% 397133
#% 413592
#% 427921
#% 719598
#! Previous research on cluster-based retrieval has been inconclusive as to whether it does bring improved retrieval effectiveness over document-based retrieval. Recent developments in the language modeling approach to IR have motivated us to re-examine this problem within this new retrieval framework. We propose two new models for cluster-based retrieval and evaluate them on several TREC collections. We show that cluster-based retrieval can perform consistently across collections of realistic size, and significant improvements over document-based retrieval can be obtained in a fully automatic manner and without relevance information provided by human.

#index 766431
#* Corpus structure, language models, and ad hoc information retrieval
#@ Oren Kurland;Lillian Lee
#t 2004
#c 13
#% 158687
#% 218992
#% 262096
#% 280856
#% 340899
#% 340901
#% 340948
#% 375017
#% 397127
#% 425011
#% 458396
#% 642974
#% 719598
#% 722904
#! Most previous work on the recently developed language-modeling approach to information retrieval focuses on document-specific characteristics, and therefore does not take into account the structure of the surrounding corpus. We propose a novel algorithmic framework in which information provided by document-based language models is enhanced by the incorporation of information drawn from clusters of similar documents. Using this framework, we develop a suite of new algorithms. Even the simplest typically outperforms the standard language-modeling approach in precision and recall, and our new interpolation algorithm posts statistically significant improvements for both metrics over all three corpora tested.

#index 766432
#* Document clustering by concept factorization
#@ Wei Xu;Yihong Gong
#t 2004
#c 13
#% 262059
#% 313959
#% 316478
#% 342621
#% 342659
#% 397147
#% 425010
#% 466675
#% 643008
#% 1838081
#% 1860761
#! In this paper, we propose a new data clustering method called concept factorization that models each concept as a linear combination of the data points, and each data point as a linear combination of the concepts. With this model, the data clustering task is accomplished by computing the two sets of linear coefficients, and this linear coefficients computation is carried out by finding the non-negative solution that minimizes the reconstruction error of the data points. The cluster label of each data point can be easily derived from the obtained linear coefficients. This method differs from the method of clustering based on non-negative matrix factorization (NMF) \citeXu03 in that it can be applied to data containing negative values and the method can be implemented in the kernel space. Our experimental results show that the proposed data clustering method and its variations performs best among 11 algorithms and their variations that we have evaluated on both TDT2 and Reuters-21578 corpus. In addition to its good performance, the new method also has the merit in its easy and reliable derivation of the clustering results.

#index 766433
#* Learning to cluster web search results
#@ Hua-Jun Zeng;Qi-Cai He;Zheng Chen;Wei-Ying Ma;Jinwen Ma
#t 2004
#c 13
#% 144023
#% 218992
#% 232650
#% 262045
#% 269217
#% 281186
#% 340951
#% 577326
#! Organizing Web search results into clusters facilitates users' quick browsing through search results. Traditional clustering techniques are inadequate since they don't generate clusters with highly readable names. In this paper, we reformalize the clustering problem as a salient phrase ranking problem. Given a query and the ranked list of documents (typically a list of titles and snippets) returned by a certain Web search engine, our method first extracts and ranks salient phrases as candidate cluster names, based on a regression model learned from human labeled training data. The documents are assigned to relevant salient phrases to form candidate clusters, and the final clusters are generated by merging these candidate clusters. Experimental results verify our method's feasibility and effectiveness.

#index 766434
#* Document clustering via adaptive subspace iteration
#@ Tao Li;Sheng Ma;Mitsunori Ogihara
#t 2004
#c 13
#% 36672
#% 51647
#% 115608
#% 210173
#% 248790
#% 248792
#% 252836
#% 273891
#% 304423
#% 309128
#% 313959
#% 316709
#% 318790
#% 375388
#% 444007
#% 457710
#% 469422
#% 487995
#% 629648
#% 635713
#% 643008
#% 656714
#% 679885
#% 722936
#% 729918
#% 730050
#! Document clustering has long been an important problem in information retrieval. In this paper, we present a new clustering algorithm ASI1 , which uses explicitly modeling of the subspace structure associated with each cluster. ASI simultaneously performs data reduction and subspace identification via an iterative alternating optimization procedure. Motivated from the optimization procedure, we then provide a novel method to determine the number of clusters. We also discuss the connections of ASI with various existential clustering approaches. Finally, extensive experimental results on real data sets show the effectiveness of ASI algorithm.

#index 766435
#* Restrictive clustering and metaclustering for self-organizing document collections
#@ Stefan Siersdorfer;Sergej Sizov
#t 2004
#c 13
#% 99690
#% 190581
#% 209021
#% 279755
#% 316709
#% 387427
#% 420077
#% 465754
#% 481290
#% 482655
#% 678757
#% 722902
#! This paper addresses the problem of automatically structuring heterogenous document collections by using clustering methods. In contrast to traditional clustering, we study restrictive methods and ensemble-based meta methods that may decide to leave out some documents rather than assigning them to inappropriate clusters with low confidence. These techniques result in higher cluster purity, better overall accuracy, and make unsupervised self-organization more robust. Our comprehensive experimental studies on three different real-world data collections demonstrate these benefits. The proposed methods seem particularly suitable for automatically substructuring personal email folders or personal Web directories that are populated by focused crawlers, and they can be combined with supervised classification techniques.

#index 766436
#* Feature selection using linear classifier weights: interaction with classification models
#@ Dunja Mladenić;Janez Brank;Marko Grobelnik;Natasa Milic-Frayling
#t 2004
#c 13
#% 92233
#% 197394
#% 269217
#% 466266
#% 993986
#! This paper explores feature scoring and selection based on weights from linear classification models. It investigates how these methods combine with various learning models. Our comparative analysis includes three learning algorithms: Naïve Bayes, Perceptron, and Support Vector Machines (SVM) in combination with three feature weighting methods: Odds Ratio, Information Gain, and weights from linear models, the linear SVM and Perceptron. Experiments show that feature selection using weights from linear SVMs yields better classification performance than other feature weighting methods when combined with the three explored learning algorithms. The results support the conjecture that it is the sophistication of the feature weighting method rather than its apparent compatibility with the learning algorithm that improves classification performance.

#index 766437
#* Web-page classification through summarization
#@ Dou Shen;Zheng Chen;Qiang Yang;Hua-Jun Zeng;Benyu Zhang;Yuchang Lu;Wei-Ying Ma
#t 2004
#c 13
#% 132779
#% 190581
#% 194251
#% 197394
#% 200694
#% 248810
#% 297550
#% 309115
#% 309116
#% 330765
#% 330780
#% 340884
#% 342702
#% 344447
#% 348178
#% 375017
#% 376266
#% 458379
#% 465754
#% 466263
#% 578415
#% 642980
#% 729939
#% 815256
#% 853817
#! Web-page classification is much more difficult than pure-text classification due to a large variety of noisy information embedded in Web pages. In this paper, we propose a new Web-page classification algorithm based on Web summarization for improving the accuracy. We first give empirical evidence that ideal Web-page summaries generated by human editors can indeed improve the performance of Web-page classification algorithms. We then propose a new Web summarization-based classification algorithm and evaluate it along with several other state-of-the-art text summarization algorithms on the LookSmart Web directory. Experimental results show that our proposed summarization-based classification algorithm achieves an approximately 8.8% improvement as compared to pure-text-based classification algorithm. We further introduce an ensemble classifier using the improved summarization algorithm and show that it achieves about 12.9% improvement over pure-text based methods.

#index 766438
#* Parameterized generation of labeled datasets for text categorization based on a hierarchical directory
#@ Dmitry Davidov;Evgeniy Gabrilovich;Shaul Markovitch
#t 2004
#c 13
#% 99690
#% 131222
#% 136350
#% 169777
#% 190581
#% 260001
#% 269217
#% 287214
#% 309141
#% 340940
#% 344447
#% 348148
#% 348184
#% 397149
#% 430761
#% 458379
#% 735138
#% 763708
#% 770810
#! Although text categorization is a burgeoning area of IR research, readily available test collections in this field are surprisingly scarce. We describe a methodology and system (named ACCIO) for automatically acquiring labeled datasets for text categorization from the World Wide Web, by capitalizing on the body of knowledge encoded in the structure of existing hierarchical directories such as the Open Directory. We define parameters of categories that make it possible to acquire numerous datasets with desired properties, which in turn allow better control over categorization experiments. In particular, we develop metrics that estimate the difficulty of a dataset by examining the host directory structure. These metrics are shown to be good predictors of categorization accuracy that can be achieved on a dataset, and serve as efficient heuristics for generating datasets subject to user's requirements. A large collection of automatically generated datasets are made available for other researchers to use.

#index 766439
#* Information retrieval using word senses: root sense tagging approach
#@ Sang-Bum Kim;Hee-Cheol Seo;Hae-Chang Rim
#t 2004
#c 13
#% 131434
#% 144031
#% 279755
#% 280819
#% 292686
#% 324129
#% 420471
#% 642994
#% 748550
#! Information retrieval using word senses is emerging as a good research challenge on semantic information retrieval. In this paper, we propose a new method using word senses in information retrieval: root sense tagging method. This method assigns coarse-grained word senses defined in WordNet to query terms and document terms by unsupervised way using co-occurrence information constructed automatically. Our sense tagger is crude, but performs consistent disambiguation by considering only the single most informative word as evidence to disambiguate the target word. We also allow multiple-sense assignment to alleviate the problem caused by incorrect disambiguation.Experimental results on a large-scale TREC collection show that our approach to improve retrieval effectiveness is successful, while most of the previous work failed to improve performances even on small text collection. Our method also shows promising results when is combined with pseudo relevance feedback and state-of-the-art retrieval function such as BM25.

#index 766440
#* An effective approach to document retrieval via utilizing WordNet and recognizing phrases
#@ Shuang Liu;Fang Liu;Clement Yu;Weiyi Meng
#t 2004
#c 13
#% 109190
#% 131434
#% 136350
#% 144029
#% 144031
#% 169729
#% 169768
#% 194301
#% 218978
#% 245788
#% 286069
#% 385946
#% 387427
#% 411760
#% 532186
#% 642994
#% 741839
#% 756821
#% 853813
#% 1279327
#! Noun phrases in queries are identified and classified into four types: proper names, dictionary phrases, simple phrases and complex phrases. A document has a phrase if all content words in the phrase are within a window of a certain size. The window sizes for different types of phrases are different and are determined using a decision tree. Phrases are more important than individual terms. Consequently, documents in response to a query are ranked with matching phrases given a higher priority. We utilize WordNet to disambiguate word senses of query terms. Whenever the sense of a query term is determined, its synonyms, hyponyms, words from its definition and its compound words are considered for possible additions to the query. Experimental results show that our approach yields between 23% and 31% improvements over the best-known results on the TREC 9, 10 and 12 collections for short (title only) queries, without using Web data.

#index 766441
#* Web-a-where: geotagging web content
#@ Einat Amitay;Nadav Har'El;Ron Sivan;Aya Soffer
#t 2004
#c 13
#% 330677
#% 480467
#% 815280
#% 815326
#% 815922
#% 854798
#% 854802
#% 854814
#% 854815
#% 854817
#% 854830
#% 855309
#% 855310
#% 855312
#% 855313
#! We describe Web-a-Where, a system for associating geography with Web pages. Web-a-Where locates mentions of places and determines the place each name refers to. In addition, it assigns to each page a geographic focus --- a locality that the page discusses as a whole. The tagging process is simple and fast, aimed to be applied to large collections of Web pages and to facilitate a variety of location-based applications and data analyses.Geotagging involves arbitrating two types of ambiguities: geo/non-geo and geo/geo. A geo/non-geo ambiguity occurs when a place name also has a non-geographic meaning, such as a person name (e.g., Berlin) or a common word (Turkey). Geo/geo ambiguity arises when distinct places have the same name, as in London, England vs. London, Ontario.An implementation of the tagger within the framework of the WebFountain data mining system is described, and evaluated on several corpora of real Web pages. Precision of up to 82% on individual geotags is achieved. We also evaluate the relative contribution of various heuristics the tagger employs, and evaluate the focus-finding algorithm using a corpus pretagged with localities, showing that as many as 91% of the foci reported are correct up to the country level.

#index 766442
#* Focused named entity recognition using machine learning
#@ Li Zhang;Yue Pan;Tong Zhang
#t 2004
#c 13
#% 136350
#% 144013
#% 194251
#% 287197
#% 288614
#% 340951
#% 420507
#% 425039
#% 722822
#% 740995
#% 742437
#% 751563
#% 855108
#% 1306053
#! In this paper we study the problem of finding most topical named entities among all entities in a document, which we refer to as focused named entity recognition. We show that these focused named entities are useful for many natural language processing applications, such as document summarization, search result ranking, and entity detection and tracking. We propose a statistical model for focused named entity recognition by converting it into a classification problem. We then study the impact of various linguistic features and compare a number of classification algorithms. From experiments on an annotated Chinese news corpus, we demonstrate that the proposed method can achieve near human-level accuracy.

#index 766443
#* Learning phonetic similarity for matching named entity translations and mining new translations
#@ Wai Lam;Ruizhang Huang;Pik-Shan Cheung
#t 2004
#c 13
#% 122671
#% 227736
#% 280826
#% 397146
#% 400061
#% 786574
#% 854812
#! We propose a novel named entity matching model which considers both semantic and phonetic clues. The matching is formulated as an optimization problem. One major component is a phonetic matching model which exploits similarity at the phoneme level. We investigate three learning algorithms for obtaining the similarity information of basic phoneme units based on training examples. By applying this proposed named entity matching model, we also develop a mining framework for discovering new, unseen named entity translations from online daily Web news. This framework harvests comparable news in different languages using an existing bilingual dictionary. It is able to discover new name translations not found in the dictionary.

#index 766444
#* Text classification and named entities for new event detection
#@ Giridhar Kumaran;James Allan
#t 2004
#c 13
#% 144034
#% 278107
#% 311034
#% 316546
#% 350859
#% 577297
#% 643016
#% 677440
#% 815107
#! New Event Detection is a challenging task that still offers scope for great improvement after years of effort. In this paper we show how performance on New Event Detection (NED) can be improved by the use of text classification techniques as well as by using named entities in a new way. We explore modifications to the document representation in a vector space-based NED system. We also show that addressing named entities preferentially is useful only in certain situations. A combination of all the above results in a multi-stage NED system that performs much better than baseline single-stage NED systems.

#index 766445
#* Assigning identifiers to documents to enhance the clustering property of fulltext indexes
#@ Fabrizio Silvestri;Salvatore Orlando;Raffaele Perego
#t 2004
#c 13
#% 118771
#% 290703
#% 397151
#% 420491
#% 451536
#% 570319
#% 648114
#% 656274
#% 737340
#% 768902
#% 786632
#! Web Search Engines provide a large-scale text document retrieval service by processing huge Inverted File indexes. Inverted File indexes allow fast query resolution and good memory utilization since their d-gaps representation can be effectively and efficiently compressed by using variable length encoding methods. This paper proposes and evaluates some algorithms aimed to find an assignment of the document identifiers which minimizes the average values of d-gaps, thus enhancing the effectiveness of traditional compression methods. We ran several tests over the Google contest collection in order to validate the techniques proposed. The experiments demonstrated the scalability and effectiveness of our algorithms. Using the proposed algorithms, we were able to sensibly improve (up to 20.81%) the compression ratios of several encoding schemes.

#index 766446
#* Filtering algorithms for information retrieval models with named attributes and proximity operators
#@ Christos Tryfonopoulos;Manolis Koubarakis;Yannis Drougas
#t 2004
#c 13
#% 158911
#% 267451
#% 297191
#% 340911
#% 345713
#% 465061
#% 508414
#% 543556
#% 659995
#% 723452
#% 731408
#% 765501
#! In the selective dissemination of information (or publish/subscribe) paradigm, clients subscribe to a server with continuous queries (or profiles) that express their information needs. Clients can also publish documents to servers. Whenever a document is published, the continuous queries satisfying this document are found and notifications are sent to appropriate clients. This paper deals with the filtering problem that needs to be solved effciently by each server: Given a database of continuous queries db and a document d, find all queries q ∈ db that match d. We present data structures and indexing algorithms that enable us to solve the filtering problem efficiently for large databases of queries expressed in the model AWP which is based on named attributes with values of type text, and word proximity operators.

#index 766447
#* Hourly analysis of a very large topically categorized web query log
#@ Steven M. Beitzel;Eric C. Jensen;Abdur Chowdhury;David Grossman;Ophir Frieder
#t 2004
#c 13
#% 194299
#% 296646
#% 306468
#% 308574
#% 323131
#% 323135
#% 340888
#% 342961
#% 349283
#% 401405
#% 424261
#% 438557
#% 577302
#% 590523
#% 590526
#% 643069
#% 655487
#% 722310
#% 730009
#% 730066
#! We review a query log of hundreds of millions of queries that constitute the total query traffic for an entire week of a general-purpose commercial web search service. Previously, query logs have been studied from a single, cumulative view. In contrast, our analysis shows changes in popularity and uniqueness of topically categorized queries across the hours of the day. We examine query traffic on an hourly basis by matching it against lists of queries that have been topically pre-categorized by human editors. This represents 13% of the query traffic. We show that query traffic from particular topical categories differs both from the query stream as a whole and from other categories. This analysis provides valuable insight for improving retrieval effectiveness and efficiency. It is also relevant to the development of enhanced query disambiguation, routing, and caching algorithms.

#index 766448
#* A collaborative filtering algorithm and evaluation metric that accurately model the user experience
#@ Matthew R. McLaughlin;Jonathan L. Herlocker
#t 2004
#c 13
#% 173879
#% 202011
#% 314933
#% 330687
#% 420539
#% 734590
#% 1650569
#! Collaborative Filtering (CF) systems have been researched for over a decade as a tool to deal with information overload. At the heart of these systems are the algorithms which generate the predictions and recommendations.In this article we empirically demonstrate that two of the most acclaimed CF recommendation algorithms have flaws that result in a dramatically unacceptable user experience.In response, we introduce a new Belief Distribution Algorithm that overcomes these flaws and provides substantially richer user modeling. The Belief Distribution Algorithm retains the qualities of nearest-neighbor algorithms which have performed well in the past, yet produces predictions of belief distributions across rating values rather than a point rating value.In addition, we illustrate how the exclusive use of the mean absolute error metric has concealed these flaws for so long, and we propose the use of a modified Precision metric for more accurately evaluating the user experience.

#index 766449
#* An automatic weighting scheme for collaborative filtering
#@ Rong Jin;Joyce Y. Chai;Luo Si
#t 2004
#c 13
#% 46803
#% 173879
#% 211044
#% 280852
#% 309192
#% 309204
#% 375017
#% 495929
#% 528156
#% 578684
#% 655897
#% 1650569
#! Collaborative filtering identifies information interest of a particular user based on the information provided by other similar users. The memory-based approaches for collaborative filtering (e.g., Pearson correlation coefficient approach) identify the similarity between two users by comparing their ratings on a set of items. In these approaches, different items are weighted either equally or by some predefined functions. The impact of rating discrepancies among different users has not been taken into consideration. For example, an item that is highly favored by most users should have a smaller impact on the user-similarity than an item for which different types of users tend to give different ratings. Even though simple weighting methods such as variance weighting try to address this problem, empirical studies have shown that they are ineffective in improving the performance of collaborative filtering. In this paper, we present an optimization algorithm to automatically compute the weights for different items based on their ratings from training users. More specifically, the new weighting scheme will create a clustered distribution for user vectors in the item space by bringing users of similar interests closer and separating users of different interests more distant. Empirical studies over two datasets have shown that our new weighting scheme substantially improves the performance of the Pearson correlation coefficient method for collaborative filtering.

#index 766450
#* Using bayesian priors to combine classifiers for adaptive filtering
#@ Yi Zhang
#t 2004
#c 13
#% 132583
#% 169774
#% 169777
#% 219048
#% 219049
#% 219050
#% 219051
#% 262085
#% 340901
#% 340904
#% 340940
#% 340941
#% 340948
#% 397149
#% 458379
#% 465895
#% 466572
#! An adaptive information filtering system monitors a document stream to identify the documents that match information needs specified by user profiles. As the system filters, it also refines its knowledge about the user's information needs based on long-term observations of the document stream and periodic feedback(training data) from the user. Low variance profile learning algorithms, such as Rocchio, work well at the early stage of filtering when the system has very few training data. Low bias profile learning algorithms, such as Logistic Regression, work well at the later stage of filtering when the system has accumulated enough training data.However, an empirical system needs to works well consistently at all stages of filtering process. This paper addresses this problem by proposing a new technique to combine different text classification algorithms via a constrained maximum likelihood Bayesian prior. This technique provides a trade off between bias and variance, and the combined classifier may achieve a consistent good performance at different stages of filtering. We implemented the proposed technique to combine two complementary classification algorithms: Rocchio and logistic regression. The new algorithm is shown to compare favorably with Rocchio, Logistic Regression, and the best methods in the TREC-9 and TREC-11 adaptive filtering tracks.

#index 766451
#* A nonparametric hierarchical bayesian framework for information filtering
#@ Kai Yu;Volker Tresp;Shipeng Yu
#t 2004
#c 13
#% 173879
#% 202011
#% 220709
#% 266281
#% 301259
#% 304425
#% 465928
#% 528156
#% 528182
#% 578684
#% 1499473
#% 1650569
#% 1673052
#! Information filtering has made considerable progress in recent years. The predominant approaches are content-based methods and collaborative methods. Researchers have largely concentrated on either of the two approaches since a principled unifying framework is still lacking. This paper suggests that both approaches can be combined under a hierarchical Bayesian framework. Individual content-based user profiles are generated and collaboration between various user models is achieved via a common learned prior distribution. However, it turns out that a parametric distribution (e.g. Gaussian) is too restrictive to describe such a common learned prior distribution. We thus introduce a nonparametric common prior, which is a sample generated from a Dirichlet process which assumes the role of a hyper prior. We describe effective means to learn this nonparametric distribution, and apply it to learn users' information needs. The resultant algorithm is simple and understandable, and offers a principled solution to combine content-based filtering and collaborative filtering. Within our framework, we are now able to interpret various existing techniques from a unifying point of view. Finally we demonstrate the empirical success of the proposed information filtering methods.

#index 766452
#* Automatic image annotation by using concept-sensitive salient objects for image content representation
#@ Jianping Fan;Yuli Gao;Hangzai Luo;Guangyou Xu
#t 2004
#c 13
#% 232707
#% 294852
#% 318785
#% 349208
#% 451622
#% 465916
#% 522573
#% 589738
#% 635755
#% 642989
#% 642990
#% 721163
#% 722927
#% 1855132
#% 1855544
#% 1858012
#! Multi-level annotation of images is a promising solution to enable more effective semantic image retrieval by using various keywords at different semantic levels. In this paper, we propose a multi-level approach to annotate the semantics of natural scenes by using both the dominant image components and the relevant semantic concepts. In contrast to the well-known image-based and region-based approaches, we use the salient objects as the dominant image components to achieve automatic image annotation at the content level. By using the salient objects for image content representation, a novel image classification technique is developed to achieve automatic image annotation at the concept level. To detect the salient objects automatically, a set of detection functions are learned from the labeled image regions by using Support Vector Machine (SVM) classifiers with an automatic scheme for searching the optimal model parameters. To generate the semantic concepts, finite mixture models are used to approximate the class distributions of the relevant salient objects. An adaptive EM algorithm has been proposed to determine the optimal model structure and model parameters simultaneously. We have also demonstrated that our algorithms are very effective to enable multi-level annotation of natural scenes in a large-scale dataset.

#index 766453
#* A search engine for historical manuscript images
#@ Toni M. Rath;R. Manmatha;Victor Lavrenko
#t 2004
#c 13
#% 239578
#% 262096
#% 397145
#% 457912
#% 642989
#% 642990
#% 738485
#! Many museum and library archives are digitizing their large collections of handwritten historical manuscripts to enable public access to them. These collections are only available in image formats and require expensive manual annotation work for access to them. Current handwriting recognizers have word error rates in excess of 50% and therefore cannot be used for such material. We describe two statistical models for retrieval in large collections of handwritten manuscripts given a text query. Both use a set of transcribed page images to learn a joint probability distribution between features computed from word images and their transcriptions. The models can then be used to retrieve unlabeled images of handwritten documents given a text query. We show experiments with a training set of 100 transcribed pages and a test set of 987 handwritten page images from the George Washington collection. Experiments show that the precision at 20 documents is about 0.4 to 0.5 depending on the model. To the best of our knowledge, this is the first automatic retrieval system for historical manuscripts using text queries, without manual transcription of the original corpus.

#index 766454
#* Display time as implicit feedback: understanding task effects
#@ Diane Kelly;Nicholas J. Belkin
#t 2004
#c 13
#% 169803
#% 187999
#% 267753
#% 292517
#% 303510
#% 320432
#% 345765
#% 349274
#% 397130
#% 731615
#! Recent research has had some success using the length of time a user displays a document in their web browser as implicit feedback for document preference. However, most studies have been confined to specific search domains, such as news, and have not considered the effects of task on display time, and the potential impact of this relationship on the effectiveness of display time as implicit feedback. We describe the results of an intensive naturalistic study of the online information-seeking behaviors of seven subjects during a fourteen-week period. Throughout the study, subjects' online information-seeking activities were monitored with various pieces of logging and evaluation software. Subjects were asked to identify the tasks with which they were working, classify the documents that they viewed according to these tasks, and evaluate the usefulness of the documents. Results of a user-centered analysis demonstrate no general, direct relationship between display time and usefulness, and that display times differ significantly according to specific task, and according to specific user.

#index 766455
#* Human versus machine in the topic distillation task
#@ Mingfang Wu;Gheorghe Muresan;Alistair McLean;Muh-Chyun (Morris) Tang;Ross Wilkinson;Yuelin Li;Hyuk-Jin Lee;Nichloas J. Belkin
#t 2004
#c 13
#% 137473
#% 142618
#% 218992
#% 218994
#% 219024
#% 232698
#% 262061
#% 268073
#% 282905
#% 309089
#% 325001
#% 329092
#% 340919
#% 340957
#! This paper reports on and discusses a set of user experiments using the TREC 2003 Web interactive track protocol. The focus is on comparing humans and machine algorithms in terms of performance in a topic distillation task. We also investigated the effect of the search results layout in supporting the users' effort.We have demonstrated that machines can perform nearly as well as people on the topic distillation task. Given a system tailored to the task there is significant performance improvement and finally, given a presentation that supports the task, there is strong user satisfaction.

#index 766456
#* Learning effective ranking functions for newsgroup search
#@ Wensi Xi;Jesper Lind;Eric Brill
#t 2004
#c 13
#% 118759
#% 144011
#% 144012
#% 169809
#% 169811
#% 169813
#% 184491
#% 194276
#% 218978
#% 232677
#% 232703
#% 260000
#% 273925
#% 330765
#% 340919
#% 343142
#% 536403
#% 577301
#% 642992
#% 773034
#! Web communities are web virtual broadcasting spaces where people can freely discuss anything. While such communities function as discussion boards, they have even greater value as large repositories of archived information. In order to unlock the value of this resource, we need an effective means for searching archived discussion threads. Unfortunately the techniques that have proven successful for searching document collections and the Web are not ideally suited to the task of searching archived community discussions. In this paper, we explore the problem of creating an effective ranking function to predict the most relevant messages to queries in community search. We extract a set of predictive features from the thread trees of newsgroup messages as well as features of message authors and lexical distribution within a message thread. Our final results indicate that when using linear regression with this feature set, our search system achieved a 28.5% performance improvement compared to our baseline system.

#index 766457
#* Language-specific models in multilingual topic tracking
#@ Leah S. Larkey;Fangfang Feng;Margaret Connell;Victor Lavrenko
#t 2004
#c 13
#% 144034
#% 340901
#% 397158
#% 575570
#% 575571
#% 575573
#% 575578
#% 575581
#% 704026
#% 807746
#% 815159
#% 995518
#! Topic tracking is complicated when the stories in the stream occur in multiple languages. Typically, researchers have trained only English topic models because the training stories have been provided in English. In tracking, non-English test stories are then machine translated into English to compare them with the topic models. We propose a native language hypothesis stating that comparisons would be more effective in the original language of the story. We first test and support the hypothesis for story link detection. For topic tracking the hypothesis implies that it should be preferable to build separate language-specific topic models for each language in the stream. We compare different methods of incrementally building such native language topic models.

#index 766458
#* Web taxonomy integration through co-bootstrapping
#@ Dell Zhang;Wee Sun Lee
#t 2004
#c 13
#% 235377
#% 252011
#% 280817
#% 302391
#% 309208
#% 311034
#% 330767
#% 348187
#% 376266
#% 387427
#% 515992
#% 529190
#% 531444
#% 642997
#% 643964
#% 646003
#% 1289178
#! We address the problem of integrating objects from a source taxonomy into a master taxonomy. This problem is not only currently pervasive on the web, but also important to the emerging semantic web. A straightforward approach to automating this process would be to learn a classifier that can classify objects from the source taxonomy into categories of the master taxonomy. The key insight is that the availability of the source taxonomy data could be helpful to build better classifiers for the master taxonomy if their categorizations have some semantic overlap. In this paper, we propose a new approach, co-bootstrapping, to enhance the classification by exploiting such implicit knowledge. Our experiments with real-world web data show substantial improvements in the performance of taxonomy integration.

#index 766459
#* Evaluation of an extraction-based approach to answering definitional questions
#@ Jinxi Xu;Ralph Weischedel;Ana Licuanan
#t 2004
#c 13
#% 144074
#% 280850
#% 815101
#% 815850
#% 815902
#% 816173
#% 853647
#! This paper evaluates an extraction-based approach to answering definitional questions. Our system extracted useful linguistic constructs called linguistic features from raw text using information extraction tools and formulated answers based on such features. The features employed include appositives, copulas, structured patterns, relations, propositions and raw sentences. The features were ranked based on feature type and similarity to a question profile. Redundant features were detected using a simple heuristic-based strategy. The approach achieved state of the art performance at the TREC 2003 QA evaluation. Component analysis of the system was carried out using an automatic scoring function called Rouge (Lin and Hovy, 2003). Major findings include 1) answers using linguistic features are significantly better than those using raw sentences; 2) the most useful features are appositives and copulas; 3) question profiles, as a means of modeling user interests, can significantly improve system performance; 4) the Rouge scores are closely correlated with subjective evaluation results, indicating the suitability of using Rouge for evaluating definitional QA systems.

#index 766460
#* Query based event extraction along a timeline
#@ Hai Leong Chieu;Yoong Keok Lee
#t 2004
#c 13
#% 194251
#% 283171
#% 287196
#% 309096
#% 318409
#% 340883
#% 397132
#% 529158
#% 577220
#% 731864
#% 740900
#% 741106
#% 786578
#% 815850
#% 817550
#% 1272344
#! In this paper, we present a framework and a system that extracts events relevant to a query from a collection C of documents, and places such events along a timeline. Each event is represented by a sentence extracted from C, based on the assumption that "important" events are widely cited in many documents for a period of time within which these events are of interest. In our experiments, we used queries that are event types ("earthquake") and person names (e.g. "George Bush"). Evaluation was performed using G8 leader names as queries: comparison made by human evaluators between manually and system generated timelines showed that although manually generated timelines are on average more preferable, system generated timelines are sometimes judged to be better than manually constructed ones.

#index 766461
#* Sentence completion
#@ Korinna Grabski;Tobias Scheffer
#t 2004
#c 13
#% 44933
#% 120649
#% 188587
#% 214556
#% 280413
#% 387427
#% 403341
#% 480482
#% 529804
#% 589459
#% 1271949
#! We discuss a retrieval model in which the task is to complete a sentence, given an initial fragment, and given an application specific document collection. This model is motivated by administrative and call center environments, in which users have to write documents with a certain repetitiveness. We formulate the problem setting and discuss appropriate performance metrics. We present an index-based retrieval algorithm and a cluster-based approach, and evaluate our algorithms using collections of emails that have been written by two distinct service centers.

#index 766462
#* Block-level link analysis
#@ Deng Cai;Xiaofei He;Ji-Rong Wen;Wei-Ying Ma
#t 2004
#c 13
#% 190581
#% 249110
#% 262061
#% 268073
#% 268079
#% 271060
#% 290830
#% 309151
#% 309779
#% 330676
#% 340919
#% 341009
#% 348173
#% 413617
#% 438553
#% 577301
#% 754078
#% 1394469
#! Link Analysis has shown great potential in improving the performance of web search. PageRank and HITS are two of the most popular algorithms. Most of the existing link analysis algorithms treat a web page as a single node in the web graph. However, in most cases, a web page contains multiple semantics and hence the web page might not be considered as the atomic node. In this paper, the web page is partitioned into blocks using the vision-based page segmentation algorithm. By extracting the page-to-block, block-to-page relationships from link structure and page layout analysis, we can construct a semantic graph over the WWW such that each node exactly represents a single semantic topic. This graph can better describe the semantic structure of the web. Based on block-level link analysis, we proposed two new algorithms, Block Level PageRank and Block Level HITS, whose performances we study extensively using web data.

#index 766463
#* Usefulness of hyperlink structure for query-biased topic distillation
#@ Vassilis Plachouras;Iadh Ounis
#t 2004
#c 13
#% 169781
#% 262061
#% 268079
#% 290830
#% 340928
#% 397126
#% 397161
#% 411760
#% 642982
#% 730008
#% 1387535
#% 1387536
#% 1387553
#! In this paper, we introduce an information theoretic method for estimating the usefulness of the hyperlink structure induced from the set of retrieved documents. We evaluate the effectiveness of this method in the context of an optimal Bayesian decision mechanism, which selects the most appropriate retrieval approaches on a per-query basis for two TREC tasks. The estimation of the hyperlink structure's usefulness is stable when we use different weighting schemes, or when we employ sampling of documents to reduce the computational overhead. Next, we evaluate the effectiveness of the hyperlink structure's usefulness in a realistic setting, by setting the thresholds of a decision mechanism automatically. Our results show that improvements over the baselines are obtained.

#index 766464
#* Block-based web search
#@ Deng Cai;Shipeng Yu;Ji-Rong Wen;Wei-Ying Ma
#t 2004
#c 13
#% 144011
#% 169809
#% 169811
#% 184491
#% 211514
#% 232677
#% 273925
#% 328532
#% 340919
#% 504890
#% 577281
#% 577301
#% 658747
#% 729027
#% 748583
#% 1394469
#! Multiple-topic and varying-length of web pages are two negative factors significantly affecting the performance of web search. In this paper, we explore the use of page segmentation algorithms to partition web pages into blocks and investigate how to take advantage of block-level evidence to improve retrieval performance in the web context. Because of the special characteristics of web pages, different page segmentation method will have different impact on web search performance. We compare four types of methods, including fixed-length page segmentation, DOM-based page segmentation, vision-based page segmentation, and a combined method which integrates both semantic and fixed-length properties. Experiments on block-level query expansion and retrieval are performed. Among the four approaches, the combined method achieves the best performance for web search. Our experimental results also show that such a semantic partitioning of web pages effectively deals with the problem of multiple drifting topics and mixed lengths, and thus has great potential to boost up the performance of current web search engines.

#index 766465
#* A hybrid statistical/linguistic model for generating news story gists
#@ William P. Doran;Nicola Stokes;Eamonn Newman;John Dunnion;Joe Carthy
#t 2004
#c 13
#% 816173
#! In this paper, we describe a News Story Gisting system that generates a 10-word short summary of a news story. This system uses a machine learning technique to combine linguistic, statistical and positional information in order to generate an appropriate summary. We also present the results of an automatic evaluation of this system with respect to the performance of other baseline summarisers using the new ROUGE evaluation metric.

#index 766466
#* Image based gisting in CLIR
#@ Mark Sanderson;Robert Pasley
#t 2004
#c 13
#% 855126
#! In this paper, we describe research which could lead to a novel approach to gathering an overview of a document in a foreign language. The research explores how much of the meaning of a document could be represented using images by researching the ability of subjects to derive the search term that might have been used to return a set of images from an image library. The Google image search engine was used to retrieve the images for this experiment, which uses English throughout. The results were analysed with respect to a previous paper [1] exploring ability to recognise concrete objects in hierarchies. It was found that there is a tendency to use one particular level of categorization.

#index 766467
#* Classifying racist texts using a support vector machine
#@ Edel Greevy;Alan F. Smeaton
#t 2004
#c 13
#% 457935
#! In this poster we present an overview of the techniques we used to develop and evaluate a text categorisation system to automatically classify racist texts. Detecting racism is difficult because the presence of indicator words is insufficient to indicate racist texts, unlike some other text classification tasks. Support Vector Machines (SVM) are used to automatically categorise web pages based on whether or not they are racist. Different interpretations of what constitutes a term are taken, and in this poster we look at three representations of a web page within an SVM -- bag-of-words, bigrams and part-of-speech tags.

#index 766468
#* Discovery of aggregate usage profiles based on clustering information needs
#@ Azreen Azman;Iadh Ounis
#t 2004
#c 13
#% 312874
#% 420134
#! We present an alternative technique for discovering aggregate usage profiles from Web access logs. The technique is based on clustering information needs inferred from users' browsing paths. Browsing paths are extracted from users' access logs. Information need is inferred from each browsing path by using the Ostensive Model[1]. The technique is evaluated in a document recommendation application. We compare the performance of our technique against the well-established transaction-based technique proposed in [2]. Based on an initial evaluation, the results are encouraging.

#index 766469
#* Merging retrieval results in hierarchical peer-to-peer networks
#@ Jie Lu;Jamie Callan
#t 2004
#c 13
#% 722312
#% 730035

#index 766470
#* The effect of back-formulating questions in question answering evaluation
#@ Tetsuya Sakai;Yoshimi Saito;Yumi Ichimura;Tomoharu Kokubu;Makoto Koyama
#t 2004
#c 13

#index 766471
#* Effect of varying number of documents in blind feedback: analysis of the 2003 NRRC RIA workshop "bf_numdocs" experiment suite
#@ Jesse Montgomery;Luo Si;Jamie Callan;David A. Evans
#t 2004
#c 13
#% 766497

#index 766472
#* Eye-tracking analysis of user behavior in WWW search
#@ Laura A. Granka;Thorsten Joachims;Geri Gay
#t 2004
#c 13
#% 577224
#% 590523
#! We investigate how users interact with the results page of a WWW search engine using eye-tracking. The goal is to gain insight into how users browse the presented abstracts and how they select links for further exploration. Such understanding is valuable for improved interface design, as well as for more accurate interpretations of implicit feedback (e.g. clickthrough) for machine learning. The following presents initial results, focusing on the amount of time spent viewing the presented abstracts, the total number of abstract viewed, as well as measures of how thoroughly searchers evaluate their results set.

#index 766473
#* Subwebs for specialized search
#@ Raman Chandrasekar;Harr Chen;Simon Corston-Oliver;Eric Brill
#t 2004
#c 13
#% 249110
#% 262061
#% 290830
#% 466891
#% 615723
#! We describe a method to define and use subwebs, user-defined neighborhoods of the Internet. Subwebs help improve search performance by inducing a topic-specific page relevance bias over a collection of documents. Subwebs may be automatically identified using a simple algorithm we describe, and used to provide highly-relevant topic-specific information retrieval. Using subwebs in a Help and Support topic, we see marked improvements in precision compared to generic search engine results.

#index 766474
#* Comparison of using passages and documents for blind relevance feedback in information retrieval
#@ Zhenmei Gu;Ming Luo
#t 2004
#c 13
#% 262084
#% 329114
#% 342707
#! This paper compares document blind feedback and passage blind feedback in Information Retrieval (IR), based on the work during the NRRC 2003 Reliable Information Access Summer workshop. The analysis of our experimental results shows overall consistency on the performance impact of using passages and documents for blind feedback. However, it is observed that the behavior of passage blind feedback, compared to document blind feedback, is both system dependent and topic dependent. The relationships between the performance impact of passage blind feedback and the number of feedback terms and the topic's average relevant document length, respectively, are examined to illustrate these dependencies.

#index 766475
#* Measuring pseudo relevance feedback & CLIR
#@ Paul Clough;Mark Sanderson
#t 2004
#c 13
#% 262047
#% 262084
#% 334924
#% 397143
#! In this poster, we report on the effects of pseudo relevance feedback (PRF) for a cross language image retrieval task using a test collection. Typically PRF has been shown to improve retrieval performance in previous CLIR experiments based on average precision at a fixed rank. However our experiments have shown that queries in which no relevant documents are returned also increases. Because query reformulation for cross language is likely to be harder than with monolingual searching, a great deal of user dissatisfaction would be associated with this scenario. We propose that an additional effectiveness measure based on failed queries may better reflect user satisfaction than average precision alone.

#index 766476
#* A two-stage mixture model for pseudo feedback
#@ Tao Tao;ChengXiang Zhai
#t 2004
#c 13
#% 310512
#% 340899
#! Pseudo feedback is a commonly used technique to improve information retrieval performance. It assumes a few top-ranked documents to be relevant, and learns from them to improve the retrieval accuracy. A serious problem is that the performance is often very sensitive to the number of pseudo feedback documents. In this poster, we address this problem in a language modeling framework. We propose a novel two-stage mixture model, which is less sensitive to the number of pseudo feedback documents than an effective existing feedback model. The new model can tolerate a more flexible setting of the number of pseudo feedback documents without the danger of losing much retrieval accuracy.

#index 766477
#* Natural language processing for browse help
#@ Eric Crestan;Claude de Loupy
#t 2004
#c 13
#% 184486
#% 262107
#% 411762
#! In this paper, we will present three "browsing" systems that should save user's time. The first uses named entities and gives a way to reduce search space. By using a information visualization system, the user can comprehend more easily the content of a corpus or a document. Named entities are highlighted for quick reading, temporal and geographic representation gives a global view of the result of a query. All these browse and search helps seem to be very useful. Nevertheless, an evaluation would give more practical results.

#index 766478
#* Triangulation without translation
#@ James Mayfield;Paul McNamee
#t 2004
#c 13
#% 340894
#% 730026
#% 732844
#% 787505
#! Transitive retrieval and triangulation have been proposed as ways to improve cross-language retrieval quality when translation resources have poor lexical coverage. We demonstrate that cross-language retrieval is viable for European languages with no translation resources at all; that transitive retrieval without translation does not suffer the drop-off in retrieval quality sometimes reported for transitive retrieval with translation; and that triangulation that combines multiple transitive runs with no translation can boost performance over direct translation-free retrieval.

#index 766479
#* A session-based search engine
#@ Smitha Sriram;Xuehua Shen;Chengxiang Zhai
#t 2004
#c 13
#% 591792
#% 642983
#% 643028
#! In this poster, we describe a novel session-based search engine, which puts the search in context. The search engine has a number of session-based features including expansion of the current query with user query history and clickthrough data (title and summary of clicked web pages) in the same search session and the session boundary recognition through temporal closeness and probabilistic similarity between query terms. In addition, the search engine visualizes the rank change of web pages as different queries are submitted in the same search session to help the user reformulate the query.

#index 766480
#* Evaluation of filtering current news search results
#@ Steven M. Beitzel;Eric C. Jensen;Abdur Chowdhury;David Grossman;Ophir Frieder
#t 2004
#c 13
#% 270282
#% 397194
#% 577300
#% 730078
#! We describe an evaluation of result set filtering techniques for providing ultra-high precision in the task of presenting related news for general web queries. In this task, the negative user experience generated by retrieving non-relevant documents has a much worse impact than not retrieving relevant ones. We adapt cost-based metrics from the document filtering domain to this result filtering problem in order to explicitly examine the tradeoff between missing relevant documents and retrieving non-relevant ones. A large manual evaluation of three simple threshold filters shows that the basic approach of counting matching title terms outperforms also incorporating selected abstract terms based on part-of-speech or higher-level linguistic structures. Simultaneously, leveraging these cost-based metrics allows us to explicitly determine what other tasks would benefit from these alternative techniques.

#index 766481
#* The document as an ergodic markov chain
#@ Eduard Hoenkamp;Dawei Song
#t 2004
#c 13
#% 340899
#% 572499
#! In recent years, statistical language models are being proposed as alternative to the vector space model. Viewing documents as language samples introduces the issue of defining a joint probability distribution over the terms.The present paper models a document as the result of a Markov process. It argues that this process is ergodic, which is theoretically plausible, and easy to verify in practice.The theoretical result is that the joint distribution can be easily obtained. This can also be applied for search resolutions other than the document level. We verified this in an experiment on query expansion demonstrating both the validity and the practicability of the method. This holds a promise for general language models.

#index 766482
#* Expertise community detection
#@ Raymond D'Amore
#t 2004
#c 13
#% 290830
#% 413613
#! Providing knowledge workers with access to experts and communities-of-practice is central to sharing expertise and crucial to organizational performance, adaptation, and even survival. This paper covers ongoing research to develop an Expert Locator prototype, a model-based system for detecting experts and broader communities-of-practice. The underlying expertise model is extensible and supports aggregation of evidence across diverse sources. The prototype is being used to locate critical expertise in key project areas, and current evaluation indicates its potential effectiveness.

#index 766483
#* Learning patterns to answer open domain questions on the web
#@ Dmitri Roussinov;Jose Robles
#t 2004
#c 13
#% 196896
#% 330619
#% 348163
#% 397160
#% 401086
#! While being successful in providing keyword based access to web pages, commercial search portals still lack the ability to answer questions expressed in a natural language. We present a probabilistic approach to automated question answering on the Web, based on trainable patterns, answer triangulation and semantic filtering. In contrast to the other "shallow" approaches, our approach is entirely self-learning. It does not require any manually created scoring and filtering rules while still performing comparably. It also performs better than other fully trainable approaches.

#index 766484
#* Email is a stage: discovering people roles from email archives
#@ Anton Leuski
#t 2004
#c 13
#% 380007
#% 466263

#index 766485
#* Searching databases for sematically-related schemas
#@ Gauri Shah;Tanveer Syeda-Mahmood
#t 2004
#c 13
#% 307632
#% 331769
#% 333990
#% 348187
#% 480645
#% 572314
#% 654459
#% 660001
#% 993982
#! In this paper, we address the problem of searching schema databases for semantically-related schemas. We first give a method of finding semantic similarity between pair-wise schemas based on tokenization, part-of-speech tagging, word expansion, and ontology matching. We then address the problem of indexing the schema database through a semantic hash table. Matching schemas in the database are found by hashing the query attributes and recording peaks in the histogram of schema hits. Results indicated a 90% improvement in search performance while maintaining high precision and recall.

#index 766486
#* Topic prediction based on comparative retrieval rankings
#@ Chris Buckley
#t 2004
#c 13
#% 397161
#! A new measure, AnchorMap, is introduced to evaluate how close two document retrieval rankings are to each other. It is shown that AnchorMap scores, when run on a set of initial ranked document lists from 8 different systems, are very highly correlated with categorization of topics as easy or hard, and separately, are highly correlated with those topics on which blind feedback works. In another experiment, AnchorMap is used to compare the initial ranked document list from a single system against the ranked document list from that system after blind feedback. Again, high AnchorMap values are highly correlated with both topic difficulty and successful application of blind feedback. Both experiments are examples of using properties of a topic which are independent of relevance information to predict the actual performance of IR systems on the topic. Initial experiments to attempt to improve retrieval performance based upon AnchorMap failed; the causes for failure are discussed.

#index 766487
#* Context-based question-answering evaluation
#@ Elizabeth D. Liddy;Anne R. Diekema;Ozgur Yilmazel
#t 2004
#c 13
#! In this poster, we will present the results of efforts we have undertaken to conduct evaluations of a QA system in a real world environment and to understand the nature of the dimensions on which users evaluate QA systems when given full reign to comment on whatever dimensions they deem important.

#index 766488
#* Design of an e-book user interface and visualizations to support reading for comprehension
#@ Yixing Sun;David J. Harper;Stuart N. K. Watt
#t 2004
#c 13
#% 231590
#% 735074
#! Current e-Book browsers provide minimal support for comprehending the organization, narrative structure, and themes, of large complex books. In order to build an understanding of such books, readers should be provided with user interfaces that present, and relate, the organizational, narrative and thematic structures. We propose adapting information retrieval techniques for the purpose of discovering these structures, and sketch three distinctive visualizations for presenting these structures to the e-Book reader. These visualizations are presented within an initial design for an e-Book browser.

#index 766489
#* Toward better weighting of anchors
#@ David Hawking;Trystan Upstill;Nick Craswell
#t 2004
#c 13
#% 268079
#% 340928
#% 577339
#! Okapi BM25 scoring of anchor text surrogate documents has been shown to facilitate effective ranking in navigational search tasks over web data. We hypothesize that even better ranking can be achieved in certain important cases, particularly when anchor scores must be fused with content scores, by avoiding length normalisation and by reducing the attentuation of scores associated with high tf. Preliminary results are presented.

#index 766490
#* Aggregated feature retrieval for MPEG-7 via clustering
#@ Jiamin Ye;Alan F. Smeaton
#t 2004
#c 13
#% 420506
#% 1387575
#! In this paper, we describe an approach to combining text and visual features from MPEG-7 descriptions of video. A video retrieval process is aligned to a text retrieval process based on the TF*IDF vector space model via clustering of low-level visual features. Our assumption is that shots within the same cluster are not only similar visually but also semantically, to a certain extent. Our experiments on the TRECVID2002 and TRECVID2003 collections show that adding extra meaning to a shot based on the shots from the same cluster is useful when each video in a collection contains a high proportion of similar shots, for example in documentaries.

#index 766491
#* Answer models for question answering passage retrieval
#@ Andrés Corrada-Emmanuel;W. Bruce Croft
#t 2004
#c 13
#% 262096
#% 815868
#! Answer patterns have been shown to improve the perfor-mance of open-domain factoid QA systems. Their use, however, requires either constructing the patterns manually or developing algorithms for learning them automatically. We present here a simpler approach that extends the techniques of language modeling to create answer models. These are language models trained on the correct answers to training questions. We show how they fit naturally into a probabilis-tic model for answer passage retrieval and demonstrate their effectiveness on the TREC 2002 QA Corpus.

#index 766492
#* Collaborative filing in a document repository
#@ Harris Wu;Michael D. Gordon
#t 2004
#c 13
#% 46809
#% 237074
#% 375017
#! We introduce an emergent, collaborative filing system. In such a system, an individual is allowed to organize a subset of documents in a repository into a personal hierarchy and share the hierarchy with others. The system generates a "consensus" hierarchy from all users' personal hierarchies, which provides a full, common, and emergent view of all documents. We believe that collaborative filing helps translate personal, tacit knowledge into sharable structures, which help the user as well a community of which he or she is a part. Our filing system is suitable for any documents from text to multimedia files. Initial results on an experimental website show promise. For a knowledge task involving extensive document retrieval, hierarchies are not only used frequently but are also effective in identifying high quality documents. One surprising finding is how often subjects use others' personal hierarchies, and upon close examination, social networks play a key role as well.

#index 766493
#* A study of topic similarity measures
#@ Ryen W. White;Joemon M. Jose
#t 2004
#c 13
#% 159108
#% 227800
#% 786511
#! In this poster we describe an investigation of topic similarity measures. We elicit assessments on the similarity of 10 pairs of topic from 76 subjects and use these as a benchmark to assess how well each measure performs. The measures have the potential to form the basis of a predictive technique, for adaptive search systems. The results of our evaluation show that measures based on the level of correlation between topics concords most with general subject perceptions of search topic similarity.

#index 766494
#* Effectiveness of web page classification on finding list answers
#@ Hui Yang;Tat-Seng Chua
#t 2004
#c 13
#% 136350
#! List question answering (QA) offers a unique challenge in effectively and efficiently locating a complete set of distinct answers from huge corpora or the Web. In TREC-12, the median average F1 performance of list QA systems was only 6.9%. This paper exploits the wealth of freely available text and link structures on the Web to seek complete answers to list questions. We employ natural language parsing, web page classification and clustering to find reliable list answers. We also study the effectiveness of web page classification on both the recall and uniqueness of answers for web-based list QA.

#index 766495
#* Detection and translation of OOV terms prior to query time
#@ Ying Zhang;Phil Vines
#t 2004
#c 13
#% 400061
#% 458405
#% 766427
#! Accurate cross-language information retrieval requires that query terms be correctly translated. Several new techniques to improve the translation of out of vocabulary terms in English-Chinese cross-language information retrieval have been developed. However, these require queries and a document collection to enable translation disambiguation. Although effective, they involve much processing and searching of the Web at query time, and may not be practical in a production web search engine. In this work, we consider what tasks maybe carried out beforehand, the goal being to reduce the processing required at query time. We have successfully developed new techniques to extract and translate out of vocabulary terms using the Web and add them into a translation dictionary prior to query time.

#index 766496
#* Evaluation of the real and perceived value of automatic and interactive query expansion
#@ Yael Nemeth;Bracha Shapira;Meirav Taeib-Maimon
#t 2004
#c 13
#% 214709
#% 232719
#% 298183
#% 643001
#! The paper describes a user study examining methods for improving users queries, specifically interactive and automatic query expansion and advanced search options. The user study includes subjective and objective evaluation of the effect of the above methods and a comparison between the real and perceived effect.

#index 766497
#* The NRRC reliable information access (RIA) workshop
#@ Donna Harman;Chris Buckley
#t 2004
#c 13
#% 397161

#index 766498
#* On evaluating web search with very few relevant documents
#@ Ian Soboroff
#t 2004
#c 13
#% 397163
#! Many common web searches by their nature have a very small number of relevant documents. Homepage and "namedpage" searching are known-item searches where there is only a single relevant document. Topic distillation is a special kind of topical relevance search where the user wishes to find a few key web sites rather than every relevant web page. Because these types of searches are so common, web search evaluations have come to focus on tasks where there are very few relevant documents. Evaluations with few relevant documents pose special challenges for current metrics. In particular, the TREC 2003 topic distillation evaluation is unable to distinguish most submitted runs from each other.

#index 766499
#* A music recommender based on audio features
#@ Qing Li;Byeong Man Kim;Dong Hai Guan;Duk whan Oh
#t 2004
#c 13
#% 330687
#% 724556
#% 1650569
#! Many collaborative music recommender systems (CMRS) have succeeded in capturing the similarity among users or items based on ratings, however they have rarely considered about the available information from the multimedia such as genres, let alone audio features from the media stream. Such information is valuable and can be used to solve several problems in RS. In this paper, we design a CMRS based on audio features of the multimedia stream. In the CMRS, we provide recommendation service by our proposed method where a clustering technique is used to integrate the audio features of music into the collaborative filtering (CF) framework in hopes of achieving better performance. Experiments are carried out to demonstrate that our approach is feasible.

#index 766500
#* Information extraction using two-phase pattern discovery
#@ Liping Ma;John Shepherd
#t 2004
#c 13
#% 210985
#% 316709
#% 480648
#% 482623
#% 654469
#% 732670
#% 795638
#! This paper presents a new two-phase pattern (2PP) discovery technique for information extraction. 2PP consists of orthographic pattern discovery (OPD) and semantic pattern discovery (SPD) where the OPD determines the structural features from an identified region of a document and the SPD discovers a dominant semantic pattern for the region via inference, apposition and analogy. Then the discovered pattern is applied back into the region to extract required data items through pattern matching. We evaluated 2PP using 6500 data items and obtained effective result.

#index 766501
#* A search engine for imaged documents in PDF files
#@ Yue Lu;Li Zhang;Chew Lim Tan
#t 2004
#c 13
#% 263214
#% 493341
#% 738467
#! Large quantities of documents in the Internet and digital libraries are simply scanned and archived in image format, many of which are packed in PDF files. The word search tool provided by Adobe Reader/Acrobat does not work for these imaged documents. In this paper, we present a search engine to deal with this issue for imaged documents in PDF files. The experimental results show an encouraging performance.

#index 766502
#* Context sensitive vocabulary and its application in protein secondary structure prediction
#@ Yan Liu;Jaime Carbonell;Judith Klein-Seetharaman;Vanathi Gopalakrishnan
#t 2004
#c 13
#% 280817
#% 995511
#! Protein secondary structure prediction is an important step towards understanding the relation between protein sequence and structure. However, most current prediction methods use features difficult for biologists to interpret. In this paper, we present a new method that applies information retrieval techniques to solve the problem:we extract a context sensitive biological vocabulary for protein sequences and apply text classification methods to predict protein secondary structure. Experimental results show that our method performs comparably to the state-of-art methods. Furthermore, the context sensitive vocabularies can serve as a useful tool to discover meaningful regular expression patterns for protein structures.

#index 766503
#* Formal multiple-bernoulli models for language modeling
#@ Donald Metzler;Victor Lavrenko;W. Bruce Croft
#t 2004
#c 13
#% 262096
#% 287253
#% 340948
#% 642974

#index 766504
#* User biased document language modelling
#@ L. Azzopardi;M. Girolami;C. J. van Rijsbergen
#t 2004
#c 13
#% 262096
#% 280819
#% 324129
#% 340948
#% 397129
#! Capitalizing on the intuitive underlying assumptions of Language Modelling for Ad-Hoc Retrieval we present a novel approach that is capable of injecting the user's context of the document collection into the retrieval process. The preliminary findings from the evaluation undertaken suggest that improved IR performance is possible under certain circumstances. This motivates further investigation to determine the extent and significance of this improved performance.

#index 766505
#* Information retrieval for language tutoring: an overview of the REAP project
#@ Kevyn Collins-Thompson;Jamie Callan
#t 2004
#c 13
#% 281251
#% 340901
#% 590523
#% 742424

#index 766506
#* A unified model of literal mining and link analysis for ranking web resources
#@ Yinghui Xu;Kyoji Umemura
#t 2004
#c 13
#% 268079
#% 348178
#% 719598
#! Web link analysis has been proved to provide significant enhancement to the precision of Web search in practice. The PageRank algorithm, which is used in Google Search Engine, plays an important role on improving the quality of its resuls by employing the explicit hyperlink structure among the Web pages. The prestige of Web pages defined by PageRank is purely derived from surfer random walk on the Web graph without textual content content consideration. However, in the practical sense, user surfing behavior is far from random jumping. In this paper, we present a unified model for a more accurate page rank. User's surfing is guided by a probabilistic model that is based on literal matching between connected pages. The result shows that our proposed ranking algorithms do perform better than the original PageRank.

#index 766507
#* Automatic recognition of reading levels from user queries
#@ Xiaoyong Liu;W. Bruce Croft;Paul Oh;David Hart
#t 2004
#c 13
#% 342740

#index 766508
#* A joint framework for collaborative and content filtering
#@ Justin Basilico;Thomas Hofmann
#t 2004
#c 13
#% 173879
#% 266281
#% 1650569
#! This paper proposes a novel, unified, and systematic approach to combine collaborative and content-based filtering for ranking and user preference prediction. The framework incorporates all available information by coupling together multiple learning problems and using a suitable kernel or similarity function between user-item pairs. We propose and evaluate an on-line algorithm (JRank)that generalizes perceptron learning using this framework and shows significant improvement over other approaches.

#index 766509
#* Refining term weights of documents using term dependencies
#@ Hee-soo Kim;Ikkyu Choi;Minkoo Kim
#t 2004
#c 13
#% 152934
#% 262090
#% 262096
#% 387427
#% 642999
#! When processing raw documents in Information Retrieval (IR) System, a term-weighting scheme is used to calculate the importance of each term which occurs in a document. However, most term-weighting schemes assume that a term is independent of the other terms. Term dependency is an indispensable consequence of language use [1]. Therefore, this assumption can make the information of a document being lost. In this paper, we propose new approach to refine term weights of documents using term dependencies discovered from a set of documents. Then, we evaluate our method with two experiments based on the vector space model [2] and the language model [3].

#index 766510
#* Multiple sources of evidence for XML retrieval
#@ Börkur Sigurbjörnsson;Jaap Kamps;Maarten de Rijke
#t 2004
#c 13
#% 169811
#! Document-centric XML collections contain text-rich documents, marked up with XML tags. The tags add lightweight semantics to the text. Querying such collections calls for a hybrid query language: the text-rich nature of the documents suggest a content-oriented (IR) approach, while the mark-up allows users to add structural constraints to their IR queries. We will show how evidence for relevancy from different sources helps to answer such hybrid queries. We evaluate our methods using the INEX 2003 test set, and show that structural hints in hybrid queries help to improve retrieval effectiveness.

#index 766511
#* Verifying a Chinese collection for text categorization
#@ Yuen-Hsien Tseng;William John Teahan
#t 2004
#c 13
#% 642987
#! This article describes the development of a free test collection for Chinese text categorization. A novel retrieval-based approach was developed to detect duplicates and label inconsistency in this corpus and in Reuters-21578 for comparison. The method was able to detect certain types of similar and/or duplicated documents that were overlooked by an alternative repetition-based method [1]. Experiments showed that effectiveness was not affected by the confusing documents.

#index 766512
#* Query-related data extraction of hidden web documents
#@ Y. L. Hedley;M. Younas;A. James;M. Sanderson
#t 2004
#c 13
#% 309783
#% 340146
#% 340998
#% 406493
#% 447946
#% 660365
#! The larger amount of information on the Web is stored in document databases and is not indexed by general-purpose search engines (i.e., Google and Yahoo). Such information is dynamically generated through querying databases - which are referred to as Hidden Web databases. Documents returned in response to a user query are typically presented using template-generated Web pages. This paper proposes a novel approach that identifies Web page templates by analysing the textual contents and the adjacent tag structures of a document in order to extract query-related data. Preliminary results demonstrate that our approach effectively detects templates and retrieves data with high recall and precision.

#index 766513
#* The patent retrieval task in the fourth NTCIR workshop
#@ Atsushi Fujii;Makoto Iwayama;Noriko Kando
#t 2004
#c 13
#! This paper describes the Patent Retrieval Task in the Fourth NTCIR Workshop, and the test collections produced in this task. We perform the invalidity search task, in which each participant group searches a patent collection for the patents that can invalidate the demand in an existing claim. We also perform the automatic patent map generation task, in which the patents associated with a specific topic are organized in a multi-dimensional matrix.

#index 766514
#* Measuring ineffectiveness
#@ Ellen M. Voorhees
#t 2004
#c 13
#% 309093
#% 397163
#! An evaluation methodology that targets ineffective topics is needed to support research on obtaining more consistent retrieval across topics. Using average values of traditional evaluation measures is not an appropriate methodology because it emphasizes effective topics: poorly performing topics' scores are by definition small, and they are therefore difficult to distinguish from the noise inherent in retrieval evaluation. We examine two new measures that emphasize a system's worst topics. While these measures focus on different aspects of retrieval behavior than traditional measures, the measures are less stable than traditional measures and the margin of error associated with the new measures is large relative to the observed differences in scores.

#index 766515
#* Information retrieval using hierarchical dirichlet processes
#@ Philip J. Cowans
#t 2004
#c 13
#% 262096
#! An information retrieval method is proposed using a hierarchical Dirichlet process as a prior on the parameters of a set of multinomial distributions. The resulting method naturally includes a number of features found in other popular methods. Specifically, tf.idf-like term weighting and document length normalisation are recovered. The new method is compared with Okapi BM-25 [3] and the Twenty-One model [1] on TREC data and is shown to give better performance.

#index 766516
#* Broken plural detection for arabic information retrieval
#@ Abduelbaset Goweder;Massimo Poesio;Anne De Roeck
#t 2004
#c 13
#% 177564
#% 270945
#% 397158
#! Due to the high number of inflectional variations of Arabic words, empirical results suggest that stemming is essential for Arabic information retrieval. However, current light stemming algorithms do not extract the correct stem of irregular (so-called broken) plurals, which constitute ~10% of Arabic texts and ~41% of plurals. Although light stemming in particular has led to improvements in information retrieval [5, 6], the effects of broken plurals on the performance of information retrieval systems has not been examined.We propose a light stemmer that incorporates a broken plural recognition component, and evaluate it within the context of information retrieval. Our results show that identifying broken plurals and reducing them to their correct stems does result in a significant improvement in the performance of information retrieval systems.

#index 766517
#* A study of methods for normalizing user ratings in collaborative filtering
#@ Rong Jin;Luo Si
#t 2004
#c 13
#% 173879
#% 730049
#% 1650569
#! The goal of collaborative filtering is to make recommendations for a test user by utilizing the rating information of users who share interests similar to the test user. Because ratings are determined not only by user interests but also the rating habits of users, it is important to normalize ratings of different users to the same scale. In this paper, we compare two different normalization strategies for user ratings, namely the Gaussian normalization method and the decoupling normalization method. Particularly, we incorporated these two rating normalization methods into two collaborative filtering algorithms, and evaluated their effectiveness on the EachMovie dataset. The experiment results have shown that the decoupling method for rating normalization is more effective than the Gaussian normalization method in improving the performance of collaborative filtering algorithms.

#index 766518
#* A review of relevance feedback experiments at the 2003 reliable information access (RIA) workshop.
#@ Robert H. Warren;Ting Liu
#t 2004
#c 13
#% 766471
#% 766525
#! We review here the results of one of the experiments performed at the 2003 Reliable Information Access (RIA) Workshop, hosted by Mitre Corporation and the Northeast Regional Research Center (NRRC). The experiment concentrates on query expansion using relevance feedback and explores the behaviour of several information retrieval systems using variable numbers of relevant documents.

#index 766519
#* Supporting federated information sharing communities
#@ Bicheng Liu;David J. Harper;Stuart Watt
#t 2004
#c 13
#% 343151
#% 429359
#% 438553
#! In this paper we describe the concept of Federated Information Sharing Communities (FISC), and associated architecture, which provide a way for organisations, distributed workgroups and individuals to build up a federated community based on their common interests over the World Wide Web. To support communities, we develop capabilities that go beyond the generic retrieval of documents to include the ability to retrieve people, their interests and inter-relationships. We focus on providing social awareness "in the large" to help users understand the members within a community and the relationships between them. Within the FISC framework, we provide viewpoint retrieval to enable a user to construct visual contextual views of the community from the perspective of any community member. To evaluate these ideas we develop test beds to compare individual component technologies such as user and group profile construction and similarity matching, and we develop prototypes to explore the broader architecture and usage issues.

#index 766520
#* The effect of document retrieval quality on factoid question answering performance
#@ Kevyn Collins-Thompson;Jamie Callan;Egidio Terra;Charles L.A. Clarke
#t 2004
#c 13
#% 184493
#% 413613
#% 642979

#index 766521
#* Exploiting hyperlink recommendation evidence in navigational web search
#@ Trystan Upstill;Stephen Robertson
#t 2004
#c 13
#% 268079
#% 309749
#% 641770

#index 766522
#* Context-based methods for text categorisation
#@ D. S. Hunnisett;W. J. Teahan
#t 2004
#c 13
#% 260001
#% 642987
#! We propose several context-based methods for text categorization. One method, a small modification to the PPM compression-based model which is known to significantly degrade compression performance, counter-intuitively has the opposite effect on categorization performance. Another method, called C-measure, simply counts the presence of higher order character contexts, and outperforms all other approaches investigated.

#index 766523
#* eMailSift: mining-based approaches to email classification
#@ Manu Aery;Sharma Chakravarthy
#t 2004
#c 13
#% 152934
#% 316709
#% 406493
#% 445369

#index 766524
#* Constructing a text corpus for inexact duplicate detection
#@ Jack G. Conrad;Cindy P. Schriber
#t 2004
#c 13
#% 169779
#% 207677
#% 255137
#% 345087
#% 504572
#% 730067
#! As online document collections continue to expand, both on the Web and in proprietary environments, the need for duplicate detection becomes more critical. The goal of this work is to facilitate (a) investigations into the phenomenon of near duplicates and (b) algorithmic approaches to minimizing its negative effect on search results. Harnessing the expertise of both client-users and professional searchers, we establish principled methods to generate a test collection for identifying and handling inexact duplicate documents.

#index 766525
#* Why current IR engines fail
#@ Chris Buckley
#t 2004
#c 13
#% 766497
#! Observations from a unique investigation of failure analysis of Information Retrieval (IR) research engines are presented. The Reliable Information Access (RIA) Workshop invited seven leading IR research groups to supply both their systems and their experts to an effort to analyze why their systems fail on some topics and whether the failures are due to system flaws, approach flaws, or the topic itself. There were surprising results from this cross-system failure analysis. One is that despite systems retrieving very different documents, the major cause of failure for any particular topic was almost always the same across all systems. Another is that relationships between aspects of a topic are not especially important for state-of-the-art systems; the systems are failing at a much more basic level where the top-retrieved documents are not reflecting some aspect at all.

#index 766526
#* Automatic sense disambiguation for acronyms
#@ Manuel Zahariev
#t 2004
#c 13
#% 269217
#% 748550
#% 815883
#% 842595
#! A machine learning methodology for the disambiguation of acronym senses is presented, which starts from an acronym sense dictionary. Training data is automatically extracted from downloaded documents identified from the results of search engine queries. Leave-one-out cross-validation on 9,963 documents with 47 acronym forms achieves accuracy 92.58% and Fß=1=91.52%.

#index 766527
#* Filtering for personal web information agents
#@ Gabriel L. Somlo;Adele E. Howe
#t 2004
#c 13
#% 169717
#% 234992
#% 252753
#% 262087
#% 420534
#% 549443
#% 1279298

#index 766528
#* Evaluating content-based filters for image and video retrieval
#@ Michael G. Christel;Neema Moraveji;Chang Huang
#t 2004
#c 13
#% 172811
#% 522598
#! This paper investigates the level of metadata accuracy required for image filters to be valuable to users. Access to large digital image and video collections is hampered by ambiguous and incomplete metadata attributed to imagery. Though improvements are constantly made in the automatic derivation of semantic feature concepts such as indoor, outdoor, face, and cityscape, it is unclear how good these improvements should be and under what circumstances they are effective. This paper explores the relationship between metadata accuracy and effectiveness of retrieval using an amateur photo collection, documentary video, and news video. The accuracy of the feature classification is varied from performance typical of automated classifications today to ideal performance taken from manually generated truth data. Results establish an accuracy threshold at which semantic features can be useful, and empirically quantify the collection size when filtering first shows its effectiveness.

#index 766529
#* Semantic video classification by integrating unlabeled samples for classifier training
#@ Jianping Fan;Hangzai Luo
#t 2004
#c 13
#% 311027
#% 730148
#% 766452
#% 1855648
#! Semantic video classification has become an active research topic to enable more effective video retrieval and knowledge discovery from large-scale video databases. However, most existing techniques for classifier training require a large number of hand-labeled samples to learn correctly. To address this problem, we have proposed a semi-supervised framework to achieve incremental classifier training by integrating a limited number of labeled samples with a large number of unlabeled samples. Specifically, this emi-supervised framework includes: (a) Modeling the semantic video concepts by using the finite mixture models to approximate the class distributions of the relevant salient objects; (b) Developing an adaptive EM algorithm to integrate the unlabeled samples to achieve parameter estimation and model selection simultaneously; The experimental results in a certain domain of medical videos are also provided.

#index 766530
#* Implicit queries (IQ) for contextualized search
#@ Susan Dumais;Edward Cutrell;Raman Sarin;Eric Horvitz
#t 2004
#c 13
#% 272917
#% 577300
#% 642983
#% 768292
#! The Implicit Query (IQ) prototype is a system which automatically generates context-sensitive searches based on a user's current computing activities. In the demo, we show IQ running when users are reading or composing email. Queries are automatically generated by analyzing the email message, and results are presented in a small pane adjacent to the current window to provide peripheral awareness of related information.

#index 766531
#* An implicit system for predicting interests
#@ Ryen W. White;Joemon M. Jose
#t 2004
#c 13
#% 730076
#! We demonstrate an adaptive search system that works proactively to help searchers find relevant information. The system observes searcher interaction, uses what it sees to model information needs and chooses additional query terms. The system watches for changes in the topic of the search and selects retrieval strategies that reflect the extent to which the topic is seen to change.

#index 766532
#* Geotemporal querying of multilingual documents
#@ Fredric C. Gey;Aitao Chen;Ray Larson;Kim Carl
#t 2004
#c 13
#! This demonstration utilizes a geographic information system interface to display multilingual news documents in time and space by extracting place names from text and matching them to a multilingual multi-script gazetteer which identifies the latitude and longitude of the location.

#index 766533
#* ACES: a contextual engine for search
#@ Xuehua Shen;Smitha Sriram;Chengxiang Zhai
#t 2004
#c 13
#% 642983
#% 643028
#% 766479

#index 766534
#* Armadillo: harvesting information for the semantic web
#@ Sam Chapman;Alexiei Dingli;Fabio Ciravegna
#t 2004
#c 13
#% 577318

#index 766535
#* UKSearch: search with automatically acquired domain knowledge
#@ Udo Kruschwitz;Hala Al-Bakour
#t 2004
#c 13
#% 719140

#index 766536
#* Geographic information retrieval (GIR): searching where and what
#@ Ray R. Larson;Patricia Frontiera
#t 2004
#c 13

#index 766537
#* Improving document representation by accumulating relevance feedback (abstract only): the relevance feedback accumulation algorithm
#@ Razvan Stefan Bot
#t 2004
#c 13
#! This paper presents a document representation improvement technique named the Relevance Feedback Accumulation (RFA) algorithm. Using prior relevance feedback assessments and a data mining measure called support this algorithm improves document representations and generates higher quality indexes. At the same time, the algorithm is efficient and scalable, suited for retrieval systems managing large document collections. The results of the preliminary evaluation reveal that the RFA algorithm is able to reduce the index dimensionality while improving retrieval effectiveness.

#index 766538
#* Supporting federated information sharing communities (abstract only)
#@ Bicheng Liu
#t 2004
#c 13
#! Increasingly, the World Wide Web is being viewed as a means of creating web communities rather than simply as a means of publishing and delivering documents and services. In this research we develop the concept of Federated Information Sharing Communities (FISC), and associated architecture, that enables community-centred information systems to be constructed. Such systems provide a way for organisations, distributed workgroups and individuals to build up a federated community based on their common interests over the World Wide Web. To support communities, we develop capabilities that go beyond the generic retrieval of documents to include the ability to retrieve people, their interests and inter-relationships. We focus on providing social awareness "in the large" to help users understand the members within a community and the relationships between them: who is working on what topic, and who is working with whom. Within the FISC framework, we provide a viewpoint retrieval service to enable a user to construct visual contextual views of the community from the perspective of any community member. To evaluate these ideas we develop test beds to compare individual component technologies such as user and group profile construction and similarity matching, and we develop prototypes (Web Network and "CiteSeer Community") to explore the broader architecture and usage issues.

#index 766539
#* Toponym resolution in text (abstract only): "which sheffield is it?"
#@ Jochen L. Leidner
#t 2004
#c 13
#! Named entity tagging comprises the sub-tasks of identifying a text span and classifying it, but this view ignores the relationship between the entities and the world. Spatial and temporal entities ground events in space-time, and this relationship is vital for applications such as question answering and event tracking. There is much recent work regarding the temporal dimension (Setzer and Gaizauskas 2002, Mani and Wilson 2000), but no detailed study of the spatial dimension.I propose to investigate how spatial named entities (which are often referentially ambiguous) can be automatically resolved with respect to an extensional coordinate model (toponym resolution). To this end, various information sources including linguistic cue patterns, co-occurrence information, discourse/positional information, world knowledge (such as size and population) as well as minimality heuristics (Leidner et al. 2003) will be combined in a supervised machine learning regime.The major contributions of this research project will be a corpus of text manually annotated for spatial named entities with their model correlates as a training and evaluation resource, a novel method to spatially ground toponyms in text and a component-based evaluation based on this new reference corpus.

#index 766540
#* Sharing knowledge online (abstract only): a dream or reality?
#@ Melanie Gnasa
#t 2004
#c 13
#! The Web provides a global platform for knowledge sharing. However, several shortcomings still arise from the absence of personalization and collaboration in Web searches. More effective retrieval techniques could be provided by means of transforming explicit knowledge into implicit knowledge. The approach presented in this paper is based on a peer-to-peer architecture and aims at complementing classical Web searches in terms of personalized ranking lists. These local rankings can be accumulated and evaluated in order to supplement the process of knowledge generation by building Virtual Knowledge Communities. Furthermore, the aggregation of ranking lists can be used to identify topics as well as communities of interest. Together with social aspects for community support, a framework for congenial Web search is defined.

#index 766541
#* Reliability and verification of natural language text on the world wide web (abstract only)
#@ Melanie J. Martin
#t 2004
#c 13
#! The hypothesis that information on the Web can be verified automatically, with minimal user interaction, will be tested by building and evaluating an interactive system. In this paper, verification is defined as a reasonable determination of the truth or correctness of a statement by examination, research, or comparison with similar text. The system will contain modules for reliability ranking, query processing, document retrieval, and document clustering based on agreement. The query processing and document retrieval components will use standard IR techniques. The reliability module will estimate the likelihood that a statement on the Web can be trusted using standards developed by information scientists, as well as linguistic aspects of the page and the link structure of associated web pages. The clustering module will cluster relevant documents based on whether or not they agree or disagree with the statement to be verified. Relevant references are discussed.

#index 766542
#* An artificial intelligence approach to information retrieval (abstract only)
#@ Andrew Trotman
#t 2004
#c 13
#! Current approaches to information retrieval rely on the creativity of individuals to develop new algorithms. In this investigation the use of genetic algorithms (GA) and genetic programming (GP) to learn IR algorithms is examined.Document structure weighting is a technique whereby different parts of a document (title, abstract, etc.) contribute unevenly to the overall document weight during ranking. Near optimal weights can be learned with a GA. Doing so shows a statistically significant 5% relative improvement in MAP for vector space inner product and Croft's probabilistic ranking, but no improvement for BM25. Two applications of this approach are suggested: offline learning, and relevance feedback.In a second set of experiments, a new ranking function was learned using GP. This new function yields a statistically significant 11% relative improvement on unseen queries tested on the training documents. Portability tests to different collections (not used in training) demonstrate the performance of the new function exceeds vector space and probability, and slightly exceeds BM25. Learning weights for this new function is proposed.The application of genetic learning to stemming and thesaurus construction is discussed. Stemming rules such as those of the Porter algorithm are candidates for GP learning whereas synonym sets are candidates for GA learning.

#index 766543
#* Understanding combination of evidence using generative probabilistic models for information retrieval (abstract only)
#@ Paul Ogilvie
#t 2004
#c 13
#! Structured documents, rich information needs, and detailed information about users are becoming more pervasive within everyday computing usage. Applications such as Question Answering, reading tutors, and XML retrieval demand more robust retrieval on richly annotated documents. In order to effectively serve these applications, the community will need a better understanding of the combination of evidence. In this work, I propose that the use of simple generative probabilistic models will be an effective framework for these problems. Statistical language models, which are a special case of generative probabilistic models, have been used extensively within recent Information Retrieval research. Their flexibility has been very effective in adapting to numerous tasks and problems. I propose to extend the statistical language modeling framework to handle rich information needs and documents with structural and linguistic annotations. Much of the prior work on combination of evidence has had few well-studied theoretical contributions, so I also propose to develop a sounder theoretical basis which gives more predictable results.

#index 766544
#* Discovering and representing the contextual and narrative structure of e-books to support reading and comprehension (abstract only)
#@ Yixing Sun
#t 2004
#c 13
#! A person reading a book needs to build an understanding based on the available textual materials. As a result of a survey of users' reading behaviours and of existing e-Book user interfaces, we found that most of these interfaces provide poor support for the actual processes of reading and comprehension. In particular, there is generally minimal support for understanding the overall structure (or contextual structure) and the narrative structure of a book. We propose adapting topic tracking and detection techniques to discover the narrative threads within a book, and hence its narrative structure. The contextual and narrative structures will be presented to the user through purpose-designed visualisations, which will be integrated and linked within a newly developed e-Book browser. We have chosen to use the Bible as our test corpus, as it has a rich narrative structure, and relatively complex contextual structure. Evaluation of the interface, and its components, will be done through field studies involving actual readers of the Bible, to assess the effectiveness of the user interface in enhancing a user's experience.

#index 766545
#* Supporting multiple information-seeking strategies in a single system framework (abstract only)
#@ Xiaojun Yuan
#t 2004
#c 13
#! This research explores the relationship between information-seeking strategies (ISSs) and information retrieval (IR) system design. When people seek information they engage in a variety of ISSs in order to search for specific items, learn about the contents of the database, evaluate retrieved information, and so on.The theoretical foundations of the work are based on the information-seeking episode model developed by Belkin (1996), and the multi-facet classification scheme of information behaviors proposed by Cool & Belkin (2002).The goal of this research is to construct and evaluate an interactive retrieval system which uses different combinations of IR techniques to support different ISSs. Example IR techniques include comparison using exact and probabilistic matching algorithms; summarization of information objects using titles, snippets or abstracts; visualization techniques such as lists or classified results; and navigation techniques such as scrolling or following links. By designing a retrieval system with diverse strategies in mind, we can adaptively support multiple ISSs, permitting a user to move seamlessly from one strategy to another, choosing instantiations of each support technique tailored to the specific ISS.The research will be conducted in a series of four steps. (1) Develop an object-oriented framework for representing basic IR techniques. (2) Design, implement and evaluate systems which support individual ISSs such as browsing and searching. (3) Specify an interaction structure for guiding and controlling sequences of different supporting techniques.(4) Design, implement, and evaluate a dynamically adaptive system supporting multiple ISSs in comparison to a non-adaptive baseline system.

#index 818199
#* The Portinari project: IR helps art and culture
#@ João Candido Portinari
#t 2005
#c 13
#! In May 30, 1983, The New York Times published the article"Brazil Gathers Archive On Its Painter, Portinari". The author,Warren Hoge, narrates: "/The late Candido Portinari is consideredhere to be the greatest artist Brazil has ever produced, yet allbut a few of his 4,000 paintings are out of public view. They havebecome dispersed in private collections in so many places that hisbiographer compared their fate to that of Brazil's 18th-centuryrevolutionary hero Tiradentes, whose body was dismembered andstrewn along a 300-mile turnpike. The inaccessibility ofPortinari's work is particularly vexing to his enthusiasts becausehis own dedication to producing an epic view of Brazil for hiscountrymen was such that he continued painting even after doctorswarned that exposure to paint was killing him. He died of leadpoisoning at the age of 58. Now, in a pioneering effort for LatinAmerica, a team of experts in Rio is busy assembling the far-flungpieces of Portinari's obra into an exhaustive computerized archive."/We are trying to rescue what is authentically ours'/, saidJoão Candido Portinari, the painter's 44-year-old son, whois the coordinator of the group of researchers who make theirheadquarters on the leafy campus of Rio's Pontifical CatholicUniversity. A telecommunications engineer with a Ph.D. from theMassachusetts Institute of Technology and a former chairman of theuniversity's mathematics department, Mr. Portinari has broughtexacting technical standards to the task. Now four years into theproject, the 14-member team has compiled photographic,technical.This presentation aims at describing the 26-year effortundertaken by the Portinari Project to locate, document, and recordall prints, drawings and paintings created by the Brazilian artistCandido Portinari (1903-1962), as well as all documents.It is important to highlite the role of science and technologyin this endeavour. It was fascinating to watch, over the years, howmuch, as the Portinari Project evolved, the advances of science andtechnology, and especially those related to IR, were able toaddress important challenges encountered in its development. Inthis presentation, a few examples of this process shall bedescribed, such as the problem of identifying false paintings, thedigital preservation of color images, the building of a complexmultimedia knowledge database, etc.We also present the social work of art education developed inmaking all this material available /in loco/ to a wide audience inBrazil and abroad, including school children, especially childrenfrom poverty-stricken families. This social inclusion action stemsfrom the fact that Portinari was deeply concerned to devoting hislife, as an artist and also as a political person, to social andhuman values. In his speech /Art in the United Nations/ at the.Palais des Nations., Geneve, Switzerland, Von Lauestein Massaraniobserved that.*/Portinari bequeathed to his native Brazil paintings of greatpoetical intensity. He grew up on the vast coffee plantations ofBrodosqui in the state of Sao Paulo and it is this social settingwhich provided the inspiration for his work. All its human,cultural and religious aspects, captured by Portinari.s brush,reveal him as a .chronicler. of the concerns of twentieth-centuryBrazil. A rich plastic quality and variety of expressions arequalitites which give singular appeal to this deeply inspired work.His paintings are the successful achievement of his objectivethroughout his life: to arouse a feeling of the dignity of man, offraternity and community spirit/*.*From these concerns, faithfully reflected in Portinari.sartistic legacy, we have been able to build an exceedingly powerfultool for developing a social acton that has already involved morethan 500,000 children for all over Brazil, as shall be demonstratedin our talk.And finally we present one of the most important results of theproject: "Candido Portinari - Catalogue /Raisonné/",published in September 2004, in 5 volumes, containing 4,991 works,with all their images and their technical, bibliographic andhistorical data. An example of the complexity of this undertakingis the work involved in /dating/ Portinari's oeuvre. Half of theworks were undated and, /for each/ of these 2,500 works, it wasnecessary to cross it with 30,000 documents, to search if in aletter (6,000), in a periodical clipping (12,000), in a historicalphotograph (1,200), in an oral history recording (74 interviewed,130-hour total recordings), etc., one could find information thatwould eventually lead to the date the work was created. ThePortinari Project web site (http://www.portinari.org.br) presentsall 4,991 paintings, drawing and prints, and all 30,000 entering aword contained in the work's description. The Catalogue/Raisonnß/ also comprises a detailed chrono-biography of theartist's life and times, compiled from the 30,000 documents thatform the Portinari Project archives. To complete this 26-year task,the team visited almost all Brazilian states and more than 20countries in the three Americas, Europe and the Middle Orient.

#index 818200
#* The future of media, blogs and innovation: new IR challenges?
#@ Fernando Flores
#t 2005
#c 13
#! An axiom of every good investor is not to buy shares when the goodness of them is already into newspapers. Before, the information was circulating in some form, for example from mouth-to-mouth, closed circles, or newsletters. Nowadays the news can also occur in blogs that point to public or private communities that discuss topics that traditional media do not carry or even hide. Nowadays, standard communication media are trapped in a Cartesian or Platonic correspondence assumption. They want to tell us how things really are, how they have occurred, and how they will happen, disregarding a concrete world of problems where opportunities and threats live in real time for people. Searching and exploring the world of blogs can create an acceleration of innovation and a dissolution of the previous status quo. Here, the search unit is not a word, but actions, worries, opportunities, threats, etc. That is, people living and pursuing shared goals with others. Which new searching tools can help to find trends, innovations and ideas taking consciousness in the context described above? Can IR help to end with this illusion of pseudo-objectivity and manipulation of passive individuals.

#index 818201
#* Challenges in running a commercial search engine
#@ Amit Singhal
#t 2005
#c 13
#! These are exciting times for Information Retrieval. Web search engines have brought IR to the masses. It now affects the lives of hundreds of millions of people, and growing, as Internet search companies launch ever more products based on techniques developed in These are exciting times for Information Retrieval. Web search engines have brought IR to the masses. It now affects the lives of hundreds of millions of people, and growing, as Internet search companies launch ever more products based on techniques developed in IR research.The real world poses unique challenges for search algorithms. They operate at unprecedented scales, and over a wide diversity of information. In addition, we have entered an unprecedented world of "Adversarial Information Retrieval". The lure of billions of dollars of commerce, guided by search engines, motivates all kinds of people to try all kinds of tricks to get their sites to the top of the search results.What techniques do people use to defeat IR algorithms? What are the evaluation challenges for a web search engine? How much impact has IR had on search engines? How does Google serve over 250 Million queries a day, often with sub-second response times? This talk will show that the world of algorithm and system design for commercial search engines can be described by two of Murphy's Laws: a) If anything can go wrong, it will, and b) If anything cannot go wrong, it will anyway.

#index 818202
#* Orthogonal locality preserving indexing
#@ Deng Cai;Xiaofei He
#t 2005
#c 13
#% 118749
#% 248027
#% 280819
#% 280822
#% 309129
#% 340910
#% 643008
#% 729437
#% 766418
#% 766419
#% 766420
#! We consider the problem of document indexing and representation. Recently, Locality Preserving Indexing (LPI) was proposed for learning a compact document subspace. Different from Latent Semantic Indexing which is optimal in the sense of global Euclidean structure, LPI is optimal in the sense of local manifold structure. However, LPI is extremely sensitive to the number of dimensions. This makes it difficult to estimate the intrinsic dimensionality, while inaccurately estimated dimensionality would drastically degrade its performance. One reason leading to this problem is that LPI is non-orthogonal. Non-orthogonality distorts the metric structure of the document space. In this paper, we propose a new algorithm called Orthogonal LPI. Orthogonal LPI iteratively computes the mutually orthogonal basis functions which respect the local geometrical structure. Moreover, our empirical study shows that OLPI can have more locality preserving power than LPI. We compare the new algorithm to LSI and LPI. Extensive experimental results show that Orthogonal LPI obtains better performance than both LSI and LPI. More crucially, it is insensitive to the number of dimensions, which makes it an efficient data preprocessing method for text clustering, classification, retrieval, etc.

#index 818203
#* Why spectral retrieval works
#@ Holger Bast;Debapriyo Majumdar
#t 2005
#c 13
#% 169777
#% 248027
#% 280822
#% 309129
#% 321635
#% 329569
#% 338443
#% 340910
#% 569795
#% 643002
#% 746445
#% 766420
#% 1279294
#! We argue that the ability to identify pairs of related terms is at the heart of what makes spectral retrieval work in practice. Schemes such as latent semantic indexing (LSI) and its descendants have this ability in the sense that they can be viewed as computing a matrix of term-term relatedness scores which is then used to expand the given documents (not the queries). For almost all existing spectral retrieval schemes, this matrix of relatedness scores depends on a fixed low-dimensional subspace of the original term space. We instead vary the dimension and study for each term pair the resultin curve of relatedness scores. We find that it is actually the shape of this curve which is indicative for the term-pair relatedness, and not any of the individual relatedness scores on the curve. We derive two simple, parameterless algorithms that detect this shape and that consistently outperform previous methods on a number of test collections. Our curves also shed light on the effectiveness of three fundamental types of variations of the basic LSI scheme.

#index 818204
#* Better than the real thing?: iterative pseudo-query processing using cluster-based language models
#@ Oren Kurland;Lillian Lee;Carmel Domshlak
#t 2005
#c 13
#% 118728
#% 219049
#% 248223
#% 248226
#% 262084
#% 262096
#% 280856
#% 340899
#% 340901
#% 340948
#% 342707
#% 458396
#% 643005
#% 643043
#% 719598
#% 742666
#% 766430
#% 766431
#% 766476
#% 766497
#% 766525
#% 818241
#% 995518
#! We present a novel approach to pseudo-feedback-based ad hoc retrieval that uses language models induced from both documents and clusters. First, we treat the pseudo-feedback documents produced in response to the original query as a set of pseudo-query that themselves can serve as input to the retrieval process. Observing that the documents returned in response to the pseudo-query can then act as pseudo-query for subsequent rounds, we arrive at a formulation of pseudo-query-based retrieval as an iterative process. Experiments show that several concrete instantiations of this idea, when applied in conjunction with techniques designed to heighten precision, yield performance results rivaling those of a number of previously-proposed algorithms, including the standard language-modeling approach. The use of cluster-based language models is a key contributing factor to our algorithms' success.

#index 818205
#* The maximum entropy method for analyzing retrieval measures
#@ Javed A. Aslam;Emine Yilmaz;Virgiliu Pavlu
#t 2005
#c 13
#% 57485
#% 115608
#% 194269
#% 194284
#% 211044
#% 227819
#% 248074
#% 305866
#% 309093
#% 314739
#% 719991
#% 770834
#! We present a model, based on the maximum entropy method, for analyzing various measures of retrieval performance such as average precision, R-precision, and precision-at-cutoffs. Our methodology treats the value of such a measure as a constraint on the distribution of relevant documents in an unknown list, and the maximum entropy distribution can be determined subject to these constraints. For good measures of overall performance (such as average precision), the resulting maximum entropy distributions are highly correlated with actual distributions of relevant documents in lists as demonstrated through TREC data; for poor measures of overall performance, the correlation is weaker. As such, the maximum entropy method can be used to quantify the overall quality of a retrieval measure. Furthermore, for good measures of overall performance (such as average precision), we show that the corresponding maximum entropy distributions can be used to accurately infer precision-recall curves and the values of other measures of performance, and we demonstrate that the quality of these inferences far exceeds that predicted by simple retrieval measure correlation, as demonstrated through TREC data.

#index 818206
#* A study of factors affecting the utility of implicit relevance feedback
#@ Ryen W. White;Ian Ruthven;Joemon M. Jose
#t 2005
#c 13
#% 115473
#% 169803
#% 214709
#% 297571
#% 345736
#% 731615
#% 810927
#! Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system. IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly. In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search. Our findings suggest that all three of these factors contribute to the utility of IRF.

#index 818207
#* Context-sensitive information retrieval using implicit feedback
#@ Xuehua Shen;Bin Tan;ChengXiang Zhai
#t 2005
#c 13
#% 290150
#% 309792
#% 330705
#% 340948
#% 348155
#% 577224
#% 577329
#% 643028
#% 731615
#% 754126
#% 766454
#% 766479
#% 766530
#% 807660
#! A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored. In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting. We propose several context-sensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents. We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set. Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.

#index 818208
#* User term feedback in interactive text-based image retrieval
#@ Chen Zhang;Joyce Y. Chai;Rong Jin
#t 2005
#c 13
#% 54435
#% 87101
#% 214709
#% 231567
#% 280840
#% 300542
#% 340899
#% 340901
#% 342101
#% 423024
#% 580076
#% 642984
#% 642989
#% 642990
#% 721163
#% 730166
#% 780690
#% 1857498
#% 1858013
#! To alleviate the vocabulary problem, this paper investigates the role of user term feedback in interactive text-based image retrieval. Term feedback refers to the feedback from a user on specific terms regarding their relevance to a target image. Previous studies have indicated the effectiveness of term feedback in interactive text retrieval [14]. However, the term feedback has not shown to be effective in our experiments on text-based image retrieval. Our results indicate that, although term feedback has a positive effect by allowing users to identify more relevant terms, it also has a strong negative effect by providing more opportunities for users to specify irrelevant terms. To understand these different effects and their implications on the potential of term feedback, this paper further presents analysis of important factors that contribute to the utility of term feedback and discusses the outlook of term feedback in interactive text-based image retrieval.

#index 818209
#* Active feedback in ad hoc information retrieval
#@ Xuehua Shen;ChengXiang Zhai
#t 2005
#c 13
#% 118726
#% 169717
#% 262112
#% 340899
#% 342707
#% 464268
#% 466419
#% 466887
#% 565531
#% 577224
#% 642975
#% 714351
#% 731615
#% 1272282
#% 1775158
#! Information retrieval is, in general, an iterative search process, in which the user often has several interactions with a retrieval system for an information need. The retrieval system can actively probe a user with questions to clarify the information need instead of just passively responding to user queries. A basic question is thus how a retrieval system should propose questions to the user so that it can obtain maximum benefits from the feedback on these questions. In this paper, we study how a retrieval system can perform active feedback, i.e., how to choose documents for relevance feedback so that the system can learn most from the feedback information. We present a general framework for such an active feedback problem, and derive several practical algorithms as special cases. Empirical evaluation of these algorithms shows that the performance of traditional relevance feedback (presenting the top K documents) is consistently worse than that of presenting documents with more diversity. With a diversity-based selection algorithm, we obtain fewer relevant documents, however, these fewer documents have more learning benefits.

#index 818210
#* Improving collection selection with overlap awareness in P2P search engines
#@ Matthias Bender;Sebastian Michel;Peter Triantafillou;Gerhard Weikum;Christian Zimmer
#t 2005
#c 13
#% 194246
#% 278831
#% 280856
#% 282422
#% 287463
#% 322884
#% 340175
#% 340176
#% 342397
#% 342708
#% 397133
#% 411437
#% 413594
#% 433982
#% 446419
#% 451536
#% 482108
#% 505869
#% 643011
#% 654463
#% 807433
#% 1015359
#! Collection selection has been a research issue for years. Typically, in related work, precomputed statistics are employed in order to estimate the expected result quality of each collection, and subsequently the collections are ranked accordingly. Our thesis is that this simple approach is insufficient for several applications in which the collections typically overlap. This is the case, for example, for the collections built by autonomous peers crawling the web. We argue for the extension of existing quality measures using estimators of mutual overlap among collections and present experiments in which this combination outperforms CORI, a popular approach based on quality estimation. We outline our prototype implementation of a P2P web search engine, coined MINERVA, that allows handling large amounts of data in a distributed and self-organizing manner. We conduct experiments which show that taking overlap into account during collection selection can drastically decrease the number of collections that have to be contacted in order to reach a satisfactory level of recall, which is a great step toward the feasibility of distributed web search.

#index 818211
#* Server selection methods in hybrid portal search
#@ David Hawking;Paul Thomas
#t 2005
#c 13
#% 194246
#% 262065
#% 273926
#% 280853
#% 280856
#% 287463
#% 296646
#% 301225
#% 330787
#% 340928
#% 342680
#% 413594
#% 590523
#% 607815
#% 643012
#% 722311
#% 751830
#% 765465
#% 766489
#% 768913
#% 993962
#! The TREC.GOV collection makes a valuable web testbed for distributed information retrieval methods because it is naturally partitioned and includes 725 web-oriented queries with judged answers. It can usefully model aspects of government and large corporate portals. Analysis of the.gov data shows that a purely distributed approach would not be feasible for providing search on a.gov portal because of the large number (17,000+) of web sites and the high proportion that do not provide a search interface. An alternative hybrid approach, combining both distributed and centralized techniques, is proposed and server selection methods are evaluated within this framework using web-oriented evaluation methodology. A number of well-known algorithms are compared against representatives (highest anchor ranked page (HARP) and anchor weighted sum (AWSUM)) of a family of new selection methods which use link anchortext extracted from an auxiliary crawl to provide descriptions of sites which are not themselves crawled. Of the previously published methods, ReDDE substantially outperformed three variants of CORI and also outperformed a method based on Kullback-Leibler Divergence (extended) except on topic distillation. HARP and AWSUM performed best overall but were outperformed on the topic distillation task by extended KL Divergence.

#index 818212
#* Modeling search engine effectiveness for federated search
#@ Luo Si;Jamie Callan
#t 2005
#c 13
#% 184489
#% 227891
#% 262063
#% 282422
#% 316535
#% 340948
#% 344448
#% 387427
#% 447946
#% 584845
#% 643011
#% 643012
#% 722312
#% 730035
#% 783473
#% 852010
#! Federated search links multiple search engines into a single, virtual search system. Most prior research of federated search focused on selecting search engines that have the most relevant contents, but ignored the retrieval effectiveness of individual search engines. This omission can cause serious problems when federating search engines of different qualities.This paper proposes a federated search technique that uses utility maximization to model the retrieval effectiveness of each search engine in a federated search environment. The new algorithm ranks the available resources by explicitly estimating the amount of relevant material that each resource can return, instead of the amount of relevant material that each resource contains. An extensive set of experiments demonstrates the effectiveness of the new algorithm.

#index 818213
#* A utility theoretic approach to determining optimal wait times in distributed information retrieval
#@ Kartik Hosanagar
#t 2005
#c 13
#% 194246
#% 194275
#% 227027
#% 240955
#% 282422
#% 290869
#% 340936
#% 387427
#% 396911
#% 397125
#% 481748
#% 536240
#% 656701
#% 754451
#% 768292
#! Distributed IR systems query a large number of IR servers, merge the retrieved results and display them to users. Since different servers handle collections of different sizes, have different processing and bandwidth capacities, there can be considerable heterogeneity in their response times. The broker in the distributed IR system thus has to make decisions regarding terminating searches based on perceived value of waiting -- retrieving more documents -- and the costs imposed on users by waiting for more responses. In this paper, we apply utility theory to formulate the broker's decision problem. The problem is a stochastic nonlinear program. We use Monte Carlo simulations to demonstrate how the optimal wait time may be determined in the context of a comparison shopping engine that queries multiple store websites for price and product information. We use data gathered from 30 stores for a set of 60 books. Our research demonstrates how a broker can leverage information about past retrievals regarding distributions of server response time and relevance scores to optimize its performance. Our main contribution is the formulation of the decision model for optimal wait time and proposal of a solution method. Our results suggest that the optimal wait time is highly sensitive to the manner in which users value from a set of retrieved results differs from the sum of user value from each result evaluated independently. We also find that the optimal wait time increases with the size of the distributed collections, but only if user utility from a set of results is nearly equal to the sum of utilities from each result.

#index 818214
#* Robustness of adaptive filtering methods in a cross-benchmark evaluation
#@ Yiming Yang;Shinjae Yoo;Jian Zhang;Bryan Kisiel
#t 2005
#c 13
#% 219049
#% 262085
#% 262087
#% 340941
#% 420507
#% 575571
#% 642998
#% 730034
#% 766450
#! This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering. Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings. We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function. Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11. Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with b=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.

#index 818215
#* A probabilistic model for retrospective news event detection
#@ Zhiwei Li;Bin Wang;Mingjing Li;Wei-Ying Ma
#t 2005
#c 13
#% 262042
#% 262043
#% 278107
#% 311027
#% 397133
#% 577297
#% 643016
#% 766444
#! Retrospective news event detection (RED) is defined as the discovery of previously unidentified events in historical news corpus. Although both the contents and time information of news articles are helpful to RED, most researches focus on the utilization of the contents of news articles. Few research works have been carried out on finding better usages of time information. In this paper, we do some explorations on both directions based on the following two characteristics of news articles. On the one hand, news articles are always aroused by events; on the other hand, similar articles reporting the same event often redundantly appear on many news sources. The former hints a generative model of news articles, and the latter provides data enriched environments to perform RED. With consideration of these characteristics, we propose a probabilistic model to incorporate both content and time information in a unified framework. This model gives new representations of both news articles and news events. Furthermore, based on this approach, we build an interactive RED system, HISCOVERY, which provides additional functions to present events, Photo Story and Chronicle.

#index 818216
#* Scalable collaborative filtering using cluster-based smoothing
#@ Gui-Rong Xue;Chenxi Lin;Qiang Yang;WenSi Xi;Hua-Jun Zeng;Yong Yu;Zheng Chen
#t 2005
#c 13
#% 158687
#% 173879
#% 220709
#% 220711
#% 280852
#% 309192
#% 309204
#% 330687
#% 420515
#% 465928
#% 495929
#% 528156
#% 528182
#% 577368
#% 734590
#% 734593
#% 1650569
#! Memory-based approaches for collaborative filtering identify the similarity between two users by comparing their ratings on a set of items. In the past, the memory-based approach has been shown to suffer from two fundamental problems: data sparsity and difficulty in scalability. Alternatively, the model-based approach has been proposed to alleviate these problems, but this approach tends to limit the range of users. In this paper, we present a novel approach that combines the advantages of these two approaches by introducing a smoothing-based method. In our approach, clusters generated from the training data provide the basis for data smoothing and neighborhood selection. As a result, we provide higher accuracy as well as increased efficiency in recommendations. Empirical studies on two datasets (EachMovie and MovieLens) show that our new proposed approach consistently outperforms other state-of-art collaborative filtering algorithms.

#index 818217
#* OCFS: optimal orthogonal centroid feature selection for text categorization
#@ Jun Yan;Ning Liu;Benyu Zhang;Shuicheng Yan;Zheng Chen;Qiansheng Cheng;Weiguo Fan;Wei-Ying Ma
#t 2005
#c 13
#% 36399
#% 269218
#% 324288
#% 376266
#% 385563
#% 387427
#% 465754
#% 728350
#% 729437
#% 763708
#% 769964
#% 770774
#% 770843
#% 783523
#% 817843
#% 1828410
#! Text categorization is an important research area in many Information Retrieval (IR) applications. To save the storage space and computation time in text categorization, efficient and effective algorithms for reducing the data before analysis are highly desired. Traditional techniques for this purpose can generally be classified into feature extraction and feature selection. Because of efficiency, the latter is more suitable for text data such as web documents. However, many popular feature selection techniques such as Information Gain (IG) andχ2-test (CHI) are all greedy in nature and thus may not be optimal according to some criterion. Moreover, the performance of these greedy methods may be deteriorated when the reserved data dimension is extremely low. In this paper, we propose an efficient optimal feature selection algorithm by optimizing the objective function of Orthogonal Centroid (OC) subspace learning algorithm in a discrete solution space, called Orthogonal Centroid Feature Selection (OCFS). Experiments on 20 Newsgroups (20NG), Reuters Corpus Volume 1 (RCV1) and Open Directory Project (ODP) data show that OCFS is consistently better than IG and CHI with smaller computation time especially when the reduced dimension is extremely small.

#index 818218
#* SimFusion: measuring similarity using unified relationship matrix
#@ Wensi Xi;Edward A. Fox;Weiguo Fan;Benyu Zhang;Zheng Chen;Jun Yan;Dong Zhuang
#t 2005
#c 13
#% 18616
#% 49501
#% 86371
#% 194299
#% 215225
#% 219047
#% 220706
#% 268079
#% 280852
#% 281209
#% 290830
#% 310567
#% 342961
#% 438136
#% 452854
#% 479659
#% 577273
#% 584932
#% 643023
#% 754089
#% 783508
#% 836019
#% 963669
#% 1656290
#! In this paper we use a Unified Relationship Matrix (URM) to represent a set of heterogeneous data objects (e.g., web pages, queries) and their interrelationships (e.g., hyperlinks, user click-through sequences). We claim that iterative computations over the URM can help overcome the data sparseness problem and detect latent relationships among heterogeneous data objects, thus, can improve the quality of information applications that require com- bination of information from heterogeneous sources. To support our claim, we present a unified similarity-calculating algorithm, SimFusion. By iteratively computing over the URM, SimFusion can effectively integrate relationships from heterogeneous sources when measuring the similarity of two data objects. Experiments based on a web search engine query log and a web page collection demonstrate that SimFusion can improve similarity measurement of web objects over both traditional content based algorithms and the cutting edge SimRank algorithm.

#index 818219
#* An application of text categorization methods to gene ontology annotation
#@ Kazuhiro Seki;Javed Mostafa
#t 2005
#c 13
#% 280817
#% 378612
#% 406493
#% 465754
#% 728350
#% 752313
#% 1223735
#! This paper describes an application of IR and text categorization methods to a highly practical problem in biomedicine, specifically, Gene Ontology (GO) annotation. GO annotation is a major activity in most model organism database projects and annotates gene functions using a controlled vocabulary. As a first step toward automatic GO annotation, we aim to assign GO domain codes given a specific gene and an article in which the gene appears, which is one of the task challenges at the TREC 2004 Genomics Track. We approached the task with careful consideration of the specialized terminology and paid special attention to dealing with various forms of gene synonyms, so as to exhaustively locate the occurrences of the target gene. We extracted the words around the gene occurrences and used them to represent the gene for GO domain code annotation. As a classifier, we adopted a variant of k-Nearest Neighbor (kNN) with supervised term weighting schemes to improve the performance, making our method among the top-performing systems in the TREC official evaluation. Moreover, it is demonstrated that our proposed framework is successfully applied to another task of the Genomics Track, showing comparable results to the best performing system.

#index 818220
#* Combining eye movements and collaborative filtering for proactive information retrieval
#@ Kai Puolamäki;Jarkko Salojärvi;Eerika Savia;Jaana Simola;Samuel Kaski
#t 2005
#c 13
#% 85672
#% 202011
#% 220711
#% 292193
#% 448826
#% 451582
#% 458673
#% 528182
#% 722904
#% 731615
#% 734592
#% 770816
#% 790469
#% 1134825
#! We study a new task, proactive information retrieval by combining implicit relevance feedback and collaborative filtering. We have constructed a controlled experimental setting, a prototype application, in which the users try to find interesting scientific articles by browsing their titles. Implicit feedback is inferred from eye movement signals, with discriminative hidden Markov models estimated from existing data in which explicit relevance feedback is available. Collaborative filtering is carried out using the User Rating Profile model, a state-of-the-art probabilistic latent variable model, computed using Markov Chain Monte Carlo techniques. For new document titles the prediction accuracy with eye movements, collaborative filtering, and their combination was significantly better than by chance. The best prediction accuracy still leaves room for improvement but shows that proactive information retrieval and combination of many sources of relevance feedback is feasible.

#index 818221
#* Accurately interpreting clickthrough data as implicit feedback
#@ Thorsten Joachims;Laura Granka;Bing Pan;Helene Hembrooke;Geri Gay
#t 2005
#c 13
#% 57484
#% 169774
#% 169803
#% 316718
#% 320432
#% 345591
#% 478627
#% 577224
#% 590523
#% 737637
#% 752129
#% 752177
#% 754099
#% 766454
#% 766472
#% 823348
#% 1272396
#! This paper examines the reliability of implicit feedback generated from clickthrough data in WWW search. Analyzing the users' decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. While this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average.

#index 818222
#* Information retrieval system evaluation: effort, sensitivity, and reliability
#@ Mark Sanderson;Justin Zobel
#t 2005
#c 13
#% 144074
#% 232685
#% 236052
#% 262102
#% 309093
#% 309095
#% 375017
#% 397163
#% 766409
#! The effectiveness of information retrieval systems is measured by comparing performance on a common set of queries and documents. Significance tests are often used to evaluate the reliability of such comparisons. Previous work has examined such tests, but produced results with limited application. Other work established an alternative benchmark for significance, but the resulting test was too stringent. In this paper, we revisit the question of how such tests should be used. We find that the t-test is highly reliable (more so than the sign or Wilcoxon test), and is far more reliable than simply showing a large percentage difference in effectiveness measures between IR systems. Our results show that past empirical work on significance tests over-estimated the error of such tests. We also re-consider comparisons between the reliability of precision at rank 10 and mean average precision, arguing that past comparisons did not consider the assessor effort required to compute such measures. This investigation shows that assessor effort would be better spent building test collections with more topics, each assessed in less detail.

#index 818223
#* Detecting phrase-level duplication on the world wide web
#@ Dennis Fetterly;Mark Manasse;Marc Najork
#t 2005
#c 13
#% 255137
#% 309749
#% 565488
#% 577370
#% 590524
#% 679843
#% 728115
#% 772018
#! Two years ago, we conducted a study on the evolution of web pages over time. In the course of that study, we discovered a large number of machine-generated "spam" web pages emanating from a handful of web servers in Germany. These spam web pages were dynamically assembled by stitching together grammatically well-formed German sentences drawn from a large collection of sentences. This discovery motivated us to develop techniques for finding other instances of such "slice and dice" generation of web pages, where pages are automatically generated by stitching together phrases drawn from a limited corpus. We applied these techniques to two data sets, a set of 151 million web pages collected in December 2002 and a set of 96 million web pages collected in June 2004. We found a number of other instances of large-scale phrase-level replication within the two data sets. This paper describes the algorithms we used to discover this type of replication, and highlights the results of our data mining.

#index 818224
#* Using ODP metadata to personalize search
#@ Paul Alexandru Chirita;Wolfgang Nejdl;Raluca Paiu;Christian Kohlschütter
#t 2005
#c 13
#% 198058
#% 281251
#% 290830
#% 309779
#% 330769
#% 340932
#% 341651
#% 348173
#% 397169
#% 577329
#% 608627
#% 641963
#% 1016162
#% 1016177
#! The Open Directory Project is clearly one of the largest collaborative efforts to manually annotate web pages. This effort involves over 65,000 editors and resulted in metadata specifying topic and importance for more than 4 million web pages. Still, given that this number is just about 0.05 percent of the Web pages indexed by Google, is this effort enough to make a difference? In this paper we discuss how these metadata can be exploited to achieve high quality personalized web search. First, we address this by introducing an additional criterion for web page ranking, namely the distance between a user profile defined using ODP topics and the sets of ODP topics covered by each URL returned in regular web search. We empirically show that this enhancement yields better results than current web search using Google. Then, in the second part of the paper, we investigate the boundaries of biasing PageRank on subtopics of the ODP in order to automatically extend these metadata to the whole web.

#index 818225
#* Exploiting the hierarchical structure for link analysis
#@ Gui-Rong Xue;Qiang Yang;Hua-Jun Zeng;Yong Yu;Zheng Chen
#t 2005
#c 13
#% 214028
#% 262061
#% 268073
#% 268079
#% 283833
#% 290830
#% 340919
#% 348173
#% 387427
#% 482652
#% 503228
#% 510723
#% 565488
#% 754088
#% 766462
#% 769506
#! Link analysis algorithms have been extensively used in Web information retrieval. However, current link analysis algorithms generally work on a flat link graph, ignoring the hierarchal structure of the Web graph. They often suffer from two problems: the sparsity of link graph and biased ranking of newly-emerging pages. In this paper, we propose a novel ranking algorithm called Hierarchical Rank as a solution to these two problems, which considers both the hierarchical structure and the link structure of the Web. In this algorithm, Web pages are first aggregated based on their hierarchical structure at directory, host or domain level and link analysis is performed on the aggregated graph. Then, the importance of each node on the aggregated graph is distributed to individual pages belong to the node based on the hierarchical structure. This algorithm allows the importance of linked Web pages to be distributed in the Web page space even when the space is sparse and contains new pages. Experimental results on the .GOV collection of TREC 2003 and 2004 show that hierarchical ranking algorithm consistently outperforms other well-known ranking algorithms, including the PageRank, BlockRank and LayerRank. In addition, experimental results show that link aggregation at the host level is much better than link aggregation at either the domain or directory levels.

#index 818226
#* Web-page summarization using clickthrough data
#@ Jian-Tao Sun;Dou Shen;Hua-Jun Zeng;Qiang Yang;Yuchang Lu;Zheng Chen
#t 2005
#c 13
#% 200694
#% 316520
#% 330780
#% 340884
#% 387791
#% 413615
#% 451355
#% 679873
#% 744549
#% 754124
#% 766437
#% 788796
#% 805877
#% 816173
#! Most previous Web-page summarization methods treat a Web page as plain text. However, such methods fail to uncover the full knowledge associated with a Web page needed in building a high-quality summary, because many of these methods do not consider the hidden relationships in the Web. Uncovering the hidden knowledge is important in building good Web-page summarizers. In this paper, we extract the extra knowledge from the clickthrough data of a Web search engine to improve Web-page summarization. Wefirst analyze the feasibility in utilizing the clickthrough data to enhance Web-page summarization and then propose two adapted summarization methods that take advantage of the relationships discovered from the clickthrough data. For those pages that are not covered by the clickthrough data, we design a thematic lexicon approach to generate implicit knowledge for them. Our methods are evaluated on a dataset consisting of manually annotated pages as well as a large dataset that is crawled from the Open Directory Project website. The experimental results indicate that significant improvements can be achieved through our proposed summarizer as compared to the summarizers that do not use the clickthrough data.

#index 818227
#* Topic themes for multi-document summarization
#@ Sanda Harabagiu;Finley Lacatusu
#t 2005
#c 13
#% 198058
#% 741058
#% 742437
#% 755863
#% 786578
#% 815876
#% 815893
#% 815909
#% 816062
#% 855377
#% 939732
#% 1251360
#% 1272344
#% 1476276
#! The problem of using topic representations for multi-document summarization (MDS) has received considerable attention recently. In this paper, we describe five different topic representations and introduce a novel representation of topics based on topic themes. We present eight different methods of generating MDS and evaluate each of these methods on a large set of topics used in past DUC workshops. Our evaluation results show a significant improvement in the quality of summaries based on topic themes over MDS methods that use other alternative topic representations.

#index 818228
#* Do summaries help?
#@ Kathleen McKeown;Rebecca J. Passonneau;David K. Elson;Ani Nenkova;Julia Hirschberg
#t 2005
#c 13
#% 198294
#% 815133
#% 815920
#% 816139
#% 816173
#% 855375
#% 938679
#% 995468
#% 1478826
#! We describe a task-based evaluation to determine whether multi-document summaries measurably improve user performance whe using online news browsing systems for directed research. We evaluated the multi-document summaries generated by Newsblaster, a robust news browsing system that clusters online news articles and summarizes multiple articles on each event. Four groups of subjects were asked to perform the same time-restricted fact-gathering tasks, reading news under different conditions: no summaries at all, single sentence summaries drawn from one of the articles, Newsblaster multi-document summaries, and human summaries. Our results show that, in comparison to source documents only, the quality of reports assembled using Newsblaster summaries was significantly better and user satisfaction was higher with both Newsblaster and human summaries.

#index 818229
#* Optimization strategies for complex queries
#@ Trevor Strohman;Howard Turtle;W. Bruce Croft
#t 2005
#c 13
#% 194247
#% 198335
#% 213786
#% 228097
#% 252608
#% 394709
#% 730065
#% 789959
#! Previous research into the efficiency of text retrieval systems has dealt primarily with methods that consider inverted lists in sequence; these methods are known as term-at-a-time methods. However, the literature for optimizing document-at-a-time systems remains sparse.We present an improvement to the max_score optimization, which is the most efficient known document-at-a-time scoring method. Like max_score, our technique, called term bounded max_score, is guaranteed to return exactly the same scores and documents as an unoptimized evaluation, which is particularly useful for query model research. We simulated our technique to explore the problem space, then implemented it in Indri, our large scale language modeling search engine. Tests with the GOV2 corpus on title queries show our method to be 23% faster than max_score alone, and 61% faster than our document-at-a-time baseline. Our optimized query times are competitive with conventional term-at-a-time systems on this year's TREC Terabyte task.

#index 818230
#* Simplified similarity scoring using term ranks
#@ Vo Ngoc Anh;Alistair Moffat
#t 2005
#c 13
#% 218982
#% 253191
#% 262096
#% 290703
#% 309093
#% 340886
#% 397123
#% 397128
#% 397183
#% 723328
#% 786632
#! We propose a method for document ranking that combines a simple document-centric view of text, and fast evaluation strategies that have been developed in connection with the vector space model. The new method defines the importance of a term within a document qualitatively rather than quantitatively, and in doing so reduces the need for tuning parameters. In addition, the method supports very fast query processing, with most of the computation carried out on small integers, and dynamic pruning an effective option. Experiments on a wide range of TREC data show that the new method provides retrieval effectiveness as good as or better than the Okapi BM25 formulation, and variants of language models.

#index 818231
#* Efficiently decodable and searchable natural language adaptive compression
#@ Nieves R. Brisaboa;Antonio Fariña;Gonzalo Navarro;José R. Paramá
#t 2005
#c 13
#% 57849
#% 68236
#% 262101
#% 290703
#% 311799
#% 320454
#% 375076
#% 387427
#% 393450
#% 401434
#% 420492
#% 438325
#% 1387565
#! We address the problem of adaptive compression of natural language text, focusing on the case where low bandwidth is available and the receiver has little processing power, as in mobile applications. Our technique achieves compression ratios around 32% and requires very little effort from the receiver. This tradeoff, not previously achieved with alternative techniques, is obtained by breaking the usual symmetry between sender and receiver dominant in statistical adaptive compression. Moreover, we show that our technique can be adapted to avoid decompression at all in cases where the receiver only wants to detect the presence of some keywords in the document. This is useful in scenarios such as selective dissemination of information, news clipping, alert systems, text categorization, and clustering. Thanks to the asymmetry we introduce, the receiver can search the compressed text much faster than the plain text. This was previously achieved only in semistatic compression scenarios.

#index 818232
#* Efficient and self-tuning incremental query expansion for top-k query processing
#@ Martin Theobald;Ralf Schenkel;Gerhard Weikum
#t 2005
#c 13
#% 144029
#% 169729
#% 169781
#% 169806
#% 194277
#% 212665
#% 213786
#% 218978
#% 228097
#% 262084
#% 337285
#% 340886
#% 397123
#% 397376
#% 397378
#% 397608
#% 480330
#% 480819
#% 591792
#% 642994
#% 643566
#% 659255
#% 730007
#% 730065
#% 763882
#% 766440
#% 766671
#% 768903
#% 777931
#% 783474
#% 783506
#% 804915
#% 1015265
#% 1016183
#! We present a novel approach for efficient and self-tuning query expansion that is embedded into a top-k query processor with candidate pruning. Traditional query expansion methods select expansion terms whose thematic similarity to the original query terms is above some specified threshold, thus generating a disjunctive query with much higher dimensionality. This poses three major problems: 1) the need for hand-tuning the expansion threshold, 2) the potential topic dilution with overly aggressive expansion, and 3) the drastically increased execution cost of a high-dimensional query. The method developed in this paper addresses all three problems by dynamically and incrementally merging the inverted lists for the potential expansion terms with the lists for the original query terms. A priority queue is used for maintaining result candidates, the pruning of candidates is based on Fagin's family of top-k algorithms, and optionally probabilistic estimators of candidate scores can be used for additional pruning. Experiments on the TREC collections for the 2004 Robust and Terabyte tracks demonstrate the increased efficiency, effectiveness, and scalability of our approach.

#index 818233
#* Title extraction from bodies of HTML documents and its application to web page retrieval
#@ Yunhua Hu;Guomao Xin;Ruihua Song;Guoping Hu;Shuming Shi;Yunbo Cao;Hang Li
#t 2005
#c 13
#% 271065
#% 311037
#% 346637
#% 464612
#% 480824
#% 531459
#% 642992
#% 729978
#% 752607
#% 754078
#% 754108
#% 783474
#% 978382
#% 1264984
#% 1279271
#! This paper is concerned with automatic extraction of titles from the bodies of HTML documents. Titles of HTML documents should be correctly defined in the title fields; however, in reality HTML titles are often bogus. It is desirable to conduct automatic extraction of titles from the bodies of HTML documents. This is an issue which does not seem to have been investigated previously. In this paper, we take a supervised machine learning approach to address the problem. We propose a specification on HTML titles. We utilize format information such as font size, position, and font weight as features in title extraction. Our method significantly outperforms the baseline method of using the lines in largest font size as title (20.9%-32.6% improvement in F1 score). As application, we consider web page retrieval. We use the TREC Web Track data for evaluation. We propose a new method for HTML documents retrieval using extracted titles. Experimental results indicate that the use of both extracted titles and title fields is almost always better than the use of title fields alone; the use of extracted titles is particularly helpful in the task of named page finding (23.1% -29.0% improvements).

#index 818234
#* Multi-label informed latent semantic indexing
#@ Kai Yu;Shipeng Yu;Volker Tresp
#t 2005
#c 13
#% 266426
#% 269226
#% 309129
#% 722809
#% 743284
#% 763708
#% 766418
#! Latent semantic indexing (LSI) is a well-known unsupervised approach for dimensionality reduction in information retrieval. However if the output information (i.e. category labels) is available, it is often beneficial to derive the indexing not only based on the inputs but also on the target values in the training data set. This is of particular importance in applications with multiple labels, in which each document can belong to several categories simultaneously. In this paper we introduce the multi-label informed latent semantic indexing (MLSI) algorithm which preserves the information of inputs and meanwhile captures the correlations between the multiple outputs. The recovered "latent semantics" thus incorporate the human-annotated category information and can be used to greatly improve the prediction accuracy. Empirical study based on two data sets, Reuters-21578 and RCV1, demonstrates very encouraging results.

#index 818235
#* Text classification with kernels on the multinomial manifold
#@ Dell Zhang;Xi Chen;Wee Sun Lee
#t 2005
#c 13
#% 260001
#% 280817
#% 304899
#% 304917
#% 387427
#% 402289
#% 458379
#% 464267
#% 640416
#% 722813
#% 743284
#% 770755
#% 771841
#% 1673021
#% 1860548
#% 1860941
#! Support Vector Machines (SVMs) have been very successful in text classification. However, the intrinsic geometric structure of text data has been ignored by standard kernels commonly used in SVMs. It is natural to assume that the documents are on the multinomial manifold, which is the simplex of multinomial models furnished with the Riemannian structure induced by the Fisher information metric. We prove that the Negative Geodesic Distance (NGD) on the multinomial manifold is conditionally positive definite (cpd), thus can be used as a kernel in SVMs. Experiments show the NGD kernel on the multinomial manifold to be effective for text classification, significantly outperforming standard kernels on the ambient Euclidean space.

#index 818236
#* Multi-labelled classification using maximum entropy method
#@ Shenghuo Zhu;Xiang Ji;Wei Xu;Yihong Gong
#t 2005
#c 13
#% 226495
#% 280817
#% 311034
#% 397142
#% 420507
#% 770783
#% 783478
#% 854813
#! Many classification problems require classifiers to assign each single document into more than one category, which is called multi-labelled classification. The categories in such problems usually are neither conditionally independent from each other nor mutually exclusive, therefore it is not trivial to directly employ state-of-the-art classification algorithms without losing information of relation among categories. In this paper, we explore correlations among categories with maximum entropy method and derive a classification algorithm for multi-labelled documents. Our experiments show that this method significantly outperforms the combination of single label approach.

#index 818238
#* Relevance information: a loss of entropy but a gain for IDF?
#@ Arjen P. de Vries;Thomas Roelleke
#t 2005
#c 13
#% 54413
#% 169781
#% 176530
#% 184490
#% 375017
#% 643003
#% 742666
#% 766429
#% 1348341
#! When investigating alternative estimates for term discriminativeness, we discovered that relevance information and idf are much closer related than formulated in classical literature. Therefore, we revisited the justification of idf as it follows from the binary independent retrieval (BIR) model. The main result is a formal framework uncovering the close relationship of a generalised idf and the BIR model. The framework makes explicit how to incorporate relevance information into any retrieval function that involves an idf-component.In addition to the idf-based formulation of the BIR model, we propose Poisson-based estimates as an alternative to the classical estimates, this being motivated by the superiority of Poisson-based estimates for the within-document term frequencies. The main experimental finding is that a Poisson-based idf is superior to the classical idf, where the superiority is particularly evident for long queries.

#index 818239
#* Linear discriminant model for information retrieval
#@ Jianfeng Gao;Haoliang Qi;Xinsong Xia;Jian-Yun Nie
#t 2005
#c 13
#% 33917
#% 132779
#% 169781
#% 190581
#% 262096
#% 269217
#% 280850
#% 287253
#% 340899
#% 397129
#% 413593
#% 564279
#% 577224
#% 729437
#% 766414
#% 766428
#% 817439
#% 854636
#% 1272396
#! This paper presents a new discriminative model for information retrieval (IR), referred to as linear discriminant model (LDM), which provides a flexible framework to incorporate arbitrary features. LDM is different from most existing models in that it takes into account a variety of linguistic features that are derived from the component models of HMM that is widely used in language modeling approaches to IR. Therefore, LDM is a means of melding discriminative and generative models for IR. We present two algorithms of parameter learning for LDM. One is to optimize the average precision (AP) directly using an iterative procedure. The other is a perceptron-based algorithm that minimizes the number of discordant document-pairs in a rank list. The effectiveness of our approach has been evaluated on the task of ad hoc retrieval using six English and Chinese TREC test sets. Results show that (1) in most test sets, LDM significantly outperforms the state-of-the-art language modeling approaches and the classical probabilistic retrieval model; (2) it is more appropriate to train LDM using a measure of AP rather than likelihood if the IR system is graded on AP; and (3) linguistic features (e.g. phrases and dependences) are effective for IR if they are incorporated properly.

#index 818240
#* Integrating word relationships into language models
#@ Guihong Cao;Jian-Yun Nie;Jing Bai
#t 2005
#c 13
#% 169729
#% 262096
#% 280850
#% 280851
#% 340899
#% 340948
#% 397128
#% 397129
#% 397205
#% 413593
#% 448725
#% 766428
#% 766440
#% 770864
#! In this paper, we propose a novel dependency language modeling approach for information retrieval. The approach extends the existing language modeling approach by relaxing the independence assumption. Our goal is to build a language model in which various word relationships can be integrated. In this work, we integrate two types of relationship extracted from WordNet and co-occurrence relationships respectively. The integrated model has been tested on several TREC collections. The results show that our model achieves substantial and significant improvements with respect to the models without these relationships. These results clearly show the benefit of integrating word relationships into language models for IR.

#index 818241
#* PageRank without hyperlinks: structural re-ranking using links induced by language models
#@ Oren Kurland;Lillian Lee
#t 2005
#c 13
#% 218982
#% 218992
#% 262096
#% 268079
#% 280850
#% 290830
#% 340899
#% 340948
#% 342621
#% 342660
#% 397126
#% 427921
#% 719598
#% 730070
#% 742666
#% 746885
#% 766406
#% 766430
#% 766431
#% 766476
#% 770864
#% 818204
#% 938687
#% 995518
#% 1272053
#! Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web search, we propose a structural re-ranking approach to ad hoc information retrieval: we reorder the documents in an initially retrieved set by exploiting asymmetric relationships between them. Specifically, we consider generation links, which indicate that the language model induced from one document assigns high probability to the text of another; in doing so, we take care to prevent bias against long documents. We study a number of re-ranking criteria based on measures of centrality in the graphs formed by generation links, and show that integrating centrality into standard language-model-based retrieval is quite effective at improving precision at top ranks.

#index 818242
#* Controlling overlap in content-oriented XML retrieval
#@ Charles L. A. Clarke
#t 2005
#c 13
#% 340914
#% 397375
#% 411762
#% 642993
#% 654493
#% 766415
#% 766416
#% 766417
#% 783474
#% 783526
#% 1721849
#% 1721852
#% 1721854
#% 1721855
#% 1721858
#% 1721862
#% 1721867
#! The direct application of standard ranking techniques to retrieve individual elements from a collection of XML documents often produces a result set in which the top ranks are dominated by a large number of elements taken from a small number of highly relevant documents. This paper presents and evaluates an algorithm that re-ranks this result set, with the aim of minimizing redundant content while preserving the benefits of element retrieval, including the benefit of identifying topic-focused components contained within relevant documents. The test collection developed by the INitiative for the Evaluation of XML Retrieval (INEX) forms the basis for the evaluation.

#index 818243
#* Publish/subscribe functionality in IR environments using structured overlay networks
#@ Christos Tryfonopoulos;Stratos Idreos;Manolis Koubarakis
#t 2005
#c 13
#% 158911
#% 219048
#% 297191
#% 340175
#% 340176
#% 508414
#% 523066
#% 730035
#% 765501
#% 765673
#% 766446
#% 1711099
#% 1715596
#! We study the problem of offering publish/subscribe functionality on top of structured overlay networks using data models and languages from IR. We show how to achieve this by extending the distributed hash table Chord and present a detailed experimental evaluation of our proposals.

#index 818244
#* Learning to extract information from semi-structured text using a discriminative context free grammar
#@ Paul Viola;Mukund Narasimhan
#t 2005
#c 13
#% 197394
#% 279755
#% 302390
#% 464434
#% 549447
#% 643004
#% 648984
#% 782283
#% 816181
#% 854636
#% 855119
#% 1250184
#% 1673026
#! In recent work, conditional Markov chain models (CMM) have been used to extract information from semi-structured text (one example is the Conditional Random Field [10]). Applications range from finding the author and title in research papers to finding the phone number and street address in a web page. The CMM framework combines a priori knowledge encoded as features with a set of labeled training data to learn an efficient extraction process. We will show that similar problems can be solved more effectively by learning a discriminative context free grammar from training data. The grammar has several distinct advantages: long range, even global, constraints can be used to disambiguate entity labels; training data is used more efficiently; and a set of new more powerful features can be introduced. The grammar based approach also results in semantic information (encoded in the form of a parse tree) which could be used for IR applications like question answering. The specific problem we consider is of extracting personal contact, or address, information from unstructured sources such as documents and emails. While linear-chain CMMs perform reasonably well on this task, we show that a statistical parsing approach results in a 50% reduction in error rate. This system also has the advantage of being interactive, similar to the system described in [9]. In cases where there are multiple errors, a single user correction can be propagated to correct multiple errors automatically. Using a discriminatively trained grammar, 93.71% of all tokens are labeled correctly (compared to 88.43% for a CMM) and 72.87% of records have all tokens labeled correctly (compared to 45.29% for the CMM).

#index 818245
#* Web-based acquisition of Japanese katakana variants
#@ Takeshi Masuyama;Hiroshi Nakagawa
#t 2005
#c 13
#% 317975
#% 400061
#% 938728
#% 939750
#% 939824
#! This paper describes a method of detecting Japanese Katakana variants from a large corpus. Katakana words, which are mainly used as loanwords, cause problems with information retrieval and so on, because transliteration creates several variations in spelling and all of these can be orthographic. Previous works manually defined Katakana rewrite rules such as %Y (be) and %t%' (ve) being replaceable with each other, for generating variants and also defined the weight of each operation to edit one string into another to detect these variants. However, these previous researches have not been able to keep up with the ever-increasing number of loanwords and their variants. With our method proposed in this paper, the weight of each edit operation is mechanically assigned based on Web data. In experiments, it performed almost as well as one with manually determined weights. Thus, the advantages of our method are: 1) need no expertise in linguistics to determine weight of each operation, and 2) able to keep up with new Katakana loanwords only by collecting text data from Web and acquiring new weights of edit operations automatically. It also achieved 98.6% recall and 86.3% precision in the task of extracting Katakana variant pairs from 38 year's worth of corpora of Japanese newspaper articles.

#index 818246
#* On the collective classification of email "speech acts"
#@ Vitor R. Carvalho;William W. Cohen
#t 2005
#c 13
#% 2856
#% 207677
#% 211044
#% 248810
#% 722754
#% 766484
#% 769942
#% 816181
#! We consider classification of email messages as to whether or not they contain certain "email acts", such as a request or a commitment. We show that exploiting the sequential correlation among email messages in the same thread can improve email-act classification. More specifically, we describe a new text-classification algorithm based on a dependency-network based collective classification method, in which the local classifiers are maximum entropy models based on words and certain relational features. We show that statistically significant improvements over a bag-of-words baseline classifier can be obtained for some, but not all, email-act classes. Performance improvements obtained by collective classification appears to be consistent across many email acts suggested by prior speech-act theory.

#index 818247
#* Using term informativeness for named entity detection
#@ Jason D. M. Rennie;Tommi Jaakkola
#t 2005
#c 13
#% 278107
#% 375017
#% 458379
#% 477984
#% 716271
#% 816060
#! Informal communication (e-mail, bulletin boards) poses a difficult learning environment because traditional grammatical and lexical information are noisy. Other information is necessary for tasks such as named entity detection. How topic-centric, or informative, a word is can be valuable information. It is well known that informative words are best modeled by "heavy-tailed" distributions, such as mixture models. However, informativeness scores do not take full advantage of this fact. We introduce a new informativeness score that directly utilizes mixture model likelihood to identify informative words. We use the task of extracting restaurant names from bulletin board posts as a way to determine effectiveness. We find that our "mixture score" is weakly effective alone and highly effective when combined with Inverse Document Frequency. We compare against other informativeness criteria and find that only Residual IDF is competitive against our combined IDF/Mixture score.

#index 818248
#* Automatic music video summarization based on audio-visual-text analysis and alignment
#@ Changsheng Xu;Xi Shao;Namunu C. Maddage;Mohan S. Kankanhalli
#t 2005
#c 13
#% 49490
#% 166097
#% 333679
#% 341778
#% 395959
#% 451654
#% 457898
#% 496356
#% 722757
#% 730142
#% 730153
#% 730196
#% 780702
#% 970451
#! In this paper, we propose a novel approach for automatic music video summarization based on audio-visual-text analysis and alignment. The music video is separated into the music and video tracks. For the music track, the chorus is detected based on music structure analysis. For the video track, we first segment the shots and classify the shots into close-up face shots and non-face shots, then we extract the lyrics and detect the most repeated lyrics from the shots. The music video summary is generated based on the alignment of boundaries of the detected chorus, shot class and the most repeated lyrics from the music video. The experiments on chorus detection, shot classification, and lyrics detection using 20 English music videos are described. Subjective user studies have been conducted to evaluate the quality and effectiveness of summary. The comparisons with the summaries based on our previous method and the manual method indicate that the results of summarization using the proposed method are better at meeting users' expectations.

#index 818249
#* A phonotactic-semantic paradigm for automatic spoken document classification
#@ Bin Ma;Haizhou Li
#t 2005
#c 13
#% 319372
#% 729437
#% 741441
#% 766436
#% 816154
#% 1860761
#! We demonstrate a phonotactic-semantic paradigm for spoken document categorization. In this framework, we define a set of acoustic words instead of lexical words to represent acoustic activities in spoken languages. The strategy for acoustic vocabulary selection is studied by comparing different feature selection methods. With an appropriate acoustic vocabulary, a voice tokenizer converts a spoken document into a text-like document of acoustic words. Thus, a spoken document can be represented by a count vector, named a bag-of-sounds vector, which characterizes a spoken document's semantic domain. We study two phonotactic-semantic classifiers, the support vector machine classifier and the latent semantic analysis classifier, and their properties. The phonotactic-semantic framework constitutes a new paradigm in spoken document classification, as demonstrated by its success in the spoken language identification task. It achieves 18.2% error reduction over state-of-the-art benchmark performance on the 1996 NIST Language Recognition Evaluation database.

#index 818250
#* Boosted decision trees for word recognition in handwritten document retrieval
#@ Nicholas R. Howe;Toni M. Rath;R. Manmatha
#t 2005
#c 13
#% 136350
#% 239578
#% 262096
#% 296534
#% 340899
#% 724345
#% 738473
#% 738485
#% 753296
#% 766453
#% 1389541
#% 1502433
#! Recognition and retrieval of historical handwritten material is an unsolved problem. We propose a novel approach to recognizing and retrieving handwritten manuscripts, based upon word image classification as a key step. Decision trees with normalized pixels as features form the basis of a highly accurate AdaBoost classifier, trained on a corpus of word images that have been resized and sampled at a pyramid of resolutions. To stem problems from the highly skewed distribution of class frequencies, word classes with very few training samples are augmented with stochastically altered versions of the originals. This increases recognition performance substantially. On a standard corpus of 20 pages of handwritten material from the George Washington collection the recognition performance shows a substantial improvement in performance over previous published results (75% vs 65%). Following word recognition, retrieval is done using a language model over the recognized words. Retrieval performance also shows substantially improved results over previously published results on this database. Recognition/retrieval results on a more challenging database of 100 pages from the George Washington collection are also presented.

#index 818251
#* Generic soft pattern models for definitional question answering
#@ Hang Cui;Min-Yen Kan;Tat-Seng Chua
#t 2005
#c 13
#% 279755
#% 754067
#% 766459
#% 815868
#% 816173
#% 855279
#% 939726
#% 1279275
#! This paper explores probabilistic lexico-syntactic pattern matching, also known as soft pattern matching. While previous methods in soft pattern matching are ad hoc in computing the degree of match, we propose two formal matching models: one based on bigrams and the other on the Profile Hidden Markov Model (PHMM). Both models provide a theoretically sound method to model pattern matching as a probabilistic process that generates token sequences. We demonstrate the effectiveness of these models on definition sentence retrieval for definitional question answering. We show that both models significantly outperform state-of-the-art manually constructed patterns. A critical difference between the two models is that the PHMM technique handles language variations more effectively but requires more training data to converge. We believe that both models can be extended to other areas where lexico-syntactic pattern matching can be applied.

#index 818252
#* Evaluation of resources for question answering evaluation
#@ Jimmy Lin
#t 2005
#c 13
#% 262097
#% 262102
#% 262105
#% 309093
#% 309127
#% 397163
#% 397164
#% 571493
#% 575572
#% 766409
#% 816187
#% 867119
#! Controlled and reproducible laboratory experiments, enabled by reusable test collections, represent a well-established methodology in modern information retrieval research. In order to confidently draw conclusions about the performance of different retrieval methods using test collections, their reliability and trustworthiness must first be established. Although such studies have been performed for ad hoc test collections, currently available resources for evaluating question answering systems have not been similarly analyzed. This study evaluates the quality of answer patterns and lists of relevant documents currently employed in automatic question answering evaluation, and concludes that they are not suitable for post-hoc experimentation. These resources, created from runs submitted by TREC QA track participants, do not produce fair and reliable assessments of systems that did not participate in the original evaluations. Potential solutions for addressing this evaluation gap and their shortcomings are discussed.

#index 818253
#* Question answering passage retrieval using dependency relations
#@ Hang Cui;Renxu Sun;Keya Li;Min-Yen Kan;Tat-Seng Chua
#t 2005
#c 13
#% 144074
#% 232677
#% 280851
#% 287253
#% 642979
#% 740915
#% 741890
#% 741891
#% 766428
#% 817421
#! State-of-the-art question answering (QA) systems employ term-density ranking to retrieve answer passages. Such methods often retrieve incorrect passages as relationships among question terms are not considered. Previous studies attempted to address this problem by matching dependency relations between questions and answers. They used strict matching, which fails when semantically equivalent relationships are phrased differently. We propose fuzzy relation matching based on statistical models. We present two methods for learning relation mapping scores from past QA pairs: one based on mutual information and the other on expectation maximization. Experimental results show that our method significantly outperforms state-of-the-art density-based passage retrieval methods by up to 78% in mean reciprocal rank. Relation matching also brings about a 50% improvement in a system enhanced by query expansion.

#index 818254
#* A study of relevance propagation for web search
#@ Tao Qin;Tie-Yan Liu;Xu-Dong Zhang;Zheng Chen;Wei-Ying Ma
#t 2005
#c 13
#% 262061
#% 268079
#% 290830
#% 309151
#% 330676
#% 330766
#% 340919
#% 348173
#% 387427
#% 590523
#% 1656296
#! Different from traditional information retrieval, both content and structure are critical to the success of Web information retrieval. In recent years, many relevance propagation techniques have been proposed to propagate content information between web pages through web structure to improve the performance of web search. In this paper, we first propose a generic relevance propagation framework, and then provide a comparison study on the effectiveness and efficiency of various representative propagation models that can be derived from this generic framework. We come to many conclusions that are useful for selecting a propagation model in real-world search applications, including 1) sitemap-based propagation models outperform hyperlink-based models in sense of both effectiveness and efficiency, and 2) sitemap-based term propagation is easier to be integrated into real-world search engines because of its parallel offline implementation and acceptable complexity. Some other more detailed study results are also reported in the paper.

#index 818255
#* Relevance weighting for query independent evidence
#@ Nick Craswell;Stephen Robertson;Hugo Zaragoza;Michael Taylor
#t 2005
#c 13
#% 211820
#% 218982
#% 268079
#% 309151
#% 397126
#% 510723
#% 577339
#% 641770
#% 642982
#% 766416
#% 766462
#% 783474
#! A query independent feature, relating perhaps to document content, linkage or usage, can be transformed into a static, per-document relevance weight for use in ranking. The challenge is to find a good function to transform feature values into relevance scores. This paper presents FLOE, a simple density analysis method for modelling the shape of the transformation required, based on training data and without assuming independence between feature and baseline. For a new query independent feature, it addresses the questions: is it required for ranking, what sort of transformation is appropriate and, after adding it, how successful was the chosen transformation? Based on this we apply sigmoid transformations to PageRank, indegree, URL Length and ClickDistance, tested in combination with a BM25 baseline.

#index 818256
#* Detecting dominant locations from search queries
#@ Lee Wang;Chuang Wang;Xing Xie;Josh Forman;Yansheng Lu;Wei-Ying Ma;Ying Li
#t 2005
#c 13
#% 375017
#% 480467
#% 730051
#% 742162
#% 757661
#% 766441
#% 815280
#% 815922
#% 854802
#% 855310
#! Accurately and effectively detecting the locations where search queries are truly about has huge potential impact on increasing search relevance. In this paper, we define a search query's dominant location (QDL) and propose a solution to correctly detect it. QDL is geographical location(s) associated with a query in collective human knowledge, i.e., one or few prominent locations agreed by majority of people who know the answer to the query. QDL is a subjective and collective attribute of search queries and we are able to detect QDLs from both queries containing geographical location names and queries not containing them. The key challenges to QDL detection include false positive suppression (not all contained location names in queries mean geographical locations), and detecting implied locations by the context of the query. In our solution, a query is recursively broken into atomic tokens according to its most popular web usage for reducing false positives. If we do not find a dominant location in this step, we mine the top search results and/or query logs (with different approaches discussed in this paper) to discover implicit query locations. Our large-scale experiments on recent MSN Search queries show that our query location detection solution has consistent high accuracy for all query frequency ranges.

#index 818257
#* When will information retrieval be "good enough"?
#@ James Allan;Ben Carterette;Joshua Lewis
#t 2005
#c 13
#% 1358
#% 67565
#% 152836
#% 154991
#% 169809
#% 214709
#% 232677
#% 340957
#% 766409
#% 766525
#% 791716
#! We describe a user study that examined the relationship between the quality of an Information Retrieval system and the effectiveness of its users in performing a task. The task involves finding answer facets of questions pertaining to a collection of newswire documents over a six month period. We artificially created sets of ranked lists at increasing levels of quality by blending the output of a state-of-the-art retrieval system with truth data created by annotators. Subjects performed the task by using these ranked lists to guide their labeling of answer passages in the retrieved articles. We found that as system accuracy improves, subject time on task and error rate decrease, and the rate of finding new correct answers increases. There is a large intermediary region in which the utility difference is not significant; our results suggest that there is some threshold of accuracy for this task beyond which user utility improves rapidly, but more experiments are needed to examine the area around that threshold closely.

#index 818258
#* Modeling task-genre relationships for IR in the workplace
#@ Luanne Freund;Elaine G. Toms;Charles L.A. Clarke
#t 2005
#c 13
#% 312701
#% 413757
#% 608365
#% 723328
#% 764564
#% 768898
#% 787547
#! Context influences the search process, but to date research has not definitively identified which aspects of context are the most influential for information retrieval, and thus are worthy of integration in today's retrieval systems. In this research, we isolated for examination two aspects of context: task and document genre and examined the relationship between them within a software engineering work domain. In this domain, the nature of the task has an impact on decisions of relevance and usefulness, and the document collection contains a distinctive set of genre. Our data set was a document repository created and used by our target population. The document surrogates were meta-tagged by purpose and document type. Correspondence analysis of this categorical data identified some specific relationships between genres and tasks, as well as four broad dimensions of variability underlying these relationships. These results have the potential to inform the design of a contextual retrieval system by refining search results for this domain.

#index 818259
#* Personalizing search via automated analysis of interests and activities
#@ Jaime Teevan;Susan T. Dumais;Eric Horvitz
#t 2005
#c 13
#% 169803
#% 214709
#% 309095
#% 309792
#% 399057
#% 413615
#% 577329
#% 614050
#% 642983
#% 643028
#% 731615
#% 742666
#% 751830
#% 754126
#% 771571
#% 1710964
#! We formulate and study search algorithms that consider a user's prior interactions with a wide variety of content to personalize that user's current Web search. Rather than relying on the unrealistic assumption that people will precisely specify their intent when searching, we pursue techniques that leverage implicit information about the user's interests. This information is used to re-rank Web search results within a relevance feedback framework. We explore rich models of user interests, built from both search-related information, such as previously issued queries and previously visited Web pages, and other information about the user such as documents and email the user has read and created. Our research suggests that rich representations of the user and the corpus are important for personalization, but that it is possible to approximate these representations and provide efficient client-side algorithms for personalizing search. We show that such personalization algorithms can significantly improve on current Web search.

#index 818260
#* The loquacious user: a document-independent source of terms for query expansion
#@ Diane Kelly;Vijay Deepak Dollu;Xin Fu
#t 2005
#c 13
#% 54435
#% 86528
#% 144076
#% 169806
#% 214709
#% 232719
#% 262067
#% 306468
#% 308745
#% 329090
#% 397161
#% 642985
#% 643000
#% 643001
#% 766525
#! In this paper we investigate the effectiveness of a document-independent technique for eliciting feedback from users about their information problems. We propose that such a technique can be used to elicit terms from users for use in query expansion and as a follow-up when ambiguous queries are initially posed by users. We design a feedback form to obtain additional information from users, administer the form to users after initial querying, and create a series of experimental runs based on the information that we obtained from the form. Results demonstrate that the form was successful at eliciting more information from users and that this additional information significantly improved retrieval performance. Our results further demonstrate a strong relationship between query length and performance.

#index 818261
#* A study of the dirichlet priors for term frequency normalisation
#@ Ben He;Iadh Ounis
#t 2005
#c 13
#% 218982
#% 340948
#% 375017
#% 411760
#% 748738
#% 1715606
#! In Information Retrieval (IR), the Dirichlet Priors have been applied to the smoothing technique of the language modeling approach. In this paper, we apply the Dirichlet Priors to the term frequency normalisation of the classical BM25 probabilistic model and the Divergence from Randomness PL2 model. The contributions of this paper are twofold. First, through extensive experiments on four TREC collections, we show that the newly generated models, to which the Dirichlet Priors normalisation is applied, provide robust and effective performance. Second, we propose a novel theoretically-driven approach to the automatic parameter tuning of the Dirichlet Priors normalisation. Experiments show that this tuning approach optimises the retrieval performance of the newly generated Dirichlet Priors-based weighting models.

#index 818262
#* A Markov random field model for term dependencies
#@ Donald Metzler;W. Bruce Croft
#t 2005
#c 13
#% 35937
#% 46803
#% 109190
#% 111303
#% 120104
#% 157910
#% 262096
#% 287253
#% 340948
#% 397205
#% 411760
#% 413593
#% 464434
#% 577224
#% 649567
#% 766414
#% 766428
#% 789959
#% 827581
#% 1715627
#! This paper develops a general, formal framework for modeling term dependencies via Markov random fields. The model allows for arbitrary text features to be incorporated as evidence. In particular, we make use of features based on occurrences of single terms, ordered phrases, and unordered phrases. We explore full independence, sequential dependence, and full dependence variants of the model. A novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data. Ad hoc retrieval experiments are presented on several newswire and web collections, including the GOV2 collection used at the TREC 2004 Terabyte Track. The results show significant improvements are possible by modeling dependencies, especially on the larger web collections.

#index 818263
#* An exploration of axiomatic approaches to information retrieval
#@ Hui Fang;ChengXiang Zhai
#t 2005
#c 13
#% 46803
#% 67565
#% 111303
#% 120104
#% 169741
#% 169781
#% 218982
#% 253191
#% 262037
#% 262096
#% 340948
#% 342819
#% 384634
#% 406493
#% 766412
#! Existing retrieval models generally do not offer any guarantee for optimal retrieval performance. Indeed, it is even difficult, if not impossible, to predict a model's empirical performance analytically. This limitation is at least partly caused by the way existing retrieval models are developed where relevance is only coarsely modeled at the level of documents and queries as opposed to a finer granularity level of terms. In this paper, we present a new axiomatic approach to developing retrieval models based on direct modeling of relevance with formalized retrieval constraints defined at the level of terms. The basic idea of this axiomatic approach is to search in a space of candidate retrieval functions for one that can satisfy a set of reasonable retrieval constraints. To constrain the search space, we propose to define a retrieval function inductively and decompose a retrieval function into three component functions. Inspired by the analysis of the existing retrieval functions with the inductive definition, we derive several new retrieval functions using the axiomatic retrieval framework. Experiment results show that the derived new retrieval functions are more robust and less sensitive to parameter settings than the existing retrieval functions with comparable optimal performance.

#index 818264
#* Gravitation-based model for information retrieval
#@ Shuming Shi;Ji-Rong Wen;Qing Yu;Ruihua Song;Wei-Ying Ma
#t 2005
#c 13
#% 120104
#% 169781
#% 169811
#% 176530
#% 218982
#% 248058
#% 262069
#% 262096
#% 287253
#% 321635
#% 340948
#% 387427
#% 389801
#% 411760
#% 448725
#% 642992
#% 719598
#% 766412
#% 783474
#! This paper proposes GBM (gravitation-based model), a physical model for information retrieval inspired by Newton's theory of gravitation. A mapping is built in this model from concepts of information retrieval (documents, queries, relevance, etc) to those of physics (mass, distance, radius, attractive force, etc). This model actually provides a new perspective on IR problems. A family of effective term weighting functions can be derived from it, including the well-known BM25 formula. This model has some advantages over most existing ones: First, because it is directly based on basic physical laws, the derived formulas and algorithms can have their explicit physical interpretation. Second, the ranking formulas derived from this model satisfy more intuitive heuristics than most of existing ones, thus have the potential to behave empirically better and to be used safely on various settings. Finally, a new approach for structured document retrieval derived from this model is more reasonable and behaves better than existing ones.

#index 818265
#* Impedance coupling in content-targeted advertising
#@ Berthier Ribeiro-Neto;Marco Cristo;Paulo B. Golgher;Edleno Silva de Moura
#t 2005
#c 13
#% 111303
#% 169718
#% 209706
#% 219047
#% 246087
#% 281156
#% 348135
#% 370075
#% 387427
#% 411248
#% 617186
#% 725236
#! The current boom of the Web is associated with the revenues originated from on-line advertising. While search-based advertising is dominant, the association of ads with a Web page (during user navigation) is becoming increasingly important. In this work, we study the problem of associating ads with a Web page, referred to as content-targeted advertising, from a computer science perspective. We assume that we have access to the text of the Web page, the keywords declared by an advertiser, and a text associated with the advertiser's business. Using no other information and operating in fully automatic fashion, we propose ten strategies for solving the problem and evaluate their effectiveness. Our methods indicate that a matching strategy that takes into account the semantics of the problem (referred to as AAK for "ads and keywords") can yield gains in average precision figures of 60% compared to a trivial vector-based strategy. Further, a more sophisticated impedance coupling strategy, which expands the text of the Web page to reduce vocabulary impedance with regard to an advertisement, can yield extra gains in average precision of 50%. These are first results. They suggest that great accuracy in content-targeted advertising can be attained with appropriate algorithms.

#index 818266
#* Improving web search results using affinity graph
#@ Benyu Zhang;Hua Li;Yi Liu;Lei Ji;Wensi Xi;Weiguo Fan;Zheng Chen;Wei-Ying Ma
#t 2005
#c 13
#% 249110
#% 262112
#% 290830
#% 309141
#% 387427
#% 448843
#% 482659
#% 642975
#% 642981
#% 754089
#! In this paper, we propose a novel ranking scheme named Affinity Ranking (AR) to re-rank search results by optimizing two metrics: (1) diversity -- which indicates the variance of topics in a group of documents; (2) information richness -- which measures the coverage of a single document to its topic. Both of the two metrics are calculated from a directed link graph named Affinity Graph (AG). AG models the structure of a group of documents based on the asymmetric content similarities between each pair of documents. Experimental results in Yahoo! Directory, ODP Data, and Newsgroup data demonstrate that our proposed ranking algorithm significantly improves the search performance. Specifically, the algorithm achieves 31% improvement in diversity and 12% improvement in information richness relatively within the top 10 search results.

#index 818267
#* Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval
#@ Elad Yom-Tov;Shai Fine;David Carmel;Adam Darlow
#t 2005
#c 13
#% 194246
#% 280856
#% 393059
#% 397159
#% 397161
#% 400847
#% 577224
#% 729437
#% 750769
#% 766408
#% 766497
#% 766525
#! In this article we present novel learning methods for estimating the quality of results returned by a search engine in response to a query. Estimation is based on the agreement between the top results of the full query and the top results of its sub-queries. We demonstrate the usefulness of quality estimation for several applications, among them improvement of retrieval, detecting queries for which no relevant content exists in the document collection, and distributed information retrieval. Experiments on TREC data demonstrate the robustness and the effectiveness of our learning algorithms.

#index 818268
#* Iterative translation disambiguation for cross-language information retrieval
#@ Christof Monz;Bonnie J. Dorr
#t 2005
#c 13
#% 16220
#% 78171
#% 165307
#% 236052
#% 262046
#% 279755
#% 316881
#% 375017
#% 397146
#% 420472
#% 561328
#% 732845
#% 735137
#% 740900
#% 747988
#% 748738
#% 786536
#% 817459
#% 843645
#! Finding a proper distribution of translation probabilities is one of the most important factors impacting the effectiveness of a cross-language information retrieval system. In this paper we present a new approach that computes translation probabilities for a given query by using only a bilingual dictionary and a monolingual corpus in the target language. The algorithm combines term association measures with an iterative machine learning approach based on expectation maximization. Our approach considers only pairs of translation candidates and is therefore less sensitive to data-sparseness issues than approaches using higher n-grams. The learned translation probabilities are used as query term weights and integrated into a vector-space retrieval system. Results for English-German cross-lingual retrieval show substantial improvements over a baseline using dictionary lookup without term weighting.

#index 818269
#* Bootstrapping dictionaries for cross-language information retrieval
#@ Kornél Markó;Stefan Schulz;Olena Medelyan;Udo Hahn
#t 2005
#c 13
#% 169777
#% 262048
#% 420520
#% 642979
#% 747947
#% 766425
#% 766426
#% 766427
#% 786574
#% 815319
#% 854181
#% 854571
#! The bottleneck for dictionary-based cross-language information retrieval is the lack of comprehensive dictionaries, in particular for many different languages. We here introduce a methodology by which multilingual dictionaries (for Spanish and Swedish) emerge automatically from simple seed lexicons. These seed lexicons are automatically generated, by cognate mapping, from (previously manually constructed) Portuguese and German as well as English sources. Lexical and semantic hypotheses are then validated and new ones iteratively generated by making use of co-occurrence patterns of hypothesized translation synonyms in parallel corpora. We evaluate these newly derived dictionaries on a large medical document collection within a cross-language retrieval setting.

#index 818270
#* A maximum coherence model for dictionary-based cross-language information retrieval
#@ Yi Liu;Rong Jin;Joyce Y. Chai
#t 2005
#c 13
#% 218988
#% 232656
#% 316881
#% 340895
#% 340901
#% 397144
#% 397145
#% 397146
#% 420077
#% 420472
#% 561304
#% 735135
#% 786536
#! One key to cross-language information retrieval is how to efficiently resolve the translation ambiguity of queries given their short length. This problem is even more challenging when only bilingual dictionaries are available, which is the focus of this paper. In the previous research of cross-language information retrieval using bilingual dictionaries, the word co-occurrence statistics is used to determine the most likely translations of queries. In this paper, we propose a novel statistical model, named ``maximum coherence model'', which estimates the translation probabilities of query words that are consistent with the word co-occurrence statistics. Unlike the previous work, where a binary decision is made for the selection of translations, the new model maintains the uncertainty in translating query words when their sense ambiguity is difficult to resolve. Furthermore, this new model is able to estimate translations of multiple query words simultaneously. This is in contrast to many previous approaches where translations of individual query words are determined independently. Empirical studies with TREC datasets have shown that the maximum coherence model achieves a relative 10% - 40% improvement in cross-language information retrieval, comparing to other approaches that also use word co-occurrence statistics for sense disambiguation.

#index 818271
#* Hidden Markov models for automatic annotation and content-based retrieval of images and video
#@ Arnab Ghoshal;Pavel Ircing;Sanjeev Khudanpur
#t 2005
#c 13
#% 457912
#% 642989
#% 642990
#% 721163
#% 722927
#% 1502531
#! This paper introduces a novel method for automatic annotation of images with keywords from a generic vocabulary of concepts or objects for the purpose of content-based image retrieval. An image, represented as sequence of feature-vectors characterizing low-level visual features such as color, texture or oriented-edges, is modeled as having been stochastically generated by a hidden Markov model, whose states represent concepts. The parameters of the model are estimated from a set of manually annotated (training) images. Each image in a large test collection is then automatically annotated with the a posteriori probability of concepts present in it. This annotation supports content-based search of the image-collection via keywords. Various aspects of model parameterization, parameter estimation, and image annotation are discussed. Empirical retrieval results are presented on two image-collections | COREL and key-frames from TRECVID. Comparisons are made with two other recently developed techniques on the same datasets.

#index 818272
#* Evaluating the impact of selection noise in community-based web search
#@ Oisín Boydell;Barry Smyth;Cathal Gurrin;Alan F. Smeaton
#t 2005
#c 13
#% 219036
#% 313959
#% 457912
#% 466078
#% 642989
#% 642990
#% 668807
#% 721163
#% 740915
#% 780862
#% 783524
#% 803556
#% 815320
#% 1502531
#% 1715615
#% 1715635
#! The I-SPY meta-search engine uses a technique called collaborative Web search to leverage the past search behaviour (queries and selections) of a community of users in order to promote search results that are relevant to the community. In this paper we describe recent studies to clarify the benefits of this approach in situations when the behaviour of users cannot be relied upon in terms of their ability to consistently select relevant results during search sessions.

#index 818273
#* A database centric view of semantic image annotation and retrieval
#@ Gustavo Carneiro;Nuno Vasconcelos
#t 2005
#c 13
#% 120270
#% 137711
#% 190581
#% 212690
#% 213673
#% 219847
#% 262096
#% 272527
#% 443889
#% 457912
#% 529829
#% 587953
#% 627094
#% 642990
#% 722927
#% 729437
#% 1502499
#% 1502531
#! We introduce a new model for semantic annotation and retrieval from image databases. The new model is based on a probabilistic formulation that poses annotation and retrieval as classification problems, and produces solutions that are optimal in the minimum probability of error sense. It is also database centric, by establishing a one-to-one mapping between semantic classes and the groups of database images that share the associated semantic labels. In this work we show that, under the database centric probabilistic model, optimal annotation and retrieval can be implemented with algorithms that are conceptually simple, computationally efficient, and do not require prior semantic segmentation of training images. Due to its simplicity, the annotation and retrieval architecture is also amenable to sophisticated parameter tuning, a property that is exploited to investigate the role of feature selection in the design of optimal annotation and retrieval systems. Finally, we demonstrate the benefits of simply establishing a one-to-one mapping between keywords and the states of the semantic classification problem over the more complex, and currently popular, joint modeling of keyword and visual feature distributions. The database centric probabilistic retrieval model is compared to existing semantic labeling and retrieval methods, and shown to achieve higher accuracy than the previously best published results, at a fraction of their computational cost.

#index 818274
#* Analysis of factoid questions for effective relation extraction
#@ Eugene Agichtein;Silviu Cucerzan;Eric Brill
#t 2005
#c 13
#% 590523
#% 754068
#% 815868
#% 816175
#% 817419
#% 854668
#! We present an analysis of the structured relationships observed in a randomly sampled set of question-like queries submitted to a search engine for a popular online encyclopedic document collection. Our study shows that a relatively small number of binary relationships account for most of the queries in the sample. This empirically validates an approach of analyzing query logs to identify the relationships most relevant to user needs and populating corresponding fact tables from the collection for factoid question answering. Our analysis shows that such an approach can lead to substantial coverage of user questions.

#index 818275
#* A testbed for people searching strategies in the WWW
#@ Javier Artiles;Julio Gonzalo;Felisa Verdejo
#t 2005
#c 13
#! This paper describes the creation of a testbed to evaluate people searching strategies on the World-Wide-Web. This task involves resolving person names' ambiguity and locating relevant information characterising every individual under the same name.

#index 818276
#* Measure-based metasearch
#@ Javed A. Aslam;Virgiliu Pavlu;Emine Yilmaz
#t 2005
#c 13
#% 232703
#% 413613
#! We propose a simple method for converting many standard measures of retrieval performance into metasearch algorithms. Our focus is both on the analysis of retrieval measures themselves and on the development of new metasearch algorithms. Given the conversion method proposed, our experimental results using TREC data indicate that system-oriented measures of overall retrieval performance (such as average precision) yield good metasearch algorithms whose performance equals or exceeds that of benchmark techniques such as CombMNZ and Condorcet.

#index 818277
#* A geometric interpretation of r-precision and its correlation with average precision
#@ Javed A. Aslam;Emine Yilmaz;Virgiliu Pavlu
#t 2005
#c 13
#% 309093
#! We consider two of the most commonly cited measures of retrieval performance: average precision and R-precision. It is well known that average precision and R-precision are highly correlated and similarly robust measures of performance, though the reasons for this are not entirely clear. In this paper, we give a geometric argument which shows that under a very reasonable set of assumptions, average precision and R-precision both approximate the area under the precision-recall curve, thus explaining their high correlation. We further demonstrate through the use of TREC data that the similarity or difference between average precision and R-precision is largely governed by the adherence to, or violation of, these reasonable assumptions.

#index 818278
#* Probabilistic hyperspace analogue to language
#@ Leif Azzopardi;Mark Girolami;Malcolm Crowe
#t 2005
#c 13
#% 237519
#% 340946
#% 340948
#% 353872
#% 643049
#! Song and Bruza [6] introduce a framework for Information Retrieval(IR) based on Gardenfor's three tiered cognitive model; Conceptual Spaces[4]. They instantiate a conceptual space using Hyperspace Analogue to Language (HAL[3] to generate higher order concepts which are later used for ad-hoc retrieval. In this poster, we propose an alternative implementation of the conceptual space by using a probabilistic HAL space (pHAL). To evaluate whether converting to such an implementation is beneficial we have performed an initial investigation comparing the concept combination of HAL against pHAL for the task of query expansion. Our experiments indicate that pHAL outperforms the original HAL method and that better query term selection methods can improve performance on both HAL and pHAL.

#index 818279
#* Basic issues on the processing of web queries
#@ Claudine Badue;Ramurti Barbosa;Paulo Golgher;Berthier Ribeiro-Neto;Nivio Ziviani
#t 2005
#c 13
#% 249153
#% 290830
#% 578337
#% 728102
#! In this paper we study three basic and key issues related to Web query processing: load balance, broker behavior, and performance by individual index servers. Our study, while preliminary, does reveal interesting tradeoffs: (1) load unbalance at low query arrival rates can be controlled with a simple measure of randomizing the distribution of documents among the index servers, (2) the broker is not a bottleneck, and (3) disk utilization is higher than CPU utilization.

#index 818280
#* An interface to search human movements based on geographic and chronological metadata
#@ Wilma Bainbridge;Ryen W. White;Douglas W. Oard
#t 2005
#c 13
#% 378480
#% 397132
#! Historians and scholars can better understand historic events by studying the geographic and chronological activity of individuals who witnessed them. A lack of adequate tools to help users study these activities can hinder the process of learning and discovery. In this paper we present an interface to address this problem that contains three components: a map, a timeline, and a text representation of a survivor's movements. These components simultaneously provide query input (where users can specify their needs) and dynamic results display (where users can immediately see the effect of their decisions). The results of a pilot study show that users reacted positively to the interface.

#index 818281
#* Automatic web query classification using labeled and unlabeled training data
#@ Steven M. Beitzel;Eric C. Jensen;Ophir Frieder;David Grossman;David D. Lewis;Abdur Chowdhury;Aleksandr Kolcz
#t 2005
#c 13
#% 174315
#% 279755
#% 642982
#% 730051
#! Accurate topical categorization of user queries allows for increased effectiveness, efficiency, and revenue potential in general-purpose web search systems. Such categorization becomes critical if the system is to return results not just from a general web collection but from topic-specific databases as well. Maintaining sufficient categorization recall is very difficult as web queries are typically short, yielding few features per query. We examine three approaches to topical categorization of general web queries: matching against a list of manually labeled queries, supervised learning of classifiers, and mining of selectional preference rules from large unlabeled query logs. Each approach has its advantages in tackling the web query classification recall problem, and combining the three techniques allows us to classify a substantially larger proportion of queries than any of the individual techniques. We examine the performance of each approach on a real web query stream and show that our combined method accurately classifies 46% of queries, outperforming the recall of the best single approach by nearly 20%, with a 7% improvement in overall effectiveness.

#index 818282
#* Surrogate scoring for improved metasearch precision
#@ Steven M. Beitzel;Eric C. Jensen;Ophir Frieder;Abdur Chowdhury;Greg Pass
#t 2005
#c 13
#% 269218
#% 309093
#% 413613
#% 730066
#% 784148
#% 807457
#! We describe a method for improving the precision of metasearch results based upon scoring the visual features of documents' surrogate representations. These surrogate scores are used during fusion in place of the original scores or ranks provided by the underlying search engines. Visual features are extracted from typical search result surrogate information, such as title, snippet, URL, and rank. This approach specifically avoids the use of search engine-specific scores and collection statistics that are required by most traditional fusion strategies. This restriction correctly reflects the use of metasearch in practice, in which knowledge of the underlying search engines' strategies cannot be assumed. We evaluate our approach using a precision-oriented test collection of manually-constructed binary relevance judgments for the top ten results from ten web search engines over 896 queries. We show that our visual fusion approach significantly outperforms the rCombMNZ fusion algorithm by 5.71%, with 99% confidence, and the best individual web search engine by 10.9%, with 99% confidence.

#index 818283
#* Detecting action-items in e-mail
#@ Paul N. Bennett;Jaime Carbonell
#t 2005
#c 13
#% 118731
#% 260001
#% 344447
#% 815166

#index 818284
#* Characterization of a simple case of the reassignment of document identifiers as a pattern sequencing problem
#@ Roi Blanco;Alvaro Barreiro
#t 2005
#c 13
#% 277417
#% 290703
#% 570319
#% 656274
#% 766445
#% 1715618
#! In this poster, we analyze recent work in the document identifiers reassignment problem. After that, we present a formalization of a simple case of the problem as a PSP (Pattern Sequencing Problem). This may facilitate future work as it opens a new research line to solve the general problem.

#index 818285
#* Testing algorithms is like testing students
#@ David Bodoff;Pu Li
#t 2005
#c 13
#% 262105
#% 397163
#! In this paper, we apply methods from educational testing to measure the reliability of an IR collection.

#index 818286
#* Evaluating the impact of selection noise in community-based web search
#@ Oisín Boydell;Barry Smyth;Cathal Gurrin;Alan F. Smeaton
#t 2005
#c 13
#% 803556
#% 1715615
#% 1715635
#! The I-SPY meta-search engine uses a technique called collaborative Web search to leverage the past search behaviour (queries and selections) of a community of users in order to promote search results that are relevant to the community. In this paper we describe recent studies to clarify the benefits of this approach in situations when the behaviour of users cannot be relied upon in terms of their ability to consistently select relevant results during search sessions.

#index 818287
#* Expectation of f-measures: tractable exact computation and some empirical observations of its properties
#@ Kian Ming Adam Chai
#t 2005
#c 13
#% 194284
#% 340904
#% 375017
#% 397135
#! We derive a tractable and exact computation for the expectation of F-measures. We also demonstrate the non-convexity of this expectation, and investigate errors of approximating the expectation under different settings.

#index 818288
#* Search engines and how students think they work
#@ E. N. Efthimiadis;D. G. Hendry
#t 2005
#c 13
#% 214028
#! To investigate the nature of people's understandings for how search engines work, we collected data from 232 undergraduate and graduate students. Students were asked to "draw a labeled sketch of how search engines work." A reference model was constructed and each sketch was analyzed and compared against it for completeness. The paper presents preliminary results and discusses the implications for educational assessment and curriculum design on the one hand, and information system design on the other.

#index 818289
#* On evaluation of adaptive topic tracking systems
#@ Tamer Elsayed;Douglas W. Oard
#t 2005
#c 13
#! Summative evaluation methods for supervised adaptive topic tracking systems convolve the effect of system decisions on present utility with the effect on future utility. This paper describes a new formative evaluation approach that focuses on future utility for use in the design stage of adaptive systems. Topic model quality is assessed at a predefined set of points using a fixed document set to enhance comparability. Experiments using a vector-space topic tracking system illustrate the utility of this approach to formative evaluation.

#index 818290
#* Top subset retrieval on large collections using sorted indices
#@ Paul Ferguson;Alan F. Smeaton;Cathal Gurrin;Peter Wilkins
#t 2005
#c 13
#% 212665
#% 340886
#% 348311
#% 1715629
#! In this poster we describe alternative inverted index structures that reduce the time required to process queries, produce a higher query throughput and still return high quality results to the end user. We give results based upon the TREC Terabyte dataset showing improvements that these indices give in terms of effectiveness and efficiency.

#index 818291
#* Relation between PLSA and NMF and implications
#@ Eric Gaussier;Cyril Goutte
#t 2005
#c 13
#% 457933
#% 458673
#% 643008
#% 938716
#% 1650298
#! Non-negative Matrix Factorization (NMF, [5]) and Probabilistic Latent Semantic Analysis (PLSA, [4]) have been successfully applied to a number of text analysis tasks such as document clustering. Despite their different inspirations, both methods are instances of multinomial PCA [1]. We further explore this relationship and first show that PLSA solves the problem of NMF with KL divergence, and then explore the implications of this relationship.

#index 818292
#* The impact of evaluation on multilingual text retrieval
#@ Julio Gonzalo;Carol Peters
#t 2005
#c 13
#% 732844
#% 1717292
#! We summarize the impact of the first five years of activity of the Cross-Language Evaluation Forum (CLEF) on multilingual text retrieval system performance and show how the CLEF evaluation campaigns have contributed to advances in the state-of-the-art.

#index 818293
#* Using Oracle® for natural language document retrieval an automatic query reformulation approach
#@ Jens Grivolla
#t 2005
#c 13
#! In corporate applications, vast amounts of data are often stored in database systems such as Oracle. Apart from structured information this can include text documents which cannot easily be retrieved using traditional SQL queries.Oracle includes means to deal with full text document retrieval (called Oracle Text) that offer special query operators for searches inside text fields. We have explored the effect of these different operators for queries derived from natural language queries. This article compares the retrieval performances achieved with different automatic reformulations from natural language to Oracle SQL queries.

#index 818294
#* Customizing information access according to domain and task knowledge: the ontoExplo system
#@ Nathalie Hernandez;Josiane Mothe;Sandra Poulain
#t 2005
#c 13
#% 643964
#% 762605
#! In this paper we present a system that allows a user to explore or mine a document collection. This system is based on domain and task knowledge modelled in the form of ontologies and allows direct access both to information as it is stored and to information that is built from it. The system has been developed in Java.

#index 818295
#* Evaluating semantic indexing techniques through cross-language fingerprinting
#@ Eduard Hoenkamp;Sander van Dijk
#t 2005
#c 13
#% 262870
#% 572499
#! Users in search of on-line document sources are usually looking for content, not words. Hence, IR researchers generally agree that search techniques should be geared toward the meaning underlying documents rather than toward the text itself. The most visible examples of such techniques are Latent Semantic Analysis (LSA), and the Hyperspace Analog to Language (HAL). If these techniques really uncover semantic dependencies, then they should be applicable across languages. We investigated this using electronic versions of three kinds of translated material: a novel, a popular treatise about cosmology, and a data base of technical specifications. We used the analogy of fingerprinting used in forensics to establish if individuals are related. Genetic fingerprinting uses enzymes to split the DNA and then compare the resulting band patterns. Likewise, in our research we use queries to split a document into fragments. If a search technique really isolates fragments related to the query, then a document and its translation should have similar band patterns. In this paper we (1) present the fingerprinting technique, (2) introduce the material used, and (3) report preliminary results of an evaluation for two semantic indexing techniques.

#index 818296
#* Live visual relevance feedback for query formulation
#@ Eduard Hoenkamp;Gijs van Dinther
#t 2005
#c 13
#% 572499
#! Users browsing the Internet seem relatively satisfied with the performance of search engines. An optimistic explanation would be the high quality of search engines. A more pessimistic one would be that people just adapt easily to any new technology. A third explanation is people's ignorance about recall: as they simply don't know what relevant documents are missed, they can hardly be expected to worry about them. And so they easily conceive the result as the best they can get. To allow the user to better assess the quality of the search results, an algorithm was developed that computes a visual representation of the document space in the neighborhood of the user's query.The paper (1) outlines the algorithm, (2) shows how users can explore the neighborhood of a query, and (3) demonstrates how users can guess more judiciously whether they need to further elaborate their query to improve retrieval results.

#index 818297
#* A dual index model for contextual information retrieval
#@ Xiangji Huang;Yan Rui Huang;Miao Wen
#t 2005
#c 13
#% 723328
#! In this paper, we propose a dual index model for contextual IR. For each query, we search against both document level and passage level indexes, and use the corresponding merge function to update the weights for both documents and paragraphs by combining the results from both indexes according to the granularity information in metadata. Experiments on 2004 TREC data show that a significant improvement can be made by using the dual index model.

#index 818298
#* Predicting query difficulty on the web by learning visual clues
#@ Eric C. Jensen;Steven M. Beitzel;David Grossman;Ophir Frieder;Abdur Chowdhury
#t 2005
#c 13
#% 397161
#% 807457
#! We describe a method for predicting query difficulty in a precision-oriented web search task. Our approach uses visual features from retrieved surrogate document representations (titles, snippets, etc.) to predict retrieval effectiveness for a query. By training a supervised machine learning algorithm with manually evaluated queries, visual clues indicative of relevance are discovered. We show that this approach has a moderate correlation of 0.57 with precision at 10 scores from manual relevance judgments of the top ten documents retrieved by ten web search engines over 896 queries. Our findings indicate that difficulty predictors which have been successful in recall-oriented ad-hoc search, such as clarity metrics, are not nearly as correlated with engine performance in precision-oriented tasks such as this, yielding a maximum correlation of 0.3. Additionally, relying only on visual clues avoids the need for collection statistics that are required by these prior approaches. This enables our approach to be employed in environments where these statistics are unavailable or costly to retrieve, such as metasearch.

#index 818299
#* Finding semantically similar questions based on their answers
#@ Jiwoon Jeon;W. Bruce Croft;Joon Ho Lee
#t 2005
#c 13
#% 262096
#% 309126
#% 589715
#! A large number of question and answer pairs can be collected from question and answer boards and FAQ pages on the Web. This paper proposes an automatic method of finding the questions that have the same meaning. The method can detect semantically similar questions that have little word overlap because it calculates question-question similarities by using the corresponding answers as well as the questions. We develop two different similarity measures based on language modeling and compare them with the traditional similarity measures. Experimental results show that semantically similar questions pairs can be effectively found with the proposed similarity measures.

#index 818300
#* Study of cross lingual information retrieval using on-line translation systems
#@ Rong Jin;Joyce Y. Chai
#t 2005
#c 13
#% 232656
#% 262047
#% 340895
#% 397144
#% 397145
#% 740915
#! Typical cross language retrieval requires special linguistic resources, such as bilingual dictionaries and parallel corpus. In this study, we focus on the cross lingual retrieval problem that only uses online translation systems. We compare two approaches: a translation-based approach that directly translates queries into the language of documents and then applies traditional information retrieval techniques; and a model-based approach that first learns a statistical translation model from the translations acquired from an online translation system and then applies the learned statistical model to cross lingual information retrieval. Our empirical study with ImageCLEF has shown the model-based approach performs significantly better than the translation-based approach.

#index 818301
#* 3D viewpoint-based photo search and information browsing
#@ Rieko Kadobayashi;Katsumi Tanaka
#t 2005
#c 13
#! We propose a new photo search method that uses three-dimensional (3D) viewpoints as queries. 3D viewpoint-based image retrieval is especially useful for searching collections of archaeological photographs,which contain many different images of the same object. Our method is designed to enable users to retrieve images that contain the same object but show a different view, and to browse groups of images taken from a similar viewpoint. We also propose using 3D scenes to query by example, which means that users do not have the problem of trying to formulate appropriate queries. This combination gives users an easy way of accessing not only photographs but also archived information.

#index 818302
#* Examination and enhancement of a ring-structured graphical search interface based on usability testing
#@ Tomoko Kajiyama;Noriko Kand;Shin'ichi Satoh
#t 2005
#c 13
#% 387427
#% 522728
#! We evaluated the interactive retrieval functionality of the Concentric Ring View according to a series of usability studies. This is a ring structure-based graphical user interface, like a planisphere, for image retrieval with multi-faceted metadata. Attribute values for each facet are arranged on a ring, and retrieved images are displayed inside using search keys derived from the attribute values on the bottom part of the rings. By rotating the rings, users can browse retrieved images while adjusting search keys. The first usability test conducted with thirty six participants confirmed that: (i) novice users, even junior high school students, could use this interface; (ii) users could find images better than anticipated; and (iii) the interface was good at choosing the first relevant image, but users could not refine retrieval because they were unable to reuse retrieved results. To solve this problem, we added two functionalities, personal history for reuse and relevance feedback. With these improvements, we named the new version of the interface Concentric Ring View F+. A second usability test with seven participants confirmed the effectiveness of this newer interface.

#index 818303
#* Short comings of latent models in supervised settings
#@ Vijay Krishnan
#t 2005
#c 13
#% 280819
#% 722904
#% 766422
#% 1650298
#! The Aspect Model [1, 2] and the Latent Dirichlet Allocation Model [3, 4] are latent generative models proposed with the objective of modeling discrete data such as text. Though it is not explicitly published (to the best of our knowledge), it is reasonably well known in there search community that the Aspect Model does not perform very well in supervised settings and also that latent models are frequently not identifiable, i.e. their optimal parameters are not unique.In this paper, we make a much stronger claim about the pitfalls of commonly-used latent models. By constructing a small, synthetic, but by no means unrealistic corpus, we show that latent models have inherent limitations that prevent them from recovering semantically meaningful parameters from data generated from a reasonable generative distribution. In fact, our experiments with supervised classification using the Aspect Model, showed that its performance was rather poor, even worse than Naive Bayes, leading us to the synthetic study.We also analyze the scenario of using tempered EM and show that it would not plug the above shortcomings. Our analysis suggests that there is also some scope for improvement in the Latent Dirichlet Allocation Model(LDA) [3, 4]. We then use our insight into the shortcomings of these models, to come up with a promising variant of the LDA, that does not suffer from the aforesaid drawbacks. This could potentially lead to much better performance and model fit, in the supervised scenario.

#index 818304
#* Major topic detection and its application to opinion summarization
#@ Lun-Wei Ku;Li-Ying Lee;Tung-Ho Wu;Hsin-Hsi Chen
#t 2005
#c 13
#% 309098
#% 577246
#% 577355

#index 818305
#* Using query term order for result summarisation
#@ Shao Fen Liang;Siobhan Devlin;John Tait
#t 2005
#c 13
#! We report on two experiments performed to test the importance of Term Order in automatic summarisation. Experiment one was undertaken as part of DUC 2004 to which three systems were submitted, each with a different summarisation approach. The system that used document Term Order outperformed those that did not use Term Order in the ROUGE evaluation. Experiment two made use of human evaluations of search engine results, comparing our Query Term Order summaries with a simulation of current Google search engine result summaries in terms of summary quality. Our QTO system's summaries aided users' relevance judgements to a significantly greater extent than Google's.

#index 818306
#* Profile-based event tracking
#@ Baoli Li;Wenjie Li;Qin Lu;Mingli Wu
#t 2005
#c 13
#% 350859
#% 730043
#! In this research, we focus on tracking topics that originate and evolve from a specific event. Intuitively, a few key elements of a target event, such as date, location, and persons involved, would be enough for making a decision on whether a test story is on-topic. Consequently, a profile-based event tracking method is proposed. We attempt to build an event profile from the given on-topic stories by robust information retrieval technologies. A feature selection metric and a recognized event clause are utilized to determine most (if not all) key semantic elements of the target event. Preliminary experiments on the TDT2 mandarin corpus show that this profile-based event tracking method is promising.

#index 818307
#* Analysis of recursive feature elimination methods
#@ Fan Li;Yiming Yang
#t 2005
#c 13
#% 425048

#index 818308
#* Assessing the term independence assumption in blind relevance feedback
#@ Jimmy Lin;G. Craig Murray
#t 2005
#c 13
#% 282424
#% 298183
#% 326522
#! When applying blind relevance feedback for ad hoc document retrieval, is it possible to identify, a priori, the set of query terms that will most improve retrieval performance? Can this complex problem be reduced into the simpler one of making independent decisions about the performance effects of each query term? Our experiments suggest that, for the selection of terms for blind relevance feedback, the term independence assumption may be empirically justified.

#index 818309
#* Revisiting the effect of topic set size on retrieval error
#@ Wei-Hao Lin;Alexander Hauptmann
#t 2005
#c 13
#% 309093
#% 397163

#index 818310
#* Information sharing through rational links and viewpoint retrieval
#@ Bicheng Liu;David J. Harper;Stuart Watt
#t 2005
#c 13
#% 429359
#% 723329
#% 766519
#! In this paper we present the concept of Federated Information Sharing Communities (FISC), which leverages organisational and social relationships with document content to provide community-centred information sharing and communication environments. Prominence is given to capabilities that go beyond the generic retrieval of documents to include the ability to retrieve people, their interests and inter-relationships. We focus on providing social awareness "in the large" to help users understand the members within community and the relationships between them. Within the FISC framework, we provide viewpoint retrieval to enable a user to construct member-specific view(s) of the community, based on their various topic interests. As proof of concept, we present the first FISC prototype based on the twenty-five year SIGIR collection and examples of operational results.

#index 818311
#* Mining multimedia salient concepts for incremental information extraction
#@ João Magalhães;Stefan Rüger
#t 2005
#c 13
#% 318785
#% 345829
#% 457912
#% 642989
#% 642991
#% 919561
#! We propose a novel algorithm for extracting information by mining the feature space clusters and then assigning salient concepts to them. Bayesian techniques for extracting concepts from multimedia usually suffer either from lack of data or from too complex concepts to be represented by a single statistical model. An incremental information extraction approach, working at different levels of abstraction, would be able to handle concepts of varying complexities. We present the results of our research on the initial part of an incremental approach, the extraction of the most salient concepts from multimedia information.

#index 818312
#* Translating pieces of words
#@ Paul McNamee;James Mayfield
#t 2005
#c 13
#% 561157
#% 643018
#% 732848
#% 740915
#% 748442
#! Translation for cross-language information retrieval need not be word-based. We show that character n-grams in one language can be 'translated' into character n-grams of another language. We demonstrate that such translations produce retrieval results on par with, and often exceeding, those of word-based and stem-based translation.

#index 818313
#* Cross-language text classification
#@ J. Scott Olsson;Douglas W. Oard;Jan Hajič
#t 2005
#c 13
#% 741900

#index 818314
#* A temporally adaptive content-based relevance ranking algorithm
#@ Jukka Perkiö;Wray Buntine;Henry Tirri
#t 2005
#c 13
#% 340883
#% 766447
#% 779958
#% 788043
#! In information retrieval relevance ranking of the results is one of the most important single tasks there are. There are many diffierent ranking algorithms based on the content of the documents or on some external properties e.g. link structure of html documents.We present a temporally adaptive content-based relevance ranking algorithm that explicitly takes into account the temporal behavior of the underlying statistical properties of the documents in the form of a statistical topic model. more we state that our algorithm can be used on top of any ranking algorithm.

#index 818315
#* Automated evaluation of search engine performance via implicit user feedback
#@ Himanshu Sharma;Bernard J. Jansen
#t 2005
#c 13
#% 731615
#% 731620
#! Measuring the information retrieval effectiveness of Web search engines can be expensive if human relevance judgments are required to evaluate search results. Using implicit user feedback for search engine evaluation provides a cost and time effective manner of addressing this problem. Web search engines can use human evaluation of search results without the expense of human evaluators. An additional advantage of this approach is the availability of real time data regarding system performance. Wecapture user relevance judgments actions such as print, save and bookmark, sending these actions and the corresponding document identifiers to a central server via a client application. We use this implicit feedback to calculate performance metrics, such as precision. We can calculate an overall system performance metric based on a collection of weighted metrics.

#index 818316
#* Dependency relation matching for answer selection
#@ Renxu Sun;Hang Cui;Keya Li;Min-Yen Kan;Tat-Seng Chua
#t 2005
#c 13
#% 818253

#index 818317
#* Using dragpushing to refine centroid text classifiers
#@ Songbo Tan;Xueqi Cheng;Bin Wang;Hongbo Xu;Moustafa M. Ghanem;Yike Guo
#t 2005
#c 13
#% 577232
#! We present a novel algorithm, DragPushing, for automatic text classification. Using a training data set, the algorithm first calculates the prototype vectors, or centroids, for each of the available document classes. Using misclassified examples, it then iteratively refines these centroids; by dragging the centroid of a correct class towards a misclassified example and in the same time pushing the centroid of an incorrect class away from the misclassified example. The algorithm is simple to implement and is computationally very efficient. Evaluation experiments conducted on two benchmark collections show that its classification accuracy is comparable to that of more complex methods, such as support vector machines (SVM).

#index 818318
#* Scalable hierarchical topic detection: exploring a sample based approach
#@ Dolf Trieschnigg;Wessel Kraaij
#t 2005
#c 13
#% 118771
#% 296738
#% 397148
#% 730043
#% 1783133
#! Hierarchical topic detection is a new task in the TDT 2004 evaluation program, which aims to organize an unstructured news collection in a directed acyclic graph (DAG) structure, reflecting the topics discussed. We present a scalable architecture for HTD and compare several alternative choices for agglomerative clustering and DAG optimization in order to minimize the HTD cost metric.

#index 818319
#* Noun sense induction using web search results
#@ Goldee Udani;Shachi Dave;Anthony Davis;Tim Sibley
#t 2005
#c 13
#% 741083
#% 748550
#% 1250281
#! This paper presents an algorithm for unsupervised noun sense induction, based on clustering of Web search results. The algorithm does not utilize labeled training instances or any other external knowledge source. Preliminary results on a small dataset show that this technique provides two advantages over other techniques in the literature: it detects real-world senses not found in dictionaries or other lexical resources, and it does not require that the number of word senses be specified in advance.

#index 818320
#* Self-organizing distributed collaborative filtering
#@ Jun Wang;Marcel J. T. Reinders;Reginald L. Lagendijk;Johan Pouwelse
#t 2005
#c 13
#% 342687
#% 495929
#% 1650569
#! We propose a fully decentralized collaborative filtering approach that is self-organizing and operates in a distributed way. The relevances between downloading files (items) are stored locally at these items in so called item-based buddy tables and are updated each time that the items are downloaded. We then propose to use the language model to build recommendations for the different users based on the buddy tables of those items a user has downloaded previously. We have tested and compared our distributed collaborative filtering approach to centralized collaborative filtering and showed that it has similar performance. It is therefore a promising technique to facilitate recommendations in peer-to-peer networks.

#index 818321
#* Dirichlet PageRank
#@ Xuanhui Wang;Azadeh Shakery;Tao Tao
#t 2005
#c 13
#% 268079
#% 290830
#% 340948
#% 766462
#! PageRank has been known to be a successful algorithm in ranking web sources. In order to avoid the rank sink problem, PageRank assumes that a surfer, being in a page, jumps to a random page with a certain probability. In the standard PageRank algorithm, the jumping probabilities are assumed to be the same for all the pages, regardless of the page properties. This is not the case in the real world, since presumably a surfer would more likely follow the out-links of a high-quality hub page than follow the links of a low-quality one. In this poster, we propose a novel algorithm "Dirichlet PageRank" to address this problem by adapting exible jumping probabilities based on the number of out-links in a page. Empirical results on TREC data show that our method outperforms the standard PageRank algorithm.

#index 818322
#* A retrospective study of probabilistic context-based retrieval
#@ H. C. Wu;R. W. P. Luk;K. F. Wong;K. L. Kwok;W. J. Li
#t 2005
#c 13
#% 169781
#% 194251
#% 253191
#% 262096
#% 280834
#% 292684
#% 643049
#% 643053
#! We propose a novel probabilistic retrieval model which weights terms according to their contexts in documents. The term weighting function of our model is similar to the language model and the binary independence model. The retrospective experiments (i.e., relevance information is present) illustrate the potential of our probabilistic context-based retrieval where the precision at the top 30 documents is about 43% for TREC-6 data and 52% for TREC-7 data.

#index 818323
#* Indexing emails and email threads for retrieval
#@ Yejun Wu;Douglas W. Oard
#t 2005
#c 13
#! Electronic mail poses a number of unusual challenges for the design of information retrieval systems and test collections, including informal expression, conversational structure, variable document granularity (e.g., messages, threads, or longer-term interactions), a naturally occuring integration between free text and structural metadata, and incompletely characterized user needs. This paper reports on initial experiments with a large collection of public mailing lists from the World Wide Web consortium that will be used for the TREC 2005 Enterprise Search Track. Automatic subject-line threading and removal of duplicated text were found to have little effect in a small pilot study. Those observations motivated development of a question typology and more detailed analysis of collection characteristics; preliminary results for both are reported.

#index 818324
#* Intelligent fusion of structural and citation-based evidence for text classification
#@ Baoping Zhang;Yuxin Chen;Weiguo Fan;Edward A. Fox;Marcos Andrép Gonçalves;Marco Cristo;Pàvel Calado
#t 2005
#c 13
#% 124073
#% 169718
#% 740768
#! This paper shows how different measures of similarity derived from the citation information and the structural content (e.g., title, abstract) of the collection can be fused to improve classification effectiveness. To discover the best fusion framework, we apply Genetic Programming (GP) techniques. Our experiments with the ACM Computing Classification Scheme, using documents from the ACM Digital Library, indicate that GP can discover similarity functions superior to those based solely on a single type of evidence. Effectiveness of the similarity functions discovered through simple majority voting is better than that of content-based as well as combination-based Support Vector Machine classifiers. Experiments also were conducted to compare the performance between GP techniques and other fusion techniques such as Genetic Algorithms (GA) and linear fusion. Empirical results show that GP was able to discover better similarity functions than other fusion techniques.

#index 818325
#* Mining translations of OOV terms from the web through cross-lingual query expansion
#@ Ying Zhang;Fei Huang;Stephan Vogel
#t 2005
#c 13
#% 400061
#% 735134
#% 766425
#% 766495
#% 855296
#! Translating out-of-vocabulary (OOV) terms is a great challenge for the Cross-lingual Information Retrieval and Data-driven Machine Translation systems. Several approaches have been proposed to mine translations for OOV terms from the web, especially from pages containing mixed languages. In this paper, we propose a novel approach to automatically translate OOV terms on the fly through cross-lingual query expansion. The proposed approach does not require any web crawling and has achieved an inclusion rate of 95% and overall translation accuracy of 90%, outperforming state-of-the-art OOV translation techniques.

#index 818326
#* On redundancy of training corpus for text categorization: a perspective of geometry
#@ Shuigeng Zhou;Jihong Guan
#t 2005
#c 13
#% 262050
#% 280817
#% 642986
#% 642987

#index 818327
#* An industrial-strength content-based music recommendation system
#@ Pedro Cano;Markus Koppenberger;Nicolas Wack
#t 2005
#c 13
#! We present a metadata free system for the interaction with massive collections of music, the MusicSurfer. MusicSurfer automatically extracts descriptions related to instrumentation, rhythm and harmony from music audio signals. Together with efficient similarity metrics, the descriptions allow navigation of multimillion track music collections in a flexible and efficient way without the need of metadata or human ratings.

#index 818328
#* SPIN: searching personal information networks
#@ Soumen Chakrabarti;Jeetendra Mirchandani;Arnab Nandi
#t 2005
#c 13
#% 451595
#% 642983
#% 660011
#% 788107

#index 818329
#* A CLIR interface to a web search engine
#@ Philipp Daumke;Stefan Schulz;Kornél Markó
#t 2005
#c 13
#% 329775

#index 818330
#* Music-to-knowledge (M2K): a prototyping and evaluation environment for music information retrieval research
#@ J. Stephen Downie;Andreas F. Ehmann;David Tcheng
#t 2005
#c 13

#index 818331
#* A wireless natural language search engine
#@ Jochen L. Leidner
#t 2005
#c 13
#% 475237
#! Web search using stationary (desktop) computers has become a pervasive activity. The mobile user in need of information, however, faces several problems in his or her quest to satisfy an information need. Mobile devices have small displays, and mobile user interfaces are often less then usable, because they impose the desktop Web search paradigm on the mobile user. We present a wireless search engine based on natural language queries transmitted via popular Small Message Service (SMS) text messages. Besides traditional keyword based queries, the system can accept questions or phrases and returns responses that contain likely answers (Figure 1) instead of traditional lists of hyperlinks. The additional precision gained from performing a linguistic analysis of the query helps extracting answers from Web pages directly, which requires no navigation. The system is implemented using a NLIR system residing on a server, which can translate questions or phrases into search engine queries or queries to SOAP Web services, where a gateway mediates between the mobile network and the Internet (Figure 2). Whereas on the desktop keyboard-based search still prevails, we find that in a mobile context question answering techniques can help overcome the output constraints.

#index 818332
#* The recap system for identifying information flow
#@ Donald Metzler;Yaniv Bernstein;W. Bruce Croft;Alistair Moffat;Justin Zobel
#t 2005
#c 13

#index 818333
#* Hierarchical text summarization for WAP-enabled mobile devices
#@ Dragomir Radev;Omer Kareem;Jahna Otterbacher
#t 2005
#c 13
#% 342962
#% 449746
#! We present WAP MEAD, a WAP-enabled text summarization system. It incorporates a state-of-the art text summarizer enhanced to produce hierarchical summaries that are appropriate for various types of mobile devices, including cellular phones.

#index 818334
#* Manjal: a text mining system for MEDLINE
#@ Aditya Kumar Sehgal;Padmini Srinivasan
#t 2005
#c 13

#index 818335
#* UCAIR: a personalized search toolbar
#@ Xuehua Shen;Bin Tan;ChengXiang Zhai
#t 2005
#c 13
#% 577224
#% 731615
#% 754126
#% 818207

#index 818336
#* A web mining research platform
#@ David Sherfesee;Niall O'Driscoll
#t 2005
#c 13
#! We demonstrate the Alexa Web Mining Platform, a data mining and web service publication platform designed to enable analysis of Alexa's massive web data store. The system provides researchers and developers high speed access to our web crawl, crawl metadata, long term storage, and data publication utilities. We demonstrate the system's capabilities and user interface.

#index 818337
#* Multi-faceted information retrieval system for large scale email archives
#@ Ville H. Tuulos;Jukka Perkiö;Henry Tirri
#t 2005
#c 13
#% 779958
#% 788043

#index 879562
#* Quantum haystacks
#@ C. J. 'Keith' van Rijsbergen
#t 2006
#c 13
#% 797994
#! This acceptance talk is a curious mixture of personal history and developing ideas in the context of the growing field of IR covering several decades. I want to concentrate on models and theories, interpreted loosely, and try and give an insight into where I have got to in my thinking, where the ideas came from, and where I believe we are going.In the last few years I have been working on the development of what might be coined as a design language for IR. It takes its inspiration from Quantum Mechanics, but by analogy only. The mathematical objects represent documents; these objects might be vectors (or density operators) in an n-dimensional vector space (usually a Hilbert space). A request for information, or a query, is taken as an observable and is represented as a linear operator on the space. Linear operators can be expressed as matrices. Such an operator, Hermitian, has a set of eigenvectors forming a basis for the space; which we interpret as a point of view or perspective from which to understand the space. Thus any document-vector can be located with respect to the basis, and we can calculate an inner product between such a vector and any basis vector, which may be interpreted as a probability of relevance. The probability of observing any given eigenvector is now given by the square of that inner product assuming all vectors are normalised. Hence we connect the probability of observation to the geometry of the space. Furthermore, the subspaces of the space make up a lattice structure which is equivalent to a logic. This makes up the entire mathematical structure, and the language for handling this structure is linear algebra: vectors, matrices, projections, inner-products, neatly captured by the Dirac notation used in quantum mechanics. Our probability is slightly different from classical probability, the same for logic; we end up with quantum logic and quantum probability.A commitment to this kind of mathematical structure, with which to model objects and processes in IR, depends on two critical assumptions.The distances in the space between objects are a source of important relationships with respect to relevance and aboutness.The observation of a property such as relevance or aboutness is user dependent in the sense that a potential interaction is specified by a user through an operator which when measured achieves outcomes with a probability determined by the geometry of the space..The geometry of this mathematical structure and the probability defined on it are closely connected by the following theorem due to Gleason (1957). One may summarise this theorem by saying that the probability of a subspace is given by a simple algorithm derived from a projection onto the subspace and a special kind of operator, namely a statistical operator, or density matrix. And conversely, that given a probability measure on the subspaces then we can encode that measure uniquely through such an algorithm. This is a very powerful theorem and its consequences remain to be explored.So how did I get to this point and form of abstraction? Most of my research work can be divided into contributions to the following areas:ClusteringEvaluationProbabilistic ModelsLogic ModelsGeometry.In all these areas I have attempted to search for underlying mathematical structures that would lead to computations. These topics have in common that they depend on the construction of measures on a space which in some sense determines the usefulness or effectiveness of the structure. For clustering one considers mapping from metric spaces to ultrametic spaces and measure the closeness of fit. In the case of evaluation, one starts with a relational conjoint structure and imposes some constraints given by what is to be measured, one then constructs a numerical representation of this structure leading to such measures as F (or E). For probabilistic models the main difficulty is concerned with deciding on an appropriate event space on which to define the 'right' probability measures. For me the most significant example in this context was the attempt to construct a Logical Uncertainty Principle which formulated a measure of uncertainty on incomplete logical constructs. This attempt left unspecified the exact form of the measure. In the Geometry of IR I finally managed to formulate that measure as a projection-valued measure.This way of thinking did not appear out of nowhere. It was heavily influenced by the work of Fairthorne(1961) whose work on Brouwerian Logic (an Intuitionistic Logic) was picked up by Salton in his early book on IR. At an earlier stage MacKay (1950) wrote a paper that opened with, 'This paper relates to the borderline linking experimental and theoretical physics with mathematical logic, and covers at several points ground which is common to the theory of communication.' He goes on to define an 'information-operator' which is very similar in scope and intent to the Hermitian operator above. Maron, who collaborated with MacKay, stated in his 1965 paper, 'Therefore, it can be argued that index descriptions should not be viewed as properties of documents: They function to relate documents and users.' One can see that the development of these early ideas was continued to the construction of the Geometry of IR.What does it leave to be done? An attempt should be made to use this design language to build an IR system. On the theoretical front it is worth considering whether it would be better to start with a transition probability space rather than a Hilbert space as Von Neumann did in 1937 (translated in 1981). The assumption that closed linear subspaces will be the elements of our logic can be challenged, as perhaps a construction with different elements is possible. It is not obvious what the best form of conditional probability might be in these spaces. Agreeing on a form of conditionalisation is intimately tied up with how to model contextuality. There is some evidence to suggest that contextuality plays a role in modelling the conjuncton of concepts (Widdows, 2004). Such contexts have been modelled in quantum theory almost from the beginning, for example, Gleason's theorem precludes noncontextual hidden variable theories.

#index 879563
#* Social networks, incentives, and search
#@ Jon Kleinberg
#t 2006
#c 13
#% 220708
#% 300078
#% 337235
#% 338466
#% 344448
#% 447946
#% 451536
#% 636008
#% 643087
#% 752206
#% 765248
#% 818212
#% 836506
#% 1289399
#% 1715596
#% 1768420
#! The role of network structure has grown in significance over the past ten years in the field of information retrieval, stimulated to a great extent by the importance of link analysis in the development of Web search techniques [4]. This body of work has focused primarily on the network that is most clearly visible on the Web: the network of hyperlinks connecting documents to documents. But the Web has always contained a second network, less explicit but equally important, and this is the social network on its users, with latent person-to-person links encoding a variety of relationships including friendship, information exchange, and influence. Developments over the past few years --- including the emergence of social networking systems and rich social media, as well as the availability of large-scale e-mail and instant messenging datasets --- have highlighted the crucial role played by on-line social networks, and at the same time have made them much easier to uncover and analyze. There is now a considerable opportunity to exploit the information content inherent in these networks, and this prospect raises a number of interesting research challenge.Within this context, we focus on some recent efforts to formalize the problem of searching a social network. The goal is to capture the issues underlying a variety of related scenarios: a member of a social networking system such as MySpace seeks a piece of information that may be held by a friend of a friend [27, 28]; an employee in a large company searches his or her network of colleagues for expertise in a particular subject [9]; a node in a decentralized peer-to-peer file-sharing system queries for a file that is likely to be a small number of hops away [2, 6, 16, 17]; or a user in a distributed IR or federated search setting traverses a network of distributed resources connected by links that may not just be informational but also economic or contractual [3, 5, 7, 8, 13, 18, 21]. In their most basic forms, these scenarios have some essential features in common: a node in a network, without global knowledge, must find a short path to a desired "target" node (or to one of several possible target nodes).To frame the underlying problem, we go back to one of the most well-known pieces of empirical social network analysis --- Stanley Milgram's research into the small-world phenomenon, also known as the "six degrees of separation" [19, 24, 25]. The form of Milgram's experiments, in which randomly chosen starters had to forward a letter to a designated target individual, established not just that short chains connecting far-flung pairs of people are abundant in large social networks, but also that the individuals in these networks, operating with purely local information about their own friends and acquaintances, are able to actually find these chains [10]. The Milgram experiments thus constituted perhaps the earliest indication that large-scale social networks are structured to support this type of decentralized search. Within a family of random-graph models proposed by Watts and Strogatz [26], we have shown that the ability of a network to support this type of decentralized search depends in subtle ways on how its "long-range" connections are correlated with the underlying spatial or organizational structure in which it is embedded [10, 11]. Recent studies using data on communication within organizations [1] and the friendships within large on-line communities [15] have established the striking fact that real social networks closely match some of the structural features predicted by these mathematical models.If one looks further at the on-line settings that provide the initial motivation for these issues, there is clearly interest from many directions in their long-term economic implications --- essentially, the consequences that follow from viewing distributed information retrieval applications, peer-to-peer systems, or social-networking sites as providing marketplaces for information and services. How does the problem of decentralized search in a network change when the participants are not simply agents following a fixed algorithm, but strategic actors who make decisions in their own self-interest, and may demand compensation for taking part in a protocol? Such considerations bring us into the realm of algorithmic game theory, an active area of current research that uses game-theoretic notions to quantify the performance of systems in which the participants follow their own self-interest [20, 23] In a simple model for decentralized search in the presence of incentives, we find that performance depends crucially on both the rarity of the information and the richness of the network topology [12] --- if the network is too structurally impoverished, an enormous investment may be required to produce a path from a query to an answer.

#index 879564
#* Information retrieval at Boeing: plans and successes
#@ Radha Radhakrishnan
#t 2006
#c 13

#index 879565
#* Learning user interaction models for predicting web search result preferences
#@ Eugene Agichtein;Eric Brill;Susan Dumais;Robert Ragno
#t 2006
#c 13
#% 169803
#% 220711
#% 268079
#% 269217
#% 406493
#% 433965
#% 577224
#% 731615
#% 805200
#% 818221
#% 823348
#% 840846
#% 879567
#! Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance. We present a real-world study of modeling the behavior of web search users to predict web search result preferences. Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks. Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected "noisy" user behavior. We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods. We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone. We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.

#index 879566
#* User performance versus precision measures for simple search tasks
#@ Andrew Turpin;Falk Scholer
#t 2006
#c 13
#% 208931
#% 248065
#% 262036
#% 262102
#% 309089
#% 309093
#% 312689
#% 340921
#% 561315
#% 590523
#% 729027
#% 766409
#% 818221
#% 818257
#% 857180
#! Several recent studies have demonstrated that the type of improvements in information retrieval system effectiveness reported in forums such as SIGIR and TREC do not translate into a benefit for users. Two of the studies used an instance recall task, and a third used a question answering task, so perhaps it is unsurprising that the precision based measures of IR system effectiveness on one-shot query evaluation do not correlate with user performance on these tasks. In this study, we evaluate two different information retrieval tasks on TREC Web-track data: a precision-based user task, measured by the length of time that users need to find a single document that is relevant to a TREC topic; and, a simple recall-based task, represented by the total number of relevant documents that users can identify within five minutes. Users employ search engines with controlled mean average precision (MAP) of between 55% and 95%. Our results show that there is no significant relationship between system effectiveness measured by MAP and the precision-based task. A significant, but weak relationship is present for the precision at one document returned metric. A weak relationship is present between MAP and the simple recall-based task.

#index 879567
#* Improving web search ranking by incorporating user behavior information
#@ Eugene Agichtein;Eric Brill;Susan Dumais
#t 2006
#c 13
#% 169803
#% 220711
#% 268079
#% 269217
#% 309095
#% 387427
#% 406493
#% 433965
#% 577224
#% 731615
#% 773037
#% 783474
#% 783482
#% 805200
#% 818221
#% 823348
#% 840846
#% 879565
#! We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting. We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features. We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine. We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.

#index 879568
#* Contextual search and name disambiguation in email using graphs
#@ Einat Minkov;William W. Cohen;Andrew Y. Ng
#t 2006
#c 13
#% 54413
#% 165111
#% 230530
#% 230532
#% 302391
#% 314740
#% 577273
#% 719598
#% 741058
#% 743922
#% 770864
#% 805896
#% 815924
#% 818218
#% 818241
#% 818246
#% 838532
#% 839728
#% 840840
#% 844425
#% 858035
#% 939909
#% 1016176
#% 1272396
#% 1289460
#! Similarity measures for text have historically been an important tool for solving information retrieval problems. In many interesting settings, however, documents are often closely connected to other documents, as well as other non-textual objects: for instance, email messages are connected to other messages via header information. In this paper we consider extended similarity metrics for documents and other objects embedded in graphs, facilitated via a lazy graph walk. We provide a detailed instantiation of this framework for email data, where content, social networks and a timeline are integrated in a structural graph. The suggested framework is evaluated for two email-related problems: disambiguating names in email documents, and threading. We show that reranking schemes based on the graph-walk similarity measures often outperform baseline methods, and that further improvements can be obtained by use of appropriate learning methods.

#index 879569
#* Thread detection in dynamic text message streams
#@ Dou Shen;Qiang Yang;Jian-Tao Sun;Zheng Chen
#t 2006
#c 13
#% 66085
#% 67565
#% 120109
#% 262042
#% 309131
#% 430755
#% 453337
#% 643015
#% 766444
#% 766456
#% 779872
#% 805871
#! Text message stream is a newly emerging type of Web data which is produced in enormous quantities with the popularity of Instant Messaging and Internet Relay Chat. It is beneficial for detecting the threads contained in the text stream for various applications, including information retrieval, expert recognition and even crime prevention. Despite its importance, not much research has been conducted so far on this problem due to the characteristics of the data in which the messages are usually very short and incomplete. In this paper, we present a stringent definition of the thread detection task and our preliminary solution to it. We propose three variations of a single-pass clustering algorithm for exploiting the temporal information in the streams. An algorithm based on linguistic features is also put forward to exploit the discourse structure information. We conducted several experiments to compare our approaches with some existing algorithms on a real dataset. The results show that all three variations of the single-pass algorithm outperform the basic single-pass algorithm. Our proposed algorithm based on linguistic features improves the performance relatively by 69.5% and 9.7% when compared with the basic single-pass algorithm and the best variation algorithm in terms of F1 respectively.

#index 879570
#* Formal models for expert finding in enterprise corpora
#@ Krisztian Balog;Leif Azzopardi;Maarten de Rijke
#t 2006
#c 13
#% 10648
#% 262096
#% 266191
#% 280850
#% 312701
#% 319704
#% 340948
#% 411117
#% 426885
#% 730082
#% 766482
#% 768898
#% 818222
#% 1499466
#! Searching an organization's document repositories for experts provides a cost effective solution for the task of expert finding. We present two general strategies to expert searching given a document collection which are formalized using generative probabilistic models. The first of these directly models an expert's knowledge based on the documents that they are associated with, whilst the second locates documents on topic, and then finds the associated expert. Forming reliable associations is crucial to the performance of expert finding systems. Consequently, in our evaluation we compare the different approaches, exploring a variety of associations along with other operational parameters (such as topicality). Using the TREC Enterprise corpora, we show that the second strategy consistently outperforms the first. A comparison against other unsupervised techniques, reveals that our second model delivers excellent performance.

#index 879571
#* Spoken document retrieval from call-center conversations
#@ Jonathan Mamou;David Carmel;Ron Hoory
#t 2006
#c 13
#% 219917
#% 280815
#% 397159
#% 838500
#% 939386
#! We are interested in retrieving information from conversational speech corpora, such as call-center data. This data comprises spontaneous speech conversations with low recording quality, which makes automatic speech recognition (ASR) a highly difficult task. For typical call-center data, even state-of-the-art large vocabulary continuous speech recognition systems produce a transcript with word error rate of 30% or higher. In addition to the output transcript, advanced systems provide word confusion networks (WCNs), a compact representation of word lattices associating each word hypothesis with its posterior probability. Our work exploits the information provided by WCNs in order to improve retrieval performance. In this paper, we show that the mean average precision (MAP) is improved using WCNs compared to the raw word transcripts. Finally, we analyze the effect of increasing ASR word error rate on search effectiveness. We show that MAP is still reasonable even under extremely high error rate.

#index 879572
#* Towards efficient automated singer identification in large music databases
#@ Jialie Shen;Bin Cui;John Shepherd;Kian-Lee Tan
#t 2006
#c 13
#% 137711
#% 190581
#% 235377
#% 413600
#% 451727
#% 562963
#% 643010
#% 864555
#% 899377
#% 1767443
#! Automated singer identification is important in organising, browsing and retrieving data in large music databases. In this paper, we propose a novel scheme, called Hybrid Singer Identifier (HSI), for automated singer recognition. HSI can effectively use multiple low-level features extracted from both vocal and non-vocal music segments to enhance the identification process with a hybrid architecture and build profiles of individual singer characteristics based on statistical mixture models. Extensive experimental results conducted on a large music database demonstrate the superiority of our method over state-of-the-art approaches.

#index 879573
#* Music structure based vector space retrieval
#@ Namunu C. Maddage;Haizhou Li;Mohan S. Kankanhalli
#t 2006
#c 13
#% 194192
#% 204646
#% 281389
#% 286746
#% 309101
#% 395959
#% 574397
#% 614035
#% 780702
#% 792820
#% 818249
#% 849879
#! This paper proposes a novel framework for music content indexing and retrieval. The music structure information, i.e., timing, harmony and music region content, is represented by the layers of the music structure pyramid. We begin by extracting this layered structure information. We analyze the rhythm of the music and then segment the signal proportional to the inter-beat intervals. Thus, the timing information is incorporated in the segmentation process, which we call Beat Space Segmentation. To describe Harmony Events, we propose a two-layer hierarchical approach to model the music chords. We also model the progression of instrumental and vocal content as Acoustic Events. After information extraction, we propose a vector space modeling approach which uses these events as the indexing terms. In query-by-example music retrieval, a query is represented by a vector of the statistics of the n-gram events. We then propose two effective retrieval models, a hard-indexing scheme and a soft-indexing scheme. Experiments show that the vector space modeling is effective in representing the layered music information, achieving 82.5% top-5 retrieval accuracy using 15-sec music clips as the queries. The soft-indexing outperforms hard-indexing in general.

#index 879574
#* AggregateRank: bringing order to web sites
#@ Guang Feng;Tie-Yan Liu;Ying Wang;Ying Bao;Zhiming Ma;Xu-Dong Zhang;Wei-Ying Ma
#t 2006
#c 13
#% 66165
#% 480635
#% 565488
#% 713033
#% 754088
#% 754127
#% 765411
#% 769514
#% 1279489
#% 1676573
#! Since the website is one of the most important organizational structures of the Web, how to effectively rank websites has been essential to many Web applications, such as Web search and crawling. In order to get the ranks of websites, researchers used to describe the inter-connectivity among websites with a so-called HostGraph in which the nodes denote websites and the edges denote linkages between websites (if and only if there are hyperlinks from the pages in one website to the pages in the other, there will be an edge between these two websites), and then adopted the random walk model in the HostGraph. However, as pointed in this paper, the random walk over such a HostGraph is not reasonable because it is not in accordance with the browsing behavior of web surfers. Therefore, the derivate rank cannot represent the true probability of visiting the corresponding website.In this work, we mathematically proved that the probability of visiting a website by the random web surfer should be equal to the sum of the PageRank values of the pages inside that website. Nevertheless, since the number of web pages is much larger than that of websites, it is not feasible to base the calculation of the ranks of websites on the calculation of PageRank. To tackle this problem, we proposed a novel method named AggregateRank rooted in the theory of stochastic complement, which cannot only approximate the sum of PageRank accurately, but also have a lower computational complexity than PageRank. Both theoretical analysis and experimental evaluation show that AggregateRank is a better method for ranking websites than previous methods.

#index 879575
#* Respect my authority!: HITS without hyperlinks, utilizing cluster-based language models
#@ Oren Kurland;Lillian Lee
#t 2006
#c 13
#% 118771
#% 218992
#% 248226
#% 262045
#% 262096
#% 268079
#% 282905
#% 310567
#% 337781
#% 340899
#% 340932
#% 340948
#% 342621
#% 342660
#% 375017
#% 427921
#% 719598
#% 741081
#% 766406
#% 766430
#% 766431
#% 818204
#% 818209
#% 818241
#% 818266
#% 819773
#% 838528
#% 939968
#% 940042
#% 940051
#% 1272053
#% 1717308
#! We present an approach to improving the precision of an initial document ranking wherein we utilize cluster information within a graph-based framework. The main idea is to perform reranking based on centrality within bipartite graphs of documents (on one side) and clusters (on the other side), on the premise that these are mutually reinforcing entities. Links between entities are created via consideration of language models induced from them.We find that our cluster-document graphs give rise to much better retrieval performance than previously proposed document-only graphs do. For example, authority-based reranking of documents via a HITS-style cluster-based approach outperforms a previously-proposed PageRank-inspired algorithm applied to solely-document graphs. Moreover, we also show that computing authority scores for clusters constitutes an effective method for identifying clusters containing a large percentage of relevant documents.

#index 879576
#* Topical link analysis for web search
#@ Lan Nie;Brian D. Davison;Xiaoguang Qi
#t 2006
#c 13
#% 262061
#% 268073
#% 268079
#% 290830
#% 309779
#% 348173
#% 387427
#% 438136
#% 766462
#% 800183
#% 807297
#! Traditional web link-based ranking schemes use a single score to measure a page's authority without concern of the community from which that authority is derived. As a result, a resource that is highly popular for one topic may dominate the results of another topic in which it is less authoritative. To address this problem, we suggest calculating a score vector for each page to distinguish the contribution from different topics, using a random walk model that probabilistically combines page topic distribution and link structure. We show how to incorporate the topical model within both PageRank and HITS without affecting the overall property and still render insight into topic-level transition. Experiments on multiple datasets indicate that our technique outperforms other ranking approaches that incorporate textual analysis.

#index 879577
#* The role of knowledge in conceptual retrieval: a study in the domain of clinical medicine
#@ Jimmy Lin;Dina Demner-Fushman
#t 2006
#c 13
#% 144031
#% 169729
#% 169768
#% 198055
#% 262096
#% 321635
#% 332081
#% 411760
#% 741888
#% 766115
#% 766428
#% 766439
#% 766497
#% 789959
#% 815867
#% 818253
#% 818258
#% 818262
#% 853813
#% 939748
#% 958454
#! Despite its intuitive appeal, the hypothesis that retrieval at the level of "concepts" should outperform purely term-based approaches remains unverified empirically. In addition, the use of "knowledge" has not consistently resulted in performance gains. After identifying possible reasons for previous negative results, we present a novel framework for "conceptual retrieval" that articulates the types of knowledge that are important for information seeking. We instantiate this general framework in the domain of clinical medicine based on the principles of evidence-based medicine (EBM). Experiments show that an EBM-based scoring algorithm dramatically outperforms a state-of-the-art baseline that employs only term statistics. Ablation studies further yield a better understanding of the performance contributions of different components. Finally, we discuss how other domains can benefit from knowledge-based approaches.

#index 879578
#* A parallel derivation of probabilistic information retrieval models
#@ Thomas Roelleke;Jun Wang
#t 2006
#c 13
#% 118752
#% 169781
#% 184490
#% 218982
#% 262096
#% 397126
#% 411760
#% 458402
#% 719598
#% 758200
#% 799246
#% 818238
#% 872019
#! This paper investigates in a stringent athematical formalism the parallel derivation of three grand probabilistic retrieval models: binary independent retrieval (BIR), Poisson model (PM), and language modelling (LM).The investigation has been motivated by a number of questions. Firstly, though sharing the same origin, namely the probability of relevance, the models differ with respect to event spaces. How can this be captured in a consistent notation, and can we relate the event spaces? Secondly, BIR and PM are closely related, but how does LM fit in? Thirdly, how are tf-idf and probabilistic models related? .The parallel investigation of the models leads to a number of formalised results: BIR and PM assume the collection to be a set of non-relevant documents, whereas LM assumes the collection to be a set of terms from relevant documents.PM can be viewed as a bridge connecting BIR and LM.A BIR-LM equivalence explains BIR as a special LM case.PM explains tf-idf, and both, BIR and LM probabilities express tf-idf in a dual way..

#index 879579
#* Semantic term matching in axiomatic approaches to information retrieval
#@ Hui Fang;ChengXiang Zhai
#t 2006
#c 13
#% 144029
#% 169729
#% 218978
#% 229348
#% 262084
#% 262096
#% 280851
#% 288166
#% 316881
#% 340895
#% 340948
#% 342707
#% 375017
#% 397146
#% 406493
#% 420472
#% 766412
#% 766440
#% 786536
#% 815320
#% 818240
#% 818263
#% 838530
#! A common limitation of many retrieval models, including the recently proposed axiomatic approaches, is that retrieval scores are solely based on exact (i.e., syntactic) matching of terms in the queries and documents, without allowing distinct but semantically related terms to match each other and contribute to the retrieval score. In this paper, we show that semantic term matching can be naturally incorporated into the axiomatic retrieval model through defining the primitive weighting function based on a semantic similarity function of terms. We define several desirable retrieval constraints for semantic term matching and use such constraints to extend the axiomatic model to directly support semantic term matching based on the mutual information of terms computed on some document set. We show that such extension can be efficiently implemented as query expansion. Experiment results on several representative data sets show that, with mutual information computed over the documents in either the target collection for retrieval or an external collection such as the Web, our semantic expansion consistently and substantially improves retrieval accuracy over the baseline axiomatic retrieval model. As a pseudo feedback method, our method also outperforms a state-of-the-art language modeling feedback method.

#index 879580
#* On-line spam filter fusion
#@ Thomas R. Lynam;Gordon V. Cormack;David R. Cheriton
#t 2006
#c 13
#% 132938
#% 169774
#% 184496
#% 219050
#% 219052
#% 251145
#% 269217
#% 317950
#% 340940
#% 344447
#% 413613
#% 552056
#% 738972
#% 766450
#% 783514
#% 786633
#% 879631
#! We show that a set of independently developed spam filters may be combined in simple ways to provide substantially better filtering than any of the individual filters. The results of fifty-three spam filters evaluated at the TREC 2005 Spam Track were combined post-hoc so as to simulate the parallel on-line operation of the filters. The combined results were evaluated using the TREC methodology, yielding more than a factor of two improvement over the best filter. The simplest method -- averaging the binary classifications returned by the individual filters -- yields a remarkably good result. A new method -- averaging log-odds estimates based on the scores returned by the individual filters -- yields a somewhat better result, and provides input to SVM- and logistic-regression-based stacking methods. The stacking methods appear to provide further improvement, but only for very large corpora. Of the stacking methods, logistic regression yields the better result. Finally, we show that it is possible to select a priori small subsets of the filters that, when combined, still outperform the best individual filter by a substantial margin.

#index 879581
#* Building bridges for web query classification
#@ Dou Shen;Jian-Tao Sun;Qiang Yang;Zheng Chen
#t 2006
#c 13
#% 310567
#% 341006
#% 465754
#% 642982
#% 730051
#% 735077
#% 818281
#% 853542
#% 853543
#% 853544
#% 853545
#! Web query classification (QC) aims to classify Web users' queries, which are often short and ambiguous, into a set of target categories. QC has many applications including page ranking in Web search, targeted advertisement in response to queries, and personalization. In this paper, we present a novel approach for QC that outperforms the winning solution of the ACM KDDCUP 2005 competition, whose objective is to classify 800,000 real user queries. In our approach, we first build a bridging classifier on an intermediate taxonomy in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the above intermediate taxonomy. A major innovation is that by leveraging the similarity distribution over the intermediate taxonomy, we do not need to retrain a new classifier for each new set of target categories, and therefore the bridging classifier needs to be trained only once. In addition, we introduce category selection as a new method for narrowing down the scope of the intermediate taxonomy based on which we classify the queries. Category selection can improve both efficiency and effectiveness of the online classification. By combining our algorithm with the winning solution of KDDCUP 2005, we made an improvement by 9.7% and 3.8% in terms of precision and F1 respectively compared with the best results of KDDCUP 2005.

#index 879582
#* ProbFuse: a probabilistic approach to data fusion
#@ David Lillis;Fergus Toolan;Rem Collier;John Dunnion
#t 2006
#c 13
#% 144010
#% 169774
#% 194246
#% 194275
#% 232703
#% 268078
#% 306497
#% 309133
#% 309211
#% 316534
#% 340934
#% 340936
#% 342710
#% 348344
#% 397125
#% 413613
#% 420464
#% 672628
#% 766409
#% 784148
#! Data fusion is the combination of the results of independent searches on a document collection into one single output result set. It has been shown in the past that this can greatly improve retrieval effectiveness over that of the individual results.This paper presents probFuse, a probabilistic approach to data fusion. ProbFuse assumes that the performance of the individual input systems on a number of training queries is indicative of their future performance. The fused result set is based on probabilities of relevance calculated during this training process. Retrieval experiments using data from the TREC ad hoc collection demonstrate that probFuse achieves results superior to that of the popular CombMNZ fusion algorithm.

#index 879583
#* Using web-graph distance for relevance feedback in web search
#@ Sergei Vassilvitskii;Eric Brill
#t 2006
#c 13
#% 214709
#% 290830
#% 309095
#% 310567
#% 548494
#% 577329
#% 751569
#% 807395
#% 813718
#% 818254
#% 818259
#! We study the effect of user supplied relevance feedback in improving web search results. Rather than using query refinement or document similarity measures to rerank results, we show that the web-graph distance between two documents is a robust measure of their relative relevancy. We demonstrate how the use of this metric can improve the rankings of result URLs, even when the user only rates one document in the dataset. Our research suggests that such interactive systems can significantly improve search results.

#index 879584
#* Improving the estimation of relevance models using large external corpora
#@ Fernando Diaz;Donald Metzler
#t 2006
#c 13
#% 153019
#% 298183
#% 340901
#% 340953
#% 397160
#% 397177
#% 466263
#% 818262
#% 1715627
#% 1808946
#! Information retrieval algorithms leverage various collection statistics to improve performance. Because these statistics are often computed on a relatively small evaluation corpus, we believe using larger, non-evaluation corpora should improve performance. Specifically, we advocate incorporating external corpora based on language modeling. We refer to this process as external expansion. When compared to traditional pseudo-relevance feedback techniques, external expansion is more stable across topics and up to 10% more effective in terms of mean average precision. Our results show that using a high quality corpus that is comparable to the evaluation corpus can be as, if not more, effective than using the web. Our results also show that external expansion outperforms simulated relevance feedback. In addition, we propose a method for predicting the extent to which external expansion will improve retrieval performance. Our new measure demonstrates positive correlation with improvements in mean average precision.

#index 879585
#* Regularized estimation of mixture models for robust pseudo-relevance feedback
#@ Tao Tao;ChengXiang Zhai
#t 2006
#c 13
#% 218978
#% 298183
#% 340899
#% 340901
#% 340948
#% 342707
#% 397127
#% 766471
#% 766497
#% 818204
#% 827581
#% 843728
#! Pseudo-relevance feedback has proven to be an effective strategy for improving retrieval accuracy in all retrieval models. However the performance of existing pseudo feedback methods is often affected significantly by some parameters, such as the number of feedback documents to use and the relative weight of original query terms; these parameters generally have to be set by trial-and-error without any guidance. In this paper, we present a more robust method for pseudo feedback based on statistical language models. Our main idea is to integrate the original query with feedback documents in a single probabilistic mixture model and regularize the estimation of the language model parameters in the model so that the information in the feedback documents can be gradually added to the original query. Unlike most existing feedback methods, our new method has no parameter to tune. Experiment results on two representative data sets show that the new method is significantly more robust than a state-of-the-art baseline language modeling approach for feedback with comparable or better retrieval accuracy.

#index 879586
#* Context-sensitive semantic smoothing for the language modeling approach to genomic IR
#@ Xiaohua Zhou;Xiaohua Hu;Xiaodan Zhang;Xia Lin;Il-Yeol Song
#t 2006
#c 13
#% 118739
#% 262096
#% 280850
#% 280851
#% 340899
#% 340948
#% 342707
#% 397128
#% 397129
#% 572500
#% 766430
#% 818227
#% 818240
#% 829971
#% 838530
#% 1669516
#% 1742106
#! Semantic smoothing, which incorporates synonym and sense information into the language models, is effective and potentially significant to improve retrieval performance. The implemented semantic smoothing models, such as the translation model which statistically maps document terms to query terms, and a number of works that have followed have shown good experimental results. However, these models are unable to incorporate contextual information. Thus, the resulting translation might be mixed and fairly general. To overcome this limitation, we propose a novel context-sensitive semantic smoothing method that decomposes a document or a query into a set of weighted context-sensitive topic signatures and then translate those topic signatures into query terms. In detail, we solve this problem through (1) choosing concept pairs as topic signatures and adopting an ontology-based approach to extract concept pairs; (2) estimating the translation model for each topic signature using the EM algorithm; and (3) expanding document and query models based on topic signature translations. The new smoothing method is evaluated on TREC 2004/05 Genomics Track collections and significant improvements are obtained. The MAP (mean average precision) achieves a 33.6% maximal gain over the simple language model, as well as a 7.8% gain over the language model with context-insensitive semantic smoothing.

#index 879587
#* LDA-based document models for ad-hoc retrieval
#@ Xing Wei;W. Bruce Croft
#t 2006
#c 13
#% 262096
#% 280819
#% 280851
#% 340901
#% 340948
#% 643056
#% 722904
#% 766430
#% 788094
#% 876017
#% 1310058
#! Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.

#index 879588
#* Adapting ranking SVM to document retrieval
#@ Yunbo Cao;Jun Xu;Tie-Yan Liu;Hang Li;Yalou Huang;Hsiao-Wuen Hon
#t 2006
#c 13
#% 169777
#% 309095
#% 323131
#% 340899
#% 438557
#% 466229
#% 577224
#% 734915
#% 766414
#% 818239
#% 840583
#% 840846
#% 840853
#! The paper is concerned with applying learning to rank to document retrieval. Ranking SVM is a typical method of learning to rank. We point out that there are two factors one must consider when applying Ranking SVM, in general a "learning to rank" method, to document retrieval. First, correctly ranking documents on the top of the result list is crucial for an Information Retrieval system. One must conduct training in a way that such ranked results are accurate. Second, the number of relevant documents can vary from query to query. One must avoid training a model biased toward queries with a large number of relevant documents. Previously, when existing methods that include Ranking SVM were applied to document retrieval, none of the two factors was taken into consideration. We show it is possible to make modifications in conventional Ranking SVM, so it can be better used for document retrieval. Specifically, we modify the "Hinge Loss" function in Ranking SVM to deal with the problems described above. We employ two methods to conduct optimization on the loss function: gradient descent and quadratic programming. Experimental results show that our method, referred to as Ranking SVM for IR, can outperform the conventional Ranking SVM and other existing methods for document retrieval on two datasets.

#index 879589
#* A study of statistical models for query translation: finding a good unit of translation
#@ Jianfeng Gao;Jian-Yun Nie
#t 2006
#c 13
#% 132779
#% 262047
#% 277467
#% 340895
#% 375017
#% 397146
#% 420472
#% 729437
#% 735134
#% 741071
#% 788647
#% 815858
#% 817439
#% 818239
#% 818270
#% 848153
#% 854674
#% 939364
#% 939365
#% 939398
#% 1742104
#! This paper presents a study of three statistical query translation models that use different units of translation. We begin with a review of a word-based translation model that uses co-occurrence statistics for resolving translation ambiguities. The translation selection problem is then formulated under the framework of graphic model resorting to which the modeling assumptions and limitations of the co-occurrence model are discussed, and the research of finding better translation units is motivated. Then, two other models that use larger, linguistically motivated translation units (i.e., noun phrase and dependency triple) are presented. For each model, the modeling and training methods are described in detail. All query translation models are evaluated using TREC collections. Results show that larger translation units lead to more specific models that usually achieve better translation and cross-language information retrieval results.

#index 879590
#* Combining bidirectional translation and synonymy for cross-language information retrieval
#@ Jianqiang Wang;Douglas W. Oard
#t 2006
#c 13
#% 262046
#% 316902
#% 340901
#% 458409
#% 561312
#% 569161
#% 643017
#% 732851
#% 741935
#% 786534
#% 810921
#% 817596
#% 843645
#% 877183
#% 939951
#! This paper introduces a general framework for the use of translation probabilities in cross-language information retrieval based on the notion that information retrieval fundamentally requires matching what the searcher means with what the author of a document meant. That perspective yields a computational formulation that provides a natural way of combining what have been known as query and document translation. Two well-recognized techniques are shown to be a special case of this model under restrictive assumptions. Cross-language search results are reported that are statistically indistinguishable from strong monolingual baselines for both French and Chinese documents.

#index 879591
#* Probabilistic model for definitional question answering
#@ Kyoung-Soo Han;Young-In Song;Hae-Chang Rim
#t 2006
#c 13
#% 184490
#% 278107
#% 309126
#% 750863
#% 854191
#% 939970
#% 1676562
#! This paper proposes a probabilistic model for definitional question answering (QA) that reflects the characteristics of the definitional question. The intention of the definitional question is to request the definition about the question target. Therefore, an answer for the definitional question should contain the content relevant to the topic of the target, and have a representation form of the definition style. Modeling the problem of definitional QA from both the topic and definition viewpoints, the proposed probabilistic model converts the task of answering the definitional questions into that of estimating the three language models: topic language model, definition language model, and general language model. The proposed model systematically combines several evidences in a probabilistic framework. Experimental results show that a definitional QA system based on the proposed probabilistic model is comparable to state-of-the-art systems.

#index 879592
#* Answering complex questions with random walk models
#@ Sanda Harabagiu;Finley Lacatusu;Andrew Hickl
#t 2006
#c 13
#% 196896
#% 198058
#% 340899
#% 340954
#% 708948
#% 755863
#% 854663
#% 858036
#% 939357
#% 939732
#% 939748
#% 939968
#! We present a novel framework for answering complex questions that relies on question decomposition. Complex questions are decomposed by a procedure that operates on a Markov chain, by following a random walk on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations. Decomposed questions discovered during this random walk are then submitted to a state-of-the-art Question Answering (Q/A) system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a Multi-Document Summarization (MDS) system. In our evaluations, we show that access to the decompositions generated using this method can significantly enhance the relevance and comprehensiveness of summary-length answers to complex questions.

#index 879593
#* A framework to predict the quality of answers with non-textual features
#@ Jiwoon Jeon;W. Bruce Croft;Joon Ho Lee;Soyeon Park
#t 2006
#c 13
#% 211044
#% 228356
#% 262054
#% 262096
#% 268079
#% 290830
#% 309150
#% 340948
#% 397126
#% 461204
#% 730070
#% 809937
#% 838397
#% 838398
#% 838472
#% 854646
#% 854813
#% 866259
#! New types of document collections are being developed by various web services. The service providers keep track of non-textual features such as click counts. In this paper, we present a framework to use non-textual features to predict the quality of documents. We also show our quality measure can be successfully incorporated into the language modeling-based retrieval model. We test our approach on a collection of question and answer pairs gathered from a community based question answering service where people ask and answer questions. Experimental results using our quality measure show a significant improvement over our baseline.

#index 879594
#* Latent semantic analysis for multiple-type interrelated data objects
#@ Xuanhui Wang;Jian-Tao Sun;Zheng Chen;ChengXiang Zhai
#t 2006
#c 13
#% 118749
#% 200694
#% 224113
#% 280817
#% 280819
#% 280852
#% 290830
#% 309129
#% 316143
#% 321053
#% 340884
#% 387427
#% 397137
#% 451475
#% 528182
#% 577273
#% 643008
#% 643023
#% 730166
#% 804808
#% 818203
#% 818218
#% 840840
#% 1650569
#! Co-occurrence data is quite common in many real applications. Latent Semantic Analysis (LSA) has been successfully used to identify semantic relations in such data. However, LSA can only handle a single co-occurrence relationship between two types of objects. In practical applications, there are many cases where multiple types of objects exist and any pair of these objects could have a pairwise co-occurrence relation. All these co-occurrence relations can be exploited to alleviate data sparseness or to represent objects more meaningfully. In this paper, we propose a novel algorithm, M-LSA, which conducts latent semantic analysis by incorporating all pairwise co-occurrences among multiple types of objects. Based on the mutual reinforcement principle, M-LSA identifies the most salient concepts among the co-occurrence data and represents all the objects in a unified semantic space. M-LSA is general and we show that several variants of LSA are special cases of our algorithm. Experiment results show that M-LSA outperforms LSA on multiple applications, including collaborative filtering, text clustering, and text categorization.

#index 879595
#* Identifying comparative sentences in text documents
#@ Nitin Jindal;Bing Liu
#t 2006
#c 13
#% 118040
#% 190581
#% 269217
#% 280487
#% 376266
#% 577256
#% 577355
#% 723399
#% 742368
#% 755835
#% 756207
#% 769892
#% 769967
#% 778732
#% 786539
#% 805873
#% 815915
#% 828958
#% 854646
#% 855279
#% 855282
#% 936239
#% 1250238
#% 1250367
#! This paper studies the problem of identifying comparative sentences in text documents. The problem is related to but quite different from sentiment/opinion sentence identification or classification. Sentiment classification studies the problem of classifying a document or a sentence based on the subjective opinion of the author. An important application area of sentiment/opinion identification is business intelligence as a product manufacturer always wants to know consumers' opinions on its products. Comparisons on the other hand can be subjective or objective. Furthermore, a comparison is not concerned with an object in isolation. Instead, it compares the object with others. An example opinion sentence is "the sound quality of CD player X is poor". An example comparative sentence is "the sound quality of CD player X is not as good as that of CD player Y". Clearly, these two sentences give different information. Their language constructs are quite different too. Identifying comparative sentences is also useful in practice because direct comparisons are perhaps one of the most convincing ways of evaluation, which may even be more important than opinions on each individual object. This paper proposes to study the comparative sentence identification problem. It first categorizes comparative sentences into different types, and then presents a novel integrated pattern discovery and supervised learning approach to identifying comparative sentences from text documents. Experiment results using three types of documents, news articles, consumer reviews of products, and Internet forum postings, show a precision of 79% and recall of 81%. More detailed results are given in the paper.

#index 879596
#* Tackling concept drift by temporal inductive transfer
#@ George Forman
#t 2006
#c 13
#% 204531
#% 262059
#% 316478
#% 342600
#% 458379
#% 722935
#% 763708
#% 799043
#% 881471
#% 926881
#% 998561
#% 1289518
#% 1699624
#! Machine learning is the mainstay for text classification. However, even the most successful techniques are defeated by many real-world applications that have a strong time-varying component. To advance research on this challenging but important problem, we promote a natural, experimental framework-the Daily Classification Task-which can be applied to large time-based datasets, such as Reuters RCV1.In this paper we dissect concept drift into three main subtypes. We demonstrate via a novel visualization that the recurrent themes subtype is present in RCV1. This understanding led us to develop a new learning model that transfers induced knowledge through time to benefit future classifier learning tasks. The method avoids two main problems with existing work in inductive transfer: scalability and the risk of negative transfer. In empirical tests, it consistently showed more than 10 points F-measure improvement for each of four Reuters categories tested.

#index 879597
#* Evaluation in (XML) information retrieval: expected precision-recall with user modelling (EPRUM)
#@ Benjamin Piwowarski;Georges Dupret
#t 2006
#c 13
#% 57485
#% 262105
#% 309093
#% 387427
#% 575729
#% 766415
#! Standard Information Retrieval (IR) metrics assume a simple model where documents are understood as independent units. Such an assumption is not adapted to new paradigms like XML or Web IR where retrievable informations are parts of documents or sets of related documents. Moreover, classical hypotheses assumes that the user ignores the structural or logical context of document elements and hence the possibility of navigation between units. EPRUM is a generalisation of Precision-Recall (PR) that aims at allowing the user to navigate or browse in the corpus structure. Like the Cumulated Gain metrics, it is able to handle continuous valued relevance. We apply and compare EPRUM in the context of XML Retrieval -- a very active field for evaluation metrics. We also explain how EPRUM can be used in other IR paradigms.

#index 879598
#* Minimal test collections for retrieval evaluation
#@ Ben Carterette;James Allan;Ramesh Sitaraman
#t 2006
#c 13
#% 262097
#% 262102
#% 262105
#% 309093
#% 340890
#% 561315
#% 766409
#% 766410
#% 818201
#% 818222
#% 838529
#! Accurate estimation of information retrieval evaluation metrics such as average precision require large sets of relevance judgments. Building sets large enough for evaluation of real-world implementations is at best inefficient, at worst infeasible. In this work we link evaluation with test collection construction to gain an understanding of the minimal judging effort that must be done to have high confidence in the outcome of an evaluation. A new way of looking at average precision leads to a natural algorithm for selecting documents to judge and allows us to estimate the degree of confidence by defining a distribution over possible document judgments. A study with annotators shows that this method can be used by a small group of researchers to rank a set of systems in under three hours with 95% confidence. Information retrieval metrics such as average precision require large sets of relevance judgments to be accurately estimated. Building these sets is infeasible and often inefficient for many real-world retrieval implementations. We present a new way of looking at average precision that allows us to estimate the confidence in an evaluation based on the size of the test collection. We use this to build an algorithm for selecting the best documents to judge to have maximum confidence in an evaluation with a minimal number of relevance judgments. A study with annotators shows how the algorithm can be used by a small group of researchers to quickly rank a set of systems with 95% confidence.

#index 879599
#* Dynamic test collections: measuring search effectiveness on the live web
#@ Ian Soboroff
#t 2006
#c 13
#% 255137
#% 262097
#% 262102
#% 262105
#% 309746
#% 345087
#% 397163
#% 480136
#% 643005
#% 754058
#% 754090
#% 756015
#% 766409
#% 766498
#% 818222
#% 838529
#! Existing methods for measuring the quality of search algorithms use a static collection of documents. A set of queries and a mapping from the queries to the relevant documents allow the experimenter to see how well different search engines or engine configurations retrieve the correct answers. This methodology assumes that the document set and thus the set of relevant documents are unchanging. In this paper, we abandon the static collection requirement. We begin with a recent TREC collection created from a web crawl and analyze how the documents in that collection have changed over time. We determine how decay of the document collection affects TREC systems, and present the results of an experiment using the decayed collection to measure a live web search system. We employ novel measures of search effectiveness that are robust despite incomplete relevance information. Lastly, we propose a methodology of "collection maintenance" which supports measuring search performance both for a single system and between systems run at different points in time.

#index 879600
#* Finding near-duplicate web pages: a large-scale evaluation of algorithms
#@ Monika Henzinger
#t 2006
#c 13
#% 201935
#% 204673
#% 255137
#% 347225
#% 504572
#% 571725
#% 728115
#% 818223
#% 963669
#! Broder et al.'s [3] shingling algorithm and Charikar's [4] random projection based approach are considered "state-of-the-art" algorithms for finding near-duplicate web pages. Both algorithms were either developed at or used by popular web search engines. We compare the two algorithms on a very large scale, namely on a set of 1.6B distinct web pages. The results show that neither of the algorithms works well for finding near-duplicate pairs on the same site, while both achieve high precision for near-duplicate pairs on different sites. Since Charikar's algorithm finds more near-duplicate pairs on different sites, it achieves a better precision overall, namely 0.50 versus 0.38 for Broder et al.'s algorithm. We present a combined algorithm which achieves precision 0.79 with 79% of the recall of the other algorithms.

#index 879601
#* Structure-driven crawler generation by example
#@ Márcio L. A. Vidal;Altigran S. da Silva;Edleno S. de Moura;João M. B. Cavalcanti
#t 2006
#c 13
#% 103525
#% 273927
#% 281251
#% 330609
#% 342400
#% 348138
#% 413642
#% 480126
#% 503234
#% 572327
#% 654469
#% 754108
#% 760838
#% 762334
#% 783694
#% 805846
#% 827127
#! Many Web IR and Digital Library applications require a crawling process to collect pages with the ultimate goal of taking advantage of useful information available on Web sites. For some of these applications the criteria to determine when a page is to be present in a collection are related to the page content. However, there are situations in which the inner structure of the pages provides a better criteria to guide the crawling process than their content. In this paper, we present a structure-driven approach for generating Web crawlers that requires a minimum effort from users. The idea is to take as input a sample page and an entry point to a Web site and generate a structure-driven crawler based on navigation patterns, sequences of patterns for the links a crawler has to follow to reach the pages structurally similar to the sample page. In the experiments we have carried out, structure-driven crawlers generated by our new approach were able to collect all pages that match the samples given, including those pages added after their generation.

#index 879602
#* Building implicit links from content for forum search
#@ Gu Xu;Wei-Ying Ma
#t 2006
#c 13
#% 40098
#% 262059
#% 268079
#% 290830
#% 309141
#% 342621
#% 348172
#% 348173
#% 465747
#% 465754
#% 503228
#% 508272
#% 528008
#% 577230
#% 577329
#% 643050
#% 766456
#% 805895
#% 818225
#% 818321
#! The objective of Web forums is to create a shared space for open communications and discussions of specific topics and issues. The tremendous information behind forum sites is not fully-utilized yet. Most links between forum pages are automatically created, which means the link-based ranking algorithm cannot be applied efficiently. In this paper, we proposed a novel ranking algorithm which tries to introduce the content information into link-based methods as implicit links. The basic idea is derived from the more focused random surfer: the surfer may more likely jump to a page which is similar to what he is reading currently. In this manner, we are allowed to introduce the content similarities into the link graph as a personalization bias. Our method, named Fine-grained Rank (FGRank), can be efficiently computed based on an automatically generated topic hierarchy. Not like the topic-sensitive PageRank, our method only need to compute single PageRank score for each page. Another contribution of this paper is to present a very efficient algorithm for automatically generating topic hierarchy and map each page in a large-scale collection onto the computed hierarchy. The experimental results show that the proposed method can improve retrieval performance, and reveal that content-based link graph is also important compared with the hyper-link graph.

#index 879603
#* Generalizing PageRank: damping functions for link-based ranking algorithms
#@ Ricardo Baeza-Yates;Paolo Boldi;Carlos Castillo
#t 2006
#c 13
#% 22242
#% 249144
#% 255170
#% 262061
#% 290830
#% 309145
#% 340141
#% 348173
#% 378057
#% 387427
#% 433672
#% 510723
#% 593994
#% 754088
#% 763566
#% 769428
#% 783528
#% 803035
#% 805895
#% 807318
#% 807320
#% 807657
#% 832329
#% 882035
#! This paper introduces a family of link-based ranking algorithms that propagate page importance through links. In these algorithms there is a damping function that decreases with distance, so a direct link implies more endorsement than a link through a long path. PageRank is the most widely known ranking function of this family.The main objective of this paper is to determine whether this family of ranking techniques has some interest per se, and how different choices for the damping function impact on rank quality and on convergence speed. Even though our results suggest that PageRank can be approximated with other simpler forms of rankings that may be computed more efficiently, our focus is of more speculative nature, in that it aims at separating the kernel of PageRank, that is, link-based importance propagation, from the way propagation decays over paths.We focus on three damping functions, having linear, exponential, and hyperbolic decay on the lengths of the paths. The exponential decay corresponds to PageRank, and the other functions are new. Our presentation includes algorithms, analysis, comparisons and experiments that study their behavior under different parameters in real Web graph data.Among other results, we show how to calculate a linear approximation that induces a page ordering that is almost identical to PageRank's using a fixed small number of iterations; comparisons were performed using Kendall's τ on large domain datasets.

#index 879604
#* Capturing collection size for distributed non-cooperative retrieval
#@ Milad Shokouhi;Justin Zobel;Falk Scholer;S. M. M. Tahaghoghi
#t 2006
#c 13
#% 268079
#% 306468
#% 306497
#% 333932
#% 340146
#% 340928
#% 387427
#% 413594
#% 413635
#% 447946
#% 643012
#% 722311
#% 729027
#% 747116
#% 764562
#% 765465
#% 783473
#% 805863
#% 818211
#% 993964
#! Modern distributed information retrieval techniques require accurate knowledge of collection size. In non-cooperative environments, where detailed collection statistics are not available, the size of the underlying collections must be estimated. While several approaches for the estimation of collection size have been proposed, their accuracy has not been thoroughly evaluated. An empirical analysis of past estimation approaches across a variety of collections demonstrates that their prediction accuracy is low. Motivated by ecological techniques for the estimation of animal populations, we propose two new approaches for the estimation of collection size. We show that our approaches are significantly more accurate that previous methods, and are more efficient in use of resources required to perform the estimation.

#index 879605
#* Probabilistic latent query analysis for combining multiple retrieval sources
#@ Rong Yan;Alexander G. Hauptmann
#t 2006
#c 13
#% 120104
#% 194275
#% 280819
#% 340934
#% 342710
#% 642982
#% 766414
#% 780805
#% 818267
#% 840002
#% 855048
#% 1389537
#% 1727324
#! Combining the output from multiple retrieval sources over the same document collection is of great importance to a number of retrieval tasks such as multimedia retrieval, web retrieval and meta-search. To merge retrieval sources adaptively according to query topics, we propose a series of new approaches called probabilistic latent query analysis (pLQA), which can associate non-identical combination weights with latent classes underlying the query space. Compared with previous query independent and query-class based combination methods, the proposed approaches have the advantage of being able to discover latent query classes automatically without using prior human knowledge, to assign one query to a mixture of query classes, and to determine the number of query classes under a model selection principle. Experimental results on two retrieval tasks, i.e., multimedia retrieval and meta-search, demonstrate that the proposed methods can uncover sensible latent classes from training data, and can achieve considerable performance gains.

#index 879606
#* User modeling for full-text federated search in peer-to-peer networks
#@ Jie Lu;Jamie Callan
#t 2006
#c 13
#% 144034
#% 194275
#% 320839
#% 342961
#% 469923
#% 496286
#% 730035
#% 747115
#% 754126
#% 771571
#% 818207
#% 818259
#% 832349
#% 1715594
#% 1715596
#! User modeling for information retrieval has mostly been studied to improve the effectiveness of information access in centralized repositories. In this paper we explore user modeling in the context of full-text federated search in peer-to-peer networks. Our approach models a user's persistent, long-term interests based on past queries, and uses the model to improve search efficiency for future queries that represent interests similar to past queries. Our approach also enables queries representing a user's transient, ad-hoc interests to be automatically recognized so that search for these queries can rely on a relatively large search radius to avoid sacrificing effectiveness for efficiency. Experimental results demonstrate that our approach can significantly improve the efficiency of full-text federated search without degrading its accuracy. Furthermore, the proposed approach does not require a large amount of training data, and is robust to a range of parameter values.

#index 879607
#* Distributed query sampling: a quality-conscious approach
#@ James Caverlee;Ling Liu;Joonsoo Bae
#t 2006
#c 13
#% 194246
#% 262063
#% 273926
#% 280853
#% 282422
#% 301225
#% 309133
#% 333932
#% 340146
#% 342741
#% 387427
#% 397125
#% 480479
#% 481748
#% 591588
#% 643011
#% 643012
#% 665561
#% 765465
#% 766469
#% 800569
#% 809418
#% 818211
#! We present an adaptive distributed query-sampling framework that is quality-conscious for extracting high-quality text database samples. The framework divides the query-based sampling process into an initial seed sampling phase and a quality-aware iterative sampling phase. In the second phase the sampling process is dynamically scheduled based on estimated database size and quality parameters derived during the previous sampling process. The unique characteristic of our adaptive query-based sampling framework is its self-learning and self-configuring ability based on the overall quality of all text databases under consideration. We introduce three quality-conscious sampling schemes for estimating database quality, and our initial results show that the proposed framework supports higher-quality document sampling than existing approaches.

#index 879608
#* Load balancing for term-distributed parallel retrieval
#@ Alistair Moffat;William Webber;Justin Zobel
#t 2006
#c 13
#% 86533
#% 86534
#% 104439
#% 183334
#% 219013
#% 221973
#% 249153
#% 298181
#% 342397
#% 504881
#% 508264
#% 578337
#% 578872
#% 617222
#% 1683906
#! Large-scale web and text retrieval systems deal with amounts of data that greatly exceed the capacity of any single machine. To handle the necessary data volumes and query throughput rates, parallel systems are used, in which the document and index data are split across tightly-clustered distributed computing systems. The index data can be distributed either by document or by term. In this paper we examine methods for load balancing in term-distributed parallel architectures, and propose a suite of techniques for reducing net querying costs. In combination, the techniques we describe allow a 30% improvement in query throughput when tested on an eight-node parallel computer system.

#index 879609
#* Hybrid index maintenance for growing text collections
#@ Stefan Büttcher;Charles L. A. Clarke;Brad Lushman
#t 2006
#c 13
#% 86532
#% 169814
#% 172922
#% 188587
#% 249989
#% 378239
#% 397151
#% 655485
#% 747117
#% 838465
#% 838541
#% 874704
#% 1742088
#! We present a new family of hybrid index maintenance strategies to be used in on-line index construction for monotonically growing text collections. These new strategies improve upon recent results for hybrid index maintenance in dynamic text retrieval systems. Like previous techniques, our new method distinguishes between short and long posting lists: While short lists are maintained using a merge strategy, long lists are kept separate and are updated in-place. This way, costly relocations of long posting lists are avoided.We discuss the shortcomings of previous hybrid methods and give an experimental evaluation of the new technique, showing that its index maintenance performance is superior to that of the earlier methods, especially when the amount of main memory available to the indexing system is small. We also present a complexity analysis which proves that, under a Zipfian term distribution, the asymptotical number of disk accesses performed by the best hybrid maintenance strategy is linear in the size of the text collection, implying the asymptotical optimality of the proposed strategy.

#index 879610
#* Type less, find more: fast autocompletion search with a succinct index
#@ Holger Bast;Ingmar Weber
#t 2006
#c 13
#% 18082
#% 41684
#% 82523
#% 169729
#% 213786
#% 249989
#% 252304
#% 273714
#% 290703
#% 301263
#% 330705
#% 594028
#% 643566
#% 643571
#% 752077
#% 766461
#% 786632
#% 810908
#% 823464
#% 1699616
#! We consider the following full-text search autocompletion feature. Imagine a user of a search engine typing a query. Then with every letter being typed, we would like an instant display of completions of the last query word which would lead to good hits. At the same time, the best hits for any of these completions should be displayed. Known indexing data structures that apply to this problem either incur large processing times for a substantial class of queries, or they use a lot of space. We present a new indexing data structure that uses no more space than a state-of-the-art compressed inverted index, but with 10 times faster query processing times. Even on the large TREC Terabyte collection, which comprises over 25 million documents, we achieve, on a single machine and with the index on disk, average response times of one tenth of a second. We have built a full-fledged, interactive search engine that realizes the proposed autocompletion feature combined with support for proximity search, semi-structured (XML) text, subword and phrase completion, and semantic tags.

#index 879611
#* Pruned query evaluation using pre-computed impacts
#@ Vo Ngoc Anh;Alistair Moffat
#t 2006
#c 13
#% 194247
#% 198335
#% 212665
#% 213786
#% 228097
#% 274490
#% 290703
#% 340886
#% 340887
#% 730065
#% 805862
#% 818229
#% 818230
#% 818232
#% 857180
#% 1683906
#! Exhaustive evaluation of ranked queries can be expensive, particularly when only a small subset of the overall ranking is required, or when queries contain common terms. This concern gives rise to techniques for dynamic query pruning, that is, methods for eliminating redundant parts of the usual exhaustive evaluation, yet still generating a demonstrably "good enough" set of answers to the query. In this work we propose new pruning methods that make use of impact-sorted indexes. Compared to exhaustive evaluation, the new methods reduce the amount of computation performed, reduce the amount of memory required for accumulators, reduce the amount of data transferred from disk, and at the same time allow performance guarantees in terms of precision and mean average precision. These strong claims are backed by experiments using the TREC Terabyte collection and queries.

#index 879612
#* Mining dependency relations for query expansion in passage retrieval
#@ Renxu Sun;Chai-Huat Ong;Tat-Seng Chua
#t 2006
#c 13
#% 184489
#% 218978
#% 287253
#% 289079
#% 642979
#% 741891
#% 818253
#! Classical query expansion techniques such as the local context analysis (LCA) make use of term co-occurrence statistics to incorporate additional contextual terms for enhancing passage retrieval. However, relevant contextual terms do not always co-occur frequently with the query terms and vice versa. Hence the use of such methods often brings in noise, which leads to reduced precision. Previous studies have demonstrated the importance of relationship analysis for natural language queries in passage retrieval. However, they found that without query expansion, the performance is not satisfactory for short queries. In this paper, we present two novel query expansion techniques that make use of dependency relation analysis to extract contextual terms and relations from external corpuses. The techniques are used to enhance the performance of density based and relation based passage retrieval frameworks respectively. We compare the performance of the resulting systems with LCA in a density based passage retrieval system (DBS) and a relation based system without any query expansion (RBS) using the factoid questions from the TREC-12 QA task. The results show that in terms of MRR scores, our relation based term expansion method with DBS outperforms the LCA by 9.81%, while our relation expansion method outperforms RBS by 17.49%.

#index 879613
#* What makes a query difficult?
#@ David Carmel;Elad Yom-Tov;Adam Darlow;Dan Pelleg
#t 2006
#c 13
#% 269217
#% 397161
#% 397201
#% 408396
#% 642975
#% 729437
#% 766497
#% 818267
#% 838509
#! This work tries to answer the question of what makes a query difficult. It addresses a novel model that captures the main components of a topic and the relationship between those components and topic difficulty. The three components of a topic are the textual expression describing the information need (the query or queries), the set of documents relevant to the topic (the Qrels), and the entire collection of documents. We show experimentally that topic difficulty strongly depends on the distances between these components. In the absence of knowledge about one of the model components, the model is still useful by approximating the missing component based on the other components. We demonstrate the applicability of the difficulty model for several uses such as predicting query difficulty, predicting the number of topic aspects expected to be covered by the search results, and analyzing the findability of a specific domain.

#index 879614
#* On ranking the effectiveness of searches
#@ Vishwa Vinay;Ingemar J. Cox;Natasa Milic-Frayling;Ken Wood
#t 2006
#c 13
#% 36672
#% 375017
#% 397161
#% 818267
#% 826267
#! There is a growing interest in estimating the effectiveness of search. Two approaches are typically considered: examining the search queries and examining the retrieved document sets. In this paper, we take the latter approach. We use four measures to characterize the retrieved document sets and estimate the quality of search. These measures are (i) the clustering tendency as measured by the Cox-Lewis statistic, (ii) the sensitivity to document perturbation, (iii) the sensitivity to query perturbation and (iv) the local intrinsic dimensionality. We present experimental results for the task of ranking 200 queries according to the search effectiveness over the TREC (discs 4 and 5) dataset. Our ranking of queries is compared with the ranking based on the average precision using the Kendall t statistic. The best individual estimator is the sensitivity to document perturbation and yields Kendall t of 0.521. When combined with the clustering tendency based on the Cox-Lewis statistic and the query perturbation measure, it results in Kendall t of 0.562 which to our knowledge is the highest correlation with the average precision reported to date.

#index 879615
#* Document clustering with prior knowledge
#@ Xiang Ji;Wei Xu
#t 2006
#c 13
#% 118771
#% 280817
#% 296738
#% 304876
#% 313959
#% 316509
#% 464291
#% 464631
#% 466263
#% 466675
#% 466890
#% 732539
#% 766433
#% 766435
#! Document clustering is an important tool for text analysis and is used in many different applications. We propose to incorporate prior knowledge of cluster membership for document cluster analysis and develop a novel semi-supervised document clustering model. The method models a set of documents with weighted graph in which each document is represented as a vertex, and each edge connecting a pair of vertices is weighted with the similarity value of the two corresponding documents. The prior knowledge indicates pairs of documents that known to belong to the same cluster. Then, the prior knowledge is transformed into a set of constraints. The document clustering task is accomplished by finding the best cuts of the graph under the constraints. We apply the model to the Normalized Cut method to demonstrate the idea and concept. Our experimental evaluations show that the proposed document clustering model reveals remarkable performance improvements with very limited training samples, and hence is a very effective semi-supervised classification tool.

#index 879616
#* Text clustering with extended user feedback
#@ Yifen Huang;Tom M. Mitchell
#t 2006
#c 13
#% 252011
#% 266292
#% 464291
#% 465754
#% 466263
#% 769881
#% 799753
#% 1250186
#% 1289485
#! Text clustering is most commonly treated as a fully automated task without user feedback. However, a variety of researchers have explored mixed-initiative clustering methods which allow a user to interact with and advise the clustering algorithm. This mixed-initiative approach is especially attractive for text clustering tasks where the user is trying to organize a corpus of documents into clusters for some particular purpose (e.g., clustering their email into folders that reflect various activities in which they are involved). This paper introduces a new approach to mixed-initiative clustering that handles several natural types of user feedback. We first introduce a new probabilistic generative model for text clustering (the SpeClustering model) and show that it outperforms the commonly used mixture of multinomials clustering model, even when used in fully autonomous mode with no user input. We then describe how to incorporate four distinct types of user feedback into the clustering algorithm, and provide experimental evidence showing substantial improvements in text clustering when this user feedback is incorporated.

#index 879617
#* Near-duplicate detection by instance-level constrained clustering
#@ Hui Yang;Jamie Callan
#t 2006
#c 13
#% 201935
#% 255137
#% 340948
#% 345087
#% 464608
#% 466890
#% 571725
#% 730067
#% 769944
#% 809293
#% 838508
#! For the task of near-duplicated document detection, both traditional fingerprinting techniques used in database community and bag-of-word comparison approaches used in information retrieval community are not sufficiently accurate. This is due to the fact that the characteristics of near-duplicated documents are different from that of both "almost-identical" documents in the data cleaning task and "relevant" documents in the search task. This paper presents an instance-level constrained clustering approach for near-duplicate detection. The framework incorporates information such as document attributes and content structure into the clustering process to form near-duplicate clusters. Gathered from several collections of public comments sent to U.S. government agencies on proposed new regulations, the experimental results demonstrate that our approach outperforms other near-duplicate detection algorithms and as about as effective as human assessors.

#index 879618
#* Less is more: probabilistic models for retrieving fewer relevant documents
#@ Harr Chen;David R. Karger
#t 2006
#c 13
#% 248214
#% 262096
#% 262105
#% 262112
#% 308048
#% 458369
#% 642975
#% 642976
#% 766406
#% 766514
#% 818239
#! Traditionally, information retrieval systems aim to maximize thenumber of relevant documents returned to a user within some windowof the top. For that goal, the probability ranking principle, whichranks documents in decreasing order of probability of relevance, isprovably optimal. However, there are many scenarios in which thatranking does not optimize for the users information need. Oneexample is when the user would be satisfied with some limitednumber of relevant documents, rather than needing all relevantdocuments. We show that in such a scenario, an attempt to returnmany relevant documents can actually reduce the chances of findingany relevant documents.We consider a number of information retrieval metrics from theliterature, including the rank of the first relevant result, the%no metric that penalizes a system only for retrieving no relevantresults near the top, and the diversity of retrieved results whenqueries have multiple interpretations. We observe that given aprobabilistic model of relevance, it is appropriate to rank so asto directly optimize these metrics in expectation. While doing somay be computationally intractable, we show that a simple greedyoptimization algorithm that approximately optimizes the givenobjectives produces rankings for TREC queries that outperform thestandard approach based on the probability ranking principle.

#index 879619
#* High accuracy retrieval with multiple nested ranker
#@ Irina Matveeva;Chris Burges;Timo Burkard;Andy Laucius;Leon Wong
#t 2006
#c 13
#% 198701
#% 253188
#% 309095
#% 340892
#% 476873
#% 724559
#% 734915
#% 766406
#% 818221
#% 823317
#% 840846
#% 1717308
#! High precision at the top ranks has become a new focus of research in information retrieval. This paper presents the multiple nested ranker approach that improves the accuracy at the top ranks by iteratively re-ranking the top scoring documents. At each iteration, this approach uses the RankNet learning algorithm to re-rank a subset of the results. This splits the problem into smaller and easier tasks and generates a new distribution of the results to be learned by the algorithm. We evaluate this approach using different settings on a data set labeled with several degrees of relevance. We use the normalized discounted cumulative gain (NDCG) to measure the performance because it depends not only on the position but also on the relevance score of the document in the ranked list. Our experiments show that making the learning algorithm concentrate on the top scoring results improves precision at the top ten documents in terms of the NDCG score.

#index 879620
#* Semantic search via XML fragments: a high-precision approach to IR
#@ Jennifer Chu-Carroll;John Prager;Krzysztof Czuba;David Ferrucci;Pablo Duboue
#t 2006
#c 13
#% 144031
#% 309124
#% 340914
#% 420471
#% 577373
#% 642993
#% 742424
#% 787550
#% 843652
#% 853813
#% 855282
#% 939971
#% 1015258
#% 1041075
#! In some IR applications, it is desirable to adopt a high precision search strategy to return a small set of documents that are highly focused and relevant to the user's information need. With these applications in mind, we investigate semantic search using the XML Fragments query language on text corpora automatically pre-processed to encode semantic information useful for retrieval. We identify three XML Fragment operations that can be applied to a query to conceptualize, restrict, or relate terms in the query. We demonstrate how these operations can be used to address four different query-time semantic needs: to specify target information type, to disambiguate keywords, to specify search term context, or to relate select terms in the query. We demonstrate the effectiveness of our semantic search technology through a series of experiments using the two applications in which we embed this technology and show that it yields significant improvement in precision in the search results.

#index 879621
#* Elicitation of term relevance feedback: an investigation of term source and context
#@ Diane Kelly;Xin Fu
#t 2006
#c 13
#% 54435
#% 86528
#% 169732
#% 214709
#% 232719
#% 313719
#% 329090
#% 329092
#% 329094
#% 329114
#% 346553
#% 642985
#% 643000
#% 643001
#% 655488
#% 766496
#% 818260
#% 1455912
#! Term relevance feedback has had a long history in information retrieval. However, research on interactive term relevance feedback has yielded mixed results. In this paper, we investigate several aspects related to the elicitation of term relevance feedback: the display of document surrogates, the technique for identifying or selecting terms, and sources of expansion terms. We conduct a between subjects experiment (n=61) of three term relevance feedback interfaces using the 2005 TREC HARD collection, and evaluate each interface with respect to query length and retrieval performance. Results demonstrate that queries created with each experimental interface significantly outperformed corresponding baseline queries, even though there were no differences in performance between interface conditions. Results also demonstrate that pseudo-relevance feedback runs outperformed both baseline and experimental runs as assessed by recall-oriented measures, but that user-generated terms improved precision.

#index 879622
#* Find-similar: similarity browsing as a search tool
#@ Mark D. Smucker;James Allan
#t 2006
#c 13
#% 60987
#% 111303
#% 118726
#% 118728
#% 144034
#% 157521
#% 157907
#% 188076
#% 214709
#% 217248
#% 218992
#% 259990
#% 262036
#% 262096
#% 280857
#% 297550
#% 309088
#% 316513
#% 323131
#% 340901
#% 340948
#% 427921
#% 717133
#% 742666
#% 752177
#% 789959
#% 810906
#% 818262
#% 1275346
#! Search systems have for some time provided users with the ability to request documents similar to a given document. Interfaces provide this feature via a link or button for each document in the search results. We call this feature find-similar or similarity browsing. We examined find-similar as a search tool, like relevance feedback, for improving retrieval performance. Our investigation focused on find-similar's document-to-document similarity, the reexamination of documents during a search, and the user's browsing pattern. Find-similar with a query-biased similarity, avoiding the reexamination of documents, and a breadth-like browsing pattern achieved a 23% increase in the arithmetic mean average precision and a 66% increase in the geometric mean average precision over our baseline retrieval. This performance matched that of a more traditionally styled iterative relevance feedback technique.

#index 879623
#* Exploring the limits of single-iteration clarification dialogs
#@ Jimmy Lin;Philip Wu;Dina Demner-Fushman;Eileen Abels
#t 2006
#c 13
#% 4676
#% 169729
#% 186518
#% 214709
#% 218992
#% 219014
#% 235292
#% 280793
#% 309089
#% 340951
#% 766497
#% 818253
#% 818257
#! Single-iteration clarification dialogs, as implemented in the TREC HARD track, represent an attempt to introduce interaction into ad hoc retrieval, while preserving the many benefits of large-scale evaluations. Although previous experiments have not conclusively demonstrated performance gains resulting from such interactions, it is unclear whether these findings speak to the nature of clarification dialogs, or simply the limitations of current systems. To probe the limits of such interactions, we employed a human intermediary to formulate clarification questions and exploit user responses. In addition to establishing a plausible upper bound on performance, we were also able to induce an "ontology of clarifications" to characterize human behavior. This ontology, in turn, serves as the input to a regression model that attempts to determine which types of clarification questions are most helpful. Our work can serve to inform the design of interactive systems that initiate user dialogs.

#index 879624
#* Large scale semi-supervised linear SVMs
#@ Vikas Sindhwani;S. Sathiya Keerthi
#t 2006
#c 13
#% 78887
#% 190581
#% 304876
#% 466263
#% 763708
#% 876050
#! Large scale learning is often realistic only in a semi-supervised setting where a small set of labeled examples is available together with a large collection of unlabeled data. In many information retrieval and data mining applications, linear classifiers are strongly preferred because of their ease of implementation, interpretability and empirical performance. In this work, we present a family of semi-supervised linear support vector classifiers that are designed to handle partially-labeled sparse datasets with possibly very large number of examples and features. At their core, our algorithms employ recently developed modified finite Newton techniques. Our contributions in this paper are as follows: (a) We provide an implementation of Transductive SVM (TSVM) that is significantly more efficient and scalable than currently used dual techniques, for linear classification problems involving large, sparse datasets. (b) We propose a variant of TSVM that involves multiple switching of labels. Experimental results show that this variant provides an order of magnitude further improvement in training efficiency. (c) We present a new algorithm for semi-supervised learning based on a Deterministic Annealing (DA) approach. This algorithm alleviates the problem of local minimum in the TSVM optimization procedure while also being computationally attractive. We conduct an empirical study on several document classification tasks which confirms the value of our methods in large scale semi-supervised settings.

#index 879625
#* Graph-based text classification: learn from your neighbors
#@ Ralitsa Angelova;Gerhard Weikum
#t 2006
#c 13
#% 248810
#% 309142
#% 316709
#% 338741
#% 342596
#% 451536
#% 464434
#% 593940
#% 731608
#% 731611
#% 769884
#% 771846
#% 799737
#% 1558464
#! Automatic classification of data items, based on training samples, can be boosted by considering the neighborhood of data items in a graph structure (e.g., neighboring documents in a hyperlink environment or co-authors and their publications for bibliographic data entries). This paper presents a new method for graph-based classification, with particular emphasis on hyperlinked text documents but broader applicability. Our approach is based on iterative relaxation labeling and can be combined with either Bayesian or SVM classifiers on the feature spaces of the given data items. The graph neighborhood is taken into consideration to exploit locality patterns while at the same time avoiding overfitting. In contrast to prior work along these lines, our approach employs a number of novel techniques: dynamically inferring the link/class pattern in the graph in the run of the iterative relaxation labeling, judicious pruning of edges from the neighborhood graph based on node dissimilarities and node degrees, weighting the influence of edges based on a distance metric between the classification labels of interest and weighting edges by content similarity measures. Our techniques considerably improve the robustness and accuracy of the classification outcome, as shown in systematic experimental comparisons with previously published methods on three different real-world datasets.

#index 879626
#* Constructing informative prior distributions from domain knowledge in text classification
#@ Aynur Dayanik;David D. Lewis;David Madigan;Vladimir Menkov;Alexander Genkin
#t 2006
#c 13
#% 46803
#% 109191
#% 169806
#% 194283
#% 344447
#% 375017
#% 397135
#% 402289
#% 406493
#% 420507
#% 424722
#% 458379
#% 464465
#% 642998
#% 763708
#% 769908
#% 840583
#% 1250186
#% 1289485
#% 1289518
#% 1478820
#! Supervised learning approaches to text classification are in practice often required to work with small and unsystematically collected training sets. The alternative to supervised learning is usually viewed to be building classifiers by hand, using a domain expert's understanding of which features of the text are related to the class of interest. This is expensive, requires a degree of sophistication about linguistics and classification, and makes it difficult to use combinations of weak predictors. We propose instead combining domain knowledge with training examples in a Bayesian framework. Domain knowledge is used to specify a prior distribution for the parameters of a logistic regression model, and labeled training data is used to produce a posterior distribution, whose mode we take as the final classifier. We show on three text categorization data sets that this approach can rescue what would otherwise be disastrously bad training situations, producing much more effective classifiers.

#index 879627
#* Unifying user-based and item-based collaborative filtering approaches by similarity fusion
#@ Jun Wang;Arjen P. de Vries;Marcel J. T. Reinders
#t 2006
#c 13
#% 173879
#% 251145
#% 280852
#% 330687
#% 397127
#% 397153
#% 420515
#% 452563
#% 528156
#% 734592
#% 734593
#% 734594
#% 766449
#% 818216
#% 840924
#% 1650569
#% 1742072
#! Memory-based methods for collaborative filtering predict new ratings by averaging (weighted) ratings between, respectively, pairs of similar users or items. In practice, a large number of ratings from similar users or similar items are not available, due to the sparsity inherent to rating data. Consequently, prediction quality can be poor. This paper re-formulates the memory-based collaborative filtering problem in a generative probabilistic framework, treating individual user-item ratings as predictors of missing ratings. The final rating is estimated by fusing predictions from three sources: predictions based on ratings of the same item by other users, predictions based on different item ratings made by the same user, and, third, ratings predicted based on data from other but similar users rating other but similar items. Existing user-based and item-based approaches correspond to the two simple cases of our framework. The complete model is however more robust to data sparsity, because the different types of ratings are used in concert, while additional ratings from similar users towards similar items are employed as a background model to smooth the predictions. Experiments demonstrate that the proposed methods are indeed more robust against data sparsity and give better recommendations.

#index 879628
#* Personalized recommendation driven by information flow
#@ Xiaodan Song;Belle L. Tseng;Ching-Yung Lin;Ming-Ting Sun
#t 2006
#c 13
#% 173879
#% 202011
#% 268079
#% 290830
#% 307247
#% 323130
#% 342596
#% 420121
#% 452563
#% 577217
#% 577356
#% 722904
#% 729923
#% 754098
#% 754107
#% 769955
#% 790459
#% 801785
#% 813966
#% 823373
#% 1650569
#! We propose that the information access behavior of a group of people can be modeled as an information flow issue, in which people intentionally or unintentionally influence and inspire each other, thus creating an interest in retrieving or getting a specific kind of information or product. Information flow models how information is propagated in a social network. It can be a real social network where interactions between people reside; it can be, moreover, a virtual social network in that people only influence each other unintentionally, for instance, through collaborative filtering. We leverage users' access patterns to model information flow and generate effective personalized recommendations. First, an early adoption based information flow (EABIF) network describes the influential relationships between people. Second, based on the fact that adoption is typically category specific, we propose a topic-sensitive EABIF (TEABIF) network, in which access patterns are clustered with respect to the categories. Once an item has been accessed by early adopters, personalized recommendations are achieved by estimating whom the information will be propagated to with high probabilities. In our experiments with an online document recommendation system, the results demonstrate that the EABIF and the TEABIF can respectively achieve an improved (precision, recall) of (91.0%, 87.1%) and (108.5%, 112.8%) compared to traditional collaborative filtering, given an early adopter exists.

#index 879629
#* Analysis of a low-dimensional linear model under recommendation attacks
#@ Sheng Zhang;Yi Ouyang;James Ford;Fillia Makedon
#t 2006
#c 13
#% 173879
#% 224113
#% 280852
#% 330687
#% 338443
#% 397153
#% 397155
#% 420515
#% 465928
#% 528156
#% 734592
#% 754097
#% 783438
#% 836266
#% 844357
#! Collaborative filtering techniques have become popular in the past decade as an effective way to help people deal with information overload. Recent research has identified significant vulnerabilities in collaborative filtering techniques. Shilling attacks, in which attackers introduce biased ratings to influence recommendation systems, have been shown to be effective against memory-based collaborative filtering algorithms. We examine the effectiveness of two popular shilling attacks (the random attack and the average attack) on a model-based algorithm that uses Singular Value Decomposition (SVD) to learn a low-dimensional linear model. Our results show that the SVD-based algorithm is much more resistant to shilling attacks than memory-based algorithms. Furthermore, we develop an attack detection method directly built on the SVD-based algorithm and show that this method detects random shilling attacks with high detection rates and very low false alarm rates.

#index 879630
#* Evaluating evaluation metrics based on the bootstrap
#@ Tetsuya Sakai
#t 2006
#c 13
#% 144074
#% 236052
#% 309093
#% 397163
#% 411762
#% 816367
#% 818222
#% 818309
#% 948924
#% 1676567
#! This paper describes how the Bootstrap approach to statistics can be applied to the evaluation of IR effectiveness metrics. First, we argue that Bootstrap Hypothesis Tests deserve more attention from the IR community, as they are based on fewer assumptions than traditional statistical significance tests. We then describe straightforward methods for comparing the sensitivity of IR metrics based on Bootstrap Hypothesis Tests. Unlike the heuristics-based "swap" method proposed by Voorhees and Buckley, our method estimates the performance difference required to achieve a given significance level directly from Bootstrap Hypothesis Test results. In addition, we describe a simple way of examining the accuracy of rank correlation between two metrics based on the Bootstrap Estimate of Standard Error. We demonstrate the usefulness of our methods using test collections and runs from the NTCIR CLIR track for comparing seven IR metrics, including those that can handle graded relevance and those based on the Geometric Mean.

#index 879631
#* Statistical precision of information retrieval evaluation
#@ Gordon V. Cormack;Thomas R. Lynam
#t 2006
#c 13
#% 133892
#% 144074
#% 236052
#% 262097
#% 262102
#% 262105
#% 309093
#% 397163
#% 766409
#% 766410
#% 818222
#% 855048
#% 857180
#! We introduce and validate bootstrap techniques to compute confidence intervals that quantify the effect of test-collection variability on average precision (AP) and mean average precision (MAP) IR effectiveness measures. We consider the test collection in IR evaluation to be a representative of a population of materially similar collections, whose documents are drawn from an infinite pool with similar characteristics. Our model accurately predicts the degree of concordance between system results on randomly selected halves of the TREC-6 ad hoc corpus. We advance a framework for statistical evaluation that uses the same general framework to model other sources of chance variation as a source of input for meta-analysis techniques.

#index 879632
#* A statistical method for system evaluation using incomplete judgments
#@ Javed A. Aslam;Virgil Pavlu;Emine Yilmaz
#t 2006
#c 13
#% 184486
#% 262034
#% 262097
#% 262102
#% 730072
#% 766409
#% 818276
#% 855048
#! We consider the problem of large-scale retrieval evaluation, and we propose a statistical method for evaluating retrieval systems using incomplete judgments. Unlike existing techniques that (1) rely on effectively complete, and thus prohibitively expensive, relevance judgment sets, (2) produce biased estimates of standard performance measures, or (3) produce estimates of non-standard measures thought to be correlated with these standard measures, our proposed statistical technique produces unbiased estimates of the standard measures themselves.Our proposed technique is based on random sampling. While our estimates are unbiased by statistical design, their variance is dependent on the sampling distribution employed; as such, we derive a sampling distribution likely to yield low variance estimates. We test our proposed technique using benchmark TREC data, demonstrating that a sampling pool derived from a set of runs can be used to efficiently and effectively evaluate those runs. We further show that these sampling pools generalize well to unseen runs. Our experiments indicate that highly accurate estimates of standard performance measures can be obtained using a number of relevance judgments as small as 4% of the typical TREC-style judgment pool.

#index 879633
#* Learning to advertise
#@ Anísio Lacerda;Marco Cristo;Marcos André Gonçalves;Weiguo Fan;Nivio Ziviani;Berthier Ribeiro-Neto
#t 2006
#c 13
#% 54970
#% 124073
#% 312693
#% 385254
#% 387427
#% 411248
#% 572481
#% 608027
#% 733838
#% 740768
#% 769475
#% 773034
#% 785390
#% 804914
#% 818265
#% 838503
#% 854961
#% 950864
#% 987170
#! Content-targeted advertising, the task of automatically associating ads to a Web page, constitutes a key Web monetization strategy nowadays. Further, it introduces new challenging technical problems and raises interesting questions. For instance, how to design ranking functions able to satisfy conflicting goals such as selecting advertisements (ads) that are relevant to the users and suitable and profitable to the publishers and advertisers? In this paper we propose a new framework for associating ads with web pages based on Genetic Programming (GP). Our GP method aims at learning functions that select the most appropriate ads, given the contents of a Web page. These ranking functions are designed to optimize overall precision and minimize the number of misplacements. By using a real ad collection and web pages from a newspaper, we obtained a gain over a state-of-the-art baseline method of 61.7% in average precision. Further, by evolving individuals to provide good ranking estimations, GP was able to discover ranking functions that are very effective in placing ads in web pages while avoiding irrelevant ones.

#index 879634
#* Getting work done on the web: supporting transactional queries
#@ Yunyao Li;Rajasekar Krishnamurthy;Shivakumar Vaithyanathan;H. V. Jagadish
#t 2006
#c 13
#% 278107
#% 292563
#% 340928
#% 397126
#% 577339
#% 590523
#% 642982
#% 746867
#% 754059
#% 764558
#% 766441
#% 805878
#% 855310
#% 1676560
#! Many searches on the web have a transactional intent. We argue that pages satisfying transactional needs can be distinguished from the more common pages that have some information and links, but cannot be used to execute a transaction. Based on this hypothesis, we provide a recipe for constructing a transaction annotator. By constructing an annotator with one corpus and then demonstrating its classification performance on another,we establish its robustness. Finally, we show experimentally that a search procedure that exploits such pre-annotation greatly outperforms traditional search for retrieving transactional pages.

#index 879635
#* You are what you say: privacy risks of public mentions
#@ Dan Frankowski;Dan Cosley;Shilad Sen;Loren Terveen;John Riedl
#t 2006
#c 13
#% 220707
#% 300184
#% 301553
#% 330687
#% 397153
#% 433968
#% 576761
#% 576762
#% 577233
#% 577355
#% 727866
#% 742048
#% 754061
#% 754097
#% 755205
#% 854646
#% 860114
#% 993988
#! In today's data-rich networked world, people express many aspects of their lives online. It is common to segregate different aspects in different places: you might write opinionated rants about movies in your blog under a pseudonym while participating in a forum or web site for scholarly discussion of medical ethics under your real name. However, it may be possible to link these separate identities, because the movies, journal articles, or authors you mention are from a sparse relation space whose properties (e.g., many items related to by only a few users) allow re-identification. This re-identification violates people's intentions to separate aspects of their life and can have negative consequences; it also may allow other privacy violations, such as obtaining a stronger identifier like name and address.This paper examines this general problem in a specific setting: re-identification of users from a public web movie forum in a private movie ratings dataset. We present three major results. First, we develop algorithms that can re-identify a large proportion of public users in a sparse relation space. Second, we evaluate whether private dataset owners can protect user privacy by hiding data; we show that this requires extensive and undesirable changes to the dataset, making it impractical. Third, we evaluate two methods for users in a public forum to protect their own privacy, suppression and misdirection. Suppression doesn't work here either. However, we show that a simple misdirection strategy works well: mention a few popular items that you haven't rated.

#index 879636
#* A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization
#@ Ani Nenkova;Lucy Vanderwende;Kathleen McKeown
#t 2006
#c 13
#% 194251
#% 262112
#% 316519
#% 428369
#% 742224
#% 816173
#% 817466
#% 846586
#% 854191
#% 855375
#% 938729
#% 995507
#% 995508
#% 1264943
#! The usual approach for automatic summarization is sentence extraction, where key sentences from the input documents are selected based on a suite of features. While word frequency often is used as a feature in summarization, its impact on system performance has not been isolated. In this paper, we study the contribution to summarization of three factors related to frequency: content word frequency, composition functions for estimating sentence importance from word frequency, and adjustment of frequency weights based on context. We carry out our analysis using datasets from the Document Understanding Conferences, studying not only the impact of these features on automatic summarizers, but also their role in human summarization. Our research shows that a frequency based summarizer can achieve performance comparable to that of state-of-the-art systems, but only with a good composition function; context sensitivity improves performance and significantly reduces repetition.

#index 879637
#* Information graphics: an untapped resource for digital libraries
#@ Sandra Carberry;Stephanie Elzer;Seniz Demir
#t 2006
#c 13
#% 9197
#% 120270
#% 147680
#% 228351
#% 239577
#% 292187
#% 316199
#% 420485
#% 437405
#% 529156
#% 625272
#% 635755
#% 780433
#% 939359
#% 1707775
#! Information graphics are non-pictorial graphics such as bar charts and line graphs that depict attributes of entities and relations among entities. Most information graphics appearing in popular media have a communicative goal or intended message; consequently, information graphics constitute a form of language. This paper argues that information graphics are a valuable knowledge resource that should be retrievable from a digital library and that such graphics should be taken into account when summarizing a multimodal document for subsequent indexing and retrieval. But to accomplish this, the information graphic must be understood and its message recognized. The paper presents our Bayesian system for recognizing the primary message of one kind of information graphic (simple bar charts) and discusses the potential role of an information graphic's message in indexing graphics and summarizing multimodal documents.

#index 879638
#* News to go: hierarchical text summarization for mobile devices
#@ Jahna Otterbacher;Dragomir Radev;Omer Kareem
#t 2006
#c 13
#% 330780
#% 342534
#% 342962
#% 577322
#% 577323
#% 754091
#% 818228
#% 818333
#% 853647
#! We present an evaluation of a novel hierarchical text summarization method that allows users to view summaries of Web documents from small, mobile devices. Unlike previous approaches, ours does not require the documents to be in HTML since it infers a hierarchical structure automatically. Currently, the method is used to summarize news articles sent to a Web mail account in plain text format. Subjects used a Web-enabled mobile phone emulator to access the account's inbox and view the summarized news articles. They then used the summaries to complete several information-seeking tasks, which involved answering factual questions about the stories. In comparing the hierarchical text summary setting to that in which subjects were given the full text articles, there was no significant difference in task accuracy or the time taken to complete the task. However, in the hierarchical summarization setting, the number of bytes transferred per user request is less than half that of the full text case. Finally, in comparing the new method to three other summarization methods, subjects achieved significantly better accuracy on the tasks when using hierarchical summaries.

#index 879639
#* Clustering of search results using temporal attributes
#@ Omar Alonso;Michael Gertz
#t 2006
#c 13
#% 262045
#% 296738
#% 768285
#% 805898
#% 854249
#! Clustering of search results is an important feature in many of today's information retrieval applications. The notion of hit list clustering appears in Web search engines and enterprise search engines as a mechanism that allows users to further explore the coverage of a query. However, there has been little work on exposing temporal attributes for constructing and presentation of clusters. These attributes appear in documents as part of the textual content, e.g., as a date and time token or as a temporal reference in a sentence. In this paper, we outline a model and describe a prototype that shows the main ideas.

#index 879640
#* A complex document information processing prototype
#@ S. Argamon;G. Agam;O. Frieder;D. Grossman;D. Lewis;G. Sohn;K. Voorhees
#t 2006
#c 13
#% 879673
#% 879691
#! We developed a prototype for integrated retrieval and aggregation of diverse information contained in scanned paper documents. Such complex document information processing combines several forms of image processing together with textual/linguistic processing to enable effective analysis of complex document collections, a necessity for a wide range of applications. This is the first system to attempt integrated retrieval from complex documents; we report its current capabilities.

#index 879641
#* Inferring document relevance via average precision
#@ Javed A. Aslam;Emine Yilmaz
#t 2006
#c 13
#% 818205
#% 879632
#! We consider the problem of evaluating retrieval systems using a limited number of relevance judgments. Recent work has demonstrated that one can accurately estimate average precision via a judged pool corresponding to a relatively small random sample of documents. In this work, we demonstrate that given values or estimates of average precision, one can accurately infer the relevances of unjudged documents. Combined, we thus show how one can efficiently and accurately infer a large judged pool from a relatively small number of judged documents, thus permitting accurate and efficient retrieval evaluation on a large scale.

#index 879642
#* Automatic construction of known-item finding test beds
#@ Leif Azzopardi;Maarten de Rijke
#t 2006
#c 13
#% 280851
#% 340146

#index 879643
#* Adaptive query-based sampling for distributed IR
#@ Leif Azzopardi;Mark Baillie;Fabio Crestani
#t 2006
#c 13
#% 282422
#% 340146
#% 729437

#index 879644
#* PENG: integrated search of distributed news archives
#@ Mark Baillie;Fabio Crestani;Monica Landoni
#t 2006
#c 13
#% 879643

#index 879645
#* Examining assessor attributes at HARD 2005
#@ Mark Baillie;Ian Ruthven
#t 2006
#c 13

#index 879646
#* User expectations from XML element retrieval
#@ Stamatina Betsi;Mounia Lalmas;Anastasios Tombros;Theodora Tsikrika
#t 2006
#c 13
#% 766415
#% 1721882
#! The primary aim of XML element retrieval is to return to users XML elements, rather than whole documents. This poster describes a small study, in which we elicited users' expectations, i.e. their anticipated experience, when interacting with an XML retrieval system, as compared to a traditional 'flat' document retrieval system.

#index 879647
#* Theoretical benchmarks of XML retrieval
#@ Tobias Blanke;Mounia Lalmas
#t 2006
#c 13
#% 342819
#% 398897
#! This poster investigates the use of theoretical benchmarks to describe the matching functions of XML retrieval systems and the properties of specificity and exhaustivity in XML retrieval. Theoretical benchmarks concern the formal representation of qualitative properties of IR models. To this end, Situation Theory framework for the meta-evaluation of XML retrieval is presented.

#index 879648
#* Question classification with log-linear models
#@ Phil Blunsom;Krystle Kocik;James R. Curran
#t 2006
#c 13
#% 815303
#% 938666
#! Question classification has become a crucial step in modern question answering systems. Previous work has demonstrated the effectiveness of statistical machine learning approaches to this problem. This paper presents a new approach to building a question classifier using log-linear models. Evidence from a rich and diverse set of syntactic and semantic features is evaluated, as well as approaches which exploit the hierarchical structure of the question classes.

#index 879649
#* Community-based snippet-indexes for pseudo-anonymous personalization in web search
#@ Oisín Boydell;Barry Smyth
#t 2006
#c 13
#% 340916
#% 399057
#% 433674
#% 478627
#% 803556
#! We describe and evaluate an approach to personalizing Web search that involves post-processing the results returned by some underlying search engine so that they re .ect the interests of a community of like-minded searchers.To do this we leverage the search experiences of the community by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our approach seeks to build a community-based snippet index that re .ects the evolving interests of a group of searchers. This index is then sed to re-rank the results returned by the underlying search engine by boosting the ranking of key results that have been freq ently selected for similar q eries by community members in the past.

#index 879650
#* Bias and the limits of pooling
#@ Chris Buckley;Darrin Dimmick;Ian Soboroff;Ellen Voorhees
#t 2006
#c 13
#% 194301
#% 262097
#% 262102
#! Modern retrieval test collections are built through a process called pooling in which only a sample of the entire document set is judged for each topic. The idea behind pooling is to find enough relevant documents such that when unjudged documents are assumed to be nonrelevant the resulting judgment set is sufficiently complete and unbiased. As document sets grow larger, a constant-size pool represents an increasingly small percentage of the document set, and at some point the assumption of approximately complete judgments must become invalid.This paper demonstrates that the AQUAINT 2005 test collection exhibits bias caused by pools that were too shallow for the document set size despite having many diverse runs contribute to the pools. The existing judgment set favors relevant documents that contain topic title words even though relevant documents containing few topic title words are known to exist in the document set. The paper concludes with suggested modifications to traditional pooling and evaluation methodology that may allow very large reusable test collections to be built.

#index 879651
#* Term proximity scoring for ad-hoc retrieval on very large text collections
#@ Stefan Büttcher;Charles L. A. Clarke;Brad Lushman
#t 2006
#c 13
#% 1387547
#! We propose an integration of term proximity scoring into Okapi BM25. The relative retrieval effectiveness of our retrieval method, compared to pure BM25, varies from collection to collection.We present an experimental evaluation of our method and show that the gains achieved over BM25 as the size of the underlying text collection increases. We also show that for stemmed queries the impact of term proximity scoring is larger than for unstemmed queries.

#index 879652
#* An exploratory web log study of multitasking
#@ Nikolai (Nick) Buzikashvili
#t 2006
#c 13
#% 401407
#% 449294
#% 872034
#! The Web search multitasking study based on automatic task session detection procedure is described. The results of the study: 1) multitasking is very rare, 2) it usually covers only 2 task sessions, 3) it is frequently formed into a temporal inclusion of an interrupting task session into the interrupted session, 4) the quantitative characteristics of multitasking greatly differ from the characteristics of sequential execution of one and several tasks. A searcher minimizes task switching costs: he avoids multitasking and while multitasking he uses cheapest manner of task switching.

#index 879653
#* Tensor space model for document analysis
#@ Deng Cai;Xiaofei He;Jiawei Han
#t 2006
#c 13
#% 321635
#% 846431
#! Vector Space Model (VSM) has been at the core of information retrieval for the past decades. VSM considers the documents as vectors in high dimensional space.In such a vector space, techniques like Latent Semantic Indexing (LSI), Support Vector Machines (SVM), Naive Bayes, etc., can be then applied for indexing and classification. However, in some cases, the dimensionality of the document space might be extremely large, which makes these techniques infeasible due to the curse of dimensionality. In this paper, we propose a novel Tensor Space Model for document analysis. We represent documents as the second order tensors, or matrices. Correspondingly, a novel indexing algorithm called Tensor Latent Semantic Indexing (TensorLSI) is developed in the tensor space. Our theoretical analysis shows that TensorLSI is much more computationally efficient than the conventional Latent Semantic Indexing, which makes it applicable for extremely large scale data set. Several experimental results on standard document data sets demonstrate the efficiency and effectiveness of our algorithm.

#index 879654
#* First large-scale information retrieval experiments on turkish texts
#@ Fazli Can;Seyit Kocberber;Erman Balcik;Cihan Kaynak;H. Cagdas Ocalan;Onur M. Vursavas
#t 2006
#c 13
#% 46803
#% 290703
#% 766409
#! We present the results of the first large-scale Turkish information retrieval experiments performed on a TREC-like test collection. The test bed, which has been created for this study, contains 95.5 million words, 408,305 documents, 72 ad hoc queries and has a size of about 800MB. All documents come from the Turkish newspaper Milliyet. We implement and apply simple to sophisticated stemmers and various query-document matching functions and show that truncating words at a prefix length of 5 creates an effective retrieval environment in Turkish. However, a lemmatizer-based stemmer provides significantly better effectiveness over a variety of matching functions.

#index 879655
#* Learning a ranking from pairwise preferences
#@ Ben Carterette;Desislava Petkova
#t 2006
#c 13
#% 169774
#% 340936
#% 995515
#! We introduce a novel approach to combining rankings from multiple retrieval systems. We use a logistic regression model or an SVM to learn a ranking from pairwise document preferences. Our approach requires no training data or relevance scores, and outperforms a popular voting algorithm.

#index 879656
#* Automated performance assessment in interactive QA
#@ Joyce Y. Chai;Tyler Baldwin;Chen Zhang
#t 2006
#c 13
#% 465914
#! In interactive question answering (QA), users and systems take turns to ask questions and provide answers. In such an interactive setting, user questions largely depend on the answers provided by the system. One question is whether user follow-up questions can provide feedback for the system to automatically assess its performance (e.g., assess whether a correct answer is delivered). This self-awareness can make QA systems more intelligent for information seeking, for example, by adapting better strategies to cope with problematic situations. Therefore, this paper describes our initial investigation in addressing this problem. Our results indicate that interaction context can provide useful cues for automated performance assessment in interactive QA.

#index 879657
#* Stylistic text segmentation
#@ Paul J. Chase;Shlomo Argamon
#t 2006
#c 13
#% 280835
#% 448786
#% 449749
#% 741058
#% 786504
#! This paper focuses on a method for the stylistic segmentation of text documents. Our technique involves mapping the change in a feature throughout a text. We use the linguistic features of conjunction and modality, through taxonomies from Systemic Functional Linguistics. This segmentation has applications in automated summarization, particularly of large documents.

#index 879658
#* On hierarchical web catalog integration with conceptual relationships in thesaurus
#@ Ing-Xiang Chen;Jui-Chi Ho;Cheng-Zen Yang
#t 2006
#c 13
#% 309141
#% 330767
#% 729927
#% 1676614
#! Web catalog integration is an interesting problem in current digital content management. Past studies have shown that using a flattened structure with auxiliary information extracted from the source catalog can improve the integration results. However, the nature of a flattened structure ignores the hierarchical relationships, and thus the performance improvement of catalog integration may be reduced. In this paper, we propose an enhanced hierarchical catalog integration (EHCI) approach with conceptual thesauri extracted from the source catalog. The results show that our enhanced hierarchical integration approach effectively boosts the accuracy of hierarchical catalog integration.

#index 879659
#* Rpref: a generalization of Bpref towards graded relevance judgments
#@ Jan De Beer;Marie-Francine Moens
#t 2006
#c 13
#% 43863
#% 280041
#% 312689
#% 575729
#% 766409
#! We present rpref ; our generalization of the bpref evaluation metric for assessing the quality of search engine results, given graded rather than binary user relevance judgments.

#index 879660
#* A new web page summarization method
#@ Qian Diao;Jiulong Shan
#t 2006
#c 13
#% 262112
#% 316519
#% 938761
#% 1272053
#! In this paper, we present a novel multi-webpage summarization algorithm. It adds the graph based ranking algorithm into the framework of Maximum Marginal Relevance (MMR) method, to not only capture the main topic of the web pages but also eliminate the redundancy existing in the sentences of the summary result. The experiment result indicates that the new approach has the better performance than the previous methods.

#index 879661
#* NMF and PLSI: equivalence and a hybrid algorithm
#@ Chris Ding;Tao Li;Wei Peng
#t 2006
#c 13
#% 766434
#% 818291
#% 1250561
#% 1650298
#! In this paper, we show that PLSI and NMF optimize the same objective function, although PLSI and NMF are different algorithms as verified by experiments. In addition, we also propose a new hybrid method that runs PLSI and NMF alternatively to achieve better solutions.

#index 879662
#* Using historical data to enhance rank aggregation
#@ Miriam Fernández;David Vallet;Pablo Castells
#t 2006
#c 13
#% 232703
#% 340934
#% 342710
#% 728360
#! Rank aggregation is a pervading operation in IR technology. We hypothesize that the performance of score-based aggregation may be affected by artificial, usually meaningless deviations consistently occurring in the input score distributions, which distort the combined result when the individual biases differ from each other. We propose a score-based rank aggregation model where the source scores are normalized to a common distribution before being combined. Early experiments on available data from several TREC collections are shown to support our proposal.

#index 879663
#* Enterprise search behaviour of software engineers
#@ Luanne Freund;Elaine G. Toms
#t 2006
#c 13
#% 323131
#% 323135
#% 577339
#% 581916
#% 764564
#% 818258
#! Technical professionals spend ~25% of their time at work searching for information, and have specialized information needs that are not well-served by generic enterprise search tools. In this study, we investigated how a group of software engineers use a workplace search system. We identify patterns of search behaviour specific to this group and distinct from general web and intranet search patterns, and make design recommendations for search systems that will better serve the needs of this group.

#index 879664
#* Evaluating sources of query expansion terms
#@ Xin Fu;Diane Kelly
#t 2006
#c 13
#% 187997
#% 643001
#% 818260
#% 879621
#! This study investigates the effectiveness of retrieval systems and human users in generating terms for query expansion. We compare three sources of terms: system generated terms, terms users select from top-ranked sentences, and user generated terms. Results demonstrate that overall the system generated more effective expansion terms than users, but that users' selection of terms improved precision at the top of the retrieved document list.

#index 879665
#* Comparing two blind relevance feedback techniques
#@ Daqing He;Yefei Peng
#t 2006
#c 13
#% 118726

#index 879666
#* Information retrieval with commonsense knowledge
#@ Ming-Hung Hsu;Hsin-Hsi Chen
#t 2006
#c 13
#% 54413
#% 198055
#% 766439
#% 766440
#% 783633
#% 1674954
#! This paper employs ConceptNet, which covers a rich set of commonsense concepts, to retrieve images with text descriptions by focusing on spatial relationships. Evaluation on test data of the 2005 ImageCLEF shows that integrating commonsense knowledge in information retrieval is feasible.

#index 879667
#* Refining hierarchical taxonomy structure via semi-supervised learning
#@ Ruizhang Huang;Zhigang Zhang;Wai Lam
#t 2006
#c 13
#% 528008
#% 769881
#% 770782
#% 806594

#index 879668
#* Quantative analysis of the impact of judging inconsistency on the performance of relevance feedback
#@ Xiangyu Jin;James French;Jonathan Michel
#t 2006
#c 13
#% 528208
#% 1740552
#! Practical constrains of user interfaces make the user's judgment (during the feedback loop) deviate from real thoughts (when the full document is read).This is often overlooked in evaluation of relevance feedback.This paper quantitatively analyze the impact of judging inconsistency on the performance of relevance feedback.

#index 879669
#* Swordfish: an unsupervised Ngram based approach to morphological analysis
#@ Chris Jordan;John Healy;Vlado Keselj
#t 2006
#c 13
#% 387427
#% 741043
#% 741122
#! Extracting morphemes from words is a nontrivial task. Rule based stemming approaches such as Porter's algorithm have encountered some success, however they are restricted by their ability to identify a limited number of affixes and are language dependent. When dealing with languages with many affixes, rule based approaches generally require many more rules to deal with all the possible word forms. Deriving these rules requires a larger effort on the part of linguists and in some instances can be simply impractical. We propose an unsupervised ngram based approach, named Swordfish. Using ngram probabilities in the corpus, possible morphemes are identified. We look at two possible methods for identifying candidate morphemes, one using joint probabilities between two ngrams, and the second based on log odds between prefix probabilities. Initial results indicate the joint probability approach to be better for English while the prefix ratio approach is better for Finnish and Turkish.

#index 879670
#* Authorship attribution with thousands of candidate authors
#@ Moshe Koppel;Jonathan Schler;Shlomo Argamon;Eran Messeri
#t 2006
#c 13
#% 46803
#% 578558
#% 735077
#% 748026
#! In this paper, we use a blog corpus to demonstrate that we can often identify the author of an anonymous text even where there are many thousands of candidate authors. Our approach combines standard information retrieval methods with a text categorization meta-learning scheme that determines when to even venture a guess.

#index 879671
#* Simple questions to improve pseudo-relevance feedback results
#@ Giridhar Kumaran;James Allan
#t 2006
#c 13
#% 54435
#% 218978
#% 340901
#% 642985
#% 643001
#% 766497
#% 783506
#! We explore interactive methods to further improve the performance of pseudo-relevance feedback. Studies \citeria suggest that new methods for tackling difficult queries are required. Our approach is to gather more information about the query from the user by asking her simple questions. The equally simple responses are used to modify the original query. Our experiments using the TREC Robust Track queries show that we can obtain a significant improvement in mean average precision averaging around 5% over pseudo-relevance feedback. This improvement is also spread across more queries compared to ordinary pseudo-relevance feedback, as suggested by geometric mean average precision.

#index 879672
#* Is XML retrieval meaningful to users?: searcher preferences for full documents vs. elements
#@ Birger Larsen;Anastasios Tombros;Saadia Malik
#t 2006
#c 13
#% 920505
#% 1674746
#! The aim of this study is to investigate whether element retrieval (as opposed to full-text retrieval) is meaningful and useful for searchers when carrying out information-seeking tasks. Our results suggest that searchers find the structural breakdown of documents useful when browsing within retrieved documents, and provide support for the usefulness of element retrieval in interactive settings.

#index 879673
#* Building a test collection for complex document information processing
#@ D. Lewis;G. Agam;S. Argamon;O. Frieder;D. Grossman;J. Heard
#t 2006
#c 13
#% 162362
#% 879640
#! Research and development of information access technology for scanned paper documents has been hampered by the lack of public test collections of realistic scope and complexity. As part of a project to create a prototype system for search and mining of masses of document images, we are assembling a 1.5 terabyte dataset to support evaluation of both end-to-end complex document information processing (CDIP) tasks (e.g., text retrieval and data mining) as well as component technologies such as optical character recognition (OCR), document structure analysis, signature matching, and authorship attribution.

#index 879674
#* Enhancing topic tracking with temporal information
#@ Baoli Li;Wenjie Li;Qin Lu
#t 2006
#c 13
#% 735078
#% 789854
#% 818306
#! In this paper, we propose a new strategy with time granularity reasoning for utilizing temporal information in topic tracking. Compared with previous ones, our work has four distinguished characteristics. Firstly, we try to determine a set of topic times for a target topic from the given on-topic stories. It helps to avoid the negative influence from other irrelevant times. Secondly, we take into account time granularity variance when deciding whether a coreference relationship exists between two times. Thirdly, both publication time and times presented in texts are considered. Finally, as time is only one attribute of a topic, we increase the similarity between a story and a target topic only when they are related not only temporally but also semantically. Experiments on two TDT corpora show that our method makes good use of temporal information in news stories.

#index 879675
#* A comparative study of the effect of search feature design on user experience in digital libraries (DLs)
#@ Yuelin Li;Xiangmin Zhang;Ying Zhang;Jingjing Liu
#t 2006
#c 13
#% 185254
#% 643000
#! This study investigates the impact of different search feature designs in DLs on user search experience. The results indicate that the impact is significant in terms of the number of queries issued, search steps, zero-hits pages returned, and search errors.

#index 879676
#* Representing clusters for retrieval
#@ Xiaoyong Liu;W. Bruce Croft
#t 2006
#c 13
#% 228105
#% 427921
#% 719598
#% 766430
#% 766431

#index 879677
#* One-sided measures for evaluating ranked retrieval effectiveness with spontaneous conversational speech
#@ Baolong Liu;Douglas W. Oard
#t 2006
#c 13
#% 220382
#% 262105
#% 378480
#% 575729
#! Early speech retrieval experiments focused on news broadcasts, for which adequate Automatic Speech Recognition (ASR) accuracy could be obtained. Like newspapers, news broadcasts are a manually selected and arranged set of stories. Evaluation designs reflected that, using known story boundaries as a basis for evaluation. Substantial advances in ASR accuracy now make it possible to build search systems for some types of spontaneous conversational speech, but present evaluation designs continue to rely on known topic boundaries that are no longer well matched to the nature of the materials. We propose a new class of measures for speech retrieval based on manual annotation of points at which a user with specific topical interests would wish replay to begin.

#index 879678
#* Combining fields in known-item email search
#@ Craig Macdonald;Iadh Ounis
#t 2006
#c 13
#! Emails are examples of structured documents with various fields. These fields can be exploited to enhance the retrieval effectiveness of an Information Retrieval (IR) system that mailing list archives. In recent experiments of the TREC2005 Enterprise track, various fields were applied to varying degrees of success by the participants. In his work, using a field-based weighting model, we investigate the retrieval performance attainable by each field, and examine when fields evidence should be combined or not.

#index 879679
#* Improving QA retrieval using document priors
#@ James Mayfield;Paul McNamee
#t 2006
#c 13
#% 280850
#% 742218
#% 818253
#! We present a simple way to improve document retrieval for question answering systems. The method biases the retrieval system toward documents that contain words that have appeared in other documents containing answers to the same type of question. The method works with virtually any retrieval system, and exhibits a statistically significant performance improvement over a strong baseline.

#index 879680
#* Content-based video retrieval: does video's semantic visual feature matter?
#@ Xiangming Mu
#t 2006
#c 13
#! A new shot level video browsing method based on semantic visual features (e.g., car, mountain, and fire) is proposed to facilitate content-based retrieval. The video's binary semantic feature vector is utilized to calculate the score of similarity between two shot keyframes. The score is then used to browse the "similar" keyframes in terms of semantic visual features. A pilot user study was conducted to better understand users' behaviors in video retrieval context. Three video retrieval and browsing systems are compared: temporal neighbor, semantic visual feature, and fused browsing system. The initial results indicated that the semantic visual feature browsing was effective and efficient for Visual Centric tasks, but not for Non-visual Centric tasks.

#index 879681
#* Action modeling: language models that predict query behavior
#@ G. Craig Murray;Jimmy Lin;Abdur Chowdhury
#t 2006
#c 13
#% 581916
#% 818259
#% 838547
#! We present a novel language modeling approach to capturing the query reformulation behavior of Web search users. Based on a framework that categorizes eight different types of "user moves" (adding/removing query terms, etc.), we treat search sessions as sequence data and build n-gram language models to capture user behavior. We evaluated our models in a prediction task. The results suggest that useful patterns of activity can be extracted from user histories. Furthermore, by examining prediction performance under different order n-gram models, we gained insight into the amount of history/context that is associated with different types of user actions. Our work serves as the basis for more refined user models.

#index 879682
#* A method of rating the credibility of news documents on the web
#@ Ryosuke Nagura;Yohei Seki;Noriko Kando;Masaki Aono
#t 2006
#c 13
#! We propose a method to rate the credibility of news articles using three clues: (1) commonality of the contents of articles among different news publishers; (2) numerical agreement versus contradiction of numerical values reported in the articles; and (3) objectivity based on subjective speculative phrases and news sources. We tested this method on news stories taken from seven different news sites on the Web. The average agreement between the system-produced "credibility" and the manual judgments of three human assessors on the 52 sample articles was 69.1%. The limitations of the current approach and future directions are discussed.

#index 879683
#* An analysis of the coupling between training set and neighborhood sizes for the kNN classifier
#@ J. Scott Olsson
#t 2006
#c 13
#% 318412
#% 642986
#% 763708
#! We consider the relationship between training set size and the parameter k for the k-Nearest Neighbors (kNN) classifier. When few examples are available, we observe that accuracy is sensitive to k and that best k tends to increase with training size. We explore the subsequent risk that k tuned on partitions will be suboptimal after aggregation and re-training. This risk is found to be most severe when little data is available. For larger training sizes, accuracy becomes increasingly stable with respect to k and the risk decreases.

#index 879684
#* Fact-focused novelty detection: a feasibility study
#@ Jahna Otterbacher;Dragomir Radev
#t 2006
#c 13
#% 184486
#% 207677
#% 818257
#% 859365
#! Methods for detecting sentences in an input document set, which are both relevant and novel with respect to an information need, would be of direct benefit to many systems, such as extractive text summarizers. However, satisfactory levels of agreement between judges performing this task manually have yet to demonstrated, leaving researchers to conclude that the task is too subjective. In previous experiments, judges were asked to first identify sentences that are relevant to a general topic, and then to eliminate sentences from the list that do not contain new information. Currently, a new task is proposed, in which annotators perform the same procedure, but within the context of a specific, factual information need. In the experiment, satisfactory levels of agreement between independent annotators were achieved on the first step of identifying sentences containing relevant information relevant. However, the results indicate that judges do not agree on which sentences contain novel information.

#index 879685
#* Unity: relevance feedback using user query logs
#@ Jignashu Parikh;Shyam Kapur
#t 2006
#c 13
#% 771927
#! The exponential growth of the Web and the increasing ability of web search engines to index data have led to a problem of plenty. The number of results returned per query is typically in the order of millions of documents for many common queries. Although there is the benefit of added coverage for every query, the problem of ranking these documents and giving the best results gets worse. The problem is even more difficult in case of temporal and ambiguous queries. We try to address this problem using feedback from user query logs. We leverage a technology called Units for generating query refinements which are shown as Also try queries on Yahoo! Search. We consider these refinements as sub-concepts which help define user intent and use them to improve search relevance. The results obtained via live testing on Yahoo! Search are encouraging.

#index 879686
#* Improving personalized web search using result diversification
#@ Filip Radlinski;Susan Dumais
#t 2006
#c 13
#% 262112
#% 397133
#% 754126
#% 805841
#% 805863
#% 818259
#! We present and evaluate methods for diversifying search results to improve personalized web search. A common personalization approach involves reranking the top N search results such that documents likely to be preferred by the user are presented higher. The usefulness of reranking is limited in part by the number and diversity of results considered. We propose three methods to increase the diversity of the top results and evaluate the effectiveness of these methods.

#index 879687
#* Using small XML elements to support relevance
#@ Georgina Ramirez;Thijs Westerveld;Arjen P. de Vries
#t 2006
#c 13
#% 766416
#% 803542
#% 818242
#% 1733303
#! Small XML elements are often estimated relevant by the retrieval model but they are not desirable retrieval units. This paper presents a generic model that exploits the information obtained from small elements. We identify relationships between small and relevant elements and use this linking information to reinforce the relevance of other elements before removing the small ones. Our experiments using the INEX testbed show the effectiveness of our approach.

#index 879688
#* Give me just one highly relevant document: P-measure
#@ Tetsuya Sakai
#t 2006
#c 13
#% 309093
#% 397163
#% 397180
#% 879630
#% 948924
#! We introduce an evaluation metric called P-measure for the task of retrieving

#index 879689
#* Feature diversity in cluster ensembles for robust document clustering
#@ Xavier Sevillano;Germán Cobo;Francesc Alías;Joan Claudi Socoró
#t 2006
#c 13
#% 296738
#% 304915
#% 344447
#% 731938
#% 854822
#! The performance of document clustering systems depends on employing optimal text representations, which are not only difficult to determine beforehand, but also may vary from one clustering problem to another. As a first step towards building robust document clusterers, a strategy based on feature diversity and cluster ensembles is presented in this work. Experiments conducted on a binary clustering problem show that our method is robust to near-optimal model order selection and able to detect constructive interactions between different document representations in the test bed.

#index 879690
#* Lightening the load of document smoothing for better language modeling retrieval
#@ Mark D. Smucker;James Allan
#t 2006
#c 13
#% 262096
#% 340948
#% 397129
#! We hypothesized that language modeling retrieval would improve if we reduced the need for document smoothing to provide an inverse document frequency (IDF) like effect. We created inverse collection frequency (ICF) weighted query models as a tool to partially separate the IDF-like role from document smoothing. Compared to maximum likelihood estimated (MLE) queries, the ICF weighted queries achieved a 6.4\% improvement in mean average precision on description queries. The ICF weighted queries performed better with less document smoothing than that required by MLE queries. Language modeling retrieval may benefit from a means to separately incorporate an IDF-like behavior outside of document smoothing.

#index 879691
#* The effect of OCR errors on stylistic text classification
#@ Sterling Stuart Stein;Shlomo Argamon;Ophir Frieder
#t 2006
#c 13
#% 290482
#% 544499
#% 879673
#! Recently, interest is growing in non-topical text classification tasks such as genre classification, sentiment analysis, and authorship profiling. We study to what extent OCR errors affect stylistic text classification from scanned documents. We find that even a relatively high level of errors in the OCRed documents does not substantially affect stylistic classification accuracy.

#index 879692
#* History repeats itself: repeat queries in Yahoo's logs
#@ Jaime Teevan;Eytan Adar;Rosie Jones;Michael Potts
#t 2006
#c 13
#% 233808
#% 642985
#% 643057
#% 805898
#% 832099
#% 860086
#! Thanks to the ubiquity of the Internet search engine search box, users have come to depend on search engines both to find and re-find information. However, re-finding behavior has not been significantly addressed. Here we look at re-finding queries issued to the Yahoo! search engine by 114 users over a year.

#index 879693
#* Early precision measures: implications from the downside of blind feedback
#@ Stephen Tomlinson
#t 2006
#c 13
#% 766497
#! We report the statistically significant mean impacts of blind feedback, as implemented by 7 participants for the 2003 Reliable Information Access (RIA) Workshop, on 30 retrieval measures, including several primary recall measures not originally reported. We find that blind feedback was detrimental to measures focused on the first relevant item even when it boosted "early precision" measures such as mean Precision@10, implying that the conventional reporting of ad hoc precision needs enhancement.

#index 879694
#* An experimental study on automatically labeling hierarchical clusters using statistical features
#@ Pucktada Treeratpituk;Jamie Callan
#t 2006
#c 13
#% 144023
#% 413609
#% 766433
#% 783483

#index 879695
#* Strict and vague interpretation of XML-retrieval queries
#@ Andrew Trotman;Mounia Lalmas
#t 2006
#c 13
#% 766447
#% 1721851
#% 1721852
#! Structural hints in XML-retrieval queries can be used to specify both the granularity of the search result (the target element) and where in a document to search (support elements). These hints might be interpreted either strictly or vaguely, but does it matter if an XML search engine interprets these in one way and the user in another? The performance of all runs submitted to INEX 2005 content and structure (CAS) tasks were measured for each of four different interpretations of CAS. Runs that perform well for one interpretation of target elements do so regardless of the interpretation of support elements; but how to interpret the target element does matter. This suggests that to perform well on all CAS queries it is necessary to know how the target structure specification should be interpreted. We extend the NEXI query language to include this, and hypothesize that using this will increase the overall performance of search engines.

#index 879696
#* Why structural hints in queries do not help XML-retrieval
#@ Andrew Trotman;Mounia Lalmas
#t 2006
#c 13
#% 818222
#% 838388
#% 1674739
#% 1721851
#! For many years it has been commonly held that a user who adds structural "hints" to a query will improve precision in an element retrieval search. At INEX 2005 we conducted an experiment to test this assumption. We present the unexpected result that structural hints in queries do not improve precision. An analysis of the topics and the judgments suggests that this is because users are particularly bad at giving structural hints.

#index 879697
#* Searching the web using composed pages
#@ Ramakrishna Varadarajan;Vagelis Hristidis;Tao Li
#t 2006
#c 13
#% 330678
#% 453464
#% 838422

#index 879698
#* A study of real-time query expansion effectiveness
#@ Ryen W. White;Gary Marchionini
#t 2006
#c 13
#% 218978
#% 642985
#% 643001
#% 943042
#! In this poster, we describe the study of an interface technique that provides a list of suggested additional query terms as a searcher types a search query, in effect offering interactive query expansion (IQE) options while the query is formulated. Analysis of the results shows that offering IQE during query formulation leads to better quality initial queries, and an increased uptake of query expansion. These findings have implications for how IQE should be offered in retrieval interfaces.

#index 879699
#* A graph-based framework for relation propagation and its application to multi-label learning
#@ Ming Wu;Rong Jin
#t 2006
#c 13
#% 266408
#% 840852
#! Label propagation exploits the structure of the unlabeled documents by propagating the label information of the training documents to the unlabeled documents. The limitation with the existing label propagation approaches is that they can only deal with a single type of objects. We propose a framework, named "relation propagation", that allows for information propagated among multiple types of objects. Empirical studies with multi-label text categorization showed that the proposed algorithm is more effective than several semi-supervised learning algorithms in that it is capable of exploring the correlation among different categories and the structure of unlabeled documents simultaneously.

#index 879700
#* Measuring similarity of semi-structured documents with context weights
#@ Christopher C. Yang;Nan Liu
#t 2006
#c 13
#% 642993
#% 766417
#% 1715599
#! In this work, we study similarity measures for text-centric XML documents based on an extended vector space model, which considers both document content and structure. Experimental results based on a benchmark showed superior performance of the proposed measure over the baseline which ignores structural knowledge of XML documents.

#index 879701
#* Incorporating query difference for learning retrieval functions in information retrieval
#@ Hongyuan Zha;Zhaohui Zheng;Haoying Fu;Gordon Sun
#t 2006
#c 13
#! We discuss information retrieval methods that aim at serving a diverse stream of user queries. We propose methods that emphasize the importance of taking into consideration of query difference in learning effective retrieval functions. We formulate the problem as a multi-task learning problem using a risk minimization framework. In particular, we show how to calibrate the empirical risk to incorporate query difference in terms of introducing nuisance parameters in the statistical models, and we also propose an alternating optimization method to simultaneously learn the retrieval function and the nuisance parameters. We illustrate the effectiveness of the proposed methods using modeling data extracted from a commercial search engine.

#index 879702
#* Concept-based biomedical text retrieval
#@ Ming Zhong;Xiangji Huang
#t 2006
#c 13
#! One challenging problem for biomedical text retrieval is to find accurate synonyms or name variants for biomedical entities. In this paper, we propose a new concept-based approach to tackle this problem. In this approach, a set of concepts instead of keywords will be extracted from a query first. Then these concepts will be used for retrieval purpose. The experiment results show that the proposed approach can boost the retrieval performance and it generates very good results on 2005 TREC Genomics data sets.

#index 879703
#* The TIJAH XML information retrieval system
#@ Henk Ernst Blok;Vojkan Mihajlović;Georgina Ramírez;Thijs Westerveld;Djoerd Hiemstra;Arjen P. de Vries
#t 2006
#c 13
#% 838389
#% 1721851

#index 879704
#* A location annotation system for personal photos
#@ Chufeng Chen;Michael Oakes;John Tait
#t 2006
#c 13
#% 1742099

#index 879705
#* Appraisal navigator
#@ Navendu Garg;Kenneth Bloom;Shlomo Argamon
#t 2006
#c 13
#% 838522
#! Much interesting text n the web consists largely of opinionated or evaluative text, as opposed to directly informative text. The new field of 'sentiment analysis' seeks to characterize such aspects of natural language text, as opposed to just the bare facts. We suggest that 'appraisal expression extraction' should be viewed as a fundamental task for sentiment analysis. We define an 'appraisal expression' to be a piece of text expressing some evaluative stance towards a particular object. The task is to find these elements and characterize the type and orientation (positive or negative) of the evaluative stance, as well as its target and possibly its source. Potential applications of these methods include new approaches to the now-traditional tasks of sentiment classification and pinion mining, as well as possibly for adversarial textual analysis and intention detection for intelligence applications.

#index 879706
#* A platform for Okapi-based contextual information retrieval
#@ Xiangji Huang;Miao Wen;Aijun An;Yan-Rui Huang
#t 2006
#c 13
#% 818297
#! We present an extensible java-based platform for contextual retrieval based on the probabilistic information retrieval model. Modules for dual indexes, relevance feedback with blind or machine learning approaches and query expansion with context are integrated into the Okapi system to deal with the contextual information. This platform allows easy extension to include other types of contextual information.

#index 879707
#* Project contexts to situate personal information
#@ William Jones;Harry Bruce;Austin Foxley
#t 2006
#c 13
#! The Personal Project Planner prototype works as an extension to the file manager to provide people with rich-text overlays to their information (folders, files and also email, web pages, notes). Rich-text, document-like project plans can be created which then provide a context in which to create or reference the email messages, electronic documents, web pages, etc. that are needed to complete the plan. The user can later locate an information item such as an email message with reference to the plan (e.g., as an alternative to a mostly context-free search through the inbox or sent mail). The Planner explores a possibility that an effective organization of project-related information can emerge as a natural by-product of efforts to plan and structure the project.

#index 879708
#* Cheshire3: retrieving from tera-scale grid-based digital libraries
#@ Ray R. Larson;Robert Sanderson
#t 2006
#c 13
#% 760777
#% 878626
#% 1709414

#index 879709
#* DeWild: a tool for searching the web using wild cards
#@ Haobin Li;Davood Rafiei
#t 2006
#c 13
#% 262084
#% 754068

#index 879710
#* Searching for expertise using the terrier platform
#@ Craig Macdonald;Iadh Ounis
#t 2006
#c 13

#index 879711
#* DiLight: an ontology-based information access system for e-learning environments
#@ Ming Mao;Yefei Peng;Daqing He
#t 2006
#c 13
#% 156337

#index 879712
#* Supporting semantic visual feature browsing in contentbased video retrieval
#@ Xiangming Mu
#t 2006
#c 13
#! A new shot level video retrieval system that supports semantic visual features (e.g., car, mountain, and fire) browsing is developed to facilitate content-based retrieval. The video's binary semantic feature vector is utilized to calculate the score of similarity between two shot keyframes. The score is then used to browse the "similar" keyframes in terms of semantic visual features.

#index 879713
#* MathFind: a math-aware search engine
#@ Rajesh Munavalli;Robert Miner
#t 2006
#c 13
#% 319273

#index 947200
#* SIGIR 2005 Doctoral Consortium
#@ David J. Harper
#t 2005
#c 13

#index 987190
#* Strategy follows technology
#@ Edwin van Huis
#t 2007
#c 13
#! In strategic management there has been a debate over many years. Already in 1962 Alfred Chandler had stated: Structure follows Strategy. In the nineteen eighties, Michael Porter modified Chandler's dictum about structure following strategy by introducing a second level of structure: organizational structure follows strategy, which in turn follows structure. So the question became: what is leading what?. Technology has in this debate been seen as a part of either the structure of the organisation itself, or part of the development of the environment in which the organisation tries to survive by adapting. The notion that technological advancement can also change the paradigmas of organisational strategy-development is new. This has mainly to do with the impact of the technological changes on the workflow and procedures of organisations. Never before they were so profound as in our days. Technological change affects us on different levels of our strategic development. I will give three examples of changes that are occurring or have occurred in "Sound and Vision". The first is the introduction of RFID transmitters in admission rings for the Sound and Vision experience. The second is the setup of a back office media asset management, storage and distribution structure for the Public Broadcasters. The third is the development of the archive towards becoming a Media-Application Service Provider.

#index 987191
#* 2007 Athena Lecturer Award introduction
#@ Karen Spärck Jones
#t 2007
#c 13

#index 987192
#* Natural language and the information layer
#@ Karen Spärck Jones
#t 2007
#c 13

#index 987193
#* Personalized query expansion for the web
#@ Paul - Alexandru Chirita;Claudiu S. Firan;Wolfgang Nejdl
#t 2007
#c 13
#% 131434
#% 144029
#% 169729
#% 193105
#% 198058
#% 218978
#% 268225
#% 277669
#% 280840
#% 288614
#% 308756
#% 309095
#% 326522
#% 340882
#% 348155
#% 348173
#% 397159
#% 397161
#% 397162
#% 577301
#% 577329
#% 643001
#% 740900
#% 754125
#% 754126
#% 766406
#% 766439
#% 766440
#% 805904
#% 818259
#% 869492
#% 869536
#% 879576
#% 907515
#! The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval. In this paper we propose to improve such Web queries by expanding them with terms collected from each user's Personal Information Repository, thus implicitly personalizing the search output. We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri. Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings. Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query. A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.

#index 987194
#* Using query contexts in information retrieval
#@ Jing Bai;Jian-Yun Nie;Guihong Cao;Hugues Bouchard
#t 2007
#c 13
#% 78171
#% 83855
#% 144029
#% 162359
#% 167554
#% 169729
#% 218978
#% 229348
#% 280851
#% 320944
#% 340901
#% 340948
#% 342707
#% 399057
#% 413615
#% 420471
#% 642983
#% 735077
#% 748550
#% 766423
#% 766430
#% 818207
#% 818224
#% 818239
#% 818259
#% 850133
#% 879579
#% 879586
#% 894253
#% 1261589
#! User query is an element that specifies an information need, but it is not the only one. Studies in literature have found many contextual factors that strongly influence the interpretation of a query. Recent studies have tried to consider the user's interests by creating a user profile. However, a single profile for a user may not be sufficient for a variety of queries of the user. In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query. The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations. In this paper, both types of context are integrated in an IR model based on language modeling. Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.

#index 987195
#* Towards task-based personal information management evaluations
#@ David Elsweiler;Ian Ruthven
#t 2007
#c 13
#% 65965
#% 116374
#% 187999
#% 259946
#% 303510
#% 415104
#% 579439
#% 642983
#% 751800
#% 832099
#% 860036
#% 955704
#% 1783152
#! Personal Information Management (PIM) is a rapidly growing area of research concerned with how people store, manage and refind information. A feature of PIM research is that many systems have been designed to assist users manage and refind information, but very few have been evaluated. This has been noted by several scholars and explained by the difficulties involved in performing PIM evaluations. The difficulties include that people re-find information from within unique personal collections; researchers know little about the tasks that cause people to re-find information; and numerous privacy issues concerning personal information. In this paper we aim to facilitate PIM evaluations by addressing each of these difficulties. In the first part, we present a diary study of information re-finding tasks. The study examines the kind of tasks that require users to refind information and produces a taxonomy of refinding tasks for email messages and web pages. In the second part, we propose a task-based evaluation methodology based on our findings and examine the feasibility of the approach using two different methods of task creation.

#index 987196
#* Utility-based information distillation over temporally sequenced documents
#@ Yiming Yang;Abhimanyu Lad;Ni Lao;Abhay Harpale;Bryan Kisiel;Monica Rogati
#t 2007
#c 13
#% 219049
#% 262085
#% 262087
#% 262112
#% 397133
#% 411762
#% 575571
#% 642975
#% 642998
#% 643014
#% 730034
#% 766450
#% 818214
#% 938670
#% 939970
#% 940038
#% 940039
#! This paper examines a new approach to information distillation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework. It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs ("tasks" with multiple queries). Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings. For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task. Answer keys (nuggets) were generated for each query and a semi-automatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses. We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty. Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.

#index 987197
#* Effective missing data prediction for collaborative filtering
#@ Hao Ma;Irwin King;Michael R. Lyu
#t 2007
#c 13
#% 173879
#% 202011
#% 280852
#% 330687
#% 397153
#% 420539
#% 452563
#% 528156
#% 643007
#% 734592
#% 734594
#% 766448
#% 766449
#% 770816
#% 818216
#% 840924
#% 879627
#% 1650569
#! Memory-based collaborative filtering algorithms have been widely adopted in many popular recommender systems, although these approaches all suffer from data sparsity and poor prediction quality problems. Usually, the user-item matrix is quite sparse, which directly leads to inaccurate recommendations. This paper focuses the memory-based collaborative filtering problems on two crucial factors: (1) similarity computation between users or items and (2) missing data prediction algorithms. First, we use the enhanced Pearson Correlation Coefficient (PCC) algorithm by adding one parameter which overcomes the potential decrease of accuracy when computing the similarity of users or items. Second, we propose an effective missing data prediction algorithm, in which information of both users and items is taken into account. In this algorithm, we set the similarity threshold for users and items respectively, and the prediction algorithm will determine whether predicting the missing data or not. We also address how to predict the missing data by employing a combination of user and item information. Finally, empirical studies on dataset MovieLens have shown that our newly proposed method outperforms other state-of-the-art collaborative filtering algorithms and it is more robust against data sparsity.

#index 987198
#* Efficient bayesian hierarchical user modeling for recommendation system
#@ Yi Zhang;Jonathan Koren
#t 2007
#c 13
#% 219048
#% 220711
#% 266281
#% 277480
#% 280852
#% 495929
#% 578684
#% 719598
#% 766449
#% 766451
#% 769908
#% 818214
#% 840962
#% 879626
#% 879627
#% 907526
#% 1250186
#! A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user's interest. A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model. Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive. The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications. This paper proposes a new fast learning technique to learn a large number of individual user profiles. The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.

#index 987199
#* Robust test collections for retrieval evaluation
#@ Ben Carterette
#t 2007
#c 13
#% 71616
#% 211044
#% 262097
#% 262102
#% 340936
#% 643020
#% 730072
#% 766410
#% 857180
#% 879598
#% 879599
#% 879632
#% 879655
#% 995515
#! Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments. While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems. In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments. We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort. Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems. Even the smallest sets of judgments can be useful for evaluation of new systems.

#index 987200
#* Reliable information retrieval evaluation with incomplete and biased judgements
#@ Stefan Büttcher;Charles L. A. Clarke;Peter C. K. Yeung;Ian Soboroff
#t 2007
#c 13
#% 197394
#% 248065
#% 262102
#% 411762
#% 458379
#% 466263
#% 561315
#% 766409
#% 879632
#% 879641
#% 907496
#% 907614
#! Information retrieval evaluation based on the pooling method is inherently biased against systems that did not contribute to the pool of judged documents. This may distort the results obtained about the relative quality of the systems evaluated and thus lead to incorrect conclusions about the performance of a particular ranking technique. We examine the magnitude of this effect and explore how it can be countered by automatically building an unbiased set of judgements from the original, biased judgements obtained through pooling. We compare the performance of this method with other approaches to the problem of incomplete judgements, such as bpref, and show that the proposed method leads to higher evaluation accuracy, especially if the set of manual judgements is rich in documents, but highly biased against some systems.

#index 987201
#* Alternatives to Bpref
#@ Tetsuya Sakai
#t 2007
#c 13
#% 262097
#% 262102
#% 340890
#% 397163
#% 411762
#% 643020
#% 766409
#% 879566
#% 879598
#% 879599
#% 879630
#% 879632
#% 879659
#% 907496
#% 948924
#! Recently, a number of TREC tracks have adopted a retrieval effectiveness metric called bpref which has been designed for evaluation environments with incomplete relevance data. A graded-relevance version of this metric called rpref has also been proposed. However, we show that the application of Q-measure, normalised Discounted Cumulative Gain (nDCG) or Average Precision (AveP)to condensed lists, obtained by ?ltering out all unjudged documents from the original ranked lists, is actually a better solution to the incompleteness problem than bpref. Furthermore, we show that the use of graded relevance boosts the robustness of IR evaluation to incompleteness and therefore that Q-measure and nDCG based on condensed lists are the best choices. To this end, we use four graded-relevance test collections from NTCIR to compare ten different IR metrics in terms of system ranking stability and pairwise discriminative power.

#index 987202
#* An interactive algorithm for asking and incorporating feature feedback into support vector machines
#@ Hema Raghavan;James Allan
#t 2007
#c 13
#% 86528
#% 115521
#% 170649
#% 232719
#% 340905
#% 344447
#% 458379
#% 464465
#% 643001
#% 714351
#% 722797
#% 763708
#% 769908
#% 799753
#% 879616
#% 879626
#% 961194
#% 1045638
#! Standard machine learning techniques typically require ample training data in the form of labeled instances. In many situations it may be too tedious or costly to obtain sufficient labeled data for adequate classifier performance. However, in text classification, humans can easily guess the relevance of features, that is, words that are indicative of a topic, thereby enabling the classifier to focus its feature weights more appropriately in the absence of sufficient labeled data. We will describe an algorithm for tandem learning that begins with a couple of labeled instances, and then at each iteration recommends features and instances for a human to label. Tandem learning using an "oracle" results in much better performance than learning on only features or only instances. We find that humans can emulate the oracle to an extent that results in performance (accuracy) comparable to that of the oracle. Our unique experimental design helps factor out system error from human error, leading to a better understanding of when and why interactive feature selection works.

#index 987203
#* Learn from web search logs to organize search results
#@ Xuanhui Wang;ChengXiang Zhai
#t 2007
#c 13
#% 169781
#% 190581
#% 214711
#% 218992
#% 262045
#% 281186
#% 297550
#% 310567
#% 321635
#% 325001
#% 330617
#% 375017
#% 397161
#% 577224
#% 754124
#% 766433
#% 818207
#% 818267
#% 823348
#% 869501
#% 879567
#% 879594
#% 879613
#% 1715593
#! Effective organization of search results is critical for improving the utility of any search engine. Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly. However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the user's perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster. In this paper, we propose to address these two deficiencies by (1) learning "interesting aspects" of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users. We evaluate our proposed method on a commercial search engine log data. Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.

#index 987204
#* Regularized clustering for documents
#@ Fei Wang;Changshui Zhang;Tao Li
#t 2007
#c 13
#% 118771
#% 143194
#% 190581
#% 252836
#% 262059
#% 313959
#% 329562
#% 397147
#% 420083
#% 420507
#% 466675
#% 593047
#% 643008
#% 722902
#% 724227
#% 729437
#% 766434
#% 881468
#% 915294
#% 961218
#% 1705532
#% 1705533
#! In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatictopic extraction, and fast information retrieval or filtering. In this paper, we propose a novel method for clustering documents using regularization. Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer. So we call our algorithm Clustering with Local and Global Regularization (CLGR). We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods. Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.

#index 987205
#* Towards automatic extraction of event and place semantics from flickr tags
#@ Tye Rattenbury;Nathaniel Good;Mor Naaman
#t 2007
#c 13
#% 280408
#% 340889
#% 378541
#% 394984
#% 480467
#% 642997
#% 643520
#% 760826
#% 765412
#% 766441
#% 784438
#% 855601
#% 869482
#% 881054
#% 903606
#% 1374354
#% 1728792
#! We describe an approach for extracting semantics of tags, unstructured text-labels assigned to resources on the Web, based on each tag's usage patterns. In particular, we focus on the problem of extracting place and event semantics for tags that are assigned to photos on Flickr, a popular photo sharing website that supports time and location (latitude/longitude) metadata. We analyze two methods inspired by well-known burst-analysis techniques and one novel method: Scale-structure Identification. We evaluate the methods on a subset of Flickr data, and show that our Scale-structure Identification method outperforms the existing techniques. The approach and methods described in this work can be used in other domains such as geo-annotated web pages, where text terms can be extracted and associated with usage patterns.

#index 987206
#* Hierarchical classification for automatic image annotation
#@ Jianping Fan;Yuli Gao;Hangzai Luo
#t 2007
#c 13
#% 198058
#% 311034
#% 318785
#% 408466
#% 592183
#% 718437
#% 766452
#% 823326
#% 829014
#% 884044
#% 1502498
#% 1673052
#! In this paper, a hierarchical classification framework has been proposed for bridging the semantic gap effectively and achieving multi-level image annotation automatically. First, the semantic gap between the low-level computable visual features and users' real information needs is partitioned into four smaller gaps, and multiple approachesallare proposed to bridge these smaller gaps more effectively. To learn more reliable contextual relationships between the atomic image concepts and the co-appearances of salient objects, a multi-modal boosting algorithm is proposed. To enable hierarchical image classification and avoid inter-level error transmission, a hierarchical boosting algorithm is proposed by incorporating concept ontology and multi-task learning to achieve hierarchical image classifier training with automatic error recovery. To bridge the gap between the computable image concepts and the users' real information needs, a novel hyperbolic visualization framework is seamlessly incorporated to enable intuitive query specification and evaluation by acquainting the users with a good global view of large-scale image collections. Our experiments on large-scale image databases have also obtained very positive results.

#index 987207
#* Laplacian optimal design for image retrieval
#@ Xiaofei He;Wanli Min;Deng Cai;Kun Zhou
#t 2007
#c 13
#% 318785
#% 341269
#% 780687
#% 780807
#% 789031
#% 812506
#% 839873
#% 876080
#% 961218
#% 1272282
#% 1857498
#! Relevance feedback is a powerful technique to enhance Content-Based Image Retrieval (CBIR) performance. It solicits the user's relevance judgments on the retrieved images returned by the CBIR systems. The user's labeling is then used to learn a classifier to distinguish between relevant and irrelevant images. However, the top returnedimages may not be the most informative ones. The challenge is thus to determine which unlabeled images would be the most informative (i.e., improve the classifier the most) if they were labeled and used as training samples. In this paper, we propose a novel active learning algorithm, called Laplacian Optimal Design (LOD), for relevance feedback image retrieval. Our algorithm is based on aregression model which minimizes the least square error on the measured (or, labeled) images and simultaneously preserves the local geometrical structure of the image space. Specifically, we assume that if two images are sufficiently close to each other, then their measurements (or, labels) are close as well. By constructing a nearest neighbor graph, the geometrical structure of the image space can be described by the graph Laplacian. We discuss how results from the field of optimal experimental design may be used to guide our selection of a subset of images, which gives us the most amount of information. Experimental results on Corel database suggest that theproposed approach achieves higher precision in relevance feedback image retrieval.

#index 987208
#* Fast generation of result snippets in web search
#@ Andrew Turpin;Yohannes Tsegay;David Hawking;Hugh E. Williams
#t 2007
#c 13
#% 194251
#% 262036
#% 268079
#% 280835
#% 290703
#% 340916
#% 397130
#% 443121
#% 449746
#% 449749
#% 577339
#% 723279
#% 747116
#% 804805
#% 860861
#% 869534
#% 936965
#! The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users. In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets. We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library. These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process. Using simulation, we examine snippet generation performance for different size RAM caches. Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality. This scheme effectively doubles the number of documents that can fit in a fixed size cache.

#index 987209
#* The influence of caption features on clickthrough patterns in web search
#@ Charles L. A. Clarke;Eugene Agichtein;Susan Dumais;Ryen W. White
#t 2007
#c 13
#% 262036
#% 280835
#% 325001
#% 397130
#% 590523
#% 751869
#% 754059
#% 766472
#% 783482
#% 805878
#% 818221
#% 818226
#% 818233
#% 879565
#% 879567
#% 907550
#% 954948
#! Web search engines present lists of captions, comprising title, snippet, and URL, to help users decide which search results to visit. Understanding the influence of features of these captions on Web search behavior may help validate algorithms and guidelines for their improved generation. In this paper we develop a methodology to use clickthrough logs from a commercial search engine to study user behavior when interacting with search result captions. The findings of our study suggest that relatively simple caption features such as the presence of all terms query terms, the readability of the snippet, and the length of the URL shown in the caption, can significantly influence users' Web search behavior.

#index 987210
#* CollabSum: exploiting multiple document clustering for collaborative single document summarizations
#@ Xiaojun Wan;Jianwu Yang
#t 2007
#c 13
#% 194251
#% 262112
#% 268079
#% 288614
#% 290830
#% 292221
#% 296738
#% 340884
#% 340885
#% 340971
#% 378481
#% 387427
#% 397136
#% 397137
#% 428369
#% 742104
#% 742224
#% 755863
#% 816173
#% 818266
#% 855043
#% 1275213
#! Almost all existing methods conduct the summarization tasks for single documents separately without interactions for each document under the assumption that the documents are considered independent of each other. This paper proposes a novel framework called CollabSum for collaborative single document summarizations by making use of mutual influences of multiple documents within a cluster context. In this study, CollabSum is implemented by first employing the clustering algorithm to obtain appropriate document clusters and then exploiting the graph-ranking based algorithm for collaborative document summarizations within each cluster. Both the with-document and cross-document relationships between sentences are incorporated in the algorithm. Experiments on the DUC2001 and DUC2002 datasets demonstrate the encouraging performance of the proposed approach. Different clustering algorithms have been investigated and we find that the summarization performance relies positively on the quality of document cluster.

#index 987211
#* Information re-retrieval: repeat queries in Yahoo's logs
#@ Jaime Teevan;Eytan Adar;Rosie Jones;Michael A. S. Potts
#t 2007
#c 13
#% 194299
#% 233808
#% 284796
#% 306468
#% 308574
#% 342961
#% 642983
#% 643057
#% 655487
#% 751830
#% 766447
#% 766472
#% 805898
#% 832099
#% 859488
#% 879692
#% 881544
#% 954970
#% 956495
#% 1025295
#% 1392483
#! People often repeat Web searches, both to find new information on topics they have previously explored and to re-find information they have seen in the past. The query associated with a repeat search may differ from the initial query but can nonetheless lead to clicks on the same results. This paper explores repeat search behavior through the analysis of a one-year Web query log of 114 anonymous users and a separate controlled survey of an additional 119 volunteers. Our study demonstrates that as many as 40% of all queries are re-finding queries. Re-finding appears to be an important behavior for search engines to explicitly support, and we explore how this can be done. We demonstrate that changes to search engine results can hinder re-finding, and provide a way to automatically detect repeat searches and predict repeat clicks.

#index 987212
#* Studying the use of popular destinations to enhance web search interaction
#@ Ryen W. White;Mikhail Bilenko;Silviu Cucerzan
#t 2007
#c 13
#% 46803
#% 148007
#% 214709
#% 272821
#% 284931
#% 296646
#% 590526
#% 642985
#% 803556
#% 823348
#% 869501
#% 872034
#% 879567
#% 943042
#% 943516
#% 956495
#% 1275193
#% 1289268
#! We present a novel Web search interaction feature which, for a given query, provides links to websites frequently visited by other users with similar information needs. These popular destinations complement traditional search results, allowing direct navigation to authoritative resources for the query topic. Destinations are identified using the history of search and browsing behavior of many users over an extended time period, whose collective behavior provides a basis for computing source authority. We describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries, as well as with traditional, unaided Web search. Results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks, with best performance obtained from mining past user behavior at query-level granularity.

#index 987213
#* Neighborhood restrictions in geographic IR
#@ Steven Schockaert;Martine De Cock
#t 2007
#c 13
#% 311903
#% 397186
#% 421122
#% 489435
#% 527030
#% 766536
#% 804806
#% 1671782
#% 1676433
#% 1916144
#! Geographic information retrieval (GIR) systems allow users to specify a geographic context, in addition to a more traditional query, enabling the system to pinpoint interesting search results whose relevancy is location-dependent. In particular local search services have become a widely used mechanism to find businesses, such as hotels, restaurants, and shops, which satisfy a geographical restriction. Unfortunately, many useful types of geographic restrictions are currently not supported in these systems, including restrictions that specify the neighborhood in which the business should be located. As the boundaries of city neighborhoods are not readily available, automated techniques to construct representations of the spatial extent of neighborhoods are required to support this kind of restrictions. In this paper, we propose such a technique, using fuzzy footprints to cope with the inherent vagueness of most neighborhood boundaries, and we provide experimental results that demonstrate the potential of our technique in a local search setting.

#index 987214
#* Efficient document retrieval in main memory
#@ Trevor Strohman;W. Bruce Croft
#t 2007
#c 13
#% 109187
#% 194247
#% 198335
#% 212665
#% 213786
#% 290703
#% 333854
#% 340886
#% 340887
#% 648114
#% 818229
#% 818230
#% 879611
#% 893128
#% 907504
#% 963669
#% 1683906
#! Disk access performance is a major bottleneck in traditional information retrieval systems. Compared to system memory, disk bandwidth is poor, and seek times are worse. We circumvent this problem by considering query evaluation strategies in main memory. We show how new accumulator trimming techniques combined with inverted list skipping can produce extremely high performance retrieval systems without resorting to methods that may harm effectiveness. We evaluate our techniques using Galago, a new retrieval system designed for efficient query processing. Our system achieves a 69% improvement in query throughput over previous methods.

#index 987215
#* The impact of caching on search engines
#@ Ricardo Baeza-Yates;Aristides Gionis;Flavio Junqueira;Vanessa Murdock;Vassilis Plachouras;Fabrizio Silvestri
#t 2007
#c 13
#% 194299
#% 228097
#% 321588
#% 340888
#% 394709
#% 577302
#% 805864
#% 818229
#% 860861
#% 907504
#% 978378
#% 1013724
#% 1834787
#! In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs.caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.

#index 987216
#* Pruning policies for two-tiered inverted index with correctness guarantee
#@ Alexandros Ntoulas;Junghoo Cho
#t 2007
#c 13
#% 210172
#% 212665
#% 290830
#% 298181
#% 333854
#% 339621
#% 340886
#% 340887
#% 340888
#% 342397
#% 387427
#% 397608
#% 406493
#% 410276
#% 577302
#% 659255
#% 736649
#% 754058
#% 805864
#% 807320
#% 869471
#% 907505
#% 1015265
#% 1016177
#% 1016183
#! The Web search engines maintain large-scale inverted indexes which are queried thousands of times per second by users eager for information. In order to cope with the vast amounts of query loads, search engines prune their index to keep documents that are likely to be returned as top results, and use this pruned index to compute the first batches of results. While this approach can improve performance by reducing the size of the index, if we compute the top results only from the pruned index we may notice a significant degradation in the result quality: if a document should be in the top results but was not included in the pruned index, it will be placed behind the results computed from the pruned index. Given the fierce competition in the online search market, this phenomenon is clearly undesirable. In this paper, we study how we can avoid any degradation of result quality due to the pruning-based performance optimization, while still realizing most of its benefit. Our contribution is a number of modifications in the pruning techniques for creating the pruned index and a new result computation algorithm that guarantees that the top-matching pages are always placed at the top search results, even though we are computing the first batch from the pruned index most of the time. We also show how to determine the optimal size of a pruned index and we experimentally evaluate our algorithms on a collection of 130 million Web pages.

#index 987217
#* Topic segmentation with shared topic detection and alignment of multiple documents
#@ Bingjun Sun;Prasenjit Mitra;C. Lee Giles;John Yen;Hongyuan Zha
#t 2007
#c 13
#% 115608
#% 280819
#% 340950
#% 397214
#% 406493
#% 413573
#% 448786
#% 464434
#% 466892
#% 643015
#% 722904
#% 729918
#% 742204
#% 747756
#% 769928
#% 770826
#% 786553
#% 815855
#% 840840
#% 907601
#% 1390151
#! Topic detection and tracking and topic segmentation play an important role in capturing the local and sequential information of documents. Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains. In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights. The basic idea is that the optimal segmentation maximizes MI (or WMI). Our approach can detect shared topics among documents. It can find the optimal boundaries in a document, and align segments among documents at the same time. It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment. Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents. Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation. Utilizing information from multiple documents can tremendously improve the performance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.

#index 987218
#* Analyzing feature trajectories for event detection
#@ Qi He;Kuiyu Chang;Ee-Peng Lim
#t 2007
#c 13
#% 262042
#% 309096
#% 316546
#% 340995
#% 350859
#% 577220
#% 577297
#% 643014
#% 643016
#% 765412
#% 766444
#% 797693
#% 823344
#% 824666
#% 1728326
#! We consider the problem of analyzing word trajectories in both time and frequency domains, with the specific goal of identifying important and less-reported, periodic and aperiodic words. A set of words with identical trends can be grouped together to reconstruct an event in a completely un-supervised manner. The document frequency of each word across time is treated like a time series, where each element is the document frequency - inverse document frequency (DFIDF) score at one time point. In this paper, we 1) first applied spectral analysis to categorize features for different event characteristics: important and less-reported, periodic and aperiodic; 2) modeled aperiodic features with Gaussian density and periodic features with Gaussian mixture densities, and subsequently detected each feature's burst by the truncated Gaussian approach; 3) proposed an unsupervised greedy event detection algorithm to detect both aperiodic and periodic events. All of the above methods can be applied to time series data in general. We extensively evaluated our methods on the 1-year Reuters News Corpus [3] and showed that they were able to uncover meaningful aperiodic and periodic events.

#index 987219
#* New event detection based on indexing-tree and named entity
#@ Kuo Zhang;Juan Zi;Li Gang Wu
#t 2007
#c 13
#% 115608
#% 144034
#% 262042
#% 311034
#% 340995
#% 350859
#% 445316
#% 465754
#% 577297
#% 643016
#% 677440
#% 735078
#% 766444
#% 939869
#! New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e. not reported previously). With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately. In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically. Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy. In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories. Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.

#index 987220
#* Multiple-signal duplicate detection for search evaluation
#@ Scott Huffman;April Lehman;Alexei Stolboushkin;Howard Wong-Toi;Fan Yang;Hein Roehrig
#t 2007
#c 13
#% 136350
#% 201935
#% 204673
#% 255137
#% 290482
#% 347225
#% 411762
#% 504572
#% 654447
#% 879600
#% 978157
#% 1815525
#! We consider the problem of duplicate document detection for search evaluation. Given a query and a small number of web results for that query, we show how to detect duplicate web documents with precision ~0.91 and recall ~77. In contrast, Charikar's algorithm, designed for duplicate detection in an indexing pipeline, achieves precision ~0.91 but with a recall of ~0.58. Our improvement in recall while maintaining high precision comes from combining three ideas. First, because we are only concerned with duplicate detection among results for the same query, the number of pairwise comparisons is small. Therefore we can afford to compute multiple pairwise signals for each pair of documents. A model learned with standard machine-learning techniques improves recall to ~0.68 with precision ~0.90. Second, most duplicate detection has focused on text analysis of the HTML contents of a document. In some web pages the HTML is not a good indicator of the final contents of the page. We use extended fetching techniques to fill in frames and execute Java script. Including signals based on our richer fetches further improves the recall to ~0.75 and the precision to ~0.91. Finally, we also explore using signals based on the query. Comparing contextual snippets based on the richer fetches improves the recall to ~0.77. We show that the overall accuracy of this final model approaches that of human judges.

#index 987221
#* Robust classification of rare queries using web knowledge
#@ Andrei Z. Broder;Marcus Fontoura;Evgeniy Gabrilovich;Amruta Joshi;Vanja Josifovski;Tong Zhang
#t 2007
#c 13
#% 169729
#% 262084
#% 298183
#% 309095
#% 729437
#% 730051
#% 798291
#% 818281
#% 840583
#% 844287
#% 853542
#% 853543
#% 853544
#% 853545
#% 879581
#% 894253
#% 1289518
#% 1712908
#! We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in real-time with the query volume of a commercial web search engine. We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.

#index 987222
#* Random walks on the click graph
#@ Nick Craswell;Martin Szummer
#t 2007
#c 13
#% 310567
#% 313959
#% 330617
#% 340899
#% 577224
#% 783482
#% 805200
#% 818221
#% 851306
#% 879565
#% 879567
#! Search engines can record which documents were clicked for which query, and use these query-document pairs as "soft" relevance judgments. However, compared to the true judgments, click logs give noisy and sparse relevance information. We apply a Markov random walk model to a large click log, producing a probabilistic ranking of documents for a given query. A key advantage of the model is its ability to retrieve relevant documents that have not yet been clicked for that query and rank those effectively. We conduct experiments on click logs from image search, comparing our ("backward") random walk model to a different ("forward") random walk, varying parameters such as walk length and self-transition probability. The most effective combination is a long backward walk with high self-transition probability.

#index 987223
#* Supporting multiple information-seeking strategies in a single system framework
#@ Xiaojun Yuan;Nicholas J. Belkin
#t 2007
#c 13
#% 29585
#% 64896
#% 142618
#% 717133
#! This paper reports on an experiment comparing the retrieval effectiveness of an interactive information retrieval (IIR) system which adapts to support different information seeking strategies, with that of a standard baseline IIR system. The experiment, with 32 subjects each searching on 8 different topics, indicates that using the integrated IIR system resulted in significantly better performance, including user satisfaction with search results, significantly more effective interaction, and significantly better usability than using the baseline system.

#index 987224
#* Investigating the querying and browsing behavior of advanced search engine users
#@ Ryen W. White;Dan Morris
#t 2007
#c 13
#% 27049
#% 85443
#% 169803
#% 197860
#% 270279
#% 280839
#% 284931
#% 296646
#% 306468
#% 308745
#% 309767
#% 319118
#% 325198
#% 345262
#% 397130
#% 642985
#% 643000
#% 655487
#% 722310
#% 751830
#% 754059
#% 766472
#% 818259
#% 818260
#% 869501
#% 956495
#% 987212
#! One way to help all users of commercial Web search engines be more successful in their searches is to better understand what those users with greater search expertise are doing, and use this knowledge to benefit everyone. In this paper we study the interaction logs of advanced search engine users (and those not so advanced) to better understand how these user groups search. The results show that there are marked differences in the queries, result clicks, post-query browsing, and search success of users we classify as advanced (based on their use of query operators), relative to those classified as non-advanced. Our findings have implications for how advanced users should be supported during their searches, and how their interactions could be used to help searchers of all experience levels find more relevant information and learn improved searching strategies.

#index 987225
#* Term feedback for information retrieval with language models
#@ Bin Tan;Atulya Velivelli;Hui Fang;ChengXiang Zhai
#t 2007
#c 13
#% 54435
#% 169732
#% 194298
#% 214709
#% 218978
#% 280840
#% 300542
#% 340901
#% 342707
#% 346553
#% 642985
#% 643001
#% 766496
#% 769967
#% 818209
#% 818260
#% 838547
#% 879621
#! In this paper we study term-based feedback for information retrieval in the language modeling approach. With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents, taking full control of the query expansion process. We propose a cluster-based method for selecting terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback. Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback. They are helpful even when there are no relevant documents in the top.

#index 987226
#* A support vector method for optimizing average precision
#@ Yisong Yue;Thomas Finley;Filip Radlinski;Thorsten Joachims
#t 2007
#c 13
#% 169774
#% 190581
#% 309095
#% 340899
#% 390723
#% 425031
#% 466229
#% 770788
#% 770854
#% 818262
#% 829043
#% 840846
#% 840882
#% 875974
#% 879588
#% 879655
#! Machine learning is commonly used to improve ranked retrieval systems. Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems. Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive. In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP. We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea. In most cases we show our method to produce statistically significant improvements in MAP scores.

#index 987227
#* Ranking with multiple hyperplanes
#@ Tao Qin;Xu-Dong Zhang;De-Sheng Wang;Tie-Yan Liu;Wei Lai;Hang Li
#t 2007
#c 13
#% 169777
#% 269217
#% 272510
#% 309095
#% 330769
#% 387427
#% 411762
#% 577224
#% 577355
#% 654466
#% 734915
#% 766414
#% 807296
#% 815924
#% 838490
#% 840846
#% 840853
#% 879588
#% 879619
#% 939346
#% 1016177
#% 1272365
#% 1860941
#! The central problem for many applications in Information Retrieval is ranking and learning to rank is considered as a promising approach for addressing the issue. Ranking SVM, for example, is a state-of-the-art method for learning to rank and has been empirically demonstrated to be effective. In this paper, we study the issue of learning to rank, particularly the approach of using SVM techniques to perform the task. We point out that although Ranking SVM is advantageous, it still has shortcomings. Ranking SVM employs a single hyperplane in the feature space as the model for ranking, which is too simple to tackle complex ranking problems. Furthermore, the training of Ranking SVM is also computationally costly. In this paper, we look at an alternative approach to Ranking SVM, which we call "Multiple Hyperplane Ranker" (MHR), and make comparisons between the two approaches. MHR takes the divide-and-conquer strategy. It employs multiple hyperplanes to rank instances and finally aggregates the ranking results given by the hyperplanes. MHR contains Ranking SVM as a special case, and MHR can overcome the shortcomings which Ranking SVM suffers from. Experimental results on two information retrieval datasets show that MHR can outperform Ranking SVM in ranking.

#index 987228
#* A regression framework for learning ranking functions using relative relevance judgments
#@ Zhaohui Zheng;Keke Chen;Gordon Sun;Hongyuan Zha
#t 2007
#c 13
#% 55490
#% 57484
#% 86371
#% 185289
#% 262096
#% 411762
#% 577224
#% 731615
#% 734915
#% 818221
#% 823348
#% 828494
#% 840846
#% 869483
#% 872020
#% 907517
#% 1674802
#! Effective ranking functions are an essential part of commercial search engines. We focus on developing a regression framework for learning ranking functions for improving relevance of search engines serving diverse streams of user queries. We explore supervised learning methodology from machine learning, and we distinguish two types of relevance judgments used as the training data: 1) absolute relevance judgments arising from explicit labeling of search results; and 2) relative relevance judgments extracted from user click throughs of search results or converted from the absolute relevance judgments. We propose a novel optimization framework emphasizing the use of relative relevance judgments. The main contribution is the development of an algorithm based on regression that can be applied to objective functions involving preference data, i.e., data indicating that a document is more relevant than another with respect to a query. Experimental results are carried out using data sets obtained from a commercial search engine. Our results show significant improvements of our proposed methods over some existing methods.

#index 987229
#* An exploration of proximity measures in information retrieval
#@ Tao Tao;ChengXiang Zhai
#t 2007
#c 13
#% 46803
#% 106122
#% 111303
#% 120104
#% 134878
#% 144011
#% 169809
#% 262096
#% 280864
#% 328532
#% 340899
#% 340901
#% 340948
#% 342707
#% 397129
#% 413592
#% 642979
#% 719598
#% 766412
#% 810627
#% 818262
#% 818263
#% 879651
#% 1387547
#! In most existing retrieval models, documents are scored primarily based on various kinds of term statistics such as within-document frequencies, inverse document frequencies, and document lengths. Intuitively, the proximity of matched query terms in a document can also be exploited to promote scores of documents in which the matched query terms are close to each other. Such a proximity heuristic, however, has been largely under-explored in the literature; it is unclear how we can model proximity and incorporate a proximity measure into an existing retrieval model. In this paper,we systematically explore the query term proximity heuristic. Specifically, we propose and study the effectiveness of five different proximity measures, each modeling proximity from a different perspective. We then design two heuristic constraints and use them to guide us in incorporating the proposed proximity measures into an existing retrieval model. Experiments on five standard TREC test collections show that one of the proposed proximity measures is indeed highly correlated with document relevance, and by incorporating it into the KL-divergence language model and the Okapi BM25 model, we can significantly improve retrieval performance.

#index 987230
#* Estimation and use of uncertainty in pseudo-relevance feedback
#@ Kevyn Collins-Thompson;Jamie Callan
#t 2007
#c 13
#% 209021
#% 262096
#% 298183
#% 399890
#% 413578
#% 729437
#% 789959
#% 818267
#% 827581
#% 843728
#% 876011
#% 879585
#% 907544
#% 939332
#! Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feed-back model by resampling a given query's top-retrieved documents, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects. The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.

#index 987231
#* Latent concept expansion using markov random fields
#@ Donald Metzler;W. Bruce Croft
#t 2007
#c 13
#% 35937
#% 109190
#% 287253
#% 298182
#% 298183
#% 340901
#% 342707
#% 677414
#% 766428
#% 766430
#% 766431
#% 818262
#% 838532
#% 840882
#% 879587
#% 940042
#% 976952
#! Query expansion, in the form of pseudo-relevance feedback or relevance feedback, is a common technique used to improve retrieval effectiveness. Most previous approaches have ignored important issues, such as the role of features and the importance of modeling term dependencies. In this paper, we propose a robust query expansion technique based onthe Markov random field model for information retrieval. The technique, called latent concept expansion, provides a mechanism for modeling term dependencies during expansion. Furthermore, the use of arbitrary features within the model provides a powerful framework for going beyond simple term occurrence features that are implicitly used by most other expansion techniques. We evaluate our technique against relevance models, a state-of-the-art language modeling query expansion technique. Our model demonstrates consistent and significant improvements in retrieval effectiveness across several TREC data sets. We also describe how our technique can be used to generate meaningful multi-term concepts for tasks such as query suggestion/reformulation.

#index 987232
#* A study of Poisson query generation model for information retrieval
#@ Qiaozhu Mei;Hui Fang;ChengXiang Zhai
#t 2007
#c 13
#% 142400
#% 169781
#% 262096
#% 280819
#% 280850
#% 340899
#% 340901
#% 340948
#% 397127
#% 397129
#% 642976
#% 719598
#% 722904
#% 742513
#% 766412
#% 766430
#% 766431
#% 766503
#% 879578
#% 879587
#% 940042
#! Many variants of language models have been proposed for information retrieval. Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model. In this paper, we propose and study a new family of query generation models based on Poisson distribution. We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent. However, based on different smoothing methods, the two families of models behave differently. We show that the Poisson model has several advantages, including naturally accommodating per-term smoothing and modeling accurate background more efficiently. We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections. The results show that while their basic models perform comparably, the Poisson model can out perform multinomial model with per-term smoothing. The performance can be further improved with two-stage smoothing.

#index 987233
#* Deconstructing nuggets: the stability and reliability of complex question answering evaluation
#@ Jimmy Lin;Pengyi Zhang
#t 2007
#c 13
#% 262102
#% 309127
#% 312689
#% 766409
#% 818222
#% 818252
#% 867119
#% 879598
#% 939970
#% 940039
#! A methodology based on "information nuggets" has recently emerged as the de facto standard by which answers to complex questions are evaluated. After several implementations in the TREC question answering tracks, the community has gained a better understanding of its many characteristics. This paper focuses on one particular aspect of the evaluation: the human assignment of nuggets to answer strings, which serves as the basis of the F-score computation. As a byproduct of the TREC 2006 ciQA task, identical answer strings were independently evaluated twice, which allowed us to assess the consistency of human judgments. Based on these results, we explored simulations of assessor behavior that provide a method to quantify scoring variations. Understanding these variations in turn lets researchers be more confident in their comparisons of systems.

#index 987234
#* Interesting nuggets and their impact on definitional question answering
#@ Kian-Wei Kor;Tat-Seng Chua
#t 2007
#c 13
#% 262112
#% 818251
#% 939636
#% 939970
#! Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets. This is insufficient as they do not address the novelty factor that a definitional nugget must also possess. This paper proposes to address the deficiency by building a "Human Interest Model" from external knowledge. It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic. We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.

#index 987235
#* A probabilistic graphical model for joint answer ranking in question answering
#@ Jeongwoo Ko;Eric Nyberg;Luo Si
#t 2007
#c 13
#% 92146
#% 277467
#% 297171
#% 309124
#% 340953
#% 388024
#% 643014
#% 816157
#% 816175
#! Graphical models have been applied to various information retrieval and natural language processing tasks in the recent literature. In this paper, we apply a probabilistic graphical model for answer ranking in question answering. This model estimates the joint probability of correctness of all answer candidates, from which the probability of correctness of an individual candidate can be inferred. The joint prediction model can estimate both the correctness of individual answers as well as their correlations, which enables a list of accurate and comprehensive answers. This model was compared with a logistic regression model which directly estimates the probability of correctness of each individual answer candidate. An extensive set of empirical results based on TREC questions demonstrates the effectiveness of the joint model for answer ranking. Furthermore, we combine the joint model with the logistic regression model to improve the efficiency and accuracy of answer ranking.

#index 987236
#* Structured retrieval for question answering
#@ Matthew W. Bilotti;Paul Ogilvie;Jamie Callan;Eric Nyberg
#t 2007
#c 13
#% 111303
#% 187773
#% 278107
#% 309124
#% 340948
#% 342710
#% 642979
#% 642992
#% 642993
#% 740916
#% 742399
#% 755863
#% 818253
#% 867119
#! Bag-of-words retrieval is popular among Question Answering (QA) system developers, but it does not support constraint checking and ranking on the linguistic and semantic information of interest to the QA system. We present anapproach to retrieval for QA, applying structured retrieval techniques to the types of text annotations that QA systems use. We demonstrate that the structured approach can retrieve more relevant results, more highly ranked, compared with bag-of-words, on a sentence retrieval task. We also characterize the extent to which structured retrieval effectiveness depends on the quality of the annotations.

#index 987237
#* On the robustness of relevance measures with incomplete judgments
#@ Tanuja Bompada;Chi-Chao Chang;John Chen;Ravi Kumar;Rajesh Shenoy
#t 2007
#c 13
#% 236052
#% 262102
#% 309093
#% 340892
#% 375017
#% 387427
#% 411762
#% 561315
#% 766407
#% 766409
#% 818220
#% 818221
#% 818222
#% 879632
#% 907496
#% 1715617
#! We investigate the robustness of three widely used IR relevance measures for large data collections with incomplete judgments. The relevance measures we consider are the bpref measure introduced by Buckley and Voorhees [7], the inferred average precision (infAP) introduced by Aslam and Yilmaz [4], and the normalized discounted cumulative gain (NDCG) measure introduced by Järvelin and Kekäläinen [8]. Our main results show that NDCG consistently performs better than both bpref and infAP. The experiments are performed on standard TREC datasets, under different levels of incompleteness of judgments, and using two different evaluation methods, namely, the Kendall correlation measures order between system rankings and pairwise statistical significance testing; the latter may be of independent interest.

#index 987238
#* Test theory for assessing IR test collections
#@ David Bodoff;Pu Li
#t 2007
#c 13
#% 262102
#% 262105
#% 309093
#% 318407
#% 397163
#% 818222
#! How good is an IR test collection? A series of papers in recent years has addressed the question by empirically enumerating the consistency of performance comparisons using alternate subsets of the collection. In this paper we propose using Test Theory, which is based on analysis of variance and is specifically designed to assess test collections. Using the method, we not only can measure test reliability after the fact, but we can estimate the test collection's reliability before it is even built or used. We can also determine an optimal allocation of resources before the fact, e.g. whether to invest in more judges or queries. The method, which is in widespread use in the field of educational testing, complements data-driven approaches to assessing test collections. Whereas the data-driven method focuses on test results, test theory focuses on test designs. It offers unique practical results, as well as insights about the variety and implications of alternative test designs.

#index 987239
#* Strategic system comparisons via targeted relevance judgments
#@ Alistair Moffat;William Webber;Justin Zobel
#t 2007
#c 13
#% 133888
#% 133889
#% 133892
#% 194269
#% 262034
#% 262097
#% 262102
#% 309093
#% 411762
#% 561315
#% 766409
#% 818221
#% 818222
#% 879561
#% 879598
#% 879631
#% 879632
#% 907496
#! Relevance judgments are used to compare text retrieval systems. Given a collection of documents and queries, and a set of systems being compared, a standard approach to forming judgments is to manually examine all documents that are highly ranked by any of the systems. However, not all of these relevance judgments provide the same benefit to the final result, particularly if the aim is to identify which systems are best, rather than to fully order them. In this paper we propose new experimental methodologies that can significantly reduce the volume of judgments required in system comparisons. Using rank-biased precision, a recently proposed effectiveness measure, we show that judging around 200 documents for each of 50 queries in a TREC-scale system evaluation containing over 100 runs is sufficient to identify the best systems.

#index 987240
#* FRank: a ranking method with fidelity loss
#@ Ming-Feng Tsai;Tie-Yan Liu;Tao Qin;Hsin-Hsi Chen;Wei-Ying Ma
#t 2007
#c 13
#% 169781
#% 269217
#% 309095
#% 378462
#% 411762
#% 577224
#% 734915
#% 766414
#% 818225
#% 818254
#% 840846
#% 879567
#% 879588
#! Ranking problem is becoming important in many fields, especially in information retrieval (IR). Many machine learning techniques have been proposed for ranking problem, such as RankSVM, RankBoost, and RankNet. Among them, RankNet, which is based on a probabilistic ranking framework, is leading to promising results and has been applied to a commercial Web search engine. In this paper we conduct further study on the probabilistic ranking framework and provide a novel loss function named fidelity loss for measuring loss of ranking. The fidelity loss notonly inherits effective properties of the probabilistic ranking framework in RankNet, but possesses new properties that are helpful for ranking. This includes the fidelity loss obtaining zero for each document pair, and having a finite upper bound that is necessary for conducting query-level normalization. We also propose an algorithm named FRank based on a generalized additive model for the sake of minimizing the fedelity loss and learning an effective ranking function. We evaluated the proposed algorithm for two datasets: TREC dataset and real Web search dataset. The experimental results show that the proposed FRank algorithm outperforms other learning-based ranking methods on both conventional IR problem and Web search.

#index 987241
#* AdaRank: a boosting algorithm for information retrieval
#@ Jun Xu;Hang Li
#t 2007
#c 13
#% 169777
#% 235377
#% 262096
#% 302391
#% 309095
#% 340899
#% 387427
#% 425051
#% 465746
#% 577224
#% 734915
#% 766414
#% 803033
#% 818225
#% 818254
#% 823360
#% 840846
#% 840882
#% 879588
#% 1665205
#% 1674802
#! In this paper we address the issue of learning to rank for document retrieval. In the task, a model is automatically created with some training data and then is utilized for ranking of documents. The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain). Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data. Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs. To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures. Our algorithm, referred to as AdaRank, repeatedly constructs 'weak rankers' on the basis of reweighted training data and finally linearly combines the weak rankers for making ranking predictions. We prove that the training process of AdaRank is exactly that of enhancing the performance measure used. Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.

#index 987242
#* A combined component approach for finding collection-adapted ranking functions based on genetic programming
#@ Humberto Mossri de Almeida;Marcos André Gonçalves;Marco Cristo;Pável Calado
#t 2007
#c 13
#% 46803
#% 124073
#% 169774
#% 218982
#% 253191
#% 290703
#% 320326
#% 387427
#% 411235
#% 420464
#% 740768
#% 773034
#% 803033
#% 804914
#% 835230
#% 840583
#% 879633
#% 943956
#% 987170
#! In this paper, we propose a new method to discover collection-adapted ranking functions based on Genetic Programming (GP). Our Combined Component Approach (CCA)is based on the combination of several term-weighting components (i.e.,term frequency, collection frequency, normalization) extracted from well-known ranking functions. In contrast to related work, the GP terminals in our CCA are not based on simple statistical information of a document collection, but on meaningful, effective, and proven components. Experimental results show that our approach was able to outper form standard TF-IDF, BM25 and another GP-based approach in two different collections. CCA obtained improvements in mean average precision up to 40.87% for the TREC-8 collection, and 24.85% for the WBR99 collection (a large Brazilian Web collection), over the baseline functions. The CCA evolution process also was able to reduce the overtraining, commonly found in machine learning methods, especially genetic programming, and to converge faster than the other GP-based approach used for comparison.

#index 987243
#* Feature selection for ranking
#@ Xiubo Geng;Tie-Yan Liu;Tao Qin;Hang Li
#t 2007
#c 13
#% 169777
#% 243727
#% 243728
#% 262096
#% 269217
#% 340899
#% 387427
#% 411762
#% 465754
#% 466266
#% 577224
#% 720198
#% 722929
#% 722935
#% 766414
#% 770857
#% 818225
#% 818254
#% 840846
#% 857180
#% 879588
#% 922066
#% 1860915
#! Ranking is a very important topic in information retrieval. While algorithms for learning ranking models have been intensively studied, this is not the case for feature selection, despite of its importance. The reality is that many feature selection methods used in classification are directly applied to ranking. We argue that because of the striking differences between ranking and classification, it is better to develop different feature selection methods for ranking. To this end, we propose a new feature selection method in this paper. Specifically, for each feature we use its value to rank the training instances, and define the ranking accuracy in terms of a performance measure or a loss function as the importance of the feature. We also define the correlation between the ranking results of two features as the similarity between them. Based on the definitions, we formulate the feature selection issue as an optimization problem, for which it is to find the features with maximum total importance scores and minimum total similarity scores. We also demonstrate how to solve the optimization problem in an efficient way. We have tested the effectiveness of our feature selection method on two information retrieval datasets and with two ranking models. Experimental results show that our method can outperform traditional feature selection methods for the ranking task.

#index 987244
#* Relaxed online SVMs for spam filtering
#@ D. Sculley;Gabriel M. Wachman
#t 2007
#c 13
#% 309208
#% 310557
#% 393059
#% 458379
#% 832098
#% 879580
#% 881477
#% 1860547
#! Spam is a key problem in electronic communication, including large-scale email systems and the growing number of blogs. Content-based filtering is one reliable method of combating this threat in its various forms, but some academic researchers and industrial practitioners disagree on how best to filter spam. The former have advocated the use of Support Vector Machines (SVMs) for content-based filtering, as this machine learning methodology gives state-of-the-art performance for text classification. However, similar performance gains have yet to be demonstrated for online spam filtering. Additionally, practitioners cite the high cost of SVMs as reason to prefer faster (if less statistically robust) Bayesian methods. In this paper, we offer a resolution to this controversy. First, we show that online SVMs indeed give state-of-the-art classification performance on online spam filtering on large benchmark data sets. Second, we show that nearly equivalent performance may be achieved by a Relaxed Online SVM (ROSVM) at greatly reduced computational cost. Our results are experimentally verified on email spam, blog spam, and splog detection tasks.

#index 987245
#* Know your neighbors: web spam detection using the web topology
#@ Carlos Castillo;Debora Donato;Aristides Gionis;Vanessa Murdock;Fabrizio Silvestri
#t 2007
#c 13
#% 258598
#% 290482
#% 309145
#% 340141
#% 387427
#% 590524
#% 754088
#% 807297
#% 824694
#% 832098
#% 869470
#% 869471
#% 879625
#% 881557
#% 907509
#% 912202
#% 915286
#% 1016177
#% 1699583
#! Web spam can significantly deteriorate the quality of search engine results. Thus there is a large incentive for commercial search engines to detect spam pages efficiently and accurately. In this paper we present a spam detection system that combines link-based and content-based features, and uses the topology of the Web graph by exploiting the link dependencies among the Web pages. We find that linked hosts tend to belong to the same class: either both are spam or both are non-spam. We demonstrate three methods of incorporating the Web graph topology into the predictions obtained by our base classifier: (i) clustering the host graph, and assigning the label of all hosts in the cluster by majority vote, (ii) propagating the predicted labels to neighboring hosts, and (iii) using the predicted labels of neighboring hosts as new features and retraining the classifier. The result is an accurate system for detecting Web spam, tested on a large and public dataset, using algorithms that can be applied in practice to large-scale Web data.

#index 987246
#* DiffusionRank: a possible penicillin for web spamming
#@ Haixuan Yang;Irwin King;Michael R. Lyu
#t 2007
#c 13
#% 315967
#% 464615
#% 593047
#% 754088
#% 794860
#% 807341
#% 840846
#% 869471
#% 879567
#% 879603
#% 1016177
#! While the PageRank algorithm has proven to be very effective for ranking Web pages, the rank scores of Web pages can be manipulated. To handle the manipulation problem and to cast a new insight on the Web structure, we propose a ranking algorithm called DiffusionRank. DiffusionRank is motivated by the heat diffusion phenomena, which can be connected to Web ranking because the activities flow on the Web can be imagined as heat flow, the link from a page to another can be treated as the pipe of an air-conditioner, and heat flow can embody the structure of the underlying Web graph. Theoretically we show that DiffusionRank can serve as a generalization of PageRank when the heat diffusion co-efficient γ tends to infinity. In such a case 1=γ= 0, DiffusionRank (PageRank) has low ability of anti-manipulation. When γ = 0, DiffusionRank obtains the highest ability of anti-manipulation, but in such a case, the web structure is completely ignored. Consequently, γ is an interesting factor that can control the balance between the ability of preserving the original Web and the ability of reducing the effect of manipulation. It is found empirically that, when γ = 1, DiffusionRank has a Penicillin-like effect on the link manipulation. Moreover, DiffusionRank can be employed to find group-to-group relations on the Web, to divide the Web graph into several parts, and to find link communities. Experimental results show that the DiffusionRank algorithm achieves the above mentioned advantages as expected.

#index 987247
#* Towards musical query-by-semantic-description using the CAL500 data set
#@ Douglas Turnbull;Luke Barrington;David Torres;Gert Lanckriet
#t 2007
#c 13
#% 115608
#% 137711
#% 592090
#% 642990
#% 849879
#% 876634
#% 885438
#% 975105
#% 1502531
#! Query-by-semantic-description (QBSD)is a natural paradigm for retrieving content from large databases of music. A major impediment to the development of good QBSD systems for music information retrieval has been the lack of a cleanly-labeled, publicly-available, heterogeneous data set of songs and associated annotations. We have collected the Computer Audition Lab 500-song (CAL500) data set by having humans listen to and annotate songs using a survey designed to capture 'semantic associations' between music and words. We adapt the supervised multi-class labeling (SML) model, which has shown good performance on the task of image retrieval, and use the CAL500 data to learn a model for music retrieval. The model parameters are estimated using the weighted mixture hierarchies expectation-maximization algorithm which has been specifically designed to handle real-valued semantic association between words and songs, rather than binary class labels. The output of the SML model, a vector of class-conditional probabilities, can be interpreted as a semantic multinomial distribution over a vocabulary. By also representing a semantic query as a query multinomial distribution, we can quickly rank order the songs in a database based on the Kullback-Leibler divergence between the query multinomial and each song's semantic multinomial. Qualitative and quantitative results demonstrate that our SML model can both annotate a novel song with meaningful words and retrieve relevant songs given a multi-word, text-based query.

#index 987248
#* A music search engine built upon audio-based and web-based similarity measures
#@ Peter Knees;Tim Pohle;Markus Schedl;Gerhard Widmer
#t 2007
#c 13
#% 46803
#% 194192
#% 280819
#% 387427
#% 465754
#% 479462
#% 818327
#% 876634
#% 879573
#% 905103
#% 1548057
#% 1775412
#! An approach is presented to automatically build a search engine for large-scale music collections that can be queried through natural language. While existing approaches depend on explicit manual annotations and meta-data assigned to the individual audio pieces, we automatically derive descriptions by making use of methods from Web Retrieval and Music Information Retrieval. Based on the ID3 tags of a collection of mp3 files, we retrieve relevant Web pages via Google queries and use the contents of these pages to characterize the music pieces and represent them by term vectors. By incorporating complementary information about acous tic similarity we are able to both reduce the dimensionality of the vector space and improve the performance of retrieval, i.e. the quality of the results. Furthermore, the usage of audio similarity allows us to also characterize audio pieces when there is no associated information found on the Web.

#index 987249
#* Building simulated queries for known-item topics: an analysis using six european languages
#@ Leif Azzopardi;Maarten de Rijke;Krisztian Balog
#t 2007
#c 13
#% 280851
#% 340146
#% 342709
#% 348308
#% 372658
#% 397161
#% 413551
#% 448736
#% 561315
#% 732845
#% 810629
#% 874504
#% 879610
#% 879642
#% 1674983
#% 1916136
#! There has been increased interest in the use of simulated queries for evaluation and estimation purposes in Information Retrieval. However, there are still many unaddressed issues regarding their usage and impact on evaluation because their quality, in terms of retrieval performance, is unlike real queries. In this paper, wefocus on methods for building simulated known-item topics and explore their quality against real known-item topics. Using existing generation models as our starting point, we explore factors which may influence the generation of the known-item topic. Informed by this detailed analysis (on six European languages) we propose a model with improved document and term selection properties, showing that simulated known-item topics can be generated that are comparable to real known-item topics. This is a significant step towards validating the potential usefulness of simulated queries: for evaluation purposes, and becausebuilding models of querying behavior provides a deeper insight into the querying process so that better retrieval mechanisms can be developed to support the user.

#index 987250
#* Cross-lingual query suggestion using query logs of different languages
#@ Wei Gao;Cheng Niu;Jian-Yun Nie;Ming Zhou;Jian Hu;Kam-Fai Wong;Hsiao-Wuen Hon
#t 2007
#c 13
#% 144074
#% 232656
#% 262047
#% 280826
#% 340895
#% 340960
#% 342961
#% 397143
#% 397145
#% 397146
#% 420520
#% 577224
#% 579944
#% 641976
#% 740915
#% 766425
#% 766427
#% 768632
#% 818268
#% 838398
#% 939627
#% 1558464
#! Query suggestion aims to suggest relevant queries for a given query, which help users better specify their information needs. Previously, the suggested terms are mostly in the same language of the input query. In this paper, we extend it to cross-lingual query suggestion (CLQS): for a query in one language, we suggest similar or relevant queries in other languages. This is very important to scenarios of cross-language information retrieval (CLIR) and cross-lingual keyword bidding for search engine advertisement. Instead of relying on existing query translation technologies for CLQS, we present an effective means to map the input query of one language to queries of the other language in the query log. Important monolingual and cross-lingual information such as word translation relations and word co-occurrence statistics, etc. are used to estimate the cross-lingual query similarity with a discriminative model. Benchmarks show that the resulting CLQS system significantly out performs a baseline system based on dictionary-based query translation. Besides, the resulting CLQS is tested with French to English CLIR tasks on TREC collections. The results demonstrate higher effectiveness than the traditional query translation methods.

#index 987251
#* Hits on the web: how does it compare?
#@ Marc A. Najork;Hugo Zaragoza;Michael J. Taylor
#t 2007
#c 13
#% 253188
#% 268079
#% 282905
#% 290830
#% 309151
#% 309779
#% 330707
#% 340932
#% 411762
#% 466574
#% 577328
#% 577337
#% 799632
#% 799636
#% 818255
#% 840846
#% 1016177
#! This paper describes a large-scale evaluation of theeffectiveness of HITS in comparison with other link-based rankingalgorithms, when used in combination with a state-of-the-art textretrieval algorithm exploiting anchor text. We quantified theireffectiveness using three common performance measures: the meanreciprocal rank, the mean average precision, and the normalizeddiscounted cumulative gain measurements. The evaluation is based ontwo large data sets: a breadth-first search crawl of 463 millionweb pages containing 17.6 billion hyperlinks and referencing 2.9billion distinct URLs; and a set of 28,043 queries sampled from aquery log, each query having on average 2,383 results, about 17 ofwhich were labeled by judges. We found that HITS outperformsPageRank, but is about as effective as web-page in-degree. The sameholds true when any of the link-based features are combined withthe text retrieval algorithm. Finally, we studied the relationshipbetween query specificity and the effectiveness of selectedfeatures, and found that link-based features perform better forgeneral queries, whereas BM25F performs better for specificqueries.

#index 987252
#* Hits hits TREC: exploring IR evaluation results with network analysis
#@ Stefano Mizzaro;Stephen Robertson
#t 2007
#c 13
#% 290830
#% 309093
#% 340890
#% 397163
#% 818222
#% 857180
#% 879631
#% 907493
#% 924376
#! We propose a novel method of analysing data gathered fromTREC or similar information retrieval evaluation experiments. We define two normalized versions of average precision, that we use to construct a weighted bipartite graph of TREC systems and topics. We analyze the meaning of well known - and somewhat generalized - indicators fromsocial network analysis on the Systems-Topics graph. We apply this method to an analysis of TREC 8 data; amongthe results, we find that authority measures systems performance, that hubness of topics reveals that some topics are better than others at distinguishing more or less effective systems, that with current measures a system that wants to be effective in TREC needs to be effective on easy topics, and that by using different effectiveness measures this is no longer the case.

#index 987253
#* Combining content and link for classification using matrix factorization
#@ Shenghuo Zhu;Kai Yu;Yun Chi;Yihong Gong
#t 2007
#c 13
#% 197394
#% 248810
#% 280819
#% 290830
#% 309142
#% 420495
#% 430761
#% 464267
#% 466574
#% 643008
#% 656794
#% 818234
#% 818241
#% 840965
#% 881557
#% 1558464
#% 1650403
#% 1835183
#! The world wide web contains rich textual contents that areinterconnected via complex hyperlinks. This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample. It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure. The research in this direction has recently received considerable attention but are still in an early stage. Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features. Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors. Further analysis can be performed based on the compact representation of web pages. In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.

#index 987254
#* Federated text retrieval from uncooperative overlapped collections
#@ Milad Shokouhi;Justin Zobel
#t 2007
#c 13
#% 172898
#% 194246
#% 301225
#% 340146
#% 413594
#% 447946
#% 567255
#% 643011
#% 643012
#% 722311
#% 722312
#% 747116
#% 783473
#% 783692
#% 807433
#% 818211
#% 818212
#% 879604
#% 879607
#% 985828
#% 1682430
#% 1682446
#% 1684716
#! In federated text retrieval systems, the query is sent to multiple collections at the same time. The results returned by collections are gathered and ranked by a central broker that presents them to the user. It is usually assumed that the collections have little overlap. However, in practice collections may share many common documents as either exact or near duplicates, potentially leading to high numbers of duplicates in the final results. Considering the natural band width restrictions and efficiency issues of federated search, sendingqueries to redundant collections leads to unnecessary costs. We propose a novel method for estimating the rate of over-lap among collections based on sampling. Then, using theestimated overlap statistics, we propose two collection selection methods that aim to maximize the number of unique relevant documents in the final results. We show experimentally that, although our estimates of overlap are not in exact, our suggested techniques can significantly improve the search effectiveness when collections overlap.

#index 987255
#* Evaluating sampling methods for uncooperative collections
#@ Paul Thomas;David Hawking
#t 2007
#c 13
#% 194246
#% 268114
#% 280856
#% 309133
#% 309748
#% 342741
#% 480328
#% 643012
#% 807320
#% 869499
#% 879604
#% 907547
#! Many server selection methods suitable for distributed information retrieval applications rely, in the absence of cooperation, on the availability of unbiased samples of documents from the constituent collections. We describe a number of sampling methods which depend only on the normal query-response mechanism of the applicable search facilities. We evaluate these methods on a number of collections typical of a personal metasearch application. Results demonstrate that biases exist for all methods, particularly toward longer documents, and that in some cases these biases can be reduced but not eliminated by choice of parameters.We also introduce a new sampling technique, "multiple queries", which produces samples of similar quality to the best current techniques but with significantly reduced cost.

#index 987256
#* Updating collection representations for federated search
#@ Milad Shokouhi;Mark Baillie;Leif Azzopardi
#t 2007
#c 13
#% 194246
#% 227891
#% 262063
#% 280856
#% 287463
#% 301225
#% 340146
#% 341699
#% 413594
#% 447946
#% 643012
#% 722312
#% 731406
#% 768913
#% 783473
#% 800569
#% 809418
#% 818211
#% 852010
#% 879604
#% 985828
#% 1392444
#% 1682446
#! To facilitate the search for relevant information across a setof online distributed collections, a federated information retrieval system typically represents each collection, centrally, by a set of vocabularies or sampled documents. Accurate retrieval is therefore related to how precise each representation reflects the underlying content stored in that collection. As collections evolve over time, collection representations should also be updated to reflect any change, however, a current solution has not yet been proposed. In this study we examine both the implications of out-of-date representation sets on retrieval accuracy, as well as proposing three different policies for managing necessary updates. Each policyis evaluated on a testbed of forty-four dynamic collections over an eight-week period. Our findings show that out-of-date representations significantly degrade performance overtime, however, adopting a suitable update policy can minimise this problem.

#index 987257
#* A time machine for text search
#@ Klaus Berberich;Srikanta Bedathur;Thomas Neumann;Gerhard Weikum
#t 2007
#c 13
#% 118741
#% 201921
#% 228097
#% 262096
#% 287070
#% 290703
#% 340887
#% 369764
#% 387427
#% 466506
#% 479648
#% 481928
#% 643566
#% 728195
#% 798967
#% 867054
#% 879611
#% 907505
#% 956535
#% 1016154
#% 1016183
#% 1392437
#% 1688264
#! Text search over temporally versioned document collections such as web archives has received little attention as a research problem. As a consequence, there is no scalable and principled solution to search such a collection as of a specified time. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search. We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results. In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance. These techniques can be formulated as optimization problems that can be solved to near-optimality. Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets. Results unequivocally show that our methods make it possible to build an efficient "time machine" scalable to large versioned text collections.

#index 987258
#* Principles of hash-based text retrieval
#@ Benno Stein
#t 2007
#c 13
#% 232764
#% 249321
#% 255137
#% 329569
#% 340910
#% 347225
#% 479649
#% 479973
#% 594029
#% 762054
#% 805905
#% 818202
#% 879600
#% 879617
#! Hash-based similarity search reduces a continuous similarity relation to the binary concept "similar or not similar": two feature vectors are considered as similar if they are mapped on the same hash key. From its runtime performance this principle is unequaled--while being unaffected by dimensionality concerns at the same time. Similarity hashing is applied with great success for near similarity search in large document collections, and it is considered as a key technology for near-duplicate detection and plagiarism analysis. This papers reveals the design principles behind hash-based search methods and presents them in a unified way. We introduce new stress statistics that are suited to analyze the performance of hash-based search methods, and we explain the rationale of their effectiveness. Based on these insights, we show how optimum hash functions for similarity search can be derived. We also present new results of a comparative study between different hash-based search methods.

#index 987259
#* Compressed permuterm index
#@ Paolo Ferragina;Rossano Venturini
#t 2007
#c 13
#% 212287
#% 282232
#% 290703
#% 339936
#% 387427
#% 643571
#% 823464
#% 874900
#% 936965
#% 956437
#% 991192
#% 1077150
#% 1702461
#! Recently [Manning et al., 2007] resorted the Permuterm indexof Garfield (1976) as a time-efficient and elegant solution to the string dictionary problem in which pattern queries may possibly include one wild-card symbol (called, Tolerant Retrieval problem). Unfortunately the Permuterm index is space inefficient because its quadruples the dictionary size. In this paper we propose the Compressed Permuterm Index which solves the Tolerant Retrieval problem in optimal query time, i.e. time proportional to the length of the searched pattern, and space close to the k-th order empirical entropy of the indexed dictionary. Our index can be used to solve also more sophisticated queries which involve several wild-card symbols, or require to prefix-match multiple fields in a database of records.The result is based on an elegant variant of the Burrows-Wheeler Transform defined on a dictionary of strings of variable length, which allows to easily adapt known compressed indexes [Makinen-Navarro, 2007] to solve the Tolerant Retrieval problem. Experiments show that our index supports fast queries within a space occupancy that is close to the one achievable by compressing the string dictionary via gzip, bzip or ppmdi. This improves known approaches based on front-coding by more than 50% in absolute space occupancy, still guaranteeing comparable query time.

#index 987260
#* Query performance prediction in web search environments
#@ Yun Zhou;W. Bruce Croft
#t 2007
#c 13
#% 280851
#% 280864
#% 397161
#% 642992
#% 783506
#% 818262
#% 818267
#% 879613
#% 879614
#% 907544
#! Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist. In this paper, we present three techniques to address these challenges. We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding. Our evaluation is mainly performed on the GOV2 collection. In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types. To assist prediction under the mixed-query situation, a novel query classifier is adopted. Results show that our prediction of web query performance is substantially more accurate than the current state-of-the-art prediction techniques. Consequently, our paper provides a practical approach to performance prediction in real-world web settings.

#index 987261
#* Broad expertise retrieval in sparse data environments
#@ Krisztian Balog;Toine Bogers;Leif Azzopardi;Maarten de Rijke;Antal van den Bosch
#t 2007
#c 13
#% 115608
#% 279755
#% 340901
#% 397145
#% 411117
#% 426885
#% 730082
#% 740900
#% 818240
#% 879570
#% 907525
#% 913206
#% 939868
#% 940042
#% 1275180
#! Expertise retrieval has been largely unexplored on data other than the W3C collection. At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas. We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people. For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site. Using this test set, we conduct two series of experiments. The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set. The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set. Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.

#index 987262
#* A semantic approach to contextual advertising
#@ Andrei Broder;Marcus Fontoura;Vanja Josifovski;Lance Riedel
#t 2007
#c 13
#% 116149
#% 309743
#% 387427
#% 730065
#% 740191
#% 818265
#% 838515
#% 869484
#% 879633
#! Contextual advertising or Context Match (CM) refers to the placement of commercial textual advertisements within the content of a generic web page, while Sponsored Search (SS) advertising consists in placing ads on result pages from a web search engine, with ads driven by the originating query. In CM there is usually an intermediary commercial ad-network entity in charge of optimizing the ad selection with the twin goal of increasing revenue (shared between the publisher and the ad-network) and improving the user experience. With these goals in mind it is preferable to have ads relevant to the page content, rather than generic ads. The SS market developed quicker than the CM market, and most textual ads are still characterized by "bid phrases" representing those queries where the advertisers would like to have their ad displayed. Hence, the first technologies for CM have relied on previous solutions for SS, by simply extracting one or more phrases from the given page content, and displaying ads corresponding to searches on these phrases, in a purely syntactic approach. However, due to the vagaries of phrase extraction, and the lack of context, this approach leads to many irrelevant ads. To overcome this problem, we propose a system for contextual ad matching based on a combination of semantic and syntactic features.

#index 987263
#* How well does result relevance predict session satisfaction?
#@ Scott B. Huffman;Michael Hochster
#t 2007
#c 13
#% 109187
#% 309093
#% 323135
#% 328524
#% 340892
#% 340921
#% 411762
#% 420475
#% 450040
#% 590523
#% 751830
#% 754059
#% 766409
#% 818222
#% 818257
#% 879566
#! Per-query relevance measures provide standardized, repeatable measurements of search result quality, but they ignore much of what users actually experience in a full search session. This paper examines how well we can approximate a user's ultimate session-level satisfaction using a simple relevance metric. We find that thisrelationship is surprisingly strong. By incorporating additional properties of the query itself, we construct a model which predicts user satisfaction even more accurately than relevance alone.

#index 987264
#* A new approach for evaluating query expansion: query-document term mismatch
#@ Tonya Custis;Khalid Al-Kofahi
#t 2007
#c 13
#% 54413
#% 54435
#% 118726
#% 144029
#% 169729
#% 169768
#% 169779
#% 184493
#% 262084
#% 262096
#% 262097
#% 280819
#% 280851
#% 287253
#% 309093
#% 397143
#% 643033
#% 730007
#% 766409
#% 766410
#% 766428
#% 768903
#% 783475
#% 818222
#% 836019
#% 838529
#% 840583
#% 879579
#% 879598
#% 879613
#% 879631
#% 907493
#! The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and relevant documents. Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems. Query Expansion (QE) is one method for dealing with term mismatch. IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets. While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch. In this paper, we propose a new approach for evaluating query expansion techniques. The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.

#index 987265
#* Performance prediction using spatial autocorrelation
#@ Fernando Diaz
#t 2007
#c 13
#% 340890
#% 342710
#% 464449
#% 766408
#% 766431
#% 818254
#% 838528
#% 879598
#% 879613
#% 879614
#% 879632
#% 893735
#% 907544
#% 1392447
#! Evaluation of information retrieval systems is one of the core tasks in information retrieval. Problems include the inability to exhaustively label all documents for a topic, generalizability from a small number of topics, and incorporating the variability of retrieval systems. Previous work addresses the evaluation of systems, the ranking of queries by difficulty, and the ranking of individual retrievals by performance. Approaches exist for the case of few and even no relevance judgments. Our focus is on zero-judgment performance prediction of individual retrievals. One common shortcoming of previous techniques is the assumption of uncorrelated document scores and judgments. If documents are embedded in a high-dimensional space (as they often are), we can apply techniques from spatial data analysis to detect correlations between document scores. We find that the low correlation between scores of topically close documents often implies a poor retrieval performance. When compared to a state of the art baseline, we demonstrate that the spatial analysis of retrieval scores provides significantly better prediction performance. These new predictors can also be incorporated with classic predictors to improve performance further. We also describe the first large-scale experiment to evaluate zero-judgment performance prediction for a massive number of retrieval systems over a variety collections in several languages.

#index 987266
#* An outranking approach for rank aggregation in information retrieval
#@ Mohamed Farah;Daniel Vanderpooten
#t 2007
#c 13
#% 86371
#% 169774
#% 184496
#% 194246
#% 232703
#% 278831
#% 309253
#% 316534
#% 330769
#% 340959
#% 387427
#% 397125
#% 420464
#% 728195
#% 728360
#% 801673
#% 879582
#! Research in Information Retrieval usually shows performanceimprovement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.). In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking. In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another. We show that the proposed method deals well with the Information Retrieval distinctive features. Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.

#index 987267
#* Enhancing relevance scoring with chronological term rank
#@ Adam D. Troy;Guo-Qiang Zhang
#t 2007
#c 13
#% 118737
#% 218982
#% 387427
#% 397123
#% 397128
#% 783474
#% 810627
#% 818230
#% 879651
#% 1387547
#! We introduce a new relevance scoring technique that enhances existing relevance scoring schemes with term position information. This technique uses chronological term rank (CTR) which captures the positions of terms as they occur in the sequence of words in a document. CTR is both conceptually and computationally simple when compared to other approaches that use document structure information, such as term proximity, term order and document features. CTR works well when paired with Okapi BM25. We evaluate the performance of various combinations of CTR with Okapi BM25 in order to identify the most effective formula. We then compare the performance of the selected approach against the performance of existing methods such as Okapi BM25, pivoted length normalization and language models. Significant improvements are seen consistently across a variety of TREC data and topic sets, measured by the major retrieval performance metrics. This seems to be the first use of this statistic for relevance scoring. There is likely to be greater retrieval improvements possible using chronological term rank enhanced methods in future work.

#index 987268
#* ARSA: a sentiment-aware model for predicting sales performance using blogs
#@ Yang Liu;Xiangji Huang;Aijun An;Xiaohui Yu
#t 2007
#c 13
#% 577360
#% 722904
#% 754107
#% 769350
#% 783529
#% 786841
#% 805873
#% 815915
#% 818215
#% 823332
#% 838522
#% 854646
#% 869516
#% 869596
#% 881518
#% 881529
#% 907490
#% 938687
#% 939346
#% 1650298
#! Due to its high popularity, Weblogs (or blogs in short) present a wealth of information that can be very helpful in assessing the general public's sentiments and opinions. In this paper, we study the problem of mining sentiment information from blogs and investigate ways to use such information for predicting product sales performance. Based on an analysis of the complex nature of sentiments, we propose Sentiment PLSA (S-PLSA), in which a blog entry is viewed as a document generated by a number of hidden sentiment factors. Training an S-PLSA model on the blog data enables us to obtain a succinct summary of the sentiment information embedded in the blogs. We then present ARSA, an autoregressive sentiment-aware model, to utilize the sentiment information captured by S-PLSA for predicting product sales performance. Extensive experiments were conducted on a movie data set. We compare ARSA with alternative models that do not take into account the sentiment information, as well as a model with a different feature selection method. Experiments confirm the effectiveness and superiority of the proposed approach.

#index 987269
#* Vocabulary independent spoken term detection
#@ Jonathan Mamou;Bhuvana Ramabhadran;Olivier Siohan
#t 2007
#c 13
#% 219917
#% 280815
#% 309207
#% 321337
#% 342742
#% 840426
#% 879571
#% 939386
#% 968469
#% 1275583
#! We are interested in retrieving information from speech data like broadcast news, telephone conversations and roundtable meetings. Today, most systems use large vocabulary continuous speech recognition tools to produce word transcripts; the transcripts are indexed and query terms are retrieved from the index. However, query terms that are not part of the recognizer's vocabulary cannot be retrieved, and the recall of the search is affected. In addition to the output word transcript, advanced systems provide also phonetic transcripts, against which query terms can be matched phonetically. Such phonetic transcripts suffer from lower accuracy and cannot be an alternative to word transcripts.We present a vocabulary independent system that can handle arbitrary queries, exploiting the information provided by having both word transcripts and phonetic transcripts. A speech recognizer generates word confusion networks and phonetic lattices. The transcripts are indexed for query processing and ranking purpose.The value of the proposed method is demonstrated by the relative high performance ofour system, which received the highest overall ranking for US English speech data in the recent NIST Spoken Term Detection evaluation.

#index 987270
#* Improving text classification for oral history archives with temporal domain knowledge
#@ J. Scott Olsson;Douglas W. Oard
#t 2007
#c 13
#% 272995
#% 293977
#% 464465
#% 465754
#% 769908
#% 775173
#% 818313
#% 879596
#% 879626
#% 1289518
#% 1392472
#% 1562061
#% 1916129
#! This paper describes two new techniques for increasing the accuracy oftopic label assignment to conversational speech from oral history interviews using supervised machine learning in conjunction with automatic speech recognition. The first, time-shifted classification, leverages local sequence information from the order in which the story is told. The second, temporal label weighting, takes the complementary perspective by using the position within an interview to bias label assignment probabilities. These methods, when used in combination, yield between 6% and 15% relative improvements in classification accuracy using a clipped R-precision measure that models the utility of label sets as segment summaries in interactive speech retrieval applications.

#index 987271
#* Indexing confusion networks for morph-based spoken document retrieval
#@ Ville T. Turunen;Mikko Kurimo
#t 2007
#c 13
#% 144074
#% 309207
#% 321635
#% 710751
#% 711615
#% 879571
#% 969345
#% 995504
#! In this paper, we investigate methods for improving the performance of morph-based spoken document retrieval in Finnish by extracting relevant index terms from confusion networks. Our approach uses morpheme-like subword units ("morphs") for recognition and indexing. This alleviates the problem of out-of-vocabulary words, especially with inflectional languages like Finnish. Confusion networks offer a convenient representation of alternative recognition candidates by aligning mutually exclusive terms and by giving the posterior probability of each term. The rank of the competing terms and their posterior probability is used to estimate term frequency for indexing. Comparing against 1-best recognizer transcripts, we show that retrieval effectiveness is significantly improved. Finally, the effect of pruning in recognition is analyzed, showing that when recognition speed is increased, the reduction in retrieval performance due to the increase in the 1-best error rate can be compensated by using confusion networks.

#index 987272
#* Context sensitive stemming for web search
#@ Fuchun Peng;Nawaaz Ahmed;Xin Li;Yumao Lu
#t 2007
#c 13
#% 92696
#% 144034
#% 208934
#% 218978
#% 218985
#% 232644
#% 241238
#% 375017
#% 387427
#% 411762
#% 448857
#% 642985
#% 747738
#% 783506
#% 869501
#% 871579
#% 879567
#% 879579
#% 879612
#! Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms. Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation. In this paper, we propose a context sensitive stemming method that addresses these two issues. Two unique properties make our approach feasible for Web Search. First, based on statistical language modeling, we perform context sensitive analysis on the query side. We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine. This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time. Second, our approach performs a context sensitive document matching for those expanded variants. This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision. Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queriesand 1.8% over all query traffic.

#index 987273
#* Detecting, categorizing and clustering entity mentions in Chinese text
#@ Wenjie Li;Donglei Qian;Qin Lu;Chunfa Yuan
#t 2007
#c 13
#% 815207
#% 815876
#% 815922
#% 816205
#% 855291
#% 900932
#% 938670
#% 939351
#% 1195329
#% 1677145
#! The work presented in this paper is motivated by the practical need for content extraction, and the available data source and evaluation benchmark from the ACE program. The Chinese Entity Detection and Recognition (EDR) task is of particular interest to us. This task presents us several language-independent and language-dependent challenges, e.g. rising from the complication of extraction targets and the problem of word segmentation, etc. In this paper, we propose a novel solution to alleviate the problems special in the task. Mention detection takes advantages of machine learning approaches and character-based models. It manipulates different types of entities being mentioned and different constitution units (i.e. extents and heads) separately. Mentions referring to the same entity are linked together by integrating most-specific-first and closest-first rule based pairwise clustering algorithms. Types of mentions and entities are determined by head-driven classification approaches. The implemented system achieves ACE value of 66.1 when evaluated on the EDR 2005 Chinese corpus, which has been one of the top-tier results. Alternative approaches to mention detection and clustering are also discussed and analyzed.

#index 987274
#* Knowledge-intensive conceptual retrieval and passage extraction of biomedical literature
#@ Wei Zhou;Clement Yu;Neil Smalheiser;Vetle Torvik;Jie Hong
#t 2007
#c 13
#% 78171
#% 169729
#% 226545
#% 387427
#% 766440
#% 879577
#% 879702
#% 906618
#! This paper presents a study of incorporating domain-specific knowledge (i.e., information about concepts and relationships between concepts in a certain domain) in an information retrieval (IR) system to improve its effectiveness in retrieving biomedical literature. The effects of different types of domain-specific knowledge in performance contribution are examined. Based on the TREC platform, we show that appropriate use of domain-specific knowledge in a proposed conceptual retrieval model yields about 23% improvement over the best reported result in passage retrieval in the Genomics Track of TREC 2006.

#index 987275
#* Heavy-tailed distributions and multi-keyword queries
#@ Surajit Chaudhuri;Kenneth Church;Arnd Christian König;Liying Sui
#t 2007
#c 13
#% 212665
#% 290703
#% 340886
#% 387508
#% 397150
#% 580738
#% 781169
#! Intersecting inverted indexes is a fundamental operation for many applications in information retrieval and databases. Efficient indexing for this operation is known to be a hard problem for arbitrary data distributions. However, text corpora used in Information Retrieval applications often have convenient power-law constraints (also known as Zipf's Law and long tails) that allow us to materialize carefully chosen combinations of multi-keyword indexes, which significantly improve worst-case performance without requiring excessive storage. These multi-keyword indexes limit the number of postings accessed when computing arbitrary index intersections. Our evaluation on an e-commerce collection of 20 million products shows that the indexes of up to four arbitrary keywords can be intersected while accessing less than 20% of the postings in the largest single-keyword index.

#index 987276
#* ESTER: efficient search on text, entities, and relations
#@ Holger Bast;Alexandru Chitea;Fabian Suchanek;Ingmar Weber
#t 2007
#c 13
#% 125386
#% 399056
#% 577318
#% 642993
#% 810015
#% 869521
#% 875010
#% 879610
#% 913802
#% 956564
#% 1655411
#% 1682433
#! We present ESTER, a modular and highly efficient system for combined full-text and ontology search. ESTER builds on a query engine that supports two basic operations: prefix search and join. Both of these can be implemented very efficiently with a compact index, yet in combination provide powerful querying capabilities. We show how ESTER can answer basic SPARQL graph-pattern queries on the ontology by reducing them to a small number of these two basic operations. ESTER further supports a natural blend of such semantic queries with ordinary full-text queries. Moreover, the prefix search operation allows for a fully interactive and proactive user interface, which after every keystroke suggests to the user possible semantic interpretations of his or her query, and speculatively executes the most likely of these interpretations. As a proof of concept, we applied ESTER to the English Wikipedia, which contains about 3 million documents, combined with the recent YAGO ontology, which contains about 2.5 million facts. For a variety of complex queries, ESTER achieves worst-case query processing times of a fraction of a second, on a single machine, with an index size of about 4 GB.

#index 987277
#* Web text retrieval with a P2P query-driven index
#@ Gleb Skobeltsyn;Toan Luu;Ivana Podnar Zarko;Martin Rajman;Karl Aberer
#t 2007
#c 13
#% 333854
#% 610851
#% 766420
#% 768521
#% 800509
#% 818210
#% 839329
#% 839358
#% 888182
#% 907458
#% 907503
#% 956640
#% 1055120
#% 1180871
#% 1709424
#% 1715596
#! In this paper, we present a query-driven indexing/retrieval strategy for efficient full text retrieval from large document collections distributed within a structured P2P network. Our indexing strategy is based on two important properties: (1) the generated distributed index stores posting lists for carefully chosen indexing term combinations, and (2) the posting lists containing too many document references are truncated to a bounded number of their top-ranked elements. These two properties guarantee acceptable storage and bandwidth requirements, essentially because the number of indexing term combinations remains scalable and the transmitted posting lists never exceed a constant size. However, as the number of generated term combinations can still become quite large, we also use term statistics extracted from available query logs to index only such combinations that are frequently present in user queries. Thus, by avoiding the generation of superfluous indexing term combinations, we achieve an additional substantial reduction in bandwidth and storage consumption. As a result, the generated distributed index corresponds to a constantly evolving query-driven indexing structure that efficiently follows current information needs of the users. More precisely, our theoretical analysis and experimental results indicate that, at the price of a marginal loss in retrieval quality for rare queries, the generated index size and network traffic remain manageable even for web-size document collections. Furthermore, our experiments show that at the same time the achieved retrieval quality is fully comparable to the one obtained with a state-of-the-art centralized query engine.

#index 987278
#* Using gradient descent to optimize language modeling smoothing parameters
#@ Donald Metzler
#t 2007
#c 13
#% 397129
#% 840846
#% 907546

#index 987279
#* Locality discriminating indexing for document classification
#@ Jiani Hu;Weihong Deng;Jun Guo;Weiran Xu
#t 2007
#c 13
#% 344447
#% 766418
#! This paper introduces a locality discriminating indexing (LDI) algorithm for document classification. Based on the hypothesis that samples from different classes reside in class-specific manifold structures, LDI seeks for a projection which best preserves the within-class local structures while suppresses the between-class overlap. Comparative experiments show that the proposed method isable to derives compact discriminating document representations for classification.

#index 987280
#* Management of keyword variation with frequency based generation of word forms in IR
#@ Kimmo Kettunen
#t 2007
#c 13
#% 996869
#% 1664777
#! This paper presents a new management method for morphological variation of keywords. The method is called FCG, Frequent Case Generation. It is based on the skewed distributions of word forms in natural languages and is suitable for languages that have either fair amount of morphological variation or are morphologically very rich. The proposed method has been evaluated so far with four languages, Finnish, Swedish, German and Russian, which show varying degrees of morphological complexity.

#index 987281
#* OMES: a new evaluation strategy using optimal matching for document clustering
#@ Xiaojun Wan
#t 2007
#c 13
#% 296738
#% 387427
#! Existing measures for evaluating clustering results (e.g. F-measure) have the limitation of overestimating cluster quality because they usually adopt the greedy matching between classes (reference clusters) and clusters (system clusters) to allow multiple classes to correspond to one same cluster, which is in fact a locally optimal solution. This paper proposes a new evaluation strategy to overcome the limitation of existing evaluation measures by using optimal matching in graph theory. A weighted bipartite graph is built with classes and clusters as two disjoint sets of vertices and the edge weight between any class and any cluster is computed using a basic metric. Then the total weight of the optimal matching in the graph is acquired and we use it to evaluate the quality of the clusters. The optimal matching allows only one-to-one matching between classes and clusters and a globally optimal solution can be achieved. A preliminary study is performed to demonstrate the effectiveness of the proposed evaluation strategy.

#index 987282
#* Revisiting the dependence language model for information retrieval
#@ Loïc Maisonnasse;Eric Gaussier;Jean-Pierre Chevallet
#t 2007
#c 13
#% 262096
#% 397205
#% 766428
#! In this paper, we revisit the dependence language modelfor information retrieval proposed in [1], and show that thismodel is deficient from a theoretical point of view. We thenpropose a new model, well founded theoretically, for integratingdependencies between terms in the language model.This new model is simpler, yet more general, than the oneproposed in [1], and yields similar results in our experiments,on both syntactic and semantic dependencies.

#index 987283
#* Quantify query ambiguity using ODP metadata
#@ Guang Qiu;Kangmiao Liu;Jiajun Bu;Chun Chen;Zhiming Kang
#t 2007
#c 13
#% 292686
#% 995516
#! Query ambiguity prevents existing retrieval systems from returning reasonable results for every query. As there is already lots of work done on resolving ambiguity, vague queries could be handled using corresponding approaches separately if they can be identified in advance. Quantification of the degree of (lack of) ambiguity laysthe groundwork for the identification. In this poster, we propose such a measure using query topics based on the topic structure selected from the Open Directory Project (ODP) taxonomy. We introduce clarity score to quantify the lack of ambiguity with respect to data sets constructed from the TREC collections and the rank correlation test results demonstrate a strong positive association between the clarity scores and retrieval precisions for queries.

#index 987284
#* Combining error-correcting output codes and model-refinement for text categorization
#@ Songbo Tan;Yuefen Wang
#t 2007
#c 13
#% 413589
#% 464777
#% 465754
#% 466762
#% 838502
#! In this work, we explore the use of error-correcting output codes (ECOC) to enhance the performance of centroid text classifier. The framework is to decompose one multi-class problem into multiple binary problems and then learn the individual binary classification problems by centroid classifier. However, this kind of decomposition incurs considerable bias for centroid classifier, which results in noticeable degradation of performance. To address this issue, we use Model-Refinement to adjust this so-called bias.

#index 987285
#* User-oriented text segmentation evaluation measure
#@ Martin Franz;J. Scott McCarley;Jian-Ming Xu
#t 2007
#c 13
#% 278106
#% 350859
#% 448786
#! The paper describes a user oriented performance evaluation measure for text segmentation. Experiments show that the proposed measure differentiates well between error distributions with varying user impact.

#index 987286
#* Story segmentation of broadcast news in Arabic, Chinese and English using multi-window features
#@ Martin Franz;Jian-Ming Xu
#t 2007
#c 13
#% 211044
#% 817469
#% 1265039
#! The paper describes a maximum entropy based story segmentation system for Arabic, Chinese and English. In experiments with broadcast news data from TDT-3, TDT-4, and corpora collected in the DARPA GALE project we obtain a substantial performance gain using multiple overlapping windows for text-based features.

#index 987287
#* Recommending citations for academic papers
#@ Trevor Strohman;W. Bruce Croft;David Jensen
#t 2007
#c 13
#% 730089
#% 794860
#! We approach the problem of academic literature search by considering an unpublished manuscript as a query to a search system. We use the text of previous literature as well as the citation graph that connects it to find relevant related material. We evaluate our technique with manual and automatic evaluation methods, and find an order of magnitude improvement in mean average precision as compared to a text similarity baseline.

#index 987288
#* Exploration of the tradeoff between effectiveness and efficiency for results merging in federated search
#@ Suleyman Cetintas;Luo Si
#t 2007
#c 13
#% 1674907
#! Federated search is the task of retrieving relevant documents from different information resources. One of the main research problems in federated search is to combine the results from different sources into a single ranked list. Recent work proposed a regression based method to download some documents from each ranked list of the different sources, calculated comparable scores for the documents and estimated mapping functions that transform source-specific scores into comparable scores. Experiments have shown that downloading more documents improves the accuracy of results merging. However downloading more documents increases the computation and communication costs. This paper proposes a utility based optimization method that enables the system to automatically decide on the desired number of training documents to download according to the user's need for effectiveness and efficiency.

#index 987289
#* Understanding the relationship of information need specificity to search query length
#@ Nina Phan;Peter Bailey;Ross Wilkinson
#t 2007
#c 13
#% 284796
#% 340928
#% 893635
#! When searching, people's information needs flowthrough to expressing an information retrieval request posed to asearch engine. We hypothesise that the degree of specificity of anIR request might correspond to the length of a search query. Ourresults show a strong correlation between decreasing query lengthand increasing broadness or generality of the IR request. We foundan average cross-over point of specificity from broad to narrow of 3words in the query. These results have implications for searchengines in responding to queries of differing lengths.

#index 987290
#* An effective snippet generation method using the pseudo relevance feedback technique
#@ Youngjoong Ko;Hongkuk An;Jungyun Seo
#t 2007
#c 13
#% 54435
#% 280835
#% 860863
#! A (page or web) snippet is document excerpts allowing a user to understand if a document is indeed relevant without accessing it. This paper proposes an effective snippet generation method. The pseudo relevance feedback technique and text summarization techniques are applied to salient sentences extraction for generating good quality snippets. In the experimental results, the proposed method showed much better performance than other methods including Google and Naver.

#index 987291
#* Probability ranking principle via optimal expected rank
#@ H. C. Wu;Robert W. P. Luk;K. F. Wong
#t 2007
#c 13
#% 109207
#% 324129
#% 340899
#% 799246
#! This paper presents a new perspective of the probability ranking principle (PRP) by defining retrieval effectiveness in terms of our novel expected rank measure of a set of documents for a particular query. This perspective is based on preserving decision preferences, and it imposes weaker conditions on PRP than the utility-theoretic perspective of PRP.

#index 987292
#* Combining term-based and event-based matching for question answering
#@ Michael Wiegand;Jochen L. Leidner;Dietrich Klakow
#t 2007
#c 13
#% 642978
#% 855160
#! In question answering, two main kinds of matching methods for finding answer sentences for a question are term-based approaches -- which are simple, efficient, effective, and yield high recall -- and event-based approaches that take syntactic and semantic information into account. The latter often sacrifice recall for increased precision, but actually capture the meaning of the events denoted by the textual units of a passage or sentence. We propose a robust, data-driven method that learns the mapping between questions and answers using logistic regression and show that combining term-based and event-based approaches significantly outperforms the individual methods.

#index 987293
#* Confluence: enhancing contextual desktop search
#@ Karl Anders Gyllstrom;Craig Soules;Alistair Veitch
#t 2007
#c 13
#% 642983
#% 835189
#! We present Confluence, an enhancement to a desktop file search tool called Confluence which extracts conceptual relationships between files by their temporal access patterns in the file system. A limitation of a purely file-based approach is that as file operations are increasingly abstracted by applications, their correlation to a user's activity weakens and thereby reduces the applicability of their temporal patterns. To deal with this problem, we augment the file event stream with a stream of window focus events from the UI layer. We present 3 algorithms that analyze this new stream, extracting the user's task information which informs the existing Confluence algorithms. We present results and conclusions from a preliminary user study on Confluence.

#index 987294
#* Estimating the value of automatic disambiguation
#@ Paul Thomas;Tom Rowlands
#t 2007
#c 13
#% 169768
#% 642994
#% 754126
#% 766440
#% 818259
#% 838547
#% 853813
#! A common motivation for personalised search systems is the ability to disambiguate queries based on some knowledge of a user's interests. An analysis of log files from three search providers, covering a range of scenarios, suggests that this sort of disambiguation would be of marginal use for more specialised providers but may be of use for whole-of-Web search.

#index 987295
#* A generic framework for machine transliteration
#@ A. Kumaran;Tobias Kellner
#t 2007
#c 13
#% 730025
#% 740915
#% 854584
#% 855302
#% 938673

#index 987296
#* Where to start reading a textual XML document?
#@ Jaap Kamps;Marijn Koolen;Mounia Lalmas
#t 2007
#c 13
#% 872022
#% 878916
#% 1387539
#% 1387540
#! In structured information retrieval, the aim is to exploit document structure to retrieve relevant components, allowing the user to go straight to the relevant material. This paper looks at the so-called best entry points (BEPs), which are intended to give the user the best starting point to access the relevant information in the document. We examine the relationship between BEPs and relevant components in the INEX 2006 ad hoc assessments. Our main findings are the following: First, although documents are short, assessors often choose the best entry point some distance from the start of the document. Second, many of the best entry points coincide with the first relevant character in relevant documents, showing a strong relation between the BEP and relevant text. Third, we find browsing BEPs in articles with a single relevant passages, and container BEPs or context BEPs in articles with more relevant passages.

#index 987297
#* Novelty detection using local context analysis
#@ Ronald T. Fernández;David E. Losada
#t 2007
#c 13
#% 298183
#% 643014
#% 893432

#index 987298
#* Intra-assessor consistency in question answering
#@ Ian Ruthven;Leif Azzopardi Glasgow;Mark Baillie;Ralf Bierig;Emma Nicol;Simon Sweeney;Murat Yakici
#t 2007
#c 13
#% 309127
#% 816187
#% 940039
#! In this paper we investigate the consistency of answer assessment in a complex question answering task examining features of assessor consistency, types of answers and question type.

#index 987299
#* Towards robust query expansion: model selection in the language modeling framework
#@ Mattan Winaver;Oren Kurland;Carmel Domshlak
#t 2007
#c 13
#% 144076
#% 262084
#% 340899
#% 340901
#% 397161
#% 719598
#% 742666
#% 766525
#% 879585
#! We propose a language-model-based approach for addressing the performance robustness problem -- with respect to free-parameters' values -- of pseudo-feedback-based query-expansion methods. Given a query, we create a set of language models representing different forms of its expansion by varying the parameters' values of some expansion method; then, we select a single model using criteria originally proposed for evaluating the performance of using the original query, or for deciding whether to employ expansion at all. Experimental results show that these criteria are highly effective in selecting relevance language models that are not only significantly more effective than poor performing ones, but that also yield performance that is almost indistinguishable from that of manually optimized relevance models.

#index 987300
#* Automatic classification of web pages into bookmark categories
#@ Chris Staff;Ian Bugeja
#t 2007
#c 13
#% 247268
#% 248218
#% 342671
#% 400726
#% 746868
#% 766437
#! We describe a technique to automatically classify a web page into an existing bookmark category to help a user to bookmark a page. HyperBK compares a bag-of-words representation of the page to descriptions of categories in the user's bookmark file. Unlike default web browser dialog boxes in which the user may be presented with the category into which he or she saved the last bookmarked file, HyperBK also offers the category most similar to the page being bookmarked. The user can also opt to create a new category; or save the page elsewhere. In an evaluation, the user's preferred category was offered on average 61% of the time.

#index 987301
#* What emotions do news articles trigger in their readers?
#@ Kevin Hsin-Yih Lin;Changhua Yang;Hsin-Hsi Chen
#t 2007
#c 13
#% 1215357
#! We study the classification of news articles into emotions they invoke in their readers. Our work differs from previous studies, which focused on the classification of documents into their authors' emotions instead of the readers'. We use various combinations of feature sets to find the best combination for identifying the emotional influences of news articles on readers.

#index 987302
#* Evaluating discourse-based answer extraction for why-question answering
#@ Suzan Verberne;Lou Boves;Nelleke Oostdijk;Peter-Arno Coppen
#t 2007
#c 13
#% 719598
#% 854106
#% 995462

#index 987303
#* Topic segmentation using weighted lexical links (WLL)
#@ Laurianne Sitbon;Patrice Bellot
#t 2007
#c 13
#% 448786
#% 742204
#% 748583
#% 815186
#% 817489
#! This paper presents two new approaches of lexical chains for topic segmentation using weighted lexical chains (WLC) or weighted lexical links (WLL) between repeated occurrences of lemmas along the text. The main advantage of using these new approaches is the suppression of the empirical parameter called hiatus in lexical chain processing. An evaluation according to the WindowDiff measure on a large automatically built corpus shows slight improvements in WLL compared to state-of-the-art methods based on lexical chains.

#index 987304
#* Lexical analysis for modeling web query reformulation
#@ Alessandro Bozzon;Paul - Alexandru Chirita;Claudiu S. Firan;Wolfgang Nejdl
#t 2007
#c 13
#% 284796
#% 323131
#% 397161
#% 643057
#! Modeling Web query reformulation processes is still an unsolved problem. In this paper we argue that lexical analysis is highly beneficial for this purpose. We propose to use the variation in Query Clarity, as well as the Part-Of-Speech pattern transitions as indicators of user's search actions. Experiments with a log of 2.4 million queries showed our techniques to be more flexible than the current approaches, while also providing us with interesting insights into user's Web behavioral patterns.

#index 987305
#* Bridging the digital divide: understanding information access practices in an indian village community
#@ Mounia Lalmas;Ramnath Bhat;Maxine Frank;David Frohlich;Matt Jones
#t 2007
#c 13
#% 835027
#% 967281
#! For digital library and information retrieval technologies to provide solutions for bridging the digital divide in developing countries, we need to understand the information access practices of remote and often poor communities in these countries. We must understand the information needs of these communities, and the best means to provide them access to relevant information. To this end, we investigated the current information access practices in an Indian village.

#index 987306
#* BordaConsensus: a new consensus function for soft cluster ensembles
#@ Xavier Sevillano;Francesc Alías;Joan Claudi Socoró
#t 2007
#c 13
#% 296738
#% 340936
#% 722902
#% 766435
#% 803762
#! Consensus clustering is the task of deriving a single labeling by applying a consensus function on a cluster ensemble. This work introduces BordaConsensus, a new consensus function for soft cluster ensembles based on the Borda voting scheme. In contrast to classic, hard consensus functions that operate on labelings, our proposal considers cluster membership information, thus being able to tackle multiclass clustering problems. Initial small scale experiments reveal that, compared to state-of-the-art consensus functions, BordaConsensus constitutes a good performance vs. complexity trade-off.

#index 987307
#* A flexible retrieval system of shapes in binary images
#@ Gloria Bordogna;Luca Ghilardi;Simone Milesi;Marco Pagani
#t 2007
#c 13
#% 123923
#% 555600
#! This poster overviews the main characteristics of a flexible retrieval systems of shapes present in binary images and discusses some evaluation results. The system applies multiple indexing criteria of the shapes synthesizing distinct characteristics such as global features of the objects contour (Fourier Coefficients), boundary irregularities (Multifractal Spectrum), presence of concavities and convexities on the boundary (Contour Scale Space distribution). The system is flexible since it allows customizing the retrieval function to fit an application need. The query is a binary image containing the desired shape and a set of parameters specifying the distinct importance of the shape characteristics that must be taken into account to evaluate the relevance of the retrieved shapes. The retrieval function is then defined as a Flexible Multicriteria fusion Function producing ranked results. The evaluation experiments showed that this system can be suited to different retrieval purposes, and that generally the combination of the distinct shape indexing criteria increases both Recall and Precision with respect to the application of any single indexing criterion alone.

#index 987308
#* Semantic text classification of disease reporting
#@ Yi Zhang;Bing Liu
#t 2007
#c 13
#% 269217
#% 280817
#% 741891
#% 854933
#! Traditional text classification studied in the IR literature is mainly based on topics. That is, each class or category represents a particular topic, e.g., sports, politics or sciences. However, many real-world text classification problems require more refined classification based on some semantic aspects. For example, in a set of documents about a particular disease, some documents may report the outbreak of the disease, some may describe how to cure the disease, some may discuss how to prevent the disease, and yet some others may include all the above information. To classify text at this semantic level, the traditional "bag of words" model is no longer sufficient. In this paper, we report a text classification study at the semantic level and show that sentence semantic and structure features are very useful for such kind of classification. Our experimental results based on a disease outbreak dataset demonstrated the effectiveness of the proposed approach.

#index 987309
#* Evaluating relevant in context: document retrieval with a twist
#@ Jaap Kamps;Mounia Lalmas;Jovan Pehcevski
#t 2007
#c 13
#% 575729
#% 810914
#! The Relevant in Context retrieval task is document or article retrieval with a twist, where not only the relevant articles should be retrieved but also the relevant information within each article (captured by a set of XML elements) should be correctly identified. Our main research question is: how to evaluate the Relevant in Context task? We propose a generalized average precision measure that meets two main requirements: i) the score reflects the ranked list of articles inherent in the result list, and at the same time ii) the score also reflects how well the retrieved information per article (i.e., the set of elements) corresponds to the relevant information. The resulting measure was used at INEX 2006.

#index 987310
#* IDF revisited: a simple new derivation within the Robertson-Spärck Jones probabilistic model
#@ Lillian Lee
#t 2007
#c 13
#% 232645
#% 248223
#% 262037
#% 290703
#% 648481
#% 766412
#% 816060
#% 818238
#! There have been a number of prior attempts to theoretically justify the effectiveness of the inverse document frequency (IDF). Those that take as their starting point Robertson and Sparck Jones's probabilistic model are based on strong or complex assumptions. We show that a more intuitively plausible assumption suffices. Moreover, the new assumption, while conceptually very simple, provides a solution to an estimation problem that had been deemed intractable by Robertson and Walker (1997).

#index 987311
#* Validity and power of t-test for comparing MAP and GMAP
#@ Gordon V. Cormack;Thomas R. Lynam
#t 2007
#c 13
#% 375017
#% 818222
#% 907493
#! We examine the validity and power of the t-test, Wilcoxon test, and sign test in determining whether or not the difference in performance between two IR systems is significant. Empirical tests conducted on subsets of the TREC2004 Robust Retrieval collection indicate that the p-values computed by these tests for the difference in mean average precision (MAP) between two systems are very accurate fora wide range of sample sizes and significance estimates. Similarly, these tests have good power, with the t-test proving superior overall. The t-test is also valid for comparing geometric mean average precision (GMAP), exhibiting slightly superior accuracy and slightly inferior power than for MAPcomparison.

#index 987312
#* Model-averaged latent semantic indexing
#@ Miles Efron
#t 2007
#c 13
#% 280822
#% 387427
#% 643002
#% 811488
#% 818203
#! This poster introduces a novel approach to information retrieval that uses statistical model averaging to improve latent semantic indexing (LSI). Instead of choosing a single dimensionality $k$ for LSI , we propose using several models of differing dimensionality to inform retrieval. To manage this ensemble we weight each model's contribution to an extent inversely proportional to its AIC (Akaike information criterion). Thus each model contributes proportionally to its expected Kullback-Leibler divergence from the distribution that generated the data. We present results on three standard IR test collections, demonstrating significant improvement over both the traditional vector space model and single-model LSI.

#index 987313
#* Characterizing the value of personalizing search
#@ Jaime Teevan;Susan T. Dumais;Eric Horvitz
#t 2007
#c 13
#% 214709
#% 309095
#% 722310
#! We investigate the diverse goals that people have when they issue the same query to a search engine, and the ability of current search engines to address such diversity. We quantify the potential value of personalizing search results based on this analysis. Great variance was found in the results that different individuals rated as relevant for the same query -- even when the same information goal was expressed. Our analysis suggests that while search engines do a good job of ranking results to maximize global happiness, they do not do a very good job for specific individuals.

#index 987314
#* Improving retrieval accuracy by weighting document types with clickthrough data
#@ Peter C. K. Yeung;Charles L. A. Clarke;Stefan Büttcher
#t 2007
#c 13
#% 783474
#% 818258
#% 879567
#! For enterprise search, there exists a relationship between work task and document type that can be used to refine search results. In this poster, we adapt the popular Okapi BM25 scoring function to weight term frequency based on the relevance of a document type to a work task. Also, we use click frequency for each task-type pair to estimate a realistic weight. Using the W3C collection from the TREC Enterprise track for evaluations, our approach leads to significant improvements on search precision.

#index 987315
#* Protecting source privacy in federated search
#@ Wei Jiang;Luo Si;Jing Li
#t 2007
#c 13
#% 261357
#% 319994
#% 389801
#% 664654
#! Many information sources contain information that can only be accessed through search-specific search engines. Federated search provides search solutions of this type of hidden information that cannot be searched by conventional search engines. In many scenarios of federated search, such as the search among health care providers or among intelligence agencies, an individual information source does not want to disclose the source of the search results to users or other sources. Therefore, this paper proposes a two-step federated search protocol that protects the privacy of information sources. As far as we know, this is the first attempt to address the research problem of protecting source privacy in federated text search.

#index 987316
#* Applying ranking SVM in query relaxation
#@ Ciya Liao;Thomas Chang
#t 2007
#c 13
#% 577224
#% 879588
#! We propose an approach QRRS (Query Relaxative Ranking SVM) that divides a ranking function into different relaxation steps, so that only cheap features are used in Ranking SVM of early steps for query efficiency. We show search quality in the approach is improved compared to conventional Ranking SVM.

#index 987317
#* Learning to rank collections
#@ Jingfang Xu;Xing Li
#t 2007
#c 13
#% 194246
#% 577224
#% 643012
#% 734915
#% 840846
#% 993964
#! Collection selection, ranking collections according to user query is crucial in distributed search. However, few features are used to rank collections in the current collection selection methods, while hundreds of features are exploited to rank web pages in web search. The lack of features affects the efficiency of collection selection in distributed search. In this paper, we exploit some new features and learn to rank collections with them through SVM and RankingSVM respectively. Experimental results show that our features are beneficial to collection selection, and the learned ranking functions outperform the classical CORI algorithm.

#index 987318
#* VideoReach: an online video recommendation system
#@ Tao Mei;Bo Yang;Xian-Sheng Hua;Linjun Yang;Shi-Qiang Yang;Shipeng Li
#t 2007
#c 13
#% 280817
#% 1712775
#% 1857498
#% 1858178
#! This paper presents a novel online video recommendation system called VideoReach, which alleviates users' efforts on finding the most relevant videos according to current viewings without a sufficient collection of user profiles as required in traditional recommenders. In this system, video recommendation is formulated as finding a list of relevant videos in terms of multimodal relevance (i.e. textual, visual, and aural relevance) and user click-through. Since different videos have different intra-weights of relevance within an individual modality and inter-weights among different modalities, we adopt relevance feedback to automatically find optimal weights by user click-though, as well as an attention fusion function to fuse multimodal relevance. We use 20 clips as the representative test videos, which are searched by top 10 queries from more than 13k online videos, and report superior performance compared with an existing video site.

#index 987319
#* Modelling epistemic uncertainty in ir evaluation
#@ Murat Yakici;Mark Baillie;Ian Ruthven;Fabio Crestani
#t 2007
#c 13
#% 879650
#% 1039842
#! Modern information retrieval (IR) test collections violate the completeness assumption of the Cranfield paradigm. In order to maximise the available resources, only a sample of documents (i.e. the pool) are judged for relevance by a human assessor(s). The subsequent evaluation protocol does not make any distinctions between assessed or unassesseddocuments, as documents that are not in the pool are assumedto be not relevant for the topic. This is beneficial from a practical point of view, as the relative performance can be compared with confidence if the experimental conditions are fair for all systems. However, given the incompleteness of relevance assessments, two forms of uncertainty emerge during evaluation. The first is Aleatory uncertainty, which refers to variation in system performance across the topic set, which is often addressed through the use of statistical significance tests. The second form of uncertainty is Epistemic, which refers to the amount of knowledge (or ignorance) we have about the estimate of a system's performance. Epistemic uncertainty is a consequence of incompleteness and is not addressed by the current evaluation protocol. In this study, we present a first attempt at modelling both aleatory and epistemic uncertainty associatedwith IR evaluation. We aim to account for both the variability associated with system performance and the amount of knowledge known about the performance estimate.

#index 987320
#* On the importance of preserving the part-order in shape retrieval
#@ Arne Schuldt;Björn Gottfried;Ole Osterhagen;Otthein Herzog
#t 2007
#c 13
#! This paper discusses the importance of part-order-preservation in shape matching. A part descriptor is introduced that supports both preserving and abandoning the order of parts. The evaluation shows that retrieval results are improved by almost 38% if the original ordering is preserved.

#index 987321
#* The relationship between IR effectiveness measures and user satisfaction
#@ Azzah Al-Maskari;Mark Sanderson;Paul Clough
#t 2007
#c 13
#% 309095
#% 411762
#% 420508
#% 835027
#! This paper presents an experimental study of users assessing the quality of Google web search results. In particular we look at how users' satisfaction correlates with the effectiveness of Google as quantified by IR measures such as precision and the suite of Cumulative Gain measures (CG, DCG, NDCG). Results indicate strong correlation between users' satisfaction, CG and precision, moderate correlation with DCG, with perhaps surprisingly negligible correlation with NDCG. The reasons for the low correlation with NDCG are examined.

#index 987322
#* A multi-criteria content-based filtering system
#@ Gabriella Pasi;Gloria Bordogna;Robert Villa
#t 2007
#c 13
#% 124004
#% 218979
#% 754106
#! In this paper we present a novel filtering system, based on a new model which reshapes the aims of content-based filtering. The filtering system has been developed within the EC project PENG, aimed at providing news professionals, such as journalists, with a system supporting both filtering and retrieval capabilities. In particular, we suggest that in tackling the problem of information overload, it is necessary for filtering systems to take into account multiple aspects of incoming documents in order to estimate their relevance to a user's profile, and in order to help users better understand documents, as distinct from solely attempting to either select relevant material from a stream, or block inappropriate material. Aiming to so this, a filtering model based on multiple criteria has been defined, based on the ideas gleamed in the project requirements stage. The filtering model is briefly described in this paper.

#index 987323
#* Boosting static pruning of inverted files
#@ Roi Blanco;Alvaro Barreiro
#t 2007
#c 13
#% 340887
#! This paper revisits the static term-based pruning technique presented in Carmel et al., SIGIR 2001 for ad-hoc retrieval, addressing different issues concerning its algorithmic design not yet taken into account. Although the original technique is able to retain precision when a considerable part of the inverted file is removed, we show that it is possible to improve precision in some scenarios if some key design features are properly selected.

#index 987324
#* Resource monitoring in information extraction
#@ Jochen L. Leidner
#t 2007
#c 13
#% 642389
#% 742553
#! It is often argued that in information extraction (IE), certain machine learning (ML) approaches save development time over others, or that certain ML methods (e.g. Active Learning) require less training data than others, thus saving development cost. However, such development cost claims are not normally backed up by controlled studies which show that such development cost savings actually occur. This situation in Language Engineering is contrasted with Software Engineering in general, where a lot of studies investigating system development cost have been carried out. We argue for the need of controlled studies that measure actual system development time in language engineering. To this end, we carry out an experiment in resource monitoring for an IE task: three named entity taggers for the same "surprise" domain are developed in parallel, using competing methods. Their human development time is accounted forusing a logging facility.We report development cost results for parallel implementations of a named entity tagger and present a breakdown of the development time for the three alternative methods. We are not aware of detailed previous parallel studies that detail how system development time is spent when creating a named entity tagger.

#index 987325
#* The DILIGENT framework for distributed information retrieval
#@ Fabio Simeoni;Fabio Crestani;Ralf Bierig
#t 2007
#c 13
#% 194246
#% 722312
#% 783473
#% 852010

#index 987326
#* Varying approaches to topical web query classification
#@ Steven M. Beitzel;Eric C. Jensen;Abdur Chowdhury;Ophir Frieder
#t 2007
#c 13
#% 844287
#% 853542
#% 879581
#! Topical classification of web queries has drawn recent interest because of the promise it offers in improving retrieval effectiveness and efficiency. However, much of this promise depends on whether classification is performed before or after the query is used to retrieve documents. We examine two previously unaddressed issues in query classification: pre versus post-retrieval classification effectiveness and the effect of training explicitly from classified queries versus bridging a classifier trained using a document taxonomy. Bridging classifiers map the categories of a document taxonomy onto those of a query classification problem to provide sufficient training data. We find that training classifiers explicitly from manually classified queries outperforms the bridged classifier by 48% in F1 score. Also, a pre-retrieval classifier using only the query terms performs merely 11% worse than the bridged classifier which requires snippets from retrieved documents.

#index 987327
#* A comparison of pooled and sampled relevance judgments
#@ Ian Soboroff
#t 2007
#c 13
#% 879632
#% 879650
#% 907496
#! Test collections are most useful when they are reusable, that is, when they can be reliably used to rank systems that did not contribute to the pools. Pooled relevance judgments for very large collections may not be reusable for two easons: they will be very sparse and not sufficiently complete, and they may be biased in the sense that theywill unfairly rank some class of systems. The TREC 2006 terabyte track judged both a pool and a deep random sample in order to measure the effects of sparseness and bias.

#index 987328
#* Clustering short texts using wikipedia
#@ Somnath Banerjee;Krishnan Ramanathan;Ajay Gupta
#t 2007
#c 13
#% 55490
#% 727861
#! Subscribers to the popular news or blog feeds (RSS/Atom) often face the problem of information overload as these feed sources usually deliver large number of items periodically. One solution to this problem could be clustering similar items in the feed reader to make the information more manageable for a user. Clustering items at the feed reader end is a challenging task as usually only a small part of the actual article is received through the feed. In this paper, we propose a method of improving the accuracy of clustering short texts by enriching their representation with additional features from Wikipedia. Empirical results indicate that this enriched representation of text items can substantially improve the clustering accuracy when compared to the conventional bag of words representation.

#index 987329
#* Estimating collection size with logistic regression
#@ Jingfang Xu;Sheng Wu;Xing Li
#t 2007
#c 13
#% 413635
#% 643012
#% 869499
#% 879604
#! Collection size is an important feature to represent the content summaries of a collection, and plays a vital role in collection selection for distributed search. In uncooperative environments, collection size estimation algorithms are adopted to estimate the sizes of collections with their search interfaces. This paper proposes heterogeneous capture (HC) algorithm, in which the capture probabilities of documents are modeled with logistic regression. With heterogeneous capture probabilities, HC algorithm estimates collection size through conditional maximum likelihood. Experimental results on real web data show that our HC algorithm outperforms both multiple capture-recapture and capture history algorithms.

#index 987330
#* Selection and ranking of text from highly imperfect transcripts for retrieval of video content
#@ Alexander Haubold
#t 2007
#c 13
#% 789355
#! In the domain of video content retrieval, we present an approach for selecting words and phrases from highly imperfect automatically generated transcripts. Extracted terms are ranked according to their descriptiveness and presented to the user in a multimedia browser interface. We use sense querying from the WordNet lexical database for our method of text selection and ranking. Evaluation of 679 video summarization tasks from 442 users shows that the method of ranking and emphasizing terms according to descriptiveness results in higher accuracy responses in less time compared to the baseline of no ranking.

#index 987331
#* Enhancing patent retrieval by citation analysis
#@ Atsushi Fujii
#t 2007
#c 13
#% 240201
#% 268079
#% 289079
#! This paper proposes a method to combine text-based and citation-based retrieval methods in the invalidity patent search. Using the NTCIR-6 test collection including eight years of USPTO patents, we show the effectiveness of our method experimentally.

#index 987332
#* MRF based approach for sentence retrieval
#@ Keke Cai;Chun Chen;Kangmiao Liu;Jiajun Bu;Peng Huang
#t 2007
#c 13
#% 818262
#! This poster focuses on the study of term context dependence in the application of sentence retrieval. Based on Markov Random Field (MRF), three forms of dependence among query terms are considered. Under different assumptions of term dependence relationship, three feature functions are defined, with the purpose to utilize association features between query terms in sentence to evaluate the relevance of sentence. Experimental results have proven the efficiency of the proposed retrieval models in improving the performance of sentence retrieval.

#index 987333
#* Improving weak ad-hoc queries using wikipedia asexternal corpus
#@ Yinghao Li;Wing Pong Robert Luk;Kei Shiu Edward Ho;Fu Lai Korris Chung
#t 2007
#c 13
#% 169781
#% 438557
#% 766525
#% 818262
#% 879584
#% 907493
#% 1676545
#! In an ad-hoc retrieval task, the query is usually short and the user expects to find the relevant documents in the first several result pages. We explored the possibilities of using Wikipedia's articles as an external corpus to expand ad-hoc queries. Results show promising improvements over measures that emphasize on weak queries.

#index 987334
#* Fine-grained named entity recognition and relation extraction for question answering
#@ Changki Lee;Yi-Gyu Hwang;Myung-Gil Jang
#t 2007
#c 13
#% 815283
#% 815868
#% 816181
#% 818274
#% 854688
#% 1676610

#index 987335
#* World knowledge in broad-coverage information filtering
#@ Bennett A. Hagedorn;Massimiliano Ciaramita;Jordi Atserias
#t 2007
#c 13
#% 316548
#% 722903
#% 854646
#% 1392432
#% 1665151

#index 987336
#* The influence of basic tokenization on biomedical document retrieval
#@ Dolf Trieschnigg;Wessel Kraaij;Franciska de Jong
#t 2007
#c 13
#% 799695
#! Tokenization is a fundamental preprocessing step in Information Retrieval systems in which text is turned into index terms. This paper quantifies and compares the influence of various simple tokenization techniques on document retrieval effectiveness in two domains: biomedicine and news. As expected, biomedical retrieval is more sensitive to small changes in the tokenization method. The tokenization strategy can make the difference between a mediocre and well performing IR system, especially in the biomedical domain.

#index 987337
#* Using clustering to enhance text classification
#@ Antonia Kyriakopoulou;Theodore Kalamboukis
#t 2007
#c 13
#% 406493
#% 464780
#% 466263
#% 722930
#% 727899
#! This paper addresses the problem of learning to classify textsby exploiting information derived from clustering both training and testing sets. The incorporation of knowledge resulting from clustering into the feature space representation of the texts is expected to boost the performance of a classifier. Experiments conducted on several widely used datasets demonstrate the effectiveness of the proposed algorithm especially for small training sets.

#index 987338
#* A fact/opinion classifier for news articles
#@ Adam Stepinski;Vibhu Mittal
#t 2007
#c 13
#% 746867
#% 1269535
#! Many online news/blog aggregators like Google, Yahoo and MSN allow users to browse/search many hundreds of news sources. This results in dozens, often hundreds, of stories about the same event. While the news aggregators cluster these stories, allowing the user to efficiently scan the major news items at any given time, they do not currently allow alternative browsing mechanisms within the clusters. Furthermore, their intra-cluster ranking mechanisms are often based on a notion of authority/popularity of the source. In many cases, this leads to the classic power law phenomenon -- the popular stories/sources are the ones that are already popular/authoritative, thus reinforcing one dominant viewpoint. Ideally, these aggregators would exploit the availability of the tremendous number of sources to identify the various dominant threads or viewpoints about a story and highlight these threads for the users. This paper presents an initial limited approach to such an interface: it classifies articles into two categories: fact and opinion. We show that the combination of (i) a classifier trained on a small (140K) training set of editorials/reports and (ii) an interactive user interface that ameliorates classification errors by re-ordering the presentation can be effective in highlighting different underlying viewpoints in a story-cluster. We briefly discuss the classifier used here, the training set and the UI and report on some initial anecdotal user feedback and evaluation.

#index 987339
#* Matching resumes and jobs based on relevance models
#@ Xing Yi;James Allan;W. Bruce Croft
#t 2007
#c 13
#% 340899
#% 340901
#! We investigate the difficult problem of matching semi-structured resumes and jobs in a large scale real-world collection. We compare standard approaches to Structured Relevance Models (SRM), an extensionof relevance-based language model for modeling and retrieving semi-structured documents. Preliminary experiments show that the SRM approach achieved promising performance and performed better than typical unstructured relevance models.

#index 987340
#* The utility of linguistic rules in opinion mining
#@ Xiaowen Ding;Bing Liu
#t 2007
#c 13
#% 746885
#% 769892
#% 936239
#% 939896
#% 1261566
#! Online product reviews are one of the important opinion sources on the Web. This paper studies the problem of determining the semantic orientations (positive or negative) of opinions expressed on product features in reviews. Most existing approaches use a set of opinion words for the purpose. However, the semantic orientations of many words are context dependent. In this paper, we propose to use some linguistic rules to deal with the problem together with a new opinion aggregation function. Extensive experiments show that these rules and the function are highly effective. A system, called Opinion Observer, has also been built.

#index 987341
#* A comparison of sentence retrieval techniques
#@ Niranjan Balasubramanian;James Allan;W. Bruce Croft
#t 2007
#c 13
#% 340901
#% 740915
#% 818262
#% 838398
#% 838508
#% 879584
#% 980527
#% 1272053
#! Identifying redundant information in sentences is useful for several applications such as summarization, document provenance, detecting text reuse and novelty detection. The task of identifying redundant information in sentences is defined as follows: Given a query sentence the task is to retrieve sentences from a given collection that express all or some subset of the information present in the query sentence. Sentence retrieval techniques rank sentences based on some measure of their similarity to a query. The effectiveness of such techniques depends on the similarity measure used to rank sentences. An effective retrieval model should be able to handle low word overlap between query and candidate sentences and go beyond just word overlap. Simple language modeling techniques like query likelihood retrieval have outperformed TF-IDF and word overlap based methods for ranking sentences. In this paper, we compare the performance of sentence retrieval using different language modeling techniques for the problem of identifying redundant information.

#index 987342
#* High-dimensional visual vocabularies for image retrieval
#@ Joao Magalhaes;Stefan Rueger
#t 2007
#c 13
#% 457912
#% 465895
#! In this paper we formulate image retrieval by text query as a vector space classification problem. This is achieved by creating a high-dimensional visual vocabulary that represents the image documents in great detail. We show how the representation of these image documents enables the application of well known text retrieval techniques such as Rocchio tf-idf and naíve Bayes to the semantic image retrieval problem. We tested these methods on a Corel images subset and achieve state-of-the-art retrieval performance using the proposed methods.

#index 987343
#* A web page topic segmentation algorithm based on visual criteria and content layout
#@ Idir Chibane;Bich-Lien Doan
#t 2007
#c 13
#% 324987
#% 330765
#% 480135
#% 577281
#% 1394469
#! This paper presents experiments using an algorithm of web page topic segmentation that show significant precision improvement in the retrieval of documents issued from the Web track corpus of TREC 2001. Instead of processing the whole document, a web page is segmented into different semantic blocks according to visual criteria (such as horizontal lines, colors) and structural tags (such as headings ~, paragraph ). We conclude that combining visual and content layout criteria gives the best results for increasing the precision: the ranking of the page is calculated for relevant segments of pages resulting from the segmentation algorithm.

#index 987344
#* Document clustering: an optimization problem
#@ Ao Feng
#t 2007
#c 13
#% 36672
#! Clustering algorithms have been widely used in information retrieval applications. However, it is difficult to define an objective "best" result. This article analyzes some document clustering algorithms and illustrates that they are equivalent to the optimization problem of some global functions. Experiments show their good performance, but there are still counter-examples where they fail to return the optimal solution. We argue that Monte-Carlo algorithms in the global optimization framework have the potential to find better solutions than traditional clustering, and they are able to handle more complex structures.

#index 987345
#* Finding similar experts
#@ Krisztian Balog;Maarten de Rijke
#t 2007
#c 13
#% 754059
#% 1275180
#! The task of finding people who are experts on a topic has recently received increased attention. We introduce a different expert finding task for which a small number of example experts is given (instead of a natural language query), and the system's task is to return similar experts. We define, compare, and evaluate a number of ways of representing experts, and investigate how the size of theinitial example set affects performance. We show that morefine-grained representations of candidates result in higher performance, and larger sample sets as input lead to improved precision.

#index 987346
#* Active learning for class imbalance problem
#@ Seyda Ertekin;Jian Huang;C. Lee Giles
#t 2007
#c 13
#% 722797
#% 916781
#% 1271973
#! The class imbalance problem has been known to hinder the learning performance of classification algorithms. Various real-world classification tasks such as text categorization suffer from this phenomenon. We demonstrate that active learning is capable of solving the problem.

#index 987347
#* Strategies for retrieving plagiarized documents
#@ Benno Stein;Sven Meyer zu Eissen;Martin Potthast
#t 2007
#c 13
#% 571725
#% 1688264
#% 1719552
#! For the identification of plagiarized passages in large document collections we present retrieval strategies which rely on stochastic sampling and chunk indexes. Using the entire Wikipedia corpus we compile n-gram indexes and compare them to a new kind of fingerprint index in a plagiarism analysis use case. Our index provides an analysis speed-up by factor 1.5 and is an order of magnitude smaller, while being equivalent in terms of precision and recall.

#index 987348
#* Generative modeling of persons and documents for expert search
#@ Pavel Serdyukov;Djoerd Hiemstra;Maarten Fokkinga;Peter M. G. Apers
#t 2007
#c 13
#% 342707
#% 768898
#% 869649
#% 879570
#! In this paper we address the task of automatically finding an expert within the organization, known as the expert search problem. We present the theoretically-based probabilistic algorithm which models retrieved documents as mixtures of expert candidate language models. Experiments show that our approach outperforms existing theoretically sound solutions.

#index 987349
#* Random walk term weighting for information retrieval
#@ Roi Blanco;Christina Lioma
#t 2007
#c 13
#% 1272053
#% 1299755
#! We present a way of estimating term weights for Information Retrieval (IR), using term co-occurrence as a measure of dependency between terms.We use the random walk graph-based ranking algorithm on a graph that encodes terms and co-occurrence dependencies in text, from which we derive term weights that represent a quantification of how a term contributes to its context. Evaluation on two TREC collections and 350 topics shows that the random walk-based term weights perform at least comparably to the traditional tf-idf term weighting, while they outperform it when the distance between co-occurring terms is between 6 and 30 terms.

#index 987350
#* Comparing query logs and pseudo-relevance feedbackfor web-search query refinement
#@ Ryen W. White;Charles L. A. Clarke;Silviu Cucerzan
#t 2007
#c 13
#% 218978
#% 309095
#% 397161
#% 754059
#% 869501
#! Query logs and pseudo-relevance feedback (PRF) offer ways in which terms to refine Web searchers' queries can be selected, offered to searchers, and used to improve search effectiveness. In this poster we present a study of these techniques that aims to characterize the degree of similarity between them across a set of test queries, and the same set broken out by query type. The results suggest that: (i) similarity increases with the amount of evidence provided to the PRF algorithm, (ii) similarity is higherwhen titles/snippets are used for PRF than full-text, and (iii) similarity is higher for navigational than informational queries. The findings have implications for the combined usage of query logs and PRF in generating query refinement alternatives.

#index 987351
#* Automatic extension of non-english wordnets
#@ Katja Hofmann;Erik Tjong Kim Sang
#t 2007
#c 13
#% 282421

#index 987352
#* First experiments searching spontaneous Czech speech
#@ Pavel Ircing;Douglas W. Oard;Jan Hoidekr
#t 2007
#c 13
#% 818313
#% 939619
#% 1916129

#index 987353
#* Power and bias of subset pooling strategies
#@ Gordon V. Cormack;Thomas R. Lynam
#t 2007
#c 13
#% 262097
#% 375017
#% 766409
#% 818222
#% 879598
#% 879632
#! We define a method to estimate the random and systematic errors resulting from incomplete relevance assessments.Mean Average Precision (MAP) computed over a large number of topics with a shallow assessment pool substantially outperforms -- for the same adjudication effort MAP computed over fewer topics with deeper pools, and P@k computed with pools of the same depth. Move-to-front pooling,previously reported to yield substantially better rank correlation, yields similar power, and lower bias, compared tofixed-depth pooling.

#index 987354
#* Problems with Kendall's tau
#@ Mark Sanderson;Ian Soboroff
#t 2007
#c 13
#% 262105
#% 340892
#% 766409
#% 766410
#% 818267
#% 838529
#% 907496
#! This poster describes a potential problem with a relatively well used measure in Information Retrieval research: Kendall's Tau rank correlation coefficient. The coefficient is best known for its use in determining the similarity of test collections when ranking sets of retrieval runs. Threshold values for the coefficient have been defined and used in a number of published studies in information retrieval. However, this poster presents results showing that basing decisions on such thresholds is not as reliableas has been assumed.

#index 987355
#* Opinion holder extraction from author and authority viewpoints
#@ Yohei Seki
#t 2007
#c 13
#% 939898
#! Opinion holder extraction research is important for discriminating between opinions that are viewed from different perspectives. In this paper, we describe our experience of participation in the NTCIR-6 Opinion Analysis Pilot Task by focusing on opinion holder extraction results in Japanese and English. Our approach to opinion holder extraction was based on the discrimination between author and authority viewpoints in opinionated sentences, and the evaluation results were fair with respect to the Japanese documents.

#index 987356
#* Incorporating term dependency in the dfr framework
#@ Jie Peng;Craig Macdonald;Ben He;Vassilis Plachouras;Iadh Ounis
#t 2007
#c 13
#% 397205
#% 818262
#! Term dependency, or co-occurrence, has been studied in language modelling, for instance by Metzler & Croft who showed that retrieval performance could be significantlyenhanced using term dependency information. In this work, weshow how term dependency can be modelled within the Divergence From Randomness (DFR) framework. We evaluate our term dependency model on the two adhoc retrieval tasks using the TREC .GOV2 Terabyte collection. Furthermore, we examine the effect of varying the term dependency window size on the retrieval performance of the proposed model. Our experiments show that term dependency can indeed besuccessfully incorporated within the DFR framework.

#index 987357
#* Hits on question answer portals: exploration of link analysis for author ranking
#@ Pawel Jurczyk;Eugene Agichtein
#t 2007
#c 13
#% 290830
#% 879593
#! Question-Answer portals such as Naver and Yahoo! Answers are growing in popularity. However, despite the increased popularity, the quality of answers is uneven, and while some users usually provide good answers, many others often provide bad answers. Hence, estimating the authority, or the expected quality of users, is a crucial task for this emerging domain, with potential applications to answer ranking and to incentive mechanism design. We adapt a powerful link analysis methodology from the web domain as a first step towards estimating authority in Question Answer portals. Our experimental results over more than 3 million answers from Yahoo! Answers are promising, and warrant further exploration along the lines outlined in this poster.

#index 987358
#* Heads and tails: studies of web search with common and rare queries
#@ Doug Downey;Susan Dumais;Eric Horvitz
#t 2007
#c 13
#% 284796
#% 310567
#% 1650298
#! A large fraction of queries submitted to Web search enginesoccur very infrequently. We describe search log studiesaimed at elucidating behaviors associated with rare andcommon queries. We present several analyses and discussresearch directions.

#index 987359
#* Dimensionality reduction for dimension-specific search
#@ Zi Huang;Hengtao Shen;Xiaofang Zhou;Dawei Song;Stefan Rüger
#t 2007
#c 13
#% 342828
#% 766418
#% 882477
#% 1727507
#! Dimensionality reduction plays an important role in efficient similarity search, which is often based on k-nearest neighbor (k-NN) queries over a high-dimensional feature space. In this paper, we introduce a novel type of k-NN query, namely conditional k-NN (ck-NN), which considers dimension-specific constraint in addition to the inter-point distances. However, existing dimensionality reduction methods are not applicable to this new type of queries. We propose a novel Mean-Std (standard deviation) guided Dimensionality Reduction (MSDR) to support a pruning based efficient ck-NN query processing strategy. Our preliminary experimental results on 3D protein structure data demonstrate that the MSDR method is promising.

#index 987360
#* An effective method for finding best entry points in semi-structured documents
#@ Eugen Popovici;Pierre-François Marteau;Gildas Ménier
#t 2007
#c 13
#% 730085
#% 872022
#% 872023
#% 878916
#! Focused structured document retrieval employs the concept of best entry point (BEP), which is intended to provide optimal starting-point from which users can browse to relevant document components [4]. In this paper we describe and evaluate a method for finding BEPs in XML documents. Experiments conducted within the framework of INEX 2006 evaluation campaign on the Wikipedia XML collection [2] shown the effectiveness of the proposed approach.

#index 987361
#* Query rewriting using active learning for sponsored search
#@ Wei Vivian Zhang;Xiaofei He;Benjamin Rey;Rosie Jones
#t 2007
#c 13
#% 869501
#% 876080
#! Sponsored search is a major revenue source for search companies. Web searchers can issue any queries, while advertisement keywords are limited. Query rewriting technique effectively matches user queries with relevant advertisement keywords, thus increases the amount of web advertisements available. The match relevance is critical for clicks. In this study, we aim to improve query rewriting relevance. For this purpose, we use an active learning algorithm called Transductive Experimental Design to select the most informative samples to train the query rewriting relevance model. Experiments show that this approach significantly improves model accuracy and rewriting relevance.

#index 987362
#* An analysis of peer-to-peer file-sharing system queries
#@ Linh Thai Nguyen;Dongmei Jia;Wai Gen Yee;Ophir Frieder
#t 2007
#c 13
#% 730035
#% 741411
#% 766447
#% 1180871
#! Many studies focus on the Web, but yet, few focus on peer-to-peer file-sharing system queries despite their massive scale in terms of Internet traffic. We analyzed several million queries collected on the Gnutella network and differentiated our findings from those of Web queries.

#index 987363
#* Investigating the relevance of sponsored results for web ecommerce queries
#@ Bernard J. Jansen
#t 2007
#c 13
#% 296646
#% 306468
#% 874713
#% 906824
#! Are sponsored links, the primary business model for Web search engines, providing Web consumers with relevant results? This research addresses this issue by investigating the relevance of sponsored and non-sponsored links for ecommerce queries from the major search engines. The results show that average relevance ratings for sponsored and non-sponsored links are virtually the same, although the relevance ratings for sponsored links are statistically higher. We used 108 ecommerce queries and 8,256 retrieved links for these queries from three major Web search engines, Google, MSN, and Yahoo!. We present the implications for Web search engines and sponsored search as a long-term business model as well as a mechanism for finding relevant information for searchers.

#index 987364
#* Viewing online searching within a learning paradigm
#@ Bernard J. Jansen;Brian Smith;Danielle L. Booth
#t 2007
#c 13
#! In this research, we investigate whether one can model online searching as a learning paradigm. We examined the searching characteristics of 41 participants engaged in 246 searching tasks. We classified the searching tasks according to Anderson and Krathwohl's Taxonomy, an updated version of Bloom's taxonomy. Anderson and Krathwohl is a six level categorization of cognitive learning. Research results show that Applying takes the most searching effort as measured by queries per session and specific topics searched per sessions. The categories of Remembering and Understanding, which are lower-order learning levels, exhibit searching characteristics similar to the higher order categories of Evaluating and Creating. It seems that searchers rely primarily on their internal knowledge and use searching primarily as fact checking and verification when engaged in Evaluating and Creating. Implications are that the commonly held notions of Web searchers having simple information goals may not be correct. We discuss the implications for Web searching, including designing interfaces to support exploration and alternate views.

#index 987365
#* More efficient parallel computation of pagerank
#@ John R. Wicks;Amy Greenwald
#t 2007
#c 13
#% 1016164
#% 1742089

#index 987366
#* Using similarity links as shortcuts to relevant web pages
#@ Mark D. Smucker;James Allan
#t 2007
#c 13
#% 230521
#% 348148
#% 590525
#% 807657
#% 879583
#% 879622
#! Successful navigation from a relevant web page to other relevant pages depends on the page linking to other relevant pages. We measured the distance to travel from relevant page to relevant page and found a bimodal distribution of distances peaking at 4 and 15 hops. In an attempt to make it easier to navigate among relevant pages, we added content similarity links to pages. With these additional links, significantly more relevant documents were close to each other. A browser plug-in or other tool that provides links to pages similar to a given page should increase the ability of web users to find relevant pages via navigation.

#index 987367
#* Fast exact maximum likelihood estimation for mixture of language models
#@ Yi Zhang;Wei Xu
#t 2007
#c 13
#% 342707
#% 397133
#% 766429
#% 879586
#! A common language modeling approach assumes the data D is generated from a mixture of several language models. EM algorithm is usually used to find the maximum likelihood estimation of one unknown mixture component, given the mixture weights and the other language models. In this paper, we provide an efficient algorithm of O(k) complexity to find the exact solution, where k is the number of words occurred at least once in D. Another merit is that the probabilities of many words are exactly zeros, which means that the mixture language model also serves as a feature selection technique.

#index 987368
#* TimedTextRank: adding the temporal dimension to multi-document summarization
#@ Xiaojun Wan
#t 2007
#c 13
#% 832333
#! Graph-ranking based algorithms (e.g. TextRank) have been proposed for multi-document summarization in recent years. However, these algorithms miss an important dimension, the temporal dimension, for summarizing evolving topics. For an evolving topic, recent documents are usually more important than earlier documents because recent documents contain much more novel information than earlier documents and a novelty-oriented summary should be more appropriate to reflect the changing topic. We propose the TimedTextRank algorithm to make use of the temporal information of documents based on the graph-ranking based algorithm. A preliminary study is performed to demonstrate the effectiveness of the proposed TimedTextRank algorithm for dynamic multi-document summarization.

#index 987369
#* Winnowing wheat from the chaff: propagating trust to sift spam from the web
#@ Lan Nie;Baoning Wu;Brian D. Davison
#t 2007
#c 13
#% 754098
#% 912202
#% 956607
#% 1016177
#! The Web today includes many pages intended to deceive search engines, and attain an unwarranted result ranking. Since the links among web pages are used to calculate authority, ranking systems would benefit from knowing which pages contain content to be trusted and which do not. We propose and compare various trust propagation methods to estimate the trustworthiness of each page. We find that a non-trust-preserving propagation method is able to achieve close to a fifty percent improvement over TrustRank in separating spam from non-spam pages.

#index 987370
#* Feature engineering for mobile (SMS) spam filtering
#@ Gordon V. Cormack;José María Gómez Hidalgo;Enrique Puertas Sánz
#t 2007
#c 13
#% 894515
#! Mobile spam in an increasing threat that may be addressed using filtering systems like those employed against email spam. We believe that email filtering techniques require some adaptation to reach good levels of performance on SMS spam, especially regarding message representation. In order to test this assumption, we have performed experiments on SMS filtering using top performing email spam filters on mobile spam messages using a suitable feature representation, with results supporting our hypothesis.

#index 987371
#* Ranking by community relevance
#@ Lan Nie;Brian D. Davison;Baoning Wu
#t 2007
#c 13
#% 348173
#% 766462
#% 879576
#% 1269896
#! A web page may be relevant to multiple topics; even when nominally on a single topic, the page may attract attention (and thus links) from multiple communities. Instead of indiscriminately summing the authority provided by all pages, we decompose a web page into separate subnodes with respect to each community pointing to it. By considering the relevance of these communities, we are able to better model the query-specific reputation for each potential result. We apply a total of 125 queries to the TREC .GOV dataset to demonstrate how the use of community relevance can improve ranking performance.

#index 987372
#* Query suggestion based on user landing pages
#@ Silviu Cucerzan;Ryen W. White
#t 2007
#c 13
#% 310567
#% 348155
#% 590523
#% 765412
#% 869501
#% 1264955
#! This poster investigates a novel query suggestion technique that selects query refinements through a combination of many users' post-query navigation patterns and the query logs of a large search engine. We compare this technique, which uses the queries that retrieve in the top-ranked search results places where searchers end up after post-query browsing (i.e., the landing pages), with an approach based on query refinements from user search sessions extracted from query logs. Our findings demonstrate the effectiveness of using landing pages for the direct generation of query suggestions, as well as the complementary nature of the suggestions it generates with regard to traditional query log based refinement methodologies.

#index 987373
#* Making mind and machine meet: a study of combining cognitive and algorithmic relevance feedback
#@ Chirag Shah;Diane Kelly;Xin Fu
#t 2007
#c 13
#% 92696
#% 720198
#% 742666
#% 879621
#! Using Saracevic's relevance types, we explore approaches to combining algorithm and cognitive relevance in a term relevance feedback scenario. Data collected from 21 users who provided relevance feedback about terms suggested by a system for 50 TREC HARD topics are used. The former type of feedback is considered as cognitive relevance and the latter type is considered as algorithm relevance. We construct retrieval runs using these two types of relevance feedback and experiment with ways of combining them with simple Boolean operators. Results show minimal differences in performance with respect to the different techniques.

#index 987374
#* Using collaborative queries to improve retrieval for difficult topics
#@ Xin Fu;Diane Kelly;Chirag Shah
#t 2007
#c 13
#% 144076
#% 803556
#% 818221
#! We describe a preliminary analysis of queries created by 81 users for 4 topics from the TREC Robust Track. Our goal was to explore the potential benefits of using queries created by multiple users on retrieval performance for difficult topics. We first examine the overlap in users' queries and the overlap in results with respect to different queries for the same topic. We then explore the potential benefits of combining users' queries in various ways. Our results provide some evidence that having access to multiple users' queries can improve retrieval for individual searchers and for difficult topics.

#index 987375
#* Retrieval of discussions from enterprise mailing lists
#@ Maheedhar Kolla;Olga Vechtomova
#t 2007
#c 13
#% 324192
#% 326522
#% 746885
#% 818323
#! Mailing list archives in an enterprise are a valuable source for employees to dig into the past proceedings of the organization that could be relevant to their present task. Going through the proceedings of discussions about certain topics might be cumbersome and regular search techniques might not work in this context due to the genre that the documents belong to. In this paper, we propose methods, based on theory of subjectivity, to retrieve email messages that could contain argumentative discussions about the topic that the user is interested in.

#index 987376
#* Effects of highly agreed documents in relevancy prediction
#@ Andres R. Masegosa;Hideo Joho;Joemon M. Jose
#t 2007
#c 13
#% 44876
#% 262105
#% 397164
#% 799040
#% 805200
#% 850133
#% 879565
#% 893636
#% 1269504
#! Finding significant contextual features is a challenging task in the development of interactive information retrieval (IR) systems. This paper investigated a simple method to facilitate such a task by looking at aggregated relevance judgements of retrieved documents. Our study suggested that the agreement on relevance judgements can indicate the effectiveness of retrieved documents as the source of significant features. The effect of highly agreed documents gives us practical implication for the design of adaptive search models in interactive IR systems.

#index 987377
#* Detecting word substitutions: PMI vs. HMM
#@ Dmitri Roussinov;SzeWang Fong;David Skillicorn
#t 2007
#c 13
#% 816193
#% 939916
#% 1678452
#% 1719428
#! Those who want to conceal the content of their communications can do so by replacing words that might trigger attention. For example, instead of writing "The bomb is in position", a terrorist may chose to write "The flower is in position." The substituted sentence would sound a bit "odd" for a human reader and it has been shown in prior research that such oddity is detectable by text mining approaches. However, the importance of each component in the suggested oddity detection approach has not been thoroughly investigated. Also, the approach has not been compared with such an obvious candidate for the task as Hidden Markov Models (HMM). In this work, we explore further oddity detection algorithms reported earlier, specifically those based on pointwise mutual information (PMI) and Hidden Markov Models (HMM).

#index 987378
#* Workload sampling for enterprise search evaluation
#@ Tom Rowlands;David Hawking;Ramesh Sankaranarayana
#t 2007
#c 13
#% 262097
#% 309093
#% 818222
#% 857180
#% 879565
#% 947825
#! In real world use of test collection methods, it is essential that the query test set be representative of the work load expected in the actual application. Using a random sample of queries from a media company's query log as a 'gold standard' test set we demonstrate that biases in sitemap-derived and top n query sets can lead to significant perturbations in engine rankings and big differences in estimated performance levels.

#index 987379
#* Document layout and color driven image retrieval
#@ Pere Obrador
#t 2007
#c 13
#% 420476
#% 593030
#% 838539
#! This paper presents a contribution to image indexing applied to the document creation task. The presented method ranks a set of photographs based on how well they aesthetically work within a predefined document. Color harmony, document visual balance and image quality are taken into consideration. A user study conducted on people with a range of expertise in document creation helped gather the right visual features to consider by the algorithm. This shows some benefits for the traditional document creation task, as well as for the case of ever-changing web page banner colors and layout.

#index 987380
#* Large-scale cluster-based retrieval experiments on Turkish texts
#@ Ismail Sengor Altingovde;Rifat Ozcan;Huseyin Cagdas Ocalan;Fazli Can;Özgür Ulusoy
#t 2007
#c 13
#% 787544
#% 879654
#% 1663085
#! We present cluster-based retrieval (CBR) experiments on the largest available Turkish document collection. Our experiments evaluate retrieval effectiveness and efficiency on both an automatically generated clustering structure and a manual classification of documents. In particular, we compare CBR effectiveness with full-text search (FS) and evaluate several implementation alternatives for CBR. Our findings reveal that CBR yields comparable effectiveness figures with FS. Furthermore, by using a specifically tailored cluster-skipping inverted index we significantly improve in-memory query processing efficiency of CBR in comparison to other traditional CBR techniques and even FS.

#index 987381
#* Improving active learning recall via disjunctive boolean constraints
#@ Emre Velipasaoglu;Hinrich Schütze;Jan O. Pedersen
#t 2007
#c 13
#% 169717
#% 170649
#% 280817
#% 344447
#% 879626
#% 1289485
#! Active learning efficiently hones in on the decision boundary between relevant and irrelevant documents, but in the process can miss entire clusters of relevant documents, yielding classifiers with low recall. In this paper, we propose a method to increase active learning recall by constraining sampling to a document subset rich in relevant examples.

#index 987382
#* Creativity support: information discovery and exploratory search
#@ Eunyee Koh;Andruid Kerne;Rodney Hill
#t 2007
#c 13
#% 769227
#% 857478
#% 874454
#! We are developing support for creativity in learning through information discovery and exploratory search. Users engage in creative tasks, such as inventing new products and services. The system supports evolving information needs. It gathers and presents relevant information visually using images and text. Users are able to search, browse, and explore results from multiple queries and interact with information elements by manipulating design and expressing interest. A field study was conducted to evaluate the system in an undergraduate class. The results demonstrated the efficacy of our system for developing creative ideas. Exposure to diverse information in visual and interactive forms is shown to support students engaged in invention tasks.

#index 987383
#* MQX: multi-query engine for compressed XML data
#@ Xiaoling Wang;Aoying Zhou;Juzhen He;Wilfred Ng
#t 2007
#c 13
#% 654451
#% 1016140
#% 1016180
#% 1669485

#index 987384
#* ISKODOR: unified user modeling for integrated searching
#@ Melanie Gnasa;Armin B. Cremers;Douglas W. Oard
#t 2007
#c 13
#% 879567
#% 879606
#% 1599303
#! ISKODOR integrates personal collections, peer search, and centralized search services. User modeling in ISKODOR fills three roles: discovery of sites with suitable information stores, context-based query interpretation, and automatic profile-based filtering of new information. Explanation and control are achieved through graphical depiction of sources, explicit feedback regard ingutility, and explicit control over peer association behavior and information sharing.

#index 987385
#* Babel: a machine transliteration workbench
#@ A. Kumaran;Tobias Kellner
#t 2007
#c 13
#% 740915
#% 854584
#% 938673

#index 987386
#* X-Site: a workplace search tool for software engineers
#@ Peter C.K. Yeung;Luanne Freund;Charles L.A. Clarke
#t 2007
#c 13
#% 818258
#% 1392512
#! Professionals in the workplace need high-precision search tools capable of retrieving information that is useful and appropriate to the task at hand. One approach to identifying content, which is not only relevant but also useful, is to make use of the task context of the search. We present X-Site, an enterprise search engine for the software engineering domain that exploits relationships between user's tasks and document genres in the collection to improve retrieval precision.

#index 987387
#* The wild thing goes local
#@ Kenneth Ward Church;Bo Thiesson
#t 2007
#c 13
#% 321455
#% 943837
#% 1265071
#! Suppose you are on a mobile device with no keyboard (e.g., a cell phone) and you want to perform a "near me" search. Where is the nearest pizza? How do you enter queries quickly? T9? The Wild Thing encourages users to enter patterns with implicit and explicit wild cards (regular expressions). The search engine uses Microsoft Local Live logs to find the most likely queries for a particular location. For example, 7#6 is short-hand for the regular expression: /^[PQRS].*[ ][MNO].*/, which matches "post office" in many places (but "Space Needle" in Seattle). Some queries are more local than others. Pizza is likely everywhere, whereas "Boeing Company," is very likely in Seattle and Chicago, moderately likely nearby, and somewhat likely elsewhere. Smoothing is important. Not every query is observed everywhere.

#index 987388
#* DiscoverInfo: a tool for discovering information with relevance and novelty
#@ Chirag Shah;Gary Marchionini
#t 2007
#c 13

#index 987389
#* Radio Oranje: searching the queen's speech(es)
#@ Willemijn Heeren;Laurens van der Werff;Roeland Ordelman;Arjan van Hessen;Franciska de Jong
#t 2007
#c 13
#! The 'Radio Oranje' demonstrator shows an attractive multimedia user experience in the cultural heritage domain based on a collection of mono-media audio documents. It supports online search and browsing of the collection using indexing techniques, specialized content visualizations and a related photo database.

#index 987390
#* Mobile interface of the memoria project
#@ Ricardo Dias;Rui Jesus;Rute Frias;Nuno Correia
#t 2007
#c 13
#% 1696239
#! This project develops tools to manage personal memories that include a multimedia retrieval system and user interfaces for different devices. This paper and demonstration presents the mobile interface which allows browsing, retrieving, and taking pictures that are automatically annotated with GPS data and audio information. The multimedia retrieval system uses multimodal information: visual content, GPS metadata and audio information. The interface was evaluated in a cultural heritage site.

#index 987391
#* A full-text retrieval toolkit for mobile desktop search
#@ Wei Chen;Jiajun Bu;Kangmiao Liu;Chun Chen;Chen Zhang
#t 2007
#c 13

#index 987392
#* EXPOSE: searching the web for expertise
#@ Fabian Kaiser;Holger Schwarz;Mihály Jakob
#t 2007
#c 13

#index 987393
#* Text categorization for streams
#@ D. L. Thomas;W. J. Teahan
#t 2007
#c 13
#% 588022
#% 642987
#% 766522
#% 1577708
#! We describe a novel system for evaluating and performing stream-based text categorization. Stream-based text categorization considers the text being categorized as a stream of symbols, which differs from the traditional feature-based approach which relies on extracting features from the text. The system implements character-based languages models--specifically models based on the PPM text compression scheme--as well as count-based measures such as R-Measure and C-Measure. Use of the system demonstrates that all of these techniques outperform SVM, a feature-based classifier, at stream-related classification tasks such as authorship ascription.

#index 987394
#* Search results using timeline visualizations
#@ Omar Alonso;Michael Gertz;Ricardo Baeza-Yates
#t 2007
#c 13

#index 987395
#* Wikipedia in the pocket: indexing technology for near-duplicate detection and high similarity search
#@ Martin Potthast
#t 2007
#c 13
#% 1719552
#! We develop and implement a new indexing technology which allows us to use complete (and possibly very large) documents as queries, while having a retrieval performance comparable to a standard term query. Our approach aims at retrieval tasks such as near duplicate detection and high similarity search. To demonstrate the performance of our technology we have compiled the search index "Wikipedia in the Pocket", which contains about 2 million English and German Wikipedia articles.1 This index--along with a search interface--fits on a conventional CD (0.7 gigabyte). The ingredients of our indexing technology are similarity hashing and minimal perfect hashing.

#index 987396
#* Nexus: a real time QA system
#@ Kisuh Ahn;Bonnie Webber
#t 2007
#c 13

#index 987397
#* Geographic ranking for a local search engine
#@ Tony Abou-Assaleh;Weizheng Gao
#t 2007
#c 13
#% 869471
#% 869491
#! Traditional ranking schemes of the relevance of a Web page to a user query in a search engine are less appropriate when the search term contains geographic information. Often, geographic entities, such as addresses, city names, and location names, appear only once or twice in a Web page, and are typically not in a heading or larger font. Consequently, an alternative ranking approach to the traditional weighted tf*idf relevance ranking is need. Further, if a Web site contains a geographic entity, it is often the case that its in- and out-neighbours do not refer to the same entity, although they may refer to other geographic entities. We present a local search engine that applies a novel ranking algorithm suitable for ranking Web pages with geographic content. We describe its major components: geographic ranking, focused crawling, geographic extractor, and the related web-sites feature.

#index 987398
#* Focused ranking in a vertical search engine
#@ Philip O'Brien;Tony Abou-Assaleh
#t 2007
#c 13
#% 348173
#! Since the debut of PageRank and HITS, hyperlink-induced Web document ranking has come a long way. The Web has become increasingly vast and topically diverse. Such vastness has led many into the area of topic-sensitive ranking and its variants. We address the high dimensionality of the Web by providing tools for focused search. A focused search engine is one which seeks coverage over a subset of topics of the Web and presents users with relevant search results in a known domain. This demonstration will introduce readers to the GenieKnows.com Vertical Search Engine.

#index 987399
#* A "do-it-yourself" evaluation service for music information retrieval systems
#@ M. Cameron Jones;Mert Bay;J. Stephen Downie;Andreas F. Ehmann
#t 2007
#c 13
#% 818330

#index 987400
#* IR-Toolbox: an experiential learning tool for teaching IR
#@ Efthimis N. Efthimiadis;Nathan G. Freier
#t 2007
#c 13

#index 987401
#* Beyond classical measures: how to evaluate the effectiveness of interactive information retrieval system?
#@ Azzah Al-Maskari
#t 2007
#c 13
#% 133894
#% 879566
#! This research explores the relationship between Information Retrieval (IR) systems' effectiveness and users' performance (accuracy and speed) and their satisfaction with the retrieved results (precision of the results, completeness of the results and overall system success). Previous studies have concluded that improvements in IR systems based on increase in IR effectiveness measures do not reflect on improvement in users' performance. This work aims at exploiting factors that can possibly be considered as confounding variables in Interactive Information Retrieval (IIR) evaluation. In this research, we look at substantive approaches to evaluate IIR systems. We aim to build an interactive evaluation framework that brings together aspects of systems' effectiveness and users' performance and satisfaction. This research also involves developing methods for capturing users' satisfaction with the retrieved results of IR systems, as well as examination how users assess their own performance in task completion. Furthermore, we are also interested in identifying evaluation measures which are used in batch mode (non-interactive experiment), but correlate well in interactive IR system. Thus, by the end of this research, we hope to develop a valid and reliable metrics for IIR evaluation. A first study was set up to explore the relationship between system effectiveness as quantified by traditional measures, such as precision and recall, and users' effectiveness and satisfaction of the results, though this study was limited to few users. The tasks involve finding images for recall-based tasks. It was concluded that no direct relationship between system effectiveness and users' performance. People learn to adapt to a system regardless to its effectiveness. This study recommends that a combination of measures (e.g. system effectiveness, user performance and satisfaction) to be used to evaluate IIR systems. Based on our observation from this study, we found that users' familiarity of the search topic has increased their performance. Thus, we set up a second experiment to investigate how users' satisfaction correlate with some IR effectiveness measures such as precision and the suite of Cumulative Gain measures (CG, DCG, NDCG) in simple web searching tasks. Results from this study have shown that CG and Precision are better than NDCG at reflecting users' satisfaction with the results of an IR system. We have also concluded that users of web search engines, in the context of simple search task, are more concerned with precision than completeness of the search. This stemmed from the stronger correlation between users' satisfaction with the success of overall search and their satisfaction with the accuracy of the search results than with their satisfaction with the completeness of the search. Many scholars such as [1], [2], [3], and [4] have recommended considering perceptions of the users as important as IR effectiveness measures, and both should be interpreted as measures of effectiveness. Therefore, the issue in IIR evaluation should not be focusing on maximizing the retrieval performance, by refining IR techniques alone, but also understanding users' satisfaction, behaviors and information needs. This raises the need for more investigation on measures that translate users' performance and satisfaction as the criterion of a system. Future plans are to incorporate variables domain knowledge, motivation, task complexity and search behaviours on user performance and users evaluation of IR system performance when evaluating interactive IR systems; this is in an attempt to explore the suitability of different measures in IIR evaluation. Thus, the proposed approach adopts a systematic and multidimensional approach to evaluation including not only classical traditional evaluation measures, such as precision and recall, but also interactive non-traditional measures, such as users' characteristics and their satisfaction.

#index 987402
#* People search in the enterprise
#@ Krisztian Balog
#t 2007
#c 13
#% 869649
#% 879570
#% 987261
#% 1275180

#index 987403
#* Efficient integration of proximity for text, semi-structured and graph retrieval
#@ Andreas Broschart
#t 2007
#c 13

#index 987404
#* Attention-based information retrieval
#@ Georg Buscher
#t 2007
#c 13
#% 448826
#% 731615
#! In the proposed PhD thesis, it will be examined how attention data from the user, especially generated by an eye tracker, can be exploited in order to enhance and personalize information retrieval methods.

#index 987405
#* A summarisation logic for structured documents
#@ Jan Frederik Forst
#t 2007
#c 13
#! The logical approach to Information Retrieval tries to model the relevance of a document given a query as the logical implication between documents and queries. In early work, van Rijsbergen states that the retrieval status value of a document given a query is proportional to the degree of implication between a document and a query. Based on this, several probabilistic logics for information retrieval have been conceived, which add an additional layer of abstraction to the information retrieval task: probabilistic models for the retrieval of documents are expressed in those logics, rather than implemented directly. The aim of the research presented here is to develop a logic for the IR task of document summarisation. Such a summarisation logic adds an abstraction layer to the summarisation task, similarly to the way a logic for document retrieval adds a layer to the retrieval task. Probabilistic models for document summarisation will thus be expressed as logical formulae, with the actual implementation hidden by the logical expressions. The "extract-worthyness" of textual components in this logic is measured as the degree of implication from those textual components to their surrounding contexts, providing a measure of how much said components are "about" the context within which they are situated.

#index 987406
#* Information-behaviour modeling with external cues
#@ Michael Huggett
#t 2007
#c 13
#% 18078
#% 25942
#% 65946
#! Much of human activity defines an information context. We awaken, start work, and hold meetings at roughly the same time every day, and retrieve the same information items (day planners, itineraries, schedules, agendas, reports, menus, web pages, etc.) for many of these activities. Information retrieval systems in general lack sensitivity to such recurrent context, requiring users to remember and re-enter search cues for objects regardless of how regularly or consistently the objects are used, and to develop ad-hoc storage strategies. We propose that in addition to semantic cues, information objects should also be indexed by temporal and sensory cues, such as clock time and location, so that objects can be retrieved by external environmental context, in addition to any internal semantic content. Our cue-event-object (CEO) model uses a network representation to associate information objects with the times and conditions (location, weather, etc.) when they are typically used. Users can query the system to review their activities, revealing what they do at particular times, and which information objects tend to be most often used and when. The system can also pre-fetch items that have proven useful in past similar situations. The CEO model is incremental, real-time, and dynamic, maintaining an accurate summary even as a user's information behaviour changes over time. Such environmentally-aware systems have applications in personal information management, mobile devices, and smart homes. As a memory prosthesis, the model can support autonomous living for the cognitively impaired. We present a comprehensive research agenda based on some promising preliminary findings.

#index 987407
#* Fuzzy temporal and spatial reasoning for intelligent information retrieval
#@ Steven Schockaert
#t 2007
#c 13
#% 319244
#% 1274843
#% 1788741
#! Temporal and spatial information in text documents is often expressed in a qualitative way. Moreover, both are frequently affected by vagueness, calling for appropriate extensions of traditional frameworks for qualitative reasoning about time and space. Our research aims at defining such extensions based on fuzzy set theory, and applying the resulting frameworks to two important kinds of intelligent information retrieval, viz. temporal question answering and geographic information retrieval.

#index 987408
#* Paragraph retrieval for why-question answering
#@ Suzan Verberne
#t 2007
#c 13
#% 816183
#% 854106

#index 987409
#* Global resources for peer-to-peer text retrieval
#@ Hans Friedrich Witschel
#t 2007
#c 13
#% 836065
#% 1715595
#! The thesis presented in this paper tackles selected issues in unstructured peer-to-peer information retrieval (P2PIR) systems, using world knowledge for solving P2PIR problems. A first part uses so-called reference corpora for estimating global term weights such as IDF instead of sampling them from the distributed collection. A second part of the work will be dedicated to the question of query routing in unstructured P2PIR systems using peer resource descriptions and world knowledge for query expansion.

#index 987410
#* Automatic query-time generation of retrieval expert coefficients for multimedia retrieval
#@ Peter Wilkins
#t 2007
#c 13
#% 340934
#% 340938
#% 389801
#% 784148
#% 903602
#! Content-based Multimedia Information Retrieval can be defined as the task of matching a multi-modal information need against various components of a multimedia corpus and retrieving relevant elements. Generally the matching and retrieval takes place across multiple 'features' which can either be visual or audio, or can be high-level or low-level, and each of which can be seen to be an independent retrieval expert. The task of answering a query can thus be formulated as a data fusion problem. Depending on the query, each expert may perform differently and so retrieval coefficients can be used to weight each expert to increase overall performance. Previous approaches to expert coefficient generation have included query-independent coefficients, identification of query-classes and machine learning methods, to name a few. The approach I propose is different, as it seeks to dynamically create expert coefficients which are query-dependent. This approach is based upon earlier experiments where an initial correlation was observed between the score distribution of a retrieval expert, and its relative performance when compared against other experts for that query. I have created a basic method which leverages these observations to create query-time coefficients which achieve comparable performance to oracle-determined query-independent weights, for the experts and collections used in the aforementioned experiment. Previous research which examinedscore distribution did so with respect to relevance, whereas this work seeks to compare expert scores for a given query to determine relative performance. In my work I aim to explore this correlation by eliminating potential bias from the data collections, the retrieval experts and the queries used in experiments to obtain more robust observations. Using and extending previous investigations into data fusion, I will explore where data fusion succeeds in multimedia retrieval, and where it does not. I then aim to refine and extend my existing techniques for automatic coefficient generation to incorporate the new observations, so as to improve performance. Finally I will combine this approach with existing data fusion methods, such as query-class coefficients, with each approach complimenting the other to achieve further performance improvements.

#index 1074049
#* Delighting Chinese users: the Google China experience
#@ Kai-Fu Lee
#t 2008
#c 13
#! Google entered China market as a late-comer in late-2005, with no local employees, an inadequate product line, and small market share. This talk will discuss Google China's efforts to build up a team, learn about local user needs, apply its global innovation model, and won over users in the past 2.5 years. This talk will cover the results of our user studies, and our key findings about Chinese users for searching and using the Internet. It will also discuss how these findings were applied to our products, and how these products gained traction in the market place. It will also discuss Google's progress in Chinese search relevance, search user experience, and key technology areas where we innovated. This talk will also discuss the process of internationalization - how Google hired locally, and applied its global 20% project approach to encourage truly relevant local innovations. It will discuss several examples of these innovations - from product innovations like the weather map, the input method editor, SMS greetings search, to research innovations like parallel SVM/SVD. Google China's progress dispelled the myth that multinational Internet companies cannot succeed in China. The key ingredients, like in any other success story, are: focus on the customer, embrace the corporate culture, empower local flexibility, and of course, innovate, innovate, innovate.

#index 1074050
#* Guilt by association as a search principle
#@ Limsoon Wong
#t 2008
#c 13
#! The exploitation of fundamental invariants is among the most elegant solutions to many computational problems in a wide variety of domains. One of the more powerful approaches to exploit invariants is the principle of "guilt by association". In particular, the principle of guilt by association is the foundation of remote homolog detection, protein function prediction, disease subtype diagnosis, treatment plan prognosis, and other challenges in computational biology. The principle suggests that two entities are in a specific relationship if they exhibit invariant properties underlying that relationship. For example, a protein is predicted to have a particular biological function if it exhibits the underlying invariant properties of that functional group---viz., guilty by association to other members of that functional group through the shared invariant properties. In my talk, I plan to present several facets of guilt by association in the computational prediction of protein function and draw parallels of these facets in information retrieval. Specifically, I plan to touch on the following facets: (a) the issue of chance associations; (b) novel generalizable forms of association; (c) fusion of multiple heterogeneous sources of evidence; (d) the dichotomy of knowing to a high degree of reliability that two entities are in some relationship and yet not knowing what that relationship is. I hope this talk will be, for the informational retrieval community, a window to the opportunities in computational biology that may benefit from the depth and variety of solutions information retrieval has to offer.

#index 1074051
#* On iterative intelligent medical search
#@ Gang Luo;Chunqiang Tang
#t 2008
#c 13
#% 262096
#% 340948
#% 956635
#! Searching for medical information on the Web has become highly popular, but it remains a challenging task because searchers are often uncertain about their exact medical situations and unfamiliar with medical terminology. To address this challenge, we have built an intelligent medical Web search engine called iMed, which uses medical knowledge and an interactive questionnaire to help searchers form queries. This paper focuses on iMed's iterative search advisor, which integrates medical and linguistic knowledge to help searchers improve search results iteratively. Such an iterative process is common for general Web search, and especially crucial for medical Web search, because searchers often miss desired search results due to their limited medical knowledge and the task's inherent difficulty. iMed's iterative search advisor helps the searcher in several ways. First, relevant symptoms and signs are automatically suggested based on the searcher's description of his situation. Second, instead of taking for granted the searcher's answers to the questions, iMed ranks and recommends alternative answers according to their likelihoods of being the correct answers. Third, related MeSH medical phrases are suggested to help the searcher refine his situation description. We demonstrate the effectiveness of iMed's iterative search advisor by evaluating it using real medical case records and USMLE medical exam questions.

#index 1074052
#* Effective and efficient user interaction for long queries
#@ Giridhar Kumaran;James Allan
#t 2008
#c 13
#% 54435
#% 214709
#% 231567
#% 232719
#% 288541
#% 340901
#% 397161
#% 410276
#% 411762
#% 643001
#% 748499
#% 766497
#% 783506
#% 818260
#% 878912
#% 879583
#% 879613
#% 879622
#% 879671
#% 987321
#% 1126945
#! Handling long queries can involve either pruning the query to retain only the important terms (reduction), or expanding the query to include related concepts (expansion). While automatic techniques to do so exist, roughly 25% performance improvements in terms of MAP have been realized in past work through interactive variants. We show that selectively reducing or expanding a query leads to an average improvement of 51% in MAP over the baseline for standard TREC test collections. We demonstrate how user interaction can be used to achieve this improvement. Most interaction techniques present users with a fixed number of options for all queries. We achieve improvements by interacting less with the user, i.e., we present techniques to identify the optimal number of options to present to users, resulting in an interface with an average of 70% fewer options to consider. Previous algorithms supporting interactive reduction and expansion are exponential in nature. To extend their utility to operational environments, we present techniques to make the complexity of the algorithms polynomial. We finally present an analysis of long queries that continue to exhibit poor performance in spite of our new techniques.

#index 1074053
#* How do users find things with PubMed?: towards automatic utility evaluation with user simulations
#@ Jimmy Lin;Mark D. Smucker
#t 2008
#c 13
#% 118728
#% 157907
#% 169779
#% 214709
#% 232719
#% 309089
#% 325198
#% 340921
#% 342965
#% 375017
#% 578873
#% 752177
#% 818257
#% 822126
#% 872033
#% 879565
#% 879566
#% 879622
#% 946521
#% 967291
#% 1076657
#% 1678005
#! In the context of document retrieval in the biomedical domain, this paper explores the complex relationship between the quality of initial query results and the overall utility of an interactive retrieval system. We demonstrate that a content-similarity browsing tool can compensate for poor retrieval results, and that the relationship between retrieval performance and overall utility is non-linear. Arguments are advanced with user simulations, which characterize the relevance of documents that a user might encounter with different browsing strategies. With broader implications to IR, this work provides a case study of how user simulations can be exploited as a formative tool for automatic utility evaluation. Simulation-based studies provide researchers with an additional evaluation tool to complement interactive and Cranfield-style experiments.

#index 1074054
#* Towards breaking the quality curse.: a web-querying approach to web people search.
#@ Dmitri V. Kalashnikov;Rabia Nuray-Turan;Sharad Mehrotra
#t 2008
#c 13
#% 255137
#% 375017
#% 747890
#% 783704
#% 805885
#% 818275
#% 830526
#% 838408
#% 869502
#% 871766
#% 880399
#% 956570
#% 967274
#% 1016335
#% 1119132
#% 1271267
#% 1271313
#% 1274820
#% 1275012
#% 1408793
#% 1688084
#! Searching for people on the Web is one of the most common query types to the web search engines today. However, when a person name is queried, the returned webpages often contain documents related to several distinct namesakes who have the queried name. The task of disambiguating and finding the webpages related to the specific person of interest is left to the user. Many Web People Search (WePS) approaches have been developed recently that attempt to automate this disambiguation process. Nevertheless, the disambiguation quality of these techniques leaves a major room for improvement. This paper presents a new server-side WePS approach. It is based on collecting co-occurrence information from theWeb and thus it uses theWeb as an external data source. A skyline-based classification technique is developed for classifying the collected co-occurrence information in order to make clustering decisions. The clustering technique is specifically designed to (a) handle the dominance that exists in data and (b) to adapt to a given clustering quality measure. These properties allow the framework to get a major advantage in terms of result quality over all the latest WePS techniques we are aware of, including all the 18 methods covered in the recent WePS competition [2].

#index 1074055
#* An unsupervised framework for extracting and normalizing product attributes from multiple web sites
#@ Tak-Lam Wong;Wai Lam;Tik-Shun Wong
#t 2008
#c 13
#% 431536
#% 464434
#% 480824
#% 729913
#% 770844
#% 788107
#% 867052
#% 912223
#% 915340
#% 989660
#% 989662
#% 1022260
#% 1275209
#! We have developed an unsupervised framework for simultaneously extracting and normalizing attributes of products from multiple Web pages originated from different sites. Our framework is designed based on a probabilistic graphical model that can model the page-independent content information and the page-dependent layout information of the text fragments in Web pages. One characteristic of our framework is that previously unseen attributes can be discovered from the clue contained in the layout format of the text fragments. Our framework tackles both extraction and normalization tasks by jointly considering the relationship between the content and layout information. Dirichlet process prior is employed leading to another advantage that the number of discovered product attributes is unlimited. An unsupervised inference algorithm based on variational method is presented. The semantics of the normalized attributes can be visualized by examining the term weights in the model. Our framework can be applied to a wide range of Web mining applications such as product matching and retrieval. We have conducted extensive experiments from four different domains consisting of over 300 Web pages from over 150 different Web sites, demonstrating the robustness and effectiveness of our framework.

#index 1074056
#* Enhancing web search by promoting multiple search engine use
#@ Ryen W. White;Matthew Richardson;Mikhail Bilenko;Allison P. Heath
#t 2008
#c 13
#% 194246
#% 280041
#% 292151
#% 309095
#% 340921
#% 342680
#% 397161
#% 420508
#% 722312
#% 731040
#% 734050
#% 751800
#% 807394
#% 818267
#% 854636
#% 879567
#% 956495
#% 956541
#% 963898
#% 987209
#% 1055851
#! Any given Web search engine may provide higher quality results than others for certain queries. Therefore, it is in users' best interest to utilize multiple search engines. In this paper, we propose and evaluate a framework that maximizes users' search effective-ness by directing them to the engine that yields the best results for the current query. In contrast to prior work on meta-search, we do not advocate for replacement of multiple engines with an aggregate one, but rather facilitate simultaneous use of individual engines. We describe a machine learning approach to supporting switching between search engines and demonstrate its viability at tolerable interruption levels. Our findings have implications for fluid competition between search engines.

#index 1074057
#* Score standardization for inter-collection comparison of retrieval systems
#@ William Webber;Alistair Moffat;Justin Zobel
#t 2008
#c 13
#% 236052
#% 262102
#% 309093
#% 411762
#% 818205
#% 818222
#% 857180
#% 879561
#% 879566
#% 879630
#% 879632
#% 987209
#% 987238
#% 987252
#% 987263
#% 987321
#% 1019124
#% 1095876
#! The goal of system evaluation in information retrieval has always been to determine which of a set of systems is superior on a given collection. The tool used to determine system ordering is an evaluation metric such as average precision, which computes relative, collection-specific scores. We argue that a broader goal is achievable. In this paper we demonstrate that, by use of standardization, scores can be substantially independent of a particular collection, allowing systems to be compared even when they have been tested on different collections. Compared to current methods, our techniques provide richer information about system performance, improved clarity in outcome reporting, and greater simplicity in reviewing results from disparate sources.

#index 1074058
#* The good and the bad system: does the test collection predict users' effectiveness?
#@ Azzah Al-Maskari;Mark Sanderson;Paul Clough;Eija Airio
#t 2008
#c 13
#% 309089
#% 309093
#% 340921
#% 397208
#% 719991
#% 818221
#% 818257
#% 835027
#% 879566
#% 987263
#! Test collections are extensively used in the evaluation of information retrieval systems. Crucial to their use is the degree to which results from them predict user effectiveness. At first, past studies did not substantiate a relationship between system and user effectiveness; more recently, however, correlations have begun to emerge. The results of this paper strengthen and extend those findings. We introduce a novel methodology for investigating the relationship, which shows great success in establishing a significant correlation between system and user effectiveness. It is shown that users behave differently and discern differences between pairs of systems that have a very small absolute difference in test collection effectiveness. Our results strengthen the use of test collections in IR evaluation, confirming that users' effectiveness can be predicted successfully.

#index 1074059
#* Retrieval sensitivity under training using different measures
#@ Ben He;Craig Macdonald;Iadh Ounis
#t 2008
#c 13
#% 109187
#% 262097
#% 309093
#% 340948
#% 411762
#% 561163
#% 731621
#% 766409
#% 818262
#% 840846
#% 857180
#% 879619
#% 907496
#% 907546
#% 907584
#% 987201
#% 987226
#% 1024555
#% 1392469
#! Various measures, such as binary preference (bpref), inferred average precision (infAP), and binary normalised discounted cumulative gain (nDCG) have been proposed as alternatives to mean average precision (MAP) for being less sensitive to the relevance judgements completeness. As the primary aim of any system building is to train the system to respond to user queries in a more robust and stable manner, in this paper, we investigate the importance of the choice of the evaluation measure for training, under different levels of evaluation incompleteness. We simulate evaluation incompleteness by sampling from the relevance assessments. Through large-scale experiments on two standard TREC test collections, we examine retrieval sensitivity when training - i.e. if a training process, based on any of the four discussed measures has an impact on the final retrieval performance. Experimental results show that training by bpref, infAP and nDCG provides significantly better retrieval performance than training by MAP when relevance judgements completeness is extremely low. When relevance judgements completeness increases, the measures behave more similarly.

#index 1074060
#* Attack resistant collaborative filtering
#@ Bhaskar Mehta;Wolfgang Nejdl
#t 2008
#c 13
#% 220711
#% 754097
#% 783438
#% 836153
#% 848642
#% 879629
#% 881512
#% 936910
#% 1001283
#% 1001290
#% 1269893
#! The widespread deployment of recommender systems has lead to user feedback of varying quality. While some users faithfully express their true opinion, many provide noisy ratings which can be detrimental to the quality of the generated recommendations. The presence of noise can violate modeling assumptions and may thus lead to instabilities in estimation and prediction. Even worse, malicious users can deliberately insert attack profiles in an attempt to bias the recommender system to their benefit. While previous research has attempted to study the robustness of various existing Collaborative Filtering (CF) approaches, this remains an unsolved problem. Approaches such as Neighbor Selection algorithms, Association Rules and Robust Matrix Factorization have produced unsatisfactory results. This work describes a new collaborative algorithm based on SVD which is accurate as well as highly stable to shilling. This algorithm exploits previously established SVD based shilling detection algorithms, and combines it with SVD based-CF. Experimental results show a much diminished effect of all kinds of shilling attacks. This work also offers significant improvement over previous Robust Collaborative Filtering frameworks.

#index 1074061
#* EigenRank: a ranking-oriented approach to collaborative filtering
#@ Nathan N. Liu;Qiang Yang
#t 2008
#c 13
#% 173879
#% 220711
#% 268079
#% 330687
#% 348173
#% 408396
#% 411762
#% 420515
#% 420539
#% 452563
#% 528156
#% 564279
#% 577224
#% 577329
#% 730049
#% 734592
#% 799632
#% 818216
#% 840846
#% 987197
#% 1272396
#% 1650569
#! A recommender system must be able to suggest items that are likely to be preferred by the user. In most systems, the degree of preference is represented by a rating score. Given a database of users' past ratings on a set of items, traditional collaborative filtering algorithms are based on predicting the potential ratings that a user would assign to the unrated items so that they can be ranked by the predicted ratings to produce a list of recommended items. In this paper, we propose a collaborative filtering approach that addresses the item ranking problem directly by modeling user preferences derived from the ratings. We measure the similarity between users based on the correlation between their rankings of the items rather than the rating values and propose new collaborative filtering algorithms for ranking items based on the preferences of similar users. Experimental results on real world movie rating data sets show that the proposed approach outperforms traditional collaborative filtering algorithms significantly on the NDCG measure for evaluating ranked results.

#index 1074062
#* Personalized active learning for collaborative filtering
#@ Abhay S. Harpale;Yiming Yang
#t 2008
#c 13
#% 116165
#% 236729
#% 280817
#% 495929
#% 565531
#% 643007
#% 722797
#% 788069
#% 799753
#% 956521
#! Collaborative Filtering (CF) requires user-rated training examples for statistical inference about the preferences of new users. Active learning strategies identify the most informative set of training examples through minimum interactions with the users. Current active learning approaches in CF make an implicit and unrealistic assumption that a user can provide rating for any queried item. This paper introduces a new approach to the problem which does not make such an assumption. We personalize active learning for the user, and query for only those items which the user can provide rating for. We propose an extended form of Bayesian active learning and use the Aspect Model for CF to illustrate and examine the idea. A comparative evaluation of the new method and a well-established baseline method on benchmark datasets shows statistically significant improvements with our method over the performance of the baseline method that is representative for existing approaches which do not take personalization into account.

#index 1074063
#* A boosting algorithm for learning bipartite ranking functions with partially labeled data
#@ Massih Reza Amini;Tuong Vinh Truong;Cyril Goutte
#t 2008
#c 13
#% 169777
#% 252011
#% 311027
#% 375017
#% 397136
#% 420464
#% 458699
#% 466263
#% 643005
#% 734915
#% 769882
#% 875948
#% 957995
#% 1279293
#% 1378224
#% 1455666
#! This paper presents a boosting based algorithm for learning a bipartite ranking function (BRF) with partially labeled data. Until now different attempts had been made to build a BRF in a transductive setting, in which the test points are given to the methods in advance as unlabeled data. The proposed approach is a semi-supervised inductive ranking algorithm which, as opposed to transductive algorithms, is able to infer an ordering on new examples that were not used for its training. We evaluate our approach using the TREC-9 Ohsumed and the Reuters-21578 data collections, comparing against two semi-supervised classification algorithms for ROCArea (AUC), uninterpolated average precision (AUP), mean precision@50 (TP) and Precision-Recall (PR) curves. In the most interesting cases where there are an unbalanced number of irrelevant examples over relevant ones, we show our method to produce statistically significant improvements with respect to these ranking measures.

#index 1074064
#* Directly optimizing evaluation measures in learning to rank
#@ Jun Xu;Tie-Yan Liu;Min Lu;Hang Li;Wei-Ying Ma
#t 2008
#c 13
#% 235377
#% 252473
#% 262096
#% 309095
#% 340899
#% 387427
#% 577224
#% 734915
#% 803033
#% 823360
#% 829043
#% 840846
#% 879588
#% 948863
#% 983820
#% 987226
#% 987227
#% 987240
#% 987241
#% 987242
#% 1035577
#% 1674802
#! One of the central issues in learning to rank for information retrieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures used in information retrieval such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG). Several such algorithms including SVMmap and AdaRank have been proposed and their effectiveness has been verified. However, the relationships between the algorithms are not clear, and furthermore no comparisons have been conducted between them. In this paper, we conduct a study on the approach of directly optimizing evaluation measures in learning to rank for Information Retrieval (IR). We focus on the methods that minimize loss functions upper bounding the basic loss function defined on the IR measures. We first provide a general framework for the study and analyze the existing algorithms of SVMmap and AdaRank within the framework. The framework is based on upper bound analysis and two types of upper bounds are discussed. Moreover, we show that we can derive new algorithms on the basis of this analysis and create one example algorithm called PermuRank. We have also conducted comparisons between SVMmap, AdaRank, PermuRank, and conventional methods of Ranking SVM and RankBoost, using benchmark datasets. Experimental results show that the methods based on direct optimization of evaluation measures can always outperform conventional methods of Ranking SVM and RankBoost. However, no significant difference exists among the performances of the direct optimization methods themselves.

#index 1074065
#* Query dependent ranking using K-nearest neighbor
#@ Xiubo Geng;Tie-Yan Liu;Tao Qin;Andrew Arnold;Hang Li;Heung-Yeung Shum
#t 2008
#c 13
#% 262096
#% 269217
#% 288541
#% 340899
#% 387427
#% 411762
#% 577224
#% 642982
#% 734915
#% 754059
#% 766414
#% 805878
#% 818281
#% 829975
#% 840583
#% 840846
#% 869534
#% 879581
#% 983820
#% 987226
#% 987241
#% 987326
#% 1685165
#% 1705503
#! Many ranking models have been proposed in information retrieval, and recently machine learning techniques have also been applied to ranking model construction. Most of the existing methods do not take into consideration the fact that significant differences exist between queries, and only resort to a single function in ranking of documents. In this paper, we argue that it is necessary to employ different ranking models for different queries and onduct what we call query-dependent ranking. As the first such attempt, we propose a K-Nearest Neighbor (KNN) method for query-dependent ranking. We first consider an online method which creates a ranking model for a given query by using the labeled neighbors of the query in the query feature space and then rank the documents with respect to the query using the created model. Next, we give two offline approximations of the method, which create the ranking models in advance to enhance the efficiency of ranking. And we prove a theory which indicates that the approximations are accurate in terms of difference in loss of prediction, if the learning algorithm used is stable with respect to minor changes in training examples. Our experimental results show that the proposed online and offline methods both outperform the baseline method of using a single ranking function.

#index 1074066
#* Asymmetric distance estimation with sketches for similarity search in high-dimensional spaces
#@ Wei Dong;Moses Charikar;Kai Li
#t 2008
#c 13
#% 205305
#% 214073
#% 249321
#% 347225
#% 479649
#% 479973
#% 635689
#% 741122
#% 741169
#% 762054
#% 783500
#% 784995
#% 903626
#% 967009
#! Efficient similarity search in high-dimensional spaces is important to content-based retrieval systems. Recent studies have shown that sketches can effectively approximate L1 distance in high-dimensional spaces, and that filtering with sketches can speed up similarity search by an order of magnitude. It is a challenge to further reduce the size of sketches, which are already compact, without compromising accuracy of distance estimation. This paper presents an efficient sketch algorithm for similarity search with L2 distances and a novel asymmetric distance estimation technique. Our new asymmetric estimator takes advantage of the original feature vector of the query to boost the distance estimation accuracy. We also apply this asymmetric method to existing sketches for cosine similarity and L1 distance. Evaluations with datasets extracted from images and telephone records show that our L2 sketch outperforms existing methods, and the asymmetric estimators consistently improve the accuracy of different sketch methods. To achieve the same search quality, asymmetric estimators can reduce the sketch size by 10% to 40%.

#index 1074068
#* Reorganizing compressed text
#@ Nieves R. Brisaboa;Antonio Fariña;Susana Ladra;Gonzalo Navarro
#t 2008
#c 13
#% 57849
#% 249989
#% 262101
#% 311799
#% 320454
#% 387427
#% 420492
#% 453572
#% 516361
#% 931332
#% 936965
#% 954624
#% 987214
#% 1404881
#% 1739409

#index 1074069
#* User adaptation: good results from poor systems
#@ Catherine L. Smith;Paul B. Kantor
#t 2008
#c 13
#% 262075
#% 340921
#% 818257
#% 859457
#% 879566
#% 966984
#! Several recent studies have found only a weak relationship between the performance of a retrieval system and the "success" achievable by human searchers. We hypothesize that searchers are successful precisely because they alter their behavior. To explore the possible causal relation between system performance and search behavior, we control system performance, hoping to elicit adaptive search behaviors. 36 subjects each completed 12 searches using either a standard system or one of two degraded systems. Using a general linear model, we isolate the main effect of system performance, by measuring and removing main effects due to searcher variation, topic difficulty, and the position of each search in the time series. We find that searchers using our degraded systems are as successful as those using the standard system, but that, in achieving this success, they alter their behavior in ways that could be measured, in real time, by a suitably instrumented system. Our findings suggest, quite generally, that some aspects of behavioral dynamics may provide unobtrusive indicators of system performance.

#index 1074070
#* Exploring folksonomy for personalized search
#@ Shengliang Xu;Shenghua Bao;Ben Fei;Zhong Su;Yong Yu
#t 2008
#c 13
#% 220710
#% 330769
#% 348173
#% 399057
#% 750863
#% 805877
#% 818224
#% 818259
#% 855601
#% 860021
#% 869504
#% 869536
#% 869548
#% 919706
#% 956544
#% 956552
#% 956579
#% 987193
#% 987313
#% 1053979
#% 1409929
#% 1655418
#! As a social service in Web 2.0, folksonomy provides the users the ability to save and organize their bookmarks online with "social annotations" or "tags". Social annotations are high quality descriptors of the web pages' topics as well as good indicators of web users' interests. We propose a personalized search framework to utilize folksonomy for personalized search. Specifically, three properties of folksonomy, namely the categorization, keyword, and structure property, are explored. In the framework, the rank of a web page is decided not only by the term matching between the query and the web page's content but also by the topic matching between the user's interests and the web page's topics. In the evaluation, we propose an automatic evaluation framework based on folksonomy data, which is able to help lighten the common high cost in personalized search evaluations. A series of experiments are conducted using two heterogeneous data sets, one crawled from Del.icio.us and the other from Dogear. Extensive experimental results show that our personalized search approach can significantly improve the search quality.

#index 1074071
#* To personalize or not to personalize: modeling queries with variation in user intent
#@ Jaime Teevan;Susan T. Dumais;Daniel J. Liebling
#t 2008
#c 13
#% 232684
#% 290830
#% 309095
#% 348173
#% 397161
#% 754059
#% 805878
#% 818221
#% 818224
#% 818259
#% 838547
#% 879613
#% 933280
#% 954949
#% 956495
#% 956541
#% 956552
#% 956632
#% 987260
#% 1015626
#! In most previous work on personalized search algorithms, the results for all queries are personalized in the same manner. However, as we show in this paper, there is a lot of variation across queries in the benefits that can be achieved through personalization. For some queries, everyone who issues the query is looking for the same thing. For other queries, different people want very different results even though they express their need in the same way. We examine variability in user intent using both explicit relevance judgments and large-scale log analysis of user behavior patterns. While variation in user behavior is correlated with variation in explicit relevance judgments the same query, there are many other factors, such as result entropy, result quality, and task that can also affect the variation in behavior. We characterize queries using a variety of features of the query, the results returned for the query, and people's interaction history with the query. Using these features we build predictive models to identify queries that can benefit from personalization.

#index 1074072
#* The opposite of smoothing: a language model approach to ranking query-specific document clusters
#@ Oren Kurland
#t 2008
#c 13
#% 218978
#% 218992
#% 228105
#% 248226
#% 262045
#% 262096
#% 268079
#% 337521
#% 340899
#% 340901
#% 340948
#% 342660
#% 375017
#% 413594
#% 427921
#% 719598
#% 766430
#% 766431
#% 818241
#% 838528
#% 878454
#% 879575
#% 879584
#% 879587
#% 879676
#% 952491
#% 989620
#% 995518
#% 1074119
#% 1682423
#! Exploiting information induced from (query-specific) clustering of top-retrieved documents has long been proposed as means for improving precision at the very top ranks of the returned results. We present a novel language model approach to ranking query-specific clusters by the presumed percentage of relevant documents that they contain. While most previous cluster ranking approaches focus on the cluster as a whole, our model also exploits information induced from documents associated with the cluster. Our model substantially outperforms previous approaches for identifying clusters containing a high relevant-document percentage. Furthermore, using the model to produce document ranking yields precision-at-top-ranks performance that is consistently better than that of the initial ranking upon which clustering is performed; the performance also favorably compares with that of a state-of-the-art pseudo-feedback retrieval method.

#index 1074073
#* Enhancing text clustering by leveraging Wikipedia semantics
#@ Jian Hu;Lujun Fang;Yang Cao;Hua-Jun Zeng;Hua Li;Qiang Yang;Zheng Chen
#t 2008
#c 13
#% 169777
#% 198058
#% 280817
#% 344447
#% 458379
#% 577355
#% 757306
#% 961685
#% 1117027
#% 1250362
#% 1250381
#% 1269899
#% 1275012
#% 1289518
#! Most traditional text clustering methods are based on "bag of words" (BOW) representation based on frequency statistics in a set of documents. BOW, however, ignores the important information on the semantic relationships between key terms. To overcome this problem, several methods have been proposed to enrich text representation with external resource in the past, such as WordNet. However, many of these approaches suffer from some limitations: 1) WordNet has limited coverage and has a lack of effective word-sense disambiguation ability; 2) Most of the text representation enrichment strategies, which append or replace document terms with their hypernym and synonym, are overly simple. In this paper, to overcome these deficiencies, we first propose a way to build a concept thesaurus based on the semantic relations (synonym, hypernym, and associative relation) extracted from Wikipedia. Then, we develop a unified framework to leverage these semantic relations in order to enhance traditional content similarity measure for text clustering. The experimental results on Reuters and OHSUMED datasets show that with the help of Wikipedia thesaurus, the clustering performance of our method is improved as compared to previous methods. In addition, with the optimized weights for hypernym, synonym, and associative concepts that are tuned with the help of a few labeled data users provided, the clustering performance can be further improved.

#index 1074074
#* Knowledge transformation from word space to document space
#@ Tao Li;Chris Ding;Yi Zhang;Bo Shao
#t 2008
#c 13
#% 36672
#% 280819
#% 309128
#% 342621
#% 342659
#% 375388
#% 387427
#% 464291
#% 722902
#% 729918
#% 769881
#% 770782
#% 770830
#% 823343
#% 881468
#% 881487
#% 915294
#% 1117063
#! In most IR clustering problems, we directly cluster the documents, working in the document space, using cosine similarity between documents as the similarity measure. In many real-world applications, however, we usually have knowledge on the word side and wish to transform this knowledge to the document (concept) side. In this paper, we provide a mechanism for this knowledge transformation. To the best of our knowledge, this is the first model for such type of knowledge transformation. This model uses a nonnegative matrix factorization model X = FSGT, where X is the word document semantic matrix, F is the posterior probability of a word belonging to a word cluster and represents knowledge in the word space, G is the posterior probability of a document belonging to a document cluster and represents knowledge in the document space, and S is a scaled matrix factor which provides a condensed view of X. We show how knowledge on words can improve document clustering, i.e, knowledge in the word space is transformed into the document space. We perform extensive experiments to validate our approach.

#index 1074075
#* A study of learning a merge model for multilingual information retrieval
#@ Ming-Feng Tsai;Yu-Ting Wang;Hsin-Hsi Chen
#t 2008
#c 13
#% 169781
#% 309133
#% 456926
#% 570318
#% 577224
#% 722312
#% 734915
#% 818212
#% 840846
#% 852323
#% 983820
#% 987240
#% 1674907
#! This paper proposes a learning approach for the merging process in multilingual information retrieval (MLIR). To conduct the learning approach, we also present a large number of features that may influence the MLIR merging process; these features are mainly extracted from three levels: query, document, and translation. After the feature extraction, we then use the FRank ranking algorithm to construct a merge model; to our knowledge, this practice is the first attempt to use a learning-based ranking algorithm to construct a merge model for MLIR merging. In our experiments, three test collections for the task of crosslingual information retrieval (CLIR) in NTCIR3, 4, and 5 are employed to assess the performance of our proposed method; moreover, several merging methods are also carried out for a comparison, including traditional merging methods, the 2-step merging strategy, and the merging method based on logistic regression. The experimental results show that our method can significantly improve merging quality on two different types of datasets. In addition to the effectiveness, through the merge model generated by FRank, our method can further identify key factors that influence the merging process; this information might provide us more insight and understanding into MLIR merging.

#index 1074076
#* Bilingual topic aspect classification with a few training examples
#@ Yejun Wu;Douglas W. Oard
#t 2008
#c 13
#% 99690
#% 279755
#% 280817
#% 309100
#% 316901
#% 324129
#% 340942
#% 344447
#% 387427
#% 465754
#% 704026
#% 741058
#% 807750
#% 817596
#% 818313
#% 832331
#% 879590
#% 879683
#% 939570
#% 987270
#! This paper explores topic aspect (i.e., subtopic or facet) classification for English and Chinese collections. The evaluation model assumes a bilingual user who has found documents on a topic and identified a few passages in each language on aspects of that topic. Additional passages are then automatically labeled using a k-Nearest-Neighbor classifier and local (i.e., result set) Latent Semantic Analysis. Experiments show that when few training examples are available in either language, classification using training examples from both languages can often achieve higher effectiveness than using training examples from just one language. When the total number of training examples is held constant, classification effectiveness correlates positively with the fraction of same-language training examples in the training set. These results suggest that supervised classification can benefit from hand-annotating a few same-language examples, and that when performing classification in bilingual collections it is useful to label some examples in each language.

#index 1074077
#* Crosslingual location search
#@ Tanuja Joshi;Joseph Joy;Tobias Kellner;Udayan Khurana;A Kumaran;Vibhuti Sengar
#t 2008
#c 13
#% 319508
#% 356410
#% 654467
#% 737420
#% 741114
#% 818244
#% 854584
#% 987295
#% 1035399
#% 1272124
#! Address geocoding, the process of finding the map location for a structured postal address, is a relatively well-studied problem. In this paper we consider the more general problem of crosslingual location search, where the queries are not limited to postal addresses, and the language and script used in the search query is different from the one in which the underlying data is stored. To the best of our knowledge, our system is the first crosslingual location search system that is able to geocode complex addresses. We use a statistical machine transliteration system to convert location names from the script of the query to that of the stored data. However, we show that it is not sufficient to simply feed the resulting transliterations into a monolingual geocoding system, as the ambiguity inherent in the conversion drastically expands the location search space and significantly lowers the quality of results. The strength of our approach lies in its integrated, end-to-end nature: we use abstraction and fuzzy search (in the text domain) to achieve maximum coverage despite transliteration ambiguities, while applying spatial constraints (in the geographic domain) to focus only on viable interpretations of the query. Our experiments with structured and unstructured queries in a set of diverse languages and scripts (Arabic, English, Hindi and Japanese) searching for locations in different regions of the world, show full crosslingual location search accuracy at levels comparable to that of commercial monolingual systems. We achieve these levels of performance using techniques that may be applied to crosslingual searches in any language/script, and over arbitrary spatial data.

#index 1074078
#* A study of methods for negative relevance feedback
#@ Xuanhui Wang;Hui Fang;ChengXiang Zhai
#t 2008
#c 13
#% 218978
#% 218992
#% 223810
#% 232646
#% 289079
#% 309088
#% 340899
#% 340948
#% 342707
#% 766525
#% 790839
#% 818207
#% 818267
#% 879613
#% 1019183
#! Negative relevance feedback is a special case of relevance feedback where we do not have any positive example; this often happens when the topic is difficult and the search results are poor. Although in principle any standard relevance feedback technique can be applied to negative relevance feedback, it may not perform well due to the lack of positive examples. In this paper, we conduct a systematic study of methods for negative relevance feedback. We compare a set of representative negative feedback methods, covering vector-space models and language models, as well as several special heuristics for negative feedback. Evaluating negative feedback methods requires a test set with sufficient difficult topics, but there are not many naturally difficult topics in the existing test collections. We use two sampling strategies to adapt a test collection with easy topics to evaluate negative feedback. Experiment results on several TREC collections show that language model based negative feedback methods are generally more effective than those based on vector-space models, and using multiple negative models is an effective heuristic for negative feedback. Our results also show that it is feasible to adapt test collections with easy topics for evaluating negative feedback methods through sampling.

#index 1074079
#* A bayesian logistic regression model for active relevance feedback
#@ Zuobing Xu;Ram Akella
#t 2008
#c 13
#% 116165
#% 118726
#% 236729
#% 342707
#% 384911
#% 464268
#% 466580
#% 466887
#% 735358
#% 818209
#% 869526
#% 875997
#% 879626
#% 1392451
#! Relevance feedback, which traditionally uses the terms in the relevant documents to enrich the user's initial query, is an effective method for improving retrieval performance. The traditional relevance feedback algorithms lead to overfitting because of the limited amount of training data and large term space. This paper introduces an online Bayesian logistic regression algorithm to incorporate relevance feedback information. The new approach addresses the overfitting problem by projecting the original feature space onto a more compact set which retains the necessary information. The new set of features consist of the original retrieval score, the distance to the relevant documents and the distance to non-relevant documents. To reduce the human evaluation effort in ascertaining relevance, we introduce a new active learning algorithm based on variance reduction to actively select documents for user evaluation. The new active learning algorithm aims to select feedback documents to reduce the model variance. The variance reduction approach leads to capturing relevance, diversity and uncertainty of the unlabeled documents in a principled manner. These are the critical factors of active learning indicated in previous literature. Experiments with several TREC datasets demonstrate the effectiveness of the proposed approach.

#index 1074080
#* A cluster-based resampling method for pseudo-relevance feedback
#@ Kyung Soon Lee;W. Bruce Croft;James Allan
#t 2008
#c 13
#% 73372
#% 81478
#% 218978
#% 262096
#% 289079
#% 329698
#% 340901
#% 743634
#% 750863
#% 766430
#% 766431
#% 783514
#% 818204
#% 818266
#% 838528
#% 843728
#% 879575
#% 879584
#% 879585
#% 907557
#% 987230
#% 987231
#! Typical pseudo-relevance feedback methods assume the top-retrieved documents are relevant and use these pseudo-relevant documents to expand terms. The initial retrieval set can, however, contain a great deal of noise. In this paper, we present a cluster-based resampling method to select better pseudo-relevant documents based on the relevance model. The main idea is to use document clusters to find dominant documents for the initial retrieval set, and to repeatedly feed the documents to emphasize the core topics of a query. Experimental results on large-scale web TREC collections show significant improvements over the relevance model. For justification of the resampling approach, we examine relevance density of feedback documents. A higher relevance density will result in greater retrieval accuracy, ultimately approaching true relevance feedback. The resampling approach shows higher relevance density than the baseline relevance model on all collections, resulting in better retrieval accuracy in pseudo-relevance feedback. This result indicates that the proposed method is effective for pseudo-relevance feedback.

#index 1074081
#* Selecting good expansion terms for pseudo-relevance feedback
#@ Guihong Cao;Jian-Yun Nie;Jianfeng Gao;Stephen Robertson
#t 2008
#c 13
#% 92696
#% 218978
#% 340901
#% 340948
#% 342707
#% 458379
#% 818239
#% 879585
#% 891559
#% 987194
#% 987229
#% 987231
#! Pseudo-relevance feedback assumes that most frequent terms in the pseudo-feedback documents are useful for the retrieval. In this study, we re-examine this assumption and show that it does not hold in reality - many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. We also show that good expansion terms cannot be distinguished from bad ones merely on their distributions in the feedback documents and in the whole collection. We then propose to integrate a term classification process to predict the usefulness of expansion terms. Multiple additional features can be integrated in this process. Our experiments on three TREC collections show that retrieval effectiveness can be much improved when term classification is used. In addition, we also demonstrate that good terms should be identified directly according to their possible impact on the retrieval effectiveness, i.e. using supervised learning, instead of unsupervised learning.

#index 1074082
#* Learning to rank with partially-labeled data
#@ Kevin Duh;Katrin Kirchhoff
#t 2008
#c 13
#% 218978
#% 232703
#% 266426
#% 290830
#% 302391
#% 309095
#% 340948
#% 464615
#% 577224
#% 734915
#% 743284
#% 780688
#% 840846
#% 875948
#% 916788
#% 983899
#% 987226
#% 987228
#% 987240
#% 987241
#% 987243
#! Ranking algorithms, whose goal is to appropriately order a set of objects/documents, are an important component of information retrieval systems. Previous work on ranking algorithms has focused on cases where only labeled data is available for training (i.e. supervised learning). In this paper, we consider the question whether unlabeled (test) data can be exploited to improve ranking performance. We present a framework for transductive learning of ranking functions and show that the answer is affirmative. Our framework is based on generating better features from the test data (via KernelPCA) and incorporating such features via Boosting, thus learning different ranking functions adapted to the individual test queries. We evaluate this method on the LETOR (TREC, OHSUMED) dataset and demonstrate significant improvements.

#index 1074083
#* Learning to rank with SoftRank and Gaussian processes
#@ John Guiver;Edward Snelson
#t 2008
#c 13
#% 309095
#% 360691
#% 577224
#% 783474
#% 829028
#% 840846
#% 891549
#% 916792
#% 976949
#% 1035577
#! In this paper we address the issue of learning to rank for document retrieval using Thurstonian models based on sparse Gaussian processes. Thurstonian models represent each document for a given query as a probability distribution in a score space; these distributions over scores naturally give rise to distributions over document rankings. However, in general we do not have observed rankings with which to train the model; instead, each document in the training set is judged to have a particular relevance level: for example "Bad", "Fair", "Good", or "Excellent". The performance of the model is then evaluated using information retrieval (IR) metrics such as Normalised Discounted Cumulative Gain (NDCG). Recently Taylor et al. presented a method called SoftRank which allows the direct gradient optimisation of a smoothed version of NDCG using a Thurstonian model. In this approach, document scores are represented by the outputs of a neural network, and score distributions are created artificially by adding random noise to the scores. The SoftRank mechanism is a general one; it can be applied to different IR metrics, and make use of different underlying models. In this paper we extend the SoftRank framework to make use of the score uncertainties which are naturally provided by a Gaussian process (GP), which is a probabilistic non-linear regression model. We further develop the model by using sparse Gaussian process techniques, which give improved performance and efficiency, and show competitive results against baseline methods when tested on the publicly available LETOR OHSUMED data set. We also explore how the available uncertainty information can be used in prediction and how it affects model performance.

#index 1074084
#* Learning to rank at query-time using association rules
#@ Adriano A. Veloso;Humberto M. Almeida;Marcos A. Gonçalves;Wagner Meira Jr.
#t 2008
#c 13
#% 124073
#% 152934
#% 253191
#% 320326
#% 376266
#% 387427
#% 397142
#% 577224
#% 734915
#% 740768
#% 766414
#% 803033
#% 818239
#% 840846
#% 879588
#% 983820
#% 987170
#% 987226
#% 987227
#% 987240
#% 987241
#% 987242
#! Some applications have to present their results in the form of ranked lists. This is the case of many information retrieval applications, in which documents must be sorted according to their relevance to a given query. This has led the interest of the information retrieval community in methods that automatically learn effective ranking functions. In this paper we propose a novel method which uncovers patterns (or rules) in the training data associating features of the document with its relevance to the query, and then uses the discovered rules to rank documents. To address typical problems that are inherent to the utilization of association rules (such as missing rules and rule explosion), the proposed method generates rules on a demand-driven basis, at query-time. The result is an extremely fast and effective ranking method. We conducted a systematic evaluation of the proposed method using the LETOR benchmark collections. We show that generating rules on a demand-driven basis can boost ranking performance, providing gains ranging from 12% to 123%, outperforming the state-of-the-art methods that learn to rank, with no need of time-consuming and laborious pre-processing. As a highlight, we also show that additional information, such as query terms, can make the generated rules more discriminative, further improving ranking performance.

#index 1074085
#* Learning to rank with ties
#@ Ke Zhou;Gui-Rong Xue;Hongyuan Zha;Yong Yu
#t 2008
#c 13
#% 287253
#% 321635
#% 340899
#% 387427
#% 411762
#% 448194
#% 564279
#% 577224
#% 818221
#% 840846
#% 840882
#% 879588
#% 983820
#% 983825
#% 987226
#% 987228
#% 987240
#% 987241
#% 989628
#! Designing effective ranking functions is a core problem for information retrieval and Web search since the ranking functions directly impact the relevance of the search results. The problem has been the focus of much of the research at the intersection of Web search and machine learning, and learning ranking functions from preference data in particular has recently attracted much interest. The objective of this paper is to empirically examine several objective functions that can be used for learning ranking functions from preference data. Specifically, we investigate the roles of ties in the learning process. By ties, we mean preference judgments that two documents have equal degree of relevance with respect to a query. This type of data has largely been ignored or not properly modeled in the past. In this paper, we analyze the properties of ties and develop novel learning frameworks which combine ties and preference data using statistical paired comparison models to improve the performance of learned ranking functions. The resulting optimization problems explicitly incorporating ties and preference data are solved using gradient boosting methods. Experimental studies are conducted using three publicly available data sets which demonstrate the effectiveness of the proposed new methods.

#index 1074086
#* Query-sensitive mutual reinforcement chain and its application in query-oriented multi-document summarization
#@ Furu Wei;Wenjie Li;Qin Lu;Yanxiang He
#t 2008
#c 13
#% 268079
#% 282905
#% 387791
#% 397137
#% 641979
#% 755863
#% 787502
#% 816173
#% 826267
#% 869500
#% 915338
#% 938761
#% 939968
#% 943826
#% 956570
#% 975019
#% 992302
#% 992305
#% 1019065
#% 1272053
#! Sentence ranking is the issue of most concern in document summarization. Early researchers have presented the mutual reinforcement principle (MR) between sentence and term for simultaneous key phrase and salient sentence extraction in generic single-document summarization. In this work, we extend the MR to the mutual reinforcement chain (MRC) of three different text granularities, i.e., document, sentence and terms. The aim is to provide a general reinforcement framework and a formal mathematical modeling for the MRC. Going one step further, we incorporate the query influence into the MRC to cope with the need for query-oriented multi-document summarization. While the previous summarization approaches often calculate the similarity regardless of the query, we develop a query-sensitive similarity to measure the affinity between the pair of texts. When evaluated on the DUC 2005 dataset, the experimental results suggest that the proposed query-sensitive MRC (Qs-MRC) is a promising approach for summarization.

#index 1074087
#* Comments-oriented document summarization: understanding documents with readers' feedback
#@ Meishan Hu;Aixin Sun;Ee-Peng Lim
#t 2008
#c 13
#% 268079
#% 280835
#% 280838
#% 309095
#% 316143
#% 787502
#% 816173
#% 818226
#% 855043
#% 881063
#% 910167
#% 956502
#% 956544
#% 987210
#% 1019161
#! Comments left by readers on Web documents contain valuable information that can be utilized in different information retrieval tasks including document search, visualization, and summarization. In this paper, we study the problem of comments-oriented document summarization and aim to summarize a Web document (e.g., a blog post) by considering not only its content, but also the comments left by its readers. We identify three relations (namely, topic, quotation, and mention) by which comments can be linked to one another, and model the relations in three graphs. The importance of each comment is then scored by: (i) graph-based method, where the three graphs are merged into a multi-relation graph; (ii) tensor-based method, where the three graphs are used to construct a 3rd-order tensor. To generate a comments-oriented summary, we extract sentences from the given Web document using either feature-biased approach or uniform-document approach. The former scores sentences to bias keywords derived from comments; while the latter scores sentences uniformly with comments. In our experiments using a set of blog posts with manually labeled sentences, our proposed summarization methods utilizing comments showed significant improvement over those not using comments. The methods using feature-biased sentence extraction approach were observed to outperform that using uniform-document approach.

#index 1074088
#* Multi-document summarization using cluster-based link analysis
#@ Xiaojun Wan;Jianwu Yang
#t 2008
#c 13
#% 280835
#% 283171
#% 290830
#% 296738
#% 318409
#% 387427
#% 397138
#% 428369
#% 786578
#% 787502
#% 811320
#% 815920
#% 816173
#% 817519
#% 818225
#% 818227
#% 818241
#% 818266
#% 832329
#% 879575
#% 939539
#% 1116996
#% 1265053
#! The Markov Random Walk model has been recently exploited for multi-document summarization by making use of the link relationships between sentences in the document set, under the assumption that all the sentences are indistinguishable from each other. However, a given document set usually covers a few topic themes with each theme represented by a cluster of sentences. The topic themes are usually not equally important and the sentences in an important theme cluster are deemed more salient than the sentences in a trivial theme cluster. This paper proposes the Cluster-based Conditional Markov Random Walk Model (ClusterCMRW) and the Cluster-based HITS Model (ClusterHITS) to fully leverage the cluster-level information. Experimental results on the DUC2001 and DUC2002 datasets demonstrate the good effectiveness of our proposed summarization models. The results also demonstrate that the ClusterCMRW model is more robust than the ClusterHITS model, with respect to different cluster numbers.

#index 1074089
#* Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization
#@ Dingding Wang;Tao Li;Shenghuo Zhu;Chris Ding
#t 2008
#c 13
#% 280835
#% 313959
#% 340884
#% 340971
#% 342621
#% 387427
#% 397136
#% 397137
#% 428369
#% 449746
#% 742224
#% 770830
#% 787502
#% 815920
#% 816173
#% 818227
#% 823343
#% 858036
#% 881468
#% 891559
#% 987208
#% 1096117
#% 1275040
#% 1275213
#% 1275220
#! Multi-document summarization aims to create a compressed summary while retaining the main characteristics of the original set of documents. Many approaches use statistics and machine learning techniques to extract sentences from documents. In this paper, we propose a new multi-document summarization framework based on sentence-level semantic analysis and symmetric non-negative matrix factorization. We first calculate sentence-sentence similarities using semantic analysis and construct the similarity matrix. Then symmetric matrix factorization, which has been shown to be equivalent to normalized spectral clustering, is used to group sentences into clusters. Finally, the most informative sentences are selected from each group to form the summary. Experimental results on DUC2005 and DUC2006 data sets demonstrate the improvement of our proposed framework over the implemented existing summarization systems. A further study on the factors that benefit the high performance is also conducted.

#index 1074090
#* Algorithmic mediation for collaborative exploratory search
#@ Jeremy Pickens;Gene Golovchinsky;Chirag Shah;Pernilla Qvarfordt;Maribeth Back
#t 2008
#c 13
#% 184496
#% 240162
#% 240738
#% 246877
#% 340936
#% 643036
#% 905164
#% 987373
#% 998795
#% 1289575
#% 1677696
#% 1677962
#! We describe a new approach to information retrieval: algorithmic mediation for intentional, synchronous collaborative exploratory search. Using our system, two or more users with a common information need search together, simultaneously. The collaborative system provides tools, user interfaces and, most importantly, algorithmically-mediated retrieval to focus, enhance and augment the team's search and communication activities. Collaborative search outperformed post hoc merging of similarly instrumented single user runs. Algorithmic mediation improved both collaborative search (allowing a team of searchers to find relevant information more efficiently and effectively), and exploratory search (allowing the searchers to find relevant information that cannot be found while working individually).

#index 1074091
#* Exploiting correlated keywords to improve approximate information filtering
#@ Christian Zimmer;Christos Tryfonopoulos;Gerhard Weikum
#t 2008
#c 13
#% 2833
#% 152934
#% 194246
#% 282422
#% 297191
#% 311808
#% 322884
#% 340175
#% 344448
#% 479795
#% 516235
#% 519953
#% 555267
#% 765501
#% 818210
#% 824652
#% 824682
#% 838393
#% 907503
#% 960250
#% 987277
#% 1113158
#% 1709393
#% 1914871
#! Information filtering, also referred to as publish/subscribe, complements one-time searching since users are able to subscribe to information sources and be notified whenever new documents of interest are published. In approximate information filtering only selected information sources, that are likely to publish documents relevant to the user interests in the future, are monitored. To achieve this functionality, a subscriber exploits statistical metadata to identify promising publishers and index its continuous query only in those publishers. The statistics are maintained in a directory, usually on a per-keyword basis, thus disregarding possible correlations among keywords. Using this coarse information, poor publisher selection may lead to poor filtering performance and thus loss of interesting documents.1 Based on the above observation, this work extends query routing techniques from the domain of distributed information retrieval in peer-to-peer (P2P) networks, and provides new algorithms for exploiting the correlation among keywords in a filtering setting. We develop and evaluate two algorithms based on single-key and multi-key statistics and utilize two different synopses (Hash Sketches and KMV synopses) to compactly represent publishers. Our experimental evaluation using two real-life corpora with web and blog data demonstrates the filtering effectiveness of both approaches and highlights the different tradeoffs.

#index 1074092
#* A user browsing model to predict search engine click data from past observations.
#@ Georges E. Dupret;Benjamin Piwowarski
#t 2008
#c 13
#% 577224
#% 590523
#% 766472
#% 818221
#% 879565
#% 879567
#% 946521
#% 956495
#% 1035578
#% 1269878
#% 1275193
#% 1682438
#! Search engine click logs provide an invaluable source of relevance information but this information is biased because we ignore which documents from the result list the users have actually seen before and after they clicked. Otherwise, we could estimate document relevance by simple counting. In this paper, we propose a set of assumptions on user browsing behavior that allows the estimation of the probability that a document is seen, thereby providing an unbiased estimate of document relevance. To train, test and compare our model to the best alternatives described in the Literature, we gather a large set of real data and proceed to an extensive cross-validation experiment. Our solution outperforms very significantly all previous models. As a side effect, we gain insight into the browsing behavior of users and we can compare it to the conclusions of an eye-tracking experiments by Joachims et al. [12]. In particular, our findings confirm that a user almost always see the document directly after a clicked document. They also explain why documents situated just after a very relevant document are clicked more often.

#index 1074093
#* Learning query intent from regularized click graphs
#@ Xiao Li;Ye-Yi Wang;Alex Acero
#t 2008
#c 13
#% 252011
#% 262059
#% 310567
#% 330617
#% 748465
#% 748550
#% 785366
#% 805878
#% 844287
#% 879567
#% 879581
#% 961218
#% 987221
#% 987222
#% 987326
#! This work presents the use of click graphs in improving query intent classifiers, which are critical if vertical search and general-purpose search services are to be offered in a unified user interface. Previous works on query classification have primarily focused on improving feature representation of queries, e.g., by augmenting queries with search engine results. In this work, we investigate a completely orthogonal approach --- instead of enriching feature representation, we aim at drastically increasing the amounts of training data by semi-supervised learning with click graphs. Specifically, we infer class memberships of unlabeled queries from those of labeled ones according to their proximities in a click graph. Moreover, we regularize the learning with click graphs by content-based classification to avoid propagating erroneous labels. We demonstrate the effectiveness of our algorithms in two different applications, product intent and job intent classification. In both cases, we expand the training data with automatically labeled queries by over two orders of magnitude, leading to significant improvements in classification performance. An additional finding is that with a large amount of training data obtained in this fashion, classifiers using only query words/phrases as features can work remarkably well.

#index 1074094
#* Retrieval and feedback models for blog feed search
#@ Jonathan L. Elsas;Jaime Arguello;Jamie Callan;Jaime G. Carbonell
#t 2008
#c 13
#% 268079
#% 340901
#% 643012
#% 750863
#% 818262
#% 879584
#% 1074094
#! Blog feed search poses different and interesting challenges from traditional ad hoc document retrieval. The units of retrieval, the blogs, are collections of documents, the blog posts. In this work we adapt a state-of-the-art federated search model to the feed retrieval task, showing a significant improvement over algorithms based on the best performing submissions in the TREC 2007 Blog Distillation task[12]. We also show that typical query expansion techniques such as pseudo-relevance feedback using the blog corpus do not provide any significant performance improvement and in many cases dramatically hurt performance. We perform an in-depth analysis of the behavior of pseudo-relevance feedback for this task and develop a novel query expansion technique using the link structure in Wikipedia. This query expansion technique provides significant and consistent performance improvements for this task, yielding a 22% and 14% improvement in MAP over the unexpanded query for our baseline and federated algorithms respectively.

#index 1074095
#* Learning to reduce the semantic gap in web image retrieval and annotation
#@ Changhu Wang;Lei Zhang;Hong-Jiang Zhang
#t 2008
#c 13
#% 280819
#% 318785
#% 465014
#% 642990
#% 722904
#% 722927
#% 733366
#% 860956
#% 875973
#% 884044
#% 903625
#% 975105
#% 1502531
#! We study in this paper the problem of bridging the semantic gap between low-level image features and high-level semantic concepts, which is the key hindrance in content-based image retrieval. Piloted by the rich textual information of Web images, the proposed framework tries to learn a new distance measure in the visual space, which can be used to retrieve more semantically relevant images for any unseen query image. The framework differentiates with traditional distance metric learning methods in the following ways. 1) A ranking-based distance metric learning method is proposed for image retrieval problem, by optimizing the leave-one-out retrieval performance on the training data. 2) To be scalable, millions of images together with rich textual information have been crawled from the Web to learn the similarity measure, and the learning framework particularly considers the indexing problem to ensure the retrieval efficiency. 3) To alleviate the noises in the unbalanced labels of images and fully utilize the textual information, a Latent Dirichlet Allocation based topic-level text model is introduced to define pairwise semantic similarity between any two images. The learnt distance measure can be directly applied to applications such as content-based image retrieval and search-based image annotation. Experimental results on the two applications in a two million Web image database show both the effectiveness and efficiency of the proposed framework.

#index 1074096
#* A lattice-based approach to query-by-example spoken document retrieval
#@ Tee Kiah Chia;Khe Chai Sim;Haizhou Li;Hwee Tou Ng
#t 2008
#c 13
#% 218984
#% 262096
#% 280851
#% 287253
#% 292340
#% 316901
#% 340899
#% 648114
#% 710751
#% 724582
#% 750863
#% 786587
#% 879571
#% 939386
#% 940043
#% 991016
#% 1016312
#! Recent efforts on the task of spoken document retrieval (SDR) have made use of speech lattices: speech lattices contain information about alternative speech transcription hypotheses other than the 1-best transcripts, and this information can improve retrieval accuracy by overcoming recognition errors present in the 1-best transcription. In this paper, we look at using lattices for the query-by-example spoken document retrieval task - retrieving documents from a speech corpus, where the queries are themselves in the form of complete spoken documents (query exemplars). We extend a previously proposed method for SDR with short queries to the query-by-example task. Specifically, we use a retrieval method based on statistical modeling: we compute expected word counts from document and query lattices, estimate statistical models from these counts, and compute relevance scores as divergences between these models. Experimental results on a speech corpus of conversational English show that the use of statistics from lattices for both documents and query exemplars results in better retrieval accuracy than using only 1-best transcripts for either documents, or queries, or both. In addition, we investigate the effect of stop word removal which further improves retrieval accuracy. To our knowledge, our work is the first to have used a lattice-based approach to query-by-example spoken document retrieval.

#index 1074097
#* A few examples go a long way: constructing query models from elaborate query formulations
#@ Krisztian Balog;Wouter Weerkamp;Maarten de Rijke
#t 2008
#c 13
#% 144029
#% 262096
#% 280850
#% 287253
#% 340899
#% 340901
#% 342707
#% 406493
#% 766429
#% 766497
#% 766525
#% 818204
#% 879585
#% 1019095
#% 1024552
#! We address a specific enterprise document search scenario, where the information need is expressed in an elaborate manner. In our scenario, information needs are expressed using a short query (of a few keywords) together with examples of key reference pages. Given this setup, we investigate how the examples can be utilized to improve the end-to-end performance on the document retrieval task. Our approach is based on a language modeling framework, where the query model is modified to resemble the example pages. We compare several methods for sampling expansion terms from the example pages to support query-dependent and query-independent query expansion; the latter is motivated by the wish to increase "aspect recall", and attempts to uncover aspects of the information need not captured by the query. For evaluation purposes we use the CSIRO data set created for the TREC 2007 Enterprise track. The best performance is achieved by query models based on query-independent sampling of expansion terms from the example documents.

#index 1074098
#* A unified and discriminative model for query refinement
#@ Jiafeng Guo;Gu Xu;Hang Li;Xueqi Cheng
#t 2008
#c 13
#% 73441
#% 115462
#% 144029
#% 218978
#% 232644
#% 310567
#% 411762
#% 438557
#% 464434
#% 643057
#% 754125
#% 770844
#% 829043
#% 840947
#% 869501
#% 939629
#% 939973
#% 987272
#% 1019146
#! This paper addresses the issue of query refinement, which involves reformulating ill-formed search queries in order to enhance relevance of search results. Query refinement typically includes a number of tasks such as spelling error correction, word splitting, word merging, phrase segmentation, word stemming, and acronym expansion. In previous research, such tasks were addressed separately or through employing generative models. This paper proposes employing a unified and discriminative model for query refinement. Specifically, it proposes a Conditional Random Field (CRF) model suitable for the problem, referred to as Conditional Random Field for Query Refinement (CRF-QR). Given a sequence of query words, CRF-QR predicts a sequence of refined query words as well as corresponding refinement operations. In that sense, CRF-QR differs greatly from conventional CRF models. Two types of CRF-QR models, namely a basic model and an extended model are introduced. One merit of employing CRF-QR is that different refinement tasks can be performed simultaneously and thus the accuracy of refinement can be enhanced. Furthermore, the advantages of discriminative models over generative models can be fully leveraged. Experimental results demonstrate that CRF-QR can significantly outperform baseline methods. Furthermore, when CRF-QR is used in web search, a significant improvement of relevance can be obtained.

#index 1074099
#* Query expansion using gaze-based feedback on the subdocument level
#@ Georg Buscher;Andreas Dengel;Ludger van Elst
#t 2008
#c 13
#% 280809
#% 292193
#% 309095
#% 329114
#% 577301
#% 731615
#% 754126
#% 766454
#% 766472
#% 805200
#% 818221
#% 818259
#% 823348
#% 828998
#% 838547
#% 869536
#% 879565
#% 879567
#% 912935
#% 954948
#% 987193
#% 987404
#% 1048693
#% 1677822
#! We examine the effect of incorporating gaze-based attention feedback from the user on personalizing the search process. Employing eye tracking data, we keep track of document parts the user read in some way. We use this information on the subdocument level as implicit feedback for query expansion and reranking. We evaluated three different variants incorporating gaze data on the subdocument level and compared them against a baseline based on context on the document level. Our results show that considering reading behavior as feedback yields powerful improvements of the search result accuracy of ca. 32% in the general case. However, the extent of the improvements varies depending on the internal structure of the viewed documents and the type of the current information need.

#index 1074100
#* Affective feedback: an investigation into the role of emotions in the information seeking process
#@ Ioannis Arapakis;Joemon M. Jose;Philip D. Gray
#t 2008
#c 13
#% 118726
#% 169803
#% 212362
#% 214709
#% 220711
#% 265307
#% 302085
#% 306472
#% 340974
#% 449291
#% 729027
#% 731615
#% 762422
#% 994400
#% 994535
#% 1014987
#% 1014990
#% 1015002
#% 1031049
#% 1742134
#! User feedback is considered to be a critical element in the information seeking process, especially in relation to relevance assessment. Current feedback techniques determine content relevance with respect to the cognitive and situational levels of interaction that occurs between the user and the retrieval system. However, apart from real-life problems and information objects, users interact with intentions, motivations and feelings, which can be seen as critical aspects of cognition and decision-making. The study presented in this paper serves as a starting point to the exploration of the role of emotions in the information seeking process. Results show that the latter not only interweave with different physiological, psychological and cognitive processes, but also form distinctive patterns, according to specific task, and according to specific user.

#index 1074101
#* Optimizing relevance and revenue in ad search: a query substitution approach
#@ Filip Radlinski;Andrei Broder;Peter Ciccolo;Evgeniy Gabrilovich;Vanja Josifovski;Lance Riedel
#t 2008
#c 13
#% 46803
#% 169729
#% 262084
#% 279755
#% 298183
#% 340901
#% 342707
#% 642985
#% 643057
#% 740191
#% 869501
#% 987221
#% 987262
#% 1289518
#% 1712908
#! The primary business model behind Web search is based on textual advertising, where contextually relevant ads are displayed alongside search results. We address the problem of selecting these ads so that they are both relevant to the queries and profitable to the search engine, showing that optimizing ad relevance and revenue is not equivalent. Selecting the best ads that satisfy these constraints also naturally incurs high computational costs, and time constraints can lead to reduced relevance and profitability. We propose a novel two-stage approach, which conducts most of the analysis ahead of time. An offine preprocessing phase leverages additional knowledge that is impractical to use in real time, and rewrites frequent queries in a way that subsequently facilitates fast and accurate online matching. Empirical evaluation shows that our method optimized for relevance matches a state-of-the-art method while improving expected revenue. When optimizing for revenue, we see even more substantial improvements in expected revenue.

#index 1074102
#* A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval
#@ Min Zhang;Xingyao Ye
#t 2008
#c 13
#% 111303
#% 750863
#% 805873
#% 838521
#% 854646
#% 939897
#% 956510
#% 1261565
#! Opinion retrieval is a task of growing interest in social life and academic research, which is to find relevant and opinionate documents according to a user's query. One of the key issues is how to combine a document's opinionate score (the ranking score of to what extent it is subjective or objective) and topic relevance score. Current solutions to document ranking in opinion retrieval are generally ad-hoc linear combination, which is short of theoretical foundation and careful analysis. In this paper, we focus on lexicon-based opinion retrieval. A novel generation model that unifies topic-relevance and opinion generation by a quadratic combination is proposed in this paper. With this model, the relevance-based ranking serves as the weighting factor of the lexicon-based sentiment ranking function, which is essentially different from the popular heuristic linear combination approaches. The effect of different sentiment dictionaries is also discussed. Experimental results on TREC blog datasets show the significant effectiveness of the proposed unified model. Improvements of 28.1% and 40.3% have been obtained in terms of MAP and p@10 respectively. The conclusion is not limited to blog environment. Besides the unified generation model, another contribution is that our work demonstrates that in the opinion retrieval task, a Bayesian approach to combining multiple ranking functions is superior to using a linear combination. It is also applicable to other result re-ranking applications in similar scenario.

#index 1074103
#* Discriminative probabilistic models for passage based retrieval
#@ Mengqiu Wang;Luo Si
#t 2008
#c 13
#% 144011
#% 144012
#% 169809
#% 169811
#% 169813
#% 184491
#% 194298
#% 232677
#% 232703
#% 306510
#% 328532
#% 340901
#% 413592
#% 658625
#% 729437
#% 766414
#% 789959
#% 818222
#% 818239
#% 836002
#! The approach of using passage-level evidence for document retrieval has shown mixed results when it is applied to a variety of test beds with different characteristics. One main reason of the inconsistent performance is that there exists no unified framework to model the evidence of individual passages within a document. This paper proposes two probabilistic models to formally model the evidence of a set of top ranked passages in a document. The first probabilistic model follows the retrieval criterion that a document is relevant if any passage in the document is relevant, and models each passage independently. The second probabilistic model goes a step further and incorporates the similarity correlations among the passages. Both models are trained in a discriminative manner. Furthermore, we present a combination approach to combine the ranked lists of document retrieval and passage-based retrieval. An extensive set of experiments have been conducted on four different TREC test beds to show the effectiveness of the proposed discriminative probabilistic models for passage-based retrieval. The proposed algorithms are compared with a state-of-the-art document retrieval algorithm and a language model approach for passage-based retrieval. Furthermore, our combined approach has been shown to provide better results than both document retrieval and passage-based retrieval approaches.

#index 1074104
#* A new probabilistic retrieval model based on the dirichlet compound multinomial distribution
#@ Zuobing Xu;Ram Akella
#t 2008
#c 13
#% 169781
#% 261550
#% 262096
#% 287253
#% 340899
#% 340901
#% 340948
#% 342707
#% 397129
#% 458369
#% 719598
#% 766412
#% 840903
#% 875981
#% 879585
#! The classical probabilistic models attempt to capture the Ad hoc information retrieval problem within a rigorous probabilistic framework. It has long been recognized that the primary obstacle to effective performance of the probabilistic models is the need to estimate a relevance model. The Dirichlet compound multinomial (DCM) distribution, which relies on hierarchical Bayesian modeling techniques, or the Polya Urn scheme, is a more appropriate generative model than the traditional multinomial distribution for text documents. We explore a new probabilistic model based on the DCM distribution, which enables efficient retrieval and accurate ranking. Because the DCM distribution captures the dependency of repetitive word occurrences, the new probabilistic model is able to model the concavity of the score function more effectively. To avoid the empirical tuning of retrieval parameters, we design several parameter estimation algorithms to automatically set model parameters. Additionally, we propose a pseudo-relevance feedback algorithm based on the latent mixture modeling of the Dirichlet compound multinomial distribution to further improve retrieval accuracy. Finally, our experiments show that both the baseline probabilistic retrieval algorithm based on the DCM distribution and the corresponding pseudo-relevance feedback algorithm outperform the existing language modeling systems on several TREC retrieval tasks.

#index 1074105
#* TF-IDF uncovered: a study of theories and probabilities
#@ Thomas Roelleke;Jun Wang
#t 2008
#c 13
#% 169781
#% 176530
#% 262096
#% 411760
#% 570316
#% 642974
#% 643003
#% 793374
#% 818238
#% 879578
#% 987232
#! Interpretations of TF-IDF are based on binary independence retrieval, Poisson, information theory, and language modelling. This paper contributes a review of existing interpretations, and then, TF-IDF is systematically related to the probabilities P(q|d) and P(d|q). Two approaches are explored: a space of independent, and a space of disjoint terms. For independent terms, an "extreme" query/non-query term assumption uncovers TF-IDF, and an analogy of P(d|q) and the probabilistic odds O(r|d, q) mirrors relevance feedback. For disjoint terms, a relationship between probability theory and TF-IDF is established through the integral + 1/x dx = log x. This study uncovers components such as divergence from randomness and pivoted document length to be inherent parts of a document-query independence (DQI) measure, and interestingly, an integral of the DQI over the term occurrence probability leads to TF-IDF.

#index 1074106
#* Separate and inequal: preserving heterogeneity in topical authority flows
#@ Lan Nie;Brian D. Davison
#t 2008
#c 13
#% 281214
#% 309095
#% 309747
#% 309868
#% 310514
#% 348173
#% 438136
#% 766462
#% 800183
#% 869485
#% 879576
#% 880399
#% 987371
#% 1269896
#! Web pages, like people, are often known by others in a variety of contexts. When those contexts are sufficiently distinct, a page's importance may be better represented by multiple domains of authority, rather than by one that indiscriminately mixes reputations. In this work we determine domains of authority by examining the contexts in which a page is cited. However, we find that it is not enough to determine separate domains of authority; our model additionally determines the local flow of authority based upon the relative similarity of the source and target authority domains. In this way, we differentiate both incoming and outgoing hyperlinks by topicality and importance rather than treating them indiscriminately. We find that this approach compares favorably to other topical ranking methods on two real-world datasets and produces an approximately 10% improvement in precision and quality of the top ten results over PageRank.

#index 1074107
#* BrowseRank: letting web users vote for page importance
#@ Yuting Liu;Bin Gao;Tie-Yan Liu;Ying Zhang;Zhiming Ma;Shuyuan He;Hang Li
#t 2008
#c 13
#% 224113
#% 268079
#% 282905
#% 309095
#% 309151
#% 348173
#% 387427
#% 411762
#% 799632
#% 805895
#% 805897
#% 987212
#% 1016177
#! This paper proposes a new method for computing page importance, referred to as BrowseRank. The conventional approach to compute page importance is to exploit the link graph of the web and to build a model based on that graph. For instance, PageRank is such an algorithm, which employs a discrete-time Markov process as the model. Unfortunately, the link graph might be incomplete and inaccurate with respect to data for determining page importance, because links can be easily added and deleted by web content creators. In this paper, we propose computing page importance by using a 'user browsing graph' created from user behavior data. In this graph, vertices represent pages and directed edges represent transitions between pages in the users' web browsing history. Furthermore, the lengths of staying time spent on the pages by users are also included. The user browsing graph is more reliable than the link graph for inferring page importance. This paper further proposes using the continuous-time Markov process on the user browsing graph as a model and computing the stationary probability distribution of the process as page importance. An efficient algorithm for this computation has also been devised. In this way, we can leverage hundreds of millions of users' implicit voting on page importance. Experimental results show that BrowseRank indeed outperforms the baseline methods such as PageRank and TrustRank in several tasks.

#index 1074108
#* Exploring traversal strategy for web forum crawling
#@ Yida Wang;Jiang-Ming Yang;Wei Lai;Rui Cai;Lei Zhang;Wei-Ying Ma
#t 2008
#c 13
#% 255137
#% 268079
#% 281251
#% 340924
#% 480479
#% 577330
#% 805879
#% 807302
#% 879600
#% 879601
#% 956507
#% 961599
#% 1055716
#! In this paper, we study the problem of Web forum crawling. Web forum has now become an important data source of many Web applications; while forum crawling is still a challenging task due to complex in-site link structures and login controls of most forum sites. Without carefully selecting the traversal path, a generic crawler usually downloads many duplicate and invalid pages from forums, and thus wastes both the precious bandwidth and the limited storage space. To crawl forum data more effectively and efficiently, in this paper, we propose an automatic approach to exploring an appropriate traversal strategy to direct the crawling of a given target forum. In detail, the traversal strategy consists of the identification of the skeleton links and the detection of the page-flipping links. The skeleton links instruct the crawler to only crawl valuable pages and meanwhile avoid duplicate and uninformative ones; and the page-flipping links tell the crawler how to completely download a long discussion thread which is usually shown in multiple pages in Web forums. The extensive experimental results on several forums show encouraging performance of our approach. Following the discovered traversal strategy, our forum crawler can archive more informative pages in comparison with previous related work and a commercial generic crawler.

#index 1074109
#* Finding question-answer pairs from online forums
#@ Gao Cong;Long Wang;Chin-Yew Lin;Young-In Song;Yueheng Sun
#t 2008
#c 13
#% 268079
#% 309126
#% 340899
#% 340918
#% 464996
#% 818241
#% 838397
#% 838398
#% 848650
#% 879595
#% 939368
#% 939614
#% 939776
#% 939968
#% 987235
#% 1269817
#% 1274819
#! Online forums contain a huge amount of valuable user generated content. In this paper we address the problem of extracting question-answer pairs from forums. Question-answer pairs extracted from forums can be used to help Question Answering services (e.g. Yahoo! Answers) among other applications. We propose a sequential patterns based classification method to detect questions in a forum thread, and a graph based propagation method to detect answers for questions in the same thread. Experimental results show that our techniques are very promising.

#index 1074110
#* Retrieval models for question and answer archives
#@ Xiaobing Xue;Jiwoon Jeon;W. Bruce Croft
#t 2008
#c 13
#% 262096
#% 280851
#% 309126
#% 397128
#% 740915
#% 838397
#% 838398
#% 838464
#% 879593
#% 939939
#% 1056829
#! Retrieval in a question and answer archive involves finding good answers for a user's question. In contrast to typical document retrieval, a retrieval model for this task can exploit question similarity as well as ranking the associated answers. In this paper, we propose a retrieval model that combines a translation-based language model for the question part with a query likelihood approach for the answer part. The proposed model incorporates word-to-word translation probabilities learned through exploiting different sources of information. Experiments show that the proposed translation based language model for the question part outperforms baseline methods significantly. By combining with the query likelihood language model for the answer part, substantial additional effectiveness improvements are obtained.

#index 1074111
#* Predicting information seeker satisfaction in community question answering
#@ Yandong Liu;Jiang Bian;Eugene Agichtein
#t 2008
#c 13
#% 262102
#% 269218
#% 319666
#% 561315
#% 754059
#% 838398
#% 854668
#% 879565
#% 879593
#% 893431
#% 926881
#% 946521
#% 956495
#% 956517
#% 958454
#% 987212
#% 987233
#% 987298
#% 1035587
#% 1272280
#% 1275193
#! Question answering communities such as Naver and Yahoo! Answers have emerged as popular, and often effective, means of information seeking on the web. By posting questions for other participants to answer, information seekers can obtain specific answers to their questions. Users of popular portals such as Yahoo! Answers already have submitted millions of questions and received hundreds of millions of answers from other participants. However, it may also take hours --and sometime days-- until a satisfactory answer is posted. In this paper we introduce the problem of predicting information seeker satisfaction in collaborative question answering communities, where we attempt to predict whether a question author will be satisfied with the answers submitted by the community participants. We present a general prediction model, and develop a variety of content, structure, and community-focused features for this task. Our experimental results, obtained from a largescale evaluation over thousands of real questions and user ratings, demonstrate the feasibility of modeling and predicting asker satisfaction. We complement our results with a thorough investigation of the interactions and information seeking patterns in question answering communities that correlate with information seeker satisfaction. Our models and predictions could be useful for a variety of applications such as user intent inference, answer ranking, interface design, and query suggestion and routing.

#index 1074112
#* Discovering key concepts in verbose queries
#@ Michael Bendersky;W. Bruce Croft
#t 2008
#c 13
#% 46803
#% 184489
#% 218978
#% 262096
#% 278107
#% 306506
#% 397127
#% 420487
#% 466806
#% 495937
#% 529158
#% 719598
#% 766430
#% 766431
#% 811278
#% 818262
#% 838532
#% 855293
#% 869484
#% 879587
#% 926881
#% 987231
#% 987232
#% 987260
#! Current search engines do not, in general, perform well with longer, more verbose queries. One of the main issues in processing these queries is identifying the key concepts that will have the most impact on effectiveness. In this paper, we develop and evaluate a technique that uses query-dependent, corpus-dependent, and corpus-independent features for automatic extraction of key concepts from verbose queries. We show that our method achieves higher accuracy in the identification of key concepts than standard weighting methods such as inverse document frequency. Finally, we propose a probabilistic model for integrating the weighted key concepts identified by our method into a query, and demonstrate that this integration significantly improves retrieval effectiveness for a large set of natural language description queries derived from TREC topics on several newswire and web collections.

#index 1074113
#* Ambiguous queries: test collections need more sense
#@ Mark Sanderson
#t 2008
#c 13
#% 46809
#% 109187
#% 131434
#% 144031
#% 169768
#% 262112
#% 292686
#% 297550
#% 642975
#% 642994
#% 643017
#% 748825
#% 817846
#% 818266
#% 872033
#% 879618
#% 1024548
#% 1074133
#% 1271267
#! Although there are many papers examining ambiguity in Information Retrieval, this paper shows that there is a whole class of ambiguous word that past research has barely explored. It is shown that the class is more ambiguous than other word types and is commonly used in queries. The lack of test collections containing ambiguous queries is highlighted and a method for creating collections from existing resources is described. Tests using the new collection show the impact of query ambiguity on an IR system: it is shown that conventional systems are incapable of dealing effectively with such queries and that current assumptions about how to improve search effectiveness do not hold when searching on this common query type.

#index 1074114
#* Automatically identifying localizable queries
#@ Michael J. Welch;Junghoo Cho
#t 2008
#c 13
#% 218978
#% 248218
#% 284796
#% 306468
#% 309122
#% 361100
#% 413615
#% 420077
#% 577224
#% 577329
#% 591792
#% 637576
#% 754125
#% 805878
#% 818221
#% 832349
#% 869510
#% 878624
#% 881575
#% 1264957
#% 1650665
#! Personalization of web search results as a technique for improving user satisfaction has received notable attention in the research community over the past decade. Much of this work focuses on modeling and establishing a profile for each user to aid in personalization. Our work takes a more query-centric approach. In this paper, we present a method for efficient, automatic identification of a class of queries we define as localizable from a web search engine query log. We determine a set of relevant features and use conventional machine learning techniques to classify queries. Our experiments find that our technique is able to identify localizable queries with 94% accuracy.

#index 1074115
#* Real-time automatic tag recommendation
#@ Yang Song;Ziming Zhuang;Huajing Li;Qiankun Zhao;Jia Li;Wang-Chien Lee;C. Lee Giles
#t 2008
#c 13
#% 224113
#% 342621
#% 342659
#% 345829
#% 466675
#% 818218
#% 823328
#% 855601
#% 870226
#% 956579
#% 1006351
#% 1294111
#% 1650569
#% 1712595
#! Tags are user-generated labels for entities. Existing research on tag recommendation either focuses on improving its accuracy or on automating the process, while ignoring the efficiency issue. We propose a highly-automated novel framework for real-time tag recommendation. The tagged training documents are treated as triplets of (words, docs, tags), and represented in two bipartite graphs, which are partitioned into clusters by Spectral Recursive Embedding (SRE). Tags in each topical cluster are ranked by our novel ranking algorithm. A two-way Poisson Mixture Model (PMM) is proposed to model the document distribution into mixture components within each cluster and aggregate words into word clusters simultaneously. A new document is classified by the mixture model based on its posterior probabilities so that tags are recommended according to their ranks. Experiments on large-scale tagging datasets of scientific documents (CiteULike) and web pages del.icio.us) indicate that our framework is capable of making tag recommendation efficiently and effectively. The average tagging time for testing a document is around 1 second, with over 88% test documents correctly labeled with the top nine tags we suggested.

#index 1074116
#* Efficient top-k querying over social-tagging networks
#@ Ralf Schenkel;Tom Crecelius;Mouna Kacimi;Sebastian Michel;Thomas Neumann;Josiane X. Parreira;Gerhard Weikum
#t 2008
#c 13
#% 169781
#% 268079
#% 309095
#% 330687
#% 452563
#% 643566
#% 722754
#% 734590
#% 768903
#% 813966
#% 818232
#% 855601
#% 869548
#% 879611
#% 881523
#% 893128
#% 905319
#% 956515
#% 956516
#% 956521
#% 956544
#% 956578
#% 967452
#% 989578
#% 989643
#% 1019186
#% 1035588
#% 1396093
#% 1667787
#% 1683911
#! Online communities have become popular for publishing and searching content, as well as for finding and connecting to other users. User-generated content includes, for example, personal blogs, bookmarks, and digital photos. These items can be annotated and rated by different users, and these social tags and derived user-specific scores can be leveraged for searching relevant content and discovering subjectively interesting items. Moreover, the relationships among users can also be taken into consideration for ranking search results, the intuition being that you trust the recommendations of your close friends more than those of your casual acquaintances. Queries for tag or keyword combinations that compute and rank the top-k results thus face a large variety of options that complicate the query processing and pose efficiency challenges. This paper addresses these issues by developing an incremental top-k algorithm with two-dimensional expansions: social expansion considers the strength of relations among users, and semantic expansion considers the relatedness of different tags. It presents a new algorithm, based on principles of threshold algorithms, by folding friends and related tags into the search space in an incremental on-demand manner. The excellent performance of the method is demonstrated by an experimental evaluation on three real-world datasets, crawled from deli.cio.us, Flickr, and LibraryThing.

#index 1074117
#* Social tag prediction
#@ Paul Heymann;Daniel Ramage;Hector Garcia-Molina
#t 2008
#c 13
#% 152934
#% 248810
#% 269217
#% 348165
#% 430761
#% 465754
#% 770810
#% 840882
#% 855601
#% 869608
#% 905319
#% 1035588
#% 1065406
#! In this paper, we look at the "social tag prediction" problem. Given a set of objects, and a set of tags applied to those objects by users, can we predict whether a given tag could/should be applied to a particular object? We investigated this question using one of the largest crawls of the social bookmarking system del.icio.us gathered to date. For URLs in del.icio.us, we predicted tags based on page text, anchor text, surrounding hosts, and other tags applied to the URL. We found an entropy-based metric which captures the generality of a particular tag and informs an analysis of how well that tag can be predicted. We also found that tag-based association rules can produce very high-precision predictions as well as giving deeper understanding into the relationships between tags. Our results have implications for both the study of tagging systems as potential information retrieval tools, and for the design of such systems.

#index 1074118
#* Spectral geometry for simultaneously clustering and ranking query search results
#@ Ying Liu;Wenyuan Li;Yongjing Lin;Liping Jing
#t 2008
#c 13
#% 262045
#% 290830
#% 296738
#% 342622
#% 467419
#% 643008
#% 755463
#% 766433
#% 806594
#% 912246
#% 982756
#% 987241
#% 987242
#! How best to present query search results is an important problem in search engines and information retrieval systems. When a single query retrieves many results, simply showing them as a long list will provide users with poor overview. Nowadays, ranking and clustering query search results have been two useful separate post-processing techniques to organize retrieved documents. In this paper, we proposed a spectral analysis method based on the content similarity networks to integrate the clustering and ranking techniques for improving literature search. The new approach organizes all these search results into categories intelligently and simultaneously rank the results in each category. A variety of theoretical and empirical studies have demonstrated that the presented method performs well in real applications, especially in biomedical literature retrieval. Moreover, any free text information can be analyzed with the new method, i.e., the proposed approach can be applied to various information systems, such as Web search engines and literature search service.

#index 1074119
#* A rank-aggregation approach to searching for optimal query-specific clusters
#@ Oren Kurland;Carmel Domshlak
#t 2008
#c 13
#% 218978
#% 218992
#% 262045
#% 262096
#% 330769
#% 340899
#% 340901
#% 340948
#% 342660
#% 342707
#% 375017
#% 427921
#% 719598
#% 742666
#% 766430
#% 766431
#% 818241
#% 818267
#% 838528
#% 857180
#% 879575
#% 879584
#% 879676
#% 952491
#% 987230
#% 1074072
#! To improve the precision at the very top ranks of a document list presented in response to a query, researchers suggested to exploit information induced from clustering of documents highly ranked by some initial search. We propose a novel model for ranking such (query-specific) clusters by the presumed percentage of relevant documents that they contain. The model is based on (i) proposing a palette of "witness" cluster properties that purportedly correlate with this percentage, (ii) devising concrete quantitative measures for these properties, and (iii) ordering the clusters via aggregation of rankings induced by these individual measures. Empirical evaluation shows that our model is consistently more effective than previously suggested methods in detecting clusters containing a high relevant-document percentage. Furthermore, the precision-at-top-ranks performance of this model transcends that of standard document-based retrieval, and competes with that of a state-of-the-art document-based retrieval approach.

#index 1074120
#* A comparative evaluation of different link types on enhancing document clustering
#@ Xiaodan Zhang;Xiaohua Hu;Xiaohua Zhou
#t 2008
#c 13
#% 211526
#% 248810
#% 282905
#% 300971
#% 309142
#% 413608
#% 464434
#% 466896
#% 466922
#% 495944
#% 728756
#% 807657
#% 879625
#% 907567
#% 915335
#% 1032049
#% 1081275
#% 1272053
#% 1275223
#! With a growing number of works utilizing link information in enhancing document clustering, it becomes necessary to make a comparative evaluation of the impacts of different link types on document clustering. Various types of links between text documents, including explicit links such as citation links and hyperlinks, implicit links such as co-authorship links, and pseudo links such as content similarity links, convey topic similarity or topic transferring patterns, which is very useful for document clustering. In this study, we adopt a Relaxation Labeling (RL)-based clustering algorithm, which employs both content and linkage information, to evaluate the effectiveness of the aforementioned types of links for document clustering on eight datasets. The experimental results show that linkage is quite effective in improving content-based document clustering. Furthermore, a series of interesting findings regarding the impacts of different link types on document clustering are discovered through our experiments.

#index 1074121
#* SpotSigs: robust and efficient near duplicate detection in large web collections
#@ Martin Theobald;Jonathan Siddharth;Andreas Paepcke
#t 2008
#c 13
#% 201935
#% 249321
#% 255137
#% 329790
#% 345087
#% 347225
#% 387427
#% 443393
#% 464608
#% 479973
#% 504572
#% 529801
#% 571725
#% 724866
#% 730067
#% 769944
#% 805905
#% 879600
#% 879617
#% 880399
#% 898309
#% 907504
#% 956507
#% 978157
#% 1022281
#! Motivated by our work with political scientists who need to manually analyze large Web archives of news sites, we present SpotSigs, a new algorithm for extracting and matching signatures for near duplicate detection in large Web crawls. Our spot signatures are designed to favor natural-language portions of Web pages over advertisements and navigational bars. The contributions of SpotSigs are twofold: 1) by combining stopword antecedents with short chains of adjacent content terms, we create robust document signatures with a natural ability to filter out noisy components of Web pages that would otherwise distract pure n-gram-based approaches such as Shingling; 2) we provide an exact and efficient, self-tuning matching algorithm that exploits a novel combination of collection partitioning and inverted index pruning for high-dimensional similarity search. Experiments confirm an increase in combined precision and recall of more than 24 percent over state-of-the-art approaches such as Shingling or I-Match and up to a factor of 3 faster execution times than Locality Sensitive Hashing (LSH), over a demonstrative "Gold Set" of manually assessed near-duplicate news articles as well as the TREC WT10g Web collection.

#index 1074122
#* Local text reuse detection
#@ Jangwon Seo;W. Bruce Croft
#t 2008
#c 13
#% 201935
#% 255137
#% 345087
#% 347225
#% 504572
#% 571725
#% 616528
#% 654447
#% 838508
#% 879600
#% 944350
#% 978157
#% 1775534
#% 1857793
#! Text reuse occurs in many different types of documents and for many different reasons. One form of reuse, duplicate or near-duplicate documents, has been a focus of researchers because of its importance in Web search. Local text reuse occurs when sentences, facts or passages, rather than whole documents, are reused and modified. Detecting this type of reuse can be the basis of new tools for text analysis. In this paper, we introduce a new approach to detecting local text reuse and compare it to other approaches. This comparison involves a study of the amount and type of reuse that occurs in real documents, including TREC newswire and blog collections.

#index 1074123
#* TSCAN: a novel method for topic summarization and content anatomy
#@ Chien Chin Chen;Meng Chang Chen
#t 2008
#c 13
#% 262042
#% 290830
#% 340884
#% 340885
#% 387427
#% 397137
#% 577220
#% 783535
#% 816173
#% 823344
#% 823383
#% 869604
#% 1269588
#% 1272053
#! A topic is defined as a seminal event or activity along with all directly related events and activities. It is represented as a chronological sequence of documents by different authors published on the Internet. In this paper, we define a task called topic anatomy, which summarizes and associates core parts of a topic graphically so that readers can understand the content easily. The proposed topic anatomy model, called TSCAN, derives the major themes of a topic from the eigenvectors of a temporal block association matrix. Then, the significant events of the themes and their summaries are extracted by examining the constitution of the eigenvectors. Finally, the extracted events are associated through their temporal closeness and context similarity to form the evolution graph of the topic. Experiments based on the official TDT4 corpus demonstrate that the generated evolution graphs comprehensibly describe the storylines of topics. Moreover, in terms of content coverage and consistency, the produced summaries are superior to those of other summarization methods based on human composed reference summaries.

#index 1074124
#* A new rank correlation coefficient for information retrieval
#@ Emine Yilmaz;Javed A. Aslam;Stephen Robertson
#t 2008
#c 13
#% 115608
#% 340890
#% 340892
#% 348165
#% 453464
#% 728355
#% 730072
#% 766409
#% 766410
#% 838529
#% 907496
#% 983654
#% 987354
#! In the field of information retrieval, one is often faced with the problem of computing the correlation between two ranked lists. The most commonly used statistic that quantifies this correlation is Kendall's Τ. Often times, in the information retrieval community, discrepancies among those items having high rankings are more important than those among items having low rankings. The Kendall's Τ statistic, however, does not make such distinctions and equally penalizes errors both at high and low rankings. In this paper, we propose a new rank correlation coefficient, AP correlation (Τap), that is based on average precision and has a probabilistic interpretation. We show that the proposed statistic gives more weight to the errors at high rankings and has nice mathematical properties which make it easy to interpret. We further validate the applicability of the statistic using experimental data.

#index 1074125
#* Learning from labeled features using generalized expectation criteria
#@ Gregory Druck;Gideon Mann;Andrew McCallum
#t 2008
#c 13
#% 170649
#% 236729
#% 464465
#% 466263
#% 722904
#% 769908
#% 799753
#% 879616
#% 879626
#% 940031
#% 961194
#% 983878
#% 987202
#% 1250186
#% 1707845
#! It is difficult to apply machine learning to new domains because often we lack labeled problem instances. In this paper, we provide a solution to this problem that leverages domain knowledge in the form of affinities between input features and classes. For example, in a baseball vs. hockey text classification problem, even without any labeled data, we know that the presence of the word puck is a strong indicator of hockey. We refer to this type of domain knowledge as a labeled feature. In this paper, we propose a method for training discriminative probabilistic models with labeled features and unlabeled instances. Unlike previous approaches that use labeled features to create labeled pseudo-instances, we use labeled features directly to constrain the model's predictions on unlabeled instances. We express these soft constraints using generalized expectation (GE) criteria --- terms in a parameter estimation objective function that express preferences on values of a model expectation. In this paper we train multinomial logistic regression models using GE criteria, but the method we develop is applicable to other discriminative probabilistic models. The complete objective function also includes a Gaussian prior on parameters, which encourages generalization by spreading parameter weight to unlabeled features. Experimental results on text classification data sets show that this method outperforms heuristic approaches to training classifiers with labeled features. Experiments with human annotators show that it is more beneficial to spend limited annotation time labeling features rather than labeling instances. For example, after only one minute of labeling features, we can achieve 80% accuracy on the ibm vs. mac text classification problem using GE-FL, whereas ten minutes labeling documents results in an accuracy of only 77%

#index 1074126
#* A simple and efficient sampling method for estimating AP and NDCG
#@ Emine Yilmaz;Evangelos Kanoulas;Javed A. Aslam
#t 2008
#c 13
#% 248065
#% 411762
#% 748738
#% 766409
#% 879598
#% 879632
#% 907496
#% 987201
#% 987237
#% 987239
#% 987327
#! We consider the problem of large scale retrieval evaluation. Recently two methods based on random sampling were proposed as a solution to the extensive effort required to judge tens of thousands of documents. While the first method proposed by Aslam et al. [1] is quite accurate and efficient, it is overly complex, making it difficult to be used by the community, and while the second method proposed by Yilmaz et al., infAP [14], is relatively simple, it is less efficient than the former since it employs uniform random sampling from the set of complete judgments. Further, none of these methods provide confidence intervals on the estimated values. The contribution of this paper is threefold: (1) we derive confidence intervals for infAP, (2) we extend infAP to incorporate nonrandom relevance judgments by employing stratified random sampling, hence combining the efficiency of stratification with the simplicity of random sampling, (3) we describe how this approach can be utilized to estimate nDCG from incomplete judgments. We validate the proposed methods using TREC data and demonstrate that these new methods can be used to incorporate nonrandom samples, as were available in TREC Terabyte track '06.

#index 1074127
#* A general optimization framework for smoothing language models on graph structures
#@ Qiaozhu Mei;Duo Zhang;ChengXiang Zhai
#t 2008
#c 13
#% 78171
#% 262096
#% 268079
#% 280850
#% 280856
#% 290830
#% 340899
#% 340901
#% 340948
#% 342707
#% 593047
#% 719598
#% 750863
#% 766430
#% 766431
#% 818241
#% 818254
#% 838528
#% 879575
#% 940042
#% 1041735
#! Recent work on language models for information retrieval has shown that smoothing language models is crucial for achieving good retrieval performance. Many different effective smoothing methods have been proposed, which mostly implement various heuristics to exploit corpus structures. In this paper, we propose a general and unified optimization framework for smoothing language models on graph structures. This framework not only provides a unified formulation of the existing smoothing heuristics, but also serves as a road map for systematically exploring smoothing methods for language models. We follow this road map and derive several different instantiations of the framework. Some of the instantiations lead to novel smoothing methods. Empirical results show that all such instantiations are effective with some outperforming the state of the art smoothing methods.

#index 1074128
#* Deep classification in large-scale text hierarchies
#@ Gui-Rong Xue;Dikan Xing;Qiang Yang;Yong Yu
#t 2008
#c 13
#% 280817
#% 287214
#% 297550
#% 309141
#% 318412
#% 344447
#% 465747
#% 465754
#% 466078
#% 466501
#% 571073
#% 642986
#% 735077
#% 763708
#% 766428
#% 783478
#% 829975
#% 987262
#% 1035583
#! Most classification algorithms are best at categorizing the Web documents into a few categories, such as the top two levels in the Open Directory Project. Such a classification method does not give very detailed topic-related class information for the user because the first two levels are often too coarse. However, classification on a large-scale hierarchy is known to be intractable for many target categories with cross-link relationships among them. In this paper, we propose a novel deep-classification approach to categorize Web documents into categories in a large-scale taxonomy. The approach consists of two stages: a search stage and a classification stage. In the first stage, a category-search algorithm is used to acquire the category candidates for a given document. Based on the category candidates, we prune the large-scale hierarchy to focus our classification effort on a small subset of the original hierarchy. As a result, the classification model is trained on the small subset before being applied to assign the category for a new document. Since the category candidates are sufficiently close to each other in the hierarchy, a statistical-language-model based classifier using n-gram features is exploited. Furthermore, the structure of the taxonomy can be utilized in this stage to improve the performance of classification. We demonstrate the performance of our proposed algorithms on the Open Directory Project with over 130,000 categories. Experimental results show that our proposed approach can reach 51.8% on the measure of Mi-F1 at the 5th level, which is 77.7% improvement over top-down based SVM classification algorithms.

#index 1074129
#* Topic-bridged PLSA for cross-domain text classification
#@ Gui-Rong Xue;Wenyuan Dai;Qiang Yang;Yong Yu
#t 2008
#c 13
#% 127850
#% 280819
#% 311027
#% 318412
#% 458379
#% 464631
#% 465754
#% 466263
#% 770847
#% 770858
#% 840898
#% 879615
#% 956522
#% 983828
#! In many Web applications, such as blog classification and new-sgroup classification, labeled data are in short supply. It often happens that obtaining labeled data in a new domain is expensive and time consuming, while there may be plenty of labeled data in a related but different domain. Traditional text classification ap-proaches are not able to cope well with learning across different domains. In this paper, we propose a novel cross-domain text classification algorithm which extends the traditional probabilistic latent semantic analysis (PLSA) algorithm to integrate labeled and unlabeled data, which come from different but related domains, into a unified probabilistic model. We call this new model Topic-bridged PLSA, or TPLSA. By exploiting the common topics between two domains, we transfer knowledge across different domains through a topic-bridge to help the text classification in the target domain. A unique advantage of our method is its ability to maximally mine knowledge that can be transferred between domains, resulting in superior performance when compared to other state-of-the-art text classification approaches. Experimental eval-uation on different kinds of datasets shows that our proposed algorithm can improve the performance of cross-domain text classification significantly.

#index 1074130
#* trNon-greedy active learning for text categorization using convex ansductive experimental design
#@ Kai Yu;Shenghuo Zhu;Wei Xu;Yihong Gong
#t 2008
#c 13
#% 132697
#% 236729
#% 292664
#% 420507
#% 466419
#% 642998
#% 722797
#% 763708
#% 840868
#% 875997
#% 876080
#% 987207
#% 987361
#% 1272282
#! In this paper we propose a non-greedy active learning method for text categorization using least-squares support vector machines (LSSVM). Our work is based on transductive experimental design (TED), an active learning formulation that effectively explores the information of unlabeled data. Despite its appealing properties, the optimization problem is however NP-hard and thus--like most of other active learning methods--a greedy sequential strategy to select one data example after another was suggested to find a suboptimum. In this paper we formulate the problem into a continuous optimization problem and prove its convexity, meaning that a set of data examples can be selected with a guarantee of global optimum. We also develop an iterative algorithm to efficiently solve the optimization problem, which turns out to be very easy-to-implement. Our text categorization experiments on two text corpora empirically demonstrated that the new active learning algorithm outperforms the sequential greedy algorithm, and is promising for active text categorization applications.

#index 1074131
#* Classifiers without borders: incorporating fielded text from neighboring web pages
#@ Xiaoguang Qi;Brian D. Davison
#t 2008
#c 13
#% 169781
#% 248810
#% 269217
#% 281251
#% 309142
#% 309145
#% 348148
#% 348173
#% 348178
#% 413663
#% 430761
#% 466896
#% 466922
#% 679872
#% 730061
#% 783474
#% 840583
#% 869649
#% 879576
#% 879625
#% 880399
#% 907509
#% 907566
#% 913206
#% 987245
#% 987253
#% 1709423
#% 1734215
#! Accurate web page classification often depends crucially on information gained from neighboring pages in the local web graph. Prior work has exploited the class labels of nearby pages to improve performance. In contrast, in this work we utilize a weighted combination of the contents of neighbors to generate a better virtual document for classification. In addition, we break pages into fields, finding that a weighted combination of text from the target and fields of neighboring pages is able to reduce classification error by more than a third. We demonstrate performance on a large dataset of pages from the Open Directory Project and validate the approach using pages from a crawl from the Stanford WebBase. Interestingly, we find no value in anchor text and unexpected value in page titles (and especially titles of parent pages) in the virtual document.

#index 1074132
#* Evaluation over thousands of queries
#@ Ben Carterette;Virgil Pavlu;Evangelos Kanoulas;Javed A. Aslam;James Allan
#t 2008
#c 13
#% 262102
#% 818222
#% 818276
#% 879598
#% 879632
#% 879650
#% 907496
#% 917404
#% 987199
#% 987238
#% 1019126
#% 1392447
#! Information retrieval evaluation has typically been performed over several dozen queries, each judged to near-completeness. There has been a great deal of recent work on evaluation over much smaller judgment sets: how to select the best set of documents to judge and how to estimate evaluation measures when few judgments are available. In light of this, it should be possible to evaluate over many more queries without much more total judging effort. The Million Query Track at TREC 2007 used two document selection algorithms to acquire relevance judgments for more than 1,800 queries. We present results of the track, along with deeper analysis: investigating tradeoffs between the number of queries and number of judgments shows that, up to a point, evaluation over more queries with fewer judgments is more cost-effective and as reliable as fewer queries with more judgments. Total assessor effort can be reduced by 95% with no appreciable increase in evaluation errors.

#index 1074133
#* Novelty and diversity in information retrieval evaluation
#@ Charles L.A. Clarke;Maheedhar Kolla;Gordon V. Cormack;Olga Vechtomova;Azin Ashkan;Stefan Büttcher;Ian MacKinnon
#t 2008
#c 13
#% 262105
#% 262112
#% 324129
#% 326522
#% 397161
#% 411762
#% 590523
#% 642975
#% 766409
#% 818221
#% 838536
#% 840846
#% 867119
#% 872020
#% 879565
#% 879567
#% 879618
#% 944349
#% 987200
#% 987226
#% 987321
#% 1024548
#% 1026428
#! Evaluation measures act as objective functions to be optimized by information retrieval systems. Such objective functions must accurately reflect user requirements, particularly when tuning IR systems and learning ranking functions. Ambiguity in queries and redundancy in retrieved documents are poorly reflected by current evaluation measures. In this paper, we present a framework for evaluation that systematically rewards novelty and diversity. We develop this framework into a specific evaluation measure, based on cumulative gain. We demonstrate the feasibility of our approach using a test collection based on the TREC question answering track.

#index 1074134
#* Relevance assessment: are judges exchangeable and does it matter
#@ Peter Bailey;Nick Craswell;Ian Soboroff;Paul Thomas;Arjen P. de Vries;Emine Yilmaz
#t 2008
#c 13
#% 129694
#% 207677
#% 208931
#% 262097
#% 262105
#% 309095
#% 397164
#% 763733
#% 766409
#% 879632
#% 907496
#% 987327
#% 1024552
#% 1074126
#% 1149091
#% 1783188
#! We investigate to what extent people making relevance judgements for a reusable IR test collection are exchangeable. We consider three classes of judge: "gold standard" judges, who are topic originators and are experts in a particular information seeking task; "silver standard" judges, who are task experts but did not create topics; and "bronze standard" judges, who are those who did not define topics and are not experts in the task. Analysis shows low levels of agreement in relevance judgements between these three groups. We report on experiments to determine if this is sufficient to invalidate the use of a test collection for measuring system performance when relevance assessments have been created by silver standard or bronze standard judges. We find that both system scores and system rankings are subject to consistent but small differences across the three assessment sets. It appears that test collections are not completely robust to changes of judge when these judges vary widely in task and topic expertise. Bronze standard judges may not be able to substitute for topic and task experts, due to changes in the relative performance of assessed systems, and gold standard judges are preferred.

#index 1074135
#* Intuition-supporting visualization of user's performance based on explicit negative higher-order relevance
#@ Heikki Keskustalo;Kalervo Järvelin;Ari Pirkola;Jaana Kekäläinen
#t 2008
#c 13
#% 83851
#% 109187
#% 234793
#% 235918
#% 311866
#% 340892
#% 397164
#% 411762
#% 987196
#% 1001644
#% 1019103
#! Modeling the beyond-topical aspects of relevance are currently gaining popularity in IR evaluation. For example, the discounted cumulated gain (DCG) measure implicitly models some aspects of higher-order relevance via diminishing the value of relevant documents seen later during retrieval (e.g., due to information cumulated, redundancy, and effort). In this paper, we focus on the concept of negative higher-order relevance (NHOR) made explicit via negative gain values in IR evaluation. We extend the computation of DCG to allow negative gain values, perform an experiment in a laboratory setting, and demonstrate the characteristics of NHOR in evaluation. The approach leads to intuitively reasonable performance curves emphasizing, from the user's point of view, the progression of retrieval towards success or failure. We discuss normalization issues when both positive and negative gain values are allowed and conclude by discussing the usage of NHOR to characterize test collections.

#index 1074136
#* Relevance judgments between TREC and Non-TREC assessors
#@ Azzah Al-Maskari;Mark Sanderson;Paul Clough
#t 2008
#c 13
#% 309095
#% 312689
#% 397164
#% 793013
#% 879566
#! This paper investigates the agreement of relevance assessments between official TREC judgments and those generated from an interactive IR experiment. Results show that 63% of documents judged relevant by our users matched official TREC judgments. Several factors contributed to differences in the agreements: the number of retrieved relevant documents; the number of relevant documents judged; system effectiveness per topic and the ranking of relevant documents.

#index 1074137
#* Evaluation measures for preference judgments
#@ Ben Carterette;Paul N. Bennett
#t 2008
#c 13
#% 1415710
#! There has been recent interest in collecting user or assessor preferences, rather than absolute judgments of relevance, for the evaluation or learning of ranking algorithms. Since measures like precision, recall, and DCG are defined over absolute judgments, evaluation over preferences will require new evaluation measures that explicitly model them. We describe a class of such measures and compare absolute and preference measures over a large TREC collection.

#index 1074138
#* Exploring evaluation metrics: GMAP versus MAP
#@ Sri Devi Ravana;Alistair Moffat
#t 2008
#c 13
#% 907493
#% 1074057
#% 1415776
#! In retrieval experiments, an effectiveness metrics is used to generate a score for each system-topic pair being tested. It is then usual to average the system-topic scores to obtain a system score, which is used for the purpose of system comparison. In this paper we explore the ramifications of using the geometric mean (GMAP), rather than the arithmetic mean (MAP) when computing an aggregate system score from a set of system-topic scores. We find that GMAP does indeed handle variability in topic difficulty more consistently than does the usual MAP aggregation method.

#index 1074139
#* A new interpretation of average precision
#@ Stephen Robertson
#t 2008
#c 13
#% 879597
#% 879632
#% 909448
#% 987239
#! We consider the question of whether Average Precision, as a measure of retrieval effectiveness, can be regarded as deriving from a model of user searching behaviour. It turns out that indeed it can be so regarded, under a very simple stochastic model of user behaviour.

#index 1074140
#* Comparing metrics across TREC and NTCIR:: the robustness to pool depth bias
#@ Tetsuya Sakai
#t 2008
#c 13
#% 262102
#% 411762
#% 766409
#% 879630
#% 907496
#% 1072570
#% 1095876

#index 1074141
#* Relevance thresholds in system evaluations
#@ Falk Scholer;Andrew Turpin
#t 2008
#c 13
#% 309089
#% 340921
#% 818257
#% 857180
#% 879566
#% 987321
#! We introduce and explore the concept of an individual's relevance threshold as a way of reconciling differences in outcomes between batch and user experiments.

#index 1074142
#* Precision-at-ten considered redundant
#@ William Webber;Alistair Moffat;Justin Zobel;Tetsuya Sakai
#t 2008
#c 13
#% 309093
#% 397163
#% 411762
#% 818222
#% 879566
#% 1095876
#! Information retrieval systems are compared using evaluation metrics, with researchers commonly reporting results for simple metrics such as precision-at-10 or reciprocal rank together with more complex ones such as average precision or discounted cumulative gain. In this paper, we demonstrate that complex metrics are as good as or better than simple metrics at predicting the performance of the simple metrics on other topics. Therefore, reporting of results from simple metrics alongside complex ones is redundant.

#index 1074143
#* Structuring collections with Scatter/Gather extensions
#@ Omar Alonso;Justin Talbot
#t 2008
#c 13
#% 118771
#% 214711
#! A major component of sense-making is organizing--grouping, labeling, and summarizing--the data at hand in order to form a useful mental model, a necessary precursor to identifying missing information and to reasoning about the data. Previous work has shown the Scatter/Gather model to be useful in exploratory activities that occur when users encounter unknown document collections. However, the topic structure communicated by Scatter/Gather is closely tied to the behavior of the underlying clustering algorithm; this structure may not reflect the mental model most applicable to the information need. In this paper we describe the initial design of a mixed-initiative information structuring tool that leverages aspects of the well-studied Scatter/Gather model but permits the user to impose their own desired structure when necessary.

#index 1074144
#* Text collections for FIRE
#@ Prasenjit Majumder;Mandar Mitra;Dipasree Pal;Ayan Bandyopadhyay;Samaresh Maiti;Sukanya Mitra;Aparajita Sen;Sukomal Pal
#t 2008
#c 13
#% 411760
#% 840583
#! The aim of the Forum for Information Retrieval Evaluation (FIRE) is to create a Cranfield-like evaluation framework in the spirit of TREC, CLEF and NTCIR, for Indian Language Information Retrieval. For the first year, six Indian languages have been selected: Bengali, Hindi, Marathi, Punjabi, Tamil, and Telugu. This poster describes the tasks as well as the document and topic collections that are to be used at the FIRE workshop.

#index 1074145
#* A longitudinal study of real-time search assistance adoption
#@ Peter Anick;Raj Gopal Kantamneni
#t 2008
#c 13
#% 214709
#% 642985
#% 943042
#% 987350
#! We present findings from a log based study designed to track the adoption of features of a new real-time query refinement interface deployed on the Yahoo search engine. Several trends from the first four months are noted and discussed.

#index 1074146
#* TopicRank: bringing insight to users
#@ Ivan Berlocher;Kyung-il Lee;Kono Kim
#t 2008
#c 13
#% 118771
#% 748463
#% 766433
#% 857482
#% 1029800

#index 1074147
#* Talking the talk vs. walking the walk: salience of information needs in querying vs. browsing
#@ Mikhail Bilenko;Ryen W. White;Matthew Richardson;G. Craig Murray
#t 2008
#c 13
#% 818259
#% 869536
#% 987198
#% 987222
#% 1396086
#! Traditional information retrieval models assume that users express their information needs via text queries (i.e., their "talk"). In this poster, we consider Web browsing behavior outside of interactions with retrieval systems (i.e., users' "walk") as an alternative source of signal describing users' information needs, and compare it to the query-expressed information needs on a large dataset. Our findings demonstrate that information needs expressed in different behavior modalities are largely non-overlapping, and that past behavior in each modality is the most accurate predictor of future behavior in that modality. Results also show that browsing data provides a stronger source of signal than search queries due to its greater volume, which explains previous work that has found implicit behavioral data to be a valuable source of information for user modeling and personalization.

#index 1074148
#* Exploring mouse movements for inferring query intent
#@ Qi Guo;Eugene Agichtein
#t 2008
#c 13
#% 446992
#% 869483
#% 881570
#! Clickthrough on search results have been successfully used to infer user interest and preferences, but are often noisy and potentially ambiguous. We explore the potential of a complementary, more sensitive signal -mouse movements- in providing insights into the intent behind a web search query. We report preliminary results of studying user mouse movements on search result pages, with the goal of inferring user intent - in particular, to explore whether we can automatically distinguish the different query classes such as navigational vs. informational. Our preliminary exploration confirms the value of studying mouse movements for user intent inference, and suggests interesting avenues for future exploration.

#index 1074149
#* Emulating query-biased summaries using document titles
#@ Hideo Joho;David Hannah;Joemon M. Jose
#t 2008
#c 13
#% 262036
#% 717120
#% 783474
#% 987208
#% 987209

#index 1074150
#* Hierarchical naive bayes models for representing user profiles
#@ J. F. Huete;L. M. de Campos;J. M. Fernandez-Luna;M. A. Rueda-Morales
#t 2008
#c 13
#% 220706
#% 246831
#% 734590
#% 866537
#! In this paper, we show how a user profile can be enhanced when a more detailed description of the products is included. Two main assumptions have been considered: the first implies that the set of features used to describe an item can be organized into a well-defined set of components or categories, and the second is that the user's rating for a given item is obtained by combining user opinions of the relevance of each component.

#index 1074151
#* A topical PageRank based algorithm for recommender systems
#@ Liyan Zhang;Kai Zhang;Chunping Li
#t 2008
#c 13
#% 832334
#% 879576
#% 961613
#! In this paper, we propose a Topical PageRank based algorithm for recommender systems, which aim to rank products by analyzing previous user-item relationships, and recommend top-rank items to potentially interested users. We evaluate our algorithm on MovieLens dataset and empirical experiments demonstrate that it outperforms other state-of-the-art recommending algorithms.

#index 1074152
#* The impact of history length on personalized search
#@ Yangbo Zhu;Jamie Callan;Jaime Carbonell
#t 2008
#c 13
#% 284796
#% 309095
#% 818207
#% 987313
#! Personalized search is a promising way to better serve different users' information needs. Search history is one of the major information sources for search personalization. We investigated the impact of history length on the effectiveness of personalized ranking. We carried out task-based user study for Web search, and obtained ranked relevance judgments for all queries. Query contexts derived from previous queries in the same task are used to re-rank results for the current query. Experimental results show that the performance of personalization generally improves as more queries are accumulated, but most of the benefits come from a few immediately preceding queries.

#index 1074153
#* User preference choices for complex question answering
#@ Mingfang Wu;Falk Scholer;Andrew Turpin
#t 2008
#c 13
#% 907495
#% 940039
#% 987298
#! Question answering systems increasingly need to deal with complex information needs that require more than simple factoid answers. The evaluation of such systems is usually carried out using precision- or recall-based system performance metrics. Previous work has demonstrated that when users are shown two search result lists side-by-side, they can reliably differentiate between the qualities of the lists. We investigate the consistency between this user-based approach and system-oriented metrics in the question answering environment. Our initial results indicate that the two methodologies show a high level of disagreement.

#index 1074154
#* Towards personalized distributed information retrieval
#@ Mark J. Carman;Fabio Crestani
#t 2008
#c 13
#% 340146
#% 643012
#% 818207
#% 1392446
#! Our aim is to investigate if and how the performance of Distributed Information Retrieval (DIR) systems can be improved through personalization. Toward this aim we are building a testbed of document collections and corresponding personalized relevance judgments. In this paper we discuss our intended approach for personalizing the three different phases of the DIR process. We also describe the test collection we are building and discuss our methodology for evaluating personalized DIR using relevance information taken from social bookmarking data.

#index 1074155
#* Task-aware search personalization
#@ Julia Luxenburger;Shady Elbassuoni;Gerhard Weikum
#t 2008
#c 13
#% 309095
#% 340948
#% 397161
#% 783506
#% 818259
#% 987193
#! Search personalization has been pursued in many ways, in order to provide better result rankings and better overall search experience to individual users [5]. However, blindly applying personalization to all user queries, for example, by a background model derived from the user's long-term query-and-click history, is not always appropriate for aiding the user in accomplishing her actual task. User interests change over time, a user sometimes works on very different categories of tasks within a short timespan, and history-based personalization may impede a user's desire of discovering new topics. In this paper we propose a personalization framework that is selective in a twofold sense. First, it selectively employs personalization techniques for queries that are expected to benefit from prior history information, while refraining from undue actions otherwise. Second, we introduce the notion of tasks representing different granularity levels of a user profile, ranging from very specific search goals to broad topics, and base our reasoning selectively on query-relevant user tasks. These considerations are cast into a statistical language model for tasks, queries, and documents, supporting both judicious query expansion and result re-ranking. The effectiveness of our method is demonstrated by an empirical user study.

#index 1074156
#* Personal vs non-personal blogs: initial classification experiments
#@ Erik Elgersma;Maarten de Rijke
#t 2008
#c 13
#% 928355
#% 1250370
#! We address the task of separating personal from non-personal blogs, and report on a set of baseline experiments where we compare the performance on a small set of features across a set of five classifiers. We show that with a limited set of features a performance of up to 90% can be obtained.

#index 1074157
#* Exploiting subjectivity analysis in blogs to improve political leaning categorization
#@ Maojin Jiang;Shlomo Argamon
#t 2008
#c 13
#! In this paper, we address a relatively new and interesting text categorization problem: classify a political blog as either liberal or conservative, based on its political leaning. Our subjectivity analysis based method is twofold: 1) we identify subjective sentences that contain at least two strong subjective clues based on the General Inquirer dictionary; 2) from subjective sentences identified, we extract opinion expressions and other features to build political leaning classifiers. Experimental results with a political blog corpus we built show that by using features from subjective sentences can significantly improve the classification performance. In addition, by extracting opinion expressions from subjective sentences, we are able to reveal opinions that are characteristic of a specific political leaning to some extent.

#index 1074158
#* Ranking opinionated blog posts using OpinionFinder
#@ Ben He;Craig Macdonald;Iadh Ounis
#t 2008
#c 13
#% 855279
#% 943811
#! The aim of an opinion finding system is not just to retrieve relevant documents, but to also retrieve documents that express an opinion towards the query target entity. In this work, we propose a way to use and integrate an opinion-identification toolkit, OpinionFinder, into the retrieval process of an Information Retrieval (IR) system, such that opinionated, relevant documents are retrieved in response to a query. In our experiments, we vary the number of top-ranked documents that must be parsed in response to a query, and investigate the effect on opinion retrieval performance and required parsing time. We find that opinion finding retrieval performance is improved by integrating OpinionFinder into the retrieval system, and that retrieval performance grows as more posts are parsed by OpinionFinder. However, the benefit eventually tails off at a deep rank, suggesting that an optimal setting for the system has been achieved.

#index 1074159
#* Searching blogs and news: a study on popular queries
#@ Aixin Sun;Meishan Hu;Ee-Peng Lim
#t 2008
#c 13
#% 1742093
#! Blog/news search engines are very important channels to reach information about the real-time happenings. In this paper, we study the popular queries collected over one year period and compare their search results returned by a blog search engine (i.e., Technorati) and a news search engine (i.e., Google News). We observed that the numbers of hits returned by the two search engines for the same set of queries were highly correlated, suggesting that blogs often provide commentary to current events reported in news. As many popular queries are related to some events, we further observed a high cohesiveness among the returned search results for these queries.

#index 1074160
#* Aggregated click-through data in a homogeneous user community
#@ Mingfang Wu;Andrew Turpin;Justin Zobel
#t 2008
#c 13
#% 577224
#% 754059
#% 879567
#% 1064771
#! There are many proposed methods for using clickthrough data for common queries to improve the quality of search results returned for that query. In this study we examine the search behaviour of users in a close-knit community on such queries. We argue that the benefit of using aggregated clickthrough data varies from task to task: it may improve document rankings for navigational or specific informational queries, but is less likely to be of value to users issuing a broad informational query.

#index 1074161
#* To tag or not to tag -: harvesting adjacent metadata in large-scale tagging systems
#@ Adriana Budura;Sebastian Michel;Philippe Cudré-Mauroux;Karl Aberer
#t 2008
#c 13
#% 643566
#% 855601
#% 881054
#% 956515
#% 956544
#% 956579
#! We present HAMLET, a suite of principles, scoring models and algorithms to automatically propagate metadata along edges in a document neighborhood. As a showcase scenario we consider tag prediction in community-based Web 2.0 tagging applications. Experiments using real-world data demonstrate the viability of our approach in large-scale environments where tags are scarce. To the best of our knowledge, HAMLET is the first system to promote an efficient and precise reuse of shared metadata in highly dynamic, large-scale Web 2.0 tagging systems.

#index 1074162
#* Exploring question subjectivity prediction in community QA
#@ Baoli Li;Yandong Liu;Ashwin Ram;Ernest V. Garcia;Eugene Agichtein
#t 2008
#c 13
#% 642977
#% 855282
#% 938687
#! In this paper we begin to investigate how to automatically determine the subjectivity orientation of questions posted by real users in community question answering (CQA) portals. Subjective questions seek answers containing private states, such as personal opinion and experience. In contrast, objective questions request objective, verifiable information, often with support from reliable sources. Knowing the question orientation would be helpful not only for evaluating answers provided by users, but also for guiding the CQA engine to process questions more intelligently. Our experiments on Yahoo! Answers data show that our method exhibits promising performance.

#index 1074163
#* On the evolution of the yahoo! answers QA community
#@ Yandong Liu;Eugene Agichtein
#t 2008
#c 13
#% 309783
#% 730022
#% 1035587
#! While question answering communities have been gaining popularity for several years, we wonder if the increased popularity actually improves or degrades the user experience. In addition, automatic QA systems, which utilize different sources such as search engines and social media, are emerging rapidly. QA communities have already created abundant resources of millions of questions and hundreds of millions of answers. The question whether they will continue to serve as an effective source is of information for web search and question answering is of vital importance. In this poster, we investigate the temporal evolution of a popular QA community - Yahoo! Answers, with respect to its effectiveness in answering three basic types of questions: factoid, opinion and complex questions. Our experiments show that Yahoo! Answers keeps growing rapidly, while its overall quality as an information source for factoid question-answering degrades. However, instead of answering factoid questions, it might be more effective to answer opinion and complex questions.

#index 1074164
#* Detecting synonyms in social tagging systems to improve content retrieval
#@ Maarten Clements;Arjen P. de Vries;Marcel J.T. Reinders
#t 2008
#c 13
#% 27049
#! Collaborative tagging used in online social content systems is naturally characterized by many synonyms, causing low precision retrieval. We propose a mechanism based on user preference profiles to identify synonyms that can be used to retrieve more relevant documents by expanding the user's query. Using a popular online book catalog we discuss the effectiveness of our method over usual similarity based expansion methods.

#index 1074165
#* SOPING: a Chinese customer review mining system
#@ Chao Zhou;Guang Qiu;Kangmiao Liu;Jiajun Bu;Mingcheng Qu;Chun Chen
#t 2008
#c 13
#% 769892
#% 828958
#! With the booming development of the Web, popular Chinese forums enable people to find experienced customers' reviews for products. In order to get an all-around opinion about one product, users need to go through plenty of web pages, which is time-consuming and inefficient. Consequently, automatic review mining and summarization has become a hot research topic recently. However, previous approaches are not applicable for mining Chinese customer reviews. In this paper, we introduce SOPING, a Chinese customer review mining system that mines reviews from forums. Specifically, we propose a novel search-based approach to extract product features and a feature-oriented sentence orientation determination method. Our experimental results show that our proposed techniques are highly effective.

#index 1074166
#* Combining learn-based and lexicon-based techniques for sentiment detection without using labeled examples
#@ Songbo Tan;Yuefen Wang;Xueqi Cheng
#t 2008
#c 13
#% 815915
#% 854646
#! In this work, we propose a novel scheme for sentiment classification (without labeled examples) which combines the strengths of both "learn-based" and "lexicon-based" approaches as follows: we first use a lexicon-based technique to label a portion of informative examples from given task (or domain); then learn a new supervised classifier based on these labeled ones; finally apply this classifier to the task. The experimental results indicate that proposed scheme could dramatically outperform "learn-based" and "lexicon-based" techniques.

#index 1074167
#* Semi-supervised spam filtering: does it work?
#@ Mona Mojdeh;Gordon V. Cormack
#t 2008
#c 13
#% 269217
#% 961230
#! The results of the 2006 ECML/PKDD Discovery Challenge suggest that semi-supervised learning methods work well for spam filtering when the source of available labeled examples differs from those to be classified. We have attempted to reproduce these results using data from the 2005 and 2007 TREC Spam Track, and have found the opposite effect: methods like self-training and transductive support vector machines yield inferior classifiers to those constructed using supervised learning on the labeled data alone. We investigate differences between the ECML/PKDD and TREC data sets and methodologies that may account for the opposite results.

#index 1074168
#* Limits of opinion-finding baseline systems
#@ Craig Macdonald;Ben He;Iadh Ounis;Ian Soboroff
#t 2008
#c 13
#% 943811
#% 1074158
#! In opinion-finding, the retrieval system is tasked with retrieving not just relevant documents, but which also express an opinion towards the query target entity. Most opinion-finding systems are based on a two-stage approach, where initially the system aims to retrieve relevant documents, which are then re-ranked according to the extent to which they are detected to be of an opinionated nature. In this work, we investigate how the underlying 'baseline' retrieval system performance affects the overall opinion-finding performance. We apply two effective opinion-finding techniques to all the baseline runs submitted to the TREC 2007 Blog track, and draw new insights and conclusions.

#index 1074169
#* Web query translation via web log mining
#@ Rong Hu;Weizhu Chen;Peng Bai;Yansheng Lu;Zheng Chen;Qiang Yang
#t 2008
#c 13
#% 280826
#% 420520
#% 561160
#! This paper describes a method to automatically acquire query translation pairs by mining web click-through data. The extraction requires no crawling or Chinese words segmentation, and can capture popular translations. Experimental results on a real click-through data show that only 17.4% of the extracted queries are in the dictionary, and our method can achieve 62.2% (in top-1) to 80.0% (in top-5) precision in translating web queries. Moreover, the extracted translations are semantically relevant to the source query, which is particularly useful for Cross-Lingual Information Retrieval (CLIR).

#index 1074170
#* Analyzing web text association to disambiguate abbreviation in queries
#@ Xing Wei;Fuchun Peng;Benoit Dumoulin
#t 2008
#c 13
#% 340901
#% 722904
#% 818240
#% 879587
#% 1392435
#! We introduce a statistical model for abbreviation disambiguation in Web search, based on analysis of Web data resources, including anchor text, click log and query log. By combining evidence from multiple sources, we are able to accurately disambiguate the abbreviation in queries. Experiments on real Web search queries show promising results.

#index 1074171
#* Bloggers as experts: feed distillation using expert retrieval models
#@ Krisztian Balog;Maarten de Rijke;Wouter Weerkamp
#t 2008
#c 13
#% 879570
#% 907525
#% 913206
#! We address the task of (blog) feed distillation: to find blogs that are principally devoted to a given topic. The task may be viewed as an association finding task, between topics and bloggers. Under this view, it resembles the expert finding task, for which a range of models have been proposed. We adopt two language modeling-based approaches to expert finding, and determine their effectiveness as feed distillation strategies. The two models capture the idea that a human will often search for key blogs by spotting highly relevant posts (the Posting model) or by taking global aspects of the blog into account (the Blogger model). Results show the Blogger model outperforms the Posting model and delivers state-of-the art performance, out-of-the-box.

#index 1074172
#* Search effectiveness with a breadth-first crawl
#@ Dennis Fetterly;Nick Craswell;Vishwa Vinay
#t 2008
#c 13
#% 268087
#% 330609
#% 453327
#% 818255
#% 1022233
#! Previous scalability experiments found that early precision improves as collection size increases. However, that was under the assumption that a collection's documents are all sampled with uniform probability from the same population. We contrast this to a large breadth-first web crawl, an important scenario in real-world Web search, where the early documents have quite different characteristics from the later documents.

#index 1074173
#* Guide focused crawler efficiently and effectively using on-line topical importance estimation
#@ Ziyu Guan;Can Wang;Chun Chen;Jiajun Bu;Junfeng Wang
#t 2008
#c 13
#% 281251
#% 577330
#% 835231
#% 843879
#! Focused crawling is a critical technique for topical resource discovery on the Web. We propose a new frontier prioritizing algorithm, namely, the OTIE (On-line Topical Importance Estimation) algorithm, which efficiently and effectively combines link-based and content-based analysis to evaluate the priority of an uncrawled URL in the frontier. We then demonstrate OTIE's advantages over traditional prioritizing algorithms by real crawling experiments.

#index 1074174
#* Web page retrieval in ubiquitous sensor environments
#@ Takuya Maekawa;Yutaka Yanagisawa;Yasushi Sakurai;Yasue Kishino;Koji Kamei;Takeshi Okadome
#t 2008
#c 13
#% 187271
#% 268078
#% 577300
#% 728107
#! This paper proposes new concept of query free web search for daily living. We ordinarily benefit from additional information about our daily activities that we are currently engaged in. When washing a coffee maker, for example, we receive the benefit if we obtain such information as 'cleaning a coffee maker with vinegar removes its stain well.' Our proposed method automatically searches for a web page including such information relates to an activity of daily living when the activity is performed. We assume that wireless sensor nodes are attached to daily objects to detect object use; our method makes a query from the names of objects which are used. Then, the method retrieves a web page relates to the activity of daily living by using the query.

#index 1074175
#* Automatic document prior feature selection for web retrieval
#@ Jie Peng;Craig Macdonald;Iadh Ounis
#t 2008
#c 13
#% 340934
#% 397126
#% 1674994
#! Document prior features, such as Pagerank and URL depth, can improve the retrieval effectiveness of Web Information Retrieval (IR) systems. However, not all queries equally benefit from the application of a document prior feature. This paper aims to investigate whether the retrieval performance can be further enhanced by selecting the best document prior feature on a per-query basis. We present a novel method for selecting the best document prior feature on a per-query basis. We evaluate our technique on the TREC .GOV Web test collection and its associated TREC 2003 Web search tasks. Our experiments demonstrate the effectiveness and robustness of our proposed selection method.

#index 1074176
#* Using parsimonious language models on web data
#@ Rianne Kaptein;Rongmei LI;Djoerd Hiemstra;Jaap Kamps
#t 2008
#c 13
#% 766429
#! In this paper we explore the use of parsimonious language models for web retrieval. These models are smaller thus more efficient than the standard language models and are therefore well suited for large-scale web retrieval. We have conducted experiments on four TREC topic sets, and found that the parsimonious language model results in improvement of retrieval effectiveness over the standard language model for all data-sets and measures. In all cases the improvement is significant, and more substantial than in earlier experiments on newspaper/newswire data.

#index 1074177
#* Query preprocessing: improving web search through a Vietnamese word tokenization approach
#@ Doan Nguyen
#t 2008
#c 13
#% 956612
#! In this poster paper, we propose a novel approach to improve web search relevancy by tokenizing a Vietnamese query text prior submitting it to a search engine. Evaluations demonstrate its effectiveness and practical value.

#index 1074178
#* AdImage: video advertising by image matching and ad scheduling optimization
#@ Wei-Shing Liao;Kuan-Ting Chen;Winston H. Hsu
#t 2008
#c 13
#% 281730
#% 760805
#% 836518
#% 997189

#index 1074179
#* Bag-of-visual-words expansion using visual relatedness for video indexing
#@ Yu-Gang Jiang;Chong-Wah Ngo
#t 2008
#c 13
#% 724320
#% 760805
#% 945194
#% 990321
#! Bag-of-visual-words (BoW) has been popular for visual classification in recent years. In this paper, we propose a novel BoW expansion method to alleviate the effect of visual word correlation problem. We achieve this by diffusing the weights of visual words in BoW based on visual word relatedness, which is rigorously defined within a visual ontology. The proposed method is tested in video indexing experiment on TRECVID-2006 video retrieval benchmark, and an improvement of 7% over the traditional BoW is reported.

#index 1074180
#* A word shape coding method for camera-based document images
#@ Linlin Li;Chew Lim Tan
#t 2008
#c 13
#% 263214
#% 399594
#% 443727
#% 778730
#% 940443
#! This paper reports a word shape coding method to facilitate retrieval of camera-based document images without OCR. Due to perspective distortion, many reported word shape coding methods fail on camera-based images. In this paper, the problem is addressed by approximating the perspective transformation with an affine transformation, and employing an affine invariant, namely length ratio, to represent the connected components. Components in a document image are classified into a few clusters, each of which is assigned with a representative symbol. Retrieval are based on "words" comprising of symbols. The experiment results showed that the proposed method achieved an average retrieval precision of 93.43% and recall of 94.22%.

#index 1074181
#* Term clouds as surrogates for user generated speech
#@ Manos Tsagkias;Martha Larson;Maarten de Rijke
#t 2008
#c 13
#% 501923
#% 902258
#% 955013
#% 956649
#% 956704
#% 987388
#! User generated spoken audio remains a challenge for Automatic Speech Recognition (ASR) technology and content-based audio surrogates derived from ASR-transcripts must be error robust. An investigation of the use of term clouds as surrogates for podcasts demonstrates that ASR term clouds closely approximate term clouds derived from human-generated transcripts across a range of cloud sizes. A user study confirms the conclusion that ASR-clouds are viable surrogates for depicting the content of podcasts.

#index 1074182
#* A faceted interface for multimedia search
#@ Robert Villa;Nicholas Gildea;Joemon M. Jose
#t 2008
#c 13
#% 857477
#! With the rapid increase in online video services, video retrieval systems are becoming increasingly important search tools to many users in many different fields. In this poster we present a novel video retrieval interface, which supports the creation of multiple search "facets", to aid users carrying out complex, multi-faceted search tasks. The interface allows multiple searches to be executed and viewed simultaneously, and allows material to be reorganized between the facets. An experiment is presented which compares the faceted interface to a tabbed interface similar to that on modern web browsers, and some preliminary results are given.

#index 1074183
#* WISA: a novel web image semantic analysis system
#@ Hongtao Xu;Xiangdong Zhou;Lan Lin
#t 2008
#c 13
#% 232710
#% 905199
#% 957856
#% 990253
#! We present a novel Web Image Semantic Analysis (WISA) system, which explores the problem of adaptively modeling the distributions of the semantic labels of the web image on its surrounding text. To deal with this problem, we employ a new piecewise penalty weighted regression model to learn the weights of the contributions of the different parts of the surrounding text to the semantic labels of images. Experimental results on a real web image data set show that it can improve the performance of web image semantic annotation significantly.

#index 1074184
#* One-button search extracts wider interests: an empirical study with video bookmarking search
#@ Masayuki Okamoto;Masaaki Kikuchi;Tomohiro Yamasaki
#t 2008
#c 13
#% 292181
#% 987321
#% 1008126
#% 1698881
#! This poster presents an overview of the characteristics of a one-button information retrieval interface with closed captions from TV watching activities, which is intended to lighten the burden of remembering and entering query terms while watching TV. We investigated this interface with an experimental system named Video Bookmarking Search, which estimates query terms from closed captions with named-entity recognition and sentence labeling techniques. According to an empirical evaluation for 1,138 search queries from 206 bookmarks using seven actual TV shows on city life, travel, health, and cuisine, we found wider queries and search results are acceptable through the query-input-free interface, despite the fact that the number of queries and search results that are directly relevant to the users' original intentions is not high. The main reason is a watching user's interest is wider than what is expressed with query terms.

#index 1074185
#* Product retrieval for grocery stores
#@ Petteri Nurmi;Eemil Lagerspetz;Wray Buntine;Patrik Floréen;Joonas Kukkonen
#t 2008
#c 13
#% 452633
#% 641930
#% 783474
#! We introduce a grocery retrieval system that maps shopping lists written in natural language into actual products in a grocery store. We have developed the system using nine months of shopping basket data from a large Finnish supermarket. To evaluate the system, we used 70 real shopping lists gathered from customers of the supermarket. Our system achieves over 80% precision for products at rank one, and the precision is around 70% for products at rank 5.

#index 1074186
#* A reranking model for genomics aspect search
#@ Qinmin Hu;Xiangji Huang
#t 2008
#c 13
#! In this paper, we propose a reranking model to improve the aspect-level performance in the biomedical domain. This model iteratively computes the maximum hidden aspect for every retrieved passage and then reranks these passages from aspect subsets. The experimental results show the improvements of the aspect-level performance up to 27.14% for 2006 Genomics topics and 27.09% for 2007 Genomics topics.

#index 1074187
#* Improving biomedical document retrieval using domain knowledge
#@ Shuguang Wang;Milos Hauskrecht
#t 2008
#c 13
#% 280819
#% 466574
#% 907543
#% 987274
#! Research articles typically introduce new results or findings and relate them to knowledge entities of immediate relevance. However, a large body of context knowledge related to the results is often not explicitly mentioned in the article. To overcome this limitation the state-of-the-art information retrieval approaches rely on the latent semantic analysis in which terms in articles are projected to a lower dimensional latent space and best possible matches in this space are identified. However, this approach may not perform well enough if the number of explicit knowledge entities in the articles is too small compared to the amount of knowledge in the domain. We address the problem by exploiting a domain knowledge layer, a rich network of relations among knowledge entities in the domain extracted from a large corpus of documents. The knowledge layer supplies the context knowledge that lets us relate different knowledge entities and hence improve the information retrieval performance. We develop and study a new framework for i) learning and aggregating the relations in the knowledge layer from the literature corpus; ii) and for exploiting these relations to improve the information-retrieval of relevant documents.

#index 1074188
#* Kleio: a knowledge-enriched information retrieval system for biology
#@ Chikashi Nobata;Philip Cotter;Naoaki Okazaki;Brian Rea;Yutaka Sasaki;Yoshimasa Tsuruoka;Jun'ichi Tsujii;Sophia Ananiadou
#t 2008
#c 13
#% 910657
#% 920159
#% 939559
#% 1041329
#! Kleio is an advanced information retrieval (IR) system developed at the UK National Centre for Text Mining (NaCTeM)1. The system offers textual and metadata searches across MEDLINE and provides enhanced searching functionality by leveraging terminology management technologies.

#index 1074189
#* Enhancing keyword-based botanical information retrieval with information extraction
#@ Xiaoya Tang
#t 2008
#c 13
#% 278109
#% 730063
#! Keyword-based retrieval matches search terms and documents via term co-occurrence. Such an approach does not allow matching based on the specific plant characteristic descriptions that are often used in botanical text retrieval. This study applies information extraction techniques to automatically extract plant characteristic information from text and allows users to search using such information in combination with keywords. An evaluation experiment was conducted using actual users. The results indicate that this approach enhances task-based retrieval performance.

#index 1074190
#* How medical expertise influences web search interaction
#@ Ryen W. White;Susan Dumais;Jaime Teevan
#t 2008
#c 13
#% 345262
#% 751596
#% 807420
#% 1275193
#! Domain expertise can have an important influence on how people search. In this poster we present findings from a log-based study into how medical domain experts search the Web for information related to their expertise, as compared with non-experts. We find differences in sites visited, query vocabulary, and search behavior. The findings have implications for the automatic identification of domain experts from interaction logs, and the use of domain knowledge in applications such as query suggestion or page recommendation to support non-experts.

#index 1074191
#* Generating diverse katakana variants based on phonemic mapping
#@ Kazuhiro Seki;Hiroyuki Hattori;Kuniaki Uehara
#t 2008
#c 13
#% 741114
#! In Japanese, it is quite common for the same word to be written in several different ways. This is especially true for katakana words which are typically used for transliterating foreign languages. This ambiguity becomes critical for automatic processing such as information retrieval (IR). To tackle this problem, we propose a simple but effective approach to generating katakana variants by considering phonemic representation of the original language for a given word. The proposed approach is evaluated through an assessment of the variants it generates. Also, the impact of the generated variants on IR is studied in comparison to an existing approach using katakana rewriting rules.

#index 1074192
#* Exploiting sequential dependencies for expert finding
#@ Pavel Serdyukov;Henning Rode;Djoerd Hiemstra
#t 2008
#c 13
#% 340901
#% 397146
#% 907525
#% 987231
#% 987261
#% 1019135
#! We propose an expert finding method based on assumption of sequential dependence between a candidate expert and the query terms in the scope of a document. We assume that the strength of relation of a candidate to the document's content depends on its position in this document with respect to the positions of the query terms. The experiments on the official Enterprise TREC data demonstrate the advantage of our method over the method based on independence of query terms and persons in a document.

#index 1074193
#* Modeling expert finding as an absorbing random walk
#@ Pavel Serdyukov;Henning Rode;Djoerd Hiemstra
#t 2008
#c 13
#% 869649
#% 907525
#% 987261
#! We introduce a novel approach to expert finding based on multi-step relevance propagation from documents to related candidates. Relevance propagation is modeled with an absorbing random walk. The evaluation on the two official Enterprise TREC data sets demonstrates the advantage of our method over the state-of-the-art method based on one-step propagation.

#index 1074194
#* A scalable assistant librarian: hierarchical subject classification of books
#@ Steven P. Crain;Jian Huang;Hongyuan Zha
#t 2008
#c 13
#% 770763
#% 770796
#% 1392455
#! In this paper, we discuss our work in progress towards a scalable hierarchical classification system for books using the Library of Congress subject hierarchy. We examine the characteristics of this domain which make the problem very challenging, and we look at several appropriate performance measurements. We show that both Hieron and Hierarchical Support Vector Machines perform moderately well.

#index 1074195
#* Information retrieval on bug locations by learning co-located bug report clusters
#@ Ing-Xiang Chen;Hojun Jaygarl;Cheng-Zen Yang;Ping-Jung Wu
#t 2008
#c 13
#% 296646
#% 723328
#% 754105
#% 800130
#! Bug locating usually involves intensive search activities and incurs unpredictable cost of labor and time. An issue of information retrieval on bug locations is particularly addressed to facilitate identifying bugs from software code. In this paper, a novel bug retrieval approach with co-location shrinkage (CS) is proposed. The proposed approach has been implemented in open-source software projects collected from real-world repositories, and consistently improves the retrieval accuracy of a state-of-the-art Support Vector Machine (SVM) model.

#index 1074196
#* Summarization of compressed text images: an experience on Indic script documents
#@ Utpal Garain
#t 2008
#c 13
#% 263214
#% 263223
#% 731007
#% 854190
#% 900333
#% 989714
#! Automatic summarization of JBIG2 coded textual images is discussed. Compressed images are partially decompressed to compute relevant features. The feature extraction method is free from using any character recognition module. Summary sentences are ranked. Experiment considers documents in Indic scripts that lack in having any efficient OCR systems. Script independent aspect of the approach is highlighted through use of two most popular Indic scripts. Sentence selection efficiency of about 61% is achieved when judged against man-made summarization. A nonparametric (distribution-free) rank statistic shows a correlation coefficient of 0.33 as a measure of the (minimum) strength of the associations between sentence ranking by machine and human.

#index 1074197
#* A method for transferring retrieval scores between collections with non-overlapping vocabularies
#@ Fernando D. Diaz
#t 2008
#c 13
#% 397145
#% 1002316
#! We present a method for projecting retrieval scores across two corpora with a shared, parallel corpus.

#index 1074198
#* Improving relevance feedback in language modeling with score regularization
#@ Fernando D. Diaz
#t 2008
#c 13
#% 118728
#% 153019
#% 223810
#% 827581
#% 1002316
#% 1019183
#! We demonstrate that regularization can improve feedback in a language modeling framework.

#index 1074199
#* Theoretical bounds on and empirical robustness of score regularization to different similarity measures
#@ Fernando D. Diaz
#t 2008
#c 13
#% 375017
#% 857180
#% 1002316
#! We present theoretical bounds and empirical robustness of score regularization given changes in the similarity measure.

#index 1074200
#* A study of query length
#@ Avi Arampatzis;Jaap Kamps
#t 2008
#c 13
#% 815861
#% 878624
#% 987249
#! We analyse query length, and fit power-law and Poisson distributions to four different query sets. We provide a practical model for query length, based on the truncation of a Poisson distribution for short queries and a power-law distribution for longer queries, that better fits real query length distributions than earlier proposals.

#index 1074201
#* Don't have a stemmer?: be un+concern+ed
#@ Paul McNamee;Charles Nicholas;James Mayfield
#t 2008
#c 13
#% 144034
#% 208934
#% 732848
#% 854592
#! The choice of indexing terms used to represent documents crucially determines how e ective subsequent retrieval will be. IR systems commonly use rule-based stemmers to normalize surface word forms to combat the problem of not finding documents that contain words related to query terms by inflectional or derivational morphology. But such stemmers are not available in all languages. In this paper we explore the effectiveness of unsupervised morphological segmentation as an alternative to stemming using test sets in thirteen European languages. We find that unsupervised segmentation is significantly better than unnormalized words, in several cases by more than 20%. However, rule-based stemming, if available, is better in low complexity languages. We also compare these methods to the use of character n-grams, finding that on average n-grams yield the best performance.

#index 1074202
#* Parsimonious concept modeling
#@ Edgar Meij;Dolf Trieschnigg;Maarten de Rijke;Wessel Kraaij
#t 2008
#c 13
#% 287205
#% 340901
#% 766429
#% 1002315
#% 1275012

#index 1074203
#* Parsimonious relevance models
#@ Edgar Meij;Wouter Weerkamp;Krisztian Balog;Maarten de Rijke
#t 2008
#c 13
#% 218978
#% 232644
#% 262096
#% 340901
#% 342707
#% 642985
#% 766429
#% 810906
#% 818204
#! We describe a method for applying parsimonious language models to re-estimate the term probabilities assigned by relevance models. We apply our method to six topic sets from test collections in five different genres. Our parsimonious relevance models (i) improve retrieval effectiveness in terms of MAP on all collections, (ii) significantly outperform their non-parsimonious counterparts on most measures, and (iii) have a precision enhancing effect, unlike other blind relevance feedback methods.

#index 1074204
#* Author-topic evolution analysis using three-way non-negative Paratucker
#@ Wei Peng;Tao Li
#t 2008
#c 13
#% 722902
#% 729918
#% 881468
#% 1116992
#! Analyzing three-way data has attracted a lot of attention recently due to the intrinsic rich structures in real-world datasets. The PARATUCKER model has been proposed to combine the axis capabilities of the Parafac model and the structural generality of the Tucker model. However, no algorithms have been developed for fitting the PARATUCKER model. In this paper, we propose TANPT algorithm to solve the PARATUCKER model. We apply the algorithm for temporal relation co-clustering on author-topic evolution. Experiments on DBLP datasets demonstrate its effectiveness.

#index 1074205
#* Exploiting proximity feature in bigram language model for information retrieval
#@ Seung-Hoon Na;Jungi Kim;In-Su Kang;Jong-Hyeok Lee
#t 2008
#c 13
#% 287253
#% 340948
#% 766428
#% 987229
#! Language modeling approaches have been effectively dealing with the dependency among query terms based on N-gram such as bigram or trigram models. However, bigram language models suffer from adjacency-sparseness problem which means that dependent terms are not always adjacent in documents, but can be far from each other, sometimes with distance of a few sentences in a document. To resolve the adjacency-sparseness problem, this paper proposes a new type of bigram language model by explicitly incorporating the proximity feature between two adjacent terms in a query. Experimental results on three test collections show that the proposed bigram language model significantly improves previous bigram model as well as Tao's approach, the state-of-art method for proximity-based method.

#index 1074206
#* Measuring concept relatedness using language models
#@ Dolf Trieschnigg;Edgar Meij;Maarten de Rijke;Wessel Kraaij
#t 2008
#c 13
#% 279755
#% 375017
#% 465914
#% 764597
#% 958367
#! Over the years, the notion of concept relatedness has attracted considerable attention. A variety of approaches, based on ontology structure, information content, association, or context have been proposed to indicate the relatedness of abstract ideas. We propose a method based on the cross entropy reduction between language models of concepts which are estimated based on document-concept assignments. The approach shows improved or competitive results compared to state-of-the-art methods on two test sets in the biomedical domain.

#index 1074207
#* Query-drift prevention for robust query expansion
#@ Liron Zighelnic;Oren Kurland
#t 2008
#c 13
#% 144076
#% 262084
#% 340899
#% 340901
#% 340948
#% 342707
#% 766431
#% 987230
#! Pseudo-feedback-based automatic query expansion yields effective retrieval performance on average, but results in performance inferior to that of using the original query for many information needs. We address an important cause of this robustness issue, namely, the query drift problem, by fusing the results retrieved in response to the original query and to its expanded form. Our approach posts performance that is significantly better than that of retrieval based only on the original query and more robust than that of retrieval using the expanded query.

#index 1074208
#* Adaptive label-driven scaling for latent semantic indexing
#@ Xiaojun Quan;Enhong Chen;Qiming Luo;Hui Xiong
#t 2008
#c 13
#% 280822
#% 987312
#% 1275008
#! This paper targets on enhancing Latent Semantic Indexing (LSI) by exploiting category labels. Specifically, in the term-document matrix, the vector for each term either appearing in labels or semantically close to labels is scaled before performing Singular Value Decomposition (SVD) to boost its impact on the generated left singular vectors. As a result, the similarities among documents in the same category are increased. Furthermore, an adaptive scaling strategy is designed to better utilize the hierarchical structure of categories. Experimental results show that the proposed approach is able to significantly improve the performance of hierarchical text categorization.

#index 1074209
#* Fixed-threshold SMO for Joint Constraint Learning Algorithm of Structural SVM
#@ Changki Lee;HyunKi Kim;Myung-Gil Jang
#t 2008
#c 13
#% 340903
#% 770763
#% 881477
#% 987226
#% 987244
#! In this paper, we describe a fixed-threshold sequential minimal optimization (FSMO) for a joint constraint learning algorithm of structural classification SVM problems. Because FSMO uses the fact that the joint constraint formulation of structural SVM has b=0, FSMO breaks down the quadratic programming (QP) problems of structural SVM into a series of smallest QP problems, each involving only one variable. By using only one variable, FSMO is advantageous in that each QP sub-problem does not need subset selection.

#index 1074210
#* Posterior probabilistic clustering using NMF
#@ Chris Ding;Tao Li;Dijun Luo;Wei Peng
#t 2008
#c 13
#% 280819
#% 643008
#% 881468
#! We introduce the posterior probabilistic clustering (PPC), which provides a rigorous posterior probability interpretation for Nonnegative Matrix Factorization (NMF) and removes the uncertainty in clustering assignment. Furthermore, PPC is closely related to probabilistic latent semantic indexing (PLSI).

#index 1074211
#* On document splitting in passage detection
#@ Nazli Goharian;Saket S.R. Mengle
#t 2008
#c 13
#% 232677
#% 1052670
#! Passages can be hidden within a text to circumvent their disallowed transfer. Such release of compartmentalized information is of concern to all corporate and governmental organization. We explore the methodology to detect such hidden passages within a document. A document is divided into passages using various document splitting techniques, and a text classifier is used to categorize such passages. We present a novel document splitting technique called dynamic windowing, which significantly improves precision, recall and F1 measure.

#index 1074212
#* Learning with support vector machines for query-by-multiple-examples
#@ Dell Zhang;Wee Sun Lee
#t 2008
#c 13
#% 252011
#% 840882
#% 881477
#% 936239
#% 1077150
#! We explore an alternative Information Retrieval paradigm called Query-By-Multiple-Examples (QBME) where the information need is described not by a set of terms but by a set of documents. Intuitive ideas for QBME include using the centroid of these documents or the well-known Rocchio algorithm to construct the query vector. We consider this problem from the perspective of text classification, and find that a better query vector can be obtained through learning with Support Vector Machines (SVMs). For online queries, we show how SVMs can be learned from one-class examples in linear time. For offline queries, we show how SVMs can be learned from positive and unlabeled examples together in linear or polynomial time. The effectiveness and efficiency of the proposed approaches have been confirmed by our experiments on four real-world datasets.

#index 1074213
#* Question classification with semantic tree kernel
#@ Yan Pan;Yong Tang;Luxin Lin;Yemin Luo
#t 2008
#c 13
#% 642977
#% 815303
#% 879648
#% 905498
#! Question Classification plays an important role in most Question Answering systems. In this paper, we exploit semantic features in Support Vector Machines (SVMs) for Question Classification. We propose a semantic tree kernel to incorporate semantic similarity information. A diverse set of semantic features is evaluated. Experimental results show that SVMs with semantic features, especially semantic classes, can significantly outperform the state-of-the-art systems.

#index 1074214
#* Generalising multiple capture-recapture to non-uniform sample sizes
#@ Paul Thomas
#t 2008
#c 13
#% 268114
#% 340146
#% 342741
#% 879604
#% 987255
#! Algorithms in distributed information retrieval often rely on accurate knowledge of the size of a collection. The "multiple capture-recapture" method of Shokouhi et al. is one of the more reliable algorithms for determining collection size, but it relies on samples with a uniform number of documents. Such uniform samples are often hard to obtain in a working system. A simple generalisation of multiple capture-recapture does not rely on uniform sample sizes. Simulations show it is as accurate as the original method even when sample sizes vary considerably, making it a useful technique in real tools.

#index 1074215
#* Predicting when browsing context is relevant to search
#@ Mandar Rahurkar;Silviu Cucerzan
#t 2008
#c 13
#% 449288
#% 643028
#% 754126
#% 835027
#% 869536
#% 987193
#% 987313
#! We investigate a representative case of sudden information need change of Web users. By analyzing search engine query logs, we show that the majority of queries submitted by users after browsing documents in the news domain are related to the most recently browsed document. We investigate ways of identifying whether a query is a good candidate for contextualization conditioned on the most recently browsed document by a user. We build a successful classifier for this task, which achieves 96% precision at 90% recall.

#index 1074216
#* XML-aided phrase indexing for hypertext documents
#@ Miro Lehtonen;Antoine Doucet
#t 2008
#c 13
#% 78171
#% 629631
#% 1100810
#% 1264302
#% 1414372
#% 1715620
#! We combine techniques of XML Mining and Text Mining for the benefit of Information Retrieval. By manipulating the word sequence according to the XML structure of the marked-up text, we strengthen phrase boundaries so that they are more obvious to the algorithms that extract multiword sequences from text. Consequently, the quality of the indexed phrases improves, which has a positive effect on the average precision measured by the INEX 2007 standards.

#index 1074217
#* Proximity-aware scoring for XML retrieval
#@ Andreas Broschart;Ralf Schenkel
#t 2008
#c 13
#% 280834
#% 879651
#% 1098409
#% 1387547
#! Proximity-aware scoring functions lead to significant effectiveness improvements for text retrieval. For XML IR, we can sometimes enhance the retrieval quality by exploiting knowledge about the document structure combined with established text IR methods. This paper introduces modified proximity scores that take the document structure into account and demonstrates the effect for the INEX benchmark.

#index 1074218
#* Locating relevant text within XML documents
#@ Jaap Kamps;Marijn Koolen;Mounia Lalmas
#t 2008
#c 13
#% 878916
#% 987309
#! Traditional document retrieval has shown to be a competitive approach in XML element retrieval, which is counter-intuitive since the element retrieval task requests all and only relevant document parts to be retrieved. This paper conducts a comparative analysis of document and element retrieval, highlights the relative strengths and weaknesses of both approaches, and explains the relative effectiveness of document retrieval approaches at element retrieval tasks.

#index 1074219
#* A flexible extension of XPath to improve XML querying
#@ Ernesto Damiani;Stefania Marrara;Gabriella Pasi
#t 2008
#c 13
#% 169811
#% 340914
#% 799245
#% 869672
#! This work presents a flexible XML selection language, FleXPath which allows the formulation of flexible constraints on both structure and content of XML documents. Some experimental results, obtained with a preliminary prototype, are described in order to show that the idea promises good results.

#index 1074220
#* Combining document- and paragraph-based entity ranking
#@ Henning Rode;Pavel Serdyukov;Djoerd Hiemstra
#t 2008
#c 13
#% 879570
#% 1019135
#% 1100827
#! We study entity ranking on the INEX entity track and propose a simple graph-based ranking approach that enables to combine scores on document and paragraph level. The combined approach improves the retrieval results not only on the INEX testset, but similarly on TREC's expert finding task.

#index 1074221
#* Re-ranking search results using document-passage graphs
#@ Michael Bendersky;Oren Kurland
#t 2008
#c 13
#% 144011
#% 169809
#% 340948
#% 413592
#% 766430
#% 818241
#% 838528
#% 879575
#% 939968
#% 1043052
#! We present a novel passage-based approach to re-ranking documents in an initially retrieved list so as to improve precision at top ranks. While most work on passage-based document retrieval ranks a document based on the query similarity of its constituent passages, our approach leverages information about the centrality of the document passages with respect to the initial document list. Passage centrality is induced over a bipartite document-passage graph, wherein edge weights represent document-passage similarities. Empirical evaluation shows that our approach yields effective re-ranking performance. Furthermore, the performance is superior to that of previously proposed passage-based document ranking methods.

#index 1074222
#* Utilizing phrase based semantic information for term dependency
#@ Yang Xu;Fan Ding;Bin Wang
#t 2008
#c 13
#% 287253
#% 766428
#% 818262
#% 1019084
#! Previous work on term dependency has not taken into account semantic information underlying query phrases. In this work, we study the impact of utilizing phrase based concepts for term dependency. We use Wikipedia to separate important and less important term dependencies, and treat them accordingly as features in a linear feature-based retrieval model. We compare our method with a Markov Random Field (MRF) model on four TREC document collections. Our experimental results show that utilizing phrase based concepts improves the retrieval effectiveness of term dependency, and reduces the size of the feature set to large extent.

#index 1074223
#* Inferring the most important types of a query: a semantic approach
#@ David Vallet;Hugo Zaragoza
#t 2008
#c 13
#% 956703
#% 1019189
#! In this paper we present a technique for ranking the most important types or categories for a given query. Rather than trying to find the category of the query, known as query categorization, our approach seeks to find the most important types related to the query results. Not necessarily the query category falls into this ranking of types and therefore our approach can be complementary.

#index 1074224
#* On multiword entity ranking in peer-to-peer search
#@ Yuval Merhav;Ophir Frieder
#t 2008
#c 13
#% 1052723
#! Previously [2], we postulated the advantage of using entity extraction to implement a new Peer-to-Peer (P2P) search framework for reducing network traffic and providing a trade off between precision and recall. We now propose an entity ranking method designed for the 'short documents' characteristic of P2P, which significantly improves both precision and recall in 'top results' P2P search. We construct a dynamic entity corpus using n-grams statistics and metadata, study its reliability, and use it to identify correlations between user query terms.

#index 1074225
#* Site-based dynamic pruning for query processing in search engines
#@ Ismail Sengor Altingovde;Engin Demir;Fazli Can;Özgür Ulusoy
#t 2008
#c 13
#% 213786
#% 330678
#% 340887
#% 787544
#% 818230
#% 987214
#% 1051061
#% 1392439
#! Web search engines typically index and retrieve at the page level. In this study, we investigate a dynamic pruning strategy that allows the query processor to first determine the most promising websites and then proceed with the similarity computations for those pages only within these sites.

#index 1074226
#* Exploiting MDS Projections for Cross-language IR
#@ Rafael E. Banchs;Andreas Kaltenbrunner
#t 2008
#c 13
#% 807745
#! In this paper, we describe some preliminary work on using monolingual projections of document collections for performing cross-language information retrieval tasks. The proposed methodology uses multidimensional scaling for projecting the vector-space representations of a given multilingual document collection into spaces of lower dimensionality. An independent projection is computed for each different language, and the structural similarities of the resulting projections are exploited for information retrieval tasks.

#index 1074227
#* Local approximation of PageRank and reverse PageRank
#@ Ziv Bar-Yossef;Li-Tal Mashiach
#t 2008
#c 13
#% 783528
#% 1016177
#! We consider the problem of approximating the PageRank of a target node using only local information provided by a link server. We prove that local approximation of PageRank is feasible if and only if the graph has low in-degree and admits fast PageRank convergence. While natural graphs, such as the web graph, are abundant with high in-degree nodes, making local PageRank approximation too costly, we show that reverse natural graphs tend to have low indegree while maintaining fast PageRank convergence. It follows that calculating Reverse PageRank locally is frequently more feasible than computing PageRank locally. Finally, we demonstrate the usefulness of Reverse PageRank in five different applications.

#index 1074228
#* Improving text classification accuracy using topic modeling over an additional corpus
#@ Somnath Banerjee
#t 2008
#c 13
#% 722904
#% 763708
#% 812535
#% 1250362
#! The World Wide Web has many document repositories that can act as valuable sources of additional data for various machine learning tasks. In this paper, we propose a method of improving text classification accuracy by using such an additional corpus that can easily be obtained from the web. This additional corpus can be unlabeled and independent of the given classification task. The method proposed here uses topic modeling to extract a set of topics from the additional corpus. Those extracted topics then act as additional features of the data of the given classification task. An evaluation on the RCV1 dataset shows significant improvement over a baseline method.

#index 1074229
#* An algorithm for text categorization
#@ Anestis Gkanogiannis;Theodore Kalamboukis
#t 2008
#c 13
#% 248225
#! A novel and efficient learning algorithm is proposed for the binary linear classification problem. The algorithm is trained using the Rocchio's relevance feedback technique and builds a classifier by the intermediate hyperplane of two common tangent hyperplanes for the given category and its complement. Experimental results presented are very encouraging and justify the need for further research.

#index 1074230
#* Hypergraph partitioning for document clustering: a unified clique perspective
#@ Tianming Hu;Hui Xiong;Wenjun Zhou;Sam Yuan Sung;Hangzai Luo
#t 2008
#c 13
#% 300163
#% 304423
#% 413610
#% 754412
#% 893372
#! Hypergraph partitioning has been considered as a promising method to address the challenges of high dimensionality in document clustering. With documents modeled as vertices and the relationship among documents captured by the hyperedges, the goal of graph partitioning is to minimize the edge cut. Therefore, the definition of hyperedges is vital to the clustering performance. While several definitions of hyperedges have been proposed, a systematic understanding of desired characteristics of hyperedges is still missing. To that end, in this paper, we first provide a unified clique perspective of the definition of hyperedges, which serves as a guide to define hyperedges. With this perspective, based on the concepts of hypercliques and shared (reverse) nearest neighbors, we propose three new types of clique hyperedges and analyze their properties regarding purity and size issues. Finally, we present an extensive evaluation using real-world document datasets. The experimental results show that, with shared (reverse) nearest neighbor based hyperedges, the clustering performance can be improved significantly in terms of various external validation measures without the need for fine tuning of parameters.

#index 1074231
#* Pagerank based clustering of hypertext document collections
#@ Konstantin Avrachenkov;Vladimir Dobrynin;Danil Nemirovsky;Son Kim Pham;Elena Smirnova
#t 2008
#c 13
#% 249110
#% 348173
#% 1404189
#% 1663632
#! Clustering hypertext document collection is an important task in Information Retrieval. Most clustering methods are based on document content and do not take into account the hyper-text links. Here we propose a novel PageRank based clustering (PRC) algorithm which uses the hypertext structure. The PRC algorithm produces graph partitioning with high modularity and coverage. The comparison of the PRC algorithm with two content based clustering algorithms shows that there is a good match between PRC clustering and content based clustering.

#index 1074232
#* An alignment-based pattern representation model for information extraction
#@ Seokhwan Kim;Minwoo Jeong;Gary Geunbae Lee
#t 2008
#c 13
#% 742100
#% 814996
#% 815069
#% 817447
#% 1288554
#% 1410932

#index 1074233
#* Relational distance-based collaborative filtering
#@ Wei Zhang
#t 2008
#c 13
#% 266281
#% 330687
#% 805841
#! In this paper, we present a novel hybrid recommender system called RelationalCF, which integrate content and demographic information into a collaborative filtering framework by using relational distance computation approaches without the effort of form transformation and feature construction. Our experiments suggest that the effective combination of various kinds of information based on relational distance approaches provides improved accurate recommendations than other approaches.

#index 1074234
#* Minexml: bridging unstructured query with structured resources via mediated query
#@ Keng Hoon Gan;Phang Keat Keong;Saravadee Sae Tan;Tang Enya Kong
#t 2008
#c 13

#index 1074235
#* Clustering search results for mobile terminals
#@ Michiko Yasukawa;Hidetoshi Yokoo
#t 2008
#c 13
#% 1917647
#! Mobile terminals such as cell phones are much more restricted in terms of input/output functionality and, therefore, some special techniques must be incorporated to enable them to be easily used for Web searching. Further, searching for a location name is related to a dazzling variety of topics. We relate these two factors to each other to yield a new search system for map and text information. Presenting search results as clusters is helpful for users, especially in a mobile environment. The system makes mobile web searching easier and more efficient.

#index 1074236
#* Refining search results with facet landscapes
#@ Mark Sifer;Jian Lin
#t 2008
#c 13
#% 268066
#% 857482
#% 910807

#index 1074237
#* Ice-tea: an interactive cross-language search engine with translation enhancement
#@ Dan Wu;Daqing He
#t 2008
#c 13
#% 747080

#index 1074238
#* Cross-lingual search over 22 european languages
#@ Blaž Fortuna;Jan Rupnik;Boštjan Pajntar;Marko Grobelnik;Dunja Mladenič
#t 2008
#c 13
#! In this paper we present a system for cross-lingual information retrieval, which can handle tens of languages and millions of documents. Functioning of the system is demonstrated on corpus of European Legislation (22 languages, more than 400,000 documents per language). The system uses an interactive web-interface, which can take advantage of a predefined thesaurus allowing the user to dynamically re-rank the retrieval results based on the mapping onto a predefined thesaurus.

#index 1074239
#* Social recommendations at work
#@ Tom Crecelius;Mouna Kacimi;Sebastian Michel;Thomas Neumann;Josiane X. Parreira;Ralf Schenkel;Gerhard Weikum
#t 2008
#c 13
#% 734590
#! Online communities have become popular for publishing and searching content, and also for connecting to other users. User-generated content includes, for example, personal blogs, bookmarks, and digital photos. Items can be annotated and rated by different users, and users can connect to others that are usually friends and/or share common interests. We demonstrate a social recommendation system that takes advantages of users connections and tagging behavior to compute recommendations of items in such communities. The advantages can be verified via comparison to a standard IR technique.

#index 1074240
#* Bilkent news portal: a personalizable system with new event detection and tracking capabilities
#@ Fazli Can;Seyit Kocberber;Ozgur Baglioglu;Suleyman Kardas;Huseyin Cagdas Ocalan;Erkan Uyar
#t 2008
#c 13
#% 119916
#% 960275
#% 1036762

#index 1074241
#* Geographic IR and visualization in time and space
#@ Ray R. Larson
#t 2008
#c 13
#% 903380
#% 1125723
#! This demonstration will show how graphical geospatial query specifications can be used to obtain sets of georeferenced data ranked by probability of relevance, and displayed geographically and temporally in a geospatial browser with temporal support.

#index 1074242
#* Fine-grained relevance feedback for XML retrieval
#@ Hanglin Pan;Ralf Schenkel;Gerhard Weikum
#t 2008
#c 13
#% 161238
#% 430770
#% 1021954
#! This demonstration presents an XML IR system that allows users to give feedback of different granularities and types, using Dempster-Shafer theory of evidence to compute expanded and reweighted queries.

#index 1074243
#* Dynamic visualization of music classification systems
#@ Kris West;J. Stephen Downie;Xiao Hu;M. Cameron Jones
#t 2008
#c 13
#% 995366

#index 1074244
#* From concepts to implementation and visualization: tools from a team-based approach to ir
#@ Uma Murthy;Ricardo da Silva Torres;Edward A. Fox;Logambigai Venkatachalam;Seungwon Yang;Marcos A. Gonçalves
#t 2008
#c 13
#% 782504
#% 987400
#% 1682049
#! Researchers have been studying and developing teaching materials for information retrieval (IR), such as [3]. Toolkits also have been built that provide hands-on experience to students. For example, IR-Toolbox [4] is an effort to close the gap between the students' understanding of IR concepts and real-life indexing and search systems. Such tools might be good for helping students in non-technical areas such as in the Library and Information Science field to develop their conceptual model of search engines. However, they do not cover emerging topics and skills, such as content-based image retrieval (CBIR) and fusion search. Although there is open source software (such as those in http://www.searchtools.com/tools/tools-opensource.html) that can be used to teach basic and advanced IR topics, they require a student to have high-level technical knowledge and to spend a long time to gain a practical understanding of these topics. We present a new and rapid approach to teach basic and advanced IR topics, such as text retrieval, web-based IR, CBIR, and fusion search, to Computer Science (CS) graduate students. We designed projects that would help students grasp the abovementioned IR topics. Students, working in teams, were given a practical application to start with -- the Superimposed Application for Image Description and Retrieval [5]. SAIDR (earlier, SIERRA) allows users to associate parts of images with multimedia information such as text annotations. Also, users may retrieve information in one of two 2 ways: (1) Perform text-based retrieval on annotations; (2) Perform CBIR on images and parts of images that look like a query image (or part of a query image). Each team was asked to build an enhancement for this application, involving text retrieval and/or CBIR, in three weeks time. The sub-projects are described in Table 1. The outcome of this activity was that students learned about IR concepts while being able to relate their applicability to a real world problem (Figure 1). Details of these projects may be found at http://collab.dlib.vt.edu/runwiki/wiki.pl?TabletPcImageRetrievalSuperimposedInformation. We will demonstrate the tools developed along with the IR concepts they illustrate (Table 1). We believe these tools may aid others to learn about basic and advanced topics in IR.

#index 1074245
#* Exploiting XML structure to improve information retrieval in peer-to-peer systems
#@ Judith Winter
#t 2008
#c 13
#! With the advent of XML as a standard for representation and exchange of structured documents, a growing amount of XML-documents are being stored in Peer-to-Peer (P2P) networks. Cur¬rent research on P2P search engines proposes the use of Informa¬tion Retrieval (IR) techniques to perform content-based search, but does not take into account structural features of documents. P2P systems typically have no central index, thus avoiding single-points-of-failures, but distribute all information among participating peers. Accordingly, a querying peer has only limited access to the index information and should select carefully which peers can help answering a given query by contributing resources such as local index information or CPU time for ranking computations. Bandwidth consumption is a major issue. To guarantee scalability, P2P systems have to reduce the number of peers involved in the retrieval process. As a result, the retrieval quality in terms of recall and precision may suffer substantially. In the proposed thesis, document structure is considered as an extra source of information to improve the retrieval quality of XML-documents in a P2P environment. The thesis centres on the following questions: how can structural information help to improve the retrieval of XML-documents in terms of result quality such as precision, recall, and specificity? Can XML structure support the routing of queries in distributed environments, especially the selection of promising peers? How can XML IR techniques be used in a P2P network while minimizing bandwidth consumption and considering performance aspects? To answer these questions and to analyze possible achievements, a search engine is proposed that exploits structural hints expressed explicitly by the user or implicitly by the self-describing structure of XML-documents. Additionally, more focused and specific results are obtained by providing ranked retrieval units that can be either XML-documents as a whole or the most relevant passages of theses documents. XML information retrieval techniques are applied in two ways: to select those peers participating in the retrieval process, and to compute the relevance of documents. The indexing approach includes both content and structural information of documents. To support efficient execution of multi term queries, index keys consist of rare combinations of (content, structure)-tuples. Performance is increased by using only fixedsized posting lists: frequent index keys are combined with each other iteratively until the new combination is rare, with a posting list size under a pre-set threshold. All posting lists are sorted by taking into account classical IR measures such as term frequency and inverted term frequency as well as weights for potential retrieval units of a document, with a slight bias towards documents on peers with good collections regarding the current index key and with good peer characteristics such as online times, available bandwidth, and latency. When extracting the posting list for a specific query, a re-ordering on the posting list is performed that takes into account the structural similarity between key and query. According to this preranking, peers are selected that are expected to hold information about potentially relevant documents and retrieval units The final ranking is computed in parallel on those selected peers. The computation is based on an extension of the vector space model and distinguishes between weights for different structures of the same content. This allows weighting XML elements with respect to their discriminative power, e.g. a title will be weighted much higher than a footnote. Additionally, relevance is computed as a mixture of content relevance and structural similarity between a given query and a potential retrieval unit. Currently, a first prototype for P2P Information Retrieval of XML-documents called SPIRIX is being implemented. Experiments to evaluate the proposed techniques and use of structural hints will be performed on a distributed version of the INEX Wikipedia Collection.

#index 1074246
#* Affective feedback: an investigation into the role of emotions in the information seeking process
#@ Ioannis Arapakis
#t 2008
#c 13
#% 214709
#! User feedback is considered to be a critical element in the information seeking process. An important aspect of the feedback cycle is relevance assessment that has progressively become a popular practice in web searching activities and interactive information retrieval (IR). The value of relevance assessment lies in the disambiguation of the user's information need, which is achieved by applying various feedback techniques. Such techniques vary from explicit to implicit and help determine the relevance of the retrieved documents. The former type of feedback is usually obtained through the explicit and intended indication of documents as relevant (positive feedback) or irrelevant (negative feedback). Explicit feedback is a robust method for improving a system's overall retrieval performance and producing better query reformulations [1], at the expense of users' cognitive resources. On the other hand, implicit feedback techniques tend to collect information on search behavior in a more intelligent and unobtrusive manner. By doing so, they disengage the users from the cognitive burden of document rating and relevance judgments. Information-seeking activities such as reading time, saving, printing, selecting and referencing have been all treated as indicators of relevance, despite the lack of sufficient evidence to support their effectiveness [2]. Besides their apparent differences, both categories of feedback techniques determine document relevance with respect to the cognitive and situational levels of the interactive dialogue that occurs between the user and the retrieval system [5]. However, this approach does not account for the dynamic interplay and adaptation that takes place between the different dialogue levels, but most importantly it does not consider the affective dimension of interaction. Users interact with intentions, motivations and feelings apart from real-life problems and information objects, which are all critical aspects of cognition and decision-making [3][4]. By evaluating users' affective response towards an information object (e.g. a document), prior and post to their exposure to it, a more accurate understanding of the object's properties and degree of relevance to the current information need may be facilitated. Furthermore, systems that can detect and respond accordingly to user emotions could potentially improve the naturalness of human-computer interaction and progressively optimize their retrieval strategy. The current study investigates the role of emotions in the information seeking process, as the latter are communicated through multi-modal interaction, and reconsiders relevance feedback with respect to what occurs on the affective level of interaction as well.

#index 1074247
#* Exploring and measuring dependency trees for informationretrieval
#@ Chang Liu
#t 2008
#c 13
#% 355752
#! Natural language processing techniques are believed to hold a tremendous potential to supplement the purely quantitative methods of text information retrieval. This has led to the emergence of a large number of NLP-based IR research projects over the last few years, even though the empirical evidence to support this has often been inadequate. Most contributions of NLP to IR mainly concentrate on document representation and compound term matching strategies. Researchers have noted that the simple term-based representation of document content such as vector representation is usually inadequate for accurate discrimination. The "bag of words" representation does not invoke linguistic considerations and allow modelling of relationships between subsets of words. However, even though a variety of content indicator such as syntactic phrase have been tried and investigated for representing documents rather than single terms in IR systems, the matching strategy over those representation still cannot go beyond traditional statistical techniques that measure term co-occurrence characteristics and proximity in analyzing text structure. In this paper, we propose a novel IR strategy (SIR) with NLP techniques involved at the syntactic level. Within SIR, documents and query representation are built on the basis of a syntactic data structure of the natural language text - the dependency tree, in which syntactic relationships between words are identified and structured in the form of a tree. In order to capture the syntactic relations between words in their hierarchical structural representation, the matching strategy in SIR upgrades from the traditional statistical techniques by introducing a similarity measure method executing on the graph representation level as the key determiner. A basic IR experiment is designed and implemented on the TREC data to evaluate if this novel IR model is feasible. Experimental results indicate that this approach has the potential to outperform the standard bag of words IR model, especially in response to syntactical structured queries.

#index 1074248
#* The search for expertise: to the documents and beyond
#@ Pavel Serdyukov
#t 2008
#c 13
#% 1074192
#% 1074193
#% 1415734

#index 1074249
#* Task detection for activity-based desktop search
#@ Sergey Chernov
#t 2008
#c 13
#% 845342
#% 987195
#! The desktop search tools provide powerful query capabilities and result presentation techniques. However, they do not take the user context into account. We propose to exploit collected information about user activities with desktop files and applications for activity-based desktop search. When I prepare for a project review and type in a search box the name of a colleague, I expect to find her last deliverable draft, but not her email with a paper review or our joint conference presentation. Ideally, the desktop search system should be able to infer my current task from the logs of my previous activities and present task-specific search results.

#index 1074250
#* Using a mediated query approach for matching unstructured query with structured resources
#@ Keng Hoon Gan
#t 2008
#c 13
#% 642993
#% 879696
#% 1721851
#% 1742095

#index 1074251
#* Understanding system implementation and user behavior in a collaborative information seeking environment
#@ Chirag Shah
#t 2008
#c 13
#% 246877
#% 816372
#% 987374
#% 1074090

#index 1074252
#* Biomedical cross-language information retrieval
#@ Dolf Trieschnigg
#t 2008
#c 13
#% 1677221

#index 1074253
#* Towards a combined model for search and navigation of annotated documents
#@ Edgar Meij
#t 2008
#c 13
#% 1074202
#% 1074203
#% 1074206
#% 1914879

#index 1074254
#* Context and linking in retrieval from personal digital archives
#@ Liadh Kelly
#t 2008
#c 13
#! Advances in digital capture and storage technologies mean that it is now possible to capture and store one's entire life experiences in personal digital archives. These vast personal archives (or Human Digital Memories (HDMs)) pose new challenges and opportunities for the research community, not the least of which is developing effective means of retrieval from HDMs. Personal archive retrieval research is still in its infancy and there is much scope for novel research. My PhD proposes to develop effective HDM retrieval algorithms by combining rich sources of context associated with items, such as location and people present data, with information obtained by linking HDM items in novel ways.

#index 1074255
#* Extending language modeling techniques to models of search and browsing activity in a digital library
#@ G. Craig Murray
#t 2008
#c 13
#! Users searching for information in a digital library or on the WWW can be modeled as individuals moving through a semantic space by issuing queries and clicking on hyperlinks. As they go, they emit a stream of interaction data. Most of it is linguistic data. Lots of it is captured in logs. Some of it is used to guess what the user is searching for. But to most information retrieval systems, each user interaction is a stateless point in this space. There is a timeline connecting each of these points, but systems seldom make use of this as sequence data, in part because there is no clear way to systematically characterize the meaningful relations within a sequence of user activity. It is a problem of pragmatics as much as it is of semantics--the fact that a user clicked on a particular link, or added a particular term to their query, has meaning primarily in relation to the preceding actions. A remaining challenge in IR is to extract features of the user interaction data that will give meaning to those relations. Meanwhile, from the user's perspective each of these points in time and semantic space are just part of a path of exploration. To the user, the exact terms in a query, or the specific words surrounding a hypertext link, may be less important than the trajectory those terms establish in relation to the user's path. Identifying the meaningful relations between queries and page views within a sequence of activity increases our understanding of users and their information needs. Formally, we can model query and browsing behaviors as surface forms of a hidden process. What is missing is a layer of abstraction for mapping sequences of interaction in a way that is both descriptive of users' needs and useful to automation. The work I describe is an effort to identify features of data in logs of query and browsing activity that are highly predictive of certain types of behavior. Sequences of interaction data from individual users are modeled as sequences of expression. Statistical modeling techniques that are effective for modeling sequences in natural language processing and bioinformatics are examined for their ability to model sequences of interaction between an information searcher and an information retrieval system. Queries and click-throughs in this stream of interaction can be tagged with features such as semantic coordinates, timing, frequency of use, type of action, etc. By analyzing large collections of interaction sequences it is possible to identify frequent patterns of user behavior. From these patterns we can make predictions about future interactions. For example, certain patterns of link following in a digital library are highly predictive of users' next steps while other patterns are not. General models of user interaction are useful for design and evaluation of search interfaces. Individual models of user interaction are useful for personalized search and customized content. Yet very little research has been done to investigate which features are optimal for modeling user queries and browsing as interaction sequences. An important first step is to identify informative features and the relationships between features. I propose to construct models of user behavior based on user data in logs of query and browsing activity and to identify features that are highly predictive of certain types of user behaviors. I examine activity within search sessions on a digital library as a microcosm of larger systems. I expect to find features that are useful in predictive models of user behavior both at an individual and aggregate level. Where possible, I hope to identify meaningful relationships between those features. The work has implications beyond the scope of digital libraries, to larger systems and broader search domains.

#index 1227574
#* Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval
#@ James Allan;Javed Aslam;Mark Sanderson;ChengXiang Zhai;Justin Zobel
#t 2009
#c 13
#! Welcome to the 32nd ACM SIGIR International Conference on Research & Development in Information Retrieval. This year's conference reflects the growing diversity of the field, with papers on a wide range of topics that reach well beyond the traditional problems of search on large collections of text. The conference received 494 submissions of full papers from 34 countries, a similar number to previous years and similarly distributed, with around a third from each of North America, Europe, and Asia-Pacific. Of these, 78 were accepted for full presentation, and a further 26 were offered the opportunity of presentation as posters. There were in addition 256 poster submissions, of which 86 were selected, and 12 of the 36 submissions of candidate demonstrations were successful. SIGIR has had a long-standing tradition of experimental rigor, and in 2009, as in the past, we have required that successful submissions meet equally our expectations of originality and of demonstrated validity. We believe that we have been successful in choosing a program of high standard. The SIGIR program involves not only papers, posters, and demonstrations, but also six tutorials, eight workshops, a doctoral consortium with 12 students selected from 24 applicants, and two invited talks. One invited talk is given by the 2009 Salton Award winner. This award is a triennial event at SIGIR and we celebrate Susan T. Dumais' contribution to our community. The other invited talk is given by Albert-László Barabási. We are deeply grateful to Albert-László Barabási and Susan T. Dumais for their participation in SIGIR this year and for the opportunity this gives to our attendees.

#index 1227575
#* An interdisciplinary perspective on information retrieval
#@ Susan T. Dumais
#t 2009
#c 13

#index 1227576
#* From networks to human behavior
#@ Albert-László Barabási
#t 2009
#c 13
#! Highly interconnected networks with amazingly complex topology describe systems as diverse as the World Wide Web, our cells, social systems or the economy. Recent studies indicate that these networks are the result of self-organizing processes governed by simple but generic laws, resulting in architectural features that makes them much more similar to each other than one would have expected by chance. I will discuss the amazing order characterizing our interconnected world and its implications to network robustness and spreading processes. Finally, most of these networks are driven by the temporal patterns characterizing human activity. I will use communication and web browsing data to show that there is deep order in the temporal domain of human dynamics, and discuss the different ways to understand and model the emerging patterns.

#index 1227577
#* Context-aware query classification
#@ Huanhuan Cao;Derek Hao Hu;Dou Shen;Daxin Jiang;Jian-Tao Sun;Enhong Chen;Qiang Yang
#t 2009
#c 13
#% 152934
#% 321635
#% 330617
#% 348155
#% 464434
#% 590523
#% 642982
#% 754059
#% 766447
#% 770844
#% 805878
#% 818281
#% 818313
#% 833890
#% 838531
#% 844287
#% 869550
#% 879581
#% 987221
#% 1074093
#% 1074098
#% 1083721
#% 1130878
#% 1190074
#% 1206824
#% 1673026
#! Understanding users'search intent expressed through their search queries is crucial to Web search and online advertisement. Web query classification (QC) has been widely studied for this purpose. Most previous QC algorithms classify individual queries without considering their context information. However, as exemplified by the well-known example on query "jaguar", many Web queries are short and ambiguous, whose real meanings are uncertain without the context information. In this paper, we incorporate context information into the problem of query classification by using conditional random field (CRF) models. In our approach, we use neighboring queries and their corresponding clicked URLs (Web pages) in search sessions as the context information. We perform extensive experiments on real world search logs and validate the effectiveness and effciency of our approach. We show that we can improve the F1 score by 52% as compared to other state-of-the-art baselines.

#index 1227578
#* Refined experts: improving classification in large taxonomies
#@ Paul N. Bennett;Nam Nguyen
#t 2009
#c 13
#% 169358
#% 280817
#% 280866
#% 309141
#% 325001
#% 375017
#% 458379
#% 465747
#% 466078
#% 466501
#% 763708
#% 770796
#% 783478
#% 786633
#% 818266
#% 829975
#% 875967
#% 876017
#% 961135
#% 983883
#% 983905
#% 1074128
#% 1672984
#% 1674784
#! While large-scale taxonomies--especially for web pages--have been in existence for some time, approaches to automatically classify documents into these taxonomies have met with limited success compared to the more general progress made in text classification. We argue that this stems from three causes: increasing sparsity of training data at deeper nodes in the taxonomy, error propagation where a mistake made high in the hierarchy cannot be recovered, and increasingly complex decision surfaces in higher nodes in the hierarchy. While prior research has focused on the first problem, we introduce methods that target the latter two problems--first by biasing the training distribution to reduce error propagation and second by propagating up "first-guess" expert information in a bottom-up manner before making a refined top down choice. Finally, we present an empirical study demonstrating that the suggested changes lead to 10--30% improvements in F1 scores versus an accepted competitive baseline, hierarchical SVMs.

#index 1227579
#* Dynamicity vs. effectiveness: studying online clustering for scatter/gather
#@ Weimao Ke;Cassidy R. Sugimoto;Javed Mostafa
#t 2009
#c 13
#% 64898
#% 118771
#% 144023
#% 214711
#% 218992
#% 387427
#% 413645
#% 427921
#% 465754
#% 766434
#% 907522
#! We proposed and implemented a novel clustering algorithm called LAIR2, which has constant running time average for on-the-fly Scatter/Gather browsing [4]. Our experiments showed that when running on a single processor, the LAIR2 on-line clustering algorithm was several hundred times faster than a parallel Buckshot algorithm running on multiple processors [11]. This paper reports on a study that examined the effectiveness of the LAIR2 algorithm in terms of clustering quality and its impact on retrieval performance. We conducted a user study on 24 subjects to evaluate on-the-fly LAIR2 clustering in Scatter/Gather search tasks by comparing its performance to the Buckshot algorithm, a classic method for Scatter/Gather browsing [4]. Results showed significant differences in terms of subjective perceptions of clustering quality. Subjects perceived that the LAIR2 algorithm produced significantly better quality clusters than the Buckshot method did. Subjects felt that it took less effort to complete the tasks with the LAIR2 system, which was more effective in helping them in the tasks. Interesting patterns also emerged from subjects' comments in the final open-ended questionnaire. We discuss implications and future research.

#index 1227580
#* Web searching for daily living
#@ Takuya Maekawa;Yutaka Yanagisawa;Yasushi Sakurai;Yasue Kishino;Koji Kamei;Takeshi Okadome
#t 2009
#c 13
#% 169774
#% 187271
#% 262036
#% 268078
#% 325011
#% 330769
#% 420577
#% 452612
#% 577300
#% 654466
#% 728107
#% 754115
#% 869510
#% 987404
#% 1147433
#% 1269362
#! The new concept proposed in this paper is a query free web search that automatically retrieves a web page including information related to the daily activity that we are currently engaged in for automatically displaying the page on Internet-connected domestic appliances around us such as televisions. When we are washing a coffee maker, for example, a web page is retrieved that includes tips such as `cleaning a coffee maker with vinegar removes stains well.' A method designed on the basis of this concept automatically searches for a web page by using a query constructed from the use of ordinary household objects that is detected by sensors attached to the objects. An in-situ experiment tests a variety of IR techniques and the experiment confirmed that our daily activities can produce related web pages with high accuracy.

#index 1227581
#* Global ranking by exploiting user clicks
#@ Shihao Ji;Ke Zhou;Ciya Liao;Zhaohui Zheng;Gui-Rong Xue;Olivier Chapelle;Gordon Sun;Hongyuan Zha
#t 2009
#c 13
#% 55490
#% 111304
#% 190581
#% 268079
#% 464434
#% 479726
#% 577224
#% 818221
#% 840846
#% 987203
#% 987228
#% 989628
#% 1035578
#% 1074092
#! It is now widely recognized that user interactions with search results can provide substantial relevance information on the documents displayed in the search results. In this paper, we focus on extracting relevance information from one source of user interactions, i.e., user click data, which records the sequence of documents being clicked and not clicked in the result set during a user search session. We formulate the problem as a global ranking problem, emphasizing the importance of the sequential nature of user clicks, with the goal to predict the relevance labels of all the documents in a search session. This is distinct from conventional learning to rank methods that usually design a ranking model defined on a single document; in contrast, in our model the relational information among the documents as manifested by an aggregation of user clicks is exploited to rank all the documents jointly. In particular, we adapt several sequential supervised learning algorithms, including the conditional random field (CRF), the sliding window method and the recurrent sliding window method, to the global ranking problem. Experiments on the click data collected from a commercial search engine demonstrate that our methods can outperform the baseline models for search results re-ranking.

#index 1227582
#* Good abandonment in mobile and PC internet search
#@ Jane Li;Scott Huffman;Akihito Tokuda
#t 2009
#c 13
#% 779037
#% 783482
#% 818221
#% 823348
#% 860086
#% 879567
#% 949163
#% 954948
#% 987208
#% 989628
#% 1004294
#% 1047345
#% 1083643
#% 1190135
#% 1250383
#! Query abandonment by search engine users is generally considered to be a negative signal. In this paper, we explore the concept of good abandonment. We define a good abandonment as an abandoned query for which the user's information need was successfully addressed by the search results page, with no need to click on a result or refine the query. We present an analysis of abandoned internet search queries across two modalities (PC and mobile) in three locales. The goal is to approximate the prevalence of good abandonment, and to identify types of information needs that may lead to good abandonment, across different locales and modalities. Our study has three key findings: First, queries potentially indicating good abandonment make up a significant portion of all abandoned queries. Second, the good abandonment rate from mobile search is significantly higher than that from PC search, across all locales tested. Third, classified by type of information need, the major classes of good abandonment vary dramatically by both locale and modality. Our findings imply that it is a mistake to uniformly consider query abandonment as a negative signal. Further, there is a potential opportunity for search engines to drive additional good abandonment, especially for mobile search users, by improving search features and result snippets.

#index 1227583
#* Efficient query expansion for advertisement search
#@ Haofen Wang;Yan Liang;Linyun Fu;Gui-Rong Xue;Yong Yu
#t 2009
#c 13
#% 252328
#% 310567
#% 342621
#% 387427
#% 818265
#% 879610
#% 879633
#% 963376
#% 987221
#% 987262
#% 987361
#% 1019092
#% 1019150
#% 1039685
#% 1055694
#% 1055713
#% 1074101
#% 1083721
#! Online advertising represents a growing part of the revenues of major Internet service providers such as Google and Yahoo. A commonly used strategy is to place advertisements (ads) on the search result pages according to the users' submitted queries. Relevant ads are likely to be clicked by a user and to increase the revenues of both advertisers and publishers. However, bid phrases defined by ad-owners are usually contained in limited number of ads. Directly matching user queries with bid phrases often results in finding few appropriate ads. To address this shortcoming, query expansion is often used to increase the chances to match the ads. Nevertheless, query expansion on top of the traditional inverted index faces efficiency issues such as high time complexity and heavy I/O costs. Moreover, precision cannot always be improved, sometimes even hurt due to the involvement of additional noise. In this paper, we propose an efficient ad search solution relying on a block-based index able to tackle the issues associated with query expansion. Our index structure places clusters of similar bid phrases in corresponding blocks with their associated ads. It reduces the number of merge operations significantly during query expansion and allows sequential scans rather than random accesses, saving I/O costs. We adopt flexible block sizes according to the clustering results of bid phrases to further optimize the index structure for efficient ad search. The pre-computation of such clusters is achieved through an agglomerative iterative clustering algorithm. Finally, we adapt the spreading activation mechanism to return the top-k relevant ads, improving search precision. The experimental results of our prototype, AdSearch, show that we can indeed return a larger number of relevant ads without sacrificing execution speed.

#index 1227584
#* Query dependent pseudo-relevance feedback based on wikipedia
#@ Yang Xu;Gareth J.F. Jones;Bin Wang
#t 2009
#c 13
#% 262092
#% 298183
#% 340901
#% 342707
#% 375388
#% 641976
#% 783474
#% 783506
#% 818267
#% 838531
#% 879585
#% 891559
#% 955496
#% 987193
#% 987231
#% 987333
#% 1019105
#% 1026891
#% 1074080
#% 1074081
#% 1074094
#% 1074113
#! Pseudo-relevance feedback (PRF) via query-expansion has been proven to be e®ective in many information retrieval (IR) tasks. In most existing work, the top-ranked documents from an initial search are assumed to be relevant and used for PRF. One problem with this approach is that one or more of the top retrieved documents may be non-relevant, which can introduce noise into the feedback process. Besides, existing methods generally do not take into account the significantly different types of queries that are often entered into an IR system. Intuitively, Wikipedia can be seen as a large, manually edited document collection which could be exploited to improve document retrieval effectiveness within PRF. It is not obvious how we might best utilize information from Wikipedia in PRF, and to date, the potential of Wikipedia for this task has been largely unexplored. In our work, we present a systematic exploration of the utilization of Wikipedia in PRF for query dependent expansion. Specifically, we classify TREC topics into three categories based on Wikipedia: 1) entity queries, 2) ambiguous queries, and 3) broader queries. We propose and study the effectiveness of three methods for expansion term selection, each modeling the Wikipedia based pseudo-relevance information from a different perspective. We incorporate the expansion terms into the original query and use language modeling IR to evaluate these methods. Experiments on four TREC test collections, including the large web collection GOV2, show that retrieval performance of each type of query can be improved. In addition, we demonstrate that the proposed method out-performs the baseline relevance model in terms of precision and robustness.

#index 1227585
#* Segment-level display time as implicit feedback: a comparison to eye tracking
#@ Georg Buscher;Ludger van Elst;Andreas Dengel
#t 2009
#c 13
#% 169803
#% 280809
#% 309095
#% 320432
#% 340974
#% 731615
#% 766454
#% 805200
#% 879567
#% 907516
#% 946521
#% 1048693
#% 1055671
#% 1065168
#% 1074099
#% 1074100
#% 1077044
#% 1677822
#! We examine two basic sources for implicit relevance feedback on the segment level for search personalization: eye tracking and display time. A controlled study has been conducted where 32 participants had to view documents in front of an eye tracker, query a search engine, and give explicit relevance ratings for the results. We examined the performance of the basic implicit feedback methods with respect to improved ranking and compared their performance to a pseudo relevance feedback baseline on the segment level and the original ranking of a Web search engine. Our results show that feedback based on display time on the segment level is much coarser than feedback from eye tracking. But surprisingly, for re-ranking and query expansion it did work as well as eye-tracking-based feedback. All behavior-based methods performed significantly better than our non-behavior-based baseline and especially improved poor initial rankings of the Web search engine. The study shows that segment-level display time yields comparable results as eye-tracking-based feedback. Thus, it should be considered in future personalization systems as an inexpensive but precise method for implicit feedback.

#index 1227586
#* Addressing morphological variation in alphabetic languages
#@ Paul McNamee;Charles Nicholas;James Mayfield
#t 2009
#c 13
#% 144034
#% 189867
#% 208934
#% 232648
#% 232692
#% 241238
#% 252608
#% 280850
#% 643047
#% 732845
#% 732848
#% 743635
#% 819777
#% 854592
#% 874254
#% 948375
#% 985823
#% 987311
#% 1226515
#% 1432341
#% 1664747
#! The selection of indexing terms for representing documents is a key decision that limits how effective subsequent retrieval can be. Often stemming algorithms are used to normalize surface forms, and thereby address the problem of not finding documents that contain words related to query terms through infectional or derivational morphology. However, rule-based stemmers are not available for every language and it is unclear which methods for coping with morphology are most effective. In this paper we investigate an assortment of techniques for representing text and compare these approaches using data sets in eighteen languages and five different writing systems. We find character n-gram tokenization to be highly effective. In half of the languages examined n-grams outperform unnormalized words by more than 25%; in highly infective languages relative improvements over 50% are obtained. In languages with less morphological richness the choice of tokenization is not as critical and rule-based stemming can be an attractive option, if available. We also conducted an experiment to uncover the source of n-gram power and a causal relationship between the morphological complexity of a language and n-gram effectiveness was demonstrated.

#index 1227587
#* Web derived pronunciations for spoken term detection
#@ Dogan Can;Erica Cooper;Arnab Ghoshal;Martin Jansche;Sanjeev Khudanpur;Bhuvana Ramabhadran;Michael Riley;Murat Saraclar;Abhinav Sethy;Morgan Ulinski;Christopher White
#t 2009
#c 13
#% 251664
#% 309207
#% 479726
#% 740394
#% 987269
#% 1275583
#% 1402057
#! Indexing and retrieval of speech content in various forms such as broadcast news, customer care data and on-line media has gained a lot of interest for a wide range of applications, from customer analytics to on-line media search. For most retrieval applications, the speech content is typically first converted to a lexical or phonetic representation using automatic speech recognition (ASR). The first step in searching through indexes built on these representations is the generation of pronunciations for named entities and foreign language query terms. This paper summarizes the results of the work conducted during the 2008 JHU Summer Workshop by the Multilingual Spoken Term Detection team, on mining the web for pronunciations and analyzing their impact on spoken term detection. We will first present methods to use the vast amount of pronunciation information available on the Web, in the form of IPA and ad-hoc transcriptions. We describe techniques for extracting candidate pronunciations from Web pages and associating them with orthographic words, filtering out poorly extracted pronunciations, normalizing IPA pronunciations to better conform to a common transcription standard, and generating phonemic representations from ad-hoc transcriptions. We then present an analysis of the effectiveness of using these pronunciations to represent Out-Of-Vocabulary (OOV) query terms on the performance of a spoken term detection (STD) system. We will provide comparisons of Web pronunciations against automated techniques for pronunciation generation as well as pronunciations generated by human experts. Our results cover a range of speech indexes based on lattices, confusion networks and one-best transcriptions at both word and word fragments levels.

#index 1227588
#* Combining LVCSR and vocabulary-independent ranked utterance retrieval for robust speech search
#@ J. Scott Olsson;Douglas W. Oard
#t 2009
#c 13
#% 169774
#% 171586
#% 194246
#% 218984
#% 232703
#% 309133
#% 321337
#% 340934
#% 413613
#% 420464
#% 784148
#% 879571
#% 879582
#% 928422
#% 968469
#% 995504
#% 1215368
#% 1270669
#% 1276314
#% 1674976
#% 1775144
#% 1916129
#! Well tuned Large-Vocabulary Continuous Speech Recognition (LVCSR) has been shown to generally be more effective than vocabulary-independent techniques for ranked retrieval of spoken content when one or the other approach is used alone. Tuning LVCSR systems to a topic domain can be costly, however, and the experiments in this paper show that Out-Of-Vocabulary (OOV) query terms can significantly reduce retrieval effectiveness when that tuning is not performed. Further experiments demonstrate, however, that retrieval effectiveness for queries with OOV terms can be substantially improved by combining evidence from LVCSR with additional evidence from vocabulary-independent Ranked Utterance Retrieval (RUR). The combination is performed by using relevance judgments from held-out topics to learn generic (i.e., topic-independent), smooth, non-decreasing transformations from LVCSR and RUR system scores to probabilities of topical relevance. Evaluated using a CLEF collection that includes topics, spontaneous conversational speech audio, and relevance judgments, the system recovers 57% of the mean uninterpolated average precision that could have been obtained through LVCSR domain tuning for very short queries (or 41% for longer queries).

#index 1227589
#* Risky business: modeling and exploiting uncertainty in information retrieval
#@ Jianhan Zhu;Jun Wang;Ingemar J. Cox;Michael J. Taylor
#t 2009
#c 13
#% 169781
#% 248214
#% 262096
#% 288166
#% 340899
#% 340948
#% 375017
#% 411760
#% 642974
#% 734592
#% 840903
#% 872020
#% 879618
#% 945863
#% 1227591
#! Most retrieval models estimate the relevance of each document to a query and rank the documents accordingly. However, such an approach ignores the uncertainty associated with the estimates of relevancy. If a high estimate of relevancy also has a high uncertainty, then the document may be very relevant or not relevant at all. Another document may have a slightly lower estimate of relevancy but the corresponding uncertainty may be much less. In such a circumstance, should the retrieval engine risk ranking the first document highest, or should it choose a more conservative (safer) strategy that gives preference to the second document? There is no definitive answer to this question, as it depends on the risk preferences of the user and the information retrieval system. In this paper we present a general framework for modeling uncertainty and introduce an asymmetric loss function with a single parameter that can model the level of risk the system is willing to accept. By adjusting the risk preference parameter, our approach can effectively adapt to users' different retrieval strategies. We apply this asymmetric loss function to a language modeling framework and a practical risk-aware document scoring function is obtained. Our experiments on several TREC collections show that our "risk-averse" approach significantly improves the Jelinek-Mercer smoothing language model, and a combination of our "risk-averse" approach and the Jelinek-Mercer smoothing method generally outperforms the Dirichlet smoothing method. Experimental results also show that the "risk-averse" approach, even without smoothing from the collection statistics, performs as well as three commonly-adopted retrieval models, namely, the Jelinek-Mercer and Dirichlet smoothing methods, and BM25 model.

#index 1227590
#* Approximating true relevance distribution from a mixture model based on irrelevance data
#@ Peng Zhang;Yuexian Hou;Dawei Song
#t 2009
#c 13
#% 194301
#% 223810
#% 232646
#% 306468
#% 340901
#% 340948
#% 375017
#% 590524
#% 731620
#% 818206
#% 1019183
#% 1074078
#% 1074080
#! Pseudo relevance feedback (PRF), which has been widely applied in IR, aims to derive a distribution from the top n pseudo relevant documents D. However, these documents are often a mixture of relevant and irrelevant documents. As a result, the derived distribution is actually a mixture model, which has long been limiting the performance of PRF. This is particularly the case when we deal with difficult queries where the truly relevant documents in D are very sparse. In this situation, it is often easier to identify a small number of seed irrelevant documents, which can form a seed irrelevant distribution. Then, a fundamental and challenging problem arises: solely based on the mixed distribution and a seed irrelevance distribution, how to automatically generate an optimal approximation of the true relevance distribution? In this paper, we propose a novel distribution separation model (DSM) to tackle this problem. Theoretical justifications of the proposed algorithm are given. Evaluation results from our extensive simulated experiments on several large scale TREC data sets demonstrate the effectiveness of our method, which outperforms a well respected PRF Model, the Relevance Model (RM), as well as the use of RM on D with the seed negative documents directly removed.

#index 1227591
#* Portfolio theory of information retrieval
#@ Jun Wang;Jianhan Zhu
#t 2009
#c 13
#% 169781
#% 262112
#% 330687
#% 340899
#% 340948
#% 411762
#% 757953
#% 872020
#% 879618
#% 879627
#% 879693
#% 891559
#% 1074133
#% 1227589
#! This paper studies document ranking under uncertainty. It is tackled in a general situation where the relevance predictions of individual documents have uncertainty, and are dependent between each other. Inspired by the Modern Portfolio Theory, an economic theory dealing with investment in financial markets, we argue that ranking under uncertainty is not just about picking individual relevant documents, but about choosing the right combination of relevant documents. This motivates us to quantify a ranked list of documents on the basis of its expected overall relevance (mean) and its variance; the latter serves as a measure of risk, which was rarely studied for document ranking in the past. Through the analysis of the mean and variance, we show that an optimal rank order is the one that balancing the overall relevance (mean) of the ranked list against its risk level (variance). Based on this principle, we then derive an efficient document ranking algorithm. It generalizes the well-known probability ranking principle (PRP) by considering both the uncertainty of relevance predictions and correlations between retrieved documents. Moreover, the benefit of diversification is mathematically quantified; we show that diversifying documents is an effective way to reduce the risk of document ranking. Experimental results in text retrieval confirm performance.

#index 1227592
#* A statistical comparison of tag and query logs
#@ Mark J. Carman;Mark Baillie;Robert Gwadera;Fabio Crestani
#t 2009
#c 13
#% 316
#% 279755
#% 577224
#% 878624
#% 912522
#% 915270
#% 939629
#% 956544
#% 956552
#% 967260
#% 1035588
#! We investigate tag and query logs to see if the terms people use to annotate websites are similar to the ones they use to query for them. Over a set of URLs, we compare the distribution of tags used to annotate each URL with the distribution of query terms for clicks on the same URL. Understanding the relationship between the distributions is important to determine how useful tag data may be for improving search results and conversely, query data for improving tag prediction. In our study, we compare both term frequency distributions using vocabulary overlap and relative entropy. We also test statistically whether the term counts come from the same underlying distribution. Our results indicate that the vocabulary used for tagging and searching for content are similar but not identical. We further investigate the content of the websites to see which of the two distributions (tag or query) is most similar to the content of the annotated/searched URL. Finally, we analyze the similarity for different categories of URLs in our sample to see if the similarity between distributions is dependent on the topic of the website or the popularity of the URL.

#index 1227593
#* Simultaneously modeling semantics and structure of threaded discussions: a sparse coding approach and its applications
#@ Chen Lin;Jiang-Ming Yang;Rui Cai;Xin-Jing Wang;Wei Wang
#t 2009
#c 13
#% 280819
#% 290830
#% 722904
#% 788043
#% 788094
#% 805871
#% 875959
#% 879569
#% 879602
#% 879628
#% 956516
#% 1055680
#% 1074109
#% 1274819
#! The huge amount of knowledge in web communities has motivated the research interests in threaded discussions. The dynamic nature of threaded discussions poses lots of challenging problems for computer scientists. Although techniques such as semantic models and structural models have been shown to be useful in a number of areas, they are inefficient in understanding threaded discussions due to three reasons: (I) as most of users read existing messages before posting, posts in a discussion thread are temporally dependent on the previous ones; It causes the semantics and structure to be coupled with each other in threaded discussions; (II) in online discussion threads, there are a lot of junk posts which are useless and may disturb content analysis; and (III) it is very hard to judge the quality of a post. In this paper, we propose a sparse coding-based model named SMSS to Simultaneously Model Semantics and Structure of threaded discussions. The model projects each post into a topic space, and approximates each post by a linear combination of previous posts in the same discussion thread. Meanwhile, the model also imposes two sparse constraints to force a sparse post reconstruction in the topic space and a sparse post approximation from previous posts. The sparse properties effectively take into account the characteristics of threaded discussions. Towards the above three problems, we demonstrate the competency of our model in three applications: reconstructing reply structure of threaded discussions, identifying junk posts, and finding experts in a given board/sub-board in web communities. Experimental results show encouraging performance of the proposed SMSS model in all these applications.

#index 1227594
#* Enhancing cluster labeling using wikipedia
#@ David Carmel;Haggai Roitman;Naama Zwerdling
#t 2009
#c 13
#% 118771
#% 413609
#% 787502
#% 807363
#% 813043
#% 878454
#% 879613
#% 961697
#% 961706
#% 975019
#% 1074073
#% 1077150
#% 1250362
#% 1275012
#! This work investigates cluster labeling enhancement by utilizing Wikipedia, the free on-line encyclopedia. We describe a general framework for cluster labeling that extracts candidate labels from Wikipedia in addition to important terms that are extracted directly from the text. The "labeling quality" of each candidate is then evaluated by several independent judges and the top evaluated candidates are recommended for labeling. Our experimental results reveal that the Wikipedia labels agree with manual labels associated by humans to a cluster, much more than with significant terms that are extracted directly from the text. We show that in most cases even when human's associated label appears in the text, pure statistical methods have difficulty in identifying them as good descriptors. Furthermore, our experiments show that for more than 85% of the clusters in our test collection, the manual label (or an inflection, or a synonym of it) appears in the top five labels recommended by our system.

#index 1227595
#* Compressing term positions in web indexes
#@ Hao Yan;Shuai Ding;Torsten Suel
#t 2009
#c 13
#% 118767
#% 230434
#% 290703
#% 397151
#% 420491
#% 570319
#% 577220
#% 587845
#% 656274
#% 737340
#% 766445
#% 768902
#% 786632
#% 818262
#% 864446
#% 865740
#% 867054
#% 879651
#% 891145
#% 987229
#% 1055710
#% 1404894
#% 1715627
#! Large search engines process thousands of queries per second on billions of pages, making query processing a major factor in their operating costs. This has led to a lot of research on how to improve query throughput, using techniques such as massive parallelism, caching, early termination, and inverted index compression. We focus on techniques for compressing term positions in web search engine indexes. Most previous work has focused on compressing docID and frequency data, or position information in other types of text collections. Compression of term positions in web pages is complicated by the fact that term occurrences tend to cluster within documents but not across document boundaries, making it harder to exploit clustering effects. Also, typical access patterns for position data are different from those for docID and frequency data. We perform a detailed study of a number of existing and new techniques for compressing position data in web indexes. We also study how to efficiently access position data for ranking functions that take proximity features into account.

#index 1227596
#* Brute force and indexed approaches to pairwise document similarity comparisons with MapReduce
#@ Jimmy Lin
#t 2009
#c 13
#% 212665
#% 213786
#% 290703
#% 340886
#% 340948
#% 723279
#% 956506
#% 963669
#% 987214
#% 1023422
#% 1074053
#% 1215321
#% 1683906
#! This paper explores the problem of computing pairwise similarity on document collections, focusing on the application of "more like this" queries in the life sciences domain. Three MapReduce algorithms are introduced: one based on brute force, a second where the problem is treated as large-scale ad hoc retrieval, and a third based on the Cartesian product of postings lists. Each algorithm supports one or more approximations that trade effectiveness for efficiency, the characteristics of which are studied experimentally. Results show that the brute force algorithm is the most efficient of the three when exact similarity is desired. However, the other two algorithms support approximations that yield large efficiency gains without significant loss of effectiveness.

#index 1227597
#* Efficiency trade-offs in two-tier web search systems
#@ Ricardo Baeza-Yates;Vanessa Murdock;Claudia Hauff
#t 2009
#c 13
#% 144034
#% 184486
#% 194246
#% 262096
#% 268079
#% 301225
#% 340948
#% 397161
#% 397204
#% 413594
#% 458379
#% 481748
#% 567255
#% 728102
#% 764562
#% 768913
#% 818267
#% 987215
#% 987216
#% 999298
#% 1074067
#! Search engines rely on searching multiple partitioned corpora to return results to users in a reasonable amount of time. In this paper we analyze the standard two-tier architecture for Web search with the difference that the corpus to be searched for a given query is predicted in advance. We show that any predictor better than random yields time savings, but this decrease in the processing time yields an increase in the infrastructure cost. We provide an analysis and investigate this trade-off in the context of two different scenarios on real-world data. We demonstrate that in general the decrease in answer time is justified by a small increase in infrastructure cost.

#index 1227598
#* A classification-based approach to question answering in discussion boards
#@ Liangjie Hong;Brian D. Davison
#t 2009
#c 13
#% 309126
#% 464996
#% 741892
#% 818299
#% 838397
#% 838398
#% 848650
#% 853850
#% 879593
#% 939368
#% 939776
#% 940017
#% 956516
#% 1016338
#% 1019165
#% 1035587
#% 1055679
#% 1074109
#% 1074110
#% 1083720
#% 1221065
#% 1251648
#% 1270283
#% 1274819
#% 1299677
#% 1721292
#! Discussion boards and online forums are important platforms for people to share information. Users post questions or problems onto discussion boards and rely on others to provide possible solutions and such question-related content sometimes even dominates the whole discussion board. However, to retrieve this kind of information automatically and effectively is still a non-trivial task. In addition, the existence of other types of information (e.g., announcements, plans, elaborations, etc.) makes it difficult to assume that every thread in a discussion board is about a question. We consider the problems of identifying question-related threads and their potential answers as classification tasks. Experimental results across multiple datasets demonstrate that our method can significantly improve the performance in both question detection and answer finding subtasks. We also do a careful comparison of how different types of features contribute to the final result and show that non-content features play a key role in improving overall performance. Finally, we show that a ranking scheme based on our classification approach can yield much better performance than prior published methods.

#index 1227599
#* Ranking community answers by modeling question-answer relationships via analogical reasoning
#@ Xin-Jing Wang;Xudong Tu;Dan Feng;Lei Zhang
#t 2009
#c 13
#% 290830
#% 397160
#% 424806
#% 722914
#% 730022
#% 815916
#% 816157
#% 838397
#% 838398
#% 879593
#% 956516
#% 956517
#% 987235
#% 987236
#% 1019165
#% 1035587
#% 1052711
#% 1055718
#% 1074110
#% 1074111
#% 1125908
#! The method of finding high-quality answers has significant impact on user satisfaction in community question answering systems. However, due to the lexical gap between questions and answers as well as spam typically existing in user-generated content, filtering and ranking answers is very challenging. Previous solutions mainly focus on generating redundant features, or finding textual clues using machine learning techniques; none of them ever consider questions and their answers as relational data but instead model them as independent information. Moreover, they only consider the answers of the current question, and ignore any previous knowledge that would be helpful to bridge the lexical and semantic gap. We assume that answers are connected to their questions with various types of latent links, i.e. positive indicating high-quality answers, negative links indicating incorrect answers or user-generated spam, and propose an analogical reasoning-based approach which measures the analogy between the new question-answer linkages and those of relevant knowledge which contains only positive links; the candidate answer which has the most analogous link is assumed to be the best answer. We conducted experiments based on 29.8 million Yahoo!Answer question-answer threads and showed the effectiveness of our approach.

#index 1227600
#* A syntactic tree matching approach to finding similar questions in community-based qa services
#@ Kai Wang;Zhaoyan Ming;Tat-Seng Chua
#t 2009
#c 13
#% 287253
#% 766428
#% 818253
#% 838397
#% 838398
#% 854791
#% 1019151
#% 1055718
#% 1074110
#% 1665151
#! While traditional question answering (QA) systems tailored to the TREC QA task work relatively well for simple questions, they do not suffice to answer real world questions. The community-based QA systems offer this service well, as they contain large archives of such questions where manually crafted answers are directly available. However, finding similar questions in the QA archive is not trivial. In this paper, we propose a new retrieval framework based on syntactic tree structure to tackle the similar question matching problem. We build a ground-truth set from Yahoo! Answers, and experimental results show that our method outperforms traditional bag-of-word or tree kernel based methods by 8.3% in mean average precision. It further achieves up to 50% improvement by incorporating semantic features as well as matching of potential answers. Our model does not rely on training, and it is demonstrated to be robust against grammatical errors as well.

#index 1227601
#* On social networks and collaborative recommendation
#@ Ioannis Konstas;Vassilios Stathopoulos;Joemon M. Jose
#t 2009
#c 13
#% 169803
#% 280852
#% 378577
#% 784963
#% 813966
#% 903609
#% 915344
#% 956515
#% 975021
#% 987222
#% 1127466
#% 1131212
#% 1131216
#% 1131223
#% 1131224
#! Social network systems, like last.fm, play a significant role in Web 2.0, containing large amounts of multimedia-enriched data that are enhanced both by explicit user-provided annotations and implicit aggregated feedback describing the personal preferences of each user. It is also a common tendency for these systems to encourage the creation of virtual networks among their users by allowing them to establish bonds of friendship and thus provide a novel and direct medium for the exchange of data. We investigate the role of these additional relationships in developing a track recommendation system. Taking into account both the social annotation and friendships inherent in the social graph established among users, items and tags, we created a collaborative recommendation system that effectively adapts to the personal information needs of each user. We adopt the generic framework of Random Walk with Restarts in order to provide with a more natural and efficient way to represent social networks. In this work we collected a representative enough portion of the music social network last.fm, capturing explicitly expressed bonds of friendship of the user as well as social tags. We performed a series of comparison experiments between the Random Walk with Restarts model and a user-based collaborative filtering method using the Pearson Correlation similarity. The results show that the graph model system benefits from the additional information embedded in social knowledge. In addition, the graph model outperforms the standard collaborative filtering method.

#index 1227602
#* Learning to recommend with social trust ensemble
#@ Hao Ma;Irwin King;Michael R. Lyu
#t 2009
#c 13
#% 173879
#% 280852
#% 330687
#% 397153
#% 452563
#% 643007
#% 734592
#% 734594
#% 766449
#% 840924
#% 879627
#% 987197
#% 987198
#% 1001279
#% 1055691
#% 1073982
#% 1074061
#% 1130901
#% 1275183
#% 1650569
#! As an indispensable technique in the field of Information Filtering, Recommender System has been well studied and developed both in academia and in industry recently. However, most of current recommender systems suffer the following problems: (1) The large-scale and sparse data of the user-item matrix seriously affect the recommendation quality. As a result, most of the recommender systems cannot easily deal with users who have made very few ratings. (2) The traditional recommender systems assume that all the users are independent and identically distributed; this assumption ignores the connections among users, which is not consistent with the real world recommendations. Aiming at modeling recommender systems more accurately and realistically, we propose a novel probabilistic factor analysis framework, which naturally fuses the users' tastes and their trusted friends' favors together. In this framework, we coin the term Social Trust Ensemble to represent the formulation of the social trust restrictions on the recommender systems. The complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations, while the experimental results show that our method performs better than the state-of-the-art approaches.

#index 1227603
#* Fast nonparametric matrix factorization for large-scale collaborative filtering
#@ Kai Yu;Shenghuo Zhu;John Lafferty;Yihong Gong
#t 2009
#c 13
#% 278011
#% 840924
#% 875976
#% 891549
#% 987198
#% 1073982
#% 1211838
#! With the sheer growth of online user data, it becomes challenging to develop preference learning algorithms that are sufficiently flexible in modeling but also affordable in computation. In this paper we develop nonparametric matrix factorization methods by allowing the latent factors of two low-rank matrix factorization methods, the singular value decomposition (SVD) and probabilistic principal component analysis (pPCA), to be data-driven, with the dimensionality increasing with data size. We show that the formulations of the two nonparametric models are very similar, and their optimizations share similar procedures. Compared to traditional parametric low-rank methods, nonparametric models are appealing for their flexibility in modeling complex data dependencies. However, this modeling advantage comes at a computational price--it is highly challenging to scale them to large-scale problems, hampering their application to applications such as collaborative filtering. In this paper we introduce novel optimization algorithms, which are simple to implement, which allow learning both nonparametric matrix factorization models to be highly efficient on large-scale problems. Our experiments on EachMovie and Netflix, the two largest public benchmarks to date, demonstrate that the nonparametric models make more accurate predictions of user ratings, and are computationally comparable or sometimes even faster in training, in comparison with previous state-of-the-art parametric matrix factorization models.

#index 1227604
#* Building enriched document representations using aggregated anchor text
#@ Donald Metzler;Jasmine Novak;Hang Cui;Srihari Reddy
#t 2009
#c 13
#% 169781
#% 232703
#% 232710
#% 252328
#% 268079
#% 290830
#% 309749
#% 309779
#% 411762
#% 577224
#% 642992
#% 766414
#% 766430
#% 766431
#% 783474
#% 805878
#% 818254
#% 818262
#% 838528
#% 840846
#% 879575
#% 907542
#% 940042
#% 987229
#% 987260
#% 1055705
#% 1055856
#% 1074127
#! It is well known that anchor text plays a critical role in a variety of search tasks performed over hypertextual domains, including enterprise search, wiki search, and web search. It is common practice to enrich a document's standard textual representation with all of the anchor text associated with its incoming hyperlinks. However, this approach does not help match relevant pages with very few inlinks. In this paper, we propose a method for overcoming anchor text sparsity by enriching document representations with anchor text that has been aggregated across the hyperlink graph. This aggregation mechanism acts to smooth, or diffuse, anchor text within a domain. We rigorously evaluate our proposed approach on a large web search test collection. Our results show the approach significantly improves retrieval effectiveness, especially for longer, more difficult queries.

#index 1227605
#* Using anchor texts with their hyperlink structure for web search
#@ Zhicheng Dou;Ruihua Song;Jian-Yun Nie;Ji-Rong Wen
#t 2009
#c 13
#% 46803
#% 262096
#% 268079
#% 290830
#% 309095
#% 316520
#% 340928
#% 590523
#% 643069
#% 750865
#% 754125
#% 783474
#% 805878
#% 840846
#% 1055705
#! As a good complement to page content, anchor texts have been extensively used, and proven to be useful, in commercial search engines. However, anchor texts have been assumed to be independent, whether they come from the same Web site or not. Intuitively, an anchor text from unrelated Web sites should be considered as stronger evidence than that from the same site. This paper proposes two new methods to take into account the possible relationships between anchor texts. We consider two relationships in this paper: links from the same site and links from related sites. The importance assigned to the anchor texts in these two situations is discounted. Experimental results show that these two new models outperform the baseline model which assumes independence between hyperlinks.

#index 1227606
#* Link analysis for private weighted graphs
#@ Jun Sakuma;Shigenobu Kobayashi
#t 2009
#c 13
#% 264249
#% 290830
#% 743280
#% 839729
#% 963800
#% 1002377
#% 1068712
#% 1073980
#! Link analysis methods have been used successfully for knowledge discovery from the link structure of mutually linking entities. Existing link analysis methods have been inherently designed based on the fact that the entire link structure of the target graph is observable such as public web documents; however, link information in graphs in the real world, such as human relationship or economic activities, is rarely open to public. If link analysis can be performed using graphs with private links in a privacy-preserving way, it enables us to rank entities connected with private ties, such as people, organizations, or business transactions. In this paper, we present a secure link analysis for graphs with private links by means of cryptographic protocols. Our solutions are designed as privacy-preserving expansions of well-known link analysis methods, PageRank and HITS. The outcomes of our protocols are completely equivalent to those of PageRank and HITS. Furthermore, our protocols theoretically guarantee that the private link information possessed by each node is not revealed to other nodes. %We demonstrate the efficiency of our solution by experimental studies, comparing with existing solutions, such as secure function evaluation, decentralized spectral analysis, and privacy-preserving link-analysis.

#index 1227607
#* Learning to rank for quantity consensus queries
#@ Somnath Banerjee;Soumen Chakrabarti;Ganesh Ramakrishnan
#t 2009
#c 13
#% 280834
#% 301241
#% 340953
#% 577224
#% 939944
#% 987226
#% 987235
#% 1019061
#% 1019132
#% 1019135
#% 1022234
#% 1055712
#% 1074139
#% 1133171
#% 1288643
#% 1392465
#! Web search is increasingly exploiting named entities like persons, places, businesses, addresses and dates. Entity ranking is also of current interest at INEX and TREC. Numerical quantities are an important class of entities, especially in queries about prices and features related to products, services and travel. We introduce Quantity Consensus Queries (QCQs), where each answer is a tight quantity interval distilled from evidence of relevance in thousands of snippets. Entity search and factoid question answering have benefited from aggregating evidence from multiple promising snippets, but these do not readily apply to quantities. Here we propose two new algorithms that learn to aggregate information from multiple snippets. We show that typical signals used in entity ranking, like rarity of query words and their lexical proximity to candidate quantities, are very noisy. Our algorithms learn to score and rankquantity intervals directly, combining snippet quantity and snippet text information. We report on experiments using hundreds of QCQs with ground truth taken from TREC QA, Wikipedia Infoboxes, and other sources, leading to tens of thousands of candidate snippets and quantities. Our algorithms yield about 20% better MAP and NDCG compared to the best-known collective rankers, and are 35% better than scoring snippets independent of each other.

#index 1227608
#* Learning in a pairwise term-term proximity framework for information retrieval
#@ Ronan Cummins;Colm O'Riordan
#t 2009
#c 13
#% 124073
#% 262096
#% 321635
#% 324129
#% 503222
#% 803033
#% 810627
#% 818263
#% 871571
#% 879651
#% 987229
#% 1055856
#% 1074205
#% 1387547
#% 1402202
#! Traditional ad hoc retrieval models do not take into account the closeness or proximity of terms. Document scores in these models are primarily based on the occurrences or non-occurrences of query-terms considered independently of each other. Intuitively, documents in which query-terms occur closer together should be ranked higher than documents in which the query-terms appear far apart. This paper outlines several term-term proximity measures and develops an intuitive framework in which they can be used to fully model the proximity of all query-terms for a particular topic. As useful proximity functions may be constructed from many proximity measures, we use a learning approach to combine proximity measures to develop a useful proximity function in the framework. An evaluation of the best proximity functions show that there is a significant improvement over the baseline ad hoc retrieval model and over other more recent methods that employ the use of single proximity measures.

#index 1227609
#* Robust sparse rank learning for non-smooth ranking measures
#@ Zhengya Sun;Tao Qin;Qing Tao;Jue Wang
#t 2009
#c 13
#% 387427
#% 411762
#% 840841
#% 961223
#% 983820
#% 987241
#% 1035577
#% 1066718
#% 1074083
#% 1083633
#% 1705511
#! Recently increasing attention has been focused on directly optimizing ranking measures and inducing sparsity in learning models. However, few attempts have been made to relate them together in approaching the problem of learning to rank. In this paper, we consider the sparse algorithms to directly optimize the Normalized Discounted Cumulative Gain (NDCG) which is a widely-used ranking measure. We begin by establishing a reduction framework under which we reduce ranking, as measured by NDCG, to the importance weighted pairwise classification. Furthermore, we provide a sound theoretical guarantee for this reduction, bounding the realized NDCG regret in terms of a properly weighted pairwise classification regret, which implies that good performance can be robustly transferred from pairwise classification to ranking. Based on the converted pairwise loss function, it is conceivable to take into account sparsity in ranking models and to come up with a gradient possessing certain performance guarantee. For the sake of achieving sparsity, a novel algorithm named RSRank has also been devised, which performs L1 regularization using truncated gradient descent. Finally, experimental results on benchmark collection confirm the significant advantage of RSRank in comparison with several baseline methods.

#index 1227610
#* Named entity recognition in query
#@ Jiafeng Guo;Gu Xu;Xueqi Cheng;Hang Li
#t 2009
#c 13
#% 280819
#% 280839
#% 590523
#% 709765
#% 722904
#% 742424
#% 754059
#% 805878
#% 815341
#% 818281
#% 830520
#% 875959
#% 879581
#% 879589
#% 951636
#% 956503
#% 956659
#% 989630
#% 1019130
#% 1055706
#! This paper addresses the problem of Named Entity Recognition in Query (NERQ), which involves detection of the named entity in a given query and classification of the named entity into predefined classes. NERQ is potentially useful in many applications in web search. The paper proposes taking a probabilistic approach to the task using query log data and Latent Dirichlet Allocation. We consider contexts of a named entity (i.e., the remainders of the named entity in queries) as words of a document, and classes of the named entity as topics. The topic model is constructed by a novel and general learning method referred to as WS-LDA (Weakly Supervised Latent Dirichlet Allocation), which employs weakly supervised learning (rather than unsupervised learning) using partially labeled seed entities. Experimental results show that the proposed method based on WS-LDA can accurately perform NERQ, and outperform the baseline methods.

#index 1227611
#* A 2-poisson model for probabilistic coreference of named entities for improved text retrieval
#@ Seung-Hoon Na;Hwee Tou Ng
#t 2009
#c 13
#% 71745
#% 169781
#% 197856
#% 340948
#% 342707
#% 420464
#% 643030
#% 740995
#% 766440
#% 818222
#% 853697
#% 939351
#% 1077150
#% 1271267
#% 1916144
#! Text retrieval queries frequently contain named entities. The standard approach of term frequency weighting does not work well when estimating the term frequency of a named entity, since anaphoric expressions (like he, she, the movie, etc) are frequently used to refer to named entities in a document, and the use of anaphoric expressions causes the term frequency of named entities to be underestimated. In this paper, we propose a novel 2-Poisson model to estimate the frequency of anaphoric expressions of a named entity, without explicitly resolving the anaphoric expressions. Our key assumption is that the frequency of anaphoric expressions is distributed over named entities in a document according to the probabilities of whether the document is elite for the named entities. This assumption leads us to formulate our proposed Co-referentially Enhanced Entity Frequency (CEEF). Experimental results on the text collection of TREC Blog Track show that CEEF achieves significant and consistent improvements over state-of-the-art retrieval methods using standard term frequency estimation. In particular, we achieve a 3% increase of MAP over the best performing run of TREC 2008 Blog Track.

#index 1227612
#* Mining employment market via text block detection and adaptive cross-domain information extraction
#@ Tak-Lam Wong;Wai Lam;Bo Chen
#t 2009
#c 13
#% 531459
#% 643004
#% 723243
#% 729978
#% 763713
#% 766464
#% 867052
#% 916788
#% 983899
#% 1214639
#% 1261539
#% 1270196
#% 1650352
#! We have developed an approach for analyzing online job advertisements in different domains (industries) from different regions worldwide. Our approach is able to extract precise information from the text content supporting useful employment market analysis locally and globally. A major component in our approach is an information extraction framework which is composed of two challenging tasks. The first task is to detect unformatted text blocks automatically based on an unsupervised learning model. Identifying these useful text blocks through this learning model allows the generation of highly effective features for the next task which is text fragment extraction learning. The task of text fragment extraction learning is formulated as a domain adaptation model for text fragment classification. One advantage of our approach is that it can easily adapt to a large number of online job advertisements in different and new domains. Extensive experiments have been conducted to demonstrate the effectiveness and flexibility of our approach.

#index 1227613
#* A proximity language model for information retrieval
#@ Jinglei Zhao;Yeogirl Yun
#t 2009
#c 13
#% 109190
#% 169777
#% 176550
#% 280850
#% 287253
#% 298182
#% 306494
#% 340899
#% 397205
#% 413593
#% 746910
#% 750863
#% 766428
#% 818262
#% 987229
#! The proximity of query terms in a document is a very important information to enable ranking models go beyond the "bag of word" assumption in information retrieval. This paper studies the integration of term proximity information into the unigram language modeling. A new proximity language model (PLM) is proposed which views query terms' proximity centrality as the Dirichlet hyper-parameter that weights the parameters of the unigram document language model. Several forms of proximity measure are developed to be used in PLM which could compute a query term's proximate centrality in a specific document. In experiments, the proximity language model is compared with the basic language model and previous works that combine the proximity information with language model using linear score combination. The experiment results show that the proposed model performs better in both top precision and average precision.

#index 1227614
#* Positional language models for information retrieval
#@ Yuanhua Lv;ChengXiang Zhai
#t 2009
#c 13
#% 106122
#% 134878
#% 144011
#% 169809
#% 232677
#% 262096
#% 280834
#% 287253
#% 292684
#% 328532
#% 340899
#% 340901
#% 340948
#% 342707
#% 413592
#% 642979
#% 766430
#% 818262
#% 879651
#% 987229
#% 1019135
#% 1041733
#% 1166534
#% 1387547
#% 1415737
#! Although many variants of language models have been proposed for information retrieval, there are two related retrieval heuristics remaining "external" to the language modeling approach: (1) proximity heuristic which rewards a document where the matched query terms occur close to each other; (2) passage retrieval which scores a document mainly based on the best matching passage. Existing studies have only attempted to use a standard language model as a "black box" to implement these heuristics, making it hard to optimize the combination parameters. In this paper, we propose a novel positional language model (PLM) which implements both heuristics in a unified language model. The key idea is to define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of "soft" passage retrieval. We propose and study several representative density functions and several different PLM-based document ranking strategies. Experiment results on standard TREC test collections show that the PLM is effective for passage retrieval and performs better than a state-of-the-art proximity-based retrieval model.

#index 1227615
#* A bayesian learning approach to promoting diversity in ranking for biomedical information retrieval
#@ Xiangji Huang;Qinmin Hu
#t 2009
#c 13
#% 169781
#% 262112
#% 397133
#% 642975
#% 818266
#% 879702
#% 879706
#% 915281
#! In this paper, we propose a Bayesian learning approach to promoting diversity for information retrieval in biomedicine and a re-ranking model to improve retrieval performance in the biomedical domain. First, the re-ranking model computes the maximum posterior probability of the hidden property corresponding to each retrieved passage. Then it iteratively groups the passages into subsets according to their properties. Finally, these passages are re-ranked from the subsets as our output. There is no need for our proposed method to use any external biomedical resource. We evaluate our Bayesian learning approach by conducting extensive experiments on the TREC 2004-2007 Genomics data sets. The experimental results show the effectiveness of the proposed Bayesian learning approach for promoting diversity in ranking for biomedical information retrieval on four years TREC data sets.

#index 1227616
#* Sources of evidence for vertical selection
#@ Jaime Arguello;Fernando Diaz;Jamie Callan;Jean-Francois Crespo
#t 2009
#c 13
#% 194246
#% 280856
#% 287463
#% 340146
#% 397161
#% 567255
#% 643012
#% 844287
#% 853542
#% 853543
#% 879581
#% 946523
#% 985828
#% 1074093
#% 1166523
#! Web search providers often include search services for domain-specific subcollections, called verticals, such as news, images, videos, job postings, company summaries, and artist profiles. We address the problem of vertical selection, predicting relevant verticals (if any) for queries issued to the search engine's main web search page. In contrast to prior query classification and resource selection tasks, vertical selection is associated with unique resources that can inform the classification decision. We focus on three sources of evidence: (1) the query string, from which features are derived independent of external resources, (2) logs of queries previously issued directly to the vertical, and (3) corpora representative of vertical content. We focus on 18 different verticals, which differ in terms of semantics, media type, size, and level of query traffic. We compare our method to prior work in federated search and retrieval effectiveness prediction. An in-depth error analysis reveals unique challenges across different verticals and provides insight into vertical selection for future work.

#index 1227617
#* Adaptation of offline vertical selection predictions in the presence of user feedback
#@ Fernando Diaz;Jaime Arguello
#t 2009
#c 13
#% 47152
#% 397161
#% 448736
#% 577224
#% 578873
#% 822126
#% 844287
#% 869500
#% 879581
#% 956546
#% 987249
#% 1055694
#% 1073970
#% 1074093
#% 1130811
#% 1130878
#% 1166523
#% 1227616
#% 1392432
#! Web search results often integrate content from specialized corpora known as verticals. Given a query, one important aspect of aggregated search is the selection of relevant verticals from a set of candidate verticals. One drawback to previous approaches to vertical selection is that methods have not explicitly modeled user feedback. However, production search systems often record a variety of feedback information. In this paper, we present algorithms for vertical selection which adapt to user feedback. We evaluate algorithms using a novel simulator which models performance of a vertical selector situated in realistic query traffic.

#index 1227618
#* A probabilistic topic-based ranking framework for location-sensitive domain information retrieval
#@ Huajing Li;Zhisheng Li;Wang-Chien Lee;Dik Lun Lee
#t 2009
#c 13
#% 86950
#% 268079
#% 309095
#% 330677
#% 480467
#% 722904
#% 766441
#% 838407
#% 874993
#% 891549
#% 982560
#% 989618
#% 989621
#% 1016368
#% 1030810
#% 1055707
#% 1650298
#! It has been observed that many queries submitted to search engines are location-sensitive. Traditional search techniques fail to interpret the significance of such geographical clues and as such are unable to return highly relevant search results. Although there have been efforts in the literature to support location-aware information retrieval, critical challenges still remain in terms of search result quality and data scalability. In this paper, we propose an innovative probabilistic ranking framework for domain information retrieval where users are interested in a set of location-sensitive topics. Our proposed method recognizes the geographical distribution of topic influence in the process of ranking documents and models it accurately using probabilistic Gaussian Process classifiers. Additionally, we demonstrate the effectiveness of the proposed ranking framework by implementing it in a Web search service for NBA news. Extensive performance evaluation is performed on real Web document collections, which confirms that our proposed mechanism works significantly better (around 29.7% averagely using DCG20 measure) than other popular location-aware information retrieval techniques in ranking quality.

#index 1227619
#* Entropy-biased models for query representation on the click graph
#@ Hongbo Deng;Irwin King;Michael R. Lyu
#t 2009
#c 13
#% 46803
#% 310567
#% 330617
#% 577329
#% 818238
#% 869501
#% 878624
#% 956552
#% 987203
#% 987222
#% 989578
#% 989628
#% 1035578
#% 1055675
#% 1055676
#% 1074071
#% 1074092
#% 1074093
#% 1074105
#% 1130854
#% 1130921
#% 1712595
#! Query log analysis has received substantial attention in recent years, in which the click graph is an important technique for describing the relationship between queries and URLs. State-of-the-art approaches based on the raw click frequencies for modeling the click graph, however, are not noise-eliminated. Nor do they handle heterogeneous query-URL pairs well. In this paper, we investigate and develop a novel entropy-biased framework for modeling click graphs. The intuition behind this model is that various query-URL pairs should be treated differently, i.e., common clicks on less frequent but more specific URLs are of greater value than common clicks on frequent and general URLs. Based on this intuition, we utilize the entropy information of the URLs and introduce a new concept, namely the inverse query frequency (IQF), to weigh the importance (discriminative ability) of a click on a certain URL. The IQF weighting scheme is never explicitly explored or statistically examined for any bipartite graphs in the information retrieval literature. We not only formally define and quantify this scheme, but also incorporate it with the click frequency and user frequency information on the click graph for an effective query representation. To illustrate our methodology, we conduct experiments with the AOL query log data for query similarity analysis and query suggestion tasks. Experimental results demonstrate that considerable improvements in performance are obtained with our entropy-biased models. Moreover, our method can also be applied to other bipartite graphs.

#index 1227620
#* Click-through prediction for news queries
#@ Arnd Christian König;Michael Gamon;Qiang Wu
#t 2009
#c 13
#% 235061
#% 262043
#% 279755
#% 324129
#% 580738
#% 805848
#% 840846
#% 879567
#% 879613
#% 936965
#% 956546
#% 989628
#% 995020
#% 1055713
#% 1074093
#% 1166523
#% 1206844
#% 1206920
#! A growing trend in commercial search engines is the display of specialized content such as news, products, etc. interleaved with web search results. Ideally, this content should be displayed only when it is highly relevant to the search query, as it competes for space with "regular" results and advertisements. One measure of the relevance to the search query is the click-through rate the specialized content achieves when displayed; hence, if we can predict this click-through rate accurately, we can use this as the basis for selecting when to show specialized content. In this paper, we consider the problem of estimating the click-through rate for dedicated news search results. For queries for which news results have been displayed repeatedly before, the click-through rate can be tracked online; however, the key challenge for which previously unseen queries to display news results remains. In this paper we propose a supervised model that offers accurate prediction of news click-through rates and satisfies the requirement of adapting quickly to emerging news events.

#index 1227621
#* Smoothing clickthrough data for web search ranking
#@ Jianfeng Gao;Wei Yuan;Xiao Li;Kefeng Deng;Jian-Yun Nie
#t 2009
#c 13
#% 17144
#% 158687
#% 309095
#% 342961
#% 361100
#% 476738
#% 577224
#% 783482
#% 818221
#% 818239
#% 840846
#% 879567
#% 987222
#% 989578
#% 989628
#% 1073905
#% 1074093
#% 1074107
#% 1130854
#! Incorporating features extracted from clickthrough data (called clickthrough features) has been demonstrated to significantly improve the performance of ranking models for Web search applications. Such benefits, however, are severely limited by the data sparseness problem, i.e., many queries and documents have no or very few clicks. The ranker thus cannot rely strongly on clickthrough features for document ranking. This paper presents two smoothing methods to expand clickthrough data: query clustering via Random Walk on click graphs and a discounting method inspired by the Good-Turing estimator. Both methods are evaluated on real-world data in three Web search domains. Experimental results show that the ranking models trained on smoothed clickthrough features consistently outperform those trained on unsmoothed features. This study demonstrates both the importance and the benefits of dealing with the sparseness problem in clickthrough data.

#index 1227622
#* Predicting user interests from contextual information
#@ Ryen W. White;Peter Bailey;Liwei Chen
#t 2009
#c 13
#% 124010
#% 169739
#% 187999
#% 309095
#% 344925
#% 397191
#% 397203
#% 399057
#% 433965
#% 590523
#% 807420
#% 818206
#% 818207
#% 818259
#% 818260
#% 835027
#% 853542
#% 869536
#% 879567
#% 881540
#% 893640
#% 955711
#% 956495
#% 987198
#% 1004294
#% 1019076
#% 1043044
#% 1064768
#% 1074147
#% 1093793
#% 1166518
#% 1275346
#% 1395335
#! Search and recommendation systems must include contextual information to effectively model users' interests. In this paper, we present a systematic study of the effectiveness of five variant sources of contextual information for user interest modeling. Post-query navigation and general browsing behaviors far outweigh direct search engine interaction as an information-gathering activity. Therefore we conducted this study with a focus on Website recommendations rather than search results. The five contextual information sources used are: social, historic, task, collection, and user interaction. We evaluate the utility of these sources, and overlaps between them, based on how effectively they predict users' future interests. Our findings demonstrate that the sources perform differently depending on the duration of the time window used for future prediction, and that context overlap outperforms any isolated source. Designers of Website suggestion systems can use our findings to provide improved support for post-query navigation and general browsing behaviors.

#index 1227623
#* A comparison of query and term suggestion features for interactive searching
#@ Diane Kelly;Karl Gyllstrom;Earl W. Bailey
#t 2009
#c 13
#% 194299
#% 232713
#% 310567
#% 329090
#% 346553
#% 411762
#% 420524
#% 643001
#% 730007
#% 742666
#% 803556
#% 869501
#% 879621
#% 936914
#% 943042
#% 987212
#% 1130854
#% 1415709
#! Query formulation is one of the most difficult and important aspects of information seeking and retrieval. Two techniques, term relevance feedback and query suggestion, provide methods to help users formulate queries, but each is limited in different ways. In this research we combine these two techniques by automatically creating query suggestions using term relevance feedback techniques. To evaluate our approach, we conducted an interactive information retrieval study with 55 subjects and 20 topics. Each subject completed four topics, half with a term suggestion system and half with a query suggestion system. We also investigated the source of the suggestions: approximately half of all subjects were provided with system-generated suggestions, while half were provided with user-generated suggestions. Results show that subjects used more query suggestions than term suggestions and saved more documents with these suggestions, even though there were no significant differences in performance. Subjects preferred the query suggestion system and rated it higher along a number of dimensions including its ability to help them think of new approaches to searching. Qualitative data provided insight into subjects' usage and ratings, and indicated that subjects often used the suggestions even when they did not click on them.

#index 1227624
#* An aspectual interface for supporting complex search tasks
#@ Robert Villa;Iván Cantador;Hideo Joho;Joemon M. Jose
#t 2009
#c 13
#% 116374
#% 118771
#% 187999
#% 303510
#% 781141
#% 782282
#% 857477
#% 857478
#% 857479
#% 1014717
#% 1131881
#! With the increasing importance of search systems on the web, there is a continuing push to design interfaces which are a better match with the kinds of real-world tasks in which users are engaged. In this paper, we consider how broad, complex search tasks may be supported via the search interface. In particular, we consider search tasks which may be composed of multiple aspects, or multiple related subtasks. For example, in decision making tasks the user may investigate multiple possible solutions before settling on a single, final solution, while other tasks, such as report writing, may involve searching on multiple interrelated topics. A search interface is presented which is designed to support such broad search tasks, allowing a user to create search aspects, each of which models an independent subtask of some larger task. The interface is built on the intuition that users should be able to structure their searching environment when engaged on complex search tasks, where the act of structuring and organization may aid the user in understanding his or her task. A user study was carried out which compared our aspectual interface to a standard web-search interface. The results suggest that an aspectual interface can aid users when engaged in broad search tasks where the search aspects must be identified during searching; for a task where search aspects were pre-defined, no advantage over the baseline was found. Results for a decision making task were less clear cut, but show some evidence for improved task performance.

#index 1227625
#* Combining audio content and social context for semantic music discovery
#@ Douglas R. Turnbull;Luke Barrington;Gert Lanckriet;Mehrdad Yazdani
#t 2009
#c 13
#% 251145
#% 340934
#% 577224
#% 577298
#% 734915
#% 743284
#% 763697
#% 771841
#% 839912
#% 840066
#% 987247
#% 987248
#% 1077150
#% 1415773
#% 1767359
#% 1767718
#! When attempting to annotate music, it is important to consider both acoustic content and social context. This paper explores techniques for collecting and combining multiple sources of such information for the purpose of building a query-by-text music retrieval system. We consider two representations of the acoustic content (related to timbre and harmony) and two social sources (social tags and web documents). We then compare three algorithms that combine these information sources: calibrated score averaging (CSA), RankBoost, and kernel combination support vector machines (KC-SVM). We demonstrate empirically that each of these algorithms is superior to algorithms that use individual information sources.

#index 1227626
#* Automatic video tagging using content redundancy
#@ Stefan Siersdorfer;Jose San Pedro;Mark Sanderson
#t 2009
#c 13
#% 260001
#% 262043
#% 290830
#% 340995
#% 347225
#% 397133
#% 650944
#% 722756
#% 780859
#% 818266
#% 818916
#% 879617
#% 956507
#% 987220
#% 987222
#% 990298
#% 997096
#% 1002005
#% 1002006
#% 1055702
#% 1132021
#% 1657461
#% 1667787
#% 1775571
#! The analysis of the leading social video sharing platform YouTube reveals a high amount of redundancy, in the form of videos with overlapping or duplicated content. In this paper, we show that this redundancy can provide useful information about connections between videos. We reveal these links using robust content-based video analysis techniques and exploit them for generating new tag assignments. To this end, we propose different tag propagation methods for automatically obtaining richer video annotations. Our techniques provide the user with additional information about videos, and lead to enhanced feature representations for applications such as automatic data organization and search. Experiments on video clustering and classification as well as a user evaluation demonstrate the viability of our approach.

#index 1227627
#* CompositeMap: a novel framework for music similarity measure
#@ Bingjun Zhang;Jialie Shen;Qiaoliang Xiang;Ye Wang
#t 2009
#c 13
#% 321455
#% 741122
#% 741169
#% 849875
#% 849879
#% 898309
#% 961137
#% 987247
#% 1073988
#% 1077150
#% 1086216
#% 1137661
#% 1767413
#! With the continuing advances in data storage and communication technology, there has been an explosive growth of music information from different application domains. As an effective technique for organizing, browsing, and searching large data collections, music information retrieval is attracting more and more attention. How to measure and model the similarity between different music items is one of the most fundamental yet challenging research problems. In this paper, we introduce a novel framework based on a multimodal and adaptive similarity measure for various applications. Distinguished from previous approaches, our system can effectively combine music properties from different aspects into a compact signature via supervised learning. In addition, an incremental Locality Sensitive Hashing algorithm has been developed to support efficient retrieval processes with different kinds of queries. Experimental results based on two large music collections reveal various advantages of the proposed framework including effectiveness, efficiency, adaptiveness, and scalability.

#index 1227628
#* Quantifying performance and quality gains in distributed web search engines
#@ B Barla Cambazoglu;Vassilis Plachouras;Ricardo Baeza-Yates
#t 2009
#c 13
#% 194246
#% 268087
#% 290703
#% 330609
#% 348136
#% 643013
#% 807302
#% 878657
#% 1132154
#! Distributed search engines based on geographical partitioning of a central Web index emerge as a feasible solution to the immense growth of the Web, user bases, and query traffic. However, there is still lack of research in quantifying the performance and quality gains that can be achieved by such architectures. In this paper, we develop various cost models to evaluate the performance benefits of a geographically distributed search engine architecture based on partial index replication and query forwarding. Specifically, we focus on possible performance gains due to the distributed nature of query processing and Web crawling processes. We show that any response time gain achieved by distributed query processing can be utilized to improve search relevance as the use of complex but more accurate algorithms can now be enabled for document ranking. We also show that distributed Web crawling leads to better Web coverage and try to see if this improves the search quality. We verify the validity of our claims over large, real-life datasets via simulations.

#index 1227629
#* SUSHI: scoring scaled samples for server selection
#@ Paul Thomas;Milad Shokouhi
#t 2009
#c 13
#% 194246
#% 268114
#% 273926
#% 280856
#% 282422
#% 342741
#% 453327
#% 481748
#% 643012
#% 660301
#% 722312
#% 729027
#% 783473
#% 818211
#% 818212
#% 818221
#% 852010
#% 879604
#% 995515
#% 1174737
#% 1392444
#! Modern techniques for distributed information retrieval use a set of documents sampled from each server, but these samples have been underutilised in server selection. We describe a new server selection algorithm, SUSHI, which unlike earlier algorithms can make full use of the text of each sampled document and which does not need training data. SUSHI can directly optimise for many common cases, including high precision retrieval, and by including a simple stopping condition can do so while reducing network traffic. Our experiments compare SUSHI with alternatives and show it achieves the same effectiveness as the best current methods while being substantially more efficient, selecting as few as 20% as many servers.

#index 1227630
#* Effective query expansion for federated search
#@ Milad Shokouhi;Leif Azzopardi;Paul Thomas
#t 2009
#c 13
#% 36672
#% 218978
#% 227891
#% 280856
#% 340146
#% 340901
#% 342679
#% 342710
#% 342961
#% 447946
#% 643012
#% 722312
#% 783473
#% 818211
#% 818221
#% 822126
#% 838532
#% 852010
#% 869501
#% 879604
#% 985828
#% 1055676
#% 1074081
#% 1392444
#! While query expansion techniques have been shown to improve retrieval performance in a centralized setting, they have not been well studied in a federated setting. In this paper, we consider how query expansion may be adapted to federated environments and propose several new methods: where focused expansions are used in a selective fashion to produce specific queries for each source (or a set of sources). On a number of different testbeds, we show that focused query expansion can significantly outperform the previously proposed global expansion method, and---contrary to earlier work---show that query expansion can improve performance over standard federated retrieval. These findings motivate further research examining the different methods for query expansion, and other forms of system and user interaction, in order to continue improving the performance of interactive federated search systems.

#index 1227631
#* On rank correlation and the distance between rankings
#@ Ben Carterette
#t 2009
#c 13
#% 51647
#% 340890
#% 643020
#% 757953
#% 766409
#% 857180
#% 879630
#% 983654
#% 987201
#% 987353
#% 987354
#% 1019124
#% 1074124
#! Rank correlation statistics are useful for determining whether a there is a correspondence between two measurements, particularly when the measures themselves are of less interest than their relative ordering. Kendall's - in particular has found use in Information Retrieval as a "meta-evaluation" measure: it has been used to compare evaluation measures, evaluate system rankings, and evaluate predicted performance. In the meta-evaluation domain, however, correlations between systems confound relationships between measurements, practically guaranteeing a positive and significant estimate of - regardless of any actual correlation between the measurements. We introduce an alternative measure of distance between rankings that corrects this by explicitly accounting for correlations between systems over a sample of topics, and moreover has a probabilistic interpretation for use in a test of statistical significance. We validate our measure with theory, simulated data, and experiment.

#index 1227632
#* Score adjustment for correction of pooling bias
#@ William Webber;Laurence A. F. Park
#t 2009
#c 13
#% 262097
#% 766409
#% 818221
#% 879598
#% 879632
#% 907496
#% 987199
#% 987201
#% 987239
#% 1002317
#% 1074126
#% 1074132
#% 1095876
#% 1130812
#% 1130864
#% 1130865
#! Information retrieval systems are evaluated against test collections of topics, documents, and assessments of which documents are relevant to which topics. Documents are chosen for relevance assessment by pooling runs from a set of existing systems. New systems can return unassessed documents, leading to an evaluation bias against them. In this paper, we propose to estimate the degree of bias against an unpooled system, and to adjust the system's score accordingly. Bias estimation can be done via leave-one-out experiments on the existing, pooled systems, but this requires the problematic assumption that the new system is similar to the existing ones. Instead, we propose that all systems, new and pooled, be fully assessed against a common set of topics, and the bias observed against the new system on the common topics be used to adjust scores on the existing topics. We demonstrate using resampling experiments on TREC test sets that our method leads to a marked reduction in error, even with only a relatively small number of common topics, and that the error decreases as the number of topics increases.

#index 1227633
#* Towards methods for the collective gathering and quality control of relevance assessments
#@ Gabriella Kazai;Natasa Milic-Frayling;Jamie Costello
#t 2009
#c 13
#% 262102
#% 309127
#% 320355
#% 340890
#% 751818
#% 766410
#% 857180
#% 987353
#% 1065099
#% 1074126
#% 1074134
#% 1095875
#! Growing interest in online collections of digital books and video content motivates the development and optimization of adequate retrieval systems. However, traditional methods for collecting relevance assessments to tune system performance are challenged by the nature of digital items in such collections, where assessors are faced with a considerable effort to review and assess content by extensive reading, browsing, and within-document searching. The extra strain is caused by the length and cohesion of the digital item and the dispersion of topics within it. We propose a method for the collective gathering of relevance assessments using a social game model to instigate participants' engagement. The game provides incentives for assessors to follow a predefined review procedure and makes provisions for the quality control of the collected relevance judgments. We discuss the approach in detail, and present the results of a pilot study conducted on a book corpus to validate the approach. Our analysis reveals intricate relationships between the affordances of the system, the incentives of the social game, and the behavior of the assessors. We show that the proposed game design achieves two designated goals: the incentive structure motivates endurance in assessors and the review process encourages truthful assessment.

#index 1227634
#* On the local optimality of LambdaRank
#@ Pinar Donmez;Krysta M. Svore;Christopher J.C. Burges
#t 2009
#c 13
#% 770763
#% 840846
#% 840947
#% 976949
#% 983820
#% 987226
#% 987241
#% 1039843
#! A machine learning approach to learning to rank trains a model to optimize a target evaluation measure with repect to training data. Currently, existing information retrieval measures are impossible to optimize directly except for models with a very small number of parameters. The IR community thus faces a major challenge: how to optimize IR measures of interest directly. In this paper, we present a solution. Specifically, we show that LambdaRank, which smoothly approximates the gradient of the target measure, can be adapted to work with four popular IR target evaluation measures using the same underlying gradient construction. It is likely, therefore, that this construction is extendable to other evaluation measures. We empirically show that LambdaRank finds a locally optimal solution for mean NDCG@10, mean NDCG, MAP and MRR with a 99% confidence rate. We also show that the amount of effective training data varies with IR measure and that with a sufficiently large training set size, matching the training optimization measure to the target evaluation measure yields the best accuracy.

#index 1227635
#* Document selection methodologies for efficient and effective learning-to-rank
#@ Javed A. Aslam;Evangelos Kanoulas;Virgil Pavlu;Stefan Savev;Emine Yilmaz
#t 2009
#c 13
#% 262102
#% 324129
#% 643036
#% 734915
#% 750863
#% 840846
#% 840882
#% 879598
#% 879632
#% 881477
#% 907496
#% 907546
#% 1074132
#! Learning-to-rank has attracted great attention in the IR community. Much thought and research has been placed on query-document feature extraction and development of sophisticated learning-to-rank algorithms. However, relatively little research has been conducted on selecting documents for learning-to-rank data sets nor on the effect of these choices on the efficiency and effectiveness of learning-to-rank algorithms. In this paper, we employ a number of document selection methodologies, widely used in the context of evaluation--depth-k pooling, sampling (infAP, statAP), active-learning (MTC), and on-line heuristics (hedge). Certain methodologies, e.g. sampling and active-learning, have been shown to lead to efficient and effective evaluation. We investigate whether they can also enable efficient and effective learning-to-rank. We compare them with the document selection methodology used to create the LETOR datasets. Further, all of the utilized methodologies are different in nature, and thus they construct training data sets with different properties, such as the proportion of relevant documents in the data or the similarity among them. We study how such properties affect the efficiency, effectiveness, and robustness of learning-to-rank collections.

#index 1227636
#* An improved markov random field model for supporting verbose queries
#@ Matthew Lease
#t 2009
#c 13
#% 218982
#% 262096
#% 268079
#% 306494
#% 324129
#% 340899
#% 340901
#% 397205
#% 742399
#% 750863
#% 766428
#% 818262
#% 827581
#% 940010
#% 976952
#% 980535
#% 987231
#% 987332
#% 1019124
#% 1024555
#% 1074052
#% 1074112
#% 1715627
#! Recent work in supervised learning of term-based retrieval models has shown significantly improved accuracy can often be achieved via better model estimation. In this paper, we show retrieval accuracy with Metzler and Croft's Markov random field (MRF) approach can be similarly improved via supervised learning. While the original MRF method estimates a parameter for each of its three feature classes from data, parameters within each class are set via a uniform weighting scheme adopted from the standard unigram. We conjecture greater MRF retrieval accuracy should be possible by better estimating within-class parameters, particularly for verbose queries employing natural language terms. Retrieval experiments with these queries on three TREC document collections show our improved MRF consistently out-performs both the original MRF and supervised unigram baselines. Additional experiments using blind-feedback and evaluation with optimal weighting demonstrate both the immediate value and further potential of our method.

#index 1227637
#* Placing flickr photos on a map
#@ Pavel Serdyukov;Vanessa Murdock;Roelof van Zwol
#t 2009
#c 13
#% 397129
#% 466078
#% 480467
#% 730144
#% 766441
#% 780723
#% 809461
#% 869516
#% 907542
#% 955010
#% 967244
#% 987205
#% 1016371
#% 1055701
#% 1055704
#% 1055707
#% 1055857
#% 1055919
#% 1074081
#% 1125816
#% 1130922
#% 1131843
#% 1166511
#% 1190131
#! In this paper we investigate generic methods for placing photos uploaded to Flickr on the World map. As primary input for our methods we use the textual annotations provided by the users to predict the single most probable location where the image was taken. Central to our approach is a language model based entirely on the annotations provided by users. We define extensions to improve over the language model using tag-based smoothing and cell-based smoothing, and leveraging spatial ambiguity. Further we demonstrate how to incorporate GeoNames\footnote{http://www.geonames.org visited May 2009}, a large external database of locations. For varying levels of granularity, we are able to place images on a map with at least twice the precision of the state-of-the-art reported in the literature.

#index 1227638
#* An automatic translation of tags for multimedia contents using folksonomy networks
#@ Tae-Gil Noh;Seong-Bae Park;Hee-Geun Yoon;Sang-Jo Lee;Se-Young Park
#t 2009
#c 13
#% 340895
#% 735135
#% 818268
#% 833065
#% 869504
#% 879589
#% 951637
#% 956515
#% 1053979
#% 1132472
#% 1655418
#! This paper proposes a novel method to translate tags attached to multimedia contents for cross-language retrieval. The main issue in this problem is the sense disambiguation of tags given with few textual contexts. In order to solve this problem, the proposed method represents both tags and its translation candidates as networks of co-occurring tags since a network allows richer expression of contexts than other expressions such as co-occurrence vectors. The method translates a tag by selecting the optimal one from possible candidates based on a network similarity even when neither the textual contexts nor sophisticated language resources are available. The experiments on the MIR Flickr-2008 test set show that the proposed method achieves 90.44% accuracy in translating tags from English into German, which is significantly higher than the baseline methods of a frequency based translation and a co-occurrence-based translation.

#index 1227639
#* CrowdReranking: exploring multiple search engines for visual search reranking
#@ Yuan Liu;Tao Mei;Xian-Sheng Hua
#t 2009
#c 13
#% 268079
#% 309095
#% 724204
#% 724320
#% 836904
#% 860956
#% 867805
#% 905105
#% 990300
#% 997240
#% 997242
#% 1055702
#% 1131845
#% 1132501
#% 1389558
#! Most existing approaches to visual search reranking predominantly focus on mining information within the initial search results. However, the initial ranked list cannot provide enough cues for reranking by itself due to the typically unsatisfying visual search performance. This paper presents a new method for visual search reranking called CrowdReranking, which is characterized by mining relevant visual patterns from image search results of multiple search engines which are available on the Internet. Observing that different search engines might have different data sources for indexing and methods for ranking, it is reasonable to assume that there exist different search results yet certain common visual patterns relevant to a given query among those results. We first construct a set of visual words based on the local image patches collected from multiple image search engines. We then explicitly detect two kinds of visual patterns, i.e., salient and concurrent patterns, among the visual words. Theoretically, we formalize reranking as an optimization problem on the basis of the mined visual patterns and propose a close-form solution. Empirically, we conduct extensive experiments on several real-world search engines and one benchmark dataset, and show that the proposed CrowdReranking is superior to the state-of-the-art works.

#index 1227640
#* Including summaries in system evaluation
#@ Andrew Turpin;Falk Scholer;Kalvero Jarvelin;Mingfang Wu;J. Shane Culpepper
#t 2009
#c 13
#% 285
#% 248065
#% 262036
#% 262105
#% 309089
#% 340892
#% 340921
#% 340957
#% 397164
#% 411762
#% 413586
#% 766409
#% 766410
#% 818221
#% 818257
#% 835027
#% 838529
#% 857180
#% 879566
#% 907496
#% 987354
#% 1074058
#% 1149990
#% 1192696
#! In batch evaluation of retrieval systems, performance is calculated based on predetermined relevance judgements applied to a list of documents returned by the system for a query. This evaluation paradigm, however, ignores the current standard operation of search systems which require the user to view summaries of documents prior to reading the documents themselves. In this paper we modify the popular IR metrics MAP and P@10 to incorporate the summary reading step of the search process, and study the effects on system rankings using TREC data. Based on a user study, we establish likely disagreements between relevance judgements of summaries and of documents, and use these values to seed simulations of summary relevance in the TREC data. Re-evaluating the runs submitted to the TREC Web Track, we find the average correlation between system rankings and the original TREC rankings is 0.8 (Kendall τ), which is lower than commonly accepted for system orderings to be considered equivalent. The system that has the highest MAP in TREC generally remains amongst the highest MAP systems when summaries are taken into account, but other systems become equivalent to the top ranked system depending on the simulated summary relevance. Given that system orderings alter when summaries are taken into account, the small amount of effort required to judge summaries in addition to documents (19 seconds vs 88 seconds on average in our data) should be undertaken when constructing test collections.

#index 1227641
#* When more is less: the paradox of choice in search engine use
#@ Antti Oulasvirta;Janne P. Hukkinen;Barry Schwartz
#t 2009
#c 13
#% 272913
#% 296646
#% 590523
#% 766447
#% 766472
#% 793014
#% 804805
#% 805898
#% 860724
#% 874715
#% 954948
#% 987209
#% 1678005
#! In numerous everyday domains, it has been demonstrated that increasing the number of options beyond a handful can lead to paralysis and poor choice and decrease satisfaction with the choice. Were this so-called paradox of choice to hold in search engine use, it would mean that increasing recall can actually work counter to user satisfaction if it implies choice from a more extensive set of result items. The existence of this effect was demonstrated in an experiment where users (N=24) were shown a search scenario and a query and were required to choose the best result item within 30 seconds. Having to choose from six results yielded both higher subjective satisfaction with the choice and greater confidence in its correctness than when there were 24 items on the results page. We discuss this finding in the wider context of "choice architecture"--that is, how result presentation affects choice and satisfaction.

#index 1227642
#* Where to stop reading a ranked list?: threshold optimization using truncated score distributions
#@ Avi Arampatzis;Jaap Kamps;Stephen Robertson
#t 2009
#c 13
#% 280854
#% 340934
#% 340938
#% 340941
#% 344447
#% 1392434
#! Ranked retrieval has a particular disadvantage in comparison with traditional Boolean retrieval: there is no clear cut-off point where to stop consulting results. This is a serious problem in some setups. We investigate and further develop methods to select the rank cut-off value which optimizes a given effectiveness measure. Assuming no other input than a system's output for a query--document scores and their distribution--the task is essentially a score-distributional threshold optimization problem. The recent trend in modeling score distributions is to use a normal-exponential mixture: normal for relevant, and exponential for non-relevant document scores. We discuss the two main theoretical problems with the current model, support incompatibility and non-convexity, and develop new models that address them. The main contributions of the paper are two truncated normal-exponential models, varying in the way the out-truncated score ranges are handled. We conduct a range of experiments using the TREC 2007 and 2008 Legal Track data, and show that the truncated models lead to significantly better results.

#index 1227643
#* The wisdom of the few: a collaborative filtering approach based on expert opinions from the web
#@ Xavier Amatriain;Neal Lathia;Josep M. Pujol;Haewoon Kwak;Nuria Oliver
#t 2009
#c 13
#% 173879
#% 266281
#% 330687
#% 342687
#% 420539
#% 734594
#% 766448
#% 790459
#% 805841
#% 813966
#% 818216
#% 848642
#% 874158
#% 879629
#% 982676
#% 987197
#% 987261
#% 1044003
#% 1055682
#% 1074060
#% 1116993
#% 1704266
#! Nearest-neighbor collaborative filtering provides a successful means of generating recommendations for web users. However, this approach suffers from several shortcomings, including data sparsity and noise, the cold-start problem, and scalability. In this work, we present a novel method for recommending items to users based on expert opinions. Our method is a variation of traditional collaborative filtering: rather than applying a nearest neighbor algorithm to the user-rating data, predictions are computed using a set of expert neighbors from an independent dataset, whose opinions are weighted according to their similarity to the user. This method promises to address some of the weaknesses in traditional collaborative filtering, while maintaining comparable accuracy. We validate our approach by predicting a subset of the Netflix data set. We use ratings crawled from a web portal of expert reviews, measuring results both in terms of prediction accuracy and recommendation list precision. Finally, we explore the ability of our method to generate useful recommendations, by reporting the results of a user-study where users prefer the recommendations generated by our approach.

#index 1227644
#* Personalized tag recommendation using graph-based ranking on multi-type interrelated objects
#@ Ziyu Guan;Jiajun Bu;Qiaozhu Mei;Chun Chen;Can Wang
#t 2009
#c 13
#% 282905
#% 751818
#% 791402
#% 805877
#% 813966
#% 855601
#% 875948
#% 905319
#% 956515
#% 956579
#% 1034713
#% 1055704
#% 1055739
#% 1074115
#% 1127458
#% 1130854
#! Social tagging is becoming increasingly popular in many Web 2.0 applications where users can annotate resources (e.g. Web pages) with arbitrary keywords (i.e. tags). A tag recommendation module can assist users in tagging process by suggesting relevant tags to them. It can also be directly used to expand the set of tags annotating a resource. The benefits are twofold: improving user experience and enriching the index of resources. However, the former one is not emphasized in previous studies, though a lot of work has reported that different users may describe the same concept in different ways. We address the problem of personalized tag recommendation for text documents. In particular, we model personalized tag recommendation as a "query and ranking" problem and propose a novel graph-based ranking algorithm for interrelated multi-type objects. When a user issues a tagging request, both the document and the user are treated as a part of the query. Tags are then ranked by our graph-based ranking algorithm which takes into consideration both relevance to the document and preference of the user. Finally, the top ranked tags are presented to the user as suggestions. Experiments on a large-scale tagging data set collected from Del.icio.us have demonstrated that our proposed algorithm significantly outperforms algorithms which fail to consider the diversity of different users' interests.

#index 1227645
#* Leveraging sources of collective wisdom on the web for discovering technology synergies
#@ Cai-Nicolas Ziegler;Stefan Jung
#t 2009
#c 13
#% 198058
#% 202011
#% 387427
#% 447948
#% 465914
#% 641963
#% 805841
#% 805849
#% 869500
#% 907533
#% 956570
#! One of the central tasks of R&D strategy and portfolio management at large technology companies and research institutions refers to the identification of technological synergies throughout the organization. These efforts are geared towards saving resources by consolidating scattered expertise, sharing best practices, and reusing available technologies across multiple product lines. In the past, this task has been done in a manual evaluation process by technical domain experts. While feasible, the major drawback of this approach is the enormous effort in terms of availability and time: For a structured and complete analysis every combination of any two technologies has to be rated explicitly. We present a novel approach that recommends technological synergies in an automated fashion, making use of abundant collective wisdom from the Web, both in pure textual form as well as classification ontologies. Our method has been deployed for practical support of the synergy evaluation process within our company. We have also conducted empirical evaluations based on randomly selected technology pairs so as to benchmark the accuracy of our approach, as compared to a group of general computer science technologists as well as a control group of domain experts.

#index 1227646
#* Query side evaluation: an empirical analysis of effectiveness and effort
#@ Leif Azzopardi
#t 2009
#c 13
#% 2689
#% 280850
#% 340901
#% 340948
#% 375076
#% 397161
#% 449285
#% 804915
#% 810629
#% 818260
#% 818267
#% 874504
#% 879650
#% 987249
#% 1074200
#% 1415713
#! Typically, Information Retrieval evaluation focuses on measuring the performance of the system's ability at retrieving relevant information, and not the query's ability. However, the effectiveness of a retrieval system is strongly influenced by the quality of the query submitted. In this paper, the effectiveness and effort of querying is empirically examined in the context of the Principle of Least Effort, Zipf's Law and the Law of Diminishing Returns. This query focused investigation leads to a number of novel findings which should prove useful in the development of future retrieval methods and evaluation techniques. While, also motivating further research into query side evaluation.

#index 1227647
#* Reducing long queries using query quality predictors
#@ Giridhar Kumaran;Vitor R. Carvalho
#t 2009
#c 13
#% 78171
#% 144034
#% 184489
#% 262096
#% 269217
#% 375017
#% 397161
#% 577224
#% 750863
#% 818267
#% 879583
#% 879614
#% 907544
#% 907546
#% 939376
#% 1019166
#% 1074052
#% 1074112
#% 1130909
#% 1130990
#% 1265149
#% 1415713
#% 1415785
#! Long queries frequently contain many extraneous terms that hinder retrieval of relevant documents. We present techniques to reduce long queries to more effective shorter ones that lack those extraneous terms. Our work is motivated by the observation that perfectly reducing long TREC description queries can lead to an average improvement of 30% in mean average precision. Our approach involves transforming the reduction problem into a problem of learning to rank all sub-sets of the original query (sub-queries) based on their predicted quality, and selecting the top sub-query. We use various measures of query quality described in the literature as features to represent sub-queries, and train a classifier. Replacing the original long query with the top-ranked sub-query chosen by the ranker results in a statistically significant average improvement of 8% on our test sets. Analysis of the results shows that query reduction is well-suited for moderately-performing long queries, and a small set of query quality predictors are well-suited for the task of ranking sub-queries.

#index 1227648
#* Extracting structured information from user queries with semi-supervised conditional random fields
#@ Xiao Li;Ye-Yi Wang;Alex Acero
#t 2009
#c 13
#% 464434
#% 643004
#% 654469
#% 748550
#% 818244
#% 843654
#% 939377
#% 939527
#% 989662
#% 1013670
#% 1074055
#% 1074093
#% 1074125
#% 1264824
#! When search is against structured documents, it is beneficial to extract information from user queries in a format that is consistent with the backend data structure. As one step toward this goal, we study the problem of query tagging which is to assign each query term to a pre-defined category. Our problem could be approached by learning a conditional random field (CRF) model (or other statistical models) in a supervised fashion, but this would require substantial human-annotation effort. In this work, we focus on a semi-supervised learning method for CRFs that utilizes two data sources: (1) a small amount of manually-labeled queries, and (2) a large amount of queries in which some word tokens have derived labels, i.e., label information automatically obtained from additional resources. We present two principled ways of encoding derived label information in a CRF model. Such information is viewed as hard evidence in one setting and as soft evidence in the other. In addition to the general methodology of how to use derived labels in semi-supervised CRFs, we also present a practical method on how to obtain them by leveraging user click data and an in-domain database that contains structured documents. Evaluation on product search queries shows the effectiveness of our approach in improving tagging accuracies.

#index 1227649
#* Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval
#@ James Allan;Javed Aslam;Mark Sanderson;ChengXiang Zhai;Justin Zobel
#t 2009
#c 13

#index 1227650
#* The impact of crawl policy on web search effectiveness
#@ Dennis Fetterly;Nick Craswell;Vishwa Vinay
#t 2009
#c 13
#% 268087
#% 281166
#% 411762
#% 480136
#% 577330
#% 577370
#% 728102
#% 754058
#% 754090
#% 807302
#% 956536
#% 987211
#% 987251
#% 1022233
#% 1035570
#% 1055714
#% 1599360
#! Crawl selection policy has a direct influence on Web search effectiveness, because a useful page that is not selected for crawling will also be absent from search results. Yet there has been little or no work on measuring this effect. We introduce an evaluation framework, based on relevance judgments pooled from multiple search engines, measuring the maximum potential NDCG that is achievable using a particular crawl. This allows us to evaluate different crawl policies and investigate important scenarios like selection stability over multiple iterations. We conduct two sets of crawling experiments at the scale of 1~billion and 100~million pages respectively. These show that crawl selection based on PageRank, indegree and trans-domain indegree all allow better retrieval effectiveness than a simple breadth-first crawl of the same size. PageRank is the most reliable and effective method. Trans-domain indegree can outperform PageRank, but over multiple crawl iterations it is less effective and more unstable. Finally we experiment with combinations of crawl selection methods and per-domain page limits, which yield crawls with greater potential NDCG than PageRank.

#index 1227651
#* Optimizing search engine revenue in sponsored search
#@ Yunzhang Zhu;Gang Wang;Junli Yang;Dakan Wang;Jun Yan;Jian Hu;Zheng Chen
#t 2009
#c 13
#% 577224
#% 766472
#% 818221
#% 879633
#! Displaying sponsored ads alongside the search results is a key monetization strategy for search engine companies. Since users are more likely to click ads that are relevant to their query, it is crucial for search engine to deliver the right ads for the query and the order in which they are displayed. There are several works investigating on how to learn a ranking function to maximize the number of ad clicks. In this paper, we address a new revenue optimization problem and aim to answer the question: how to construct a ranking model that can deliver high quality ads to the user as well as maximize search engine revenue? We introduce two novel methods from di fferent machine learning perspectives, and both of them take the revenue component into careful considerations. The algorithms are built upon the click-through log data with real ad clicks and impressions. The extensively experimental results verify the proposed algorithm that can produce more revenue than other methods as well as avoid losing relevance accuracy. To provide deep insight into the importance of each feature to search engine revenue, we extract twelve basic features from four categories. The experimental study provides a feature ranking list according to the revenue benefit of each feature.

#index 1227652
#* Web-derived resources for web information retrieval: from conceptual hierarchies to attribute hierarchies
#@ Marius Paşca;Enrique Alfonseca
#t 2009
#c 13
#% 309127
#% 742092
#% 756964
#% 815297
#% 939601
#% 956503
#% 956564
#% 987250
#% 1022765
#% 1260755
#% 1270282
#% 1271271
#% 1712125
#! A weakly-supervised extraction method identifies concepts within conceptual hierarchies, at the appropriate level of specificity (e.g., Bank vs. Institution), to which attributes (e.g., routing number) extracted from unstructured text best apply. The extraction exploits labeled classes of instances acquired from a combination of Web documents and query logs, and inserted into existing conceptual hierarchies. The correct concept is identified within the top three positions on average over gold-standard attributes, which corresponds to higher accuracy than in alternative experiments.

#index 1227653
#* Spam filter evaluation with imprecise ground truth
#@ Gordon V. Cormack;Aleksander Kolcz
#t 2009
#c 13
#% 344447
#% 448801
#% 879580
#% 960411
#% 961230
#% 987244
#% 1279287
#! When trained and evaluated on accurately labeled datasets, online email spam filters are remarkably effective, achieving error rates an order of magnitude better than classifiers in similar applications. But labels acquired from user feedback or third-party adjudication exhibit higher error rates than the best filters -- even filters trained using the same source of labels. It is appropriate to use naturally occuring labels -- including errors -- as training data in evaluating spam filters. Erroneous labels are problematic, however, when used as ground truth to measure filter effectiveness. Any measurement of the filter's error rate will be augmented and perhaps masked by the label error rate. Using two natural sources of labels, we demonstrate automatic and semi-automatic methods that reduce the influence of labeling errors on evaluation, yielding substantially more precise measurements of true filter error rates.

#index 1227654
#* Telling experts from spammers: expertise ranking in folksonomies
#@ Michael G. Noll;Ching-man Au Yeung;Nicholas Gibbins;Christoph Meinel;Nigel Shadbolt
#t 2009
#c 13
#% 290830
#% 413655
#% 662755
#% 889651
#% 946524
#% 956516
#% 958000
#% 991856
#% 1006352
#% 1035588
#% 1052961
#% 1116996
#% 1415732
#% 1667787
#! With a suitable algorithm for ranking the expertise of a user in a collaborative tagging system, we will be able to identify experts and discover useful and relevant resources through them. We propose that the level of expertise of a user with respect to a particular topic is mainly determined by two factors. Firstly, an expert should possess a high quality collection of resources, while the quality of a Web resource depends on the expertise of the users who have assigned tags to it. Secondly, an expert should be one who tends to identify interesting or useful resources before other users do. We propose a graph-based algorithm, SPEAR (SPamming-resistant Expertise Analysis and Ranking), which implements these ideas for ranking users in a folksonomy. We evaluate our method with experiments on data sets collected from Delicious.com comprising over 71,000 Web documents, 0.5 million users and 2 million shared bookmarks. We also show that the algorithm is more resistant to spammers than other methods such as the original HITS algorithm and simple statistical measures.

#index 1227655
#* Detecting spammers and content promoters in online video social networks
#@ Fabrício Benevenuto;Tiago Rodrigues;Virgílio Almeida;Jussara Almeida;Marcos Gonçalves
#t 2009
#c 13
#% 268079
#% 296738
#% 318412
#% 458379
#% 465754
#% 466229
#% 577367
#% 772018
#% 878224
#% 916790
#% 954660
#% 956578
#% 958000
#% 974033
#% 987245
#% 1002005
#% 1002006
#% 1002007
#% 1006352
#% 1016177
#% 1022744
#% 1034559
#% 1084472
#% 1125907
#% 1131929
#% 1860941
#! A number of online video social networks, out of which YouTube is the most popular, provides features that allow users to post a video as a response to a discussion topic. These features open opportunities for users to introduce polluted content, or simply pollution, into the system. For instance, spammers may post an unrelated video as response to a popular one aiming at increasing the likelihood of the response being viewed by a larger number of users. Moreover, opportunistic users--promoters--may try to gain visibility to a specific video by posting a large number of (potentially unrelated) responses to boost the rank of the responded video, making it appear in the top lists maintained by the system. Content pollution may jeopardize the trust of users on the system, thus compromising its success in promoting social interactions. In spite of that, the available literature is very limited in providing a deep understanding of this problem. In this paper, we go a step further by addressing the issue of detecting video spammers and promoters. Towards that end, we manually build a test collection of real YouTube users, classifying them as spammers, promoters, and legitimates. Using our test collection, we provide a characterization of social and content attributes that may help distinguish each user class. We also investigate the feasibility of using a state-of-the-art supervised classification algorithm to detect spammers and promoters, and assess its effectiveness in our test collection. We found that our approach is able to correctly identify the majority of the promoters, misclassifying only a small percentage of legitimate users. In contrast, although we are able to detect a significant fraction of spammers, they showed to be much harder to distinguish from legitimate users.

#index 1227656
#* AdOn: an intelligent overlay video advertising system
#@ Jinlian Guo;Tao Mei;Falin Liu;Xian-Sheng Hua
#t 2009
#c 13
#% 997256
#% 1074178
#% 1131870
#! This paper presents a new video advertising system, called AdOn, which supports intelligent overlay video ads. Unlike most current ad-networks such as Youtube that overlay the ads at fixed positions in the videos (e.g., on the bottom fifth of videos 15 seconds in), AdOn is able to automatically detect a set of spatio-temporal nonintrusive positions and associate the contextually relevant ads with these positions. The overlay positions are obtained on the basis of video structuring, face and text detection, as well as visual saliency analysis, so that the intrusiveness to the users can be minimized. The ads are selected according to content-based multimodal relevance so that advertising relevance can be maximized. AdOn represents one of the first attempts towards intelligent overlay video advertising by leveraging video content analysis techniques.

#index 1227657
#* Agreement among statistical significance tests for information retrieval evaluation at varying sample sizes
#@ Mark D. Smucker;James Allan;Ben Carterette
#t 2009
#c 13
#% 1019124
#% 1074132
#! Research has shown that little practical difference exists between the randomization, Student's paired t, and bootstrap tests of statistical significance for TREC ad-hoc retrieval experiments with 50 topics. We compared these three tests on runs with topic sizes down to 10 topics. We found that these tests show increasing disagreement as the number of topics decreases. At smaller numbers of topics, the randomization test tended to produce smaller p-values than the t-test for p-values less than 0.1. The bootstrap exhibited a systematic bias towards p-values strictly less than the t-test with this bias increasing as the number of topics decreased. We recommend the use of the randomization test although the t-test appears to be suitable even when the number of topics is small.

#index 1227658
#* Annotation of URLs: more than the sum of parts
#@ Max Hinne;Wessel Kraaij;Stephan Raaijmakers;Suzan Verberne;Theo van der Weide;Maarten van der Heijden
#t 2009
#c 13
#% 311034
#% 805200
#% 838469
#% 1055675
#% 1166510
#% 1173691
#% 1173700
#! Recently a number of studies have demonstrated that search engine logfiles are an important resource to determine the relevance relation between URLs and query terms. We hypothesized that the queries associated with a URL could also be presented as useful URL metadata in a search engine result list, e.g. for helping to determine the semantic category of a URL. We evaluated this hypothesis by a classification experiment based on the DMOZ dataset. Our method can also annotate URLs that have no associated queries.

#index 1227659
#* Automatic URL completion and prediction using fuzzy type-ahead search
#@ Jiannan Wang;Guoliang Li;Jianhua Feng;Chen Li
#t 2009
#c 13
#% 333854
#% 879610
#% 1190092
#% 1217199
#! Type-ahead search is a new information-access paradigm, in which systems can find answers to keyword queries "on-the-fly" as a user types in a query. It improves traditional autocomplete search by allowing query keywords to appear at different places in an answer. In this paper we study the problem of automatic URL completion and prediction using fuzzy type-ahead search. That is, we interactively find relevant URLs that contain words matching query keywords, even approximately, as the user types in a query. Supporting fuzzy search is very important when the user has limited knowledge about URLs. We describe the design and implementation of our method, and report the experimental results on Firefox.

#index 1227660
#* Beyond session segmentation: predicting changes in search intent with client-side user interactions
#@ Qi Guo;Eugene Agichtein
#t 2009
#c 13
#% 869483
#% 1130878
#! Effective search session segmentation "grouping queries according to common task or intent" can be useful for improving relevance, search evaluation, and query suggestion. Previous work has largely attempted to segment search sessions off-line, after the fact. In contrast, we present preliminary investigation of predicting, in real time, whether a user is about to switch interest - that is, whether the user is about to finish the current search and switch to another search task (or stop searching altogether). We explore an approach for this task using client-side user behavior such as clicks, scrolls, and mouse movements, contextualized by the content of the search result pages and previous searches. Our experiments over thousands of real searches show that we can identify context and user behavior patterns that indicate that a user is about to switch to a new search task. These preliminary results can be helpful for more effective query suggestion and personalization.

#index 1227661
#* Blog distillation using random walks
#@ Mostafa Keikha;Mark James Carman;Fabio Crestani
#t 2009
#c 13
#% 290830
#% 987222
#% 1074094
#% 1715628
#! This paper addresses the blog distillation problem. That is, given a user query find the blogs most related to the query topic. We model the blogosphere as a single graph that includes extra information besides the content of the posts. By performing a random walk on this graph we extract most relevant blogs for each query. Our experiments on the TREC'07 data set show 15% improvement in MAP and 8% improvement in Precision@10 over the Language Modeling baseline.

#index 1227662
#* A case for improved evaluation of query difficulty prediction
#@ Falk Scholer;Steven Garcia
#t 2009
#c 13
#% 397161
#% 804915
#% 944349
#% 1130990
#% 1192696
#% 1415713
#! Query difficulty prediction aims to identify, in advance, how well an information retrieval system will perform when faced with a particular search request. The current standard evaluation methodology involves calculating a correlation coefficient, to indicate how strongly the predicted query difficulty is related with an actual system performance measure, usually Average Precision. We run a series of experiments based on predictors that have been shown to perform well in the literature, comparing these across different TREC runs. Our results demonstrate that the current evaluation methodology is severely limited. Although it can be used to demonstrate the performance of a predictor for a single system, such performance is not consistent over a variety of retrieval systems. We conclude that published results in the query difficulty area are generally not comparable, and recommend that prediction be evaluated against a spectrum of underlying search systems.

#index 1227663
#* Characterizing the subjectivity of topics
#@ Marc-Allen Cartright;Elif Aktolga;Jeffrey Dalton
#t 2009
#c 13
#% 1250238
#! A document or web page in isolation may appear completely reasonable, but may represent a biased perspective on the topic being discussed. Given the topic of a document, we propose new metrics provocativeness and balance that suggest when the topic could be controversial. We explore the use of these metrics to characterize the subjectivity of the topics in the TREC Blog Track.

#index 1227664
#* Classifying library catalogue by author profiling
#@ Tadashi Nomoto
#t 2009
#c 13
#% 751593
#! This paper presents a novel approach to classifying library records by making use of what we call "author profile," a representation of an author's expertise along a library classification. Coupled with a string kernel classifier, the idea is shown to bring a significant improvement over a baseline.

#index 1227665
#* Cluster-based query expansion
#@ Inna Gelfer Kalmanovich;Oren Kurland
#t 2009
#c 13
#% 262084
#% 280864
#% 340901
#% 342707
#% 766430
#% 766431
#% 766525
#% 818204
#% 879587
#% 940042
#% 1074080
#! We demonstrate the merits of using document clusters that are created offline to improve the overall effectiveness and performance robustness of a state-of-the-art pseudo-feedback-based query expansion method -- the relevance model.

#index 1227666
#* Comparing both relevance and robustness in selection of web ranking functions
#@ Fan Li;Xin Li;Shihao Ji;Zhaohui Zheng
#t 2009
#c 13
#% 411762
#% 907544
#% 987228
#% 991164
#% 1035577
#! In commercial search engines, a ranking function is selected for deployment mainly by comparing the relevance measurements over candidates. In this paper we suggest to select Web ranking functions according to both their relevance and robustness to the changes that may lead to relevance degradation over time. We argue that the ranking robustness can be effectively measured by taking into account the ranking score distribution across Web pages. We then improve NDCG with two new metrics and show their superiority in terms of stability to ranking score turbulence and stability in function selection.

#index 1227667
#* A comparison of retrieval-based hierarchical clustering approaches to person name disambiguation
#@ Christof Monz;Wouter Weerkamp
#t 2009
#c 13
#% 1077150
#% 1271267
#% 1271279
#% 1271313
#! This paper describes a simple clustering approach to person name disambiguation of retrieved documents. The methods are based on standard IR concepts and do not require any task-specific features. We compare different term-weighting and indexing methods and evaluate their performance against the Web People Search task (WePS). Despite their simplicity these approaches achieve very competitive performance.

#index 1227668
#* Compression-based document length prior for language models
#@ Javier Parapar;David E. Losada;Álvaro Barreiro
#t 2009
#c 13
#% 218982
#% 750863
#% 1041733
#% 1715613
#% 1815525
#! The inclusion of document length factors has been a major topic in the development of retrieval models. We believe that current models can be further improved by more refined estimations of the document's scope. In this poster we present a new document length prior that uses the size of the compressed document. This new prior is introduced in the context of Language Modeling with Dirichlet smoothing. The evaluation performed on several collections shows significant improvements in effectiveness.

#index 1227669
#* Concept representation based video indexing
#@ Meng Wang;Yan Song;Xian-Sheng Hua
#t 2009
#c 13
#% 884801
#% 997067
#! This poster introduces a novel concept-based video indexing approach. It is developed based on a rich set of base concepts, of which the models are available. Then, for a given concept with several labeled samples, we combine the base concepts to fit it and its model can thus be obtained accordingly. Empirical results demonstrate that this method can achieve great performance even with very limited labeled data. We have compared different representation approaches including both sparse and non-sparse methods. Our conclusion is that the sparse method will lead to much better performance.

#index 1227670
#* Context transfer in search advertising
#@ Hila Becker;Andrei Broder;Evgeniy Gabrilovich;Vanja Josifovski;Bo Pang
#t 2009
#c 13
#% 269218
#% 590523
#% 754059
#% 805878
#% 818265
#% 853542
#% 1074101
#% 1130852
#% 1130910
#! We define and study the process of context transfer in search advertising, which is the transition of a user from the context of Web search to the context of the landing page that follows an ad-click. We conclude that in the vast majority of cases, the user is shown one of three types of pages, which can be accurately distinguished using automatic text classification.

#index 1227671
#* Counting ancestors to estimate authority
#@ Jian Wang;Brian D. Davison
#t 2009
#c 13
#% 2833
#% 268079
#% 290830
#% 411762
#% 577219
#% 766462
#% 1022742
#! The AncestorRank algorithm calculates an authority score by using just one characteristic of the web graph-the number of ancestors per node. For scalability, we estimate the number of ancestors by using a probabilistic counting algorithm. We also consider the case in which ancestors which are closer to the node have more influence than those farther from the node. Thus we further apply a decay factor delta on the contributions from successively earlier ancestors. The resulting authority score is used in combination with a content-based ranking algorithm. Our experiments show that as long as delta is in the range of [0.1, 0.9], AncestorRank can greatly improve BM25 performance, and in our experiments is often better than PageRank.

#index 1227672
#* Cross language name matching
#@ J. Scott McCarley
#t 2009
#c 13
#% 81669
#% 340897
#% 757830
#% 939865
#! Cross language information retrieval methods are used to determine which segments of Arabic language documents match name-based English queries. We investigate and contrast a word-based translation model with a character-based transliteration model in order to handle spelling variation and previously unseen names. We measure performance by making a novel use of the training data from the 2007 ACE Entity Translation

#index 1227673
#* Deep versus shallow judgments in learning to rank
#@ Emine Yilmaz;Stephen Robertson
#t 2009
#c 13
#% 309095
#% 840846
#% 1074132
#! Much research in learning to rank has been placed on developing sophisticated learning methods, treating the training set as a given. However, the number of judgments in the training set directly aff ects the quality of the learned system. Given the expense of obtaining relevance judgments for constructing training data, one often has a limited budget in terms of how many judgments he can get. The major problem then is how to distribute this judgment e ffort across diff erent queries. In this paper, we investigate the tradeo ff between the number of queries and the number of judgments per query when training sets are constructed. In particular, we show that up to a limit, training sets with more queries but shallow (less) judgments per query are more cost effective than training sets with less queries but deep (more) judgments per query.

#index 1227674
#* Developing energy efficient filtering systems
#@ Leif Azzopardi;Wim Vanderbauwhede;Mahmoud Moadeli
#t 2009
#c 13
#% 124004
#% 340901
#% 813070
#! Processing large volumes of information generally requires massive amounts of computational power, which consumes a significant amount of energy. An emerging challenge is the development of ``environmentally friendly'' systems that are not only efficient in terms of time, but also energy efficient. In this poster, we outline our initial efforts at developing greener filtering systems by employing Field Programmable Gate Arrays (FPGA) to perform the core information processing task. FPGAs enable code to be executed in parallel at a chip level, while consuming only a fraction of the power of a standard (von Neuman style) processor. On a number of test collections, we demonstrate that the FPGA filtering system performs 10-20 times faster than the Itanium based implementation, resulting in considerable energy savings.

#index 1227675
#* Enhancing topical ranking with preferences from click-through data
#@ Yi Chang;Anlei Dong;Ciya Liao;Zhaohui Zheng
#t 2009
#c 13
#% 818221
#% 987228
#! To overcome the training data insufficiency problem for dedicated model in topical ranking, this paper proposes to utilize click-through data to improve learning. The efficacy of click-through data is explored under the framework of preference learning. The empirical experiment on a commercial search engine shows that, the model trained with the dedicated labeled data combined with skip-next preferences could beat the baseline model and the generic model in NDCG5 for 4.9% and 2.4% respectively.

#index 1227676
#* Equivalence between nonnegative tensor factorization and tensorial probabilistic latent semantic analysis
#@ Wei Peng
#t 2009
#c 13
#% 329569
#% 729437
#% 1050550
#% 1074204
#% 1130902
#! This paper establishes a connection between NMF and PLSA on multi-way data, called NTF and T-PLSA respectively. Two types of T-PLSA models are proven to be equivalent to non-negative PARAFAC and non-negative Tucker3. This paper also shows that by running NTF and T-PLSA alternatively, they can jump out of each other's local minima and achieve a better clustering solution.

#index 1227677
#* The ESA retrieval model revisited
#@ Maik Anderka;Benno Stein
#t 2009
#c 13
#% 228088
#% 262870
#% 1270220
#% 1270222
#% 1270267
#% 1275012
#% 1415756
#! Among the retrieval models that have been proposed in the last years, the ESA model of Gabrilovich and Markovitch received much attention. The authors report on a significant improvement in the retrieval performance, which is explained with the semantic concepts introduced by the document collection underlying ESA. Their explanation appears plausible but our analysis shows that the connections are more involved and that the "concept hypothesis" does not hold. In our contribution we analyze several properties that in fact affect the retrieval performance. Moreover, we introduce a formalization of ESA, which reveals its close connection to existing retrieval models.

#index 1227678
#* Estimating query performance using class predictions
#@ Kevyn Collins-Thompson;Paul N. Bennett
#t 2009
#c 13
#% 907544
#% 944349
#% 956632
#% 987283
#% 987299
#% 995516
#% 1130851
#% 1130909
#! We investigate using topic prediction data, as a summary of document content, to compute measures of search result quality. Unlike existing quality measures such as query clarity that require the entire content of the top-ranked results, class-based statistics can be computed efficiently online, because class information is compact enough to precompute and store in the index. In an empirical study we compare the performance of class-based statistics to their language-model counterparts for predicting two measures: query difficulty and expansion risk. Our findings suggest that using class predictions can offer comparable performance to full language models while reducing computation overhead.

#index 1227679
#* Evaluating effects of machine translation accuracy on cross-lingual patent retrieval
#@ Atsushi Fujii;Masao Utiyama;Mikio Yamamoto;Takehito Utsuro
#t 2009
#c 13
#% 815902
#! We organized a machine translation (MT) task at the Seventh NTCIR Workshop. Participating groups were requested to machine translate sentences in patent documents and also search topics for retrieving patent documents across languages. We analyzed the relationship between the accuracy of MT and its effects on the retrieval accuracy.

#index 1227680
#* Evaluating web search using task completion time
#@ Ya Xu;David Mease
#t 2009
#c 13
#% 818257
#% 879566
#! We consider experiments to measure the quality of a web search algorithm based on how much total time users take to complete assigned search tasks using that algorithm. We first analyze our data to verify that there is in fact a negative relationship between a user's total search time and a user's satisfaction for the types of tasks under consideration. Secondly, we fit a model with the user's total search time as the response to compare two different search algorithms. Finally, we propose an alternative experimental design which we demonstrate to be a substantial improvement over our current design in terms of variance reduction and efficiency.

#index 1227681
#* An evaluation of entity and frequency based query completion methods
#@ Edgar Meij;Peter Mika;Hugo Zaragoza
#t 2009
#c 13
#% 577224
#% 642985
#% 869501
#% 1126949
#% 1130855
#! We present a semantic approach to suggesting query completions which leverages entity and type information. When compared to a frequency-based approach, we show that such information mostly helps rare queries.

#index 1227682
#* Evolutionary document summarization for disaster management
#@ Dingding Wang;Li Zheng;Tao Li;Yi Deng
#t 2009
#c 13
#% 340883
#% 787502
#% 814023
#! In this poster, we develop an evolutionary document summarization system for discovering the changes and differences in each phase of a disaster evolution. Given a collection of document streams describing an event, our system generates a short summary delivering the main development theme of the event by extracting the most representative and discriminative sentences at each phase. Experimental results on the collection of press releases for Hurricane Wilma in 2005 demonstrate the efficacy of our proposal.

#index 1227683
#* Experiments in CLIR using fuzzy string search based on surface similarity
#@ Sethuramalingam Subramaniam;Anil Kumar Singh;Pradeep Dasigi;Vasudeva Varma
#t 2009
#c 13
#% 818222
#% 854888
#% 939535
#% 939824
#! Cross Language Information Retrieval (CLIR) between languages of the same origin is an interesting topic of research. The similarity of the writing systems used for these languages can be used effectively to not only improve CLIR, but to overcome the problems of textual variations, textual errors, and even the lack of linguistic resources like stemmers to an extent. We have conducted CLIR experiments between three languages which use writing systems (scripts) of Brahmi-origin, namely Hindi, Bengali and Marathi. We found significant improvements for all the six language pairs using a method for fuzzy text search based on Surface Similarity. In this paper we report these results and compare them with a baseline CLIR system and a CLIR system that uses Scaled Edit Distance (SED) for fuzzy string matching.

#index 1227684
#* Feature selection for automatic taxonomy induction
#@ Hui Yang;Jamie Callan
#t 2009
#c 13
#% 577285
#% 756964
#% 816164
#% 1328333
#! Most existing automatic taxonomy induction systems exploit one or more features to induce a taxonomy; nevertheless there is no systematic study examining which are the best features for the task under various conditions. This paper studies the impact of using different features on taxonomy induction for different types of relations and for terms at different abstraction levels. The evaluation shows that different conditions need different technologies or different combination of the technologies. In particular, co-occurrence and lexico-syntactic patterns are good features for is-a, sibling and part-of relations; contextual, co-occurrence, patterns, and syntactic features work well for concrete terms; co-occurrence works well for abstract terms.

#index 1227685
#* Finding advertising keywords on video scripts
#@ Jung-Tae Lee;Hyungdong Lee;Hee-Seon Park;Young-In Song;Hae-Chang Rim
#t 2009
#c 13
#% 854813
#% 869484
#% 1040862
#! A key to success to contextual in-video advertising is finding advertising keywords on video contents effectively, but there has been little literature in the area so far. This paper presents some preliminary results of our learning-based system that finds relevant advertising keywords on particular scene of video contents using their scripts. The system is trained with not only features proven useful in earlier studies but novel features that reflect the situation of a targeted scene. Experimental results show that the new features are potentially helpful for enhancing the accuracy of keyword extraction for contextual in-video advertising.

#index 1227686
#* Fitting score distribution for blog opinion retrieval
#@ Ben He;Jie Peng;Iadh Ounis
#t 2009
#c 13
#% 1130915
#! Current blog opinion retrieval approaches cannot be applied if the topic relevance and opinion score distributions by rank are dissimilar. This problem severely limits the feasibility of these approaches. We propose to tackle this problem by fitting the distribution of opinion scores, which replaces the original topic relevance score distribution with the simulated one. Our proposed score distribution fitting method markedly enhances the feasibility of a state-of-the-art dictionary-based opinion retrieval approach. Evaluation on a standard TREC blog test collection shows significant improvements over high quality topic relevance baselines.

#index 1227687
#* A graph-based approach to mining multilingual word associations from wikipedia
#@ Zheng Ye;Xiangji Huang;Hongfei Lin
#t 2009
#c 13
#% 262046
#% 816207
#% 961685
#% 1130890
#% 1275012
#! In this paper, we propose a graph-based approach to constructing a multilingual association dictionary from Wikipedia, in which we exploit two kinds of links in Wikipedia articles to associate multilingual words and concepts together in a graph. The mined association dictionary is applied in cross language information retrieval (CLIR) to verify its quality. We evaluate our approach on four CLIR data sets and the experimental results show that it is possible to mine a good multilingual association dictionary from Wikipedia articles.

#index 1227688
#* Has adhoc retrieval improved since 1994?
#@ Timothy G. Armstrong;Alistair Moffat;William Webber;Justin Zobel
#t 2009
#c 13
#% 857180
#% 1074057
#! Evaluation forums such as TREC allow systematic measurement and comparison of information retrieval techniques. The goal is consistent improvement, based on reliable comparison of the effectiveness of different approaches and systems. In this paper we report experiments to determine whether this goal has been achieved. We ran five publicly available search systems, in a total of seventeen different configurations, against nine TREC adhoc-style collections, spanning 1994 to 2005. These runsets were then used as a benchmark for reassessing the relative effectiveness of the original TREC runs for those collections. Surprisingly, there appears to have been no overall improvement in effectiveness for either median or top-end TREC submissions, even after allowing for several possible confounds. We therefore question whether the effectiveness of adhoc information retrieval has improved over the past decade and a half.

#index 1227689
#* High precision retrieval using relevance-flow graph
#@ Jangwon Seo;Jiwoon Jeon
#t 2009
#c 13
#% 719598
#% 742399
#% 987229
#% 987267
#% 1665166

#index 1227690
#* Identifying the original contribution of a document via language modeling
#@ Benyah Shaparenko;Thorsten Joachims
#t 2009
#c 13
#% 46803
#% 722904
#% 989633
#! One goal of text mining is to provide readers with automatic methods for quickly finding the key ideas in individual documents and whole corpora. To this effect, we propose a statistically well-founded method for identifying the original ideas that a document contributes to a corpus, focusing on self-referential diachronic corpora such as research publications, blogs, email, and news articles. Our statistical model of passage impact defines (interesting) original content through a combination of impact and novelty, and it can be used to identify the most original passages in a document. Unlike heuristic approaches, this statistical model is extensible and open to analysis. We evaluate the approach on both synthetic and real data, showing that the passage impact model outperforms a heuristic baseline method.

#index 1227691
#* The importance of manual assessment in link discovery
#@ Wei Che Huang;Andrew Trotman;Shlomo Geva
#t 2009
#c 13
#% 1019082
#% 1130858
#! Using a ground truth extracted from the Wikipedia, and a ground truth created through manual assessment, we show that the apparent performance advantage seen in machine learning approaches to link discovery are an artifact of trivial links that are actively rejected by manual assessors.

#index 1227692
#* Improving search relevance for implicitly temporal queries
#@ Donald Metzler;Rosie Jones;Fuchun Peng;Ruiqiang Zhang
#t 2009
#c 13
#% 730070
#% 766408
#% 805839
#% 1130999

#index 1227693
#* Improving user confidence in cultural heritage aggregated results
#@ Junte Zhang;Alia Amin;Henriette S. M. Cramer;Vanessa Evers;Lynda Hardman
#t 2009
#c 13
#% 271272
#% 1065246
#% 1128905
#% 1150174
#% 1190282
#! State of the art web search systems enable aggregation of information from many sources. Users are challenged to assess the reliability of information from different sources. We report on an empirical user study on the effect of displaying credibility ratings of multiple cultural heritage sources (e.g. museum websites, art blogs) on users' search performance and selection. The results of our online interactive study (N=122) show that when explicitly presenting these ratings, people become significantly more confident in their selection of information from aggregated results.

#index 1227694
#* Incorporating prior knowledge into a transductive ranking algorithm for multi-document summarization
#@ Massih R. Amini;Nicolas Usunier
#t 2009
#c 13
#% 318409
#% 464465
#! This paper presents a transductive approach to learn ranking functions for extractive multi-document summarization. At the first stage, the proposed approach identifies topic themes within a document collection, which help to identify two sets of relevant and irrelevant sentences to a question. It then iteratively trains a ranking function over these two sets of sentences by optimizing a ranking loss and fitting a prior model built on keywords. The output of the function is used to find further relevant and irrelevant sentences. This process is repeated until a desired stopping criterion is met.

#index 1227695
#* Integrating clusters created offline with query-specific clusters for document retrieval
#@ Lior Meister;Oren Kurland;Inna Gelfer Kalmanovich
#t 2009
#c 13
#% 262084
#% 766430
#% 766431
#% 952491
#! Previous work on cluster-based document retrieval has used either static document clusters that are created offline, or query-specific (dynamic) document clusters that are created from top-retrieved documents. We present the potential merit of integrating these two types of clusters.

#index 1227696
#* Integrating phrase inseparability in phrase-based model
#@ Lixin Shi;Jian-Yun Nie
#t 2009
#c 13
#% 35937
#% 109190
#% 232647
#% 748700
#% 766428
#% 818262
#% 1130850
#! In this paper, we propose a new phrase-based IR model, which integrates a measure of "inseparability" of phrases. Our experiments show its high potential to produce large improvements in retrieval effectiveness.

#index 1227697
#* Is spam an issue for opinionated blog post search?
#@ Craig Macdonald;Iadh Ounis;Ian Soboroff
#t 2009
#c 13
#! In opinion-finding, the retrieval system is tasked with retrieving not just relevant documents, but those that also express an opinion towards the query target entity. This task has been studied in the context of the blogosphere by groups participating in the 2006-2008 TREC Blog tracks. Spam blogs (splogs) are thought to be a problem on the blogosphere. In this paper, we investigate the extent to which spam has affected the participating groups' retrieval systems over the three years of the TREC Blog track opinion-finding task. Our results show that spam can be an issue, with most systems retrieving some spam for every topic. However, removing spam from the rankings does not markedly change the relative performance of opinion-finding approaches.

#index 1227698
#* Is this urgent?: exploring time-sensitive information needs in collaborative question answering
#@ Yandong Liu;Nitya Narasimhan;Venu Vasudevan;Eugene Agichtein
#t 2009
#c 13
#% 838398
#% 1179994
#% 1264815
#! As online Collaborative Question Answering (CQA) servicessuch as Yahoo! Answers and Baidu Knows are attracting users, questions, and answers at an explosive rate, the truly urgent and important questions are increasingly getting lost in the crowd. That is, questions that require immediate responses are pushed out of the way by the trivial but more recently arriving questions. Unlike other questions in collaborative question answering (CQA) for which users might be willing to wait until good answers appear, urgent questions are likely to be of interest to the asker only if answered in the next few minutes or hours. For such questions, late responses are either not useful or are simply not applicable. Unfortunately, current collaborative question-answering systems do not distinguish urgent questions from the rest, and could thus be ineffective for urgent information needs. We explore text- and data- mining methods for automatically identifying urgent questions in the CQA setting. Our results indicate that modeling the question context (i.e., the particular forum/category where the question was posted) can increase classification accuracy compared to the text of the question alone.

#index 1227699
#* It pays to be picky: an evaluation of thread retrieval in online forums
#@ Jonathan L. Elsas;Jaime G. Carbonell
#t 2009
#c 13
#% 750863
#% 848650
#% 1074094
#% 1074109
#% 1130914
#! Online forums host a rich information exchange, often with contributions from many subject matter experts. In this work we evaluate algorithms for thread retrieval in a large and active online forum community. We compare methods that utilize thread structure to a naïve method that treats a thread as a single document. We find that thread structure helps, and additionally selective methods of thread scoring, which only use evidence from a small number of messages in the thread, significantly and consistently outperform inclusive methods which use all the messages in the thread.

#index 1227700
#* Knowledge transformation for cross-domain sentiment classification
#@ Tao Li;Vikas Sindhwani;Chris Ding;Yi Zhang
#t 2009
#c 13
#% 881468
#% 1127964
#! With the explosion of user-generated web2.0 content in the form of blogs, wikis and discussion forums, the Internet has rapidly become a massive dynamic repository of public opinion on an unbounded range of topics. A key enabler of opinion extraction and summarization is sentiment classification: the task of automatically identifying whether a given piece of text expresses positive or negative opinion towards a topic of interest. Building high-quality sentiment classifiers using standard text categorization methods is challenging due to the lack of labeled data in a target domain. In this paper, we consider the problem of cross-domain sentiment analysis: can one, for instance, download rated movie reviews from rottentomatoes.com or IMBD discussion forums, learn linguistic expressions and sentiment-laden terms that generally characterize opinionated reviews and then successfully transfer this knowledge to the target domain, thereby building high-quality sentiment models without manual effort? We outline a novel sentiment transfer mechanism based on constrained non-negative matrix tri-factorizations of term-document matrices in the source and target domains. We report some preliminary results with this approach.

#index 1227701
#* K-tree: large scale document clustering
#@ Christopher M. De Vries;Shlomo Geva
#t 2009
#c 13
#% 420057
#% 878916
#! We introduce K-tree in an information retrieval context. It is an efficient approximation of the k-means clustering algorithm. Unlike k-means it forms a hierarchy of clusters. It has been extended to address issues with sparse representations. We compare performance and quality to CLUTO using document collections. The K-tree has a low time complexity that is suitable for large document collections. This tree structure allows for efficient disk based implementations where space requirements exceed that of main memory.

#index 1227702
#* A latent topic model for linked documents
#@ Zhen Guo;Shenghuo Zhu;Yun Chi;Zhongfei Zhang;Yihong Gong
#t 2009
#c 13
#% 280819
#% 313959
#% 420495
#% 466574
#% 643008
#% 722904
#! Documents in many corpora, such as digital libraries and webpages, contain both content and link information. To explicitly consider the document relations represented by links, in this paper we propose a citation-topic (CT) model which assumes a probabilistic generative process for corpora. In the CT model a given document is modeled as a mixture of a set of topic distributions, each of which is borrowed (cited) from a document that is related to the given document. Moreover, the CT model contains a random process for selecting the related documents according to the structure of the generative model determined by links and therefore, the transitivity of the relations among documents is captured. We apply the CT model on the document clustering task and the experimental comparisons against several state-of-the-art approaches demonstrate very promising performances.

#index 1227703
#* Measuring constraint violations in information retrieval
#@ Ronan Cummins;Colm O'Riordan
#t 2009
#c 13
#% 766412
#% 818263
#% 1154026
#! Recently, an inductive approach to modelling term-weighting function correctness has provided a number of axioms (constraints), to which all good term-weighting functions should adhere. These constraints have been shown to be theoretically and empirically sound in a number of works. It has been shown that when a term-weighting function breaks one or more of the constraints, it typically indicates sub-optimality of that function. This elegant inductive approach may more accurately model the human process of determining the relevance a document. It is intuitive that a person's notion of relevance changes as terms that are either on or off-topic are encountered in a given document. Ultimately, it would be desirable to be able to mathematically determine the performance of term-weighting functions without the need for test collections. Many modern term-weighting functions do not satisfy the constraints in an unconditional manner. However, the degree to which these functions violate the constraints has not been investigated. A comparison between weighting functions from this perspective may shed light on the poor performance of certain functions in certain settings. Moreover, if a correlation exists between performance and the number of violations, measuring the degree of violation could help more accurately predict how a certain scheme will perform on a given collection.

#index 1227704
#* Measuring the descriptiveness of web comments
#@ Martin Potthast
#t 2009
#c 13
#% 860956
#% 881063
#% 907489
#% 1019161
#% 1127964
#% 1227677
#% 1275012
#% 1415758
#! This paper investigates whether Web comments are of descriptive nature, that is, whether the combined text of a set of comments is similar in topic to the commented object. If so, comments may be used in place of the respective object in all kinds of cross-media retrieval tasks. Our experiments reveal that comments on textual objects are indeed descriptive: 10 comments suffice to expect a high similarity between the comments and the commented text; 100-500 comments suffice to replace the commented text in a ranking task, and to measure the contribution of the commenters beyond the commented text.

#index 1227705
#* Mining product reviews based on shallow dependency parsing
#@ Qi Zhang;Yuanbin Wu;Tao Li;Mitsunori Ogihara;Joseph Johnson;Xuanjing Huang
#t 2009
#c 13
#% 769892
#% 854646
#% 938706
#% 939897
#% 1191715
#! This paper presents a novel method for mining product reviews, where it mines reviews by identifying product features, expressions of opinions and relations between them. By taking advantage of the fact that most of product features are phrases, a concept of shallow dependency parsing is introduced, which extends traditional dependency parsing to phrase level. This concept is then implemented for extracting relation between product features and expressions of opinions. Experimental evaluations show that the mining task can benefit from shallow dependency parsing.

#index 1227706
#* Modeling facial expressions and peripheral physiological signals to predict topical relevance
#@ Ioannis Arapakis;Ioannis Konstas;Joemon M. Jose;Ioannis Kompatsiaris
#t 2009
#c 13
#% 1031049
#% 1071150
#% 1074100
#! By analyzing explicit & implicit feedback information retrieval systems can determine topical relevance and tailor search criteria to the user's needs. In this paper we investigate whether it is possible to infer what is relevant by observing user affective behaviour. The sensory data employed range between facial expressions and peripheral physiological signals. We extract a set of features from the signals and analyze the data using classification methods, such as SVM and KNN. The results of our initial evaluation indicate that prediction of relevance is possible, to a certain extent, and implicit feedback models can benefit from taking into account user affective behavior.

#index 1227707
#* Modeling search response time
#@ Dan Zhang;Luo Si
#t 2009
#c 13
#% 230432
#% 302906
#% 443556
#% 754451
#% 818213
#! Modeling the response time of search engines is an important task for many applications such as resource selection in federated text search. Limited research has been conducted to address this task. Prior research calculated the search response time of all queries in the same way either with the average response time of several sample queries or with a single probability distribution, which is irrelevant to the characteristics of queries. However, the search response time may vary a lot for different types of queries. This paper proposes a novel query-specific and source-specific approach to model search response time. Some training data is acquired by measuring the search response time of some sample queries from a search engine. Then, a query-specific model is estimated with the training data and their corresponding response times by utilizing Ridge Regression. The obtained model can be used to predict search response times for new queries. A set of empirical studies are conducted to show the effectiveness of the proposed method.

#index 1227708
#* Multiclass VisualRank: image ranking method in clustered subsets based on visual features
#@ Mitsuru Ambai;Yuichi Yoshida
#t 2009
#c 13
#% 268079
#% 313959
#% 760805
#% 1119135
#! This paper proposes Multiclass VisualRank, a method that expands the idea of VisualRank into more than one category of images. Multiclass VisualRank divides images retrieved from search engines into several categories based on distinctive patterns of visual features, and gives ranking within the category. Experimental results show that our method can extract several different image categories relevant to given keyword and gives good ranking scores to retrieved images.

#index 1227709
#* Multiple approaches to analysing query diversity
#@ Paul Clough;Mark Sanderson;Murad Abouammoh;Sergio Navarro;Monica Paramita
#t 2009
#c 13
#% 879686
#% 1074113
#% 1074133
#% 1126949
#% 1166473
#% 1174573
#% 1206662
#! In this paper we examine user queries with respect to diversity: providing a mix of results across different interpretations. Using two query log analysis techniques (click entropy and reformulated queries), 14.9 million queries from the Microsoft Live Search log were analysed. We found that a broad range of query types may benefit from diversification. Additionally, although there is a correlation between word ambiguity and the need for diversity, the range of results users may wish to see for an ambiguous query stretches well beyond traditional notions of word sense.

#index 1227710
#* Multiview clustering: a late fusion approach using latent models
#@ Eric Bruno;Stephane Marchand-Maillet
#t 2009
#c 13
#% 329569
#% 785334
#% 818291
#% 840840
#% 983949
#! Multi-view clustering is an important problem in information retrieval due to the abundance of data offering many perspectives and generating multi-view representations. We investigate in this short note a late fusion approach for multi-view clustering based on the latent modeling of cluster-cluster relationships. We derive a probabilistic multi-view clustering model outperforming an early-fusion approach based on multi-view feature correlation analysis.

#index 1227711
#* On efficient posting list intersection with multicore processors
#@ Shirish Tatikonda;Flavio Junqueira;B. Barla Cambazoglu;Vassilis Plachouras
#t 2009
#c 13
#% 69316
#% 864446
#% 987214
#% 987215
#% 1055871
#% 1667821

#index 1227712
#* On perfect document rankings for expert search
#@ Craig Macdonald;Iadh Ounis
#t 2009
#c 13
#% 879570
#% 907525
#! Expert search systems often employ a document search component to identify on-topic documents, which are then used to identify people likely to have relevant expertise. This work investigates the impact of the retrieval effectiveness of the underlying document search component. It has been previously shown that applying techniques to the underlying document search component that normally improve the effectiveness of a document search engine also have a positive impact on the retrieval effectiveness of the expert search engine. In this work, we experiment with fictitious perfect document rankings, to attempt to identify an upper-bound in expert search system performance. Our surprising results infer that non-relevant documents can bring useful expertise evidence, and that removing these does not lead to an upper-bound in retrieval performance.

#index 1227713
#* On single-pass indexing with MapReduce
#@ Richard M. C. McCreadie;Craig Macdonald;Iadh Ounis
#t 2009
#c 13
#% 655485
#% 963669
#% 983467
#% 1136940
#! Indexing is an important Information Retrieval (IR) operation, which must be parallelised to support large-scale document corpora. We propose a novel adaptation of the state-of-the-art single-pass indexing algorithm in terms of the MapReduce programming model. We then experiment with this adaptation, in the context of the Hadoop MapReduce implementation. In particular, we explore the scale of improvements that can be achieved when using firstly more processing hardware and secondly larger corpora. Our results show that indexing speed increases in a close to linear fashion when scaling corpus size or number of processing machines. This suggests that the proposed indexing implementation is viable to support upcoming large-scale corpora.

#index 1227714
#* On the relative age of spam and ham training samples for email filtering
#@ Gordon V. Cormack;Jose-Marcio Martins da Cruz
#t 2009
#c 13
#! Email spam filters are commonly trained on a sample of spam and ham (non-spam) messages. We investigate the effect on filter performance of using samples of spam and ham messages sent months before those to be filtered. Our results show that filter performance deteriorates with the overall age of spam and ham samples, but at different rates. Spam and ham samples of different ages may be mixed to advantage, provided temporal cues are elided

#index 1227715
#* Page hunt: improving search engines using human computation games
#@ Hao Ma;Raman Chandrasekar;Chris Quirk;Abhishek Gupta
#t 2009
#c 13
#% 751818
#% 1065099
#! There has been a lot of work on evaluating and improving the relevance of web search engines. In this paper, we suggest using human computation games to elicit data from players that can be used to improve search. We describe Page Hunt, a single-player game. The data elicited using Page Hunt has several applications including providing metadata for pages, providing query alterations for use in query refinement, and identifying ranking issues. We describe an experiment with over 340 game players, and highlight some interesting aspects of the data obtained.

#index 1227716
#* Personalized music emotion recognition
#@ Yi-Hsuan Yang;Yu-Ching Lin;Homer Chen
#t 2009
#c 13
#% 435036
#% 1074062
#% 1767361
#! In recent years, there has been a dramatic proliferation of research on information retrieval based on highly subjective concepts such as emotion, preference and aesthetic. Such retrieval methods are fascinating but challenging since it is difficult to built a general retrieval model that performs equally well to everyone. In this paper, we propose two novel methods, bag-of-users model and residual modeling, to accommodate the individual differences for emotion-based music retrieval. The proposed methods are intuitive and generally applicable to other information retrieval tasks that involve subjective perception. Evaluation result shows the effectiveness of the proposed methods.

#index 1227717
#* Predicting stopping behaviour: a preliminary analysis
#@ Elaine G. Toms;Luanne Freund
#t 2009
#c 13
#% 448860
#% 804884
#% 1124986
#! The analysis of search transaction logs often characterizes a search session but rarely looks at the end point. When do users stop, and what cues are present suggesting that stopping is eminent? In this preliminary analysis of the logs of 288 search sessions conducted in a laboratory setting, we identified the activity performed by participants as well as search transitions that were invoked over the course of a search session. The 4331 search transitions (15 per task on average) contained a total of 9295 actions. We isolated the final transition in each search session for detailed analysis. As hypothesized some behaviours are predictable, and suggestive of stopping behavior, with the potential for modeling.

#index 1227718
#* Protein identification as an information retrieval problem
#@ Yiming Yang;Subramaniam Ganapathy;Abhay Harpale
#t 2009
#c 13
#% 340948
#% 832642
#% 1424275
#! We present the first interdisciplinary work on transforming a popular problem in proteomics, i.e. protein identification from tandem mass spectra, to an Information Retrieval (IR) problem. We present an empirical comparison of popular IR approaches, such as those available from Indri and Lemur toolkits on benchmark datasets, to representative popular baselines in the proteomics literature. Our experiments demonstrate statistically significant evidence that popular IR approaches outperform representative baseline approaches in proteomics.

#index 1227719
#* Query sampling for ranking learning in web search
#@ Linjun Yang;Li Wang;Bo Geng;Xian-Sheng Hua
#t 2009
#c 13
#% 236729
#% 309095
#% 397161
#% 577224
#% 722797
#% 983820
#! Learning to rank has become a popular approach to build a ranking model for Web search recently. Based on our observation, the constitution of the training set will greatly influence the performance of the learned ranking model. Meanwhile, the number of queries in Web search is nearly infinite and the human labeling cost is expensive, hence a subset of queries need to be carefully selected for training. In this paper, we develop a greedy algorithm to sample the queries, by simultaneously taking the query density, difficulty and diversity into consideration. The experimental results on a collected Web search dataset comprising 2024 queries show that the proposed method can lead to a more informative training set for building an effective model.

#index 1227720
#* A ranking approach to keyphrase extraction
#@ Xin Jiang;Yunhua Hu;Hang Li
#t 2009
#c 13
#% 495937
#% 817555
#! This paper addresses the issue of automatically extracting keyphrases from a document. Previously, this problem was formalized as classification and learning methods for classification were utilized. This paper points out that it is more essential to cast the problem as ranking and employ a learning to rank method to perform the task. Specifically, it employs Ranking SVM, a state-of-art method of learning to rank, in keyphrase extraction. Experimental results on three datasets show that Ranking SVM significantly outperforms the baseline methods of SVM and Naive Bayes, indicating that it is better to exploit learning to rank techniques in keyphrase extraction.

#index 1227721
#* Reciprocal rank fusion outperforms condorcet and individual rank learning methods
#@ Gordon V. Cormack;Charles L A Clarke;Stefan Buettcher
#t 2009
#c 13
#% 413613
#% 577224
#% 734915
#% 857180
#% 983820
#% 987241
#! Reciprocal Rank Fusion (RRF), a simple method for combining the document rankings from multiple IR systems, consistently yields better results than any individual system, and better results than the standard method Condorcet Fuse. This result is demonstrated by using RRF to combine the results of several TREC experiments, and to build a meta-learner that ranks the LETOR 3 dataset better than any previously reported method

#index 1227722
#* Relevance criteria for e-commerce: a crowdsourcing-based experimental analysis
#@ Omar Alonso;Stefano Mizzaro
#t 2009
#c 13
#% 235918
#% 260244
#% 1015625
#% 1015626
#% 1047347
#% 1150163
#! We discuss the concept of relevance criteria in the context of e-Commerce search. A vast body of research literature describes the beyond-topical criteria used to determine the relevance of the document to the need. We argue that in an e-Commerce scenario there are some differences, and novel and different criteria can be used to determine relevance. We experimentally validate this hypothesis by means of Amazon Mechanical Turk using a crowdsourcing approach.

#index 1227723
#* A relevance model based filter for improving ad quality
#@ Hema Raghavan;Dustin Hillard
#t 2009
#c 13
#% 956546
#% 987222
#% 1650569
#! Recently there has been a surge in research that predicts retrieval relevance using historical click-through data. While a larger number of clicks between a query and a document provides a stronger ``confidence" of relevance, most models in the literature that learn from clicks are error-prone as they do not take into account any confidence estimates. Sponsored Search models are especially prone to this error as they are typically trained on search engine logs in order to predict click-through-rate (CTR). The estimated CTR ultimately determines the rank at which an ad is shown and also impacts the price (cost-per-click) for the advertiser. In this paper, we improve a model that applies collaborative filtering on click data by training a filter that has been trained to predict pure relevance. Applying the filter to ads that have seen few clicks on live traffic results in improved CTR and click-yield (CY). Additionally, in offline experiments we find that using features based on the \emph{organic} results improves the relevance based filter's performance.

#index 1227724
#* A relevance-based topic model for news event tracking
#@ Viet Ha-Thuc;Yelena Mejova;Christopher Harris;Padmini Srinivasan
#t 2009
#c 13
#% 722904
#% 823344
#% 1131187
#! Event tracking is the task of discovering temporal patterns of popular events from text streams. Existing approaches for event tracking have two limitations: scalability and inability to rule out non-relevant portions in text streams. In this study, we propose a novel approach to tackle these limitations. To demonstrate the approach, we track news events across a collection of weblogs spanning a two-month time period.

#index 1227725
#* Revisiting logical imaging for information retrieval
#@ Guido Zuccon;Leif Azzopardi;Cornelis J. van Rijsbergen
#t 2009
#c 13
#% 194291
#% 227802
#! Retrieval with Logical Imaging is derived from belief revision and provides a novel mechanism for estimating the relevance of a document through logical implication (i.e. P(q-d). In this poster, we perform the first comprehensive evaluation of Logical Imaging (LI) in Information Retrieval (IR) across several TREC test Collections. When compared against standard baseline models, we show that LI fails to improve performance. This failure can be attributed to a nuance within the model that means non-relevant documents are promoted in the ranking, while relevant documents are demoted. This is an important contribution because it not only contextualizes the effectiveness of LI, but crucially explains why it fails. By addressing this nuance, future LI models could be significantly improved.

#index 1227726
#* A robust retrieval system of polyphonic music based on chord progression similarity
#@ Pierre Hanna;Thomas Rocher;Matthias Robine
#t 2009
#c 13
#% 261882
#% 574397
#% 937890
#! Retrieval systems for polyphonic music rely on the automatic estimation of similarity between two musical pieces. In the case of symbolic music, existing systems either consider a monophonic reduction based on melody or propose algorithms with high complexity. In this paper, we propose a new approach. Musical pieces are represented as a sequence of chords which are estimated from groups of notes sounding at the same time. A root and a mode are associated to each chord. Local alignment is then applied for estimating a similarity score between these sequences. Experiments performed on MIDI files collected on the Internet show that the system proposed allows the retrieval of different versions of the same song.

#index 1227727
#* Score and rank convergence of HITS
#@ Enoch Peserico;Luca Pretto
#t 2009
#c 13
#% 224113
#% 252608
#% 290830
#% 879575
#% 987251
#% 987252
#% 987357
#% 1019074
#% 1041782
#! How many iterations does the (ever more) popular HITS algorithm require to converge in score and, perhaps more importantly, in rank (i.e. to get the nodes of a graph "in the right order")? After pinning down the elusive notion of convergence in rank we provide the first non-trivial bounds on the convergence of HITS. A "worst case" example, requiring a number of iterations superexponential in the size of the target graph to achieve even "mild" convergence, suggests the need for greater caution in the experimental evaluation of the algorithm - as recent results of poor performance (e.g. vs. SALSA) might be due to insufficient iterations, rather than to an intrinsic deficiency of HITS. An almost matching upper bound shows that, as long as one employs exponential acceleration e.g. through a "squaring trick", a polynomial running time (practical in many application domains) always provides strong convergence guarantees.

#index 1227728
#* A search engine in a few lines.: yes, we can!
#@ Stefan Savev
#t 2009
#c 13
#! Many research implementations of search engines are written in C, C++, or Java. They are difficult to understand and modify because they are at least a few thousand lines of code and contain many low-level details. In this paper, we show how to achieve a much shorter and higher level implementation: one in about a few hundred lines. We accomplish this result through the use of a high-level functional programming language, F#, and some of its features such as sequences, pipes and structured input and output. By using a search engine implementation as a case study, we argue that functional programming fits the domain of Information Retrieval problems much better than imperative/OO languages like C++ and Java. Functional programming languages are ideal for rapid algorithm prototyping and data exploration in the field of Information Retrieval (IR). Additionally, our implementation can be used as case study in an IR course since it is a very high level, but nevertheless executable specification of a search engine.

#index 1227729
#* Search engine predilection towards news media providers
#@ Leif Azzopardi;Ciaran Owens
#t 2009
#c 13
#% 298221
#% 398025
#% 754060
#% 773040
#% 881525
#% 1415751
#! In this poster paper, we present a preliminary study on the predilection of web search engines towards various online news media provider sites using an access based measure.

#index 1227730
#* Searching documentation using text, OCR, and image
#@ Tom Yeh;Boris Katz
#t 2009
#c 13
#% 724320
#% 766409
#! We describe a mixed-modality method to index and search software documentation in three ways: plain text, OCR text of embedded figures, and visual features of these figures. Using a corpus of 102 computer books with a total of 62,943 pages and 75,800 figures, we empirically demonstrate that our method achieves better precision/recall than do alternatives based on single modalities.

#index 1227731
#* Selecting hierarchical clustering cut points for web person-name disambiguation
#@ Jun Gong;Douglas W. Oard
#t 2009
#c 13
#% 279755
#% 1271267
#! Hierarchical clustering is often used to cluster person-names referring to the same entities. Since the correct number of clusters for a given person-name is not known a priori, some way of deciding where to cut the resulting dendrogram to balance risks of over- or under-clustering is needed. This paper reports on experiments in which outcome-specific and result-set measures are used to learn a global similarity threshold. Results on the Web People Search (WePS)-2 task indicate that approximately 85% of the optimal F1 measure can be achieved on held-out data.

#index 1227732
#* Serendipitous search via wikipedia: a query log analysis
#@ Tetsuya Sakai;Kenichi Nogami
#t 2009
#c 13
#! We analyse the query log of a click-oriented Japanese search engine that utilises the link structures of Wikipedia for encouraging the user to change his information need and to perform repeated, serendipitous, exploratory search. Our results show that users tend to make transitions within the same query type: from person names to person names, from place names to place names, and so on.

#index 1227733
#* Spoken information retrieval for turkish broadcast news
#@ Siddika Parlak;Murat Saraclar
#t 2009
#c 13
#% 987271
#% 1036762
#% 1767103
#! Speech Retrieval systems utilize automatic speech recognition (ASR) to generate textual data for indexing. However, automatic transcriptions include errors, either because of out-of-vocabulary (OOV) words or due to ASR inaccuracy. In this work, we address spoken information retrieval in Turkish, a morphologically rich language where OOV rates are high. We apply several techniques, such as using subword units and indexing alternative hypotheses, to cope with the OOV problem and ASR inaccuracy. Experiments are performed on our Turkish Broadcast News (BN) Corpus which also incorporates a spoken IR collection. Results indicate that word segmentation is quite useful but the efficiency of indexing alternative hypotheses depends on retrieval type.

#index 1227734
#* A study of inter-annotator agreement for opinion retrieval
#@ Adam Bermingham;Alan F. Smeaton
#t 2009
#c 13
#! Evaluation of sentiment analysis, like large-scale IR evaluation, relies on the accuracy of human assessors to create judgments. Subjectivity in judgments is a problem for relevance assessment and even more so in the case of sentiment annotations. In this study we examine the degree to which assessors agree upon sentence-level sentiment annotation. We show that inter-assessor agreement is not contingent on document length or frequency of sentiment but correlates positively with automated opinion retrieval performance. We also examine the individual annotation categories to determine which categories pose most difficulty for annotators.

#index 1227735
#* SugarCube: quantification of topic propagation in the blogosphere using percolation theory
#@ Ali Azimi Bolourian;Yashar Moshfeghi;C. J. van Rijsbergen
#t 2009
#c 13
#! Blogs facilitate online debates and discussions for millions of people around the world. Identifying the most popular and prevailing topics discussed in the Blogosphere is a crucial task. This poster describes our novel approach to the quantification of the level of topic propagation in the Blogosphere. Our model uses graph-theoretic representations of the Blogosphere's link structures that allows it to deduce the `Percolation Threshold', which is then used in the quantification and definition of a global topic. The result of our experiments on a blog collection shows that our model is able to quantify the propagation of topics. Moreover, our model is successful in identifying specific topics that propagate throughout the Blogosphere and classifies them as `Global'.

#index 1227736
#* System scoring using partial prior information
#@ Sri Devi Ravana;Laurence A. Park;Alistair Moffat
#t 2009
#c 13
#% 748738
#% 810906
#% 1074057
#! We introduce smoothing of retrieval effectiveness scores, which balances results from prior incomplete query sets against limited additional complete information, in order to obtain more refined system orderings than would be possible on the new queries alone.

#index 1227737
#* Tag-based object similarity computation using term space dimension reduction
#@ Yong Ki Lee;Sung Jun Lee;Jonghun Park
#t 2009
#c 13
#% 348165
#% 1077150
#! In this paper, we propose a novel approach for measuring similarity between web objects. Our similarity measure is defined based on the representation of a web object as a collection of tags. Precisely, we first construct a vector space in which multiple terms are mapped into a single dimension by using information available from Open Directory Project and Delicious.com. Then we position web objects in the vector space and apply the traditional cosine measure for similarity computation. We demonstrate that the proposed similarity computation method is able to overcome the limitation of traditional vector space approach while at the same time require less computational cost compares to LSI (Latent Semantic Indexing).

#index 1227738
#* Tagging products using image classification
#@ Brian Tomasik;Phyo Thiha;Douglas Turnbull
#t 2009
#c 13
#% 724320
#% 760805
#% 883971
#% 883972
#! Associating labels with online products can be a labor-intensive task. We study the extent to which a standard "bag of visual words" image classifier can be used to tag products with useful information, such as whether a sneaker has laces or velcro straps. Using Scale Invariant Feature Transform (SIFT) image descriptors at random keypoints, a hierarchical visual vocabulary, and a variant of nearest-neighbor classification, we achieve accuracies between 66% and 98% on 2- and 3-class classification tasks using several dozen training examples. We also increase accuracy by combining information from multiple views of the same product.

#index 1227739
#* Template-independent wrapper for web forums
#@ Qi Zhang;Yang Shi;Xuanjing Huang;Lide Wu
#t 2009
#c 13
#% 397605
#% 464434
#% 1065289
#% 1269910
#! This paper presents a novel work on the task of extracting data from Web forums. Millions of users contribute rich information to Web forum everyday, which has become an important resource for manyWeb applications, such as product opinion retrieval, social network analysis, and so on. The novelty of the proposed algorithm is that it can not only extract the pure text but also distinguish between the original post and replies. Experimental results on a large number of real Web forums indicate that the proposed algorithm can correctly ex

#index 1227740
#* Temporal collaborative filtering with adaptive neighbourhoods
#@ Neal Lathia;Stephen Hailes;Licia Capra
#t 2009
#c 13
#% 734590
#% 1116993
#% 1358068
#! Collaborative Filtering aims to predict user tastes, by minimising the mean error produced when predicting hidden user ratings. The aim of a deployed recommender system is to iteratively predict users' preferences over a dynamic, growing dataset, and system administrators are confronted with the problem of having to continuously tune the parameters calibrating their CF algorithm. In this work, we formalise CF as a time-dependent, iterative prediction problem. We then perform a temporal analysis of the Netflix dataset, and evaluate the temporal performance of two CF algorithms. We show that, due to the dynamic nature of the data, certain prediction methods that improve prediction accuracy on the Netflix probe set do not show similar improvements over a set of iterative train-test experiments with growing data. We then address the problem of parameter selection and update, and propose a method to automatically assign and update per-user neighbourhood sizes that (on the temporal scale) outperforms setting global parameters.

#index 1227741
#* Temporal query substitution for ad search
#@ Wen Zhang;Jun Yan;Shuicheng Yan;Ning Liu;Zheng Chen
#t 2009
#c 13
#% 765412
#% 805839
#% 1074101
#! Recently, information retrieval researchers have witnessed the increasing interest in query substitution for ad search. Most previous works substitute search queries via content based query similarities, and few of them take the temporal characteristics of queries into consideration. In this extended abstract, we propose a novel temporal similarity measurement for query substitution in ad search task. We firstly extract temporal features, such as burst and periodicity, from query frequency curves and then define the temporal query similarity by integrating these new features with the temporal query frequency distribution. Compared to the traditional temporal similarity measurements such as correlation coefficient, our proposed approach is more effective owing to the explicit extraction of high-level semantic query temporal features for similarity measure. The experimental results demonstrate that the proposed similarity measure can make the ads more relevant to user search queries compared to ad search without temporal features.

#index 1227742
#* Term-based commercial intent analysis
#@ Azin Ashkan;Charles L.A. Clarke
#t 2009
#c 13
#% 590523
#% 754059
#% 869550
#% 956622
#% 1682429
#! In this work, we investigate the contribution of query terms and their corresponding ad click rates on commercial intent of queries. A probabilistic model is proposed following the hypothesis that a query is likely to receive ad clicks based on contributions from its individual terms.

#index 1227743
#* Topic (query) selection for IR evaluation
#@ Jianhan Zhu;Jun Wang;Vishwa Vinay;Ingemar J. Cox
#t 2009
#c 13
#% 262102
#% 309093
#% 818222
#% 987252
#! The need for evaluating large amounts of topics (queries) makes IR evaluation an uneasy task. In this paper, we study a topic selection problem for IR evaluation. The selection criterion is based on the overall difficulty of the chosen set, as well as the uncertainty of the final IR metric applied to the systems. Our preliminary experiments demonstrate that our approach helps to identify a set of topics that provides confident estimates of systems' performance while keeping the requirement of the query difficulty.

#index 1227744
#* Topic prerogative feature selection using multiple query examples for automatic video retrieval
#@ P. Punitha;Joemon M. Jose;Anuj Goyal
#t 2009
#c 13
#% 645687
#% 1132501
#! Well acceptance of relevance feedback and collaborative systems has given the users to express their preferences in terms of multiple query examples. The technology devised to utilize these user preferences, is expected to mine the semantic knowledge embedded within these query examples. In this paper, we propose a video mining framework based on dynamic learning from queries, using a statistical model for topic prerogative feature selection. The proposed method is specifically designed for multiple query example scenarios. The effectiveness of the proposed framework has been established with an extensive experimentation on TRECVid2007 data collection. The results reveal that our approach achieves a performance that is in par with the best results for this corpus without the requirement of any textual data.

#index 1227745
#* Topic set size redux
#@ Ellen M. Voorhees
#t 2009
#c 13
#% 309093
#% 318407
#% 397163
#% 818222
#% 879630
#% 1074057
#% 1074132
#! The cost as well as the power and reliability of a retrieval test collection are all proportional to the number of topics included in it. Test collections created through community evaluations such as TREC generally use 50 topics. Prior work estimated the reliability of 50-topic sets by extrapolating confidence levels from those of smaller sets, and concluded that 50 topics are sufficient to have high confidence in a comparison, especially when the comparison is statistically significant. Using topic sets that actually contain 50 topics, this paper shows that statistically significant differences can be wrong, even when statistical significance is accompanied by moderately large (10%) relative differences in scores. Further, using standardized evaluation scores rather than raw evaluation scores does not increase the reliability of these paired comparisons. Researchers should continue to be skeptical of conclusions demonstrated on only a single test collection.

#index 1227746
#* Transforming patents into prior-art queries
#@ Xiaoibng Xue;W. Bruce Croft
#t 2009
#c 13
#% 281396
#% 783530
#% 843731
#% 987331
#! Searching for prior-art patents is an essential step for the patent examiner to validate or invalidate a patent application. In this paper, we consider the whole patent as the query, which reduces the burden on the user, and also makes many more potential search features available. We explore how to automatically transform the query patent into an effective search query, especially focusing on the effect of different patent fields. Experiments show that the background summary of a patent is the most useful source of terms for generating a query, even though most previous work used the patent claims.

#index 1227747
#* Two-stage query segmentation for information retrieval
#@ Michael Bendersky;W. Bruce Croft;David A. Smith
#t 2009
#c 13
#% 766428
#% 818239
#% 818262
#% 1055706
#% 1055856
#% 1074098
#% 1715627
#! Modeling term dependence has been shown to have a significant positive impact on retrieval. Current models, however, use sequential term dependencies, leading to an increased query latency, especially for long queries. In this paper, we examine two query segmentation models that reduce the number of dependencies. We find that two-stage segmentation based on both query syntactic structure and external information sources such as query logs, attains retrieval performance comparable to the sequential dependence model, while achieving a 50% reduction in query latency.

#index 1227748
#* Undergraduates' evaluations of assigned search topics
#@ Earl W. Bailey;Diane Kelly;Karl Gyllstrom
#t 2009
#c 13
#% 411762
#% 879621
#% 1227623
#! This paper evaluates undergraduate students' knowledge, interests and experiences with 20 topics from the TREC Robust Track collection. The goal is to characterize these topics along several dimensions to help researchers make more informed decisions about which topics are most appropriate to use in experimental IIR evaluations with undergraduate student subjects.

#index 1227749
#* A unified inverted index for an efficient image and text retrieval
#@ Jonathan Mamou;Yosi Mass;Michal Shmueli-Scheuer;Benjamin Sznajder
#t 2009
#c 13
#% 655302
#% 818938
#% 857113
#! We present an efficient method for approximate search in a combination of several metric spaces -- which are a generalization of low level image features -- using an inverted index. Our approximation gives very high recall with subsecond response time on a real data set of one million images extracted from Flickr. We further exploit the inverted index to improve efficiency of the query processing by combining our search in metric features with search in associated textual metadata.

#index 1227750
#* Usefulness of click-through data in expert search
#@ Craig Macdonald;Ryen W. White
#t 2009
#c 13
#% 312701
#% 879567
#% 879570
#% 907525
#! The task in expert finding is to identify members of an organisation with relevant expertise on a given topic. Typically, an expert search engine uses evidence from the authors of on-topic documents found in the organisation's intranet by search engines. The search result click-through behaviour of many intranet search engine users provides an additional source of evidence to identify topically-relevant documents, and via document authorship, experts. In this poster, we assess the usefulness of click-through log data for expert finding. We find that ranking authors based solely on the clicks their documents receive is reasonably effective at correctly identifying relevant experts. Moreover, we show that this evidence can successfully be integrated with an existing expert search engine to increase its retrieval effectiveness.

#index 1227751
#* User-centric multi-criteria information retrieval
#@ Shawn R. Wolfe;Yi Zhang
#t 2009
#c 13
#% 987266
#% 1000205
#! Information retrieval models usually represent content only, and not other considerations, such as authority, cost, and recency. How could multiple criteria be utilized in information retrieval, and how would it effect the results? In our experiments, using multiple user-centric criteria always produced better results than a single criteria.

#index 1227752
#* Users' stopping behaviors and estimates of recall
#@ Maureen Dostert;Diane Kelly
#t 2009
#c 13
#% 1358
#% 804884
#% 1603272
#! This paper investigates subjects' stopping behaviors and estimates of recall in an interactive information retrieval (IIR) experiment. Subjects completed four recall-oriented search tasks and were asked to estimate how many of the relevant documents they believed they had found after each task. Subjects also responded to an interview question probing their reasons for stopping a search. Results show that most subjects believed they found about 51-60% of the relevant documents and that this estimate was correlated positively with number of documents saved and actual recall, even though subjects' recall estimates were inaccurate. Reasons given for stopping search are also explored.

#index 1227753
#* Using dynamic markov compression to detect vandalism in the wikipedia
#@ Kelly Y. Itakura;Charles L. A. Clarke
#t 2009
#c 13
#% 38374
#% 961230
#% 1415780
#! We apply the Dynamic Markov Compression model to detect spam edits in the Wikipedia. The method appears to outperform previous efforts based on compression models, providing performance comparable to methods based on manually constructed rules.

#index 1227754
#* Using wikipedia categories for ad hoc search
#@ Rianne Kaptein;Marijn Koolen;Jaap Kamps
#t 2009
#c 13
#% 766429
#! In this paper we explore the use of category information for ad hoc retrieval in Wikipedia. We show that techniques for entity ranking exploiting this category information can also be applied to ad hoc topics and lead to significant improvements. Automatically assigned target categories are good surrogates for manually assigned categories, which perform only slightly better.

#index 1227755
#* Visualizing the problems with the INEX topics
#@ Andrew Trotman;Maria del Rocio Gomez Crisostomo;Mounia Lalmas
#t 2009
#c 13
#% 309093
#% 420508
#% 879630
#% 879696
#% 1173703
#! Topics form a crucial component of a test collection. We show, through visualization, that the INEX 2008 topics have shortcomings, which questions their validity for evaluating XML retrieval effectiveness.

#index 1227756
#* What queries are likely to recur in web search?
#@ Dell Zhang;Jinsong Lu
#t 2009
#c 13
#% 577302
#% 860861
#% 878624
#% 987211
#% 987215
#% 1035574
#% 1130854
#% 1190098
#! We study the recurrence dynamics of queries in Web search by analysing a large real-world query log dataset. We find that query frequency is more useful in predicting collective query recurrence whereas query recency is more useful in predicting individual query recurrence. Our findings provide valuable insights for understanding and improving Web search.

#index 1227757
#* When is query performance prediction effective?
#@ Claudia Hauff;Leif Azzopardi
#t 2009
#c 13
#% 218978
#% 262084
#% 397161
#% 879614
#% 987260
#! The utility of Query Performance Prediction (QPP) methods is commonly evaluated by reporting correlation coefficients to denote how well the methods perform at predicting the retrieval performance of a set of queries. However, a quintessential question remains unexplored: how strong does the correlation need to be in order to realize an increase in retrieval performance? In this work, we address this question in the context of Selective Query Expansion (SQE) and perform a large-scale experiment. The results show that to consistently and predictably improve retrieval effectiveness in the ideal SQE setting, a Kendall's Tau correlation of tau=0.5 is required, a threshold which most existing query performance prediction methods fail to reach.

#index 1227758
#* Who said what to whom?: capturing the structure of debates
#@ Rianne Kaptein;Maarten Marx;Jaap Kamps
#t 2009
#c 13
#% 766429
#% 1256692
#! Transcripts of meetings are a document genre characterized by a complex narrative structure. The essence is not only what is said, but also by who and to whom. This paper investigates whether we can use semantic annotations like the speaker in order to capture this debate structure, as well as the related content of the debate. The structure is visualized in a graph, while the content is condensed into word clouds, that are created using a parsimonious language model. Evaluation shows that both tools adequately capture the structure and content of the debate at an aggregated level.

#index 1227759
#* EvaluatIR: an online tool for evaluating and comparing IR systems
#@ Timothy G. Armstrong;Alistair Moffat;William Webber;Justin Zobel
#t 2009
#c 13
#% 857180

#index 1227760
#* Expertise search in academia using facets
#@ Duncan McDougall;Craig Macdonald
#t 2009
#c 13

#index 1227761
#* Exploiting social context for expertise propagation
#@ Greg P. Milette;Michael K. Schneider;Kathy Ryall;Robert Hyland
#t 2009
#c 13
#% 983883

#index 1227762
#* Social networks and discovery in the enterprise (SaND)
#@ Inbal Ronen;Elad Shahar;Sigalit Ur;Erel Uziel;Sivan Yogev;Naama Zwerdling;David Carmel;Ido Guy;Nadav Har'el;Shila Ofek-Koifman
#t 2009
#c 13
#% 1047412
#% 1215458

#index 1227763
#* Sifting micro-blogging stream for events of user interest
#@ Maxim Grinev;Maria Grineva;Alexander Boldakov;Leonid Novak;Andrey Syssoev;Dmitry Lizorkin
#t 2009
#c 13
#% 1022338
#% 1190121
#! Micro-blogging is a new form of social communication that encourages users to share information about anything they are seeing or doing, the motivation facilitated by the ability to post brief text messages through a variety of devices. Twitter, the most popular micro-blogging tool, is exhibiting rapid growth [3]: up to 11% of online Americans are using Twitter by December 2008, compared to 6% in May 2008. Due to its nature, micro-blogosphere has unique features: (i) It is a source of extremely up-to-date information about what is happening in the world; (ii) It captures the wisdom of millions of people and covers a broad range of domains. These features make micro-blogosphere more than a popular medium of social communication: we believe that it has additionally become a valuable source of extremely up-to-date news on virtually any subject of user interest. Making use of micro-blogosphere in this new role we meet the following challenges: (A) Since any given subject is generally mentioned in the micro-blogging stream on the continuous basis, a method is needed for locating periods of news on this subject. (B) Additionally, even for such periods, stream filtering is required for removing noise and for extracting messages that best describe the news. To address these challenges we make and exploit the following observations: (A) For an arbitrary subject, events that catch user interest gain distinguishably more attention than the average mentioning of the subject resulting in message activity bursts for it. (B) Most of the messages in an activity burst describe common event in close variations - either rephrased or "retweeted" between the users. We demonstrate TweetSieve - a system that allows obtaining news on any given subject by sifting the Twitter stream. Our work is related to frequecy-based analysis applied to blogs [1], but higher latency and lower coverage in blogs makes the analysis less effective than in case of micro-blogs. In TweetSieve demo, the user is able to express the subject of her interest by an arbitrary search string. The system shows the period of events occuring for the subject and outputs tweets that best describe each of the events. Figure 1 shows a screenshot of the system for "Semantic search" as a sample subject. The underlying process consists of two steps: Identifying activity bursts. Counting the messages matching the search string in the stream over time, the frequency curve is constructed. Activity bursts in the curve are identified by taking the periods of frequency exceeding the standard deviation from the average. Selecting messages that best describe news events. For the set of all messages matching the search string in an activity burst, we apply the message-granular variation of our keyphrase extraction algorithm [2] that is specifically suited to efficiently filtering noisy data. The algorithm clusters messages with respect to their similarity to each other and chooses central messages from the most dense clusters. As the similarity measure we use Jaccard coefficient for the "bag of words" representation of messages. The demonstration illustrates the potential of our approach in bringing news acquisition to a new level of promptness and coverage range.

#index 1227764
#* Incentives for social annotation
#@ Heather Roinestad;John Burgoon;Benjamin Markines;Filippo Menczer
#t 2009
#c 13
#% 1065099
#% 1065415
#! The effectiveness of community-driven annotation, such as social bookmarking, depends on user participation. Since the participation of many users is motivated by selfish reasons, an effective way to encourage participation is to create useful or entertaining applications. We demo two such tools -- a browser extension and a game.

#index 1227765
#* Accommodating colorblind users in image search
#@ Meng Wang;Bo Liu;Linjun Yang;Xian-Sheng Hua
#t 2009
#c 13
#! There are about 8% of men and 0.8% of women suffering from colorblindness. Due to certain loss of color information, the existing image search techniques may not provide satisfactory results for these users. In this demonstration, we show an image search system that can accommodate colorblind users. It can help these special users find and enjoy what they want by providing multiple services for them, including search results reranking, image recoloring and color indication.

#index 1227766
#* Generic similarity search engine demonstrated by an image retrieval application
#@ David Novak;Michal Batko;Pavel Zezula
#t 2009
#c 13
#% 857113
#! We introduce a generic engine for large-scale similarity search and demonstrate it on a set of 100 million Flickr images.

#index 1227767
#* Pharos: an audiovisual search platform
#@ Alessandro Bozzon;Marco Brambilla;Piero Fraternali;Francesco Nucci;Stefan Debald;Eric Moore;Wolfgang Neidl;Michel Plu;Patrick Aichroth;Olli Pihlajamaa;Cyril Laurier;Serge Zagorac;Gerhard Backfried;Daniel Weinland;Vincenzo Croce
#t 2009
#c 13

#index 1227768
#* Agate: information gathering for risk monitoring
#@ Arnaud Saval;Yann Mombrun
#t 2009
#c 13
#! Internet sources provide new ways to acquire information about risk and to follow up the evolution of natural disasters in real time. We will present first, an architecture dedicated to unstructured information processing. Then, we will show how the spatial and temporal representation extended with semantic properties answers information ambiguity problem. Agate platform was evaluated by a non-governmental organization which filtered alerts according to types of disaster and their locations.

#index 1227769
#* wikiSearch: enabling interactivity in search
#@ Elaine G. Toms;Tayze Mackenzie;Chris Jordan;Sam Hall
#t 2009
#c 13
#! wikiSearch, is a search engine customized for the Wikipedia corpus but with design features that may be generalized to other search systems. Its features enhance basic functionality and enable more fluid interactivity while supporting both workflow in the search process and the experimental process used in lab testing.

#index 1227770
#* CDPRanking: discovering and ranking cross-document paths between entities
#@ Wei Jin;Xin Wu
#t 2009
#c 13
#% 1032042

#index 1227771
#* Context-based health information retrieval
#@ Carla Teixeira Lopes
#t 2009
#c 13
#% 169777
#% 901496

#index 1227772
#* Modeling uncertainty in video retrieval: a retrieval model for uncertain semantic representations of videos
#@ Robin Aly
#t 2009
#c 13
#% 73046
#% 1292876

#index 1227773
#* Toponym ambiguity in geographical information retrieval
#@ Davide Buscaldi
#t 2009
#c 13
#% 420471
#% 642994
#! The objectives of this research work is to study the effects of toponym (place name) ambiguity in the Geographical Information Retrieval (GIR) task. Our experience with GIR systems shows that toponym ambiguity may be an important factor in the inability of these systems to take advantage from geographical knowledge. Previous studies over ambiguity and Information Retrieval (IR) suggested that disambiguation may be useful in some specific IR scenario. We suppose that GIR may constitute such a scenario. This preliminary study was carried out over the WordNet based, manually disambiguated collection developed for the CLIR-WSD task, using the GeoCLEF collection of 100 geographically related topics. The employed GIR system was based on the GeoWorSE system that participated in GeoCLEF 2008. The experiments were carried out considering the manual disambiguation and comparing this result with those obtained by randomly disambiguating the document collection and those obtained by using always the most common referent. The obtained results show no significant difference in the overall results, although the work gave an insight into some errors that are produced by toponym ambiguity and how they may affect the results. These preliminary results also suggest that WordNet is not a suitable resource for the planned research.

#index 1227774
#* Exploiting temporal information in retrieval of archived documents
#@ Nattiya Kanhabua
#t 2009
#c 13
#% 755899
#% 1107069
#! In a text retrieval community, many researchers have shown a good quality of searching a current snapshot of the Web. However, only a small number have demonstrated a good quality of searching a long-term archival domain, where documents are preserved for a long time, i.e., ten years or more. In such a domain, a search application is not only applicable for archivists or historians, but also in a context of national library and enterprise search (searching document repositories, emails, etc.). In the rest of this paper, we will explain three problems of searching document archives and propose possible approaches to solve these problems. Our main research question is: How to improve the quality of search in a document archive using temporal information?

#index 1227775
#* Using document structure for automatic summarization
#@ Aurélien Bossard
#t 2009
#c 13
#% 1737144

#index 1227776
#* Topic structure for information retrieval
#@ Jiyin He
#t 2009
#c 13
#% 1074396
#% 1415785
#! In my research, I propose a coherence measure, with the goal of discovering and using topic structures within and between documents, of which I explore its extensions and applications in information retrieval.

#index 1227777
#* Using computational community interest as an indicator for ranking
#@ Xiaozhong Liu
#t 2009
#c 13
#% 722904
#! Ranking documents in response to users' information needs is a challenging task, due, in part, to the dynamic nature of users' interests with respect to a query. I hypothesize that the interests of a given user are similar to the interests of the broader community of which he is a part and propose an innovative method that uses social media to characterize the interests of the community and use this characterization to improve future rankings. By generating a community interest vector (CIV) for a given query, we use community interest to alter the ranking score of individual documents retrieved by the query. The CIV is based on a continuously updated set of recent (daily or past few hours) user-oriented text data. The user-oriented data can be user blogs or user comment tagged news. Preliminary evaluation shows that the new ranking method significantly improves ranking performance.

#index 1227778
#* Affective adaptive retrieval: study of emotion in adaptive retrieval
#@ Yashar Moshfeghi
#t 2009
#c 13
#% 1292861

#index 1227779
#* A study on performance volatility in information retrieval
#@ Mehdi Hosseini
#t 2009
#c 13
#% 878912
#% 1074057
#! A common practice in comparative evaluation of information retrieval (IR) systems is to create a test collection comprising a set of topics (queries), a document corpus, and relevance judgments, and to monitor the performance of retrieval systems over such a collection. A typical evaluation of a system involves computing a performance metric, e.g., Average Precision (AP), for each topic and then using the average performance metric, e.g., Mean Average Precision (MAP) to express the overall system performance. However, averages do not capture all the important aspects of system performance, and used alone may not thoroughly express system effectiveness, i.e., average of performance can mask large variance in individual topic effectiveness. The author hypothesis is that, in addition to the average of overall performance, attention needs to be paid to how a system performance varies across topics. This variability can be measured by calculating the standard deviation (SD) of individual performance scores. We refer to this performance variation as Volatility.

#index 1227780
#* Novelty detection across different source types and languages
#@ Johannes Schanda
#t 2009
#c 13

#index 1227781
#* Personalizing information retrieval using task features, topic knowledge, and task product
#@ Jingjing Liu
#t 2009
#c 13
#% 378486
#% 818259
#% 907515
#% 907516
#! Personalization of information retrieval tailors search towards individual users to meet their particular information needs. Personalization systems obtain additional information about users and their contexts beyond the queries they submit to the systems, and use this information to bring the desired documents to top ranks. The additional information can come from various sources: user preferences, user behaviors, contexts, etc. [1] To avoid users taking extra effort in providing explicit preferences, most personalization approaches have adopted an implicit strategy to obtain users' interests from their behaviors and/or contexts, such as query history, browsing history, and so on. Task, topic knowledge, and desktop information have been used as evidence for personalization. Tailoring display time threshold based on task information was found to improve implicit relevance feedback performance [5]. User's familiarity with search topics was found to be positively correlated with reading time but negatively correlated with search efficacy [3]. This indicated the possibility of inferring topic familiarity from searching behavior. Desktop information was also found to be a good source for personalization [2, 4], and personalization using only those files relevant to user queries are more effective than using the entire desktop data [2]. Since search often happens in a work task environment, we examine how user-generated products and retained documents can help improve search performance. To these ends, this study looks at how the following factors can help personalize search: features of user's work tasks (including task stage and task type), user's familiarity with work task topic, user's saving and using behaviors, and task product(s) that the user generated for the work task. Work tasks are designed to include multiple sub-tasks, each being a stage. Two types of sub-task interdependence are considered: parallel, where the sub-tasks do not depend upon each other, or dependent, where one sub-task depends upon the accomplishment of other sub-task(s). The study examines the interaction effects of these factors, dwell time, and document usefulness. It also looks at a personalization technique that extracts terms for query expansion from work task product(s) and user behaviors. There are three research questions: RQ1: Does the stage of the user's task help predict document usefulness from dwell time in the parallel and the dependent tasks, respectively? RQ2. Does the user's familiarity with work task topic help predict document usefulness from dwell time in the parallel and the dependent tasks, respectively? RQ3. Do user's task product(s) and saving and using behaviors help with query disambiguation? Twenty-four participants are recruited, each coming three times (as three experiment sessions) to a usability laboratory working on three sub-tasks in a general task, either a parallel or a dependent. Take the parallel task as an example. It asks the participants to write a three-section article on hybrid cars, and each section is finished in one session. The three sections focus on Honda Civic sedan hybrid, Nissan Altima sedan hybrid, and Toyota Camry sedan hybrid, respectively. When searching for information, half of the participants use a query expansion condition, where the system recommends search terms based on their work in previous sessions, and the other half use a non-query expansion system condition. Data are collected by three major means: logging software that records user-system interactions, an eye tracker that records eye movement, and questionnaires that elicit users' background information and their perceptions on a number of aspects. The results will provide new evidence on personalizing search by taking account of the examined contextual factors.

#index 1227782
#* Exploiting memory cues in personal lifelog retrieval
#@ Yi Chen
#t 2009
#c 13
#% 751830
#% 936919
#% 1047409
#! In recent years personal lifelogs (PLs) have become an emerging field of research. PLs are collections of digital data taken from an individual's life experiences, gathered from both digital and physical worlds. PLs collections can potentially be collected over periods of years and thus can be very large. In our research group, four researchers have been engaged in collection of individual one-year-long PLs data sets. These collections include logs of computer and mobile phone activities, digital photos, in particular passively captured Microsoft SenseCam images, geo-location (via GPS), surrounding people or objects (via Bluetooth), and various biometric data. The complex data types and heterogeneous structure of this corpus brings great challenges to traditional content based information retrieval (IR). Yet, the rich connections integral to personal experience offer exciting potential opportunities to leverage features from human memory and associated models to support retrieval. My PhD project aims to develop an interface to assist IR from PLs. In doing this I plan to exploit features in human memory, in particular the mechanisms in associative memory models. Previous studies in personal information re-finding have explored the use of generally well-remembered attributes or metadata of the search targets, such as date, item type/format, authors of documents [1]. There have also been systems which utilize associated computer items or real life events (e.g. [2, 3]) to assist re-finding tasks. However, few of them looked into exactly what types of associated items/events people tend to recall. I plan to explore associations among PL items, as well as their attributes regarding their role in an individual's memory, since I believe that some associations and types of metadata which are available and feasible for use, may have been omitted in existing systems; due to the methods used in previous research where the users' behaviour may have been guided by the searching or management tools available to them. As indicated by some information seeking studies (e.g. [4]), different search context, search motivation, or personal differences such as habits, may lead to varied recall of contents and information seeking behaviours. For this reason, I will also investigate: the influences on personal information re-finding behaviour of context, lifestyle, and differences in prior personal experiences of IR tools. Results from these studies will be used to explore personalisation in search, e.g. to dynamically increase the importance of geo-location in scoring of search results for subjects who travel frequently. As indicated by [4], people tend to make small steps to approach the targets they are looking for, rather than trying to do this in a single search with a "perfect query" comprising all of the relevant details, partially because of their trouble in recalling them. To relieve users from the heavy cognitive burden of recalling the exact target, and on the other hand to reduce the rate of inaccurate queries caused by false recall, my proposed interface will be based on browsing and recognizing, instead of traditional recalling and searching. For example, a user will be able to browse and narrow results by recognizing landmarks and estimating the target activities' time range from the user's digital or physical life [5]. An important issue in my work will be to consider the challenges of evaluating my work with only a very limited number of PL datasets. To partially address this issue, I am currently in engaged in a number of smaller scale diary studies of searching experiences for larger numbers of subjects.

#index 1450826
#* Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval
#@ Fabio Crestani;Stéphane Marchand-Maillet;Hsin-Hsi Chen;Efthimis N. Efthimiadis;Jacques Savoy
#t 2010
#c 13
#! Welcome to the 33rd ACM SIGIR International Conference of Research and Development on Information Retrieval. SIGIR 2010 has attracted a record-breaking number of papers signalling once again the importance of information retrieval research. We continue to see a steady growth of research output as well as a growing diversity of subjects in our field, where emerging topics, such as learning to rank, social media search, query logs analysis, recommender systems or advertising and search, are now reaching a relative maturity. This year we observe a continued interest in foundational aspects of IR, such as IR theory and evaluation studies, and also a growing interest on traditional topics, such a clustering and classification. If we want to summarize this SIGIR conference with a single word or phrase, we can suggest "users" or "users and queries" indicating the importance of users in search. But, we will let you discover this for yourself while delving through these conference proceedings. There were 520 full paper submissions representing the work of IR researchers in more than 39 countries. Of these, 87 (16.7%) were accepted, representing the different geographic areas as follows: 42 from the Americas, 25 from Europe -- Africa and 20 from Asia -- Pacific. In addition to the full papers, a further 5 were offered the opportunity of presentation as posters. There were 90 (30.7%) posters, 10 (50%) demonstrations, 11 (52%) tutorials and 9 (50%) workshops accepted for inclusion in the technical program. A doctoral consortium with 11 PhD candidates is also part of the technical program. It is worth noting that more than half of the submitted papers (293, or 56%) have a student as the first author, an encouraging sign for the growth and vitality of the IR community. We are grateful to the keynote speakers Donna Harman from NIST, and Gary Flake from Microsoft, who agreed to share their ideas with the community.

#index 1450827
#* Is the cranfield paradigm outdated?
#@ Donna Harman
#t 2010
#c 13

#index 1450828
#* Refactoring the search problem
#@ Gary William Flake
#t 2010
#c 13
#! The most common way of framing the search problem is as an exchange between a user and a database, where the user issues queries and the database replies with results that satisfy constraints imposed by the query but that also optimize some notion of relevance. There are several variations to this basic model that augment the dialogue between humans and machines through query refinement, relevance feedback, and other mechanism. However, rarely is this problem ever posed in a way in which the properties of the client and server are fundamentally different and in a way in which exploiting the differences can be used to yield substantially different experiences. I propose a reframing of the basic search problem which presupposes that servers are scalable on most dimensions but suffer from low communication latencies while clients have lower scalability but support vastly richer user interactions because of lower communication latencies. Framed in this manner, there is clear utility in refactoring the search problem so that user interactions are processed fluidly by a client while the server is relegated to pre-computing the properties of a result set that cannot be efficiently left to the client. I will demonstrate Pivot, an experimental client application that allows the user to visually interact with thousands of search results at once, while using facetted-based exploration in a zoomable interface. I will argue that the evolving structure of the Web will tend to push all IR-based applications in a similar direction, which has the algorithmic intelligence increasingly split between clients and servers. Put another way, my claim is that future clients will be neither thin nor dumb.

#index 1450829
#* Prototype hierarchy based clustering for the categorization and navigation of web collections
#@ Zhao-Yan Ming;Kai Wang;Tat-Seng Chua
#t 2010
#c 13
#% 280849
#% 309141
#% 464291
#% 643068
#% 754076
#% 770782
#% 806594
#% 807295
#% 1074110
#% 1074128
#% 1190075
#% 1190277
#% 1227594
#% 1227600
#% 1292559
#% 1328333
#! This paper presents a novel prototype hierarchy based clustering (PHC) framework for the organization of web collections. It solves simultaneously the problem of categorizing web collections and interpreting the clustering results for navigation. By utilizing prototype hierarchies and the underlying topic structures of the collections, PHC is modeled as a multi-criterion optimization problem based on minimizing the hierarchy evolution, maximizing category cohesiveness and inter-hierarchy structural and semantic resemblance. The flexible design of metrics enables PHC to be a general framework for applications in various domains. In the experiments on categorizing 4 collections of distinct domains, PHC achieves 30% improvement in ¼F1 over the state-of-the-art techniques. Further experiments provide insights on performance variations with abstract and concrete domains, completeness of the prototype hierarchy, and effects of different combinations of optimization criteria.

#index 1450830
#* Person name disambiguation by bootstrapping
#@ Minoru Yoshida;Masaki Ikeda;Shingo Ono;Issei Sato;Hiroshi Nakagawa
#t 2010
#c 13
#% 173879
#% 309128
#% 397147
#% 747890
#% 805885
#% 838408
#% 855094
#% 938728
#% 939515
#% 1074054
#% 1213625
#% 1264823
#% 1271267
#% 1271313
#% 1278509
#% 1411052
#! In this paper, we report our system that disambiguates person names in Web search results. The system uses named entities, compound key words, and URLs as features for document similarity calculation, which typically show high precision but low recall clustering results. We propose to use a two-stage clustering algorithm by bootstrapping to improve the low recall values, in which clustering results of the first stage are used to extract features used in the second stage clustering. Experimental results revealed that our algorithm yields better score than the best systems at the latest WePS workshop.

#index 1450831
#* Self-taught hashing for fast similarity search
#@ Dell Zhang;Jun Wang;Deng Cai;Jinsong Lu
#t 2010
#c 13
#% 200694
#% 234905
#% 260001
#% 280817
#% 313959
#% 326812
#% 344447
#% 376266
#% 402289
#% 410276
#% 457998
#% 458379
#% 479649
#% 593047
#% 724290
#% 766418
#% 860956
#% 879600
#% 881477
#% 891060
#% 898309
#% 916799
#% 983899
#% 986037
#% 987258
#% 987347
#% 1073923
#% 1077150
#% 1083671
#% 1117691
#% 1126600
#% 1189164
#% 1190099
#% 1211829
#% 1215859
#% 1227596
#% 1385997
#% 1697462

#index 1450832
#* Personalizing information retrieval for multi-session tasks: the roles of task stage and task type
#@ Jingjing Liu;Nicholas J. Belkin
#t 2010
#c 13
#% 187999
#% 267753
#% 581916
#% 766454
#% 818206
#% 835027
#% 907516
#% 948379
#% 955711
#% 1077044
#% 1126944
#! Dwell time as a user behavior has been found in previous studies to be an unreliable predictor of document usefulness, with contextual factors such as the user's task needing to be considered in its interpretation. Task stage has been shown to influence search behaviors including usefulness judgments, as has task type. This paper reports on an investigation of how task stage and task type may help predict usefulness from the time that users spend on retrieved documents, over the course of several information seeking episodes. A 3-stage controlled experiment was conducted with 24 participants, each coming 3 times to work on 3 sub-tasks of a general task, couched either as "parallel" or "dependent" task type. The full task was to write a report on the general topic, with interim documents produced for each sub-task. Results show that task stage can help in inferring document usefulness from decision time, especially in the parallel task. The findings can be used to increase accuracy in predicting document usefulness and accordingly in personalizing search for multi-session tasks.

#index 1450833
#* Predicting searcher frustration
#@ Henry A. Feild;James Allan;Rosie Jones
#t 2010
#c 13
#% 449291
#% 761674
#% 805200
#% 977371
#% 987263
#% 987321
#% 1006312
#% 1019124
#% 1190348
#% 1210687
#% 1275193
#% 1292474
#% 1348349
#% 1355038
#! When search engine users have trouble finding information, they may become frustrated, possibly resulting in a bad experience (even if they are ultimately successful). In a user study in which participants were given difficult information seeking tasks, half of all queries submitted resulted in some degree of self-reported frustration. A third of all successful tasks involved at least one instance of frustration. By modeling searcher frustration, search engines can predict the current state of user frustration and decide when to intervene with alternative search strategies to prevent the user from becoming more frustrated, giving up, or switching to another search engine. We present several models to predict frustration using features extracted from query logs and physical sensors. We are able to predict frustration with a mean average precision of 65% from the physical sensors, and 87% from the query log features.

#index 1450834
#* The good, the bad, and the random: an eye-tracking study of ad quality in web search
#@ Georg Buscher;Susan T. Dumais;Edward Cutrell
#t 2010
#c 13
#% 590523
#% 754059
#% 766472
#% 818221
#% 879566
#% 949162
#% 954948
#% 954949
#% 987209
#% 987224
#% 996725
#% 1053505
#% 1074069
#% 1130852
#% 1166518
#% 1183067
#% 1187374
#% 1190081
#% 1404621
#% 1678005
#! We investigate how people interact with Web search engine result pages using eye-tracking. While previous research has focused on the visual attention devoted to the 10 organic search results, this paper examines other components of contemporary search engines, such as ads and related searches. We systematically varied the type of task (informational or navigational), the quality of the ads (relevant or irrelevant to the query), and the sequence in which ads of different quality were presented. We measured the effects of these variables on the distribution of visual attention and on task performance. Our results show significant effects of each variable. The amount of visual attention that people devote to organic results depends on both task type and ad quality. The amount of visual attention that people devote to ads depends on their quality, but not the type of task. Interestingly, the sequence and predictability of ad quality is also an important factor in determining how much people attend to ads. When the quality of ads varied randomly from task to task, people paid little attention to the ads, even when they were good. These results further our understanding of how attention devoted to search results is influenced by other page elements, and how previous search experiences influence how people attend to the current page.

#index 1450835
#* Ranking using multiple document types in desktop search
#@ Jinyoung Kim;W. Bruce Croft
#t 2010
#c 13
#% 194246
#% 397161
#% 413594
#% 577224
#% 642983
#% 642992
#% 643012
#% 783474
#% 987195
#% 987249
#% 1065099
#% 1130914
#% 1227616
#% 1264113
#% 1292493
#% 1292595
#% 1292597
#% 1392496
#! A typical desktop environment contains many document types (email, presentations, web pages, pdfs, etc.) each with different metadata. Predicting which types of documents a user is looking for in the context of a given query is a crucial part of providing effective desktop search. The problem is similar to selecting resources in distributed IR, but there are some important differences. In this paper, we quantify the impact of type prediction in producing a merged ranking for desktop search and introduce a new prediction method that exploits type-specific metadata. In addition, we show that type prediction performance and search effectiveness can be further enhanced by combining existing methods of type prediction using discriminative learning models. Our experiments employ pseudo-desktop collections and a human computation game for acquiring realistic and reusable queries.

#index 1450836
#* Acquisition of instance attributes via labeled and related instances
#@ Enrique Alfonseca;Marius Pasca;Enrique Robledo-Arnuncio
#t 2010
#c 13
#% 129144
#% 742092
#% 756964
#% 786511
#% 815297
#% 829971
#% 938705
#% 956503
#% 956564
#% 1083705
#% 1089602
#% 1127393
#% 1136348
#% 1269447
#% 1270281
#% 1270651
#% 1275208
#% 1275209
#% 1712125
#! This paper presents a method for increasing the quality of automatically extracted instance attributes by exploiting weakly-supervised and unsupervised instance relatedness data. This data consists of (a) class labels for instances and (b) distributional similarity scores. The method organizes the text-derived data into a graph, and automatically propagates attributes among related instances, through random walks over the graph. Experiments on various graph topologies illustrate the advantage of the method over both the original attribute lists and a per-class attribute extractor, both in terms of the number of attributes extracted per instance and the accuracy of the top-ranked attributes.

#index 1450837
#* Relevance and ranking in online dating systems
#@ Fernando Diaz;Donald Metzler;Sihem Amer-Yahia
#t 2010
#c 13
#% 300078
#% 458860
#% 480302
#% 632060
#% 894444
#% 987339
#% 1016203
#% 1035578
#% 1130811
#% 1130920
#% 1190055
#% 1292528
#% 1292681
#% 1328151
#% 1338562
#! Match-making systems refer to systems where users want to meet other individuals to satisfy some underlying need. Examples of match-making systems include dating services, resume/job bulletin boards, community based question answering, and consumer-to-consumer marketplaces. One fundamental component of a match-making system is the retrieval and ranking of candidate matches for a given user. We present the first in-depth study of information retrieval approaches applied to match-making systems. Specifically, we focus on retrieval for a dating service. This domain offers several unique problems not found in traditional information retrieval tasks. These include two-sided relevance, very subjective relevance, extremely few relevant matches, and structured queries. We propose a machine learned ranking function that makes use of features extracted from the uniquely rich user profiles that consist of both structured and unstructured attributes. An extensive evaluation carried out using data gathered from a real online dating service shows the benefits of our proposed methodology with respect to traditional match-making baseline systems. Our analysis also provides deep insights into the aspects of match-making that are particularly important for producing highly relevant matches.

#index 1450838
#* Scalability of findability: effective and efficient IR operations in large information networks
#@ Weimao Ke;Javed Mostafa
#t 2010
#c 13
#% 643013
#% 722311
#% 731617
#% 818210
#% 878911
#% 879606
#% 907459
#% 919207
#% 987277
#% 1131121
#% 1418196
#% 1711388
#% 1768420
#! It is crucial to study basic principles that support adaptive and scalable retrieval functions in large networked environments such as the Web, where information is distributed among dynamic systems. We conducted experiments on decentralized IR operations on various scales of information networks and analyzed effectiveness, efficiency, and scalability of various search methods. Results showed network structure, i.e., how distributed systems connect to one another, is crucial for retrieval performance. Relying on partial indexes of distributed systems, some level of network clustering enabled very efficient and effective discovery of relevant information in large scale networks. For a given network clustering level, search time was well explained by a poly-logarithmic relation to network size (i.e., the number of distributed systems), indicating a high scalability potential for searching in a growing information space. In addition, network clustering only involved local self-organization and required no global control - clustering time remained roughly constant across the various scales of networks.

#index 1450839
#* Caching search engine results over incremental indices
#@ Roi Blanco;Edward Bortnikov;Flavio Junqueira;Ronny Lempel;Luca Telloli;Hugo Zaragoza
#t 2010
#c 13
#% 255137
#% 268079
#% 330706
#% 340141
#% 340888
#% 387427
#% 451536
#% 480136
#% 577302
#% 790566
#% 805864
#% 860861
#% 956536
#% 1016222
#% 1074067
#% 1089473
#% 1190098
#% 1404875
#% 1834787
#! A Web search engine must update its index periodically to incorporate changes to the Web. We argue in this paper that index updates fundamentally impact the design of search engine result caches, a performance-critical component of modern search engines. Index updates lead to the problem of cache invalidation: invalidating cached entries of queries whose results have changed. Naive approaches, such as flushing the entire cache upon every index update, lead to poor performance and in fact, render caching futile when the frequency of updates is high. Solving the invalidation problem efficiently corresponds to predicting accurately which queries will produce different results if re-evaluated, given the actual changes to the index. To obtain this property, we propose a framework for developing invalidation predictors and define metrics to evaluate invalidation schemes. We describe concrete predictors using this framework and compare them against a baseline that uses a cache invalidation scheme based on time-to-live (TTL). Evaluation over Wikipedia documents using a query log from the Yahoo! search engine shows that selective invalidation of cached search results can lower the number of unnecessary query evaluations by as much as 30% compared to a baseline scheme, while returning results of similar freshness. In general, our predictors enable fewer unnecessary invalidations and fewer stale results compared to a TTL-only scheme for similar freshness of results.

#index 1450840
#* Query forwarding in geographically distributed search engines
#@ B. Barla Cambazoglu;Emre Varol;Enver Kayaaslan;Cevdet Aykanat;Ricardo Baeza-Yates
#t 2010
#c 13
#% 194246
#% 577302
#% 805864
#% 893126
#% 987215
#% 1132154
#% 1166527
#% 1190098
#% 1227597
#% 1227628
#% 1280763
#% 1292508
#% 1399951
#! Query forwarding is an important technique for preserving the result quality in distributed search engines where the index is geographically partitioned over multiple search sites. The key component in query forwarding is the thresholding algorithm by which the forwarding decisions are given. In this paper, we propose a linear-programming-based thresholding algorithm that significantly outperforms the current state-of-the-art in terms of achieved search efficiency values. Moreover, we evaluate a greedy heuristic for partial index replication and investigate the impact of result cache freshness on query forwarding performance. Finally, we present some optimizations that improve the performance further, under certain conditions. We evaluate the proposed techniques by simulations over a real-life setting, using a large query log and a document collection obtained from Yahoo!.

#index 1450841
#* A joint probabilistic classification model for resource selection
#@ Dzung Hong;Luo Si;Paul Bracke;Michael Witt;Tim Juchcinski
#t 2010
#c 13
#% 194246
#% 194275
#% 227891
#% 262063
#% 262096
#% 280856
#% 282422
#% 287463
#% 301225
#% 340146
#% 344448
#% 567255
#% 643012
#% 722312
#% 783473
#% 789959
#% 818212
#% 1130914
#% 1174737
#% 1227616
#% 1227629
#% 1292595
#% 1292723
#% 1392444
#! Resource selection is an important task in Federated Search to select a small number of most relevant information sources. Current resource selection algorithms such as GlOSS, CORI, ReDDE, Geometric Average and the recent classification-based method focus on the evidence of individual information sources to determine the relevance of available sources. Current algorithms do not model the important relationship information among individual sources. For example, an information source tends to be relevant to a user query if it is similar to another source with high probability of being relevant. This paper proposes a joint probabilistic classification model for resource selection. The model estimates the probability of relevance of information sources in a joint manner by considering both the evidence of individual sources and their relationship. An extensive set of experiments have been conducted on several datasets to demonstrate the advantage of the proposed model.

#index 1450842
#* Temporal click model for sponsored search
#@ Wanhong Xu;Eren Manavoglu;Erick Cantu-Paz
#t 2010
#c 13
#% 348135
#% 757953
#% 818221
#% 879567
#% 946521
#% 956546
#% 963669
#% 1035578
#% 1055687
#% 1074092
#% 1166517
#% 1171607
#% 1190055
#% 1190056
#% 1214675
#% 1246499
#! Previous studies on search engine click modeling have identified two presentation factors that affect users' behavior: (1) position bias: the same result will get a different number of clicks when displayed in different positions and (2) externalities: the same result might get more clicks when displayed with results of relatively lower quality than when shown with higher quality results. In this paper we focus on analyzing the sequence of user actions to model users' click behavior on sponsored listings shown on the search results page. We first show that temporal click sequences are good indicators of externalities in the advertising domain. We then describe the positional rationality hypothesis to explain both the position bias and the externalities, and based on this hypothesis we further propose the temporal click model (TCM), a Bayesian framework that is scalable and computationally efficient. To the best of our knowledge, this is the first attempt in the literature to estimate positional bias, externalities and unbiased user-perceived ad quality from user click logs in a combined model. We finally evaluate the proposed model on two real datasets, each containing over 100 million ad impressions obtained from a commercial search engine. The experimental results show that TCM outperforms two other competitive methods at click prediction.

#index 1450843
#* Freshness matters: in flowers, food, and web authority
#@ Na Dai;Brian D. Davison
#t 2010
#c 13
#% 268079
#% 280834
#% 282905
#% 309095
#% 754090
#% 766462
#% 769488
#% 807658
#% 810054
#% 867050
#% 869600
#% 915286
#% 918685
#% 956620
#% 1019135
#% 1019188
#% 1074107
#% 1121272
#% 1166533
#% 1190060
#% 1194312
#% 1227614
#% 1292715
#% 1355016
#! The collective contributions of billions of users across the globe each day result in an ever-changing web. In verticals like news and real-time search, recency is an obvious significant factor for ranking. However, traditional link-based web ranking algorithms typically run on a single web snapshot without concern for user activities associated with the dynamics of web pages and links. Therefore, a stale page popular many years ago may still achieve a high authority score due to its accumulated in-links. To remedy this situation, we propose a temporal web link-based ranking scheme, which incorporates features from historical author activities. We quantify web page freshness over time from page and in-link activity, and design a web surfer model that incorporates web freshness, based on a temporal web graph composed of multiple web snapshots at different time points. It includes authority propagation among snapshots, enabling link structures at distinct time points to influence each other when estimating web page authority. Experiments on a real-world archival web corpus show our approach improves upon PageRank in both relevance and freshness of the search results.

#index 1450844
#* The importance of anchor text for ad hoc search revisited
#@ Marijn Koolen;Jaap Kamps
#t 2010
#c 13
#% 309151
#% 330787
#% 340928
#% 397126
#% 453327
#% 577330
#% 642992
#% 643069
#% 729027
#% 735073
#% 768904
#% 807302
#% 838460
#% 907496
#% 987251
#% 1227604
#% 1227650
#! It is generally believed that propagated anchor text is very important for effective Web search as offered by the commercial search engines. "Google Bombs" are a notable illustration of this. However, many years of TREC Web retrieval research failed to establish the effectiveness of link evidence for ad hoc retrieval on Web collections. The ultimate resolution to this dilemma was that typical Web search is very different from the traditional ad hoc methodology. So far, however, no one has established why link information, like incoming link degree or anchor text, does not help ad hoc retrieval effectiveness. Several possible explanations were given, including the collections being too small for anchors to be effective, and the density of the link graph being too low. The new TREC 2009 Web Track collection is substantially larger than previous collections and has a dense link graph. Our main finding is that propagated anchor text outperforms full-text retrieval in terms of early precision, and in combination with it, gives an improvement in overall precision. We then analyse the impact of link density and collection size by down-sampling the number of links and the number of pages respectively. Other findings are that, contrary to expectations, (inter-server) link density has little impact on effectiveness, while the size of the collection has a substantial impact on the quantity, quality and effectiveness of anchor text. We also compare the diversity of the search results of anchor text and full-text approaches, which show that anchor text performs significantly better than full-text search and confirm our findings for the ad hoc search task.

#index 1450845
#* Ready to buy or just browsing?: detecting web searcher goals from interaction data
#@ Qi Guo;Eugene Agichtein
#t 2010
#c 13
#% 590523
#% 754059
#% 805200
#% 805878
#% 823348
#% 869483
#% 869550
#% 879565
#% 946521
#% 954948
#% 956495
#% 956546
#% 956622
#% 987222
#% 1048694
#% 1055694
#% 1074071
#% 1074093
#% 1074099
#% 1074148
#% 1083721
#% 1130909
#% 1166521
#% 1173691
#% 1185582
#% 1214754
#% 1227577
#% 1269878
#% 1275193
#% 1355037
#% 1384641
#! An improved understanding of the relationship between search intent, result quality, and searcher behavior is crucial for improving the effectiveness of web search. While recent progress in user behavior mining has been largely focused on aggregate server-side click logs, we present a new class of search behavior models that also exploit fine-grained user interactions with the search results. We show that mining these interactions, such as mouse movements and scrolling, can enable more effective detection of the user's search goals. Potential applications include automatic search evaluation, improving search ranking, result presentation, and search advertising. We describe extensive experimental evaluation over both controlled user studies, and logs of interaction data collected from hundreds of real users. The results show that our method is more effective than the current state-of-the-art techniques, both for detection of searcher goals, and for an important practical application of predicting ad clicks for a given search session.

#index 1450846
#* Learning to efficiently rank
#@ Lidan Wang;Jimmy Lin;Donald Metzler
#t 2010
#c 13
#% 169780
#% 262096
#% 340887
#% 766414
#% 818229
#% 818262
#% 838532
#% 840846
#% 879611
#% 879651
#% 976952
#% 987215
#% 987216
#% 987229
#% 1055856
#% 1227636
#% 1227747
#% 1268491
#% 1355019
#! It has been shown that learning to rank approaches are capable of learning highly effective ranking functions. However, these approaches have mostly ignored the important issue of efficiency. Given that both efficiency and effectiveness are important for real search engines, models that are optimized for effectiveness may not meet the strict efficiency requirements necessary to deploy in a production environment. In this work, we present a unified framework for jointly optimizing effectiveness and efficiency. We propose new metrics that capture the tradeoff between these two competing forces and devise a strategy for automatically learning models that directly optimize the tradeoff metrics. Experiments indicate that models learned in this way provide a good balance between retrieval effectiveness and efficiency. With specific loss functions, learned models converge to familiar existing ones, which demonstrates the generality of our framework. Finally, we show that our approach naturally leads to a reduction in the variance of query execution times, which is important for query load balancing and user satisfaction.

#index 1450847
#* Ranking for the conversion funnel
#@ Abraham Bagherjeiran;Andrew O. Hatch;Adwait Ratnaparkhi
#t 2010
#c 13
#% 818265
#% 879633
#% 987262
#% 1040857
#% 1055713
#% 1055899
#% 1292471
#! In contextual advertising advertisers show ads to users so that they will click on them and eventually purchase a product. Optimizing this action sequence, called the conversion funnel, is the ultimate goal of advertising. Advertisers, however, often have very different sub-goals for their ads such as purchase, request for a quote, or simply a site visit. Often an improvement for one advertiser's goal comes at the expense of others. A single ranking function must balance these different goals in order to make an efficient system for all advertisers. We propose a ranking method that globally balances the goals of all advertisers, while simultaneously improving overall performance. Our method has been shown to improve significantly over the baseline in online traffic at a major ad network.

#index 1450848
#* How good is a span of terms?: exploiting proximity to improve web retrieval
#@ Krysta M. Svore;Pallika H. Kanani;Nazan Khan
#t 2010
#c 13
#% 169781
#% 309095
#% 324129
#% 783474
#% 810627
#% 818262
#% 879651
#% 907546
#% 987229
#% 987272
#% 1227614
#% 1227634
#% 1292709
#% 1387547
#% 1404894
#% 1415737
#% 1715627
#! Ranking search results is a fundamental problem in information retrieval. In this paper we explore whether the use of proximity and phrase information can improve web retrieval accuracy. We build on existing research by incorporating novel ranking features based on flexible proximity terms with recent state-of-the-art machine learning ranking models. We introduce a method of determining the goodness of a set of proximity terms that takes advantage of the structured nature of web documents, document metadata, and phrasal information from search engine user query logs. We perform experiments on a large real-world Web data collection and show that using the goodness score of flexible proximity terms can improve ranking accuracy over state-of-the-art ranking methods by as much as 13%. We also show that we can improve accuracy on the hardest queries by as much as 9% relative to state-of-the-art approaches.

#index 1450849
#* Learning to rank only using training data from related domain
#@ Wei Gao;Peng Cai;Kam-Fai Wong;Aoying Zhou
#t 2010
#c 13
#% 46803
#% 190581
#% 262096
#% 309095
#% 340948
#% 734915
#% 770847
#% 840846
#% 983820
#% 983905
#% 987226
#% 1074063
#% 1074082
#% 1130923
#% 1159262
#% 1193635
#% 1261539
#% 1268491
#% 1272110
#% 1292485
#% 1292566
#% 1292657
#% 1305479
#% 1338581
#% 1377382
#% 1442578
#% 1456843
#! Like traditional supervised and semi-supervised algorithms, learning to rank for information retrieval requires document annotations provided by domain experts. It is costly to annotate training data for different search domains and tasks. We propose to exploit training data annotated for a related domain to learn to rank retrieved documents in the target domain, in which no labeled data is available. We present a simple yet effective approach based on instance-weighting scheme. Our method first estimates the importance of each related-domain document relative to the target domain. Then heuristics are studied to transform the importance of individual documents to the pairwise weights of document pairs, which can be directly incorporated into the popular ranking algorithms. Due to importance weighting, ranking model trained on related domain is highly adaptable to the data of target domain. Ranking adaptation experiments on LETOR3.0 dataset [27] demonstrate that with a fair amount of related-domain training data, our method significantly outperforms the baseline without weighting, and most of time is not significantly worse than an "ideal" model directly trained on target domain.

#index 1450850
#* Optimal meta search results clustering
#@ Claudio Carpineto;Giovanni Romano
#t 2010
#c 13
#% 375017
#% 722902
#% 813043
#% 837616
#% 945521
#% 1077150
#% 1202162
#% 1203767
#% 1280751
#! By analogy with merging documents rankings, the outputs from multiple search results clustering algorithms can be combined into a single output. In this paper we study the feasibility of meta search results clustering, which has unique features compared to the general meta clustering problem. After showing that the combination of multiple search results clusterings is empirically justified, we cast meta clustering as an optimization problem of an objective function measuring the probabilistic concordance between the clustering combination and the single clusterings. We then show, using an easily computable upper bound on such a function, that a simple stochastic optimization algorithm delivers reasonable approximations of the optimal value very efficiently, and we also provide a method for labeling the generated clusters with the most agreed upon cluster labels. Optimal meta clustering with meta labeling is applied to three description-centric, state-of-the-art search results clustering algorithms. The performance improvement is demonstrated through a range of evaluation techniques (i.e., internal, classification-oriented, and information retrieval-oriented), using suitable test collections of search results with document-level relevance judgments per subtopic.

#index 1450851
#* Analysis of structural relationships for hierarchical cluster labeling
#@ Markus Muhr;Roman Kern;Michael Granitzer
#t 2010
#c 13
#% 118771
#% 413609
#% 807363
#% 813043
#% 878454
#% 879613
#% 961706
#% 1077150
#% 1190075
#% 1227594
#% 1243237
#! Cluster label quality is crucial for browsing topic hierarchies obtained via document clustering. Intuitively, the hierarchical structure should influence the labeling accuracy. However, most labeling algorithms ignore such structural properties and therefore, the impact of hierarchical structures on the labeling accuracy is yet unclear. In our work we integrate hierarchical information, i.e. sibling and parent-child relations, in the cluster labeling process. We adapt standard labeling approaches, namely Maximum Term Frequency, Jensen-Shannon Divergence, Chi Square Test, and Information Gain, to take use of those relationships and evaluate their impact on 4 different datasets, namely the Open Directory Project, Wikipedia, TREC Ohsumed and the CLEF IP European Patent dataset. We show, that hierarchical relationships can be exploited to increase labeling accuracy especially on high-level nodes.

#index 1450852
#* On the existence of obstinate results in vector space models
#@ Milos Radovanović;Alexandros Nanopoulos;Mirjana Ivanović
#t 2010
#c 13
#% 240038
#% 321635
#% 375017
#% 420534
#% 982755
#% 992326
#% 1130833
#% 1211798
#% 1287267
#! The vector space model (VSM) is a popular and widely applied model in information retrieval (IR). VSM creates vector spaces whose dimensionality is usually high (e.g., tens of thousands of terms). This may cause various problems, such as susceptibility to noise and difficulty in capturing the underlying semantic structure, which are commonly recognized as different aspects of the "curse of dimensionality." In this paper, we investigate a novel aspect of the dimensionality curse, which is referred to as hubness and manifested by the tendency of some documents (called hubs) to be included in unexpectedly many search result lists. Hubness may impact VSM considerably since hubs can become obstinate results, irrelevant to a large number of queries, thus harming the performance of an IR system and the experience of its users. We analyze the origins of hubness, showing it is primarily a consequence of high (intrinsic) dimensionality of data, and not a result of other factors such as sparsity and skewness of the distribution of term frequencies. We describe the mechanisms through which hubness emerges by exploring the behavior of similarity measures in high-dimensional vector spaces. Our consideration begins with the classical VSM (tf-idf term weighting and cosine similarity), but the conclusions generalize to more advanced variations, such as Okapi BM25. Moreover, we explain why hubness may not be easily mitigated by dimensionality reduction, and propose a similarity adjustment scheme that takes into account the existence of hubs. Experimental results over real data indicate that significant improvement can be obtained through consideration of hubness.

#index 1450853
#* Social media recommendation based on people and tags
#@ Ido Guy;Naama Zwerdling;Inbal Ronen;David Carmel;Erel Uziel
#t 2010
#c 13
#% 124010
#% 220708
#% 220709
#% 320432
#% 345371
#% 397155
#% 414514
#% 578684
#% 911038
#% 1016092
#% 1017565
#% 1047412
#% 1055739
#% 1130978
#% 1132926
#% 1169569
#% 1169572
#% 1190122
#% 1215458
#% 1227601
#% 1227762
#% 1287226
#% 1287230
#% 1292590
#% 1396094
#% 1668087
#! We study personalized item recommendation within an enterprise social media application suite that includes blogs, bookmarks, communities, wikis, and shared files. Recommendations are based on two of the core elements of social media - people and tags. Relationship information among people, tags, and items, is collected and aggregated across different sources within the enterprise. Based on these aggregated relationships, the system recommends items related to people and tags that are related to the user. Each recommended item is accompanied by an explanation that includes the people and tags that led to its recommendation, as well as their relationships with the user and the item. We evaluated our recommender system through an extensive user study. Results show a significantly better interest ratio for the tag-based recommender than for the people-based recommender, and an even better performance for a combined recommender. Tags applied on the user by other people are found to be highly effective in representing that user's topics of interest.

#index 1450854
#* A network-based model for high-dimensional information filtering
#@ Nikolaos Nanas;Manolis Vavalis;Anne De Roeck
#t 2010
#c 13
#% 29587
#% 80399
#% 217255
#% 262085
#% 280849
#% 290149
#% 406493
#% 424011
#% 465031
#% 465754
#% 766450
#% 987196
#% 1201880
#! The Vector Space Model has been and to a great extent still is the de facto choice for profile representation in content-based Information Filtering. However, user profiles represented as weighted keyword vectors have inherent dimensionality problems. As the number of profile keywords increases, the vector representation becomes ambiguous, due to the exponential increase in the volume of the vector space and in the number of possible keyword combinations. We argue that the complexity and dynamics of Information Filtering require user profile representations which are resilient and resistant to this "curse of dimensionality". A user profile has to be able to incorporate many features and to adapt to a variety of interest changes. We propose an alternative, network-based profile representation that meets these challenging requirements. Experiments show that the network profile representation can more effectively capture additional information about a user's interests and thus achieve significant performance improvements over a vector-based representation comprising the same weighted keywords.

#index 1450855
#* Temporal diversity in recommender systems
#@ Neal Lathia;Stephen Hailes;Licia Capra;Xavier Amatriain
#t 2010
#c 13
#% 414514
#% 566642
#% 734590
#% 813966
#% 879686
#% 1127465
#% 1166473
#% 1214666
#% 1227740
#! Collaborative Filtering (CF) algorithms, used to build web-based recommender systems, are often evaluated in terms of how accurately they predict user ratings. However, current evaluation techniques disregard the fact that users continue to rate items over time: the temporal characteristics of the system's top-N recommendations are not investigated. In particular, there is no means of measuring the extent that the same items are being recommended to users over and over again. In this work, we show that temporal diversity is an important facet of recommender systems, by showing how CF data changes over time and performing a user survey. We then evaluate three CF algorithms from the point of view of the diversity in the sequence of recommendation lists they produce over time. We examine how a number of characteristics of user rating patterns (including profile size and time between rating) affect diversity. We then propose and evaluate set methods that maximise temporal recommendation diversity without extensively penalising accuracy.

#index 1450856
#* Serendipitous recommendations via innovators
#@ Noriaki Kawamae
#t 2010
#c 13
#% 173879
#% 220711
#% 330687
#% 397133
#% 452563
#% 643007
#% 734590
#% 805841
#% 879628
#% 975021
#% 993934
#% 1127472
#% 1200360
#% 1287276
#! To realize services that provide serendipity, this paper assesses the surprise of each user when presented recommendations. We propose a recommendation algorithm that focuses on the search time that, in the absence of any recommendation, each user would need to find a desirable and novel item by himself. Following the hypothesis that the degree of user's surprise is proportional to the estimated search time, we consider both innovators' preferences and trends for identifying items with long estimated search times. To predict which items the target user is likely to purchase in the near future, the candidate items, this algorithm weights each item that innovators have purchased and that reflect one or more current trends; it then lists them in order of decreasing weight. Experiments demonstrate that this algorithm outputs recommendations that offer high user/item coverage, a low Gini coefficient, and long estimated search times, and so offers a high degree of recommendation serendipitousness.

#index 1450857
#* On statistical analysis and optimization of information retrieval effectiveness metrics
#@ Jun Wang;Jianhan Zhu
#t 2010
#c 13
#% 94370
#% 169781
#% 218982
#% 235918
#% 262105
#% 262112
#% 288166
#% 309093
#% 340899
#% 375017
#% 411760
#% 411762
#% 740915
#% 746909
#% 818080
#% 818205
#% 840846
#% 872020
#% 879618
#% 879631
#% 879632
#% 879693
#% 891559
#% 987226
#% 1035577
#% 1074126
#% 1077150
#% 1166534
#% 1211826
#% 1227591
#% 1442574
#! This paper presents a new way of thinking for IR metric optimization. It is argued that the optimal ranking problem should be factorized into two distinct yet interrelated stages: the relevance prediction stage and ranking decision stage. During retrieval the relevance of documents is not known a priori, and the joint probability of relevance is used to measure the uncertainty of documents' relevance in the collection as a whole. The resulting optimization objective function in the latter stage is, thus, the expected value of the IR metric with respect to this probability measure of relevance. Through statistically analyzing the expected values of IR metrics under such uncertainty, we discover and explain some interesting properties of IR metrics that have not been known before. Our analysis and optimization framework do not assume a particular (relevance) retrieval model and metric, making it applicable to many existing IR models and metrics. The experiments on one of resulting applications have demonstrated its significance in adapting to various IR metrics.

#index 1450858
#* Information-based models for ad hoc IR
#@ Stéphane Clinchant;Eric Gaussier
#t 2010
#c 13
#% 169781
#% 218982
#% 342707
#% 406493
#% 411760
#% 750863
#% 766412
#% 840903
#% 867050
#% 875981
#% 1074104
#% 1415705
#% 1415721
#% 1415740
#! We introduce in this paper the family of information-based models for ad hoc information retrieval. These models draw their inspiration from a long-standing hypothesis in IR, namely the fact that the difference in the behaviors of a word at the document and collection levels brings information on the significance of the word for the document. This hypothesis has been exploited in the 2-Poisson mixture models, in the notion of eliteness in BM25, and more recently in DFR models. We show here that, combined with notions related to burstiness, it can lead to simpler and better models.

#index 1450859
#* Score distribution models: assumptions, intuition, and robustness to score manipulation
#@ Evangelos Kanoulas;Keshi Dai;Virgil Pavlu;Javed A. Aslam
#t 2010
#c 13
#% 280854
#% 340934
#% 340938
#% 340941
#% 642988
#% 838527
#% 1227642
#% 1392434
#! Inferring the score distribution of relevant and non-relevant documents is an essential task for many IR applications (e.g. information filtering, recall-oriented IR, meta-search, distributed IR). Modeling score distributions in an accurate manner is the basis of any inference. Thus, numerous score distribution models have been proposed in the literature. Most of the models were proposed on the basis of empirical evidence and goodness-of-fit. In this work, we model score distributions in a rather different, systematic manner. We start with a basic assumption on the distribution of terms in a document. Following the transformations applied on term frequencies by two basic ranking functions, BM25 and Language Models, we derive the distribution of the produced scores for all documents. Then we focus on the relevant documents. We detach our analysis from particular ranking functions. Instead, we consider a model for precision-recall curves, and given this model, we present a general mathematical framework which, given any score distribution for all retrieved documents, produces an analytical formula for the score distribution of relevant documents that is consistent with the precision-recall curves that follow the aforementioned model. In particular, assuming a Gamma distribution for all retrieved documents, we show that the derived distribution for the relevant documents resembles a Gaussian distribution with a heavy right tail.

#index 1450860
#* Geometric representations for multiple documents
#@ Jangwon Seo;W. Bruce Croft
#t 2010
#c 13
#% 144076
#% 169809
#% 232703
#% 340901
#% 340948
#% 342660
#% 413592
#% 766431
#% 783473
#% 794860
#% 827955
#% 987230
#% 1058018
#% 1130914
#% 1227699
#% 1292733
#% 1312018
#% 1415722
#% 1415748
#! Combining multiple documents to represent an information object is well-known as an effective approach for many Information Retrieval tasks. For example, passages can be combined to represent a document for retrieval, document clusters are represented using combinations of the documents they contain, and feedback documents can be combined to represent a query model. Various techniques for combination have been introduced, and among them, representation techniques based on concatenation and the arithmetic mean are frequently used. Some recent work has shown the potential of a new representation technique using the geometric mean. However, these studies lack a theoretical foundation explaining why the geometric mean should have advantages for representing multiple documents. In this paper, we show that the arithmetic mean and the geometric mean are approximations to the center of mass in certain geometries, and show empirically that the geometric mean is closer to the center. Through experiments with two IR tasks, we show the potential benefits for geometric representations, including a geometry-based pseudo-relevance feedback method that outperforms state-of-the-art techniques.

#index 1450861
#* Using statistical decision theory and relevance models for query-performance prediction
#@ Anna Shtok;Oren Kurland;David Carmel
#t 2010
#c 13
#% 262084
#% 280864
#% 340899
#% 340901
#% 340948
#% 397161
#% 766497
#% 804915
#% 818267
#% 879613
#% 879614
#% 907544
#% 987230
#% 987260
#% 987265
#% 987299
#% 1074072
#% 1074080
#% 1130851
#% 1130990
#% 1217048
#% 1392447
#% 1415713
#! We present a novel framework for the query-performance prediction task. That is, estimating the effectiveness of a search performed in response to a query in lack of relevance judgments. Our approach is based on using statistical decision theory for estimating the utility that a document ranking provides with respect to an information need expressed by the query. To address the uncertainty in inferring the information need, we estimate utility by the expected similarity between the given ranking and those induced by relevance models; the impact of a relevance model is based on its presumed representativeness of the information need. Specific query-performance predictors instantiated from the framework substantially outperform state-of-the-art predictors over five TREC corpora.

#index 1450862
#* Active learning for ranking through expected loss optimization
#@ Bo Long;Olivier Chapelle;Ya Zhang;Yi Chang;Zhaohui Zheng;Belle Tseng
#t 2010
#c 13
#% 169717
#% 236729
#% 466095
#% 466576
#% 565531
#% 734915
#% 823360
#% 879598
#% 1073903
#% 1074021
#% 1227635
#% 1227673
#% 1227719
#% 1674802
#! Learning to rank arises in many information retrieval applications, ranging from Web search engine, online advertising to recommendation system. In learning to rank, the performance of a ranking model is strongly affected by the number of labeled examples in the training set; on the other hand, obtaining labeled examples for training data is very expensive and time-consuming. This presents a great need for the active learning approaches to select most informative examples for ranking learning; however, in the literature there is still very limited work to address active learning for ranking. In this paper, we propose a general active learning framework, Expected Loss Optimization (ELO), for ranking. The ELO framework is applicable to a wide range of ranking functions. Under this framework, we derive a novel algorithm, Expected DCG Loss Optimization (ELO-DCG), to select most informative examples. Furthermore, we investigate both query and document level active learning for raking and propose a two-stage ELO-DCG algorithm which incorporate both query and document selection into active learning. Extensive experiments on real-world Web search data sets have demonstrated great potential and effective-ness of the proposed framework and algorithms.

#index 1450863
#* Image search by concept map
#@ Hao Xu;Jingdong Wang;Xian-Sheng Hua;Shipeng Li
#t 2010
#c 13
#% 219847
#% 309095
#% 619830
#% 755467
#% 780805
#% 860956
#% 1021278
#% 1038781
#% 1047298
#% 1071133
#% 1131992
#% 1132499
#% 1268613
#% 1279769
#% 1279779
#% 1400151
#% 1537126
#! In this paper, we present a novel image search system, image search by concept map. This system enables users to indicate not only what semantic concepts are expected to appear but also how these concepts are spatially distributed in the desired images. To this end, we propose a new image search interface to enable users to formulate a query, called concept map, by intuitively typing textual queries in a blank canvas to indicate the desired spatial positions of the concepts. In the ranking process, by interpreting each textual concept as a set of representative visual instances, the concept map query is translated into a visual instance map, which is then used to evaluate the relevance of the image in the database. Experimental results demonstrate the effectiveness of the proposed system.

#index 1450864
#* Generalized syntactic and semantic models of query reformulation
#@ Amac Herdagdelen;Massimiliano Ciaramita;Daniel Mahler;Maria Holmqvist;Keith Hall;Stefan Riezler;Enrique Alfonseca
#t 2010
#c 13
#% 194298
#% 218978
#% 232713
#% 262084
#% 284796
#% 330617
#% 342961
#% 449294
#% 458630
#% 866266
#% 869500
#% 869501
#% 869651
#% 947827
#% 995464
#% 1130878
#% 1251678
#% 1270732
#% 1279761
#% 1280748
#% 1292473
#% 1470633
#! We present a novel approach to query reformulation which combines syntactic and semantic information by means of generalized Levenshtein distance algorithms where the substitution operation costs are based on probabilistic term rewrite functions. We investigate unsupervised, compact and efficient models, and provide empirical evidence of their effectiveness. We further explore a generative model of query reformulation and supervised combination methods providing improved performance at variable computational costs. Among other desirable properties, our similarity measures incorporate information-theoretic interpretations of taxonomic relations such as specification and generalization.

#index 1450865
#* Evaluating verbose query processing techniques
#@ Samuel Huston;W. Bruce Croft
#t 2010
#c 13
#% 144077
#% 184489
#% 218978
#% 464434
#% 853701
#% 855293
#% 1055706
#% 1074098
#% 1074112
#% 1173692
#% 1181094
#% 1227610
#% 1227647
#! Verbose or long queries are a small but significant part of the query stream in web search, and are common in other applications such as collaborative question answering (CQA). Current search engines perform well with keyword queries but are not, in general, effective for verbose queries. In this paper, we examine query processing techniques which can be applied to verbose queries prior to submission to a search engine in order to improve the search engine's results. We focus on verbose queries that have sentence-like structure, but are not simple "wh-" questions, and assume the search engine is a "black box." We evaluated the output of two search engines using queries from a CQA service and our results show that, among a broad range of techniques, the most effective approach is to simply reduce the length of the query. This can be achieved effectively by removing "stop structure" instead of only stop words. We show that the process of learning and removing stop structure from a query can be effectively automated.

#index 1450866
#* SED: supervised experimental design and its application to text classification
#@ Yi Zhen;Dit-Yan Yeung
#t 2010
#c 13
#% 116165
#% 132697
#% 169717
#% 170649
#% 280817
#% 420507
#% 450951
#% 451056
#% 464268
#% 565531
#% 642998
#% 722797
#% 734915
#% 763708
#% 770771
#% 869526
#% 875997
#% 876080
#% 987207
#% 1074130
#% 1272282
#% 1274885
#% 1279772
#! In recent years, active learning methods based on experimental design achieve state-of-the-art performance in text classification applications. Although these methods can exploit the distribution of unlabeled data and support batch selection, they cannot make use of labeled data which often carry useful information for active learning. In this paper, we propose a novel active learning method for text classification, called supervised experimental design (SED), which seamlessly incorporates label information into experimental design. Experimental results show that SED outperforms its counterparts which either discard the label information even when it is available or fail to exploit the distribution of unlabeled data.

#index 1450867
#* Temporally-aware algorithms for document classification
#@ Thiago Salles;Leonardo Rocha;Gisele L. Pappa;Fernando Mourão;Wagner Meira, Jr.;Marcos Gonçalves
#t 2010
#c 13
#% 151318
#% 204531
#% 269217
#% 275837
#% 433674
#% 445382
#% 466408
#% 577283
#% 745649
#% 881477
#% 998561
#% 998570
#% 1032046
#% 1035585
#% 1055665
#% 1077150
#% 1130831
#! Automatic Document Classification (ADC) is still one of the major information retrieval problems. It usually employs a supervised learning strategy, where we first build a classification model using pre-classified documents and then use this model to classify unseen documents. The majority of supervised algorithms consider that all documents provide equally important information. However, in practice, a document may be considered more or less important to build the classification model according to several factors, such as its timeliness, the venue where it was published in, its authors, among others. In this paper, we are particularly concerned with the impact that temporal effects may have on ADC and how to minimize such impact. In order to deal with these effects, we introduce a temporal weighting function (TWF) and propose a methodology to determine it for document collections. We applied the proposed methodology to ACM-DL and Medline and found that the TWF of both follows a lognormal. We then extend three ADC algorithms (namely kNN, Rocchio and Naïve Bayes) to incorporate the TWF. Experiments showed that the temporally-aware classifiers achieved significant gains, outperforming (or at least matching) state-of-the-art algorithms.

#index 1450868
#* Multilabel classification with meta-level features
#@ Siddharth Gopal;Yiming Yang
#t 2010
#c 13
#% 120634
#% 169718
#% 190581
#% 219052
#% 269217
#% 302391
#% 311034
#% 318412
#% 340904
#% 465754
#% 840882
#% 950571
#% 961134
#% 987226
#% 987241
#% 1264044
#% 1301004
#! Effective learning in multi-label classification (MLC) requires an appropriate level of abstraction for representing the relationship between each instance and multiple categories. Current MLC methods have been focused on learning-to-map from instances to ranked lists of categories in a relatively high-dimensional space. The fine-grained features in such a space may not be sufficiently expressive for characterizing discriminative patterns, and worse, make the model complexity unnecessarily high. This paper proposes an alternative approach by transforming conventional representations of instances and categories into a relatively small set of link-based meta-level features, and leveraging successful learning-to-rank retrieval algorithms (e.g., SVM-MAP) over this reduced feature space. Controlled experiments on multiple benchmark datasets show strong empirical evidence for the strength of the proposed approach, as it significantly outperformed several state-of-the-art methods, including Rank-SVM, ML-kNN and IBLR-ML (Instance-based Logistic Regression for Multi-label Classification) in most cases.

#index 1450869
#* Estimation of statistical translation models based on mutual information for ad hoc information retrieval
#@ Maryam Karimzadehgan;ChengXiang Zhai
#t 2010
#c 13
#% 67565
#% 111303
#% 120104
#% 144029
#% 169729
#% 218978
#% 229348
#% 252472
#% 262096
#% 280826
#% 280851
#% 340897
#% 340899
#% 340901
#% 340948
#% 342707
#% 375017
#% 397128
#% 397145
#% 406493
#% 740915
#% 766430
#% 766431
#% 766440
#% 818240
#% 838508
#% 838530
#% 879587
#% 940042
#% 1074110
#! As a principled approach to capturing semantic relations of words in information retrieval, statistical translation models have been shown to outperform simple document language models which rely on exact matching of words in the query and documents. A main challenge in applying translation models to ad hoc information retrieval is to estimate a translation model without training data. Existing work has relied on training on synthetic queries generated based on a document collection. However, this method is computationally expensive and does not have a good coverage of query words. In this paper, we propose an alternative way to estimate a translation model based on normalized mutual information between words, which is less computationally expensive and has better coverage of query words than the synthetic query method of estimation. We also propose to regularize estimated translation probabilities to ensure sufficient probability mass for self-translation. Experiment results show that the proposed mutual information-based estimation method is not only more efficient, but also more effective than the synthetic query-based method, and it can be combined with pseudo-relevance feedback to further improve retrieval accuracy. The results also show that the proposed regularization strategy is effective and can improve retrieval accuracy for both synthetic query-based estimation and mutual information-based estimation.

#index 1450870
#* DivQ: diversification for keyword search over structured databases
#@ Elena Demidova;Peter Fankhauser;Xuan Zhou;Wolfgang Nejdl
#t 2010
#c 13
#% 262112
#% 411762
#% 857482
#% 875061
#% 879618
#% 893143
#% 960287
#% 1015325
#% 1063536
#% 1074133
#% 1077150
#% 1166473
#% 1190089
#% 1190093
#% 1206662
#% 1227591
#% 1227709
#% 1328120
#% 1409940
#% 1409952
#! Keyword queries over structured databases are notoriously ambiguous. No single interpretation of a keyword query can satisfy all users, and multiple interpretations may yield overlapping results. This paper proposes a scheme to balance the relevance and novelty of keyword search results over structured databases. Firstly, we present a probabilistic model which effectively ranks the possible interpretations of a keyword query over structured data. Then, we introduce a scheme to diversify the search results by re-ranking query interpretations, taking into account redundancy of query results. Finally, we propose α-nDCG-W and WS-recall, an adaptation of α-nDCG and S-recall metrics, taking into account graded relevance of subtopics. Our evaluation on two real-world datasets demonstrates that search results obtained using the proposed diversification algorithms better characterize possible answers available in the database than the results of the initial relevance ranking.

#index 1450871
#* Finding support sentences for entities
#@ Roi Blanco;Hugo Zaragoza
#t 2010
#c 13
#% 169781
#% 815107
#% 857180
#% 869535
#% 907510
#% 939939
#% 1019189
#% 1039849
#% 1074223
#% 1100822
#% 1166525
#% 1297088
#% 1343447
#% 1404888
#% 1415743
#! We study the problem of finding sentences that explain the relationship between a named entity and an ad-hoc query, which we refer to as entity support sentences. This is an important sub-problem of entity ranking which, to the best of our knowledge, has not been addressed before. In this paper we give the first formalization of the problem, how it can be evaluated, and present a full evaluation dataset. We propose several methods to rank these sentences, namely retrieval-based, entity-ranking based and position-based. We found that traditional bag-of-words models perform relatively well when there is a match between an entity and a query in a given sentence, but they fail to find a support sentence for a substantial portion of entities. This can be improved by incorporating small windows of context sentences and ranking them appropriately.

#index 1450872
#* Estimating probabilities for effective data fusion
#@ David Lillis;Lusheng Zhang;Fergus Toolan;Rem W. Collier;David Leonard;John Dunnion
#t 2010
#c 13
#% 194246
#% 232703
#% 309133
#% 340934
#% 340936
#% 342710
#% 413613
#% 413634
#% 420464
#% 784148
#% 879582
#% 1392446
#% 1415738
#! Data Fusion is the combination of a number of independent search results, relating to the same document collection, into a single result to be presented to the user. A number of probabilistic data fusion models have been shown to be effective in empirical studies. These typically attempt to estimate the probability that particular documents will be relevant, based on training data. However, little attempt has been made to gauge how the accuracy of these estimations affect fusion performance. The focus of this paper is twofold: firstly, that accurate estimation of the probability of relevance results in effective data fusion; and secondly, that an effective approximation of this probability can be made based on less training data that has previously been employed. This is based on the observation that the distribution of relevant documents follows a similar pattern in most high-quality result sets. Curve fitting suggests that this can be modelled by a simple function that is less complex than other models that have been proposed. The use of existing IR evaluation metrics is proposed as a substitution for probability calculations. Mean Average Precision is used to demonstrate the effectiveness of this approach, with evaluation results demonstrating competitive performance when compared with related algorithms with more onerous requirements for training data.

#index 1450873
#* Incorporating post-click behaviors into a click model
#@ Feimin Zhong;Dong Wang;Gang Wang;Weizhu Chen;Yuchen Zhang;Zheng Chen;Haixun Wang
#t 2010
#c 13
#% 411762
#% 577224
#% 715096
#% 734915
#% 818221
#% 823348
#% 840846
#% 879565
#% 879567
#% 956546
#% 1035578
#% 1074092
#% 1166522
#% 1190055
#% 1190056
#% 1214728
#% 1214754
#% 1810385
#! Much work has attempted to model a user's click-through behavior by mining the click logs. The task is not trivial due to the well-known position bias problem. Some break-throughs have been made: two newly proposed click models, DBN and CCM, addressed this problem and improved document relevance estimation. However, to further improve the estimation, we need a model that can capture more sophisticated user behaviors. In particular, after clicking a search result, a user's behavior (such as the dwell time on the clicked document, and whether there are further clicks on the clicked document) can be highly indicative of the relevance of the document. Unfortunately, such measures have not been incorporated in previous click models. In this paper, we introduce a novel click model, called the post-click click model (PCC), which provides an unbiased estimation of document relevance through leveraging both click behaviors on the search page and post-click behaviors beyond the search page. The PCC model is based on the Bayesian approach, and because of its incremental nature, it is highly scalable to large scale and constantly growing log data. Extensive experimental results illustrate that the proposed method significantly outperforms the state of the art methods merely relying on click logs.

#index 1450874
#* Interactive retrieval based on faceted feedback
#@ Lanbo Zhang;Yi Zhang
#t 2010
#c 13
#% 54435
#% 280851
#% 342681
#% 642985
#% 657203
#% 766450
#% 766525
#% 818209
#% 818260
#% 879621
#% 961194
#% 987225
#% 1338741
#! Motivated by the commonly used faceted search interface in e-commerce, this paper investigates interactive relevance feedback mechanism based on faceted document metadata. In this mechanism, the system recommends a group of document facet-value pairs, and lets users select relevant ones to restrict the returned documents. We propose four facet-value pair recommendation approaches and two retrieval models that incorporate user feedback on document facets. Evaluated based on user feedback collected through Amazon Mechanical Turk, our experimental results show that the Boolean filtering approach, which is widely used in faceted search in e-commerce, doesn't work well for text document retrieval, due to the incompleteness (low recall) of metadata assignment in semi-structured text documents. Instead, a soft model performs more effectively. The faceted feedback mechanism can also be combined with document-based relevance feedback and pseudo relevance feedback to further improve the retrieval performance.

#index 1450875
#* A comparison of general vs personalised affective models for the prediction of topical relevance
#@ Ioannis Arapakis;Konstantinos Athanasakos;Joemon M. Jose
#t 2010
#c 13
#% 169803
#% 214709
#% 413615
#% 818220
#% 818221
#% 818259
#% 839970
#% 848656
#% 879565
#% 879567
#% 994400
#% 1031049
#% 1074100
#% 1093778
#% 1279814
#% 1292861
#! Information retrieval systems face a number of challenges, originating mainly from the semantic gap problem. Implicit feedback techniques have been employed in the past to address many of these issues. Although this was a step towards the right direction, a need to personalise and tailor the search experience to the user-specific needs has become evident. In this study we examine ways of personalising affective models trained on facial expression data. Using personalised data we adapt these models to individual users and compare their performance to a general model. The main goal is to determine whether the behavioural differences of users have an impact on the models' ability to determine topical relevance and if, by personalising them, we can improve their accuracy. For modelling relevance we extract a set of features from the facial expression data and classify them using Support Vector Machines. Our initial evaluation indicates that accounting for individual differences and applying personalisation introduces, in most cases, a noticeable improvement in the models' performance.

#index 1450876
#* Understanding web browsing behaviors through Weibull analysis of dwell time
#@ Chao Liu;Ryen W. White;Susan Dumais
#t 2010
#c 13
#% 169803
#% 320432
#% 340974
#% 378486
#% 437011
#% 731615
#% 751830
#% 766454
#% 805200
#% 818221
#% 879565
#% 879567
#% 946521
#% 956495
#% 1074107
#% 1130852
#% 1214728
#% 1227585
#% 1292474
#! Dwell time on Web pages has been extensively used for various information retrieval tasks. However, some basic yet important questions have not been sufficiently addressed, eg, what distribution is appropriate to model the distribution of dwell times on a Web page, and furthermore, what the distribution tells us about the underlying browsing behaviors. In this paper, we draw an analogy between abandoning a page during Web browsing and a system failure in reliability analysis, and propose to model the dwell time using the Weibull distribution. Using this distribution provides better goodness-of-fit to real world data, and it uncovers some interesting patterns of user browsing behaviors not previously reported. For example, our analysis reveals that Web browsing in general exhibits a significant "negative aging" phenomenon, which means that some initial screening has to be passed before a page is examined in detail, giving rise to the browsing behavior that we call "screen-and-glean." In addition, we demonstrate that dwell time distributions can be reasonably predicted purely based on low-level page features, which broadens the possible applications of this study to situations where log data may be unavailable.

#index 1450877
#* Segmentation of multi-sentence questions: towards effective question retrieval in cQA services
#@ Kai Wang;Zhao-Yan Ming;Xia Hu;Tat-Seng Chua
#t 2010
#c 13
#% 464996
#% 742204
#% 748583
#% 838397
#% 838398
#% 879595
#% 1035587
#% 1074109
#% 1074110
#% 1227600
#% 1269817
#! Existing question retrieval models work relatively well in finding similar questions in community-based question answering (cQA) services. However, they are designed for single-sentence queries or bag-of-word representations, and are not sufficient to handle multi-sentence questions complemented with various contexts. Segmenting questions into parts that are topically related could assist the retrieval system to not only better understand the user's different information needs but also fetch the most appropriate fragments of questions and answers in cQA archive that are relevant to user's query. In this paper, we propose a graph based approach to segmenting multi-sentence questions. The results from user studies show that our segmentation model outperforms traditional systems in question segmentation by over 30% in user's satisfaction. We incorporate the segmentation model into existing cQA question retrieval framework for more targeted question matching, and the empirical evaluation results demonstrate that the segmentation boosts the question retrieval performance by up to 12.93% in Mean Average Precision and 11.72% in Top One Precision. Our model comes with a comprehensive question detector equipped with both lexical and syntactic features.

#index 1450878
#* Mining the blogosphere for top news stories identification
#@ Yeha Lee;Hun-young Jung;Woosang Song;Jong-Hyeok Lee
#t 2010
#c 13
#% 262042
#% 262043
#% 262112
#% 280819
#% 340899
#% 397133
#% 577220
#% 577297
#% 643016
#% 722904
#% 750863
#% 766444
#% 766460
#% 960414
#% 982760
#% 987218
#% 987219
#% 1074133
#% 1130912
#% 1166473
#% 1227614
#% 1292749
#% 1742093
#! The analysis of query logs from blog search engines show that news-related queries occupy a significant portion of the logs. This raises a interesting research question on whether the blogosphere can be used to identify important news stories. In this paper, we present novel approaches to identify important news story headlines from the blogosphere for a given day. The proposed system consists of two components based on the language model framework, the query likelihood and the news headline prior. For the query likelihood, we propose several approaches to estimate the query language model and the news headline language model. We also suggest several criteria to evaluate the news headline prior that is the prior belief about the importance or newsworthiness of the news headline for a given day. Experimental results show that our system significantly outperforms a baseline system. Specifically, the proposed approach gives 2.62% and 10.19% further increases in MAP and P@5 over the best performing result of the TREC'09 Top Stories Identification Task.

#index 1450879
#* Proximity-based opinion retrieval
#@ Shima Gerani;Mark James Carman;Fabio Crestani
#t 2010
#c 13
#% 120104
#% 262096
#% 577355
#% 722308
#% 727877
#% 815915
#% 854646
#% 939769
#% 1019145
#% 1074102
#% 1074168
#% 1227614
#% 1261565
#% 1280262
#! Blog post opinion retrieval aims at finding blog posts that are relevant and opinionated about a user's query. In this paper we propose a simple probabilistic model for assigning relevant opinion scores to documents. The key problem is how to capture opinion expressions in the document, that are related to the query topic. Current solutions enrich general opinion lexicons by finding query-specific opinion lexicons using pseudo-relevance feedback on external corpora or the collection itself. In this paper we use a general opinion lexicon and propose using proximity information in order to capture opinion term relatedness to the query. We propose a proximity-based opinion propagation method to calculate the opinion density at each point in a document. The opinion density at the position of a query term in the document can then be considered as the probability of opinion about the query term at that position. The effect of different kernels for capturing the proximity is also discussed. Experimental results on the BLOG06 dataset show that the proposed method provides significant improvement over standard TREC baselines and achieves a 2.5% increase in MAP over the best performing run in the TREC 2008 blog track.

#index 1450880
#* Evaluating and predicting answer quality in community QA
#@ Chirag Shah;Jefferey Pomerantz
#t 2010
#c 13
#% 194269
#% 751570
#% 956517
#% 1047396
#% 1074111
#! Question answering (QA) helps one go beyond traditional keywords-based querying and retrieve information in more precise form than given by a document or a list of documents. Several community-based QA (CQA) services have emerged allowing information seekers pose their information need as questions and receive answers from their fellow users. A question may receive multiple answers from multiple users and the asker or the community can choose the best answer. While the asker can thus indicate if he was satisfied with the information he received, there is no clear way of evaluating the quality of that information. We present a study to evaluate and predict the quality of an answer in a CQA setting. We chose Yahoo! Answers as such CQA service and selected a small set of questions, each with at least five answers. We asked Amazon Mechanical Turk workers to rate the quality of each answer for a given question based on 13 different criteria. Each answer was rated by five different workers. We then matched their assessments with the actual asker's rating of a given answer. We show that the quality criteria we used faithfully match with asker's perception of a quality answer. We furthered our investigation by extracting various features from questions, answers, and the users who posted them, and training a number of classifiers to select the best answer using those features. We demonstrate a high predictability of our trained models along with the relative merits of each of the features for such prediction. These models support our argument that in case of CQA, contextual information such as a user's profile, can be critical in evaluating and predicting content quality.

#index 1450881
#* Adaptive near-duplicate detection via similarity learning
#@ Hannaneh Hajishirzi;Wen-tau Yih;Aleksander Kolcz
#t 2010
#c 13
#% 249321
#% 255137
#% 311808
#% 345087
#% 347225
#% 479973
#% 544011
#% 571725
#% 728115
#% 769944
#% 843714
#% 879600
#% 956507
#% 1023422
#% 1074121
#% 1082243
#% 1338611
#! In this paper, we present a novel near-duplicate document detection method that can easily be tuned for a particular domain. Our method represents each document as a real-valued sparse k-gram vector, where the weights are learned to optimize for a specified similarity function, such as the cosine similarity or the Jaccard coefficient. Near-duplicate documents can be reliably detected through this improved similarity measure. In addition, these vectors can be mapped to a small number of hash-values as document signatures through the locality sensitive hashing scheme for efficient similarity computation. We demonstrate our approach in two target domains: Web news articles and email messages. Our method is not only more accurate than the commonly used methods such as Shingles and I-Match, but also shows consistent improvement across the domains, which is a desired property lacked by existing methods.

#index 1450882
#* A content based approach for discovering missing anchor text for web search
#@ Xing Yi;James Allan
#t 2010
#c 13
#% 262096
#% 309749
#% 340901
#% 411762
#% 642992
#% 677148
#% 730090
#% 766430
#% 766431
#% 840846
#% 879575
#% 940042
#% 1055705
#% 1074127
#% 1077150
#% 1130855
#% 1227604
#! Although anchor text provides very useful information for web search, a large portion of web pages have few or no incoming hyperlinks (anchors), which is known as the anchor text sparsity problem. In this paper, we propose a language modeling based technique for overcoming anchor text sparsity by discovering a web page's plausible missing anchor text from its similar web pages' in-link anchor text. We design experiments with two publicly available TREC web corpora (GOV2 and ClueWeb09) to evaluate different approaches for discovering missing anchor text. Experimental results show that our approach can effectively discover plausible missing anchor terms. We then use the web named page finding task in the TREC Terabyte track to explore the utility of missing anchor text information discovered by our approach for helping retrieval. Experimental results show that our approach can statistically significantly improve retrieval performance, compared with several approaches that only use anchor text aggregated over the web graph.

#index 1450883
#* Uncovering social spammers: social honeypots + machine learning
#@ Kyumin Lee;James Caverlee;Steve Webb
#t 2010
#c 13
#% 642030
#% 740850
#% 769885
#% 769925
#% 847219
#% 926881
#% 957992
#% 961230
#% 996883
#% 1006352
#% 1055738
#% 1126250
#% 1127962
#% 1132890
#% 1190131
#% 1227655
#! Web-based social systems enable new community-based opportunities for participants to engage, share, and interact. This community value and related services like search and advertising are threatened by spammers, content polluters, and malware disseminators. In an effort to preserve community value and ensure longterm success, we propose and evaluate a honeypot-based approach for uncovering social spammers in online social systems. Two of the key components of the proposed approach are: (1) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. We describe the conceptual framework and design considerations of the proposed approach, and we present concrete observations from the deployment of social honeypots in MySpace and Twitter. We find that the deployed social honeypots identify social spammers with low false positive rates and that the harvested spam data contains signals that are strongly correlated with observable profile features (e.g., content, friend information, posting patterns, etc.). Based on these profile features, we develop machine learning based classifiers for identifying previously unknown spammers with high precision and a low rate of false positives.

#index 1450884
#* Studying trailfinding algorithms for enhanced web search
#@ Adish Singla;Ryen White;Jeff Huang
#t 2010
#c 13
#% 49499
#% 51397
#% 64880
#% 148007
#% 151411
#% 204872
#% 268112
#% 272821
#% 577224
#% 717133
#% 728111
#% 751830
#% 805200
#% 807420
#% 879567
#% 936914
#% 956495
#% 956533
#% 987212
#% 1055676
#% 1074133
#% 1275193
#% 1292591
#% 1431377
#% 1450902
#! Search engines return ranked lists of Web pages in response to queries. These pages are starting points for post-query navigation, but may be insufficient for search tasks involving multiple steps. Search trails mined from toolbar logs start with a query and contain pages visited by one user during post-query navigation. Implicit endorsements from many trails can enhance result ranking. Rather than using trails solely to improve ranking, it may also be worth providing trail information directly to users. In this paper, we quantify the benefit that users currently obtain from trail-following and compare different methods for finding the best trail for a given query and each top-ranked result. We compare the relevance, topic coverage, topic diversity, and utility of trails selected using different methods, and break out findings by factors such as query type and origin relevance. Our findings demonstrate value in trails, highlight interesting differences in the performance of trailfinding algorithms, and show we can find best-trails for a query that outperform the trails most users follow. Findings have implications for enhancing Web information seeking using trails.

#index 1450885
#* Context-aware ranking in web search
#@ Biao Xiang;Daxin Jiang;Jian Pei;Xiaohui Sun;Enhong Chen;Hang Li
#t 2010
#c 13
#% 284796
#% 577224
#% 818207
#% 818221
#% 818259
#% 853543
#% 869501
#% 869536
#% 881540
#% 956552
#% 987211
#% 987212
#% 1083721
#% 1166517
#% 1190074
#% 1227577
#! The context of a search query often provides a search engine meaningful hints for answering the current query better. Previous studies on context-aware search were either focused on the development of context models or limited to a relatively small scale investigation under a controlled laboratory setting. Particularly, about context-aware ranking for Web search, the following two critical problems are largely remained unsolved. First, how can we take advantage of different types of contexts in ranking? Second, how can we integrate context information into a ranking model? In this paper, we tackle the above two essential problems analytically and empirically. We develop different ranking principles for different types of contexts. Moreover, we adopt a learning-to-rank approach and integrate the ranking principles into a state-of-the-art ranking model by encoding the context information as features of the model. We empirically test our approach using a large search log data set obtained from a major commercial search engine. Our evaluation uses both human judgments and implicit user click data. The experimental results clearly show that our context-aware ranking approach improves the ranking of a commercial search engine which ignores context information. Furthermore, our method outperforms a baseline method which considers context information in ranking.

#index 1450886
#* Collecting high quality overlapping labels at low cost
#@ Hui Yang;Anton Mityagin;Krysta M. Svore;Sergey Markov
#t 2010
#c 13
#% 219753
#% 309095
#% 340892
#% 1083692
#% 1264744
#! This paper studies quality of human labels used to train search engines' rankers. Our specific focus is performance improvements obtained by using overlapping relevance labels, which is by collecting multiple human judgments for each training sample. The paper explores whether, when, and for which samples one should obtain overlapping training labels, as well as how many labels per sample are needed. The proposed selective labeling scheme collects additional labels only for a subset of training samples, specifically for those that are labeled relevant by a judge. Our experiments show that this labeling scheme improves the NDCG of two Web search rankers on several real-world test sets, with a low labeling overhead of around 1.4 labels per sample. This labeling scheme also outperforms several methods of using overlapping labels, such as simple k-overlap, majority vote, the highest labels, etc. Finally, the paper presents a study of how many overlapping labels are needed to get the best improvement in retrieval accuracy.

#index 1450887
#* Multi-style language model for web scale information retrieval
#@ Kuansan Wang;Xiaolong Li;Jianfeng Gao
#t 2010
#c 13
#% 131322
#% 262096
#% 280850
#% 280851
#% 288166
#% 340899
#% 340901
#% 340948
#% 397126
#% 397128
#% 397129
#% 642992
#% 766412
#% 766428
#% 783474
#% 838532
#% 956523
#% 1019761
#% 1077150
#% 1166534
#% 1181094
#% 1237597
#% 1292709
#% 1399978
#! Web documents are typically associated with many text streams, including the body, the title and the URL that are determined by the authors, and the anchor text or search queries used by others to refer to the documents. Through a systematic large scale analysis on their cross entropy, we show that these text streams appear to be composed in different language styles, and hence warrant respective language models to properly describe their properties. We propose a language modeling approach to Web document retrieval in which each document is characterized by a mixture model with components corresponding to the various text streams associated with the document. Immediate issues for such a mixture model arise as all the text streams are not always present for the documents, and they do not share the same lexicon, making it challenging to properly combine the statistics from the mixture components. To address these issues, we introduce an 'open-vocabulary' smoothing technique so that all the component language models have the same cardinality and their scores can simply be linearly combined. To ensure that the approach can cope with Web scale applications, the model training algorithm is designed to require no labeled data and can be fully automated with few heuristics and no empirical parameter tunings. The evaluation on Web document ranking tasks shows that the component language models indeed have varying degrees of capabilities as predicted by the cross-entropy analysis, and the combined mixture model outperforms the state-of-the-art BM25F based system.

#index 1450888
#* Combining coregularization and consensus-based self-training for multilingual text categorization
#@ Massih R. Amini;Cyril Goutte;Nicolas Usunier
#t 2010
#c 13
#% 252011
#% 266292
#% 375017
#% 425065
#% 466263
#% 770846
#% 875962
#% 881477
#% 1275645
#% 1377376
#! We investigate the problem of learning document classifiers in a multilingual setting, from collections where labels are only partially available. We address this problem in the framework of multiview learning, where different languages correspond to different views of the same document, combined with semi-supervised learning in order to benefit from unlabeled documents. We rely on two techniques, coregularization and consensus-based self-training, that combine multiview and semi-supervised learning in different ways. Our approach trains different monolingual classifiers on each of the views, such that the classifiers' decisions over a set of unlabeled examples are in agreement as much as possible, and iteratively labels new examples from another unlabeled training set based on a consensus across language-specific classifiers. We derive a boosting-based training algorithm for this task, and analyze the impact of the number of views on the semi-supervised learning results on a multilingual extension of the Reuters RCV1/RCV2 corpus using five different languages. Our experiments show that coregularization and consensus-based self-training are complementary and that their combination is especially effective in the interesting and very common situation where there are few views (languages) and few labeled documents available.

#index 1450889
#* Towards subjectifying text clustering
#@ Sajib Dasgupta;Vincent Ng
#t 2010
#c 13
#% 313959
#% 464291
#% 643008
#% 785341
#% 854646
#% 915244
#% 987202
#% 1250186
#% 1274862
#% 1279294
#% 1338589
#! Although it is common practice to produce only a single clustering of a dataset, in many cases text documents can be clustered along different dimensions. Unfortunately, not only do traditional text clustering algorithms fail to produce multiple clusterings of a dataset, the only clustering they produce may not be the one that the user desires. In this paper, we propose a simple active clustering algorithm that is capable of producing multiple clusterings of the same data according to user interest. In comparison to previous work on feedback-oriented clustering, the amount of user feedback required by our algorithm is minimal. In fact, the feedback turns out to be as simple as a cursory look at a list of words. Experimental results are very promising: our system is able to generate clusterings along the user-specified dimensions with reasonable accuracies on several challenging text classification tasks, thus providing suggestive evidence that our approach is viable.

#index 1450890
#* EUSUM: extracting easy-to-understand english summaries for non-native readers
#@ Xiaojun Wan;Huiying Li;Jianguo Xiao
#t 2010
#c 13
#% 190581
#% 194251
#% 397136
#% 755863
#% 787502
#% 815920
#% 816173
#% 818227
#% 861988
#% 939396
#% 939549
#% 961700
#% 1074088
#% 1166525
#% 1251709
#% 1260778
#% 1264737
#% 1272344
#% 1280217
#% 1306081
#% 1338622
#! In this paper we investigate a novel and important problem in multi-document summarization, i.e., how to extract an easy-to-understand English summary for non-native readers. Existing summarization systems extract the same kind of English summaries from English news documents for both native and non-native readers. However, the non-native readers have different English reading skills because they have different English education and learning backgrounds. An English summary which can be easily understood by native readers may be hardly understood by non-native readers. We propose to add the dimension of reading easiness or difficulty to multi-document summarization, and the proposed EUSUM system can produce easy-to-understand summaries according to the English reading skills of the readers. The sentence-level reading easiness (or difficulty) is predicted by using the SVM regression method. And the reading easiness score of each sentence is then incorporated into the summarization process. Empirical evaluation and user study have been performed and the results demonstrate that the EUSUM system can produce more easy-to-understand summaries for non-native readers than existing summarization systems, with very little sacrifice of the summary's informativeness.

#index 1450891
#* Visual summarization of web pages
#@ Binxing Jiao;Linjun Yang;Jizheng Xu;Feng Wu
#t 2010
#c 13
#% 46803
#% 281225
#% 324984
#% 344930
#% 397175
#% 400726
#% 577224
#% 730189
#% 838445
#% 869467
#% 1055673
#% 1119135
#% 1183297
#% 1392489
#! Visual summarization is a attractive new scheme to summarize web pages, which can help achieve a more friendly user experience in search and re-finding tasks by allowing users quickly get the idea of what the web page is about and helping users recall the visited web page. In this paper, we perform a careful study on the recently proposed visual summarization approaches, including the thumbnail of the web page snapshot, the internal image in the web page which is representative of the content in the page, and the visual snippet which is a synthesized image based on the internal image, the title, and the logo found in the web page. Moreover, since the internal image based summarization approach hardly works when the representative internal images are unavailable, we propose a new strategy, which retrieves the representative image from the external to summarize the web page. The experimental results suggest that the various summarization approaches have respective advantages on different types of web pages. While internal images and thumbnails can provide a reliable summarization on web pages with dominant images and web pages with simple structure respectively, the external images are regarded as a useful information to complement the internal images and are demonstrated very useful in helping users understanding new web pages . The visual snippet performs well on the re-finding tasks since it incorporates the title and logo which are advantageous on identifying the visited web pages.

#index 1450892
#* Learning more powerful test statistics for click-based retrieval evaluation
#@ Yisong Yue;Yue Gao;Oliver Chapelle;Ya Zhang;Thorsten Joachims
#t 2010
#c 13
#% 577224
#% 766409
#% 805200
#% 857180
#% 879565
#% 879567
#% 879598
#% 946521
#% 1074092
#% 1130811
#% 1190055
#% 1214757
#% 1264744
#% 1338557
#% 1355034
#% 1400034
#% 1450912
#! Interleaving experiments are an attractive methodology for evaluating retrieval functions through implicit feedback. Designed as a blind and unbiased test for eliciting a preference between two retrieval functions, an interleaved ranking of the results of two retrieval functions is presented to the users. It is then observed whether the users click more on results from one retrieval function or the other. While it was shown that such interleaving experiments reliably identify the better of the two retrieval functions, the naive approach of counting all clicks equally leads to a suboptimal test. We present new methods for learning how to score different types of clicks so that the resulting test statistic optimizes the statistical power of the experiment. This can lead to substantial savings in the amount of data required for reaching a target confidence level. Our methods are evaluated on an operational search engine over a collection of scientific articles.

#index 1450893
#* Query similarity by projecting the query-flow graph
#@ Ilaria Bordino;Carlos Castillo;Debora Donato;Aristides Gionis
#t 2010
#c 13
#% 310567
#% 330617
#% 577273
#% 728105
#% 823348
#% 869501
#% 869651
#% 952897
#% 975027
#% 987212
#% 987222
#% 989578
#% 1055677
#% 1064167
#% 1089474
#% 1096052
#% 1127383
#% 1130854
#% 1130868
#% 1130877
#% 1130878
#% 1173699
#% 1348915
#% 1386840
#% 1400017
#! Defining a measure of similarity between queries is an interesting and difficult problem. A reliable query-similarity measure can be used in a variety of applications such as query recommendation, query expansion, and advertising. In this paper, we exploit the information present in query logs in order to develop a measure of semantic similarity between queries. Our approach relies on the concept of the query-flow graph. The query-flow graph aggregates query reformulations from many users: nodes in the graph represent queries, and two queries are connected if they are likely to appear as part of the same search goal. Our query similarity measure is obtained by projecting the graph (or appropriate subgraphs of it) on a low-dimensional Euclidean space. Our experiments show that the measure we obtain captures a notion of semantic similarity between queries and it is useful for diversifying query recommendations.

#index 1450894
#* The demographics of web search
#@ Ingmar Weber;Carlos Castillo
#t 2010
#c 13
#% 290830
#% 349281
#% 351450
#% 453320
#% 576761
#% 590523
#% 803833
#% 803834
#% 818259
#% 874715
#% 956508
#% 1019163
#% 1035574
#% 1132900
#% 1166492
#! How does the web search behavior of "rich" and "poor" people differ? Do men and women tend to click on difffferent results for the same query? What are some queries almost exclusively issued by African Americans? These are some of the questions we address in this study. Our research combines three data sources: the query log of a major US-based web search engine, profile information provided by 28 million of its users (birth year, gender and ZIP code), and US-census information including detailed demographic information aggregated at the level of ZIP code. Through this combination we can annotate each query with, e.g. the average per-capita income in the ZIP code it originated from. Though conceptually simple, this combination immediately creates a powerful user modeling tool. The main contributions of this work are the following. First, we provide a demographic description of a large sample of search engine users in the US and show that it agrees well with the distribution of the US population. Second, we describe how different segments of the population differ in their search behavior, e.g. with respect to the queries they formulate or the URLs they click. Third, we explore applications of our methodology to improve web search relevance and to provide better query suggestions. These results enable a wide range of applications including improving web search and advertising where, for instance, targeted advertisements for "family vacations" could be adapted to the (expected) income.

#index 1450895
#* A user behavior model for average precision and its generalization to graded judgments
#@ Georges Dupret;Benjamin Piwowarski
#t 2010
#c 13
#% 857180
#% 1074139
#% 1095876
#% 1214757
#% 1268490
#% 1355034
#! We explore a set of hypothesis on user behavior that are potentially at the origin of the (Mean) Average Precision (AP) metric. This allows us to propose a more realistic version of AP where users click non-deterministically on relevant documents and where the number of relevant documents in the collection needs not be known in advance. We then depart from the assumption that a document is either relevant or irrelevant and we use instead relevance judgment similar to editorial labels used for Discounted Cumulated Gain (DCG). We assume that clicked documents provide users with a certain level of "utility" and that a user ends a search when she gathered enough utility. Based on the query logs of a commercial search engine we show how to evaluate the utility associated with a label from the record of past user interactions with the search engine and we show how the two different user models can be evaluated based on their ability to predict accurately future clicks. Finally, based on these user models, we propose a measure that captures the relative quality of two rankings.

#index 1450896
#* The effect of assessor error on IR system evaluation
#@ Ben Carterette;Ian Soboroff
#t 2010
#c 13
#% 262097
#% 262102
#% 262105
#% 340890
#% 561315
#% 818222
#% 857180
#% 879598
#% 879632
#% 907496
#% 1074132
#% 1074134
#% 1130866
#! Recent efforts in test collection building have focused on scaling back the number of necessary relevance judgments and then scaling up the number of search topics. Since the largest source of variation in a Cranfield-style experiment comes from the topics, this is a reasonable approach. However, as topic set sizes grow, and researchers look to crowdsourcing and Amazon's Mechanical Turk to collect relevance judgments, we are faced with issues of quality control. This paper examines the robustness of the TREC Million Query track methods when some assessors make significant and systematic errors. We find that while averages are robust, assessor errors can have a large effect on system rankings.

#index 1450897
#* Reusable test collections through experimental design
#@ Ben Carterette;Evangelos Kanoulas;Virgil Pavlu;Hui Fang
#t 2010
#c 13
#% 184486
#% 262102
#% 561315
#% 879598
#% 987199
#% 987200
#% 1019126
#% 1130864
#% 1130865
#% 1227631
#% 1355039
#! Portable, reusable test collections are a vital part of research and development in information retrieval. Reusability is difficult to assess, however. The standard approach--simulating judgment collection when groups of systems are held out, then evaluating those held-out systems--only works when there is a large set of relevance judgments to draw on during the simulation. As test collections adapt to larger and larger corpora, it becomes less and less likely that there will be sufficient judgments for such simulation experiments. Thus we propose a methodology for information retrieval experimentation that collects evidence for or against the reusability of a test collection while judgments are being made. Using this methodology along with the appropriate statistical analyses, researchers will be able to estimate the reusability of their test collections while building them and implement "course corrections" if the collection does not seem to be achieving desired levels of reusability. We show the robustness of our design to inherent sources of variance, and provide a description of an actual implementation of the framework for creating a large test collection.

#index 1450898
#* Do user preferences and evaluation measures line up?
#@ Mark Sanderson;Monica Lestari Paramita;Paul Clough;Evangelos Kanoulas
#t 2010
#c 13
#% 133894
#% 167557
#% 309089
#% 340921
#% 590523
#% 642975
#% 818257
#% 879566
#% 907493
#% 907495
#% 987321
#% 1074058
#% 1074069
#% 1074124
#% 1074133
#% 1130811
#% 1150163
#% 1166473
#% 1190055
#! This paper presents results comparing user preference for search engine rankings with measures of effectiveness computed from a test collection. It establishes that preferences and evaluation measures correlate: systems measured as better on a test collection are preferred by users. This correlation is established for both "conventional web retrieval" and for retrieval that emphasizes diverse results. The nDCG measure is found to correlate best with user preferences compared to a selection of other well known measures. Unlike previous studies in this area, this examination involved a large population of users, gathered through crowd sourcing, exposed to a wide range of retrieval systems, test collections and search tasks. Reasons for user preferences were also gathered and analyzed. The work revealed a number of new results, but also showed that there is much scope for future work refining effectiveness measures to better capture user preferences.

#index 1450899
#* Estimating advertisability of tail queries for sponsored search
#@ Sandeep Pandey;Kunal Punera;Marcus Fontoura;Vanja Josifovski
#t 2010
#c 13
#% 280819
#% 722904
#% 818265
#% 869550
#% 879633
#% 956546
#% 1055713
#% 1074101
#% 1130909
#% 1190106
#% 1190161
#% 1190162
#% 1211806
#% 1227742
#% 1280759
#% 1292472
#! Sponsored search is one of the major sources of revenue for search engines on the World Wide Web. It has been observed that while showing ads for every query maximizes short-term revenue, irrelevant ads lead to poor user experience and less revenue in the long-term. Hence, it is in search engines' interest to place ads only for queries that are likely to attract ad-clicks. Many algorithms for estimating query advertisability exist in literature, but most of these methods have been proposed for and tested on the frequent or "head" queries. Since query frequencies on search engine are known to be distributed as a power-law, this leaves a huge fraction of the queries uncovered. In this paper we focus on the more challenging problem of estimating query advertisability for infrequent or "tail" queries. These require fundamentally different methods than head queries: for e.g., tail queries are almost all unique and require the estimation method to be online and inexpensive. We show that previously proposed methods do not apply to tail queries, and when modified for our scenario they do not work well. Further, we give a simple, yet effective, approach, which estimates query advertisability using only the words present in the queries. We evaluate our approach on a real-world dataset consisting of search engine queries and user clicks. Our results show that our simple approach outperforms a more complex one based on regularized regression.

#index 1450900
#* Exploring reductions for long web queries
#@ Niranjan Balasubramanian;Giridhar Kumaran;Vitor R. Carvalho
#t 2010
#c 13
#% 411762
#% 577224
#% 1074112
#% 1130851
#% 1173692
#% 1227636
#% 1227647
#% 1280762
#% 1292594
#% 1355019
#% 1450964
#! Long queries form a difficult, but increasingly important segment for web search engines. Query reduction, a technique for dropping unnecessary query terms from long queries, improves performance of ad-hoc retrieval on TREC collections. Also, it has great potential for improving long web queries (upto 25% improvement in NDCG@5). However, query reduction on the web is hampered by the lack of accurate query performance predictors and the constraints imposed by search engine architectures and ranking algorithms. In this paper, we present query reduction techniques for long web queries that leverage effective and efficient query performance predictors. We propose three learning formulations that combine these predictors to perform automatic query reduction. These formulations enable trading of average improvements for the number of queries impacted, and enable easy integration into the search engine's architecture for rank-time query reduction. Experiments on a large collection of long queries issued to a commercial search engine show that the proposed techniques significantly outperform baselines, with more than 12% improvement in NDCG@5 in the impacted set of queries. Extension to the formulations such as result interleaving further improves results. We find that the proposed techniques deliver consistent retrieval gains where it matters most: poorly performing long web queries.

#index 1450901
#* Positional relevance model for pseudo-relevance feedback
#@ Yuanhua Lv;ChengXiang Zhai
#t 2010
#c 13
#% 106122
#% 134878
#% 194298
#% 218978
#% 328532
#% 340899
#% 340901
#% 340948
#% 342707
#% 413592
#% 577301
#% 818262
#% 879598
#% 879651
#% 987229
#% 987231
#% 1074081
#% 1074099
#% 1227608
#% 1227613
#% 1227614
#% 1292730
#% 1387547
#! Pseudo-relevance feedback is an effective technique for improving retrieval results. Traditional feedback algorithms use a whole feedback document as a unit to extract words for query expansion, which is not optimal as a document may cover several different topics and thus contain much irrelevant information. In this paper, we study how to effectively select from feedback documents those words that are focused on the query topic based on positions of terms in feedback documents. We propose a positional relevance model (PRM) to address this problem in a unified probabilistic way. The proposed PRM is an extension of the relevance model to exploit term positions and proximity so as to assign more weights to words closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. We develop two methods to estimate PRM based on different sampling processes. Experiment results on two large retrieval datasets show that the proposed PRM is effective and robust for pseudo-relevance feedback, significantly outperforming the relevance model in both document-based feedback and passage-based feedback.

#index 1450902
#* Assessing the scenic route: measuring the value of search trails in web logs
#@ Ryen W. White;Jeff Huang
#t 2010
#c 13
#% 51397
#% 148007
#% 204872
#% 268112
#% 272821
#% 309505
#% 325203
#% 378486
#% 577224
#% 717133
#% 728111
#% 751830
#% 754059
#% 805200
#% 807420
#% 879567
#% 936914
#% 956495
#% 987211
#% 987212
#% 1055676
#% 1074133
#% 1130852
#% 1137804
#% 1227622
#% 1275193
#% 1292591
#% 1355035
#% 1450884
#! Search trails mined from browser or toolbar logs comprise queries and the post-query pages that users visit. Implicit endorsements from many trails can be useful for search result ranking, where the presence of a page on a trail increases its query relevance. Follow-ing a search trail requires user effort, yet little is known about the benefit that users obtain from this activity versus, say, sticking with the clicked search result or jumping directly to the destination page at the end of the trail. In this paper, we present a log-based study estimating the user value of trail following. We compare the relevance, topic coverage, topic diversity, novelty, and utility of full trails over that provided by sub-trails, trail origins (landing pages), and trail destinations (pages where trails end). Our findings demonstrate significant value to users in following trails, especially for certain query types. The findings have implications for the design of search systems, including trail recommendation systems that display trails on search result pages.

#index 1450903
#* Human performance and retrieval precision revisited
#@ Mark D. Smucker;Chandra Prakash Jethani
#t 2010
#c 13
#% 309089
#% 340921
#% 411762
#% 818257
#% 879566
#% 1074058
#% 1074069
#% 1074141
#% 1095876
#% 1227721
#% 1227748
#! Several studies have found that the Cranfield approach to evaluation can report significant performance differences between retrieval systems for which little to no performance difference is found for humans completing tasks with these systems. We revisit the relationship between precision and performance by measuring human performance on tightly controlled search tasks and with user interfaces offering limited interaction. We find that human performance and retrieval precision are strongly related. We also find that users change their relevance judging behavior based on the precision of the results. This change in behavior coupled with the well-known lack of perfect inter-assessor agreement can reduce the measured performance gains predicted by increased precision.

#index 1450904
#* Extending average precision to graded relevance judgments
#@ Stephen E. Robertson;Evangelos Kanoulas;Emine Yilmaz
#t 2010
#c 13
#% 309095
#% 340892
#% 411762
#% 575729
#% 818205
#% 818221
#% 840846
#% 879630
#% 907496
#% 987226
#% 987241
#% 987321
#% 1035577
#% 1074134
#% 1074139
#% 1227634
#% 1227673
#% 1292527
#% 1348075
#% 1456843
#% 1715667
#! Evaluation metrics play a critical role both in the context of comparative evaluation of the performance of retrieval systems and in the context of learning-to-rank (LTR) as objective functions to be optimized. Many different evaluation metrics have been proposed in the IR literature, with average precision (AP) being the dominant one due a number of desirable properties it possesses. However, most of these measures, including average precision, do not incorporate graded relevance. In this work, we propose a new measure of retrieval effectiveness, the Graded Average Precision (GAP). GAP generalizes average precision to the case of multi-graded relevance and inherits all the desirable characteristics of AP: it has a nice probabilistic interpretation, it approximates the area under a graded precision-recall curve and it can be justified in terms of a simple but moderately plausible user model. We then evaluate GAP in terms of its informativeness and discriminative power. Finally, we show that GAP can reliably be used as an objective metric in learning to rank by illustrating that optimizing for GAP using SoftRank and LambdaRank leads to better performing ranking functions than the ones constructed by algorithms tuned to optimize for AP or NDCG even when using AP or NDCG as the test metrics.

#index 1450905
#* PRES: a score metric for evaluating recall-oriented information retrieval applications
#@ Walid Magdy;Gareth J.F. Jones
#t 2010
#c 13
#% 144074
#% 248065
#% 309093
#% 348308
#% 375017
#% 387427
#% 561315
#% 766409
#% 810906
#% 874504
#% 879650
#% 907496
#% 987249
#% 1074139
#% 1095876
#% 1130863
#% 1130924
#% 1131204
#% 1292765
#% 1415710
#% 1494802
#! Information retrieval (IR) evaluation scores are generally designed to measure the effectiveness with which relevant documents are identified and retrieved. Many scores have been proposed for this purpose over the years. These have primarily focused on aspects of precision and recall, and while these are often discussed with equal importance, in practice most attention has been given to precision focused metrics. Even for recall-oriented IR tasks of growing importance, such as patent retrieval, these precision based scores remain the primary evaluation measures. Our study examines different evaluation measures for a recall-oriented patent retrieval task and demonstrates the limitations of the current scores in comparing different IR systems for this task. We introduce PRES, a novel evaluation metric for this type of application taking account of recall and the user's search effort. The behaviour of PRES is demonstrated on 48 runs from the CLEF-IP 2009 patent retrieval track. A full analysis of the performance of PRES shows its suitability for measuring the retrieval effectiveness of systems from a recall focused perspective taking into account the user's expected search effort.

#index 1450906
#* Content-enriched classifier for web video classification
#@ Bin Cui;Ce Zhang;Gao Cong
#t 2010
#c 13
#% 78171
#% 165110
#% 194202
#% 280817
#% 309208
#% 344447
#% 430757
#% 451617
#% 564044
#% 658962
#% 748600
#% 763697
#% 770846
#% 903617
#% 915364
#% 996175
#% 1019151
#% 1083703
#% 1665151
#% 1860941
#! With the explosive growth of online videos, automatic real-time categorization of Web videos plays a key role for organizing, browsing and retrieving the huge amount of videos on the Web. Previous work shows that, in addition to text features, content features of videos are also useful for Web video classification. Unfortunately, extracting content features is computationally prohibitive for real-time video classification. In this paper we propose a novel video classification framework that is able to exploit both content and text features for video classification while avoiding the expensive computation of extracting content features at classification time. The main idea of our approach is to utilize the content features extracted from training data to enrich the text based semantic kernels, yielding content-enriched semantic kernels. The content-enriched semantic kernels enable to utilize both content and text features for classifying new videos without extracting their content features. The experimental results show that our approach significantly outperforms the state-of-the-art video classification methods.

#index 1450907
#* Robust audio identification for MP3 popular music
#@ Wei Li;Yaduo Liu;Xiangyang Xue
#t 2010
#c 13
#% 341278
#% 342720
#% 413600
#% 839923
#% 845616
#% 894890
#% 970454
#% 970597
#% 996163
#% 1081982
#% 1221075
#% 1756699
#% 1775832
#! Audio identification via fingerprint has been an active research field with wide applications for years. Many technical papers were published and commercial software systems were also employed. However, most of these previously reported methods work on the raw audio format in spite of the fact that nowadays compressed format audio, especially MP3 music, has grown into the dominant way to store on personal computers and transmit on the Internet. It would be interesting if a compressed unknown audio fragment is able to be directly recognized from the database without the fussy and time-consuming decompression-identification-recompression procedure. So far, very few algorithms run directly in the compressed domain for music information retrieval, and most of them take advantage of MDCT coefficients or derived energy type of features. As a first attempt, we propose in this paper utilizing compressed-domain spectral entropy as the audio feature to implement a novel audio fingerprinting algorithm. The compressed songs stored in a music database and the possibly distorted compressed query excerpts are first partially decompressed to obtain the MDCT coefficients as the intermediate result. Then by grouping granules into longer blocks, remapping the MDCT coefficients into 192 new frequency lines to unify the frequency distribution of long and short windows, and defining 9 new subbands which cover the main frequency bandwidth of popular songs in accordance with the scale-factor bands of short windows, we calculate the spectral entropy of all consecutive blocks and come to the final fingerprint sequence by means of magnitude relationship modeling. Experiments show that such fingerprints exhibit strong robustness against various audio signal distortions like recompression, noise interference, echo addition, equalization, band-pass filtering, pitch shifting, and slight time-scale modification etc. For 5s-long query examples which might be severely degraded, an average top-five retrieval precision rate of more than 90% can be obtained in our test data set composed of 1822 popular songs.

#index 1450908
#* Effective music tagging through advanced statistical modeling
#@ Jialie Shen;Wang Meng;Shuichang Yan;HweeHwa Pang;Xiansheng Hua
#t 2010
#c 13
#% 387427
#% 571831
#% 643010
#% 729437
#% 787639
#% 876634
#% 879572
#% 937890
#% 987247
#% 1073988
#% 1174741
#% 1227625
#% 1227627
#% 1767359
#% 1775543
#! Music information retrieval (MIR) holds great promise as a technology for managing large music archives. One of the key components of MIR that has been actively researched into is music tagging. While significant progress has been achieved, most of the existing systems still adopt a simple classification approach, and apply machine learning classifiers directly on low level acoustic features. Consequently, they suffer the shortcomings of (1) poor accuracy, (2) lack of comprehensive evaluation results and the associated analysis based on large scale datasets, and (3) incomplete content representation, arising from the lack of multimodal and temporal information integration. In this paper, we introduce a novel system called MMTagger that effectively integrates both multimodal and temporal information in the representation of music signal. The carefully designed multilayer architecture of the proposed classification framework seamlessly combines Multiple Gaussian Mixture Models (GMMs) and Support Vector Machine (SVM) into a single framework. The structure preserves more discriminative information, leading to more accurate and robust tagging. Experiment results obtained with two large music collections highlight the various advantages of our multilayer framework over state of the art techniques.

#index 1450909
#* Properties of optimally weighted data fusion in CBMIR
#@ Peter Wilkins;Alan F. Smeaton;Paul Ferguson
#t 2010
#c 13
#% 111303
#% 144076
#% 184496
#% 232703
#% 348344
#% 420464
#% 645687
#% 659265
#% 730182
#% 784148
#% 839962
#% 840002
#% 879605
#% 903632
#% 976952
#% 1677682
#! Content-Based Multimedia Information Retrieval (CBMIR) systems which leverage multiple retrieval experts (En) often employ a weighting scheme when combining expert results through data fusion. Typically however a query will comprise multiple query images (Im) leading to potentially N × M weights to be assigned. Because of the large number of potential weights, existing approaches impose a hierarchy for data fusion, such as uniformly combining query image results from a single retrieval expert into a single list and then weighting the results of each expert. In this paper we will demonstrate that this approach is sub-optimal and leads to the poor state of CBMIR performance in benchmarking evaluations. We utilize an optimization method known as Coordinate Ascent to discover the optimal set of weights (|En| ⋅ |Im|) which demonstrates a dramatic difference between known results and the theoretical maximum. We find that imposing common combinatorial hierarchies for data fusion will half the optimal performance that can be achieved. By examining the optimal weight sets at the topic level, we observe that approximately 15% of the weights (from set |En| ⋅ |Im|) for any given query, are assigned 70%-82% of the total weight mass for that topic. Furthermore we discover that the ideal distribution of weights follows a log-normal distribution. We find that we can achieve up to 88% of the performance of fully optimized query using just these 15% of the weights. Our investigation was conducted on TRECVID evaluations 2003 to 2007 inclusive and ImageCLEFPhoto 2007, totalling 181 search topics optimized over a combined collection size of 661,213 images and 1,594 topic images.

#index 1450910
#* To translate or not to translate?
#@ Chia-Jung Lee;Chin-Hui Chen;Shao-Hang Kao;Pu-Jen Cheng
#t 2010
#c 13
#% 211043
#% 262046
#% 262047
#% 280826
#% 340895
#% 397143
#% 397144
#% 556840
#% 562054
#% 748444
#% 766425
#% 939575
#% 1074112
#! Query translation is an important task in cross-language information retrieval (CLIR) aiming to translate queries into languages used in documents. The purpose of this paper is to investigate the necessity of translating query terms, which might differ from one term to another. Some untranslated terms cause irreparable performance drop while others do not. We propose an approach to estimate the translation probability of a query term, which helps decide if it should be translated or not. The approach learns regression and classification models based on a rich set of linguistic and statistical properties of the term. Experiments on NTCIR-4 and NTCIR-5 English-Chinese CLIR tasks demonstrate that the proposed approach can significantly improve CLIR performance. An in-depth analysis is also provided for discussing the impact of untranslated out-of-vocabulary (OOV) query terms and translation quality of non-OOV query terms on CLIR performance.

#index 1450911
#* Multilingual PRF: english lends a helping hand
#@ Manoj K. Chinnakotla;Karthik Raman;Pushpak Bhattacharyya
#t 2010
#c 13
#% 169729
#% 262084
#% 280851
#% 298183
#% 318416
#% 324129
#% 340899
#% 340901
#% 342707
#% 579944
#% 732844
#% 748337
#% 750863
#% 783506
#% 838532
#% 843728
#% 879585
#% 907493
#% 987231
#% 1074081
#% 1132411
#% 1132418
#% 1215368
#% 1227584
#% 1715628
#! In this paper, we present a novel approach to Pseudo-Relevance Feedback (PRF) called Multilingual PRF (MultiPRF). The key idea is to harness multilinguality. Given a query in a language, we take the help of another language to ameliorate the well known problems of PRF, viz. (a) The expansion terms from PRF are primarily based on co-occurrence relationships with query terms, and thus other terms which are lexically and semantically related, such as morphological variants and synonyms, are not explicitly captured, and (b) PRF is quite sensitive to the quality of the initially retrieved top k documents and is thus not robust. In MultiPRF, given a query in language L1, it is translated into language L2 and PRF is performed on a collection in language L2 and the resultant feedback model is translated from L2 back into L1. The final feedback model is obtained by combining the translated model with the original feedback model of the query in L1. Experiments were performed on standard CLEF collections in languages with widely differing characteristics, viz., French, German, Finnish and Hungarian with English as the assisting language. We observe that MultiPRF outperforms PRF and is more robust with consistent and significant improvements in the above widely differing languages. A thorough analysis of the results reveal that the second language helps in obtaining both co-occurrence based conceptual terms as well as lexically and semantically related terms. Additionally, the use of the second language collection reduces the sensitivity to performance of initial retrieval, thereby making it more robust.

#index 1450912
#* Comparing the sensitivity of information retrieval metrics
#@ Filip Radlinski;Nick Craswell
#t 2010
#c 13
#% 309089
#% 397163
#% 577224
#% 590523
#% 728115
#% 818222
#% 818257
#% 879566
#% 907495
#% 987263
#% 1074058
#% 1074134
#% 1130811
#% 1181094
#% 1450892
#! Information retrieval effectiveness is usually evaluated using measures such as Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision (MAP) and Precision at some cutoff (Precision@k) on a set of judged queries. Recent research has suggested an alternative, evaluating information retrieval systems based on user behavior. Particularly promising are experiments that interleave two rankings and track user clicks. According to a recent study, interleaving experiments can identify large differences in retrieval effectiveness with much better reliability than other click-based methods. We study interleaving in more detail, comparing it with traditional measures in terms of reliability, sensitivity and agreement. To detect very small differences in retrieval effectiveness, a reliable outcome with standard metrics requires about 5,000 judged queries, and this is about as reliable as interleaving with 50,000 user impressions. Amongst the traditional measures, NDCG has the strongest correlation with interleaving. Finally, we present some new forms of analysis, including an approach to enhance interleaving sensitivity.

#index 1450913
#* Efficient partial-duplicate detection based on sequence matching
#@ Qi Zhang;Yue Zhang;Haomin Yu;Xuanjing Huang
#t 2010
#c 13
#% 69503
#% 115462
#% 187084
#% 249321
#% 255137
#% 345087
#% 347225
#% 479973
#% 504572
#% 544011
#% 616528
#% 654447
#% 723279
#% 769944
#% 815336
#% 879600
#% 956507
#% 963669
#% 1004299
#% 1065411
#% 1074121
#% 1074122
#% 1166531
#% 1227596
#% 1270458
#! With the ever-increasing growth of the Internet, numerous copies of documents become serious problem for search engine, opinion mining and many other web applications. Since partial-duplicates only contain a small piece of text taken from other sources and most existing near-duplicate detection approaches focus on document level, partial duplicates can not be dealt with well. In this paper, we propose a novel algorithm to realize the partial-duplicate detection task. Besides the similarities between documents, our proposed algorithm can simultaneously locate the duplicated parts. The main idea is to divide the partial-duplicate detection task into two subtasks: sentence level near-duplicate detection and sequence matching. For evaluation, we compare the proposed method with other approaches on both English and Chinese web collections. Experimental results appear to support that our proposed method is effectively and efficiently to detect both partial-duplicates on large web collections.

#index 1450914
#* Discriminative models of integrating document evidence and document-candidate associations for expert search
#@ Yi Fang;Luo Si;Aditya P. Mathur
#t 2010
#c 13
#% 120104
#% 750863
#% 766414
#% 836002
#% 879570
#% 907525
#% 976952
#% 1019135
#% 1083734
#% 1130856
#% 1130922
#% 1133171
#% 1268491
#% 1392465
#% 1400942
#% 1415732
#% 1415733
#% 1415734
#! Generative models such as statistical language modeling have been widely studied in the task of expert search to model the relationship between experts and their expertise indicated in supporting documents. On the other hand, discriminative models have received little attention in expert search research, although they have been shown to outperform generative models in many other information retrieval and machine learning applications. In this paper, we propose a principled relevance-based discriminative learning framework for expert search and derive specific discriminative models from the framework. Compared with the state-of-the-art language models for expert search, the proposed research can naturally integrate various document evidence and document-candidate associations into a single model without extra modeling assumptions or effort. An extensive set of experiments have been conducted on two TREC Enterprise track corpora (i.e., W3C and CERC) to demonstrate the effectiveness and robustness of the proposed framework.

#index 1450915
#* Vertical selection in the presence of unlabeled verticals
#@ Jaime Arguello;Fernando Diaz;Jean-François Paiement
#t 2010
#c 13
#% 194246
#% 411762
#% 413594
#% 643012
#% 748550
#% 987228
#% 987317
#% 1100145
#% 1130914
#% 1130923
#% 1166523
#% 1227080
#% 1227616
#% 1227617
#% 1227629
#% 1261539
#% 1292575
#% 1292595
#% 1392444
#! Vertical aggregation is the task of incorporating results from specialized search engines or verticals (e.g., images, video, news) into Web search results. Vertical selection is the subtask of deciding, given a query, which verticals, if any, are relevant. State of the art approaches use machine learned models to predict which verticals are relevant to a query. When trained using a large set of labeled data, a machine learned vertical selection model outperforms baselines which require no training data. Unfortunately, whenever a new vertical is introduced, a costly new set of editorial data must be gathered. In this paper, we propose methods for reusing training data from a set of existing (source) verticals to learn a predictive model for a new (target) vertical. We study methods for learning robust, portable, and adaptive cross-vertical models. Experiments show the need to focus on different types of features when maximizing portability (the ability for a single model to make accurate predictions across multiple verticals) than when maximizing adaptability (the ability for a single model to make accurate predictions for a specific vertical). We demonstrate the efficacy of our methods through extensive experimentation for 11 verticals

#index 1450916
#* iCollaborate: harvesting value from enterprise web usage
#@ Ajinkya Kale;Thomas Burris;Bhavesh Shah;T L Prasanna Venkatesan;Lakshmanan Velusamy;Manish Gupta;Melania Degerattu
#t 2010
#c 13
#% 249321
#! We are in a phase of 'Participatory Web' in which users add value' to the information on the web by publishing, tagging and sharing. The Participatory Web has enormous potential for an enterprise because unlike the users of the internet an enterprise is a community that shares common goals, assumptions, vocabulary and interest and has reliable user identification and mutual trust along with a central governance and incentives to collaborate. Everyday, the employees of an organization locate content relevant to their work on the web. Finding this information takes time, expertise and creativity, which costs an organization money. That is, the web pages employees find are knowledge assets owned by the enterprise. This investment in web-based knowledge assets is lost every time the enterprise fails to capture and reuse them. iCollaborate is tooled to capture user's web interaction, persist and analyze it, and feed that interaction back into the community - the enterprise.

#index 1450917
#* Exploring desktop resources based on user activity analysis
#@ Yukun Li;Xiangyu Zhang;Xiaofeng Meng
#t 2010
#c 13
#% 860036
#! Relocation in personal desktop resources is an interesting and promising research topic. This demonstration illustrates a new perspective in exploring desktop resources to help users re-find expected data resources more effectively. Different from existing works, our prototype OrientSpace has two features: automatically extract and maintain user tasks to support task-based exploration, and support vague search by exploiting associations between desktop resources.

#index 1450918
#* A data-parallel toolkit for information retrieval
#@ Dennis Fetterly;Frank McSherry
#t 2010
#c 13
#% 255137
#% 1166529
#% 1468421

#index 1450919
#* Finding and filtering information for children
#@ Desmond Elliot;Richard Glassey;Tamara Polajnar;Leif Azzopardi
#t 2010
#c 13
#! Children face several challenges when using information access systems. These include formulating queries, judging the relevance of documents, and focusing attention on interface cues, such as query suggestions, while typing queries. It has also been shown that children want a personalised Web experience and prefer content presented to them that matches their long-term entertainment and education needs. To this end, we have developed an interaction-based information filtering system to address these challenges.

#index 1450920
#* Automatic content linking: speech-based just-in-time retrieval for multimedia archives
#@ Andrei Popescu-Belis;Jonathan Kilgour;Peter Poller;Alexandre Nanchen;Erik Boertjes;Joost de Wit
#t 2010
#c 13
#% 445168
#% 768292
#! The Automatic Content Linking Device monitors a conversation and uses automatically recognized words to retrieve documents that are of potential use to the participants. The document set includes project related reports or emails, transcribed snippets of past meetings, and websites. Retrieval results are displayed at regular intervals.

#index 1450921
#* Si-Fi: interactive similar item finder
#@ Inbeom Hwang;Minsuk Kahng;Sung Eun Park;Jinwook Seo;Sang-goo Lee
#t 2010
#c 13
#% 1328120

#index 1450922
#* Suggesting related topics in web search
#@ Santosh Raju;Shaishav Kumar;Raghavendra Udupa
#t 2010
#c 13
#% 869501
#% 869651
#! Suggesting topics that are related to user's goal or interest is very important in web search. However, search engines today focus on suggesting mainly reformulations and lexical variants of the query mined from query logs. In this demonstration, we show a system that can suggest related topics for a query based on the top search results for the query. It can help users in exploring the topics related to their information need. The topic suggestion system can be integrated with any search engine or it can be easily installed on the client machine as a browser plugin.

#index 1450923
#* Agro-Gator: digesting experts, logs, and N-grams
#@ Michael Huggett
#t 2010
#c 13
#% 1213622
#! As research includes more and larger user studies, a significant problem lies in combining the many types of data files into a single table suitable for analysis by common statistical tools. We have developed a data-aggregation tool that combines user logs, expert scoring, and task/session attributes. The tool also integrates the n-grams derived from a given sequence of actions in the user tasks. The tool provides a GUI for quick and easy configuration.

#index 1450924
#* Medical search and classification tools for recommendation
#@ Jimmy Xiangji Huang;Aijun An;Qinmin Hu
#t 2010
#c 13
#! their patients' records from paper to computer, enormous amounts of electronic medical records (EMR) have become available for medical research. Some of the EMR data are well-structured, for which traditional database management systems can provide effective retrieval and management functions. However, most of the EMR data (such as progress notes and consultation letters) are in free text formats. How to effectively and efficiently retrieve and discover useful information from the vast amount of such semi-structured data is a challenge faced by medical professionals. Without proper tools, the rich information and knowledge buried in the medical health records are unavailable for clinical research and decision-making. The objective of our research is to develop text analytics tools that are capable of parsing clinical medical data so that predefined search subjects that correspond to a list of medical diagnoses can be extracted. In addition to this particular core functionality, it is also desired that several important assets should be present within the text-analytics tools in order to improve its overall ability to be used as recommendation tools. In this research, we work with research scientists at the Institute for Clinical Evaluative Sciences (ICES) in Toronto and examine a number of techniques for structuring and processing free text documents in order to effectively and efficiently search and analyze vast amount of medical records. We implement several powerful medical text analytics tools for clinical data searching and classification. For data classification, our tools sort through a great amount of patientrecords to identify the likelihood of a patient having myocardial infarction (MI) or hypertension (HTN), and classify the patients accordingly. Our tools can also identify the likelihood of a patient being a smoker, previous smoker or non-smoker based on the text data of medical records.

#index 1450925
#* Multilingual people search
#@ Shaishav Kumar;Raghavendra Udupa
#t 2010
#c 13
#% 1470635
#% 1697421
#! People Search is an important search service with multiple applications (eg. looking up a friend on Facebook, finding colleagues in corporate email directories etc). With the proportion of non-English users on a steady rise, people search services are being used by users from diverse language demographics. Users may issue name search queries against these directories in languages other than the language of the directory, in which case the present monolingual name search approaches will not work. In this demo, we present a Multilingual People Search system capable of performing fast name lookups on large user directories, independent of the directory language. Our system has applications in areas like social networking, enterprise search and email address book search.

#index 1450926
#* Closed form solution of similarity algorithms
#@ Yuanzhe Cai;Miao Zhang;Chris Ding;Sharma Chakravarthy
#t 2010
#c 13
#% 411762
#% 577273
#% 818218
#% 1292521
#! Algorithms defining similarities between objects of an information network are important of many IR tasks. SimRank algorithm and its variations are popularly used in many applications. Many fast algorithms are also developed. In this note, we first reformulate them as random walks on the network and express them using forward and backward transition probably in a matrix form. Second, we show that P-Rank (SimRank is only the special case of P-Rank) has a unique solution of eeT when decay factor c is equal to 1. We also show that SimFusion algorithm is a special case of P-Rank algorithm and prove that the similarity matrix of SimFusion is the product of PageRank vector. Our experiments on the web datasets show that for P-Rank the decay factor c doesn't seriously affect the similarity accuracy and accuracy of P-Rank is also higher than SimFusion and SimRank.

#index 1450927
#* Blog snippets: a comments-biased approach
#@ Javier Parapar;Jorge López-Castro;Álvaro Barreiro
#t 2010
#c 13
#% 643014
#% 814952
#% 816173
#% 1074087
#! In the last years Blog Search has been a new exciting task in Information Retrieval. The presence of user generated information with valuable opinions makes this field of huge interest. In this poster we use part of this information, the readers' comments, to improve the quality of post snippets with the objective of enhancing the user access to the relevant posts in a result list. We propose a simple method for snippet generation based on sentence selection, using the comments to guide the selection process. We evaluated our approach with standard TREC methodology in the Blogs06 collection showing significant improvements up to 32% in terms of MAP over the baseline.

#index 1450928
#* SIGIR: scholar vs. scholars' interpretation
#@ James Lanagan;Alan F. Smeaton
#t 2010
#c 13
#% 723329
#% 1024549
#! Google Scholar allows researchers to search through a free and extensive source of information on scientific publications. In this paper we show that within the limited context of SIGIR proceedings, the rankings created by Google Scholar are both significantly different and very negatively correlated with those of domain experts.

#index 1450929
#* Effective query expansion with the resistance distance based term similarity metric
#@ Shuguang Wang;Milos Hauskrecht
#t 2010
#c 13
#% 46803
#% 218978
#% 1227584
#! In this paper, we define a new query expansion method that relies on term similarity metric derived from the electric resistance network. This proposed metric lets us measure the mutual relevancy in between terms and between their groups. This paper shows how to define this metric automatically from the document collection, and then apply it in query expansion for document retrieval tasks. The experiments show this method can be used to find good expansion terms of search queries and improve document retrieval performance on two TREC genomic track datasets.

#index 1450930
#* A method to automatically construct a user knowledge model in a forum environment
#@ Ahmad Kardan;Mehdi Garakani;Bamdad Bahrani
#t 2010
#c 13
#% 1067443
#% 1098236
#% 1396085
#! Having a mechanism to validate the opinions and to identify experts in a forum could help people to favor one opinion against another. To achieve this, some solutions have already been introduced, including social network analysis techniques and reputation modeling. However, neither of these solutions considers the users' knowledge to identify an expert. In this paper, a novel method is proposed which estimates users' knowledge based on the forum itself, and identifies the possible areas of expertise associated with each user.

#index 1450931
#* Learning to rank audience for behavioral targeting
#@ Ning Liu;Jun Yan;Dou Shen;Depin Chen;Zheng Chen;Ying Li
#t 2010
#c 13
#% 1190081
#! Behavioral Targeting (BT) is a recent trend of online advertising market. However, some classical BT solutions, which predefine the user segments for BT ads delivery, are sometimes too large to numerous long-tail advertisers, who cannot afford to buy any large user segments due to budget consideration. In this extend abstract, we propose to rank users according to their probability of interest in an advertisement in a learning to rank framework. We propose to extract three types of features between user behaviors such as search queries, ad click history etc and the ad content provided by advertisers. Through this way, a long-tail advertiser can select a certain number of top ranked users as needed from the user segments for ads delivery. In the experiments, we use a 30-days' ad click-through log from a commercial search engine. The results show that using our proposed features under a learning to rank framework, we can well rank users who potentially interest in an advertisement.

#index 1450932
#* Multi-modal query expansion for web video search
#@ Bailan Feng;Juan Cao;Zhineng Chen;Yongdong Zhang;Shouxun Lin
#t 2010
#c 13
#% 1279769
#! Query expansion is an effective method to improve the usability of multimedia search. Most existing multimedia search engines are able to automatically expand a list of textual query terms based on text search techniques, which can be called textual query expansion (TQE). However, the annotations (title and tag) around web videos are generally noisier for text-only query expansion and search matching. In this paper, we propose a novel multi-modal query expansion (MMQE) framework for web video search to solve the issue. Compared with traditional methods, MMQE provides a more intuitive query suggestion by transforming tex-tual query to visual presentation based on visual clustering. Paral-lel to this, MMQE can enhance the process of search matching with strong pertinence of intent-specific query by joining textual, visual and social cues from both metadata and content of videos. Experimental results on real web videos from YouTube demon-strate the effectiveness of the proposed method.

#index 1450933
#* Context aware query classification using dynamic query window and relationship net
#@ Nazli Goharian;Saket S.R. Mengle
#t 2010
#c 13
#% 464434
#% 1203762
#% 1227577
#% 1432787
#! The context of the user queries, preceding a given query, is utilized to improve the effectiveness of query classification. Earlier efforts utilize fixed number of preceding queries to derive such context information. We propose and evaluate an approach (DQW) that identifies a set of unambiguous preceding queries in a dynamically determined window to utilize in classifying an ambiguous query. Furthermore, utilizing a relationship-net (R-net) that represents relationships among known categories, we improve the classification effectiveness for those ambiguous queries whose predicted category in this relationship-net is related to the category of a query within the window. Our results indicate that the hybrid approach (DQW+R-net) statistically significantly improves the Conditional Random Field (CRF) query classification approach when static query windowing and hierarchical taxonomy are used (SQW+Tax), in terms of precision (10.8%), recall (13.2%), and F1 measure (11.9%).

#index 1450934
#* Predicting query potential for personalization, classification or regression?
#@ Chen Chen;Muyun Yang;Sheng Li;Tiejun Zhao;Haoliang Qi
#t 2010
#c 13
#% 818224
#% 818259
#% 838547
#% 956552
#% 1074071
#% 1357833
#! The goal of predicting query potential for personalization is to determine which queries can benefit from personalization. In this paper, we investigate which kind of strategy is better for this task: classification or regression. We quantify the potential benefits of personalizing search results using two implicit click-based measures: Click entropy and Potential@N. Meanwhile, queries are characterized by query features and history features. Then we build C-SVM classification model and epsilon-SVM regression model respectively according to these two measures. The experimental results show that the classification model is a better choice for predicting query potential for personalization.

#index 1450935
#* The impact of collection size on relevance and diversity
#@ Marijn Koolen;Jaap Kamps
#t 2010
#c 13
#% 453327
#! It has been observed that precision increases with collection size. One explanation could be that the redundancy of information increases, making it easier to find multiple documents conveying the same information. Arguably, a user has no interest in reading the same information over and over, but would prefer a set of diverse search results covering multiple aspects of the search topic. In this paper, we look at the impact of the collection size on the relevance and diversity of retrieval results by down-sampling the collection. Our main finding is that we can we can improve diversity by randomly removing the majority of the results--this will significantly reduce the redundancy and only marginally affect the subtopic coverage.

#index 1450936
#* Spatial relationships in visual graph modeling for image categorization
#@ Trong-Ton Pham;Philippe Mulhem;Loic Maisonnasse
#t 2010
#c 13
#% 262096
#% 1071110
#! In this paper, a language model adapted to graph-based representation of image content is proposed and assessed. The full indexing and retrieval processes are evaluated on two different image corpora. We show that using the spatial relationships with graph model has a positive impact on the results of standard Language Model (LM) and outperforms the baseline built upon the current state-of-the-art Support Vector Machine (SVM) classification method.

#index 1450937
#* A picture is worth a thousand search results: finding child-oriented multimedia results with collAge
#@ Karl Gyllstrom;Marie-Francine Moens
#t 2010
#c 13
#% 449291
#! We present a simple and effective approach to complement search results for children's web queries with child-oriented multimedia results, such as coloring pages and music sheets. Our approach determines appropriate media types for a query by searching Google's database of frequent queries for co-occurrences of a query's terms (e.g., "dinosaurs") with preselected multimedia terms (e.g., "coloring pages"). We show the effectiveness of this approach through an online user evaluation.

#index 1450938
#* Query recovery of short user queries: on query expansion with stopwords
#@ Johannes Leveling;Gareth J.F. Jones
#t 2010
#c 13
#% 218978
#% 323131
#% 566935
#% 879692
#% 1237599
#% 1916037
#! User queries to search engines are observed to predominantly contain inflected content words but lack stopwords and capitalization. Thus, they often resemble natural language queries after case folding and stopword removal. Query recovery aims to generate a linguistically well-formed query from a given user query as input to provide natural language processing tasks and cross-language information retrieval (CLIR). The evaluation of query translation shows that translation scores (NIST and BLEU) decrease after case folding, stopword removal, and stemming. A baseline method for query recovery reconstructs capitalization and stopwords, which considerably increases translation scores and significantly increases mean average precision for a standard CLIR task.

#index 1450939
#* Where to start filtering redundancy?: a cluster-based approach
#@ Ronald T. Fernandez;Javier Parapar;David E. Losada;Alvaro Barreiro
#t 2010
#c 13
#% 643014
#% 1074080
#! Novelty detection is a difficult task, particularly at sentence level. Most of the approaches proposed in the past consist of re-ordering all sentences following their novelty scores. However, this re-ordering has usually little value. In fact, a naive baseline with no novelty detection capabilities yields often better performance than any state-of-the-art novelty detection mechanism. We argue here that this is because current methods initiate too early the novelty detection process. When few sentences have been seen, it is unlikely that the user is negatively affected by redundancy. Therefore, re-ordering the first sentences may be harmful in terms of performance. We propose here a query-dependent method based on cluster analysis to determine where we must start filtering redundancy.

#index 1450940
#* Flickr group recommendation based on tensor decomposition
#@ Nan Zheng;Qiudan Li;Shengcai Liao;Leiming Zhang
#t 2010
#c 13
#% 314933
#% 1071127
#% 1083671
#% 1131953
#% 1279896
#! Over the last few years, Flickr has gained massive popularity and groups in Flickr are one of the main ways for photo diffusion. However, the huge volume of groups brings troubles for users to decide which group to choose. In this paper, we propose a tensor decomposition-based group recommendation model to suggest groups to users which can help tackle this problem. The proposed model measures the latent associations between users and groups by considering both semantic tags and social relations. Experimental results show the usefulness of the proposed model.

#index 1450941
#* Robust music identification based on low-order zernike moment in the compressed domain
#@ Wei Li;Yaduo Liu;Xiangyang Xue
#t 2010
#c 13
#% 342720
#! In this paper, we devise a novel robust music identification algorithm utilizing compressed-domain audio Zernike moment adapted from image processing techniques as the pivotal feature. Audio fingerprint derived from this feature exhibits strong robustness against various audio signal distortions including the challenging pitch shifting and time-scale modification. Experiments show that in our test dataset composed of 1822 popular songs, a 5s music query example which might have been severely corrupted is still sufficient to identify its original near-duplicate copy, with more than 90% top five precision rate.

#index 1450942
#* Estimating interference in the QPRP for subtopic retrieval
#@ Guido Zuccon;Leif Azzopardi;Claudia Hauff;C.J. Keith van Rijsbergen
#t 2010
#c 13
#% 642975
#% 786511
#% 1073970
#% 1074133
#% 1166473
#% 1227591
#% 1697443
#! The Quantum Probability Ranking Principle (QPRP) has been recently proposed, and accounts for interdependent document relevance when ranking. However, to be instantiated, the QPRP requires a method to approximate the "interference" between two documents. In this poster, we empirically evaluate a number of different methods of approximation on two TREC test collections for subtopic retrieval. It is shown that these approximations can lead to significantly better retrieval performance over the state of the art.

#index 1450943
#* Query quality: user ratings and system predictions
#@ Claudia Hauff;Franciska de Jong;Diane Kelly;Leif Azzopardi
#t 2010
#c 13
#% 397161
#% 768905
#% 879632
#% 987260
#% 1415713
#! Numerous studies have examined the ability of query performance prediction methods to estimate a query's quality for system effectiveness measures (such as average precision). However, little work has explored the relationship between these methods and user ratings of query quality. In this poster, we report the findings from an empirical study conducted on the TREC ClueWeb09 corpus, where we compared and contrasted user ratings of query quality against a range of query performance prediction methods. Given a set of queries, it is shown that user ratings of query quality correlate to both system effectiveness measures and a number of pre-retrieval predictors.

#index 1450944
#* Multi-field learning for email spam filtering
#@ Wuying Liu;Ting Wang
#t 2010
#c 13
#% 551723
#% 987244
#% 1127962
#% 1860547
#! Through the investigation of email document structure, this paper proposes a multi-field learning (MFL) framework, which breaks the multi-field document Text Classification (TC) problem into several sub-document TC problems, and makes the final category prediction by weighted linear combination of several sub-document TC results. Many previous statistical TC algorithms can be easily rebuilt within the MFL framework via turning binary result to spamminess score, which is a real number and reflects the likelihood that the classified email is spam. The experimental results in the TREC spam track show that the performances of many TC algorithms can be improved within the MFL framework.

#index 1450945
#* Language-model-based pro/con classification of political text
#@ Rawia Awadallah;Maya Ramanath;Gerhard Weikum
#t 2010
#c 13
#% 1127964
#% 1166534
#% 1261563
#% 1261565
#! Given a controversial political topic, our aim is to classify documents debating the topic into pro or con. Our approach extracts topic related terms, pro/con related terms, and pairs of topic related and pro/con related terms and uses them as the basis for constructing a pro query and a con query. Following standard LM techniques, a document is classified as pro or con depending on which of the query likelihoods is higher for the document. Our experiments show that our approach is promising.

#index 1450946
#* Intent boundary detection in search query logs
#@ Chieh-Jen Wang;Kevin Hsin-Yih Lin;Hsin-Hsi Chen
#t 2010
#c 13
#% 1130878
#% 1173690
#! Identifying intent boundary in search query logs is important for learning users' behaviors and applying their experiences. Time-based, query-based, and cluster-based approaches are proposed. Experiments show that the integration of intent clusters and dynamic time model performs the best.

#index 1450947
#* Semi-supervised spam filtering using aggressive consistency learning
#@ Mona Mojdeh;Gordon V. Cormack
#t 2010
#c 13
#% 987245
#% 1074167
#! A graph based semi-supervised method for email spam filtering, based on the local and global consistency method, yields low error rates with very few labeled examples. The motivating application of this method is spam filters with access to very few labeled message. For example, during the initial deployment of a spam filter, only a handful of labeled examples are available but unlabeled examples are plentiful. We demonstrate the performance of our approach on TREC 2007 and CEAS 2008 email corpora. Our results compare favorably with the best-known methods, using as few as just two labeled examples: one spam and one non-spam.

#index 1450948
#* Entropy descriptor for image classification
#@ Hongyu Li;Junyu Niu;Jiachen Chen;Huibo Liu
#t 2010
#c 13
#% 342706
#% 990309
#% 1305503
#! This paper presents a novel entropy descriptor in the sense of geometric manifolds. With this descriptor, entropy cycles can be easily designed for image classification. Minimizing this entropy leads to an optimal entropy cycle where images are connected in the semantic order. During classification, the training step is to find an optimal entropy cycle in each class. In the test step, an unknown image is grouped into a class if the entropy increase as the result of inserting the image into the cycle of this class is relatively least. The proposed approach can generalize well on difficult image classification problems where images with same objects are taken in multiple views. Experimental results show that this entropy descriptor performs well in image classification and has potential in the image-based modeling retrieval.

#index 1450949
#* Has portfolio theory got any principles?
#@ Guido Zuccon;Leif Azzopardi;C.J. "Keith" van Rijsbergen
#t 2010
#c 13
#% 1227591
#% 1450942
#% 1697443
#! Recently, Portfolio Theory (PT) has been proposed for Information Retrieval. However, under non-trivial conditions PT violates the original Probability Ranking Principle (PRP). In this poster, we shall explore whether PT upholds a different ranking principle based on Quantum Theory, i.e. the Quantum Probability Ranking Principle (QPRP), and examine the relationship between this new model and the new ranking principle. We make a significant contribution to the theoretical development of PT and show that under certain circumstances PT upholds the QPRP, and thus guarantees an optimal ranking according to the QPRP. A practical implication of this finding is that the parameters of PT can be automatically estimated via the QPRP, instead of resorting to extensive parameter tuning.

#index 1450950
#* Re-examination on lam% in spam filtering
#@ Haoliang Qi;Muyun Yang;Xiaoning He;Sheng Li
#t 2010
#c 13
#% 840882
#! Logistic average misclassification percentage (lam%) is a key measure for the spam filtering performance. This paper demonstrates that a spam filter can achieve a perfect 0.00% in lam%, the minimal value in theory, by simply setting a biased threshold during the classifier modeling. At the same time, the overall classification performance reaches only a low accuracy. The result suggests that the role of lam% for spam filtering evaluation should be re-examined.

#index 1450951
#* Unsupervised estimation of dirichlet smoothing parameters
#@ Jangwon Seo;W. Bruce Croft
#t 2010
#c 13
#% 340948
#% 818263
#% 913206
#% 1041733
#% 1285404
#! A standard approach for determining a Dirichlet smoothing parameter is to choose a value which maximizes a retrieval performance metric using training data consisting of queries and relevance judgments. There are, however, situations where training data does not exist or the queries and relevance judgments do not reflect typical user information needs for the application. We propose an unsupervised approach for estimating a Dirichlet smoothing parameter based on collection statistics. We show empirically that this approach can suggest a plausible Dirichlet smoothing parameter value in cases where relevance judgments cannot be used.

#index 1450952
#* Comparing click-through data to purchase decisions for retrieval evaluation
#@ Katja Hofmann;Bouke Huurnink;Marc Bron;Maarten de Rijke
#t 2010
#c 13
#% 208931
#% 818221
#% 1130811
#% 1173703
#% 1432775
#! Traditional retrieval evaluation uses explicit relevance judgments which are expensive to collect. Relevance assessments inferred from implicit feedback such as click-through data can be collected inexpensively, but may be less reliable. We compare assessments derived from click-through data to another source of implicit feedback that we assume to be highly indicative of relevance: purchase decisions. Evaluating retrieval runs based on a log of an audio-visual archive, we find agreement between system rankings and purchase decisions to be surprisingly high.

#index 1450953
#* Personalize web search results with user's location
#@ Yumao Lu;Fuchun Peng;Xing Wei;Benoit Dumoulin
#t 2010
#c 13
#% 1190103
#% 1396090
#! We build a probabilistic model to identify implicit local intent queries, and leverage user's physical location to improve Web search results for these queries. Evaluation on commercial search engine shows significant improvement on search relevance and user experience.

#index 1450954
#* Using search session context for named entity recognition in query
#@ Junwu Du;Zhimin Zhang;Jun Yan;Yan Cui;Zheng Chen
#t 2010
#c 13
#% 464434
#% 1227577
#% 1227610
#! Recently, the problem of Named Entity Recognition in Query (NERQ) is attracting increasingly attention in the field of information retrieval. However, the lack of context information in short queries makes some classical named entity recognition (NER) algorithms fail. In this paper, we propose to utilize the search session information before a query as its context to address this limitation. We propose to improve two classical NER solutions by utilizing the search session context, which are known as Conditional Random Field (CRF) based solution and Topic Model based solution respectively. In both approaches, the relationship between current focused query and previous queries in the same session are used to extract novel context aware features. Experimental results on real user search session data show that the NERQ algorithms using search session context performs significantly better than the algorithms using only information of the short queries.

#index 1450955
#* Evaluating whole-page relevance
#@ Peter Bailey;Nick Craswell;Ryen W. White;Liwei Chen;Ashwin Satyanarayana;S.M.M. Tahaghoghi
#t 2010
#c 13
#% 857180
#% 987209
#% 989668
#% 1126946
#% 1227640
#! Whole page relevance defines how well the surface-level repre-sentation of all elements on a search result page and the corre-sponding holistic attributes of the presentation respond to users' information needs. We introduce a method for evaluating the whole-page relevance of Web search engine results pages. Our key contribution is that the method allows us to investigate aspects of component relevance that are difficult or impossible to judge in isolation. Such aspects include component-level information redundancy and cross-component coherence. The method we describe complements traditional document relevance measurement, affords comparative relevance assessment across multiple search engines, and facilitates the study of important factors such as brand presentation effects and component-level quality.

#index 1450956
#* Predicting escalations of medical queries based on web page structure and content
#@ Ryen W. White;Eric Horvitz
#t 2010
#c 13
#% 751861
#% 1183067
#% 1278069
#! Logs of users' searches on Web health topics can exhibit signs of escalation of medical concerns, where initial queries about common symptoms are followed by queries about serious, rare illnesses. We present an effort to predict such escalations based on the structure and content of pages encountered during medical search sessions. We construct and then characterize the performance of classifiers that predict whether an escalation will occur after the access of a page. Our findings have implications for ranking algorithms and the design of search interfaces.

#index 1450957
#* Contextual video advertising system using scene information inferred from video scripts
#@ Bong-Jun Yi;Jung-Tae Lee;Hyun-Wook Woo;Hae-Chang Rim
#t 2010
#c 13
#% 789959
#% 869484
#% 1227685
#! With the rise of digital video consumptions, contextual video advertising demands have been increasing in recent years. This paper presents a novel video advertising system that selects relevant text ads for a given video scene by automatically identifying the situation of the scene. The situation information of video scenes is inferred from available video scripts. Experimental results show that the use of the situation information enhances the accuracy of ad retrieval for video scenes. The proposed system represents one of the pioneer video advertising systems using contextual information obtained from video scripts.

#index 1450958
#* Cross-language retrieval using link-based language models
#@ Benjamin Roth;Dietrich Klakow
#t 2010
#c 13
#% 561145
#% 1250010
#% 1412035
#% 1432220
#! We propose a cross-language retrieval model that is solely based on Wikipedia as a training corpus. The main contributions of our work are: 1. A translation model based on linked text in Wikipedia and a term weighting method associated with it. 2. A combination scheme to interpolate the link translation model with retrieval based on Latent Dirichlet Allocation. On the CLEF 2000 data we achieve improvement with respect to the best German-English system at the bilingual track (non-significant) and improvement against a baseline based on machine translation (significant).

#index 1450959
#* Search system requirements of patent analysts
#@ Leif Azzopardi;Wim Vanderbauwhede;Hideo Joho
#t 2010
#c 13
#% 1131207
#! Patent search tasks are difficult and challenging, often requiring expert patent analysts to spend hours, even days, sourcing relevant information. To aid them in this process, analysts use Information Retrieval systems and tools to cope with their retrieval tasks. With the growing interest in patent search, it is important to determine their requirements and expectations of the tools and systems that they employ. In this poster, we report a subset of the findings of a survey of patent analysts conducted to elicit their search requirements.

#index 1450960
#* On performance of topical opinion retrieval
#@ Giambattista Amati;Giuseppe Amodeo;Valerio Capozio;Carlo Gaibisso;Giorgio Gambosi
#t 2010
#c 13
#% 1127964
#% 1261565
#% 1292562
#% 1392464
#% 1415716
#! We investigate the effectiveness of both the standard evaluation measures and the opinion component for topical opinion retrieval. We analyze how relevance is affected by opinions by perturbing relevance ranking by the outcomes of opinion-only classifiers built by Monte Carlo sampling. Topical opinion rankings are obtained by either re-ranking or filtering the documents of a first-pass retrieval of topic relevance. The proposed approach establishes the correlation between the accuracy and the precision of the classifier and the performance of the topical opinion retrieval. Among other results, it is possible to assess the effectiveness of the opinion component by comparing the effectiveness of the relevance baseline with the topical opinion ranking.

#index 1450961
#* Improving sentence retrieval with an importance prior
#@ Leif Azzopardi;Ronald T. Fernández;David E. Losada
#t 2010
#c 13
#% 643014
#% 814952
#% 838537
#% 980527
#% 1404888
#! The retrieval of sentences is a core task within Information Retrieval. In this poster we employ a Language Model that incorporates a prior which encodes the importance of sentences within the retrieval model. Then, in a set of comprehensive experiments using the TREC Novelty Tracks, we show that including this prior substantially improves retrieval effectiveness, and significantly outperforms the current state of the art in sentence retrieval.

#index 1450962
#* Focused access to sparsely and densely relevant documents
#@ Paavo Arvola;Jaana Kekäläinen;Marko Junkkari
#t 2010
#c 13
#% 878916
#% 1074218
#% 1483555
#! XML retrieval provides a focused access to the relevant content of documents. However, in evaluation, full document retrieval has appeared competitive to focused XML retrieval. We analyze the density of relevance in documents, and show that in sparsely relevant documents focused retrieval performs better, whereas in densely relevant documents the performance of focused and document retrieval is equal.

#index 1450963
#* Text document clustering with metric learning
#@ Jinlong Wang;Shunyao Wu;Huy Quan Vu;Gang Li
#t 2010
#c 13
#% 464291
#% 769881
#% 1176863
#% 1663626
#! One reason for semi-supervised clustering fail to deliver satisfactory performance in document clustering is that the transformed optimization problem could have many candidate solutions, but existing methods provide no mechanism to select a suitable one from all those candidates. This paper alleviates this problem by posing the same task as a soft-constrained optimization problem, and introduces the salient degree measure as an information guide to control the searching of an optimal solution. Experimental results show the effectiveness of the proposed method in the improvement of the performance, especially when the amount of priori domain knowledge is limited.

#index 1450964
#* Predicting query performance on the web
#@ Niranjan Balasubramanian;Giridhar Kumaran;Vitor R. Carvalho
#t 2010
#c 13
#% 397161
#% 818267
#% 879567
#! Predicting the performance of web queries is useful for several applications such as automatic query reformulation and automatic spell correction. In the web environment, accurate performance prediction is challenging because measures such as clarity that work well on homogeneous TREC-like collections, are not as effective and are often expensive to compute. We present Rank-time Performance Prediction (RAPP), an effective and efficient approach for online performance prediction on the web. RAPP uses retrieval scores, and aggregates of the rank-time features used by the document- ranking algorithm to train regressors for query performance prediction. On a set of over 12,000 queries sampled from the query logs of a major search engine, RAPP achieves a linear correlation of 0.78 with DCG@5, and 0.52 with NDCG@5. Analysis of prediction accuracy shows that hard queries are easier to identify while easy queries are harder to identify.

#index 1450965
#* Hashtag retrieval in a microblogging environment
#@ Miles Efron
#t 2010
#c 13
#% 342707
#% 1133171
#% 1392466
#! Microblog services let users broadcast brief textual messages to people who "follow" their activity. Often these posts contain terms called hashtags, markers of a post's meaning, audience, etc. This poster treats the following problem: given a user's stated topical interest, retrieve useful hashtags from microblog posts. Our premise is that a user interested in topic x might like to find hashtags that are often applied to posts about x. This poster proposes a language modeling approach to hashtag retrieval. The main contribution is a novel method of relevance feedback based on hashtags. The approach is tested on a corpus of data harvested from twitter.com.

#index 1450966
#* Crowdsourcing a wikipedia vandalism corpus
#@ Martin Potthast
#t 2010
#c 13
#% 1016107
#% 1227753
#% 1355288
#% 1415780
#! We report on the construction of the PAN Wikipedia vandalism corpus, PAN-WVC-10, using Amazon's Mechanical Turk. The corpus compiles 32452 edits on 28468 Wikipedia articles, among which 2391 vandalism edits have been identified. 753 human annotators cast a total of 193022 votes on the edits, so that each edit was reviewed by at least 3 annotators, whereas the achieved level of agreement was analyzed in order to label an edit as "regular" or "vandalism." The corpus is available free of charge.

#index 1450967
#* MEMOSE: search engine for emotions in multimedia documents
#@ Kathrin Knautz;Tobias Siebenlist;Wolfgang G. Stock
#t 2010
#c 13
#% 867259
#% 1203773
#% 1380097
#% 1434478
#! The MEMOSE (Media Emotion Search) system is a specialized search engine for fundamental emotions in all kinds of emotional-laden documents. We apply a controlled vocabulary for basic emotions, a slide control to adjust the intensities of the emotions and the approach of broad folksonomies. The paper describes the indexing and the retrieval tool of MEMOSE and results from its evaluation.

#index 1450968
#* Hierarchical pitman-yor language model for information retrieval
#@ Saeedeh Momtazi;Dietrich Klakow
#t 2010
#c 13
#% 262096
#% 340948
#% 939624
#! In this paper, we propose a new application of Bayesian language model based on Pitman-Yor process for information retrieval. This model is a generalization of the Dirichlet distribution. The Pitman-Yor process creates a power-law distribution which is one of the statistical properties of word frequency in natural language. Our experiments on Robust04 indicate that this model improves the document retrieval performance compared to the commonly used Dirichlet prior and absolute discounting smoothing techniques.

#index 1450969
#* Entity summarization of news articles
#@ Gianluca Demartini;Malik Muhammad Saad Missen;Roi Blanco;Hugo Zaragoza
#t 2010
#c 13
#% 818255
#% 1343447
#% 1415743
#! In this paper we study the problem of entity retrieval for news applications and the importance of the news trail history (i.e. past related articles) to determine the relevant entities in current articles. We construct a novel entity-labeled corpus with temporal information out of the TREC 2004 Novelty collection. We develop and evaluate several features, and show that an article's history can be exploited to improve its summarization.

#index 1450970
#* The power of naive query segmentation
#@ Matthias Hagen;Martin Potthast;Benno Stein;Christof Braeutigam
#t 2010
#c 13
#% 869501
#% 1055706
#% 1074098
#% 1215246
#% 1227747
#% 1310431
#% 1697475
#! We address the problem of query segmentation: given a keyword query submitted to a search engine, the task is to group the keywords into phrases, if possible. Previous approaches to the problem achieve good segmentation performance on a gold standard but are fairly intricate. Our method is easy to implement and comes with a comparable accuracy.

#index 1450971
#* Clicked phrase document expansion for sponsored search ad retrieval
#@ Dustin Hillard;Chris Leggetter
#t 2010
#c 13
#% 348155
#% 730065
#% 956546
#% 1227648
#! We present a document expansion approach that uses Conditional Random Field (CRF) segmentation to automatically extract salient phrases from ad titles. We then supplement the ad document with query segments that are probable translations of the document phrases, as learned from a large commercial search engine's click logs. Our approach provides a significant improvement in DCG and interpolated precision and recall on a large set of human labeled query-ad pairs.

#index 1450972
#* Three web-based heuristics to determine a person's or institution's country of origin
#@ Markus Schedl;Klaus Seyerlehner;Dominik Schnitzer;Gerhard Widmer;Cornelia Schiketanz
#t 2010
#c 13
#% 375017
#% 387427
#% 987247
#% 987248
#! We propose three heuristics to determine the country of origin of a person or institution via text-based IE from the Web. We evaluate all methods on a collection of music artists and bands, and show that some heuristics outperform earlier work on the topic by terms of coverage, while retaining similar precision levels. We further investigate an extension using country-specific synonym lists.

#index 1450973
#* Exploiting click-through data for entity retrieval
#@ Bodo Billerbeck;Gianluca Demartini;Claudiu Firan;Tereza Iofciu;Ralf Krestel
#t 2010
#c 13
#% 987222
#% 989578
#% 1019135
#% 1022234
#% 1214758
#! We present an approach for answering Entity Retrieval queries using click-through information in query log data from a commercial Web search engine. We compare results using click graphs and session graphs and present an evaluation test set making use of Wikipedia "List of" pages.

#index 1450974
#* Feature subset non-negative matrix factorization and its applications to document understanding
#@ Dingding Wang;Chris Ding;Tao Li
#t 2010
#c 13
#% 340884
#% 724227
#% 787502
#% 816173
#% 915294
#% 983869
#! In this paper, we propose feature subset non-negative matrix factorization (NMF), which is an unsupervised approach to simultaneously cluster data points and select important features. We apply our proposed approach to various document understanding tasks including document clustering, summarization, and visualization. Experimental results demonstrate the effectiveness of our approach for these tasks.

#index 1450975
#* Learning to rank query reformulations
#@ Van Dang;Michael Bendersky;W. Bruce Croft
#t 2010
#c 13
#% 397161
#% 869501
#% 987241
#% 1130855
#% 1227647
#% 1355020
#% 1415713
#! Query reformulation techniques based on query logs have recently proven to be effective for web queries. However, when initial queries have reasonably good quality, these techniques are often not reliable enough to identify the helpful reformulations among the suggested queries. In this paper, we show that we can use as few as two features to rerank a list of reformulated queries, or expanded queries to be specific, generated by a log-based query reformulation technique. Our results across five TREC collections suggest that there are consistently more useful reformulations in the first two positions in the new ranked list than there were initially, which leads to statistically significant improvements in retrieval effectiveness.

#index 1450976
#* Many are better than one: improving multi-document summarization via weighted consensus
#@ Dingding Wang;Tao Li
#t 2010
#c 13
#% 340884
#% 787502
#% 816173
#% 1100099
#! Given a collection of documents, various multi-document summarization methods have been proposed to generate a short summary. However, few studies have been reported on aggregating different summarization methods to possibly generate better summarization results. We propose a weighted consensus summarization method to combine the results from single summarization systems. Experimental results on DUC2004 data sets demonstrate the performance improvement by aggregating multiple summarization systems, and our proposed weighted consensus summarization method outperforms other combination methods.

#index 1450977
#* Exploring the use of labels to shortcut search trails
#@ Ryen W. White;Raman Chandrasekar
#t 2010
#c 13
#% 27049
#% 280840
#% 340928
#% 754125
#% 823348
#% 956495
#% 956544
#% 987212
#% 1292493
#! Search trails comprising queries and Web page views are created as searchers engage in information-seeking activity online. During known-item search (where the objective may be to locate a target Web page), searchers may waste valuable time repeatedly reformulating queries as they attempt to locate an elusive page. Trail shortcuts help users bypass unnecessary queries and get them to their desired destination faster. In this poster we present a comparative oracle study of techniques to shortcut sub-optimal search trails using labels derived from social bookmarking, anchor text, query logs, and a human-computation game. We show that labels can help users reach target pages efficiently, that the label sources perform differently, and that shortcuts are potentially most useful when the target is challenging to find.

#index 1450978
#* Investigating the suboptimality and instability of pseudo-relevance feedback
#@ Raghavendra Udupa;Abhijit Bhole
#t 2010
#c 13
#% 342707
#% 750863
#% 1074081
#! Although Pseudo-Relevance Feedback (PRF) techniques improve average retrieval performance at the price of high variance, not much is known about their optimality and the reasons for their instability. In this work, we study more than 800 topics from several test collections including the TREC Robust Track and show that PRF techniques are highly suboptimal, i.e. they do not make the fullest utilization of pseudo-relevant documents and under-perform. A careful selection of expansion terms from the pseudo-relevant document with the help of an oracle can actually improve retrieval performance dramatically (by 60%). Further, we show that instability in PRF techniques is mainly due to wrong selection of expansion terms from the pseudo-relevant documents. Our findings emphasize the need to revisit the problem of term selection to make a break through in PRF.

#index 1450979
#* From fusion to re-ranking: a semantic approach
#@ Annalina Caputo;Pierpaolo Basile;Giovanni Semeraro
#t 2010
#c 13
#% 232703
#% 342617
#% 375017
#% 1019124
#% 1271309
#% 1271329
#% 1432216
#% 1432236
#% 1494769
#% 1494773
#! A number of works have shown that the aggregation of several Information Retrieval (IR) systems works better than each system working individually. Nevertheless, early investigation in the context of CLEF Robust-WSD task, in which semantics is involved, showed that aggregation strategies achieve only slight improvements. This paper proposes a re-ranking approach which relies on inter-document similarities. The novelty of our idea is twofold: the output of a semantic based IR system is exploited to re-weigh documents and a new strategy based on Semantic Vectors is used to compute inter-document similarities.

#index 1450980
#* High precision opinion retrieval using sentiment-relevance flows
#@ Seung-Wook Lee;Jung-Tae Lee;Young-In Song;Hae-Chang Rim
#t 2010
#c 13
#% 1074102
#% 1227689
#! Opinion retrieval involves the measuring of opinion score of a document about the given topic. We propose a new method, namely sentiment-relevance flow, that naturally unifies the topic relevance and the opinionated nature of a document. Experiments conducted over a large-scaled Web corpus show that the proposed approach improves performance of opinion retrieval in terms of precision at top ranks.

#index 1450981
#* Ontology-enriched multi-document summarization in disaster management
#@ Lei Li;Dingding Wang;Chao Shen;Tao Li
#t 2010
#c 13
#% 935763
#% 1130998
#% 1498105
#% 1669905
#! In this poster, we propose a novel document summarization approach named Ontology-enriched Multi-Document Summarization(OMS) for utilizing background knowledge to improve summarization results. OMS first maps the sentences of input documents onto an ontology, then links the given query to a specific node in the ontology, and finally extracts the summary from the sentences in the subtree rooted at the query node. By using the domain-related ontology, OMS can better capture the semantic relevance between the query and the sentences, and thus lead to better summarization results. As a byproduct, the final summary generated by OMS can be represented as a tree showing the hierarchical relationships of the extracted sentences. Evaluation results on the collection of press releases by Miami-Dade County Department of Emergency Management during Hurricane Wilma in 2005 demonstrate the efficacy of OMS.

#index 1450982
#* Multi-view clustering of multilingual documents
#@ Young-Min Kim;Massih-Reza Amini;Cyril Goutte;Patrick Gallinari
#t 2010
#c 13
#% 329569
#% 397139
#% 722904
#% 785334
#% 1211706
#% 1227710
#! We propose a new multi-view clustering method which uses clustering results obtained on each view as a voting pattern in order to construct a new set of multi-view clusters. Our experiments on a multilingual corpus of documents show that performance increases significantly over simple concatenation and another multi-view clustering technique.

#index 1450983
#* A stack decoder approach to approximate string matching
#@ Juan M. Huerta
#t 2010
#c 13
#% 174161
#% 333679
#% 817876
#% 854061
#% 1306538
#! We present a new efficient algorithm for top-N match retrieval of sequential patterns. Our approach is based on an incremental approximation of the string edit distance using index information and a stack based search. Our approach produces hypotheses with average edit error of about 0.29 edits from the optimal SED result while using only about 5% of the CPU computation.

#index 1450984
#* Late fusion of compact composite descriptors for retrieval from heterogeneous image databases
#@ Savvas A. Chatzichristofis;Avi Arampatzis
#t 2010
#c 13
#% 883971
#% 1040539
#% 1292546
#% 1350606
#% 1665814
#! Compact composite descriptors (CCDs) are global image features, capturing more than one types of information at the same time in a very compact representation. Their quality has so far been evaluated in retrieval from several homogeneous databases containing images of only the type that each CCD is intended for, and has been found better than other descriptors in the literature such as the MPEG-7 descriptors. In this study, we consider heterogeneous databases and investigate query-time fusion techniques for CCDs. The results show that fusion is beneficial, even with simple score normalization and combination methods due to the compatibility of the score distributions produced by the CCDs considered.

#index 1450985
#* Inferring user intent in web search by exploiting social annotations
#@ Jose M. Conde;David Vallet;Pablo Castells
#t 2010
#c 13
#% 822126
#% 956544
#% 1074070
#% 1192277
#! In this paper, we present a folksonomy-based approach for implicit user intent extraction during a Web search process. We present a number of result re-ranking techniques based on this representation that can be applied to any Web search engine. We perform a user experiment the results of which indicate that this type of representation is better at context extraction than using the actual textual content of the document.

#index 1450986
#* Query term ranking based on dependency parsing of verbose queries
#@ Jae Hyun Park;W. Bruce Croft
#t 2010
#c 13
#% 577224
#% 1074112
#% 1227647
#% 1292594
#! Query term ranking approaches are used to select effective terms from a verbose query by ranking terms. Features used for query term ranking and selection in previous work do not consider grammatical relationships between terms. To address this issue, we use syntactic features extracted from dependency parsing results of verbose queries. We also modify the method for measuring the effectiveness of query terms for query term ranking.

#index 1450987
#* A ranking approach to target detection for automatic link generation
#@ Jiyin He;Maarten de Rijke
#t 2010
#c 13
#% 818262
#% 987241
#% 1019082
#% 1130858
#! We focus on the task of target detection in automatic link generation with Wikipedia, i.e., given an N-gram in a snippet of text, find the relevant Wikipedia concepts that explain or provide background knowledge for it. We formulate the task as a ranking problem and investigate the effectiveness of learning to rank approaches and of the features that we use to rank the target concepts for a given N-gram. Our experiments show that learning to rank approaches outperform traditional binary classification approaches. Also, our proposed features are effective both in binary classification and learning to rank settings.

#index 1450988
#* Probabilistic latent maximal marginal relevance
#@ Shengbo Guo;Scott Sanner
#t 2010
#c 13
#% 262112
#% 406493
#% 722904
#% 1074025
#% 1190056
#! Diversity has been heavily motivated in the information retrieval literature as an objective criterion for result sets in search and recommender systems. Perhaps one of the most well-known and most used algorithms for result set diversification is that of Maximal Marginal Relevance (MMR). In this paper, we show that while MMR is somewhat ad-hoc and motivated from a purely pragmatic perspective, we can derive a more principled variant via probabilistic inference in a latent variable graphical model. This novel derivation presents a formal probabilistic latent view of MMR (PLMMR) that (a) removes the need to manually balance relevance and diversity parameters, (b) shows that specific definitions of relevance and diversity metrics appropriate to MMR emerge naturally, and (c) formally derives variants of latent semantic indexing (LSI) similarity metrics for use in PLMMR. Empirically, PLMMR outperforms MMR with standard term frequency based similarity and diversity metrics since PLMMR maximizes latent diversity in the results.

#index 1450989
#* Using local precision to compare search engines in consumer health information retrieval
#@ Carla Teixeira Lopes;Cristina Ribeiro
#t 2010
#c 13
#% 1077150
#! We have conducted a user study to evaluate several generalist and health-specific search engines on health information retrieval. Users evaluated the relevance of the top 30 documents of 4 search engines in two different health information needs. We introduce the concepts of local and global precision and analyze how they affect the evaluation. Results show that Google surpasses the precision of all other engines, including the health-specific ones, and that precision differs with the type of clinical question and its medical specialty.

#index 1450990
#* multi Searcher: can we support people to get information from text they can't read or understand?
#@ Farag Ahmed;Andreas Nürnberger
#t 2010
#c 13
#% 306485
#% 607985
#% 1015007
#! The goal of the proposed tool multi Searcher is to answer this research question: can we expect people to be able to get information from text in languages they can not read or understand? The proposed tool multi Searcher provides users with interactive contextual information that describes the translation in the user's own language so that the user has a certain degree of confidence about the translation. Therefore, the user is considered as an integral part of the retrieval process. The tool provides possibilities to interactively select relevant terms from contextual information in order to improve the translation and thus improve the cross lingual information retrieval (CLIR) process.

#index 1450991
#* Linking wikipedia to the web
#@ Rianne Kaptein;Pavel Serdyukov;Jaap Kamps
#t 2010
#c 13
#% 340928
#% 397126
#% 1100832
#! We investigate the task of finding links from Wikipedia pages to external web pages. Such external links significantly extend the information in Wikipedia with information from the Web at large, while retaining the encyclopedic organization of Wikipedia. We use a language modeling approach to create a full-text and anchor text runs, and experiment with different document priors. In addition we explore whether social bookmarking site Delicious can be exploited to further improve our performance. We have constructed a test collection of 53 topics, which are Wikipedia pages on different entities. Our findings are that the anchor text index is a very effective method to retrieve home pages. Url class and anchor text length priors and their combination leads to the best results. Using Delicious on its own does not lead to very good results, but it does contain valuable information. Combining the best anchor text run and the Delicious run leads to further improvements.

#index 1450992
#* Short text classification in twitter to improve information filtering
#@ Bharath Sriram;Dave Fuhry;Engin Demir;Hakan Ferhatosmanoglu;Murat Demirbas
#t 2010
#c 13
#% 987328
#% 1040837
#% 1055680
#% 1074225
#% 1292559
#% 1298864
#! In microblogging services such as Twitter, the users may become overwhelmed by the raw data. One solution to this problem is the classification of short text messages. As short texts do not provide sufficient word occurrences, traditional classification methods such as "Bag-Of-Words" have limitations. To address this problem, we propose to use a small set of domain-specific features extracted from the author's profile and text. The proposed approach effectively classifies the text to a predefined set of generic classes such as News, Events, Opinions, Deals, and Private Messages.

#index 1450993
#* A framework for BM25F-based XML retrieval
#@ Kelly Y. Itakura;Charles L.A. Clarke
#t 2010
#c 13
#% 783474
#% 801831
#% 878916
#% 1243045
#% 1263220
#% 1263231
#% 1415728
#% 1674728
#% 1721850
#% 1733301
#! We evaluate a framework for BM25F-based XML element retrieval. The framework gathers contextual information associated with each XML element into an associated field, which we call a characteristic field. The contents of the element and the contents of the characteristic field are then treated as distinct fields for BM25F weighting purposes. Evidence supporting this framework is drawn from both our own experiments and experiments reported in related work.

#index 1450994
#* Can search systems detect users' task difficulty?: some behavioral signals
#@ Jingjing Liu;Chang Liu;Jacek Gwizdka;Nicholas J. Belkin
#t 2010
#c 13
#% 56825
#% 860649
#% 907516
#% 1384094
#! In this paper, we report findings on how user behaviors vary in tasks with different difficulty levels as well as of different types. Two behavioral signals: document dwell time and number of content pages viewed per query, were found to be able to help the system detect when users are working with difficult tasks.

#index 1450995
#* Query log analysis in the context of information retrieval for children
#@ Sergio Duarte Torres;Djoerd Hiemstra;Pavel Serdyukov
#t 2010
#c 13
#% 320342
#% 575733
#% 878624
#% 1130878
#% 1210687
#! In this paper we analyze queries and sessions intended to satisfy children's information needs using a large-scale query log. The aim of this analysis is twofold: i) To identify differences between such queries and sessions, and general queries and sessions; ii) To enhance the query log by including annotations of queries, sessions, and actions for future research on information retrieval for children. We found statistically significant differences between the set of general purpose and queries seeking for content intended for children. We show that our findings are consistent with previous studies on the physical behavior of children using Web search engines.

#index 1450996
#* Transitive history-based query disambiguation for query reformulation
#@ Karim Filali;Anish Nair;Chris Leggetter
#t 2010
#c 13
#% 838547
#% 869501
#% 881540
#% 1083721
#% 1130868
#% 1190074
#! We present a probabilistic model of a user's search history and a target query reformulation. We derive a simple transitive similarity algorithm for disambiguating queries and improving history-based query reformulation accuracy. We compare the merits of this approach to other methods and present results on both examples assessed by human editors and on automatically-labeled click data.

#index 1450997
#* Using flickr geotags to predict user travel behaviour
#@ Maarten Clements;Pavel Serdyukov;Arjen P. de Vries;Marcel J.T. Reinders
#t 2010
#c 13
#% 411762
#% 987205
#% 1190131
#% 1400036
#% 1695791
#! We propose a method to predict a user's favourite locations in a city, based on his Flickr geotags in other cities. We define a similarity between the geotag distributions of two users based on a Gaussian kernel convolution. The geotags of the most similar users are then combined to rerank the popular locations in the target city personalised for this user. We show that this method can give personalised travel recommendations for users with a clear preference for a specific type of landmark.

#index 1450998
#* Metrics for assessing sets of subtopics
#@ Filip Radlinski;Martin Szummer;Nick Craswell
#t 2010
#c 13
#% 262112
#% 642975
#% 879618
#% 1074025
#% 1074133
#% 1312812
#! To evaluate the diversity of search results, test collections have been developed that identify multiple intents for each query. Intents are the different meanings or facets that should be covered in a search results list. This means that topic development involves proposing a set of intents for each query. We propose four measurable properties of query-to-intent mappings, allowing for more principled topic development for such test collections.

#index 1450999
#* Learning to select rankers
#@ Niranjan Balasubramanian;James Allan
#t 2010
#c 13
#% 169774
#% 194276
#% 348304
#% 397161
#% 734915
#% 879582
#% 987240
#% 987266
#! Combining evidence from multiple retrieval models has been widely studied in the context of of distributed search, metasearch and rank fusion. Much of the prior work has focused on combining retrieval scores (or the rankings) assigned by different retrieval models or ranking algorithms. In this work, we focus on the problem of choosing between retrieval models using performance estimation. We propose modeling the differences in retrieval performance directly by using rank-time features - features that are available to the ranking algorithms - and the retrieval scores assigned by the ranking algorithms. Our experimental results show that when choosing between two rankers, our approach yields significant improvements over the best individual ranker.

#index 1451000
#* VisualSum: an interactive multi-document summarizationsystem using visualization
#@ Yi Zhang;Dingding Wang;Tao Li
#t 2010
#c 13
#% 340884
#% 816173
#% 1292520
#! Given a collection of documents, most of existing multidocument summarization methods automatically generate a static summary for all the users. However, different users may have different opinions on the documents, thus there is a necessity for improving users' interactions in the summarization process. In this paper, we propose an interactive document summarization system using information visualization techniques.

#index 1451001
#* Web page publication time detection and its application for page rank
#@ Zhumin Chen;Jun Ma;Chaoran Cui;Hongxing Rui;Shaomang Huang
#t 2010
#c 13
#% 864619
#% 960414
#% 987251
#% 1016336
#% 1024551
#% 1161152
#% 1227692
#! Publication Time (P-time for short) of Web pages is often required in many application areas. In this paper, we address the issue of P-time detection and its application for page rank. We first propose an approach to extract P-time for a page with explicit P-time displayed on its body. We then present a method to infer P-time for a page without P-time. We further introduce a temporal sensitive page rank model using P-time. Experiments demonstrate that our methods outperform the baseline methods significantly.

#index 1451002
#* HCC: a hierarchical co-clustering algorithm
#@ Jingxuan Li;Tao Li
#t 2010
#c 13
#% 118771
#% 881468
#% 989596
#% 1074074
#! In this poster, we develop a novel method, called HCC, for hierarchical co-clustering. HCC brings together two interrelated but distinct themes from clustering: hierarchical clustering and co-clustering. The goal of the former theme is to organize clusters into a hierarchy that facilitates browsing and navigation, while the goal of the latter theme is to cluster different types of data simultaneously by making use of the relationship information. Our initial empirical results are promising and they demonstrate that simultaneously attempting both these goals in a single model leads to improvements over models that focus on a single goal.

#index 1451003
#* Retrieval system evaluation: automatic evaluation versus incomplete judgments
#@ Claudia Hauff;Franciska de Jong
#t 2010
#c 13
#% 340890
#% 643020
#% 879598
#% 879632
#% 1697427
#! In information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with reduced amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The proposed methods in both areas are commonly evaluated by comparing their performance estimates for a set of systems to a ground truth (provided for instance by evaluating the set of systems according to mean average precision). In contrast, in this poster we compare an automatic system evaluation approach directly to two evaluations based on incomplete manual relevance assessments. For the particular case of TREC's Million Query track, we show that the automatic evaluation leads to results which are highly correlated to those achieved by approaches relying on incomplete manual judgments.

#index 1451004
#* Aspect presence verification conditional on other aspects
#@ Dmitri Roussinov
#t 2010
#c 13
#% 746929
#% 766525
#% 838532
#% 879584
#% 989590
#% 1114500
#% 1392482
#! I have shown that the presence of difficult query aspects that are revealed only implicitly (e.g. exploration, opposition, achievements, cooperation, risks) can be improved by taking advantage of the known presence of other, easier to verify query aspects. The approach proceeds by mining a large external corpus and results in substantial improvements in re-ranking the subset of the top retrieved documents.

#index 1451005
#* The value of visual elements in web search
#@ Marilyn Ostergren;Seung-yon Yu;Efthimis N. Efthimiadis
#t 2010
#c 13
#% 452622
#% 954948
#% 954949
#% 1047419
#% 1053505
#! We used eye-tracking equipment to observe 36 participants as they performed three search tasks using three graphically-enhanced web search interfaces (Kartoo, SearchMe and Viewzi). In this poster we describe findings of the study focusing on how the presentation of SERP results influences how the user scans and attends to the results, and the user satisfaction with these search engines.

#index 1451006
#* Diversification of search results using webgraphs
#@ Praveen Chandar;Ben Carterette
#t 2010
#c 13
#% 262112
#% 290830
#% 642975
#% 1074133
#% 1166473
#% 1292596
#! A set of words is often insufficient to express a user's information need. In order to account for various information needs associated with a query, diversification seems to be a reasonable strategy. By diversifying the result set, we increase the probability of results being relevant to the user's information needs when the given query is ambiguous. A diverse result set must contain a set of documents that cover various subtopics for a given query. We propose a graph based method which exploits the link structure of the web to return a ranked list that provides complete coverage for a query. Our method not only provides diversity to the results set, but also avoids excessive redundancy. Moreover, the probability of relevance of a document is conditioned on the documents that appear before it in the result list. We show the effectiveness of our method by comparing it with a query-likelihood model as the baseline.

#index 1451007
#* Capturing page freshness for web search
#@ Na Dai;Brian D. Davison
#t 2010
#c 13
#% 268079
#% 766462
#% 805839
#% 1450843
#! Freshness has been increasingly realized by commercial search engines as an important criteria for measuring the quality of search results. However, most information retrieval methods focus on the relevance of page content to given queries without considering the recency issue. In this work, we mine page freshness from web user maintenance activities and incorporate this feature into web search. We first quantify how fresh the web is over time from two distinct perspectives--the page itself and its in-linked pages--and then exploit a temporal correlation between two types of freshness measures to quantify the confidence of page freshness. Results demonstrate page freshness can be better quantified when combining with temporal freshness correlation. Experiments on a real-world archival web corpus show that incorporating the combined page freshness into the searching process can improve ranking performance significantly on both relevance and freshness.

#index 1451008
#* S-PLASA+: adaptive sentiment analysis with application to sales performance prediction
#@ Yang Liu;Xiaohui Yu;Xiangji Huang;Aijun An
#t 2010
#c 13
#% 823332
#% 987268
#% 990210
#% 1767909
#! Analyzing the large volume of online reviews would produce useful knowledge that could be of economic values to vendors and other interested parties. In particular, the sentiments expressed in the online reviews have been shown to be strongly correlated with the sales performance of products. In this paper, we present an adaptive sentiment analysis model called S-PLSA+, which aims to capture the hidden sentiment factors in the reviews with the capability to be incrementally updated as more data become available. We show how S-PLSA+ can be applied to sales performance prediction using an ARSA model developed in previous literature. A case study is conducted in the movie domain, and results from preliminary experiments confirm the effectiveness of the proposed model.

#index 1451009
#* Supervised query modeling using wikipedia
#@ Edgar Meij;Maarten de Rijke
#t 2010
#c 13
#% 818221
#% 879584
#% 1074132
#% 1074133
#% 1166473
#% 1227584
#% 1330556
#% 1715627
#! We use Wikipedia articles to semantically inform the generation of query models. To this end, we apply supervised machine learning to automatically link queries to Wikipedia articles and sample terms from the linked articles to re-estimate the query model. On a recent large web corpus, we observe substantial gains in terms of both traditional metrics and diversity measures.

#index 1451010
#* A two-stage model for blog feed search
#@ Wouter Weerkamp;Krisztian Balog;Maarten de Rijke
#t 2010
#c 13
#% 1074094
#% 1074171
#% 1130913
#% 1130914
#% 1223498
#! We consider blog feed search: identifying relevant blogs for a given topic. An individual's search behavior often involves a combination of exploratory behavior triggered by salient features of the information objects being examined plus goal-directed in-depth information seeking behavior. We present a two-stage blog feed search model that directly builds on this insight. We first rank blog posts for a given topic, and use their parent blogs as selection of blogs that we rank using a blog-based model.

#index 1451011
#* Machine learned ranking of entity facets
#@ Roelof van Zwol;Lluís Garcia Pueyo;Mridul Muralidharan;Börkur Sigurbjörnsson
#t 2010
#c 13
#% 1035578
#% 1400029
#! The research described in this paper forms the backbone of a service that enables the faceted search experience of the Yahoo! search engine. We introduce an approach for a machine learned ranking of entity facets based on user click feedback and features extracted from three different ranking sources. The objective of the learned model is to predict the click-through rate on an entity facet. In an empirical evaluation we compare the performance of gradient boosted decision trees (GBDT) against a linear combination of features on two different click feedback models using the raw click-through rate (CTR), and click over expected clicks (COEC). The results show a significant improvement in retrieval performance, in terms of discounted cumulated gain, when ranking entity facets with GBDT trained on the COEC model. Most notably this is true when evaluated against the CTR test set.

#index 1451012
#* User comments for news recommendation in social media
#@ Jia Wang;Qing Li;Yuanzhu Peter Chen
#t 2010
#c 13
#% 268079
#% 316548
#% 808761
#% 1001296
#! Reading and Commenting online news is becoming a common user behavior in social media. Discussion in the form of comments following news postings can be effectively facilitated if the service provider can recommend articles based on not only the original news itself but also the thread of changing comments. This turns the traditional news recommendation to a "discussion moderator" that can intelligently assist online forums. In this work, we present a framework to recommend relevant information in the forum-based social media using user comments. When incorporating user comments, we consider structural and semantic information carried by them. Experiments indicate that our proposed solutions provide an effective recommendation service.

#index 1451013
#* Incorporating global information into named entity recognition systems using relational context
#@ Yuval Merhav;Filipe Mesquita;Denilson Barbosa;Wai Gen Yee;Ophir Frieder
#t 2010
#c 13
#% 1249541
#! The state-of-the-art in Named Entity Recognition relies on a combination of local features of the text and global knowledge to determine the types of the recognized entities. This is problematic in some cases, resulting in entities being classified as belonging to the wrong type. We show that using global information about the corpus improves the accuracy of type identification. We explore the notion of a global domain frequency that relates relation identifying terms with pairs of entity types which are used in that relation. We use this to identify entities whose types are not compatible with the terms they co-occur in the text. Our results on a large corpus of social media content allows the identification of mistyped entities with 70% accuracy.

#index 1451014
#* Achieving high accuracy retrieval using intra-document term ranking
#@ Hyun-Wook Woo;Jung-Tae Lee;Seung-Wook Lee;Young-In Song;Hae-Chang Rim
#t 2010
#c 13
#% 262096
#% 321635
#% 879619
#! Most traditional ranking models roughly score the relevance of a given document by observing simple term statistics, such as the occurrence of query terms within the document or within the collection. Intuitively, the relative importance of query terms with regard to other individual non-query terms in a document can also be exploited to promote the ranks of documents in which the query is dedicated as the main topic. In this paper, we introduce a simple technique named intra-document term ranking, which involves ranking all the terms in a document according to their relative importance within that particular document. We demonstrate that the information regarding the rank positions of given query terms within the intra-document term ranking can be useful for enhancing the precision of top-retrieved results by traditional ranking models. Experiments are conducted on three standard TREC test collections.

#index 1451015
#* Author interest topic model
#@ Noriaki Kawamae
#t 2010
#c 13
#% 769906
#% 989621
#! This paper presents a hierarchical topic model that simultaneously captures topics and author's interests. Our proposal, the Author Interest Topic model (AIT), introduces a latent variable with a separate probability distribution over topics into each document. Experiments on a research paper corpus show that the AIT is useful as a generative model.

#index 1451016
#* On the relationship between effectiveness and accessibility
#@ Leif Azzopardi;Richard Bache
#t 2010
#c 13
#% 375017
#% 1130863
#% 1292493
#% 1292722
#% 1415751
#% 1697451
#! Typically the evaluation of Information Retrieval (IR) systems is focused upon two main system attributes: efficiency and effectiveness. However, it has been argued that it is also important to consider accessibility, i.e. the extent to which the IR system makes information easily accessible. But, it is unclear how accessibility relates to typical IR evaluation, and specifically whether there is a trade-off between accessibility and effectiveness. In this poster, we empirically explore the relationship between effectiveness and accessibility to determine whether the two objectives i.e. maximizing effectiveness and maximizing accessibility, are compatible, or not. To this aim, we empirically examine this relationship using two popular IR models and explore the trade-off between access and performance as these models are tuned.

#index 1451017
#* Visual concept-based selection of query expansions for spoken content retrieval
#@ Stevan Rudinac;Martha Larson;Alan Hanjalic
#t 2010
#c 13
#% 905105
#% 1697434
#% 1697478
#! In this paper we present a novel approach to semantic-theme-based video retrieval that considers entire videos as retrieval units and exploits automatically detected visual concepts to improve the results of retrieval based on spoken content. We deploy a query prediction method that makes use of a coherence indicator calculated on top returned documents and taking into account the information about visual concepts presence in videos to make a choice between query expansion methods. The main contribution of our approach is in its ability to exploit noisy shot-level concept detection to improve semantic-theme-based video retrieval. Strikingly, improvement is possible using an extremely limited set of concepts. In the experiments performed on TRECVID 2007 and 2008 datasets our approach shows an interesting performance improvement compared to the best performing baseline.

#index 1451018
#* Mining adjacent markets from a large-scale ads video collection for image advertising
#@ Guwen Feng;Xin-Jing Wang;Lei Zhang;Wei-Ying Ma
#t 2010
#c 13
#% 1131876
#% 1246496
#! The research on image advertising is still in its infancy. Most previous approaches suggest ads by directly matching an ad to a query image, which lacks the power to identify ads from adjacent market. In this paper, we tackle the problem by mining knowledge on adjacent markets from ads videos with a novel Multi-Modal Dirichlet Process Mixture Sets model, which is a unified model of (video frames) clustering and (ads) ranking. Our approach is not only capable of discovering relevant ads (e.g. car ads for a query car image), but also suggesting ads from adjacent markets (e.g. tyre ads). Experimental results show that our proposed approach is fairly effective.

#index 1451019
#* A co-learning framework for learning user search intents from rule-generated training data
#@ Jun Yan;Zeyu Zheng;Li Jiang;Yan Li;Shuicheng Yan;Zheng Chen
#t 2010
#c 13
#% 1158288
#! Learning to understand user search intents from their online behaviors is crucial for both Web search and online advertising. However, it is a challenging task to collect and label a sufficient amount of high quality training data for various user intents such as "compare products", "plan a travel", etc. Motivated by this bottleneck, we start with some user common sense, i.e. a set of rules, to generate training data for learning to predict user intents. The rule-generated training data are however hard to be used since these data are generally imperfect due to the serious data bias and possible data noises. In this paper, we introduce a Co-learning Framework (CLF) to tackle the problem of learning from biased and noisy rule-generated training data. CLF firstly generates multiple sets of possibly biased and noisy training data using different rules, and then trains the individual user search intent classifiers over different training datasets independently. The intermediate classifiers are then used to categorize the training data themselves as well as the unlabeled data. The confidently classified data by one classifier are added to other training datasets and the incorrectly classified ones are instead filtered out from the training datasets. The algorithmic performance of this iterative learning procedure is theoretically guaranteed.

#index 1451020
#* Learning the click-through rate for rare/new ads from similar ads
#@ Kushal S. Dave;Vasudeva Varma
#t 2010
#c 13
#% 448194
#% 956546
#% 1292738
#! Ads on the search engine (SE) are generally ranked based on their Click-through rates (CTR). Hence, accurately predicting the CTR of an ad is of paramount importance for maximizing the SE's revenue. We present a model that inherits the click information of rare/new ads from other semantically related ads. The semantic features are derived from the query ad click-through graphs and advertisers account information. We show that the model learned using these features give a very good prediction for the CTR values.

#index 1451021
#* Graphical models for text: a new paradigm for text representation and processing
#@ Charu Aggarwal;Peixiang Zhao
#t 2010
#c 13
#! Almost all text applications use the well known vector-space model for text representation and analysis. While the vector-space model has proven itself to be an effective and efficient representation for mining purposes, it does not preserve information about the ordering of the words in the representation. In this paper, we will introduce the concept of distance graph representations of text data. Such representations preserve distance and ordering information between the words, and provide a much richer representation of the underlying text. This approach enables knowledge discovery from text which is not possible with the use of a pure vector-space representation, because it loses much less information about the ordering of the underlying words. Furthermore, this representation does not require the development of new mining and management techniques. This is because the technique can also be converted into a structural version of the vector-space representation, which allows the use of all existing tools for text. In addition, existing techniques for graph and XML data can be directly leveraged with this new representation. Thus, a much wider spectrum of algorithms is available for processing this representation.

#index 1451022
#* A survival modeling approach to biomedical search result diversification using wikipedia
#@ Xiaoshi Yin;Jimmy Xiangji Huang;Xiaofeng Zhou;Zhoujun Li
#t 2010
#c 13
#% 340901
#% 1697454
#! In this paper, we propose a probabilistic survival model derived from the survival analysis theory for measuring aspect novelty. The retrieved documents' query-relevance and novelty are combined at the aspect level for re-ranking. Experiments conducted on the TREC 2006 and 2007 Genomics collections demonstrate the effectiveness of the proposed approach in promoting ranking diversity for biomedical information retrieval.

#index 1451023
#* Low cost evaluation in information retrieval
#@ Ben Carterette;Evangelos Kanoulas;Emine Yilmaz
#t 2010
#c 13
#! Search corpora are growing larger and larger: over the last 10 years, the IR research community has moved from the several hundred thousand documents on the TREC disks to the tens of millions of U.S. government web pages of GOV2 to the one billion general-interest web pages in the new ClueWeb09 collection. But traditional means of acquiring relevance judgments and evaluating - e.g. pooling documents to calculate average precision - do not seem to scale well to these new large collections. They require substantially more cost in human assessments for the same reliability in evaluation; if the additional cost goes over the assessing budget, errors in evaluation are inevitable. Some alternatives to pooling that support low-cost and reliable evaluation have recently been proposed. A number of them have already been used in TREC and other evaluation forums (TREC Million Query, Legal, Chemical, Web, Relevance Feedback Tracks, CLEF Patent IR, INEX). Evaluation via implicit user feedback (e.g. clicks) and crowdsourcing have also recently gained attention in the community. Thus it is important that the methodologies, the analysis they support, and their strengths and weaknesses are well-understood by the IR community. Furthermore, these approaches can support small research groups attempting to start investigating new tasks on new corpora with relatively low cost. Even groups that do not participate in TREC, CLEF, or other evaluation conferences can benefit from understanding how these methods work, how to use them, and what they mean as they build test collections for tasks they are interested in. The goal of this tutorial is to provide attendees with a comprehensive overview of techniques to perform low cost (in terms of judgment effort) evaluation. A number of topics will be covered, including alternatives to pooling, evaluation measures robust to incomplete judgments, evaluating with no relevance judgments, statistical inference of evaluation metrics, inference of relevance judgments, query selection, techniques to test the reliability of the evaluation and reusability of the constructed collections. The tutorial should be of interest to a wide range of attendees. Those new to the field will come away with a solid understanding of how low cost evaluation methods can be applied to construct inexpensive test collections and evaluate new IR technology, while those with intermediate knowledge will gain deeper insights and further understand the risks and gains of low cost evaluation. Attendees should have a basic knowledge of the traditional evaluation framework (Cranfield) and metrics (such as average precision and nDCG), along with some basic knowledge on probability theory and statistics. More advanced concepts will be explained during the tutorial.

#index 1451024
#* Learning to rank for information retrieval
#@ Tie-Yan Liu
#t 2010
#c 13
#! This tutorial is concerned with a comprehensive introduction to the research area of learning to rank for information retrieval. In the first part of the tutorial, we will introduce three major approaches to learning to rank, i.e., the pointwise, pairwise, and listwise approaches, analyze the relationship between the loss functions used in these approaches and the widely-used IR evaluation measures, evaluate the performance of these approaches on the LETOR benchmark datasets, and demonstrate how to use these approaches to solve real ranking applications. In the second part of the tutorial, we will discuss some advanced topics regarding learning to rank, such as relational ranking, diverse ranking, semi-supervised ranking, transfer ranking, query-dependent ranking, and training data preprocessing. In the third part, we will briefly mention the recent advances on statistical learning theory for ranking, which explain the generalization ability and statistical consistency of different ranking methods. In the last part, we will conclude the tutorial and show several future research directions.

#index 1451025
#* Introduction to probabilistic models in IR
#@ Victor P. Lavrenko
#t 2010
#c 13
#! Most of today's state-of-the-art retrieval models, including BM25 and language modeling, are grounded in probabilistic principles. Having a working understanding of these principles can help researchers understand existing retrieval models better and also provide industrial practitioners with an understanding of how such models can be applied to real world problems. This half-day tutorial will cover the fundamentals of two dominant probabilistic frameworks for Information Retrieval: the classical probabilistic model and the language modeling approach. The elements of the classical framework will include the probability ranking principle, the binary independence model, the 2-Poisson model, and the widely used BM25 model. Within language modeling framework, we will discuss various distributional assumptions and smoothing techniques. Special attention will be devoted to the event spaces and independence assumptions underlying each approach. The tutorial will outline several techniques for modeling term dependence and addressing vocabulary mismatch. We will also survey applications of probabilistic models in the domains of cross-language and multimedia retrieval. The tutorial will conclude by suggesting a set of open problems in probabilistic models of IR. Attendees should have a basic familiarity with probability and statistics. A brief refresher of basic concepts, including random variables, event spaces, conditional probabilities, and independence will be given at the beginning of the tutorial. In addition to slides, some hands on exercises and examples will be used throughout the tutorial.

#index 1451026
#* Multimedia information retrieval
#@ Stefan Rueger
#t 2010
#c 13
#! This tutorial is concerned with creating the best possible multimedia search experience. The intriguing bit here is that the query itself can be a multimedia excerpt: For example, when you walk around in an unknown place and stumble across an interesting landmark, would it not be great if you could just take a picture with your mobile phone and send it to a service that finds a similar picture in a database and tells you more about the building - and about its significance for that matter? The ideas for this type of search have been around for a decade, but this tutorial will look at recent successes and take stock of the state-of-the-art. It examines the full matrix of a variety of query modes versus document types. How do you retrieve a music piece by humming? What if you want to find news video clips on forest fires using a still image? The tutorial discusses underlying techniques and common approaches to facilitate multimedia search engines: metadata driven search; piggy-back text search where automated processes create text surrogates for multimedia; automated image annotation; content-based search. The latter is studied in more depth looking at features and distances, and how to effectively combine them for efficient retrieval, to a point where the participants have the ingredients and recipe in their hands for building their own visual search engines. Supporting users in their resource discovery mission when hunting for multimedia material is not a technological indexing problem alone. We will briefly look at interactive ways of engaging with repositories through browsing and relevance feedback, roping in geographical context, and providing visual summaries for videos. The tutorial emphasises state-of-the-art research in the area of multimedia information retrieval, which gives an indication of the research and development trends and, thereby, a glimpse of the future world.

#index 1451027
#* Web retrieval: the role of users
#@ Ricardo Baeza-Yates;Yoelle Maarek
#t 2010
#c 13
#! Web retrieval methods have evolved through three major steps in the last decade or so. They started from standard document-centric IR in the early days of the Web, then made a major step forward by leveraging the structure of the Web, using link analysis techniques in both crawling and ranking challenges. A more recent, no less important but maybe more discrete step forward, has been to enter the user in this equation in two ways: (1) implicitly, through the analysis of usage data captured by query logs, and session and click information in general, the goal being to improve ranking as well as to measure user's happiness and engagement; (2) explicitly, by offering novel interactive features; the goal here being to better answer users' needs. In this tutorial, we will cover the user-related challenges associated with the implicit and explicit role of users in Web retrieval. We will review and discuss challenges associated with two types of activities, namely: usage data analysis and metrics and user interaction. The goal of this tutorial is to teach the key principles and technologies behind the activities and challenges briefly outlined above, bring new understanding and insights to the attendees, and hopefully foster future research.

#index 1451028
#* Information retrieval challenges in computational advertising
#@ Andrei Broder;Evgeniy Gabrilovich;Vanja Josifovski
#t 2010
#c 13
#! Computational advertising is an emerging scientific sub-discipline, at the intersection of large scale search and text analysis, information retrieval, statistical modeling, machine learning, classification, optimization, and microeconomics. The central challenge of computational advertising is to find the "best match" between a given user in a given context and a suitable advertisement. The aim of this tutorial is to present the state of the art in Computational Advertising, in particular in its IR-related aspects, and to expose the participants to the current research challenges in this field. The tutorial does not assume any prior knowledge of Web advertising, and will begin with a comprehensive background survey. Going deeper, our focus will be on using a textual representation of the user context to retrieve relevant ads. At first approximation, this process can be reduced to a conventional setup by constructing a query that describes the user context and executing the query against a large inverted index of ads. We show how to augment this approach using query expansion and text classification techniques tuned for the ad-retrieval problem. In particular, we show how to use the Web as a repository of query-specific knowledge and use the Web search results retrieved by the query as a form of a relevance feedback and query expansion. We also present solutions that go beyond the conventional bag of words indexing by constructing additional features using a large external taxonomy and a lexicon of named entities obtained by analyzing the entire Web as a corpus. The last part of the tutorial will be devoted to a potpourri of recent research results and open problems inspired by Computational Advertising challenges in text summarization, natural language generation, named entity recognition, computer-human interaction, and other SIGIR-relevant areas.

#index 1451029
#* Extraction of open-domain class attributes from text: building blocks for faceted search
#@ Marius Pasca
#t 2010
#c 13
#! Knowledge automatically extracted from text captures instances, classes of instances and relations among them. In particular, the acquisition of class attributes (e.g., "top speed", "body style" and "number of cylinders" for the class of "sports cars") from text is a particularly appealing task and has received much attention recently, given its natural fit as a building block towards the far-reaching goal of constructing knowledge bases from text. This tutorial provides an overview of extraction methods developed in the area of Web-based information extraction, with the purpose of acquiring attributes of open-domain classes. The attributes are extracted for classes organized either as a flat set or hierarchically. The extraction methods operate over unstructured or semi-structured text available within collections of Web documents, or over relatively more intriguing data sources consisting of anonymized search queries. The methods take advantage of weak supervision provided in the form of seed examples or small amounts of annotated data, or draw upon knowledge already encoded within human-compiled resources (e.g., Wikipedia). The more ambitious methods, aiming at acquiring as many accurate attributes from text as possible for hundreds or thousands of classes covering a wide range of domains of interest, need to be designed to scale to Web collections. This restriction has significant consequences on the overall complexity and choice of underlying tools, in order for the extracted attributes to ultimately aid information retrieval in general and Web search in particular, by producing relevant attributes for open-domain classes, along with other types of relations among instances or among classes.

#index 1451030
#* From federated to aggregated search
#@ Fernando Diaz;Mounia Lalmas;Milad Shokouhi
#t 2010
#c 13
#! Federated search refers to the brokered retrieval of content from a set of auxiliary retrieval systems instead of from a single, centralized retrieval system. Federated search tasks occur in, for example, digital libraries (where documents from several retrieval systems must be seamlessly merged) or peer-to-peer information retrieval (where documents distributed across a network of local indexes must be retrieved). In the context of web search, aggregated search refers to the integration of non-web content (e.g. images, videos, news articles, maps, tweets) into a web search result page. This is in contrast with classic web search where users are presented with a ranked list consisting exclusively of general web documents. As in other federated search situations, the non-web content is often retrieved from auxiliary retrieval systems (e.g. image or video databases, news indexes). Although aggregated search can be seen as an instance of federated search, several aspects make aggregated search a unique and compelling research topic. These include large sources of evidence (e.g. click logs) for deciding what non-web items to return, constrained interfaces (e.g. mobile screens), and a very heterogeneous set of available auxiliary resources (e.g. images, videos, maps, news articles). Each of these aspects introduces problems and opportunities not addressed in the federated search literature. Aggregated search is an important future research direction for information retrieval. All major search engines now provide aggregated search results. As the number of available auxiliary resources grows, deciding how to effectively surface content from each will become increasingly important. The goal of this tutorial is to provide an overview of federated search and aggregated search techniques for an intermediate information retrieval researcher. At the same time, the content will be valuable for practitioners in industry. We will take the audience through the most influential work in these areas and describe how they relate to real world aggregated search systems. We will also list some of the new challenges confronted in aggregated search and discuss directions for future work.

#index 1451031
#* Estimating the query difficulty for information retrieval
#@ David Carmel;Elad Yom-Tov
#t 2010
#c 13
#! Many information retrieval (IR) systems suffer from a radical variance in performance when responding to users' queries. Even for systems that succeed very well on average, the quality of results returned for some of the queries is poor. Thus, it is desirable that IR systems will be able to identify "difficult" queries in order to handle them properly. Understanding why some queries are inherently more difficult than others is essential for IR, and a good answer to this important question will help search engines to reduce the variance in performance, hence better servicing their customer needs. The high variability in query performance has driven a new research direction in the IR field on estimating the expected quality of the search results, i.e. the query difficulty, when no relevance feedback is given. Estimating the query difficulty is a significant challenge due to the numerous factors that impact retrieval performance. Many prediction methods have been proposed recently. However, as many researchers observed, the prediction quality of state-of-the-art predictors is still too low to be widely used by IR applications. The low prediction quality is due to the complexity of the task, which involves factors such as query ambiguity, missing content, and vocabulary mismatch. The goal of this tutorial is to expose participants to the current research on query performance prediction (also known as query difficulty estimation). Participants will become familiar with states-of-the-art performance prediction methods, and with common evaluation methodologies for prediction quality. We will discuss the reasons that cause search engines to fail for some of the queries, and provide an overview of several approaches for estimating query difficulty. We then describe common methodologies for evaluating the prediction quality of those estimators, and some experiments conducted recently with their prediction quality, as measured over several TREC benchmarks. We will cover a few potential applications that can utilize query difficulty estimators by handling each query individually and selectively based on its estimated difficulty. Finally we will summarize with a discussion on open issues and challenges in the field.

#index 1451032
#* Search and browse log mining for web information retrieval: challenges, methods, and applications
#@ Daxin Jiang;Jian Pei;Hang Li
#t 2010
#c 13
#! Huge amounts of search log data have been accumulated in various search engines. Currently, a commercial search engine receives billions of queries and collects tera-bytes of log data on any single day. Other than search log data, browse logs can be collected by client-side browser plug-ins, which record the browse information if users' permissions are granted. Such massive amounts of search/browse log data, on the one hand, provide great opportunities to mine the wisdom of crowds and improve search results as well as online advertisement. On the other hand, designing effective and efficient methods to clean, model, and process large scale log data also presents great challenges. In this tutorial, we focus on mining search and browse log data for Web information retrieval. We consider a Web information retrieval system consisting of four components, namely, query understanding, document understanding, query-document matching, and user understanding. Accordingly, we organize the tutorial materials along these four aspects. For each aspect, we will survey the major tasks, challenges, fundamental principles, and state-of-the-art methods. The goal of this tutorial is to provide a systematic survey on large-scale search/browse log mining to the IR community. It will help IR researchers to get familiar with the core challenges and promising directions in log mining. At the same time, this tutorial may also serve the developers of Web information retrieval systems as a comprehensive and in-depth reference to the advanced log mining techniques.

#index 1451033
#* Information retrieval for e-discovery
#@ David D. Lewis
#t 2010
#c 13
#! Discovery, the process under which parties to legal cases must reveal documents relevant to the disputed issues is a core aspect of trials in the United States, and a lesser but important factor in other countries. Discovery on documents stored in computerized systems (known variously as electronic discovery, e-discovery, e-disco, EDD, and ED) is increasingly the major factor in discovery, and has become a multi-billion dollar industry. I will discuss the basics of e-discovery, the scale and diversity of the materials involved, and the economics of identifying and reviewing potentially responsive material. I will then focus on three major IR areas of interest: search, supervised machine learning (including text classification and relevance feedback), and interface support for manual relevance assessment. For each, I will discuss technologies currently used in e-discovery, the evaluation methods applicable to measuring effectiveness, and existing research results not yet seeing commercial practice. I will also outline research directions that, if successfully pursued, would potentially be of great interest in e-discovery applications. A particular focus will be on areas where researchers can make progress without access to operational e-discovery environments or "realistic" test collections. Connections will be drawn with the use of IR in related tasks, such as enterprise search, criminal investigations, intelligence analysis, historical research, truth and reconciliation commissions, and freedom of information (open records or sunshine law) requests.

#index 1451034
#* On the mono- and cross-language detection of text reuse and plagiarism
#@ Alberto Barrón-Cedeño
#t 2010
#c 13
#% 815882
#% 1558415
#! Plagiarism, the unacknowledged reuse of text, has increased in recent years due to the large amount of texts readily available. For instance, recent studies claim that nowadays a high rate of student reports include plagiarism, making manual plagiarism detection practically infeasible. Automatic plagiarism detection tools assist experts to analyse documents for plagiarism. Nevertheless, the lack of standard collections with cases of plagiarism has prevented accurate comparing models, making differences hard to appreciate. Seminal efforts on the detection of text reuse [2] have fostered the composition of standard resources for the accurate evaluation and comparison of methods. The aim of this PhD thesis is to address three of the main problems in the development of better models for automatic plagiarism detection: (i) the adequate identification of good potential sources for a given suspicious text; (ii) the detection of plagiarism despite modifications, such as words substitution and paraphrasing (special stress is given to cross-language plagiarism); and (iii) the generation of standard collections of cases of plagiarism and text reuse in order to provide a framework for accurate comparison of models. Regarding difficulties (i) and (ii) , we have carried out preliminary experiments over the METER corpus [2]. Given a suspicious document dq and a collection of potential source documents D, the process is divided in two steps. First, a small subset of potential source documents D* in D is retrieved. The documents d in D* are the most related to dq and, therefore, the most likely to include the source of the plagiarised fragments in it. We performed this stage on the basis of the Kullback-Leibler distance, over a subsample of document's vocabularies. Afterwards, a detailed analysis is carried out comparing dq to every d in D* in order to identify potential cases of plagiarism and their source. This comparison was made on the basis of word n-grams, by considering n = {2, 3}. These n-gram levels are flexible enough to properly retrieve plagiarised fragments and their sources despite modifications [1]. The result is offered to the user to take the final decision. Further experiments were done in both stages in order to compare other similarity measures, such as the cosine measure, the Jaccard coefficient and diverse fingerprinting and probabilistic models. One of the main weaknesses of currently available models is that they are unable to detect cross-language plagiarism. Approaching the detection of this kind of plagiarism is of high relevance, as the most of the information published is written in English, and authors in other languages may find it attractive to make use of direct translations. Our experiments, carried out over parallel and a comparable corpora, show that models of "standard" cross-language information retrieval are not enough. In fact, if the analysed source and target languages are related in some way (common linguistic ancestors or technical vocabulary), a simple comparison based on character n-grams seems to be the option. However, in those cases where the relation between the implied languages is weaker, other models, such as those based on statistical machine translation, are necessary [3]. We plan to perform further experiments, mainly to approach the detection of cross-language plagiarism. In order to do that, we will use the corpora developed under the framework of the PAN competition on plagiarism detection (cf. PAN@CLEF: http://pan.webis.de). Models that consider cross-language thesauri and comparison of cognates will also be applied.

#index 1451035
#* User interface designs to support the social transfer of web search expertise
#@ Neema Moraveji
#t 2010
#c 13
#! While there are many ways to develop search expertise, I maintain that most members of the general public do so in an inefficient manner. One reason is that, with current tools, is difficult to observe experts as a means of acquiring search expertise in a scalable fashion. This calls for a redesign of computer-mediated communication tools to make individual search strategies visible to other users. I present a research agenda to investigate this claim, which draws upon theories of social learning. I use design-based research to build novel systems that enable imitation-based learning of search expertise.

#index 1451036
#* Leveraging user interaction and collaboration for improving multilingual information access in digital libraries
#@ Juliane Stiller
#t 2010
#c 13
#! The goal of interactive cross-lingual information retrieval systems is to support users in formulating effective queries and selecting the documents which satisfy their information needs regardless of the language of these documents. This dissertation aims at harnessing user-system interaction, extracting the added value and integrating it back into the system to improve cross-lingual information retrieval for successive users. To achieve this, user input at different interaction points will be evaluated. This will, among others, include interaction during user-assisted query translations, implicit and explicit relevance feedback and social tags. To leverage this input, explorative studies need to be conducted to determine beneficial user input and the methods of extracting it.

#index 1451037
#* Entity information management in complex networks
#@ Yi Fang
#t 2010
#c 13
#% 1450914
#% 1498354
#! Entity information management (EIM) deals with organizing, processing and delivering information about entities. Its emergence is a result of satisfying more sophisticated information needs that go beyond document search. In the recent years, entity retrieval has attracted much attention in the IR community. INEX has started the XML Entity Ranking track since 2007 and TREC has launched the Entity track since 2009 to investigate the problem of related entity finding. Some EIM problems go beyond retrieval and ranking such as: 1) entity profiling, which is about characterizing a specific entity, and 2) entity distillation, which is about discovering the trend about an entity. These problems have received less attention while they have many important applications. On the other hand, the entities in the real world or in the Web environment are usually not isolated. They are connected or related with each other in one way or another. For example, the coauthorship makes the authors with similar research interests be connected. The emergence of social media such as Facebook, Twitter and Youtube has further interweaved the related entities in a much larger scale. Millions of users in these sites can become friends, fans or followers of others, or taggers or commenters of different types of entities (e.g., bookmarks, photos and videos). These networks are complex in the sense that they are heterogeneous with multiple types of entities and of interactions, they are large-scale, they are multi-lingual, and they are dynamic. These features of the complex networks go beyond traditional social network analysis and require further research. In this proposed research, I investigate entity information management in the environment of complex networks. The main research question is: how can the EIM tasks be facilitated by modeling the content and structure of complex networks? The research is in the intersection of content based information retrieval and complex network analysis, which deals with both unstructured text data and structured networks. The specific targeting EIM tasks are entity retrieval, entity profiling and entity distillation. In addition to the main research question, the following questions are considered: How can we accomplish a EIM task involving diverse entity and interaction types? How to model the evolution of entity profiles as well as the underlying complex networks? How can the existing cross-language IR work be leveraged to build entity profiles with multi-lingual evidence? I propose to use probabilistic models and discriminative models in particular to address the above research questions. In my research, I have developed discriminative models for expert search to integrate arbitrary document features [3] and to learn flexible combination strategies to rank experts in heterogeneous information sources [1]. Discriminative graphical models are proposed to jointly discover homepages by inference on the homepage dependence network [2]. The dependence of table elements is exploited to collectively perform the entity retrieval task [4]. These works have shown the power of discriminative models for entity search and the benefits of utilizing the dependencies among related entities. What I would like to do next is to develop a unified probabilistic framework to investigate the research questions raised in this proposal.

#index 1451038
#* Finding people and their utterances in social media
#@ Wouter Weerkamp
#t 2010
#c 13
#% 1330556
#% 1451010
#! Since its introduction, social media, "a group of internet-based applications that (...) allow the creation and exchange of user generated content" [1], has attracted more and more users. Over the years, many platforms have arisen that allow users to publish information, communicate with others, connect to like-minded, and share anything a users wants to share. Text-centric examples are mailing lists, forums, blogs, community question answering, collaborative knowledge sources, social networks, and microblogs, with new platforms starting all the time. Given the volume of information available in social media, ways of accessing this information intelligently are needed; this is the scope of my research. Why should we care about information in social media? Here are three examples that motivate my interest. (A) Viewpoint research; someone wants to take note of the viewpoints on a particular issue. (B) Answers to problems; many problems have been encountered before, and people have shared solutions. (C) Product development; gaining insight into how people use a product and what features they wish for, eases the development of new products. Looking at these examples of information need in social media, we observe that they revolve not just around relevance in the traditional sense (i.e., objects relevant to a given topic), but also around criteria like credibility, authority, viewpoints, expertise, and experiences. However, these additional aspects are typically conditioned on the topical relevance of information objects. In social media, "information objects" come in several types but many are utterances created by people (blog posts, emails, questions, answers, tweets). People and their utterances offer two natural entry points to information contained in social media: utterances that are relevant and people that are of interest. I focus on three tasks in which the interaction between the two is key.

#index 1451039
#* Leveraging user-generated content for news search
#@ Richard M.C. McCreadie
#t 2010
#c 13
#! Over the last few years both availability and accessibility of current news stories on the Web have dramatically improved. In particular, users can now access news from a variety of sources hosted on the Web, from newswire presences such as the New York Times, to integrated news search within Web search engines. However, of central interest is the emerging impact that user-generated content (UGC) is having on this online news landscape. Indeed, the emergence of Web 2.0 has turned a static news consumer base into a dynamic news machine, where news stories are summarised and commented upon. In summary, value is being added to each news story in terms of additional content. Importantly, however, while there has been movement in commercial circles to exploit this extra value to enrich online news, there has been little research from the academic community on how can be achieved. Indeed, the main purpose of this thesis is to research practical techniques for the integration of UGC to improve the news search component of the most ubiquitous of Web tools, i.e the Web search engine.

#index 1451040
#* User centered story tracking
#@ Ilija Subasic
#t 2010
#c 13
#% 1280540
#! Using data collections available on the Internet has for many people became the main medium for staying informed about the world. Many of these collections are in nature dynamic, evolving as the subjects they describe change. The goal of different research areas is to identify and highlight these changes to better enable readers to track stories. In this work we restrict ourselves to news collections and investigate "real-life" effectiveness and usability of temporal text mining (TTM) story tracking methods. We propose a new story tracking method and build a tool to support it. Additionally, we investigate the effectiveness and usability of story tracking methods and define a new frameworks for automatic and user oriented evaluation. We built methods and tools which allow for understanding, discovery, and search through user interaction. Although there are many TTM methods developed there is a lack of common evaluation procedure. Therefore, we propose an evaluation framework for measuring how different TTM methods discover novel "facts". Apart from the automatic evaluation we are interested in how can users interact with pattens and learn about the underlying subjects of the story they track. For this purpose we propose a user testing environment that measures speed and accuracy in which users can use story tracking methods to discover predefined sets of ground-truth sentences.

#index 1451041
#* Reverse annotation based retrieval from large document image collections
#@ Pramod Sankar K.
#t 2010
#c 13
#! A number of projects are dedicated to creating digital libraries from scanned books, such as Google Books, UDL, Digital Library of India (DLI), etc. The ability to search in the content of document images is essential for the usability and popularity of these DLs. In this work, we aim toward building a retrieval system over 120K document images coming from 1000 scanned books of Telugu literature. This is a challenge because: i) OCRs are not robust enough for Indian languages, especially the Telugu script, ii) the document images contain large number of degradations and artifacts, iii) scalability to large collections is hard. Moreover, users expect that the search system accept text queries and retrieve relevant results in interactive times. We propose a Reverse Annotation framework [1], that labels word-images by their equivalent text label in the offline phase. Reverse Annotation applies a retrieval based approach to recognition. Unlike traditional annotation/recognition that identifies keywords for data, Reverse Annotation identifies data that corresponds to a given keyword. It first selects a set of keywords which are considered useful for labeling and retrieval, such as those that repeat often, and ignoring stopwords and rare-words. Exemplars are obtained for each word from a crude OCR or human annotations. The labels are then propagated across the rest of the collection by matching words in the image-feature space. Since such a matching is computationally expensive, scalability is achieved using a fast approximate nearest neighbor technique based on Hierarchical K-Means. Once text labels are assigned, each document image is considered a bag-of-words over the labeled keywords. A standard search engine is used to build a search index for quick online retrieval. An example query and the retrieved results are shown in Figure 1. We are unaware of any conventional OCRs which can retrieve such images for the given query. There are three major contributions of our work: i) recognizing the entire document collection together, instead of one-at-a-time; this means that the repetition of words in the test set is effectively used for improving accuracy, ii) speeding up recognition by clustering multiple instances of a given word, iii) recognising at the word-level, avoiding the pitfalls of character segmentation and recognition. Other OCR techniques that use word-level context still rely on inaccurate component-level classification. Using the techniques developed from this work, we were able to successfully build a retrieval system over our challenging dataset. To the best of our knowledge, this is the largest collection of document images that has been made searchable for any Indian language. Our algorithm is easily scalable to larger collections, and directly applicable to documents from other language scripts. The first issue to discuss, is the fraction of word-images that remain unrecognized at the end of the Reverse Annotation phase. Rare-words, nouns etc. are not labeled in the test set. It is important to estimate the cost of not being able to answer such queries. If this cost is indeed high, we need to explore methods to label such infrequently occurring words in the collection. Needless to say, such methods should be computationally efficient without compromising on accuracy. The other major issue to discuss is the evaluation of retrieval results. The true recall of the retrieval system cannot be computed, since it is impossible to identify every occurrence of the given query in such large data. Questions to be considered include: whether precision alone is a sufficient indicator of retrieval performance; whether there is some better document-level effectiveness assessment possible; and how best to estimate the relative satisfaction of the user's information need.

#index 1451042
#* Learning hidden variable models for blog retrieval
#@ Mengqiu Wang
#t 2010
#c 13
#% 1074094
#! We describe probabilistic models that leverage individual blog post evidence to improve blog seed retrieval performances. Our model offers a intuitive and principled method to combine multiple posts in scoring a whole blog site by treating individual posts as hidden variables. When applied to the seed retrieval task, our model yields state-of-the-art results on the TREC 2007 Blog Distillation Task dataset.

#index 1451043
#* Investigation on smoothing and aggregation methods in blog retrieval
#@ Mostafa Keikha
#t 2010
#c 13
#% 40313
#% 1074094
#% 1227661
#! Recently, user generated data is growing rapidly and becoming one of the most important source of information in the web. Blogosphere (the collection of blogs on the web) is one of the main source of information in this category. In my work for my PhD, I mainly focussed on the blog distillation task which is: given a user query find the blogs that are most related to the query topic [3]. There are some properties of blogs that make blog analysis different from usual text analysis. One of these properties is related to the time stamp assigned to each post; it is possible that the topics of a blog change over the time and this can affect blog relevance to the query. Also each post in a blog can have viewer generated comments that can change the relevance of the blog to the query if these are considered as part of the content of the blog. Another property is related to the meaning of the links between blogs which are different than links between websites. Finally, blog distillation is different from traditional ad-hoc search since the retrieval unit is a blog (a collection of posts), instead of a single document. With this view, blog distillation is similar to the task of resource selection in federated search [1]. Researchers have applied different methods from similar problems to blog distillation like ad-hoc search methods, expert search algorithms or methods from resource selection in distributed information retrieval. Based on our preliminary experiments, I decided to divide the blog distillation problem into two sub-problems. First of all, I want to use mentioned properties of blogs to retrieve the most relevant posts for a given query. This part is very similar to the ad hoc retrieval. After that, I want to aggregate relevance of posts in each blog and calculate relevance of the blog. This part requires the development of a cross-modal aggregation model that combines the different blog relevance clues found in the blogosphere. We use structure based smoothing methods for improving posts retrieval. The idea behind these smoothing methods is to change the score of a document based on the score of its similar or related documents. We model the blogosphere as a single graph that represents relations between posts and terms [2]. The idea is that in accordance with the Clustering Hypothesis, related documents should have similar scores for the same query. To model the relatedness between posts, we define a new measure which takes into account both content similarity and temporal distance. In more recent work, in the aggregation part of the problem, we model each post as evidence about relevance of a blog to the query, and use aggregation methods like Ordered Weighted Averaging operators to combine the evidence. The ordered weighted averaging operator, commonly called OWA operator, was introduced by Yager [4]. OWA provides a parametrized class of mean type aggregation operators, that can generate OR operator (Max), AND operator (Min) and any other aggregation operator between them. For the next steps, I'm thinking about capturing the temporal properties of the blogs. Bloggers can change their interests over the time or write about different topics periodically. Capturing these changes and using them in the retrieval is one the future woks that I'm interested in. Also, studying the relations between blogs and news and their effect on each other is an interesting problem.

#index 1451044
#* Aiming for user experience in information retrieval: towards user-centered relevance (UCR)
#@ Frans van der Sluis;Betsy. van Dijk;Egon L. van den Broek
#t 2010
#c 13
#% 867123

#index 1598331
#* Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval
#@ Wei-Ying Ma;Jian-Yun Nie;Ricardo Baeza-Yates;Tat-Seng Chua;W. Bruce Croft
#t 2011
#c 13
#! Welcome to the 34th ACM SIGIR International Conference on Research and Development in Information Retrieval. The record number of papers in this year's conference represents both the breadth and depth of the research being done in this vibrant field, both in academia and industry. We have done our best to ensure that these papers meet high standards of quality in terms of presentation, citations, and experimental methodology. At the same time, we have tried to be flexible in the application of these criteria in order to accept papers describing novel and innovative work that may be somewhat unconventional. The conference received 543 full paper submissions this year, with 240 (44%) coming from Asia and Pacific region, 185 (34%) from the Americas, and 112 (21%) from Europe (the rest were "unknown"). Of these papers, 108 (19.9%) were accepted, up from the acceptance rate of 16.7% in last year's conference. The top five countries in terms of accepted papers were the U.S.A. (52), China (18), Germany (7), and then the U.K. and Spain (both 5). In addition, 274 short papers were submitted to the poster track, of which 89 (32.5%) were accepted. In the other categories, there were 15 (42.8%) demonstrations, 8 workshops, and 11 half-day tutorials accepted. In terms of the technical areas that the accepted papers cover, using the primary keyword assigned by the authors, the top five areas are document representation and content analysis (20%), retrieval models and ranking (17%), users and interactive IR (13%), queries and query analysis (11%), and filtering and recommendation (11%). Perhaps the only surprise there is the increase in the number of papers in filtering and recommendation. We believe that the papers at this year's conference provide an excellent cross-section of what is going on in our field. We hope that you find that reading them and listening to the presenters to be a rewarding experience. SIGIR uses a two-tier double blind review system. For the full papers, the first step is that at least three first-tier reviewers read every paper and provide ratings and comments. Then two additional reviewers, referred to as the primary or secondary area chairs, study those reviews, and introduce their own opinions and summaries where appropriate by making additional comments. In some cases, the area chairs initiate the discussion among the first-tier reviewers to work out any controversial issues or significant differences of opinion. A new step introduced this year was to request author feedback for specific issues in some papers. Another change this year was that final decisions for nearly all papers were made by the two area chairs together with the reviewers. At the program committee meeting in Barcelona, the program chairs and some area chairs went over the reviews, obtained additional input, and made decisions in the few cases where the area chairs had requested more discussion.

#index 1598332
#* Future of the web and search
#@ Qi Lu
#t 2011
#c 13
#! No one doubts that we have only scratched the surface of what is possible with the Web. The day is coming fast when the Web will become almost a virtual mind reader. Your intent, interests, and needs will be instantly perceived and the information you want will be promptly delivered -- whether you ask for it directly or not -- based on a deep understanding of the meaning of words in your query, knowledge of your preferences and patterns, what others have done before you, your location, and more. In this talk, I will share some of my thoughts about where the Web is heading and how search will be transformed to align to this new Web, laying out some specifics behind Microsoft's vision to empower people with knowledge.

#index 1598333
#* Beyond search: statistical topic models for text analysis
#@ ChengXiang Zhai
#t 2011
#c 13
#! Search is generally a means to the end of finishing a task. While the current search engines are useful to users for finding relevant information, they offer little help to users for further digesting and analyzing the overwhelming found information needed for finishing a complex task. In this talk, I will discuss how statistical topic models can be used to help users analyze and digest the found relevant information and turn search results into actionable knowledge needed to complete a task. I will present several general statistical topic models for extracting and analyzing topics and their patterns in text, and show sample applications of such models in tasks such as opinion integration, comparative summarization, contextual topic trend analysis, and event impact analysis. The talk will conclude with a discussion of novel challenges raised in extending a search engine to an analysis engine that can go beyond search to provide more complete support for users to finish their tasks.

#index 1598334
#* Modeling and analysis of cross-session search tasks
#@ Alexander Kotov;Paul N. Bennett;Ryen W. White;Susan T. Dumais;Jaime Teevan
#t 2011
#c 13
#% 280817
#% 310567
#% 330617
#% 375017
#% 449294
#% 754059
#% 805200
#% 805878
#% 805898
#% 818207
#% 823348
#% 879567
#% 881540
#% 955711
#% 956495
#% 961704
#% 987211
#% 1019076
#% 1047436
#% 1047437
#% 1083721
#% 1130852
#% 1130868
#% 1130878
#% 1166521
#% 1227577
#% 1227622
#% 1267762
#% 1292754
#% 1355038
#% 1384094
#% 1399965
#% 1400099
#% 1450832
#% 1455264
#! The information needs of search engine users vary in complexity, depending on the task they are trying to accomplish. Some simple needs can be satisfied with a single query, whereas others require a series of queries issued over a longer period of time. While search engines effectively satisfy many simple needs, searchers receive little support when their information needs span session boundaries. In this work, we propose methods for modeling and analyzing user search behavior that extends over multiple search sessions. We focus on two problems: (i) given a user query, identify all of the related queries from previous sessions that the same user has issued, and (ii) given a multi-query task for a user, predict whether the user will return to this task in the future. We model both problems within a classification framework that uses features of individual queries and long-term user search behavior at different granularity. Experimental evaluation of the proposed models for both tasks indicates that it is possible to effectively model and analyze cross-session search behavior. Our findings have implications for improving search for complex information needs and designing search engine features to support cross-session search tasks.

#index 1598335
#* The economics in interactive information retrieval
#@ Leif Azzopardi
#t 2011
#c 13
#% 296645
#% 323131
#% 818260
#% 822126
#% 835027
#% 874504
#% 879622
#% 881943
#% 966981
#% 987249
#% 1051038
#% 1074069
#% 1074200
#% 1077044
#% 1227591
#% 1227623
#% 1227646
#% 1331574
#% 1405707
#% 1450833
#% 1455247
#% 1480199
#% 1482250
#% 1526568
#! Searching is inherently an interactive process usually requiring numerous iterations of querying and assessing in order to find the desired amount of relevant information. Essentially, the search process can be viewed as a combination of inputs (queries and assessments) which are used to "produce" output (relevance). Under this view, it is possible to adapt microeconomic theory to analyze and understand the dynamics of Interactive Information Retrieval. In this paper, we approach the search process as an economics problem and conduct extensive simulations on TREC test collections analyzing various combinations of inputs in the "production" of relevance. The analysis reveals that the total Cumulative Gain (output) obtained during the course of a search session is functionally related to querying and assessing (inputs), and this can be characterized mathematically by the Cobbs-Douglas production function. Further analysis using cost models, that are grounded using cognitive load as the cost, reveals which search strategies minimize the cost of interaction for a given level of output. This paper demonstrates how economics can be applied to formally model the search process. This development establishes the theoretical foundations of Interactive Information Retrieval, providing numerous directions for empirical experimentation that are motivated directly from theory.

#index 1598336
#* Seeding simulated queries with user-study data for personal search evaluation
#@ David Elsweiler;David E. Losada;José C. Toucedo;Ronald T. Fernandez
#t 2011
#c 13
#% 214751
#% 372658
#% 579439
#% 609436
#% 642983
#% 642992
#% 750863
#% 751830
#% 751851
#% 805898
#% 832099
#% 860036
#% 905333
#% 987195
#% 987211
#% 987249
#% 1037636
#% 1083901
#% 1292597
#% 1355035
#% 1384094
#% 1392496
#% 1450835
#% 1526567
#% 1587390
#! In this paper we perform a lab-based user study (n=21) of email re-finding behaviour, examining how the characteristics of submitted queries change in different situations. A number of logistic regression models are developed on the query data to explore the relationship between user- and contextual- variables and query characteristics including length, field submitted to and use of named entities. We reveal several interesting trends and use the findings to seed a simulated evaluation of various retrieval models. Not only is this an enhancement of existing evaluation methods for Personal Search, but the results show that different models are more effective in different situations, which has implications both for the design of email search tools and for the way algorithms for Personal Search are evaluated.

#index 1598337
#* Understanding re-finding behavior in naturalistic email interaction logs
#@ David Elsweiler;Morgan Harvey;Martin Hacker
#t 2011
#c 13
#% 148007
#% 199528
#% 214751
#% 297545
#% 318453
#% 340394
#% 415117
#% 609436
#% 642983
#% 751830
#% 751851
#% 770857
#% 805898
#% 832099
#% 860036
#% 956495
#% 987195
#% 987211
#% 1083900
#% 1083901
#% 1355035
#% 1587390
#! In this paper we present a longitudinal, naturalistic study of email behavior (n=47) and describe our efforts at isolating re-finding behavior in the logs through various qualitative and quantitative analyses. The presented work underlines the methodological challenges faced with this kind of research, but demonstrates that it is possible to isolate re-finding behavior from email interaction logs with reasonable accuracy. Using the approaches developed we uncover interesting aspects of email re-finding behavior that have so far been impossible to study, such as how various features of email-clients are used in re-finding and the difficulties people encounter when using these. We explain how our findings could influence the design of email-clients and outline our thoughts on how future, more in depth analyses, can build on the work presented here to achieve a fuller understanding of email behavior and the support that people need.

#index 1598338
#* People searching for people: analysis of a people search engine log
#@ Wouter Weerkamp;Richard Berendsen;Bogomil Kovachev;Edgar Meij;Krisztian Balog;Maarten de Rijke
#t 2011
#c 13
#% 169777
#% 245817
#% 281396
#% 287209
#% 296646
#% 590523
#% 727917
#% 754059
#% 950658
#% 956495
#% 1024550
#% 1077038
#% 1227577
#% 1227610
#% 1292473
#% 1333455
#% 1348355
#% 1400010
#% 1432775
#% 1482286
#% 1536504
#% 1536532
#% 1697423
#% 1742093
#! Recent years show an increasing interest in vertical search: searching within a particular type of information. Understanding what people search for in these "verticals" gives direction to research and provides pointers for the search engines themselves. In this paper we analyze the search logs of one particular vertical: people search engines. Based on an extensive analysis of the logs of a search engine geared towards finding people, we propose a classification scheme for people search at three levels: (a) queries, (b) sessions, and (c) users. For queries, we identify three types, (i) event-based high-profile queries (people that become "popular" because of an event happening), (ii) regular high-profile queries (celebrities), and (iii) low-profile queries (other, less-known people). We present experiments on automatic classification of queries. On the session level, we observe five types: (i) family sessions (users looking for relatives), (ii) event sessions (querying the main players of an event), (iii) spotting sessions (trying to "spot" different celebrities online), (iv) polymerous sessions (sessions without a clear relation between queries), and (v) repetitive sessions (query refinement and copying). Finally, for users we identify four types: (i) monitors, (ii) spotters, (iii) followers, and (iv) polymers. Our findings not only offer insight into search behavior in people search engines, but they are also useful to identify future research directions and to provide pointers for search engine improvements.

#index 1598339
#* Learning search tasks in queries and web pages via graph regularization
#@ Ming Ji;Jun Yan;Siyu Gu;Jiawei Han;Xiaofei He;Wei Vivian Zhang;Zheng Chen
#t 2011
#c 13
#% 248810
#% 590523
#% 754059
#% 785366
#% 805878
#% 869527
#% 956503
#% 961218
#% 1043040
#% 1074093
#% 1131829
#% 1190102
#% 1214717
#% 1227610
#% 1268061
#% 1292540
#% 1400033
#% 1495579
#! As the Internet grows explosively, search engines play a more and more important role for users in effectively accessing online information. Recently, it has been recognized that a query is often triggered by a search task that the user wants to accomplish. Similarly, many web pages are specifically designed to help accomplish a certain task. Therefore, learning hidden tasks behind queries and web pages can help search engines return the most useful web pages to users by task matching. For instance, the search task that triggers query "thinkpad T410 broken" is to maintain a computer, and it is desirable for a search engine to return the Lenovo troubleshooting page on the top of the list. However, existing search engine technologies mainly focus on topic detection or relevance ranking, which are not able to predict the task that triggers a query and the task a web page can accomplish. In this paper, we propose to simultaneously classify queries and web pages into the popular search tasks by exploiting their content together with click-through logs. Specifically, we construct a taskoriented heterogeneous graph among queries and web pages. Each pair of objects in the graph are linked together as long as they potentially share similar search tasks. A novel graph-based regularization algorithm is designed for search task prediction by leveraging the graph. Extensive experiments in real search log data demonstrate the effectiveness of our method over state-of-the-art classifiers, and the search performance can be significantly improved by using the task prediction results as additional information.

#index 1598340
#* Intentions and attention in exploratory health search
#@ Marc-Allen Cartright;Ryen W. White;Eric Horvitz
#t 2011
#c 13
#% 310567
#% 345262
#% 571398
#% 643519
#% 751596
#% 751861
#% 857478
#% 956495
#% 1275193
#% 1278069
#% 1450956
#% 1573489
#! We study information goals and patterns of attention in explorato-ry search for health information on the Web, reporting results of a large-scale log-based study. We examine search activity associated with the goal of diagnosing illness from symptoms versus more general information-seeking about health and illness. We decom-pose exploratory health search into evidence-based and hypothe-sis-directed information seeking. Evidence-based search centers on the pursuit of details and relevance of signs and symptoms. Hypothesis-directed search includes the pursuit of content on one or more illnesses, including risk factors, treatments, and therapies for illnesses, and on the discrimination among different diseases under the uncertainty that exists in advance of a confirmed diag-nosis. These different goals of exploratory health search are not independent, and transitions can occur between them within or across search sessions. We construct a classifier that identifies medically-related search sessions in log data. Given a set of search sessions flagged as health-related, we show how we can identify different intentions persisting as foci of attention within those sessions. Finally, we discuss how insights about foci dynamics can help us better understand exploratory health search behavior and better support health search on the Web.

#index 1598341
#* User behavior in zero-recall ecommerce queries
#@ Gyanit Singh;Nish Parikh;Neel Sundaresn
#t 2011
#c 13
#% 1358
#% 309767
#% 319118
#% 345262
#% 766472
#% 851306
#% 853850
#% 879567
#% 956495
#% 987221
#% 1130842
#% 1166518
#% 1190250
#% 1356224
#% 1450884
#% 1450902
#% 1482204
#% 1536581
#! User expectation and experience for web search and eCommerce (product) search are quite different. Product descriptions are concise as compared to typical web documents. User expectation is more specific to find the right product. The difference in the publisher and searcher vocabulary (in case of product search the seller and the buyer vocabulary) combined with the fact that there are fewer products to search over than web documents result in observable numbers of searches that return no results (zero recall searches). In this paper we describe a study of zero recall searches. Our study is focused on eCommerce search and uses data from a leading eCommerce site's user click stream logs. There are 3 main contributions of our study: 1) The cause of zero recall searches; 2) A study of user's reaction and recovery from zero recall; 3) A study of differences in behavior of power users versus novice users to zero recall searches.

#index 1598342
#* Bagging gradient-boosted trees for high precision, low variance ranking models
#@ Yasser Ganjisaffar;Rich Caruana;Cristina Videira Lopes
#t 2011
#c 13
#% 132583
#% 156421
#% 209021
#% 309095
#% 312728
#% 350335
#% 387427
#% 400847
#% 734915
#% 840846
#% 881477
#% 983820
#% 987226
#% 987240
#% 987241
#% 1100070
#% 1211826
#% 1268491
#% 1442575
#% 1451238
#% 1456843
#% 1482463
#% 1699645
#! Recent studies have shown that boosting provides excellent predictive performance across a wide variety of tasks. In Learning-to-rank, boosted models such as RankBoost and LambdaMART have been shown to be among the best performing learning methods based on evaluations on public data sets. In this paper, we show how the combination of bagging as a variance reduction technique and boosting as a bias reduction technique can result in very high precision and low variance ranking models. We perform thousands of parameter tuning experiments for LambdaMART to achieve a high precision boosting model. Then we show that a bagged ensemble of such LambdaMART boosted models results in higher accuracy ranking models while also reducing variance as much as 50%. We report our results on three public learning-to-rank data sets using four metrics. Bagged LamdbaMART outperforms all previously reported results on ten of the twelve comparisons, and bagged LambdaMART outperforms non-bagged LambdaMART on all twelve comparisons. For example, wrapping bagging around LambdaMART increases NDCG@1 from 0.4137 to 0.4200 on the MQ2007 data set; the best prior results in the literature for this data set is 0.4134 by RankBoost.

#index 1598343
#* Learning to rank for freshness and relevance
#@ Na Dai;Milad Shokouhi;Brian D. Davison
#t 2011
#c 13
#% 268079
#% 309095
#% 577224
#% 642982
#% 734915
#% 750863
#% 769488
#% 783474
#% 805839
#% 840846
#% 960414
#% 1074065
#% 1130999
#% 1166523
#% 1190093
#% 1227692
#% 1355016
#% 1355017
#% 1355030
#% 1399946
#% 1399966
#% 1442577
#% 1450843
#% 1450846
#% 1536521
#% 1697416
#% 1697425
#! Freshness of results is important in modern web search. Failing to recognize the temporal aspect of a query can negatively affect the user experience, and make the search engine appear stale. While freshness and relevance can be closely related for some topics (e.g., news queries), they are more independent in others (e.g., time insensitive queries). Therefore, optimizing one criterion does not necessarily improve the other, and can even do harm in some cases. We propose a machine-learning framework for simultaneously optimizing freshness and relevance, in which the trade-off is automatically adaptive to query temporal characteristics. We start by illustrating different temporal characteristics of queries, and the features that can be used for capturing these properties. We then introduce our supervised framework that leverages the temporal profile of queries (inferred from pseudo-feedback documents) along with the other ranking features to improve both freshness and relevance of search results. Our experiments on a large archival web corpus demonstrate the efficacy of our techniques.

#index 1598344
#* A cascade ranking model for efficient ranked retrieval
#@ Lidan Wang;Jimmy Lin;Donald Metzler
#t 2011
#c 13
#% 194247
#% 198335
#% 340887
#% 736300
#% 818229
#% 840846
#% 879588
#% 879619
#% 987215
#% 987216
#% 987241
#% 1019084
#% 1041566
#% 1074067
#% 1227642
#% 1263583
#% 1290542
#% 1292550
#% 1348062
#% 1355019
#% 1355057
#% 1450846
#% 1482186
#! There is a fundamental tradeoff between effectiveness and efficiency when designing retrieval models for large-scale document collections. Effectiveness tends to derive from sophisticated ranking functions, such as those constructed using learning to rank, while efficiency gains tend to arise from improvements in query evaluation and caching strategies. Given their inherently disjoint nature, it is difficult to jointly optimize effectiveness and efficiency in end-to-end systems. To address this problem, we formulate and develop a novel cascade ranking model, which unlike previous approaches, can simultaneously improve both top k ranked effectiveness and retrieval efficiency. The model constructs a cascade of increasingly complex ranking functions that progressively prunes and refines the set of candidate documents to minimize retrieval latency and maximize result set quality. We present a novel boosting algorithm for learning such cascades to directly optimize the tradeoff between effectiveness and efficiency. Experimental results show that our cascades are faster and return higher quality results than comparable ranking models.

#index 1598345
#* Relevant knowledge helps in choosing right teacher: active query selection for ranking adaptation
#@ Peng Cai;Wei Gao;Aoying Zhou;Kam-Fai Wong
#t 2011
#c 13
#% 116165
#% 169717
#% 209021
#% 309095
#% 466095
#% 565531
#% 770847
#% 818209
#% 823360
#% 879598
#% 1073903
#% 1074079
#% 1108902
#% 1130923
#% 1134122
#% 1159262
#% 1195836
#% 1227673
#% 1227719
#% 1227720
#% 1261539
#% 1268491
#% 1272110
#% 1292485
#% 1292528
#% 1292566
#% 1338581
#% 1434053
#% 1442578
#% 1450849
#% 1450862
#% 1450931
#% 1451259
#% 1456843
#% 1472892
#% 1484559
#% 1665126
#! Learning to adapt in a new setting is a common challenge to our knowledge and capability. New life would be easier if we actively pursued supervision from the right mentor chosen with our relevant but limited prior knowledge. This variant principle of active learning seems intuitively useful to many domain adaptation problems. In this paper, we substantiate its power for advancing automatic ranking adaptation, which is important in web search since it's prohibitive to gather enough labeled data for every search domain for fully training domain-specific rankers. For the cost-effectiveness, it is expected that only those most informative instances in target domain are collected to annotate while we can still utilize the abundant ranking knowledge in source domain. We propose a unified ranking framework to mutually reinforce the active selection of informative target-domain queries and the appropriate weighting of source training data as related prior knowledge. We select to annotate those target queries whose documents' order most disagrees among the members of a committee built on the mixture of source training data and the already selected target data. Then the replenished labeled set is used to adjust the importance of source queries for enhancing their rank transfer. This procedure iterates until labeling budget exhausts. Based on LETOR3.0 and Yahoo! Learning to Rank Challenge data sets, our approach significantly outperforms the random query annotation commonly used in ranking adaptation and the active rank learner on target-domain data only.

#index 1598346
#* SCENE: a scalable two-stage personalized news recommendation system
#@ Lei Li;Dingding Wang;Tao Li;Daniel Knox;Balaji Padmanabhan
#t 2011
#c 13
#% 173879
#% 271082
#% 280819
#% 282505
#% 297675
#% 301590
#% 330687
#% 389155
#% 397155
#% 479973
#% 528156
#% 643056
#% 722904
#% 734592
#% 754106
#% 956521
#% 989613
#% 1127465
#% 1190124
#% 1356185
#% 1396086
#% 1399999
#% 1710959
#! Recommending news articles has become a promising research direction as the Internet provides fast access to real-time information from multiple sources around the world. Traditional news recommendation systems strive to adapt their services to individual users by virtue of both user and news content information. However, the latent relationships among different news items, and the special properties of new articles, such as short shelf lives and value of immediacy, render the previous approaches inefficient. In this paper, we propose a scalable two-stage personalized news recommendation approach with a two-level representation, which considers the exclusive characteristics (e.g., news content, access patterns, named entities, popularity and recency) of news items when performing recommendation. Also, a principled framework for news selection based on the intrinsic property of user interest is presented, with a good balance between the novelty and diversity of the recommended result. Extensive empirical experiments on a collection of news articles obtained from various news websites demonstrate the efficacy and efficiency of our approach.

#index 1598347
#* Inferring and using location metadata to personalize web search
#@ Paul N. Bennett;Filip Radlinski;Ryen W. White;Emine Yilmaz
#t 2011
#c 13
#% 261550
#% 577329
#% 766441
#% 794881
#% 807438
#% 818207
#% 818256
#% 818335
#% 823348
#% 869516
#% 881540
#% 910797
#% 946521
#% 956552
#% 1016367
#% 1035574
#% 1055919
#% 1074092
#% 1075340
#% 1132405
#% 1166492
#% 1214757
#% 1227621
#% 1358042
#% 1450873
#% 1482279
#% 1536505
#% 1916155
#! Personalization of search results offers the potential for significant improvements in Web search. Among the many observable user attributes, approximate user location is particularly simple for search engines to obtain and allows personalization even for a first-time Web search user. However, acting on user location information is difficult, since few Web documents include an address that can be interpreted as constraining the locations where the document is relevant. Furthermore, many Web documents -- such as local news stories, lottery results, and sports team fan pages -- may not correspond to physical addresses, but the location of the user still plays an important role in document relevance. In this paper, we show how to infer a more general location relevance which uses not only physical location but a more general notion of locations of interest for Web pages. We compute this information using implicit user behavioral data, characterize the most location-centric pages, and show how location information can be incorporated into Web search ranking. Our results show that a substantial fraction of Web search queries can be significantly improved by incorporating location-based features.

#index 1598348
#* Active learning to maximize accuracy vs. effort in interactive information retrieval
#@ Aibo Tian;Matthew Lease
#t 2011
#c 13
#% 280819
#% 329088
#% 342707
#% 722797
#% 879584
#% 879587
#% 879686
#% 989628
#% 1019124
#% 1074078
#% 1227591
#% 1227721
#% 1273928
#% 1274894
#% 1400011
#% 1442580
#% 1456843
#% 1536529
#! We consider an interactive information retrieval task in which the user is interested in finding several to many relevant documents with minimal effort. Given an initial document ranking, user interaction with the system produces relevance feedback (RF) which the system then uses to revise the ranking. This interactive process repeats until the user terminates the search. To maximize accuracy relative to user effort, we propose an active learning strategy. At each iteration, the document whose relevance is maximally uncertain to the system is slotted high into the ranking in order to obtain user feedback for it. Simulated feedback on the Robust04 TREC collection shows our active learning approach dominates several standard RF baselines relative to the amount of feedback provided by the user. Evaluation on Robust04 under noisy feedback and on LETOR collections further demonstrate the effectiveness of active learning, as well as value of negative feedback in this task scenario.

#index 1598349
#* CRTER: using cross terms to enhance probabilistic information retrieval
#@ Jiashu Zhao;Jimmy Xiangji Huang;Ben He
#t 2011
#c 13
#% 280834
#% 287253
#% 397205
#% 643047
#% 643555
#% 732848
#% 766428
#% 810627
#% 818262
#% 857180
#% 879651
#% 987229
#% 1019135
#% 1074217
#% 1224718
#% 1227614
#% 1387547
#! Term proximity retrieval rewards a document where the matched query terms occur close to each other. Although term proximity is known to be effective in many Information Retrieval (IR) applications, the within-document distribution of each individual query term and how the query terms associate with each other, are not fully considered. In this paper, we introduce a pseudo term, namely Cross Term, to model term proximity for boosting retrieval performance. An occurrence of a query term is assumed to have an impact towards its neighboring text, which gradually weakens with the increase of the distance to the place of occurrence. We use a shape function to characterize such an impact. A Cross Term occurs when two query terms appear close to each other and their impact shape functions have an intersection. We propose a Cross Term Retrieval (CRTER) model that combines the Cross Terms' information with basic probabilistic weighting models to rank the retrieved documents. Extensive experiments on standard TREC collections illustrate the effectiveness of our proposed CRTER model.

#index 1598350
#* A boosting approach to improving pseudo-relevance feedback
#@ Yuanhua Lv;ChengXiang Zhai;Wan Chen
#t 2011
#c 13
#% 252009
#% 262096
#% 340899
#% 340901
#% 340948
#% 342707
#% 520224
#% 577224
#% 734915
#% 766497
#% 840846
#% 879585
#% 983820
#% 987228
#% 987229
#% 987241
#% 1074081
#% 1227614
#% 1263577
#% 1268491
#% 1292491
#% 1292550
#% 1292730
#% 1292759
#% 1450901
#! Pseudo-relevance feedback has proven effective for improving the average retrieval performance. Unfortunately, many experiments have shown that although pseudo-relevance feedback helps many queries, it also often hurts many other queries, limiting its usefulness in real retrieval applications. Thus an important, yet difficult challenge is to improve the overall effectiveness of pseudo-relevance feedback without sacrificing the performance of individual queries too much. In this paper, we propose a novel learning algorithm, FeedbackBoost, based on the boosting framework to improve pseudo-relevance feedback through optimizing the combination of a set of basis feedback algorithms using a loss function defined to directly measure both robustness and effectiveness. FeedbackBoost can potentially accommodate many basis feedback methods as features in the model, making the proposed method a general optimization framework for pseudo-relevance feedback. As an application, we apply FeedbackBoost to improve pseudo feedback based on language models through combining different document weighting strategies. The experiment results demonstrate that FeedbackBoost can achieve better average precision and meanwhile dramatically reduce the number and magnitude of feedback failures as compared to three representative pseudo feedback methods and a standard learning to rank approach for pseudo feedback.

#index 1598351
#* Enhancing ad-hoc relevance weighting using probability density estimation
#@ Xiaofeng Zhou;Jimmy Xiangji Huang;Ben He
#t 2011
#c 13
#% 169781
#% 211820
#% 217268
#% 218982
#% 397126
#% 397183
#% 411760
#% 448725
#% 723304
#% 857180
#% 1032019
#% 1041733
#% 1130849
#% 1415741
#! Classical probabilistic information retrieval (IR) models, e.g. BM25, deal with document length based on a trade-off between the Verbosity hypothesis, which assumes the independence of a document's relevance of its length, and the Scope hypothesis, which assumes the opposite. Despite the effectiveness of the classical probabilistic models, the potential relationship between document length and relevance is not fully explored to improve retrieval performance. In this paper, we conduct an in-depth study of this relationship based on the Scope hypothesis that document length does have its impact on relevance. We study a list of probability density functions and examine which of the density functions fits the best to the actual distribution of the document length. Based on the studied probability density functions, we propose a length-based BM25 relevance weighting model, called BM25L, which incorporates document length as a substantial weighting factor. Extensive experiments conducted on standard TREC collections show that our proposed BM25L markedly outperforms the original BM25 model, even if the latter is optimized.

#index 1598352
#* Who should share what?: item-level social influence prediction for users and posts ranking
#@ Peng Cui;Fei Wang;Shaowei Liu;Mingdong Ou;Shiqiang Yang;Lifeng Sun
#t 2011
#c 13
#% 280819
#% 722904
#% 840924
#% 907525
#% 991655
#% 995168
#% 1035589
#% 1083624
#% 1083641
#% 1119132
#% 1214702
#% 1222654
#% 1355040
#% 1355042
#% 1399940
#% 1603196
#! People and information are two core dimensions in a social network. People sharing information (such as blogs, news, albums, etc.) is the basic behavior. In this paper, we focus on predicting item-level social influence to answer the question Who should share What, which can be extended into two information retrieval scenarios: (1) Users ranking: given an item, who should share it so that its diffusion range can be maximized in a social network; (2) Web posts ranking: given a user, what should she share to maximize her influence among her friends. We formulate the social influence prediction problem as the estimation of a user-post matrix, in which each entry represents the strength of influence of a user given a web post. We propose a Hybrid Factor Non-Negative Matrix Factorization (HF-NMF) approach for item-level social influence modeling, and devise an efficient projected gradient method to solve the HF-NMF problem. Intensive experiments are conducted and demonstrate the advantages and characteristics of the proposed method.

#index 1598353
#* Mining tags using social endorsement networks
#@ Theodoros Lappas;Kunal Punera;Tamas Sarlos
#t 2011
#c 13
#% 387427
#% 722904
#% 788094
#% 869548
#% 881054
#% 905280
#% 918842
#% 956544
#% 956579
#% 983833
#% 1035588
#% 1055704
#% 1065099
#% 1074115
#% 1083684
#% 1190129
#% 1214715
#% 1218675
#% 1279836
#% 1287227
#% 1355036
#% 1355042
#% 1399992
#% 1476460
#! Entities on social systems, such as users on Twitter, and images on Flickr, are at the core of many interesting applications: they can be ranked in search results, recommended to users, or used in contextual advertising. Such applications assume knowledge of an entity's nature and characteristic attributes. An effective way to encode such knowledge is in the form of tags. An untagged entity is practically inaccessible, since it is hard to retrieve or interact with. To address this, some platforms allow users to manually tag entities. However,while such tags can be informative, they can oftentimes be inadequate, trivial, ambiguous, or even plain false. Numerous automated tagging methods have been proposed to address these issues. However,most of them require pre-existing high-quality tags or descriptive texts for every entity that needs to be tagged. In our work, we propose a method based on social endorsements that is free from such constraints. Virtually every major social networking platform allows users to endorse entities that they find appealing. Examples include "following" Twitter users or "favoriting" Flickr photos. These endorsements are abundant and directly capture the preferences of users. In this paper, we pose and solve the problem of using the underlying social endorsement network to extract useful tags for entities in a social system. Our work leverages techniques from topic modeling to capture the interests of users and then uses them to extract relevant and descriptive tags for the entities they endorse. We perform an extensive evaluation of our proposed approach on real large-scale datasets from both Twitter and Flickr, and show that it significantly outperforms meaningful and competitive baselines.

#index 1598354
#* Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking
#@ Gabriella Kazai;Jaap Kamps;Marijn Koolen;Natasa Milic-Frayling
#t 2011
#c 13
#% 262097
#% 312689
#% 340890
#% 751818
#% 783526
#% 1047347
#% 1083692
#% 1130811
#% 1150163
#% 1151011
#% 1227633
#% 1252624
#% 1264744
#% 1312807
#% 1375847
#% 1384364
#% 1478132
#% 1573506
#% 1587349
#% 1587350
#% 1622383
#! The evaluation of information retrieval (IR) systems over special collections, such as large book repositories, is out of reach of traditional methods that rely upon editorial relevance judgments. Increasingly, the use of crowdsourcing to collect relevance labels has been regarded as a viable alternative that scales with modest costs. However, crowdsourcing suffers from undesirable worker practices and low quality contributions. In this paper we investigate the design and implementation of effective crowdsourcing tasks in the context of book search evaluation. We observe the impact of aspects of the Human Intelligence Task (HIT) design on the quality of relevance labels provided by the crowd. We assess the output in terms of label agreement with a gold standard data set and observe the effect of the crowdsourced relevance judgments on the resulting system rankings. This enables us to observe the effect of crowdsourcing on the entire IR evaluation process. Using the test set and experimental runs from the INEX 2010 Book Track, we find that varying the HIT design, and the pooling and document ordering strategies leads to considerable differences in agreement with the gold set labels. We then observe the impact of the crowdsourced relevance label sets on the relative system rankings using four IR performance metrics. System rankings based on MAP and Bpref remain less affected by different label sets while the Precision@10 and nDCG@10 lead to dramatically different system rankings, especially for labels acquired from HITs with weaker quality controls. Overall, we find that crowdsourcing can be an effective tool for the evaluation of IR systems, provided that care is taken when designing the HITs.

#index 1598355
#* A site oriented method for segmenting web pages
#@ David Fernandes;Edleno Silva de Moura;Altigran Soares da Silva;Berthier Ribeiro-Neto;Edisson Braga
#t 2011
#c 13
#% 169781
#% 169809
#% 348180
#% 387427
#% 577281
#% 722902
#% 729875
#% 729939
#% 754078
#% 754108
#% 766462
#% 766464
#% 805846
#% 807298
#% 827127
#% 907512
#% 939562
#% 949182
#% 956530
#% 1019075
#% 1032897
#% 1055709
#% 1083741
#% 1116465
#% 1130926
#% 1190692
#% 1211824
#% 1355060
#% 1497582
#! Information about how to segment a Web page can be used nowadays by applications such as segment aware Web search, classification and link analysis. In this research, we propose a fully automatic method for page segmentation and evaluate its application through experiments with four separate Web sites. While the method may be used in other applications, our main focus in this article is to use it as input to segment aware Web search systems. Our results indicate that the proposed method produces better segmentation results when compared to the best segmentation method we found in literature. Further, when applied as input to a segment aware Web search method, it produces results close to those produced when using a manual page segmentation method.

#index 1598356
#* Composite hashing with multiple information sources
#@ Dan Zhang;Fei Wang;Luo Si
#t 2011
#c 13
#% 46803
#% 190581
#% 280819
#% 290703
#% 313959
#% 387427
#% 410276
#% 420495
#% 464267
#% 479649
#% 479973
#% 616528
#% 635689
#% 724290
#% 757953
#% 762054
#% 763708
#% 855573
#% 867054
#% 881557
#% 884044
#% 891060
#% 891559
#% 898309
#% 987258
#% 1077150
#% 1126600
#% 1215859
#% 1227596
#% 1270208
#% 1450831
#% 1482300
#% 1486652
#% 1697462
#! Similarity search applications with a large amount of text and image data demands an efficient and effective solution. One useful strategy is to represent the examples in databases as compact binary codes through semantic hashing, which has attracted much attention due to its fast query/search speed and drastically reduced storage requirement. All of the current semantic hashing methods only deal with the case when each example is represented by one type of features. However, examples are often described from several different information sources in many real world applications. For example, the characteristics of a webpage can be derived from both its content part and its associated links. To address the problem of learning good hashing codes in this scenario, we propose a novel research problem -- Composite Hashing with Multiple Information Sources (CHMIS). The focus of the new research problem is to design an algorithm for incorporating the features from different information sources into the binary hashing codes efficiently and effectively. In particular, we propose an algorithm CHMIS-AW (CHMIS with Adjusted Weights) for learning the codes. The proposed algorithm integrates information from several different sources into the binary hashing codes by adjusting the weights on each individual source for maximizing the coding performance, and enables fast conversion from query examples to their binary hashing codes. Experimental results on five different datasets demonstrate the superior performance of the proposed method against several other state-of-the-art semantic hashing techniques.

#index 1598357
#* Detecting outlier sections in us congressional legislation
#@ Elif Aktolga;Irene Ros;Yannick Assogba
#t 2011
#c 13
#% 115608
#% 144012
#% 169809
#% 262096
#% 300183
#% 340899
#% 342707
#% 570886
#% 609142
#% 643068
#% 737338
#% 748583
#% 748738
#% 799207
#% 995516
#% 1213983
#% 1384727
#% 1392432
#% 1583740
#% 1698976
#! Reading congressional legislation, also known as bills, is often tedious because bills tend to be long and written in complex language. In IBM Many Bills, an interactive web-based visualization of legislation, users of different backgrounds can browse bills and quickly explore parts that are of interest to them. One task users have is to be able to locate sections that don't seem to fit with the overall topic of the bill. In this paper, we present novel techniques to determine which sections within a bill are likely to be outliers by employing approaches from information retrieval. The most promising techniques first detect the most topically relevant parts of a bill by ranking its sections, followed by a comparison between these topically relevant parts and the remaining sections in the bill. To compare sections we use various dissimilarity metrics based on Kullback-Leibler Divergence. The results indicate that these techniques are more successful than a classification based approach. Finally, we analyze how the dissimilarity metrics succeed in discriminating between sections that are strong outliers versus those that are 'milder' outliers.

#index 1598358
#* DOM based content extraction via text density
#@ Fei Sun;Dandan Song;Lejian Liao
#t 2011
#c 13
#% 248808
#% 271060
#% 324987
#% 348180
#% 378483
#% 577281
#% 577323
#% 729628
#% 729939
#% 754078
#% 807298
#% 807426
#% 810759
#% 824581
#% 869466
#% 874265
#% 1019075
#% 1116136
#% 1165358
#% 1355060
#% 1394469
#% 1400030
#% 1707737
#! In addition to the main content, most web pages also contain navigation panels, advertisements and copyright and disclaimer notices. This additional content, which is also known as noise, is typically not related to the main subject and may hamper the performance of web data mining, and hence needs to be removed properly. In this paper, we present Content Extraction via Text Density (CETD) a fast, accurate and general method for extracting content from diverse web pages, and using DOM (Document Object Model) node text density to preserve the original structure. For this purpose, we introduce two concepts to measure the importance of nodes: Text Density and Composite Text Density. In order to extract content intact, we propose a technique called DensitySum to replace Data Smoothing. The approach was evaluated with the CleanEval benchmark and with randomly selected pages from well-known websites, where various web domains and styles are tested. The average F1-scores with our method were 8.79% higher than the best scores among several alternative methods.

#index 1598359
#* Social context summarization
#@ Zi Yang;Keke Cai;Jie Tang;Li Zhang;Zhong Su;Juanzi Li
#t 2011
#c 13
#% 194251
#% 262112
#% 266370
#% 316520
#% 340884
#% 340971
#% 679873
#% 790703
#% 816173
#% 816181
#% 818226
#% 854186
#% 943826
#% 992317
#% 1040837
#% 1074087
#% 1074088
#% 1190068
#% 1214702
#% 1275213
#% 1292504
#% 1297059
#% 1306081
#% 1355042
#% 1379671
#% 1399992
#% 1481542
#% 1482397
#% 1484279
#% 1824959
#! We study a novel problem of social context summarization for Web documents. Traditional summarization research has focused on extracting informative sentences from standard documents. With the rapid growth of online social networks, abundant user generated content (e.g., comments) associated with the standard documents is available. Which parts in a document are social users really caring about? How can we generate summaries for standard documents by considering both the informativeness of sentences and interests of social users? This paper explores such an approach by modeling Web documents and social contexts into a unified framework. We propose a dual wing factor graph (DWFG) model, which utilizes the mutual reinforcement between Web documents and their associated social contexts to generate summaries. An efficient algorithm is designed to learn the proposed factor graph model.Experimental results on a Twitter data set validate the effectiveness of the proposed model. By leveraging the social context information, our approach obtains significant improvement (averagely +5.0%-17.3%) over several alternative methods (CRF, SVM, LR, PR, and DocLead) on the performance of summarization.

#index 1598360
#* Probabilistic factor models for web site recommendation
#@ Hao Ma;Chao Liu;Irwin King;Michael R. Lyu
#t 2011
#c 13
#% 330687
#% 397153
#% 452563
#% 643007
#% 734592
#% 734593
#% 734594
#% 766422
#% 766449
#% 805877
#% 818259
#% 840924
#% 869536
#% 987197
#% 987198
#% 989580
#% 1055685
#% 1064768
#% 1073982
#% 1074061
#% 1214666
#% 1227603
#% 1227622
#% 1395335
#% 1650569
#! Due to the prevalence of personalization and information filtering applications, modeling users' interests on the Web has become increasingly important during the past few years. In this paper, aiming at providing accurate personalized Web site recommendations for Web users, we propose a novel probabilistic factor model based on dimensionality reduction techniques. We also extend the proposed method to collective probabilistic factor modeling, which further improves model performance by incorporating heterogeneous data sources. The proposed method is general, and can be applied to not only Web site recommendations, but also a wide range of Web applications, including behavioral targeting, sponsored search, etc. The experimental analysis on Web site recommendation shows that our method outperforms other traditional recommendation approaches. Moreover, the complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations.

#index 1598361
#* Efficiently collecting relevance information from clickthroughs for web retrieval system evaluation
#@ Jing He;Wayne Xin Zhao;Baihan Shu;Xiaoming Li;Hongfei Yan
#t 2011
#c 13
#% 411762
#% 879565
#% 879598
#% 946521
#% 987263
#% 989628
#% 1074092
#% 1074126
#% 1089473
#% 1095876
#% 1130811
#% 1130814
#% 1166517
#% 1190055
#% 1190056
#% 1214757
#% 1450892
#% 1783188
#! Various click models have been recently proposed as a principled approach to infer the relevance of documents from the clickthrough data. The inferred document relevance is potentially useful in evaluating the Web retrieval systems. In practice, it generally requires to acquire the accurate evaluation results within minimal users' query submissions. This problem is important for speeding up search engine development and evaluation cycle and acquiring reliable evaluation results on tail queries. In this paper, we propose a reordering framework for efficient evaluation problem in the context of clickthrough based Web retrieval evaluation. The main idea is to move up the documents that contribute more for the evaluation task. In this framework, we propose four intuitions and formulate them as an optimization problem. Both user study and TREC data based experiments validate that the reordering framework results in much fewer query submissions to get accurate evaluation results with only a little harm to the users' utility.

#index 1598362
#* Unsupervised query segmentation using clickthrough for information retrieval
#@ Yanen Li;Bo-Jun Paul Hsu;ChengXiang Zhai;Kuansan Wang
#t 2011
#c 13
#% 169781
#% 248214
#% 340948
#% 397205
#% 411762
#% 549575
#% 577224
#% 742441
#% 766428
#% 818240
#% 818262
#% 838547
#% 869501
#% 879567
#% 907546
#% 987229
#% 1055706
#% 1215246
#% 1400076
#% 1450970
#% 1468142
#% 1715627
#! Query segmentation is an important task toward understanding queries accurately, which is essential for improving search results. Existing segmentation models either use labeled data to predict the segmentation boundaries, for which the training data is expensive to collect, or employ unsupervised strategy based on a large text corpus, which might be inaccurate because of the lack of relevant information. In this paper, we propose a probabilistic model to exploit clickthrough data for query segmentation, where the model parameters are estimated via an efficient EM algorithm. We further study how to properly interpret the segmentation results and utilize them to improve retrieval accuracy. Specifically, we propose an integrated language model based on the standard bigram language model to exploit the probabilistic structure obtained through query segmentation. Experiment results on two datasets show that our segmentation model outperforms existing segmentation models. Furthermore, extensive experiments on a large retrieval dataset reveals that the results of query segmentation can be leveraged to improve retrieval relevance by using the proposed integrated language model.

#index 1598363
#* Collaborative competitive filtering: learning recommender using context of user choice
#@ Shuang-Hong Yang;Bo Long;Alexander J. Smola;Hongyuan Zha;Zhaohui Zheng
#t 2011
#c 13
#% 283833
#% 330687
#% 573634
#% 739691
#% 766448
#% 840924
#% 963350
#% 1074061
#% 1108903
#% 1190124
#% 1211829
#% 1214623
#% 1214642
#% 1260273
#% 1560408
#! While a user's preference is directly reflected in the interactive choice process between her and the recommender, this wealth of information was not fully exploited for learning recommender models. In particular, existing collaborative filtering (CF) approaches take into account only the binary events of user actions but totally disregard the contexts in which users' decisions are made. In this paper, we propose Collaborative Competitive Filtering (CCF), a framework for learning user preferences by modeling the choice process in recommender systems. CCF employs a multiplicative latent factor model to characterize the dyadic utility function. But unlike CF, CCF models the user behavior of choices by encoding a local competition effect. In this way, CCF allows us to leverage dyadic data that was previously lumped together with missing data in existing CF models. We present two formulations and an efficient large scale optimization algorithm. Experiments on three real-world recommendation data sets demonstrate that CCF significantly outperforms standard CF approaches in both offline and online evaluations.

#index 1598364
#* CLR: a collaborative location recommendation framework based on co-clustering
#@ Kenneth Wai-Ting Leung;Dik Lun Lee;Wang-Chien Lee
#t 2011
#c 13
#% 245087
#% 290830
#% 310567
#% 805877
#% 879627
#% 1135166
#% 1190134
#% 1263274
#% 1400036
#% 1695791
#% 1913977
#! GPS data tracked on mobile devices contains rich information about human activities and preferences. In this paper, GPS data is used in location-based services (LBSs) to provide collaborative location recommendations. We observe that most existing LBSs provide location recommendations by clustering the User-Location matrix. Since the User-Location matrix created based on GPS data is huge, there are two major problems with these methods. First, the number of similar locations that need to be considered in computing the recommendations can be numerous. As a result, the identification of truly relevant locations from numerous candidates is challenging. Second, the clustering process on large matrix is time consuming. Thus, when new GPS data arrives, complete re-clustering of the whole matrix is infeasible. To tackle these two problems, we propose the Collaborative Location Recommendation (CLR) framework for location recommendation. By considering activities (i.e., temporal preferences) and different user classes (i.e., Pattern Users, Normal Users, and Travelers) in the recommendation process, CLR is capable of generating more precise and refined recommendations to the users compared to the existing methods. Moreover, CLR employs a dynamic clustering algorithm CADC to cluster the trajectory data into groups of similar users, similar activities and similar locations efficiently by supporting incremental update of the groups when new GPS trajectory data arrives. We evaluate CLR with a real-world GPS dataset, and confirm that the CLR framework provides more accurate location recommendations compared to the existing methods.

#index 1598365
#* Functional matrix factorizations for cold-start recommendation
#@ Ke Zhou;Shuang-Hong Yang;Hongyuan Zha
#t 2011
#c 13
#% 342767
#% 397155
#% 420515
#% 528156
#% 734592
#% 770816
#% 788069
#% 813966
#% 840924
#% 1074062
#% 1127452
#% 1190066
#% 1200881
#% 1214623
#% 1287222
#% 1291600
#% 1358747
#% 1400014
#% 1482440
#% 1536564
#% 1650569
#% 1672989
#! A key challenge in recommender system research is how to effectively profile new users, a problem generally known as cold-start recommendation. Recently the idea of progressively querying user responses through an initial interview process has been proposed as a useful new user preference elicitation strategy. In this paper, we present functional matrix factorization (fMF), a novel cold-start recommendation method that solves the problem of initial interview construction within the context of learning user and item profiles. Specifically, fMF constructs a decision tree for the initial interview with each node being an interview question, enabling the recommender to query a user adaptively according to her prior responses. More importantly, we associate latent profiles for each node of the tree --- in effect restricting the latent profiles to be a function of possible answers to the interview questions --- which allows the profiles to be gradually refined through the interview process based on user responses. We develop an iterative optimization algorithm that alternates between decision tree construction and latent profiles extraction as well as a regularization scheme that takes into account of the tree structure. Experimental results on three benchmark recommendation data sets demonstrate that the proposed fMF algorithm significantly outperforms existing methods for cold-start recommendation.

#index 1598366
#* Exploiting geographical influence for collaborative point-of-interest recommendation
#@ Mao Ye;Peifeng Yin;Wang-Chien Lee;Dik-Lun Lee
#t 2011
#c 13
#% 169774
#% 220709
#% 280852
#% 330687
#% 378577
#% 465928
#% 578684
#% 734593
#% 813966
#% 879627
#% 891559
#% 915344
#% 1055691
#% 1127466
#% 1130901
#% 1190134
#% 1214661
#% 1227601
#% 1227602
#% 1287243
#% 1400036
#% 1480830
#% 1523885
#% 1536533
#! In this paper, we aim to provide a point-of-interests (POI) recommendation service for the rapid growing location-based social networks (LBSNs), e.g., Foursquare, Whrrl, etc. Our idea is to explore user preference, social influence and geographical influence for POI recommendations. In addition to deriving user preference based on user-based collaborative filtering and exploring social influence from friends, we put a special emphasis on geographical influence due to the spatial clustering phenomenon exhibited in user check-in activities of LBSNs. We argue that the geographical influence among POIs plays an important role in user check-in behaviors and model it by power law distribution. Accordingly, we develop a collaborative recommendation algorithm based on geographical influence based on naive Bayesian. Furthermore, we propose a unified POI recommendation framework, which fuses user preference to a POI with social influence and geographical influence. Finally, we conduct a comprehensive performance evaluation over two large-scale datasets collected from Foursquare and Whrrl. Experimental results with these real datasets show that the unified collaborative recommendation approach significantly outperforms a wide spectrum of alternative recommendation approaches.

#index 1598367
#* Why searchers switch: understanding and predicting engine switching rationales
#@ Qi Guo;Ryen W. White;Yunqiao Zhang;Blake Anderson;Susan T. Dumais
#t 2011
#c 13
#% 144076
#% 169774
#% 397161
#% 734050
#% 754059
#% 805200
#% 807394
#% 879613
#% 891559
#% 956541
#% 987260
#% 1055851
#% 1074056
#% 1074071
#% 1083674
#% 1292474
#% 1355038
#% 1384094
#% 1450833
#% 1456294
#% 1537504
#% 1715212
#! Search engine switching is the voluntary transition between Web search engines. Engine switching can occur for a number of reasons, including user dissatisfaction with search results, a desire for broader topic coverage or verification, user preferences, or even unintentionally. An improved understanding of switching rationales allows search providers to tailor the search experience according to the different causes. In this paper we study the reasons behind search engine switching within a session. We address the challenge of identifying switching rationales by designing and implementing client-side instrumentation to acquire in-situ feedbacks from users. Using this feedback, we investigate in detail the reasons that users switch engines within a session. We also study the relationship between implicit behavioral signals and the switching causes, and develop and evaluate models to predict the reasons for switching. In addition, we collect editorial judgments of switching rationales by third-party judges and show that we can recover switching causes a posteriori. Our findings provide valuable insights into why users switch search engines in a session and demonstrate the relationship between search behavior and switching motivations. The findings also reveal sufficient behavioral consistency to afford accurate prediction of switching rationale, which can be used to dynamically adapt the search experience and derive more accurate competitive metrics.

#index 1598368
#* Find it if you can: a game for modeling different types of web search success using interaction data
#@ Mikhail Ageev;Qi Guo;Dmitry Lagun;Eugene Agichtein
#t 2011
#c 13
#% 306468
#% 464434
#% 766447
#% 869550
#% 879566
#% 987224
#% 987263
#% 987321
#% 1074069
#% 1074133
#% 1166521
#% 1184656
#% 1227680
#% 1227715
#% 1263597
#% 1268490
#% 1292473
#% 1348355
#% 1355034
#% 1355038
#% 1384094
#% 1400158
#% 1450833
#% 1450898
#% 1455267
#% 1482279
#! A better understanding of strategies and behavior of successful searchers is crucial for improving the experience of all searchers. However, research of search behavior has been struggling with the tension between the relatively small-scale, but controlled lab studies, and the large-scale log-based studies where the searcher intent and many other important factors have to be inferred. We present our solution for performing controlled, yet realistic, scalable, and reproducible studies of searcher behavior. We focus on difficult informational tasks, which tend to frustrate many users of the current web search technology. First, we propose a principled formalization of different types of "success" for informational search, which encapsulate and sharpen previously proposed models. Second, we present a scalable game-like infrastructure for crowdsourcing search behavior studies, specifically targeted towards capturing and evaluating successful search strategies on informational tasks with known intent. Third, we report our analysis of search success using these data, which confirm and extends previous findings. Finally, we demonstrate that our model can predict search success more effectively than the existing state-of-the-art methods, on both our data and on a different set of log data collected from regular search engine sessions. Together, our search success models, the data collection infrastructure, and the associated behavior analysis techniques, significantly advance the study of success in web search.

#index 1598369
#* Measuring improvement in user search performance resulting from optimal search tips
#@ Neema Moraveji;Daniel Russell;Jacob Bien;David Mease
#t 2011
#c 13
#% 142618
#% 187999
#% 200986
#% 284796
#% 309767
#% 345736
#% 349274
#% 623376
#% 751596
#% 751830
#% 841893
#% 857478
#% 857485
#% 869501
#% 967660
#% 987224
#% 1093773
#% 1166518
#% 1183439
#% 1190348
#% 1227623
#% 1384095
#% 1406456
#% 1451035
#% 1554270
#! Web search performance can be improved by either improving the search engine itself or by educating the user to search more efficiently. There is a large amount of literature describing techniques for measuring the former; whereas, improvements resulting from the latter are more difficult to quantify. In this paper we demonstrate an experimental methodology that proves to successfully quantify improvements from user education. The user education in our study is realized in the form of tactical search feature tips that expand user awareness of task-relevant tools and features of the search application. Initially, these tips are presented in an idealized situation: each tip is shown at the same time as the study participants are given a task that is constructed to benefit from the specific tip. However, we also present a follow-up study roughly one week later in which the search tips are no longer presented but the study participants who previously were shown search tips still demonstrate improved search efficiency compared to the control group. This research has implications for search user interface designers and the study of information retrieval systems.

#index 1598370
#* ViewSer: enabling large-scale remote user studies of web search examination and interaction
#@ Dmitry Lagun;Eugene Agichtein
#t 2011
#c 13
#% 411762
#% 483325
#% 577224
#% 879567
#% 881477
#% 954948
#% 987209
#% 1047347
#% 1053505
#% 1150163
#% 1166525
#% 1190055
#% 1227633
#% 1355037
#% 1355048
#% 1384112
#% 1384641
#% 1400034
#% 1409193
#% 1573487
#% 1573520
#! Web search behaviour studies, including eye-tracking studies of search result examination, have resulted in numerous insights to improve search result quality and presentation. Yet, eye tracking studies have been restricted in scale, due to the expense and the effort required. Furthermore, as the reach of the Web expands, it becomes increasingly important to understand how searchers around the world see and interact with the search results. To address both challenges, we introduce ViewSer, a novel methodology for performing web search examination studies remotely, at scale, and without requiring eye-tracking equipment. ViewSer operates by automatically modifying the appearance of a search engine result page, to clearly show one search result at a time as if through a "viewport", while partially blurring the rest and allowing the participant to move the viewport naturally with a computer mouse or trackpad. Remarkably, the resulting result viewing and clickthrough patterns agree closely with unrestricted viewing of results, as measured by eye-tracking equipment, validated by a study with over 100 participants. We also explore applications of ViewSer to practical search tasks, such as analyzing the search result summary (snip- pet) attractiveness, result re-ranking, and evaluating snippet quality. These experiments could have only be done previously by tracking the eye movements for a small number of subjects in the lab. In contrast, our study was performed with over 100 participants, allowing us to reproduce and extend previous findings, establishing ViewSer as a valuable tool for large-scale search behavior experiments.

#index 1598371
#* CrowdLogging: distributed, private, and anonymous search logging
#@ Henry Allen Feild;James Allan;Joshua Glatt
#t 2011
#c 13
#% 264249
#% 310567
#% 319849
#% 319994
#% 393264
#% 576761
#% 577224
#% 804736
#% 804805
#% 881544
#% 939973
#% 1019130
#% 1131000
#% 1190072
#% 1215916
#% 1292623
#% 1348355
#% 1670071
#! We describe CrowdLogging, an approach for distributed search log collection, storage, and mining, with the dual goals of preserving privacy and making the mined information broadly available. Most search log mining approaches and most privacy enhancing schemes have focused on centralized search logs and methods for disseminating them to third parties. In our approach, a user's search log is encrypted and shared in such a way that (a) the source of a search behavior artifact, such as a query, is unknown and (b) extremely rare artifacts---that is, artifacts more likely to contain private information---are not revealed. The approach works with any search behavior artifact that can be extracted from a search log, including queries, query reformulations, and query-click pairs. In this work, we: (1) present a distributed search log collection, storage, and mining framework; (2) compare several privacy policies, including differential privacy, showing the trade-offs between strong guarantees and the utility of the released data; (3) demonstrate the impact of our approach using two existing research query logs; and (4) describe a pilot study for which we implemented a version of the framework.

#index 1598372
#* Out of sight, not out of mind: on the effect of social and physical detachment on information need
#@ Elad Yom-Tov;Fernando Diaz
#t 2011
#c 13
#% 766414
#% 769882
#% 869500
#% 879613
#% 1055707
#% 1065241
#% 1074093
#% 1132405
#% 1166492
#% 1166523
#% 1214671
#% 1287882
#% 1292590
#% 1298863
#% 1355017
#% 1392432
#% 1399966
#% 1400018
#% 1561558
#! The information needs of users and the documents which answer it are frequently contingent on the different characteristics of users. This is especially evident during natural disasters, such as earthquakes and violent weather incidents, which create a strong transient information need. In this paper we investigate how the information need of users is affected by their physical detachment, as estimated by their physical location in relation to that of the event, and by their social detachment, as quantified by the number of their acquaintances who may be affected by the event. Drawing on large-scale data from three major events, we show that social and physical detachment levels of users are a major influence on their information needs, as manifested by their search engine queries. We demonstrate how knowing social and physical detachment levels can assist in improving retrieval for two applications: identifying search queries related to events and ranking results in response to event-related queries. We find that the average precision in identifying relevant search queries improves by approximately 18%, and that the average precision of ranking that uses detachment information improves by 10%.

#index 1598373
#* Scalable multi-dimensional user intent identification using tree structured distributions
#@ Vinay Jethava;Liliana Calderón-Benavides;Ricardo Baeza-Yates;Chiranjib Bhattacharyya;Devdatt Dubhashi
#t 2011
#c 13
#% 115608
#% 466257
#% 590523
#% 642982
#% 754059
#% 805878
#% 987358
#% 1043040
#% 1074071
#% 1130852
#% 1166469
#% 1166518
#% 1177247
#% 1187347
#% 1190102
#% 1195879
#% 1338677
#% 1340318
#% 1442579
#% 1682107
#% 1682429
#% 1810385
#! The problem of identifying user intent has received considerable attention in recent years, particularly in the context of improving the search experience via query contextualization. Intent can be characterized by multiple dimensions, which are often not observed from query words alone. Accurate identification of Intent from query words remains a challenging problem primarily because it is extremely difficult to discover these dimensions. The problem is often significantly compounded due to lack of representative training sample. We present a generic, extensible framework for learning the multi-dimensional representation of user intent from the query words. The approach models the latent relationships between facets using tree structured distribution which leads to an efficient and convergent algorithm, FastQ, for identifying the multi-faceted intent of users based on just the query words. We also incorporated WordNet to extend the system capabilities to queries which contain words that do not appear in the training data. Empirical results show that FastQ yields accurate identification of intent when compared to a gold standard.

#index 1598374
#* Social annotation in query expansion: a machine learning approach
#@ Yuan Lin;Hongfei Lin;Song Jin;Zheng Ye
#t 2011
#c 13
#% 218978
#% 340901
#% 342707
#% 397205
#% 476873
#% 641976
#% 783506
#% 818262
#% 838532
#% 879585
#% 956544
#% 983820
#% 987194
#% 987231
#% 1035588
#% 1074070
#% 1074080
#% 1074081
#% 1074115
#% 1227584
#% 1227592
#% 1493759
#% 1655418
#% 1667787
#! Automatic query expansion technologies have been proven to be effective in many information retrieval tasks. Most existing approaches are based on the assumption that the most informative terms in top-retrieved documents can be viewed as context of the query and thus can be used for query expansion. One problem with these approaches is that some of the expansion terms extracted from feedback documents are irrelevant to the query, and thus may hurt the retrieval performance. In social annotations, users provide different keywords describing the respective Web pages from various aspects. These features may be used to boost IR performance. However, to date, the potential of social annotation for this task has been largely unexplored. In this paper, we explore the possibility and potential of social annotation as a new resource for extracting useful expansion terms. In particular, we propose a term ranking approach based on social annotation resource. The proposed approach consists of two phases: (1) in the first phase, we propose a term-dependency method to choose the most likely expansion terms; (2) in the second phase, we develop a machine learning method for term ranking, which is learnt from the statistics of the candidate expansion terms, using ListNet. Experimental results on three TREC test collections show that the retrieval performance can be improved when the term ranking method is used. In addition, we also demonstrate that terms selected by the term-dependency method from social annotation resources are beneficial to improve the retrieval performance.

#index 1598375
#* Predicting web searcher satisfaction with existing community-based answers
#@ Qiaoling Liu;Eugene Agichtein;Gideon Dror;Evgeniy Gabrilovich;Yoelle Maarek;Dan Pelleg;Idan Szpektor
#t 2011
#c 13
#% 262504
#% 397161
#% 411762
#% 576218
#% 754059
#% 783474
#% 818267
#% 879593
#% 945788
#% 987263
#% 987354
#% 1035587
#% 1047396
#% 1055718
#% 1074071
#% 1074110
#% 1074111
#% 1077038
#% 1166519
#% 1183152
#% 1227599
#% 1270283
#% 1292492
#% 1292541
#% 1314950
#% 1355036
#% 1355038
#% 1399943
#% 1399965
#% 1399976
#% 1450833
#% 1450862
#% 1450880
#% 1470617
#! Community-based Question Answering (CQA) sites, such as Yahoo! Answers, Baidu Knows, Naver, and Quora, have been rapidly growing in popularity. The resulting archives of posted answers to questions, in Yahoo! Answers alone, already exceed in size 1 billion, and are aggressively indexed by web search engines. In fact, a large number of search engine users benefit from these archives, by finding existing answers that address their own queries. This scenario poses new challenges and opportunities for both search engines and CQA sites. To this end, we formulate a new problem of predicting the satisfaction of web searchers with CQA answers. We analyze a large number of web searches that result in a visit to a popular CQA site, and identify unique characteristics of searcher satisfaction in this setting, namely, the effects of query clarity, query-to-question match, and answer quality. We then propose and evaluate several approaches to predicting searcher satisfaction that exploit these characteristics. To the best of our knowledge, this is the first attempt to predict and validate the usefulness of CQA archives for external searchers, rather than for the original askers. Our results suggest promising directions for improving and exploiting community question answering services in pursuit of satisfying even more Web search queries.

#index 1598376
#* Competition-based user expertise score estimation
#@ Jing Liu;Young-In Song;Chin-Yew Lin
#t 2011
#c 13
#% 268079
#% 290830
#% 730082
#% 879593
#% 879655
#% 956516
#% 1019165
#% 1035587
#% 1074109
#% 1074111
#% 1083720
#% 1116996
#% 1117691
#% 1166519
#% 1190060
#% 1399976
#% 1482364
#% 1482384
#% 1536523
#! In this paper, we consider the problem of estimating the relative expertise score of users in community question and answering services (CQA). Previous approaches typically only utilize the explicit question answering relationship between askers and an-swerers and apply link analysis to address this problem. The im-plicit pairwise comparison between two users that is implied in the best answer selection is ignored. Given a question and answering thread, it's likely that the expertise score of the best answerer is higher than the asker's and all other non-best answerers'. The goal of this paper is to explore such pairwise comparisons inferred from best answer selections to estimate the relative expertise scores of users. Formally, we treat each pairwise comparison between two users as a two-player competition with one winner and one loser. Two competition models are proposed to estimate user expertise from pairwise comparisons. Using the NTCIR-8 CQA task data with 3 million questions and introducing answer quality prediction based evaluation metrics, the experimental results show that the pairwise comparison based competition model significantly outperforms link analysis based approaches (PageRank and HITS) and pointwise approaches (number of best answers and best answer ratio) for estimating the expertise of active users. Furthermore, it's shown that pairwise comparison based competi-tion models have better discriminative power than other methods. It's also found that answer quality (best answer) is an important factor to estimate user expertise.

#index 1598377
#* Learning online discussion structures by conditional random fields
#@ Hongning Wang;Chi Wang;ChengXiang Zhai;Jiawei Han
#t 2011
#c 13
#% 46803
#% 464434
#% 577224
#% 840966
#% 879569
#% 879602
#% 956516
#% 1019165
#% 1074109
#% 1214699
#% 1227593
#% 1227598
#% 1292733
#% 1417383
#% 1470659
#% 1587368
#% 1815753
#! Online forum discussions are emerging as valuable information repository, where knowledge is accumulated by the interaction among users, leading to multiple threads with structures. Such replying structure in each thread conveys important information about the discussion content. Unfortunately, not all the online forum sites would explicitly record such replying relationship, making it hard to for both users and computers to digest the information buried in a thread discussion. In this paper, we propose a probabilistic model in the Conditional Random Fields framework to predict the replying structure for a threaded online discussion. Different from previous thread reconstruction methods, most of which fail to consider dependency between the posts, we cast the problem as a supervised structure learning problem to incorporate the features describing the structural dependency among the discussion content and learn their relationship. Experiment results on three different online forums show that the proposed method can well capture the replying structures in online discussion threads, and multiple tasks such as forum search and question answering can benefit from the reconstructed replying structures.

#index 1598378
#* Mining topics on participations for community discovery
#@ Guoqing Zheng;Jinwen Guo;Lichun Yang;Shengliang Xu;Shenghua Bao;Zhong Su;Dingyi Han;Yong Yu
#t 2011
#c 13
#% 310514
#% 313959
#% 722904
#% 769906
#% 840964
#% 869480
#% 869485
#% 1055681
#% 1055740
#% 1117074
#% 1130929
#% 1211773
#% 1272187
#% 1396210
#% 1399996
#! Community discovery on large-scale linked document corpora has been a hot research topic for decades. There are two types of links. The first one, which we call d2d-link, indicates connectiveness among different documents, such as blog references and research paper citations. The other one, which we call u2u-link, represents co-occurrences or simultaneous participations of different users in one document and typically each document from u2u-link corpus has more than one user/author. Examples of u2u-link data covers email archives and research paper co-authorship networks. Community discovery in d2d-link data has achieved much success, while methods for that in u2u-link data either make no use of the textual content of the documents or make oversimplified assumptions about the users and the textual content. In this paper we propose a general approach of community discovery for u2u-link data, i.e., multiple user data, by placing topical variables on multiple authors' participations in documents. Experiments on a research proceeding co-authorship corpus and a New York Times news corpus show the effectiveness of our model.

#index 1598379
#* Authorship classification: a discriminative syntactic tree mining approach
#@ Sangkyum Kim;Hyungsul Kim;Tim Weninger;Jiawei Han;Hyun Duk Kim
#t 2011
#c 13
#% 27842
#% 184486
#% 428405
#% 464996
#% 577218
#% 578558
#% 729941
#% 729957
#% 844421
#% 852013
#% 879670
#% 916421
#% 939736
#% 984061
#% 1063502
#% 1083649
#% 1190357
#% 1194641
#% 1206650
#% 1214677
#% 1214686
#% 1251650
#% 1261581
#% 1392491
#% 1495585
#% 1676557
#% 1682064
#! In the past, there have been dozens of studies on automatic authorship classification, and many of these studies concluded that the writing style is one of the best indicators for original authorship. From among the hundreds of features which were developed, syntactic features were best able to reflect an author's writing style. However, due to the high computational complexity for extracting and computing syntactic features, only simple variations of basic syntactic features such as function words, POS(Part of Speech) tags, and rewrite rules were considered. In this paper, we propose a new feature set of k-embedded-edge subtree patterns that holds more syntactic information than previous feature sets. We also propose a novel approach to directly mining them from a given set of syntactic trees. We show that this approach reduces the computational burden of using complex syntactic structures as the feature set. Comprehensive experiments on real-world datasets demonstrate that our approach is reliable and more accurate than previous studies.

#index 1598380
#* On theme location discovery for travelogue services
#@ Mao Ye;Rong Xiao;Wang-Chien Lee;Xing Xie
#t 2011
#c 13
#% 252011
#% 308050
#% 311027
#% 340899
#% 480467
#% 565545
#% 577224
#% 766441
#% 818256
#% 891559
#% 983820
#% 1203682
#% 1358032
#% 1399973
#% 1399979
#% 1480778
#! In this paper, we aim to develop a travelogue service that discovers and conveys various travelogue digests, in form of theme locations, geographical scope, traveling trajectory and location snippet, to users. In this service, theme locations in a travelogue are the core information to discover. Thus we aim to address the problem of theme location discovery to enable the above travelogue services. Due to the inherent ambiguity of location relevance, we perform location relevance mining (LRM) in two complementary angles, relevance classification and relevance ranking, to provide comprehensive understanding of locations. Furthermore, we explore the textual (e.g., surrounding words) and geographical (e.g., geographical relationship among locations) features of locations to develop a co-training model for enhancement of classification performance. Built upon the mining result of LRM, we develop a series of techniques for provisioning of the aforementioned travelogue digests in our travelogue system. Finally, we conduct comprehensive experiments on collected travelogues to evaluate the performance of our location relevance mining techniques and demonstrate the effectiveness of the travelogue service.

#index 1598381
#* Effective sentiment stream analysis with self-augmenting training and demand-driven projection
#@ Ismael Santana Silva;Janaína Gomide;Adriano Veloso;Wagner Meira, Jr.;Renato Ferreira
#t 2011
#c 13
#% 152934
#% 197394
#% 204531
#% 273900
#% 310500
#% 342600
#% 342639
#% 498622
#% 577214
#% 729418
#% 729932
#% 729965
#% 737328
#% 854646
#% 879596
#% 1016245
#% 1074047
#% 1083714
#% 1116999
#% 1127964
#% 1184648
#% 1214635
#% 1214654
#% 1277969
#% 1301020
#% 1384224
#% 1455666
#% 1529062
#% 1535338
#! How do we analyze sentiments over a set of opinionated Twitter messages? This issue has been widely studied in recent years, with a prominent approach being based on the application of classification techniques. Basically, messages are classified according to the implicit attitude of the writer with respect to a query term. A major concern, however, is that Twitter (and other media channels) follows the data stream model, and thus the classifier must operate with limited resources, including labeled data for training classification models. This imposes serious challenges for current classification techniques, since they need to be constantly fed with fresh training messages, in order to track sentiment drift and to provide up-to-date sentiment analysis. We propose solutions to this problem. The heart of our approach is a training augmentation procedure which takes as input a small training seed, and then it automatically incorporates new relevant messages to the training data. Classification models are produced on-the-fly using association rules, which are kept up-to-date in an incremental fashion, so that at any given time the model properly reflects the sentiments in the event being analyzed. In order to track sentiment drift, training messages are projected on a demand driven basis, according to the content of the message being classified. Projecting the training data offers a series of advantages, including the ability to quickly detect trending information emerging in the stream. We performed the analysis of major events in 2010, and we show that the prediction performance remains about the same, or even increases, as the stream passes and new training messages are acquired. This result holds for different languages, even in cases where sentiment distribution changes over time, or in cases where the initial training seed is rather small. We derive lower-bounds for prediction performance, and we show that our approach is extremely effective under diverse learning scenarios, providing gains that range from 7% to 58%.

#index 1598382
#* Hypergeometric language models for republished article finding
#@ Manos Tsagkias;Maarten de Rijke;Wouter Weerkamp
#t 2011
#c 13
#% 144224
#% 226103
#% 248214
#% 260243
#% 262096
#% 280850
#% 340294
#% 340948
#% 350859
#% 355154
#% 616528
#% 642974
#% 840903
#% 875981
#% 879600
#% 955496
#% 956507
#% 1019082
#% 1026918
#% 1065411
#% 1074104
#% 1074122
#% 1130858
#% 1166531
#% 1190061
#% 1190063
#% 1192974
#% 1270458
#% 1366614
#% 1392466
#% 1450913
#% 1529959
#% 1536561
#% 1733334
#% 1742070
#! Republished article finding is the task of identifying instances of articles that have been published in one source and republished more or less verbatim in another source, which is often a social media source. We address this task as an ad hoc retrieval problem, using the source article as a query. Our approach is based on language modeling. We revisit the assumptions underlying the unigram language model taking into account the fact that in our setup queries are as long as complete news articles. We argue that in this case, the underlying generative assumption of sampling words from a document with replacement, i.e., the multinomial modeling of documents, produces less accurate query likelihood estimates. To make up for this discrepancy, we consider distributions that emerge from sampling without replacement: the central and non-central hypergeometric distributions. We present two retrieval models that build on top of these distributions: a log odds model and a bayesian model where document parameters are estimated using the Dirichlet compound multinomial distribution. We analyse the behavior of our new models using a corpus of news articles and blog posts and find that for the task of republished article finding, where we deal with queries whose length approaches the length of the documents to be retrieved, models based on distributions associated with sampling without replacement outperform traditional models based on multinomial distributions.

#index 1598383
#* Estimation methods for ranking recent information
#@ Miles Efron;Gene Golovchinsky
#t 2011
#c 13
#% 262096
#% 340901
#% 342707
#% 730070
#% 750863
#% 960414
#% 1001644
#% 1130897
#% 1132901
#% 1166533
#% 1166534
#% 1297058
#% 1384244
#% 1384287
#% 1399966
#% 1399976
#% 1450843
#% 1450965
#% 1477588
#% 1523411
#% 1536522
#% 1692327
#! Temporal aspects of documents can impact relevance for certain kinds of queries. In this paper, we build on earlier work of modeling temporal information. We propose an extension to the Query Likelihood Model that incorporates query-specific information to estimate rate parameters, and we introduce a temporal factor into language model smoothing and query expansion using pseudo-relevance feedback. We evaluate these extensions using a Twitter corpus and two newspaper article collections. Results suggest that, compared to prior approaches, our models are more effective at capturing the temporal variability of relevance associated with some topics.

#index 1598384
#* Query by document via a decomposition-based two-level retrieval approach
#@ Linkai Weng;Zhiwei Li;Rui Cai;Yaoxue Zhang;Yuezhi Zhou;Laurence T. Yang;Lei Zhang
#t 2011
#c 13
#% 255137
#% 268079
#% 279755
#% 280819
#% 300176
#% 330687
#% 342707
#% 345087
#% 347225
#% 387427
#% 397133
#% 420487
#% 578337
#% 625399
#% 722904
#% 730065
#% 746927
#% 766430
#% 855200
#% 867054
#% 874505
#% 874972
#% 879587
#% 879600
#% 961174
#% 1023422
#% 1055680
#% 1166508
#% 1250561
#% 1482205
#! Retrieving similar documents from a large-scale text corpus according to a given document is a fundamental technique for many applications. However, most of existing indexing techniques have difficulties to address this problem due to special properties of a document query, e.g. high dimensionality, sparse representation and semantic concern. Towards addressing this problem, we propose a two-level retrieval solution based on a document decomposition idea. A document is decomposed to a compact vector and a few document specific keywords by a dimension reduction approach. The compact vector embodies the major semantics of a document, and the document specific keywords complement the discriminative power lost in dimension reduction process. We adopt locality sensitive hashing (LSH) to index the compact vectors, which guarantees to quickly find a set of related documents according to the vector of a query document. Then we re-rank documents in this set by their document specific keywords. In experiments, we obtained promising results on various datasets in terms of both accuracy and performance. We demonstrated that this solution is able to index large-scale corpus for efficient similarity-based document retrieval.

#index 1598385
#* Integrating hierarchical feature selection and classifier training for multi-label image annotation
#@ Cheng Jin;Chunlei Yang
#t 2011
#c 13
#% 190581
#% 209021
#% 304876
#% 311027
#% 318785
#% 341442
#% 428926
#% 451675
#% 457912
#% 458049
#% 466263
#% 466268
#% 466725
#% 580509
#% 635755
#% 642989
#% 642990
#% 736300
#% 780804
#% 836778
#% 839852
#% 839975
#% 884025
#% 905267
#% 1148273
#% 1502499
#! It is well accepted that using high-dimensional multi-modal visual features for image content representation and classifier training may achieve more sufficient characterization of the diverse visual properties of the images and further result in higher discrimination power of the classifiers. However, training the classifiers in a high-dimensional multi-modal feature space requires a large number of labeled training images, which will further result in the problem of curse of dimensionality. To tackle this problem, a hierarchical feature subset selection algorithm is proposed to enable more accurate image classification, where the processes for feature selection and classifier training are seamlessly integrated in a single framework. First, a feature hierarchy (i.e., concept tree for automatic feature space partition and organization) is used to automatically partition high-dimensional heterogeneous multi-modal visual features into multiple low-dimensional homogeneous single-modal feature subsets according to their certain physical meanings and each of them is used to characterize one certain type of the diverse visual properties of the images. Second, principal component analysis (PCA) is performed on each homogeneous singlemodal feature subset to select the most representative feature dimensions and a weak classifier is learned simultaneously. After the weak classifiers and their representative feature dimensions are available for all these homogeneous single-modal feature subsets, they are combined to generate an ensemble image classifier and achieve hierarchical feature subset selection. Our experiments on a specific domain of natural images have also obtained very positive results.

#index 1598386
#* Efficient manifold ranking for image retrieval
#@ Bin Xu;Jiajun Bu;Chun Chen;Deng Cai;Xiaofei He;Wei Liu;Jiebo Luo
#t 2011
#c 13
#% 262096
#% 268079
#% 290830
#% 341269
#% 443984
#% 635689
#% 642989
#% 780688
#% 789800
#% 875948
#% 876080
#% 905203
#% 940440
#% 987240
#% 1034724
#% 1040539
#% 1077150
#% 1190090
#% 1211844
#% 1227644
#% 1275220
#% 1450846
#% 1450849
#% 1484439
#% 1558464
#% 1856210
#% 1857498
#! Manifold Ranking (MR), a graph-based ranking algorithm, has been widely applied in information retrieval and shown to have excellent performance and feasibility on a variety of data types. Particularly, it has been successfully applied to content-based image retrieval, because of its outstanding ability to discover underlying geometrical structure of the given image database. However, manifold ranking is computationally very expensive, both in graph construction and ranking computation stages, which significantly limits its applicability to very large data sets. In this paper, we extend the original manifold ranking algorithm and propose a new framework named Efficient Manifold Ranking (EMR). We aim to address the shortcomings of MR from two perspectives: scalable graph construction and efficient computation. Specifically, we build an anchor graph on the data set instead of the traditional k-nearest neighbor graph, and design a new form of adjacency matrix utilized to speed up the ranking computation. The experimental results on a real world image database demonstrate the effectiveness and efficiency of our proposed method. With a comparable performance to the original manifold ranking, our method significantly reduces the computational time, makes it a promising method to large scale real world retrieval problems.

#index 1598387
#* Mining weakly labeled web facial images for search-based face annotation
#@ Dayong Wang;Steven C.H. Hoi;Ying He
#t 2011
#c 13
#% 235342
#% 315986
#% 434882
#% 457912
#% 642989
#% 729344
#% 730186
#% 766452
#% 780804
#% 780809
#% 884044
#% 913845
#% 975105
#% 1038781
#% 1074095
#% 1130875
#% 1176935
#% 1211772
#% 1279781
#% 1502510
#% 1536524
#% 1775706
#! In this paper, we investigate a search-based face annotation framework by mining weakly labeled facial images that are freely available on the internet. A key component of such a search-based annotation paradigm is to build a database of facial images with accurate labels. This is however challenging since facial images on the WWW are often noisy and incomplete. To improve the label quality of raw web facial images, we propose an effective Unsupervised Label Refinement (ULR) approach for refining the labels of web facial images by exploring machine learning techniques. We develop effective optimization algorithms to solve the large-scale learning tasks efficiently, and conduct an extensive empirical study on a web facial image database with 400 persons and 40,000 web facial images. Encouraging results showed that the proposed ULR technique can significantly boost the performance of the promising search-based face annotation scheme.

#index 1598388
#* Temporal index sharding for space-time efficiency in archive search
#@ Avishek Anand;Srikanta Bedathur;Klaus Berberich;Ralf Schenkel
#t 2011
#c 13
#% 118741
#% 227941
#% 570884
#% 571296
#% 577370
#% 754058
#% 867054
#% 978374
#% 987257
#% 1024551
#% 1077150
#% 1166533
#% 1292507
#% 1392437
#% 1480887
#% 1482248
#% 1482302
#! Time-travel queries that couple temporal constraints with keyword queries are useful in searching large-scale archives of time-evolving content such as the web archives or wikis. Typical approaches for efficient evaluation of these queries involve slicing either the entire collection [20] or individual index lists [10] along the time-axis. Both these methods are not satisfactory since they sacrifice compactness of index for processing efficiency making them either too big or, otherwise, too slow. We present a novel index organization scheme that shards each index list with almost zero increase in index size but still minimizes the cost of reading index entries during query processing. Based on the optimal sharding thus btained, we develop a practically efficient sharding that takes into account the different costs of random and sequential accesses. Our algorithm merges shards from the optimal solution to allow for a few extra sequential accesses while gaining significantly by reducing the number of random accesses. We empirically establish the effectiveness of our sharding scheme with experiments over the revision history of the English Wikipedia between 2001-2005 (approx 700 GB) and an archive of U.K. governmental web sites (approx 400 GB). Our results demonstrate the feasibility of faster time-travel query processing with no space overhead.

#index 1598389
#* Inverted indexes for phrases and strings
#@ Manish Patil;Sharma V. Thankachan;Rahul Shah;Wing-Kai Hon;Jeffrey Scott Vitter;Sabrina Chandrasekaran
#t 2011
#c 13
#% 37861
#% 212665
#% 379390
#% 379448
#% 379516
#% 387427
#% 453572
#% 498264
#% 516361
#% 516513
#% 547946
#% 769697
#% 781169
#% 835496
#% 867054
#% 879611
#% 1014807
#% 1022545
#% 1078392
#% 1099208
#% 1116726
#% 1247841
#% 1267020
#% 1280254
#% 1379539
#% 1415433
#% 1488200
#% 1490339
#% 1529933
#% 1529934
#% 1529960
#% 1529961
#% 1697817
#% 1701405
#% 1920354

#index 1598390
#* Faster temporal range queries over versioned text
#@ Jinru He;Torsten Suel
#t 2011
#c 13
#% 118741
#% 420491
#% 570319
#% 656274
#% 766445
#% 838407
#% 864446
#% 867054
#% 874993
#% 956535
#% 987257
#% 1051061
#% 1055710
#% 1190095
#% 1292507
#% 1328137
#% 1392437
#% 1392439
#% 1426548
#% 1482248
#% 1482302
#% 1688264
#% 1715618
#! Versioned textual collections are collections that retain multiple versions of a document as it evolves over time. Important large-scale examples are Wikipedia and the web collection of the Internet Archive. Search queries over such collections often use keywords as well as temporal constraints, most commonly a time range of interest. In this paper, we study how to support such temporal range queries over versioned text. Our goal is to process these queries faster than the corresponding keyword-only queries, by exploiting the additional constraint. A simple approach might partition the index into different time ranges, and then access only the relevant parts. However, specialized inverted index compression techniques are crucial for large versioned collections, and a naive partitioning can negatively affect index size and query throughput. We show how to achieve high query throughput by using smart index partitioning techniques that take index compression into account. Experiments on over 85 million versions of Wikipedia articles show that queries can be executed in a few milliseconds on memory-based index structures, and only slightly more time on disk-based structures. We also show how to efficiently support the recently proposed stable top-k search primitive on top of our schemes.

#index 1598391
#* Indexing strategies for graceful degradation of search quality
#@ Shuai Ding;Sreenivas Gollapudi;Samuel Ieong;Krishnaram Kenthapadi;Alexandros Ntoulas
#t 2011
#c 13
#% 190611
#% 212665
#% 280856
#% 343843
#% 481748
#% 577361
#% 783474
#% 878657
#% 1016183
#% 1292508
#% 1450840
#% 1482223
#% 1688254
#! Large web search engines process billions of queries each day over tens of billions of documents with often very stringent requirements for a user's search experience, in particular, low latency and highly relevant search results. Index generation and serving are key to satisfying both these requirements. For example, the load to search engines can vary drastically when popular events happen around the world. In the case when the load is exceeding what the search engine can serve, queries will get dropped. This results in an un- graceful degradation in search quality. Another example that could increase the query load and affect the user's search experience are ambiguous queries which often result in the execution of multiple query alterations in the back end. In this paper, we look into the problem of designing robust indexing strategies, i.e. strategies that allow for a graceful degradation of search quality in both the above scenarios. We study the problems of index generation and serving using the notions of document allocation, server selection, and document replication. We explore the space of efficient algorithms for these problems and empirically corroborate with existing theory that it is hard to optimally solve the alocation and selection problems without any replication. We propose a greedy replication algorithm and study its performance under different choices of allocation and selection. Further, we show hat under random selection and allocation, our algorithm is optimal.

#index 1598392
#* Incremental diversification for very large sets: a streaming-based approach
#@ Enrico Minack;Wolf Siberski;Wolfgang Nejdl
#t 2011
#c 13
#% 262112
#% 300179
#% 428155
#% 642975
#% 805841
#% 879618
#% 978475
#% 1074133
#% 1166473
#% 1181244
#% 1190093
#% 1190165
#% 1206662
#% 1227591
#% 1269314
#% 1348342
#% 1400021
#% 1742093
#! Result diversification is an effective method to reduce the risk that none of the returned results satisfies a user's query intention. It has been shown to decrease query abandonment substantially. On the other hand, computing an optimally diverse set is NP-hard for the usual objectives. Existing greedy diversification algorithms require random access to the input set, rendering them impractical in the context of large result sets or continuous data. To solve this issue, we present a novel diversification approach which treats the input as a stream and processes each element in an incremental fashion, maintaining a near-optimal diverse set at any point in the stream. Our approach exhibits a linear computation and constant memory complexity with respect to input size, without significant loss of diversification quality. In an extensive evaluation on several real-world data sets, we show the applicability and efficiency of our algorithm for large result sets as well as for continuous query scenarios such as news stream subscriptions.

#index 1598393
#* Intent-aware search result diversification
#@ Rodrygo L.T. Santos;Craig Macdonald;Iadh Ounis
#t 2011
#c 13
#% 262112
#% 397161
#% 590523
#% 642975
#% 642982
#% 754059
#% 818255
#% 879618
#% 926881
#% 944349
#% 987260
#% 1019084
#% 1024548
#% 1074065
#% 1074113
#% 1074133
#% 1166473
#% 1174573
#% 1227591
#% 1227709
#% 1263586
#% 1292528
#% 1292596
#% 1400021
#% 1451031
#% 1456843
#% 1482296
#% 1599314
#% 1621236
#% 1697422
#% 1697424
#! Search result diversification has gained momentum as a way to tackle ambiguous queries. An effective approach to this problem is to explicitly model the possible aspects underlying a query, in order to maximise the estimated relevance of the retrieved documents with respect to the different aspects. However, such aspects themselves may represent information needs with rather distinct intents (e.g., informational or navigational). Hence, a diverse ranking could benefit from applying intent-aware retrieval models when estimating the relevance of documents to different aspects. In this paper, we propose to diversify the results retrieved for a given query, by learning the appropriateness of different retrieval models for each of the aspects underlying this query. Thorough experiments within the evaluation framework provided by the diversity task of the TREC 2009 and 2010 Web tracks show that the proposed approach can significantly improve state-of-the-art diversification approaches.

#index 1598394
#* Parameterized concept weighting in verbose queries
#@ Michael Bendersky;Donald Metzler;W. Bruce Croft
#t 2011
#c 13
#% 35937
#% 169781
#% 218978
#% 262096
#% 397127
#% 411760
#% 577224
#% 750863
#% 818239
#% 818262
#% 840846
#% 976952
#% 987229
#% 987231
#% 987356
#% 1074081
#% 1074112
#% 1227636
#% 1227647
#% 1306081
#% 1355019
#% 1415737
#% 1450848
#% 1450865
#% 1450900
#% 1450901
#% 1482186
#% 1482203
#% 1482361
#% 1715627
#! The majority of the current information retrieval models weight the query concepts (e.g., terms or phrases) in an unsupervised manner, based solely on the collection statistics. In this paper, we go beyond the unsupervised estimation of concept weights, and propose a parameterized concept weighting model. In our model, the weight of each query concept is determined using a parameterized combination of diverse importance features. Unlike the existing supervised ranking methods, our model learns importance weights not only for the explicit query concepts, but also for the latent concepts that are associated with the query through pseudo-relevance feedback. The experimental results on both newswire and web TREC corpora show that our model consistently and significantly outperforms a wide range of state-of-the-art retrieval models. In addition, our model significantly reduces the number of latent concepts used for query expansion compared to the non-parameterized pseudo-relevance feedback based models.

#index 1598395
#* UPS: efficient privacy protection in personalized web search
#@ Gang Chen;He Bai;Lidan Shou;Ke Chen;Yunjun Gao
#t 2011
#c 13
#% 309095
#% 387427
#% 399057
#% 637576
#% 754126
#% 818207
#% 818224
#% 818259
#% 832349
#% 838547
#% 869536
#% 874989
#% 881540
#% 956552
#% 956553
#% 983653
#% 987283
#% 1035583
#% 1074071
#% 1210603
#% 1231855
#% 1250362
#% 1292631
#% 1400126
#% 1545564
#% 1650569
#! In recent years, personalized web search (PWS) has demonstrated effectiveness in improving the quality of search service on the Internet. Unfortunately, the need for collecting private information in PWS has become a major barrier for its wide proliferation. We study privacy protection in PWS engines which capture personalities in user profiles. We propose a PWS framework called UPS that can generalize profiles in for each query according to user-specified privacy requirements. Two predictive metrics are proposed to evaluate the privacy breach risk and the query utility for hierarchical user profile. We develop two simple but effective generalization algorithms for user profiles allowing for query-level customization using our proposed metrics. We also provide an online prediction mechanism based on query utility for deciding whether to personalize a query in UPS. Extensive experiments demonstrate the efficiency and effectiveness of our framework.

#index 1598396
#* Handling data sparsity in collaborative filtering using emotion and semantic based features
#@ Yashar Moshfeghi;Benjamin Piwowarski;Joemon M. Jose
#t 2011
#c 13
#% 157521
#% 173879
#% 330687
#% 428246
#% 528156
#% 571360
#% 578684
#% 643007
#% 722904
#% 734592
#% 813966
#% 818216
#% 939346
#% 1125374
#% 1195834
#% 1227603
#% 1249487
#% 1287222
#% 1297086
#% 1299754
#% 1411263
#% 1587416
#! Collaborative filtering (CF) aims to recommend items based on prior user interaction. Despite their success, CF techniques do not handle data sparsity well, especially in the case of the cold start problem where there is no past rating for an item. In this paper, we provide a framework, which is able to tackle such issues by considering item-related emotions and semantic data. In order to predict the rating of an item for a given user, this framework relies on an extension of Latent Dirichlet Allocation, and on gradient boosted trees for the final prediction. We apply this framework to movie recommendation and consider two emotion spaces extracted from the movie plot summary and the reviews, and three semantic spaces: actor, director, and genre. Experiments with the 100K and 1M MovieLens datasets show that including emotion and semantic information significantly improves the accuracy of prediction and improves upon the state-of-the-art CF techniques. We also analyse the importance of each feature space and describe some uncovered latent groups.

#index 1598397
#* Fast context-aware recommendations with factorization machines
#@ Steffen Rendle;Zeno Gantner;Christoph Freudenthaler;Lars Schmidt-Thieme
#t 2011
#c 13
#% 330687
#% 801785
#% 870402
#% 1083671
#% 1116993
#% 1190066
#% 1214623
#% 1214666
#% 1262980
#% 1287231
#% 1287255
#% 1287260
#% 1355024
#% 1476452
#% 1476453
#% 1480681
#% 1535439
#% 1544083
#! The situation in which a choice is made is an important information for recommender systems. Context-aware recommenders take this information into account to make predictions. So far, the best performing method for context-aware rating prediction in terms of predictive accuracy is Multiverse Recommendation based on the Tucker tensor factorization model. However this method has two drawbacks: (1) its model complexity is exponential in the number of context variables and polynomial in the size of the factorization and (2) it only works for categorical context variables. On the other hand there is a large variety of fast but specialized recommender methods which lack the generality of context-aware methods. We propose to apply Factorization Machines (FMs) to model contextual information and to provide context-aware rating predictions. This approach results in fast context-aware recommendations because the model equation of FMs can be computed in linear time both in the number of context variables and the factorization size. For learning FMs, we develop an iterative optimization method that analytically finds the least-square solution for one parameter given the other ones. Finally, we show empirically that our approach outperforms Multiverse Recommendation in prediction quality and runtime.

#index 1598398
#* Filtering semi-structured documents based on faceted feedback
#@ Lanbo Zhang;Yi Zhang;Qianli Xing
#t 2011
#c 13
#% 54435
#% 397155
#% 465754
#% 642985
#% 763708
#% 766450
#% 818214
#% 879621
#% 879626
#% 961194
#% 987202
#% 987225
#% 1074125
#% 1227623
#% 1250186
#% 1450874
#! Existing adaptive filtering systems learn user profiles based on users' relevance judgments on documents. In some cases, users have some prior knowledge about what features are important for a document to be relevant. For example, a Spanish speaker may only want news written in Spanish, and thus a relevant document should contain the feature "Language: Spanish"; a researcher working on HIV knows an article with the medical subject "Subject: AIDS" is very likely to be interesting to him/her. Semi-structured documents with rich faceted metadata are increasingly prevalent over the Internet. Motivated by the commonly used faceted search interface in e-commerce, we study whether users' prior knowledge about faceted features could be exploited for filtering semi-structured documents. We envision two faceted feedback solicitation mechanisms, and propose a novel user profile learning algorithm that can incorporate user feedback on features. To evaluate the proposed work, we use two data sets from the TREC filtering track, and conduct a user study on Amazon Mechanical Turk. Our experimental results show that user feedback on faceted features is useful for filtering. The new user profile learning algorithm can effectively learn from user feedback on faceted features and performs better than several other methods adapted from the feature-based feedback techniques proposed for retrieval and text classification tasks in previous work.

#index 1598400
#* ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews
#@ Samaneh Moghaddam;Martin Ester
#t 2011
#c 13
#% 280819
#% 642990
#% 722904
#% 727877
#% 769892
#% 805873
#% 881498
#% 939896
#% 956510
#% 1055682
#% 1074055
#% 1127964
#% 1190068
#% 1250423
#% 1292576
#% 1451218
#% 1482445
#! Today, more and more product reviews become available on the Internet, e.g., product review forums, discussion groups, and Blogs. However, it is almost impossible for a customer to read all of the different and possibly even contradictory opinions and make an informed decision. Therefore, mining online reviews (opinion mining) has emerged as an interesting new research direction. Extracting aspects and the corresponding ratings is an important challenge in opinion mining. An aspect is an attribute or component of a product, e.g. 'screen' for a digital camera. It is common that reviewers use different words to describe an aspect (e.g. 'LCD', 'display', 'screen'). A rating is an intended interpretation of the user satisfaction in terms of numerical values. Reviewers usually express the rating of an aspect by a set of sentiments, e.g. 'blurry screen'. In this paper we present three probabilistic graphical models which aim to extract aspects and corresponding ratings of products from online reviews. The first two models extend standard PLSI and LDA to generate a rated aspect summary of product reviews. As our main contribution, we introduce Interdependent Latent Dirichlet Allocation (ILDA) model. This model is more natural for our task since the underlying probabilistic assumptions (interdependency between aspects and ratings) are appropriate for our problem domain. We conduct experiments on a real life dataset, Epinions.com, demonstrating the improved effectiveness of the ILDA model in terms of the likelihood of a held-out test set, and the accuracy of aspects and aspect ratings.

#index 1598401
#* Clickthrough-based latent semantic models for web search
#@ Jianfeng Gao;Kristina Toutanova;Wen-tau Yih
#t 2011
#c 13
#% 211942
#% 262096
#% 279755
#% 280819
#% 280851
#% 309095
#% 340948
#% 397128
#% 643056
#% 681851
#% 722904
#% 740915
#% 816170
#% 840846
#% 879587
#% 1227621
#% 1292709
#% 1338581
#% 1338620
#% 1399978
#% 1417055
#% 1450887
#% 1472297
#% 1481560
#% 1482292
#% 1604681
#% 1767909
#! This paper presents two new document ranking models for Web search based upon the methods of semantic representation and the statistical translation-based approach to information retrieval (IR). Assuming that a query is parallel to the titles of the documents clicked on for that query, large amounts of query-title pairs are constructed from clickthrough data; two latent semantic models are learned from this data. One is a bilingual topic model within the language modeling framework. It ranks documents for a query by the likelihood of the query being a semantics-based translation of the documents. The semantic representation is language independent and learned from query-title pairs, with the assumption that a query and its paired titles share the same distribution over semantic topics. The other is a discriminative projection model within the vector space modeling framework. Unlike Latent Semantic Analysis and its variants, the projection matrix in our model, which is used to map from term vectors into sematic space, is learned discriminatively such that the distance between a query and its paired title, both represented as vectors in the projected semantic space, is smaller than that between the query and the titles of other documents which have no clicks for that query. These models are evaluated on the Web search task using a real world data set. Results show that they significantly outperform their corresponding baseline models, which are state-of-the-art.

#index 1598402
#* Regularized latent semantic indexing
#@ Quan Wang;Jun Xu;Hang Li;Nick Craswell
#t 2011
#c 13
#% 274586
#% 280819
#% 321635
#% 722904
#% 879587
#% 963669
#% 967300
#% 1050550
#% 1108903
#% 1117375
#% 1127559
#% 1176853
#% 1229386
#% 1399406
#% 1400001
#% 1523858
#! Topic modeling can boost the performance of information retrieval, but its real-world application is limited due to scalability issues. Scaling to larger document collections via parallelization is an active area of research, but most solutions require drastic steps such as vastly reducing input vocabulary. We introduce Regularized Latent Semantic Indexing (RLSI), a new method which is designed for parallelization. It is as effective as existing topic models, and scales to larger datasets without reducing input vocabulary. RLSI formalizes topic modeling as a problem of minimizing a quadratic loss function regularized by l₂ and/or l₁ norm. This formulation allows the learning process to be decomposed into multiple sub-optimization problems which can be optimized in parallel, for example via MapReduce. We particularly propose adopting l₂ norm on topics and l₁ norm on document representations, to create a model with compact and readable topics and useful for retrieval. Relevance ranking experiments on three TREC datasets show that RLSI performs better than LSI, PLSI, and LDA, and the improvements are sometimes statistically significant. Experiments on a web dataset, containing about 1.6 million documents and 7 million terms, demonstrate a similar boost in performance on a larger corpus and vocabulary than in previous studies.

#index 1598403
#* Multimedia answering: enriching text QA with media information
#@ Liqiang Nie;Meng Wang;Zhengjun Zha;Guangda Li;Tat-Seng Chua
#t 2011
#c 13
#% 156430
#% 330619
#% 397161
#% 642978
#% 721727
#% 760850
#% 789320
#% 815303
#% 839962
#% 946522
#% 958453
#% 997240
#% 1035587
#% 1047396
#% 1055738
#% 1074112
#% 1074162
#% 1131845
#% 1131871
#% 1183152
#% 1184341
#% 1190358
#% 1227616
#% 1264814
#% 1264816
#% 1279581
#% 1298163
#% 1389558
#% 1520071
#% 1712153
#% 1735602
#% 1775905
#% 1858958
#! Existing community question-answering forums usually provide only textual answers. However, for many questions, pure texts cannot provide intuitive information, while image or video contents are more appropriate. In this paper, we introduce a scheme that is able to enrich text answers with image and video information. Our scheme investigates a rich set of techniques including question/answer classification, query generation, image and video search reranking, etc. Given a question and the community-contributed answer, our approach is able to determine which type of media information should be added, and then automatically collects data from Internet to enrich the textual answer. Different from some efforts that attempt to directly answer questions with image and video data, our approach is built based on the community-contributed textual answers and thus it is more feasible and able to deal with more complex questions. We have conducted empirical study on more than 3,000 QA pairs and the results demonstrate the effectiveness of our approach.

#index 1598404
#* Enhancing multi-label music genre classification through ensemble techniques
#@ Chris Sanden;John Z. Zhang
#t 2011
#c 13
#% 451727
#% 742990
#% 910999
#% 950571
#% 1183199
#% 1264044
#% 1279869
#% 1295813
#% 1390402
#% 1520679
#% 1606401
#% 1736146
#! In the field of Music Information Retrieval (MIR), multi-label genre classification is the problem of assigning one or more genre labels to a music piece. In this work, we propose a set of ensemble techniques, which are specific to the task of multi-label genre classification. Our goal is to enhance classification performance by combining multiple classifiers. In addition, we also investigate some existing ensemble techniques from machine learning. The effectiveness of these techniques is demonstrated through a set of empirical experiments and various related issues are discussed. To the best of our knowledge, there has been limited work on applying ensemble techniques to multi-label genre classification in the literature and we consider the results in this work as our initial efforts toward this end. The significance of our work has two folds: (1) proposing a set of ensemble techniques specific to music genre classification and (2) shedding light on further research along this direction.

#index 1598405
#* Picasso - to sing, you must close your eyes and draw
#@ Aleksandar Stupar;Sebastian Michel
#t 2011
#c 13
#% 248796
#% 464888
#% 477825
#% 479973
#% 760805
#% 762054
#% 780787
#% 818938
#% 839978
#% 952823
#% 997224
#% 1162346
#% 1279931
#% 1484454
#% 1857840
#% 1857842
#% 1858178
#! We study the problem of automatically assigning appropriate music pieces to a picture or, in general, series of pictures. This task, commonly referred to as soundtrack suggestion, is non-trivial as it requires a lot of human attention and a good deal of experience, with master pieces distinguished, e.g., with the Academy Award for Best Original Score. We put forward PICASSO to solve this task in a fully automated way. PICASSO makes use of genuine samples obtained from first-class contemporary movies. Hence, the training set can be arbitrarily large and is also inexpensive to obtain but still provides an excellent source of information. At query time, PICASSO employs a three-level algorithm. First, it selects for a given query image a ranking of the most similar screenshots taken, and subsequently, selects for each screenshot the most similar songs to the music played in the movie when the screenshot was taken. Last, it issues a top-K aggregation algorithm to find the overall best suitable songs available. We have created a large training set consisting of over 40,000 image/soundtrack samples obtained from 28 movies and evaluated the suitability of PICASSO by means of a user study.

#index 1598406
#* Enhanced results for web search
#@ Kevin Haas;Peter Mika;Paul Tarjan;Roi Blanco
#t 2011
#c 13
#% 325001
#% 397130
#% 783560
#% 818226
#% 818228
#% 838422
#% 954948
#% 987208
#% 987209
#% 1055815
#% 1063493
#% 1074092
#% 1089478
#% 1120997
#% 1131703
#% 1153052
#% 1153306
#% 1166525
#% 1190055
#% 1217267
#% 1292652
#% 1292763
#% 1333456
#% 1409918
#% 1409949
#% 1696295
#! "Ten blue links" have defined web search results for the last fifteen years -- snippets of text combined with document titles and URLs. In this paper, we establish the notion of enhanced search results that extend web search results to include multimedia objects such as images and video, intent-specific key value pairs, and elements that allow the user to interact with the contents of a web page directly from the search results page. We show that users express a preference for enhanced results both explicitly, and when observed in their search behavior. We also demonstrate the effectiveness of enhanced results in helping users to assess the relevance of search results. Lastly, we show that we can efficiently generate enhanced results to cover a significant fraction of search result pages.

#index 1598407
#* Summarizing the differences in multilingual news
#@ Xiaojun Wan;Houping Jia;Shanshan Huang;Jianguo Xiao
#t 2011
#c 13
#% 194251
#% 262112
#% 318409
#% 397136
#% 397137
#% 747081
#% 755863
#% 787502
#% 794512
#% 815920
#% 816173
#% 818227
#% 939858
#% 1074088
#% 1251647
#% 1251709
#% 1275220
#% 1292504
#% 1292747
#% 1306081
#% 1471262
#% 1471272
#% 1471273
#% 1481542
#% 1481582
#% 1482206
#% 1482428
#% 1484368
#! There usually exist many news articles written in different languages about a hot news event. The news articles in different languages are written in different ways to reflect different standpoints. For example, the Chinese news agencies and the Western news agencies have published many articles to report the same news of "Liu Xiaobo's Nobel Prize" in Chinese and English languages, respectively. The Chinese news articles and the English news articles share something about the news fact in common, but they focus on different aspects in order to reflect different standpoints about the event. In this paper, we investigate the task of multilingual news summarization for the purpose of finding and summarizing the major differences between the news articles about the same event in the Chinese and English languages. We propose a novel constrained co-ranking (C-CoRank) method for addressing this special task. The C-CoRank method adds the constraints between the difference score and the common score of each sentence to the co-ranking process. Evaluation results on the manually labeled test set with 15 news topics show the effectiveness of our proposed method, and the constrained co-ranking method can outperform a few baselines and the typical co-ranking method.

#index 1598408
#* Evolutionary timeline summarization: a balanced optimization framework via iterative substitution
#@ Rui Yan;Xiaojun Wan;Jahna Otterbacher;Liang Kong;Xiaoming Li;Yan Zhang
#t 2011
#c 13
#% 280835
#% 309096
#% 340883
#% 766444
#% 766460
#% 787502
#% 815920
#% 816173
#% 869604
#% 907510
#% 987219
#% 989601
#% 1019144
#% 1074088
#% 1190062
#% 1269818
#% 1399981
#% 1482206
#! Classic news summarization plays an important role with the exponential document growth on the Web. Many approaches are proposed to generate summaries but seldom simultaneously consider evolutionary characteristics of news plus to traditional summary elements. Therefore, we present a novel framework for the web mining problem named Evolutionary Timeline Summarization (ETS). Given the massive collection of time-stamped web documents related to a general news query, ETS aims to return the evolution trajectory along the timeline, consisting of individual but correlated summaries of each date, emphasizing relevance, coverage, coherence and cross-date diversity. ETS greatly facilitates fast news browsing and knowledge comprehension and hence is a necessity. We formally formulate the task as an optimization problem via iterative substitution from a set of sentences to a subset of sentences that satisfies the above requirements, balancing coherence/diversity measurement and local/global summary quality. The optimized substitution is iteratively conducted by incorporating several constraints until convergence. We develop experimental systems to evaluate on 6 instinctively different datasets which amount to 10251 documents. Performance comparisons between different system-generated timelines and manually created ones by human editors demonstrate the effectiveness of our proposed framework in terms of ROUGE metrics.

#index 1598409
#* Ranking related news predictions
#@ Nattiya Kanhabua;Roi Blanco;Michael Matthews
#t 2011
#c 13
#% 169781
#% 577224
#% 722904
#% 730070
#% 766408
#% 770754
#% 807756
#% 879587
#% 881498
#% 907510
#% 961152
#% 983905
#% 987226
#% 1019189
#% 1024551
#% 1133171
#% 1134178
#% 1213423
#% 1227692
#% 1268491
#% 1292696
#% 1375402
#% 1417055
#% 1450855
#% 1450871
#% 1495112
#% 1697416
#! We estimate that nearly one third of news articles contain references to future events. While this information can prove crucial to understanding news stories and how events will develop for a given topic, there is currently no easy way to access this information. We propose a new task to address the problem of retrieving and ranking sentences that contain mentions to future events, which we call ranking related news predictions. In this paper, we formally define this task and propose a learning to rank approach based on 4 classes of features: term similarity, entity-based similarity, topic similarity, and temporal similarity. Through extensive evaluations using a corpus consisting of 1.8 millions news articles and 6,000 manually judged relevance pairs, we show that our approach is able to retrieve a significant number of relevant predictions related to a given topic.

#index 1598410
#* Collective entity linking in web text: a graph-based method
#@ Xianpei Han;Le Sun;Jun Zhao
#t 2011
#c 13
#% 641979
#% 868096
#% 915344
#% 961685
#% 1019082
#% 1074073
#% 1130858
#% 1214667
#% 1250185
#% 1250381
#% 1275012
#% 1292487
#% 1470634
#% 1471184
#% 1484272
#% 1484385
#% 1484390
#% 1700523
#! Entity Linking (EL) is the task of linking name mentions in Web text with their referent entities in a knowledge base. Traditional EL methods usually link name mentions in a document by assuming them to be independent. However, there is often additional interdependence between different EL decisions, i.e., the entities in the same document should be semantically related to each other. In these cases, Collective Entity Linking, in which the name mentions in the same document are linked jointly by exploiting the interdependence between them, can improve the entity linking accuracy. This paper proposes a graph-based collective EL method, which can model and exploit the global interdependence between different EL decisions. Specifically, we first propose a graph-based representation, called Referent Graph, which can model the global interdependence between different EL decisions. Then we propose a collective inference algorithm, which can jointly infer the referent entities of all name mentions by exploiting the interdependence captured in Referent Graph. The key benefit of our method comes from: 1) The global interdependence model of EL decisions; 2) The purely collective nature of the inference algorithm, in which evidence for related EL decisions can be reinforced into high-probability decisions. Experimental results show that our method can achieve significant performance improvement over the traditional EL methods.

#index 1598411
#* From one tree to a forest: a unified solution for structured web data extraction
#@ Qiang Hao;Rui Cai;Yanwei Pang;Lei Zhang
#t 2011
#c 13
#% 271065
#% 278109
#% 330784
#% 480824
#% 577318
#% 654469
#% 705442
#% 729978
#% 805846
#% 840966
#% 879601
#% 881505
#% 989661
#% 1108847
#% 1190073
#% 1227612
#% 1380965
#! Structured data, in the form of entities and associated attributes, has been a rich web resource for search engines and knowledge databases. To efficiently extract structured data from enormous websites in various verticals (e.g., books, restaurants), much research effort has been attracted, but most existing approaches either require considerable human effort or rely on strong features that lack of flexibility. We consider an ambitious scenario -- can we build a system that (1) is general enough to handle any vertical without re-implementation and (2) requires only one labeled example site from each vertical for training to automatically deal with other sites in the same vertical? In this paper, we propose a unified solution to demonstrate the feasibility of this scenario. Specifically, we design a set of weak but general features to characterize vertical knowledge (including attribute-specific semantics and inter-attribute layout relationships). Such features can be adopted in various verticals without redesign; meanwhile, they are weak enough to avoid overfitting of the learnt knowledge to seed sites. Given a new unseen site, the learnt knowledge is first applied to identify page-level candidate attribute values, while inevitably involve false positives. To remove noise, site-level information of the new site is then exploited to boost up the true values. The site-level information is derived in an unsupervised manner, without harm to the applicability of the solution. Promising experimental performance on 80 websites in 8 distinct verticals demonstrated the feasibility and flexibility of the proposed solution.

#index 1598412
#* Improving local search ranking through external logs
#@ Klaus Berberich;Arnd Christian König;Dimitrios Lymberopoulos;Peixiang Zhao
#t 2011
#c 13
#% 169781
#% 411762
#% 643566
#% 750863
#% 765465
#% 818256
#% 874993
#% 879611
#% 946521
#% 956546
#% 987212
#% 987397
#% 989572
#% 989628
#% 1035574
#% 1055707
#% 1190103
#% 1190687
#% 1206801
#% 1225223
#% 1227616
#% 1298860
#% 1328137
#% 1355052
#% 1432737
#% 1450953
#% 1476142
#! The signals used for ranking in local search are very different from web search: in addition to (textual) relevance, measures of (geographic) distance between the user and the search result, as well as measures of popularity of the result are important for effective ranking. Depending on the query and search result, different ways to quantify these factors exist -- for example, it is possible to use customer ratings to quantify the popularity of restaurants, whereas different measures are more appropriate for other types of businesses. Hence, our approach is to capture the different notions of distance/popularity relevant via a number of external data sources (e.g., logs of customer ratings, driving-direction requests, or site accesses). In this paper we will describe the relevant signal contained in a number of such data sources in detail and present methods to integrate these external data sources into the feature generation for local search ranking. In particular, we propose novel backoff methods to alleviate the impact of skew, noise or incomplete data in these logs in a systematic manner. We evaluate our techniques on both human-judged relevance data as well as click-through data from a commercial local search engine.

#index 1598413
#* Query suggestions in the absence of query logs
#@ Sumit Bhatia;Debapriyo Majumdar;Prasenjit Mitra
#t 2011
#c 13
#% 1358
#% 340901
#% 348155
#% 397161
#% 838531
#% 869501
#% 879610
#% 935763
#% 943042
#% 964838
#% 987250
#% 987372
#% 995516
#% 1019146
#% 1083721
#% 1130854
#% 1130879
#% 1173699
#% 1227623
#% 1280764
#% 1292743
#% 1400023
#% 1417245
#% 1712595
#! After an end-user has partially input a query, intelligent search engines can suggest possible completions of the partial query to help end-users quickly express their information needs. All major web-search engines and most proposed methods that suggest queries rely on search engine query logs to determine possible query suggestions. However, for customized search systems in the enterprise domain, intranet search, or personalized search such as email or desktop search or for infrequent queries, query logs are either not available or the user base and the number of past user queries is too small to learn appropriate models. We propose a probabilistic mechanism for generating query suggestions from the corpus without using query logs. We utilize the document corpus to extract a set of candidate phrases. As soon as a user starts typing a query, phrases that are highly correlated with the partial user query are selected as completions of the partial query and are offered as query suggestions. Our proposed approach is tested on a variety of datasets and is compared with state-of-the-art approaches. The experimental results clearly demonstrate the effectiveness of our approach in suggesting queries with higher quality.

#index 1598414
#* Synthesizing high utility suggestions for rare web search queries
#@ Alpa Jain;Umut Ozertem;Emre Velipasaoglu
#t 2011
#c 13
#% 194299
#% 196896
#% 220131
#% 232713
#% 330617
#% 411762
#% 464434
#% 642985
#% 643001
#% 643057
#% 728105
#% 747738
#% 748499
#% 756361
#% 783475
#% 869501
#% 869651
#% 987222
#% 1055677
#% 1074112
#% 1130854
#% 1130855
#% 1130868
#% 1227647
#% 1338626
#% 1355020
#% 1399978
#% 1450865
#% 1450900
#% 1450975
#% 1560359
#% 1712595
#! Search engines are continuously looking into methods to alleviate users' effort in finding desired information. For this, all major search engines employ query suggestions methods to facilitate effective query formulation and reformulation. Providing high quality query suggestions is a critical task for search engines and so far most research efforts have focused on tapping various information available in search query logs to identify potential suggestions. By relying on this single source of information, suggestion providing systems often restrict themselves to only previously observed query sessions. Therefore, a critical challenge faced by query suggestions provision mechanism is that of coverage, i.e., the number of unique queries for which users are provided with suggestions, while keeping the suggestion quality high. To address this problem, we propose a novel way of generating suggestions for user search queries by moving beyond the dependency on search query logs and providing synthetic suggestions for web search queries. The key challenges in providing synthetic suggestions include identifying important concepts in a query and systematically exploring related concepts while ensuring that the resulting suggestions are relevant to the user query and of high utility. We present an end-to-end system to generate synthetic suggestions that builds upon novel query-level operations and combines information available from various textual sources. We evaluate our suggestion system over a large-scale real-world dataset of query logs and show that our methods increase the coverage of query-suggestion pairs by up to 39% without compromising the quality or the utility of the suggestions.

#index 1598415
#* Post-ranking query suggestion by diversifying search results
#@ Yang Song;Dengyong Zhou;Li-wei He
#t 2011
#c 13
#% 262112
#% 342707
#% 411762
#% 722816
#% 818221
#% 840853
#% 842388
#% 989578
#% 1019183
#% 1047785
#% 1130854
#% 1130879
#% 1166473
#% 1227578
#% 1227619
#% 1268491
#% 1348342
#% 1400023
#% 1442577
#% 1712595
#! Query suggestion refers to the process of suggesting related queries to search engine users. Most existing researches have focused on improving the relevance of suggested queries. In this paper, we introduce the concept of diversifying the content of the search results from suggested queries while keeping the suggestion relevant. Our framework first retrieves a set of query candidates from search engine logs using random walk and other techniques. We then re-rank the suggested queries by ranking them in the order which maximizes the diversification function that measures the difference between the original search results and the results from suggested queries. The diversification function we proposed includes features like ODP category, URL and domain similarity and so on. One important outcome from our research which contradicts with most existing researches is that, with the increase of suggestion relevance, the similarity between the queries actually decreases. Experiments are conducted on a large set of human-labeled data, which is randomly sampled from a commercial search engine's log. Results indicate that the post-ranking framework significantly improves the relevance of suggested queries by comparing to existing models.

#index 1598416
#* Automatic boolean query suggestion for professional search
#@ Youngho Kim;Jangwon Seo;W. Bruce Croft
#t 2011
#c 13
#% 169777
#% 169779
#% 262084
#% 298183
#% 340948
#% 397161
#% 577224
#% 869501
#% 987212
#% 1130855
#% 1131207
#% 1227647
#% 1227746
#% 1292765
#% 1415713
#% 1441635
#% 1450959
#% 1450975
#% 1455247
#% 1574742
#% 1712595
#! In professional search environments, such as patent search or legal search, search tasks have unique characteristics: 1) users interactively issue several queries for a topic, and 2) users are willing to examine many retrieval results, i.e., there is typically an emphasis on recall. Recent surveys have also verified that professional searchers continue to have a strong preference for Boolean queries because they provide a record of what documents were searched. To support this type of professional search, we propose a novel Boolean query suggestion technique. Specifically, we generate Boolean queries by exploiting decision trees learned from pseudo-labeled documents and rank the suggested queries using query quality predictors. We evaluate our algorithm in simulated patent and medical search environments. Compared with a recent effective query generation system, we demonstrate that our technique is effective and general.

#index 1598417
#* Improved video categorization from text metadata and user comments
#@ Katja Filippova;Keith B. Hall
#t 2011
#c 13
#% 194202
#% 252011
#% 451617
#% 722904
#% 733851
#% 780874
#% 780875
#% 903617
#% 996175
#% 1270702
#% 1318816
#% 1432784
#% 1450906
#% 1470631
#% 1482457
#! We consider the task of assigning categories (e.g., howto/cooking, sports/basketball, pet/dogs) to YouTube videos from video and text signals. We show that two complementary views on the data -- from the video and text perspectives -- complement each other and refine predictions. The contributions of the paper are threefold: (1) we show that a text-based classifier trained on imperfect predictions of the weakly supervised video content-based classifier is not redundant; (2) we demonstrate that a simple model which combines the predictions made by the two classifiers outperforms each of them taken independently; (3) we analyse such sources of text information as video title, description, user tags and viewers' comments and show that each of them provides valuable clues to the topic of the video.

#index 1598418
#* Multifaceted toponym recognition for streaming news
#@ Michael D. Lieberman;Hanan Samet
#t 2011
#c 13
#% 268785
#% 766441
#% 815902
#% 855305
#% 885056
#% 939899
#% 1035400
#% 1075340
#% 1120965
#% 1125719
#% 1125720
#% 1125721
#% 1135150
#% 1298864
#% 1358026
#% 1358027
#% 1358035
#% 1358036
#% 1480777
#% 1480778
#% 1480855
#% 1560254
#! News sources on the Web generate constant streams of information, describing many aspects of the events that shape our world. In particular, geography plays a key role in the news, and enabling geographic retrieval of news articles involves recognizing the textual references to geographic locations (called toponyms) present in the articles, which can be difficult due to ambiguity in natural language. Toponym recognition in news is often accomplished with algorithms designed and tested around small corpora of news articles, but these static collections do not reflect the streaming nature of online news, as evidenced by poor performance in tests. In contrast, a method for toponym recognition is presented that is tuned for streaming news by leveraging a wide variety of recognition components, both rule-based and statistical. An evaluation of this method shows that it outperforms two prominent toponym recognition systems when tested on large datasets of streaming news, indicating its suitability for this domain.

#index 1598419
#* Enriching document representation via translation for improved monolingual information retrieval
#@ Seung-Hoon Na;Hwee Tou Ng
#t 2011
#c 13
#% 131434
#% 144031
#% 169768
#% 217207
#% 262046
#% 262096
#% 280851
#% 280888
#% 340899
#% 340901
#% 340948
#% 507656
#% 579944
#% 642994
#% 732850
#% 732851
#% 735135
#% 740915
#% 746899
#% 766430
#% 766431
#% 766439
#% 786534
#% 853813
#% 879584
#% 1215368
#% 1227584
#% 1264793
#% 1269524
#% 1292730
#% 1330558
#% 1368917
#% 1400639
#% 1450901
#% 1450911
#% 1482195
#! Word ambiguity and vocabulary mismatch are critical problems in information retrieval. To deal with these problems, this paper proposes the use of translated words to enrich document representation, going beyond the words in the original source language to represent a document. In our approach, each original document is automatically translated into an auxiliary language, and the resulting translated document serves as a semantically enhanced representation for supplementing the original bag of words. The core of our translation representation is the expected term frequency of a word in a translated document, which is calculated by averaging the term frequencies over all possible translations, rather than focusing on the 1-best translation only. To achieve better efficiency of translation, we do not rely on full-fledged machine translation, but instead use monotonic translation by removing the time-consuming reordering component. Experiments carried out on standard TREC test collections show that our proposed translation representation leads to statistically significant improvements over using only the original language of the document collection.

#index 1598420
#* A novel corpus-based stemming algorithm using co-occurrence statistics
#@ Jiaul H. Paik;Dipasree Pal;Swapan K. Parui
#t 2011
#c 13
#% 94365
#% 144034
#% 241238
#% 248218
#% 296738
#% 298183
#% 411760
#% 561152
#% 643047
#% 732848
#% 741043
#% 790705
#% 874254
#% 987272
#% 989714
#% 1015014
#% 1077150
#% 1106161
#% 1227586
#% 1251173
#% 1453529
#! We present a stemming algorithm for text retrieval. The algorithm uses the statistics collected on the basis of certain corpus analysis based on the co-occurrence between two word variants. We use a very simple co-occurrence measure that reflects how often a pair of word variants occurs in a document as well as in the whole corpus. A graph is formed where the word variants are the nodes and two word variants form an edge if they co-occur. On the basis of the co-occurrence measure, a certain edge strength is defined for each of the edges. Finally, on the basis of the edge strengths, we propose a partition algorithm that groups the word variants based on their strongest neighbors, that is, the neighbors with largest strengths. Our stemming algorithm has two static parameters and does not use any other information except the co-occurrence statistics from the corpus. The experiments on TREC, CLEF and FIRE data consisting of four European and two Asian languages show a significant improvement over no-stem strategy on all the languages. Also, the proposed algorithm significantly outperforms a number of strong stemmers including the rule-based ones on a number of languages. For highly inflectional languages, a relative improvement of about 50% is obtained compared to un-normalized words and a relative improvement ranging from 5% to 16% is obtained compared to the rule based stemmer for the concerned language.

#index 1598421
#* Document clustering with universum
#@ Dan Zhang;Jingdong Wang;Luo Si
#t 2011
#c 13
#% 25998
#% 280819
#% 313959
#% 397147
#% 643008
#% 722902
#% 724227
#% 729437
#% 757953
#% 763708
#% 807421
#% 876071
#% 879617
#% 881477
#% 907522
#% 987204
#% 1074028
#% 1077150
#% 1176910
#% 1227731
#% 1274862
#% 1275120
#% 1305451
#% 1386108
#% 1550713
#! Document clustering is a popular research topic, which aims to partition documents into groups of similar objects (i.e., clusters), and has been widely used in many applications such as automatic topic extraction, document organization and filtering. As a recently proposed concept, Universum is a collection of "non-examples" that do not belong to any concept/cluster of interest. This paper proposes a novel document clustering technique -- Document Clustering with Universum, which utilizes the Universum examples to improve the clustering performance. The intuition is that the Universum examples can serve as supervised information and help improve the performance of clustering, since they are known not belonging to any meaningful concepts/clusters in the target domain. In particular, a maximum margin clustering method is proposed to model both target examples and Universum examples for clustering. An extensive set of experiments is conducted to demonstrate the effectiveness and efficiency of the proposed algorithm.

#index 1598422
#* Identifying points of interest by self-tuning clustering
#@ Yiyang Yang;Zhiguo Gong;Leong Hou U
#t 2011
#c 13
#% 224113
#% 290830
#% 313959
#% 332133
#% 349208
#% 466425
#% 731611
#% 839852
#% 987205
#% 1055701
#% 1077150
#% 1190131
#% 1190226
#% 1227637
#% 1292685
#% 1381000
#% 1400055
#% 1400074
#% 1441258
#% 1450997
#% 1828376
#! Deducing trip related information from web-scale datasets has received very large amounts of attention recently. Identifying points of interest (POIs) in geo-tagged photos is one of these problems. The problem can be viewed as a standard clustering problem of partitioning two dimensional objects. In this work, we study spectral clustering which is the first attempt for the POIs identification. However, there is no unified approach to assign the clustering parameters; especially the features of POIs are immensely varying in different metropolitans and locations. To address this, we are intent to study a self-tuning technique which can properly assign the parameters for the clustering needed. Besides geographical information, web photos inherently store rich information. These information are mutually influenced each others and should be taken into trip related mining tasks. To address this, we study reinforcement which constructs the relationship over multiple sources by iterative learning. At last, we thoroughly demonstrate our findings by web scale datasets collected from Flickr.

#index 1598423
#* Cluster-based fusion of retrieved lists
#@ Anna Khudyak Kozorovitsky;Oren Kurland
#t 2011
#c 13
#% 194276
#% 218992
#% 232703
#% 262045
#% 330769
#% 340890
#% 340936
#% 340948
#% 340963
#% 342672
#% 348344
#% 375017
#% 389801
#% 397199
#% 413613
#% 427921
#% 728357
#% 766430
#% 766431
#% 818282
#% 838528
#% 879575
#% 879582
#% 881939
#% 907557
#% 940042
#% 1019124
#% 1074072
#% 1213624
#% 1227630
#% 1263588
#% 1392446
#% 1415748
#% 1450860
#% 1536584
#% 1618640
#% 1631454
#! Methods for fusing document lists that were retrieved in response to a query often use retrieval scores (or ranks) of documents in the lists. We present a novel probabilistic fusion approach that utilizes an additional source of rich information, namely, inter-document similarities. Specifically, our model integrates information induced from clusters of similar documents created across the lists with that produced by some fusion method that relies on retrieval scores (ranks). Empirical evaluation shows that our approach is highly effective for fusion. For example, the performance of our model is consistently better than that of the standard (effective) fusion method that it integrates. The performance also transcends that of standard fusion of re-ranked lists, where list re-ranking is based on clusters created from documents in the list.

#index 1598424
#* System effectiveness, user models, and user utility: a conceptual framework for investigation
#@ Ben Carterette
#t 2011
#c 13
#% 262097
#% 262105
#% 411762
#% 1074124
#% 1074133
#% 1074137
#% 1074139
#% 1095876
#% 1166473
#% 1292528
#% 1366523
#% 1415710
#% 1450904
#% 1482378
#% 1598439
#! There is great interest in producing effectiveness measures that model user behavior in order to better model the utility of a system to its users. These measures are often formulated as a sum over the product of a discount function of ranks and a gain function mapping relevance assessments to numeric utility values. We develop a conceptual framework for analyzing such effectiveness measures based on classifying members of this broad family of measures into four distinct families, each of which reflects a different notion of system utility. Within this framework we can hypothesize about the properties that such a measure should have and test those hypotheses against user and system data. Along the way we present a collection of novel results about specific measures and relationships between them.

#index 1598425
#* Evaluating the synergic effect of collaboration in information seeking
#@ Chirag Shah;Roberto González-Ibáñez
#t 2011
#c 13
#% 246877
#% 399447
#% 793011
#% 805200
#% 949190
#% 998795
#% 1025661
#% 1047490
#% 1074090
#% 1093783
#% 1132900
#% 1133169
#% 1348357
#% 1450902
#% 1474643
#% 1523444
#! It is typically expected that when people work together, they can often accomplish goals that are difficult or even impossible for individuals. We consider this notion of the group achieving more than the sum of all individuals' achievements to be the synergic effect in collaboration. Similar expectation exists for people working in collaboration for information seeking tasks. We, however, lack a methodology and appropriate evaluation metrics for studying and measuring the synergic effect. In this paper we demonstrate how to evaluate this effect and discuss what it means to various collaborative information seeking (CIS) situations. We present a user study with four different conditions: single user, pair of users at the same computer, pair of users at different computers and co-located, and pair of users remotely located. Each of these individuals or pairs was given the same task of information seeking and usage for the same amount of time. We then combined the outputs of single independent users to form artificial pairs, and compared against the real pairs. Not surprisingly, participants using different computers (co-located or remotely located) were able to cover more information sources than those using a single computer (single user or a pair). But more interestingly, we found that real pairs with their own computers (co-located or remotely located) were able to cover more unique and useful information than that of the artificially created pairs. This indicates that those working in collaboration achieved something greater and better than what could be achieved by adding independent users, thus, demonstrating the synergic effect. Remotely located real teams were also able to formulate a wider range of queries than those pairs that were co-located or artificially created. This shows that the collaborators working remotely were able to achieve synergy while still being able to think and work independently. Through the experiments and measurements presented here, we have also contributed a unique methodology and an evaluation metric for CIS.

#index 1598426
#* Repeatable and reliable search system evaluation using crowdsourcing
#@ Roi Blanco;Harry Halpin;Daniel M. Herzig;Peter Mika;Jeffrey Pound;Henry S. Thompson;Thanh Tran Duc
#t 2011
#c 13
#% 208931
#% 561315
#% 577373
#% 783560
#% 939867
#% 1074134
#% 1131703
#% 1150163
#% 1206760
#% 1252624
#% 1263220
#% 1292565
#% 1338557
#% 1375847
#% 1400010
#% 1697468
#! The primary problem confronting any new kind of search task is how to boot-strap a reliable and repeatable evaluation campaign, and a crowd-sourcing approach provides many advantages. However, can these crowd-sourced evaluations be repeated over long periods of time in a reliable manner? To demonstrate, we investigate creating an evaluation campaign for the semantic search task of keyword-based ad-hoc object retrieval. In contrast to traditional search over web-pages, object search aims at the retrieval of information from factual assertions about real-world objects rather than searching over web-pages with textual descriptions. Using the first large-scale evaluation campaign that specifically targets the task of ad-hoc Web object retrieval over a number of deployed systems, we demonstrate that crowd-sourced evaluation campaigns can be repeated over time and still maintain reliable results. Furthermore, we show how these results are comparable to expert judges when ranking systems and that the results hold over different evaluation and relevance metrics. This work provides empirical support for scalable, reliable, and repeatable search system evaluation using crowdsourcing.

#index 1598427
#* Cross-language web page classification via dual knowledge transfer using nonnegative matrix tri-factorization
#@ Hua Wang;Heng Huang;Feiping Nie;Chris Ding
#t 2011
#c 13
#% 466263
#% 770830
#% 818313
#% 881468
#% 1055769
#% 1193637
#% 1214657
#% 1227700
#% 1261539
#% 1327693
#% 1328329
#% 1410912
#% 1464068
#% 1471292
#% 1481638
#% 1492158
#! The lack of sufficient labeled Web pages in many languages, especially for those uncommonly used ones, presents a great challenge to traditional supervised classification methods to achieve satisfactory Web page classification performance. To address this, we propose a novel Nonnegative Matrix Tri-factorization (NMTF) based Dual Knowledge Transfer (DKT) approach for cross-language Web page classification, which is based on the following two important observations. First, we observe that Web pages for a same topic from different languages usually share some common semantic patterns, though in different representation forms. Second, we also observe that the associations between word clusters and Web page classes are a more reliable carrier than raw words to transfer knowledge across languages. With these recognitions, we attempt to transfer knowledge from the auxiliary language, in which abundant labeled Web pages are available, to target languages, in which we want classify Web pages, through two different paths: word cluster approximations and the associations between word clusters and Web page classes. Due to the reinforcement between these two different knowledge transfer paths, our approach can achieve better classification accuracy. We evaluate the proposed approach in extensive experiments using a real world cross-language Web page data set. Promising results demonstrate the effectiveness of our approach that is consistent with our theoretical analyses.

#index 1598428
#* No free lunch: brute force vs. locality-sensitive hashing for cross-lingual pairwise similarity
#@ Ferhan Ture;Tamer Elsayed;Jimmy Lin
#t 2011
#c 13
#% 345087
#% 347225
#% 579944
#% 616528
#% 643017
#% 735134
#% 757445
#% 757830
#% 769944
#% 848151
#% 879600
#% 879617
#% 939408
#% 956506
#% 956507
#% 963669
#% 1023422
#% 1074121
#% 1130813
#% 1206677
#% 1215321
#% 1227596
#% 1338626
#% 1368917
#% 1426543
#% 1450831
#% 1464952
#% 1470625
#% 1471265
#% 1481560
#% 1697477
#! This work explores the problem of cross-lingual pairwise similarity, where the task is to extract similar pairs of documents across two different languages. Solutions to this problem are of general interest for text mining in the multi-lingual context and have specific applications in statistical machine translation. Our approach takes advantage of cross-language information retrieval (CLIR) techniques to project feature vectors from one language into another, and then uses locality-sensitive hashing (LSH) to extract similar pairs. We show that effective cross-lingual pairwise similarity requires working with similarity thresholds that are much lower than in typical monolingual applications, making the problem quite challenging. We present a parallel, scalable MapReduce implementation of the sort-based sliding window algorithm, which is compared to a brute-force approach on German and English Wikipedia collections. Our central finding can be summarized as "no free lunch": there is no single optimal solution. Instead, we characterize effectiveness-efficiency tradeoffs in the solution space, which can guide the developer to locate a desirable operating point based on application- and resource-specific constraints.

#index 1598429
#* An event-centric model for multilingual document similarity
#@ Jannik Strötgen;Michael Gertz;Conny Junghans
#t 2011
#c 13
#% 350859
#% 387427
#% 532049
#% 878916
#% 987219
#% 1065412
#% 1077150
#% 1117736
#% 1120965
#% 1215321
#% 1251781
#% 1275012
#% 1355045
#% 1358036
#% 1387550
#% 1472106
#% 1472167

#index 1598430
#* Posting list intersection on multicore architectures
#@ Shirish Tatikonda;B. Barla Cambazoglu;Flavio P. Junqueira
#t 2011
#c 13
#% 212665
#% 213786
#% 228097
#% 249153
#% 262099
#% 303072
#% 340886
#% 433941
#% 510483
#% 578337
#% 643000
#% 818229
#% 864446
#% 878657
#% 976948
#% 987214
#% 987215
#% 1055710
#% 1074067
#% 1171860
#% 1190097
#% 1190098
#% 1291231
#% 1328167
#% 1355057
#% 1399951
#% 1667821
#! In current commercial Web search engines, queries are processed in the conjunctive mode, which requires the search engine to compute the intersection of a number of posting lists to determine the documents matching all query terms. In practice, the intersection operation takes a significant fraction of the query processing time, for some queries dominating the total query latency. Hence, efficient posting list intersection is critical for achieving short query latencies. In this work, we focus on improving the performance of posting list intersection by leveraging the compute capabilities of recent multicore systems. To this end, we consider various coarse-grained and fine-grained parallelization models for list intersection. Specifically, we present an algorithm that partitions the work associated with a given query into a number of small and independent tasks that are subsequently processed in parallel. Through a detailed empirical analysis of these alternative models, we demonstrate that exploiting parallelism at the finest-level of granularity is critical to achieve the best performance on multicore systems. On an eight-core system, the fine-grained parallelization method is able to achieve more than five times reduction in average query processing time while still exploiting the parallelism for high query throughput.

#index 1598431
#* Timestamp-based result cache invalidation for web search engines
#@ Sadiye Alici;Ismail Sengor Altingovde;Rifat Ozcan;Berkant Barla Cambazoglu;Özgür Ulusoy
#t 2011
#c 13
#% 86532
#% 172922
#% 340887
#% 340888
#% 577302
#% 577370
#% 747117
#% 754058
#% 801833
#% 805864
#% 818262
#% 860861
#% 878624
#% 879609
#% 987215
#% 1070892
#% 1074067
#% 1166469
#% 1190098
#% 1195885
#% 1373774
#% 1399951
#% 1404875
#% 1450839
#% 1464869
#% 1558842
#% 1560147
#% 1587384
#% 1834787
#% 1870668
#! The result cache is a vital component for efficiency of large-scale web search engines, and maintaining the freshness of cached query results is the current research challenge. As a remedy to this problem, our work proposes a new mechanism to identify queries whose cached results are stale. The basic idea behind our mechanism is to maintain and compare generation time of query results with update times of posting lists and documents to decide on staleness of query results. The proposed technique is evaluated using a Wikipedia document collection with real update information and a real-life query log. We show that our technique has good prediction accuracy, relative to a baseline based on the time-to-live mechanism. Moreover, it is easy to implement and incurs less processing overhead on the system relative to a recently proposed, more sophisticated invalidation mechanism.

#index 1598432
#* Energy-price-driven query processing in multi-center web search engines
#@ Enver Kayaaslan;B. Barla Cambazoglu;Roi Blanco;Flavio P. Junqueira;Cevdet Aykanat
#t 2011
#c 13
#% 433757
#% 578337
#% 629111
#% 730066
#% 834884
#% 1034471
#% 1174227
#% 1190098
#% 1227628
#% 1246360
#% 1290542
#% 1292508
#% 1399951
#% 1417148
#% 1449175
#% 1450840
#% 1456972
#% 1514557
#% 1715628
#! Concurrently processing thousands of web queries, each with a response time under a fraction of a second, necessitates maintaining and operating massive data centers. For large-scale web search engines, this translates into high energy consumption and a huge electric bill. This work takes the challenge to reduce the electric bill of commercial web search engines operating on data centers that are geographically far apart. Based on the observation that energy prices and query workloads show high spatio-temporal variation, we propose a technique that dynamically shifts the query workload of a search engine between its data centers to reduce the electric bill. Experiments on real-life query workloads obtained from a commercial search engine show that significant financial savings can be achieved by this technique.

#index 1598433
#* Faster top-k document retrieval using block-max indexes
#@ Shuai Ding;Torsten Suel
#t 2011
#c 13
#% 157880
#% 198335
#% 212665
#% 228097
#% 330706
#% 333854
#% 340886
#% 340887
#% 387427
#% 397608
#% 570319
#% 659993
#% 730065
#% 818229
#% 818230
#% 867054
#% 878657
#% 879326
#% 879611
#% 893128
#% 987214
#% 987215
#% 987216
#% 993962
#% 1015265
#% 1019138
#% 1051061
#% 1055710
#% 1166469
#% 1190095
#% 1227595
#% 1302865
#% 1392439
#% 1399964
#% 1482300
#% 1594565
#! Large search engines process thousands of queries per second over billions of documents, making query processing a major performance bottleneck. An important class of optimization techniques called early termination achieves faster query processing by avoiding the scoring of documents that are unlikely to be in the top results. We study new algorithms for early termination that outperform previous methods. In particular, we focus on safe techniques for disjunctive queries, which return the same result as an exhaustive evaluation over the disjunction of the query terms. The current state-of-the-art methods for this case, the WAND algorithm by Broder et al. [11] and the approach of Strohman and Croft [30], achieve great benefits but still leave a large performance gap between disjunctive and (even non-early terminated) conjunctive queries. We propose a new set of algorithms by introducing a simple augmented inverted index structure called a block-max index. Essentially, this is a structure that stores the maximum impact score for each block of a compressed inverted list in uncompressed form, thus enabling us to skip large parts of the lists. We show how to integrate this structure into the WAND approach, leading to considerable performance gains. We then describe extensions to a layered index organization, and to indexes with reassigned document IDs, that achieve additional gains that narrow the gap between disjunctive and conjunctive top-k query processing.

#index 1598434
#* Utilizing marginal net utility for recommendation in e-commerce
#@ Jian Wang;Yi Zhang
#t 2011
#c 13
#% 234992
#% 280447
#% 280852
#% 301259
#% 308769
#% 314933
#% 330687
#% 414514
#% 420121
#% 730049
#% 734590
#% 734592
#% 734594
#% 751597
#% 770859
#% 829036
#% 918842
#% 1083671
#% 1200360
#% 1214666
#% 1345617
#% 1476448
#% 1614699
#% 1650569
#! Traditional recommendation algorithms often select products with the highest predicted ratings to recommend. However, earlier research in economics and marketing indicates that a consumer usually makes purchase decision(s) based on the product's marginal net utility (i.e., the marginal utility minus the product price). Utility is defined as the satisfaction or pleasure user u gets when purchasing the corresponding product. A rational consumer chooses the product to purchase in order to maximize the total net utility. In contrast to the predicted rating, the marginal utility of a product depends on the user's purchase history and changes over time. According to the Law of Diminishing Marginal Utility, many products have the decreasing marginal utility with the increase of purchase count, such as cell phones, computers, and so on. Users are not likely to purchase the same or similar product again in a short time if they already purchased it before. On the other hand, some products, such as pet food, baby diapers, would be purchased again and again. To better match users' purchase decisions in the real world, this paper explores how to recommend products with the highest marginal net utility in e-commerce sites. Inspired by the Cobb-Douglas utility function in consumer behavior theory, we propose a novel utility-based recommendation framework. The framework can be utilized to revamp a family of existing recommendation algorithms. To demonstrate the idea, we use Singular Value Decomposition (SVD) as an example and revamp it with the framework. We evaluate the proposed algorithm on an e-commerce (shop.com) data set. The new algorithm significantly improves the base algorithm, largely due to its ability to recommend both products that are new to the user and products that the user is likely to re-purchase.

#index 1598435
#* Recommending ephemeral items at web scale
#@ Ye Chen;John F. Canny
#t 2011
#c 13
#% 280819
#% 311027
#% 340948
#% 722904
#% 729437
#% 771927
#% 1035578
#% 1083671
#% 1190055
#% 1190056
#% 1214623
#% 1214642
#% 1214666
#% 1273828
#% 1292572
#% 1355025
#! We describe an innovative and scalable recommendation system successfully deployed at eBay. To build recommenders for long-tail marketplaces requires projection of volatile items into a persistent space of latent products. We first present a generative clustering model for collections of unstructured, heterogeneous, and ephemeral item data, under the assumption that items are generated from latent products. An item is represented as a vector of independently and distinctly distributed variables, while a latent product is characterized as a vector of probability distributions, respectively. The probability distributions are chosen as natural stochastic models for different types of data. The learning objective is to maximize the total intra-cluster coherence measured by the sum of log likelihoods of items under such a generative process. In the space of latent products, robust recommendations can then be derived using naive Bayes for ranking, from historical transactional data. Item-based recommendations are achieved by inferring latent products from unseen items. In particular, we develop a probabilistic scoring function of recommended items, which takes into account item-product membership, product purchase probability, and the important auction-end-time factor. With the holistic probabilistic measure of a prospective item purchase, one can further maximize the expected revenue and the more subjective user satisfaction as well. We evaluated the latent product clustering and recommendation ranking models using real-world e-commerce data from eBay, in both forms of offline simulation and online A/B testing. In the recent production launch, our system yielded 3-5 folds improvement over the existing production system in click-through, purchase-through and gross merchandising value; thus now driving 100% related recommendation traffic with billions of items at eBay. We believe that this work provides a practical yet principled framework for recommendation in the domains with affluent user self-input data.

#index 1598436
#* A unified framework for recommendations based on quaternary semantic analysis
#@ Chen Wei;Wynne Hsu;Mong Li Lee
#t 2011
#c 13
#% 173879
#% 220709
#% 301259
#% 316143
#% 330687
#% 342687
#% 452563
#% 869608
#% 1016092
#% 1052902
#% 1083671
#% 1156304
#% 1176933
#% 1183090
#% 1190122
#% 1214694
#% 1227643
#% 1327635
#! Social network systems such as FaceBook and YouTube have played a significant role in capturing both explicit and implicit user preferences for different items in the form of ratings and tags. This forms a quaternary relationship among users, items, tags and ratings. Existing systems have utilized only ternary relationships such as users-items-ratings, or users-items-tags to derive their recommendations. In this paper, we show that ternary relationships are insufficient to provide accurate recommendations. Instead, we model the quaternary relationship among users, items, tags and ratings as a 4-order tensor and cast the recommendation problem as a multi-way latent semantic analysis problem. A unified framework for user recommendation, item recommendation, tag recommendation and item rating prediction is proposed. The results of extensive experiments performed on a real world dataset demonstrate that our unified framework outperforms the state-of-the-art techniques in all the four recommendation tasks.

#index 1598437
#* Associative tag recommendation exploiting multiple textual features
#@ Fabiano Belém;Eder Martins;Tatiana Pontes;Jussara Almeida;Marcos Gonçalves
#t 2011
#c 13
#% 252473
#% 387427
#% 481290
#% 734915
#% 881477
#% 974033
#% 1055704
#% 1074084
#% 1074115
#% 1074117
#% 1089475
#% 1127458
#% 1190091
#% 1227602
#% 1227626
#% 1227644
#% 1287227
#% 1292558
#% 1355024
#% 1495608
#! This work addresses the task of recommending relevant tags to a target object by jointly exploiting three dimensions of the problem: (i) term co-occurrence with tags pre-assigned to the target object, (ii) terms extracted from multiple textual features, and (iii) several metrics of tag relevance. In particular, we propose several new heuristic methods, which extend state-of-the-art strategies by including new metrics that try to capture how accurately a candidate term describes the object's content. We also exploit two learning-to-rank (L2R) techniques, namely RankSVM and Genetic Programming, for the task of generating ranking functions that combine multiple metrics to accurately estimate the relevance of a tag to a given object. We evaluate all proposed methods in various scenarios for three popular Web 2.0 applications, namely, LastFM, YouTube and YahooVideo. We found that our new heuristics greatly outperform the methods on which they are based, producing gains in precision of up to 181%, as well as another state-of-the-art technique, with improvements in precision of up to 40% over the best baseline in any scenario. Further improvements can also be achieved with the new L2R strategies, which have the additional advantage of being quite flexible and extensible to exploit other aspects of the tag recommendation problem.

#index 1598438
#* Evaluating diversified search results using per-intent graded relevance
#@ Tetsuya Sakai;Ruihua Song
#t 2011
#c 13
#% 397163
#% 397164
#% 411762
#% 642975
#% 818205
#% 879630
#% 1074124
#% 1074133
#% 1095876
#% 1166473
#% 1263586
#% 1292528
#% 1292596
#% 1400011
#% 1450898
#% 1450904
#% 1450912
#% 1482296
#% 1536510
#% 1536529
#% 1536552
#% 1682085
#! Search queries are often ambiguous and/or underspecified. To accomodate different user needs, search result diversification has received attention in the past few years. Accordingly, several new metrics for evaluating diversification have been proposed, but their properties are little understood. We compare the properties of existing metrics given the premises that (1) queries may have multiple intents; (2) the likelihood of each intent given a query is available; and (3) graded relevance assessments are available for each intent. We compare a wide range of traditional and diversified IR metrics after adding graded relevance assessments to the TREC 2009 Web track diversity task test collection which originally had binary relevance assessments. Our primary criterion is discriminative power, which represents the reliability of a metric in an experiment. Our results show that diversified IR experiments with a given number of topics can be as reliable as traditional IR experiments with the same number of topics, provided that the right metrics are used. Moreover, we compare the intuitiveness of diversified IR metrics by closely examining the actual ranked lists from TREC. We show that a family of metrics called D#-measures have several advantages over other metrics such as α-nDCG and Intent-Aware metrics.

#index 1598439
#* Evaluating multi-query sessions
#@ Evangelos Kanoulas;Ben Carterette;Paul D. Clough;Mark Sanderson
#t 2011
#c 13
#% 349283
#% 907496
#% 987201
#% 1019103
#% 1074133
#% 1095876
#% 1263584
#% 1292528
#% 1366523
#% 1415709
#! The standard system-based evaluation paradigm has focused on assessing the performance of retrieval systems in serving the best results for a single query. Real users, however, often begin an interaction with a search engine with a sufficiently under-specified query that they will need to reformulate before they find what they are looking for. In this work we consider the problem of evaluating retrieval systems over test collections of multi-query sessions. We propose two families of measures: a model-free family that makes no assumption about the user's behavior over a session, and a model-based family with a simple model of user interactions over the session. In both cases we generalize traditional evaluation metrics such as average precision to multi-query session evaluation. We demonstrate the behavior of the proposed metrics by using the new TREC 2010 Session track collection and simulations over the TREC-9 Query track collection.

#index 1598440
#* Quantifying test collection quality based on the consistency of relevance judgements
#@ Falk Scholer;Andrew Turpin;Mark Sanderson
#t 2011
#c 13
#% 262097
#% 262102
#% 290703
#% 312689
#% 397163
#% 838536
#% 1019124
#% 1074134
#% 1130864
#% 1192696
#% 1450896
#% 1482232
#! Relevance assessments are a key component for test collection-based evaluation of information retrieval systems. This paper reports on a feature of such collections that is used as a form of ground truth data to allow analysis of human assessment error. A wide range of test collections are retrospectively examined to determine how accurately assessors judge the relevance of documents. Our results demonstrate a high level of inconsistency across the collections studied. The level of irregularity is shown to vary across topics, with some showing a very high level of assessment error. We investigate possible influences on the error, and demonstrate that inconsistency in judging increases with time. While the level of detail in a topic specification does not appear to influence the errors that assessors make, judgements are significantly affected by the decisions made on previously seen similar documents. Assessors also display an assessment inertia. Alternate approaches to generating relevance judgements appear to reduce errors. A further investigation of the way that retrieval systems are ranked using sets of relevance judgements produced early and late in the judgement process reveals a consistent influence measured across the majority of examined test collections. We conclude that there is a clear value in examining, even inserting, ground truth data in test collections, and propose ways to help minimise the sources of inconsistency when creating future test collections.

#index 1598441
#* Pseudo test collections for learning web search ranking functions
#@ Nima Asadi;Donald Metzler;Tamer Elsayed;Jimmy Lin
#t 2011
#c 13
#% 169780
#% 290830
#% 309779
#% 340890
#% 411762
#% 577224
#% 766414
#% 818221
#% 823348
#% 840846
#% 869484
#% 879565
#% 879598
#% 987200
#% 987241
#% 987262
#% 1019084
#% 1019154
#% 1040862
#% 1074082
#% 1074148
#% 1083632
#% 1150163
#% 1190094
#% 1193635
#% 1226913
#% 1227604
#% 1268491
#% 1292528
#% 1331597
#% 1355020
#% 1355039
#% 1450837
#% 1450897
#! Test collections are the primary drivers of progress in information retrieval. They provide yardsticks for assessing the effectiveness of ranking functions in an automatic, rapid, and repeatable fashion and serve as training data for learning to rank models. However, manual construction of test collections tends to be slow, labor-intensive, and expensive. This paper examines the feasibility of constructing web search test collections in a completely unsupervised manner given only a large web corpus as input. Within our proposed framework, anchor text extracted from the web graph is treated as a pseudo query log from which pseudo queries are sampled. For each pseudo query, a set of relevant and non-relevant documents are selected using a variety of web-specific features, including spam and aggregated anchor text weights. The automatically mined queries and judgments form a pseudo test collection that can be used for training ranking functions. Experiments carried out on TREC web track data show that learning to rank models trained using pseudo test collections outperform an unsupervised ranking function and are statistically indistinguishable from a model trained using manual judgments, demonstrating the usefulness of our approach in extracting reasonable quality training data "for free".

#index 1598442
#* Parallel learning to rank for information retrieval
#@ Shuaiqiang Wang;Byron J. Gao;Ke Wang;Hady W. Lauw
#t 2011
#c 13
#% 450263
#% 577224
#% 734915
#% 761547
#% 963669
#% 983820
#% 987241
#% 1292726
#! Learning to rank represents a category of effective ranking methods for information retrieval. While the primary concern of existing research has been accuracy, learning efficiency is becoming an important issue due to the unprecedented availability of large-scale training data and the need for continuous update of ranking functions. In this paper, we investigate parallel learning to rank, targeting simultaneous improvement in accuracy and efficiency.

#index 1598443
#* Learning features through feedback for blog distillation
#@ Dehong Gao;Renxian Zhang;Wenjie Li;Yiu Keung Lau;Kam Fai Wong
#t 2011
#c 13
#% 928386
#! The paper is focused on blogosphere research based on the TREC blog distillation task, and aims to explore unbiased and significant features automatically and efficiently. Feedback from faceted feeds is introduced to harvest relevant features and information gain is used to select discriminative features. The evaluation result shows that the selected feedback features can greatly improve the performance and adapt well to the terabyte data.

#index 1598444
#* Time-based relevance models
#@ Mostafa Keikha;Shima Gerani;Fabio Crestani
#t 2011
#c 13
#% 340901
#% 1074094
#% 1074171
#% 1227614
#% 1292749
#! This paper addresses blog feed retrieval where the goal is to retrieve the most relevant blog feeds for a given user query. Since the retrieval unit is a blog, as a collection of posts, performing relevance feedback techniques and selecting the most appropriate documents for query expansion becomes challenging. By assuming time as an effective parameter on the blog posts content, we propose a time-based query expansion method. In this method, we select terms for expansion using most relevant days for the query, as opposed to most relevant documents. This provide us with more trustable terms for expansion. Our preliminary experiments on Blog08 collection shows that this method can outperform state of the art relevance feedback methods in blog retrieval.

#index 1598445
#* Improved query performance prediction using standard deviation
#@ Ronan Cummins;Joemon Jose;Colm O'Riordan
#t 2011
#c 13
#% 397161
#% 1130990
#% 1263599
#% 1529948
#! Query performance prediction (QPP) is an important task in information retrieval (IR). In this paper, we (1) develop a new predictor based on the standard deviation of scores in a variable length ranked list, and (2) we show that this new predictor outperforms state-of-the-art approaches without the need for tuning.

#index 1598446
#* Learning to rank using query-level regression
#@ Jiajin Wu;Zhihao Yang;Yuan Lin;Hongfei Lin;Zheng Ye;Kan Xu
#t 2011
#c 13
#% 983820
#% 1039843
#% 1073936
#% 1074021
#% 1268491
#! In this paper, we use query-level regression as the loss function. The regression loss function has been used in pointwise methods, however pointwise methods ignore the query boundaries and treat the data equally across queries, and thus the effectiveness is limited. We show that regression is an effective loss function for learning to rank when used in query-level. We use neural network to model the ranking function and gradient descent for optimization and refer our method as ListReg. Experimental results show that ListReg significantly outperforms pointwise Regression and the state-of-the-art listwise method in most cases.

#index 1598447
#* Diversifying product search results
#@ Xiangru Chen;Haofen Wang;Xinruo Sun;Junfeng Pan;Yong Yu
#t 2011
#c 13
#% 262112
#% 1166473
#% 1190093
#! In recent years, online shopping is becoming more and more popular. Users type keyword queries on product search systems to find relevant products, accessories, and even related products. However, existing product search systems always return very similar products on the first several pages instead of taking diversity into consideration. In this paper, we propose a novel approach to address the diversity issue in the context of product search. We transform search result diversification into a combination of diversifying product categories and diversifying product attribute values within each category. The two sub-problems are optimization problems which can be reduced into well-known NP-hard problems respectively. We further leverage greedy-based approximation algorithms for efficient product search results re-ranking.

#index 1598448
#* Ad hoc IR: not much room for improvement
#@ Andrew Trotman;David Keeler
#t 2011
#c 13
#% 411760
#% 1227688
#% 1291263
#% 1292526
#% 1312832
#% 1489449
#! Ranking function performance reached a plateau in 1994. The reason for this is investigated. First the performance of BM25 is measured as the proportion of queries satisfied on the first page of 10 results -- it performs well. The performance is then compared to human performance. They perform comparably. The conclusion is there isn't much room for ranking function improvement.

#index 1598449
#* Image annotation based on recommendation model
#@ Zijia Lin;Guiguang Ding;Jianmin Wang
#t 2011
#c 13
#% 997184
#% 1034714
#% 1132472
#% 1148273
#% 1190090
#% 1214661
#! In this paper, a novel approach based on recommendation model is proposed for automatic image annotation. For any to-be-annotated image, we first select some related images with tags from training dataset according to their visual similarity. And then we estimate the initial ratings for tags of the training images based on tag ranking method and construct a rating matrix. We also construct a trust matrix based on visual similarity with a k-NN strategy. Then a recommendation model is built on both matrices to rank candidate tags for the target image. The proposed approach is evaluated using two benchmark image datasets, and experimental results have indicated its effectiveness.

#index 1598450
#* Utilizing minimal relevance feedback for ad hoc retrieval
#@ Eyal Krikon;Oren Kurland
#t 2011
#c 13
#% 194298
#% 838466
#% 1043052
#% 1482203
#% 1486654
#! Using relevance feedback can significantly improve (ad hoc) retrieval effectiveness. Yet, if little feedback is available, effectively exploiting it is a challenge. To that end, we present a novel approach that utilizes document passages. Empirical evaluation demonstrates the merits of the approach.

#index 1598451
#* Sense discrimination for physics retrieval
#@ Christina Lioma;Alok Kothari;Hinrich Schuetze
#t 2011
#c 13
#% 719598
#% 838472
#% 1697474
#! Information Retrieval in technical domains like physics is characterised by long and precise queries, whose meaning is strongly influenced by term context and domain. We treat this as a disambiguation problem, and present initial findings of a retrieval model that posits a higher probability of relevance for documents matching disambiguated query terms. Preliminary evaluation on a real-life physics test collection shows promising performance improvement.

#index 1598452
#* When documents are very long, BM25 fails!
#@ Yuanhua Lv;ChengXiang Zhai
#t 2011
#c 13
#% 169781
#% 218982
#% 766412
#! We reveal that the Okapi BM25 retrieval function tends to overly penalize very long documents. To address this problem, we present a simple yet effective extension of BM25, namely BM25L, which "shifts" the term frequency normalization formula to boost scores of very long documents. Our experiments show that BM25L, with the same computation cost, is more effective and robust than the standard BM25.

#index 1598453
#* Location and timeliness of information sources during news events
#@ Elad Yom-Tov;Fernando Diaz
#t 2011
#c 13
#% 1055707
#% 1214671
#% 1400018
#% 1598372
#! People nowadays can obtain information on current news events through media outlets, social media, and by actively seeking information using search engines. In this paper we investigate the temporal relationship between news coverage by media outlets, social media, and query logs and show that social media frequently precedes other information sources. Additionally, we demonstrate that there is strong negative correlation between the probability for reporting of an event and the distance of the information source from the event.

#index 1598454
#* What deliberately degrading search quality tells us about discount functions
#@ Paul Thomas;Timothy Jones;David Hawking
#t 2011
#c 13
#% 411762
#% 840846
#% 879566
#% 1095876
#! Deliberate degradation of search results is a common tool in user experiments. We degrade high-quality search results by inserting non-relevant documents at different ranks. The effect of these manipulations, on a number of commonly-used metrics, is counter-intuitive: the discount functions implicit in P@k, MRR, NDCG, and others do not account for the true relationship between rank and value to the user. We propose an alternative, based on visibility data.

#index 1598455
#* Collective topic modeling for heterogeneous networks
#@ Hongbo Deng;Bo Zhao;Jiawei Han
#t 2011
#c 13
#% 280819
#% 722904
#% 769906
#% 1083734
#% 1214701
#! In this paper, we propose a joint probabilistic topic model for simultaneously modeling the contents of multi-typed objects of a heterogeneous information network. The intuition behind our model is that different objects of the heterogeneous network share a common set of latent topics so as to adjust the multinomial distributions over topics for different objects collectively. Experimental results demonstrate the effectiveness of our approach for the tasks of topic modeling and object clustering.

#index 1598456
#* Graph-cut based tag enrichment
#@ Xueming Qian;Xian-Sheng Hua
#t 2011
#c 13
#% 772862
#% 1190090
#% 1323560
#% 1356643
#! In this paper, a graph cut based tag enrichment approach is proposed. We build a graph for each image with its initial tags. The graph is with two terminals. Nodes of the graph are full connected with each other. Min-cut/max-flow algorithm is utilized to find the relevant tags for the image. Experiments on Flickr dataset demonstrate the effectiveness of the proposed graph-cut based tag enrichment approach.

#index 1598457
#* Personalized social query expansion using social bookmarking systems
#@ Mohamed Reda Bouadjenek;Hakim Hacid;Mokrane Bouzeghoub;Johann Daigremont
#t 2011
#c 13
#% 946524
#% 956544
#% 1292590
#! We propose a new approach for social and personalized query expansion using social structures in the Web 2.0. While focusing on social tagging systems, the proposed approach considers (i) the semantic similarity between tags composing a query, (ii) a social proximity between the query and the user profile, and (iii) on the fly, a strategy for expanding user queries. The proposed approach has been evaluated using a large dataset crawled from del.icio.us.

#index 1598458
#* What are the real differences of children's and adults' web search
#@ Tatiana Gossen;Thomas Low;Andreas Nürnberger
#t 2011
#c 13
#% 197968
#% 449291
#% 590523
#% 861981
#% 950658
#% 1179122
#% 1450995
#! We present first results of a logfile analysis on web search engines for children. The aim of this research is to analyse fundamental facts about how children's web search behaviour differs from that of adults. We show differences to previous results, which are often based on small lab experiments. Our large-scale analysis suggests that children search queries are more information-oriented and shorter on average. Children indeed make a lot of spelling errors and often repeat searches and revisit web pages.

#index 1598459
#* Cognitive coordinating behaviors in multitasking web search
#@ Jia Tina Du
#t 2011
#c 13
#% 879652
#% 1348356
#! This paper investigates how users cognitively coordinate multitasking Web search across different information search problems. The analysis suggests that (1) multitasking is a prevalent Web search behavior including both sequential multitasking (31%) and parallel multitasking (69%); (2) multitasking is performed through a task switching process; and (3) such a process is supported and underpinned by cognitive coordination mechanisms and strategy coordination.

#index 1598460
#* Optimizing multimodal reranking for web image search
#@ Hao Li;Meng Wang;Zhisheng Li;Zheng-Jun Zha;Jialie Shen
#t 2011
#c 13
#% 997240
#% 1131845
#% 1288794
#% 1298163
#% 1318819
#% 1775905
#! In this poster, we introduce a web image search reranking approach with exploring multiple modalities. Diff erent from the conventional methods that build graph with one feature set for reranking, our approach integrates multiple feature sets that describe visual content from different aspects. We simultaneously integrate the learning of relevance scores, the weighting of different feature sets, the distance metric and the scaling for each feature set into a unified scheme. Experimental results on a large data set that contains more than 1,100 queries and 1 million images demonstrate the effectiveness of our approach.

#index 1598461
#* Multi-layer graph-based semi-supervised learning for large-scale image datasets using mapreduce
#@ Wen-Yu Lee;Liang-Chi Hsieh;Guan-Long Wu;Winston Hsu;Ya-Fan Su
#t 2011
#c 13
#% 997229
#% 1023420
#% 1131852
#% 1279774
#% 1346154
#! Semi-supervised learning is to exploit the vast amount of unlabeled data in the world. This paper proposes a scalable graph-based technique leveraging the distributed computing power of the MapReduce programming model. For a higher quality of learning, the paper also presents a multi-layer learning structure to unify both visual and textual information of image data during the learning process. Experimental results show the effectiveness of the proposed methods.

#index 1598462
#* Tackling class imbalance and data scarcity in literature-based gene function annotation
#@ Mathieu Blondel;Kazuhiro Seki;Kuniaki Uehara
#t 2011
#c 13
#% 669214
#% 1332153
#! In recent years, a number of machine learning approaches to literature-based gene function annotation have been proposed. However, due to issues such as lack of labeled data, class imbalance and computational cost, they have usually been unable to surpass simpler approaches based on string-matching. In this paper, we propose a principled machine learning approach based on kernel classifiers. We show that kernels can address the task's inherent data scarcity by embedding additional knowledge and we propose a simple yet effective solution to deal with class imbalance. From experiments on the TREC Genomics Track data, our approach achieves better F1-score than two state-of-the-art approaches based on string-matching and cross-species information.

#index 1598463
#* Bootstrapping subjectivity detection
#@ Valentin Jijkoun;Maarten de Rijke
#t 2011
#c 13
#% 956510
#% 1074203
#% 1195902
#% 1471238
#! We describe a method for automatically generating subjectivity clues for a specific topic and a set of (relevant) document, evaluating it on the task of classifying sentences w.r.t. subjectivity, with improvements over previous work.

#index 1598464
#* The effects of choice in routing relevance judgments
#@ Edith Law;Paul N. Bennett;Eric Horvitz
#t 2011
#c 13
#% 248065
#% 879598
#% 907496
#% 967660
#% 987313
#% 1074134
#% 1166492
#% 1415710
#! The emergence of human computation systems, including Mechanical Turk and games with a purpose, has made it feasible to distribute relevance judgment tasks to workers over the Web. Most human computation systems assign tasks to individuals randomly, and such assignments may match workers with tasks that they may be unqualified or unmotivated to perform. We compare two groups of workers, those given a choice of queries to judge versus those who are not, in terms of their self-rated competence and their actual performance. Results show that when given a choice of task, workers choose ones for which they have greater expertise, interests, confidence, and understanding.

#index 1598465
#* Statistical feature extraction for cross-language web content quality assessment
#@ Guang-Gang Geng;Xiao-Dong Li;Li-Ming Wang;Wei Wang;Shuo Shen
#t 2011
#c 13
#% 869534
#% 987245
#% 1022742
#! Web content quality assessment is a typical static ranking problem. Heuristic content and TFIDF features based statistical systems have proven effective for Web content quality assessment. But they are all language dependent features, which are not suitable for cross-language ranking. In this paper, we fuse a series of language-independent features including hostname features, domain registration features, two-layer hyperlink analysis features and third-party Web service features to assess the Web content quality. The experiments on ECML/PKDD 2010 Discovery Challenge cross-language datasets show that the assessment is effective.

#index 1598466
#* Exploiting endorsement information and social influence for item recommendation
#@ Cheng-Te Li;Shou-De Lin;Man-Kwan Shan
#t 2011
#c 13
#% 729923
#% 915344
#% 1227601
#% 1450853
#% 1476460
#! Social networking services possess two features: (1) capturing the social relationships among people, represented by the social network, and (2) allowing users to express their preferences on different kinds of items (e.g. photo, celebrity, pages) through endorsing buttons, represented by a kind of endorsement bipartite graph. In this work, using such information, we propose a novel recommendation method, which leverages the viral marketing in the social network and the wisdom of crowds from endorsement network. Our recommendation consists of two parts. First, given some query terms describing user's preference, we find a set of targeted influencers who have the maximum activation probability on those nodes related to the query terms in the social network. Second, based on the derived targeted influencers as key experts, we recommend items via the endorsement network. We conduct the experiments on DBLP co-authorship social network with author-reference data as the endorsement network. The results show our method can achieve effective recommendations.

#index 1598467
#* Modeling subset distributions for verbose queries
#@ Xiaobing Xue;W. Bruce Croft
#t 2011
#c 13
#% 262096
#% 818262
#% 983820
#% 1074112
#% 1195837
#% 1227647
#% 1355019
#% 1450900
#% 1482284
#! Improving verbose (or long) queries poses a new challenge for search systems. Previous techniques mainly focused on two aspects, weighting the important words or phrases and selecting the best subset query. The former does not consider how words and phrases are used in actual subset queries, while the latter ignores alternative subset queries. Recently, a novel reformulation framework has been proposed to transform the original query as a distribution of reformulated queries, which overcomes the disadvantages of previous techniques. In this paper, we apply this framework to verbose queries, where a reformulated query is specified as a subset query. Experiments on TREC collections show that the query distribution based framework outperforms the state-of-the-art techniques.

#index 1598468
#* Domain expert topic familiarity and search behavior
#@ Sarvnaz Karimi;Falk Scholer;Adam Clark;Sadegh Kharazmi
#t 2011
#c 13
#% 378486
#% 751596
#% 1166518
#! Users of information retrieval systems employ a variety of strategies when searching for information. One factor that can directly influence how searchers go about their information finding task is the level of familiarity with a search topic. We investigate how the search behavior of domain experts changes based on their previous level of familiarity with a search topic, reporting on a user study of biomedical experts searching for a range of domain-specific material. The results of our study show that topic familiarity can influence the number of queries that are employed to complete a task, the types of queries that are entered, and the overall number of query terms. Our findings suggest that biomedical search systems should enable searching through a variety of querying modes, to support the different search strategies that users were found to employ depending on their familiarity with the information that they are searching for.

#index 1598469
#* Sample selection for dictionary-based corpus compression
#@ Christopher Hoobin;Simon Puglisi;Justin Zobel
#t 2011
#c 13
#% 290703
#% 438325
#% 1054227
#% 1355055
#% 1529947
#! Compression of large text corpora has the potential to drastically reduce both storage requirements and per-document access costs. Adaptive methods used for general-purpose compression are ineffective for this application, and historically the most successful methods have been based on word-based dictionaries, which allow use of global properties of the text. However, these are dependent on the text complying with assumptions about content and lead to dictionaries of unpredictable size. In recent work we have described an LZ-like approach in which sampled blocks of a corpus are used as a dictionary against which the complete corpus is compressed, giving compression twice as effective than that of zlib. Here we explore how pre-processing can be used to eliminate redundancy in our sampled dictionary. Our experiments show that dictionary size can be reduced by 50% or more (less than 0.1% of the collection size) with no significant effect on compression or access speed.

#index 1598470
#* Evaluating medical information retrieval
#@ Bevan Koopman;Peter Bruza;Laurianne Sitbon;Michael Lawley
#t 2011
#c 13
#% 644730
#% 1409961
#! This paper presents a framework for evaluating information retrieval of medical records. We use the BLULab corpus, a large collection of real-world de-identified medical records. The collection has been hand coded by clinical terminologists using the ICD-9 medical classification system. The ICD codes are used to devise queries and relevance judgements for this collection. Results of initial test runs using a baseline IR system show that there is room for improvement in medical information retrieval. Queries and relevance judgements are made available at http://aehrc.com/med_eval

#index 1598471
#* Region-based landmark discovery by crowdsourcing geo-referenced photos
#@ Yen-Ta Huang;An-Jung Cheng;Liang-Chi Hsieh;Winston Hsu;Kuo-Wei Chang
#t 2011
#c 13
#% 1190131
#% 1484459
#! We propose a novel model for landmark discovery that locates region-based landmarks on map in contrast to the traditional point-based landmarks. The proposed method preserves more information and automatically identifies candidate regions on map by crowdsourcing geo-referenced photos. Gaussian kernel convolution is applied to remove noises and generate detected region. We adopt F1 measure to evaluate discovered landmarks and manually check the association between tags and regions. The experiment results show that more than 90% of attractions in the selected city can be correctly located by this method.

#index 1598472
#* Towards effective short text deep classification
#@ Xinruo Sun;Haofen Wang;Yong Yu
#t 2011
#c 13
#% 1275012
#% 1450992
#% 1457107
#! Recently, more and more short texts (e.g., ads, tweets) appear on the Web. Classifying short texts into a large taxonomy like ODP or Wikipedia category system has become an important mining task to improve the performance of many applications such as contextual advertising and topic detection for micro-blogging. In this paper, we propose a novel multi-stage classification approach to solve the problem. First, explicit semantic analysis is used to add more features for both short texts and categories. Second, we leverage information retrieval technologies to fetch the most relevant categories for an input short text from thousands of candidates. Finally, a SVM classifier is applied on only a few selected categories to return the final answer. Our experimental results show that the proposed method achieved significant improvements on classification accuracy compared with several existing state of art approaches.

#index 1598473
#* Temporal latent semantic analysis for collaboratively generated content: preliminary results
#@ Yu Wang;Eugene Agichtein
#t 2011
#c 13
#% 1482241
#% 1560388
#! Latent semantic analysis (LSA) has been intensively studied because of its wide application to Information Retrieval and Natural Language Processing. Yet, traditional models such as LSA only examine one (current) version of the document. However, due to the recent proliferation of collaboratively generated content such as threads in online forums, Collaborative Question Answering archives, Wikipedia, and other versioned content, the document generation process is now directly observable. In this study, we explore how this additional temporal information about the document evolution could be used to enhance the identification of latent document topics. Specifically, we propose a novel hidden-topic modeling algorithm, temporal Latent Semantic Analysis (tLSA), which elegantly extends LSA to modeling document revision history using tensor decomposition. Our experiments show that tLSA outperforms LSA on word relatedness estimation using benchmark data, and explore applications of tLSA for other tasks.

#index 1598474
#* Self-adjusting hybrid recommenders based on social network analysis
#@ Alejandro Bellogin;Pablo Castells;Ivan Cantador
#t 2011
#c 13
#% 414514
#% 813966
#% 1385585
#% 1399963
#% 1480681
#! Ensemble recommender systems successfully enhance recom-mendation accuracy by exploiting different sources of user prefe-rences, such as ratings and social contacts. In linear ensembles, the optimal weight of each recommender strategy is commonly tuned empirically, with limited guarantee that such weights are optimal afterwards. We propose a self-adjusting hybrid recommendation approach that alleviates the social cold start situation by weighting the recommender combination dynamically at recommendation time, based on social network analysis algorithms. We show empirical results where our approach outperforms the best static combination for different hybrid recommenders.

#index 1598475
#* BlogCast effect on information diffusion in a blogosphere
#@ Sang-Wook Kim;Christos Faloutsos;Jiwoon Ha
#t 2011
#c 13
#% 1366208
#! A blog service company provides a function named BlogCast that exposes quality posts on the blog main page to vitalize a blogosphere. This paper analyzes a new type of information diffusion via BlogCast. We show that there exists a strong halo effect in a blogosphere via thorough investigation on a huge volume of blog data.

#index 1598476
#* Product comparison using comparative relations
#@ Si Li;Zheng-Jun Zha;Zhaoyan Ming;Meng Wang;Tat-Seng Chua;Jun Guo;Weiran Xu
#t 2011
#c 13
#% 805873
#% 854646
#% 879595
#% 1190068
#% 1251616
#% 1475165
#! This paper proposes a novel Product Comparison approach. The comparative relations between products are first mined from both user reviews on multiple review websites and community-based question answering pairs containing product comparison information. A unified graph model is then developed to integrate the resultant comparative relations for product comparison. Experiments on popular electronic products show that the proposed approach outperforms the state-of-the-art methods.

#index 1598477
#* Collaborative cyberporn filtering with collective intelligence
#@ Lung-Hao Lee;Hsin-Hsi Chen
#t 2011
#c 13
#% 845226
#% 961629
#% 1055885
#% 1076648
#! This paper presents a user intent method to generate blacklists for collaborative cyberporn filtering. A novel porn detection framework that finds new pornographic web pages by mining user search behaviors is proposed. It employs users' clicks in search query logs to select the suspected web pages without extra human efforts to label data for training, and determines their categories with the help of URL host name and path information, but without web page content. We adopt an MSN porn data set to explore the effectiveness of our method. This user intent approach achieves high precision, while maintaining favorably low false positive rate. In addition, real-life filtering simulation reveals that our user intent method with its accumulative update strategy achieves 43.36% of blocking rate, while maintaining a steadily less than 7% of over-blocking rate.

#index 1598478
#* Do IR models satisfy the TDC retrieval constraint
#@ Stéphane Clinchant;Eric Gaussier
#t 2011
#c 13
#% 766412
#% 1154026
#% 1227703
#% 1263575
#% 1450858

#index 1598479
#* On diversifying and personalizing web search
#@ David Vallet;Pablo Castells
#t 2011
#c 13
#% 879686
#% 1166473
#% 1396090
#% 1400011
#% 1697422
#! Diversification and personalization methods are common ap-proaches to deal with the one-size-fits-all paradigm of Web search engines. We performed a user study with 190 subjects where we analyzed the effects of diversification and personalization methods in a Web search engine. The obtained results suggest that our proposed combination of diversification and personalization factors may be a way to overcome the notion of intrusiveness in personalized approaches.

#index 1598480
#* Semantic tag recommendation using concept model
#@ Chenliang Li;Anwitaman Datta;Aixin Sun
#t 2011
#c 13
#% 956515
#% 1130827
#% 1130829
#% 1130858
#% 1254852
#% 1451236
#% 1587398
#! The common tags given by multiple users to a particular document are often semantically relevant to the document and each tag represents a specific topic. In this paper, we attempt to emulate human tagging behavior to recommend tags by considering the concepts contained in documents. Specifically, we represent each document using a few most relevant concepts contained in the document, where the concept space is derived from Wikipedia. Tags are then recommended based on the tag concept model derived from the annotated documents of each tag. Evaluated on a Delicious dataset of more than 53K documents, the proposed technique achieved comparable tag recommendation accuracy as the state-of-the-art, while yielding an order of magnitude speed-up.

#index 1598481
#* Recommending interesting activity-related local entities
#@ Jie Tang;Ryen W. White;Peter Bailey
#t 2011
#c 13
#% 818256
#% 956495
#% 1055920
#% 1125717
#% 1130854
#! When searching for entities with a strong local character (e.g., a museum), people may also be interested in discovering proximal activity-related entities (e.g., a café). Geographical proximity is a necessary, but not sufficient, qualifier for recommending other entities such that they are related in a useful manner (e.g., interest in a fish market does not imply interest in nearby bookshops, but interest in other produce stores is more likely). We describe and evaluate methods to identify such activity-related local entities.

#index 1598482
#* Cross-corpus relevance projection
#@ Nima Asadi;Donald Metzler;Jimmy Lin
#t 2011
#c 13
#% 1019084
#% 1150163
#% 1226913

#index 1598483
#* Location disambiguation for geo-tagged images
#@ Zhu Zhu;Lidan Shou;Kuang Mao;Gang Chen
#t 2011
#c 13
#% 273890
#% 1190131
#% 1484413
#! In this poster, we address the problem of location disambiguation for geotagged Web photo resources. We propose an approach for analyzing and partitioning large geotagged photo collections using geographic and semantic information. By organizing the dataset in a structural scheme, we resolve the location ambiguity and clutter problem yield by massive volume of geotagged photos.

#index 1598484
#* Towards an indexing method to speed-up music retrieval
#@ Benjamin Martin;Pierre Hanna;Matthias Robine;Pascal Ferraro
#t 2011
#c 13
#% 286744
#% 546096
#% 1588370
#% 1767273
#! Computations in most music retrieval systems strongly depend on the size of data compared. We propose to enhance performances of a music retrieval system, namely a harmonic similarity evaluation method, by first indexing relevant parts of music pieces. The indexing algorithm represents each audio piece exclusively by its major repetition, using harmonic descriptions and string matching techniques. Evaluations are performed in the context of a state-of-the-art retrieval method, namely cover songs identification, and results highlight the success of our indexing system in keeping similar results while yielding a substantial gain in computation time.

#index 1598485
#* An investigation of decompounding for cross-language patent search
#@ Johannes Leveling;Walid Magdy;Gareth J.F. Jones
#t 2011
#c 13
#% 561328
#% 732850
#% 735075
#% 811358
#% 1450905
#% 1482583
#! Decompounding has been found to improve information retrieval (IR) effectiveness in general domains for languages such as German or Dutch. We investigate if cross-language patent retrieval can profit from decompounding. This poses two challenges: i) There may be few resources such as parallel corpora available for training an machine translation system for a compounding language. ii) Patents have a specific writing style and vocabulary ("patentese"), which may affect the performance of decompounding and translation methods. Experiments on data from the CLEF-IP 2010 task show that decompounding patents for translation can overcome out-of-vocabulary problems (OOV) and that decompounding improves IR performance significantly for small training corpora.

#index 1598486
#* Detecting seasonal queries by time-series analysis
#@ Milad Shokouhi
#t 2011
#c 13
#% 960414
#% 1227692
#% 1481645
#% 1536521
#! Seasonal events such as Halloween and Christmas repeat every year and initiate several temporal information needs. The impact of such events on users is often reflected in search logs in form of seasonal spikes in the frequency of related queries (e.g. "halloween costumes", "where is santa"). Many seasonal queries such as "sigir conference" mainly target fresh pages (e.g. sigir2011.org) that have less usage data such as clicks and anchor-text compared to older alternatives (e.g.sigir2009.org). Thus, it is important for search engines to correctly identify seasonal queries and make sure that their results are temporally reordered if necessary. In this poster, we focus on detecting seasonal queries using time-series analysis. We demonstrate that the seasonality of a query can be determined with high accuracy according to its historical frequency distribution.

#index 1598487
#* Learning to rank under tight budget constraints
#@ Christian Pölitz;Ralf Schenkel
#t 2011
#c 13
#% 867054
#% 1075132
#% 1206832
#% 1355019
#% 1482186
#! This paper investigates the influence of pruning feature lists to keep a given budget for the evaluation of ranking methods. We learn from a given training set how important the individual prefixes are for the ranking quality. Based on there importance we choose the best prefixes to calculate the ranking while keeping the budget.

#index 1598488
#* A novel hybrid index structure for efficient text retrieval
#@ Andreas Broschart;Ralf Schenkel
#t 2011
#c 13
#% 879651
#% 1404894
#% 1482301
#! Query processing with precomputed term pair lists can improve efficiency for some queries, but suffers from the quadratic number of index lists that need to be read. We present a novel hybrid index structure that aims at decreasing the number of index lists retrieved at query processing time, trading off a reduced number of index lists for an increased number of bytes to read. Our experiments demonstrate significant cold-cache performance gains of almost 25% on standard benchmark queries.

#index 1598489
#* A weighted curve fitting method for result merging in federated search
#@ Chuan He;Dzung Hong;Luo Si
#t 2011
#c 13
#% 722312
#% 1174737
#! Result merging is an important step in federated search to merge the documents returned from multiple source-specific ranked lists for a user query. Previous result merging methods such as Semi-Supervised Learning (SSL) and Sample- Agglomerate Fitting Estimate (SAFE) use regression methods to estimate global document scores from document ranks in individual ranked lists. SSL relies on overlapping documents that exist in both individual ranked lists and a centralized sample database. SAFE goes a step further by using both overlapping documents with accurate rank information and documents with estimated rank information for regression. However, existing methods do not distinguish the accurate rank information from the estimated information. Furthermore, all documents are assigned equal weights in regression while intuitively, documents in the top should carry higher weights. This paper proposes a weighted curve fitting method for result merging in federated search. The new method explicitly models the importance of information from overlapping documents over non-overlapping ones. It also weights documents at different positions differently. Empirically results on two datasets clearly demonstrate the advantage of the proposed algorithm.

#index 1598490
#* Effect of different docid orderings on dynamic pruning retrieval strategies
#@ Nicola Tonellotto;Craig Macdonald;Iadh Ounis
#t 2011
#c 13
#% 198335
#% 213786
#% 656274
#% 730065
#% 1173690
#% 1392439
#% 1480887
#! Document-at-a-time (DAAT) dynamic pruning strategies for information retrieval systems such as MaxScore and Wand can increase querying efficiency without decreasing effectiveness. Both work on posting lists sorted by ascending document identifier (docid). The order in which docids are assigned -- and hence the order of postings in the posting lists -- is known to have a noticeable impact on posting list compression. However, the resulting impact on dynamic pruning strategies is not well understood. In this poster, we examine the impact on the efficiency of these strategies across different docid orderings, by experimenting using the TREC ClueWeb09 corpus. We find that while the number of postings scored by dynamic pruning strategies do not markedly vary for different docid orderings, the ordering still has a marked impact on mean query response time. Moreover, when docids are assigned by lexicographical URL ordering, the benefit to response time for is more pronounced for Wand than for MaxScore.

#index 1598491
#* Time-based query performance predictors
#@ Nattiya Kanhabua;Kjetil Nørvåg
#t 2011
#c 13
#% 397161
#% 960414
#% 1195854
#% 1415713
#% 1495112
#% 1697416
#! Query performance prediction is aimed at predicting the retrieval effectiveness that a query will achieve with respect to a particular ranking model. In this paper, we study query performance prediction for a ranking model that explicitly incorporates the time dimension into ranking. Different time-based predictors are proposed as analogous to existing keyword-based predictors. In order to improve predicting performance, we combine different predictors using linear regression and neural networks. Extensive experiments are conducted using queries and relevance judgments obtained by crowdsourcing.

#index 1598492
#* Search task difficulty: the expected vs. the reflected
#@ Jingjing Liu;Nicholas J. Belkin
#t 2011
#c 13
#% 860649
#% 1126944
#% 1384094
#% 1523400
#! We report findings on how the user's perception of task difficulty changes before and after searching for information to solve tasks. We found that while in one type of task, the dependent task, this did not change, in another, the parallel task, it did. The findings have implications on designing systems that can provide assistance to users with their search and task solving strategies.

#index 1598493
#* On the suitability of diversity metrics for learning-to-rank for diversity
#@ Rodrygo L.T. Santos;Craig Macdonald;Iadh Ounis
#t 2011
#c 13
#% 1024548
#% 1268491
#% 1400021
#% 1442574
#% 1536510
#% 1598393
#! An optimally diverse ranking should achieve the maximum coverage of the aspects underlying an ambiguous or underspecified query, with minimum redundancy with respect to the covered aspects. Although evaluation metrics that reward coverage and penalise redundancy provide intuitive objective functions for learning a diverse ranking, it is unclear whether they are the most effective. In this paper, we contrast the suitability of relevance and diversity metrics as objective functions for learning a diverse ranking. Our results in the context of the diversity task of the TREC 2009 and 2010 Web tracks show that diversity metrics are not necessarily better suited for guiding a learning approach. Moreover, the suitability of these metrics is compromised as they try to penalise redundancy during the learning process.

#index 1598494
#* How diverse are web search results?
#@ Rodrygo L.T. Santos;Craig Macdonald;Iadh Ounis
#t 2011
#c 13
#% 1024548
#% 1074133
#% 1400021
#! Search result diversification has recently gained attention as a means to tackle ambiguous queries. While query ambiguity is of particular concern for the short queries commonly observed in a Web search scenario, it is unclear how much diversity is actually promoted by Web search engines (WSEs). In this paper, we assess the diversification performance of two leading WSEs in the context of the diversity task of the TREC 2009 and 2010 Web tracks. Our results show that these WSEs perform effectively for queries with multiple interpretations, but not for those open to multiple aspects related to a single interpretation. Moreover, by deploying a state-of-the-art diversification approach based on query suggestions from these WSEs themselves, we show that their diversification performance can be further improved.

#index 1598495
#* Analysis of an expert search query log
#@ Yi Fang;Naveen Somasundaram;Luo Si;Jeongwoo Ko;Aditya P. Mathur
#t 2011
#c 13
#% 296646
#% 323131
#% 1227750
#% 1455269
#% 1565539
#! Expert search has made rapid progress in modeling, algorithms and evaluations in the recent years. However, there is very few work on analyzing how users interact with expert search systems. In this paper, we conduct analysis of an expert search query log. The aim is to understand the special characteristics of expert search usage. To the best of our knowledge, this is one of the earliest work on expert search query log analysis. We find that expert search users generally issue shorter queries, more common queries, and use more advanced search features, with fewer queries in a session, than general Web search users do. This study explores a new research direction in expert search by analyzing and exploiting query logs.

#index 1598496
#* A model for expert finding in social networks
#@ Elena Smirnova
#t 2011
#c 13
#% 769906
#% 987261
#% 1195844
#% 1399976
#! Expert finding is a task of finding knowledgeable people on a given topic. State-of-the-art expertise retrieval algorithms identify matching experts based on analysis of textual content of documents experts are associated with. While powerful, these models ignore social structure that might be available. In this paper, we develop a Bayesian hierarchical model for expert finding that accounts for both social relationships and content. The model assumes that social links are determined by expertise similarity between candidates. We demonstrate the improved retrieval performance of our model over the baseline on a realistic data set.

#index 1598497
#* Transductive learning over automatically detected themes for multi-document summarization
#@ Massih-Reza Amini;Nicolas Usunier
#t 2011
#c 13
#% 466425
#% 818228
#% 840846
#% 987208
#% 1456843
#! We propose a new method for query-biased multi-document summarization, based on sentence extraction. The summary of multiple documents is created in two steps. Sentences are first clustered; where each cluster corresponds to one of the main themes present in the collection. Inside each theme, sentences are then ranked using a transductive learning-to-rank algorithm based on RankNet, in order to better identify those which are relevant to the query. The final summary contains the top-ranked sentences of each theme. Our approach is validated on DUC 2006 and DUC 2007 datasets.

#index 1598498
#* Rating-based collaborative filtering combined with additional regularization
#@ Shu Wu;Shengrui Wang
#t 2011
#c 13
#% 840846
#% 1083671
#% 1214666
#% 1541728
#! The collaborative filtering (CF) approach to recommender system has received much attention recently. However, previous work mainly focuses on improving the formula of rating prediction, e.g. by adding user and item biases, implicit feedback and time-aware factors, etc, to reach a better prediction by minimizing an objective function. However, little effort has been made on improving CF by incorporating additional regularization to the objective function. Regularization can further bound the searching range of predicted ratings. In this paper, we improve the conventional rating-based objective function by using ranking constraints as the supplementary regularization to restrict the searching of predicted ratings in smaller and more likely ranges, and develop a novel method, called RankSVD++, based on the SVD++ model. Experimental results show that RankSVD++ achieves better performance than existing main-streaming methods due to the addition of informative ranking-based regularization. The idea proposed here can also be easily incorporated to the other CF models.

#index 1598499
#* Words-of-interest selection based on temporal motion coherence for video retrieval
#@ Lei Wang;Dawei Song;Eyad Elyan
#t 2011
#c 13
#% 1058303
#% 1131856
#% 1169843
#% 1272808
#% 1279775
#! The "Bag of Visual Words" (BoW) framework has been widely used in query-by-example video retrieval to model the visual content by a set of quantized local feature descriptors. In this paper, we propose a novel technique to enhance BoW by the selection of Word-of-Interest (WoI) that utilizes the quantified temporal motion coherence of the visual words between the adjacent frames in the query example. Experiments carried out using TRECVID datasets show that our technique improves the retrieval performance of the classical BoW-based approach.

#index 1598500
#* Aggregating multiple opinion evidence in proximity-based opinion retrieval
#@ Shima Gerani;Mostafa Keikha;Fabio Crestani
#t 2011
#c 13
#% 40313
#% 1450879
#! Blog post opinion retrieval is the problem of ranking blog posts according to the likelihood that the post is relevant to the query and that the author was expressing an opinion about the topic (of the query). A recent study has proposed a method for finding the opinion density at query term positions in a document which uses the proximity of query term and opinion term as an indicator of their relatedness. The maximum opinion density between different query positions was used as an opinion score of the whole document. In this paper we investigate the effect of exploiting multiple opinion evidence of a document. We propose using the ordered weighted averaging (OWA) operator in order to combine the opinion score of different query positions for a final score of a document, in the proximity-based opinion retrieval system.

#index 1598501
#* Enhancing mobile search using web search log data
#@ Yoshiyuki Inagaki;Jiang Bian;Yi Chang;Motoko Maki
#t 2011
#c 13
#% 411762
#% 577224
#% 779037
#% 1227582
#! Mobile search is still in infancy compared with general purpose web search. With limited training data and weak relevance features, the ranking performance in mobile search is far from satisfactory. To address this problem, we propose to leverage the knowledge of Web search to enhance the ranking of mobile search. In this paper, we first develop an equivalent page conversion between web search and mobile search, then we design a few novel ranking features, generated from the click-through data in web search, for estimating the relevance of mobile search. Large scale evaluations demonstrate that the knowledge from web search is quite effective for boosting the relevance of ranking on mobile search.

#index 1598502
#* Award prediction with temporal citation network analysis
#@ Zaihan Yang;Dawei Yin;Brian D. Davison
#t 2011
#c 13
#% 879570
#% 907525
#% 1116996
#% 1450914
#% 1537469
#! Each year many ACM SIG communities will recognize an outstanding researcher through an award in honor of his or her profound impact and numerous research contributions. This work is the first to investigate an automated mechanism to help in selecting future award winners. We approach the problem as a researchers' expertise ranking problem, and propose a temporal probabilistic ranking model which combines content with citation network analysis. Experimental results based on real-world citation data and historical awardees indicate that some kinds of SIG awards are well-modeled by this approach.

#index 1598503
#* Rating prediction using feature words extracted from customer reviews
#@ Masanao Ochi;Makoto Okabe;Rikio Onai
#t 2011
#c 13
#% 769892
#! We developed a simple method of improving the accuracy of rating prediction using feature words extracted from customer reviews. Many rating predictors work well for a small and dense dataset of customer reviews. However, a practical dataset tends to be large and sparse, because it often includes too many products for each customer to buy and evaluate. Data sparseness reduces prediction accuracy. To improve accuracy, we reduced the dimension of the feature vector using feature words extracted by analyzing the relationship between ratings and accompanying review comments instead of using ratings. We applied our method to the Pranking algorithm and evaluated it on a corpus of golf course reviews supplied by a Japanese e-commerce company. We found that by successfully reducing data sparseness, our method improves prediction accuracy as measured using RankLoss.

#index 1598504
#* Ranking tags in resource collections
#@ Dimitrios Skoutas;Mohammad Alrifai
#t 2011
#c 13
#% 340936
#% 1190090
#% 1190093
#! We examine different tag ranking strategies for constructing tag clouds to represent collections of tagged objects. The proposed methods are based on random walk on graphs, diversification, and rank aggregation, and they are empirically evaluated on a data set of tagged images from Flickr.

#index 1598505
#* Identifying similar people in professional social networks with discriminative probabilistic models
#@ Suleyman Cetintas;Monica Rogati;Luo Si;Yi Fang
#t 2011
#c 13
#% 1355278
#% 1450837
#! Identifying similar professionals is an important task for many core services in professional social networks. Information about users can be obtained from heterogeneous information sources, and different sources provide different insights on user similarity. This paper proposes a discriminative probabilistic model that identifies latent content and graph classes for people with similar profile content and social graph similarity patterns, and learns a specialized similarity model for each latent class. To the best of our knowledge, this is the first work on identifying similar professionals in professional social networks, and the first work that identifies latent classes to learn a separate similarity model for each latent class. Experiments on a real-world dataset demonstrate the effectiveness of the proposed discriminative learning model.

#index 1598506
#* Intent-oriented diversity in recommender systems
#@ Saul Vargas;Pablo Castells;David Vallet
#t 2011
#c 13
#% 262112
#% 805841
#% 1074133
#% 1127465
#% 1166473
#% 1260273
#! Diversity as a relevant dimension of retrieval quality is receiving increasing attention in the Information Retrieval and Recommender Systems (RS) fields. The problem has nonetheless been approached under different views and formulations in IR and RS respectively, giving rise to different models, methodologies, and metrics, with little convergence between both fields. In this poster we explore the adaptation of diversity metrics, techniques, and principles from ad-hoc IR to the recommendation task, by introducing the notion of user profile aspect as an analogue of query intent. As a particular approach, user aspects are automatically extracted from latent item features. Empirical results support the proposed approach and provide further insights.

#index 1598507
#* Disambiguating biomedical acronyms using EMIM
#@ Nut Limsopatham;Rodrygo L.T. Santos;Craig Macdonald;Iadh Ounis
#t 2011
#c 13
#% 906618
#% 1130937
#! Expanding a query with acronyms or their corresponding 'long-forms' has not been shown to provide consistent improvements in the biomedical IR literature. The major open issue with expanding acronyms in a query is their inherent ambiguity, as an acronym can refer to multiple long-forms. At the same time, a long-form identified in a query can be expanded with its acronym(s); however, some of these may be also ambiguous and lead to poor retrieval performance. In this work, we propose the use of the EMIM (Expected Mutual Information Measure) between a long-form and its abbreviated acronym to measure ambiguity. We experiment with expanding both acronyms and long-forms identified in the queries from the adhoc task of the TREC 2004 Genomics track. Our preliminary analysis shows the potential of both acronym and long-form expansions for biomedical IR.

#index 1598508
#* Best document selection based on approximate utility optimization
#@ Hungyu Henry Lin;Yi Zhang;James Davis
#t 2011
#c 13
#% 520224
#% 577224
#% 734915
#% 1035587
#% 1673039
#! This poster describes an alternative approach to handling the best document selection problem. Best document selection is a common problem with many real world applications, but is not a well studied problem by itself; a simple solution would be to treat it as a ranking problem and to use existing ranking algorithms to rank all documents. We could then select only the first element of the sorted list. However, because ranking models optimize for all ranks, the model may sacrifice accuracy of the top rank for the sake of overall accuracy. This is an unnecessary trade-off. We begin by first defining an appropriate objective function for the domain, then create a boosting algorithm that explicitly targets this function. Based on experiments on a benchmark retrieval data set and Digg.com news commenting data set, we find that even a simple algorithm built for this specific problem gives better results than baseline algorithms that were designed for the more complicated ranking tasks.

#index 1598509
#* Forecasting counts of user visits for online display advertising with probabilistic latent class models
#@ Suleyman Cetintas;Datong Chen;Luo Si;Bin Shen;Zhanibek Datbayev
#t 2011
#c 13
#% 829139
#% 1426587
#! Display advertising is a multi-billion dollar industry where advertisers promote their products to users by having publishers display their advertisements on popular Web pages. An important problem in online advertising is how to forecast the number of user visits for a Web page during a particular period of time. Prior research addressed the problem by using traditional time-series forecasting techniques on historical data of user visits; (e.g., via a single regression model built for forecasting based on historical data for all Web pages) and did not fully explore the fact that different types of Web pages have different patterns of user visits. In this paper we propose a probabilistic latent class model to automatically learn the underlying user visit patterns among multiple Web pages. Experiments carried out on real-world data demonstrate the advantage of using latent classes in forecasting online user visits.

#index 1598510
#* Knowledge effects on document selection in search results pages
#@ Michael J. Cole;Xiangmin Zhang;Chang Liu;Nicholas J. Belkin;Jacek Gwizdka
#t 2011
#c 13
#% 946521
#% 1124986
#% 1166518
#% 1357833
#% 1697487
#! Click through events in search results pages (SERPs) are not reliable implicit indicators of document relevance. A user's task and domain knowledge are key factors in recognition and link selection and the most useful SERP document links may be those that best match the user's domain knowledge. User study participants rated their knowledge of genomics MeSH terms before conducting 2004 TREC Genomics Track tasks. Each participant's document knowledge was represented by their knowledge of the indexing MeSH terms. Results show high, intermediate, and low domain knowledge groups had similar document selection SERP rank distributions. SERP link selection distribution varied when participant knowledge of the available documents was analyzed. High domain knowledge participants usually selected a document with the highest personal knowledge rating. Low domain knowledge participants were reasonably successful at selecting available documents of which they had the most knowledge, while intermediate knowledge participants often failed to do so. This evidence for knowledge effects on SERP link selection may contribute to understanding the potential for personalization of search results ranking based on user domain knowledge.

#index 1598511
#* Learning to rank from a noisy crowd
#@ Abhimanu Kumar;Matthew Lease
#t 2011
#c 13
#% 983820
#% 1083692
#% 1450886
#% 1598511
#! We study how to best use crowdsourced relevance judgments learning to rank [1, 7]. We integrate two lines of prior work: unreliable crowd-based binary annotation for binary classification [5, 3], and aggregating graded relevance judgments from reliable experts for ranking [7]. To model varying performance of the crowd, we simulate annotation noise with varying magnitude and distributional properties. Evaluation on three LETOR test collections reveals a striking trend contrary to prior studies: single labeling outperforms consensus methods in maximizing learner accuracy relative to annotator eýort. We also see surprising consistency of the learning curve across noise distributions, as well as greater challenge with the adversarial case for multi-class labeling.

#index 1598512
#* How to count thumb-ups and thumb-downs?: an information retrieval approach to user-rating based ranking of items
#@ Dell Zhang;Robert Mao;Haitao Li;Joanne Mao
#t 2011
#c 13
#% 248214
#% 1077150
#% 1130311
#% 1263581
#! It is a common practice among Web 2.0 services to allow users to rate items on their sites. In this paper, we first point out the flaws of the popular methods for user-rating based ranking of items, and then argue that two well-known Information Retrieval (IR) techniques, namely the Probability Ranking Principle and Statistical Language Modelling, provide a simple but effective solution to this problem.

#index 1598513
#* Predicting users' domain knowledge from search behaviors
#@ Xiangmin Zhang;Michael Cole;Nicholas Belkin
#t 2011
#c 13
#% 187271
#% 1047297
#% 1166518
#! This study uses regression modeling to predict a user's domain knowledge level (DK) from implicit evidence provided by certain search behaviors. A user study (n=35) with recall-oriented search tasks in the genomic domain was conducted. A number of regression models of a person's DK, were generated using different behavior variable selection methods. The best model highlights three behavior variables as DK predictors: the number of documents saved, the average query length, and the average ranking position of documents opened. The model is validated using the split sampling method. Limitations and future research directions are discussed.

#index 1598514
#* The interactive PRP for diversifying document rankings
#@ Guido Zuccon;Leif Azzopardi;C.J. "Keith" van Rijsbergen
#t 2011
#c 13
#% 262112
#% 375017
#% 1051038
#% 1227591
#% 1697443
#! The assumptions underlying the Probability Ranking Principle (PRP) have led to a number of alternative approaches that cater or compensate for the PRP's limitations. In this poster we focus on the Interactive PRP (iPRP), which rejects the assumption of independence between documents made by the PRP. Although the theoretical framework of the iPRP is appealing, no instantiation has been proposed and investigated. In this poster, we propose a possible instantiation of the principle, performing the first empirical comparison of the iPRP against the PRP. For document diversification, our results show that the iPRP is significantly better than the PRP, and comparable to or better than other methods such as Modern Portfolio Theory.

#index 1598515
#* Detecting success in mobile search from interaction
#@ Qi Guo;Shuai Yuan;Eugene Agichtein
#t 2011
#c 13
#% 1169589
#% 1190135
#% 1227585
#% 1355038
#% 1450833
#% 1450845
#% 1450994
#% 1573487
#! Predicting searcher success and satisfaction is a key problem in Web search, which is essential for automatic evaluating and improving search engine performance. This problem has been studied actively in the desktop search setting, but not specifically for mobile search, despite many known differences between the two modalities. As mobile devices become increasingly popular for searching the Web, improving the searcher experience on such devices is becoming crucially important. In this paper, we explore the possibility of predicting searcher success and satisfaction in mobile search with a smart phone. Specifically, we investigate client-side interaction signals, including the number of browsed pages, and touch screen-specific actions such as zooming and sliding. Exploiting this information with machine learning techniques results in nearly 80% accuracy for predicting searcher success -- significantly outperforming the previous models.

#index 1598516
#* Measuring assessor accuracy: a comparison of nist assessors and user study participants
#@ Mark D. Smucker;Chandra Prakash Jethani
#t 2011
#c 13
#% 1450903
#! In many situations, humans judging document relevance are forced to trade-off accuracy for speed. The development of better interactive retrieval systems and relevance assessing platforms requires the measurement of assessor accuracy, but to date the subjective nature of relevance has prevented such measurement. To quantify assessor performance, we define relevance to be a group's majority opinion, and demonstrate the value of this approach by comparing the performance of NIST assessors to a group of assessors representative of participants in many information retrieval user studies. Using data collected as part of a user study with 48 participants, we found that NIST assessors discriminate between relevant and non-relevant documents better than the average participant in our study, but that NIST assessors' true positive rate is no better than that of the study participants. In addition, we found NIST assessors to be conservative in their judgment of relevance compared to the average participant.

#index 1598517
#* A bipartite graph based social network splicing method for person name disambiguation
#@ Jintao Tang;Qin Lu;Ting Wang;Ji Wang;Wenjie Li
#t 2011
#c 13
#% 1083721
#% 1292487
#% 1450830
#! The key issue of person name disambiguation is to discover different namesakes in massive web documents rather than simply cluster documents by using textual features. In this paper, we describe a novel person name disambiguation method based on social networks to effectively identify namesakes. The social network snippets in each document are extracted. Then, the namesakes are identified via splicing the social networks of each namesake by using the snippets as a bipartite graph. Experimental results show that our method achieves better result than the top performance of WePS-2 in identifying different namesakes.

#index 1598518
#* Link formation analysis in microblogs
#@ Dawei Yin;Liangjie Hong;Xiong Xiong;Brian D. Davison
#t 2011
#c 13
#% 881526
#% 1512414
#! Unlike a traditional social network service, a microblogging network like Twitter is a hybrid network, combining aspects of both social networks and information networks. Understanding the structure of such hybrid networks and to predict new links are important for many tasks such as friend recommendation, community detection, and network growth models. In this paper, by analyzing data collected over time, we find that 90% of new links are to people just two hops away and dynamics of friend acquisition are also related to users' account age. Finally, we compare two popular sampling methods which are widely used for network analysis and find that ForestFire does not preserve properties required for the link prediction task.

#index 1598519
#* Evolution of web search results within years
#@ Ismail Sengor Altingovde;Rifat Ozcan;Özgür Ulusoy
#t 2011
#c 13
#% 577370
#% 878624
#% 956646
#% 1587379
#! We provide a first large-scale analysis of the evolution of query results obtained from a real search engine at two distant points in time, namely, in 2007 and 2010, for a set of 630,000 real queries.

#index 1598520
#* Decayed DivRank: capturing relevance, diversity and prestige in information networks
#@ Pan Du;Jiafeng Guo;Xue-Qi Cheng
#t 2011
#c 13
#% 268079
#% 290830
#% 348173
#% 1312812
#% 1451241
#% 1482428
#! Many network-based ranking approaches have been proposed to rank objects according to different criteria, including relevance, prestige and diversity. However, existing approaches either only aim at one or two of the criteria, or handle them with additional heuristics in multiple steps. Inspired by DivRank, we propose a unified ranking model, Decayed DivRank (DDRank), to meet the three criteria simultaneously. Empirical experiments on paper citation network show that DDRank can outperform existing algorithms in capturing relevance, diversity and prestige simultaneously in ranking.

#index 1598521
#* Multi-objective optimization in learning to rank
#@ Na Dai;Milad Shokouhi;Brian D. Davison
#t 2011
#c 13
#% 309095
#% 577224
#% 987266
#% 1000205
#% 1227751
#% 1355017
#% 1450846
#% 1598343
#! Supervised learning to rank algorithms typically optimize for high relevance and ignore other facets of search quality, such as freshness and diversity. Prior work on multi-objective ranking trained rankers focused on using hybrid labels that combine overall quality of documents, and implicitly incorporate multiple criteria into quantifying ranking risks. However, these hybrid scores are usually generated based on heuristics without considering potential correlations between individual facets (e.g., freshness versus relevance). In this poster, we empirically demonstrate that the correlation between objective facets in multi-criteria ranking optimization may significantly influence the effectiveness of trained rankers with respect to each objective.

#index 1598522
#* A large-scale study of the effect of training set characteristics over learning-to-rank algorithms
#@ Evangelos Kanoulas;Stefan Savev;Pavel Metrikov;Virgil Pavlu;Javed Aslam
#t 2011
#c 13
#% 734915
#% 881477
#% 1227635
#! In this work we describe the results of a large-scale study on the effect of the distribution of labels across the different grades of relevance in the training set on the performance of trained ranking functions. In a controlled experiment we generate a large number of training datasets wih different label distributions and employ three learning to rank algo- rithms over these datasets. We investigate the effect of these distributions on the accuracy of obtained ranking functions to give an insight into the manner training sets should be constructed.

#index 1598523
#* Exploring term temporality for pseudo-relevance feedback
#@ Stewart Whiting;Yashar Moshfeghi;Joemon M. Jose
#t 2011
#c 13
#% 960414
#% 1074081
#% 1227584
#! As digital collections expand, the importance of the temporal aspect of information has become increasingly apparent. The aim of this paper is to investigate the effect of using long-term temporal profiles of terms in information retrieval by enhancing the term selection process of pseudo-relevance feedback (PRF). For this purpose, two temporal PRF approaches were introduced considering only temporal aspect and temporal along with textual aspect. Experiments used the AP88-89 and WSJ87-92 test collections with TREC Ad-Hoc Topics 51-100. Term temporal profiles are extracted from the Google Books n-grams dataset. The results show that the long-term temporal aspects of terms are capable of enhancing retrieval effectiveness.

#index 1598524
#* MSSF: a multi-document summarization framework based on submodularity
#@ Jingxuan Li;Lei Li;Tao Li
#t 2011
#c 13
#% 297675
#% 1074086
#% 1074089
#% 1223708
#% 1264797
#% 1269936
#% 1292747
#% 1470696
#! Multi-document summarization aims to distill the most representative information from a set of documents to generate a summary. Given a set of documents as input, most of existing multi-document summarization approaches utilize different sentence selection techniques to extract a set of sentences from the document set as the summary. The submodularity hidden in textual-unit similarity motivates us to incorporate this property into our solution to multi-document summarization tasks. In this poster, we propose a new principled and versatile framework for different multi-document summarization tasks using the submodular function [8].

#index 1598525
#* SEJoin: an optimized algorithm towards efficient approximate string searches
#@ Junfeng Zhou;Ziyang Chen;Jingrong Zhang
#t 2011
#c 13
#% 765463
#% 1206665
#! We investigated the problem of finding from a collection of strings those similar to a given query string based on edit distance, for which the critical operation is merging inverted lists of grams generated from the collection of strings. We present an efficient algorithm to accelerate the merging operation.

#index 1598526
#* Bag-of-visual-words vs global image descriptors on two-stage multimodal retrieval
#@ Konstantinos Zagoris;Savvas A. Chatzichristofis;Avi Arampatzis
#t 2011
#c 13
#% 1227642
#% 1484663
#% 1587366
#% 1587421
#! The Bag-Of-Visual-Words (BOVW) paradigm is fast becoming a popular image representation for Content-Based Image Retrieval (CBIR), mainly because of its better retrieval effectiveness over global feature representations on collections with images being near-duplicate to queries. In this experimental study we demonstrate that this advantage of BOVW is diminished when visual diversity is enhanced by using a secondary modality, such as text, to pre-filter images. The TOP-SURF descriptor is evaluated against Compact Composite Descriptors on a two-stage image retrieval setup, which first uses a text modality to rank the collection and then perform CBIR only on the top-K items.

#index 1598527
#* Query term ranking based on search results overlap
#@ Wei Song;Yu Zhang;Yubin Xie;Ting Liu;Sheng Li
#t 2011
#c 13
#% 818267
#% 1074112
#% 1292594
#% 1355019
#! In this paper, we propose a method to rank and assign weights to query terms according to their impact on the topic of the query. We use Search Result Overlap Ratio (SROR) to quantify the overlap of the search results of the full query and a shorten query after removing one term. Intuitively, if the overlap is small, it indicates a big topic shift and the removed term should be discriminative and important. The SROR could be used for measuring query term importance with a search engine automatically. By this way, learning based models could be trained based on a large number of automatically labeled instances and make predictions for future queries efficiently.

#index 1598528
#* Tossing coins to trim long queries
#@ Sudip Datta;Vasudeva Varma
#t 2011
#c 13
#% 577224
#% 1173692
#% 1227647
#% 1450900
#! Verbose web queries are often descriptive in nature where a term based search engine is unable to distinguish between the essential and noisy words, which can result in a drift from the user intent. We present a randomized query reduction technique that builds on an earlier learning to rank based approach. The proposed technique randomly picks only a small set of samples, instead of the exponentially many sub-queries, thus being fast enough to be useful for web search engines, while still covering wide sub-query space.

#index 1598529
#* A comparison of time-aware ranking methods
#@ Nattiya Kanhabua;Kjetil Nørvåg
#t 2011
#c 13
#% 730070
#% 766408
#% 807756
#% 1495112
#% 1697416
#! When searching a temporal document collection, e.g., news archives or blogs, the time dimension must be explicitly incorporated into a retrieval model in order to improve relevance ranking. Previous work has followed one of two main approaches: 1) a mixture model linearly combining textual similarity and temporal similarity, or 2) a probabilistic model generating a query from the textual and temporal part of a document independently. In this paper, we compare the effectiveness of different time-aware ranking methods by using a mixture model applied to all methods. Extensive evaluation is conducted using the New York Times Annotated Corpus, queries and relevance judgments obtained using the Amazon Mechanical Turk.

#index 1598530
#* Learning for graphs with annotated edges
#@ Fan Li
#t 2011
#c 13
#% 881557
#! Automatic classification with graphs containing annotated edges is an interesting problem and has many potential applications. We present a risk minimization formulation that exploits the annotated edges for classification tasks. One major advantage of our approach compared to other methods is that the weight of each edge in the graph structures in our model, including both positive and negative weights, can be learned automatically from training data based on edge features. The empirical results show that our approach can lead to significantly improved classification performance compared to several baseline approaches.

#index 1598531
#* Formulating effective questions for community-based question answering
#@ Saori Suzuki;Shin'ichi Nakayama;Hideo Joho
#t 2011
#c 13
#% 818260
#% 835027
#% 1047396
#% 1055738
#% 1074111
#% 1179994
#% 1183152
#! Community-based Question Answering (CQA) services have become a major venue for people's information seeking on the Web. However, many studies on CQA have focused on the prediction of the best answers for a given question. This paper looks into the formulation of effective questions in the context of CQA. In particular, we looked at effect of contextual factors appended to a basic question on the performance of submitted answers. This study analysed a total of 930 answers returned in response to 266 questions that were formulated by 46 participants. The results show that adding a questionnaire's personal and social attribute to the question helped improve the perceptions of answers both in information seeking questions and opinion seeking questions.

#index 1598532
#* ClusteringWiki: personalized and collaborative clustering of search results
#@ Dragos C. Anastasiu;Byron J. Gao;David Buttler
#t 2011
#c 13
#% 1077150
#% 1202162
#! How to organize and present search results plays a critical role in the utility of search engines. Due to the unprecedented scale of the Web and diversity of search results, the common strategy of ranked lists has become increasingly inadequate, and clustering has been considered as a promising alternative. Clustering divides a long list of disparate search results into a few topic-coherent clusters, allowing the user to quickly locate relevant results by topic navigation. While many clustering algorithms have been proposed that innovate on the automatic clustering procedure, we introduce ClusteringWiki, the first prototype and framework for personalized clustering that allows direct user editing of clustering results. Through a Wiki interface, the user can edit and annotate the membership, structure and labels of clusters for a personalized presentation. In addition, the edits and annotations can be shared among users as a mass collaborative way of improving search result organization and search engine utility.

#index 1598533
#* OrientSTS: spatio-temporal sequence searching in flickr
#@ Chunjie Zhou;Dongqi Liu;Xiaofeng Meng
#t 2011
#c 13
#% 989604
#% 1190134
#! Nowadays, due to the increasing user requirements of efficient and personalized services, a perfect travel plan is urgently needed. However, at present it is hard for people to make a personalized traveling plan. Most of them follow other people's general travel trajectory. So only after finishing their travel, do they know which scene is their favorite, which is not, and what is the perfect order of visits. In this research we propose a novel spatio-temporal sequence (STS) searching, which mainly includes two steps. Firstly, we propose a novel method to detect tourist features of every scene, and its difference in different seasons. Secondly, combined with personal profile and scene features, a set of interesting scenes will be chosen and each scene has a specific weight for each user. The goal of our research is to provide the traveler with the STS, which passes through as many chosen scenes as possible with the maximum weight and the minimum distance within his travel time. We propose a method based on topic model to detect scene features, and provide two approximate algorithms to mine STS: a local optimization algorithm and a global optimization algorithm. System evaluations have been conducted and the performance results show the efficiency.

#index 1598534
#* A toolkit for knowledge base population
#@ Zheng Chen;Suzanne Tamang;Adam Lee;Heng Ji
#t 2011
#c 13
#! The main goal of knowledge base population (KBP) is to distill entity information (e.g., facts of a person) from multiple unstructured and semi-structured data sources, and incorporate the information into a knowledge base (KB). In this work, we intend to release an open source KBP toolkit that is publicly available for research purposes.

#index 1598535
#* iMecho: a context-aware desktop search system
#@ Jidong Chen;Hang Guo;Wentao Wu;Wei Wang
#t 2011
#c 13
#% 987195
#% 1190074
#! In this demo, we present iMecho, a context-aware desktop search system to help users get more relevant results. Different from other desktop search engines, iMecho ranks results not only by the content of the query, but also the context of the query. It employs an Hidden Markov Model (HMM)-based user model, which is learned from user's activity logs, to estimate the query context when he submits the query. The results from keyword search are re-ranked by their relevances to the context with acceptable overhead.

#index 1598536
#* Visualizing and querying semantic social networks
#@ Aixin Sun;Anwitaman Datta;Ee-Peng Lim;Kuiyu Chang
#t 2011
#c 13
#% 844513
#% 902433
#% 1246972
#% 1485927
#! We demonstrate SSNetViz that is developed for integrating, visualizing and querying heterogeneous semantic social networks obtained from multiple information sources. A semantic social network refers to a social network graph with multi-typed nodes and links. We demonstrate various innovative features of SSNetViz with social networks from three information sources covering a similar set of entities and relationships in terrorism domain.

#index 1598537
#* What-you-retrieve-is-what-you-see: a preliminary cyber-physical search engine
#@ Lidan Shou;Ke Chen;Gang Chen;Chao Zhang;Yi Ma;Xian Zhang
#t 2011
#c 13
#% 1190687
#% 1328137
#! The cyber-physical systems (CPS) are envisioned as a class of real-time systems integrating the computing, communication and storage facilities with monitoring and control of the physical world. One interesting CPS application in the mobile Internet is to provide Web search "on the spot" regarding the physical world that a user sees, or literally WYRIWYS (What-You-Retrieve-Is-What-You-See). The objective of our work is to develop server/browser software for supporting WYRIWYS search in our prototype cyber-physical search engine. A WYRIWYS search retrieves visible Web objects and ranks them by their cyber-physical relevances (term, visual, spatial, temporal etc.). This work is distinguished from previous LWS as it provides quality Web search geared with the physical world. Therefore it suggests a very promising solution to cyber-physical Web search.

#index 1598538
#* QuickView: advanced search of tweets
#@ Xiaohua Liu;Long Jiang;Furu Wei;Ming Zhou;QuickView Team Microsoft
#t 2011
#c 13
#% 1484319
#! Tweets have become a comprehensive repository for real-time information. However, it is often hard for users to quickly get information they are interested in from tweets, owing to the sheer volume of tweets as well as their noisy and informal nature. We present QuickView, an NLP-based tweet search platform to tackle this issue. Specifically, it exploits a series of natural language processing technologies, such as tweet normalization, named entity recognition, semantic role labeling, sentiment analysis, tweet classification, to extract useful information, i.e., named entities, events, opinions, etc., from a large volume of tweets. Then, non-noisy tweets, together with the mined information, are indexed, on top of which two brand new scenarios are enabled, i.e., categorized browsing and advanced search, allowing users to effectively access either the tweets or fine-grained information they are interested in.

#index 1598539
#* Personalized video: leanback online video consumption
#@ Krishnan Ramanathan;Yogesh Sankarasubramaniam;Vidhya Govindaraju
#t 2011
#c 13
#! Current user interfaces for online video consumption are mostly browser based, lean forward, require constant interaction and provide a fragmented view of the total content available. For easier consumption, the user interface and interactions need to be redesigned for less interruptive and lean back experience. In this paper, we describe Personalized Video, an application that converts the online video experience into a personalized lean back experience. It has been implemented on the Windows platform and integrated with intuitive user interactions like gesture and face recognition. It also supports group personalization for concurrent users.

#index 1598540
#* GreenMeter: a tool for assessing the quality and recommending tags for web 2.0 applications
#@ Saulo M.R. Ricci;Dilson A. Guimarães;Fabiano M. Belém;Jussara M. Almeida;Marcos A. Gonçalves;Raquel Prates
#t 2011
#c 13
#% 1055704
#% 1495608
#% 1598437
#! We present GreenMeter, a tool for assessing the quality and recommending tags for Web 2.0 content. Its goal is to improve tag quality and the effectiveness of various information services (e.g., search, content recommendation) that rely on tags as data sources. We demonstrate an implementation of GreenMeter for the popular Last.fm application.

#index 1598541
#* JuSe: a picture dictionary query system for children
#@ Tamara Polajnar;Richard Glassey;Leif Azzopardi
#t 2011
#c 13
#% 449291
#% 825821
#% 1384136
#% 1783060
#! As adults we take for granted our capacity to express our information needs verbally and textually. However, young children also have preferences and information needs, but are just learning to be able to express themselves effectively. Consequently they encounter many barriers when trying to spell, type, and communicate their needs to a 'faceless' search engine text box. Junior Search (JuSe) is an interface that enables preschoolers and young children to search and find consumable online content (such as games for kids, videos, etc.) through adaptable picture dictionaries. Inspired by educational children's toys, rather than search engines designed for adults, JuSe incorporates a learning element by combining audio-visual and textual cues to improve written word recognition and vocabulary skills. JuSe provides an interactive learning environment that allows parents to introduce new words and concepts into the child's lexicon, as well as controlling the content and search queries.

#index 1598542
#* CrowdTracker: enabling community-based real-time web monitoring
#@ James Caverlee;Zhiyuan Cheng;Brian Eoff;Chiao-Fang Hsu;Krishna Kamath;Jeffrey McGee
#t 2011
#c 13
#% 1482254
#% 1536563
#! CrowdTracker is a community-based web monitoring system optimized for real-time web streams like Twitter, Facebook, and Google Buzz. In this demo summary, we provide an overview of the system and architecture, and outline the demonstration plan.

#index 1598543
#* The Meta-Dex Suite: generating and analyzing indexes and meta-indexes
#@ Michael Huggett;Edie Rasmussen
#t 2011
#c 13
#% 56449
#% 1415729
#! Our Meta-dex software suite extracts content and index text from a corpus of PDF files, and generates a meta-index that references entries across an entire domain. We provide tools to analyze the individual and integrated indexes, and visualize entries and books within the meta-index. The suite is scalable to very large data sets.

#index 1598544
#* Tulsa: web search for writing assistance
#@ Duo Ding;Xingping Jiang;Matthew R. Scott;Ming Zhou;Yong Yu
#t 2011
#c 13
#% 1399978
#% 1468142

#index 1598545
#* The TREC files: the (ground) truth is out there
#@ Savvas A. Chatzichristofis;Konstantinos Zagoris;Avi Arampatzis
#t 2011
#c 13
#! Traditional tools for information retrieval (IR) evaluation, such as TREC's trec_eval, have outdated command-line interfaces with many unused features, or 'switches', accumulated over the years. They are usually seen as cumbersome applications by new IR researchers, steepening the learning curve. We introduce a platform-independent application for IR evaluation with a graphical easy-to-use interface: the TREC_Files Evaluator. The application supports most of the standard measures used for evaluation in TREC, CLEF, and elsewhere, such as MAP, P10, P20, and bpref, as well as the Averaged Normalized Modified Retrieval Rank (ANMRR) proposed by MPEG for image retrieval evaluation. Additional features include a batch mode and statistical significance testing of the results against a pre-selected baseline.

#index 1598546
#* A tool for comparative IR evaluation on component level
#@ Thomas Wilhelm;Jens Kürsten;Maximilian Eibl
#t 2011
#c 13
#% 1292526
#% 1312807
#% 1496282

#index 1598547
#* Machine learning for information retrieval
#@ Luo Si;Rong Jin
#t 2011
#c 13
#! In recent years, we have witnessed successful application of machine learning techniques to a wide range of information retrieval problems, including Web search engines, recommendation systems, online advertising, etc. It is thus critical for researchers in the information retrieval community to understand the core machine learning techniques. In order to accommodate audiences with different levels of understanding of machine learning, we divide this tutorial into two sessions: the first session will focus on basic machine learning concepts and tools; in the second session, we will introduce more advanced topics in machine learning, and will present recent developments in machine learning and its application to information retrieval. Each season is self-contained. Session 1: Core Learning Technologies for Information Retrieval. This session of the tutorial will cover the core machine learning methods, basic optimization techniques and key information retrieval applications. In particular, it includes: 1). Core concepts in machine learning, such as supervised learning/unsupervised learning, bias and variance trade off, and probabilistic models; 2). Useful concepts and algorithms in optimization including the first and second order gradient methods, and Expectation and Maximization; 3). The application of machine learning methods to key information retrieval problems including text classification, collaborative filtering, clustering and learning to rank; Session 2: Emerging Learning Technologies for Information Retrieval. This session will cover more advanced machine learning techniques that have started to be utilized in information retrieval applications. In particular, it will cover: 1). Advanced Optimization Techniques including stochastic optimization and smooth minimization; 2). Emerging Learning Techniques such as Multiple-Instance Learning, Active Learning and Semi-supervised Learning. The tutorial will benefit a large body of audience in the information retrieval community, ranging from students who are new to machine learning to the seasoned researchers who would like to understand the recent advance in machine learning for information retrieval research. This tutorial will also benefit the practitioners who apply learning techniques to real-world information retrieval systems.

#index 1598548
#* Enhancing web search by mining search and browse logs
#@ Daxin Jiang;Jian Pei;Hang Li
#t 2011
#c 13
#! Huge amounts of search log data have been accumulated in various search engines. Currently, a commercial search engine receives billions of queries and collects tera-bytes of log data on any single day. Other than search log data, browse logs can be collected by client-side browser plug-ins, which record the browse information if users' permissions are granted. Such massive amounts of search/browse log data, on the one hand, provide great opportunities to mine the wisdom of crowds and improve web search results. On the other hand, designing effective and efficient methods to clean, model, and process large scale log data also presents great challenges. In this tutorial, we will focus on mining search and browse log data for search engines. We will start with an introduction of search and browse log data and an overview of frequently-used data summarization in log mining. We will then elaborate how log mining applications enhance the five major components of a search engine, namely, query understanding, document understanding, query-document matching, user understanding, and monitoring and feedbacks. For each aspect, we will survey the major tasks, fundamental principles, and state-of-the-art methods. Finally, we will discuss the challenges and future trends of log data mining. The goal of this tutorial is to provide a systematic survey on large-scale search/browse log mining to the IR community. It may help IR researchers to get familiar with the core challenges and promising directions in log mining. At the same time, this tutorial may also serve the developers of web information retrieval systems as a comprehensive and in-depth reference to the advanced log mining techniques.

#index 1598549
#* A new look at old tricks: the fertile roots of current research
#@ Paul B. Kantor
#t 2011
#c 13
#! As we face an explosion of potential new applications for the fundamental concepts and technologies of information retrieval, ranging from ad ranking to social media, from collaborative recommending to question answering systems, many researchers are spending unnecessary time reinventing ideas and relationships that are buried in the prehistory of information retrieval (which, for many researchers, means anything published before they entered graduate school). A lot of the ideas that surface as "new" in today's super-heated research environment have very firm roots in earlier developments in fields as diverse as citation analysis and pattern recognition. The purpose of this tutorial is to survey those roots, and their relation to the contemporary fruits on the tree of information retrieval, and to separate, as much as is possible in an era of increasing secrecy about methods, the problems to be solved, the algorithms for solving them, and the heuristics that are the bread and butter of a working operation. Participants will become familiar with roots in Pattern Analysis, Statistics, Information Science and other sources of key ideas that reappear in the current development of Information Retrieval as it applies to Search Engines, Social Media, and Collaborative Systems. They will be able to separate problems from algorithms, and algorithms from heuristics, in the application of these ideas to their own research and/or development activities. Course materials will be made available on a Web site two weeks prior to the tutorial. They will include links to relevant software; links to publications that will be discussed; and mechanisms for chat among the tutorial participants, before, during and after the tutorial.

#index 1598550
#* Crowdsourcing for information retrieval: principles, methods, and applications
#@ Omar Alonso;Matthew Lease
#t 2011
#c 13
#! Crowdsourcing has emerged in recent years as a promising new avenue for leveraging today's digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained particular attention as active online market places for reaching and tapping into this still largely under-utilized workforce. Crowdsourcing also offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite understanding of principles, platforms, and best practices. We will introduce the opportunities and challenges of crowdsourcing while discussing the three issues above. This will provide a basic foundation to begin crowdsourcing in the context of one's own particular tasks

#index 1598551
#* Practical online retrieval evaluation
#@ Filip Radlinski;Yisong Yue
#t 2011
#c 13
#! Online evaluation is amongst the few evaluation techniques available to the information retrieval community that is guaranteed to reflect how users actually respond to improvements developed by the community. Broadly speaking, online evaluation refers to any evaluation of retrieval quality conducted while observing user behavior in a natural context. However, it is rarely employed outside of large commercial search engines due primarily to a perception that it is impractical at small scales. The goal of this tutorial is to familiarize information retrieval researchers with state-of-the-art techniques in evaluating information retrieval systems based on natural user clicking behavior, as well as to show how such methods can be practically deployed. In particular, our focus will be on demonstrating how the Interleaving approach and other click based techniques contrast with traditional offline evaluation, and how these online methods can be effectively used in academic-scale research. In addition to lecture notes, we will also provide sample software and code walk-throughs to showcase the ease with which Interleaving and other click-based methods can be employed by students, academics and other researchers.

#index 1598552
#* Web retrieval: the role of users
#@ Ricardo Baeza-Yates;Yoelle Maarek
#t 2011
#c 13
#! Web retrieval methods have evolved through three major steps in the last decade or so. They started from standard document-centric IR in the early days of the Web, then made a major step forward by leveraging the structure of the Web, using link analysis techniques in both crawling and ranking challenges. A more recent, no less important but maybe more discrete step forward, has been to enter the user in this equation in two ways: (1) Implicitly, through the analysis of usage data captured by query logs, and session and click information in general; the goal here being to improve ranking as well as to measure user's happiness and engagement; (2) Explicitly, by offering novel interactive features; the goal here being to better answer users' needs. This half day tutorial covers the user-related challenges associated with the implicit and explicit role of users in Web retrieval. More specifically, we review and discuss challenges associated with: (1) Usage data analysis and metrics - It is critical to monitor how users take advantage and interact with Web retrieval systems, as this implicit relevant feedback aggregated at a large scale, can provide insights on users' underlying intent as well as approximate quite accurately the level of success of a given feature. Here we have to consider not only clicks statistics, the sequences of queries, the time spent in a page, the number of actions per session, etc. This is the focus of the first part of the tutorial. (2) User interaction - Given the intrinsic problems posed by the Web, the key challenge for the user is to conceive a good query to be submitted to the search system, one that leads to a manageable and relevant answer. The retrieval system must assist users during two key stages of interaction: efore the query is fully expressed and after the results are returned. After quite some stagnation on the front-end of Web retrieval, we have seen numerous novel interactive features appear in the last 3 to 4 years, as the leading commercial search engines seem to compete for users' attention. The second part of the tutorial will be dedicated to explicit user interaction. We will introduce novel material (as compared to previous versions of this tutorial that were given at SIGIR'2010, WSDM'2011 and ECIR'2011) in order to reflect recent Web search features such as Google Instant or Yahoo! Direct Search. The goal of this tutorial is to teach the key principles and technologies behind the activities and challenges briefly outlined above, bring new understanding and insights to the attendees, and hopefully foster future research. A previous version of this tutorial was offered at the ACM SIGIR'2010, WSDM'2011 and ECIR'2011.

#index 1598553
#* Information organization and retrieval with collaboratively generated content
#@ Eugene Agichtein;Evgeniy Gabrilovich
#t 2011
#c 13
#! Proliferation of ubiquitous access to the Internet enables millions of Web users to collaborate online on a variety of activities. Many of these activities result in the construction of large repositories of knowledge, either as their primary aim (e.g., Wikipedia) or as a by-product (e.g., Yahoo! Answers). In this tutorial, we will discuss organizing and exploiting Collaboratively Generated Content (CGC) for information organization and retrieval. Specifically, we intend to cover two complementary areas of the problem: (1) using such content as a powerful enabling resource for knowledge-enriched, intelligent representations and new information retrieval algorithms, and (2) development of supporting technologies for extracting, filtering, and organizing collaboratively created content. The unprecedented amounts of information in CGC enable new, knowledge-rich approaches to information access, which are significantly more powerful than the conventional word-based methods. Considerable progress has been made in this direction over the last few years. Examples include explicit manipulation of human-defined concepts and their use to augment the bag of words (cf. Explicit Semantic Analysis), using large-scale taxonomies of topics from Wikipedia or the Open Directory Project to construct additional class-based features, or using Wikipedia for better word sense disambiguation. However, the quality and comprehensiveness of collaboratively created content vary widely, and in order for this resource to be useful, a significant amount of preprocessing, filtering, and organization is necessary. Consequently, new methods for analyzing CGC and corresponding user interactions are required to effectively harness the resulting knowledge. Thus, not only the content repositories can be used to improve IR methods, but the reverse pollination is also possible, as better information extraction methods can be used for automatically collecting more knowledge, or verifying the contributed content. This natural connection between modeling the generation process of CGC and effectively using the accumulated knowledge suggests covering both areas together in a single tutorial. The intended audience of the tutorial includes IR researchers and graduate students, who would like to learn about the recent advances and research opportunities in working with collaboratively generated content. The emphasis of the tutorial is on comparing the existing approaches and presenting practical techniques that IR practitioners can use in their research. We also cover open research challenges, as well as survey available resources (software tools and data) for getting started in this research field.

#index 1598554
#* Persistence in the ephemeral: utilizing repeat behaviors for multi-session personalized search
#@ Sarah K. Tyler
#t 2011
#c 13
#% 805898
#% 954970
#% 987211
#% 1355035
#% 1392483
#! As the abundance of information on the Internet grows, an increasing burden is placed on the user to specify his or her query precisely in order to avoid extraneous results that may be relevant, but not useful. At the same time, users have a tendency to repeat their search behaviors, seeking the same URL (re-finding) as well as issuing the same query (re-searching). These repeated actions reveal a form of user preference that the search engine can utilize to personalize the results. In our approach, we personalize search results related to ongoing tasks, allowing for a different degree of strength of interest, and diversity of interest per task. We focus on high valued queries; queries that are both related to past queries and will be related to future queries given the ongoing nature of the task.

#index 1598555
#* Search engines that learn online
#@ Katja Hofmann
#t 2011
#c 13
#% 1587358
#! The goal of my research is to develop self-learning search engines, that can learn online, i.e., directly from interactions with actual users. Such systems can continuously adapt to user preferences throughout their lifetime, leading to better search performance in settings where expensive manual tuning is infeasible. Challenges that are addressed in my work include the development of effective online learning to rank algorithms for IR, user aspects, and evaluation.

#index 1598556
#* Query expansion based on a semantic graph model
#@ Xue Jiang
#t 2011
#c 13
#% 836145
#% 1227584
#% 1698872
#% 1916157
#! Query expansion is a classical topic in the field of information retrieval, which is proposed to bridge the gap between searchers' information intents and their queries. Previous researches usually expand queries based on document collections, or some external resources such as WordNet and Wikipedia [1, 2, 3, 4, 5]. However, it seems that independently using one of these resources has some defects, document collections lack semantic information of words, while WordNet and Wikipedia may not include domain-specific knowledge in certain document collection. Our work aims to combine these two kinds of resources to establish an expansion model which represents not only domain-specific information but also semantic information. In our preliminary experiments, we construct a two-layer word graph and use Random-Walk algorithm to calculate the weights of each term in pseudo-relevance feedback documents, then select the highest weighted term to expand original query. The first layer of the word graph contains terms in related documents, while the second layer contains semantic senses corresponding to these terms. These terms and semantic senses are treated as vertices of the graph and connected with each other by all possible relationships, such as mutual information and semantic similarities. We utilized mutual information, semantic similarity and uniform distribution as the weight of term-term relation, sense-sense relation and word-sense relation respectively. Though these experiments show that our expansion outperform original queries, we are troubled with some difficult problems. Given the framework of semantic graph model, we need more effort to find out an optimal graph to represent the relationships between terms and their semantic senses. We utilized a two-layer graph model in our preliminary research, where terms from different documents are treated equally. Maybe we can introduce the document as a third layer in future work, where we can differ the same terms in different documents according to document relevance and context. Then we need appropriately represent initial weights of this words, senses and relationships. Various measures for weights of terms and term relations have been proved effective in other information retrieval tasks, such as TFIDF, mutual information (MI), but there is little research on weights for semantic senses and their relations. For polysemous words, we add all of their semantic senses to the graph and assume that these senses are uniformly distributed. Actually, it is not precise for a word in a special document and query. As we know, a polysemous word may have only one or two senses in a document, and they are not uniformly distributed. Give a word, what we should do is to determine its word senses in a relevant document and estimate the distribution of these senses. Word sense disambiguation may help us in this problem. Then, there are many methods to compute word similarity according to WordNet, which we use to represent the weights of relationships between word senses. Varelas et al implemented some popular methods to compute semantic similarity by mapping terms to an ontology and examining their relationships in that ontology [4]. We also need to know which algorithm for semantic similarity is most suitable for our model. Additional, WordNet is suitable to calculate word similarity but not suitable to measure word relevance. The inner hyperlinks of Wikipedia could help us to calculate word relevance. We wish to find an effective way to combine the similarity measure from WordNet and relevance measure from Wikipedia, which may completely reflect word relationships.

#index 1598557
#* Descriptive modelling of text classification and its integration with other IR tasks
#@ Miguel Martinez-Alvarez
#t 2011
#c 13
#% 1021951
#% 1275150
#% 1392431
#! Nowadays, Information Retrieval (IR) systems have to deal with multiple sources of data available in different formats. Datasets can consist of complex and heterogeneous objects with relationships between them. In addition, information needs can vary wildly and they can include different tasks. As a result, the importance of flexibility in IR systems is rapidly growing. This fact is specially important in environments where the information required at different moments is very different and its utility may be contingent on timely implementation. In these cases, how quickly a new problem is solved is as important as how well you solve it. Current systems are usually developed for specific cases. It implies that too much engineering effort is needed to adapt them when new knowledge appears or there are changes in the requirements. Furthermore, heterogeneous and linked data present greater challenges, as well as the simultaneous application of different tasks. This research proposes the usage of descriptive approaches for three different purposes: the modelling of the specific task of Text Classification (TC), focusing on knowledge and complex data exploitation; the flexible application of models to different tasks; and the simultaneously application of different IR-tasks. This investigation will contribute to the long-term goal of achieving a descriptive and composable IR technology that provides a modular framework that knowledge engineers can compose into a task-specific solution. The ultimate goal is to develop a flexible framework that offers classifiers, retrieval models, information extractors, and other functions. In addition, those functional blocks could be customised to satisfy user needs. Descriptive approaches allow a high-level definition of algorithms which are, in some cases, as compact as mathematical formulations. One of the expected benefits is to make the implementation clearer and the knowledge transfer easier. They allow models from different tasks to be defined as modules that can be "concatenated", processing the information as a pipeline where some of the outputs of one module are the input of the following one. This combination involves minimum engineering effort due to the paradigm's "Plug & Play" capabilities offered by its functional syntax. This solution provides the flexibility needed to customise and quickly combine different IR-tasks and/or models. Classification is a desired candidate for being part of a flexible IR framework because it can be required in several situations for different purposes. In particular, descriptive approaches will improve its modelling with complex and heterogeneous objects. Furthermore, we aim to show how this approach allows to apply TC models for ad-hoc retrieval (and vice versa) and their simultaneous application for complex information needs. The main hypothesis of this research is that a seamless approach for modelling TC and its integration with other IR-tasks will provide a general framework for rapid prototyping and modelling of solutions for specific users. In addition, it will allow new complex models that take into account relationships and inference from large ontologies. The importance of flexibility for information systems and the exploitation of complex information and knowledge from heterogeneous sources are the main points for discussion. The main challenges are expressiveness and scalability. Abstraction improves flexibility and maintainability. However, it limits the modelling power. A balance between abstraction and expressiveness has to be reached. On the other hand, scalability has been traditionally a challenge for descriptive modelling. Our goal is to prove the feasibility of our approach for real-scale environments.

#index 1598558
#* Efficient and effective solutions for search engines
#@ Xiang-Fei Jia
#t 2011
#c 13
#% 1146245
#% 1489449
#% 1622397

#index 1598559
#* Modeling document scores for distributed information retrieval
#@ Ilya Markov
#t 2011
#c 13
#% 1292546
#% 1558080
#! Distributed Information Retrieval (DIR), also known as Federated Search, integrates multiple searchable collections and provides direct access to them through a unified interface [3]. This is done by a centralized broker, that receives user queries, forwards them to appropriate collections and returns merged results to users. In practice, most of federated resources do not cooperate with a broker and do not provide neither their content nor the statistics used for retrieval. This is known as uncooperative DIR. In this case a broker creates a resource representation by sending sample queries to a collection and analyzing retrieved documents. This process is called query-based sampling. The key issue here is the following: 1.1 How many documents have to be retrieved from a resource in order to obtain a representative sample? Although there have been a number of attempts to address this issue it is still not solved appropriately. For a given user query resources are ranked according to their similarity to the query or based on the number of relevant documents they contain. Since resource representations are usually incomplete, the similarity or the number of relevant documents cannot be calculated precisely. Resource selection algorithms proposed in the literature estimate these numbers based on incomplete samples. However these estimates are subjects to error. In practice, inaccurate estimates that have high error should be trusted less then the more accurate estimates with low error. Unfortunately none of the existing algorithms can make the calculation of the estimation errors possible. Therefore the following questions arise: 2.1 How to estimate resource scores so that the estimation errors can be calculated? 2.2 How to use these errors in order to improve the resource selection performance? Existing results merging algorithms estimate normalized document scores based on scores of documents that appear both in a sample and in a result list. The problem similar to the resource selection one arises. The normalized document scores are only the estimates and are subjects to error. Inaccurate estimates should be trusted less then the more accurate ones. Again none of the existing algorithms provide a way for calculating these errors. Thus the two question to be address on the results merging phase are similar to the resource selection ones: 3.1 How to estimate normalized document scores so that the estimation errors can be calculated? 3.2 How to use these errors in order to improve the results merging performance? In this work we address the above issues by applying score distribution models (SDM) to different phases of DIR [2]. In particular, we discuss the SDM-based resource selection technique that allows the calculation of resource score estimation errors and can be extended in order to calculate the number of documents to be sampled from each resource for a given query. We have performed initial experiments comparing the SDM-based resource selection technique to the state-of-the-art algorithms and we are currently experimenting with the SDM-based results merging method. We plan to apply the existing score normalization techniques from meta-search to the DIR results merging problem [1]. However, the SDM-based results merging approaches require the relevance scores to be returned together with retrieved documents. It is not yet clear how to relax this strong assumption that does not always hold in practice.

#index 1598560
#* Improving query and result list adaptation in personalized multilingual information retrieval
#@ M. Rami Ghorab
#t 2011
#c 13
#% 1365637
#% 1396084
#% 1396090
#% 1494818
#! A general characteristic of Information Retrieval (IR) and Multilingual IR (MIR) [5] systems is that if the same query was submitted by different users, the system would yield the same results, regardless of the user. On the other hand, Adaptive Hypermedia (AH) systems operate in a personalized manner where the services are adapted to the user [1]. Personalized IR (PIR) is motivated by the success in both areas, IR and AH [4]. IR systems have the advantage of scalability and AH systems have the advantage of satisfying individual user needs. The majority of studies in PIR literature have focused on monolingual IR, and relatively little work has been done concerning multilingual IR. This PhD research study aims to improve personalization in MIR systems, by improving the relevance of multilingual search results with respect to the user and not just the query. The study investigates how to model different aspects of a multilingual search user. Information about users can be demographic information, such as language and country, or information about the user's search interests. This information can be gathered explicitly by asking the user to supply the required information or implicitly by inferring the information from the user's search history. The study will then investigate how to exploit the modeled user information to personalize the user's multilingual search by performing query and result list adaptation. The main research questions that are addressed in this study are: how to improve the relevance of search results with respect to individual users in PMIR and how to construct profiles that represent aspects and interests of a multilingual search user. So far, the work carried out for this study included: (1) a proposed framework for the delivery and evaluation of PMIR [3]; and (2) exploratory experiments with search history and collection (result) re-ranking on a dataset of multilingual search logs [2]. The next stage of experimentation will involve the investigation and development of algorithms for: (1) constructing multilingual user profiles; (2) pre-translation and post-translation query expansion based on terms from the user profile; and (3) result list re-ranking based on the user's interests, and preferred language. Two types of experiments will be conducted in an in-lab setting, with a group of users from different linguistic backgrounds. In the first set of experiments, users will be asked to use a baseline web search system for their daily search activities over a period of time. The baseline system will be wrapped around one of the major search engines. Interactions with the system will be logged, and part of this information will be used for training the system (constructing user profiles from text of queries and clicked documents); the other part (remaining queries) will be used for testing the effectiveness of the query adaptation and result list adaptation algorithms, where the users will be asked to provide some personal relevance judgements. In the second set of experiments, the users will be asked to use the PMIR system to fulfill a number of defined search tasks. Quantitative and qualitative techniques will be used to evaluate different aspects of the experiments, including: (1) retrieval effectiveness, which can be measured using standard IR metrics; (2) user's performance on search tasks, which can be measured in terms of time and number of actions needed to fulfill the tasks; (3) user profile accuracy, which can be assessed by questionnaires that indicate how well the user profile depicted the users' search interests; and (4) usability and user satisfaction, which can be assessed using standard system usability questionnaires.

#index 1598561
#* Using k-Top retrieved web snippets to date temporalimplicit queries based on web content analysis
#@ Ricardo Nuno Taborda Campos
#t 2011
#c 13
#% 960414
#% 1227692
#% 1292475
#% 1697416
#! The World Wide Web (WWW) is a huge information network from which retrieving and organizing quality relevant content remains an open question for mostly all ambiguous queries. As an example, many queries have temporal implicit intents associated with them but they are not inferred by search engines. Inferring the user intentions and the period he has in mind, may therefore play an extremely important role in the improvement of the results. Our work goes in this direction. We aim to introduce a temporal analysis framework for analyzing documents in a temporal dimension in order to identify and understand the temporal nature of any given query, namely implicit ones. Our analysis is not based on metadata, but on the exploitation of temporal information from the content itself, particularly within web snippets, which are interesting pieces of concentrated information, where time clues, especially years, often appear. Our intention is to develop a language-independent solution and to model the degree of relationship between the terms and dates identified. This is the core part of the framework and the basis for both temporal query understanding and search results exploration, such as temporal clustering. We believe that inferring this knowledge is a very important step in the process of adding a temporal dimension to IR systems, thus disambiguating a large class of queries for which search engines continue to fail.

#index 1598562
#* Domain-specific information retrieval using rcommenders
#@ Wei Li
#t 2011
#c 13
#% 1489429
#! The continuing increase in the volume of information available in our daily lives is creating ever greater challenges for people to find personally useful information. One approach used to addressing this problem is Personalized Information Retrieval (PIR). PIR systems collect a user's personal information from both implicit and explicit sources to build a user profile with the objective of giving retrieval results which better meet their individual user information needs than a standard Information Retrieval (IR) system. However, in many situations there may be no opportunity to learn about the specific interests of a user and build a personal model when this user is querying on a new topic, e.g. when a user visits a museum or exhibition which is unrelated to their normal search interests. Under this condition, the experiences and behaviours of other previous users, who have made similar queries, could be used to build a model of user behavior in this domain. My PhD proposes to focus on the development of new and innovative methods of domain-specific IR. My work seeks to combine recommender algorithms trained using previous search behaviours from different searchers with a standard ranked IR method to form a domain-specific IR model to improve the search effectiveness for a user entering a query without personal prior search history on this topic. The challenges for my work are: how to provide users better results; how to train and evaluate the methods proposed in my work.

#index 1598563
#* Understanding and using contextual information in recommender systems
#@ Licai Wang
#t 2011
#c 13
#% 1127499
#% 1480674

#index 1598564
#* Multidimensional search result diversification: diverse search results for diverse users
#@ Sumit Bhatia
#t 2011
#c 13
#% 1074133
#% 1166473
#% 1400021
#% 1482296
#% 1560378
#! Hundreds of millions of people today rely on Web based Search Engines to satisfy their information needs. In order to meet the expectations of this vast and diverse user population, the search engine should present a list of results such that the probability of satisfying the average user is maximized. This leads us to the problem of Search Result Diversification. Given a user submitted query, the search engine should include results that are relevant to the user query and at the same time, diverse enough to meet the expectations of diverse user populations. However, it is not clear in what respect the results should be diversified. Much of the current work in diversity focuses on ambiguous and underspecified queries and tries to include results corresponding to diverse interpretations of the ambiguous query. This is not always sufficient. My analysis of a commercial web search engine's logs reveals that even for well-specified informational queries, click entropy is very high indicating that different users prefer different types of documents. Very recently, a diversification algorithm fine-tuned for such informational queries has been proposed. Further, high click entropies were also observed for a large fraction of transactional queries. One major goal of my PhD thesis will then be to identify the various possible dimensions along which the search results can be diversified. Having such an information will enhance our understanding about the expectations of an average user from the search engine. By utilizing aggregate statistics about queries, users and their interaction with the search engine for different queries, more concrete evidences about diverse user preferences as well as relative importance of different diversity dimensions can be derived. Once we know different diversity dimensions, the next natural question is: given a query, how can we determine the diversification requirement best suited for the query? For some queries sub-topic coverage may be more important while for others diversification with respect to document source or stylistics might be important. This problem is related to the problem of selective diversification where the goal is to identify queries for which diversification techniques should be used. However, in addition, we are also interested in identifying different diversity classes a given query belongs to. Further, for some queries it may be required to diversify along multiple diversity dimensions. In such cases, it is also important to determine the relative importance of different diversity dimensions for the given query. By utilizing past user interaction data, query level features (like query clarity, entropy, lexical features etc.) and document level features (e.g. popularity, content quality, previous click history etc.), classifiers for diversification requirements can be developed. Given a user query, once we know the type of diversity requirements for the user, an appropriate diversification technique is required. I would like to study the problem of simultaneously diversifying search results along multiple dimensions, as discussed above. One possible way here could be to build upon the nugget based framework introduced by Clarke et al. where we represent each document as a set of nuggets, each nugget corresponding to a diversity dimension.

#index 1598565
#* Sensor-aided mobile information management and retrieval
#@ Edward Y. Chang
#t 2011
#c 13
#! The number of "smart" mobile devices such as wireless phones and tablet computers has been rapidly growing. These mobile devices are equipped with a variety of sensors such as camera, gyroscope, accelerometer, compass, NFC, WiFi, GPS, etc. These sensors can be used to capture images and voice, detect motion patterns, and predict locations, to name just a few. This keynote depicts techniques in configuration, calibration, computation, and fusion for improving sensor performance and conserving power consumption. We also present novel mobile information management and retrieval applications that can benefit a great deal from enhanced sensor technologies.

#index 1598566
#* Predicting eBay listing conversion
#@ Ted Tao Yuan;Zhaohui Chen;Mike Mathieson
#t 2011
#c 13
#% 1036322
#% 1339907
#! At eBay Market Place, listing conversion rate can be measured by number of items sold divided by number of items in a sample set. For a given item, conversion rate can also be treated as the probability of sale. By investigating eBay listings' transactional patterns, as well as item attributes and user click-through data, we developed conversion models that allow us to predict a live listing's probability of sale. In this paper, we discuss the design and implementation of such conversion models. These models are highly valuable in analysis of inventory quality and ranking. Our work reveals the uniqueness of sales-oriented search at eBay and its similarity to general web search problems.

#index 1598567
#* A large scale machine learning system for recommending heterogeneous content in social networks
#@ Yanxin Shi;David Ye;Andrey Goder;Srinivas Narayanan
#t 2011
#c 13
#! The goal of the Facebook recommendation engine is to compare and rank heterogeneous types of content in order to find the most relevant recommendations based on user preference and page context. The challenges for such a recommendation engine include several aspects: 1) the online queries being processed are at very large scale; 2) with new content types and new user-generated content constantly added to the system, the candidate object set and underlying data distribution change rapidly; 3) different types of content usually have very distinct characteristics, which makes generic feature engineering difficult; and 4) unlike a search engine that can capture intention of users based on their search queries, our recommendation engine needs to focus more on users' profile and interests, past behaviors and current actions in order to infer their cognitive states. In this presentation, we would like to introduce an effective, scalable, online machine learning framework we developed in order to address the aforementioned challenges. We also want to discuss the insights, approaches and experiences we have accumulated during our research and development process.

#index 1598568
#* Review of MSR-Bing web scale speller challenge
#@ Kuansan Wang;Jan Pedersen
#t 2011
#c 13
#% 1468142
#% 1484281
#% 1560390
#! In this paper, we provide an overview of the MSR-Bing Web Scale Speller Challenge of 2011. We describe the motivation and outline the algorithmic and engineering challenges posed by this activity. The design and the evaluation methods are also reviewed, and the online resources that will remain publicly available to the community are also described. The Challenge will culminate in a workshop after the time of the writing where the top prize winners will publish their approaches. The main findings and the lessons learned will be summarized and shared in the Industry Track presentation accompanying this paper.

#index 1598569
#* Elsevier SIGIR 2011 application challenge abstract
#@ Jukka Valimaki;Remko Caprio
#t 2011
#c 13
#! Elsevier SIGIR 2011 Application Challenge is an international competition that encourages software developers to create applications that run on Elsevier's SciVerse platform. The Challenge is open to all SIGIR 2011 Conference participants.

#index 1878990
#* Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval
#@ William Hersh;Jamie Callan;Yoelle Maarek;Mark Sanderson
#t 2012
#c 13
#! We are delighted to welcome you to the 35th edition of SIGIR, the ACM International Conference on Research and Development in Information Retrieval. The conference continues its tradition of being the premier forum for research and development information retrieval, the computer science discipline behind what many call "search". The high number of submitted papers, this year again, demonstrates both the breadth and depth of the research being done in this vibrant field, both in academia and industry. We have done our best to ensure that these papers meet high standards of quality in terms of technical contribution, innovation, presentation, reference to previous work, and methodology. At the same time, we have tried to be flexible in the application of these criteria in order to consider papers describing novel and innovative work that may be somewhat unconventional. The conference received 483 full paper submissions this year. Examining the country code of the paper's contact author, we found that 185 (38%) come from the Americas; 158 (33%), Asia and Pacific region; and 140 (29%) from Europe, the Middle East and Africa. Of these, 98 (20%) were accepted, essentially the same as last year's acceptance rate and up from the 16.7% rate of the year before. There was almost no difference in the acceptance rates across the three broad regions. The top five countries in terms of accepted papers were the U.S.A. (36), China (14), the U.K. & Spain (both 7), and the Netherlands (6). In addition, 208 short papers were submitted to the poster track, of which 76 (36.5%) were accepted. In the other categories, there were 17 (47.2%) demonstrations, 4 workshops, and 16 tutorials accepted. The top five technical areas (as inferred from the primary keyword assigned by the authors) covered by the accepted papers, were queries and query analysis (18%), retrieval models and ranking (14%), web IR & social media search (13%), document representation and content analysis (11%), and users and interactive IR (9%). This was a small re-ordering of the topics from last year. SIGIR this year again used a two-tier double-blind reviewing approach. In a first stage, at least three reviewers read every paper and provided ratings and comments. Then, in a second stage, the primary and secondary Area Chairs ensured the quality of the reviewing process by studying, validating, and summarizing these reviews, and adding their own feedback and ratings. When required, Area Chairs initiated a discussion among the reviewers to resolve any controversial issues or significant differences of opinion. Once the discussion stage was completed, the two Area Chairs made the final decisions for nearly all submitted papers. At the program committee meeting held in Haifa, Israel, the Program Chairs and the attending Area Chairs went over the reviews, verified the process, gathered additional input, and made decisions in the few cases for which assistance had been requested.

#index 1878991
#* Salton award lecture: information retrieval as engineering science
#@ Norbert Fuhr
#t 2012
#c 13
#% 835027
#% 1292526
#% 1410143

#index 1878992
#* Retrieving information from the book of humanity: the personalized medicine data tsunami crashes on the beach of jeopardy
#@ Daniel R. Masys
#t 2012
#c 13
#! From a mute but eloquent alphabet of 4 characters emerges a complex biological 'literature' whose highest expression is human existence. The rapidly advancing technologies of 'nextgen sequencing' will soon make it possible to inexpensively acquire and store the characters of our complete personal genetic instruction set and make it available for health assessment and disease management. This uniquely personal form of 'big data' brings with it challenges that will be discussed in this keynote presentation. Topics will include a brief introduction to the linguistic challenges of 'biology as literature', the impact of personal molecular variation on traditional approaches to disease prevention, diagnosis and treatment, and the challenges of information retrieval when a large volume of primary observations is made that is associated with an evanescent and rapidly changing corpus of scientific interpretation of those primary observations. Experience with extracting high quality pheonotypes from electronic medical records has shown that Natural Language Processing capability is an essential information extraction function for correlation of clinical events with personal genetic variation. Any powerful set of information can be used or misused, and put those who depend upon it in jeopardy. These issues, and a lesson from the long running Jeopardy TV series, will be discussed.

#index 1878993
#* Adaptation of the concept hierarchy model with search logs for query recommendation on intranets
#@ Ibrahim Adepoju Adeyanju;Dawei Song;M-Dyaa Albakour;Udo Kruschwitz;Anne De Roeck;Maria Fasli
#t 2012
#c 13
#% 126892
#% 218978
#% 280849
#% 397159
#% 397223
#% 642985
#% 643001
#% 728105
#% 754125
#% 857130
#% 867126
#% 869501
#% 987212
#% 987222
#% 987372
#% 989578
#% 1004294
#% 1130811
#% 1130868
#% 1287229
#% 1355020
#% 1450893
#% 1450977
#% 1587393
#% 1714304
#% 1806044
#! A concept hierarchy created from a document collection can be used for query recommendation on Intranets by ranking terms according to the strength of their links to the query within the hierarchy. A major limitation is that this model produces the same recommendations for identical queries and rebuilding it from scratch periodically can be extremely inefficient due to the high computational costs. We propose to adapt the model by incorporating query refinements from search logs. Our intuition is that the concept hierarchy built from the collection and the search logs provide complementary conceptual views on the same search domain, and their integration should continually improve the effectiveness of recommended terms. Two adaptation approaches using query logs with and without click information are compared. We evaluate the concept hierarchy models (static and adapted versions) built from the Intranet collections of two academic institutions and compare them with a state-of-the-art log-based query recommender, the Query Flow Graph, built from the same logs. Our adaptive model significantly outperforms its static version and the query flow graph when tested over a period of time on data (documents and search logs) from two institutions' Intranets.

#index 1878994
#* Adaptive query suggestion for difficult queries
#@ Yang Liu;Ruihua Song;Yu Chen;Jian-Yun Nie;Ji-Rong Wen
#t 2012
#c 13
#% 115469
#% 298183
#% 310567
#% 330617
#% 340901
#% 397161
#% 411762
#% 577224
#% 643069
#% 754125
#% 818267
#% 869501
#% 879585
#% 879613
#% 907544
#% 987260
#% 987272
#% 989578
#% 1074098
#% 1083721
#% 1130854
#% 1130855
#% 1130879
#% 1355020
#% 1400023
#% 1450900
#% 1450964
#% 1451031
#% 1484281
#% 1598413
#% 1598415
#% 1712595
#! Query suggestion is a useful tool to help users formulate better queries. Although this has been found highly useful globally, its effect on different queries may vary. In this paper, we examine the impact of query suggestion on queries of different degrees of difficulty. It turns out that query suggestion is much more useful for difficult queries than easy queries. In addition, the suggestions for difficult queries should rely less on their similarity to the original query. In this paper, we use a learning-to-rank approach to select query suggestions, based on several types of features including a query performance prediction. As query suggestion has different impacts on different queries, we propose an adaptive suggestion approach that makes suggestions only for difficult queries. We carry out experiments on real data from a search engine. Our results clearly indicate that an approach targeting difficult queries can bring higher gain than a uniform suggestion approach.

#index 1878995
#* Learning to suggest: a machine learning framework for ranking query suggestions
#@ Umut Ozertem;Olivier Chapelle;Pinar Donmez;Emre Velipasaoglu
#t 2012
#c 13
#% 194299
#% 198058
#% 232713
#% 310567
#% 591792
#% 740900
#% 869501
#% 1130854
#% 1130868
#% 1173699
#% 1227619
#% 1292502
#% 1400017
#% 1560357
#% 1560359
#% 1598414
#% 1641925
#% 1712595
#% 1733306
#! We consider the task of suggesting related queries to users after they issue their initial query to a web search engine. We propose a machine learning approach to learn the probability that a user may find a follow-up query both useful and relevant, given his initial query. Our approach is based on a machine learning model which enables us to generalize to queries that have never occurred in the logs as well. The model is trained on co-occurrences mined from the search logs, with novel utility and relevance models, and the machine learning step is done without any labeled data by human judges. The learning step allows us to generalize from the past observations and generate query suggestions that are beyond the past co-occurred queries. This brings significant gains in coverage while yielding modest gains in relevance. Both offline (based on human judges) and online (based on millions of user interactions) evaluations demonstrate that our approach significantly outperforms strong baselines.

#index 1878996
#* Privacy-aware image classification and search
#@ Sergej Zerr;Stefan Siersdorfer;Jonathon Hare;Elena Demidova
#t 2012
#c 13
#% 262112
#% 269217
#% 411762
#% 451536
#% 593590
#% 736300
#% 751818
#% 760805
#% 840722
#% 954943
#% 1074133
#% 1077150
#% 1103929
#% 1166473
#% 1190089
#% 1190093
#% 1190107
#% 1190132
#% 1292558
#% 1483482
#% 1484405
#! Modern content sharing environments such as Flickr or YouTube contain a large amount of private resources such as photos showing weddings, family holidays, and private parties. These resources can be of a highly sensitive nature, disclosing many details of the users' private sphere. In order to support users in making privacy decisions in the context of image sharing and to provide them with a better overview on privacy related visual content available on the Web, we propose techniques to automatically detect private images, and to enable privacy-oriented image search. To this end, we learn privacy classifiers trained on a large set of manually assessed Flickr photos, combining textual metadata of images with a variety of visual features. We employ the resulting classification models for specifically searching for private photos, and for diversifying query results to provide users with a better coverage of private and public content. Large-scale classification experiments reveal insights into the predictive performance of different visual and textual features, and a user evaluation of query result rankings demonstrates the viability of our approach.

#index 1878997
#* Manhattan hashing for large-scale image retrieval
#@ Weihao Kong;Wu-Jun Li;Minyi Guo
#t 2012
#c 13
#% 46803
#% 424085
#% 443984
#% 450888
#% 479973
#% 762054
#% 919460
#% 987258
#% 1023422
#% 1119142
#% 1126600
#% 1148464
#% 1215859
#% 1286845
#% 1354888
#% 1450831
#% 1451253
#% 1520218
#% 1598356
#% 1598428
#% 1648823
#% 1649056
#% 1750268
#% 1750274
#% 1872343
#% 1885617
#! Hashing is used to learn binary-code representation for data with expectation of preserving the neighborhood structure in the original feature space. Due to its fast query speed and reduced storage cost, hashing has been widely used for efficient nearest neighbor search in a large variety of applications like text and image retrieval. Most existing hashing methods adopt Hamming distance to measure the similarity (neighborhood) between points in the hashcode space. However, one problem with Hamming distance is that it may destroy the neighborhood structure in the original feature space, which violates the essential goal of hashing. In this paper, Manhattan hashing (MH), which is based on Manhattan distance, is proposed to solve the problem of Hamming distance based hashing. The basic idea of MH is to encode each projected dimension with multiple bits of natural binary code (NBC), based on which the Manhattan distance between points in the hashcode space is calculated for nearest neighbor search. MH can effectively preserve the neighborhood structure in the data to achieve the goal of hashing. To the best of our knowledge, this is the first work to adopt Manhattan distance with NBC for hashing. Experiments on several large-scale image data sets containing up to one million points show that our MH method can significantly outperform other state-of-the-art methods.

#index 1878998
#* Boosting multi-kernel locality-sensitive hashing for scalable image retrieval
#@ Hao Xia;Pengcheng Wu;Steven C.H. Hoi;Rong Jin
#t 2012
#c 13
#% 56637
#% 235377
#% 249321
#% 318785
#% 321455
#% 342828
#% 347225
#% 411694
#% 479973
#% 481956
#% 762054
#% 796201
#% 860956
#% 863388
#% 916799
#% 1022281
#% 1068288
#% 1074066
#% 1148464
#% 1217189
#% 1264692
#% 1292133
#% 1327692
#% 1450831
#% 1451253
#% 1484414
#% 1649025
#% 1750274
#% 1750338
#% 1775143
#% 1775748
#% 1857498
#! Similarity search is a key challenge for multimedia retrieval applications where data are usually represented in high-dimensional space. Among various algorithms proposed for similarity search in high-dimensional space, Locality-Sensitive Hashing (LSH) is the most popular one, which recently has been extended to Kernelized Locality-Sensitive Hashing (KLSH) by exploiting kernel similarity for better retrieval efficacy. Typically, KLSH works only with a single kernel, which is often limited in real-world multimedia applications, where data may originate from multiple resources or can be represented in several different forms. For example, in content-based multimedia retrieval, a variety of features can be extracted to represent contents of an image. To overcome the limitation of regular KLSH, we propose a novel Boosting Multi-Kernel Locality-Sensitive Hashing (BMKLSH) scheme that significantly boosts the retrieval performance of KLSH by making use of multiple kernels. We conduct extensive experiments for large-scale content-based image retrieval, in which encouraging results show that the proposed method outperforms the state-of-the-art techniques.

#index 1878999
#* Diversity by proportionality: an election-based approach to search result diversification
#@ Van Dang;W. Bruce Croft
#t 2012
#c 13
#% 262112
#% 642975
#% 869501
#% 879686
#% 987231
#% 1035578
#% 1074133
#% 1130854
#% 1130855
#% 1166473
#% 1181094
#% 1227591
#% 1263586
#% 1292528
#% 1292596
#% 1355020
#% 1400011
#% 1400021
#% 1482296
#% 1536510
#% 1598393
#% 1642193
#% 1712595
#! This paper presents a different perspective on diversity in search results: diversity by proportionality. We consider a result list most diverse, with respect to some set of topics related to the query, when the number of documents it provides on each topic is proportional to the topic's popularity. Consequently, we propose a framework for optimizing proportionality for search result diversification, which is motivated by the problem of assigning seats to members of competing political parties. Our technique iteratively determines, for each position in the result ranked list, the topic that best maintains the overall proportionality. It then selects the best document on this topic for this position. We demonstrate empirically that our method significantly outperforms the top performing approach in the literature not only on our proposed metric for proportionality, but also on several standard diversity measures. This result indicates that promoting proportionality naturally leads to minimal redundancy, which is a goal of the current diversity approaches.

#index 1879000
#* Explicit relevance models in intent-oriented information retrieval diversification
#@ Saúl Vargas;Pablo Castells;David Vallet
#t 2012
#c 13
#% 232703
#% 262112
#% 642975
#% 723305
#% 734590
#% 734592
#% 879618
#% 879662
#% 1019149
#% 1074133
#% 1127472
#% 1166473
#% 1227591
#% 1400021
#% 1476448
#% 1536510
#% 1558079
#% 1558080
#% 1560356
#% 1560378
#% 1598506
#% 1625357
#% 1910512
#! The intent-oriented search diversification methods developed in the field so far tend to build on generative views of the retrieval system to be diversified. Core algorithm components in particular redundancy assessment are expressed in terms of the probability to observe documents, rather than the probability that the documents be relevant. This has been sometimes described as a view considering the selection of a single document in the underlying task model. In this paper we propose an alternative formulation of aspect-based diversification algorithms which explicitly includes a formal relevance model. We develop means for the effective computation of the new formulation, and we test the resulting algorithm empirically. We report experiments on search and recommendation tasks showing competitive or better performance than the original diversification algorithms. The relevance-based formulation has further interesting properties, such as unifying two well-known state of the art algorithms into a single version. The relevance-based approach opens alternative possibilities for further formal connections and developments as natural extensions of the framework. We illustrate this by modeling tolerance to redundancy as an explicit configurable parameter, which can be set to better suit the characteristics of the IR task, or the evaluation metrics, as we illustrate empirically.

#index 1879001
#* AspecTiles: tile-based visualization of diversified web search results
#@ Mayu Iwata;Tetsuya Sakai;Takehiro Yamamoto;Yu Chen;Yi Liu;Ji-Rong Wen;Shojiro Nishio
#t 2012
#c 13
#% 18610
#% 201992
#% 295137
#% 322995
#% 590523
#% 825416
#% 961649
#% 987617
#% 1166473
#% 1263586
#% 1339867
#% 1400021
#% 1531305
#% 1536529
#% 1536552
#% 1573488
#% 1746848
#! A diversified search result for an underspecified query generally contains web pages in which there are answers that are relevant to different aspects of the query. In order to help the user locate such relevant answers, we propose a simple extension to the standard Search Engine Result Page (SERP) interface, called AspecTiles. In addition to presenting a ranked list of URLs with their titles and snippets, AspecTiles visualizes the relevance degree of a document to each aspect by means of colored squares ("tiles"). To compare AspecTiles with the standard SERP interface in terms of usefulness, we conducted a user study involving 30 search tasks designed based on the TREC web diversity task topics as well as 32 participants. Our results show that AspecTiles has some advantages in terms of search performance, user behavior, and user satisfaction. First, AspecTiles enables the user to gather relevant information significantly more efficiently than the standard SERP interface for tasks where the user considers several different aspects of the query to be important at the same time (multi-aspect tasks). Second, AspecTiles affects the user's information seeking behavior: with this interface, we observed significantly fewer query reformulations, shorter queries and deeper examinations of ranked lists in multi-aspect tasks. Third, participants of our user study found AspecTiles significantly more useful for finding relevant information and easy to use than the standard SERP interface. These results suggest that simple interfaces like AspecTiles can enhance the search performance and search experience of the user when their queries are underspecified.

#index 1879002
#* Time-based calibration of effectiveness measures
#@ Mark D. Smucker;Charles L.A. Clarke
#t 2012
#c 13
#% 118728
#% 133894
#% 169781
#% 217268
#% 232685
#% 262036
#% 340921
#% 411762
#% 616528
#% 822126
#% 879630
#% 907493
#% 987251
#% 1051038
#% 1053505
#% 1074053
#% 1074139
#% 1095876
#% 1227640
#% 1262695
#% 1263584
#% 1292528
#% 1331574
#% 1366523
#% 1450895
#% 1450903
#% 1450904
#% 1482378
#% 1483555
#% 1526568
#% 1536510
#% 1598335
#% 1598424
#% 1631302
#% 1641985
#! Many current effectiveness measures incorporate simplifying assumptions about user behavior. These assumptions prevent the measures from reflecting aspects of the search process that directly impact the quality of retrieval results as experienced by the user. In particular, these measures implicitly model users as working down a list of retrieval results, spending equal time assessing each document. In reality, even a careful user, intending to identify as much relevant material as possible, must spend longer on some documents than on others. Aspects such as document length, duplicates and summaries all influence the time required. In this paper, we introduce a time-biased gain measure, which explicitly accommodates such aspects of the search process. By conducting an appropriate user study, we calibrate and validate the measure against the TREC 2005 Robust Track test collection. We examine properties of the measure, contrasting it to traditional effectiveness measures, and exploring its extension to other aspects and environments. As its primary benefit, the measure allows us to evaluate system performance in human terms, while maintaining the simplicity and repeatability of system-oriented tests. Overall, we aim to achieve a clearer connection between user-oriented studies and system-oriented tests, allowing us to better transfer insights and outcomes from one to the other.

#index 1879003
#* Time drives interaction: simulating sessions in diverse searching environments
#@ Feza Baskaya;Heikki Keskustalo;Kalervo Järvelin
#t 2012
#c 13
#% 133894
#% 232685
#% 272919
#% 306468
#% 309089
#% 309095
#% 340921
#% 397164
#% 407891
#% 411762
#% 835027
#% 879566
#% 1004298
#% 1019103
#% 1074069
#% 1135992
#% 1331574
#% 1405707
#% 1598335
#% 1604207
#% 1642493
#! Real life information retrieval takes place in sessions, where users search by iterating between various cognitive, perceptual and motor subtasks through an interactive interface. The sessions may follow diverse strategies, which, together with the interface characteristics, affect user effort (cost), experience and session effectiveness. In this paper we propose a pragmatic evaluation approach based on scenarios with explicit subtask costs. We study the limits of effectiveness of diverse interactive searching strategies in two searching environments (the scenarios) under overall cost constraints. This is based on a comprehensive simulation of 20 million sessions in each scenario. We analyze the effectiveness of the session strategies over time, and the properties of the most and the least effective sessions in each case. Furthermore, we will also contrast the proposed evaluation approach with the traditional one, rank based evaluation, and show how the latter may hide essential factors that affect users' performance and satisfaction - and gives even counter-intuitive results.

#index 1879004
#* Evaluating aggregated search pages
#@ Ke Zhou;Ronan Cummins;Mounia Lalmas;Joemon M. Jose
#t 2012
#c 13
#% 194246
#% 643012
#% 1074093
#% 1074133
#% 1075214
#% 1095876
#% 1166473
#% 1227616
#% 1292528
#% 1450898
#% 1450915
#% 1450955
#% 1536576
#% 1587348
#% 1598438
#% 1622356
#% 1622365
#% 1641937
#% 1642923
#% 1693908
#% 1806040
#! Aggregating search results from a variety of heterogeneous sources or verticals such as news, image and video into a single interface is a popular paradigm in web search. Although various approaches exist for selecting relevant verticals or optimising the aggregated search result page, evaluating the quality of an aggregated page is an open question. This paper proposes a general framework for evaluating the quality of aggregated search pages. We evaluate our approach by collecting annotated user preferences over a set of aggregated search pages for 56 topics and 12 verticals. We empirically demonstrate the fidelity of metrics instantiated from our proposed framework by showing that they strongly agree with the annotated user preferences of pairs of simulated aggregated pages. Furthermore, we show that our metrics agree with the majority preference more often than current diversity-based information retrieval metrics. Finally, we demonstrate the flexibility of our framework by showing that personalised historical preference data can be used to improve the performance of our proposed metrics.

#index 1879005
#* Combining inverted indices and structured search for ad-hoc object retrieval
#@ Alberto Tonon;Gianluca Demartini;Philippe Cudré-Mauroux
#t 2012
#c 13
#% 262102
#% 561315
#% 766409
#% 907496
#% 1133171
#% 1343447
#% 1366460
#% 1378482
#% 1400010
#% 1482185
#% 1482286
#% 1482367
#% 1483553
#% 1489451
#% 1598426
#% 1620194
#% 1641483
#% 1641941
#% 1653176
#% 1719970
#! Retrieving semi-structured entities to answer keyword queries is an increasingly important feature of many modern Web applications. The fast-growing Linked Open Data (LOD) movement makes it possible to crawl and index very large amounts of structured data describing hundreds of millions of entities. However, entity retrieval approaches have yet to find efficient and effective ways of ranking and navigating through those large data sets. In this paper, we address the problem of Ad-hoc Object Retrieval over large-scale LOD data by proposing a hybrid approach that combines IR and structured search techniques. Specifically, we propose an architecture that exploits an inverted index to answer keyword queries as well as a semi-structured database to improve the search effectiveness by automatically generating queries over the LOD graph. Experimental results show that our ranking algorithms exploiting both IR and graph indices outperform state-of-the-art entity retrieval techniques by up to 25% over the BM25 baseline.

#index 1879006
#* Retrieving similar discussion forum threads: a structure based approach
#@ Amit Singh;Deepak P;Dinesh Raghu
#t 2012
#c 13
#% 36672
#% 262112
#% 268079
#% 281209
#% 290830
#% 387427
#% 387791
#% 577273
#% 686225
#% 722904
#% 823364
#% 879602
#% 1044465
#% 1096119
#% 1136543
#% 1227699
#% 1247796
#% 1292733
#% 1410909
#% 1470696
#% 1521634
#% 1587368
#% 1835466
#! Online forums are becoming a popular way of finding useful information on the web. Search over forums for existing discussion threads so far is limited to keyword-based search due to the minimal effort required on part of the users. However, it is often not possible to capture all the relevant context in a complex query using a small number of keywords. Example-based search that retrieves similar discussion threads given one exemplary thread is an alternate approach that can help the user provide richer context and vastly improve forum search results. In this paper, we address the problem of finding similar threads to a given thread. Towards this, we propose a novel methodology to estimate similarity between discussion threads. Our method exploits the thread structure to decompose threads in to set of weighted overlapping components. It then estimates pairwise thread similarities by quantifying how well the information in the threads are mutually contained within each other using lexical similarities between their underlying components. We compare our proposed methods on real datasets against state-of-the-art thread retrieval mechanisms wherein we illustrate that our techniques outperform others by large margins on popular retrieval evaluation measures such as NDCG, MAP, Precision@k and MRR. In particular, consistent improvements of up to 10% are observed on all evaluation measures.

#index 1879007
#* Summarizing highly structured documents for effective search interaction
#@ Lanbo Zhang;Yi Zhang;Yunfei Chen
#t 2012
#c 13
#% 169770
#% 194251
#% 230530
#% 262036
#% 262112
#% 280835
#% 288614
#% 717120
#% 907550
#% 987209
#% 1019116
#% 1063493
#% 1190062
#% 1227640
#% 1450874
#% 1455265
#% 1478826
#% 1483552
#! As highly structured documents with rich metadata (such as products, movies, etc.) become increasingly prevalent, searching those documents has become an important IR problem. Unfortunately existing work on document summarization, especially in the context of search, has been mainly focused on unstructured documents, and little attention has been paid to highly structured documents. Due to the different characteristics of structured and unstructured documents, the ideal approaches for document summarization might be different. In this paper, we study the problem of summarizing highly structured documents in a search context. We propose a new summarization approach based on query-specific facet selection. Our approach aims to discover the important facets hidden behind a query using a machine learning approach, and summarizes retrieved documents based on those important facets. In addition, we propose to evaluate summarization approaches based on a utility function that measures how well the summaries assist users in interacting with the search results. Furthermore, we develop a game on Mechanical Turk to evaluate different summarization approaches. The experimental results show that the new summarization approach significantly outperforms two existing ones.

#index 1879008
#* TFMAP: optimizing MAP for top-n context-aware recommendation
#@ Yue Shi;Alexandros Karatzoglou;Linas Baltrunas;Martha Larson;Alan Hanjalic;Nuria Oliver
#t 2012
#c 13
#% 734592
#% 801785
#% 829043
#% 875974
#% 987226
#% 987241
#% 1035577
#% 1074061
#% 1074064
#% 1077150
#% 1083671
#% 1083696
#% 1176909
#% 1214623
#% 1214688
#% 1227601
#% 1260273
#% 1262980
#% 1268491
#% 1287255
#% 1300087
#% 1417104
#% 1442575
#% 1450857
#% 1476448
#% 1476453
#% 1535460
#% 1598363
#% 1598397
#% 1605981
#% 1619982
#% 1625387
#! In this paper, we tackle the problem of top-N context-aware recommendation for implicit feedback scenarios. We frame this challenge as a ranking problem in collaborative filtering (CF). Much of the past work on CF has not focused on evaluation metrics that lead to good top-N recommendation lists in designing recommendation models. In addition, previous work on context-aware recommendation has mainly focused on explicit feedback data, i.e., ratings. We propose TFMAP, a model that directly maximizes Mean Average Precision with the aim of creating an optimally ranked list of items for individual users under a given context. TFMAP uses tensor factorization to model implicit feedback data (e.g., purchases, clicks) with contextual information. The optimization of MAP in a large data collection is computationally too complex to be tractable in practice. To address this computational bottleneck, we present a fast learning algorithm that exploits several intrinsic properties of average precision to improve the learning efficiency of TFMAP, and to ensure its scalability. We experimentally verify the effectiveness of the proposed fast learning algorithm, and demonstrate that TFMAP significantly outperforms state-of-the-art recommendation approaches.

#index 1879009
#* Increasing temporal diversity with purchase intervals
#@ Gang Zhao;Mong Li Lee;Wynne Hsu;Wei Chen
#t 2012
#c 13
#% 330687
#% 420515
#% 564483
#% 578684
#% 734590
#% 734592
#% 766448
#% 913192
#% 1083671
#% 1393622
#% 1400014
#% 1450855
#% 1451212
#% 1560387
#% 1598434
#! The development of Web 2.0 technology has led to huge economic benefits and challenges for both e-commerce websites and online shoppers. One core technology to increase sales and consumers' satisfaction is the use of recommender systems. Existing product recommender systems consider the order of items purchased by users to obtain a list of recommended items. However, they do not consider the time interval between the products purchased. For example, there is often an interval of 2-3 months between the purchase of printer ink cartridges or refills. Thus, recommending appropriate ink cartridges one week before the user needs to replace the depleted ink cartridges would increase the likelihood of a purchase decision. In this paper, we propose to utilize the purchase interval information to improve the performance of the recommender systems for e-commerce. We design an efficient algorithm to compute the purchase intervals between product pairs from users' purchase history and integrate this information into the marginal utility model. We evaluate our approach on a real world ecommerce dataset. Experimental results demonstrate that our approach significantly improves the conversion rate and temporal diversity compared to state-of-the-art algorithms.

#index 1879010
#* Adaptive diversification of recommendation results via latent factor portfolio
#@ Yue Shi;Xiaoxue Zhao;Jun Wang;Martha Larson;Alan Hanjalic
#t 2012
#c 13
#% 202011
#% 262112
#% 280852
#% 330687
#% 411762
#% 642975
#% 722904
#% 805841
#% 813966
#% 879618
#% 879686
#% 1074062
#% 1074133
#% 1083671
#% 1195829
#% 1214623
#% 1227591
#% 1260273
#% 1357833
#% 1450855
#% 1450857
#% 1450988
#% 1476448
#% 1482296
#% 1543813
#% 1598335
#% 1598363
#% 1598393
#% 1598434
#% 1625357
#% 1746799
#% 1747068
#! This paper studies result diversification in collaborative filtering. We argue that the diversification level in a recommendation list should be adapted to the target users' individual situations and needs. Different users may have different ranges of interests -- the preference of a highly focused user might include only few topics, whereas that of the user with broad interests may encompass a wide range of topics. Thus, the recommended items should be diversified according to the interest range of the target user. Such an adaptation is also required due to the fact that the uncertainty of the estimated user preference model may vary significantly between users. To reduce the risk of the recommendation, we should take the difference of the uncertainty into account as well. In this paper, we study the adaptive diversification problem theoretically. We start with commonly used latent factor models and reformulate them using the mean-variance analysis from the portfolio theory in text retrieval. The resulting Latent Factor Portfolio (LFP) model captures the user's interest range and the uncertainty of the user preference by employing the variance of the learned user latent factors. It is shown that the correlations between items (and thus the item diversity) can be obtained by using the correlations between latent factors (topical diversity), which in return significantly reduce the computation load. Our mathematical derivation also reveals that diversification is necessary, not only for risk-averse system behavior (non-adpative), but also for the target users' individual situations (adaptive), which are represented by the distribution and the variance of the latent user factors. Our experiments confirm the theoretical insights and show that LFP succeeds in improving latent factor models by adaptively introducing recommendation diversity to fit the individual user's needs.

#index 1879011
#* Modeling the impact of short- and long-term behavior on search personalization
#@ Paul N. Bennett;Ryen W. White;Wei Chu;Susan T. Dumais;Peter Bailey;Fedor Borisyuk;Xiaoyuan Cui
#t 2012
#c 13
#% 754126
#% 771571
#% 805200
#% 807420
#% 818207
#% 818224
#% 818259
#% 832349
#% 881540
#% 919706
#% 956495
#% 956552
#% 1019076
#% 1074071
#% 1083721
#% 1130852
#% 1130878
#% 1192483
#% 1227577
#% 1227621
#% 1227622
#% 1267762
#% 1397425
#% 1399944
#% 1450873
#% 1450885
#% 1482279
#% 1536505
#% 1536511
#% 1598334
#% 1598347
#% 1641961
#% 1693905
#! User behavior provides many cues to improve the relevance of search results through personalization. One aspect of user behavior that provides especially strong signals for delivering better relevance is an individual's history of queries and clicked documents. Previous studies have explored how short-term behavior or long-term behavior can be predictive of relevance. Ours is the first study to assess how short-term (session) behavior and long-term (historic) behavior interact, and how each may be used in isolation or in combination to optimally contribute to gains in relevance through search personalization. Our key findings include: historic behavior provides substantial benefits at the start of a search session; short-term session behavior contributes the majority of gains in an extended search session; and the combination of session and historic behavior out-performs using either alone. We also characterize how the relative contribution of each model changes throughout the duration of a session. Our findings have implications for the design of search systems that leverage user behavior to personalize the search experience.

#index 1879012
#* Improving searcher models using mouse cursor activity
#@ Jeff Huang;Ryen W. White;Georg Buscher;Kuansan Wang
#t 2012
#c 13
#% 292179
#% 320432
#% 446993
#% 734975
#% 815796
#% 818221
#% 874272
#% 954948
#% 956546
#% 1035578
#% 1048693
#% 1048694
#% 1074071
#% 1074092
#% 1074148
#% 1154062
#% 1166517
#% 1183067
#% 1186546
#% 1189164
#% 1190055
#% 1190056
#% 1227581
#% 1355037
#% 1384641
#% 1450845
#% 1450873
#% 1451161
#% 1482222
#% 1536546
#% 1539775
#% 1573487
#% 1598370
#% 1765932
#! Web search components such as ranking and query suggestions analyze the user data provided in query and click logs. While this data is easy to collect and provides information about user behavior, it omits user interactions with the search engine that do not hit the server; these logs omit search data such as users' cursor movements. Just as clicks provide signals for relevance in search results, cursor hovering and scrolling can be additional implicit signals. In this work, we demonstrate a technique to extend models of the user's search result examination state to infer document relevance. We start by exploring recorded user interactions with the search results, both qualitatively and quantitatively. We find that cursor hovering and scrolling are signals telling us which search results were examined, and we use these interactions to reveal latent variables in searcher models to more accurately compute document attractiveness and satisfaction. Accuracy is evaluated by computing how well our model using these parameters can predict future clicks for a particular query. We are able to improve the click predictions compared to a basic searcher model for higher ranked search results using the additional log data.

#index 1879013
#* Personalization of search results using interaction behaviors in search sessions
#@ Chang Liu;Nicholas J. Belkin;Michael J. Cole
#t 2012
#c 13
#% 731615
#% 766454
#% 805200
#% 818206
#% 854140
#% 879565
#% 907516
#% 1126944
#% 1292528
#% 1357833
#% 1450832
#% 1455267
#! Personalization of search results offers the potential for significant improvement in information retrieval performance. User interactions with the system and documents during information-seeking sessions provide a wealth of information about user preferences and their task goals. In this paper, we propose methods for analyzing and modeling user search behavior in search sessions to predict document usefulness and then using information to personalize search results. We generate prediction models of document usefulness from behavior data collected in a controlled lab experiment with 32 participants, each completing uncontrolled searching for 4 tasks in the Web. The generated models are then tested with another data set of user search sessions in radically different search tasks and constrains. The documents predicted useful and not useful by the models are used to modify the queries in each search session using a standard relevance feedback technique. The results show that application of the models led to consistently improved performance over a baseline that did not take account of user interaction information. These findings have implications for designing systems for personalized search and improving user search experience.

#index 1879014
#* User evaluation of query quality
#@ Wan-Ching Wu;Diane Kelly;Kun Huang
#t 2012
#c 13
#% 768905
#% 803556
#% 1047489
#% 1124559
#% 1130990
#% 1287229
#% 1384095
#% 1417245
#% 1450834
#% 1450943
#% 1467729
#% 1482276
#% 1622336
#% 1745130
#% 1765751
#! Although a great deal of research has been conducted about automatic techniques for determining query quality, there have been relatively few studies about how people judge query quality. This study investigated this topic through a laboratory experiment with 40 subjects. Subjects were shown eight information problems (five fact-finding and three exploratory) and asked to evaluate queries for these problems according to several quality attributes. Subjects then evaluated search engine results pages (SERPs) for each query, which were manipulated to exhibit different levels of performance. Following this, subjects reevaluated the queries, were interviewed about their evaluation approaches and repeated the rating procedure for two information problems. Results showed that for fact-finding information problems, longer queries received higher ratings (both initial and post-SERP), and that post-SERP query ratings were more affected by the proportion of relevant documents viewed to all documents viewed rather than the ranks of the relevant documents. For exploratory information problems, subjects' ratings were highly correlated with the number of relevant documents in the SERP as well as the proportion of relevant documents viewed. Subjects adopted several approaches when evaluating query quality, which led to different quality ratings. Finally, during the reliability check subjects' initial evaluations were fairly stable, but their post-SERP evaluations significantly increased.

#index 1879015
#* Efficient in-memory top-k document retrieval
#@ J. Shane Culpepper;Matthias Petri;Falk Scholer
#t 2012
#c 13
#% 143306
#% 169817
#% 194247
#% 198335
#% 212665
#% 213786
#% 228097
#% 249989
#% 339936
#% 340886
#% 379390
#% 379448
#% 453323
#% 453572
#% 593970
#% 695217
#% 730065
#% 796700
#% 818229
#% 823464
#% 867054
#% 879611
#% 936965
#% 943013
#% 956437
#% 1099208
#% 1128430
#% 1210023
#% 1227595
#% 1267015
#% 1379539
#% 1404889
#% 1480887
#% 1490339
#% 1654514
#% 1723778
#% 1739409
#% 1894389
#! For over forty years the dominant data structure for ranked document retrieval has been the inverted index. Inverted indexes are effective for a variety of document retrieval tasks, and particularly efficient for large data collection scenarios that require disk access and storage. However, many efficiency-bound search tasks can now easily be supported entirely in memory as a result of recent hardware advances. In this paper we present a hybrid algorithmic framework for in-memory bag of-words ranked document retrieval using a self-index derived from the FM-Index, wavelet tree, and the compressed suffix tree data structures, and evaluate the various algorithmic trade-offs for performing efficient queries entirely in-memory. We compare our approach with two classic approaches to bag-of-words queries using inverted indexes, term-at-a-time (TAAT) and document-at-a-time (DAAT) query processing. We show that our framework is competitive with state-of-the-art indexing structures, and describe new capabilities provided by our algorithms that can be leveraged by future systems to improve effectiveness and efficiency for a variety of fundamental search operations.

#index 1879016
#* Index maintenance for time-travel text search
#@ Avishek Anand;Srikanta Bedathur;Klaus Berberich;Ralf Schenkel
#t 2012
#c 13
#% 118741
#% 208047
#% 213786
#% 255137
#% 287070
#% 570884
#% 571296
#% 730065
#% 867054
#% 874704
#% 879600
#% 956535
#% 987214
#% 987257
#% 1051039
#% 1070892
#% 1077150
#% 1292507
#% 1292509
#% 1392437
#% 1480887
#% 1482248
#% 1482302
#% 1526990
#% 1598388
#% 1598433
#! Time-travel text search enriches standard text search by temporal predicates, so that users of web archives can easily retrieve document versions that are considered relevant to a given keyword query and existed during a given time interval. Different index structures have been proposed to efficiently support time-travel text search. None of them, however, can easily be updated as the Web evolves and new document versions are added to the web archive. In this work, we describe a novel index structure that efficiently supports time-travel text search and can be maintained incrementally as new document versions are added to the web archive. Our solution uses a sharded index organization, bounds the number of spuriously read index entries per shard, and can be maintained using small in-memory buffers and append-only operations. We present experiments on two large-scale real-world datasets demonstrating that maintaining our novel index structure is an order of magnitude more efficient than periodically rebuilding one of the existing index structures, while query-processing performance is not adversely affected.

#index 1879017
#* Optimizing positional index structures for versioned document collections
#@ JInru He;Torsten Suel
#t 2012
#c 13
#% 118741
#% 251442
#% 310770
#% 342373
#% 420491
#% 577310
#% 654447
#% 754117
#% 768815
#% 805843
#% 818262
#% 864446
#% 867054
#% 956535
#% 960181
#% 987257
#% 1051061
#% 1126954
#% 1166469
#% 1190095
#% 1213450
#% 1292507
#% 1387547
#% 1392437
#% 1426548
#% 1482248
#% 1482302
#% 1598390
#% 1641967
#% 1688264
#! Versioned document collections are collections that contain multiple versions of each document. Important examples are Web archives, Wikipedia and other wikis, or source code and documents maintained in revision control systems. Versioned document collections can become very large, due to the need to retain past versions, but there is also a lot of redundancy between versions that can be exploited. Thus, versioned document collections are usually stored using special differential (delta) compression techniques, and a number of researchers have recently studied how to exploit this redundancy to obtain more succinct full-text index structures. In this paper, we study index organization and compression techniques for such versioned full-text index structures. In particular, we focus on the case of positional index structures, while most previous work has focused on the non-positional case. Building on earlier work in [zs:redun], we propose a framework for indexing and querying in versioned document collections that integrates non-positional and positional indexes to enable fast top-k query processing. Within this framework, we define and study the problem of minimizing positional index size through optimal substring partitioning. Experiments on Wikipedia and web archive data show that our techniques achieve significant reductions in index size over previous work while supporting very fast query processing.

#index 1879018
#* To index or not to index: time-space trade-offs in search engines with positional ranking functions
#@ Diego Arroyuelo;Senén González;Mauricio Marin;Mauricio Oyarzún;Torsten Suel
#t 2012
#c 13
#% 57849
#% 262099
#% 268079
#% 290830
#% 339936
#% 420491
#% 453572
#% 730065
#% 786632
#% 818262
#% 864446
#% 936965
#% 987208
#% 987229
#% 1077150
#% 1128430
#% 1153123
#% 1166469
#% 1190095
#% 1210023
#% 1227595
#% 1355055
#% 1387547
#% 1404894
#% 1418196
#% 1480887
#% 1529932
#% 1598344
#% 1598433
#% 1642164
#% 1667276
#% 1706757
#% 1715627
#% 1811305
#% 1812718
#% 1870667
#! Positional ranking functions, widely used in Web search engines, improve result quality by exploiting the positions of the query terms within documents. However, it is well known that positional indexes demand large amounts of extra space, typically about three times the space of a basic nonpositional index. Textual data, on the other hand, is needed to produce text snippets. In this paper, we study time-space trade-offs for search engines with positional ranking functions and text snippet generation. We consider both index-based and non-index based alternatives for positional data. We aim to answer the question of whether one should index positional data or not. We show that there is a wide range of practical time-space trade-offs. Moreover, we show that both position and textual data can be stored using about 71% of the space used by traditional positional indexes, with a minor increase in query time. This yields considerable space savings and outperforms, both in space and time, recent alternatives from the literature. We also propose several efficient compressed text representations for snippet generation, which are able to use about half of the space of current state-of-the-art alternatives with little impact in query processing time.

#index 1879019
#* Studies of the onset and persistence of medical concerns in search logs
#@ Ryen W. White;Eric Horvitz
#t 2012
#c 13
#% 310567
#% 345262
#% 571398
#% 751800
#% 818259
#% 881540
#% 956495
#% 1019076
#% 1227622
#% 1278069
#% 1399944
#% 1450956
#% 1536505
#% 1573489
#% 1598340
#! The Web provides a wealth of information about medical symptoms and disorders. Although this content is often valuable to consumers, studies have found that interaction with Web content may heighten anxiety and stimulate healthcare utilization. We present a longitudinal log-based study of medical search and browsing behavior on the Web. We characterize how users focus on particular medical concerns and how concerns persist and influence future behavior, including changes in focus of attention in searching and browsing for health information. We build and evaluate models that predict transitions from searches on symptoms to searches on health conditions, and escalations from symptoms to serious illnesses. We study the influence that the prior onset of concerns may have on future behavior, including sudden shifts back to searching on the concern amidst other searches. Our findings have implications for refining Web search and retrieval to support people pursuing diagnostic information.

#index 1879020
#* A semi-supervised approach to modeling web search satisfaction
#@ Ahmed Hassan
#t 2012
#c 13
#% 269217
#% 281174
#% 306468
#% 309095
#% 376266
#% 411762
#% 766447
#% 805200
#% 879567
#% 943049
#% 956495
#% 987263
#% 1130811
#% 1130878
#% 1166521
#% 1292528
#% 1354495
#% 1355038
#% 1384094
#% 1450833
#% 1450902
#% 1598368
#% 1641927
#% 1650581
#% 1811350
#% 1811869
#! Web search is an interactive process that involves actions from Web search users and responses from the search engine. Many research efforts have been made to address the problem of understanding search behavior in general. Some of this work focused on predicting whether a particular user has succeeded in achieving her search goal or not. Most of these studies have faced the problem of the lack of reliable labeled data to learn from. Unlike labeled data, unlabeled data recording behavioral signals in Web search is widely available in search logs. In this work, we study the plausibility of using labeled and unlabeled data to learn better models of user behavior that can be used to predict search success more effectively. We present a semi-supervised approach to modeling Web search satisfaction. The proposed approach can use either labeled data only or both labeled and unlabeled data. We show that the proposed model outperforms previous methods for modeling search success using labeled data. We also show that adding unlabeled data improves the effectiveness of the proposed models and that the proposed method outperforms other strong semi-supervised baselines.

#index 1879021
#* Social annotations: utility and prediction modeling
#@ Patrick Pantel;Michael Gamon;Omar Alonso;Kevin Haas
#t 2012
#c 13
#% 232684
#% 260244
#% 399057
#% 411762
#% 720198
#% 818259
#% 879567
#% 879581
#% 881544
#% 956544
#% 987209
#% 987263
#% 1035588
#% 1074071
#% 1214757
#% 1268491
#% 1292528
#% 1292590
#% 1474633
#% 1598406
#% 1765904
#! Social features are increasingly integrated within the search results page of the main commercial search engines. There is, however, little understanding of the utility of social features in traditional search. In this paper, we study utility in the context of social annotations, which are markings indicating that a person in the social network of the user has liked or shared a result document. We introduce a taxonomy of social relevance aspects that influence the utility of social annotations in search, spanning query classes, the social network, and content relevance. We present the results of a user study quantifying the utility of social annotations and the interplay between social relevance aspects. Through the user study we gain insights on conditions under which social annotations are most useful to a user. Finally, we present machine learned models for predicting the utility of a social annotation using the user study judgments as an optimization criterion. We model the learning task with features drawn from web usage logs, and show empirical evidence over real-world head and tail queries that the problem is learnable and that in many cases we can predict the utility of a social annotation.

#index 1879022
#* An exploration of ranking heuristics in mobile local search
#@ Yuanhua Lv;Dimitrios Lymberopoulos;Qiang Wu
#t 2012
#c 13
#% 169781
#% 218982
#% 218992
#% 280819
#% 448194
#% 722904
#% 754126
#% 766412
#% 818207
#% 818255
#% 818256
#% 860086
#% 874993
#% 881540
#% 946521
#% 949163
#% 956552
#% 1004298
#% 1047345
#% 1055697
#% 1055707
#% 1190103
#% 1206801
#% 1266460
#% 1328137
#% 1357833
#% 1476142
#% 1536505
#% 1598412
#% 1619984
#% 1619985
#% 1622355
#% 1641914
#% 1641962
#% 1728811
#% 1763376
#! Users increasingly rely on their mobile devices to search local entities, typically businesses, while on the go. Even though recent work has recognized that the ranking signals in mobile local search (e.g., distance and customer rating score of a business) are quite different from general Web search, they have mostly treated these signals as a black-box to extract very basic features (e.g., raw distance values and rating scores) without going inside the signals to understand how exactly they affect the relevance of a business. However, as it has been demonstrated in the development of general information retrieval models, it is critical to explore the underlying behaviors/heuristics of a ranking signal to design more effective ranking features. In this paper, we follow a data-driven methodology to study the behavior of these ranking signals in mobile local search using a large-scale query log. Our analysis reveals interesting heuristics that can be used to guide the exploitation of different signals. For example, users often take the mean value of a signal (e.g., rating) from the business result list as a "pivot" score, and tend to demonstrate different click behaviors on businesses with lower and higher signal values than the pivot; the clickrate of a business generally is sublinearly decreasing with its distance to the user, etc. Inspired by the understanding of these heuristics, we further propose different transformation methods to generate more effective ranking features. We quantify the improvement of the proposed new features using real mobile local search logs over a period of 14 months and show that the mean average precision can be improved by over 7%.

#index 1879023
#* Mining query subtopics from search log data
#@ Yunhua Hu;Yanan Qian;Hang Li;Daxin Jiang;Jian Pei;Qinghua Zheng
#t 2012
#c 13
#% 297550
#% 310567
#% 330617
#% 401405
#% 577224
#% 590523
#% 766433
#% 817846
#% 838469
#% 853542
#% 879567
#% 879639
#% 987203
#% 987221
#% 987222
#% 987223
#% 987326
#% 1034802
#% 1083721
#% 1130878
#% 1190055
#% 1202162
#% 1213625
#% 1227619
#% 1400099
#% 1536529
#% 1537503
#! Most queries in web search are ambiguous and multifaceted. Identifying the major senses and facets of queries from search log data, referred to as query subtopic mining in this paper, is a very important issue in web search. Through search log analysis, we show that there are two interesting phenomena of user behavior that can be leveraged to identify query subtopics, referred to as `one subtopic per search' and `subtopic clarification by keyword'. One subtopic per search means that if a user clicks multiple URLs in one query, then the clicked URLs tend to represent the same sense or facet. Subtopic clarification by keyword means that users often add an additional keyword or keywords to expand the query in order to clarify their search intent. Thus, the keywords tend to be indicative of the sense or facet. We propose a clustering algorithm that can effectively leverage the two phenomena to automatically mine the major subtopics of queries, where each subtopic is represented by a cluster containing a number of URLs and keywords. The mined subtopics of queries can be used in multiple tasks in web search and we evaluate them in aspects of the search result presentation such as clustering and re-ranking. We demonstrate that our clustering algorithm can effectively mine query subtopics with an F1 measure in the range of 0.896-0.956. Our experimental results show that the use of the subtopics mined by our approach can significantly improve the state-of-the-art methods used for search result clustering. Experimental results based on click data also show that the re-ranking of search result based on our method can significantly improve the efficiency of users' ability to find information.

#index 1879024
#* Search, interrupted: understanding and predicting search task continuation
#@ Eugene Agichtein;Ryen W. White;Susan T. Dumais;Paul N. Bennet
#t 2012
#c 13
#% 590523
#% 732901
#% 751800
#% 754059
#% 805200
#% 805878
#% 805898
#% 823348
#% 879567
#% 881540
#% 955711
#% 961704
#% 987211
#% 1019076
#% 1019124
#% 1047436
#% 1047437
#% 1083721
#% 1126944
#% 1130852
#% 1130878
#% 1166518
#% 1227622
#% 1292754
#% 1355038
#% 1399944
#% 1399965
#% 1400099
#% 1450832
#% 1450885
#% 1455264
#% 1598334
#% 1641927
#% 1642076
#% 1642132
#% 1693905
#! Many important search tasks require multiple search sessions to complete. Tasks such as travel planning, large purchases, or job searches can span hours, days, or even weeks. Inevitably, life interferes, requiring the searcher either to recover the "state" of the search manually (most common), or plan for interruption in advance (unlikely). The goal of this work is to better understand, characterize, and automatically detect search tasks that will be continued in the near future. To this end, we analyze a query log from the Bing Web search engine to identify the types of intents, topics, and search behavior patterns associated with long-running tasks that are likely to be continued. Using our insights, we develop an effective prediction algorithm that significantly outperforms both the previous state-of-the-art method, and even the ability of human judges, to predict future task continuation. Potential applications of our techniques would allow a search engine to pre-emptively "save state" for a searcher (e.g., by caching search results), perform more targeted personalization, and otherwise better support the searcher experience for interrupted search tasks.

#index 1879025
#* Multi-aspect query summarization by composite query
#@ Wei Song;Qing Yu;Zhiheng Xu;Ting Liu;Sheng Li;Ji-Rong Wen
#t 2012
#c 13
#% 262036
#% 297550
#% 309095
#% 643068
#% 754124
#% 766433
#% 769892
#% 816173
#% 857482
#% 879636
#% 956510
#% 956649
#% 987203
#% 1019116
#% 1083679
#% 1185582
#% 1214708
#% 1223708
#% 1275040
#% 1310459
#% 1343447
#% 1482374
#% 1536552
#% 1602951
#% 1642193
#! Conventional search engines usually return a ranked list of web pages in response to a query. Users have to visit several pages to locate the relevant parts. A promising future search scenario should involve: (1) understanding user intents; (2) providing relevant information directly to satisfy searchers' needs, as opposed to relevant pages. In this paper, we present a search paradigm to summarize a query's information from different aspects. Query aspects could be aligned to user intents. The generated summaries for query aspects are expected to be both specific and informative, so that users can easily and quickly find relevant information. Specifically, we use a Composite Query for Summarization" method, where a set of component queries are used for providing additional information for the original query. The system leverages the search engine to proactively gather information by submitting multiple component queries according to the original query and its aspects. In this way, we could get more relevant information for each query aspect and roughly classify information. By comparative mining the search results of different component queries, it is able to identify query (dependent) aspect words, which help to generate more specific and informative summaries. The experimental results on two data sets, Wikipedia and TREC ClueWeb2009, are encouraging. Our method outperforms two baseline methods on generating informative summaries.

#index 1879026
#* Language intent models for inferring user browsing behavior
#@ Manos Tsagkias;Roi Blanco
#t 2012
#c 13
#% 262096
#% 296646
#% 577224
#% 590523
#% 750863
#% 766454
#% 789959
#% 805878
#% 838508
#% 879567
#% 1019082
#% 1055676
#% 1130868
#% 1195879
#% 1227622
#% 1399999
#% 1450884
#% 1536561
#% 1598382
#% 1598393
#% 1598409
#% 1631336
#% 1641944
#% 1648026
#! Modeling user browsing behavior is an active research area with tangible real-world applications, e.g., organizations can adapt their online presence to their visitors browsing behavior with positive effects in user engagement, and revenue. We concentrate on online news agents, and present a semi-supervised method for predicting news articles that a user will visit after reading an initial article. Our method tackles the problem using language intent models trained on historical data which can cope with unseen articles. We evaluate our method on a large set of articles and in several experimental settings. Our results demonstrate the utility of language intent models for predicting user browsing behavior within online news sites.

#index 1879027
#* Efficient query recommendations in the long tail via center-piece subgraphs
#@ Francesco Bonchi;Raffaele Perego;Fabrizio Silvestri;Hossein Vahabi;Rossano Venturini
#t 2012
#c 13
#% 838531
#% 869501
#% 881496
#% 987358
#% 989578
#% 1089473
#% 1130854
#% 1130868
#% 1190106
#% 1400023
#% 1482300
#% 1536566
#% 1560359
#% 1587854
#% 1598414
#! We present a recommendation method based on the well-known concept of center-piece subgraph, that allows for the time/space efficient generation of suggestions also for rare, i.e., long-tail queries. Our method is scalable with respect to both the size of datasets from which the model is computed and the heavy workloads that current web search engines have to deal with. Basically, we relate terms contained into queries with highly correlated queries in a query-flow graph. This enables a novel recommendation generation method able to produce recommendations for approximately 99% of the workload of a real-world search engine. The method is based on a graph having term nodes, query nodes, and two kinds of connections: term-query and query-query. The first connects a term to the queries in which it is contained, the second connects two query nodes if the likelihood that a user submits the second query after having issued the first one is sufficiently high. On such large graph we need to compute the center-piece subgraph induced by terms contained into queries. In order to reduce the cost of the above computation, we introduce a novel and efficient method based on an inverted index representation of the model. We experiment our solution on two real-world query logs and we show that its effectiveness is comparable (and in some case better) than state-of-the-art methods for head-queries. More importantly, the quality of the recommendations generated remains very high also for long-tail queries, where other methods fail even to produce any suggestion. Finally, we extensively investigate scalability and efficiency issues and we show the viability of our method in real world search engines.

#index 1879028
#* Supporting efficient top-k queries in type-ahead search
#@ Guoliang Li;Jiannan Wang;Chen Li;Jianhua Feng
#t 2012
#c 13
#% 131061
#% 262869
#% 333854
#% 480654
#% 766461
#% 781169
#% 818232
#% 864392
#% 864459
#% 879610
#% 987276
#% 1022218
#% 1022220
#% 1075132
#% 1190092
#% 1206665
#% 1206677
#% 1217199
#% 1217200
#% 1488676
#% 1581932
#% 1594642
#% 1618134
#% 1755300
#! Type-ahead search can on-the-fly find answers as a user types in a keyword query. A main challenge in this search paradigm is the high-efficiency requirement that queries must be answered within milliseconds. In this paper we study how to answer top-k queries in this paradigm, i.e., as a user types in a query letter by letter, we want to efficiently find the k best answers. Instead of inventing completely new algorithms from scratch, we study challenges when adopting existing top-k algorithms in the literature that heavily rely on two basic list-access methods: random access and sorted access. We present two algorithms to support random access efficiently. We develop novel techniques to support efficient sorted access using list pruning and materialization. We extend our techniques to support fuzzy type-ahead search which allows minor errors between query keywords and answers. We report our experimental results on several real large data sets to show that the proposed techniques can answer top-k queries efficiently in type-ahead search.

#index 1879029
#* SimFusion+: extending simfusion towards efficient estimation on large and dynamic networks
#@ Weiren Yu;Xuemin Lin;Wenjie Zhang;Ying Zhang;Jiajin Le
#t 2012
#c 13
#% 3084
#% 577273
#% 616105
#% 754089
#% 783508
#% 806956
#% 818218
#% 1198838
#% 1292521
#% 1366465
#% 1372721
#% 1450926
#% 1451194
#% 1722875
#! SimFusion has become a captivating measure of similarity between objects in a web graph. It is iteratively distilled from the notion that "the similarity between two objects is reinforced by the similarity of their related objects". The existing SimFusion model usually exploits the Unified Relationship Matrix (URM) to represent latent relationships among heterogeneous data, and adopts an iterative paradigm for SimFusion computation. However, due to the row normalization of URM, the traditional SimFusion model may produce the trivial solution; worse still, the iterative computation of SimFusion may not ensure the global convergence of the solution. This paper studies the revision of this model, providing a full treatment from complexity to algorithms. (1) We propose SimFusion+ based on a notion of the Unified Adjacency Matrix (UAM), a modification of the URM, to prevent the trivial solution and the divergence issue of SimFusion. (2) We show that for any vertex-pair, SimFusion+ can be performed in O(1) time and O(n) space with an O(km)-time precomputation done only once, as opposed to the O(kn3) time and O(n2) space of its traditional counterpart, where n, m, and k denote the number of vertices, edges, and iterations respectively. (3) We also devise an incremental algorithm for further improving the computation of SimFusion+ when networks are dynamically updated, with performance guarantees for similarity estimation. We experimentally verify that these algorithms scale well, and the revised notion of SimFusion is able to converge to a non-trivial solution, and allows us to identify more sensible structure information in large real-world networks.

#index 1879030
#* Group matrix factorization for scalable topic modeling
#@ Quan Wang;Zheng Cao;Jun Xu;Hang Li
#t 2012
#c 13
#% 274586
#% 280819
#% 321635
#% 722904
#% 769967
#% 879587
#% 967300
#% 1399944
#% 1598402
#% 1605964
#% 1701756
#! Topic modeling can reveal the latent structure of text data and is useful for knowledge discovery, search relevance ranking, document classification, and so on. One of the major challenges in topic modeling is to deal with large datasets and large numbers of topics in real-world applications. In this paper, we investigate techniques for scaling up the non-probabilistic topic modeling approaches such as RLSI and NMF. We propose a general topic modeling method, referred to as Group Matrix Factorization (GMF), to enhance the scalability and efficiency of the non-probabilistic approaches. GMF assumes that the text documents have already been categorized into multiple semantic classes, and there exist class-specific topics for each of the classes as well as shared topics across all classes. Topic modeling is then formalized as a problem of minimizing a general objective function with regularizations and/or constraints on the class-specific topics and shared topics. In this way, the learning of class-specific topics can be conducted in parallel, and thus the scalability and efficiency can be greatly improved. We apply GMF to RLSI and NMF, obtaining Group RLSI (GRLSI) and Group NMF (GNMF) respectively. Experiments on a Wikipedia dataset and a real-world web dataset, each containing about 3 million documents, show that GRLSI and GNMF can greatly improve RLSI and NMF in terms of scalability and efficiency. The topics discovered by GRLSI and GNMF are coherent and have good readability. Further experiments on a search relevance dataset, containing 30,000 labeled queries, show that the use of topics learned by GRLSI and GNMF can significantly improve search relevance.

#index 1879031
#* Detecting quilted web pages at scale
#@ Marc Najork
#t 2012
#c 13
#% 201935
#% 255137
#% 347225
#% 571725
#% 616528
#% 728115
#% 818223
#% 879600
#% 1055708
#% 1065411
#% 1074122
#% 1166531
#% 1468421
#! Web-based advertising and electronic commerce, combined with the key role of search engines in driving visitors to ad-monetized and e-commerce web sites, has given rise to the phenomenon of web spam: web pages that are of little value to visitors, but that are created mainly to mislead search engines into driving traffic to target web sites. A large fraction of spam web pages is automatically generated, and some portion of these pages is generated by stitching together parts (sentences or paragraphs) of other web pages. This paper presents a scalable algorithm for detecting such "quilted" web pages. Previous work by the author and his collaborators introduced a sampling-based algorithm that was capable of detecting some, but by far not all quilted web pages in a collection. By contrast, the algorithm presented in this work identifies all quilted web pages, and it is scalable to very large corpora. We tested the algorithm on the half-billion page English-language subset of the ClueWeb09 collection, and evaluated its effectiveness in detecting web spam by manually inspecting small samples of the detected quilted pages. This manual inspection guided us in iteratively refining the algorithm to be more efficient in detecting real-world spam.

#index 1879032
#* Fighting against web spam: a novel propagation method based on click-through data
#@ Chao Wei;Yiqun Liu;Min Zhang;Shaoping Ma;Liyun Ru;Kuo Zhang
#t 2012
#c 13
#% 296646
#% 807297
#% 818201
#% 832098
#% 869471
#% 869549
#% 879567
#% 958003
#% 987245
#% 987369
#% 1016177
#% 1022743
#% 1074107
#% 1125900
#% 1125902
#% 1125903
#% 1194314
#% 1536557
#% 1560965
#% 1565812
#! Combating Web spam is one of the greatest challenges for Web search engines. State-of-the-art anti-spam techniques focus mainly on detecting varieties of spam strategies, such as content spamming and link-based spamming. Although these anti-spam approaches have had much success, they encounter problems when fighting against a continuous barrage of new types of spamming techniques. We attempt to solve the problem from a new perspective, by noticing that queries that are more likely to lead to spam pages/sites have the following characteristics: 1) they are popular or reflect heavy demands for search engine users and 2) there are usually few key resources or authoritative results for them. From these observations, we propose a novel method that is based on click-through data analysis by propagating the spamicity score iteratively between queries and URLs from a few seed pages/sites. Once we obtain the seed pages/sites, we use the link structure of the click-through bipartite graph to discover other pages/sites that are likely to be spam. Experiments show that our algorithm is both efficient and effective in detecting Web spam. Moreover, combining our method with some popular anti-spam techniques such as TrustRank achieves improvement compared with each technique taken individually.

#index 1879033
#* Learning hash codes for efficient content reuse detection
#@ Qi Zhang;Yan Wu;Zhuoye Ding;Xuanjing Huang
#t 2012
#c 13
#% 345087
#% 479973
#% 504572
#% 544011
#% 616528
#% 662872
#% 769944
#% 799972
#% 803596
#% 815336
#% 838508
#% 845502
#% 891060
#% 920033
#% 956506
#% 987258
#% 1055684
#% 1074121
#% 1074122
#% 1190063
#% 1270458
#% 1450831
#% 1450913
#% 1678744
#! Content reuse is extremely common in user generated mediums. Reuse detection serves as be the basis for many applications. However, along with the explosion of Internet and continuously growing uses of user generated mediums, the task becomes more critical and difficult. In this paper, we present a novel efficient and scalable approach to detect content reuse. We propose a new signature generation algorithm, which is based on learned hash functions for words. In order to deal with tens of billions of documents, we implement the detection approach on graphical processing units (GPUs). The experimental comparison in this paper involves studies of efficiency and effectiveness of the proposed approach in different types of document collections, including ClueWeb09, Tweets2011, and so on. Experimental results show that the proposed approach can achieve the same detection rates with state-of-the-art systems while uses significantly less execution time than them (from 400X to 1500X speedup).

#index 1879034
#* Explanatory semantic relatedness and explicit spatialization for exploratory search
#@ Brent Hecht;Samuel H. Carton;Mahmood Quaderi;Johannes Schöning;Martin Raubal;Darren Gergle;Doug Downey
#t 2012
#c 13
#% 270633
#% 342963
#% 619859
#% 640269
#% 754059
#% 834997
#% 881477
#% 896031
#% 958367
#% 1076374
#% 1125711
#% 1130858
#% 1183338
#% 1185582
#% 1250381
#% 1272185
#% 1272267
#% 1275012
#% 1279761
#% 1289050
#% 1328070
#% 1369940
#% 1384122
#% 1399949
#% 1415756
#% 1434134
#% 1481659
#% 1560388
#! Exploratory search, in which a user investigates complex concepts, is cumbersome with today's search engines. We present a new exploratory search approach that generates interactive visualizations of query concepts using thematic cartography (e.g. choropleth maps, heat maps). We show how the approach can be applied broadly across both geographic and non-geographic contexts through explicit spatialization, a novel method that leverages any figure or diagram -- from a periodic table, to a parliamentary seating chart, to a world map -- as a spatial search environment. We enable this capability by introducing explanatory semantic relatedness measures. These measures extend frequently-used semantic relatedness measures to not only estimate the degree of relatedness between two concepts, but also generate human-readable explanations for their estimates by mining Wikipedia's text, hyperlinks, and category structure. We implement our approach in a system called Atlasify, evaluate its key components, and present several use cases.

#index 1879035
#* A subjunctive exploratory search interface to support media studies researchers
#@ Marc Bron;Jasmijn van Gorp;Frank Nack;Maarten de Rijke;Andrei Vishneuski;Sonja de Leeuw
#t 2012
#c 13
#% 105945
#% 201992
#% 341136
#% 452641
#% 580082
#% 825806
#% 857478
#% 1014717
#% 1029800
#% 1065301
#% 1137657
#% 1185582
#% 1213448
#% 1224728
#% 1227624
#% 1268490
#% 1384094
#% 1441648
#% 1455260
#% 1523422
#% 1666690
#! Media studies concerns the study of production, content, and/or reception of various types of media. Today's continuous production and storage of media is changing the way media studies researchers work and requires the development of new search models and tools. We investigate the research cycle of media studies researchers and find that it is an iterative process consisting of several search processes in which data is gathered and the research question is refined. Changes in the research question, however, trigger new data gathering processes. Based on these outcomes we propose a subjunctive exploratory search interface to support media studies researchers in refining their research question in an earlier stage of their research. To assess the subjunctive interface we conduct a user study and compare to a traditional exploratory search interface. We find that with the subjunctive interface users explore more diverse topics than with the standard interface and that users formulate more specific research questions. Although the subjunctive interface is more complex, this does not decrease its usability. These findings suggest that the subjunctive interface supports media studies researchers. The advantage of a subjunctive interface for exploration suggests a new direction for the development of exploratory search systems.

#index 1879036
#* Task complexity, vertical display and user interaction in aggregated search
#@ Jaime Arguello;Wan-Ching Wu;Diane Kelly;Ashlee Edwards
#t 2012
#c 13
#% 187999
#% 814947
#% 860649
#% 955711
#% 1074093
#% 1126944
#% 1166523
#% 1173627
#% 1227616
#% 1251169
#% 1267046
#% 1314928
#% 1434128
#% 1450915
#% 1452772
#% 1467900
#% 1482230
#% 1536576
#% 1587348
#% 1641937
#! Aggregated search is the task of blending results from specialized search services or verticals into the Web search results. While many studies have focused on aggregated search techniques, few studies have tried to better understand how users interact with aggregated search results. This study investigates how task complexity and vertical display (the blending of vertical results into the web results) affect the use of vertical content. Twenty-nine subjects completed six search tasks of varying levels of task complexity using two aggregated search interfaces: one that blended vertical results into the web results and one that only provided indirect vertical access. Our results show that more complex tasks required significantly more interaction and that subjects completing these tasks examined more vertical results. While the amount of interaction was the same between interfaces, subjects clicked on more vertical results when these were blended into the web results. Our results also show an interaction between task complexity and vertical display; subjects clicked on more verticals when completing the more complex tasks with the interface that blended vertical results. Subjects' evaluations of the two interfaces were nearly identical, but when analyzed with respect to their interface preferences, we found a positive relationship between system evaluations and individual preferences. Subjects justified their preference using similar rationales and their comments illustrate how the display itself can influence judgments of information quality, especially in cases when the vertical results might not be relevant to the search task.

#index 1879037
#* Image ranking based on user browsing behavior
#@ Michele Trevisiol;Luca Chiarandini;Luca Maria Aiello;Alejandro Jaimes
#t 2012
#c 13
#% 290830
#% 309779
#% 762907
#% 799632
#% 1055702
#% 1074107
#% 1119135
#% 1190089
#% 1190090
#% 1190132
#% 1366525
#% 1400029
#% 1451288
#% 1482348
#% 1484562
#% 1536530
#% 1560382
#% 1587858
#% 1621235
#% 1632353
#% 1641987
#% 1649020
#% 1902329
#! Ranking of images is difficult because many factors determine their importance (e.g., popularity, quality, entertainment value, context, etc.). In social media platforms, ranking also depends on social interactions and on the visibility of the images both inside and outside those platforms. In this context, the application of standard ranking methods is not clearly understood, and neither are the subtleties associated with taking into account social interaction, internal, and external factors. In this paper, we use a large Flickr dataset and investigate these factors by performing an in-depth analysis of several ranking algorithms using both internal (i.e., within Flickr) and external (i.e., links from outside of Flickr) factors. We analyze rankings given by common metrics used in image retrieval (e.g., number of favorites), and compare them with metrics based on page views (e.g., time spent, number of views). In addition, we represent users' navigation by a graph and combine session models with some of these metrics, comparing with PageRank and BrowseRank. Our experiments show significant differences between the rankings, providing insights on the impact of social interactions, internal, and external factors in image ranking.

#index 1879038
#* Modeling concept dynamics for large scale music search
#@ Jialie Shen;HweeHwa Pang;Meng Wang;Shuicheng Yan
#t 2012
#c 13
#% 431066
#% 643010
#% 722887
#% 729437
#% 879572
#% 987247
#% 987532
#% 1077150
#% 1188997
#% 1190864
#% 1227625
#% 1227627
#% 1450907
#% 1450908
#% 1598404
#% 1740997
#% 1767359
#% 1767413
#% 1775543
#% 1775704
#% 1869114
#! Continuing advances in data storage and communication technologies have led to an explosive growth in digital music collections. To cope with their increasing scale, we need effective Music Information Retrieval (MIR) capabilities like tagging, concept search and clustering. Integral to MIR is a framework for modelling music documents and generating discriminative signatures for them. In this paper, we introduce a multimodal, layered learning framework called DMCM. Distinguished from the existing approaches that encode music as an ensemble of order-less feature vectors, our framework extracts from each music document a variety of acoustic features, and translates them into low-level encodings over the temporal dimension. From them, DMCM elucidates the concept dynamics in the music document, representing them with a novel music signature scheme called Stochastic Music Concept Histogram (SMCH) that captures the probability distribution over all the concepts. Experiment results with two large music collections confirm the advantages of the proposed framework over existing methods on various MIR tasks.

#index 1879039
#* Finding translations in scanned book collections
#@ Ismet Zeki Yalniz;R. Manmatha
#t 2012
#c 13
#% 255137
#% 280826
#% 289101
#% 320220
#% 465914
#% 571725
#% 655486
#% 735134
#% 740915
#% 786575
#% 854648
#% 874470
#% 1215368
#% 1446934
#% 1455223
#% 1484364
#% 1558415
#% 1641968
#% 1645368
#% 1701329
#! This paper describes an approach for identifying translations of books in large scanned book collections with OCR errors. The method is based on the idea that although individual sentences do not necessarily preserve the word order when translated, a book must preserve the linear progression of ideas for it to be a valid translation. Consider two books in two different languages, say English and German. The English book in the collection is represented by the sequence of words (in the order they appear in the text) which appear only once in the book. Similarly, the book in German is represented by its sequence of words which appear only once. An English-German dictionary is used to transform the word sequence of the English book into German by translating individual words in place. It is not necessary to translate all the words and this method works even with small dictionaries. Both sequences are now in German and can, therefore, be aligned using a Longest Common Subsequence (LCS) algorithm. We describe two scoring functions TRANS-cs and TRANS-its which account for both the LCS length and the lengths of the original word sequences. Experiments demonstrate that TRANS-its is particularly successful in finding translations of books and outperforms several baselines including metadata search based on matching titles and authors. Experiments performed on a Europarl parallel corpus for four language pairs, English-Finnish, English-French, English-German, English-Spanish, and a scanned book collection of 50K English-German books show that the proposed method retrieves translations of books with an average MAP score of 1.0 and a speed of 10K book pair comparisons per second on a single core.

#index 1879040
#* Predicting the ratings of multimedia items for making personalized recommendations
#@ Rani Qumsiyeh;Yiu-Kai Ng
#t 2012
#c 13
#% 342685
#% 452563
#% 805841
#% 992311
#% 1083671
#% 1083696
#% 1136456
#% 1227603
#% 1227638
#% 1366268
#% 1399972
#% 1418133
#% 1451647
#% 1476486
#% 1659467
#% 1693254
#! Existing multimedia recommenders suggest a specific type of multimedia items rather than items of different types personalized for a user based on his/her preference. Assume that a user is interested in a particular family movie, it is appealing if a multimedia recommendation system can suggest other movies, music, books, and paintings closely related to the movie. We propose a comprehensive, personalized multimedia recommendation system, denoted MudRecS, which makes recommendations on movies, music, books, and paintings similar in content to other movies, music, books, and/or paintings that a MudRecS user is interested in. MudRecS does not rely on users' access patterns/histories, connection information extracted from social networking sites, collaborated filtering methods, or user personal attributes (such as gender and age) to perform the recommendation task. It simply considers the users' ratings, genres, role players (authors or artists), and reviews of different multimedia items, which are abundant and easy to find on the Web. MudRecS predicts the ratings of multimedia items that match the interests of a user to make recommendations. The performance ofMudRecS has been compared with current state-of-the-art multimedia recommenders using various multimedia datasets, and the experimental results show that MudRecS significantly outperforms other systems in accurately predicting the ratings of multimedia items to be recommended.

#index 1879041
#* Personalized click shaping through lagrangian duality for online recommendation
#@ Deepak Agarwal;Bee-Chung Chen;Pradheep Elango;Xuanhui Wang
#t 2012
#c 13
#% 280852
#% 397133
#% 425053
#% 757953
#% 931104
#% 956521
#% 1190057
#% 1190066
#% 1214623
#% 1214666
#% 1214754
#% 1227585
#% 1356185
#% 1399999
#% 1426653
#% 1476450
#% 1536534
#% 1560391
#% 1598346
#% 1605927
#% 1606074
#% 1699611
#! Online content recommendation aims to identify trendy articles in a continuously changing dynamic content pool. Most of existing works rely on online user feedback, notably clicks, as the objective and maximize it by showing articles with highest click-through rates. Recently, click shaping was introduced to incorporate multiple objectives in a constrained optimization framework. The work showed that significant tradeoff among the competing objectives can be observed and thus it is important to consider multiple objectives. However, the proposed click shaping approach is segment-based and can only work with a few non-overlapping user segments. It remains a challenge of how to enable deep personalization in click shaping. In this paper, we tackle the challenge by proposing personalized click shaping. The main idea is to work with the Lagrangian duality formulation and explore strong convexity to connect dual and primal solutions. We show that our formulation not only allows efficient conversion from dual to primal for online personalized serving, but also enables us to solve the optimization faster by approximation. We conduct extensive experiments on a large real data set and our experimental results show that the personalized click shaping can significantly outperform the segmented one, while achieving the same ability to balance competing objectives.

#index 1879042
#* What reviews are satisfactory: novel features for automatic helpfulness voting
#@ Yu Hong;Jun Lu;Jianmin Yao;Qiaoming Zhu;Guodong Zhou
#t 2012
#c 13
#% 309095
#% 723399
#% 729625
#% 854653
#% 879588
#% 939896
#% 1035590
#% 1055682
#% 1176947
#% 1190069
#% 1261574
#% 1522188
#% 1536586
#! This paper focuses on exploring the features of product reviews that satisfy users, by which to improve the automatic helpfulness voting for the reviews on commercial websites. Compared to the previous work, which single-mindedly adopts the textual features to assess the review helpfulness, we propose that user preferences are more explicit clues to infer the opinions of users on the review helpfulness. By using the user-preference based features, we firstly implement a binary helpfulness based review classification system to divide helpful reviews and useless, and on the basis, we secondly build a Ranking SVM based automatic helpfulness voting system (AHV) which rank reviews based on their helpfulness. Experiments used a large scale dataset containing over 34,266 reviews on 1289 products to test the systems, which achieves promising performances with accuracy of up to 0.72 and NDCG@10 of 0.25, and at least 9% accuracy improvement compared to the textual-feature based helpfulness assessment.

#index 1879043
#* Automatic refinement of patent queries using concept importance predictors
#@ Parvaz Mahdabi;Linda Andersson;Mostafa Keikha;Fabio Crestani
#t 2012
#c 13
#% 397161
#% 448194
#% 766408
#% 783506
#% 816186
#% 818267
#% 855219
#% 855221
#% 955496
#% 987299
#% 987331
#% 987356
#% 1166508
#% 1227746
#% 1450905
#% 1587413
#% 1603965
#% 1642152
#% 1697431
#! Patent prior art queries are full patent applications which are much longer than standard web search topics. Such queries are composed of hundreds of terms and do not represent a focused information need. One way to make the queries more focused is to select a group of key terms as representatives. Existing works show that such a selection to reduce patent queries is a challenging task mainly because of the presence of ambiguous terms. Given this setup, we present a query modeling approach where we utilize patent-specific characteristics to generate more precise queries. We propose to automatically disambiguate query terms by employing noun phrases that are extracted using the global analysis of the patent collection. We further introduce a method for predicting whether expansion using noun phrases would improve the retrieval effectiveness. Our experiments show that we can obtain almost 20% improvement by performing query expansion using the true importance of the noun phrase queries. Based on this observation, we introduce various features that can be used to estimate the importance of the noun phrase query. We evaluated the effectiveness of the proposed method on the patent prior art search collection CLEF-IP 2010. Our experimental results indicate that the proposed features make good predictors of the noun phrase importance, and selective application of noun phrase queries using the importance predictors outperforms existing query generation methods.

#index 1879044
#* Automatic term mismatch diagnosis for selective query expansion
#@ Le Zhao;Jamie Callan
#t 2012
#c 13
#% 27049
#% 54435
#% 262037
#% 262084
#% 319273
#% 340901
#% 412887
#% 789959
#% 822126
#% 869501
#% 987272
#% 1074053
#% 1130847
#% 1130855
#% 1292650
#% 1355020
#% 1482204
#% 1482362
#! People are seldom aware that their search queries frequently mismatch a majority of the relevant documents. This may not be a big problem for topics with a large and diverse set of relevant documents, but would largely increase the chance of search failure for less popular search needs. We aim to address the mismatch problem by developing accurate and simple queries that require minimal effort to construct. This is achieved by targeting retrieval interventions at the query terms that are likely to mismatch relevant documents. For a given topic, the proportion of relevant documents that do not contain a term measures the probability for the term to mismatch relevant documents, or the term mismatch probability. Recent research demonstrates that this probability can be estimated reliably prior to retrieval. Typically, it is used in probabilistic retrieval models to provide query dependent term weights. This paper develops a new use: Automatic diagnosis of term mismatch. A search engine can use the diagnosis to suggest manual query reformulation, guide interactive query expansion, guide automatic query expansion, or motivate other responses. The research described here uses the diagnosis to guide interactive query expansion, and create Boolean conjunctive normal form (CNF) structured queries that selectively expand 'problem' query terms while leaving the rest of the query untouched. Experiments with TREC Ad-hoc and Legal Track datasets demonstrate that with high quality manual expansion, this diagnostic approach can reduce user effort by 33%, and produce simple and effective structured queries that surpass their bag of word counterparts.

#index 1879045
#* Generating reformulation trees for complex queries
#@ Xiaobing Xue;W. Bruce Croft
#t 2012
#c 13
#% 262096
#% 281396
#% 340901
#% 340948
#% 397161
#% 789959
#% 818262
#% 838397
#% 838398
#% 869501
#% 983820
#% 987231
#% 987272
#% 1074098
#% 1074112
#% 1130855
#% 1130868
#% 1227636
#% 1227647
#% 1292754
#% 1355019
#% 1450900
#% 1482284
#% 1482362
#% 1598467
#! Search queries have evolved beyond keyword queries. Many complex queries such as verbose queries, natural language question queries and document-based queries are widely used in a variety of applications. Processing these complex queries usually requires a series of query operations, which results in multiple sequences of reformulated queries. However, previous query representations, either the "bag of words" method or the recently proposed "query distribution" method, cannot effectively model these query sequences, since they ignore the relationships between two queries. In this paper, a reformulation tree framework is proposed to organize multiple sequences of reformulated queries as a tree structure, where each path of the tree corresponds to a sequence of reformulated queries. Specifically, a two-level reformulation tree is implemented for verbose queries. This tree effectively combines two query operations, i.e., subset selection and query substitution, within the same framework. Furthermore, a weight estimation approach is proposed to assign weights to each node of the reformulation tree by considering the relationships with other nodes and directly optimizing retrieval performance. Experiments on TREC collections show that this reformulation tree based representation significantly outperforms the state-of-the-art techniques.

#index 1879046
#* Proximity-based rocchio's model for pseudo relevance
#@ Jun Miao;Jimmy Xiangji Huang;Zheng Ye
#t 2012
#c 13
#% 92696
#% 106122
#% 134878
#% 262096
#% 298183
#% 306494
#% 306497
#% 326522
#% 340901
#% 342707
#% 750863
#% 818262
#% 879584
#% 879585
#% 879651
#% 915281
#% 943042
#% 987229
#% 987231
#% 1166534
#% 1227604
#% 1227613
#% 1227614
#% 1292550
#% 1292730
#% 1387547
#% 1392433
#% 1415737
#% 1450901
#% 1567726
#% 1576174
#% 1598349
#% 1697461
#! Rocchio's relevance feedback model is a classic query expansion method and it has been shown to be effective in boosting information retrieval performance. The selection of expansion terms in this method, however, does not take into account the relationship between the candidate terms and the query terms (e.g., term proximity). Intuitively, the proximity between candidate expansion terms and query terms can be exploited in the process of query expansion, since terms closer to query terms are more likely to be related to the query topic. In this paper, we study how to incorporate proximity information into the Rocchio's model, and propose a proximity-based Rocchio's model, called PRoc, with three variants. In our PRoc models, a new concept (proximity-based term frequency, ptf) is introduced to model the proximity information in the pseudo relevant documents, which is then used in three kinds of proximity measures. Experimental results on TREC collections show that our proposed PRoc models are effective and generally superior to the state-of-the-art relevance feedback models with optimal parameters.A direct comparison with positional relevance model (PRM) on the GOV2 collection also indicates our proposed model is at least competitive to the most recent progress.

#index 1879047
#* Modeling user posting behavior on social media
#@ Zhiheng Xu;Yang Zhang;Yao Wu;Qing Yang
#t 2012
#c 13
#% 722904
#% 754107
#% 788094
#% 1035587
#% 1040837
#% 1055741
#% 1081487
#% 1083624
#% 1083675
#% 1158333
#% 1214658
#% 1227655
#% 1291617
#% 1298864
#% 1338675
#% 1355042
#% 1356185
#% 1384210
#% 1384223
#% 1384224
#% 1399966
#% 1399992
#% 1400018
#% 1426611
#% 1450883
#% 1451176
#% 1476470
#% 1482547
#% 1512437
#% 1560174
#% 1561559
#% 1586597
#% 1606021
#% 1607052
#! User generated content is the basic element of social media websites. Relatively few studies have systematically analyzed the motivation to create and share content, especially from the perspective of a common user. In this paper, we perform a comprehensive analysis of user posting behavior on a popular social media website, Twitter. Specifically, we assume that user behavior is mainly influenced by three factors: breaking news, posts from social friends and user's intrinsic interest, and propose a mixture latent topic model to combine all these factors. We evaluated our model on a large-scale Twitter dataset from three different perspectives: the perplexity of held-out content, the performance of predicting retweets and the quality of generated latent topics. The results were encouraging, our model clearly outperformed its competitors.

#index 1879048
#* Friend or frenemy?: predicting signed ties in social networks
#@ Shuang-Hong Yang;Alexander J. Smola;Bo Long;Hongyuan Zha;Yi Chang
#t 2012
#c 13
#% 304876
#% 309493
#% 730089
#% 754098
#% 790459
#% 840882
#% 1001279
#% 1047390
#% 1133453
#% 1183091
#% 1190129
#% 1214642
#% 1260273
#% 1287243
#% 1384246
#% 1399997
#% 1560408
#% 1598363
#! We study the problem of labeling the edges of a social network graph (e.g., acquaintance connections in Facebook) as either positive (i.e., trust, true friendship) or negative (i.e., distrust, possible frenemy) relations. Such signed relations provide much stronger signal in tying the behavior of online users than the unipolar Homophily effect, yet are largely unavailable as most social graphs only contain unsigned edges. We show the surprising fact that it is possible to infer signed social ties with good accuracy solely based on users' behavior of decision making (or using only a small fraction of supervision information) via unsupervised and semi-supervised algorithms. This work hereby makes it possible to turn an unsigned acquaintance network (e.g. Facebook, Myspace) into a signed trust-distrust network (e.g. Epinion, Slashdot). Our results are based on a mixed effects framework that simultaneously captures users' behavior, social interactions as well as the interplay between the two. The framework includes a series of latent factor models and it also encodes the principles of balance and status from Social psychology. Experiments on Epinion and Yahoo! Pulse networks illustrate that (1) signed social ties can be predicted with high-accuracy even in fully unsupervised settings, and (2) the predicted signed ties are significantly more useful for social behavior prediction than simple Homophily.

#index 1879049
#* Social-network analysis using topic models
#@ Youngchul Cha;Junghoo Cho
#t 2012
#c 13
#% 280819
#% 290830
#% 643056
#% 722904
#% 769906
#% 779872
#% 868088
#% 869480
#% 1055681
#% 1117695
#% 1192430
#% 1272187
#% 1355041
#% 1399992
#% 1523858
#% 1536537
#% 1536553
#% 1813854
#! In this paper, we discuss how we can extend probabilistic topic models to analyze the relationship graph of popular social-network data, so that we can group or label the edges and nodes in the graph based on their topic similarity. In particular, we first apply the well-known Latent Dirichlet Allocation (LDA) model and its existing variants to the graph-labeling task and argue that the existing models do not handle popular nodes (nodes with many incoming edges) in the graph very well. We then propose possible extensions to this model to deal with popular nodes. Our experiments show that the proposed extensions are very effective in labeling popular nodes, showing significant improvements over the existing methods. Our proposed methods can be used for providing, for instance, more relevant friend recommendations within a social network.

#index 1879050
#* Cognos: crowdsourcing search for topic experts in microblogs
#@ Saptarshi Ghosh;Naveen Sharma;Fabricio Benevenuto;Niloy Ganguly;Krishna Gummadi
#t 2012
#c 13
#% 306494
#% 348173
#% 956516
#% 1355042
#% 1392432
#% 1400082
#% 1536506
#% 1536507
#% 1536509
#% 1536537
#% 1560202
#% 1560425
#% 1874776

#index 1879051
#* Automatic suggestion of query-rewrite rules for enterprise search
#@ Zhuowei Bao;Benny Kimelfeld;Yunyao Li
#t 2012
#c 13
#% 340928
#% 577339
#% 730007
#% 754125
#% 768898
#% 787547
#% 869501
#% 869548
#% 869651
#% 879634
#% 956543
#% 1071487
#% 1399933
#% 1426467
#% 1560359
#% 1581844
#% 1616237
#% 1641925
#% 1712595
#% 1742069
#! Enterprise search is challenging for several reasons, notably the dynamic terminology and jargon that are specific to the enterprise domain. This challenge is partly addressed by having domain experts maintaining the enterprise search engine and adapting it to the domain specifics. Those administrators commonly address user complaints about relevant documents missing from the top matches. For that, it has been proposed to allow administrators to influence search results by crafting query-rewrite rules, each specifying how queries of a certain pattern should be modified or augmented with additional queries. Upon a complaint, the administrator seeks a semantically coherent rule that is capable of pushing the desired documents up to the top matches. However, the creation and maintenance of rewrite rules is highly tedious and time consuming. Our goal in this work is to ease the burden on search administrators by automatically suggesting rewrite rules. This automation entails several challenges. One major challenge is to select, among many options, rules that are ``natural'' from a semantic perspective (e.g., corresponding to closely related and syntactically complete concepts). Towards that, we study a machine-learning classification approach. The second challenge is to accommodate the cross-query effect of rules---a rule introduced in the context of one query can eliminate the desired results for other queries and the desired effects of other rules. We present a formalization of this challenge as a generic computational problem. As we show that this problem is highly intractable in terms of complexity theory, we present heuristic approaches and optimization thereof. In an experimental study within IBM intranet search, those heuristics achieve near-optimal quality and well scale to large data sets.

#index 1879052
#* Time-sensitive query auto-completion
#@ Milad Shokouhi;Kira Radinsky
#t 2012
#c 13
#% 82523
#% 190265
#% 722904
#% 730070
#% 765412
#% 766447
#% 766461
#% 805839
#% 943042
#% 960414
#% 1022220
#% 1159170
#% 1190092
#% 1217200
#% 1227692
#% 1270766
#% 1338638
#% 1355016
#% 1399966
#% 1443410
#% 1450843
#% 1467831
#% 1495112
#% 1536521
#% 1537502
#% 1560365
#% 1598343
#% 1598486
#% 1697416
#% 1699616
#% 1746858
#! Query auto-completion (QAC) is a common feature in modern search engines. High quality QAC candidates enhance search experience by saving users time that otherwise would be spent on typing each character or word sequentially. Current QAC methods rank suggestions according to their past popularity. However, query popularity changes over time, and the ranking of candidates must be adjusted accordingly. For instance, while halloween might be the right suggestion after typing ha in October, harry potter might be better any other time. Surprisingly, despite the importance of QAC as a key feature in most online search engines, its temporal dynamics have been under-studied. In this paper, we propose a time-sensitive approach for query auto-completion. Instead of ranking candidates according to their past popularity, we apply time-series and rank candidates according their forecasted frequencies. Our experiments on 846K queries and their daily frequencies sampled over a period of 4.5 years show that predicting the popularity of queries solely based on their past frequency can be misleading, and the forecasts obtained by time-series modeling are substantially more reliable. Our results also suggest that modeling the temporal trends of queries can significantly improve the ranking of QAC candidates.

#index 1879053
#* A generalized hidden Markov model with discriminative training for query spelling correction
#@ Yanen Li;Huizhong Duan;ChengXiang Zhai
#t 2012
#c 13
#% 124183
#% 131061
#% 144029
#% 218978
#% 262084
#% 464434
#% 757830
#% 817577
#% 854636
#% 939973
#% 1055706
#% 1074098
#% 1130855
#% 1338621
#% 1471206
#% 1484281
#% 1560366
#% 1560390
#% 1598362
#! Query spelling correction is a crucial component of modern search engines. Existing methods in the literature for search query spelling correction have two major drawbacks. First, they are unable to handle certain important types of spelling errors, such as concatenation and splitting. Second, they cannot efficiently evaluate all the candidate corrections due to the complex form of their scoring functions, and a heuristic filtering step must be applied to select a working set of top-K most promising candidates for final scoring, leading to non-optimal predictions. In this paper we address both limitations and propose a novel generalized Hidden Markov Model with discriminative training that can not only handle all the major types of spelling errors, including splitting and concatenation errors, in a single unified framework, but also efficiently evaluate all the candidate corrections to ensure the finding of a globally optimal correction. Experiments on two query spelling correction datasets demonstrate that the proposed generalized HMM is effective for correcting multiple types of spelling errors. The results also show that it significantly outperforms the current approach for generating top-K candidate corrections, making it a better first-stage filter to enable any other complex spelling correction algorithm to have access to a better working set of candidate corrections as well as to cover splitting and concatenation errors, which no existing method in academic literature can correct.

#index 1879054
#* Learning to predict response times for online query scheduling
#@ Craig Macdonald;Nicola Tonellotto;Iadh Ounis
#t 2012
#c 13
#% 169817
#% 198335
#% 213786
#% 397125
#% 397161
#% 643566
#% 730065
#% 879611
#% 976948
#% 1089473
#% 1166469
#% 1173690
#% 1181094
#% 1190098
#% 1450846
#% 1558842
#% 1620189
#% 1642922
#% 1812718
#! Dynamic pruning strategies permit efficient retrieval by not fully scoring all postings of the documents matching a query -- without degrading the retrieval effectiveness of the top-ranked results. However, the amount of pruning achievable for a query can vary, resulting in queries taking different amounts of time to execute. Knowing in advance the execution time of queries would permit the exploitation of online algorithms to schedule queries across replicated servers in order to minimise the average query waiting and completion times. In this work, we investigate the impact of dynamic pruning strategies on query response times, and propose a framework for predicting the efficiency of a query. Within this framework, we analyse the accuracy of several query efficiency predictors across 10,000 queries submitted to in-memory inverted indices of a 50-million-document Web crawl. Our results show that combining multiple efficiency predictors with regression can accurately predict the response time of a query before it is executed. Moreover, using the efficiency predictors to facilitate online scheduling algorithms can result in a 22% reduction in the mean waiting time experienced by queries before execution, and a 7% reduction in the mean completion time experienced by users.

#index 1879055
#* Prefetching query results and its impact on search engines
#@ Simon Jonassen;B. Barla Cambazoglu;Fabrizio Silvestri
#t 2012
#c 13
#% 340888
#% 448194
#% 577302
#% 578337
#% 805864
#% 860861
#% 987215
#% 1190098
#% 1398162
#% 1399951
#% 1404875
#% 1450839
#% 1464869
#% 1536518
#% 1558842
#% 1587345
#% 1587384
#% 1598431
#% 1806024
#% 1834787
#! We investigate the impact of query result prefetching on the efficiency and effectiveness of web search engines. We propose offline and online strategies for selecting and ordering queries whose results are to be prefetched. The offline strategies rely on query log analysis and the queries are selected from the queries issued on the previous day. The online strategies select the queries from the result cache, relying on a machine learning model that estimates the arrival times of queries. We carefully evaluate the proposed prefetching techniques via simulation on a query log obtained from Yahoo! web search. We demonstrate that our strategies are able to improve various performance metrics, including the hit rate, query response time, result freshness, and query degradation rate, relative to a state-of-the-art baseline.

#index 1879056
#* Online result cache invalidation for real-time web search
#@ Xiao Bai;Flavio P. Junqueira
#t 2012
#c 13
#% 86532
#% 340888
#% 417842
#% 577302
#% 736649
#% 747117
#% 805864
#% 879609
#% 987215
#% 1019182
#% 1089473
#% 1190098
#% 1398162
#% 1399951
#% 1404875
#% 1450839
#% 1587345
#% 1598431
#% 1834787
#! Caches of results are critical components of modern Web search engines, since they enable lower response time to frequent queries and reduce the load to the search engine backend. Results in long-lived cache entries may become stale, however, as search engines continuously update their index to incorporate changes to the Web. Consequently, it is important to provide mechanisms that control the degree of staleness of cached results, ideally enabling the search engine to always return fresh results. In this paper, we present a new mechanism that identifies and invalidates query results that have become stale in the cache online. The basic idea is to evaluate at query time and against recent changes if cache hits have had their results have changed. For enhancing invalidation efficiency, the generation time of cached queries and their chronological order with respect to the latest index update are used to early prune unaffected queries. We evaluate the proposed approach using documents that change over time and query logs of the Yahoo! search engine. We show that the proposed approach ensures good query results (50% fewer stale results) and high invalidation accuracy (90% fewer unnecessary invalidations) compared to a baseline approach that makes invalidation decisions off-line. More importantly, the proposed approach induces less processing overhead, ensuring an average throughput 73% higher than that of the baseline approach.

#index 1879057
#* Learning to rank social update streams
#@ Liangjie Hong;Ron Bekkerman;Joseph Adler;Brian D. Davison
#t 2012
#c 13
#% 577224
#% 987226
#% 1035578
#% 1073982
#% 1108903
#% 1214623
#% 1214694
#% 1268491
#% 1291600
#% 1291617
#% 1300087
#% 1355024
#% 1355297
#% 1384223
#% 1417104
#% 1442575
#% 1456844
#% 1484274
#% 1560174
#% 1560356
#% 1560408
#% 1573367
#% 1586589
#% 1598363
#% 1606083
#% 1625358
#% 1625364
#% 1730808
#% 1746869
#! As online social media further integrates deeper into our lives, we spend more time consuming social update streams that come from our online connections. Although social update streams provide a tremendous opportunity for us to access information on-the-fly, we often complain about its relevance. Some of us are flooded with a steady stream of information and simply cannot process it in full. Ranking the incoming content becomes the only solution for the overwhelmed users. For some others, in contrast, the incoming information stream is pretty weak, and they have to actively search for relevant information which is quite tedious. For these users, augmenting their incoming content flow with relevant information from outside their first-degree network would be a viable solution. In that case, the problem of relevance becomes even more prominent. In this paper, we start an open discussion on how to build effective systems for ranking social updates from a unique perspective of LinkedIn -- the largest professional network in the world. More specifically, we address this problem as an intersection of learning to rank, collaborative filtering, and clickthrough modeling, while leveraging ideas from information retrieval and recommender systems. We propose a novel probabilistic latent factor model with regressions on explicit features and compare it with a number of non-trivial baselines. In addition to demonstrating superior performance of our model, we shed some light on the nature of social updates on LinkedIn and how users interact with them, which might be applicable to social update streams in general.

#index 1879058
#* Collaborative personalized tweet recommendation
#@ Kailong Chen;Tianqi Chen;Guoqing Zheng;Ou Jin;Enpeng Yao;Yong Yu
#t 2012
#c 13
#% 173879
#% 330687
#% 452563
#% 577224
#% 722904
#% 879588
#% 956521
#% 1074061
#% 1083671
#% 1130901
#% 1190066
#% 1214623
#% 1287235
#% 1355042
#% 1384223
#% 1399992
#% 1417104
#% 1470663
#% 1476470
#% 1482547
#% 1484274
#% 1535439
#% 1544083
#% 1560408
#% 1587427
#% 1591965
#% 1592235
#% 1598352
#% 1598397
#! Twitter has rapidly grown to a popular social network in recent years and provides a large number of real-time messages for users. Tweets are presented in chronological order and users scan the followees' timelines to find what they are interested in. However, an information overload problem has troubled many users, especially those with many followees and thousands of tweets arriving every day. In this paper, we focus on recommending useful tweets that users are really interested in personally to reduce the users' effort to find useful information. Many kinds of information on Twitter are available for helping recommendation, including the user's own tweet history, retweet history and social relations between users. We propose a method of making tweet recommendations based on collaborative ranking to capture personal interests. It can also conveniently integrate the other useful contextual information. Our final method considers three major elements on Twitter: tweet topic level factors, user social relation factors and explicit features such as authority of the publisher and quality of the tweet. The experiments show that all the proposed elements are important and our method greatly outperforms several baseline methods.

#index 1879059
#* Exploring social influence for recommendation: a generative model approach
#@ Mao Ye;Xingjie Liu;Wang-Chien Lee
#t 2012
#c 13
#% 124010
#% 260780
#% 266281
#% 301259
#% 330687
#% 342777
#% 577217
#% 729923
#% 733578
#% 813966
#% 878355
#% 915344
#% 955930
#% 1054718
#% 1055737
#% 1214661
#% 1227601
#% 1227602
#% 1273828
#% 1287243
#% 1355040
#% 1396104
#% 1399993
#% 1451243
#% 1476459
#% 1480830
#% 1536533
#% 1598366
#% 1650545
#% 1650569

#index 1879060
#* See-to-retrieve: efficient processing of spatio-visual keyword queries
#@ Chao Zhang;Lidan Shou;Ke Chen;Gang Chen
#t 2012
#c 13
#% 333854
#% 427199
#% 874993
#% 982560
#% 1181225
#% 1206801
#% 1206931
#% 1206997
#% 1328137
#% 1380973
#% 1464065
#% 1523828
#% 1581877
#% 1598537
#% 1786336
#! The wide proliferation of powerful smart phones equipped with multiple sensors, 3D graphical engine, and 3G connection has nurtured the creation of a new spectrum of visual mobile applications. These applications require novel data retrieval techniques which we call What-You-Retrieve-Is-What-You-See (WYRIWYS). However, state-of-the-art spatial retrieval methods are mostly distance-based and thus inapplicable for supporting WYRIWYS. Motivated by this problem, we propose a novel query called spatio-visual keyword (SVK) query, to support retrieving spatial Web objects that are both visually conspicuous and semantically relevant to the user. To capture the visual features of spatial Web objects with extents, we introduce a novel visibility metric which computes object visibility in a cumulative manner. We propose an incremental method called Complete Occlusion-map based Retrieval (COR) to answer SVK queries. This method exploits effective heuristics to prune the search space and construct a data structure called Occlusion-Map. Then the method adopts the best-first strategy to return relevant objects incrementally. Extensive experiments on real and synthetic data sets suggest that our method is effective and efficient when processing SVK queries.

#index 1879061
#* Placing images on the world map: a microblog-based enrichment approach
#@ Claudia Hauff;Geert-Jan Houben
#t 2012
#c 13
#% 262096
#% 340901
#% 340948
#% 397129
#% 1040837
#% 1132018
#% 1194140
#% 1227637
#% 1278585
#% 1355297
#% 1429406
#% 1450997
#% 1482254
#% 1528636
#% 1583918
#% 1583922
#% 1592024
#% 1607052
#% 1611804
#% 1643161
#% 1650298
#% 1697481
#% 1715211
#% 1805998
#! Estimating the geographic location of images is a task which has received increasing attention recently. Large numbers of images uploaded to platforms such as Flickr do not contain GPS-based latitude/longitude coordinates. Obtaining such geographic information is beneficial for a variety of applications including travelogues, visual place descriptions and personalized travel recommendations. While most works in this area only exploit an image's textual meta-data (tags, title, etc.) to estimate at what geographic location the image was taken, we consider an additional textual dimension: the image owner's traces on the social Web. Specifically, we hypothesize that information extracted from a person's microblog stream(s) can be utilized to improve the accuracy with which the geographic location of the images is estimated. In this paper, we investigate this hypothesis on the example of Twitter streams and find it to be confirmed. The median error distance in kilometres decreases by up to 67% in comparison to existing state-of-the-art. The best results are achieved when tweets that were posted up to two days before and after an image was taken are considered. Moreover, we also find another type of additional information useful: population density data.

#index 1879062
#* Where is who: large-scale photo retrieval by facial attributes and canvas layout
#@ Yu-Heng Lei;Yan-Ying Chen;Bor-Chun Chen;Lime Iida;Winston H. Hsu
#t 2012
#c 13
#% 520224
#% 812418
#% 902519
#% 903623
#% 955010
#% 997189
#% 1148333
#% 1197267
#% 1450863
#% 1543098
#% 1558464
#% 1587608
#% 1632598
#% 1648801
#% 1750393
#% 1750403
#! The ubiquitous availability of digital cameras has made it easier than ever to capture moments of life, especially the ones accompanied with friends and family. It is generally believed that most family photos are with faces that are sparsely tagged. Therefore, a better solution to manage and search in the tremendously growing personal or group photos is highly anticipated. In this paper, we propose a novel way to search for face photos by simultaneously considering attributes (e.g., gender, age, and race), positions, and sizes of the target faces. To better match the content and layout of the multiple faces in mind, our system allows the user to graphically specify the face positions and sizes on a query "canvas," where each attribute combination is defined as an icon for easier representation. As a secondary feature, the user can even place specific faces from the previous search results for appearance-based retrieval. The scenario has been realized on a tablet device with an intuitive touch interface. Experimenting with a large-scale Flickr dataset of more than 200k faces, the proposed formulation and joint ranking have made us achieve a hit rate of 0.420 at rank 100, significantly improving from 0.036 of the prior search scheme using attributes alone. We have also achieved an average running time of 0.0558 second by the proposed block-based indexing approach.

#index 1879063
#* Mining the web for points of interest
#@ Adam Rae;Vanessa Murdock;Adrian Popescu;Hugues Bouchard
#t 2012
#c 13
#% 464434
#% 480467
#% 766441
#% 809461
#% 815924
#% 854819
#% 855108
#% 855119
#% 869516
#% 967244
#% 997189
#% 1016371
#% 1132470
#% 1190103
#% 1190131
#% 1227637
#% 1400036
#% 1560379
#% 1949158
#! A point of interest (POI) is a focused geographic entity such as a landmark, a school, an historical building, or a business. Points of interest are the basis for most of the data supporting location-based applications. In this paper we propose to curate POIs from online sources by bootstrapping training data from Web snippets, seeded by POIs gathered from social media. This large corpus is used to train a sequential tagger to recognize mentions of POIs in text. Using Wikipedia data as the training data, we can identify POIs in free text with an accuracy that is 116% better than the state of the art POI identifier in terms of precision, and 50% better in terms of recall. We show that using Foursquare and Gowalla checkins as seeds to bootstrap training data from Web snippets, we can improve precision between 16% and 52%, and recall between 48% and 187% over the state-of-the-art. The name of a POI is not sufficient, as the POI must also be associated with a set of geographic coordinates. Our method increases the number of POIs that can be localized nearly three-fold, from 134 to 395 in a sample of 400, with a median localization accuracy of less than one kilometer.

#index 1879064
#* TwiNER: named entity recognition in targeted twitter stream
#@ Chenliang Li;Jianshu Weng;Qi He;Yuxia Yao;Anwitaman Datta;Aixin Sun;Bu-Sung Lee
#t 2012
#c 13
#% 279755
#% 815922
#% 939376
#% 983604
#% 1249541
#% 1275192
#% 1310635
#% 1338553
#% 1468142
#% 1478118
#% 1587398
#% 1591965
#% 1591966
#% 1592152
#% 1711864
#! Many private and/or public organizations have been reported to create and monitor targeted Twitter streams to collect and understand users' opinions about the organizations. Targeted Twitter stream is usually constructed by filtering tweets with user-defined selection criteria e.g. tweets published by users from a selected region, or tweets that match one or more predefined keywords. Targeted Twitter stream is then monitored to collect and understand users' opinions about the organizations. There is an emerging need for early crisis detection and response with such target stream. Such applications require a good named entity recognition (NER) system for Twitter, which is able to automatically discover emerging named entities that is potentially linked to the crisis. In this paper, we present a novel 2-step unsupervised NER system for targeted Twitter stream, called TwiNER. In the first step, it leverages on the global context obtained from Wikipedia and Web N-Gram corpus to partition tweets into valid segments (phrases) using a dynamic programming algorithm. Each such tweet segment is a candidate named entity. It is observed that the named entities in the targeted stream usually exhibit a gregarious property, due to the way the targeted stream is constructed. In the second step, TwiNER constructs a random walk model to exploit the gregarious property in the local context derived from the Twitter stream. The highly-ranked segments have a higher chance of being true named entities. We evaluated TwiNER on two sets of real-life tweets simulating two targeted streams. Evaluated using labeled ground truth, TwiNER achieves comparable performance as with conventional approaches in both streams. Various settings of TwiNER have also been examined to verify our global context + local context combo idea.

#index 1879065
#* Adaptive context features for toponym resolution in streaming news
#@ Michael D. Lieberman;Hanan Samet
#t 2012
#c 13
#% 177551
#% 268785
#% 400847
#% 427326
#% 508117
#% 729437
#% 766441
#% 855310
#% 855312
#% 885056
#% 939899
#% 1035400
#% 1075340
#% 1120965
#% 1135150
#% 1298864
#% 1332208
#% 1358026
#% 1358027
#% 1358032
#% 1358035
#% 1358036
#% 1399979
#% 1480777
#% 1480778
#% 1480855
#% 1560254
#% 1592024
#% 1598347
#% 1598418
#! News sources around the world generate constant streams of information, but effective streaming news retrieval requires an intimate understanding of the geographic content of news. This process of understanding, known as geotagging, consists of first finding words in article text that correspond to location names (toponyms), and second, assigning each toponym its correct lat/long values. The latter step, called toponym resolution, can also be considered a classification problem, where each of the possible interpretations for each toponym is classified as correct or incorrect. Hence, techniques from supervised machine learning can be applied to improve accuracy. New classification features to improve toponym resolution, termed adaptive context features, are introduced that consider a window of context around each toponym, and use geographic attributes of toponyms in the window to aid in their correct resolution. Adaptive parameters controlling the window's breadth and depth afford flexibility in managing a tradeoff between feature computation speed and resolution accuracy, allowing the features to potentially apply to a variety of textual domains. Extensive experiments with three large datasets of streaming news demonstrate the new features' effectiveness over two widely-used competing methods.

#index 1879066
#* Structural relationships for large-scale learning of answer re-ranking
#@ Aliaksei Severyn;Alessandro Moschitti
#t 2012
#c 13
#% 144031
#% 169729
#% 722925
#% 722926
#% 743284
#% 815272
#% 815896
#% 817421
#% 817422
#% 818251
#% 838398
#% 855277
#% 938695
#% 939358
#% 939617
#% 939636
#% 939837
#% 987236
#% 1083712
#% 1130832
#% 1251706
#% 1261594
#% 1288594
#% 1467033
#% 1470707
#% 1482224
#% 1496785
#% 1665151
#! Supervised learning applied to answer re-ranking can highly improve on the overall accuracy of question answering (QA) systems. The key aspect is that the relationships and properties of the question/answer pair composed of a question and the supporting passage of an answer candidate, can be efficiently compared with those captured by the learnt model. In this paper, we define novel supervised approaches that exploit structural relationships between a question and their candidate answer passages to learn a re-ranking model. We model structural representations of both questions and answers and their mutual relationships by just using an off-the-shelf shallow syntactic parser. We encode structures in Support Vector Machines (SVMs) by means of sequence and tree kernels, which can implicitly represent question and answer pairs in huge feature spaces. Such models together with the latest approach to fast kernel-based learning enabled the training of our rerankers on hundreds of thousands of instances, which previously rendered intractable for kernelized SVMs. The results on two different QA datasets, e.g., Answerbag and Jeopardy! data, show that our models deliver large improvement on passage re-ranking tasks, reducing the error in Recall of BM25 baseline by about 18%. One of the key findings of this work is that, despite its simplicity, shallow syntactic trees allow for learning complex relational structures, which exhibits a steep learning curve with the increase in the training size.

#index 1879067
#* Top-k learning to rank: labeling, ranking and evaluation
#@ Shuzi Niu;Jiafeng Guo;Yanyan Lan;Xueqi Cheng
#t 2012
#c 13
#% 129694
#% 187763
#% 262105
#% 267753
#% 309095
#% 312689
#% 340892
#% 577224
#% 734915
#% 823348
#% 840846
#% 983820
#% 987226
#% 987241
#% 1074044
#% 1074137
#% 1095876
#% 1292528
#% 1348075
#% 1400856
#% 1415710
#% 1456843
#% 1483087
#% 1536513
#% 1674801
#% 1674802
#! In this paper, we propose a novel top-k learning to rank framework, which involves labeling strategy, ranking model and evaluation measure. The motivation comes from the difficulty in obtaining reliable relevance judgments from human assessors when applying learning to rank in real search systems. The traditional absolute relevance judgment method is difficult in both gradation specification and human assessing, resulting in high level of disagreement on judgments. While the pairwise preference judgment, as a good alternative, is often criticized for increasing the complexity of judgment from O(n) to (n log n). Considering the fact that users mainly care about top ranked search results, we propose a novel top-k labeling strategy which adopts the pairwise preference judgment to generate the top k ordering items from n documents (i.e. top-k ground-truth) in a manner similar to that of HeapSort. As a result, the complexity of judgment is reduced to O(n log k). With the top-k ground-truth, traditional ranking models (e.g. pairwise or listwise models) and evaluation measures (e.g. NDCG) no longer fit the data set. Therefore, we introduce a new ranking model, namely FocusedRank, which fully captures the characteristics of the top-k ground-truth. We also extend the widely used evaluation measures NDCG and ERR to be applicable to the top-k ground-truth, referred as κ-NDCG and κ-ERR, respectively. Finally, we conduct extensive experiments on benchmark data collections to demonstrate the efficiency and effectiveness of our top-k labeling strategy and ranking models.

#index 1879068
#* Robust ranking models via risk-sensitive optimization
#@ Lidan Wang;Paul N. Bennett;Kevyn Collins-Thompson
#t 2012
#c 13
#% 169780
#% 262096
#% 397161
#% 810906
#% 818262
#% 840846
#% 879651
#% 956552
#% 987229
#% 987260
#% 991164
#% 1074065
#% 1130990
#% 1227589
#% 1227591
#% 1227634
#% 1227666
#% 1292550
#% 1355030
#% 1357833
#% 1399946
#% 1442577
#% 1482279
#% 1536578
#% 1537504
#% 1560391
#% 1598343
#% 1598347
#% 1598350
#% 1641961
#% 1872523
#! Many techniques for improving search result quality have been proposed. Typically, these techniques increase average effectiveness by devising advanced ranking features and/or by developing sophisticated learning to rank algorithms. However, while these approaches typically improve average performance of search results relative to simple baselines, they often ignore the important issue of robustness. That is, although achieving an average gain overall, the new models often hurt performance on many queries. This limits their application in real-world retrieval scenarios. Given that robustness is an important measure that can negatively impact user satisfaction, we present a unified framework for jointly optimizing effectiveness and robustness. We propose an objective that captures the tradeoff between these two competing measures and demonstrate how we can jointly optimize for these two measures in a principled learning framework. Experiments indicate that ranking models learned this way significantly decreased the worst ranking failures while maintaining strong average effectiveness on par with current state-of-the-art models.

#index 1879069
#* Dual role model for question recommendation in community question answering
#@ Fei Xu;Zongcheng Ji;Bin Wang
#t 2012
#c 13
#% 234992
#% 280819
#% 329569
#% 452563
#% 643007
#% 722904
#% 813966
#% 818299
#% 879593
#% 1019165
#% 1055679
#% 1055738
#% 1074061
#% 1127462
#% 1130900
#% 1183154
#% 1190249
#% 1260273
#% 1273828
#% 1292541
#% 1356185
#% 1399976
#% 1541728
#% 1587342
#% 1606052
#% 1642174
#% 1650545
#! Question recommendation that automatically recommends a new question to suitable users to answer is an appealing and challenging problem in the research area of Community Question Answering (CQA). Unlike in general recommender systems where a user has only a single role, each user in CQA can play two different roles (dual roles) simultaneously: as an asker and as an answerer. To the best of our knowledge, this paper is the first to systematically investigate the distinctions between the two roles and their different influences on the performance of question recommendation in CQA. Moreover, we propose a Dual Role Model (DRM) to model the dual roles of users effectively. With different indepen-dence assumptions, two variants of DRM are achieved. Finally, we present the DRM based approach to question recommendation which provides a mechanism for naturally integrating the user relation between the answerer and the asker with the content re-levance between the answerer and the question into a uni-fied probabilistic framework. Experiments using a real-world data crawled from Yahoo! Answers show that: (1) there are evident distinctions between the two roles of users in CQA. Additionally, the answerer role is more effective than the asker role for modeling candidate users in question recommendation; (2) compared with baselines utilizing a single role or blended roles based methods, our DRM based approach consistently and significantly improves the performance of question recommendation, demonstrating that our approach can model the user in CQA more reasonably and precisely.

#index 1879070
#* Vote calibration in community question-answering systems
#@ Bee-Chung Chen;Anirban Dasgupta;Xuanhui Wang;Jie Yang
#t 2012
#c 13
#% 190265
#% 956516
#% 1019165
#% 1035587
#% 1047396
#% 1055738
#% 1083720
#% 1166519
#% 1190060
#% 1227599
#% 1281893
#% 1482364
#% 1536523
#% 1536937
#% 1598376
#! User votes are important signals in community question-answering (CQA) systems. Many features of typical CQA systems, e.g. the best answer to a question, status of a user, are dependent on ratings or votes cast by the community. In a popular CQA site, Yahoo! Answers, users vote for the best answers to their questions and can also thumb up or down each individual answer. Prior work has shown that these votes provide useful predictors for content quality and user expertise, where each vote is usually assumed to carry the same weight as others. In this paper, we analyze a set of possible factors that indicate bias in user voting behavior -- these factors encompass different gaming behavior, as well as other eccentricities, e.g., votes to show appreciation of answerers. These observations suggest that votes need to be calibrated before being used to identify good answers or experts. To address this problem, we propose a general machine learning framework to calibrate such votes. Through extensive experiments based on an editorially judged CQA dataset, we show that our supervised learning method of content-agnostic vote calibration can significantly improve the performance of answer ranking and expert ranking.

#index 1879071
#* Category hierarchy maintenance: a data-driven approach
#@ Quan Yuan;Gao Cong;Aixin Sun;Chin-Yew Lin;Nadia Magnenat Thalmann
#t 2012
#c 13
#% 280492
#% 280817
#% 330767
#% 341704
#% 465747
#% 633682
#% 730047
#% 766458
#% 783483
#% 783539
#% 807374
#% 816066
#% 835018
#% 838515
#% 881494
#% 1292729
#% 1482401
#% 1642212
#% 1740996
#% 1787746
#! Category hierarchies often evolve at a much slower pace than the documents reside in. With newly available documents kept adding into a hierarchy, new topics emerge and documents within the same category become less topically cohesive. In this paper, we propose a novel automatic approach to modifying a given category hierarchy by redistributing its documents into more topically cohesive categories. The modification is achieved with three operations (namely, sprout, merge, and assign) with reference to an auxiliary hierarchy for additional semantic information; the auxiliary hierarchy covers a similar set of topics as the hierarchy to be modified. Our user study shows that the modified category hierarchy is semantically meaningful. As an extrinsic evaluation, we conduct experiments on document classification using real data from Yahoo! Answers and AnswerBag hierarchies, and compare the classification accuracies obtained on the original and the modified hierarchies. Our experiments show that the proposed method achieves much larger classification accuracy improvement compared with several baseline methods for hierarchy modification.

#index 1879072
#* When web search fails, searchers become askers: understanding the transition
#@ Qiaoling Liu;Eugene Agichtein;Gideon Dror;Yoelle Maarek;Idan Szpektor
#t 2012
#c 13
#% 397161
#% 590523
#% 754059
#% 879613
#% 987263
#% 1035587
#% 1055738
#% 1130878
#% 1173692
#% 1183152
#% 1292474
#% 1355038
#% 1384287
#% 1399965
#% 1450833
#% 1456294
#% 1592168
#% 1598367
#% 1598368
#% 1598375
#% 1598531
#% 1693899
#! While Web search has become increasingly effective over the last decade, for many users' needs the required answers may be spread across many documents, or may not exist on the Web at all. Yet, many of these needs could be addressed by asking people via popular Community Question Answering (CQA) services, such as Baidu Knows, Quora, or Yahoo! Answers. In this paper, we perform the first large-scale analysis of how searchers become askers. For this, we study the logs of a major web search engine to trace the transformation of a large number of failed searches into questions posted on a popular CQA site. Specifically, we analyze the characteristics of the queries, and of the patterns of search behavior that precede posting a question; the relationship between the content of the attempted queries and of the posted questions; and the subsequent actions the user performs on the CQA site. Our work develops novel insights into searcher intent and behavior that lead to asking questions to the community, providing a foundation for more effective integration of automated web search and social information seeking.

#index 1879073
#* Content-based retrieval for heterogeneous domains: domain adaptation by relative aggregation points
#@ Makoto P. Kato;Hiroaki Ohshima;Katsumi Tanaka
#t 2012
#c 13
#% 308745
#% 411762
#% 466263
#% 766409
#% 855602
#% 940440
#% 989592
#% 1074096
#% 1074129
#% 1083678
#% 1190135
#% 1261539
#% 1292566
#% 1357833
#% 1400008
#% 1450849
#% 1464068
#% 1472892
#% 1482373
#% 1587349
#% 1598345
#% 1598427
#! We introduce the problem of domain adaptation for content-based retrieval and propose a domain adaptation method based on relative aggregation points (RAPs). Content-based retrieval including image retrieval and spoken document retrieval enables a user to input examples as a query, and retrieves relevant data based on the similarity to the examples. However, input examples and relevant data can be dissimilar, especially when domains from which the user selects examples and from which the system retrieves data are different. In content-based geographic object retrieval, for example, suppose that a user who lives in Beijing visits Kyoto, Japan, and wants to search for relatively inexpensive restaurants serving popular local dishes by means of a content-based retrieval system. Since such restaurants in Beijing and Kyoto are dissimilar due to the difference in the average cost and areas' popular dishes, it is difficult to find relevant restaurants in Kyoto based on examples selected in Beijing. We propose a solution for this problem by assuming that RAPs in different domains correspond, which may be dissimilar but play the same role. A RAP is defined as the expectation of instances in a domain that are classified into a certain class, e.g. the most expensive restaurant, average restaurant, and restaurant serving the most popular dishes. Our proposed method constructs a new feature space based on RAPs estimated in each domain and bridges the domain difference for improving content-based retrieval in heterogeneous domains. To verify the effectiveness of our proposed method, we evaluated various methods with a test collection developed for content-based geographic object retrieval. Experimental results show that our proposed method achieved significant improvements over baseline methods. Moreover, we observed that the search performance of content-based retrieval in heterogeneous domains was significantly lower than that in homogeneous domains. This finding suggests that relevant data for the same search intent depend on the search context, that is, the location where the user searches and the domain from which the system retrieves data.

#index 1879074
#* Mixture model with multiple centralized retrieval algorithms for result merging in federated search
#@ Dzung Hong;Luo Si
#t 2012
#c 13
#% 33917
#% 227891
#% 262063
#% 280856
#% 287463
#% 319273
#% 340899
#% 342680
#% 397199
#% 643012
#% 722312
#% 789959
#% 1016164
#% 1174737
#% 1292595
#% 1392444
#% 1482223
#% 1598489
#% 1598876
#% 1616865
#% 1618643
#! Result merging is an important research problem in federated search for merging documents retrieved from multiple ranked lists of selected information sources into a single list. The state-of-the-art result merging algorithms such as Semi-Supervised Learning (SSL) and Sample-Agglomerate Fitting Estimate (SAFE) try to map document scores retrieved from different sources to comparable scores according to a single centralized retrieval algorithm for ranking those documents. Both SSL and SAFE arbitrarily select a single centralized retrieval algorithm for generating comparable document scores, which is problematic in a heterogeneous federated search environment, since a single centralized algorithm is often suboptimal for different information sources. Based on this observation, this paper proposes a novel approach for result merging by utilizing multiple centralized retrieval algorithms. One simple approach is to learn a set of combination weights for multiple centralized retrieval algorithms (e.g., logistic regression) to compute comparable document scores. The paper shows that this simple approach generates suboptimal results as it is not flexible enough to deal with heterogeneous information sources. A mixture probabilistic model is thus proposed to learn more appropriate combination weights with respect to different types of information sources with some training data. An extensive set of experiments on three datasets have proven the effectiveness of the proposed new approach.

#index 1879075
#* Reactive index replication for distributed search engines
#@ Flavio P. Junqueira;Vincent Leroy;Matthieu Morel
#t 2012
#c 13
#% 433941
#% 466086
#% 987215
#% 1075132
#% 1132154
#% 1227628
#% 1292508
#% 1355057
#% 1450840
#% 1536562
#% 1598430
#% 1598433
#% 1641920
#! Distributed search engines comprise multiple sites deployed across geographically distant regions, each site being specialized to serve the queries of local users. When a search site cannot accurately compute the results of a query, it must forward the query to other sites. This paper considers the problem of selecting the documents indexed by each site focusing on replication to increase the fraction of queries processed locally. We propose RIP, an algorithm for replicating documents and posting lists that is practical and has two important features. RIP evaluates user interests in an online fashion and uses only local data of a site. Being an online approach simplifies the operational complexity, while locality enables higher performance when processing queries and documents. The decision procedure, on top of being online and local, incorporates document popularity and user queries, which is critical when assuming a replication budget for each site. Having a replication budget reflects the hardware constraints of any given site. We evaluate RIP against the approach of replicating popular documents statically, and show that we achieve significant gains, while having the additional benefit of supporting incremental indexes.

#index 1879076
#* Personalized diversification of search results
#@ David Vallet;Pablo Castells
#t 2012
#c 13
#% 232703
#% 262112
#% 642975
#% 805841
#% 879618
#% 879686
#% 1019149
#% 1074133
#% 1127465
#% 1166473
#% 1227591
#% 1396090
#% 1400011
#% 1400021
#% 1409929
#% 1493738
#% 1587349
#% 1598506
#% 1625357
#% 1697422
#% 1697448
#% 1742130
#! Search personalization and diversification are often seen as opposing alternatives to cope with query uncertainty, where, given an ambiguous query, it is either preferable to adapt the search result to a specific aspect that may interest the user (personalization) or to regard multiple aspects in order to maximize the probability that some query aspect is relevant to the user (diversification). In this work, we question this antagonistic view, and hypothesize that these two directions may in fact be effectively combined and enhance each other. We research the introduction of the user as an explicit random variable in state of the art diversification methods, thus developing a generalized framework for personalized diversification. In order to evaluate our hypothesis, we conduct an evaluation with real users using crowdsourcing services. The obtained results suggest that the combination of personalization and diversification achieves competitive performance, improving the base-line, plain personalization, and plain diversification approaches in terms of both diversity and accuracy measures.

#index 1879077
#* Combining implicit and explicit topic representations for result diversification
#@ Jiyin He;Vera Hollink;Arjen de Vries
#t 2012
#c 13
#% 262096
#% 262112
#% 280819
#% 290482
#% 310567
#% 330617
#% 397162
#% 642975
#% 722904
#% 789959
#% 879618
#% 987222
#% 1055677
#% 1074025
#% 1074133
#% 1130899
#% 1166473
#% 1227591
#% 1292528
#% 1292596
#% 1355020
#% 1400011
#% 1400021
#% 1400099
#% 1468142
#% 1536552
#% 1550713
#% 1621236
#% 1641928
#% 1641944
#% 1642193
#! Result diversification deals with ambiguous or multi-faceted queries by providing documents that cover as many subtopics of a query as possible. Various approaches to subtopic modeling have been proposed. Subtopics have been extracted internally, e.g., from retrieved documents, and externally, e.g., from Web resources such as query logs. Internally modeled subtopics are often implicitly represented, e.g., as latent topics, while externally modeled subtopics are often explicitly represented, e.g., as reformulated queries. We propose a framework that: i)combines both implicitly and explicitly represented subtopics; and ii)allows flexible combination of multiple external resources in a transparent and unified manner. Specifically, we use a random walk based approach to estimate the similarities of the explicit subtopics mined from a number of heterogeneous resources: click logs, anchor text, and web n-grams. We then use these similarities to regularize the latent topics extracted from the top-ranked documents, i.e., the internal (implicit) subtopics. Empirical results show that regularization with explicit subtopics extracted from the right resource leads to improved diversification results, indicating that the proposed regularization with (explicit) external resources forms better (implicit) topic models. Click logs and anchor text are shown to be more effective resources than web n-grams under current experimental settings. Combining resources does not always lead to better results, but achieves a robust performance. This robustness is important for two reasons: it cannot be predicted which resources will be most effective for a given query, and it is not yet known how to reliably determine the optimal model parameters for building implicit topic models.

#index 1879078
#* Using preference judgments for novel document retrieval
#@ Praveen Chandar;Ben Carterette
#t 2012
#c 13
#% 262105
#% 642975
#% 818257
#% 1074133
#% 1105903
#% 1150163
#% 1166473
#% 1292528
#% 1312812
#% 1415710
#% 1450898
#% 1587348
#% 1648026
#! There has been considerable interest in incorporating diversity in search results to account for redundancy and the space of possible user needs. Most work on this problem is based on subtopics: diversity rankers score documents against a set of hypothesized subtopics, and diversity rankings are evaluated by assigning a value to each ranked document based on the number of novel (and redundant) subtopics it is relevant to. This can be seen as modeling a user who is always interested in seeing more novel subtopics, with progressively decreasing interest in seeing the same subtopic multiple times. We put this model to test: if it is correct, then users, when given a choice, should prefer to see a document that has more value to the evaluation. We formulate some specific hypotheses from this model and test them with actual users in a novel preference-based design in which users express a preference for document A or document B given document C. We argue that while the user study shows the subtopic model is good, there are many other factors apart from novelty and redundancy that may be influencing user preferences. From this, we introduce a new framework to construct an ideal diversity ranking using only preference judgments, with no explicit subtopic judgments whatsoever.

#index 1879079
#* Quality through flow and immersion: gamifying crowdsourced relevance assessments
#@ Carsten Eickhoff;Christopher G. Harris;Arjen P. de Vries;Padmini Srinivasan
#t 2012
#c 13
#% 561315
#% 577224
#% 751818
#% 766409
#% 857180
#% 860013
#% 860015
#% 879598
#% 1065099
#% 1150163
#% 1264744
#% 1292493
#% 1478132
#% 1526565
#% 1536499
#% 1567960
#% 1576309
#% 1587349
#% 1587350
#% 1588371
#% 1598440
#% 1666684
#! Crowdsourcing is a market of steadily-growing importance upon which both academia and industry increasingly rely. However, this market appears to be inherently infested with a significant share of malicious workers who try to maximise their profits through cheating or sloppiness. This serves to undermine the very merits crowdsourcing has come to represent. Based on previous experience as well as psychological insights, we propose the use of a game in order to attract and retain a larger share of reliable workers to frequently-requested crowdsourcing tasks such as relevance assessments and clustering. In a large-scale comparative study conducted using recent TREC data, we investigate the performance of traditional HIT designs and a game-based alternative that is able to achieve high quality at significantly lower pay rates, facing fewer malicious submissions.

#index 1879080
#* An IR-based evaluation framework for web search query segmentation
#@ Rishiraj Saha Roy;Niloy Ganguly;Monojit Choudhury;Srivatsan Laxman
#t 2012
#c 13
#% 312689
#% 818262
#% 1019133
#% 1055706
#% 1074134
#% 1195837
#% 1227747
#% 1310431
#% 1450970
#% 1560191
#% 1560364
#% 1598362
#! This paper presents the first evaluation framework for Web search query segmentation based directly on IR performance. In the past, segmentation strategies were mainly validated against manual annotations. Our work shows that the goodness of a segmentation algorithm as judged through evaluation against a handful of human annotated segmentations hardly reflects its effectiveness in an IR-based setup. In fact, state-of the-art algorithms are shown to perform as good as, and sometimes even better than human annotations a fact masked by previous validations. The proposed framework also provides us an objective understanding of the gap between the present best and the best possible segmentation algorithm. We draw these conclusions based on an extensive evaluation of six segmentation strategies, including three most recent algorithms, vis-a-vis segmentations from three human annotators. The evaluation framework also gives insights about which segments should be necessarily detected by an algorithm for achieving the best retrieval results. The meticulously constructed dataset used in our experiments has been made public for use by the research community.

#index 1879081
#* On per-topic variance in IR evaluation
#@ Stephen E. Robertson;Evangelos Kanoulas
#t 2012
#c 13
#% 318407
#% 453327
#% 879614
#% 879630
#% 879631
#% 1263583
#% 1641985
#% 1806004
#! We explore the notion, put forward by Cormack & Lynam and Robertson, that we should consider a document collection used for Cranfield-style experiments as a sample from some larger population of documents. In this view, any per-topic metric (such as average precision) should be regarded as an estimate of that metric's true value for that topic in the full population, and therefore as carrying its own per-topic variance or estimate precision or noise. As in the two mentioned papers, we explore this notion by simulating other samples from the same large population. We investigate different ways of performing this simulation. One use of this analysis is to refine the notion of statistical significance of a difference between two systems (in most such analyses, each per-topic measurement is treated as equally precise). We propose a mixed-effects model method to measure significance, and compare it experimentally with the traditional t-test.

#index 1879082
#* An uncertainty-aware query selection model for evaluation of IR systems
#@ Mehdi Hosseini;Ingemar J. Cox;Natasa Milic-Frayling;Milad Shokouhi;Emine Yilmaz
#t 2012
#c 13
#% 197394
#% 262097
#% 262102
#% 340890
#% 879598
#% 879632
#% 907496
#% 987252
#% 987265
#% 1074132
#% 1278067
#% 1450896
#% 1587347
#% 1622345
#% 1641988
#% 1697427
#% 1806006
#! We propose a mathematical framework for query selection as a mechanism for reducing the cost of constructing information retrieval test collections. In particular, our mathematical formulation explicitly models the uncertainty in the retrieval effectiveness metrics that is introduced by the absence of relevance judgments. Since the optimization problem is computationally intractable, we devise an adaptive query selection algorithm, referred to as Adaptive, that provides an approximate solution. Adaptive selects queries iteratively and assumes that no relevance judgments are available for the query under consideration. Once a query is selected, the associated relevance assessments are acquired and then used to aid the selection of subsequent queries. We demonstrate the effectiveness of the algorithm on two TREC test collections as well as a test collection of an online search engine with 1000 queries. Our experimental results show that the queries chosen by Adaptive produce reliable performance ranking of systems. The ranking is better correlated with the actual systems ranking than the rankings produced by queries that were selected using the considered baseline methods.

#index 1879083
#* Improving retrieval of short texts through document expansion
#@ Miles Efron;Peter Organisciak;Katrina Fenlon
#t 2012
#c 13
#% 262096
#% 280851
#% 337235
#% 340901
#% 534447
#% 730070
#% 750863
#% 766408
#% 766430
#% 818204
#% 940042
#% 960414
#% 1024551
#% 1174736
#% 1227665
#% 1392432
#% 1581252
#% 1587369
#% 1598383
#% 1598423
#% 1622368
#% 1641955
#% 1692327
#% 1737056
#! Collections containing a large number of short documents are becoming increasingly common. As these collections grow in number and size, providing effective retrieval of brief texts presents a significant research problem. We propose a novel approach to improving information retrieval (IR) for short texts based on aggressive document expansion. Starting from the hypothesis that short documents tend to be about a single topic, we submit documents as pseudo-queries and analyze the results to learn about the documents themselves. Document expansion helps in this context because short documents yield little in the way of term frequency information. However, as we show, the proposed technique helps us model not only lexical properties, but also temporal properties of documents. We present experimental results using a corpus of microblog (Twitter) data and a corpus of metadata records from a federated digital library. With respect to established baselines, results of these experiments show that applying our proposed document expansion method yields significant improvements in effectiveness. Specifically, our method improves the lexical representation of documents and the ability to let time influence retrieval.

#index 1879084
#* Extending BM25 with multiple query operators
#@ Roi Blanco;Paolo Boldi
#t 2012
#c 13
#% 109192
#% 169781
#% 262096
#% 268079
#% 722904
#% 750863
#% 783474
#% 799246
#% 818255
#% 818262
#% 850430
#% 879565
#% 879651
#% 1035577
#% 1055706
#% 1227614
#% 1227747
#% 1250507
#% 1268491
#% 1338596
#% 1343447
#% 1355019
#% 1442580
#% 1450848
#% 1450901
#% 1450970
#% 1476442
#% 1598362
#% 1649629
#% 1806011
#! Traditional probabilistic relevance frameworks for informational retrieval refrain from taking positional information into account, due to the hurdles of developing a sound model while avoiding an explosion in the number of parameters. Nonetheless, the well-known BM25F extension of the successful Okapi ranking function can be seen as an embryonic attempt in that direction. In this paper, we proceed along the same line, defining the notion of virtual region: a virtual region is a part of the document that, like a BM25F-field, can provide a (larger or smaller, depending on a tunable weighting parameter) evidence of relevance of the document; differently from BM25F fields, though, virtual regions are generated implicitly by applying suitable (usually, but not necessarily, positional-aware) operators to the query. This technique fits nicely in the eliteness model behind BM25 and provides a principled explanation to BM25F; it specializes to BM25(F) for some trivial operators, but has a much more general appeal. Our experiments (both on standard collections, such as TREC, and on Web-like repertoires) show that the use of virtual regions is beneficial for retrieval effectiveness.

#index 1879085
#* Rhetorical relations for information retrieval
#@ Christina Lioma;Birger Larsen;Wei Lu
#t 2012
#c 13
#% 340934
#% 397129
#% 411762
#% 449747
#% 719598
#% 729026
#% 742218
#% 766409
#% 816183
#% 817605
#% 988197
#% 1182965
#% 1227657
#% 1330512
#% 1338545
#% 1450857
#% 1536512
#% 1543971
#% 1549087
#% 1642038
#% 1648028
#% 1650855
#% 1711726
#% 1711739
#% 1732622
#! Typically, every part in most coherent text has some plausible reason for its presence, some function that it performs to the overall semantics of the text. Rhetorical relations, e.g. contrast, cause, explanation, describe how the parts of a text are linked to each other. Knowledge about this so-called discourse structure has been applied successfully to several natural language processing tasks. This work studies the use of rhetorical relations for Information Retrieval (IR): Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a document's rhetorical relations be useful to IR? We present a language model modification that considers rhetorical relations when estimating the relevance of a document to a query. Empirical evaluation of different versions of our model on TREC settings shows that certain rhetorical relations can benefit retrieval effectiveness notably (10% in mean average precision over a state-of-the-art baseline).

#index 1879086
#* Modeling higher-order term dependencies in information retrieval using query hypergraphs
#@ Michael Bendersky;W. Bruce Croft
#t 2012
#c 13
#% 35937
#% 144034
#% 169781
#% 169809
#% 169811
#% 218978
#% 232677
#% 253191
#% 262096
#% 328532
#% 411760
#% 413592
#% 577224
#% 649567
#% 750863
#% 766428
#% 766430
#% 766464
#% 818239
#% 818262
#% 840846
#% 891559
#% 976952
#% 987229
#% 987272
#% 987356
#% 1019124
#% 1074103
#% 1074112
#% 1173205
#% 1195837
#% 1227608
#% 1227614
#% 1227647
#% 1355019
#% 1415722
#% 1450848
#% 1482203
#% 1482361
#% 1482362
#% 1598394
#% 1641915
#% 1715627
#! Many of the recent, and more effective, retrieval models have incorporated dependencies between the terms in the query. In this paper, we advance this query representation one step further, and propose a retrieval framework that models higher-order term dependencies, i.e., dependencies between arbitrary query concepts rather than just query terms. In order to model higher-order term dependencies, we represent a query using a hypergraph structure -- a generalization of a graph, where a (hyper)edge connects an arbitrary subset of vertices. A vertex in a query hypergraph corresponds to an individual query concept, and a dependency between a subset of these vertices is modeled through a hyperedge. An extensive empirical evaluation using both newswire and web corpora demonstrates that query representation using hypergraphs is highly beneficial for verbose natural language queries. For these queries, query hypergraphs significantly improve the retrieval effectiveness of several state-of-the-art models that do not employ higher-order term dependencies.

#index 1879087
#* Confidence-aware graph regularization with heterogeneous pairwise features
#@ Yuan Fang;Bo-June (Paul) Hsu;Kevin Chen-Chuan Chang
#t 2012
#c 13
#% 844287
#% 879581
#% 961218
#% 987221
#% 987222
#% 1002316
#% 1051181
#% 1074093
#% 1074198
#% 1227577
#% 1227648
#% 1292540
#% 1598339
#! Conventional classification methods tend to focus on features of individual objects, while missing out on potentially valuable pairwise features that capture the relationships between objects. Although recent developments on graph regularization exploit this aspect, existing works generally assume only a single kind of pairwise feature, which is often insufficient. We observe that multiple, heterogeneous pairwise features can often complement each other and are generally more robust in modeling the relationships between objects. Furthermore, as some objects are easier to classify than others, objects with higher initial classification confidence should be weighed more towards classifying related but more ambiguous objects, an observation missing from previous graph regularization techniques. In this paper, we propose a Dirichlet-based regularization framework that supports the combination of heterogeneous pairwise features with confidence-aware prediction using limited labeled training data. Next, we showcase a few applications of our framework in information retrieval, focusing on the problem of query intent classification. Finally, we demonstrate through a series of experiments the advantages of our framework on a large-scale real-world dataset.

#index 1879088
#* A utility-theoretic ranking method for semi-automated text classification
#@ Giacomo Berardi;Andrea Esuli;Fabrizio Sebastiani
#t 2012
#c 13
#% 219051
#% 280817
#% 311034
#% 344447
#% 565531
#% 722797
#% 748738
#% 750863
#% 765519
#% 799753
#% 869526
#% 939773
#% 961194
#% 1074139
#% 1095876
#% 1195838
#% 1263573
#% 1289281
#% 1583736
#% 1682421
#! In Semi-Automated Text Classification (SATC) an automatic classifier F labels a set of unlabelled documents D, following which a human annotator inspects (and corrects when appropriate) the labels attributed by F to a subset D' of D, with the aim of improving the overall quality of the labelling. An automated system can support this process by ranking the automatically labelled documents in a way that maximizes the expected increase in effectiveness that derives from inspecting D. An obvious strategy is to rank D so that the documents that F has classified with the lowest confidence are top-ranked. In this work we show that this strategy is suboptimal. We develop a new utility-theoretic ranking method based on the notion of inspection gain, defined as the improvement in classification effectiveness that would derive by inspecting and correcting a given automatically labelled document. We also propose a new effectiveness measure for SATC-oriented ranking methods, based on the expected reduction in classification error brought about by partially inspecting a list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method above, and according to the proposed measure, our ranking method can achieve substantially higher expected reductions in classification error.

#index 1879089
#* Improving tweet stream classification by detecting changes in word probability
#@ Kyosuke Nishida;Takahide Hoshide;Ko Fujimura
#t 2012
#c 13
#% 143306
#% 204531
#% 465754
#% 466408
#% 544049
#% 735077
#% 751623
#% 876061
#% 879596
#% 915314
#% 998561
#% 1035585
#% 1055665
#% 1073941
#% 1130831
#% 1183558
#% 1205332
#% 1223316
#% 1378110
#% 1400018
#% 1450867
#% 1450992
#% 1587353
#% 1591944
#% 1711776
#% 1815033
#% 1826316
#% 1916419
#! We propose a classification model of tweet streams in Twitter, which are representative of document streams whose statistical properties will change over time. Our model solves several problems that hinder the classification of tweets; in particular, the problem that the probabilities of word occurrence change at different rates for different words. Our model switches between two probability estimates based on full and recent data for each word when detecting changes in word probability. This switching enables our model to achieve both accurate learning of stationary words and quick response to bursty words. We then explain how to implement our model by using a word suffix array, which is a full-text search index. Using the word suffix array allows our model to handle the temporal attributes of word n-grams effectively. Experiments on three tweet data sets demonstrate that our model offers statistically significant higher topic-classification accuracy than conventional temporally-aware classification models.

#index 1879090
#* Predicting quality flaws in user-generated content: the case of wikipedia
#@ Maik Anderka;Benno Stein;Nedim Lipka
#t 2012
#c 13
#% 209021
#% 309150
#% 376266
#% 400847
#% 449259
#% 629583
#% 633437
#% 727883
#% 751850
#% 791711
#% 838472
#% 860125
#% 912094
#% 956520
#% 1001096
#% 1019083
#% 1035587
#% 1048267
#% 1055812
#% 1108867
#% 1168662
#% 1181461
#% 1190277
#% 1213445
#% 1400087
#% 1415780
#% 1495149
#% 1536512
#% 1560148
#% 1642242
#% 1743920
#! The detection and improvement of low-quality information is a key concern in Web applications that are based on user-generated content; a popular example is the online encyclopedia Wikipedia. Existing research on quality assessment of user-generated content deals with the classification as to whether the content is high-quality or low-quality. This paper goes one step further: it targets the prediction of quality flaws, this way providing specific indications in which respects low-quality content needs improvement. The prediction is based on user-defined cleanup tags, which are commonly used in many Web applications to tag content that has some shortcomings. We apply this approach to the English Wikipedia, which is the largest and most popular user-generated knowledge source on the Web. We present an automatic mining approach to identify the existing cleanup tags, which provides us with a training corpus of labeled Wikipedia articles. We argue that common binary or multiclass classification approaches are ineffective for the prediction of quality flaws and hence cast quality flaw prediction as a one-class classification problem. We develop a quality flaw model and employ a dedicated machine learning approach to predict Wikipedia's most important quality flaws. Since in the Wikipedia setting the acquisition of significant test data is intricate, we analyze the effects of a biased sample selection. In this regard we illustrate the classifier effectiveness as a function of the flaw distribution in order to cope with the unknown (real-world) flaw-specific class imbalances. The flaw prediction performance is evaluated with 10,000 Wikipedia articles that have been tagged with the ten most frequent quality flaws: provided test data with little noise, four flaws can be detected with a precision close to 1.

#index 1879091
#* A knowledge-based approach for summarising opinions
#@ Marco Bonzanini
#t 2012
#c 13
#% 262094
#% 1127964
#! Automatic Document Summarisation plays a central role in the process of providing the user with a quick access to information. Applications range from the generation of news headlines, to the aggregation of opinions extracted from reviews. Traditional topic-based summarisation systems are not always able to capture the sentiments expressed in a review. Major efforts in sentiment analysis have been put in the tasks of mining and classifying reviews according to their polarity. In this research, we investigate the use of summarisation techniques applied to reviews, and we propose a knowledge-based approach to summarisation, in the context of sentiment analysis. The proposed research is focused on three different aspects. Firstly, we investigate the application of summarisation techniques to sentiment classification. Capturing the key passage of a review can be beneficial for both a sentiment classifier, and for a user who could potentially understand the polarity of a review without reading the full text. Secondly, we investigate how to combine knowledge extracted from the reviews or integrated from external sources, with the purpose of producing opinion-oriented summaries. Thirdly, we analyse the possibility of generating personalised (user-oriented or query-biased) opinion-based summaries.

#index 1879092
#* Adaptive IR for exploratory search support
#@ Daniel T.J. Backhausen
#t 2012
#c 13
#% 857478
#! Most Information Retrieval (IR) software is designed to fit a general user where users are submitting queries and the retrieval system returns a ranked list of results. Regardless of the user, the query always returns the same list of results. Individual aspects like age, gender, profession or experience are often not taken into account, for example the difference in searching between children and adults. Although long challenged by works such as Bates' berrypicking model [1], common systems still assume that the user has a static information need which remains unchanged during the seeking process. Moreover many systems are strongly optimized for lookup searches, expecting that the user is only interested in facts and not in complex problem solving. But in many everyday situations people search for information to gain knowledge which allows them to fulfill a specific work task (e.g., [3]), like answering research questions, investigating for a publication or thesis, comparing different products or learning a language. Such complex tasks can be divided into sub-tasks and generally include multiple exploratory search sessions, in which the user strongly interacts with the system. This is a longitudinal process where the searcher necessarily gathers, collects, aggregates, interprets, processes, and evaluates information objects from one or more sources. In such complex search scenarios all three activities lookup, learn, and investigate are used in conjunction with one another to bridge the users knowledge gap [2]. In each step of this process, the user faces a new situation in which knowledge and information need changes. This inuences the relevance of information objects and may direct the user to different topics, domains, or also tasks. The goal of this research is to effectively assist at fulfilling complex (work) tasks consisting of multi-session exploratory search activities. To achieve this, information retrieval needs personalization and has to close the gaps between the different search sessions. This can be done by enabling the user to collect information objects into a personal reference library and visualizing past search activities in a kind of breadcrumb or time line. Thinking one step further, a personalized IR system (PIR) has to adapt to relevant factors and commit itself to the specific user and the personal search behavior. This means the system needs to guide the user through the searching process, suggesting useful search actions like effective search strategies or query formulations and has to recommend information objects relevant to the work task and the users current situation. Thereby the system has to be aware of the user and specific contextual circumstances. General information about the user like gender or age can be fetched explicitly, allowing to adapt in a more coarse grained way (i.e. decide the way of presenting results based on the user group). Moreover integrating used applications or providing other ways to let the user explicitly manage tasks will help to understand the goal of the users search activities and will provide much better ways of user assistance. To close the gap between user and system, both behavioral and contextual information are necessary. Information about the search behavior and indirectly the users knowledge and expertise can be conveyed by logging (e.g. query logs) and examining system interactions. The fetched data should be made transparent to the user, showing what kind of information has been gathered so far. The implicit information has to be refined with other contextual information collected implicitly from different interfaces or sensors (e.g. time, location) and explicitly by direct user input from e.g. relevance feedback interactions. This will allow a more fine grained way of system adaption and offers new options in assisting the user during the long-term search activities showing personalized search strategies and possible next steps appropriate to the information need and level of experience.

#index 1879093
#* Adversarial content manipulation effects
#@ Fiana Raiber
#t 2012
#c 13
#% 268079
#% 330787
#% 340901
#% 413592
#% 590524
#% 766430
#% 912202
#! We address a question that has been somewhat overlooked throughout the transition from classical ad hoc retrieval to Web search: how is the performance of classical retrieval approaches affected by the presence of content manipulation? Our initial experiments have shown that the relative performance patterns of some classical retrieval strategies might change in the transition from non-manipulated to manipulated corpora. A natural future venue to explore is how to mix these strategies and make (some of) them more robust under presumed content manipulation conditions.

#index 1879094
#* Building reputation and trust using federated search and opinion mining
#@ Somayeh Khatiban
#t 2012
#c 13
#% 334325
#% 1074402
#% 1194137
#% 1338592
#% 1384287
#% 1400683
#% 1451030
#! The term online reputation addresses trust relationships amongst agents in dynamic open systems. These can appear as ratings, recommendations, referrals and feedback. Several reputation models and rating aggregation algorithms have been proposed. However, finding a trusted entity on the web is still an issue as all reputation systems work individually. The aim of this project is to introduce a global reputation system for electronic product reviews that aggregates people's opinions from different resources (e.g. e-commerce websites, and review) with the help federated search techniques and generate a high quality and trusted result. The first step is to choose a range of product review collections from e-commerce review systems (e.g. Amazon), online review sites (e.g. Epinions), social networks (e.g. Facebook), question and answering sites (e.g. Yahoo! Answers), and blog (e.g. My Nokia Blog) resources. By using a federated search approach the query (product name) will be broadcasted to the selected resources and the result will be a list of reputation data with various formats including star rating, text reviews, voting, video, and so on. The focus of this work is on review text data and star ratings. A number of challenges including comparison issues (e.g. scale of star ratings: five-star vs. ten-star), hierarchical reviews (e.g. comments about reviews), choice of resources (e.g. choosing relevant sources deepens upon query), display issue (e.g. easy for the user), generalization issue (e.g. apply it on other domains), synchronization problem (e.g. generate up-to-date results), and high quality and trusted reviews will be addressed. A sentiment analysis approach is subsequently used to extract high quality opinions and inform how to increase trust in the search result. The extracted opinions will be used to generate facets for the global reputation system.

#index 1879095
#* Enhancing knowledge base with knowledge transfer
#@ Si-Chi Chin
#t 2012
#c 13
#% 1464068
#! A Knowledge Base (KB) stores, organizes, and shares information pertinent to entities (i.e. KB nodes) such as people, organizations, and events. A large KB system, such as Wikipedia, relies on human curators to create and maintain the content in the systems. However, it has become challenging for human curators to sift through the rapidly growing amount of information and filter out the information irrelevant to a KB node. The area of Knowledge Base Enhancement (KBE) aims to explore and identify automatic methods to assist human curators to accelerate the process. KBE can be viewed as a special case of Information Filtering (IF). However, the lack of high-quality labelled data introduces a major challenge to train a satisfying model for the task. Transfer learning provides solutions to the problem and has explored applications in the area of text mining, whereas direct application to KBE or IF remains absent. Transfer learning is a research area in machine learning, emphasizing the reuse of previously acquired knowledge to another applicable task. The method is particularly useful in the situations where labeled instances are absent or difficult to obtain. To accelerate the growth of a KB, a transfer learning approach enables leveraging the heuristics and models learned from one KB node to another. For example, reusing the learned filtering models from Willie Nelson, a famous country singer, to Eddie Rabbitt, another country singer. Transfer learning requires three components: the target task (e.g. the problem to be solved), the source task(s) (e.g. auxiliary data, previously studied problem), and criteria to select appropriate source tasks. The objectives of my dissertation are twofold. First, it explores methods to identify informative source nodes from which to transfer. Second, it constructs a knowledge transfer network to represent the transfer learning relationship between KB nodes. This proposed research applies a transfer learning method -- Segmented Transfer (ST) -- and a knowledge representation -- Knowledge Transfer Network (KTN) -- to approach the area of KBE. The primary research questions include: What are the transferable objects in information filtering algorithms? What are the KB nodes of high transferability? What are the factors that determine the transfer learning relationship? Does it manifest on a knowledge transfer network representation? This interdisciplinary research crosses the study area of information filtering, machine learning, knowledge representation, and network analysis. This proposal motivates the problem of KBE, discusses the research methodology and proposed experiments, and reviews related works in information filtering and transfer learning. This line of research hopes to extend the application of transfer learning to KBE and to explore a new dimension of IF. The proposed ST and KTN intends to bring interdisciplinary approaches in the emerging field of KBE.

#index 1879096
#* Improving e-discovery using information retrieval
#@ Kripabandhu Ghosh
#t 2012
#c 13
#% 1598416
#% 1648449
#! E-discovery is the requirement that the documents and information in electronic form stored in corporate systems be produced as evidence in litigation. It has posed great challenges for legal experts. Legal searchers have always looked to find "any and all" evidence for a given case. Thus, a legal search system would essentially be a recall-oriented system. It has been a common practice among expert searchers to formulate Boolean queries to represent their information need. We want to work on three basic problems: Boolean query formulation - Our primary goal is to study Boolean query formulation in the light of the E-discovery task. This will include automatic Boolean query generation, expansion and learning the effect of proximity operators in Boolean searches. Data fusion - We would also like to explore the effectiveness of data fusion techniques in improving recall. Error modeling - Finally, we will work on error modeling methods for noisy legal documents.

#index 1879097
#* Opinion influence and diffusion in social network
#@ Dehong Gao
#t 2012
#c 13
#% 1280713
#! Nowadays, more and more people tend to make decisions based on the opinion information from the Internet, in addition to recommendations from offline friends or parents. For example, we may browse the resumes and comments on election candidates to determine if one candidate is qualified, or consult the consumer reports or reviews on special e-commercial websites to decide which brand of computer is suitable for one's needs. Though opinion information is rich on the Internet, [2] points out that 58% of American Internet users deem that online information is irretrievable, confusing, or conflicting with each other. Early works on opinion mining help to classify opinion polarity, to extract specific opinions and to summarize opinion texts. However, all these works are usually based on plain texts (reviews, comments or news articles). With the explosion of Web 2.0 applications, especially social network applications like blogs, discussion forums, micro-blogs, the massive individual users go to the major media websites, which leads to much more opinion materials posted on the Internet by user-shared experiences or views [3]. These opinion-rich and social network-based applications bring new perspectives for opinion mining as well. First, in addition to plain texts (reviews, newswire) in traditional opinion mining, we see new types of cyber-based text, like personal diary blogs, cyber-SMS tweets. Second, if we regard the opinions in plain text as static, the dynamic change of opinions in the social network is a new promising area, and catch increasing attention of worldwide researchers. In the social network, the opinion held by one individual is not static, but changes, which can be influenced by others. A serial of changes among different users forms the opinion propagation or diffusion in the network. This paper and my doctoral work focus on the opinion influence and diffusion in the social network, which explore the detailed process of one-to-one influence and the opinion diffusion process in the social network. The significance of this work is it can benefit many other related researches, like information maximum, viral marketing. Now some pioneering works have been conducted to investigate the role of social networks in information diffusion and influencers in the social network. These works are usually based on information diffusion models, like the cascade model (CM) or epidemic model (EM). However, we argue that it is not enough to simply apply these models to opinion influence and diffusion. 1) For both CM and EM, status shift is along specific directions, from inactive to active (CM) or from susceptible to infectious, and then, to recovered (EM). But opinion influence is more complex.

#index 1879098
#* Relevance as a subjective and situational multidimensional concept
#@ Carsten Eickhoff
#t 2012
#c 13
#% 235918
#% 720198
#% 783474
#! Relevance is the central concept of information retrieval. Although its important role is unanimously accepted among researchers, numerous different definitions of the term have emerged over the years. Considerable effort has been put into creating consistent and universally applicable descriptions of relevance in the form of relevance frameworks. Across these various formal systems of relevance, a wide range of relevance criteria has been identified. The probably most frequently used single criterion, that in some applications even becomes a synonym for relevance, is topicality. It expresses a document's topical overlap with the user's information need. For textual resources, it is often estimated based on term co-occurrences between query and document. There is, however, a significant number of further noteworthy relevance criteria. Prominent specimen are: (Currency) determines how recent and up to date the document is. Outdated information may have become invalid over time. (Availability) expresses how easy it is to obtain the document. Users might not want to invest more than a threshold amount of resources (e.g., disk space, downloading time or money) to get the document. (Readability) describes the document's readability and understandability. A document with a high topical relevance towards a given information need can become irrelevant if the user is not able to extract the desired information from it. (Credibility) contains criteria such as the document author's expertise, the publication's reputation and the document's general trustworthiness. (Novelty) describes the document's contribution to satisfying an information need with respect to the user's context. E.g., previous search results or general knowledge about the domain. It is evident that these criteria can have very different scopes. Some of them are static characteristics of the document or the author, others depend on the concrete information need at hand or even the user's search context. Currently, state-of-the-art retrieval models often treat relevance (regardless which interpretation of the term was chosen) as an atomic concept that can be expressed through topical overlap between document and query or a plain linear combination of multiple scores. Considering the broad audiences a web search engine has to serve, such a method does not seem optimal as the concrete composition of relevance will vary from person to person depending on social and educational context. Furthermore, each individual can be expected to have situational preference for certain combinations of relevance facets depending on the information need at hand. We investigate combination schemes which respect the dimension-specific relevance distributions. In particular, we developed a risk-aware method of combining relevance criteria inspired by the economic Portfolio theory. As a first stage, we applied this method for result set diversification across dimensions.

#index 1879099
#* Exploiting temporal topic models in social media retrieval
#@ Tuan A. Tran
#t 2012
#c 13
#% 1536561
#% 1560257
#% 1560381
#% 1694383
#! Many of user generated contents in the Web 2.0 center around real-world incidents such as Japanese tsunami, or general concerns such as recent economic downturn. Such type of information is always of interest to users. For instance, when a user reads a news article about a tsunami in Japan, she wants to see related Flickr photos or more tweets about it. Conventional keyword-based search is inappropriate, since it is not always trivial to formulate ad-hoc interests about the event and material. In some cases, the user might want to explore emerging topics that dominate different sources. Present systems fail to connect topically documents across media, and the user has to examine individual sources to infer the topics herself. In this work, we address a special type of user information need, temporal topic, which refers to any abstract matter active within some points or periods of time. A temporal topic can be a real-world event, e.g. the Arab Spring revolution, but can also be a less conceivable subject, e.g. the study of vacuum tube computers in 1950s. Topics can also be recurrent such as the US presidency campaigns. There are extensive studies on how to detect topics from a collection of documents, but little uses temporal topics as part of user interest to retrieve documents. We believe that temporal topic-based retrieval is a one solution to improve user experience of present IR systems, as well as to benefit other applications (e.g. topic-sensitive online advertisement). Our research goal can be defined in three research questions. The first question involves finding latent temporal topics in a social media stream, where documents are well equipped with meta-data (timestamps, geo-spatial data, etc.). Following mixture models such as LDA, we treat each document as a mix of different temporal topic models, each model is incorporated with time. A temporal topic consists of at least two types of attributes - time and representing words, as similar to [4]. The dynamics of temporal topics can be characterized in a timeline fashion [4], or using hierarchical structures [1]. The challenge lies in devising a model flexible enough to diverse and rapidly changing data without many parameter assumptions. For this, we see Bayesian nonpara-metrics [3] as one promising solution, and will extend it to temporal dimension. The second research question is how to retrieve and rank documents from different social media sites, based on their relevance to one or several given temporal topics. We identify some following challenges. The first one is representing temporal topics as queries: although there have been attempts using keywords and time window separately [2], we aim to unify time and (topical) words in a single query model. The second challenge is integrating temporal topic models into ranking models. Inspired by our previous work [4], we will use language models to capture the relevance scores between documents and topics, and investigate advanced methods to index the scores effectively. Our last question involves connecting a given document to documents in other sources (data streams or corpora) that shared one of its latent temporal topics. This task does not only provide unified insight into different social media sites, but also help improve the quality of models by data in diverse sources. However, formalizing the semantics of "similarity" for documents in different settings based on temporal topcis is tricky. One baseline method is to apply Kullback-Leibler divergence on comparable features (TF-IDF, n-grams, photo tags, timestamps,..). We can also use language models [5] to construct a language model for each candidate document, then estimate how likely it generates the document of interest within a given temporal topic.

#index 1879100
#* The essence of time: considering temporal relevance as an intent-aware ranking problem
#@ Stewart Whiting
#t 2012
#c 13
#% 642975
#% 1536521
#! Real-time news and social media quickly reflect large-scale phenomena and events. As users become exposed to this information, time plays a central role in prompting both information authorship and seeking activities. The objective of this research is to develop a retrieval system which can anticipate a user's likely temporal intent(s), considering recent or ongoing real-world events. Such a system should not only provide recent news when relevant, but also higher rank non-timestamped or even older documents which are temporally pertinent as they cover aspects related to recent event topics. Key challenges to be addressed in this work include: a suitable source and method for event detection and tracking, an intent-aware ranking approach and an evaluation methodology.

#index 1879101
#* A framework for manipulating and searching multiple retrieval types
#@ Marc-Allen Cartright;Ethem F. Can;William Dabney;Jeff Dalton;Logan Giorda;Kriste Krstovski;Xiaoye Wu;Ismet Zeki Yalniz;James Allan;R. Manmatha;David A. Smith
#t 2012
#c 13
#% 642983
#% 859913
#! Conventional retrieval systems view documents as a unit and look at different retrieval types within a document. We introduce Proteus, a frame-work for seamlessly navigating books as dynamic collections which are defined on the fly. Proteus allows us to search various retrieval types. Navigable types include pages, books, named persons, locations, and pictures in a collection of books taken from the Internet Archive. The demonstration shows the value of multi-type browsing in dynamic collections to peruse new data.

#index 1879102
#* A visual tool for bayesian data analysis: the impact of smoothing on naive bayes text classifiers
#@ Giorgio Maria Di Nunzio;Alessandro Sordoni
#t 2012
#c 13
#% 750863
#% 1215857
#! Naive-Bayes (NB) classifiers are simple probabilistic classifiers still widely used in supervised learning due to their tradeoff between efficient model training and good empirical results. One of the drawbacks of these classifiers is that in situations of data sparsity (i.e. when the size of training set is small) the maximum likelihood estimation of the probability of unseen features in these situations is equal to zero causing arithmetic anomalies. To prevent this undesirable behavior, a number of smoothing techniques have been proposed. Among these, the Bayesian approach incorporates smoothing in terms of prior knowledge about the parameters of the model usually called hyper-parameters. Our research question is: can a visualization tool help researchers to quickly assess the goodness of the performance of NB classifiers by setting optimal smoothing parameters?

#index 1879103
#* ALF: a client side logger and server for capturing user interactions in web applications
#@ Leif Azzopardi;Myles Doolan;Richard Glassey
#t 2012
#c 13
#% 869483
#% 1599370
#! This demonstration paper introduces ALF which provides a light-weight client side logging application and a server for collecting user interaction data. ALF has been designed as a loosely coupled independent service that runs in parallel with the IR web application that requires logging

#index 1879104
#* ChatNoir: a search engine for the ClueWeb09 corpus
#@ Martin Potthast;Matthias Hagen;Benno Stein;Jan Graßegger;Maximilian Michel;Martin Tippmann;Clement Welsch
#t 2012
#c 13
#% 783474
#% 1621236
#% 1642162
#! We present the ChatNoir search engine which indexes the entire English part of the ClueWeb09 corpus. Besides Carnegie Mellon's Indri system, ChatNoir is the second publicly available search engine for this corpus. It implements the classic BM25F information retrieval model including PageRank and spam likelihood. The search engine is scalable and returns the first results within three seconds, which is significantly faster than Indri. A convenient API allows for implementing reproducible experiments based on retrieving documents from the ClueWeb09 corpus. The search engine has successfully accomplished a load test involving 100,000 queries.

#index 1879105
#* CrowdTerrier: automatic crowdsourced relevance assessments with terrier
#@ Richard McCreadie;Craig Macdonald;Iadh Ounis
#t 2012
#c 13
#% 1969487
#! In this demo, we present CrowdTerrier, an infrastructure extension to the open source Terrier IR platform that enables the semi-automatic generation of relevance assessments for a variety of document ranking tasks using crowdsourcing. The aim of CrowdTerrier is to reduce the time and expertise required to effectively Crowdsource relevance assessments by abstracting away from the complexities of the crowdsourcing process. It achieves this by automating the assessment process as much as possible, via a close integration of the IR system that ranks the documents (Terrier) and the crowdsourcing marketplace that is used to assess those documents (Amazon's Mechanical Turk).

#index 1879106
#* Distilling and exploring nuggets from a corpus
#@ Vittorio Castelli;Hema Raghavan;Radu Florian;Ding-Jung Han;Xiaoqiang Luo;Salim Roukos
#t 2012
#c 13
#% 1567955
#! This paper describes a live and scalable system that automatically extracts information nuggets for entities/topics from a continuously updated corpus for effective exploration and analysis. A nugget is a piece of semantic information that (1) must be mapped semantically to the transitive closure of a pre-defined ontology, (2) is explicitly supported by text, and (3) has a natural language description that completely conveys its semantic to a user. Fig. 1 shows a type of nugget "involvement in events" for a person entity (Leon Panetta): each nugget has a short description ("meeting", "news conference") with a list of supporting passages. Our key contributions are (1) We extract nuggets and remove redundancy to produce a summary of salient information with supporting clusters of passages. (2) We present an entity/topic centric exploration interface that also allows users to navigate to other entities involved in a nugget. (3) We use the statistical NLP technologies developed over the years in the ACE ,GALE and TAC-KBP programs, including parsing, mention detection, within and cross document coreference resolution, relation detection and slot filler extraction. (4) Our system is flexible and easily adaptable across domains as demonstrated on two corpora: generic news and scientific papers. Search engines such as Google News and Scholar do not retrieve nuggets, and only remove redundancy at document level. News aggregation applications such as Evri categorize news articles based on the entities of topics but do not extract nuggets. Other systems extract richer information, but not all of it has clear semantics; e.g., Silobreaker presents results as "the relationship between X and Y in the context of [keyphrase]", leaving users with the task of interpreting the semantics as it is not tied to a clear ontology. In contrast we remove redundancy, summarize results and present nuggets that have clear semantics.

#index 1879107
#* Integrative online research-data management
#@ Michael Huggett;Edie Rasmussen
#t 2012
#c 13
#! In support of our research projects in information retrieval, we have developed an integrated multi-process software system that shepherds research data from induction through aggregation, analysis, and presentation. We combine public-domain code libraries with our own software to provide a flexible, easily- configured modular system that exposes data online for easier collaboration. The goal is to create a single online infrastructure that allows colleagues to submit, process, analyze and visualize data, and discuss and prioritize issues through a single integrated interface. We demonstrate our system within the context of the large data set provided by the Indexer's Legacy project [1].

#index 1879108
#* MaSe: create your own mash-up search interface
#@ Leif Azzopardi;Douglas Dowie;Kelly Ann Marshall;Richard Glassey
#t 2012
#c 13
#% 344923
#! MaSe provides a sandbox environment for high school students to create their own personalised search interface. It has been designed with two major goals in mind: (1) as a hands-on tutorial for school children, to excite them about programming and computing science through the development of a practical application, and (2) to enable children to design and create their own search interface without extensive programming knowledge or prior experience. Consequently, MaSe provides a way to ascertain what children would like from a search engine interface in an exploratory and creative way as they can create a working prototype. This approach contrasts with previous work on exploring children's requirements of IR systems which attempts to directly elicit user needs through more traditional methods (i.e. surveys, interviews, focus groups, etc). However, we have attempted to incorporate the design guidelines for children as identified by Large (2006) into MaSe, where: we make use of bright colours, large text fonts, spell checking and the use of icons to represent search services, as well as including a thematic experience as suggested by Large (2006), with the use of a puppy avatar and puppy dog footprints.

#index 1879109
#* myDJ: recommending karaoke songs from one's own voice
#@ Kuang Mao;Xinyuan Luo;Ke Chen;Gang Chen;Lidan Shou
#t 2012
#c 13
#% 983820
#! In this demo, we present myDJ, a karaoke recommendation system which recommends the songs people are capable to sing. Different from the existing song recommendation systems which recommend songs people like to listen, myDJ can recommend proper songs according to a subject's physical phonation area. It consists of a singer profiler to analyze the subject's phonation characters. In addition, the song profile for each song in database is extracted. To learn a ranking function, the learning to rank algorithm Listnet is applied under a list of predefined features extracted from each singer-song profile pair. In the results, proper songs which are suitable but challenging for the subject are recommended.

#index 1879110
#* PageFetch: a retrieval game for children (and adults)
#@ Leif Azzopardi;Jim Purvis;Richard Glassey
#t 2012
#c 13
#% 1292493
#% 1384136
#% 1642304
#! Children often struggle with information retrieval tasks as searching for information often requires a developed vocabulary and strong categorisation skills; neither of which are particularly developed in children under the age of 12. In a study conducted by Druin et al, it was found that in an experimental setting many children are often uninterested in searching for information online or are only interested in searching for information that is relevant to their personal interests. Consequently, children who were unmotivated were the least successful in completing information retrieval tasks in their study. It was suggested that a more effective means of engaging child participants in search studies must be developed in order to gain further insights into the searching behaviours of children. To this end we have developed a game called PageFetch which aims to engage children (aged 8 to 80) in completing search tasks through a fun and interactive search-like interface.

#index 1879111
#* Pictune: situational music recommendation from geotagged pictures
#@ Ke Chen;Gang Chen;Lidan Shou;Fei Xia
#t 2012
#c 13

#index 1879112
#* Political search trends
#@ Ingmar Weber;Venkata Rama Kiran Garimella;Erik Borra
#t 2012
#c 13
#% 1518009
#! We present Political Search Trends, a browser based web search analysis tool that (i) assigns a political leaning to web search queries, (ii) detects trending political queries in a given week, and (iii) links search queries to fact-checked statements. In terms of methodology, it showcases the power of analyzing queries leading to clicks on selected, annotated web sites of interest.

#index 1879113
#* RDF Xpress: a flexible expressive RDF search engine
#@ Shady Elbassuoni;Maya Ramanath;Gerhard Weikum
#t 2012
#c 13
#% 262112
#% 956564
#% 1292565
#% 1603792
#! We demonstrate RDF Xpress, a search engine that enables users to effectively retrieve information from large RDF knowledge bases or Linked Data Sources. RDF Xpress provides a search interface where users can combine triple patterns with keywords to form queries. Moreover, RDF Xpress supports automatic query relaxation and returns a ranked list of diverse query results.

#index 1879114
#* Sketch-based image similarity search with a pen and paper interface
#@ Ihab Al Kabary;Heiko Schuldt
#t 2012
#c 13
#% 1393474
#% 1806017
#! We present a novel and innovative user interface for query-by-sketching based image retrieval that exploits emergent interactive paper and digital pen technology. Users can draw sketches with a digital pen on interactive paper in a user-friendly way. The pen is able to capture the stroke vectors and to interactively stream them to the underlying content-based image retrieval (CBIR) system via the pen's Bluetooth interface. We present the integration of interactive paper/digital pen technology with QbS, our CBIR system tailored to Query-by-Sketching, and we demonstrate the use of the paper and pen interface together with QbS for three different collections: MIRFLICKR-25K, a cartoon collection, and a collection of medieval paper watermarks.

#index 1879115
#* Task-aware search assistant
#@ Henry Allen Feild;James Allan
#t 2012
#c 13
#% 284796
#% 1130868
#% 1130878
#% 1536532

#index 1879116
#* TweetSpector: entity-based retrieval of tweets
#@ Surender Reddy Yerva;Zoltan Miklos;Flavia Grosan;Alexandru Tandrau;Karl Aberer
#t 2012
#c 13
#! TweetSpector is a tool for demonstrating entity-based of retrieval of tweets. The various features of this tool include: entity profile creation, real-time tweet classification, active improvement of the created profiles through user feedback, and the dashboard displaying different metrics.

#index 1879117
#* YooSee: a video browsing application for young children
#@ Leif Azzopardi;Douglas Dowie;Kelly Ann Marshall
#t 2012
#c 13
#% 1210687
#! Nowadays children as young as two years old can easily interact with mobile touch screen devices and personal computers to watch online videos through services such as YouTube. However, such services present a number of challenges for young children (e.g. fine grain gestures/interactions and good typing/literacy skills). In addition, when children use such services there is a risk that they may stumble upon content that is inappropriate. YooSee is a web-based application developed using the PuppyIR framework and designed for children aged between two and six years old. YooSee enables children to: (1) search and browse through video content using an engaging, novel interaction paradigm, and (2) be able to safely enjoy moderated video content.

#index 1879118
#* Multi-platform image search using tag enrichment
#@ Jinming Min;Cristover Lopes;Johannes Leveling;Dag Schmidtke;Gareth J.F. Jones
#t 2012
#c 13
#% 1537475
#% 1537497
#! The number of images available online is growing steadily and current web search engines have indexed more than 10 billion images. Approaches to image retrieval are still often text-based and operate on image annotations and captions. Image annotations (i.e. image tags) are typically short, user-generated, and of varying quality, which increases the mismatch problem between query terms and image tags. For example, a user might enter the query "wedding dress" while all images are annotated with "bridal gown" or "wedding gown". This demonstration presents an image search system using reduction and expansion of image annotations to overcome vocabulary mismatch problems by enriching the sparse set of image tags. Our image search application accepts a written query as input and produces a ranked list of result images and annotations (i.e. image tags) as output. The system integrates methods to reduce and expand the image tag set, thus decreasing the effect of sparse image tags. It builds on different image collections such as the Wikipedia image collection (http://www.imageclef.org/wikidata) and the Microsoft Office.com ClipArt collection (http://office.microsoft.com/), but can be applied to social collections such as Flickr as well. Our demonstration system runs on PCs, tablets, and smartphones, making use of advanced user interface capabilities on mobile devices.

#index 1879119
#* IR paradigms in computational advertising
#@ Andrei Z. Broder
#t 2012
#c 13
#! The central problem in the emerging discipline of computational advertising is to find the "best match" between a given user in a given context and a suitable advertisement. The context could be a user entering a query in a search engine ("sponsored search"), a user reading a web page ("content match" and "display ads"), a user streaming a movie, and so on. In some situations, it is desirable to solve the "dual" optimization problem: rather then find the best ad given a user in a context, the goal is to identify the "best audience", i.e. the most receptive set of users and/or the most suitable contexts for a given advertising campaign. The information about the user can vary from scarily detailed to practically nil. The number of potential advertisements might be in the billions. Thus, depending on the definition of "best match" and "best audience" these problems lead to a variety of massive optimization problems, with complicated constraints, and challenging data representation and access issues. In general, the direct problem is solved in two stages: first a rough filtering is used to identify a relatively small set of ads to be considered as potential matches, followed by a more sophisticated secondary ranking where economics considerations take center stage. Historically, the filtering has been conceived as a database selection problem, and was done using simple Boolean formulae, for instance, in sponsored search the filter could be "all ads that provide a specific bid for the present query string or a subset of it". Similarly for the dual problem (audience definition) for, say, a sports car ad, the filter could be "all males in California, aged 40 or less". This "database approach" for the direct problem has been recently supplanted by an "IR approach" based on a similarity search between a carefully constructed query that captures the advertising opportunity and an annotated document corpus that represents the potential ads. Similarly, in the dual problem, the newer approach is to devise an efficient and effective representation of the users, then form a query that represents a prototypical ideal user, and finally find the users most similar to the prototype. The aim of this talk is to discuss the penetration of the IR paradigms in computational advertising and present some research challenges and opportunities in this area of enormous economic importance.

#index 1879120
#* Watson: the Jeopardy! challenge and beyond
#@ Eric W. Brown
#t 2012
#c 13
#! Watson, named after IBM founder Thomas J. Watson, was built by a team of IBM researchers who set out to accomplish a grand challenge-build a computing system that rivals a human's ability to answer questions posed in natural language with speed, accuracy and confidence. The quiz show Jeopardy! provided the ultimate test of this technology because the game's clues involve analyzing subtle meaning, irony, riddles and other complexities of natural language in which humans excel and computers traditionally fail. Watson passed its first test on Jeopardy!, beating the show's two greatest champions in a televised exhibition match, but the real test will be in applying the underlying natural language processing and analytics technology in business and across industries. In this talk I will introduce the Jeopardy! grand challenge, present an overview of Watson and the DeepQA technology upon which Watson is built, and explore future applications of this technology.

#index 1879121
#* Putting context into search and search into context
#@ Susan T. Dumais
#t 2012
#c 13
#! It is very challenging task to understand a short query, especially if that query is considered in isolation. Luckily, queries do magically appear in a search box -- rather, they are issued by real people, trying to accomplish a task, at a given point in time and space, and this "context" can be used to aid query understanding. Traditionally search engines have returned the same results to everyone who asks the same question. However, using a single ranking for everyone, in every context limits how well a search engine can do. In this talk I outline a framework to quantify the "potential for personalization", that can be used to characterize the extent to which different people have the same (or different) intents for a query. I then describe several examples of how we represent and use different kinds of context to improve search quality. Finally I conclude by highlighting some important challenges in developing such systems at Web scale including system optimization, evaluation, transparency and serendipity.

#index 1879122
#* CloudSearch and the democratization of information retrieval
#@ Daniel E. Rose
#t 2012
#c 13
#! Amazon CloudSearch is a new hosted search service, built on top of many cloud-based AWS services, and based on the same technology that powers search on Amazon's retail sites. Because of its ease of configuration and scalability, CloudSearch represents the next step in the democratization of information retrieval. This democratization process, increasing access to search for both end users and potential search providers, has continued over several decades, through technologies like early online metered search services, enterprise search software, web search, and open source search tools. CloudSearch further reduces barriers to entry, allowing a person or organization to basically say "make my content searchable" and have it happen automatically. CloudSearch may also offer an opportunity to overcome the stagnation that has occurred in search user experiences over the past 15 years. When you no longer need to be a search expert to make your content available, you're not stuck with ten blue links. Instead, you can focus on providing the kind of interaction that makes sense for your application and your users. CloudSearch enables a flowering of search applications that need not be tied to the web, and an opportunity to explore new ways of interacting with information retrieval technology.

#index 1879123
#* Entity sentiment extraction using text ranking
#@ John O'Neil
#t 2012
#c 13
#% 854646
#! Entity extraction and sentiment classification are among the most common types of information derived from documents, but the problem of directly associating entities and sentiment has received less attention. We use TextRank on a graph linking entities and sentiment-laden words and phrases. We extract from the resulting eigenvector the final sentiment weights of the entities. We then explore the algorithm's performance and accuracy, compared to a baseline.

#index 1879124
#* A hybrid model for ad-hoc information retrieval
#@ Zheng Ye;Jimmy Xiangji Huang;Jun Miao
#t 2012
#c 13
#% 1166534
#% 1292491
#% 1292526
#% 1567726
#! Many information retrieval (IR) techniques have been proposed to improve the performance, and some combinations of these techniques has been demonstrated to be effective. However, how to effectively combine them is largely unexplored. It is possible that a method reduces the positive influence of the other one even if both of them are effective separately. In this paper, we propose a new hybrid model which can simply and flexibly combine components of three different IR techniques under a uniform framework. Extensive experiments on the TREC standard collections indicate that our proposed model can outperform the best TREC systems consistently in the ad-hoc retrieval. It shows that the combination strategy in our proposed model is very effective. Meanwhile, this method is also re-useable for other researchers to test whether their new methods are additive to the current technologies.

#index 1879125
#* Exploiting paths for entity search in RDF graphs
#@ Minsuk Kahng;Sang-goo Lee
#t 2012
#c 13
#% 642992
#% 1400010
#% 1560278
#% 1641483
#% 1643139
#% 1806002
#! The field of entity search using Semantic Web (RDF) data has gained more interest recently. In this paper, we propose a probabilistic entity retrieval model for RDF graphs using paths in the graph. Unlike previous work which assumes that all descriptions of an entity are directly linked to the entity node, we assume that an entity can be described with any node that can be reached from the entity node by following paths in the RDF graph. Our retrieval model simulates the generation process of query terms from an entity node by traversing the graph. We evaluate our approach using a standard evaluation framework for entity search.

#index 1879126
#* A study of term weighting schemes using class information for text classification
#@ Youngjoong Ko
#t 2012
#c 13
#% 46803
#% 1250627
#% 1471319

#index 1879127
#* A topic model of clinical reports
#@ Corey Arnold;William Speier
#t 2012
#c 13
#% 642990
#% 722904
#% 881498
#% 1338620
#! Clinical narrative in the medical record provides perhaps the most detailed account of a patient's history. However, this information is documented in free-text, which makes it challenging to analyze. Efforts to index unstructured clinical narrative often focus on identifying predefined concepts from clinical terminologies. Less studied is the problem of analyzing the text as a whole to create temporal indices that capture relationships between learned clinical events. Topic models provide a method for analyzing large corpora of text to discover semantically related clusters of words. This work presents a topic model tailored to the clinical reporting environment that allows for individual patient timelines. Results show the model is able to identify patterns of clinical events in a cohort of brain cancer patients.

#index 1879128
#* Active query selection for learning rankers
#@ Mustafa Bilgic;Paul N. Bennett
#t 2012
#c 13
#% 879598
#% 1195853
#% 1227635
#% 1227673
#% 1450862
#! Methods that reduce the amount of labeled data needed for training have focused more on selecting which documents to label than on which queries should be labeled. One exception to this (Long et al. 2010) uses expected loss optimization (ELO) to estimate which queries should be selected but is limited to rankers that predict absolute graded relevance. In this work, we demonstrate how to easily adapt ELO to work with any ranker and show that estimating expected loss in DCG is more robust than NDCG even when the final performance measure is NDCG.

#index 1879129
#* Anticipatory search: using context to initiate search
#@ Daniel J. Liebling;Paul N. Bennett;Ryen W. White
#t 2012
#c 13
#% 766530
#% 797691
#% 805200
#% 818259
#% 838546
#% 987262
#% 1074215
#% 1131027
#% 1450885
#! Identifying content for which a user may search has a variety of applications, including ranking and recommendation. In this poster, we examine how pre-search context can be used to predict content that the user will seek before they have even specified a search query. We call this anticipatory search. Using a log-based approach, we compare different methods for predicting the content to be searched using different attributes of the pre-query context and behavioral signals from previous visitors to the most recent browse URL. Each method covers different cases and shows promise for query-free anticipatory search on the Web.

#index 1879130
#* BReK12: a book recommender for K-12 users
#@ Maria Soledad Pera;Yiu-Kai Ng
#t 2012
#c 13
#% 1399972
#% 1588390
#% 1646904
#% 1693254
#! Ideally, students in K-12 grade levels can turn to book recommenders to locate books that match their interests. Existing book recommenders, however, fail to take into account the readability levels of their users, and hence their recommendations may be unsuitable for the users. To address this issue, we introduce BReK12, a recommender that targets K-12 users and prioritizes the reading level of its users in suggesting books of interest. Empirical studies conducted using the Bookcrossing dataset show that BReK12 outperforms a number of existing recommenders (developed for general users) in identifying books appealing to K-12 users.

#index 1879131
#* Clarity re-visited
#@ Shay Hummel;Anna Shtok;Fiana Raiber;Oren Kurland;David Carmel
#t 2012
#c 13
#% 397161
#% 1130851
#% 1450964
#% 1467729
#% 1621236
#! We present a novel interpretation of Clarity [5], a widely used query performance predictor. While Clarity is commonly described as a measure of the "distance" between the language model of the top-retrieved documents and that of the collection, we show that it actually quantifies an additional property of the result list, namely, its diversity. This analysis, along with empirical evaluation, helps to explain the low prediction quality of Clarity for large-scale Web collections.

#index 1879132
#* Cluster-based one-class ensemble for classification problems in information retrieval
#@ Nedim Lipka;Benno Stein;Maik Anderka
#t 2012
#c 13
#% 251145
#% 551874
#% 770870
#% 855602
#% 915319
#% 1229248
#% 1781779
#% 1879090
#! A number of relevant information retrieval classification problems are one-class classification problems at heart. I.e., labeled data is only available for one class, the so-called target class, and common discrimination-based classification approaches, be them binary or multiclass, are not applicable. Achieving a high effectiveness when solving one-class problems is difficult anyway and it becomes even more challenging when the target class data is multimodal, which is often the case. To address these concerns we propose a cluster-based one-class ensemble that consists of four steps: (1) applying a clustering algorithm to the target class data, (2) training an individual one-class classifier for each of the identified clusters, (3) aggregating the decisions of the individual classifiers, and (4) selecting the best fitting clustering model. We evaluate our approach with four datasets: an artificially generated dataset, a dataset compiled from a known multiclass text corpus, and two datasets related to one-class problems that received much attention recently, namely authorship verification and quality flaw prediction. Our approach outperforms a one-class SVM on all four datasets.

#index 1879133
#* Collaborative filtering with short term preferences mining
#@ Diyi Yang;Tianqi Chen;Weinan Zhang;Yong Yu
#t 2012
#c 13
#% 1083671
#% 1214666
#! Recently, recommender systems have fascinated researchers and benefited a variety of people's online activities, enabling users to survive the explosive web information. Traditional collaborative filtering techniques handle the general recommendation well. However, most such approaches usually focus on long term preferences. To discover more short term factors influencing people's decisions, we propose a short term preferences model, implemented with implicit user feedback. We conduct experiments comparing the performances of different short term models, which show that our model outperforms significantly compared to those long term models.

#index 1879134
#* Creating temporally dynamic web search snippets
#@ Krysta M. Svore;Jaime Teevan;Susan T. Dumais;Anagha Kulkarni
#t 2012
#c 13
#% 262036
#% 1166523
#% 1183221
#% 1536521
#! Content on the Internet is always changing. We explore the value of biasing search result snippets towards new webpage content. We present results from a user study comparing traditional query-focused snippets with snippets that emphasize new page content for two query types: general and trending. Our results indicate that searchers prefer the inclusion of temporal information for trending queries but not for general queries, and that this is particularly valuable for pages that have not been recently crawled.

#index 1879135
#* Dependency trigram model for social relation extraction from news articles
#@ Maengsik Choi;Harksoo Kim;Bruce W. Croft
#t 2012
#c 13
#% 301241
#% 939944
#% 1558464
#! We propose a kernel-based model to automatically extract social relations such as economic relations and political relations between two people from news articles. To determine whether two people are structurally associated with each other, the proposed model uses an SVM (support vector machine) tree kernel based on trigrams of head-dependent relations between them. In the experiments with the automatic content extraction (ACE) corpus and a Korean news corpus, the proposed model outperformed the previous systems based on SVM tree kernels even though it used more shallow linguistic knowledge.

#index 1879136
#* Detecting candidate named entities in search queries
#@ Areej Alasiry;Mark Levene;Alexandra Poulovassilis
#t 2012
#c 13
#% 1019130
#% 1227610
#% 1560177
#% 1560364
#% 1591939
#! The information extraction task of Named Entities Recognition (NER) has been recently applied to search engine queries, in order to better understand their semantics. Here we concentrate on the task prior to the classification of the named entities (NEs) into a set of categories, which is the problem of detecting candidate NEs via the subtask of query segmentation.We present a novel method for detecting candidate NEs using grammar annotation and query segmentation with the aid of top-n snippets from search engine results and a web n-gram model, to accurately identify NE boundaries. The proposed method addresses the problem of accurately setting boundaries of NEs and the detection of multiple NEs in queries.

#index 1879137
#* Effect of dynamic pruning safety on learning to rank effectiveness
#@ Craig Macdonald;Nicola Tonellotto;Iadh Ounis
#t 2012
#c 13
#% 730065
#% 1268491
#% 1598342
#! A dynamic pruning strategy, such as WAND, enhances retrieval efficiency without degrading effectiveness to a given rank K, known as safe-to-rank-K. However, it is also possible for WAND to obtain more efficient but unsafe retrieval without actually significantly degrading effectiveness. On the other hand, in a modern search engine setting, dynamic pruning strategies can be used to efficiently obtain the set of documents to be re-ranked by the application of a learned model in a learning to rank setting. No work has examined the impact of safeness on the effectiveness of the learned model. In this work, we investigate the impact of WAND safeness through experiments using 150 TREC Web track topics. We find that unsafe WAND is biased towards documents with lower docids, thereby impacting effectiveness.

#index 1879138
#* Effect of written instructions on assessor agreement
#@ William Webber;Bryan Toth;Marjorie Desamito
#t 2012
#c 13
#% 312689
#% 1314931
#! Assessors frequently disagree on the topical relevance of documents. How much of this disagreement is due to ambiguity in assessment instructions? We have two assessors assess TREC Legal Track documents for relevance, some to a general topic description, others to detailed assessment guidelines. We find that detailed guidelines lead to no significant increase in agreement amongst assessors or between assessors and the official qrels.

#index 1879139
#* Effects of expertise differences in synchronous social Q&A
#@ Ryen W. White;Matthew Richardson
#t 2012
#c 13
#% 96303
#% 284937
#% 324129
#% 879570
#% 1399976
#% 1573677
#% 1587391
#! Synchronous social question-and-answer (Q&A) systems match askers to answerers and support real-time dialog between them to resolve questions. These systems typically find answerers based on the degree of expertise match with the asker's initial question. However, since synchronous social Q&A involves a dialog between asker and answerer, differences in expertise may also matter (e.g., extreme novices and experts may have difficulty establishing common ground). In this poster we use data from a live social Q&A system to explore the impact of expertise differences on answer quality and aspects of the dialog itself. The findings of our study suggest that synchronous social Q&A systems should consider the relative expertise of candidate answerers with respect to the asker, and offer interactive dialog support to help establish common ground between askers and answerers.

#index 1879140
#* Efficient estimation of aspect weights
#@ Jon Parker;Andrew Yates;Nazli Goharian;Wai Gen Yee
#t 2012
#c 13
#% 1214666
#% 1451218
#% 1592078
#% 1605982
#! Many websites encourage people to submit reviews of various products and services. We present and evaluate a novel approach to efficiently model and analyze the text within user reviews to estimate how much reviewers care about different aspects of a product (i.e., amenities, food, location, room, etc. of a hotel). Our approach performs statistically quite similar to the best existing method. However, our method for computing aspect weights is a linear time method while the current state of the art solution requires cubic time at best.

#index 1879141
#* Emotion tagging for comments of online news by meta classification with heterogeneous information sources
#@ Ying Zhang;Yi Fang;Xiaojun Quan;Lin Dai;Luo Si;Xiaojie Yuan
#t 2012
#c 13
#% 939926
#% 987301
#% 1583567
#% 1890014
#! With the rapid growth of online news services, users can actively respond to online news by making comments. Users often express subjective emotions in comments such as sadness, surprise and anger. Such emotions can help understand the preferences and perspectives of individual users, and therefore may facilitate online publishers to provide users with more relevant services. This paper tackles the task of predicting emotions for the comments of online news. To the best of our knowledge, this is the first research work for addressing the task. In particular, this paper proposes a novel Meta classification approach that exploits heterogeneous information sources such as the content of the comments and the emotion tags of news articles generated by users. The experiments on two datasets from online news services demonstrate the effectiveness of the proposed approach.

#index 1879142
#* Estimating the magic barrier of recommender systems: a user study
#@ Alan Said;Brijnesh J. Jain;Sascha Narr;Till Plumbaum;Sahin Albayrak;Christian Scheel
#t 2012
#c 13
#% 202009
#% 734590
#% 1038334
#% 1262992
#% 1287241
#! Recommender systems are commonly evaluated by trying to predict known, withheld, ratings for a set of users. Measures such as the Root-Mean-Square Error are used to estimate the quality of the recommender algorithms. This process does however not acknowledge the inherent rating inconsistencies of users. In this paper we present the first results from a noise measurement user study for estimating the magic barrier of recommender systems conducted on a commercial movie recommendation community. The magic barrier is the expected squared error of the optimal recommendation algorithm, or, the lowest error we can expect from any recommendation algorithm. Our results show that the barrier can be estimated by collecting the opinions of users on already rated items.

#index 1879143
#* Explaining neighborhood-based recommendations
#@ Sergio Cleger-Tamayo;Juan M. Fernandez-Luna;Juan F. Huete
#t 2012
#c 13
#% 319705
#% 1756064
#! Recommender Systems (RS) attempt to discover users' preferences, and to learn about them in order to anticipate their needs. The main task normally associated with a RS is to offer suggestions for items. However, for most users, RSs are black boxes, computerized oracles that give advice, but cannot be questioned. In order to improve the quality of predictions and the satisfaction of the users, explanations facilities are needed. We present a novel methodology to explain recommendations: showing predictions over a set of observed items. Our proposal has been validated by means of user studies and lab experiments using MovieLens dataset.

#index 1879144
#* Exploiting term dependence while handling negation in medical search
#@ Nut Limsopatham;Craig Macdonald;Richard McCreadie;Iadh Ounis
#t 2012
#c 13
#% 817436
#% 818262
#% 907525
#% 987356
#! In medical records, negative qualifiers, e.g. no or without, are commonly used by health practitioners to identify the absence of a medical condition. Without considering whether the term occurs in a negative or positive context, the sole presence of a query term in a medical record is insufficient to imply that the record is relevant to the query. In this paper, we show how to effectively handle such negation within a medical records information retrieval system. In particular, we propose a term representation that tackles negated language in medical records, which is further extended by considering the dependence of negated query terms. We evaluate our negation handling technique within the search task provided by the TREC Medical Records 2011 track. Our results, which show a significant improvement upon a system that does not consider negated context within records, attest the importance of handling negation.

#index 1879145
#* Exploring example-based person search in email
#@ Tan Xu;Douglas W. Oard
#t 2012
#c 13
#% 1019189
#% 1133171
#% 1382613
#! This paper describes an entity ranking model for example-based person search in email. Evaluation by comparison to manually resolved named references in Enron email yield results that correspond to typically placing the correct entity in the first or second rank.

#index 1879146
#* Exploring tag relevance for image tag re-ranking
#@ Jie Xiao;Wengang Zhou;Qi Tian
#t 2012
#c 13
#% 1190090
#% 1292880
#! In this paper, we propose to explore the relevance between tags for image tag re-ranking. The key component is to define a global tag-tag similarity matrix, which is achieved by analysis in both semantic and visual aspects. The text semantic relevance is explored by the Latent Semantic Indexing (LSI) model [1].For the visual information, the tag-relevance can be propagated by reconstructing exemplar images with visually and semantically consistent images. Based on our tag relevance matrix, a random-walk approach is leveraged to discover the significance of each tag. Finally, all tags in an image are re-ranked by their significance values. Extensive experiments show its effectiveness on an image dataset with a large tags vocabulary.

#index 1879147
#* Fast on-line learning for multilingual categorization
#@ Michelle Kovesi;Cyril Goutte;Massih-Reza Amini
#t 2012
#c 13
#% 763708
#% 1377376
#% 1450888
#! Multiview learning has been shown to be a natural and efficient framework for supervised or semi-supervised learning of multilingual document categorizers. The state-of-the-art co-regularization approach relies on alternate minimizations of a combination of language-specific categorization errors and a disagreement between the outputs of the monolingual text categorizers. This is typically solved by repeatedly training categorizers on each language with the appropriate regularizer. We extend and improve this approach by introducing an on-line learning scheme, where language-specific updates are interleaved in order to iteratively optimize the global cost in one pass. Our experimental results show that this produces similar performance as the batch approach, at a fraction of the computational cost.

#index 1879148
#* Finding interesting posts in Twitter based on retweet graph analysis
#@ Min-Chul Yang;Jung-Tae Lee;Seung-Wook Lee;Hae-Chang Rim
#t 2012
#c 13
#% 290830
#% 1560174
#% 1560422
#! Millions of posts are being generated in real-time by users in social networking services, such as Twitter. However, a considerable number of those posts are mundane posts that are of interest to the authors and possibly their friends only. This paper investigates the problem of automatically discovering valuable posts that may be of potential interest to a wider audience. Specifically, we model the structure of Twitter as a graph consisting of users and posts as nodes and retweet relations between the nodes as edges. We propose a variant of the HITS algorithm for producing a static ranking of posts. Experimental results on real world data demonstrate that our method can achieve better performance than several baseline methods.

#index 1879149
#* Finding readings for scientists from social websites
#@ Jiepu Jiang;Zhen Yue;Shuguang Han;Daqing He
#t 2012
#c 13
#% 879570
#% 1292730
#% 1588409
#! Current search systems are designed to find relevant articles, especially topically relevant ones, but the notion of relevance largely depends on search tasks. We study the specific task that scientists are searching for worth-reading articles beneficial for their research. Our study finds: users' perception of relevance and preference of reading are only moderately correlated; current systems can effectively find readings that are highly relevant to the topic, but 36% of the worth-reading articles are only marginally relevant or even non-relevant. Our system can effectively find those worth-reading but marginally relevant or non-relevant articles by taking advantages of scientists' recommendations in social websites.

#index 1879150
#* Finding web appearances of social network users via latent factor model
#@ Kailong Chen;Zhengdong Lu;Xiaoshi Yin;Yong Yu;Zaiqing Nie
#t 2012
#c 13
#% 805885
#% 1074054
#% 1450830
#% 1471184
#% 1598517
#! With the rapid growing of Web 2.0, people spend more time on social networks such as Facebook and Twitter. In order to know the people they are interacting with, finding the web appearances of them will help the social network users to a great extent. We propose a novel and effective latent factor model to find web appearances of target social network users. Our method solves the name ambiguity problem by simultaneously exploring the link structure of social networks and the web. Experiments on real-world data show the superiority of our method over several baselines.

#index 1879151
#* Fixed versus dynamic co-occurrence windows in TextRank term weights for information retrieval
#@ Wei Lu;Qikai Cheng;Christina Lioma
#t 2012
#c 13
#% 987349
#% 1578116
#% 1674717
#% 1722677
#! TextRank is a variant of PageRank typically used in graphs that represent documents, and where vertices denote terms and edges denote relations between terms. Quite often the relation between terms is simple term co-occurrence within a fixed window of k terms. The output of TextRank when applied iteratively is a score for each vertex, i.e. a term weight, that can be used for information retrieval (IR) just like conventional term frequency based term weights. So far, when computing TextRank term weights over co-occurrence graphs, the window of term co-occurrence is always fixed. This work departs from this, and considers dynamically adjusted windows of term co-occurrence that follow the document structure on a sentence- and paragraph-level. The resulting TextRank term weights are used in a ranking function that re-ranks 1000 initially returned search results in order to improve the precision of the ranking. Experiments with two IR collections show that adjusting the vicinity of term co-occurrence when computing TextRank term weights can lead to gains in early precision.

#index 1879152
#* Gender-aware re-ranking
#@ Eugene Kharitonov;Pavel Serdyukov
#t 2012
#c 13
#% 1166492
#% 1450894
#% 1536504
#% 1598347
#% 1641961
#! In this paper we study usefulness of users' gender information for improving ranking of ambiguous queries in personalized and non-contextual settings. This study is performed as a sequence of offline re-ranking experiments and it demonstrates that the proposed gender-aware ranking features provide improvements in ranking quality. It is also shown that the proposed personalized features exhibit performance superior to non-contextual ones.

#index 1879153
#* Genre classification for million song dataset using confidence-based classifiers combination
#@ Yajie Hu;Mitsunori Ogihara
#t 2012
#c 13
#% 1451214
#! We proposed a method to classify songs in the Million Song Dataset according to song genre. Since songs have several data types, we trained sub-classifiers by different types of data. These sub-classifiers are combined using both classifier authority and classification confidence for a particular instance. In the experiments, the combined classifier surpasses all of these sub-classifiers and the SVM classifier using concatenated vectors from all data types. Finally, the genre labels for the Million Song Dataset are provided.

#index 1879154
#* GLASE 0.1: eyes tell more than mice
#@ Viktors Garkavijs;Mayumi Toshima;Noriko Kando
#t 2012
#c 13
#% 954950
#% 1183321
#% 1227582
#% 1375982
#% 1480199
#! This paper proposes a prototype system called Gaze-Learning-Access-and-Search-Engine 0.1 (GLASE), which can perform image relevance ranking based on gaze data and within-session learning. We developed a search user interface that uses an eye-tracker as an input device and employed a relevance re-ranking algorithm based on the gaze length. The preliminary experimental results showed that using our gaze-driven system reduced the task completion time an average of 13.7% in a search session.

#index 1879155
#* How query extensions reflect search result abandonments
#@ Aleksandr Chuklin;Pavel Serdyukov
#t 2012
#c 13
#% 1227582
#% 1560357
#! It is often considered that high abandonment rate corresponds to poor IR system performance. However several studies suggested that there are so called good abandonments, i.e. situations when search engine result page contains enough details to satisfy the user information need without necessity to click on search results. In this work we propose to look at query extensions. We see that an extension by itself might motivate abandonment type (good or bad) for the underlying query to some degree. We also propose a way to find potentially good abandonment extensions in an automated manner.

#index 1879156
#* Identifying entity aspects in microblog posts
#@ Damiano Spina;Edgar Meij;Maarten de Rijke;Andrei Oghina;Minh Thuong Bui;Mathias Breuss
#t 2012
#c 13
#% 740900
#% 766429
#% 817472
#% 857180
#% 1471238
#% 1505090
#% 1591944
#% 1693918
#! Online reputation management is about monitoring and handling the public image of entities (such as companies) on the Web. An important task in this area is identifying "aspects" of the entity of interest (such as products, services, competitors, key people, etc.) given a stream of microblog posts referring to the entity. In this paper we compare different IR techniques and opinion target identification methods for automatically identifying aspects and find that (i) simple statistical methods such as TF.IDF are a strong baseline for the task, significantly outperforming opinion-oriented methods, and (ii) only considering terms tagged as nouns improves the results for all the methods analyzed.

#index 1879157
#* Impact of assessor disagreement on ranking performance
#@ Pavel Metrikov;Virgil Pavlu;Javed A. Aslam
#t 2012
#c 13
#% 1074134
#% 1415710
#% 1450896
#! We consider the impact of inter-assessor disagreement on the maximum performance that a ranker can hope to achieve. We demonstrate that even if a ranker were to achieve perfect performance with respect to a given assessor, when evaluated with respect to a different assessor, the measured performance of the ranker decreases significantly. This decrease in performance may largely account for observed limits on the performance of learning-to-rank algorithms.

#index 1879158
#* Incorporating statistical topic information in relevance feedback
#@ Karla L. Caballero;Ram Akella
#t 2012
#c 13
#% 879587
#% 1074079
#% 1074080
#% 1605980
#! Most of the relevance feedback algorithms only use document terms as feedback (local features) in order to update the query and re-rank the documents to show to the user. This approach is limited by the terms of those documents without any global context. We propose to use statistical topic modeling techniques in relevance feedback to incorporate a better estimate of context by including global information about the document. This is particularly helpful for difficult queries where learning the context from the interactions with the user is crucial. We propose to use the topic mixture information obtained to characterize the documents and learn their topics. Then, we rank documents incorporating positive and negative feedback by fitting a latent distribution for each class of documents online and combining all the features using Bayesian Logistic Regression. We show results using the OHSUMED dataset for 3 different variants and obtain higher performance, up to 12.5% in Mean Average Precision (MAP).

#index 1879159
#* Inferring missing relevance judgments from crowd workers via probabilistic matrix factorization
#@ Hyun Joon Jung;Matthew Lease
#t 2012
#c 13
#% 1150163
#% 1452857
#! In crowdsourced relevance judging, each crowd worker typically judges only a small number of examples, yielding a sparse and imbalanced set of judgments in which relatively few workers influence output consensus labels, particularly with simple consensus methods like majority voting. We show how probabilistic matrix factorization, a standard approach in collaborative filtering, can be used to infer missing worker judgments such that all workers influence output labels. Given complete worker judgments inferred by PMF, we evaluate impact in unsupervised and supervised scenarios. In the supervised case, we consider both weighted voting and worker selection strategies based on worker accuracy. Experiments on crowd judgments from the 2010 TREC Relevance Feedback Track show promise of the PMF approach merits further investigation and analysis.

#index 1879160
#* Investigating performance predictors using monte carlo simulation and score distribution models
#@ Ronan Cummins
#t 2012
#c 13
#% 871578
#% 1263599
#% 1392434
#% 1529948
#% 1558080
#% 1598445
#% 1748071
#% 1806029
#! The standard deviation of scores in the top k documents of a ranked list has been shown to be significantly correlated with average precision and has been the basis of a number of query performance predictors. In this paper, we outline two hypotheses that aid in understanding this correlation. Using score distribution (SD) models with known parameters, we create a large number of document rankings using Monte Carlo simulation to test the validity of these hypotheses.

#index 1879161
#* Learning to select a time-aware retrieval model
#@ Nattiya Kanhabua;Klaus Berberich;Kjetil Nørvåg
#t 2012
#c 13
#% 397161
#% 960414
#% 1495112
#% 1697416
#% 1697424
#! Time-aware retrieval models exploit one of two time dimensions, namely, (a) publication time or (b) content time (temporal expressions mentioned in documents). We show that the effectiveness for a temporal query (e.g., illinois earthquake 1968) depends significantly on which time dimension is factored into ranking results. Motivated by this, we propose a machine learning approach to select the most suitable time-aware retrieval model for a given temporal query. Our method uses three classes of features obtained from analyzing distributions over two time dimensions, a distribution over terms, and retrieval scores within top-k result documents. Experiments on real-world data with crowdsourced relevance assessments show the potential of our approach.

#index 1879162
#* Learning-based time-sensitive re-ranking for web search
#@ Po-Tzu Chang;Yen-Chieh Huang;Cheng-Lun Yang;Shou-De Lin;Pu-Jen Cheng
#t 2012
#c 13
#% 730070
#% 766447
#% 869517
#% 1270766
#% 1275591
#% 1355016
#% 1355017
#% 1536521
#% 1537502
#! To model time-dependent user intent for Web search, this paper proposes a novel method using machine learning techniques to exploit temporal features for effective time-sensitive search result re-ranking. We propose models to incorporate users' click through information for queries that are seen in the training data, and then further extend the model to deal with unseen queries considering the relationship between queries. Experiment shows significant improvement on search result ranking over original search outputs.

#index 1879163
#* Lightweight contrastive summarization for news comment mining
#@ Gobaan Raveendran;Charles L.A. Clarke
#t 2012
#c 13
#% 741106
#% 1019161
#% 1055683
#% 1270753
#! We develop and discuss a news comment miner that presents distinct viewpoints on a given theme or event. Given a query, the system uses metasearch techniques to find relevant news articles. Relevant articles are then scraped for both article content and comments. Snippets from the comments are sampled and presented to the user, based on theme popularity and contrastiveness to previously selected snippets. The system design focuses on being quicker and more lightweight than recent topic modelling approaches, while still focusing on selecting orthogonal snippets.

#index 1879164
#* Looking inside the box: context-sensitive translation for cross-language information retrieval
#@ Ferhan Ture;Jimmy Lin;Douglas W. Oard
#t 2012
#c 13
#% 262046
#% 579944
#% 643017
#% 979655
#% 1227588
#% 1471432
#% 1587403
#! Cross-language information retrieval (CLIR) today is dominated by techniques that use token-to-token mappings from bilingual dictionaries. Yet, state-of-the-art statistical translation models (e.g., using Synchronous Context-Free Grammars) are far richer, capturing multi-term phrases, term dependencies, and contextual constraints on translation choice. We present a novel CLIR framework that is able to reach inside the translation "black box" and exploit these sources of evidence. Experiments on the TREC-5/6 English-Chinese test collection show this approach to be promising.

#index 1879165
#* Making results fit into 40 characters: a study in document rewriting
#@ Johannes Leveling;Gareth J.F. Jones
#t 2012
#c 13
#% 259990
#% 342962
#% 551665
#% 1270277
#! With the increasing popularity of mobile and hand-held devices, automatic approaches for adapting results to the limited screen size of mobile devices are becoming more important. Traditional approaches for reducing the length of textual results include summarization and snippet extraction. In this study, we investigate document rewriting techniques which retain the meaning and readability of the original text. Evaluations on different document sets show that i) rewriting documents considerably reduces document length and thus, scrolling effort on devices with limited screen size, and ii) the rewritten documents have a higher readability.

#index 1879166
#* New assessment criteria for query suggestion
#@ Zhongrui Ma;Yu Chen;Ruihua Song;Tetsuya Sakai;Jiaheng Lu;Ji-Rong Wen
#t 2012
#c 13
#% 1131000
#% 1292743
#% 1400023
#% 1598413
#% 1598414
#% 1603005
#% 1712595
#! Query suggestion is a useful tool to help users express their information needs by supplying alternative queries. When evaluating the effectiveness of query suggestion algorithms, many previous studies focus on measuring whether a suggestion query is relevant or not to the input query. This assessment criterion is too simple to describe users' requirements. In this paper, we introduce two scenarios of query suggestion. The first scenario represents cases where the search result of the input query is unsatisfactory. The second scenario represents cases where the search result is satisfactory but the user may be looking for alternative solutions. Based on the two scenarios, we propose two assessment criteria. Our labeling results indicate that the new assessment criteria provide finer distinctions among query suggestions than the traditional relevance-based criterion.

#index 1879167
#* On automatically tagging web documents from examples
#@ Nicholas Joel Woodward;Weijia Xu;Kent Norsworthy
#t 2012
#c 13
#% 889273
#% 1131829
#% 1598427
#% 1598429
#! An emerging need in information retrieval is to identify a set of documents conforming to an abstract description. This task presents two major challenges to existing methods of document retrieval and classification. First, similarity based on overall content is less effective because there may be great variance in both content and subject of documents produced for similar functions, e.g. a presidential speech or a government ministry white paper. Second, the function of the document can be defined based on user interests or the specific data set through a set of existing examples, which cannot be described with standard categories. Additionally, the increasing volume and complexity of document collections demands new scalable computational solutions. We conducted a case study using web-archived data from the Latin American Government Documents Archive (LAGDA) to illustrate these problems and challenges. We propose a new hybrid approach based on Naïve Bayes inference that uses mixed n-gram models obtained from a training set to classify documents in the corpus. The approach has been developed to exploit parallel processing for large scale data set. The preliminary work shows promising results with improved accuracy for this type of retrieval problem.

#index 1879168
#* On building a reusable Twitter corpus
#@ Richard McCreadie;Ian Soboroff;Jimmy Lin;Craig Macdonald;Iadh Ounis;Dean McCullough
#t 2012
#c 13
#% 1846784
#! The Twitter real-time information network is the subject of research for information retrieval tasks such as real-time search. However, so far, reproducible experimentation on Twitter data has been impeded by restrictions imposed by the Twitter terms of service. In this paper, we detail a new methodology for legally building and distributing Twitter corpora, developed through collaboration between the Text REtrieval Conference (TREC) and Twitter. In particular, we detail how the first publicly available Twitter corpus - referred to as Tweets2011 - was distributed via lists of tweet identifiers and specialist tweet crawling software. Furthermore, we analyse whether this distribution approach remains robust over time, as tweets in the corpus are removed either by users or Twitter itself. Tweets2011 was successfully used by 58 participating groups for the TREC 2011 Microblog track, while our results attest to the robustness of the crawling methodology over time.

#index 1879169
#* On judgments obtained from a commercial search engine
#@ Emine Yilmaz;Gabriella Kazai;Nick Craswell;Saied Mehrizi Tahaghoghi
#t 2012
#c 13
#% 262105
#% 1074134
#% 1536510
#% 1598354
#! In information retrieval, relevance judgments play an important role as they are required both for evaluating the quality of retrieval systems and for training learning to rank algorithms. In recent years, numerous papers have been published using judgments obtained from a commercial search engine by researchers in industry. As typically no information is provided about the quality of these judgments, their reliability for evaluating/training retrieval systems remains questionable. In this paper, we analyze the reliability of such judgments for evaluating the quality of retrieval systems by comparing them to judgments by NIST judges at TREC.

#index 1879170
#* On the mathematical relationship between expected n-call@k and the relevance vs. diversity trade-off
#@ Kar Wai Lim;Scott Sanner;Shengbo Guo
#t 2012
#c 13
#% 262112
#% 642975
#% 879618
#% 1227591
#% 1642158
#! It has been previously noted that optimization of the n-call@k relevance objective (i.e., a set-based objective that is 1 if at least n documents in a set of k are relevant, otherwise 0) encourages more result set diversification for smaller n, but this statement has never been formally quantified. In this work, we explicitly derive the mathematical relationship between expected n-call@k and the relevance vs. diversity trade-off --- through fortuitous cancellations in the resulting combinatorial optimization, we show the trade-off is a simple and intuitive function of n (notably independent of the result set size k e n), where diversification increases as n approaches 1.

#index 1879171
#* On real-time ad-hoc retrieval evaluation
#@ Stephen E. Robertson;Evangelos Kanoulas
#t 2012
#c 13
#% 1450904
#! Lab-based evaluations typically assess the quality of a retrieval system with respect to its ability to retrieve documents that are relevant to the information need of an end user. In a real-time search task however users not only wish to retrieve the most relevant items but the most recent as well. The current evaluation framework is not adequate to assess the ability of a system to retrieve both recent and relevant items, and the one proposed in the recent TREC Microblog Track has certain flaws that quickly became apparent to the organizers. In this poster, we redefine the experiment for a real-time ad-hoc search task, by setting new submission requirements for the submitted systems/runs, proposing metrics to be used in evaluating the submissions, and suggesting a pooling strategy to be used to gather relevance judgments towards the computation of the described metrics. The proposed task can indeed assess the quality of a retrieval system with regard to retrieving both relevant and timely information.

#index 1879172
#* Opinion summarisation through sentence extraction: an investigation with movie reviews
#@ Marco Bonzanini;Miguel Martinez-Alvarez;Thomas Roelleke
#t 2012
#c 13
#% 344447
#% 938687
#% 1306081
#! In on-line reviews, authors often use a short passage to describe the overall feeling about a product or a service. A review as a whole can mention many details not in line with the overall feeling, so capturing this key passage is important to understand the overall sentiment of the review. This paper investigates the use of extractive summarisation in the context of sentiment classification. The aim is to find the summary sentence, or the short passage, which gives the overall sentiment of the review, filtering out potential noisy information. Experiments on a movie review data-set show that subjectivity detection plays a central role in building summaries for sentiment classification. Subjective extracts carry the same polarity of the full text reviews, while statistical and positional approaches are not able to capture this aspect.

#index 1879173
#* Optimizing parameters of the expected reciprocal rank
#@ Yury Logachev;Pavel Serdyukov
#t 2012
#c 13
#% 1292528
#! Most popular IR metrics are parameterized. Usually parameters of these metrics are chosen on the basis of general considerations and not adjusted by experiments with real users. Particularly, the parameters of the Expected Reciprocal Rank measure are the normalized parameters of the DCG metric, and the latter are chosen in an ad-hoc manner. We suggest an approach for adjusting parameters of the ERR metric that allows to reach maximum agreement with the real users behavior. More exactly, we optimized the parameters by maximizing Pearson weighted correlation between ERR and several online click metrics. For each click metric we managed to find the parameters of ERR that result into its higher correlation with the given online click metric.

#index 1879174
#* Ousting ivory tower research: towards a web framework for providing experiments as a service
#@ Tim Gollub;Benno Stein;Steven Burrows
#t 2012
#c 13
#% 1292526
#% 1598370
#! With its close ties to the Web, the IR community is destined to leverage the dissemination and collaboration capabilities that the Web provides today. Especially with the advent of the software as a service principle, an IR community is conceivable that publishes experiments executable by anyone over the Web. A review of recent SIGIR papers shows that we are far away from this vision of collaboration. The benefits of publishing IR experiments as a service are striking for the community as a whole, and include potential to boost research profiles and reputation. However, the additional work must be kept to a minimum and sensitive data must be kept private for this paradigm to become an accepted practice. To foster experiments as a service in IR, we present a Web framework for experiments that addresses the outlined challenges and possesses a unique set of compelling features in comparison to existing solutions. We also describe how our reference implementation is already used officially as an evaluation platform for an established international plagiarism detection competition.

#index 1879175
#* Parallelizing ListNet training using spark
#@ Shilpa Shukla;Matthew Lease;Ambuj Tewari
#t 2012
#c 13
#% 963669
#% 983820
#% 1475077
#% 1598442
#% 1783374
#! As ever-larger training sets for learning to rank are created, scalability of learning has become increasingly important to achieving continuing improvements in ranking accuracy. Exploiting independence of "summation form" computations, we show how each iteration in ListNet gradient descent can benefit from parallel execution. We seek to draw the attention of the IR community to use Spark, a newly introduced distributed cluster computing system, for reducing training time of iterative learning to rank algorithms. Unlike MapReduce, Spark is especially suited for iterative and interactive algorithms. Our results show near linear reduction in ListNet training time using Spark on Amazon EC2 clusters.

#index 1879176
#* Predicting lifespans of popular tweets in microblog
#@ Shoubin Kong;Ling Feng;Guozheng Sun;Kan Luo
#t 2012
#c 13
#% 1560174
#% 1598352
#! In microblog like Twitter, popular tweets are usually retweeted by many users. For different tweets, their lifespans (i.e., how long they will stay popular) vary. This paper presents a simple yet effective approach to predict the lifespans of popular tweets based on their static characteristics and dynamic retweeting patterns. For a potentially popular tweet, we generate a time series based on its first-hour retweeting information, and compare it with those of historic tweets of the same author and post time (at the granularity of hour). The top-k historic tweets are identified, whose mean lifespan is estimated as the lifespan of the new tweet. Our experiments on a three-month real data set from Tencent Microblog demonstrate the effectiveness of the approach.

#index 1879177
#* Preliminary study of technical terminology for the retrieval of scientific book metadata records
#@ Birger Larsen;Christina Lioma;Ingo Frommholz;Hinrich Schütze
#t 2012
#c 13
#% 789959
#! Books only represented by brief metadata (book records) are particularly hard to retrieve. One way of improving their retrieval is by extracting retrieval enhancing features from them. This work focusses on scientific (physics) book records. We ask if their technical terminology can be used as a retrieval enhancing feature. A study of 18,443 book records shows a strong correlation between their technical terminology and their likelihood of relevance. Using this finding for retrieval yields +5% precision and recall gains.

#index 1879178
#* Queries without clicks: evaluating retrieval effectiveness based on user feedback
#@ Athanasia Koumpouri;Vasiliki Simaki
#t 2012
#c 13
#% 320432
#% 805200
#% 1227582
#% 1697423
#! Until recently, the lack of user activity on search results was perceived as a sign of user dissatisfaction from retrieval performance. However, recent studies have reported that some queries might not be followed by clicks to the content of the retrieved results, because the search task can be satisfied in the list of retrieved results the user views without the need to click through them. In this paper, we propose a method for evaluating user satisfaction from the results of searches that are not followed by clickthrough activity to the retrieved results. We found that there is a strong association between some implicit measures of user activity and user's explicit satisfaction judgments. Moreover, we developed a predictive model of user satisfaction based on implicit measures, achieving accuracy up to 86%.

#index 1879179
#* Retrieval evaluation on focused tasks
#@ Besnik Fetahu;Ralf Schenkel
#t 2012
#c 13
#% 879598
#% 1489429
#% 1550724
#% 1598354
#% 1697468
#! Ranking of retrieval systems for focused tasks requires large number of relevance judgments. We propose an approach that minimizes the number of relevance judgments, where the performance measures are approximated using a Monte-Carlo sampling technique. Partial measures are taken using relevance judgments, whereas the remaining part of passages are annotated using a generated relevance probability distribution based on result rank. We define two conditions for stopping the assessment procedure when the ranking between systems is stable.

#index 1879180
#* Rewarding term location information to enhance probabilistic information retrieval
#@ Jiashu Zhao;Jimmy Xiangji Huang;Shicheng Wu
#t 2012
#c 13
#% 413603
#% 783474
#% 1292650
#% 1489440
#% 1598401
#! We investigate the effect of rewarding terms according to their locations in documents for probabilistic information retrieval. The intuition behind our approach is that a large amount of authors would summarize their ideas in some particular parts of documents. In this paper, we focus on the beginning part of documents. Several shape functions are defined to simulate the influence of term location information. We propose a Reward Term Retrieval model that combines the reward terms' information with BM25 to enhance probabilistic information retrieval performance.

#index 1879181
#* Scheduling queries across replicas
#@ Ana Freire;Craig Macdonald;Nicola Tonellotto;Iadh Ounis;Fidel Cacheda
#t 2012
#c 13
#% 730065
#% 985830
#% 1392441
#% 1642922
#% 1879054
#! For increased efficiency, an information retrieval system can split its index into multiple shards, and then replicate these shards across many query servers. For each new query, an appropriate replica for each shard must be selected, such that the query is answered as quickly as possible. Typically, the replica with the lowest number of queued queries is selected. However, not every query takes the same time to execute, particularly if a dynamic pruning strategy is applied by each query server. Hence, the replica's queue length is an inaccurate indicator of the workload of a replica, and can result in inefficient usage of the replicas. In this work, we propose that improved replica selection can be obtained by using query efficiency prediction to measure the expected workload of a replica. Experiments are conducted using 2.2k queries, over various numbers of shards and replicas for the large GOV2 collection. Our results show that query waiting and completion times can be markedly reduced, showing that accurate response time predictions can improve scheduling accuracy and attesting the benefit of the proposed scheduling algorithm.

#index 1879182
#* Re-examining search result snippet examination time for relevance estimation
#@ Dmitry Lagun;Eugene Agichtein
#t 2012
#c 13
#% 946521
#% 954949
#% 1536546
#% 1598370
#% 1647971
#! Previous studies of web search result examination have provided valuable insights in understanding and modelling searcher behavior. Yet, recent work (e.g., [3]) has been developed based on the assumption that the time a searcher spends examining a particular result abstract or snippet, correlates with result relevance. While this idea is intuitively attractive, to the best of our knowledge it has not been empirically tested. This poster investigates this hypothesis empirically, in a controlled setting, using eye tracking equipment to compare search result examination time with result relevance. Interestingly, while we replicate previous findings showing examination time to be indicative of whole-page relevance, we find that viewing time of individual results alone is a poor indicator of either absolute result relevance or even of pairwise preferences. Our results should not be taken as negating the usefulness of modeling searcher examination behavior, but rather to emphasize that snippet examination time is not in itself a good indicator of relevance.

#index 1879183
#* Sentiment identification by incorporating syntax, semantics and context information
#@ Kunpeng Zhang;Yusheng Xie;Yu Cheng;Daniel Honbo;Doug Downey;Ankit Agrawal;Wei-keng Liao;Alok Choudhary
#t 2012
#c 13
#% 464434
#% 1264800
#! This paper proposes a method based on conditional random fields to incorporate sentence structure (syntax and semantics) and context information to identify sentiments of sentences within a document. It also proposes and evaluates two different active learning strategies for labeling sentiment data. The experiments with the proposed approach demonstrate a 5-15% improvement in accuracy on Amazon customer reviews compared to existing supervised learning and rule-based methods.

#index 1879184
#* Short text classification using very few words
#@ Aixin Sun
#t 2012
#c 13
#% 397161
#% 931334
#% 1055680
#% 1292559
#% 1826347
#! We propose a simple, scalable, and non-parametric approach for short text classification. Leveraging the well studied and scalable Information Retrieval (IR) framework, our approach mimics human labeling process for a piece of short text. It first selects the most representative and topical-indicative words from a given short text as query words, and then searches for a small set of labeled short texts best matching the query words. The predicted category label is the majority vote of the search results. Evaluated on a collection of more than 12K Web snippets, the proposed approach achieves comparable classification accuracy with the baseline Maximum Entropy classifier using as few as 3 query words and top-5 best matching search hits. Among the four query word selection schemes proposed and evaluated in our experiments, term frequency together with clarity gives the best classification accuracy.

#index 1879185
#* Summarizing the differences from microblogs
#@ Dingding Wang;Mitsunori Ogihara;Tao Li
#t 2012
#c 13
#% 814023
#% 1292747
#% 1535476
#% 1598407
#% 1715628
#! With the rapid growth of social media websites, microblogging has become a popular way to spread instant news and events. Due to the dynamic and social nature of microblogs, extracting useful information from microblogs is more challenging than from the traditional news articles. In this paper we study the problem of summarizing the differences from microblogs. Given a collection of microblogs discussing an event/topic, we propose to generate a short summary delivering the differences among these microblogs, such as the different points of view for a news topic and the changes and evolution of an ongoing event.

#index 1879186
#* Survival analysis of click logs
#@ Si-Chi Chin;W. Nick Street
#t 2012
#c 13
#% 805200
#% 818221
#% 1035578
#% 1074092
#% 1190055
#% 1190056
#% 1314915
#! Click logs from search engines provide a rich opportunity to acquire implicit feedback from users. Patterns derived from the time between a posted query and a click provide information on the ranking quality, reflecting the perceived relevance of a retrieved URL. This paper applies the Kaplan-Meier estimator to study click patterns. The visualization of click curves demonstrates the interaction between the relevance and the rank position of URLs. The observed results demonstrate the potential of using click curves to predict the quality of the top-ranked results.

#index 1879187
#* Text selections as implicit relevance feedback
#@ Ryen W. White;Georg Buscher
#t 2012
#c 13
#% 731615
#% 805200
#% 946521
#% 1482279
#% 1598347
#% 1693899
#! Users' search activity has been used as implicit feedback to model search interests and improve the performance of search systems. In search engines, this behavior usually takes the form of queries and result clicks. However, richer data on how people engage with search results can now be captured at scale, creating new opportu-nities to enhance search. In this poster we focus on one type of newly-observable behavior: text selection events on search-result captions. We show that we can use text selections as implicit feedback to significantly improve search result relevance.

#index 1879188
#* Time to judge relevance as an indicator of assessor error
#@ Mark D. Smucker;Chandra Prakash Jethani
#t 2012
#c 13
#% 312689
#% 1450896
#% 1482232
#% 1598516
#! When human assessors judge documents for their relevance to a search topic, it is possible for errors in judging to occur. As part of the analysis of the data collected from a 48 participant user study, we have discovered that when the participants made relevance judgments, the average participant spent more time to make errorful judgments than to make correct judgments. Thus, in relevance assessing scenarios similar to our user study, it may be possible to use the time taken to judge a document as an indicator of assessor error. Such an indicator could be used to identify documents that are candidates for adjudication or reassessment.

#index 1879189
#* Towards alias detection without string similarity: an active learning based approach
#@ Lili Jiang;Jianyong Wang;Ping Luo;Ning An;Min Wang
#t 2012
#c 13
#% 169717
#% 413652
#% 868091
#% 1055818
#! Entity aliases commonly exist and accurately detecting these aliases plays a vital role in various applications. In this paper, we use an active-learning-based method to detect aliases without string similarity. To minimize the cost on pairwise comparison, a subset-based method restricts the alias selection within a small-scale entity set. Within each generated entity set, an active learning based logistic regression classifier is employed to predict whether a candidate is the alias of a given entity. The experimental results on three datasets clearly demonstrate that our proposed approach can effectively detect this kind of entity aliases.

#index 1879190
#* Towards zero-click mobile IR evaluation: knowing what and knowing when
#@ Tetsuya Sakai
#t 2012
#c 13
#% 768292
#% 1227582
#% 1366270
#% 1641986
#! In this poster, we propose two evaluation tasks for mobile information access. The first task evaluates the system's ability to guess what the user's query should be given a context ("Knowing What"). The second task evaluates the system's ability to decide when to proactively deploy a given query ("Knowing When"). We conduct a preliminary manual analysis of a mobile query log to limit the space of possible queries so as to design feasible and practical evaluation tasks.

#index 1879191
#* Twanchor text: a preliminary study of the value of tweets as anchor text
#@ Gilad Mishne;Jimmy Lin
#t 2012
#c 13
#% 643069
#% 824588
#% 1035588
#% 1227604
#% 1400144
#% 1450844
#! It is well known that anchor text plays an important role in search, providing signals that are often not present in the source document itself. The paper reports results of a preliminary investigation on the value of tweets and tweet conversations as anchor text. We show that using tweets as anchors improves significantly over using HTML anchors, and significantly increases recall of news item retrieval.

#index 1879192
#* Unsupervised linear score normalization revisited
#@ Ilya Markov;Avi Arampatzis;Fabio Crestani
#t 2012
#c 13
#% 232703
#% 342710
#% 995515
#% 1292546
#% 1292595
#! We give a fresh look into score normalization for merging result-lists, isolating the problem from other components. We focus on three of the simplest, practical, and widely-used linear methods which do not require any training data, i.e. MinMax, Sum, and Z-Score. We provide theoretical arguments on why and when the methods work, and evaluate them experimentally. We find that MinMax is the most robust under many circumstances, and that Sum is - in contrast to previous literature - the worst. Based on the insights gained, we propose another three simple methods which work as good or better than the baselines.

#index 1879193
#* User-aware caching and prefetching query results in web search engines
#@ Hongyuan Ma;Bin Wang
#t 2012
#c 13
#% 577302
#% 860861
#! Query results caching is an efficient technique for Web search engines. In this paper we present User-Aware Cache, a novel approach tailored for query results caching, that is based on user characteristics. We then use a trace of around 30 million queries to evaluate User-Aware Cache, as well as traditional methods and theoretical upper bounds. Experimental results show that this approach can achieve hit ratios better than state-of-the-art methods.

#index 1879194
#* Using eye-tracking with dynamic areas of interest for analyzing interactive information retrieval
#@ Vu Tuan Tran;Norbert Fuhr
#t 2012
#c 13
#% 946521
#% 954949
#% 1048693
#% 1051038
#% 1598335
#! Based on a new framework for capturing dynamic areas of interest in eye-tracking, we model the user search process as a Markov-chain. The analysis indicates possible system improvements and yields parameter estimates for the Interactive Probability Ranking Principle (IPRP).

#index 1879195
#* Using PageRank to infer user preferences
#@ Praveen Chandar;Ben Carterette
#t 2012
#c 13
#% 268079
#% 411762
#% 818276
#% 1415710
#! Recently, researchers have shown interest in the use of preference judgments for evaluation in IR literature. Although preference judgments have several advantages over absolute judgment, one of the major disadvantages is that the number of judgments needed increases polynomially as the number of documents in the pool increases. We propose a novel method using PageRank to minimize the number of judgments required to evaluate systems using preference judgments. We test the proposed hypotheses using the TREC 2004 to 2006 Terabyte dataset to show that it is possible to reduce the evaluation cost considerably. Further, we study the susceptibility of the methods due to assessor errors.

#index 1879196
#* Utilizing inter-document similarities in federated search
#@ Savva Khalaman;Oren Kurland
#t 2012
#c 13
#% 280856
#% 340146
#% 397199
#% 722312
#% 881939
#% 1227630
#% 1598423
#! We demonstrate the merits of using inter-document similarities for federated search. Specifically, we study a results merging method that utilizes information induced from clusters of similar documents created across the lists retrieved from the collections. The method significantly outperforms state-of-the-art results merging approaches.

#index 1879197
#* Want a coffee?: predicting users' trails
#@ Wen Li;Carsten Eickhoff;Arjen P. de Vries
#t 2012
#c 13
#% 1399939
#% 1482236
#% 1693933
#! Twitter and Foursquare are two well-connected platforms for sharing information where growing numbers of users post location-related messages. In contrast to the longitude-latitude geotags commonly used online, e.g., on photos and tweets, new place-tags containing category information show more human-readable high-level information rather than a pair of coordinates. This grants an opportunity for better understanding users' physical locations which can be used as context to facilitate other applications, e.g., location context-aware advertisement. In this paper, we verify the assumption that users' current trails contain cues of their future routes. The results from the preliminary experiments show promising performance of a basic Markov Chain-based model.

#index 1879198
#* Will this #hashtag be popular tomorrow?
#@ Zongyang Ma;Aixin Sun;Gao Cong
#t 2012
#c 13
#% 1560424
#% 1693881
#% 1746823
#! Hashtags are widely used in Twitter to define a shared context for events or topics. In this paper, we aim to predict hashtag popularity in near future (i.e., next day). Given a hashtag that has the potential to be popular in the next day, we construct a hashtag profile using the tweets containing the hashtag, and extract both content and context features for hashtag popularity prediction. We model this prediction problem as a classification problem and evaluate the effectiveness of the extracted features and classification models.

#index 1879199
#* $100,000 prize jackpot. call now!: identifying the pertinent features of SMS spam
#@ Henry Tan;Nazli Goharian;Micah Sherr
#t 2012
#c 13
#% 987244
#% 1019090
#% 1121895
#% 1310465
#% 1617834
#! Mobile SMS spam is on the rise and is a prevalent problem. While recent work has shown that simple machine learning techniques can distinguish between ham and spam with high accuracy, this paper explores the individual contributions of various textual features in the classification process. Our results reveal the surprising finding that simple is better: using the largest spam corpus of which we are aware, we find that using simple textual features is sufficient to provide accuracy that is nearly identical to that achieved by the best known techniques, while achieving a twofold speedup.

#index 1879200
#* Beyond bag-of-words: machine learning for query-document matching in web search
#@ Hang Li;Jun Xu
#t 2012
#c 13
#% 817577
#% 1190106
#% 1227610
#% 1399944
#% 1536566
#% 1536584
#% 1560222
#% 1591934
#% 1598394
#% 1598399
#% 1598401
#% 1598402
#% 1606370

#index 1879201
#* Methods for mining and summarizing text conversations
#@ Giuseppe Carenini;Gabrial Murray
#t 2012
#c 13
#% 1898021
#! More and more today, people are engaging in conversations via email, blogs, discussion forums, text messaging and other social media. A person may want to archive these conversations and later retrieve information about what was discussed, or analyze a conversation in real-time. What topics are covered in these conversations? What opinions are people expressing? Have any decisions been made? Have action items been assigned? This tutorial will present various natural language processing (NLP) techniques that can help answer these questions, thus creating numerous new and valuable applications that can support people in more effectively participating in these conversation. The tutorial is based on a book that we have recently published, Methods for Mining and Summarizing Text Conversations.

#index 1879202
#* Crowdsourcing for search evaluation and social-algorithmic search
#@ Matthew Lease;Omar Alonso
#t 2012
#c 13
#! The first computers were people. Today, Internet-based access to 24/7 online human crowds has led to a renaissance of research in human computation and the advent of crowdsourcing. These new opportunities have brought a disruptive shift to research and practice for how we build intelligent systems today. Not only can labeled data for training and evaluation be collected faster, cheaper, and easier than ever before, but we now see human computation being integrated into the systems themselves, operating in concert with automation. This tutorial introduces opportunities and challenges of human computation and crowdsourcing, particularly for search evaluation and developing hybrid search solutions that integrate human computation with traditional forms of automated search. We review methodology and findings of recent research and survey current generation crowdsourcing platforms now available, analyzing methods, potential, and limitations across platforms.

#index 1879203
#* (Big) usage data in web search
#@ Ricardo Baeza-Yates;Yoelle Maarek
#t 2012
#c 13

#index 1879204
#* A new look at old tricks: the fertile roots of current research
#@ Paul Kantor
#t 2012
#c 13

#index 1879205
#* Aspect-based opinion mining from product reviews
#@ Samaneh Moghaddam;Martin Ester
#t 2012
#c 13
#! "What other people think" has always been an important piece of information for most of us during the decision-making process. Today people tend to make their opinions available to other people via the Internet. As a result, the Web has become an excellent source of consumer opinions. There are now numerous Web resources containing such opinions, e.g., product reviews forums, discussion groups, and blogs. But, it is really difficult for a customer to read all of the reviews and make an informed decision on whether to purchase the product. It is also difficult for the manufacturer of the product to keep track and manage customer opinions. Also, focusing on just user ratings (stars) is not a sufficient source of information for a user or the manufacturer to make decisions. Therefore, mining online reviews (opinion mining) has emerged as an interesting new research direction. Extracting aspects and the corresponding ratings is an important challenge in opinion mining. An aspect is an attribute or component of a product, e.g. 'zoom' for a digital camera. A rating is an intended interpretation of the user satisfaction in terms of numerical values. Reviewers usually express the rating of an aspect by a set of sentiments, e.g. 'great zoom'. In this tutorial we cover opinion mining in online product reviews with the focus on aspect-based opinion mining. This problem is a key task in the area of opinion mining and has attracted a lot of researchers in the information retrieval community recently. Several opinion related information retrieval tasks can benefit from the results of aspect-based opinion mining and therefore it is considered as a fundamental problem. This tutorial covers not only general opinion mining and retrieval tasks, but also state-of-the-art methods, challenges, applications, and also future research directions of aspect-based opinion mining.

#index 1879206
#* Experimental methods for information retrieval
#@ Donald Metzler;Oren Kurland
#t 2012
#c 13
#% 309093
#% 818222
#% 857180
#% 1019124
#% 1130864
#% 1292526

#index 1879207
#* IR models: foundations and relationships
#@ Thomas Roelleke
#t 2012
#c 13
#% 86371
#% 111303
#% 169781
#% 176530
#% 219043
#% 262094
#% 262096
#% 411760
#% 458744
#% 570316
#% 643003
#% 758200
#% 789959
#% 799246
#% 818238
#% 818263
#% 879578
#% 1021951
#% 1074105
#% 1083541
#% 1263612
#% 1348341
#% 1622349
#! In IR research it is essential to know IR models. Research over the past years has consolidated the foundations of IR models. Moreover, relationships have been reported that help to use and position IR models. Knowing about the foundations and relationships of IR models can significantly improve building information management systems. The first part of this tutorial presents an in-depth consolidation of the foundations of the main IR models (TF-IDF, BM25, LM). Particular attention will be given to notation and probabilistic roots. The second part crystallises the relationships between models. Does LM embody IDF? How "heuristic" is TF-IDF? What are the probabilistic roots? How are LM and the probability of relevance related? What are the components shared by the main IR models? After the tutorial, attendees will be familiar with a consolidated view on IR models. The tutorial will be illustrative and interactive, providing opportunities to exchange controversial issues and research challenges.

#index 1879208
#* Patent information retrieval: an instance of domain-specific search
#@ Mihai Lupu
#t 2012
#c 13
#! The tutorial aims to provide the IR researchers with an understanding of how the patent system works, the challenges that patent searchers face in using the existing tools and in adopting new methods developed in academia. At the same time, the tutorial will inform the IR researcher about the unique opportunities that the patent domain provides: a large amount of multi-lingual and multi-modal documents, the widest possible span of covered domains, a highly annotated corpus and, very importantly, relevance judgements created by experts in the fields and recorded electronically in the documents. The combination of these two objectives leads to the main purpose of the tutorial: to create awareness and to encourage more emphasis on the patent domain in the IR community. Table 1 provides details on how the tutorial covers the topics of the SIGIR conference.

#index 1879209
#* Medical information retrieval: an instance of domain-specific search
#@ Allan Hanbury
#t 2012
#c 13
#% 862154
#% 1074051
#% 1278069
#% 1291575
#% 1584317
#% 1598340
#% 1641961
#% 1681360
#! Due to an explosion in the amount of medical information available, search techniques are gaining importance in the medical domain. This tutorial discusses recent results on search in the medical domain, including the outcome of surveys on end user requirements, research relevant to the field, and current medical and health search applications available. Finally, the extent to which available techniques meet user requirements are discussed, and open challenges in the field are identified.

#index 1879210
#* Visual information retrieval using Java and LIRE
#@ Oge Marques;Mathias Lux
#t 2012
#c 13
#! Visual information retrieval (VIR) is an active and vibrant research area, which attempts at providing means for organizing, indexing, annotating, and retrieving visual information (images and videos) form large, unstructured repositories. The goal of VIR is to retrieve the highest number of relevant matches to a given query (often expressed as an example image and/or a series of keywords). In its early years (1995-2000) the research efforts were dominated by content-based approaches contributed primarily by the image and video processing community. During the past decade, it was widely recognized that the challenges imposed by the semantic gap (the lack of coincidence between an image's visual contents and its semantic interpretation) required a clever use of textual metadata (in addition to information extracted from the image's pixel contents) to make image and video retrieval solutions efficient and effective. The need to bridge (or at least narrow) the semantic gap has been one of the driving forces behind current VIR research. Additionally, other related research problems and market opportunities have started to emerge, offering a broad range of exciting problems for computer scientists and engineers to work on. In this tutorial, we present an overview of visual information retrieval (VIR) concepts, techniques, algorithms, and applications. Several topics are supported by examples written in Java, using Lucene (an open-source Java-based indexing and search implementation) and LIRE (Lucene Image REtrieval), an open-source Java-based library for content-based image retrieval (CBIR) written by Mathias Lux. After motivating the topic, we briefly review the fundamentals of information retrieval, present the most relevant and effective visual descriptors currently used in VIR, the most common indexing approaches for visual descriptors, the most prominent machine learning techniques used in connection with contemporary VIR solutions, as well as the challenges associated with building real-world, large scale VIR solutions, including a brief overview of publicly available datasets used in worldwide challenges, contests, and benchmarks. Throughout the tutorial, we integrate examples using LIRE, whose main features and design principles are also discussed. Finally, we conclude the tutorial with suggestions for deepening the knowledge in the topic, including a brief discussion of the most relevant advances, open challenges, and promising opportunities in VIR and related areas. The tutorial is primarily targeted at experienced Information Retrieval researchers and practitioners interested in extending their knowledge of document-based IR to equivalent concepts, techniques, and challenges in VIR. The acquired knowledge should allow participants to derive insightful conclusions and promising avenues for further investigation.

#index 1879211
#* Large-scale graph mining and learning for information retrieval
#@ Bin Gao;Taifeng Wang;Tie-Yan Liu
#t 2012
#c 13
#% 282905
#% 466891
#% 577338
#% 805896
#% 840965
#% 875948
#% 881457
#% 983805
#% 1016177
#% 1019188
#% 1074107
#% 1292715
#% 1346154
#% 1605923
#% 1663624
#! For many information retrieval applications, we need to deal with the ranking problem on very large scale graphs. However, it is non-trivial to perform efficient and effective ranking on them. On one aspect, we need to design scalable algorithms. On another aspect, we also need to develop powerful computational infrastructure to support these algorithms. This tutorial aims at giving a timely introduction to the promising advances in the aforementioned aspects in recent years, and providing the audiences with a comprehensive view on the related literature.

#index 1879212
#* Query performance prediction for IR
#@ David Carmel;Oren Kurland
#t 2012
#c 13
#% 397161
#% 766497
#% 818267
#% 879613
#% 879614
#% 893735
#% 907544
#% 987260
#% 987265
#% 987299
#% 1065241
#% 1130851
#% 1130990
#% 1195854
#% 1263577
#% 1263599
#% 1392447
#% 1450861
#% 1450900
#% 1450964
#% 1450999
#% 1451031
#% 1467729
#% 1482276
#% 1529948
#% 1536584
#% 1587360
#% 1598445
#% 1622337
#% 1697426
#% 1748097
#! The goal of this tutorial is to expose participants to current research on query performance prediction. Participants will become familiar with state-of-the-art performance prediction methods, with common evaluation methodologies of prediction quality, and with potential applications that can utilize performance predictors. In addition, some open issues and challenges in the field will be discussed. This tutorial is an updated version of the SIGIR 2010 tutorial presented by David Carmel and Elad Yom-Tov on the same subject. This year we intend to expand on new results in the field, in particular focusing on recently developed frameworks that provide a unified model for performance prediction.

#index 1879213
#* Collaborative information seeking: art and science of achieving 1+12 in IR
#@ Chirag Shah
#t 2012
#c 13
#% 157698
#% 240162
#% 246877
#% 399447
#% 998795
#% 1074090
#% 1348064
#% 1474643
#% 1879566
#! The assumption of information seekers being independent and IR problem being individual has been challenged often in the recent past, with an argument that the next big leap in search and retrieval will come through incorporating social and collaborative aspects of information seeking. This half-day tutorial will introduce the student to theories, methodologies, and tools that focus on information retrieval/seeking in collaboration. The student will have an opportunity to learn about the social aspect of IR with a focus on collaborative information seeking (CIS) situations, systems, and evaluation techniques. The course is intended for those interested in social and collaborative aspects of IR (from both academia and industry), and requires only a general understanding of IR systems and evaluation.

#index 1879214
#* Advances on the development of evaluation measures
#@ Ben Carterette;Evangelos Kanoulas;Emine Yilmaz
#t 2012
#c 13
#% 133892
#% 248075
#% 309095
#% 411762
#% 642975
#% 840846
#% 857180
#% 1074133
#% 1074139
#% 1095876
#% 1166473
#% 1263584
#% 1292528
#% 1415709
#% 1450904
#% 1598424
#% 1598439
#% 1648026
#! The goal of the tutorial is to provide attendees with a comprehensive overview of the latest advances in the development of information retrieval evaluation measures and discuss the current challenges in the area. A number of topics are covered, including background in traditional evaluation paradigm and traditional evaluation measures, evaluation measures based on user models, advanced models of user interaction with search engines, measures based on these models, measures for novelty and diversity, and session-based measures.

#index 1988788
#* Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval
#@ Gareth J.F. Jones;Páraic Sheridan;Diane Kelly;Maarten de Rijke;Tetsuya Sakai
#t 2013
#c 13
#! Welcome to SIGIR, the 36th annual international ACM conference on research and development in Information Retrieval. SIGIR is the premier, international venue for research and development in information retrieval. We believe the breadth and diversity of research that comprises the program reflects the health of the organization and major future directions of the field. We are grateful to all those who submitted papers to the conference and gave the Committee an opportunity to evaluate their work for potential inclusion in the program. We are also grateful to the 50 Area Chairs and 204 general program committee members, who represent 30 countries and over 120 institutions, for all the hard work they put into evaluating submissions. The conference received 366 full paper submissions this year. Of these, 73 (20%) were accepted, essentially the same as last year's acceptance rate and the year before. The top five countries in terms of accepted papers (according to contact author affiliation) were the U.S.A. (28), China (9), the Netherlands, Singapore, and U.K. (5 each). The top five technical areas covered by the accepted papers (as indicated by the primary keyword assigned by paper authors) were users and interactive IR (16%), search engine architecture and scalability (15%), queries and query analysis (15%), evaluation (11%), and retrieval models and ranking (11%). This represents only a slight re-ordering of topics from last year. Two hundred fifty papers were submitted to the short papers track, which represents a 20% increase in the number of submissions made to last year's poster track. Eighty-five (34%) short papers were accepted. In addition, 46 demonstrations were proposed, of which 23 (50%) were accepted. The program also consisted of 7 workshops and 10 tutorials. Finally, the Doctoral Consortium hosted 11 students this year from 10 countries and 11 institutions. As has been customary for many years, SIGIR 2013 used a two-tier double-blind review process. In the first stage, at least three reviewers read every paper and provided ratings and comments. Papers were evaluated according to seven main criteria: relevance, originality, soundness, quality of the presentation, impact, coverage of the literature, and, for the first time, reproducibility of the results. In the second stage, the primary and secondary Area Chairs ensured the quality of the reviewing process by studying, validating, and summarizing these reviews, and adding their own feedback and ratings. Area Chairs initiated discussions among reviewers to resolve any controversial issues or significant differences of opinion. Once the discussion stage was completed, the two Area Chairs made a recommendation regarding the paper for nearly all submissions. This year we allowed Area Chairs to indicate that a paper should be accepted if room. At the program committee meeting held in Amsterdam, The Netherlands, the Program Chairs and the attending Area Chairs went over the reviews, verified the process, gathered additional input, and discussed and decided on papers that were balloted as accept if room, papers from which the primary Area Chair abstained and papers that had unusual score distributions. For papers that were balloted as accept if room, we especially considered the potential for the paper to provoke interesting and fruitful discussion at the conference. Ultimately 73 papers were selected for inclusion in the program. One important change to this year's program was renaming the poster paper submission type to short papers and increasing the length of the paper from two to four pages. Short papers were presented at the conference in poster format and two separate short paper sessions were included as part of the main conference program, rather than a single event collocated with an evening reception. We believe that increasing the length of the accompanying paper allows researchers to better communicate their experiments and results, which in turn, will allow this submission type to function as a more comprehensive and substantial container for small, but significant findings. We further believe this change better allows research presented in this format to get the attention it deserves. We would like to thank the Short Paper Co-Chairs for all the extra work they did this year managing this new format and the Short Paper reviewers for the great job they did handling both the larger volume of submissions and their increased size. We believe the large increase in number of submissions to this track indicates the community's receptiveness to this change. We hope you find this program interesting, provocative and inspiring, and that the conference provides you with a valuable opportunity to share ideas with other researchers, practitioners and students from institutions around the world. The deadline for SIGIR 2014 is, after all, only six months away!

#index 1988789
#* Learning latent friendship propagation networks with interest awareness for link prediction
#@ Jun Zhang;Chaokun Wang;Philip S. Yu;Jianmin Wang
#t 2013
#c 13
#% 577273
#% 722904
#% 730089
#% 769952
#% 955712
#% 1083675
#% 1083711
#% 1396209
#% 1451163
#% 1506202
#% 1512414
#% 1536568
#% 1617325
#% 1642046
#% 1642047
#% 1642049
#% 1948184
#! It's well known that the transitivity of friendship is a popular sociological principle in social networks. However, it's still unknown that to what extent people's friend-making behaviors follow this principle and to what extent it can benefit the link prediction task. In this paper, we try to adopt this sociological principle to explain the evolution of networks and study the latent friendship propagation. Unlike traditional link prediction approaches, we model link formation as results of individuals' friend-making behaviors combined with personal interests. We propose the Latent Friendship Propagation Network (LFPN), which depicts the evolution progress of one's egocentric network and reveals future growth potentials driven by the transitivity of friendship based on personal interests. We model individuals' social behaviors using the Latent Friendship Propagation Model (LFPM), a probabilistic generative model from which the LFPN can be learned effectively. To evaluate the power of the friendship propagation in link prediction, we design LFPN-RW which models the friend-making behavior as a random walk upon the LFPN naturally and captures the co-influence effect of the friend circles as well as personal interests to provide more accurate prediction. Experimental results on real-world datasets show that LFPN-RW outperforms the state-of-the-art approaches. This convinces that the transitivity of friendship actually plays important roles in the evolution of social networks.

#index 1988790
#* Time-aware point-of-interest recommendation
#@ Quan Yuan;Gao Cong;Zongyang Ma;Aixin Sun;Nadia Magnenat- Thalmann
#t 2013
#c 13
#% 280852
#% 330687
#% 801785
#% 838504
#% 849492
#% 1190134
#% 1214666
#% 1227601
#% 1260273
#% 1287243
#% 1358747
#% 1451212
#% 1480830
#% 1523885
#% 1598364
#% 1598366
#% 1606049
#% 1667201
#% 1697481
#% 1879059
#! The availability of user check-in data in large volume from the rapid growing location based social networks (LBSNs) enables many important location-aware services to users. Point-of-interest (POI) recommendation is one of such services, which is to recommend places where users have not visited before. Several techniques have been recently proposed for the recommendation service. However, no existing work has considered the temporal information for POI recommendations in LBSNs. We believe that time plays an important role in POI recommendations because most users tend to visit different places at different time in a day, \eg visiting a restaurant at noon and visiting a bar at night. In this paper, we define a new problem, namely, the time-aware POI recommendation, to recommend POIs for a given user at a specified time in a day. To solve the problem, we develop a collaborative recommendation model that is able to incorporate temporal information. Moreover, based on the observation that users tend to visit nearby POIs, we further enhance the recommendation model by considering geographical information. Our experimental results on two real-world datasets show that the proposed approach outperforms the state-of-the-art POI recommendation methods substantially.

#index 1988791
#* Summaries, ranked retrieval and sessions: a unified framework for information access evaluation
#@ Tetsuya Sakai;Zhicheng Dou
#t 2013
#c 13
#% 818257
#% 879630
#% 946521
#% 950043
#% 1074124
#% 1166473
#% 1227640
#% 1263584
#% 1292529
#% 1450895
#% 1482378
#% 1483555
#% 1536510
#% 1598424
#% 1598438
#% 1598439
#% 1641985
#% 1641986
#% 1648026
#% 1667279
#% 1879002
#% 1879003
#% 1879004
#% 1913790
#% 1918349
#% 2008819
#! We introduce a general information access evaluation framework that can potentially handle summaries, ranked document lists and even multi query sessions seamlessly. Our framework first builds a trailtext which represents a concatenation of all the texts read by the user during a search session, and then computes an evaluation metric called U-measure over the trailtext. Instead of discounting the value of a retrieved piece of information based on ranks, U-measure discounts it based on its position within the trailtext. U-measure takes the document length into account just like Time-Biased Gain (TBG), and has the diminishing return property. It is therefore more realistic than rank-based metrics. Furthermore, it is arguably more flexible than TBG, as it is free from the linear traversal assumption (i.e., that the user scans the ranked list from top to bottom), and can handle information access tasks other than ad hoc retrieval. This paper demonstrates the validity and versatility of the U-measure framework. Our main conclusions are: (a) For ad hoc retrieval, U-measure is at least as reliable as TBG in terms of rank correlations with traditional metrics and discriminative power; (b) For diversified search, our diversity versions of U-measure are highly correlated with state-of-the-art diversity metrics; (c) For multi-query sessions, U-measure is highly correlated with Session nDCG; and (d) Unlike rank-based metrics such as DCG, U-measure can quantify the differences between linear and nonlinear traversals in sessions. We argue that our new framework is useful for understanding the user's search behaviour and for comparison across different information access styles (e.g. examining a direct answer vs. examining a ranked list of web pages).

#index 1988792
#* Topic hierarchy construction for the organization of multi-source user generated contents
#@ Xingwei Zhu;Zhao-Yan Ming;Xiaoyan Zhu;Tat-Seng Chua
#t 2013
#c 13
#% 643068
#% 855293
#% 939601
#% 1100445
#% 1166524
#% 1227684
#% 1264995
#% 1265135
#% 1328333
#% 1337374
#% 1450829
#% 1471184
#% 1481643
#% 1642694
#% 1700573
#% 1711737
#% 1826362
#% 1925702
#! User generated contents (UGCs) carry a huge amount of high quality information. However, the information overload and diversity of UGC sources limit their potential uses. In this research, we propose a framework to organize information from multiple UGC sources by a topic hierarchy which is automatically generated and updated using the UGCs. We explore the unique characteristics of UGCs like blogs, cQAs, microblogs, etc., and introduce a novel scheme to combine them. We also propose a graph-based method to enable incremental update of the generated topic hierarchy. Using the hierarchy, users can easily obtain a comprehensive, in-depth and up-to-date picture of their topics of interests. The experiment results demonstrate how information from multiple heterogeneous sources improves the resultant topic hierarchies. It also shows that the proposed method achieves better F1 scores in hierarchy generation as compared to the state-of-the-art methods.

#index 1988793
#* Taily: shard selection using the tail of score distributions
#@ Robin Aly;Djoerd Hiemstra;Thomas Demeester
#t 2013
#c 13
#% 116040
#% 176550
#% 194246
#% 280856
#% 481748
#% 643012
#% 818262
#% 976948
#% 1227597
#% 1227629
#% 1292595
#% 1450859
#% 1482223
#% 1496276
#% 1558080
#% 1565813
#% 1598559
#% 1918390
#! Search engines can improve their efficiency by selecting only few promising shards for each query. State-of-the-art shard selection algorithms first query a central index of sampled documents, and their effectiveness is similar to searching all shards. However, the search in the central index also hurts efficiency. Additionally, we show that the effectiveness of these approaches varies substantially with the sampled documents. This paper proposes Taily, a novel shard selection algorithm that models a query's score distribution in each shard as a Gamma distribution and selects shards with highly scored documents in the tail of the distribution. Taily estimates the parameters of score distributions based on the mean and variance of the score function's features in the collections and shards. Because Taily operates on term statistics instead of document samples, it is efficient and has deterministic effectiveness. Experiments on large web collections (Gov2, CluewebA and CluewebB) show that Taily achieves similar effectiveness to sample-based approaches, and improves upon their efficiency by roughly 20% in terms of used resources and response time.

#index 1988794
#* Deciding on an adjustment for multiplicity in IR experiments
#@ Leonid Boytsov;Anna Belova;Peter Westfall
#t 2013
#c 13
#% 165307
#% 236052
#% 245771
#% 262096
#% 340948
#% 818222
#% 906600
#% 961200
#% 987311
#% 1002317
#% 1019124
#% 1041316
#% 1130864
#% 1292528
#% 1598440
#% 1642487
#% 1667279
#! We evaluate statistical inference procedures for small-scale IR experiments that involve multiple comparisons against the baseline. These procedures adjust for multiple comparisons by ensuring that the probability of observing at least one false positive in the experiment is below a given threshold. We use only publicly available test collections and make our software available for download. In particular, we employ the TREC runs and runs constructed from the Microsoft learning-to-rank (MSLR) data set. Our focus is on non-parametric statistical procedures that include the Holm-Bonferroni adjustment of the permutation test p-values, the MaxT permutation test, and the permutation-based closed testing. In TREC-based simulations, these procedures retain from 66% to 92% of individually significant results (i.e., those obtained without taking other comparisons into account). Similar retention rates are observed in the MSLR simulations. For the largest evaluated query set size (i.e., 6400), procedures that adjust for multiplicity find at most 5% fewer true differences compared to unadjusted tests. At the same time, unadjusted tests produce many more false positives.

#index 1988795
#* Addressing cold-start in app recommendation: latent user models constructed from twitter followers
#@ Jovian Lin;Kazunari Sugiyama;Min-Yen Kan;Tat-Seng Chua
#t 2013
#c 13
#% 280447
#% 280852
#% 330687
#% 397155
#% 722904
#% 734592
#% 818216
#% 983903
#% 989580
#% 1083671
#% 1127483
#% 1249487
#% 1273828
#% 1287222
#% 1345710
#% 1355042
#% 1399992
#% 1476461
#% 1536509
#% 1560425
#% 1598365
#% 1598396
#% 1605963
#% 1625347
#% 1650569
#! As a tremendous number of mobile applications (apps) are readily available, users have difficulty in identifying apps that are relevant to their interests. Recommender systems that depend on previous user ratings (i.e., collaborative filtering, or CF) can address this problem for apps that have sufficient ratings from past users. But for apps that are newly released, CF does not have any user ratings to base recommendations on, which leads to the cold-start problem. In this paper, we describe a method that accounts for nascent information culled from Twitter to provide relevant recommendation in such cold-start situations. We use Twitter handles to access an app's Twitter account and extract the IDs of their Twitter-followers. We create pseudo-documents that contain the IDs of Twitter users interested in an app and then apply latent Dirichlet allocation to generate latent groups. At test time, a target user seeking recommendations is mapped to these latent groups. By using the transitive relationship of latent groups to apps, we estimate the probability of the user liking the app. We show that by incorporating information from Twitter, our approach overcomes the difficulty of cold-start app recommendation and significantly outperforms other state-of-the-art recommendation techniques by up to 33%.

#index 1988796
#* Incorporating vertical results into search click models
#@ Chao Wang;Yiqun Liu;Min Zhang;Shaoping Ma;Meihong Zheng;Jing Qian;Kuo Zhang
#t 2013
#c 13
#% 766472
#% 805878
#% 818221
#% 946521
#% 956546
#% 1035578
#% 1074092
#% 1153650
#% 1166523
#% 1190055
#% 1190056
#% 1227616
#% 1400034
#% 1450915
#% 1587348
#% 1606083
#% 1641937
#% 1693893
#% 1693908
#% 1879074
#% 1920006
#! In modern search engines, an increasing number of search result pages (SERPs) are federated from multiple specialized search engines (called verticals, such as Image or Video). As an effective approach to interpret users' click-through behavior as feedback information, most click models were designed to reduce the position bias and improve ranking performance of ordinary search results, which have homogeneous appearances. However, when vertical results are combined with ordinary ones, significant differences in presentation may lead to user behavior biases and thus failure of state-of-the-art click models. With the help of a popular commercial search engine in China, we collected a large scale log data set which contains behavior information on both vertical and ordinary results. We also performed eye-tracking analysis to study user's real-world examining behavior. According these analysis, we found that different result appearances may cause different behavior biases both for vertical results (local effect) and for the whole result lists (global effect). These biases include: examine bias for vertical results (especially those with multimedia components), trust bias for result lists with vertical results, and a higher probability of result revisitation for vertical results. Based on these findings, a novel click model considering these biases besides position bias was constructed to describe interaction with SERPs containing verticals. Experimental results show that the new Vertical-aware Click Model (VCM) is better at interpreting user click behavior on federated searches in terms of both log-likelihood and perplexity than existing models.

#index 1988797
#* Semantic hashing using tags and topic modeling
#@ Qifan Wang;Dan Zhang;Luo Si
#t 2013
#c 13
#% 46803
#% 190581
#% 280819
#% 341704
#% 479649
#% 479973
#% 643007
#% 722904
#% 762054
#% 763708
#% 867054
#% 891060
#% 898309
#% 987258
#% 1130996
#% 1176909
#% 1215859
#% 1450831
#% 1482300
#% 1486652
#% 1598356
#% 1605963
#% 1693873
#% 1878998
#% 1884343
#% 1924919
#% 1931623
#! It is an important research problem to design efficient and effective solutions for large scale similarity search. One popular strategy is to represent data examples as compact binary codes through semantic hashing, which has produced promising results with fast search speed and low storage cost. Many existing semantic hashing methods generate binary codes for documents by modeling document relationships based on similarity in a keyword feature space. Two major limitations in existing methods are: (1) Tag information is often associated with documents in many real world applications, but has not been fully exploited yet; (2) The similarity in keyword feature space does not fully reflect semantic relationships that go beyond keyword matching. This paper proposes a novel hashing approach, Semantic Hashing using Tags and Topic Modeling (SHTTM), to incorporate both the tag information and the similarity information from probabilistic topic modeling. In particular, a unified framework is designed for ensuring hashing codes to be consistent with tag information by a formal latent factor model and preserving the document topic/semantic similarity that goes beyond keyword matching. An iterative coordinate descent procedure is proposed for learning the optimal hashing codes. An extensive set of empirical studies on four different datasets has been conducted to demonstrate the advantages of the proposed SHTTM approach against several other state-of-the-art semantic hashing techniques. Furthermore, experimental results indicate that the modeling of tag information and utilizing topic modeling are beneficial for improving the effectiveness of hashing separately, while the combination of these two techniques in the unified framework obtains even better results.

#index 1988798
#* On the measurement of test collection reliability
#@ Julián Urbano;Mónica Marrero;Diego Martín
#t 2013
#c 13
#% 262102
#% 262105
#% 309093
#% 318407
#% 397163
#% 818222
#% 818309
#% 948924
#% 987238
#% 1043056
#% 1074124
#% 1074132
#% 1195853
#% 1227745
#% 1292527
#% 1879081
#% 1988923
#! The reliability of a test collection is proportional to the number of queries it contains. But building a collection with many queries is expensive, so researchers have to find a balance between reliability and cost. Previous work on the measurement of test collection reliability relied on data-based approaches that contemplated random what if scenarios, and provided indicators such as swap rates and Kendall tau correlations. Generalizability Theory was proposed as an alternative founded on analysis of variance that provides reliability indicators based on statistical theory. However, these reliability indicators are hard to interpret in practice, because they do not correspond to well known indicators like Kendall tau correlation. We empirically established these relationships based on data from over 40 TREC collections, thus filling the gap in the practical interpretation of Generalizability Theory. We also review the computation of these indicators, and show that they are extremely dependent on the sample of systems and queries used, so much that the required number of queries to achieve a certain level of reliability can vary in orders of magnitude. We discuss the computation of confidence intervals for these statistics, providing a much more reliable tool to measure test collection reliability. Reflecting upon all these results, we review a wealth of TREC test collections, arguing that they are possibly not as reliable as generally accepted and that the common choice of 50 queries is insufficient even for stable rankings.

#index 1988799
#* A low rank structural large margin method for cross-modal ranking
#@ Xinyan Lu;Fei Wu;Siliang Tang;Zhongfei Zhang;Xiaofei He;Yueting Zhuang
#t 2013
#c 13
#% 280819
#% 387427
#% 577224
#% 642990
#% 722904
#% 760805
#% 829043
#% 840882
#% 855563
#% 961152
#% 983820
#% 983905
#% 987226
#% 1069003
#% 1083633
#% 1166508
#% 1227621
#% 1264133
#% 1442580
#% 1484424
#% 1604467
#% 1775721
#% 1856503
#% 1884410
#% 1885812
#! Cross-modal retrieval is a classic research topic in multimedia information retrieval. The traditional approaches study the problem as a pairwise similarity function problem. In this paper, we consider this problem from a new perspective as a listwise ranking problem and propose a general cross-modal ranking algorithm to optimize the listwise ranking loss with a low rank embedding, which we call Latent Semantic Cross-Modal Ranking (LSCMR). The latent low-rank embedding space is discriminatively learned by structural large margin learning to optimize for certain ranking criteria directly. We evaluate LSCMR on the Wikipedia and NUS-WIDE dataset. Experimental results show that this method obtains significant improvements over the state-of-the-art methods.

#index 1988800
#* Learning to name faces: a multimodal learning scheme for search-based face annotation
#@ Dayong Wang;Steven C.H. Hoi;Pengcheng Wu;Jianke Zhu;Ying He;Chunyan Miao
#t 2013
#c 13
#% 434882
#% 457912
#% 780804
#% 829043
#% 884027
#% 884043
#% 884044
#% 902519
#% 905209
#% 913845
#% 961218
#% 975105
#% 987226
#% 997184
#% 1038781
#% 1074064
#% 1091874
#% 1148273
#% 1149120
#% 1164188
#% 1174739
#% 1176935
#% 1264133
#% 1279781
#% 1484400
#% 1493660
#% 1495378
#% 1502510
#% 1505154
#% 1598387
#% 1649049
#% 1694055
#% 1775706
#% 1775748
#% 1775920
#% 1855337
#% 1915613
#! Automated face annotation aims to automatically detect human faces from a photo and further name the faces with the corresponding human names. In this paper, we tackle this open problem by investigating a search-based face annotation (SBFA) paradigm for mining large amounts of web facial images freely available on the WWW. Given a query facial image for annotation, the idea of SBFA is to first search for top-n similar facial images from a web facial image database and then exploit these top-ranked similar facial images and their weak labels for naming the query facial image. To fully mine those information, this paper proposes a novel framework of Learning to Name Faces (L2NF) -- a unified multimodal learning approach for search-based face annotation, which consists of the following major components: (i) we enhance the weak labels of top-ranked similar images by exploiting the "label smoothness" assumption; (ii) we construct the multimodal representations of a facial image by extracting different types of features; (iii) we optimize the distance measure for each type of features using distance metric learning techniques; and finally (iv) we learn the optimal combination of multiple modalities for annotation through a learning to rank scheme. We conduct a set of extensive empirical studies on two real-world facial image databases, in which encouraging results show that the proposed algorithms significantly boost the naming accuracy of search-based face annotation task.

#index 1988801
#* User model-based metrics for offline query suggestion evaluation
#@ Eugene Kharitonov;Craig Macdonald;Pavel Serdyukov;Iadh Ounis
#t 2013
#c 13
#% 411762
#% 561315
#% 1035578
#% 1074092
#% 1074133
#% 1130811
#% 1190055
#% 1292528
#% 1450898
#% 1450912
#% 1482378
#% 1560365
#% 1560366
#% 1598347
#% 1606083
#% 1641961
#% 1648026
#% 1712595
#% 1747072
#% 1879052
#% 1918348
#% 1919823
#! Query suggestion or auto-completion mechanisms are widely used by search engines and are increasingly attracting interest from the research community. However, the lack of commonly accepted evaluation methodology and metrics means that it is not possible to compare results and approaches from the literature. Moreover, often the metrics used to evaluate query suggestions tend to be an adaptation from other domains without a proper justification. Hence, it is not necessarily clear if the improvements reported in the literature would result in an actual improvement in the users' experience. Inspired by the cascade user models and state-of-the-art evaluation metrics in the web search domain, we address the query suggestion evaluation, by first studying the users behaviour from a search engine's query log and thereby deriving a new family of user models describing the users interaction with a query suggestion mechanism. Next, assuming a query log-based evaluation approach, we propose two new metrics to evaluate query suggestions, pSaved and eSaved. Both metrics are parameterised by a user model. pSaved is defined as the probability of using the query suggestions while submitting a query. eSaved equates to the expected relative amount of effort (keypresses) a user can avoid due to the deployed query suggestion mechanism. Finally, we experiment with both metrics using four user model instantiations as well as metrics previously used in the literature on a dataset of 6.1M sessions. Our results demonstrate that pSaved and eSaved show the best alignment with the users satisfaction amongst the considered metrics.

#index 1988802
#* Ranking document clusters using markov random fields
#@ Fiana Raiber;Oren Kurland
#t 2013
#c 13
#% 118771
#% 228105
#% 262112
#% 268079
#% 329698
#% 340948
#% 342660
#% 375017
#% 427921
#% 766430
#% 766431
#% 772018
#% 818241
#% 818262
#% 879575
#% 881477
#% 987226
#% 1074119
#% 1213624
#% 1400021
#% 1415748
#% 1450860
#% 1536512
#% 1550713
#% 1621236
#% 1631457
#% 1763374
#% 1919963
#! An important challenge in cluster-based document retrieval is ranking document clusters by their relevance to the query. We present a novel cluster ranking approach that utilizes Markov Random Fields (MRFs). MRFs enable the integration of various types of cluster-relevance evidence; e.g., the query-similarity values of the cluster's documents and query-independent measures of the cluster. We use our method to re-rank an initially retrieved document list by ranking clusters that are created from the documents most highly ranked in the list. The resultant retrieval effectiveness is substantially better than that of the initial list for several lists that are produced by effective retrieval methods. Furthermore, our cluster ranking approach significantly outperforms state-of- the-art cluster ranking methods. We also show that our method can be used to improve the performance of (state-of- the-art) results-diversification methods.

#index 1988803
#* Toward self-correcting search engines: using underperforming queries to improve search
#@ Ahmed Hassan;Ryen W. White;Yi-Min Wang
#t 2013
#c 13
#% 152934
#% 397161
#% 411762
#% 481290
#% 729418
#% 805200
#% 817472
#% 818916
#% 879565
#% 879567
#% 879613
#% 956495
#% 956541
#% 987241
#% 987260
#% 987263
#% 987321
#% 1074071
#% 1127463
#% 1130851
#% 1275193
#% 1292474
#% 1301004
#% 1355038
#% 1415709
#% 1450833
#% 1450902
#% 1537504
#% 1598347
#% 1598367
#% 1598368
#% 1641927
#% 1642132
#% 1747011
#! Search engines receive queries with a broad range of different search intents. However, they do not perform equally well for all queries. Understanding where search engines perform poorly is critical for improving their performance. In this paper, we present a method for automatically identifying poorly-performing query groups where a search engine may not meet searcher needs. This allows us to create coherent query clusters that help system design-ers generate actionable insights about necessary changes and helps learning-to-rank algorithms better learn relevance signals via spe-cialized rankers. The result is a framework capable of estimating dissatisfaction from Web search logs and learning to improve per-formance for dissatisfied queries. Through experimentation, we show that our method yields good quality groups that align with established retrieval performance metrics. We also show that we can significantly improve retrieval effectiveness via specialized rankers, and that coherent grouping of underperforming queries generated by our method is important in improving each group.

#index 1988804
#* Exploiting hybrid contexts for Tweet segmentation
#@ Chenliang Li;Aixin Sun;Jianshu Weng;Qi He
#t 2013
#c 13
#% 278106
#% 741058
#% 742204
#% 815855
#% 815922
#% 939376
#% 1249541
#% 1292645
#% 1468142
#% 1591965
#% 1591966
#% 1592152
#% 1642034
#% 1711750
#% 1711864
#% 1872274
#% 1872363
#% 1879064
#% 1913070
#% 1918350
#% 1919803
#% 1919819
#! Twitter has attracted hundred millions of users to share and disseminate most up-to-date information. However, the noisy and short nature of tweets makes many applications in information retrieval (IR) and natural language processing (NLP) challenging. Recently, segment-based tweet representation has demonstrated effectiveness in named entity recognition (NER) and event detection from tweet streams. To split tweets into meaningful phrases or segments, the previous work is purely based on external knowledge bases, which ignores the rich local context information embedded in the tweets. In this paper, we propose a novel framework for tweet segmentation in a batch mode, called HybridSeg. HybridSeg incorporates local context knowledge with global knowledge bases for better tweet segmentation. HybridSeg consists of two steps: learning from off-the-shelf weak NERs and learning from pseudo feedback. In the first step, the existing NER tools are applied to a batch of tweets. The named entities recognized by these NERs are then employed to guide the tweet segmentation process. In the second step, HybridSeg adjusts the tweet segmentation results iteratively by exploiting all segments in the batch of tweets in a collective manner. Experiments on two tweet datasets show that HybridSeg significantly improves tweet segmentation quality compared with the state-of-the-art algorithm. We also conduct a case study by using tweet segments for the task of named entity recognition from tweets. The experimental results demonstrate that HybridSeg significantly benefits the downstream applications.

#index 1988805
#* Sumblr: continuous summarization of evolving tweet streams
#@ Lidan Shou;Zhenhua Wang;Ke Chen;Gang Chen
#t 2013
#c 13
#% 210173
#% 262112
#% 310516
#% 340884
#% 787502
#% 816173
#% 856296
#% 1015261
#% 1074088
#% 1074089
#% 1272053
#% 1275040
#% 1464785
#% 1470662
#% 1488059
#% 1573368
#% 1581900
#% 1587351
#% 1598408
#% 1746875
#% 1872273
#% 1918352
#! With the explosive growth of microblogging services, short-text messages (also known as tweets) are being created and shared at an unprecedented rate. Tweets in its raw form can be incredibly informative, but also overwhelming. For both end-users and data analysts it is a nightmare to plow through millions of tweets which contain enormous noises and redundancies. In this paper, we study continuous tweet summarization as a solution to address this problem. While traditional document summarization methods focus on static and small-scale data, we aim to deal with dynamic, quickly arriving, and large-scale tweet streams. We propose a novel prototype called Sumblr (SUMmarization By stream cLusteRing) for tweet streams. We first propose an online tweet stream clustering algorithm to cluster tweets and maintain distilled statistics called Tweet Cluster Vectors. Then we develop a TCV-Rank summarization technique for generating online summaries and historical summaries of arbitrary time durations. Finally, we describe a topic evolvement detection method, which consumes online and historical summaries to produce timelines automatically from tweet streams. Our experiments on large-scale real tweets demonstrate the efficiency and effectiveness of our approach.

#index 1988806
#* The impact of solid state drive on search engine cache management
#@ Jianguo Wang;Eric Lo;Man Lung Yiu;Jiancong Tong;Gang Wang;Xiaoguang Liu
#t 2013
#c 13
#% 713
#% 198335
#% 262036
#% 340888
#% 348037
#% 397151
#% 578337
#% 729343
#% 729852
#% 730065
#% 805864
#% 829901
#% 860861
#% 902938
#% 960238
#% 978505
#% 987208
#% 987215
#% 1053488
#% 1055710
#% 1055849
#% 1063551
#% 1089473
#% 1127391
#% 1127538
#% 1129953
#% 1166469
#% 1180789
#% 1183354
#% 1190098
#% 1195885
#% 1200287
#% 1213385
#% 1213690
#% 1217151
#% 1217213
#% 1306948
#% 1328052
#% 1328139
#% 1426437
#% 1426521
#% 1468260
#% 1468306
#% 1482312
#% 1523901
#% 1523920
#% 1549844
#% 1558842
#% 1560176
#% 1581847
#% 1587192
#% 1587384
#% 1668634
#% 1765833
#% 1834787
#% 1870668
#% 1890012
#% 1899642
#% 1919741
#% 1929685
#! Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this paper, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. The results give insights to practitioners and researchers on how to adapt the infrastructure and how to redesign the caching policies for SSD-based search engines.

#index 1988807
#* Modeling user's receptiveness over time for recommendation
#@ Wei Chen;Wynne Hsu;Mong Li Lee
#t 2013
#c 13
#% 220709
#% 301259
#% 330687
#% 1214661
#% 1214666
#% 1227601
#% 1227602
#% 1270334
#% 1287243
#% 1400014
#% 1451212
#% 1536533
#% 1625364
#% 1826483
#% 1872386
#% 1879059
#! Existing recommender systems model user interests and the social influences independently. In reality, user interests may change over time, and as the interests change, new friends may be added while old friends grow apart and the new friendships formed may cause further interests change. This complex interaction requires the joint modeling of user interest and social relationships over time. In this paper, we propose a probabilistic generative model, called Receptiveness over Time Model (RTM), to capture this interaction. We design a Gibbs sampling algorithm to learn the receptiveness and interest distributions among users over time. The results of experiments on a real world dataset demonstrate that RTM-based recommendation outperforms the state-of-the-art recommendation methods. Case studies also show that RTM is able to discover the user interest shift and receptiveness change over time

#index 1988808
#* Competence-based song recommendation
#@ Lidan Shou;Kuang Mao;Xinyuan Luo;Ke Chen;Gang Chen;Tianlei Hu
#t 2013
#c 13
#% 124010
#% 194192
#% 309095
#% 387427
#% 730139
#% 983820
#% 1879109
#! Singing is a popular social activity and a good way of expressing one's feelings. One important reason for unsuccessful singing performance is because the singer fails to choose a suitable song. In this paper, we propose a novel singing competence-based song recommendation framework. It is distinguished from most existing music recommendation systems which rely on the computation of listeners' interests or similarity. We model a singer's vocal competence as singer profile, which takes voice pitch, intensity, and quality into consideration. Then we propose techniques to acquire singer profiles. We also present a song profile model which is used to construct a human annotated song database. Finally, we propose a learning-to-rank scheme for recommending songs by singer profile. The experimental study on real singers demonstrates the effectiveness of our approach and its advantages over two baseline methods. To the best of our knowledge, our work is the first to study competence-based song recommendation.

#index 1988809
#* How query cost affects search behavior
#@ Leif Azzopardi;Diane Kelly;Kathy Brennan
#t 2013
#c 13
#% 80112
#% 133894
#% 147960
#% 214711
#% 214871
#% 319640
#% 340921
#% 344926
#% 411762
#% 643000
#% 943946
#% 1051038
#% 1074069
#% 1095876
#% 1181094
#% 1227623
#% 1268490
#% 1279761
#% 1389359
#% 1405707
#% 1450895
#% 1480199
#% 1482378
#% 1598335
#% 1631716
#% 1879002
#% 1879003
#! affects how users interact with a search system. Microeconomic theory is used to generate the cost-interaction hypothesis that states as the cost of querying increases, users will pose fewer queries and examine more documents per query. A between-subjects laboratory study with 36 undergraduate subjects was conducted, where subjects were randomly assigned to use one of three search interfaces that varied according to the amount of physical cost required to query: Structured (high cost), Standard (medium cost) and Query Suggestion (low cost). Results show that subjects who used the Structured interface submitted significantly fewer queries, spent more time on search results pages, examined significantly more documents per query, and went to greater depths in the search results list. Results also showed that these subjects spent longer generating their initial queries, saved more relevant documents and rated their queries as more successful. These findings have implications for the usefulness of microeconomic theory as a way to model and explain search interaction, as well as for the design of query facilities.

#index 1988810
#* Aggregated search interface preferences in multi-session search tasks
#@ Marc Bron;Jasmijn van Gorp;Frank Nack;Lotte Belice Baltussen;Maarten de Rijke
#t 2013
#c 13
#% 308745
#% 349274
#% 565237
#% 581916
#% 814947
#% 879622
#% 947011
#% 1137657
#% 1173627
#% 1185582
#% 1267046
#% 1314928
#% 1384094
#% 1450832
#% 1467778
#% 1482230
#% 1565813
#% 1573487
#% 1587348
#% 1587372
#% 1602706
#% 1622356
#% 1624266
#% 1641937
#% 1714300
#% 1879035
#% 1879036
#% 1919737
#% 1988938
#% 2004721
#! Aggregated search interfaces provide users with an overview of results from various sources. Two general types of display exist: tabbed, with access to each source in a separate tab, and blended, which combines multiple sources into a single result page. Multi-session search tasks, e.g., a research project, consist of multiple stages, each with its own sub-tasks. Several factors involved in multi-session search tasks have been found to influence user search behavior. We investigate whether user preference for source presentation changes during a multi-session search task. The dynamic nature of multi-session search tasks makes the design of a controlled experiment a non-trivial challenge. We adopt a methodology based on triangulation and conduct two types of observational study: a longitudinal study and a laboratory study. In the longitudinal study we follow the use of tabbed and blended displays by 25 students during a project. We find that while a tabbed display is used more than a blended display, subjects repeatedly switch between displays during the project. Use of the tabbed display is motivated by a need to zoom in on a specific source, while the blended display is used to explore available material across sources whenever the information need changes. In a laboratory study 44 students completed a multi-session search task composed of three sub-tasks, the first with a tabbed display, the second and third with blended displays. The tasks were manipulated by either providing three task about the same topic or about three different topics. We find that a stable information need over multiple sub-tasks negatively influences perceived usability of the blended displays, while we do not find an influence when the information need changes.

#index 1988811
#* An incremental approach to efficient pseudo-relevance feedback
#@ Hao Wu;Hui Fang
#t 2013
#c 13
#% 198335
#% 213786
#% 228097
#% 290703
#% 298183
#% 340886
#% 340901
#% 342707
#% 397123
#% 397608
#% 730065
#% 818229
#% 818230
#% 818232
#% 867054
#% 879585
#% 879611
#% 944348
#% 987214
#% 987230
#% 1074080
#% 1074081
#% 1216713
#% 1227583
#% 1227584
#% 1450901
#% 1482285
#% 1482301
#% 1482381
#% 1598350
#% 1598433
#% 1641949
#% 1879046
#! Pseudo-relevance feedback is an important strategy to improve search accuracy. It is often implemented as a two-round retrieval process: the first round is to retrieve an initial set of documents relevant to an original query, and the second round is to retrieve final retrieval results using the original query expanded with terms selected from the previously retrieved documents. This two-round retrieval process is clearly time consuming, which could arguably be one of main reasons that hinder the wide adaptation of the pseudo-relevance feedback methods in real-world IR systems. In this paper, we study how to improve the efficiency of pseudo-relevance feedback methods. The basic idea is to reduce the time needed for the second round of retrieval by leveraging the query processing results of the first round. Specifically, instead of processing the expand query as a newly submitted query, we propose an incremental approach, which resumes the query processing results (i.e. document accumulators) for the first round of retrieval and process the second round of retrieval mainly as a step of adjusting the scores in the accumulators. Experimental results on TREC Terabyte collections show that the proposed incremental approach can improve the efficiency of pseudo-relevance feedback methods by a factor of two without sacrificing their effectiveness.

#index 1988812
#* Personalized time-aware tweets summarization
#@ Zhaochun Ren;Shangsong Liang;Edgar Meij;Maarten de Rijke
#t 2013
#c 13
#% 280819
#% 722904
#% 787502
#% 788094
#% 875959
#% 876067
#% 1190062
#% 1227602
#% 1272053
#% 1275221
#% 1305518
#% 1355042
#% 1399992
#% 1470662
#% 1536533
#% 1560408
#% 1587351
#% 1591967
#% 1598359
#% 1598408
#% 1642003
#% 1693876
#% 1693918
#% 1729391
#% 1879047
#% 1879058
#% 1879059
#% 1913299
#% 1918351
#% 1958670
#! We focus on the problem of selecting meaningful tweets given a user's interests; the dynamic nature of user interests, the sheer volume, and the sparseness of individual messages make this an challenging problem. Specifically, we consider the task of time-aware tweets summarization, based on a user's history and collaborative social influences from ``social circles.'' We propose a time-aware user behavior model, the Tweet Propagation Model (TPM), in which we infer dynamic probabilistic distributions over interests and topics. We then explicitly consider novelty, coverage, and diversity to arrive at an iterative optimization algorithm for selecting tweets. Experimental results validate the effectiveness of our personalized time-aware tweets summarization method based on TPM.

#index 1988813
#* Beliefs and biases in web search
#@ Ryen White
#t 2013
#c 13
#% 169739
#% 186518
#% 321635
#% 397160
#% 399058
#% 577224
#% 642975
#% 754060
#% 773040
#% 818259
#% 838406
#% 879567
#% 946521
#% 956495
#% 956552
#% 987209
#% 1035578
#% 1055676
#% 1250379
#% 1264744
#% 1278069
#% 1400034
#% 1450885
#% 1482279
#% 1573489
#% 1613005
#% 1693903
#% 1693905
#% 1879011
#% 1879019
#! People's beliefs, and unconscious biases that arise from those beliefs, influence their judgment, decision making, and actions, as is commonly accepted among psychologists. Biases can be observed in information retrieval in situations where searchers seek or are presented with information that significantly deviates from the truth. There is little understanding of the impact of such biases in search. In this paper we study search-related biases via multiple probes: an exploratory retrospective survey, human labeling of the captions and results returned by a Web search engine, and a large-scale log analysis of search behavior on that engine. Targeting yes-no questions in the critical domain of health search, we show that Web searchers exhibit their own biases and are also subject to bias from the search engine. We clearly observe searchers favoring positive information over negative and more than expected given base rates based on consensus answers from physicians. We also show that search engines strongly favor a particular, usually positive, perspective, irrespective of the truth. Importantly, we show that these biases can be counterproductive and affect search outcomes; in our study, around half of the answers that searchers settled on were actually incorrect. Our findings have implications for search engine design, including the development of ranking algorithms that con-sider the desire to satisfy searchers (by validating their beliefs) and providing accurate answers and properly considering base rates. Incorporating likelihood information into search is particularly important for consequential tasks, such as those with a medical focus.

#index 1988814
#* Query representation for cross-temporal information retrieval
#@ Miles Efron
#t 2013
#c 13
#% 219033
#% 251405
#% 280826
#% 280851
#% 340901
#% 504885
#% 562054
#% 614083
#% 643017
#% 730070
#% 766503
#% 807750
#% 818262
#% 935763
#% 940343
#% 987231
#% 1107069
#% 1348327
#% 1450869
#% 1483085
#% 1581252
#% 1642181
#% 1771897
#% 1918353
#! This paper addresses the problem of long-term language change in information retrieval (IR) systems. IR research has often ignored lexical drift. But in the emerging domain of massive digitized book collections, the risk of vocabulary mismatch due to language change is high. Collections such as Google Books and the Hathi Trust contain text written in the vernaculars of many centuries. With respect to IR, changes in vocabulary and orthography make 14th-Century English qualitatively different from 21st-Century English. This challenges retrieval models that rely on keyword matching. With this challenge in mind, we ask: given a query written in contemporary English, how can we retrieve relevant documents that were written in early English? We argue that search in historically diverse corpora is similar to cross-language retrieval (CLIR). By considering "modern" English and "archaic" English as distinct languages, CLIR techniques can improve what we call cross-temporal IR (CTIR). We focus on ways to combine evidence to improve CTIR effectiveness, proposing and testing several ways to handle language change during book search. We find that a principled combination of three sources of evidence during relevance feedback yields strong CTIR performance.

#index 1988815
#* Utilizing query change for session search
#@ Dongyi Guan;Sicong Zhang;Hui Yang
#t 2013
#c 13
#% 289101
#% 309146
#% 677141
#% 742769
#% 750863
#% 789959
#% 822126
#% 1074071
#% 1074098
#% 1130868
#% 1130878
#% 1272286
#% 1292473
#% 1400023
#% 1450832
#% 1450845
#% 1450893
#% 1523401
#% 1598340
#% 1621236
#% 1641985
#% 1693897
#% 1879013
#% 1967774
#! Session search is the Information Retrieval (IR) task that performs document retrieval for a search session. During a session, a user constantly modifies queries in order to find relevant documents that fulfill the information need. This paper proposes a novel query change retrieval model (QCM), which utilizes syntactic editing changes between adjacent queries as well as the relationship between query change and previously retrieved documents to enhance session search. We propose to model session search as a Markov Decision Process (MDP). We consider two agents in this MDP: the user agent and the search engine agent. The user agent's actions are query changes that we observe and the search agent's actions are proposed in this paper. Experiments show that our approach is highly effective and outperforms top session search systems in TREC 2011 and 2012.

#index 1988816
#* Leveraging conceptual lexicon: query disambiguation using proximity information for patent retrieval
#@ Parvaz Mahdabi;Shima Gerani;Jimmy Xiangji Huang;Fabio Crestani
#t 2013
#c 13
#% 232703
#% 340948
#% 1130863
#% 1227614
#% 1292546
#% 1292765
#% 1450901
#% 1450905
#% 1494806
#% 1576951
#% 1642152
#% 1643122
#% 1697451
#% 1697454
#% 1806009
#% 1827239
#% 1879043
#% 1907195
#! Patent prior art search is a task in patent retrieval where the goal is to rank documents which describe prior art work related to a patent application. One of the main properties of patent retrieval is that the query topic is a full patent application and does not represent a focused information need. This query by document nature of patent retrieval introduces new challenges and requires new investigations specific to this problem. Researchers have addressed this problem by considering different information resources for query reduction and query disambiguation. However, previous work has not fully studied the effect of using proximity information and exploiting domain specific resources for performing query disambiguation. In this paper, we first reduce the query document by taking the first claim of the document itself. We then build a query-specific patent lexicon based on definitions of the International Patent Classification (IPC). We study how to expand queries by selecting expansion terms from the lexicon that are focused on the query topic. The key problem is how to capture whether an expansion term is focused on the query topic or not. We address this problem by exploiting proximity information. We assign high weights to expansion terms appearing closer to query terms based on the intuition that terms closer to query terms are more likely to be related to the query topic. Experimental results on two patent retrieval datasets show that the proposed method is effective and robust for query expansion, significantly outperforming the standard pseudo relevance feedback (PRF) and existing baselines in patent retrieval.

#index 1988817
#* Emerging topic detection for organizations from microblogs
#@ Yan Chen;Hadi Amiri;Zhoujun Li;Tat-Seng Chua
#t 2013
#c 13
#% 262042
#% 262043
#% 311027
#% 466263
#% 722904
#% 875959
#% 881498
#% 987218
#% 1355045
#% 1400018
#% 1426611
#% 1432574
#% 1482457
#% 1536528
#% 1561556
#% 1561559
#% 1642000
#% 1693930
#% 1746888
#% 1872242
#! Microblog services have emerged as an essential way to strengthen the communications among individuals and organizations. These services promote timely and active discussions and comments towards products, markets as well as public events, and have attracted a lot of attentions from organizations. In particular, emerging topics are of immediate concerns to organizations since they signal current concerns of, and feedback by their users. Two challenges must be tackled for effective emerging topic detection. One is the problem of real-time relevant data collection and the other is the ability to model the emerging characteristics of detected topics and identify them before they become hot topics. To tackle these challenges, we first design a novel scheme to crawl the relevant messages related to the designated organization by monitoring multi-aspects of microblog content, including users, the evolving keywords and their temporal sequence. We then develop an incremental clustering framework to detect new topics, and employ a range of content and temporal features to help in promptly detecting hot emerging topics. Extensive evaluations on a representative real-world dataset based on Twitter data demonstrate that our scheme is able to characterize emerging topics well and detect them before they become hot topics.

#index 1988818
#* Query expansion using path-constrained random walks
#@ Jianfeng Gao;Gu Xu;Jinxi Xu
#t 2013
#c 13
#% 218978
#% 280851
#% 309095
#% 340899
#% 340901
#% 340948
#% 342707
#% 342961
#% 348155
#% 348173
#% 577224
#% 641976
#% 740915
#% 770864
#% 816170
#% 838532
#% 879567
#% 879568
#% 983808
#% 987222
#% 987231
#% 989578
#% 1019094
#% 1035578
#% 1074081
#% 1096052
#% 1130854
#% 1173699
#% 1190106
#% 1227584
#% 1227621
#% 1251678
#% 1343447
#% 1399978
#% 1457044
#% 1482292
#% 1484281
#% 1536566
#% 1549092
#% 1598401
#% 1693902
#% 1913630
#% 1913662
#! This paper exploits Web search logs for query expansion (QE) by presenting a new QE method based on path-constrained random walks (PCRW), where the search logs are represented as a labeled, directed graph, and the probability of picking an expansion term for an input query is computed by a learned combination of constrained random walks on the graph. The method is shown to be generic in that it covers most of the popular QE models as special cases and flexible in that it provides a principled mathematical framework in which a wide variety of information useful for QE can be incorporated in a unified way. Evaluation is performed on the Web document ranking task using a real-world data set. Results show that the PCRW-based method is very effective for the expansion of rare queries, i.e., low-frequency queries that are unseen in search logs, and that it outperforms significantly other state-of-the-art QE meth-ods.

#index 1988819
#* An experimental study on implicit social recommendation
#@ Hao Ma
#t 2013
#c 13
#% 280852
#% 330687
#% 452563
#% 643007
#% 734592
#% 734594
#% 766449
#% 790459
#% 840924
#% 987198
#% 1001279
#% 1073982
#% 1074061
#% 1083671
#% 1214661
#% 1214666
#% 1227603
#% 1260273
#% 1275183
#% 1287243
#% 1473316
#% 1476461
#% 1536533
#% 1558466
#% 1625374
#% 1650569
#! Social recommendation problems have drawn a lot of attention recently due to the prevalence of social networking sites. The experiments in previous literature suggest that social information is very effective in improving traditional recommendation algorithms. However, explicit social information is not always available in most of the recommender systems, which limits the impact of social recommendation techniques. In this paper, we study the following two research problems: (1) In some systems without explicit social information, can we still improve recommender systems using implicit social information? (2) In the systems with explicit social information, can the performance of using implicit social information outperform that of using explicit social information? In order to answer these two questions, we conduct comprehensive experimental analysis on three recommendation datasets. The result indicates that: (1) Implicit user and item social information, including similar and dissimilar relationships, can be employed to improve traditional recommendation methods. (2) When comparing implicit social information with explicit social information, the performance of using implicit information is slightly worse. This study provides additional insights to social recommendation techniques, and also greatly widens the utility and spreads the impact of previous and upcoming social recommendation approaches.

#index 1988820
#* Sentiment diversification with different biases
#@ Elif Aktolga;James Allan
#t 2013
#c 13
#% 262112
#% 642975
#% 748738
#% 818262
#% 879618
#% 1019145
#% 1074133
#% 1074158
#% 1117691
#% 1130988
#% 1166473
#% 1195913
#% 1227591
#% 1263586
#% 1292562
#% 1400021
#% 1475758
#% 1482296
#% 1560395
#% 1587424
#% 1598393
#% 1641923
#% 1876162
#% 1878999
#% 1879076
#% 1879077
#% 1918387
#% 1920033
#! Prior search result diversification work focuses on achieving topical variety in a ranked list, typically equally across all aspects. In this paper, we diversify with sentiments according to an explicit bias. We want to allow users to switch the result perspective to better grasp the polarity of opinionated content, such as during a literature review. For this, we first infer the prior sentiment bias inherent in a controversial topic -- the 'Topic Sentiment'. Then, we utilize this information in 3 different ways to diversify results according to various sentiment biases: (1) Equal diversification to achieve a balanced and unbiased representation of all sentiments on the topic; (2) Diversification towards the Topic Sentiment, in which the actual sentiment bias in the topic is mirrored to emphasize the general perception of the topic; (3) Diversification against the Topic Sentiment, in which documents about the 'minority' or outlying sentiment(s) are boosted and those with the popular sentiment are demoted. Since sentiment classification is an essential tool for this task, we experiment by gradually degrading the accuracy of a perfect classifier down to 40%, and show which diversification approaches prove most stable in this setting. The results reveal that the proportionality-based methods and our SCSF model, considering sentiment strength and frequency in the diversified list, yield the highest gains. Further, in case the Topic Sentiment cannot be reliably estimated, we show how performance is affected by equal diversification when actually an emphasis either towards or against the Topic Sentiment is desired: in the former case, an average of 6.48% is lost across all evaluation measures, whereas in the latter case this is 16.23%, confirming that bias-specific sentiment diversification is crucial.

#index 1988821
#* An information-theoretic account of static index pruning
#@ Ruey-Cheng Chen;Chia-Jung Lee
#t 2013
#c 13
#% 144074
#% 248214
#% 340886
#% 340887
#% 340899
#% 342707
#% 805862
#% 879611
#% 907504
#% 927337
#% 987323
#% 1195891
#% 1195898
#% 1302865
#% 1392436
#% 1587387
#% 1667277
#% 1919970
#! In this paper, we recast static index pruning as a model induction problem under the framework of Kullback's principle of minimum cross-entropy. We show that static index pruning has an approximate analytical solution in the form of convex integer program. Further analysis on computation feasibility suggests that one of its surrogate model can be solved efficiently. This result has led to the rediscovery of \emph{uniform pruning}, a simple yet powerful pruning method proposed in 2001 and later easily ignored by many of us. To empirically verify this result, we conducted experiments under a new design in which prune ratio is strictly controlled. Our result on standard ad-hoc retrieval benchmarks has confirmed that uniform pruning is robust to high prune ratio and its performance is currently state of the art.

#index 1988822
#* An unsupervised topic segmentation model incorporating word order
#@ Shoaib Jameel;Wai Lam
#t 2013
#c 13
#% 278106
#% 329569
#% 340950
#% 448786
#% 722904
#% 741058
#% 818262
#% 876017
#% 876067
#% 915330
#% 1117083
#% 1205915
#% 1264752
#% 1292645
#% 1412740
#% 1426424
#% 1457042
#% 1736880
#% 1747037
#% 1869234
#% 1872242
#% 1872332
#% 1906965
#% 1913070
#% 1913589
#% 1918412
#% 1967759
#% 1996495
#! We present a new unsupervised topic discovery model for a collection of text documents. In contrast to the majority of the state-of-the-art topic models, our model does not break the document's structure such as paragraphs and sentences. In addition, it preserves word order in the document. As a result, it can generate two levels of topics of different granularity, namely, segment-topics and word-topics. In addition, it can generate n-gram words in each topic. We also develop an approximate inference scheme using Gibbs sampling method. We conduct extensive experiments using publicly available data from different collections and show that our model improves the quality of several text mining tasks such as the ability to support fine grained topics with n-gram words in the correlation graph, the ability to segment a document into topically coherent sections, document classification, and document likelihood estimation.

#index 1988823
#* Pseudo test collections for training and tuning microblog rankers
#@ Richard Berendsen;Manos Tsagkias;Wouter Weerkamp;Maarten de Rijke
#t 2013
#c 13
#% 240146
#% 279755
#% 340948
#% 348308
#% 411760
#% 448736
#% 879598
#% 881477
#% 976949
#% 983905
#% 987249
#% 987356
#% 995516
#% 1074132
#% 1181094
#% 1263575
#% 1268491
#% 1292597
#% 1399992
#% 1450952
#% 1496274
#% 1536506
#% 1581252
#% 1587369
#% 1598383
#% 1598441
#% 1641934
#% 1642150
#% 1742093
#% 1853750
#% 1924107
#% 1958670
#! Recent years have witnessed a persistent interest in generating pseudo test collections, both for training and evaluation purposes. We describe a method for generating queries and relevance judgments for microblog search in an unsupervised way. Our starting point is this intuition: tweets with a hashtag are relevant to the topic covered by the hashtag and hence to a suitable query derived from the hashtag. Our baseline method selects all commonly used hashtags, and all associated tweets as relevance judgments; we then generate a query from these tweets. Next, we generate a timestamp for each query, allowing us to use temporal information in the training process. We then enrich the generation process with knowledge derived from an editorial test collection for microblog search. We use our pseudo test collections in two ways. First, we tune parameters of a variety of well known retrieval methods on them. Correlations with parameter sweeps on an editorial test collection are high on average, with a large variance over retrieval algorithms. Second, we use the pseudo test collections as training sets in a learning to rank scenario. Performance close to training on an editorial test collection is achieved in many cases. Our results demonstrate the utility of tuning and training microblog search algorithms on automatically generated training material.

#index 1988824
#* A location-based news article recommendation with explicit localized semantic analysis
#@ Jeong-Woo Son;A-Yeong Kim;Seong-Bae Park
#t 2013
#c 13
#% 722904
#% 869516
#% 879587
#% 1001296
#% 1263274
#% 1275012
#% 1287227
#% 1338553
#% 1355044
#% 1415731
#% 1480830
#% 1481659
#% 1484671
#% 1558472
#% 1560379
#% 1642705
#% 1695791
#% 1746875
#% 1846818
#% 1913977
#% 1917972
#% 1967307
#! The interest of users in handheld devices is strongly related to their location. Therefore, the user location is important, as a user context, for news article recommendation in a mobile environment. This paper proposes a novel news article recommendation that reflects the geographical context of the user. For this purpose, we propose the Explicit Localized Semantic Analysis (ELSA), an ESA-based topical representation of documents. Every location has its own geographical topics, which can be captured from the geo-tagged documents related to the location. Thus, not only news articles but locations are also represented as topic vectors. The main advantage of ELSA is that it stresses only the topics that are relevant to a given location, whereas all topics are equally important in ESA. As a result, geographical topics have different importance according to the user location in ELSA, even if they come from the same article. Another advantage of ELSA is that it allows a simple comparison of the user location and news articles, because it projects both locations and articles onto an identical space composed of Wikipedia topics. In the evaluation of ELSA with the New York Times corpus, it outperformed two simple baselines of Bag-Of-Words and LDA as well as two ESA-based methods. Rt10 of ELSA was improved up to 46.25% over other methods, and its NDCG@k was always higher than those of the others regardless of k.

#index 1988825
#* Faster upper bounding of intersection sizes
#@ Daisuke Takuma;Hiroki Yanagisawa
#t 2013
#c 13
#% 250012
#% 289178
#% 303072
#% 307424
#% 322884
#% 350564
#% 481290
#% 643566
#% 893128
#% 1127403
#% 1206733
#% 1219786
#% 1227711
#% 1328179
#% 1407167
#% 1483635
#% 1538767
#% 1578302
#% 1667821
#! There is a long history of developing efficient algorithms for set intersection, which is a fundamental operation in information retrieval and databases. In this paper, we describe a new data structure, a Cardinality Filter, to quickly compute an upper bound on the size of a set intersection. Knowing an upper bound of the size can be used to accelerate many applications such as top-k query processing in text mining. Given finite sets A and B, the expected computation time for the upper bound of the size of the intersection |A cap B| is O( (|A| + |B|) w), where w is the machine word length. This is much faster than the current best algorithm for the exact intersection, which runs in O((|A| + |B|) / √w + |A cap B|) expected time. Our performance studies show that our implementations of Cardinality Filters are from 2 to 10 times faster than existing set intersection algorithms, and the time for a top-k query in a text mining application can be reduced by half.

#index 1988826
#* Copulas for information retrieval
#@ Carsten Eickhoff;Arjen P. de Vries;Kevyn Collins-Thompson
#t 2013
#c 13
#% 83855
#% 212665
#% 235918
#% 262096
#% 290830
#% 309211
#% 340901
#% 340934
#% 342710
#% 397126
#% 413613
#% 413634
#% 420464
#% 720198
#% 783474
#% 818255
#% 823348
#% 840846
#% 879570
#% 955010
#% 1195851
#% 1210687
#% 1292562
#% 1392470
#% 1450859
#% 1457110
#% 1536555
#% 1558080
#% 1641961
#% 1674728
#% 1748071
#% 1806012
#% 1879076
#! In many domains of information retrieval, system estimates of document relevance are based on multidimensional quality criteria that have to be accommodated in a unidimensional result ranking. Current solutions to this challenge are often inconsistent with the formal probabilistic framework in which constituent scores were estimated, or use sophisticated learning methods that make it difficult for humans to understand the origin of the final ranking. To address these issues, we introduce the use of copulas, a powerful statistical framework for modeling complex multi-dimensional dependencies, to information retrieval tasks. We provide a formal background to copulas and demonstrate their effectiveness on standard IR tasks such as combining multidimensional relevance estimates and fusion of results from multiple search engines. We introduce copula-based versions of standard relevance estimators and fusion methods and show that these lead to significant performance improvements on several tasks, as evaluated on large-scale standard corpora, compared to their non-copula counterparts. We also investigate criteria for understanding the likely effect of using copula models in a given retrieval scenario.

#index 1988827
#* Opportunity model for e-commerce recommendation: right product; right time
#@ Jian Wang;Yi Zhang
#t 2013
#c 13
#% 301259
#% 730049
#% 751597
#% 829036
#% 1083671
#% 1214666
#% 1289281
#% 1345617
#% 1400014
#% 1451212
#% 1476448
#% 1517959
#% 1536564
#% 1598434
#% 1614699
#% 1625393
#% 1746841
#% 1879009
#% 1992509
#! Most of existing e-commerce recommender systems aim to recommend the right product to a user, based on whether the user is likely to purchase or like a product. On the other hand, the effectiveness of recommendations also depends on the time of the recommendation. Let us take a user who just purchased a laptop as an example. She may purchase a replacement battery in 2 years (assuming that the laptop's original battery often fails to work around that time) and purchase a new laptop in another 2 years. In this case, it is not a good idea to recommend a new laptop or a replacement battery right after the user purchased the new laptop. It could hurt the user's satisfaction of the recommender system if she receives a potentially right product recommendation at the wrong time. We argue that a system should not only recommend the most relevant item, but also recommend at the right time. This paper studies the new problem: how to recommend the right product at the right time? We adapt the proportional hazards modeling approach in survival analysis to the recommendation research field and propose a new opportunity model to explicitly incorporate time in an e-commerce recommender system. The new model estimates the joint probability of a user making a follow-up purchase of a particular product at a particular time. This joint purchase probability can be leveraged by recommender systems in various scenarios, including the zero-query pull-based recommendation scenario (e.g. recommendation on an e-commerce web site) and a proactive push-based promotion scenario (e.g. email or text message based marketing). We evaluate the opportunity modeling approach with multiple metrics. Experimental results on a data collected by a real-world e-commerce website(shop.com) show that it can predict a user's follow-up purchase behavior at a particular time with descent accuracy. In addition, the opportunity model significantly improves the conversion rate in pull-based systems and the user satisfaction/utility in push-based systems.

#index 1988828
#* Personalized ranking model adaptation for web search
#@ Hongning Wang;Xiaodong He;Ming-Wei Chang;Yang Song;Ryen W. White;Wei Chu
#t 2013
#c 13
#% 232684
#% 342621
#% 387427
#% 577224
#% 818207
#% 818221
#% 818224
#% 818259
#% 840846
#% 879565
#% 881540
#% 899257
#% 956495
#% 956552
#% 1074071
#% 1074082
#% 1159262
#% 1268491
#% 1338581
#% 1442578
#% 1450849
#% 1464068
#% 1536511
#% 1693905
#% 1755385
#% 1879011
#! Search engines train and apply a single ranking model across all users, but searchers' information needs are diverse and cover a broad range of topics. Hence, a single user-independent ranking model is insufficient to satisfy different users' result preferences. Conventional personalization methods learn separate models of user interests and use those to re-rank the results from the generic model. Those methods require significant user history information to learn user preferences, have low coverage in the case of memory-based methods that learn direct associations between query-URL pairs, and have limited opportunity to markedly affect the ranking given that they only re-order top-ranked items. In this paper, we propose a general ranking model adaptation framework for personalized search. Using a given user-independent ranking model trained offline and limited number of adaptation queries from individual users, the framework quickly learns to apply a series of linear transformations, e.g., scaling and shifting, over the parameters of the given global ranking model such that the adapted model can better fit each individual user's search preferences. Extensive experimentation based on a large set of search logs from a major commercial Web search engine confirms the effectiveness of the proposed method compared to several state-of-the-art ranking model adaptation methods.

#index 1988829
#* Task-aware query recommendation
#@ Henry Feild;James Allan
#t 2013
#c 13
#% 591792
#% 789959
#% 823348
#% 987372
#% 1083721
#% 1130868
#% 1130878
#% 1190074
#% 1400023
#% 1450996
#% 1536532
#% 1598334
#% 1598413
#% 1621236
#% 1746847
#% 1879027
#% 1879166
#! When generating query recommendations for a user, a natural approach is to try and leverage not only the user's most recently submitted query, or reference query, but also information about the current search context, such as the user's recent search interactions. We focus on two important classes of queries that make up search contexts: those that address the same information need as the reference query (on-task queries), and those that do not (off-task queries). We analyze the effects on query recommendation performance of using contexts consisting of only on-task queries, only off-task queries, and a mix of the two. Using TREC Session Track data for simulations, we demonstrate that on-task context is helpful on average but can be easily overwhelmed when off-task queries are interleaved---a common situation according to several analyses of commercial search logs. To minimize the impact of off-task queries on recommendation performance, we consider automatic methods of identifying such queries using a state of the art search task identification technique. Our experimental results show that automatic search task identification can eliminate the effect of off-task queries in a mixed context. We also introduce a novel generalized model for generating recommendations over a search context. While we only consider query text in this study, the model can handle integration over arbitrary user search behavior, such as page visits, dwell times, and query abandonment. In addition, it can be used for other types of recommendation, including personalized web search.

#index 1988830
#* A novel TF-IDF weighting scheme for effective ranking
#@ Jiaul H. Paik
#t 2013
#c 13
#% 46803
#% 94368
#% 111303
#% 169781
#% 218982
#% 219041
#% 248214
#% 262037
#% 262096
#% 321635
#% 324129
#% 324192
#% 406493
#% 411760
#% 411762
#% 750863
#% 766429
#% 818261
#% 960413
#% 987201
#% 1074132
#% 1292528
#% 1343447
#% 1558081
#% 1558471
#% 1919959
#! Term weighting schemes are central to the study of information retrieval systems. This article proposes a novel TF-IDF term weighting scheme that employs two different within document term frequency normalizations to capture two different aspects of term saliency. One component of the term frequency is effective for short queries, while the other performs better on long queries. The final weight is then measured by taking a weighted combination of these components, which is determined on the basis of the length of the corresponding query. Experiments conducted on a large number of TREC news and web collections demonstrate that the proposed scheme almost always outperforms five state of the art retrieval models with remarkable significance and consistency. The experimental results also show that the proposed model achieves significantly better precision than the existing models.

#index 1988831
#* Click model-based information retrieval metrics
#@ Aleksandr Chuklin;Pavel Serdyukov;Maarten de Rijke
#t 2013
#c 13
#% 312689
#% 411762
#% 577224
#% 766409
#% 766410
#% 838529
#% 879630
#% 987201
#% 987249
#% 987263
#% 1035578
#% 1074092
#% 1130811
#% 1154062
#% 1166517
#% 1190055
#% 1190056
#% 1195853
#% 1227582
#% 1227640
#% 1292528
#% 1450892
#% 1450912
#% 1482378
#% 1536510
#% 1598424
#% 1606083
#% 1641943
#% 1667281
#% 1693908
#% 1747008
#% 1879002
#% 1879012
#% 1924107
#% 1948141
#% 1967735
#! In recent years many models have been proposed that are aimed at predicting clicks of web search users. In addition, some information retrieval evaluation metrics have been built on top of a user model. In this paper we bring these two directions together and propose a common approach to converting any click model into an evaluation metric. We then put the resulting model-based metrics as well as traditional metrics (like DCG or Precision) into a common evaluation framework and compare them along a number of dimensions. One of the dimensions we are particularly interested in is the agreement between offline and online experimental outcomes. It is widely believed, especially in an industrial setting, that online A/B-testing and interleaving experiments are generally better at capturing system quality than offline measurements. We show that offline metrics that are based on click models are more strongly correlated with online experimental outcomes than traditional offline metrics, especially in situations when we have incomplete relevance judgements.

#index 1988832
#* Exploiting user feedback to learn to rank answers in q&a forums: a case study with stack overflow
#@ Daniel Hasan Dalip;Marcos André Gonçalves;Marco Cristo;Pavel Calado
#t 2013
#c 13
#% 137257
#% 169781
#% 309095
#% 309493
#% 376266
#% 400847
#% 448194
#% 466240
#% 879593
#% 956516
#% 1035587
#% 1166519
#% 1261594
#% 1450880
#% 1536523
#% 1641862
#% 1747117
#% 1872333
#% 1876887
#! Collaborative web sites, such as collaborative encyclopedias, blogs, and forums, are characterized by a loose edit control, which allows anyone to freely edit their content. As a consequence, the quality of this content raises much concern. To deal with this, many sites adopt manual quality control mechanisms. However, given their size and change rate, manual assessment strategies do not scale and content that is new or unpopular is seldom reviewed. This has a negative impact on the many services provided, such as ranking and recommendation. To tackle with this problem, we propose a learning to rank (L2R) approach for ranking answers in Q&A forums. In particular, we adopt an approach based on Random Forests and represent query and answer pairs using eight different groups of features. Some of these features are used in the Q&A domain for the first time. Our L2R method was trained to learn the answer rating, based on the feedback users give to answers in Q&A forums. Using the proposed method, we were able (i) to outperform a state of the art baseline with gains of up to 21% in NDCG, a metric used to evaluate rankings; we also conducted a comprehensive study of the features, showing that (ii) review and user features are the most important in the Q&A domain although text features are useful for assessing quality of new answers; and (iii) the best set of new features we proposed was able to yield the best quality rankings.

#index 1988833
#* A mutual information-based framework for the analysis of information retrieval systems
#@ Peter B. Golbus;Javed A. Aslam
#t 2013
#c 13
#% 115608
#% 411762
#% 413613
#% 577224
#% 715945
#% 734915
#% 750863
#% 766409
#% 879630
#% 907496
#% 987201
#% 1035578
#% 1074124
#% 1074126
#% 1074133
#% 1074137
#% 1095876
#% 1166473
#% 1292528
#% 1415710
#% 1450904
#% 1598438
#% 1879078
#% 2008822
#! We consider the problem of information retrieval evaluation and the methods and metrics used for such evaluations. We propose a probabilistic framework for evaluation which we use to develop new information-theoretic evaluation metrics. We demonstrate that these new metrics are powerful and generalizable, enabling evaluations heretofore not possible. We introduce four preliminary uses of our framework: (1) a measure of conditional rank correlation, information tau, a powerful meta-evaluation tool whose use we demonstrate on understanding novelty and diversity evaluation; (2) a new evaluation measure, relevance information correlation, which is correlated with traditional evaluation measures and can be used to (3) evaluate a collection of systems simultaneously, which provides a natural upper bound on metasearch performance; and (4) a measure of the similarity between rankers on judged documents, information difference, which allows us to determine whether systems with similar performance are in fact different.

#index 1988834
#* An effective implicit relevance feedback technique using affective, physiological and behavioural features
#@ Yashar Moshfeghi;Joemon M. Jose
#t 2013
#c 13
#% 169803
#% 214709
#% 302085
#% 303504
#% 320432
#% 590523
#% 717120
#% 766454
#% 818221
#% 907516
#% 987211
#% 1031049
#% 1074100
#% 1279814
#% 1292861
#% 1314948
#% 1434128
#% 1450833
#% 1450875
#% 1455248
#% 1455249
#% 1523400
#% 1598337
#% 1666682
#! The effectiveness of various behavioural signals for implicit relevance feedback models has been exhaustively studied. Despite the advantages of such techniques for a real time information retrieval system, most of the behavioural signals are noisy and therefore not reliable enough to be employed. Among many, a combination of dwell time and task information has been shown to be effective for relevance judgement prediction. However, the task information might not be available to the system at all times. Thus, there is a need for other sources of information which can be used as a substitute for task information. Recently, affective and physiological signals have shown promise as a potential source of information for relevance judgement prediction. However, their accuracy is not high enough to be applicable on their own. This paper investigates whether affective and physiological signals can be used as a complementary source of information for behavioural signals (i.e. dwell time) to create a reliable signal for relevance judgement prediction. Using a video retrieval system as a use case, we study and compare the effectiveness of the affective and physiological signals on their own, as well as in combination with behavioural signals for the relevance judgment prediction task across four different search intentions: seeking information, re-finding a particular information object, and two different entertainment intentions (i.e. entertainment by adjusting arousal level, and entertainment by adjusting mood). Our experimental results show that the effectiveness of studied signals varies across different search intentions, and when affective and physiological signals are combined with dwell time, a significant improvement can be achieved. Overall, these findings will help to implement better search engines in the future.

#index 1988835
#* Fighting search engine amnesia: reranking repeated results
#@ Milad Shokouhi;Ryen W. White;Paul Bennett;Filip Radlinski
#t 2013
#c 13
#% 233808
#% 411762
#% 577224
#% 805200
#% 818207
#% 818259
#% 823348
#% 879565
#% 879567
#% 881540
#% 946521
#% 950658
#% 956552
#% 987211
#% 1035578
#% 1074092
#% 1130811
#% 1166517
#% 1190055
#% 1190056
#% 1192483
#% 1214757
#% 1227622
#% 1355034
#% 1355035
#% 1355037
#% 1392483
#% 1442577
#% 1450885
#% 1482279
#% 1536505
#% 1536511
#% 1598347
#% 1606083
#% 1641961
#% 1693945
#% 1879011
#! Web search engines frequently show the same documents repeatedly for different queries within the same search session, in essence forgetting when the same documents were already shown to users. Depending on previous user interaction with the repeated results, and the details of the session, we show that sometimes the repeated results should be promoted, while some other times they should be demoted. Analysing search logs from two different commercial search engines, we find that results are repeated in about 40% of multi-query search sessions, and that users engage differently with repeats than with results shown for the first time. We demonstrate how statistics about result repetition within search sessions can be incorporated into ranking for personalizing search results. Our results on query logs of two large-scale commercial search engines suggest that we successfully promote documents that are more likely to be clicked by the user in the future while maintaining performance over standard measures of non-personalized relevance.

#index 1988836
#* Learning to personalize query auto-completion
#@ Milad Shokouhi
#t 2013
#c 13
#% 82523
#% 722904
#% 766461
#% 805200
#% 818259
#% 878624
#% 943042
#% 950658
#% 1022220
#% 1083721
#% 1130854
#% 1190092
#% 1217200
#% 1355051
#% 1400023
#% 1443410
#% 1450885
#% 1450894
#% 1536505
#% 1536511
#% 1560365
#% 1598347
#% 1598413
#% 1598415
#% 1619133
#% 1641944
#% 1641961
#% 1693897
#% 1699616
#% 1747072
#% 1878994
#% 1878995
#% 1879011
#% 1879052
#% 1918401
#% 1919987
#% 2008818
#! Query auto-completion (QAC) is one of the most prominent features of modern search engines. The list of query candidates is generated according to the prefix entered by the user in the search box and is updated on each new key stroke. Query prefixes tend to be short and ambiguous, and existing models mostly rely on the past popularity of matching candidates for ranking. However, the popularity of certain queries may vary drastically across different demographics and users. For instance, while instagram and imdb have comparable popularities overall and are both legitimate candidates to show for prefix i, the former is noticeably more popular among young female users, and the latter is more likely to be issued by men. In this paper, we present a supervised framework for personalizing auto-completion ranking. We introduce a novel labelling strategy for generating offline training labels that can be used for learning personalized rankers. We compare the effectiveness of several user-specific and demographic-based features and show that among them, the user's long-term search history and location are the most effective for personalizing auto-completion rankers. We perform our experiments on the publicly available AOL query logs, and also on the larger-scale logs of Bing. The results suggest that supervised rankers enhanced by personalization features can significantly outperform the existing popularity-based base-lines, in terms of mean reciprocal rank (MRR) by up to 9%.

#index 1988837
#* Cache-conscious performance optimization for similarity search
#@ Maha Alabduljalil;Xun Tang;Tao Yang
#t 2013
#c 13
#% 69494
#% 204673
#% 345087
#% 387427
#% 399258
#% 415666
#% 479973
#% 566122
#% 569940
#% 769944
#% 869500
#% 893164
#% 956506
#% 956518
#% 993947
#% 1035590
#% 1055684
#% 1227596
#% 1450881
#% 1524234
#% 1598428
#% 1693862
#% 1746916
#% 1948143
#! All-pairs similarity search can be implemented in two stages. The first stage is to partition the data and group potentially similar vectors. The second stage is to run a set of tasks where each task compares a partition of vectors with other candidate partitions. Because of data sparsity, accessing feature vectors in memory for runtime comparison in the second stage, incurs significant overhead due to the presence of memory hierarchy. This paper proposes a cache-conscious data layout and traversal optimization to reduce the execution time through size-controlled data splitting and vector coalescing. It also provides an analysis to guide the optimal choice for the parameter setting. Our evaluation with several application datasets verifies the performance gains obtained by the optimization and shows that the proposed scheme is upto 2.74x as fast as the cache-oblivious baseline.

#index 1988838
#* Efficient query construction for large scale data
#@ Elena Demidova;Xuan Zhou;Wolfgang Nejdl
#t 2013
#c 13
#% 364906
#% 479805
#% 875017
#% 956564
#% 960234
#% 960243
#% 960285
#% 960360
#% 993987
#% 1015325
#% 1044470
#% 1063536
#% 1077150
#% 1130808
#% 1167472
#% 1194650
#% 1399987
#% 1409940
#% 1644629
#% 1692271
#! In recent years, a number of open databases have emerged on the Web, providing Web users with platforms to collaboratively create structured information. As these databases are intended to accommodate heterogeneous information and knowledge, they usually comprise a very large schema and billions of instances. Browsing and searching data on such a scale is not an easy task for a Web user. In this context, interactive query construction offers an intuitive interface for novice users to retrieve information from databases neither requiring any knowledge of structured query languages, nor any prior knowledge of the database schema. However, the existing mechanisms do not scale well on large scale datasets. This paper presents a set of techniques to boost the scalability of interactive query construction, from the perspective of both, user interaction cost and performance. We connect an abstract ontology layer to the database schema to shorten the process of user-computer interaction. We also introduce a search mechanism to enable efficient exploration of query interpretation spaces over large scale data. Extensive experiments show that our approach scales well on Freebase - an open database containing more than 7,000 relational tables in more than 100 domains.

#index 1988839
#* Document identifier reassignment and run-length-compressed inverted indexes for improved search performance
#@ Diego Arroyuelo;Senén González;Mauricio Oyarzún;Victor Sepulveda
#t 2013
#c 13
#% 290703
#% 290830
#% 420491
#% 570319
#% 656274
#% 730065
#% 754117
#% 766445
#% 786632
#% 864446
#% 865740
#% 867054
#% 918057
#% 987208
#% 1016130
#% 1077150
#% 1166469
#% 1190095
#% 1332404
#% 1392439
#% 1399964
#% 1418196
#% 1480887
#% 1482300
#% 1598433
#% 1598490
#% 1715618
#% 1811305
#% 1812718
#% 1879018
#% 1925813
#! Text search engines are a fundamental tool nowadays. Their efficiency relies on a popular and simple data structure: the inverted indexes. Currently, inverted indexes can be represented very efficiently using index compression schemes. Recent investigations also study how an optimized document ordering can be used to assign document identifiers (docIDs) to the document database. This yields important improvements in index compression and query processing time. In this paper we follow this line of research, yet from a different perspective. We propose a docID reassignment method that allows one to focus on a given subset of inverted lists to improve their performance. We then use run-length encoding to compress these lists (as many consecutive 1s are generated). We show that by using this approach, not only the performance of the particular subset of inverted lists is improved, but also that of the whole inverted index. Our experimental results indicate a reduction of about 10% in the space usage of the whole index docID reassignment was focused. Also, decompression speed is up to 1.22 times faster if the runs must be explicitly decompressed and up to 4.58 times faster if implicit decompression of runs is allowed. Finally, we also improve the Document-at-a-Time query processing time of AND queries (by up to 12%), WAND queries (by up to 23%) and full (non-ranked) OR queries (by up to 86%).

#index 1988840
#* News vertical search: when and what to display to users
#@ Richard McCreadie;Craig Macdonald;Iadh Ounis
#t 2013
#c 13
#% 109187
#% 907495
#% 1047347
#% 1150163
#% 1151011
#% 1166523
#% 1173696
#% 1226913
#% 1227616
#% 1227620
#% 1264744
#% 1290415
#% 1338557
#% 1384364
#% 1399992
#% 1879004
#% 1879036
#% 1969487
#! News reporting has seen a shift toward fast-paced online reporting in new sources such as social media. Web Search engines that support a news vertical have historically relied upon articles published by major newswire providers when serving news-related queries. In this paper, we investigate to what extent real-time content from newswire, blogs, Twitter and Wikipedia sources are useful to return to the user in the current fast-paced news search setting. In particular, we perform a detailed user study using the emerging medium of crowdsourcing to determine when and where integrating news-related content from these various sources can better serve the user's news need. We sampled approximately 300 news-related search queries using Google Trends and Bitly data in real-time for two time periods. For these queries, we have crowdsourced workers compare Web search rankings for each, with similar rankings integrating real-time news content from sources such as Twitter or the blogosphere. Our results show that users exhibited a preference for rankings integrating newswire articles for only half of our queries, indicating that relying solely on newswire providers for news-related content is now insufficient. Moreover, our results show that users preferred rankings that integrate tweets more often than those that integrate newswire articles, showing the potential of using social media to better serve news queries.

#index 1988841
#* A general evaluation measure for document organization tasks
#@ Enrique Amigó;Julio Gonzalo;Felisa Verdejo
#t 2013
#c 13
#% 218992
#% 342660
#% 376266
#% 411762
#% 430746
#% 766409
#% 1074133
#% 1080077
#% 1095876
#% 1190055
#% 1213625
#% 1560356
#% 1598424
#% 1625357
#% 1626518
#% 1641985
#% 1738864
#% 1879002
#! A number of key Information Access tasks -- Document Retrieval, Clustering, Filtering, and their combinations -- can be seen as instances of a generic {\em document organization} problem that establishes priority and relatedness relationships between documents (in other words, a problem of forming and ranking clusters). As far as we know, no analysis has been made yet on the evaluation of these tasks from a global perspective. In this paper we propose two complementary evaluation measures -- Reliability and Sensitivity -- for the generic Document Organization task which are derived from a proposed set of formal constraints (properties that any suitable measure must satisfy). In addition to be the first measures that can be applied to any mixture of ranking, clustering and filtering tasks, Reliability and Sensitivity satisfy more formal constraints than previously existing evaluation metrics for each of the subsumed tasks. Besides their formal properties, its most salient feature from an empirical point of view is their strictness: a high score according to the harmonic mean of Reliability and Sensitivity ensures a high score with any of the most popular evaluation metrics in all the Document Retrieval, Clustering and Filtering datasets used in our experiments.

#index 1988842
#* Modeling click-through based word-pairs for web search
#@ Jagadeesh Jagarlamudi;Jianfeng Gao
#t 2013
#c 13
#% 218978
#% 262096
#% 280819
#% 280851
#% 287253
#% 309095
#% 340901
#% 342961
#% 397128
#% 397205
#% 579944
#% 722904
#% 740915
#% 879567
#% 879587
#% 987194
#% 989578
#% 1074081
#% 1074110
#% 1227621
#% 1338581
#% 1392435
#% 1399978
#% 1417055
#% 1417061
#% 1450887
#% 1450911
#% 1471293
#% 1471315
#% 1482292
#% 1587356
#% 1598401
#% 1697450
#% 1711809
#% 1747035
#! Statistical translation models and latent semantic analysis (LSA) are two effective approaches to exploiting click-through data for Web search ranking. While the former learns semantic relationships between query terms and document terms directly, the latter maps a document and the queries for which it has been clicked to vectors in a lower dimensional semantic space. This paper presents two document ranking models that combine the strengths of both the approaches by explicitly modeling word-pairs. The first model, called PairModel, is a monolingual ranking model based on word-pairs derived from click-through data. It maps queries and documents into a concept space spanned by these word-pairs. The second model, called Bilingual Paired Topic Model (BPTM), uses bilingual word translations and can jointly model query-document collections written in multiple languages. This model uses topics to capture term dependencies and maps queries and documents in multiple languages into a lower dimensional semantic sub-space spanned by the topics. These models are evaluated on the Web search task using real world data sets in three different languages. Results show that they consistently outperform various state-of-the-art baseline models, and the best result is obtained by interpolating PairModel and BPTM.

#index 1988843
#* Retrieving documents with mathematical content
#@ Shahab Kamali;Frank Wm. Tompa
#t 2013
#c 13
#% 198621
#% 556294
#% 754116
#% 826007
#% 879713
#% 990386
#% 1015325
#% 1065265
#% 1099072
#% 1260061
#% 1482341
#% 1616137
#% 1617803
#% 1645188
#% 1655249
#% 1668638
#% 1728015
#% 1746910
#% 1748075
#% 1918407
#! Many documents with mathematical content are published on the Web, but conventional search engines that rely on keyword search only cannot fully exploit their mathematical information. In particular, keyword search is insufficient when expressions in a document are not annotated with natural keywords or the user cannot describe her query with keywords. Retrieving documents by querying their mathematical content directly is very appealing in various domains such as education, digital libraries, engineering, patent documents, medical sciences, etc. Capturing the relevance of mathematical expressions also greatly enhances document classification in such domains. Unlike text retrieval, where keywords carry enough semantics to distinguish text documents and rank them, math symbols do not contain much semantic information on their own. In fact, mathematical expressions typically consist of few alphabetical symbols organized in rather complex structures. Hence, the structure of an expression, which describes the way such symbols are combined, should also be considered. Unfortunately, there is no standard testbed with which to evaluate the effectiveness of a mathematics retrieval algorithm. In this paper we study the fundamental and challenging problems in mathematics retrieval, that is how to capture the relevance of mathematical expressions, how to query them, and how to evaluate the results. We describe various search paradigms and propose retrieval systems accordingly. We discuss the benefits and drawbacks of each approach, and further compare them through an extensive empirical study.

#index 1988844
#* Looking ahead: query preview in exploratory search
#@ Pernilla Qvarfordt;Gene Golovchinsky;Tony Dunnigan;Elena Agapie
#t 2013
#c 13
#% 142564
#% 186518
#% 232926
#% 309792
#% 316721
#% 395451
#% 775964
#% 879610
#% 961650
#% 985819
#% 1019150
#% 1482283
#% 1891117
#! Exploratory search is a complex, iterative information seeking activity that involves running multiple queries and finding and examining many documents. We designed a query preview control that visualizes the distribution of newly-retrieved and re-retrieved documents prior to running the query. When evaluating the preview control with a control condition, we found effects on both people's information seeking behavior and improved retrieval performance. People spent more time formulating a query and were more likely to explore search results more deeply, retrieved a more diverse set of documents, and found more different relevant documents when using the preview.

#index 1988845
#* Fast document-at-a-time query processing using two-tier indexes
#@ Cristian Rossi;Edleno S. de Moura;Andre L. Carvalho;Altigran S. da Silva
#t 2013
#c 13
#% 169781
#% 213786
#% 340886
#% 649299
#% 728102
#% 730065
#% 879611
#% 987214
#% 987216
#% 1074067
#% 1418196
#% 1598433
#% 1693904
#! In this paper we present two new algorithms designed to reduce the overall time required to process top-k queries. These algorithms are based on the document-at-a-time approach and modify the best baseline we found in the literature, Blockmax WAND (BMW), to take advantage of a two-tiered index, in which the first tier is a small index containing only the higher impact entries of each inverted list. This small index is used to pre-process the query before accessing a larger index in the second tier, resulting in considerable speeding up the whole process. The first algorithm we propose, named BMW-CS, achieves higher performance, but may result in small changes in the top results provided in the final ranking. The second algorithm, named BMW-t, preserves the top results and, while slower than BMW-CS, it is faster than BMW. In our experiments, BMW-CS was more than 40 times faster than BMW when computing top 10 results, and, while it does not guarantee preserving the top results, it preserved all ranking results evaluated at this level.

#index 1988846
#* Incorporating popularity in topic models for social network analysis
#@ Youngchul Cha;Bin Bi;Chu-Cheng Hsieh;Junghoo Cho
#t 2013
#c 13
#% 268079
#% 280819
#% 301590
#% 466574
#% 643056
#% 722904
#% 769906
#% 779872
#% 868088
#% 869480
#% 983833
#% 1055681
#% 1083684
#% 1117695
#% 1176909
#% 1176959
#% 1192430
#% 1192930
#% 1264771
#% 1271481
#% 1272187
#% 1470632
#% 1476470
#% 1536553
#% 1560196
#% 1605925
#% 1625359
#% 1879049
#! Topic models are used to group words in a text dataset into a set of relevant topics. Unfortunately, when a few words frequently appear in a dataset, the topic groups identified by topic models become noisy because these frequent words repeatedly appear in "irrelevant" topic groups. This noise has not been a serious problem in a text dataset because the frequent words (e.g., the and is) do not have much meaning and have been simply removed before a topic model analysis. However, in a social network dataset we are interested in, they correspond to popular persons (e.g., Barack Obama and Justin Bieber) and cannot be simply removed because most people are interested in them. To solve this "popularity problem", we explicitly model the popularity of nodes (words) in topic models. For this purpose, we first introduce a notion of a "popularity component" and propose topic model extensions that effectively accommodate the popularity component. We evaluate the effectiveness of our models with a real-world Twitter dataset. Our proposed models achieve significantly lower perplexity (i.e., better prediction power) compared to the state-of-the-art baselines. In addition to the popularity problem caused by the nodes with high incoming edge degree, we also investigate the effect of the outgoing edge degree with another topic model extensions. We show that considering outgoing edge degree does not help much in achieving lower perplexity.

#index 1988847
#* A candidate filtering mechanism for fast top-k query processing on modern cpus
#@ Constantinos Dimopoulos;Sergey Nepomnyachiy;Torsten Suel
#t 2013
#c 13
#% 198335
#% 212665
#% 290703
#% 340886
#% 387427
#% 396728
#% 397123
#% 397608
#% 730065
#% 818229
#% 818230
#% 864446
#% 867054
#% 879611
#% 893128
#% 987214
#% 1021950
#% 1055710
#% 1166469
#% 1190095
#% 1216713
#% 1392439
#% 1399964
#% 1482300
#% 1587386
#% 1594565
#% 1598344
#% 1598433
#% 1693904
#% 1693910
#% 1948134
#! A large amount of research has focused on faster methods for finding top-k results in large document collections, one of the main scalability challenges for web search engines. In this paper, we propose a method for accelerating such top-k queries that builds on and generalizes methods recently proposed by several groups of researchers based on Block-Max Indexes. In particular, we describe a system that uses a new filtering mechanism, based on a combination of block maxima and bitmaps, that radically reduces the number of documents that have to be further evaluated. Our filtering mechanism exploits the SIMD processing capabilities of current microprocessors, and it is optimized through caching policies that select and store suitable filter structures based on properties of the query load. Our experimental evaluation shows that the mechanism results in very significant speed-ups for disjunctive top-k queries under several state-of-the-art algorithms, including a speed-up of more than a factor of 2 over the fastest previously known methods.

#index 1988848
#* Faster and smaller inverted indices with treaps
#@ Roberto Konow;Gonzalo Navarro;Charles L.A. Clarke;Alejandro López-Ortíz
#t 2013
#c 13
#% 137808
#% 212665
#% 247407
#% 250012
#% 303072
#% 319603
#% 375076
#% 379411
#% 397151
#% 453572
#% 498538
#% 580212
#% 730065
#% 786632
#% 867054
#% 879611
#% 987214
#% 1181094
#% 1190095
#% 1221039
#% 1404881
#% 1418196
#% 1480887
#% 1484078
#% 1529960
#% 1598344
#% 1598433
#% 1654514
#% 1739410
#% 1739418
#% 1779395
#% 1895915
#% 1921864
#% 1939340
#! We introduce a new representation of the inverted index that performs faster ranked unions and intersections while using less space. Our index is based on the treap data structure, which allows us to intersect/merge the document identifiers while simultaneously thresholding by frequency, instead of the costlier two-step classical processing methods. To achieve compression we represent the treap topology using compact data structures. Further, the treap invariants allow us to elegantly encode differentially both document identifiers and frequencies. Results show that our index uses about 20% less space, and performs queries up to three times faster, than state-of-the-art compact representations.

#index 1988849
#* Toward whole-session relevance: exploring intrinsic diversity in web search
#@ Karthik Raman;Paul N. Bennett;Kevyn Collins-Thompson
#t 2013
#c 13
#% 262112
#% 269217
#% 642975
#% 805200
#% 823348
#% 869517
#% 879618
#% 946521
#% 956495
#% 1039831
#% 1047436
#% 1074025
#% 1074133
#% 1130878
#% 1227621
#% 1312812
#% 1399944
#% 1415709
#% 1434128
#% 1450832
#% 1450874
#% 1450884
#% 1450975
#% 1482279
#% 1536529
#% 1598334
#% 1598439
#% 1641927
#% 1641948
#% 1642268
#% 1765695
#% 1879024
#% 1879129
#% 1919837
#! Current research on web search has focused on optimizing and evaluating single queries. However, a significant fraction of user queries are part of more complex tasks [20] which span multiple queries across one or more search sessions [26,24]. An ideal search engine would not only retrieve relevant results for a user's particular query but also be able to identify when the user is engaged in a more complex task and aid the user in completing that task [29,1]. Toward optimizing whole-session or task relevance, we characterize and address the problem of intrinsic diversity (ID) in retrieval [30], a type of complex task that requires multiple interactions with current search engines. Unlike existing work on extrinsic diversity [30] that deals with ambiguity in intent across multiple users, ID queries often have little ambiguity in intent but seek content covering a variety of aspects on a shared theme. In such scenarios, the underlying needs are typically exploratory, comparative, or breadth-oriented in nature. We identify and address three key problems for ID retrieval: identifying authentic examples of ID tasks from post-hoc analysis of behavioral signals in search logs; learning to identify initiator queries that mark the start of an ID search task; and given an initiator query, predicting which content to prefetch and rank.

#index 1988850
#* The effect of threshold priming and need for cognition on relevance calibration and assessment
#@ Falk Scholer;Diane Kelly;Wan-Ching Wu;Hanseul S. Lee;William Webber
#t 2013
#c 13
#% 167557
#% 312689
#% 397164
#% 420524
#% 720198
#% 793014
#% 803833
#% 807644
#% 1015625
#% 1015626
#% 1450896
#% 1523458
#% 1598354
#% 1598440
#% 1879138
#% 1918345
#! Human assessments of document relevance are needed for the construction of test collections, for ad-hoc evaluation, and for training text classifiers. Showing documents to assessors in different orderings, however, may lead to different assessment outcomes. We examine the effect that \defineterm{threshold priming}, seeing varying degrees of relevant documents, has on people's calibration of relevance. Participants judged the relevance of a prologue of documents containing highly relevant, moderately relevant, or non-relevant ocuments, followed by a common epilogue of documents of mixed relevance. We observe that participants exposed to only non-relevant documents in the prologue assigned significantly higher average relevance scores to prologue and epilogue documents than participants exposed to moderately or highly relevant documents in the prologue. We also examine how \defineterm{need for cognition}, an individual difference measure of the extent to which a person enjoys engaging in effortful cognitive activity, impacts relevance assessments. High need for cognition participants had a significantly higher level of agreement with expert assessors than low need for cognition participants did. Our findings indicate that assessors should be exposed to documents from multiple relevance levels early in the judging process, in order to calibrate their relevance thresholds in a balanced way, and that individual difference measures might be a useful way to screen assessors.

#index 1988851
#* Search result diversification in resource selection for federated search
#@ Dzung Hong;Luo Si
#t 2013
#c 13
#% 262112
#% 280856
#% 567255
#% 642975
#% 643012
#% 722312
#% 789959
#% 879618
#% 987254
#% 1074133
#% 1166473
#% 1174737
#% 1227629
#% 1263586
#% 1292528
#% 1292595
#% 1292596
#% 1392444
#% 1400021
#% 1450841
#% 1482223
#% 1618643
#% 1622356
#% 1878999
#% 1879000
#% 1879004
#% 1879076
#% 1879077
#% 1919835
#% 1967777
#% 1967838
#! Prior research in resource selection for federated search mainly focused on selecting a small number of information sources that are most relevant to a user query. However, result novelty and diversification are largely unexplored, which does not reflect the various kinds of information needs of users in real world applications. This paper proposes two general approaches to model both result relevance and diversification in selecting sources, in order to provide more comprehensive coverage of multiple aspects of a user query. The first approach focuses on diversifying the document ranking on a centralized sample database before selecting information sources under the framework of Relevant Document Distribution Estimation (ReDDE). The second approach first evaluates the relevance of information sources with respect to each aspect of the query, and then ranks the sources based on the novelty and relevance that they offer. Both approaches can be applied with a wide range of existing resource selection algorithms such as ReDDE, CRCS, CORI and Big Document. Moreover, this paper proposes a learning based approach to combine multiple resource selection algorithms for result diversification, which can further improve the performance. We propose a set of new metrics for resource selection in federated search to evaluate the diversification performance of different approaches. To our best knowledge, this is the first piece of work that addresses the problem of search result diversification in federated search. The effectiveness of the proposed approaches has been demonstrated by an extensive set of experiments on the federated search testbed of the Clueweb dataset.

#index 1988852
#* How do users respond to voice input errors?: lexical and phonetic query reformulation in voice search
#@ Jiepu Jiang;Wei Jeng;Daqing He
#t 2013
#c 13
#% 414601
#% 577224
#% 590523
#% 642985
#% 804805
#% 867120
#% 987211
#% 1130855
#% 1224723
#% 1237511
#% 1260691
#% 1292473
#% 1348333
#% 1355020
#% 1415709
#% 1598439
#% 1920007
#! Voice search offers users with a new search experience: instead of typing, users can vocalize their search queries. However, due to voice input errors (such as speech recognition errors and improper system interruptions), users need to frequently reformulate queries to handle the incorrectly recognized queries. We conducted user experiments with native English speakers on their query reformulation behaviors in voice search and found that users often reformulate queries with both lexical and phonetic changes to previous queries. In this paper, we first characterize and analyze typical voice input errors in voice search and users' corresponding reformulation strategies. Then, we evaluate the impacts of typical voice input errors on users' search progress and the effectiveness of different reformulation strategies on handling these errors. This study provides a clearer picture on how to further improve current voice search systems.

#index 1988853
#* Improving search result summaries by using searcher behavior data
#@ Mikhail Ageev;Dmitry Lagun;Eugene Agichtein
#t 2013
#c 13
#% 194251
#% 209021
#% 262036
#% 280835
#% 330617
#% 590523
#% 734975
#% 854668
#% 939539
#% 943527
#% 958442
#% 1048694
#% 1074093
#% 1074099
#% 1074148
#% 1074831
#% 1166525
#% 1183067
#% 1227585
#% 1260752
#% 1292652
#% 1384641
#% 1400099
#% 1403803
#% 1450834
#% 1573487
#% 1598368
#% 1742077
#% 1746855
#% 1765932
#% 1879012
#% 1879187
#% 1988860
#! Query-biased search result summaries, or "snippets", help users decide whether a result is relevant for their information need, and have become increasingly important for helping searchers with difficult or ambiguous search tasks. Previously published snippet generation algorithms have been primarily based on selecting document fragments most similar to the query, which does not take into account which parts of the document the searchers actually found useful. We present a new approach to improving result summaries by incorporating post-click searcher behavior data, such as mouse cursor movements and scrolling over the result documents. To achieve this aim, we develop a method for collecting behavioral data with precise association between searcher intent, document examination behavior, and the corresponding document fragments. In turn, this allows us to incorporate page examination behavior signals into a novel Behavior-Biased Snippet generation system (BeBS). By mining searcher examination data, BeBS infers document fragments of most interest to users, and combines this evidence with text-based features to select the most promising fragments for inclusion in the result summary. Our extensive experiments and analysis demonstrate that our method improves the quality of result summaries compared to existing state-of-the-art methods. We believe that this work opens a new direction for improving search result presentation, and we make available the code and the search behavior data used in this study to encourage further research in this area.

#index 1988854
#* Preference based evaluation measures for novelty and diversity
#@ Praveen Chandar;Ben Carterette
#t 2013
#c 13
#% 262112
#% 642975
#% 766409
#% 879630
#% 1074133
#% 1095876
#% 1130811
#% 1166473
#% 1263586
#% 1292528
#% 1400011
#% 1400021
#% 1415710
#% 1450898
#% 1536510
#% 1558079
#% 1598424
#% 1598438
#% 1622365
#% 1641937
#% 1648026
#% 1879078
#% 1910510
#! Novel and diverse document ranking is an effective strategy that involves reducing redundancy in a ranked list to maximize the amount of novel and relevant information available to users. Evaluation for novelty and diversity typically involves an assessor judging each document for relevance against a set of pre-identified subtopics, which may be disambiguations of the query, facets of an information need, or nuggets of information. Alternately, when expressing a \emph{preference} for document A or document B, users may implicitly take subtopics into account, but may also take into account other factors such as recency, readability, length, and so on, each of which may have more or less importance depending on user. A \emph{user profile} contains information about the extent to which each factor, including subtopic relevance, plays a role in the user's preference for one document over another. A preference-based evaluation can then take this user profile information into account to better model utility to the space of users. In this work, we propose an evaluation framework that not only can consider implicit factors but also handles differences in user preference due to varying underlying information need. Our proposed framework is based on the idea that a user scanning a ranked list from top to bottom and stopping at rank $k$ gains some utility from every document that is relevant their information need. Thus, we model the expected utility of a ranked list by estimating the utility of a document at a given rank using preference judgments and define evaluation measures based on the same. We validate our framework by comparing it to existing measures such as $\alpha$-nDCG, ERR-IA, and subtopic recall that require explicit subtopic judgments We show that our proposed measures correlate well with existing measures while having the potential to capture various other factors when real data is used. We also show that the proposed measures can easily handle relevance assessments against multiple user profiles, and that they are robust to noisy and incomplete judgments.

#index 1988855
#* Term level search result diversification
#@ Van Dang;Bruce W. Croft
#t 2013
#c 13
#% 144034
#% 262112
#% 280849
#% 340901
#% 340951
#% 642975
#% 643068
#% 722904
#% 869471
#% 879618
#% 1035578
#% 1074133
#% 1166473
#% 1166525
#% 1181094
#% 1227591
#% 1263586
#% 1292528
#% 1292596
#% 1400021
#% 1400099
#% 1536552
#% 1642193
#% 1878999
#% 1879077
#% 1910510
#! Current approaches for search result diversification have been categorized as either implicit or explicit. The implicit approach assumes each document represents its own topic, and promotes diversity by selecting documents for different topics based on the difference of their vocabulary. On the other hand, the explicit approach models the set of query topics, or aspects. While the former approach is generally less effective, the latter usually depends on a manually created description of the query aspects, the automatic construction of which has proven difficult. This paper introduces a new approach: term-level diversification. Instead of modeling the set of query aspects, which are typically represented as coherent groups of terms, our approach uses terms without the grouping. Our results on the ClueWeb collection show that the grouping of topic terms provides very little benefit to diversification compared to simply using the terms themselves. Consequently, we demonstrate that term-level diversification, with topic terms identified automatically from the search results using a simple greedy algorithm, significantly outperforms methods that attempt to create a full topic structure for diversification.

#index 1988856
#* Compact query term selection using topically related text
#@ K. Tamsin Maxwell;W. Bruce Croft
#t 2013
#c 13
#% 109190
#% 252328
#% 340899
#% 342707
#% 818262
#% 838532
#% 1015006
#% 1074112
#% 1074127
#% 1077150
#% 1195837
#% 1227647
#% 1270224
#% 1355019
#% 1450865
#% 1450900
#% 1450986
#% 1482284
#% 1482351
#% 1544046
#% 1560364
#% 1641915
#% 1879086
#! Many recent and highly effective retrieval models for long queries use query reformulation methods that jointly optimize term weights and term selection. These methods learn using word context and global context but typically fail to capture query context. In this paper, we present a novel term ranking algorithm, PhRank, that extends work on Markov chain frameworks for query expansion to select compact and focused terms from within a query itself. This focuses queries so that one to five terms in an unweighted model achieve better retrieval effectiveness than weighted term selection models that use up to 30 terms. PhRank terms are also typically compact and contain 1-2 words compared to competing models that use query subsets up to 7 words long. PhRank captures query context with an affinity graph constructed using word co-occurrence in pseudo-relevant documents. A random walk of the graph is used for term ranking in combination with discrimination weights. Empirical evaluation using newswire and web collections demonstrates that performance of reformulated queries is significantly improved for long queries and at least as good for short, keyword queries compared to highly competitive information retrieval (IR) models.

#index 1988857
#* Extracting query facets from search results
#@ Weize Kong;James Allan
#t 2013
#c 13
#% 44876
#% 296646
#% 340951
#% 397162
#% 460812
#% 464434
#% 577285
#% 643068
#% 756964
#% 783484
#% 817472
#% 939759
#% 956503
#% 987203
#% 1130807
#% 1202162
#% 1214708
#% 1227652
#% 1270651
#% 1328355
#% 1338626
#% 1400033
#% 1484352
#% 1598438
#% 1602951
#% 1642069
#% 1642166
#% 1642193
#% 1879023
#! Web search queries are often ambiguous or multi-faceted, which makes a simple ranked list of results inadequate. To assist information finding for such faceted queries, we explore a technique that explicitly represents interesting facets of a query using groups of semantically related terms extracted from search results. As an example, for the query ``baggage allowance'', these groups might be different airlines, different flight types (domestic, international), or different travel classes (first, business, economy). We name these groups query facets and the terms in these groups facet terms. We develop a supervised approach based on a graphical model to recognize query facets from the noisy candidates found. The graphical model learns how likely a candidate term is to be a facet term as well as how likely two terms are to be grouped together in a query facet, and captures the dependencies between the two factors. We propose two algorithms for approximate inference on the graphical model since exact inference is intractable. Our evaluation combines recall and precision of the facet terms with the grouping quality. Experimental results on a sample of web queries show that the supervised method significantly outperforms existing approaches, which are mostly unsupervised, suggesting that query facet extraction can be effectively learned.

#index 1988858
#* Modeling term dependencies with quantum language models for IR
#@ Alessandro Sordoni;Jian-Yun Nie;Yoshua Bengio
#t 2013
#c 13
#% 35937
#% 280864
#% 324129
#% 397205
#% 758200
#% 766428
#% 818262
#% 976952
#% 1019124
#% 1055856
#% 1166534
#% 1227613
#% 1227614
#% 1227636
#% 1264966
#% 1292528
#% 1312800
#% 1482184
#% 1587412
#% 1598394
#% 1641915
#% 1697443
#% 1879086
#! Traditional information retrieval (IR) models use bag-of-words as the basic representation and assume that some form of independence holds between terms. Representing term dependencies and defining a scoring function capable of integrating such additional evidence is theoretically and practically challenging. Recently, Quantum Theory (QT) has been proposed as a possible, more general framework for IR. However, only a limited number of investigations have been made and the potential of QT has not been fully explored and tested. We develop a new, generalized Language Modeling approach for IR by adopting the probabilistic framework of QT. In particular, quantum probability could account for both single and compound terms at once without having to extend the term space artificially as in previous studies. This naturally allows us to avoid the weight-normalization problem, which arises in the current practice by mixing scores from matching compound terms and from matching single terms. Our model is the first practical application of quantum probability to show significant improvements over a robust bag-of-words baseline and achieves better performance on a stronger non bag-of-words baseline.

#index 1988859
#* Search engine switching detection based on user personal preferences and behavior patterns
#@ Denis Savenkov;Dmitry Lagun;Qiaoling Liu
#t 2013
#c 13
#% 879567
#% 956495
#% 1055851
#% 1083674
#% 1083721
#% 1279288
#% 1292474
#% 1355034
#% 1355038
#% 1384094
#% 1450833
#% 1560393
#% 1598367
#% 1598368
#% 1641927
#% 1653970
#% 1693894
#% 1693899
#% 1693905
#% 1715212
#% 1879011
#% 1879020
#! Sometimes, during a search task users may switch from one search engine to another for several reasons, e.g., dissatisfaction with the current search results or desire for broader topic coverage. Detecting the fact of switching is difficult but important for understanding users' satisfaction with the search engine and the complexity of their search tasks, leading to economic significance for search providers. Previous research on switching detection mainly focused on studying different signals useful for the task and particular reasons for switching. Although it is known that switching is a personal choice of a user and different users have different search behavior, little has been done to understand how these differences could be used for switching detection. In this paper we study the effectiveness of learning personal behavior patterns for switching detection and present a personalized approach which uses user's session history containing sessions with and without switches. Experiments show that users' personal habits and behavior patterns are indeed among the most informative signals. Our findings can be used by a search log analyzer for engine switching detection and potentially other log mining problems, thus providing valuable signals for search providers to improve user experience.

#index 1988860
#* Mining touch interaction data on mobile devices to predict web search result relevance
#@ Qi Guo;Haojian Jin;Dmitry Lagun;Shuai Yuan;Eugene Agichtein
#t 2013
#c 13
#% 169803
#% 209021
#% 280809
#% 309095
#% 320432
#% 340974
#% 766454
#% 805200
#% 879567
#% 907516
#% 946521
#% 1048694
#% 1055671
#% 1074099
#% 1074148
#% 1169589
#% 1190135
#% 1292474
#% 1355038
#% 1384641
#% 1450833
#% 1450845
#% 1573487
#% 1598515
#% 1619984
#% 1641927
#% 1693899
#% 1730722
#% 1746855
#% 1765932
#% 1766010
#% 1879012
#% 1879187
#! Fine-grained search interactions in the desktop setting, such as mouse cursor movements and scrolling, have been shown valuable for understanding user intent, attention, and their preferences for Web search results. As web search on smart phones and tablets becomes increasingly popular, previously validated desktop interaction models have to be adapted for the available touch interactions such as pinching and swiping, and for the different device form factors. In this paper, we present, to our knowledge, the first in-depth study of modeling interactions on touch-enabled device for improving Web search ranking. In particular, we evaluate a variety of touch interactions on a smart phone as implicit relevance feedback, and compare them with the corresponding fine-grained interactions on a desktop computer with mouse and keyboard as the primary input devices. Our experiments are based on a dataset collected from two user studies with 56 users in total, using a specially instrumented version of a popular mobile browser to capture the interaction data. We report a detailed analysis of the similarities and differences of fine-grained search interactions between the desktop and the smart phone modalities, and identify novel patterns of touch interactions indicative of result relevance. Finally, we demonstrate significant improvements to search ranking quality by mining touch interaction data.

#index 1988861
#* Improve collaborative filtering through bordered block diagonal form matrices
#@ Yongfeng Zhang;Min Zhang;Yiqun Liu;Shaoping Ma
#t 2013
#c 13
#% 121466
#% 173879
#% 274612
#% 330687
#% 729918
#% 761304
#% 844369
#% 961617
#% 1116993
#% 1260273
#% 1331170
#% 1363271
#% 1396094
#% 1400001
#% 1565432
#% 1591208
#% 1598364
#% 1598402
#% 1605920
#% 1622579
#% 1746800
#% 1746832
#% 1746884
#% 1847975
#! Collaborative Filtering-based recommendation algorithms have achieved widespread success on the Web, but little work has been performed to investigate appropriate user-item relationship structures of rating matrices. This paper presents a novel and general collaborative filtering framework based on (Approximate) Bordered Block Diagonal Form structure of user-item rating matrices. We show formally that matrices in (A)BBDF structures correspond to community detection on the corresponding bipartite graphs, and they reveal relationships among users and items intuitionally in recommendation tasks. By this framework, general and special interests of a user are distinguished, which helps to improve prediction accuracy in collaborative filtering tasks. Experimental results on four real-world datasets, including the Yahoo! Music dataset, which is currently the largest, show that the proposed framework helps many traditional collaborative filtering algorithms, such as User-based, Item-based, SVD and NMF approaches, to make more accurate rating predictions. Moreover, by leveraging smaller and denser submatrices to make predictions, this framework contributes to the scalability of recommender systems.

#index 1988862
#* Modeling the uniqueness of the user preferences for recommendation systems
#@ Haggai Roitman;David Carmel;Yosi Mass;Iris Eiron
#t 2013
#c 13
#% 342767
#% 805841
#% 813966
#% 987542
#% 1358070
#% 1405661
#% 1482440
#! In this paper we propose a novel framework for modeling the uniqueness of the user preferences for recommendation systems. User uniqueness is determined by learning to what extent the user's item preferences deviate from those of an "average user" in the system. Based on this framework, we suggest three different recommendation strategies that trade between uniqueness and conformity. Using two real item datasets, we demonstrate the effectiveness of our uniqueness based recommendation framework.

#index 1988863
#* Timeline generation with social attention
#@ Xin Wayne Zhao;Yanwei Guo;Rui Yan;Yulan He;Xiaoming Li
#t 2013
#c 13
#% 309096
#% 342707
#% 766460
#% 1598408
#% 1711764
#! Timeline generation is an important research task which can help users to have a quick understanding of the overall evolution of any given topic. It thus attracts much attention from research communities in recent years. Nevertheless, existing work on timeline generation often ignores an important factor, the attention attracted to topics of interest (hereafter termed "social attention"). Without taking into consideration social attention, the generated timelines may not reflect users' collective interests. In this paper, we study how to incorporate social attention in the generation of timeline summaries. In particular, for a given topic, we capture social attention by learning users' collective interests in the form of word distributions from Twitter, which are subsequently incorporated into a unified framework for timeline summary generation. We construct four evaluation sets over six diverse topics. We demonstrate that our proposed approach is able to generate both informative and interesting timelines. Our work sheds light on the feasibility of incorporating social attention into traditional text mining tasks.

#index 1988864
#* Summary of the NTCIR-10 INTENT-2 task: subtopic mining and search result diversification
#@ Tetsuya Sakai;Zhicheng Dou;Takehiro Yamamoto;Yiqun Liu;Min Zhang;Makoto P. Kato;Ruihua Song;Mayu Iwata
#t 2013
#c 13
#% 411762
#% 642975
#% 879630
#% 1166473
#% 1482296
#% 1536510
#% 1598438
#% 1648026
#% 1746848
#% 1988865
#% 2008819
#! The NTCIR INTENT task comprises two subtasks: {\em Subtopic Mining}, where systems are required to return a ranked list of {\em subtopic strings} for each given query; and {\em Document Ranking}, where systems are required to return a diversified web search result for each given query. This paper summarises the novel features of the Second INTENT task at NTCIR-10 and its main findings, and poses some questions for future diversified search evaluation.

#index 1988865
#* The impact of intent selection on diversified search evaluation
#@ Tetsuya Sakai;Zhicheng Dou;Charles L.A. Clarke
#t 2013
#c 13
#% 340890
#% 411762
#% 642975
#% 1074124
#% 1536510
#% 1598438
#% 1648026
#% 1667279
#% 1988864
#! To construct a diversified search test collection, a set of possible subtopics (or intents) needs to be determined for each topic, in one way or another, and perintent relevance assessments need to be obtained. In the TREC Web Track Diversity Task, subtopics are manually developed at NIST, based on results of automatic click log analysis; in the NTCIR INTENT Task, intents are determined by manually clustering 'subtopics strings' returned by participating systems. In this study, we address the following research question: Does the choice of intents for a test collection affect relative performances of diversified search systems? To this end, we use the TREC 2012 Web Track Diversity Task data and the NTCIR-10 INTENT-2 Task data, which share a set of 50 topics but have different intent sets. Our initial results suggest that the choice of intents may affect relative performances, and that this choice may be far more important than how many intents are selected for each topic

#index 1988866
#* A novel topic model for automatic term extraction
#@ Sujian Li;Jiwei Li;Tao Song;Wenjie Li;Baobao Chang
#t 2013
#c 13
#% 722904
#% 755838
#! Automatic term extraction (ATE) aims at extracting domain-specific terms from a corpus of a certain domain. Termhood is one essential measure for judging whether a phrase is a term. Previous researches on termhood mainly depend on the word frequency information. In this paper, we propose to compute termhood based on semantic representation of words. A novel topic model, namely i-SWB, is developed to map the domain corpus into a latent semantic space, which is composed of some general topics, a background topic and a documents-specific topic. Experiments on four domains demonstrate that our approach outperforms the state-of-the-art ATE approaches.

#index 1988867
#* Estimating query representativeness for query-performance prediction
#@ Mor Sondak;Anna Shtok;Oren Kurland
#t 2013
#c 13
#% 280864
#% 340901
#% 340948
#% 397161
#% 766525
#% 987260
#% 1130851
#% 1130990
#% 1263599
#% 1450861
#% 1467729
#% 1918417
#! The query-performance prediction (QPP) task is estimating retrieval effectiveness with no relevance judgments. We present a novel probabilistic framework for QPP that gives rise to an important aspect that was not addressed in previous work; namely, the extent to which the query effectively represents the information need for retrieval. Accordingly, we devise a few query-representativeness measures that utilize relevance language models. Experiments show that integrating the most effective measures with state-of-the-art predictors in our framework often yields prediction quality that significantly transcends that of using the predictors alone.

#index 1988868
#* Tagcloud-based explanation with feedback for recommender systems
#@ Wei Chen;Wynne Hsu;Mong Li Lee
#t 2013
#c 13
#% 319705
#% 824709
#% 1001313
#% 1598436
#! Personalized recommender systems aim to push only the relevant items and information directly to the users without requiring them to browse through millions of web resources. The challenge of these systems is to achieve a high user acceptance rate on their recommendations. In this paper, we aim to increase the user acceptance of recommendations by providing more intuitive tag-based explanations of why the items are recommended. Tags are used as intermediary entities that not only relate target users to the recommended items but also understand users' intents. Our system also allows tag-based online relevance feedback. Experiment results on the Movielens dataset show that the proposed approach is able to increase the acceptance rate of recommendations and improve user satisfaction.

#index 1988869
#* Finding knowledgeable groups in enterprise corpora
#@ Shangsong Liang;Maarten de Rijke
#t 2013
#c 13
#% 879570
#% 1214668
#% 1451234
#% 1573239
#% 1633087
#% 1642027
#% 1876161
#! The task of finding groups is a natural extension of search tasks aimed at retrieving individual entities. We introduce a group finding task: given a query topic, find knowledgeable groups that have expertise on that topic. We present four general strategies to this task. The models are formalized using generative language models. Two of the models aggregate expertise scores of the experts in the same group for the task, one locates documents associated with experts in the group and then determines how closely the documents are associated with the topic, whilst the remaining model directly estimates the degree to which a group is a knowledgeable group for a given topic. We construct a test collections based on the TREC 2005 and 2006 Enterprise collections. We find significant differences between different ways of estimating the association between a topic and a group. Experiments show that our knowledgeable group finding models achieve high absolute scores.

#index 1988870
#* An LDA-smoothed relevance model for document expansion: a case study for spoken document retrieval
#@ Debasis Ganguly;Johannes Leveling;Gareth J.F. Jones
#t 2013
#c 13
#% 280815
#% 340901
#% 722904
#% 783530
#% 879587
#% 940042
#% 1537475
#% 1806005
#% 1879083
#! Document expansion (DE) in information retrieval (IR) involves modifying each document in the collection by introducing additional terms into the document. It is particularly useful to improve retrieval of short and noisy documents where the additional terms can improve the description of the document content. Existing approaches to DE assume that documents to be expanded are from a single topic. In the case of multi-topic documents this can lead to a topic bias in terms selected for DE and hence may result in poor retrieval quality due to the lack of coverage of the original document topics in the expanded document. This paper proposes a new DE technique providing a more uniform selection and weighting of DE terms from all constituent topics. We show that our proposed method significantly outperforms the most recently reported relevance model based DE method on a spoken document retrieval task for both manual and automatic speech recognition transcripts.

#index 1988871
#* Kinship contextualization: utilizing the preceding and following structural elements
#@ Muhammad A. Norozi;Paavo Arvola
#t 2013
#c 13
#% 838390
#% 1131026
#% 1489429
#% 1598872
#% 1918408
#! The textual context of an element, structurally, contains traces of evidences. Utilizing this context in scoring is called contextualization. In this study we hypothesize that the context of an XML-element originated from its \textit{preceding} and \textit{following} elements in the sequential ordering of a document improves the quality of retrieval. In the tree form of the document's structure, \textit{kinship} contextualization means, contextualization based on the horizontal and vertical elements in the \textit{kinship tree,} or elements in closer to a wider structural kinship. We have tested several variants of kinship contextualization and verified notable improvements in comparison with the baseline system and gold standards in the retrieval of focused elements.

#index 1988872
#* Displaying relevance scores for search results
#@ Guy Shani;Noam Tractinsky
#t 2013
#c 13
#% 268079
#% 588725
#% 941784
#% 961649
#% 987212
#% 1279761
#% 1357833
#% 1376021
#% 1879001
#! Internet search engines typically compute a relevance score for webpages given the query terms, and then rank the pages by decreasing relevance scores. The popular search engines do not, however, present the relevance scores that were computed during this process. We suggest that these relevance scores may contain information that can help users make conscious decisions. In this paper we evaluate in a user study how users react to the display of such scores. The results indicate that users understand graphical displays of relevance, and make decisions based on these scores. Our results suggest that in the context of exploratory search, relevance scores may cause users to explore more search results.

#index 1988873
#* Ranking-oriented nearest-neighbor based method for automatic image annotation
#@ Chaoran Cui;Jun Ma;Tao Lian;Xiaofang Wang;Zhaochun Ren
#t 2013
#c 13
#% 734915
#% 1148273
#% 1268491
#% 1356643
#! Automatic image annotation plays a critical role in keyword-based image retrieval systems. Recently, the nearest-neighbor based scheme has been proposed and achieved good performance for image annotation. Given a new image, the scheme is to first find its most similar neighbors from labeled images, and then propagate the keywords associated with the neighbors to it. Many studies focused on designing a suitable distance metric between images so that all labeled images can be ranked by their distance to the given image. However, higher accuracy in distance prediction does not necessarily lead to better ordering of labeled images. In this paper, we propose a ranking-oriented neighbor search mechanism to rank labeled images directly without going through the intermediate step of distance prediction. In particular, a new learning to rank algorithm is developed, which exploits the implicit preference information of labeled images and underlines the accuracy of the top-ranked results. Experiments on two benchmark datasets demonstrate the effectiveness of our approach for image annotation.

#index 1988874
#* IRWR: incremental random walk with restart
#@ Weiren Yu;Xuemin Lin
#t 2013
#c 13
#% 577273
#% 905209
#% 915344
#% 1047785
#% 1073984
#% 1328169
#% 1372721
#% 1531275
#% 1707460
#! Random Walk with Restart (RWR) has become an appealing measure of node proximities in emerging applications \eg recommender systems and automatic image captioning. In practice, a real graph is typically large, and is frequently updated with small changes. It is often cost-inhibitive to recompute proximities from scratch via \emph{batch} algorithms when the graph is updated. This paper focuses on the incremental computations of RWR in a dynamic graph, whose edges often change over time. The prior attempt of RWR [1] deploys \kdash to find top-$k$ highest proximity nodes for a given query, which involves a strategy to incrementally \emph{estimate} upper proximity bounds. However, due to its aim to prune needless calculation, such an incremental strategy is \emph{approximate}: in $O(1)$ time for each node. The main contribution of this paper is to devise an \emph{exact} and fast incremental algorithm of RWR for edge updates. Our solution, \IRWR\!, can incrementally compute any node proximity in $O(1)$ time for each edge update without loss of exactness. The empirical evaluations show the high efficiency and exactness of \IRWR for computing proximities on dynamic networks against its batch counterparts.

#index 1988875
#* Interpretation of coordinations, compound generation, and result fusion for query variants
#@ Johannes Leveling
#t 2013
#c 13
#% 732850
#% 735075
#% 811358
#% 869501
#% 871579
#% 1450865
#% 1494791
#% 1717317
#! We investigate interpreting coordinations (e.g. word sequences connected with coordinating conjunctions such as "and" and "or") as logical disjunctions of terms to generate a set of disjunctionfree query variants for information retrieval (IR) queries. In addition, so-called hyphen coordinations are resolved by generating full compound forms and rephrasing the original query, e.g. "rice im-and export" is transformed into "rice import and export". Query variants are then processed separately and retrieval results are merged using a standard data fusion technique. We evaluate the approach on German standard IR benchmarking data. The results show that: i) Our proposed approach to generate compounds from hyphen coordinations produces the correct results for all test topics. ii) Our proposed heuristics to identify coordinations and generate query variants based on shallow natural language processing (NLP) techniques is highly accurate on the topics and does not rely on parsing or part-of-speech tagging. iii) Using query variants to produce multiple retrieval results and merging the results decreases precision at top ranks. However, in combination with blind relevance feedback (BRF), this approach can show significant improvement over the standard BRF baseline using the original queries.

#index 1988876
#* Sentiment analysis of user comments for one-class collaborative filtering over ted talks
#@ Nikolaos Pappas;Andrei Popescu-Belis
#t 2013
#c 13
#% 722805
#% 939897
#% 1176959
#% 1471205
#% 1476448
#% 1535285
#% 1558447
#% 1967862
#! User-generated texts such as reviews, comments or discussions are valuable indicators of users' preferences. Unlike previous works which focus on labeled data from user-contributed reviews, we focus here on user comments which are not accompanied by explicit rating labels. We investigate their utility for a one-class collaborative filtering task such as bookmarking, where only the user actions are given as ground truth. We propose a sentiment-aware nearest neighbor model (SANN) for multimedia recommendations over TED talks, which makes use of user comments. The model outperforms significantly, by more than 25% on unseen data, several competitive baselines.

#index 1988877
#* Report from the NTCIR-10 1CLICK-2 Japanese subtask: baselines, upperbounds and evaluation robustness
#@ Makoto P. Kato;Tetsuya Sakai;Takehiro Yamamoto;Mayu Iwata
#t 2013
#c 13
#% 1072570
#% 1227582
#% 1641986
#% 1667279
#% 1988913
#! The One Click Access Task (1CLICK) of NTCIR requires systems to return a concise multi-document summary of web pages in response to a query which is assumed to have been submitted in a mobile context. Systems are evaluated based on information units (or iUnits), and are required to present important pieces of information first and to minimise the amount of text the user has to read. Using the official Japanese results of the second round of the 1CLICK task from NTCIR-10, we discuss our task setting and evaluation framework. Our analyses show that: (1) Simple baseline methods that leverage search engine snippets or Wikipedia are effective for 'lookup' type queries but not necessarily for other query types; (2) There is still a substantial gap between manual and automatic runs; and (3) Our evaluation metrics are relatively robust to the incompleteness of iUnits.

#index 1988878
#* Leveraging viewer comments for mood classification of music video clips
#@ Takehiro Yamamoto;Satoshi Nakamura
#t 2013
#c 13
#% 741122
#% 741169
#% 1159898
#% 1267914
#% 1598417
#% 1711776
#% 1767718
#% 1967738
#! This short paper proposes a method to classify music video clips uploaded to a video sharing service into music mood categories such as 'cheerful,' 'wistful,' and 'aggressive.' The method leverages viewer comments posted to the music video clips for the music mood classification. It extracts specific features from the comments: (1) adjectives in comments, (2) lengthened words in comments, and (3) comments in chorus sections. Our experimental results classifying 695 video clips into six mood categories showed that our method outperformed the baseline in terms of macro and micro averaged F-measures. In addition, our method outperformed the existing approaches that utilize lyrics and audio signals of songs.

#index 1988879
#* RecSys for distributed events: investigating the influence of recommendations on visitor plans
#@ Richard Schaller;Morgan Harvey;David Elsweiler
#t 2013
#c 13
#% 1127492
#% 1625359
#% 1641995
#% 1891121
#! Distributed events are collections of events taking place within a small area over the same time period and relating to a single topic. There are often a large number of events on offer and the times in which they can be visited are heavily constrained, therefore the task of choosing events to visit and in which order can be very difficult. In this work we investigate how visitors can be assisted by means of a recommender system via 2 large-scale naturalistic studies (n=860 and n=1047). We show that a recommender system can influence users to select events that result in tighter and more compact routes, thus allowing users to spend less time travelling and more time visiting events.

#index 1988880
#* Commodity query by snapping
#@ Hao Huang;Yunjun Gao;Kevin Chiew;Qinming He;Lu Chen
#t 2013
#c 13
#% 330687
#% 975151
#% 1484702
#% 1650223
#! Commodity information such as prices and public reviews is always the concern of consumers. Helping them conveniently acquire these information as an instant reference is often of practical significance for their purchase activities. Nowadays, Web 2.0, linked data clouds, and the pervasiveness of smart hand held devices have created opportunities for this demand, i.e., users could just snap a photo of any commodity that is of interest at anytime and anywhere, and retrieve the relevant information via their Internet-linked mobile devices. Nonetheless, compared with the traditional keyword-based information retrieval, extracting the hidden information related to the commodities in photos is a much more complicated and challenging task, involving techniques such as pattern recognition, knowledge base construction, semantic comprehension, and statistic deduction. In this paper, we propose a framework to address this issue by leveraging on various techniques, and evaluate the effectiveness and efficiency of this framework with experiments on a prototype.

#index 1988881
#* Composition of TF normalizations: new insights on scoring functions for ad hoc IR
#@ François Rousseau;Michalis Vazirgiannis
#t 2013
#c 13
#% 46803
#% 169781
#% 218982
#% 340948
#% 411760
#% 766412
#% 1305656
#% 1450858
#% 1598452
#% 1641914
#% 1642160
#! Previous papers in ad hoc IR reported that scoring functions should satisfy a set of heuristic retrieval constraints, providing a mathematical justification for the normalizations historically applied to the term frequency (TF). In this paper, we propose a further level of abstraction, claiming that the successive normalizations are carried out through composition. Thus we introduce a principled framework that fully explains BM25 as a variant of TF-IDF with an inverse order of function composition. Our experiments over standard datasets indicate that the respective orders of composition chosen in the original papers for both TF-IDF and BM25 are the most effective ones. Moreover, since the order is different between the two models, they also demonstrated that the order is instrumental in the design of weighting models. In fact, while considering more complex scoring functions such as BM25+, we discovered a novel weighting model in terms of order of composition that consistently outperforms all the rest. Our contribution here is twofold: we provide a unifying mathematical framework for IR and a novel scoring function discovered using this framework.

#index 1988882
#* Interoperability ranking for mobile applications
#@ Dragomir Yankov;Pavel Berkhin;Rajen Subba
#t 2013
#c 13
#% 268079
#% 290830
#% 1366270
#% 1879190
#! At present, most major app marketplaces perform ranking and recommendation based on search relevance features or marketplace ``popularity'' statistics. For instance, they check similarity between app descriptions and user search queries, or rank-order the apps according to statistics such as number of downloads, user ratings etc. Rankings derived from such signals, important as they are, are insufficient to capture the dynamics of the apps ecosystem. Consider for example the questions: In a particular user context, is app A more likely to be launched than app B? Or does app C provide complementary functionality to app D-- Answering these questions requires identifying and analyzing the dependencies between apps in the apps ecosystem. Ranking mechanisms that reflect such interdependences are thus necessary. In this paper we introduce the notion of interoperability ranking for mobile applications. Intuitively, apps with high rank are such apps which are inferred to be somehow important to other apps in the ecosystem. We demonstrate how interoperability ranking can help answer the above questions and also provide the basis for solving several problems which are rapidly attracting the attention of both researchers and the industry, such as building personalized real-time app recommender systems or intelligent mobile agents. We describe a set of methods for computing interoperability ranks and analyze their performance on real data from the Windows Phone app marketplace.

#index 1988883
#* Explicit feedback in local search tasks
#@ Dmitry Lagun;Avneesh Sud;Ryen W. White;Peter Bailey;Georg Buscher
#t 2013
#c 13
#% 214709
#% 248225
#% 306468
#% 508416
#% 818206
#% 818256
#% 1598347
#% 1765973
#! Modern search engines make extensive use of people's contextual information to finesse result rankings. Using a searcher's location provides an especially strong signal for adjusting results for certain classes of queries where people may have clear preference for local results, without explicitly specifying the location in the query direct-ly. However, if the location estimate is inaccurate or searchers want to obtain many results from a particular location, they have limited control on the location focus in the search results returned. In this paper we describe a user study that examines the effect of offering searchers more control over how local preferences are gathered and used. We studied providing users with functionality to offer explicit relevance feedback (ERF) adjacent to results automatically identi-fied as location-dependent (i.e., more from this location). They can use this functionality to indicate whether they are interested in a particular search result and desire more results from that result's location. We compared the ERF system against a baseline (NoERF) that used the same underlying mechanisms to retrieve and rank results, but did not offer ERF support. User performance was as-sessed across 12 experimental participants over 12 location-sensitive topics, in a fully counter-balanced design. We found that participants interacted with ERF frequently, and there were signs that ERF has the potential to improve success rates and lead to more efficient searching for location-sensitive search tasks than NoERF.

#index 1988884
#* Mining web search topics with diverse spatiotemporal patterns
#@ Di Jiang;Wilfred Ng
#t 2013
#c 13
#% 722904
#% 869516
#% 881498
#% 1016371
#% 1055707
#% 1355044
#% 1399973
#% 1481659
#% 1560379
#% 1641979
#% 1688451
#% 1746875
#% 1919722
#% 1948196
#! Mining the latent topics from web search data and capturing their spatiotemporal patterns have many applications in information retrieval. As web search is heavily influenced by the spatial and temporal factors, the latent topics usually demonstrate a variety of spatiotemporal patterns. In the face of the diversity of these patterns, existing models are increasingly ineffective, since they capture only one dimension of the spatiotemporal patterns (either the spatial or temporal dimension) or simply assume that there exists only one kind of spatiotemporal patterns. Such oversimplification risks distorting the latent data structure and hindering the downstream usage of the discovered topics. In this paper, we introduce the Spatiotemporal Search Topic Model (SSTM) to discover the latent topics from web search data with capturing their diverse spatiotemporal patterns simultaneously. The SSTM can flexibly support diverse spatiotemporal patterns and seamlessly integrate the unique features in web search such as query words, URLs, timestamps and search sessions. The SSTM is demonstrated as an effective exploratory tool for large-scale web search data and it performs superiorly in quantitative comparisons to several state-of-the-art topic models.

#index 1988885
#* Here and there: goals, activities, and predictions about location from geotagged queries
#@ Robert West;Ryen W. White;Eric Horvitz
#t 2013
#c 13
#% 1550750
#% 1598347
#% 1606049
#% 1619985
#% 1897312
#% 1919700
#! A significant portion of Web search is performed in mobile settings. We explore the links between users' queries on mobile devices and their locations and movement, with a focus on interpreting queries about addresses. We find that users tend to have a primary location, likely corresponding to home or workplace, and that a user's location relative to this primary location systematically influences the patterns of address searches. We apply our findings to construct a statistical model that can predict with high accuracy whether a user will be soon observed at an address that had been recently retrieved via search. Such an ability to predict that a user will transition to a location can be harnessed for multiple uses including provision of directions and traffic information, the rendering of competitive advertising, and guiding the opportunistic completion of pending tasks that can be accomplished en route to a target location.

#index 1988886
#* Optimizing top-n collaborative filtering via dynamic negative item sampling
#@ Weinan Zhang;Tianqi Chen;Jun Wang;Yong Yu
#t 2013
#c 13
#% 1074061
#% 1176959
#% 1417104
#% 1476486
#% 1879008
#% 1893820
#% 1893826
#% 1967299
#! Collaborative filtering techniques rely on aggregated user preference data to make personalized predictions. In many cases, users are reluctant to explicitly express their preferences and many recommender systems have to infer them from implicit user behaviors, such as clicking a link in a webpage or playing a music track. The clicks and the plays are good for indicating the items a user liked (i.e., positive training examples), but the items a user did not like (negative training examples) are not directly observed. Previous approaches either randomly pick negative training samples from unseen items or incorporate some heuristics into the learning model, leading to a biased solution and a prolonged training period. In this paper, we propose to dynamically choose negative training samples from the ranked list produced by the current prediction model and iteratively update our model. The experiments conducted on three large-scale datasets show that our approach not only reduces the training time, but also leads to significant performance gains.

#index 1988887
#* Bias-variance decomposition of ir evaluation
#@ Peng Zhang;Dawei Song;Jun Wang;Yuexian Hou
#t 2013
#c 13
#% 340901
#% 879631
#% 891559
#% 1074207
#% 1227590
#% 1227591
#% 1292550
#% 1879081
#! It has been recognized that, when an information retrieval (IR) system achieves improvement in mean retrieval effectiveness (e.g. mean average precision (MAP)) over all the queries, the performance (e.g., average precision (AP)) of some individual queries could be hurt, resulting in retrieval instability. Some stability/robustness metrics have been proposed. However, they are often defined separately from the mean effectiveness metric. Consequently, there is a lack of a unified formulation of effectiveness, stability and overall retrieval quality (considering both). In this paper, we present a unified formulation based on the bias-variance decomposition. Correspondingly, a novel evaluation methodology is developed to evaluate the effectiveness and stability in an integrated manner. A case study applying the proposed methodology to evaluation of query language modeling illustrates the usefulness and analytical power of our approach.

#index 1988888
#* The cluster hypothesis for entity oriented search
#@ Hadas Raviv;Oren Kurland;David Carmel
#t 2013
#c 13
#% 32813
#% 218992
#% 228105
#% 375017
#% 427921
#% 766431
#% 1074119
#% 1263596
#% 1275012
#% 1415748
#% 1489451
#% 1620194
#% 1904924
#% 1925701
#! In this work we study the cluster hypothesis for entity oriented search (EOS). Specifically, we show that the hypothesis can hold to a substantial extent for several entity similarity measures. We also demonstrate the retrieval effectiveness merits of using clusters of similar entities for EOS.

#index 1988889
#* Question retrieval with user intent
#@ Long Chen;Dell Zhang;Mark Levene
#t 2013
#c 13
#% 252011
#% 838398
#% 1074110
#% 1077150
#% 1130311
#% 1292492
#% 1747123
#! Community Question Answering (CQA) services, such as Yahoo! Answers and WikiAnswers, have become popular with users as one of the central paradigms for satisfying users' information needs. The task of question retrieval in CQA aims to resolve one's query directly by finding the most relevant questions (together with their answers) from an archive of past questions. However, as users can ask any question that they like, a large number of questions in CQA are not about objective (factual) knowledge, but about subjective (sentiment-based) opinions or social interactions. The inhomogeneous nature of CQA leads to reduced performance of standard retrieval models. To address this problem, we present a hybrid approach that blends several language modelling techniques for question retrieval, namely, the classic (query-likelihood) language model, the state-of-the-art translation-based language model, and our proposed intent-based language model. The user intent of each candidate question (objective/subjective/social) is given by a probabilistic classifier which makes use of both textual features and metadata features. Our experiments on two real-world datasets show that our approach can significantly outperform existing ones.

#index 1988890
#* Using social annotations to enhance document representation for personalized search
#@ Mohamed Reda BOUADJENEK;Hakim Hacid;Mokrane Bouzeghoub;Athena Vakali
#t 2013
#c 13
#% 590523
#% 722904
#% 869548
#% 956544
#% 1074070
#% 1130827
#% 1130901
#% 1409929
#% 1536533
#% 1697448
#! In this paper, we present a contribution to IR modeling. We propose an approach that computes on the fly, a Personalized Social Document Representation (PSDR) of each document per user based on his social activities. The PSDRs are used to rank documents with respect to a query. This approach has been intensively evaluated on a large public dataset, showing significant benefits for personalized search.

#index 1988891
#* Sopra: a new social personalized ranking function for improving web search
#@ Mohamed Reda Bouadjenek;Hakim Hacid;Mokrane Bouzeghoub
#t 2013
#c 13
#% 722904
#% 869548
#% 956544
#% 1074070
#% 1130827
#% 1194631
#% 1292505
#% 1292590
#% 1398180
#% 1409929
#% 1415717
#% 1482278
#% 1667787
#% 1697448
#% 1988890
#! We present in this paper a contribution to IR modeling by proposing a new ranking function called SoPRa that considers the social dimension of the Web. This social dimension is any social information that surrounds documents along with the social context of users. Currently, our approach relies on folksonomies for extracting these social contexts, but it can be extended to use any social meta-data, e.g. comments, ratings, tweets, etc. The evaluation performed on our approach shows its benefits for personalized search.

#index 1988892
#* Effectiveness/efficiency tradeoffs for candidate generation in multi-stage retrieval architectures
#@ Nima Asadi;Jimmy Lin
#t 2013
#c 13
#% 411762
#% 730065
#% 878624
#% 1019084
#% 1268491
#% 1355057
#% 1486652
#% 1598342
#% 1598430
#% 1598433
#% 1604467
#% 1879018
#% 1919953
#% 1948129
#! This paper examines a multi-stage retrieval architecture consisting of a candidate generation stage, a feature extraction stage, and a reranking stage using machine-learned models. Given a fixed set of features and a learning-to-rank model, we explore effectiveness/efficiency tradeoffs with three candidate generation approaches: postings intersection with SvS, conjunctive query evaluation with WAND, and disjunctive query evaluation with WAND. We find no significant differences in end-to-end effectiveness as measured by NDCG between conjunctive and disjunctive WAND, but conjunctive query evaluation is substantially faster. Postings intersection with SvS, while fast, yields substantially lower end-to-end effectiveness, suggesting that document and term frequencies remain important in the initial ranking stage. These findings show that conjunctive WAND is the best overall candidate generation strategy of those we examined.

#index 1988893
#* Finding impressive social content creators: searching for SNS illustrators using feedback on motifs and impressions
#@ Yohei Seki;Kiyoto Miyajima
#t 2013
#c 13
#% 269218
#% 411762
#% 449588
#% 955010
#% 1019093
#% 1598376
#% 1783918
#% 1874776
#% 1879583
#! We propose a method for finding impressive creators in online social network sites (SNSs). Many users are actively engaged in publishing their own works, sharing visual content on sites such as YouTube or Flickr. In this paper, we focus on the Japanese illustration-sharing SNS, Pixiv. We implement an illustrator search system based on user impression categories. The impressions of illustrators are estimated from clues in the crowdsourced social-tag annotations on their illustrations. We evaluated our system in terms of normalized discounted cumulative gain and found that using feedback on motifs and impressions for illustrations of relevant illustrators improved illustrator search by 11%.

#index 1988894
#* Self reinforcement for important passage retrieval
#@ Ricardo Ribeiro;Luís Marujo;David Martins de Matos;João P. Neto;Anatole Gershman;Jaime Carbonell
#t 2013
#c 13
#% 262112
#% 268079
#% 397137
#% 742225
#% 992313
#% 1074086
#% 1264284
#% 1272053
#% 1301029
#% 1395709
#% 1434185
#% 1450890
#% 1452595
#% 1465383
#% 1470695
#% 1474900
#% 1544046
#% 1551291
#% 1765803
#% 1918410
#! In general, centrality-based retrieval models treat all elements of the retrieval space equally, which may reduce their effectiveness. In the specific context of extractive summarization (or important passage retrieval), this means that these models do not take into account that information sources often contain lateral issues, which are hardly as important as the description of the main topic, or are composed by mixtures of topics. We present a new two-stage method that starts by extracting a collection of key phrases that will be used to help centrality-as-relevance retrieval model. We explore several approaches to the integration of the key phrases in the centrality model. The proposed method is evaluated using different datasets that vary in noise (noisy vs clean) and language (Portuguese vs English). Results show that the best variant achieves relative performance improvements of about 31% in clean data and 18% in noisy data.

#index 1988895
#* Shame to be sham: addressing content-based grey hat search engine optimization
#@ Fiana Raiber;Kevyn Collins-Thompson;Oren Kurland
#t 2013
#c 13
#% 268079
#% 340901
#% 397161
#% 829028
#% 881477
#% 912202
#% 957993
#% 987244
#% 987260
#% 1194319
#% 1415713
#% 1467729
#% 1536512
#% 1621236
#% 1967746
#! We present an initial study identifying a form of content-based grey hat search engine optimization, in which a Web page contains both potentially relevant content and manipulated content: we call such pages sham documents, because they lie in the grey area between 'ham' (clearly normal) and 'spam' (clearly fake). Sham documents are often ranked artificially high in response to certain queries, but also may contain some useful information and cannot be considered as absolute spam. We report a novel annotation effort performed with the ClueWeb09 benchmark where pages were labeled as being spam, sham, or legitimate content. Significant inter-annotator agreement rates support the claim that there are sham documents that are highly ranked by a very effective retrieval approach, yet are not spam. We also present an initial study of predictors that may indicate whether a query is the target of shamming.

#index 1988896
#* Linking transcribed conversational speech
#@ Joseph Malionek;Douglas W. Oard;Abhijeet Sangwan;John H.L. Hansen
#t 2013
#c 13
#% 575568
#% 879571
#% 1130858
#% 1622396
#% 1767046
#% 1974614
#! As large collections of historically significant recorded speech become increasingly available, scholars are faced with the challenge of making sense of what they hear. This paper proposes automatically linking conversational speech to related resources as one way of supporting that sense-making task. Experiment results with transcribed conversations suggest that this kind of linking has promise for helping to contextualize recordings of detail-oriented conversations, and that simple sliding-window bag-of-words techniques can identify some useful links.

#index 1988897
#* Flat vs. hierarchical phrase-based translation models for cross-language information retrieval
#@ Ferhan Ture;Jimmy Lin
#t 2013
#c 13
#% 232656
#% 262096
#% 280851
#% 397144
#% 643017
#% 735135
#% 740915
#% 748718
#% 766428
#% 807747
#% 807749
#% 815858
#% 816170
#% 818262
#% 818270
#% 951636
#% 979655
#% 1019124
#% 1019133
#% 1215368
#% 1471432
#% 1587403
#% 1879164
#% 1905988
#! Although context-independent word-based approaches remain popular for cross-language information retrieval, many recent studies have shown that integrating insights from modern statistical machine translation systems can lead to substantial improvements in effectiveness. In this paper, we compare flat and hierarchical phrase-based translation models for query translation. Both approaches yield significantly better results than either a token-based or a one-best translation baseline on standard test collections. The choice of model manifests interesting tradeoffs in terms of effectiveness, efficiency, and model compactness.

#index 1988898
#* Mapping queries to questions: towards understanding users' information needs
#@ Yunjun Gao;Lu Chen;Rui Li;Gang Chen
#t 2013
#c 13
#% 838398
#% 1055679
#% 1074110
#% 1270283
#% 1272053
#! In this paper, for the first time, we study the problem of mapping keyword queries to questions on community-based question answering (CQA) sites. Mapping general web queries to questions enables search engines not only to discover explicit and specific information needs (questions) behind keywords queries, but also to find high quality information (answers) for answering keyword queries. In order to map queries to questions, we propose a ranking algorithm containing three steps: Candidate Question Selection, Candidate Question Ranking, and Candidate Question Grouping. Preliminary experimental results using 60 queries from search logs of a commercial engine show that the presented approach can efficiently find the questions which capture user's information needs explicitly.

#index 1988899
#* Building a web test collection using social media
#@ Chia-Jung Lee;W. Bruce Croft
#t 2013
#c 13
#% 340901
#% 766409
#% 818262
#% 879598
#% 987200
#% 987231
#% 1074132
#% 1450865
#! Community Question Answering (CQA) platforms contain a large number of questions and associated answers. Answerers sometimes include URLs as part of the answers to provide further information. This paper describes a novel way of building a test collection for web search by exploiting the link information from this type of social media data. We propose to build the test collection by regarding CQA questions as queries and the associated linked web pages as relevant documents. To evaluate this approach, we collect approximately ten thousand CQA queries, whose answers contained links to ClueWeb09 documents after spam filtering. Experimental results using this collection show that the relative effectiveness between different retrieval models on the ClueWeb-CQA query set is consistent with that on the TREC Web Track query sets, confirming the reliability of our test collection. Further analysis shows that the large number of queries generated through this approach compensates for the sparse relevance judgments in determining significant differences.

#index 1988900
#* Document classification by topic labeling
#@ Swapnil Hingmire;Sandeep Chougule;Girish K. Palshikar;Sutanu Chakraborti
#t 2013
#c 13
#% 311027
#% 722904
#! In this paper, we propose Latent Dirichlet Allocation (LDA) [1] based document classification algorithm which does not require any labeled dataset. In our algorithm, we construct a topic model using LDA, assign one topic to one of the class labels, aggregate all the same class label topics into a single topic using the aggregation property of the Dirichlet distribution and then automatically assign a class label to each unlabeled document depending on its "closeness" to one of the aggregated topics. We present an extension to our algorithm based on the combination of Expectation-Maximization (EM) algorithm and a naive Bayes classifier. We show effectiveness of our algorithm on three real world datasets.

#index 1988901
#* Browse with a social web directory
#@ Hao Huang;Yunjun Gao;Lu Chen;Rui Li;Kevin Chiew;Qinming He
#t 2013
#c 13
#% 717133
#% 766437
#% 857478
#% 866327
#% 956515
#% 956544
#% 956589
#% 1131829
#% 1524236
#% 1560398
#! Browse with either web directories or social bookmarks is an important complementation to search by keywords in web information retrieval. To improve users' browse experiences and facilitate the web directory construction, in this paper, we propose a novel browse system called Social Web Directory (SWD for short) by integrating web directories and social bookmarks. In SWD, (1) web pages are automatically categorized to a hierarchical structure to be retrieved efficiently, and (2) the popular web pages, hottest tags, and expert users in each category are ranked to help users find information more conveniently. Extensive experimental results demonstrate the effectiveness of our SWD system.

#index 1988902
#* The bag-of-repeats representation of documents
#@ Matthias Gallé
#t 2013
#c 13
#% 235941
#% 375017
#% 642990
#% 722803
#% 876067
#% 1117083
#% 1701756
#% 1806039
#! n-gram representations of documents may improve over a simple bag-of-word representation by relaxing the independence assumption of word and introducing context. However, this comes at a cost of adding features which are non-descriptive, and increasing the dimension of the vector space model exponentially. We present new representations that avoid both pitfalls. They are based on sound theoretical notions of stringology, and can be computed in optimal asymptotic time with algorithms using data structures from the suffix family. While maximal repeats have been used in the past for similar tasks, we show how another equivalence class of repeats -- largest-maximal repeats -- obtain similar or better results, with only a fraction of the features. This class acts as a minimal generative basis of all repeated substrings. We also report their use for topic modeling, showing easier to interpret models.

#index 1988903
#* Time-aware structured query suggestion
#@ Taiki Miyanishi;Tetsuya Sakai
#t 2013
#c 13
#% 411762
#% 730070
#% 1130854
#% 1292528
#% 1400017
#% 1482240
#% 1560365
#% 1746837
#% 1879052
#! Most commercial search engines have a query suggestion feature, which is designed to capture various possible search intents behind the user's original query. However, even though different search intents behind a given query may have been popular at different time periods in the past, existing query suggestion methods neither utilize nor present such information. In this study, we propose Time-aware Structured Query Suggestion (TaSQS) which clusters query suggestions along a timeline so that the user can narrow down his search from a temporal point of view. Moreover, when a suggested query is clicked, TaSQS presents web pages from query-URL bipartite graphs after ranking them according to the click counts within a particular time period. Our experiments using data from a commercial search engine log show that the time-aware clustering and the time-aware document ranking features of TaSQS are both effective.

#index 1988904
#* What can pictures tell us about web pages?: improving document search using images
#@ Sergio Rodriguez-Vaamonde;Lorenzo Torresani;Andrew Fitzgibbon
#t 2013
#c 13
#% 400847
#% 1392489
#% 1493670
#! Traditional Web search engines do not use the images in the HTML pages to find relevant documents for a given query. Instead, they typically operate by computing a measure of agreement between the keywords provided by the user and only the text portion of each page. In this paper we study whether the content of the pictures appearing in a Web page can be used to enrich the semantic description of an HTML document and consequently boost the performance of a keyword-based search engine. We present a Web-scalable system that exploits a pure text-based search engine to find an initial set of candidate documents for a given query. Then, the candidate set is reranked using semantic information extracted from the images contained in the pages. The resulting system retains the computational efficiency of traditional text-based search engines with only a small additional storage cost needed to encode the visual information. We test our approach on the TREC 2009 Million Query Track, where we show that our use of visual content yields improvement in accuracies for two distinct text-based search engines, including the system with the best reported performance on this benchmark.

#index 1988905
#* Relating retrievability, performance and length
#@ Colin Wilkie;Leif Azzopardi
#t 2013
#c 13
#% 218982
#% 375017
#% 1130849
#% 1130863
#% 1227729
#% 1244715
#% 1292722
#% 1400106
#% 1451016
#% 1482283
#% 1574743
#% 1697451
#! Retrievability provides a different way to evaluate an Information Retrieval (IR) system as it focuses on how easily documents can be found. It is intrinsically related to retrieval performance because a document needs to be retrieved before it can be judged relevant. In this paper, we undertake an empirical investigation into the relationship between the retrievability of documents, the retrieval bias imposed by a retrieval system, and the retrieval performance, across different amounts of document length normalization. To this end, two standard IR models are used on three TREC test collections to show that there is a useful and practical link between retrievability and performance. Our findings show that minimizing the bias across the document collection leads to good performance (though not the best performance possible). We also show that past a certain amount of document length normalization the retrieval bias increases, and the retrieval performance significantly and rapidly decreases. These findings suggest that the relationship between retrievability and effectiveness may offer a way to automatically tune systems.

#index 1988906
#* Hybrid retrieval approaches to geospatial music recommendation
#@ Markus Schedl;Dominik Schnitzer
#t 2013
#c 13
#% 1541728
#% 1584316
#% 1649210
#% 1853751
#% 1868267
#% 1869114
#% 1879168
#% 1967821
#% 2004136
#! Recent advances in music retrieval and recommendation algorithms highlight the necessity to follow multimodal approaches in order to transcend limits imposed by methods that solely use audio, web, or collaborative filtering data. In this paper, we propose hybrid music recommendation algorithms that combine information on the music content, the music context, and the user context, in particular, integrating location-aware weighting of similarities. Using state-of-the-art techniques to extract audio features and contextual web features, and a novel standardized data set of music listening activities inferred from microblogs (MusicMicro), we propose several multimodal retrieval functions. The main contributions of this paper are (i) a systematic evaluation of mixture coefficients between state-of-the-art audio features and web features, using the first standardized microblog data set of music listening events for retrieval purposes and (ii) novel geospatial music recommendation approaches using location information of microblog users, and a comprehensive evaluation thereof.

#index 1988907
#* Pursuing insights about healthcare utilization via geocoded search queries
#@ Shuang-Hong Yang;Ryen W. White;Eric Horvitz
#t 2013
#c 13
#% 310567
#% 869516
#% 1055707
#% 1400018
#% 1598340
#% 1879019
#! Mobile devices provide people with a conduit to the rich infor-mation resources of the Web. With consent, the devices can also provide streams of information about search activity and location that can be used in population studies and real-time assistance. We analyzed geotagged mobile queries in a privacy-sensitive study of potential transitions from health information search to in-world healthcare utilization. We note differences in people's health infor-mation seeking before, during, and after the appearance of evidence that a medical facility has been visited. We find that we can accu-rately estimate statistics about such potential user engagement with healthcare providers. The findings highlight the promise of using geocoded search for sensing and predicting activities in the world.

#index 1988908
#* Estimating topical context by diverging from external resources
#@ Romain Deveaud;Eric SanJuan;Patrice Bellot
#t 2013
#c 13
#% 169739
#% 169781
#% 340901
#% 766440
#% 879584
#% 956564
#% 987231
#% 987333
#% 1227622
#% 1292759
#% 1608935
#% 1621236
#% 1693906
#! Improving query understanding is crucial for providing the user with information that suits her needs. To this end, the retrieval system must be able to deal with several sources of knowledge from which it could infer a topical context. The use of external sources of information for improving document retrieval has been extensively studied. Improvements with either structured or large sets of data have been reported. However, in these studies resources are often used separately and rarely combined together. We experiment in this paper a method that discounts documents based on their weighted divergence from a set of external resources. We present an evaluation of the combination of four resources on two standard TREC test collections. Our proposed method significantly outperforms a state-of-the-art Mixture of Relevance Models on one test collection, while no significant differences are detected on the other one.

#index 1988909
#* Extractive summarisation via sentence removal: condensing relevant sentences into a short summary
#@ Marco Bonzanini;Miguel Martinez-Alvarez;Thomas Roelleke
#t 2013
#c 13
#% 194251
#% 288614
#% 983540
#% 1127964
#% 1250237
#% 1484279
#% 1697484
#! Many on-line services allow users to describe their opinions about a product or a service through a review. In order to help other users to find out the major opinion about a given topic, without the effort to read several reviews, multi-document summarisation is required. This research proposes an approach for extractive summarisation, supporting different scoring techniques, such as cosine similarity or divergence, as a method for finding representative sentences. The main contribution of this paper is the definition of an algorithm for sentence removal, developed to maximise the score between the summary and the original document. Instead of ranking the sentences and selecting the most important ones, the algorithm iteratively removes unimportant sentences until a desired compression rate is reached. Experimental results show that variations of the sentence removal algorithm provide good performance.

#index 1988910
#* Is relevance hard work?: evaluating the effort of making relevant assessments
#@ Robert Villa;Martin Halvey
#t 2013
#c 13
#% 133894
#% 235918
#% 260245
#% 270279
#% 309062
#% 397164
#% 879598
#% 1253803
#% 1480199
#% 1598424
#% 1598516
#% 1627738
#! The judging of relevance has been a subject of study in information retrieval for a long time, especially in the creation of relevance judgments for test collections. While the criteria by which assessors? judge relevance has been intensively studied, little work has investigated the process individual assessors go through to judge the relevance of a document. In this paper, we focus on the process by which relevance is judged, and in particular, the degree of effort a user must expend to judge relevance. By better understanding this effort in isolation, we may provide data which can be used to create better models of search. We present the results of an empirical evaluation of the effort users must exert to judge the relevance of document, investigating the effect of relevance level and document size. Results suggest that 'relevant' documents require more effort to judge when compared to highly relevant and not relevant documents, and that effort increases as document size increases.

#index 1988911
#* Cumulative citation recommendation: classification vs. ranking
#@ Krisztian Balog;Heri Ramampiaro
#t 2013
#c 13
#% 424028
#% 575568
#% 1019082
#% 1089602
#% 1130858
#% 1592043
#% 1994954
#! Cumulative citation recommendation refers to the task of filtering a time-ordered corpus for documents that are highly relevant to a predefined set of entities. This task has been introduced at the TREC Knowledge Base Acceleration track in 2012, where two main families of approaches emerged: classification and ranking. In this paper we perform an experimental comparison of these two strategies using supervised learning with a rich feature set. Our main finding is that ranking outperforms classification on all evaluation settings and metrics. Our analysis also reveals that a ranking-based approach has more potential for future improvements.

#index 1988912
#* Is uncertain logical-matching equivalent to conditional probability?
#@ Karam Abdulahhad;Jean-Pierre Chevallet;Catherine Berrut
#t 2013
#c 13
#% 54459
#% 118746
#% 120103
#% 144069
#% 169764
#% 259460
#% 487140
#% 650814
#% 1074833
#% 1377453
#! Logic-based Information Retrieval (IR) models represent the retrieval decision as a logical implication d-q between a document d and a query q, where d and q are logical sentences. However, d-q is a binary decision, we thus need a measure to estimate the degree to which d implies q, denoted P(d-q). In this study, we revisit the Van Rijsbergen's assumptions about: 1- the logical implication -' is not the material one, and 2- P(d-q) could be estimated by the conditional probability P(q|d). More precisely, we claim that the material implication is an appropriate implication for IR, and also we mathematically prove that replacing P(d-q) by P(q|d) is a correct choice. In order to prove the Van Rijsbergen's assumption, we use the Propositional Logic and the Lattice theory. We also exploit the notion of degree of implication that is proposed by Knuth.

#index 1988913
#* Exploring semi-automatic nugget extraction for Japanese one click access evaluation
#@ Matthew Ekstrand-Abueg;Virgil Pavlu;Makoto Kato;Tetsuya Sakai;Takehiro Yamamoto;Mayu Iwata
#t 2013
#c 13
#% 520224
#% 893431
#% 1074133
#% 1227582
#% 1641986
#% 1771895
#% 1918349
#! Building test collections based on nuggets is useful evaluating systems that return documents, answers, or summaries. However, nugget construction requires a lot of manual work and is not feasible for large query sets. Towards an efficient and scalable nugget-based evaluation, we study the applicability of semi-automatic nugget extraction in the context of the ongoing NTCIR One Click Access (1CLICK) task. We compare manually-extracted and semi-automatically-extracted Japanese nuggets to demonstrate the coverage and efficiency of the semi-automatic nugget extraction. Our findings suggest that the manual nugget extraction can be replaced with a direct adaptation of the English semi-automatic nugget extraction system, especially for queries for which the user desires broad answers from free-form text.

#index 1988914
#* A study on the accuracy of Flickr's geotag data
#@ Claudia Hauff
#t 2013
#c 13
#% 340948
#% 1132018
#% 1227637
#% 1278585
#% 1429406
#% 1450997
#% 1528636
#% 1583922
#% 1649276
#% 1879061
#% 1948196
#! Obtaining geographically tagged multimedia items from social Web platforms such as Flickr is beneficial for a variety of applications including the automatic creation of travelogues and personalized travel recommendations. In order to take advantage of the large number of photos and videos that do not contain (GPS-based) latitude/longitude coordinates, a number of approaches have been proposed to estimate the geographic location where they were taken. Such location estimation methods rely on existing geotagged multimedia items as training data. Across application and usage scenarios, it is commonly assumed that the available geotagged items contain (reasonably) accurate latitude/longitude coordinates. Here, we consider this assumption and investigate how accurate the provided location data is. We conduct a study of Flickr images and videos and find that the accuracy of the geotag information is highly dependent on the popularity of the location: images/videos taken at popular (unpopular) locations, are likely to be geotagged with a high (low) degree of accuracy with respect to the ground truth.

#index 1988915
#* Recommending personalized touristic sights using google places
#@ Maya Sappelli;Suzan Verberne;Wessel Kraaij
#t 2013
#c 13
#% 46803
#% 326522
#! The purpose of the Contextual Suggestion track, an evaluation task at the TREC 2012 conference, is to suggest personalized tourist activities to an individual, given a certain location and time. In our content-based approach, we collected initial recommendations using the location context as search query in Google Places. We first ranked the recommendations based on their textual similarity to the user profiles. In order to improve the ranking of popular sights, we combined the initial ranking with rankings based on Google Search, popularity and categories. Finally, we performed filtering based on the temporal context. Overall, our system performed well above average and median, and outperformed the baseline - Google Places only -- run.

#index 1988916
#* Assessor disagreement and text classifier accuracy
#@ William Webber;Jeremy Pickens
#t 2013
#c 13
#% 312689
#% 840920
#% 1450896
#% 1558464
#! Text classifiers are frequently used for high-yield retrieval from large corpora, such as in e-discovery. The classifier is trained by annotating example documents for relevance. These examples may, however, be assessed by people other than those whose conception of relevance is authoritative. In this paper, we examine the impact that disagreement between actual and authoritative assessor has upon classifier effectiveness, when evaluated against the authoritative conception. We find that using alternative assessors leads to a significant decrease in binary classification quality, though less so ranking quality. A ranking consumer would have to go on average 25% deeper in the ranking produced by alternative-assessor training to achieve the same yield as for authoritative-assessor training.

#index 1988917
#* Author disambiguation by hierarchical agglomerative clustering with adaptive stopping criterion
#@ Lei Cen;Eduard C. Dragut;Luo Si;Mourad Ouzzani
#t 2013
#c 13
#% 722904
#% 723544
#% 757953
#% 760866
#% 809459
#% 871766
#% 913783
#% 937552
#% 1019190
#% 1211086
#% 1314445
#% 1328067
#% 1348087
#% 1498542
#% 1880313
#! Entity disambiguation is an important step in many information retrieval applications. This paper proposes new research for entity disambiguation with the focus of name disambiguation in digital libraries. In particular, pairwise similarity is first learned for publications that share the same author name string (ANS) and then a novel Hierarchical Agglomerative Clustering approach with Adaptive Stopping Criterion (HACASC) is proposed to adaptively cluster a set of publications that share a same ANS to individual clusters of publications with different author identities. The HACASC approach utilizes a mixture of kernel ridge regressions to intelligently determine the threshold in clustering. This obtains more appropriate clustering granularity than non-adaptive stopping criterion. We conduct a large scale empirical study with a dataset of more than 2 million publication record pairs to demonstrate the advantage of the proposed HACASC approach.

#index 1988918
#* Who will retweet me?: finding retweeters in twitter
#@ Zhunchen Luo;Miles Osborne;Jintao Tang;Ting Wang
#t 2013
#c 13
#% 321635
#% 1190127
#% 1268491
#% 1379671
#% 1399992
#% 1482397
#% 1512437
#% 1560174
#% 1711545
#% 1846469
#% 1906975
#! An important aspect of communication in Twitter (and other Social Network is message propagation -- people creating posts for others to share. Although there has been work on modelling how tweets in Twitter are propagated (retweeted), an untackled problem has been who will retweet a message. Here we consider the task of finding who will retweet a message posted on Twitter. Within a learning to-rank framework, we explore a wide range of features, such as retweet history, followers status, followers active time and followers interests. We find that followers who retweeted or mentioned the author's tweets frequently before and have common interests are more likely to be retweeters.

#index 1988919
#* Sequential testing in classifier evaluation yields biased estimates of effectiveness
#@ William Webber;Mossaab Bagdouri;David D. Lewis;Douglas W. Oard
#t 2013
#c 13
#% 194284
#% 222442
#% 296521
#% 763708
#% 881477
#% 1715616
#% 1812630
#% 1933123
#! It is common to develop and validate classifiers through a process of repeated testing, with nested training and/or test sets of increasing size. We demonstrate in this paper that such repeated testing leads to biased estimates of classifier effectiveness. Experiments on a range of text classification tasks under three sequential testing frameworks show all three lead to optimistic estimates of effectiveness. We calculate empirical adjustments to unbias estimates on our data set, and identify directions for research that could lead to general techniques for avoiding bias while reducing labeling costs.

#index 1988920
#* On contextual photo tag recommendation
#@ Philip J. McParlane;Yashar Moshfeghi;Joemon M. Jose
#t 2013
#c 13
#% 397145
#% 457912
#% 642989
#% 1055704
#% 1127458
#% 1447729
#% 1693864
#% 1967778
#! Image tagging is a growing application on social media websites, however, the performance of many auto-tagging methods are often poor. Recent work has exploited an image's context (e.g. time and location) in the tag recommendation process, where tags which co-occur highly within a given time interval or geographical area are promoted. These models, however, fail to address how and when different image contexts can be combined. In this paper, we propose a weighted tag recommendation model, building on an existing state-of-the-art, which varies the importance of time and location in the recommendation process, based on a given set of input tags. By retrieving more temporally and geographically relevant tags, we achieve statistically significant improvements to recommendation accuracy when testing on 519k images collected from Flickr. The result of this paper is an important step towards more effective image annotation and retrieval systems.

#index 1988921
#* Document features predicting assessor disagreement
#@ Praveen Chandar;William Webber;Ben Carterette
#t 2013
#c 13
#% 312689
#% 818276
#% 861988
#% 1074134
#% 1450896
#% 1523458
#% 1536512
#% 1693883
#% 1879138
#% 1918347
#! The notion of relevance differs between assessors, thus giving rise to assessor disagreement. Although assessor disagreement has been frequently observed, the factors leading to disagreement are still an open problem. In this paper we study the relationship between assessor disagreement and various topic independent factors such as readability and cohesiveness. We build a logistic model using reading level and other simple document features to predict assessor disagreement and rank documents by decreasing probability of disagreement. We compare the predictive power of these document-level features with that of a meta-search feature that aggregates a document's ranking across multiple retrieval runs. Our features are shown to be on a par with the meta-search feature, without requiring a large and diverse set of retrieval runs to calculate. Surprisingly, however, we find that the reading level features are negatively correlated with disagreement, suggesting that they are detecting some other aspect of document content.

#index 1988922
#* Neighbourhood preserving quantisation for LSH
#@ Sean Moran;Victor Lavrenko;Miles Osborne
#t 2013
#c 13
#% 249321
#% 347225
#% 424085
#% 1022281
#% 1750268
#% 1878997
#% 1931623
#! We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Existing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbourhood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each dimension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applicable to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hyperplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art.

#index 1988923
#* A comparison of the optimality of statistical significance tests for information retrieval evaluation
#@ Julián Urbano;Mónica Marrero;Diego Martín
#t 2013
#c 13
#% 262102
#% 818222
#% 879630
#% 987311
#% 1019124
#% 1227657
#% 1227745
#! Previous research has suggested the permutation test as the theoretically optimal statistical significance test for IR evaluation, and advocated for the discontinuation of the Wilcoxon and sign tests. We present a large-scale study comprising nearly 60 million system comparisons showing that in practice the bootstrap, t-test and Wilcoxon test outperform the permutation test under different optimality criteria. We also show that actual error rates seem to be lower than the theoretically expected 5%, further confirming that we may actually be underestimating significance.

#index 1988924
#* Towards retrieving relevant information graphics
#@ Zhuo Li;Matthew Stagitis;Sandra Carberry;Kathleen F. McCoy
#t 2013
#c 13
#% 642989
#% 1022764
#% 1040539
#% 1492470
#% 1526849
#% 1648838
#% 1674686
#% 1697415
#! Information retrieval research has made significant progress in the retrieval of text documents and images. However, relatively little attention has been given to the retrieval of information graphics (non-pictorial images such as bar charts and line graphs) despite their proliferation in popular media such as newspapers and magazines. Our goal is to build a system for retrieving bar charts and line graphs that reasons about the content of the graphic itself in deciding its relevance to the user query. This paper presents the first steps toward such a system, with a focus on identifying the category of intended message of potentially relevant bar charts and line graphs. Our learned model achieves accuracy higher than 80\% on a corpus of collected user queries.

#index 1988925
#* A test collection for entity search in DBpedia
#@ Krisztian Balog;Robert Neumayer
#t 2013
#c 13
#% 642992
#% 1100822
#% 1166534
#% 1195848
#% 1292565
#% 1343447
#% 1400010
#% 1475756
#% 1482286
#% 1489451
#% 1641483
#% 1805999
#% 1806002
#% 1806049
#% 1939477
#% 1967767
#! We develop and make publicly available an entity search test collection based on the DBpedia knowledge base. This includes a large number of queries and corresponding relevance judgments from previous benchmarking campaigns, covering a broad range of information needs, ranging from short keyword queries to natural language questions. Further, we present baseline results for this collection with a set of retrieval models based on language modeling and BM25. Finally, we perform an initial analysis to shed light on certain characteristics that make this data set particularly challenging.

#index 1988926
#* Improving LDA topic models for microblogs via tweet pooling and automatic labeling
#@ Rishabh Mehrotra;Scott Sanner;Wray Buntine;Lexing Xie
#t 2013
#c 13
#% 406493
#% 722904
#% 1077150
#% 1355042
#% 1470574
#% 1561559
#% 1563712
#% 1587367
#% 1641934
#% 1746824
#% 1913608
#% 1932851
#! Twitter, or the world of 140 characters poses serious challenges to the efficacy of topic models on short, messy text. While topic models such as Latent Dirichlet Allocation (LDA) have a long history of successful application to news articles and academic abstracts, they are often less coherent when applied to microblog content like Twitter. In this paper, we investigate methods to improve topics learned from Twitter content without modifying the basic machinery of LDA; we achieve this through various pooling schemes that aggregate tweets in a data preprocessing step for LDA. We empirically establish that a novel method of tweet pooling by hashtags leads to a vast improvement in a variety of measures for topic coherence across three diverse Twitter datasets in comparison to an unmodified LDA baseline and a variety of pooling schemes. An additional contribution of automatic hashtag labeling further improves on the hashtag pooling results for a subset of metrics. Overall, these two novel schemes lead to significantly improved LDA topic models on Twitter content.

#index 1988927
#* Exploiting semantics for improving clinical information retrieval
#@ Atanaz Babashzadeh;Jimmy Huang;Mariam Daoud
#t 2013
#c 13
#% 879706
#% 958367
#% 1227615
#% 1482195
#% 1543491
#% 1926720
#% 1967809
#! Clinical information retrieval (IR) presents several challenges including terminology mismatch and granularity mismatch. One of the main objectives in clinical IR is to fill the semantic gap among the queries and documents and go beyond keywords matching. To address these issues, in this paper we attempt to use semantic information to improve the performance of clinical IR systems by representing queries in an expressive and meaningful context. To model a query context initially we model and develop query domain ontology. The query domain ontology represents concepts closely related with query concepts. Query context represents concepts extracted from query domain ontology and weighted according to their semantic relatedness to query concept(s). The query context is then exploited in query expansion and patients records re-ranking for improving clinical retrieval performance. We evaluate our approach on the TREC Medical Records dataset. Results show that our proposed approach significantly improves the retrieval performance compare to classic keyword-based IR model.

#index 1988928
#* Relevance dimensions in preference-based IR evaluation
#@ Jinyoung Kim;Gabriella Kazai;Imed Zitouni
#t 2013
#c 13
#% 311866
#% 720198
#% 867123
#% 907495
#% 987321
#% 1015625
#% 1415710
#% 1450898
#% 1450912
#% 1455256
#% 1879078
#! Evaluation of information retrieval (IR) systems has recently been exploring the use of preference judgments over two search result lists. Unlike the traditional method of collecting relevance labels per single result, this method allows to consider the interaction between search results as part of the judging criteria. For example, one result list may be preferred over another if it has a more diverse set of relevant results, covering a wider range of user intents. In this paper, we investigate how assessors determine their preference for one list of results over another with the aim to understand the role of various relevance dimensions in preference-based evaluation. We run a series of experiments and collect preference judgments over different relevance dimensions in side-by-side comparisons of two search result lists, as well as relevance judgments for the individual documents. Our analysis of the collected judgments reveals that preference judgments combine multiple dimensions of relevance that go beyond the traditional notion of relevance centered on topicality. Measuring performance based on single document judgments and NDCG aligns well with topicality based preferences, but shows misalignment with judges' overall preferences, largely due to the diversity dimension. As a judging method, dimensional preference judging is found to lead to improved judgment quality.

#index 1988929
#* Temporal variance of intents in multi-faceted event-driven information needs
#@ Stewart Whiting;Ke Zhou;Joemon Jose;Mounia Lalmas
#t 2013
#c 13
#% 262112
#% 642975
#% 956509
#% 1166473
#% 1190102
#% 1536521
#% 1920025
#% 1967827
#! Time is often important for understanding user intent during search activity, especially for information needs related to event-driven topics. Diversity for multi-faceted information needs ensures that ranked documents optimally cover multiple facets when a user's intent is uncertain. Effective diversity is reliant on methods to (i) discover and represent facets, and (ii) determine how likely each facet is the user's intent (i.e., its popularity). Past work has developed several techniques addressing these issues, however, they have concentrated on static approaches which do not consider the temporal nature of new and evolving intents and their popularity. In many cases, what a user expects may change dramatically over time as events develop. In this work we study the temporal variance of search intents for event-driven information needs using Wikipedia. First, we model intents based upon the structure represented by the section hierarchy of Wikipedia articles closely related to the information need. Using this technique, we investigate whether temporal changes in the content structure, i.e. in a section's text, reflect the temporal popularity of the intent. We map intents taken from a query-log (as ground-truth) to Wikipedia article sections and found that a large proportion are indeed reflected in topic-related article structure. By correlating the change activity of each section with the use of the intent query over time, we found that section change activity does reflect temporal popularity of many intents. Furthermore, we show that popularity between intents changes over time for event-driven topics.

#index 1988930
#* A document rating system for preference judgements
#@ Maryam Bashir;Jesse Anderton;Jie Wu;Peter B. Golbus;Virgil Pavlu;Javed A. Aslam
#t 2013
#c 13
#% 100008
#% 577224
#% 989628
#% 1452857
#% 1806006
#% 1879067
#% 1948142
#! High quality relevance judgments are essential for the evaluation of information retrieval systems. Traditional methods of collecting relevance judgments are based on collecting binary or graded nominal judgments, but such judgments are limited by factors such as inter-assessor disagreement and the arbitrariness of grades. Previous research has shown that it is easier for assessors to make pairwise preference judgments. However, unless the preferences collected are largely transitive, it is not clear how to combine them in order to obtain document relevance scores. Another difficulty is that the number of pairs that need to be assessed is quadratic in the number of documents. In this work, we consider the problem of inferring document relevance scores from pairwise preference judgments by analogy to tournaments using the Elo rating system. We show how to combine a linear number of pairwise preference judgments from multiple assessors to compute relevance scores for every document.

#index 1988931
#* Query change as relevance feedback in session search
#@ Sicong Zhang;Dongyi Guan;Hui Yang
#t 2013
#c 13
#% 465895
#% 750863
#% 1074081
#% 1400023
#% 1621236
#% 1693905
#! Session search is the Information Retrieval (IR) task that performs document retrieval for an entire session. During a session, users often change queries to explore and investigate the information needs. In this paper, we propose to use query change as a new form of relevance feedback for better session search. Evaluation conducted over TREC 2012 Session Track shows that query change is a highly effective form of feedback as compared with existing relevance feedback methods. The proposed method outperforms the state-of-the-art relevance feedback methods for the TREC 2012 Session Track by a significant improvement of 25%.

#index 1988932
#* Ranking explanatory sentences for opinion summarization
#@ Hyun Duk Kim;Malu G. Castellanos;Meichun Hsu;ChengXiang Zhai;Umeshwar Dayal;Riddhiman Ghosh
#t 2013
#c 13
#% 71752
#% 194251
#% 324129
#% 769892
#% 1035591
#% 1250237
#% 1250423
#% 1272053
#! We introduce a novel sentence ranking problem called explanatory sentence extraction (ESE) which aims to rank sentences in opinionated text based on their usefulness for helping users understand the detailed reasons of sentiments (i.e., "explanatoriness"). We propose and study several general methods for scoring the explanatoriness of a sentence. We create new data sets and propose a new measure for evaluation. Experiment results show that the proposed methods are effective, outperforming a state of the art sentence ranking method for standard text summarization.

#index 1988933
#* The knowing camera: recognizing places-of-interest in smartphone photos
#@ Pai Peng;Lidan Shou;Ke Chen;Gang Chen;Sai Wu
#t 2013
#c 13
#% 1148473
#% 1484413
#! This paper presents a framework called Knowing Camera for real-time recognizing places-of-interest in smartphone photos, with the availability of online geotagged images of such places. We propose a probabilistic field-of-view model which captures the uncertainty in camera sensor data. This model can be used to retrieve a set of candidate images. The visual similarity computation of the candidate images relies on the sparse coding technique. We also propose an ANN filtering technique to speedup the sparse coding. The final ranking combines an uncertain geometric relevance with the visual similarity. Our preliminary experiments conducted in an urban area of a large city show promising results. The most distinguishing feature of our framework is its ability to perform well in contaminated, real-world online image database. Besides, our framework is highly scalable as it does not incur any complex data structure.

#index 1988934
#* Boosting novelty for biomedical information retrieval through probabilistic latent semantic analysis
#@ Xiangdong An;Jimmy Xiangji Huang
#t 2013
#c 13
#% 262112
#% 397133
#% 642975
#% 722904
#% 818266
#% 879706
#% 1074133
#% 1074186
#% 1227615
#% 1417055
#% 1650298
#% 2000587
#! In information retrieval, we are interested in the information that is not only relevant but also novel. In this paper, we study how to boost novelty for biomedical information retrieval through probabilistic latent semantic analysis. We conduct the study based on TREC Genomics Track data. In TREC Genomics Track, each topic is considered to have an arbitrary number of aspects, and the novelty of a piece of information retrieved, called a passage, is assessed based on the amount of new aspects it contains. In particular, the aspect performance of a ranked list is rewarded by the number of new aspects reached at each rank and penalized by the amount of irrelevant passages that are rated higher than the novel ones. Therefore, to improve aspect performance, we should reach as many aspects as possible and as early as possible. In this paper, we make a preliminary study on how probabilistic latent semantic analysis can help capture different aspects of a ranked list, and improve its performance by re-ranking. Experiments indicate that the proposed approach can greatly improve the aspect-level performance over baseline algorithm Okapi BM25.

#index 1988935
#* An adaptive evidence weighting method for medical record search
#@ Dongqing Zhu;Ben Carterette
#t 2013
#c 13
#% 818262
#% 879584
#% 1842540
#% 1879144
#% 1912656
#% 1967758
#% 1967809
#! In this paper, we present a medical record search system which is useful for identifying cohorts required in clinical studies. In particular, we propose a query-adaptive weighting method that can dynamically aggregate and score evidence in multiple medical reports (from different hospital departments or from different tests within the same department) of a patient. Furthermore, we explore several informative features for learning our retrieval model.

#index 1988936
#* Collaborative factorization for recommender systems
#@ Chaosheng Fan;Yanyan Lan;Jiafeng Guo;Zuoquan Lin;Xueqi Cheng
#t 2013
#c 13
#% 411762
#% 734594
#% 840846
#% 1074021
#% 1227602
#% 1287271
#% 1291600
#% 1417104
#% 1476486
#% 1693875
#! Recommender system has become an effective tool for information filtering, which usually provides the most useful items to users by a top-k ranking list. Traditional recommendation techniques such as Nearest Neighbors (NN) and Matrix Factorization (MF) have been widely used in real recommender systems. However, neither approaches can well accomplish recommendation task since that: (1) most NN methods leverage the neighbor's behaviors for prediction, which may suffer the severe data sparsity problem; (2) MF methods are less sensitive to sparsity, but neighbors' influences on latent factors are not fully explored, since the latent factors are often used independently. To overcome the above problems, we propose a new framework for recommender systems, called collaborative factorization. It expresses the user as the combination of his own factors and those of the neighbors', called collaborative latent factors, and a ranking loss is then utilized for optimization. The advantage of our approach is that it can both enjoy the merits of NN and MF methods. In this paper, we take the logistic loss in RankNet and the likelihood loss in ListMLE as examples, and the corresponding collaborative factorization methods are called CoF-Net and CoF-MLE. Our experimental results on three benchmark datasets show that they are more effective than several state-of-the-art recommendation methods.

#index 1988937
#* Learning to combine representations for medical records search
#@ Nut Limsopatham;Craig Macdonald;Iadh Ounis
#t 2013
#c 13
#% 217251
#% 397161
#% 907525
#% 944349
#% 1415713
#% 1467729
#% 1482195
#% 1560393
#% 1598342
#% 1598507
#% 1919731
#% 1967809
#% 1994933
#! The complexity of medical terminology raises challenges when searching medical records. For example, 'cancer', 'tumour', and 'neoplasms', which are synonyms, may prevent a traditional search system from retrieving relevant records that contain only synonyms of the query terms. Prior works use bag-of-concepts approaches, to deal with this by representing medical terms sharing the same meanings using concepts from medical resources (e.g. MeSH). The relevance scores are then combined with a traditional bag-of-words representation, when inferring the relevance of medical records. Even though the existing approaches are effective, the predicted retrieval effectiveness of either the bag-of-words or bag-of-concepts representation, which may be used to effectively model the score combination and hence improve retrieval performance, is not taken into account. In this paper, we propose a novel learning framework that models the importance of the bag-of-words and the bag-of-concepts representations, combining their scores on a per-query basis. Our proposed framework leverages retrieval performance predictors, such as the clarity score and AvIDF, calculated on both representations as learning features. We evaluate our proposed framework using the TREC Medical Records track's test collections. As our proposed framework can significantly outperform an existing approach that linearly merges the relevance scores, we conclude that retrieval performance predictors can be effectively leveraged when combining the relevance scores.

#index 1988938
#* Characterizing stages of a multi-session complex search task through direct and indirect query modifications
#@ Jiyin He;Marc Bron;Arjen P. de Vries
#t 2013
#c 13
#% 581916
#% 879622
#% 1126944
#% 1185582
#% 1213448
#% 1224723
#% 1227623
#% 1227624
#% 1384094
#% 1482230
#% 1563613
#% 1879036
#% 1972422
#! Search systems use context to effectively satisfy a user's information need as expressed by a query. Tasks are important factors in determining user context during search and many studies have been conducted that identify tasks and task stages through users' interaction behavior with search systems. The type of interaction available to users, however, depends on the type of search interface features available. Queries are the most pervasive input from users to express their information need regardless of the input method, e.g., typing keywords or clicking facets. Instead of characterizing interaction behavior in terms of interface specific components, we propose to characterize users' search behavior in terms of two types of query modification: (i) direct modification, which refers to reformulations of queries; and (ii) indirect modification, which refers to user operations on additional input components provided by various search interfaces. We investigate the utility of characterizing task stages through direct and indirect query reformulations in a case study and find that it is possible to effectively differentiate subsequent stages of the search task. We found that describing user interaction behavior in such a generic form allowed us to relate user actions to search task stages independent from the specific search interface deployed. The next step will then be to validate this idea in a setting with a wider palette of search tasks and tools.

#index 1988939
#* Informational friend recommendation in social media
#@ Shengxian Wan;Yanyan Lan;Jiafeng Guo;Chaosheng Fan;Xueqi Cheng
#t 2013
#c 13
#% 730089
#% 1074133
#% 1083671
#% 1183090
#% 1476470
#% 1688496
#% 1879058
#! It is well recognized that users rely on social media (e.g. Twitter or Digg) to fulfill two common needs (i.e. social need and informational need) that is to keep in touch with their friends in the real world and to have access to information they are interested in. Traditional friend recommendation methods in social media mainly focus on a user's social need, but seldom address their informational need (i.e. suggesting friends that can provide information one may be interested in but have not been able to obtain so far). In this paper, we propose to recommend friends according to the informational utility, which stands for the degree to which a friend satisfies the target user's unfulfilled informational need, called informational friend recommendation. In order to capture users' informational need, we view a post in social media as an item and utilize collaborative filtering techniques to predict the rating for each post. The candidate friends are then ranked according to their informational utility for recommendation. In addition, we also show how to further consider diversity in such recommendations. Experiments on benchmark datasets demonstrate that our approach can significantly outperform the traditional friend recommendation methods under informational evaluation measures.

#index 1988940
#* A weakly-supervised detection of entity central documents in a stream
#@ Ludovic Bonnefoy;Vincent Bouvier;Patrice Bellot
#t 2013
#c 13
#% 262085
#% 340941
#% 1711798
#% 1741395
#% 1826368
#% 1913329
#! Filtering a time-ordered corpus for documents that are highly relevant to an entity is a task receiving more and more attention over the years. One application is to reduce the delay between the moment an information about an entity is being first observed and the moment the entity entry in a knowledge base is being updated. Current state-of-the-art approaches are highly supervised and require training examples for each entity monitored. We propose an approach which does not require new training data when processing a new entity. To capture intrinsic characteristics of highly relevant documents our approach relies on three types of features: document centric features, entity profile related features and time features. Evaluated within the framework of the "Knowledge Base Acceleration" track at TREC 2012, it outperforms current state-of-the-art approaches.

#index 1988941
#* From keywords to keyqueries: content descriptors for the web
#@ Tim Gollub;Matthias Hagen;Maximilian Michel;Benno Stein
#t 2013
#c 13
#% 481290
#% 793417
#% 1023380
#% 1083629
#% 1130863
#% 1472100
#% 1482283
#% 1495123
#% 1631335
#% 1763374
#% 1919709
#! We introduce the concept of keyqueries as dynamic content descriptors for documents. Keyqueries are defined implicitly by the index and the retrieval model of a reference search engine: keyqueries for a document are the minimal queries that return the document in the top result ranks. Besides applications in the fields of information retrieval and data mining, keyqueries have the potential to form the basis of a dynamic classification system for future digital libraries---the modern version of keywords for content description. To determine the keyqueries for a document, we present an exhaustive search algorithm along with effective pruning strategies. For applications where a small number of diverse keyqueries is sufficient, two tailored search strategies are proposed. Our experiments emphasize the role of the reference search engine and show the potential of keyqueries as innovative document descriptors for large, fast evolving bodies of digital content such as the web.

#index 1988942
#* A financial cost metric for result caching
#@ Fethi Burak Sazoglu;B. Barla Cambazoglu;Rifat Ozcan;Ismail Sengor Altingovde;Özgür Ulusoy
#t 2013
#c 13
#% 309513
#% 860861
#% 878624
#% 978378
#% 987215
#% 1190098
#% 1246360
#% 1290542
#% 1464869
#% 1558842
#% 1587384
#% 1598432
#% 1834787
#! Web search engines cache results of frequent and/or recent queries. Result caching strategies can be evaluated using different metrics, hit rate being the most well-known. Recent works take the processing overhead of queries into account when evaluating the performance of result caching strategies and propose cost-aware caching strategies. In this paper, we propose a financial cost metric that goes one step beyond and takes also the hourly electricity prices into account when computing the cost. We evaluate the most well-known static, dynamic, and hybrid result caching strategies under this new metric. Moreover, we propose a financial-cost-aware version of the well-known LRU strategy and show that it outperforms the original LRU strategy in terms of the financial cost metric.

#index 1988943
#* Competition-based networks for expert finding
#@ Çiğdem Aslay;Neil O'Hare;Luca Maria Aiello;Alejandro Jaimes
#t 2013
#c 13
#% 879570
#% 956516
#% 1019165
#% 1055738
#% 1083720
#% 1155652
#% 1183152
#% 1560417
#% 1587391
#% 1598376
#% 1747118
#% 1879070
#! Finding experts in question answering platforms has important applications, such as question routing or identification of best answers. Addressing the problem of ranking users with respect to their expertise, we propose Competition-Based Expertise Networks (CBEN), a novel community expertise network structure based on the principle of competition among the answerers of a question. We evaluate our approach on a very large dataset from Yahoo! Answers using a variety of centrality measures. We show that it outperforms state-of-the-art network structures and, unlike previous methods, is able to consistly outperform simple metrics like best answer count. We also analyse question answering forums in Yahoo! Answers, and show that they can be characterised by factual or subjective information seeking behavior, social discussions and the conducting of polls or surveys. We find that the ability to identify experts greatly depends on the type of forum, which is directly reflected in the structural properties of the expertise networks.

#index 1988944
#* #trapped!: social media search system requirements for emergency management professionals
#@ Stefan Raue;Leif Azzopardi;Chris W. Johnson
#t 2013
#c 13
#% 665656
#% 954983
#% 1245937
#% 1429836
#% 1806058
#% 1846488
#! Social media provides a new and potentially rich source of information for emergency management services. However, extracting the relevant information from such streams poses a number of difficult challenges. In this short paper, we survey emergency management professionals to ascertain how social media is used when responding to incidents, the search strategies that they undertake, and the challenges that they face when using social media streams. This research indicates that emergency management professionals employ two main strategies when searching social media streams: keyword-centric and account-centric search strategies. Furthermore, current search interfaces are inadequate regarding the requirements of command and control environments in the emergency management domain, where the process of information seeking is collaborative in nature and needs to support multiple information seekers.

#index 1988945
#* Studying page life patterns in dynamical web
#@ Alexey Tikhonov;Ivan Bogatyy;Pavel Burangulov;Liudmila Ostroumova;Vitaliy Koshelev;Gleb Gusev
#t 2013
#c 13
#% 349550
#% 448194
#% 1536522
#% 1560174
#% 1641922
#% 1693930
#% 1746823
#% 1746858
#% 1978756
#! With the ever-increasing speed of content turnover on the web, it is particularly important to understand the patterns that pages' popularity follows. This paper focuses on the dynamical part of the web, i.e. pages that have a limited lifespan and experience a short popularity outburst within it. We classify these pages into five patterns based on how quickly they gain popularity and how quickly they lose it. We study the properties of pages that belong to each pattern and determine content topics that contain disproportionately high fractions of particular patterns. These developments are utilized to create an algorithm that approximates with reasonable accuracy the expected popularity pattern of a web page based on its URL and, if available, prior knowledge about its domain's topics.

#index 1988946
#* Fresh BrowseRank
#@ Maxim Zhukovskiy;Andrei Khropov;Gleb Gusev;Pavel Serdyukov
#t 2013
#c 13
#% 769488
#% 807658
#% 844298
#% 869600
#% 1016177
#% 1019188
#% 1074107
#% 1214725
#% 1292715
#% 1366525
#% 1450843
#% 1621235
#% 1920005
#! In the last years, a lot of attention was attracted by the problem of page authority computation based on user browsing behavior. However, the proposed methods have a number of limitations. In particular, they run on a single snapshot of a user browsing graph ignoring substantially dynamic nature of user browsing activity, which makes such methods recency unaware. This paper proposes a new method for computing page importance, referred to as Fresh BrowseRank. The score of a page by our algorithm equals to the weight in a stationary distribution of a flexible random walk, which is controlled by recency-sensitive weights of vertices and edges. Our method generalizes some previous approaches, provides better capability for capturing the dynamics of the Web and users behavior, and overcomes essential limitations of BrowseRank. The experimental results demonstrate that our method enables to achieve more relevant and fresh ranking results than the classic BrowseRank.

#index 1988947
#* Diversity and novelty in information retrieval
#@ Rodrygo L.T. Santos;Pablo Castells;Ismail Sengor Altingovde;Fazli Can
#t 2013
#c 13
#! This tutorial aims to provide a unifying account of current research on diversity and novelty in different IR domains, namely, in the context of search engines, recommender systems, and data streams.

#index 1988948
#* Entity linking and retrieval
#@ Edgar Meij;Krisztian Balog;Daan Odijk
#t 2013
#c 13
#! This full-day tutorial presents a comprehensive introduction to entity linking and retrieval. Part I provides a detailed overview of entity linking: identifying and disambiguating entity occurrences in unstructured text. Part II focuses on entity retrieval, by first considering scenarios where explicit representations of entities are available, and then moving to a setting where evidence needs to be collected and aggregated from multiple documents or even collections, thereby combining techniques from both entity linking and entity retrieval. Part III concludes the tutorial with an overview and hands-on comparative analysis of applications and publicly available toolkits and web services.

#index 1988949
#* Scalability and efficiency challenges in commercial web search engines
#@ B. Barla Cambazoglu;Ricardo Baeza-Yates
#t 2013
#c 13
#% 730066
#% 1166469
#! Commercial web search engines rely on very large compute infrastructures to be able to cope with the continuous growth of the Web and user bases. Achieving scalability and efficiency in such large-scale search engines requires making careful architectural design choices while devising algorithmic performance optimizations. Unfortunately, most details about the internal functioning of commercial web search engines remain undisclosed due to their financial value and the high level of competition in the search market. The main objective of this tutorial is to provide an overview of the fundamental scalability and efficiency challenges in commercial web search engines, bridging the existing gap between the industry and academia.

#index 1988950
#* Building test collections: an interactive tutorial for students and others without their own evaluation conference series
#@ Ian M. Soboroff
#t 2013
#c 13
#! While existing test collections and evaluation conference efforts may sufficiently support one's research, one can easily find oneself wanting to solve problems no one else is solving yet. But how can research in IR be done (or be published!) without solid data and experiments? Not everyone can talk TREC, CLEF, INEX, or NTCIR into running a track to build a collection. This tutorial aims to teach how to build a test collection using resources at hand, how to measure the quality of that collection, how to understand its limitations, and how to communicate them. The intended audience is advanced students who find themselves in need of a test collection, or actually in the process of building a test collection, to support their own research. The goal of this tutorial is to lay out issues, procedures, pitfalls, and practical advice.

#index 1988951
#* Designing search usability
#@ Tony Russell-Rose
#t 2013
#c 13
#% 2003655
#! Search is not just a box and ten blue links. Search is a journey: an exploration where what we encounter along the way changes what we seek. But in order to guide people along this journey, we must understand both the art and science of search experience design. The aim of this tutorial is to deliver a course grounded in good scholarship, integrating the latest research findings with insights derived from the practical experience of designing and optimizing an extensive range of commercial search applications. It focuses on the development of transferable, practical skills that can be learnt and practiced within a half-day session.

#index 1988952
#* The cluster hypothesis in information retrieval
#@ Oren Kurland
#t 2013
#c 13
#% 375017

#index 1988953
#* Music similarity and retrieval
#@ Peter Knees;Markus Schedl
#t 2013
#c 13
#% 1987674
#! This tutorial serves as an introductory course to the field of and state-of-the-art in music information retrieval (MIR) and in particular to music similarity estimation which is an essential component of music retrieval. Apart from explaining approaches that estimate similarity based on acoustic properties of an audio signal, we review methods that exploit (mostly textual) meta-data from the Web to build representations of music then used for similarity calculation. Additionally, topics such as (large-scale) music indexing, information extraction for music, personalization in music retrieval, and evaluation of MIR systems are addressed.

#index 1988954
#* Multimedia recommendation: technology and techniques
#@ Jialie Shen;Meng Wang;Shuicheng Yan;Peng Cui
#t 2013
#c 13
#! In recent years, we have witnessed a rapid growth in the availability of digital multimedia on various application platforms and domains. Consequently, the problem of information overload has become more and more serious. In order to tackle the challenge, various multimedia recommendation technologies have been developed by different research communities (e.g., multimedia systems, information retrieval, machine learning and computer version). Meanwhile, many commercial web systems (e.g., Flick, YouTube, and Last.fm) have successfully applied recommendation techniques to provide users personalized content and services in a convenient and flexible way. When looking back, the information retrieval (IR) community has a long history of studying and contributing recommender system design and related issues. It has been proven that the recommender systems can effectively assist users in handling information overload and provide high-quality personalization. While several courses were dedicated to multimedia retrieval in the recent decade, to the best of our knowledge, the tutorial is the first one specifically focusing on multimedia recommender systems and their applications on various domains and media contents. We plan to summarize the research along this direction and provide an impetus for further research on this important topic

#index 1988955
#* Searching in the city of knowledge: challenges and recent developments
#@ Veli Bicer;Vanessa Lopez
#t 2013
#c 13
#% 1642112
#% 1942772
#% 1986877
#! Today plenty of data is emerging from various city systems. Beyond the classical Web resources, large amounts of data are retrieved from sensors, devices, social networks, governmental applications, or service networks. In such a diversity of information, answering specific information needs of city inhabitants requires holistic IR techniques, capable of harnessing different types of city data and turned it into actionable insights to answer different queries. This tutorial will present deep insights, challenges, opportunities and techniques to make heterogeneous city data searchable and show how emerging IR techniques models can be employed to retrieve relevant information for the citizens.

#index 1988956
#* Kernel-based learning to rank with syntactic and semantic structures
#@ Alessandro Moschitti
#t 2013
#c 13
#% 743284
#% 1130832
#% 1611797
#% 1665151
#% 1879066
#! Kernel Methods (KMs) are powerful machine learning techniques that can alleviate the data representation problem as they substitute scalar product between feature vectors with similarity functions (kernels) directly defined between data instances, e.g., syntactic trees, (thus features are not needed any longer). This tutorial aims at introducing essential and simplified theory of Support Vector Machines and KMs for the design of practical applications. It will describe effective kernels for easily engineering automatic classifiers and learning to rank algorithms using structured data and semantic processing. Some examples will be drawn from Question Answering, Passage Re-ranking, Short and Long Text Categorization, Relation Extraction, Named Entity Recognition, Co-Reference Resolution. Moreover, some practical demonstrations will be given using the SVM-Light-TK (tree kernel) toolkit.

#index 1988957
#* Accurate and robust text detection: a step-in for text retrieval in natural scene images
#@ Xu-Cheng Yin;Xuwang Yin;Kaizhu Huang;Hong-Wei Hao
#t 2013
#c 13
#% 1562762
#% 1645218
#% 1856872
#% 1884391
#% 1921634
#! We propose and implement a robust text detection system, which is a prominent step-in for text retrieval in natural scene images or videos. Our system includes several key components: (1) A fast and effective pruning algorithm is designed to extract Maximally Stable Extremal Regions as character candidates using the strategy of minimizing regularized variations. (2) Character candidates are grouped into text candidates by the single-link clustering algorithm, where distance weights and threshold of clustering are learned automatically by a novel self-training distance metric learning algorithm. (3) The posterior probabilities of text candidates corresponding to non-text are estimated with an character classifier; text candidates with high probabilities are then eliminated and finally texts are identified with a text classifier. The proposed system is evaluated on the ICDAR 2011 Robust Reading Competition dataset and a publicly available multilingual dataset; the f measures are over 76% and 74% which are significantly better than the state-of-the-art performances of 71% and 65%, respectively.

#index 1988958
#* SearchResultFinder: federated search made easy
#@ Dolf Trieschnigg;Kien Tjin-Kam-Jet;Djoerd Hiemstra
#t 2013
#c 13
#% 275917
#% 465919
#% 729978
#% 805845
#% 838491
#% 1023488
#% 1292470
#! Building a federated search engine based on a large number existing web search engines is a challenge: implementing the programming interface (API) for each search engine is an exacting and time-consuming job. In this demonstration we present SearchResultFinder, a browser plugin which speeds up determining reusable XPaths for extracting search result items from HTML search result pages. Based on a single search result page, the tool presents a ranked list of candidate extraction XPaths and allows highlighting to view the extraction result. An evaluation with 148 web search engines shows that in 90% of the cases a correct XPath is suggested.

#index 1988959
#* BATC: a benchmark for aggregation techniques in crowdsourcing
#@ Quoc Viet Hung Nguyen;Thanh Tam Nguyen;Ngoc Tran Lam;Karl Aberer
#t 2013
#c 13
#% 1211801
#% 1400083
#% 1452857
#% 1526538
#! As the volumes of AI problems involving human knowledge are likely to soar, crowdsourcing has become essential in a wide range of world-wide-web applications. One of the biggest challenges of crowdsourcing is aggregating the answers collected from crowd workers; and thus, many aggregate techniques have been proposed. However, given a new application, it is difficult for users to choose the best-suited technique as well as appropriate parameter values since each of these techniques has distinct performance characteristics depending on various factors (e.g. worker expertise, question difficulty). In this paper, we develop a benchmarking tool that allows to (i) simulate the crowd and (ii) evaluate aggregate techniques in different aspects (accuracy, sensitivity to spammers, etc.). We believe that this tool will be able to serve as a practical guideline for both researchers and software developers. While researchers can use our tool to assess existing or new techniques, developers can reuse its components to reduce the development complexity.

#index 1988960
#* X-ENS: semantic enrichment of web search results at real-time
#@ Pavlos Fafalios;Yannis Tzitzikas
#t 2013
#c 13
#% 1300601
#% 1409954
#% 1653178
#% 1702431
#! While more and more semantic data are published on the Web, an important question is how typical web users can access and exploit this body of knowledge. Although, existing interaction paradigms in semantic search hide the complexity behind an easy-to-use interface, they have not managed to cover common search needs. In this paper, we present X-ENS (eXplore ENtities in Search), a web search application that enhances the classical, keyword-based, web searching with semantic information, as a means to combine the pros of both Semantic Web standards and common Web Searching. X-ENS identifies entities of interest in the snippets of the top search results which can be further exploited in a faceted interaction scheme, and thereby can help the user to limit the - often very large - search space to those hits that contain a particular piece of information. Moreover, X-ENS permits the exploration of the identified entities by exploiting semantic repositories.

#index 1988961
#* A multilingual and multiplatform application for medicinal plants prescription from medical symptoms
#@ Fernando Ruiz-Rico;David Tomás;Jose-Luis Vicedo;María-Consuelo Rubio-Sánchez
#t 2013
#c 13
#% 894519
#% 1215368
#% 1251776
#% 1541397
#% 1630432
#! This paper presents an application for medicinal plants prescription based on text classification techniques. The system receives as an input a free text describing the symptoms of a user, and retrieves a ranked list of medicinal plants related to those symptoms. In addition, a set of links to Wikipedia are also provided, enriching the information about every medicinal plant presented to the user. In order to improve the accessibility to the application, the input can be written in six different languages, adapting the results accordingly. The application interface can be accessed from different devices and platforms.

#index 1988962
#* TopicVis: a GUI for topic-based feedback and navigation
#@ Debasis Ganguly;Manisha Ganguly;Johannes Leveling;Gareth J.F. Jones
#t 2013
#c 13
#% 722904
#! This paper describes a search system which includes topic model visualization to improve the user search experience. The system graphically renders the topics in a retrieved set of documents, enables a user to selectively refine search results and allows easy navigation through information on selective topics within documents.

#index 1988963
#* Spacious: an interactive mental search interface
#@ Phong D. Vo;Hichem Sahbi
#t 2013
#c 13
#% 1095763
#! We introduce in this work a novel approach for semantic indexing and mental image search. Given semantic concepts defined by few training examples, our formulation is transductive and learns a mapping from an initial ambient space, related to low level visual features, to an output space spanned by a well defined semantic basis where data can be easily explored. With this method, searching for a mental visual target reduces to scanning data according to their coordinates in the learned semantic space. We illustrate the proposed method through our graphical user interface "Spacious", for the purpose of visualization and interactive navigation in generic image databases and satellite images.

#index 1988964
#* Online matching of web content to closed captions in IntoNow
#@ Carlos Castillo;Gianmarco De Francisci Morales;Ajay Shekhawat
#t 2013
#c 13
#% 797691
#% 1484390
#! IntoNow is a mobile application that provides a second-screen experience to television viewers. IntoNow uses the microphone of the companion device to sample the audio coming from the TV set, and compares it against a database of TV shows in order to identify the program being watched. The system we demonstrate is activated by IntoNow for specific types of shows. It retrieves information related to the program the user is watching by using closed captions, which are provided by each broadcasting network along the TV signal. It then matches the stream of closed captions in real-time against multiple sources of content. More specifically, during news programs it displays links to online news articles and the profiles of people and organizations in the news, and during music shows it displays links to songs. The matching models are machine-learned from editorial judgments, and tuned to achieve approximately 90% precision.

#index 1988965
#* ProductSeeker: entity-based product retrieval for e-commerce
#@ Hongzhi Wang;Xiaodong Zhang;Jianzhong Li;Hong Gao
#t 2013
#c 13
#% 1798424
#% 1895104
#! The retrieval results of online products information in e-commerce web sites are often difficult for users to use because of different descriptions for the same product. This paper proposes ProductSeeker, a product retrieval system organizing results according to their referring real-world entities for the conveniences of users. In the demonstration, we will present our system providing friendly interface to retrieve fresh product information and refining results according to feedback.

#index 1988966
#* YaLi: a crowdsourcing plug-in for NERD
#@ Yafang Wang;Lili Jiang;Johannes Hoffart;Gerhard Weikum
#t 2013
#c 13
#% 939376
#% 1432722
#% 1581851
#% 1711796
#% 1880463
#! We demonstrate the YaLi browser plug-in which discovers named entities in Web pages and provides background knowledge about them. The plug-in is implemented with two purposes. From a user perspective, it enriches the browsing experience with entities, helping users with their information needs. From the research perspective, we aim to improve the methods that are used for named entity recognition and disambiguation (NERD) by leveraging the plug-in as an implicit crowdsourcing platform. YaLi tracks the system's errors and the users' corrections, and also gathers implicit training data for improving NERD accuracy.

#index 1988967
#* A framework for specific term recommendation systems
#@ Thomas Lüke;Philipp Schaer;Philipp Mayr
#t 2013
#c 13
#% 27049
#% 943042
#% 1598413
#% 1624250
#% 1924163
#! In this paper we present the IRSA framework that enables the automatic creation of search term suggestion or recommendation systems (TS). Such TS are used to operationalize interactive query expansion and help users in refining their information need in the query formulation phase. Our recent research has shown TS to be more effective when specific to a certain domain. The presented technical framework allows owners of Digital Libraries to create their own specific TS constructed via OAI-harvested metadata with very little effort.

#index 1988968
#* Match the news: a firefox extension for real-time news recommendation
#@ Margarita Karkali;Dimitris Pontikis;Michalis Vazirgiannis
#t 2013
#c 13
#% 1730994
#! We present Match the News, a browser extension for real time news recommendation. Our extension works on the client side to recommend in real time recently published articles that are relevant to the web page the user is currently visiting. Match the News is fed from Google News RSS and applies syntactic matching to find the relevant articles. We implement an innovative weighting function to perform the keyword extraction task, BM25H. With BM25H we extract keywords not only relevant to currently browsed web page, but also novel with respect to the user's recent browsing history. The novelty feature in keyword extraction task results in meaningful news recommendations with regards to the web page the users currently visits. Moreover the extension offers a salient visualization of the terms corresponding to the users recent browsing history making thus the extension a comprehensive tool for real time news recommendation and self assessment.

#index 1988969
#* Answering natural language queries over linked data graphs: a distributional semantics approach
#@ André Freitas;Fabrício F. de Faria;Seán O'Riain;Edward Curry
#t 2013
#c 13
#% 1275012
#% 1667786
#% 1745991
#! This paper demonstrates Treo, a natural language query mechanism for Linked Data graphs. The approach uses a distributional semantic vector space model to semantically match user query terms with data, supporting vocabulary-independent (or schema-agnostic) queries over structured data.

#index 1988970
#* Information seeking in digital cultural heritage with PATHS
#@ Mark M. Hall;Paul D. Clough;Samuel Fernando;Paula Goodale;Mark Stevenson;Eneko Agirre;Arantxa Otegi;Aitor Soroa;Kate Fernie;Jillian Griffiths;Runar Bergheim
#t 2013
#c 13
#% 857478
#% 857482
#% 1369422
#% 1585880
#% 2003655
#! Current Information Retrieval systems for digital cultural heritage support only the actual search aspect of the information seeking process. This demonstration presents the second PATHS system which provides the exploration, analysis, and sense-making features to support the full information seeking process.

#index 1988971
#* Live nuggets extractor: a semi-automated system for text extraction and test collection creation
#@ Matthew Ekstrand-Abueg;Virgil Pavlu;Javed A. Aslam
#t 2013
#c 13
#% 1450896
#% 1693901
#% 1918349
#! The Live Nugget Extractor system provides users with a method of efficiently and accurately collecting relevant information for any web query rather than providing a simple ranked lists of documents. The system utilizes an online learning procedure to infer relevance of unjudged documents while extracting and ranking information from judged documents. This creates a set of judged and inferred relevance scores for both documents and text fragments, which can be used for test collections, summarization, and other tasks where high accuracy and large collections with minimal human effort are needed.

#index 1988972
#* TweetMogaz: a news portal of tweets
#@ Walid Magdy
#t 2013
#c 13
#% 1536506
#% 1587377
#% 1641934
#% 1879116
#% 1919955
#% 1920024
#! Twitter is currently one of the largest social hubs for users to spread and discuss news. For most of the top news stories happening, there are corresponding discussions on social media. In this demonstration TweetMogaz is presented, which is a platform for microblog search and filtering. It creates a real-time comprehensive report about what people discuss and share around news happening in certain regions. TweetMogaz reports the most popular tweets, jokes, videos, images, and news articles that people share about top news stories. Moreover, it allows users to search for specific topics. A scalable automatic technique for microblog filtering is used to obtain relevant tweets to a certain news category in a region. TweetMogaz.com demonstrates the effectiveness of our filtering technique for reporting public response toward news in different Arabic regions including Egypt and Syria in real-time.

#index 1988973
#* InfoLand: information lay-of-land for session search
#@ Jiyun Luo;Dongyi Guan;Hui Yang
#t 2013
#c 13
#% 280849
#% 1641980
#% 1693884
#! Search result clustering (SRC) is a post-retrieval process that hierarchically organizes search results. The hierarchical structure offers overview for the search results and displays an "information lay-of-land" that intents to guide the users throughout a search session. However, SRC hierarchies are sensitive to query changes, which are common among queries in the same session. This instability may leave users seemly random overviews throughout the session. We present a new tool called InfoLand that integrates external knowledge from Wikipedia when building SRC hierarchies and increase their stability. Evaluation on TREC 2010-2011 Session Tracks shows that InfoLand produces more stable results organization than a commercial search engine.

#index 1988974
#* Demonstration of citation pattern analysis for plagiarism detection
#@ Bela Gipp;Norman Meuschke;Corinna Breitinger;Mario Lipinski;Andreas Nürnberger
#t 2013
#c 13
#% 1588384
#% 1617833

#index 1988975
#* ThemeStreams: visualizing the stream of themes discussed in politics
#@ Ork de Rooij;Daan Odijk;Maarten de Rijke
#t 2013
#c 13
#% 270633
#% 1147597

#index 1988976
#* Flex-BaseX: an XML engine with a flexible extension of Xquery full-text
#@ Emanuele Panzeri;Gabriella Pasi
#t 2013
#c 13
#% 824681
#% 1022286
#% 1074219
#% 1266954
#% 1950939
#! XML is the most used language for structuring data and documents, besides being the de-facto standard for data exchange. Keyword based search has been implemented by the XQuery Full-Text language extension, allowing document fragments to be retrieved and ranked via keyword-based matching in the Information Retrieval style. In this demo the implementation of an XQuery extension allowing users to express their vague knowledge of the underlying XML structure is presented. The integration has been performed on top of the BaseX query engine; the work, as initially done by Panzeri at al. in IIR 2013 as a proof-of-concept has been further enhanced and extended.

#index 1988977
#* A portable multilingual medical directory by automatic categorization of Wikipedia articles
#@ Fernando Ruiz-Rico;María-Consuelo Rubio-Sánchez;David Tomás;Jose-Luis Vicedo
#t 2013
#c 13
#% 894519
#% 1156209
#% 1251776
#% 1541397
#% 1630432
#! Wikipedia has become one of the most important sources of information available all over the world. However, the categorization of Wikipedia articles is not standardized and the searches are mainly performed on keywords rather than concepts. In this paper we present an application that builds a hierarchical structure to organize all Wikipedia entries, so that medical articles can be reached from general to particular, using the well known Medical Subject Headings (MeSH) thesaurus. Moreover, the language links between articles will allow using the directory created in different languages. The final system can be packed and ported to mobile devices as a standalone offline application.

#index 1988978
#* Removing the mismatch headache in XML keyword search
#@ Yong Zeng;Zhifeng Bao;Tok Wang Ling;Guoliang Li
#t 2013
#c 13
#% 810052
#! In this demo, we study one category of query refinement problems in the context of XML keyword search, where what users search for do not exist in the data while useless results are returned by the search engine. It is a hidden but important problem. We refer to it as the MisMatch problem. We propose a practical yet efficient way to detect the MisMatch problem and generate helpful suggestions to users, namely MisMatch detector and suggester. Our approach can be viewed as a post-processing job of query evaluation. An online XML keyword search engine embedding the MisMatch detector and suggester has been built and is available at [1].

#index 1988979
#* A geolinguistic web application based on linked open data
#@ Emanuele Di Buccio;Giorgio Maria Di Nunzio;Gianmaria Silvello
#t 2013
#c 13
#% 1924138
#! Digital Geolinguistic systems encourage collaboration between linguists, historians, archaeologists, ethnographers, as they explore the relationship between language and cultural adaptation and change. In this demo, we propose a Linked Open Data approach for increasing the level of interoperability of geolinguistic applications and the reuse of the data. We present a case study of a geolinguistic project named Atlante Sintattico d'Italia, Syntactic Atlas of Italy (ASIt).

#index 1988980
#* Workshop on health search and discovery: helping users and advancing medicine
#@ Ryen W. White;Elad Yom-Tov;Eric Horvitz;Eugene Agichtein;William Hersh
#t 2013
#c 13
#% 838406
#% 1278069
#! This workshop brings together researchers and practitioners from industry and academia to discuss search and discovery in the medi-cal domain. The event focuses on ways to make medical and health information more accessible to laypeople (including enhancements to ranking algorithms and search interfaces), and how we can dis-cover new medical facts and phenomena from information sought online, as evidenced in query streams and other sources such as social media. This domain also offers many opportunities for appli-cations that monitor and improve quality of life of those affected by medical conditions, by providing tools to support their health-related information behavior.

#index 1988981
#* Internet advertising: theory and practice
#@ Bin Gao;Jun Yan;Dou Shen;Tie-Yan Liu
#t 2013
#c 13
#! Internet advertising, a form of advertising that utilizes the Internet to deliver marketing messages and attract customers, has seen exponential growth since its inception around twenty years ago; it has been pivotal to the success of the World Wide Web. The dramatic growth of internet advertising poses great challenges to information retrieval, machine learning, data mining and game theory, and it calls for novel technologies to be developed. The main purpose of this workshop is to bring together researchers and practitioners in the area of Internet Advertising and enable them to share their latest research results, to express their opinions, and to discuss future directions.

#index 1988982
#* SIGIR 2013 workshop on modeling user behavior for information retrieval evaluation
#@ Charles L.A. Clarke;Luanne Freund;Mark D. Smucker;Emine Yilmaz
#t 2013
#c 13
#% 1641985
#% 1879002
#! The SIGIR 2013 Workshop on Modeling User Behavior for Information Retrieval Evaluation (MUBE 2013) brings together people to discuss existing and new approaches, ways to collaborate, and other ideas and issues involved in improving information retrieval evaluation through the modeling of user behavior.

#index 1988983
#* EuroHCIR2013: the 3rd European workshop on human-computer interaction and information retrieval
#@ Max L. Wilson;Birger Larsen;Preben Hansen;Kristian Norling;Tony Russell-Rose
#t 2013
#c 13
#! A proposal summary for the EuroHCIR workshop at SIGIR2013.

#index 1988984
#* Workshop on benchmarking adaptive retrieval and recommender systems: BARS 2013
#@ Pablo Castells;Frank Hopfgartner;Alan Said;Mounia Lalmas
#t 2013
#c 13
#! Evaluating adaptive and personalized information retrieval tech-niques is known to be a difficult endeavor. The rapid evolution of novel technologies in this scope raises additional challenges that further stress the need for new evaluation approaches and method-ologies. The BARS 2013 workshop seeks to provide a specific venue for work on novel, personalization-centric benchmarking approaches to evaluate adaptive retrieval and recommender systems.

#index 1988985
#* Diversified relevance feedback
#@ Matt Crane
#t 2013
#c 13
#% 262112
#% 1641948
#% 1697422
#% 1926706
#! The need for a search engine to deal with ambiguous queries has been known for a long time (diversification). However, it is only recently that this need has become a focus within information retrieval research. How to respond to indications that a result is relevant to a query (relevance feedback) has also been a long focus of research. When thinking about the results for a query as being clustered by topic, these two areas of information retrieval research appear to be opposed to each other. Interestingly though, they both appear to improve the performance of search engines, raising the question: they can be combined or made to work with each other? When presented with an ambiguous query there are a number of techniques that can be employed to better select results. The primary technique being researched now is diversification, which aims to populate the results with a set of documents that cover different possible interpretations for the query, while maintaining a degree of relevance, as determined by the search engine. For example, given a query of "java" it is unclear whether the user, without any other information, means the programming language, the coffee, the island of Indonesia or a multitude of other meanings. In order to do this the assumption that documents are independent of each other when assessing potential relevance has to be broken. That is, a documents relevance, as calculated by the search engine, is no longer dependent only on the query, but also the other documents that have been selected. How a document is identified as being similar to previously selected documents, and the trade off between estimated relevance and topic coverage are current areas for information retrieval research. For unambiguous queries, or for search engines that do not perform diversification, it is possible to improve the results selected by reacting to information identifying a given result as truly relevant or not. This mechanism is known as relevance feedback. The most common response to relevance feedback is to investigate the documents for their most content-bearing terms, and either add, or subtract, their influence to a newly formed query which is then re-run on the remaining documents to re-order them. There has been a scant amount of research into the combination of these methods. However, Carbonell et al. [1] show that an initially diverse result set can provide a better approach for identifying the topic a user is interested in for a relevance feedback style approach. This approach was further extended by Raman et al. [4]. An important aspect of relevance feedback is the selection of documents to use. In the 2008 TREC relevance feedback track, Meij et al. [3] generated a diversified result set which outperformed other rankings as a source of feedback documents. The use of pseudo-relevance feedback (assuming the top ranked documents are relevant) to extract sub-topics for use in diversification was explored by Santos et al. [5]. These previous approaches suggest that these two ideas are more linked than expected. The ATIRE search engine [6] will be used to further explore the relationship between diversification and relevance feedback. ATIRE was selected because it is developed locally, and is designed to be small and fast. ATIRE also produces a competitive baseline, which would have placed 6th in the 2011 TREC diversity task while performing no diversification and index-time spam filtering [2], although we concede this is not equivalent to submitting a run.

#index 1988986
#* A query and patient understanding framework for medical records search
#@ Nut Limsopatham
#t 2013
#c 13
#% 1994933
#! Electronic medical records (EMRs) are being increasingly used worldwide to facilitate improved healthcare services [2,3]. They describe the clinical decision process relating to a patient, detailing the observed symptoms, the conducted diagnostic tests, the identified diagnoses and the prescribed treatments. However, medical records search is challenging, due to the implicit knowledge inherent within the medical records - such knowledge may be known by medical practitioners, but hidden to an information retrieval (IR) system [3]. For instance, the mention of a treatment such as a drug may indicate to a practitioner that a particular diagnosis has been made even if this was not explicitly mentioned in the patient's EMRs. Moreover, the fact that a symptom has not been observed by a clinician may rule out some specific diagnoses. Our work focuses on searching EMRs to identify patients with medical histories relevant to the medical condition(s) stated in a query. The resulting system can be beneficial to healthcare providers, administrators, and researchers who may wish to analyse the effectiveness of a particular medical procedure to combat a specific disease [2,4]. During retrieval, a healthcare provider may indicate a number of inclusion criteria to describe the type of patients of interest. For example, the used criteria may include personal profiles (e.g. age and gender) or some specific medical symptoms and tests, allowing to identify patients that have EMRs matching the criteria. To attain effective retrieval performance, we hypothesise that, in such a medical IR system, both the information needs and patients should be modelled based on how the medical process is developed. Specifically, our thesis states that since the medical decision process typically encompasses four aspects (symptom, diagnostic test, diagnosis, and treatment), a medical search system should take into account these aspects and apply inferences to recover possible implicit knowledge. We postulate that considering these aspects and their derived implicit knowledge at different levels of the retrieval process (namely, sentence, record, and inter-record level) enhances the retrieval performance. Indeed, we propose to build a query and patient understanding framework that can gain insights from EMRs and queries, by modelling and reasoning during retrieval in terms of the four aforementioned aspects (symptom, diagnostic test, diagnosis, and treatment) at three different levels of the retrieval process.

#index 1988987
#* Beyond relevance: on novelty and diversity in tag recommendation
#@ Fabiano Belém
#t 2013
#c 13
#% 1625357
#% 1879000
#! We propose to explicitly exploit issues related to novelty and diversity in tag recommendation tasks, an unexplored research avenue (only relevance issues have been investigated so far), in order to improve user experience and satisfaction. We propose new tag recommendation strategies to cover these issues and highlight the involved challenges.

#index 1988988
#* Effective approaches to retrieving and using expertise in social media
#@ Reyyan Yeniterzi
#t 2013
#c 13
#% 1876161
#! Expert retrieval has been widely studied especially after the introduction of Expert Finding task in the TREC's Enterprise Track in 2005 [3]. This track provided two different test collections crawled from two organizations' public-facing websites and internal emails which led to the development of many state-of-the-art algorithms on expert retrieval [1]. Until recently, these datasets were considered good representatives of the information resources available within enterprise. However, the recent growth of social media also influenced the work environment, and social media became a common communication and collaboration tool within organizations. According to a recent survey by McKinsey Global Institute [2], 29% of the companies use at least one social media tool for matching their employees to tasks, and 26% of them assess their employees' performance by using social media. This shows that intra-organizational social media became an important resource to identify expertise within organizations. In recent years, in addition to the intra-organizational social media, public social media tools like Twitter, Facebook, LinkedIn also became common environments for searching expertise. These tools provide an opportunity for their users to show their specific skills to the world which motivates recruiters to look for talented job candidates on social media, or writers and reporters to find experts for consulting on specific topics they are working on. With these motivations in mind, in this work we propose to develop expert retrieval algorithms for intra-organizational and public social media tools. Social media datasets have both challenges and advantages. In terms of challenges, they do not always contain context on one specific domain, instead one social media tool may contain discussions on technical stuff, hobbies or news concurrently. They may also contain spam posts or advertisements. Compared to well-edited enterprise documents, they are much more informal in language. Furthermore, depending on the social media platform, they may have limits on the number of characters used in posts. Even though they include the challenges stated above, they also bring some unique authority signals, such as votes, comments, follower/following information, which can be useful in estimating expertise. Furthermore, compared to previously used enterprise documents, social media provides clear associations between documents and candidates in the context of authorship information. In this work, we propose to develop expert retrieval approaches which will handle these challenges while making use of the advantages. Expert retrieval is a very useful application by itself; furthermore, it can be a step towards improving other social media applications. Social media is different than other web based tools mainly because it is dependent on its users. In social media, users are not just content consumers, but they are also the primary and sometimes the only content creators. Therefore, the quality of any user-generated content in social media depends on its creator. In this thesis, we propose to use expertise of users in order to improve the existing applications so that they can estimate the relevancy of a content not just based on the content, but also based on the expertise of the content creator. By using expertise of the content generator, we also hope to boost contents that are more reliable. We propose to apply this user's expertise information in order to improve ad-hoc search and question answering applications in social media. In this work, previous TREC enterprise datasets, available intra-organizational social media and public social media datasets will be used to test the proposed algorithms.

#index 1988989
#* The role of current working context in professional search
#@ Maya Sappelli
#t 2013
#c 13
#% 1891150
#! Today's working world of knowledge workers is changing rapidly. The available information that they need to process is ever growing. In addition, the characteristics of their work are changing as people can and do their work from home. This has resulted in the need to support knowledge workers in order to prevent burnouts. The project SWELL (http://www.swell-project.net) targets this by developing systems that support user's mental and physical well-being at work and at home. In the PhD project presented in this abstract we aim at maintaining well-being at work through information support.

#index 1988990
#* How far will you go?: characterizing and predicting online search stopping behavior using information scent and need for cognition
#@ Wan-Ching Wu
#t 2013
#c 13
#% 325203
#% 804884

#index 1988991
#* Semantic models for answer re-ranking in question answering
#@ Piero Molino
#t 2013
#c 13
#% 1251705
#% 1590304
#% 1879066
#% 1929887
#! The task of Question Answering (QA) is to find correct answers to users' questions expressed in natural language. In the last few years non-factoid QA received more attention. It focuses on causation, manner and reason questions, where the expected answer has the form of a passage of text. The presence of question and answers corpora allows the adoption of Learning to Rank (MLR) algorithms in order to out- put a sensible ranking of the candidate answers. The importance and effectiveness of linguistically motivated features, obtained from syntax, lexical semantics and semantic role labeling, was shown in literature [2-4], but there are still several different possible semantic features that have not been taken into account so far and our goal is to find out if their use could lead to performance improvement. In particular features coming from Semantic Models (SM) like Distributional Semantic Models (DSMs), Explicit Semantic Analysis (ESA), Latent Dirichlet Allocation (LDA) induced topics have never been applied to the task so far. Based on the usefulness that those models show in other tasks, we think that SM can have a significant role in improving current state-of-the-art systems' performance in answer re-ranking. The questions this research wants to answer are: 1) Do semantic features bring information that is not present in the bag-of-words and syntactic features? 2) Do they bring different information or does it overlap with that of other features? 3) Are additional semantic features useful for answer re-ranking? Does their adoption improve systems' performance? 4) Which of them is more effective and under which circumstances? We performed a preliminary evaluation of DSMs on the ResPubliQA 2010 Dataset. We built a DSM based answer scorer that represents the question and the answer as the sums of the vectors of their terms taken term-term co-occurrence matrix and calculates their cosine similarity. We replaced the term-term matrix with the ones obtained by Random Indexing (RI), Latent Semantic Analysis (LSA) and LSA over the RI. Considering each DSM on its own, the results prove that all the DSMs are better than the baseline (the standard term-term co-occurrence matrix), and the improvement is always significant. The best improvement for the MRR in English is obtained by LSA (+180%), while in Italian by LSARI (+161%). We also showed that combining the DSMs with overlap based measures via CombSum the ranking is significantly better than the baseline obtained by the overlap measures alone. For English we have obtained an improvement in MRR of about 16% and for Italian, we achieve a even higher improvement in MRR of 26%. Finally, adopting RankNet for combining the overlap features and the DSMs features, improves the MRR of about 13%. More details can be found in [1]. In order to investigate the effectiveness of the semantic features, we still need to incorporate other semantic features, such as ESA, LDA and other state-of-the-art linguistic features. Other operators for semantic compositionality, like product, tensor product and circular convolution, will also be investigated. Moreover we will experiment on different datasets, focus- ing mainly on non-factoid QA. The Yahoo! Answers Manner Questions datasets are a good starting point. A new dataset will also be collected with questions from the users of Wikiedi (a QA system over Wikipedia articles, www.wikiedi.it) and answers in the form of paragraphs from Wikipedia pages.

#index 1988992
#* Indexing and querying overlapping structures
#@ Faegheh Hasibi
#t 2013
#c 13
#% 333981
#% 1541514
#! Structural information retrieval is mostly based on hierarchy. However, in real life information is not purely hierarchical and structural elements may overlap each other. The most common example is a document with two distinct structural views, where the logical view is section/ subsection/ paragraph and the physical view is page/ line. Each single structural view of this document is a hierarchy and the components are either disjoint or nested inside each other. The overlapping issue arises when one structural element cannot be neatly nested into others. For instance, when a paragraph starts in one page and terminates in the next page. Similar situations can appear in videos and other multimedia contents, where temporal or spatial constituents of a media file may overlap each other. Querying over overlapping structures is one of the challenges of large scale search engines. For instance, FSIS (FAST Search for Internet Sites) [1] is a Microsoft search platform, which encounters overlaps while analysing content of textual data. FSIS uses a pipeline process to extract structure and semantic information of documents. The pipeline contains several components, where each component writes annotations to the input data. These annotations consist of structural elements and some of them may overlap each other. Handling overlapping structures in search engines will add a novel capability of searching, where users can ask queries such as "Find all the words that overlap two lines" or "Find the music played during Intro scene of Avatar movie". There are also other use cases, where the user of the search engine is not a person, but is a specific program with complex, non-traditional information retrieval needs. This research attempts to index overlapping structures and provide efficient query processing for large-scale search engines. The current research on overlapping structures revolves around encoding and modelling data, while indexing and query processing methods need investigations. Moreover, due to intrinsic complexity of overlaps, XML indexing and query processing techniques cannot be used for overlapping structures. Hence, my research on overlapping structures comprises three main parts: (1) an indexing method that supports both hierarchies and overlaps; (2) a query processing method based on the indexing technique and (3) a query language that is close to natural language and supports both full text and structural queries. Our approach for indexing overlaps is to adapt the PrePost [3] XML indexing method to overlapping structures. This method labels each node with its start and end positions and requires modest storage space. However, PrePost indexing cannot be used for overlapping nodes. To overcome this issue, we need to define a data model for overlapping structures. Since hierarchies are not sufficient to describe overlapping components, several data structures have been introduced by scholars. One of the most interesting data models is GODDAG [2]. GODDAG is a tree-like graph, where nodes can have multiple parentage. This model can support overlaps as well as simple inheritance. Our proposed data model for indexing overlaps is such a tree-like structure, where we can define overlapping, parent-child and ancestor-descendant relationships.

#index 1988993
#* Group-support for task-based information searching: a knowledge-based approach
#@ Thilo Boehm
#t 2013
#c 13
#% 1348064
#% 1598425
#% 1954448

#index 1988994
#* Task differentiation for personal search evaluation
#@ Seyedeh Sargol Sadeghi
#t 2013
#c 13

#index 1988995
#* Segmentation strategies for passage retrieval in audio-visual documents
#@ Petra Galuščáková
#t 2013
#c 13
#% 328532
#% 741058
#% 1261667
#% 1806005
#! The importance of Information Retrieval (IR) in audio-visual recordings has been increasing with steeply growing numbers of audio-visual documents available on-line. Compared to traditional IR methods, this task requires specific techniques, such as Passage Retrieval which can accelerate the search process by retrieving the exact relevant passage of a recording instead of the full document. In Passage Retrieval, full recordings are divided into shorter segments which serve as individual documents for the further IR setup. This technique also allows normalizing document length and applying positional information. It was shown that it can even improve retrieval results. In this work, we examine two general strategies for Passage Retrieval: blind segmentation into overlapping regular-length passages and segmentation into variable-length passages based on semantics of their content. Time-based segmentation was already shown to improve retrieval of textual documents and audio-visual recordings. Our experiments performed on the test collection used in the Search subtask of the Search and Hyperlinking Task in MediaEval Benchmarking 2012 confirm those findings and show that parameters (segment length and shift) tuning for a specific test collection can further improve the results. Our best results on this collection were achieved by using 45-second long segments with 15-second shifts. Semantic-based segmentation can be divided into three types: similarity-based (producing segments with high intra-similarity and low inter-similarity), lexical-chain-based (producing segments with frequent lexically connected words), and feature-based (combining various features which signalize a segment break in a machine-learning setting). In this work, we mainly focus on feature-based segmentation which allows exploiting various features from all modalities of the data (including segment length) in a single trainable model and produces segments which can eventually overlap. Our preliminary results show that even simple semantic-based segmentation outperforms regular segmentation. Our model is a decision tree incorporating the following features: shot segments, output of TextTiling algorithm, cue words (well, thanks, so, I, now), sentence breaks, and the length of the silence after the previous word. In terms of the MASP, the relative improvement over regular segmentation is more than 19%.

#index 1989058
#* Music similarity and retrieval
#@ Peter Knees;Markus Schedl
#t 2013
#c 13
#% 769892
#% 815915
#% 816186
#% 854646
#% 1047316
#% 1089200
#% 1190108
#% 1190207
#% 1214642
#% 1268503
#% 1355041
#% 1396086
#% 1500552
#% 1560408
#% 1572968
#% 1586590
#% 1606000
#% 1987674
#! This tutorial serves as an introductory course to the field of and state-of-the-art in music information retrieval (MIR) and in particular to music similarity estimation which is an essential component of music retrieval. Apart from explaining approaches that estimate similarity based on acoustic properties of an audio signal, we review methods that exploit (mostly textual) meta-data from the Web to build representations of music then used for similarity calculation. Additionally, topics such as (large-scale) music indexing, information extraction for music, personalization in music retrieval, and evaluation of MIR systems are addressed.

#index 1994977
#* Exploration, navigation and retrieval of information in cultural heritage: ENRICH 2013
#@ Séamus Lawless;Maristella Agosti;Paul Clough;Owen Conlan
#t 2013
#c 13
#! The Exploration, Navigation and Retrieval of Information in Cultural Heritage Workshop (ENRICH 2013) offers a forum to 1) discuss the challenges and opportunities in Information Retrieval research in the area of Cultural Heritage; 2) encourage collaboration between researchers engaged in work in this specialist area of Information Retrieval, and to foster the formation of a research community; and 3) identify a set of actions which the community should undertake to progress the research agenda. The workshop will foster a new stream of Information Retrieval research and support the design of search tools that can help end-users fully exploit the wonderful Cultural Heritage material that is available across the globe.

#index 1994978
#* SIGIR 2013 workshop on time aware information access (#TAIA2013)
#@ Fernando Diaz;Susan Dumais;Miles Efron;Kira Radinsky;Maarten de Rijke;Milad Shokouhi
#t 2013
#c 13
#! Web content increasingly reflects the current state of the physical and social world, manifested both in traditional news media sources along with user-generated publishing sites such as Twitter, Foursquare, and Facebook. At the same time, web searching increasingly reflects problems grounded in the real world. As a result of this blending of the web with the real world, we observe that the web, both in its composition and use, has incorporated many of the dynamics of the real world. Few of the problems associated with searching dynamic collections are well understood, such as defining time-sensitive relevance, understanding user query behavior over time and understanding why certain web content changes. We believe that, just as static collections often benefit from modeling topics, dynamic collections will likely benefit from temporal modeling of events and time-sensitive user interests and intents, which were rarely addressed in the literature. There have been preliminary efforts in the research and industrial communities to address algorithms, architectures, evaluation methodologies and metrics. We aim to bring together practitioners and researchers to discuss their recent breakthroughs and the challenges with addressing time-aware information access, both from the algorithmic and the architectural perspectives. This workshop is a successor to the successful SIGIR 2012 Workshop on Time Aware Information Access (#TAIA2012). Where the 2012 edition was the first to bring together a broad set of academic and industrial researchers around the topic of time-aware information access, the specific focus of this workshop is on the many time-aware benchmarking activities that are ongoing in 2013.

#index 1996879
#* Riding the multimedia big data wave
#@ John R. Smith
#t 2013
#c 13
#% 888942
#% 1279579
#% 1632170
#% 1649019
#% 2000479
#! In this talk we present a perspective across multiple industry problems, including safety and security, medical, Web, social and mobile media, and motivate the need for large-scale analysis and retrieval of multimedia data. We describe a multi-layer architecture that incorporates capabilities for audio-visual feature extraction, machine learning and semantic modeling and provides a powerful framework for learning and classifying contents of multimedia data. We discuss the role semantic ontologies for representing audio-visual concepts and relationships, which are essential for training semantic classifiers. We discuss the importance of using faceted classification schemes in particular for organizing multimedia semantic concepts in order to achieve effective learning and retrieval. We also show how training and scoring of multimedia semantics can be implemented on big data distributed computing platforms to address both massive-scale analysis and low-latency processing. We describe multiple efforts at IBM on image and video analysis and retrieval, including IBM Multimedia Analysis and Retrieval System (IMARS), and show recent results for semantic-based classification and retrieval. We conclude with future directions for improving analysis of multimedia through interactive and curriculum-based techniques for multimedia semantics-based learning and retrieval.

#index 2006697
#* Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval
#@ Gareth J.F. Jones;Páraic Sheridan;Diane Kelly;Maarten de Rijke;Tetsuya Sakai
#t 2013
#c 13
#! Welcome to SIGIR, the 36th annual international ACM conference on research and development in Information Retrieval. SIGIR is the premier, international venue for research and development in information retrieval. We believe the breadth and diversity of research that comprises the program reflects the health of the organization and major future directions of the field. We are grateful to all those who submitted papers to the conference and gave the Committee an opportunity to evaluate their work for potential inclusion in the program. We are also grateful to the 50 Area Chairs and 204 general program committee members, who represent 30 countries and over 120 institutions, for all the hard work they put into evaluating submissions. The conference received 366 full paper submissions this year. Of these, 73 (20%) were accepted, essentially the same as last year's acceptance rate and the year before. The top five countries in terms of accepted papers (according to contact author affiliation) were the U.S.A. (28), China (9), the Netherlands, Singapore, and U.K. (5 each). The top five technical areas covered by the accepted papers (as indicated by the primary keyword assigned by paper authors) were users and interactive IR (16%), search engine architecture and scalability (15%), queries and query analysis (15%), evaluation (11%), and retrieval models and ranking (11%). This represents only a slight re-ordering of topics from last year. Two hundred fifty papers were submitted to the short papers track, which represents a 20% increase in the number of submissions made to last year's poster track. Eighty-five (34%) short papers were accepted. In addition, 46 demonstrations were proposed, of which 23 (50%) were accepted. The program also consisted of 7 workshops and 10 tutorials. Finally, the Doctoral Consortium hosted 11 students this year from 10 countries and 11 institutions. As has been customary for many years, SIGIR 2013 used a two-tier double-blind review process. In the first stage, at least three reviewers read every paper and provided ratings and comments. Papers were evaluated according to seven main criteria: relevance, originality, soundness, quality of the presentation, impact, coverage of the literature, and, for the first time, reproducibility of the results. In the second stage, the primary and secondary Area Chairs ensured the quality of the reviewing process by studying, validating, and summarizing these reviews, and adding their own feedback and ratings. Area Chairs initiated discussions among reviewers to resolve any controversial issues or significant differences of opinion. Once the discussion stage was completed, the two Area Chairs made a recommendation regarding the paper for nearly all submissions. This year we allowed Area Chairs to indicate that a paper should be accepted if room. At the program committee meeting held in Amsterdam, The Netherlands, the Program Chairs and the attending Area Chairs went over the reviews, verified the process, gathered additional input, and discussed and decided on papers that were balloted as accept if room, papers from which the primary Area Chair abstained and papers that had unusual score distributions. For papers that were balloted as accept if room, we especially considered the potential for the paper to provoke interesting and fruitful discussion at the conference. Ultimately 73 papers were selected for inclusion in the program. One important change to this year's program was renaming the poster paper submission type to short papers and increasing the length of the paper from two to four pages. Short papers were presented at the conference in poster format and two separate short paper sessions were included as part of the main conference program, rather than a single event collocated with an evening reception. We believe that increasing the length of the accompanying paper allows researchers to better communicate their experiments and results, which in turn, will allow this submission type to function as a more comprehensive and substantial container for small, but significant findings. We further believe this change better allows research presented in this format to get the attention it deserves. We would like to thank the Short Paper Co-Chairs for all the extra work they did this year managing this new format and the Short Paper reviewers for the great job they did handling both the larger volume of submissions and their increased size. We believe the large increase in number of submissions to this track indicates the community's receptiveness to this change. We hope you find this program interesting, provocative and inspiring, and that the conference provides you with a valuable opportunity to share ideas with other researchers, practitioners and students from institutions around the world. The deadline for SIGIR 2014 is, after all, only six months away!

